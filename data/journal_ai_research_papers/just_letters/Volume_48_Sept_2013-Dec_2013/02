journal artificial intelligence research                 

submitted       published      

survey multi objective sequential decision making
diederik m  roijers

d m roijers uva nl

informatics institute
university amsterdam
amsterdam  netherlands

peter vamplew

p vamplew ballarat edu au

school science 
information technology engineering
university ballarat
ballarat  victoria  australia

shimon whiteson

s a whiteson uva nl

informatics institute
university amsterdam
amsterdam  netherlands

richard dazeley

r dazeley ballarat edu au

school science 
information technology engineering
university ballarat
ballarat  victoria  australia

abstract
sequential decision making problems multiple objectives arise naturally practice pose unique challenges research decision theoretic planning learning 
largely focused single objective settings  article surveys algorithms designed sequential decision making problems multiple objectives  though
growing body literature subject  little makes explicit circumstances special methods needed solve multi objective problems  therefore 
identify three distinct scenarios converting problem single objective
one impossible  infeasible  undesirable  furthermore  propose taxonomy
classifies multi objective methods according applicable scenario  nature
scalarization function  which projects multi objective values scalar ones   type
policies considered  show factors determine nature optimal solution  single policy  convex hull  pareto front  using taxonomy 
survey literature multi objective methods planning learning  finally 
discuss key applications methods outline opportunities future work 

   introduction
sequential decision problems  commonly modeled markov decision processes  mdps 
 bellman      a   occur range real world tasks robot control  kober  
peters         game playing  szita         clinical management patients  peek        
military planning  aberdeen  thiebaux    zhang         control elevators  crites
  barto         power systems  ernst  glavic    wehenkel         water supplies
 bhattacharya  lobbrecht    solomantine         therefore  development algorithms
c
    
ai access foundation  rights reserved 

firoijers  vamplew  whiteson   dazeley

automatically solving problems  either planning given model mdp  e g  
via dynamic programming methods  bellman      b  learning interaction
unknown mdp  e g   via temporal difference methods  sutton   barto        
important challenge artificial intelligence 
research topics  desirability undesirability actions
effects codified single  scalar reward function  typically  objective
autonomous agent interacting mdp maximize expected  possibly
discounted  sum rewards time  many tasks  scalar reward function
natural  e g   financial trading agent could rewarded based monetary gain
loss holdings recent time period  however  many tasks
naturally described terms multiple  possibly conflicting objectives  e g  
traffic control system minimize latency maximize throughput  autonomous
vehicle minimize travel time fuel costs  multi objective problems
widely examined many areas decision making  zeleny   cochrane        vira  
haimes        stewart        diehl   haimes        roijers  whiteson    oliehoek       
growing  albeit fragmented  literature addressing multi objective decisionmaking sequential settings 
article  present survey algorithms devised
settings  begin section   formalizing problem multi objective mdp
 momdp   then  section    motivate multi objective perspective decisionmaking  little existing literature multi objective algorithms makes explicit
multi objective approach beneficial and  crucially  cases cannot trivially reduced
single objective problem solved standard algorithms  address this 
describe three motivating scenarios multi objective algorithms 
then  section    present novel taxonomy organizes multi objective problems
terms underlying assumptions nature resulting solutions  key
difficulty existing literature authors considered many different types
problems  often without making explicit assumptions involved  differ
authors  scope applicability resulting methods  taxonomy
aims fill void 
sections     survey momdp planning learning methods  respectively  organizing according taxonomy identifying key differences
approaches examined planning learning areas  section   surveys applications
methods  covering specific applications general classes problems
momdp methods applied  section   discusses future directions field
based gaps literature identified sections      section   concludes 

   background
finite single objective markov decision process  mdp  tuple hs  a  t  r    where 
finite set states 
finite set actions 
         transition function specifying  state  action 
next state  probability next state occurring 
  

fia survey multi objective sequential decision making

r   reward function  specifying  state  action  next
state  expected immediate reward 
         probability distribution initial states 
       discount factor specifying relative importance immediate rewards 
goal agent acts environment maximize expected return
rt   function rewards received timestep onwards  typically 
return additive  boutilier  dean    hanks         i e   sum rewards 
infinite horizon mdp  return typically infinite sum  term discounted
according  

x
k rt k    
rt  
k  

rt reward obtained time t  parameter thus quantifies relative
importance short term long term rewards 
contrast  finite horizon mdp  return typically undiscounted finite sum 
i e   certain number timesteps  process terminates reward
obtained  single  multi objective methods developed finite horizon 
discounted infinite horizon  average reward settings  puterman         sake
brevity formalize infinite horizon discounted reward mdps article  
agents policy determines actions selects timestep  broadest
sense  policy condition everything known agent  state indepedent
value function v specifies expected return following initial state 
v   e r      

   

policy stationary  i e   conditions current state 
formalized           specifies  state action  probability
taking action state  specify state value function policy  
v  s    e rt     st   s  
st   s  bellman equation restates expectation recursively
stationary policies 
x
x
 s  a    r s  a      v  s    
v  s   
 s  a 




note bellman equation  forms heart standard solution algorithms
dynamic programming  bellman      b  temporal difference methods  sutton
  barto         explicitly relies assumption additive returns  important
because  explain section        multi objective settings interfere
additivity property  making planning learning methods rely bellman
equation inapplicable 
   formalizations settings  see example overview van otterlo wiering        

  

firoijers  vamplew  whiteson   dazeley

state value functions induce partial ordering policies  i e   better
equal value greater states 


s  v  s  v  s  
special case stationary policy deterministic stationary policy  one
action chosen probability   every state  deterministic stationary policy
seen mapping states actions    a  single objective mdps 
always least one optimal policy   i e       stationary deterministic 
theorem    additive infinite horizon single objective mdp  exists deterministic stationary optimal policy  see e g   howard        boutilier et al         
one optimal policy exists  share value function  known
optimal value function v  s    max v  s   bellman optimality equation defines
optimal value function recursively 
x
 s  a    r s  a      v  s    
v  s    max




note that  maximizes actions  equation makes use fact
optimal deterministic stationary policy  optimal policy maximizes
value every state  policy optimal regardless initial state distribution  
however  state independent value  equation    may well different different
initial state distributions  using   state value function translated back
state independent value function  equation    
x
v  
 s v  s  
ss

multi objective mdp  momdp   mdp reward function r  
n describes vector n rewards  one objective  instead scalar 
similarly  value function v momdp specifies expected cumulative discounted
reward vector 

x
k rk       
   
v   e 
k  

rt vector rewards received time t  difference single
objective value  equation    multi objective value  equation    policy
return  underlying sum rewards  vector rather scalar 
stationary policies  define multi objective value state 
v  s    e 


x

k rt k       st   s  

   

k  

single objective mdp  state value functions impose partial ordering

policies compared different states  e g   possible v  s    v  s  v  s    
   multi objective mdps confused mixed observability mdps  ong  png  hsu    lee 
       sometimes abbreviated momdp 

  

fia survey multi objective sequential decision making



v  s    given state  ordering complete  i e   v  s  must greater than 

equal to  less v  s   true state independent value functions 
contrast  momdp  presence multiple objectives means value
function v  s  state vector expected cumulative rewards instead scalar 
value functions supply partial ordering  even given state  example 


possible that  state s  vi  s    vi  s  vj  s    vj  s   similarly 


state independent value functions  may vi   vi vj   vj   consequently 
unlike mdp  longer determine values optimal without additional
information prioritize objectives  information provided
form scalarization function  discuss following sections 
though focus article  momdp variants constraints
specified objectives  see e g   feinberg   shwartz        altman        
goal agent maximize regular objectives meeting constraints
objectives  constrained objectives fundamentally different regular
objectives explicitly prioritized regular objectives  i e   policy
fails meet constraint inferior policy meets constraints  regardless
well policies maximize regular objectives 

   motivating scenarios
momdp setting received considerable attention  immediately obvious useful addition standard mdp specialized algorithms
needed  fact  researchers argue modeling problems explicitly multiobjective necessary  scalar reward function adequate sequential
decision making tasks  direct formulation perspective suttons reward
hypothesis  states mean goals purposes well
thought maximization expected value cumulative sum received scalar
signal  reward   
view imply multi objective problems exist  indeed 
would difficult claim  since easy think problems naturally possess
multiple objectives  instead  implication reward hypothesis resulting
momdps always converted single objective mdps additive returns 
conversion process would involve two steps  first step specify scalarization
function 
definition    scalarization function f   function projects multi objective
value v scalar value 
vw  s    f  v  s   w  
w weight vector parameterizing f  
example  f may compute linear combination values  case element
w quantifies relative importance corresponding objective  this setting discussed section         second step define single objective mdp
   http   rlai cs ualberta ca rlai rewardhypothesis html

  

firoijers  vamplew  whiteson   dazeley

figure    three motivating scenarios momdps   a  unknown weights scenario 
 b  decision support scenario   c  known weights scenario 

additive returns that  s  expected return equals scalarized value
vw  s  
though rarely  ever  makes issue explicit  research momdps rests
premise exist tasks one conversion steps impossible 
infeasible  undesirable  section  discuss three scenarios occur
 see figure    
first scenario  call unknown weights scenario  figure  a   occurs
w unknown moment planning learning must occur  consider example
public transport system aims minimize latency  i e   time commuters
need reach destinations  pollution costs  addition  assume resulting
momdp scalarized converting objective monetary cost  economists
compute cost lost productivity due commuting pollution incurs tax
must paid pollution credits purchased given price  assume credits
traded open market therefore price constantly fluctuates  transport
system complex  may infeasible compute new plan every day given latest
prices  scenario  preferable use multi objective planning method
computes set policies that  price  one policies optimal
 see planning learning phase figure  a   computationally
expensive computing single optimal policy given price  needs done
done advance  computational resources available 
then  time select policy  current weights  i e   price pollution
  

fia survey multi objective sequential decision making

credits  used determine best policy set  the selection phase   finally 
selected policy employed task  the execution phase  
unknown weights scenario  scalarization impossible planning learning
trivial policy actually needs used w known time 
contrast  second scenario  call decision support scenario  figure
 b   scalarization infeasible throughout entire decision making process
difficulty specifying w  even f   example  economists may able accurately
compute cost lost productivity due commuting  user may fuzzy
preferences defy meaningful quantification  example  transport system could
made efficient building new train line obstructs beautiful view 
human designer may able quantify loss beauty  difficulty specifying
exact scalarization especially apparent designer single person
committee legislative body whose members different preferences agendas 
system  momdp method used calculate optimal solution set
respect known constraints f w  figure  b shows  decision support
scenario proceeds similarly unknown weights scenario except that  selection
phase  user users select policy set according arbitrary preferences 
rather explicit scalarization according given weights 
cases  one still argue scalarization planning learning
possible principle  example  loss beauty quantified measuring
resulting drop housing prices neighborhoods previously enjoyed unobstructed
view  however  difficulty scalarization may impractical
but  importantly  forces users express preferences way may
inconvenient unnatural  selecting w requires weighing hypothetical
trade offs  much harder choosing set actual alternatives 
well understood phenomenon field decision analysis  clemen        
standard workflow involves presenting alternatives soliciting preferences 
subfields decision analysis multiple criteria decision making multiattribute utility theory focus multiple objectives  dyer  fishburn  steuer  wallenius   
zionts         reasons  algorithms momdps provide critical decision
support  rather forcing users specify w advance  algorithms
prune policies would optimal w  then  offer users range
alternatives select according preferences whose relative importance
easily quantified 
third scenario  call known weights scenario  figure  c   assume
w known time planning learning thus scalarization possible
feasible  however  may undesirable difficulty second step
conversion  particular  f nonlinear  resulting single objective mdp
may additive returns  see section         result  optimal policy may
non stationary  see section        stochastic  see section         cannot occur
single objective  additive  infinite horizon mdps  see theorem     consequently  mdp
difficult solve  standard methods applicable  converting mdp
one additive returns may help either cause blowup state space 
  

firoijers  vamplew  whiteson   dazeley

leaves problem intractable   therefore  even though scalarization possible
w known  may still preferable use methods specially designed momdps
rather convert problem single objective mdp  contrast unknown
weights decision support scenarios  known weights scenario  momdp
method produces one policy  executed  i e   separate selection
phase  shown figure  c 
note figure   assumes off line scenario  planning learning occurs once 
execution  however  multi objective methods employed on line settings
planning learning interleaved execution  on line version
unknown weights scenario  weights better characterized dynamic  rather
unknown  on line scenario  agent must already seen weights timesteps
    since prerequisite execution timesteps               however 
weights change time  agent may yet know weights used
timestep planning learning phase timestep 

   problem taxonomy
far  described momdp formalism proposed three motivating scenarios
it  section  discuss constitutes optimal solution  unfortunately 
simple answer question  depends several critical factors  therefore 
propose problem taxonomy  shown table    categorizes momdps according
factors describes nature optimal solution category 
taxonomy based call utility based approach  contrast many
multi objective papers follow axiomatic approach optimality momdps 
utility based approach rests following premise  execution phases
scenarios section    one policy selected collapsing value vector policy
scalar utility  using scalarization function  application scalarization
function may implicit hidden  e g   may embedded thought process
user  nonetheless occurs  scalarization function part notion utility 
i e   agent maximize  therefore  find set optimal solution
possible weight setting scalarization function  solved momdp 
utility based approach derives optimal solution set assumptions
made scalarization function  policies user allows  whether need
one multiple policies 
contrast  axiomatic approach begins axiom optimal solution set
pareto front  see section          approach limiting because  demonstrate
section  settings solution concepts suitable 
thus  take utility based approach makes possible derive solution
concept  rather assuming it  pareto front fact correct solution
   since non additive returns depend agents entire history  immediate reward function
converted mdp may depend history thus state representation converted
mdp must augmented include it 
   example axiomatic approach multi objective reinforcement learning  see survey
liu  xu  hu        

  

fia survey multi objective sequential decision making

single policy
 known weights 
deterministic
linear
scalarization

multiple policies
 unknown weights decision support 

stochastic

one deterministic stationary
policy    

monotonically one
increasing
deterministic
scalarization
non stationary
policy    

deterministic

stochastic

convex coverage set
deterministic stationary policies
   

one mixture
policy two

deterministic
stationary
policies    

pareto
coverage set
deterministic
non stationary
policies    

convex
coverage set
deterministic
stationary
policies    

table    momdp problem taxonomy showing critical factors problem
nature resulting optimal solution  columns describe whether
problem necessitates single policy multiple ones  whether policies
must deterministic  by specification  allowed stochastic  rows
describe whether scalarization function linear combination rewards
or  whether cannot assumed scalarization function merely
monotonically increasing function them  contents cell describe
optimal solution given setting looks like 

concept  utility based approach provides justification it  not  allows
appropriate solution concept derived instead 
taxonomy categorizes problem classes based assumptions scalarization function  policies user allows  whether one multiple policies
required  show leads different solution concepts  underscoring importance
carefully considering choice solution concept based available information 
discuss three factors constitute taxonomy following order 
section      discuss first factor  whether one multiple policies sought  choice
follows directly motivating scenario applicable  known weights
scenario  figure  c  implies single policy approach unknown weights decision
support scenarios  figure  a  b  imply multiple policy approach  section     
discuss second factor  whether scalarization function linear combination
rewards merely monotonically increasing function them  section      discuss
third factor  whether stochastic deterministic policies permitted 
goal taxonomy cover research momdps remaining simple
intuitive  however  due diversity research momdps  research
fit neatly taxonomy  note discrepancies discussing research
sections     
  

firoijers  vamplew  whiteson   dazeley

    single versus multiple policies
following approach vamplew et al          first distinguish problems
one policy sought ones multiple policies sought  case holds
depends three motivating scenarios discussed section   applies 
unknown weights decision support scenarios  solution momdp
consists multiple policies  though two scenarios conceptually quite different 
algorithmic perspective identical  reason characterized strict separation decision making process two phases  planning
learning phase execution phase  though on line settings  agent may go
back forth two  
planning learning phase  w unavailable  consequently  planning learning algorithm must return single policy set policies  and corresponding
multi objective values   set contain policies suboptimal
scalarizations  i e  interested undominated policies 
definition    momdp scalarization function f   set undominated
policies  u  m    subset possible policies exists w
scalarized value maximal 


u  m         w    vw vw   

   

u  m   sufficient solve m  i e   w  contains policy optimal
scalarized value  however  may contain redundant policies that  optimal
weights  optimal policy set w  policies removed
still ensuring set contains optimal policy w  fact  order solve
m  need subset undominated policies that  possible w 
least one policy set optimal  sometimes called coverage set  cs   becker 
zilberstein  lesser    goldman        
definition    momdp scalarization function f   set cs m  
coverage set subset u  m   if  every w  contains policy maximal
scalarized value  i e   if 









cs    u      w    cs        vw vw  
   
note u  m   automatically coverage set  however  u  m   unique  cs m  
need be  multiple policies value  u  m   contains
them  coverage set need contain one  addition  given cs m   

may exist policy
  cs m   v different v cs m  
scalarized value cs m   w optimal 
contrast single objective mdps  momdps whether policy cs m  
depend initial state distribution   thus important accurately specify
formulating momdp 
ideally  momdp algorithm find smallest cs m    however 
might harder finding one smaller u  m    section      specialize
coverage set two classes scalarization functions 
  

fia survey multi objective sequential decision making

execution phase  single policy chosen set returned planning
learning phase executed  unknown weights scenario  assume w revealed
planning learning complete execution begins  selecting policy
requires maximizing scalarized value policy returned set 
  argmax vw  
cs m  

decision support scenario  set manually inspected user s   select
policy execution informally  making implicit trade off objectives 
known weights scenario  w known planning learning begins  therefore 
returning multiple policies unnecessary  however  mentioned section   discussed
section        scalarization yield single objective mdp difficult
solve 
    linear versus monotonically increasing scalarization functions
second critical factor affecting constitutes optimal solution momdp
nature scalarization function  section  discuss two types scalarization
function  linear combinations rewards merely
monotonically increasing functions them 
      linear scalarization functions
common assumption scalarization function  e g   natarajan   tadepalli       
barrett   narayanan         f linear  i e   computes weighted sum
values objective 
definition    linear scalarization function computes inner product weight vector
w value vector v
vw   w v  
   
element w specifies much one unit value corresponding objective
contributes scalarized value  elements weight vector w positive real
numbers constrained sum   
linear scalarization functions simple intuitive way scalarize  one common
situation applicable rewards easily translated monetary
value  example  consider mining task different policies yield different expected
quantities various minerals  prices per kilo minerals fluctuate daily 
task formulated momdp  objective corresponding different
mineral  element v reflects expected number kilos mineral
mined scalarized value vw corresponds monetary value
everything mined  vw computed w  corresponding
 normalized  current price per kilo mineral  becomes known 
single policy setting  w known  presence multiple objectives poses
difficulties given linear f   instead  f simply applied reward vector
  

firoijers  vamplew  whiteson   dazeley

momdp  inner product computed f distributes addition  result
single objective mdp additive returns  infinite horizon setting leads to 
vw   w v   w e 


x

k rt k       e 


x

k  w rt k      

   

k  

k  

since single objective mdp additive returns  solved standard methods  yielding single policy  reflected box labeled     table    due theorem
   determinstic stationary policy suffices  however  multi objective approach still
preferable case  e g   v may easier estimate vw large continuous
momdps function approximation required  see section      
multiple policy setting  however  know w planning learning
therefore want find coverage set  f linear  u  m    automatically
coverage set  consists convex hull  substituting equation   definition
undominated set  definition     obtain definition convex hull 
definition    momdp m  convex hull  ch  subset
exists w linearly scalarized value maximal 


ch m         w    w v w v   

   

figure  a illustrates concept convex hull stationary deterministic policies 
point plot represents multi objective value given policy two objective
momdp  axes represent reward dimensions  convex hull shown set
filled circles  connected lines form convex surface   given linear f  
scalarized value policy linear function weights  illustrated
figure  b  x axis represents weight dimension    w        w     
y axis scalarized value policies  select policy  need know
values convex hull policies  form upper surface scalarized value 
illustrated black solid lines  correspond three convex hull policies
figure  a  upper surface forms piecewise linear convex function  functions
well known literature partially observable markov decision processes
 pomdps   whose relationship momdps discuss section     
u  m    ch m   contain superfluous policies  however  define
convex coverage set  ccs  specification coverage set f linear 
reflected box     table    we explain policies set deterministic
stationary section        
definition    momdp m  set ccs m   convex coverage set
subset ch m   if  every w  contains policy whose linearly scalarized value
maximal  i e   if 



ccs m   ch m    w    ccs m       w v w v      
   note term convex hull slightly different meaning multi objective literature
standard geometric definition  geometry  convex hull finite set points euclidean
space minimal subset points expressed convex
combination points convex hull  multi objective setting  interested
particular subset geometric convex hull  points convex combinations strictly
bigger  in dimensions  point s  i e   points optimal weight 

  

fia survey multi objective sequential decision making

 a 

 b 

figure    example convex hull pareto front  point  a  represents
multi objective value given policy line  b  represents linearly
scalarized value policy across values w  convex hull shown black
filled circles  a   black lines  b   pareto front consists filled
points  circles squares   a   dashed solid black lines
 b   unfilled points  a   grey lines  b   dominated 

deterministic stationary policies  difference ch m   ccs m   may
often small  therefore  terms often used interchangeably  however  case
non stationary stochastic policies  difference quite significant  ch
contain infinitely many policies  possible construct finite ccs  show
section       
      monotonically increasing scalarization functions
linear scalarization functions intuitive simple  always adequate
expressing users preferences  example  suppose mining task mentioned
above  two minerals mined three policies available   
sends mining equipment location first mineral mined   
location second mineral mined    location
minerals mined  suppose owner equipment prefers     e g  
least partially appeases clients different interests  however  may case that 
location corresponding   fewer minerals  convex hull contains
      thus  owners preference   implies she  implicitly explicitly 
employs nonlinear scalarization function 
here  consider case f nonlinear  corresponds common
notion relationship reward utility  class possibly nonlinear scalarizations strictly monotonically increasing scalarization functions  functions
adhere constraint policy changed way value increases
  

firoijers  vamplew  whiteson   dazeley

one objectives  without decreasing objectives  scalarized
value increases 
definition    scalarization function f strictly monotonically increasing if 






 i  vi vi i  vi   vi    w  vw   vw   

    

linear scalarization functions  with non zero positive weights  included class
functions  condition left hand side equation    commonly known
pareto dominance  pareto        
definition    policy pareto dominates another policy value least
high objectives strictly higher least one objective 






v p v i  vi vi i  vi   vi  

    

demanding f strictly monotonically increasing quite minimal constraint 
requires that  things equal  getting reward certain objective
always better  fact  difficult think f violates constraint without
employing highly unnatural notion reward  
three observations order strictly monotonically increasing scalarization
functions related concept pareto dominance  first  unlike linear case 
necessarily know exact shape f   instead  know belongs
particular class functions  solution concept follows thus applies strictly
monotonically increasing f   cases stronger assumptions f made 
specific solution concepts possible  however  except linearity  aware
properties f exploited solving momdps 
second  notions optimality introduced section       longer appropriate 
reason that  even though vector valued returns still additive  equation    
scalarized returns may f may longer linear  example  consider
well known tchebycheff scalarization function  perny   weng          
x
 v   p  w    max wi  pi vi  
wi  pi vi   
    
i    n

i    n

p optimistic reference point  w weights  arbitrarily small positive
constant greater    note sum righthand side makes function
strictly monotonically increasing  now  p          p         r            r           
k
w                   f  v   w      e 
k   f  rt k     w              
                loss additivity scalarized returns applying nonlinear
f important consequences methods applied  show section       
third  still identify prune policies optimal w
strictly monotonically increasing f   even though may nonlinear  consider three
   addition  f strictly monotonically increasing assumptions made 
policies pruned coverage set  thus  computing value every policy coverage
set  required selection phase  likely intractable 
   definition differs slightly perny weng         multiplied   express
maximization instead minimization  sake consistency rest article 

  

fia survey multi objective sequential decision making

labeled policies figure  a  note figure  b apply  scalarization
function longer linear   b higher value one objective  lower
value other  therefore cannot tell whether b ought preferred without
knowing w  however  c lower value objectives  thus paretodominates c  p c  f strictly monotonically increasing  scalarized value
greater c w thus discard c 
now  defer full discussion constitutes optimal solution
momdp strictly monotonically increasing scalarization function  i e   boxes        
table    depends  whether single multiple policy setting
applies  whether deterministic stochastic policies considered 
addressed section     
however  already observe that  given strictly monotonically increasing f  
use pareto front set viable policies  pareto front consists policies
pareto dominated 
definition    momdp m  pareto front set policies
pareto dominated policy  


p f  m              v p v   

    

note p f  m   set undominated policies u  m   specific strictly
monotonically increasing f   already seen special case linear f  
u  m     ch m    subset p f  m     for example  figure    pareto
front consists convex hull plus b   however  strictly monotonically increasing
f   know policy p f  m   dominated respect f   i e  
  p f  m     u  m    because  strictly monotonically increasing f

  p f  m    cannot exist w optimal  since definition exists

v p v and  since f strictly monotonically increasing  implies

vw   vw  
however  know f strictly monotonically increasing  cannot settle
subset p f  m   either  exist strictly monotonically increasing f
u  m     p f  m    perny weng        show u  m     p f  m  
tchebycheff function  equation      strictly monotonically increasing  therefore 
cannot discard policies p f  m   retain undominated set u  m  
strictly monotonically increasing f  
pareto coverage set  pcs  minimal size constructed retaining one
policy policies identical vector values p f  m    formally define
pcs follows 
definition     momdp m  set p cs m   pareto coverage set subset
p f  m   if  every policy   contains policy either dominates
equal value   i e   if 




p cs m   p f  m         p cs m    v p v v   v         
again  deterministic stationary policies difference p cs m   p f  m  
may minor  note p f  m   automatically p cs m    papers
literature therefore take p f  m   solution 
  

firoijers  vamplew  whiteson   dazeley

slightly relax constraint f   without change policies
p cs m    specifically  define monotonically increasing scalarization

function function following property holds   i  vi vi    w  vw

vw    relaxation influences set undominated policies  policies
p f  m   always dominated strictly monotonically increasing f   need
monotonically increasing f   consider example f  v   w      
monotonically increasing strictly monotonically increasing  function
dominated policies  every policy scalarized value  however 
scalarized value policy   p f  m   cannot greater scalarized
function policy p cs m    use p cs m    non strict  monotonically
increasing f   therefore  article  focus monotonically increasing f  
broader class functions 
p f  m    even p cs m    may prohibitively large contain
many policies whose values differ negligible amounts  chatterjee et al         brazdil
et al         introduce slack parameter   use define  approximate pareto
front  p f  m    p f  m   contains values policies every possible policy

policy p f  m   vi  s    vi  s   weakening
requirements domination  approach yields smaller set calculated
efficiently 
another option finding smaller set p f  m   making additional assumptions
scalarization function  example  perny  weng  goldsmith  hanna       
introduce notion fairness objectives  leading lorentz optimality 
additional assumption sum values objectives stays same 
making difference two objectives smaller yields higher scalarized value 
course strong assumption apply broadly pareto optimality  however 
apply  help reduce size optimal solution set 
    deterministic versus stochastic policies
third critical factor affecting constitutes optimal solution momdp
whether deterministic polices considered stochastic ones allowed 
applications reason exclude stochastic policies priori 
cases stochastic policies clearly undesirable even unethical  example 
policy determines clinical treatment patient  e g   work lizotte  bowling 
murphy        shortreed  laber  lizotte  stroup  pineau  murphy        
flipping coin determine course action may inappropriate  denote

set deterministic policies
set stationary policies   sets subsets




policies    finally set policies deterministic


stationary intersection sets  denoted
ds    
single objective mdps  factor critical because  due theorem   
restrict search deterministic stationary policies  i e  optimal attainable value

v   however 
attainable deterministic stationary policy  maxm v   max



ds

situation complex momdps  section  discuss focus
stochastic deterministic policies affects setting considered taxonomy 
  

fia survey multi objective sequential decision making

      deterministic stochastic policies linear scalarization
functions
f linear  result similar theorem   holds momdps due following
corollary 

corollary    momdp m  ccs m
ds   ccs    

proof  f linear  translate momdp single objective mdp 
possible w  done treating inner product reward vector w
new rewards  leaving rest problem is  since inner product distributes
addition  scalarized returns remain additive  equation     thus  every w
exists translation single objective mdp  optimal deterministic
stationary policy must exist  due theorem    hence  w exists optimal
deterministic stationary policy  therefore  exists ccs m
ds   optimal



w  consequently  cannot exist   ds w v   w v

thus ccs m
ds   ccs    
ccs m
ds   thus sufficient solving momdps linear f   even stochastic non stationary policies allowed  reflected box     table   
applies box     since optimal policy case member ccs m
ds   
i e   one best given known w 
unfortunately  result analogous corollary   holds momdps monotonically increasing f   rest section  discuss consequences
nature optimal momdp solution boxes         table   
      multiple deterministic policies monotonically increasing
scalarization functions
multiple policy setting deterministic policies allowed f nonlinear 
non stationary policies may better best stationary ones 
theorem    infinite horizon momdps  deterministic non stationary policies paretodominate deterministic stationary policies undominated deterministic stationary policies  white        
see why  consider following momdp  denoted m   adapted example
white         one state three actions a    a    a    yield rewards
                        respectively  allow deterministic stationary policies 
three possible policies           m 
ds   corresponding always taking
one actions  pareto optimal  policies following
state independent values  equation     v                 v                
v                      however  consider set possibly non stationary
m 
m 
policies m 
 including non stationary ones   construct policy ns   ds
alternates a  a    starting a    whose value vns        
                consequently  ns p         thus cannot restrict
  

firoijers  vamplew  whiteson   dazeley

attention stationary policies   consequently  multiple deterministic policies case
monotonically increasing f   need find p cs m
   includes non stationary
policies  shown box     table   
addition consider broader class policies  another consequence
defining policy indirectly via value function longer possible  standard
single objective methods  optimal policy found local action selection
respect value function  i e   every state  policy selects action
maximizes expected value  however  local selection yield non stationary policy  value function must non stationary  i e   must condition current
timestep  standard finite horizon setting  different value function computed timestep  possible infinite horizon setting 
discuss address difficulty sections     
      multiple stochastic policies monotonically increasing
scalarization functions
multiple policy setting stochastic non stationary policies  i e   full set  
allowed  cannot consider deterministic stationary policies  however 
employ stochastic stationary policies instead deterministic non stationary ones 
particular  employ mixture policy  vamplew  dazeley  barker    kelarev       
takes set n deterministic
policies  selects i th policy set 
p
probability pi   n
p
 
  
leads values linear combination
i  
values constituent policies  previous example  replace ns
policy chooses   probability p    otherwise  resulting following
values 


 p      p   

 
 
v   p  v      p   v  
 
 
 
 
fortunately  necessary explicitly represent entire p cs m   explicitly 
instead  sufficient compute ccs m
ds    necessary stochastic policies
create p cs m   easily constructed making mixture policies
policies ccs m
ds   
corollary    infinite horizon discounted momdp  infinite set mixture policies
pm constructed policies ccs m
ds    set pm  

p cs     vamplew et al         
proof  construct policy value vector convex surface  e g  
   thereblack lines figure  a  mixing policies ccs m
ds    e g   black dots 
fore  always construct mixture policy dominates policy value
surface  e g   b  show contradiction cannot policy
   white        shows infinite horizon discounted setting  arguments hold
finite horizon average reward settings 
    note always mix policies adjacent  line pair policies
mix convex surface  e g  mixing policy represented leftmost black dot
figure  a policy represented rightmost black dot lead optimal policies 
line connecting two points convex surface 

  

fia survey multi objective sequential decision making

convex surface  was  would optimal w f linear  consequently  corollary    would deterministic stationary policy least
equal value  since convex surface spans values ccs m
ds    leads
contradiction  thus  policy pareto dominate mixture policy convex
surface 
thanks corollary    sufficient compute ccs m
ds   solve momdp 
reflected box     table    surprising consequence fact  knowledge
made explicit literature  pareto optimality  though common
solution concept associated multi objective problems  actually necessary one
specific problem setting 
observation    multiple policy setting f monotonically increasing
deterministic policies considered  box     table     requires computing pareto coverage set  either f linear stochastic policies allowed  ccs m
ds   suffices 
wakuta        proves sufficiency ccs m
ds   monotonically increasing
scalarizations multiple stochastic policies  box     table    infinite horizon
momdps  different way  instead mixture policies corollary    uses stationary randomizations deterministic stationary policies  wakuta togawa       
provide similar proof average reward case 
note that  common consider non stationary stochastic policies f
nonlinear  policies typically condition current state  current state
time  agents reward history  however  setting  policies condition
reward history dominate not  example  suppose two
objectives take positive values f simply selects smaller two  i e  
f  v   w    mini vi   suppose that  given state  two actions available 
yields rewards               respectively  finally  suppose agent arrive
state one two reward histories  whose discounted sums either               
policy conditions discounted reward histories outperform policies
not  i e   optimal policy selects action yielding        reward history sums
       action yielding        reward history sums         so 
single objective mdps markov property additive returns sufficient restrict
attention policies ignore history  multi objective case  scalarized returns
longer additive therefore optimal policy depend history  examples
methods exploit fact steering approach  mannor   shimkin       
reward augmented state thresholded lexicographic ordering method geibel        
discussed section     
      single deterministic stochastic policies monotonically
increasing scalarization functions
remains address single policy setting monotonically increasing f  
nature optimal solution case follows directly reasoning given
multiple policy setting 
deterministic policies considered  single policy sought may
non stationary  reflected box     table    reasons elucidated whites
  

firoijers  vamplew  whiteson   dazeley

example  again  hard define non stationary policy local action selection 
due risk circular dependencies q values 
stochastic policies allowed  optimal policy may stochastic 
represented mixture policy two deterministic stationary policies 
reflected box     table    reasons given corollary    cases 
policies potentially benefit conditioning reward history 

   planning momdps
section  survey key approaches planning momdps  i e   computing
optimal policy coverage set undominated policies given complete model
momdp  following taxonomy presented section    first consider single policy
methods turn multiple policy methods linear monotonically increasing
scalarization functions 
    single policy planning
known weights scenario  w known planning begins  single
policy  optimal w  must discovered  since momdp transformed
single objective mdp f linear  see section         focus single policy
planning nonlinear f  
discussed section        nonlinear f cause scalarized return nonadditive  consequently  single objective dynamic programming linear programming
methods  exploit assumption additive returns employing bellman equation  applicable  however  different linear programming formulations singlepolicy planning momdps possible  key feature methods
produce stochastic policies  which  discussed section    optimal
scalarization function nonlinear  aware single policy planning
methods work arbitrary nonlinear f   methods developed two special
cases  particular  perny weng        propose linear programming method
momdps scalarized using tchebycheff function mentioned section       
tchebycheff function always w pareto optimal policy optimal 
approach find  single  policy pareto front  addition  ogryczak  perny 
weng        propose analogous method ordered weighted regret metric 
metric calculates regret objective respect estimated ideal reference
point  sorts descending order  calculates weighted sum weights
descending order 
researchers proposed single policy methods momdps constraints 
feinberg shwartz        consider momdps one regular objective objectives inequality constraints  show feasible policy exists setting 
deterministic stationary finite number timesteps n that 
prior timestep n   random actions must performed  call  m  n  
policy  show pareto optimal values achieved  m  n   policies  propose
linear programming algorithm finds  approximate policies setting 
general momdps constraints considered  particular  altman       
proposes several linear programming approaches settings 
  

fia survey multi objective sequential decision making

furnkranz  hullermeier  cheng  park        propose framework mdps
qualitative reward signals  related momdps fit neatly
taxonomy  qualitative reward signals indicate preference policies actions
without directly ascribing numeric value them  since preferences induce partial
ordering policies  policy iteration method authors propose setting
may applicable momdps nonlinear f   pareto dominance induces partial
orderings  however  authors note multi objective tasks generally numeric
feedback exploited  thus  suggest quantitative momdps
viewed subset preference based mdps  methods designed specifically
momdps may efficient general preference based methods 
    multiple policy planning linear scalarization functions
multiple policy setting linear f   seek ccs m
ds    note however 
distinction convex hull convex coverage set usually made
literature 
one might argue explicitly multi objective methods necessary setting  one could repeatedly run single objective methods obtain ccs m
ds   
however  since infinitely many possible w  obvious possible values w covered  might possible devise way run single objective
methods finite number times still guarantee ccs m
ds   produced  however 
would nontrivial result corresponding algorithm would essence
multi objective method happens use single objective methods subroutines 
one approach attempted find minimally sized ccs m
   i e   convex
coverage set deterministic necessarily stationary policies  originally proposed
white kim         translate momdp partially observable markov
decision process  pomdp   sondik         intuitive way think translation
imagine fact one true objective agent unaware
objectives momdp is  modeled pomdp defining state
tuple hs    s  s  state momdp s           n  indicates
true objective  observations thus identify s  exactly give information
s    note translation momdps pomdps one way only  every
pomdp translated equivalent momdp 
typically  agent interacting pomdp maintains belief  i e   probability
distribution states  pomdp derived momdp  belief decomposed belief s  belief s    former degenerative s 
known  latter vector size n i th element specifies probability
i th objective true one  vector analogous w linear f  
fact  reason figure  b resembles piecewise linear value functions often
depicted pomdps  difference whether x axis interpreted w
belief 
white kim        show that  finite horizon case  solution every belief
exactly solution w  solutions resulting pomdp
exactly original momdp  infinite horizon case difficult
infinite horizon pomdps undecidable  madani  hanks    condon         however 
  

firoijers  vamplew  whiteson   dazeley

sufficiently large horizon  solution finite horizon pomdp used
approximate solution infinite horizon momdp 
solve resulting pomdp  white kim        propose combination sondiks
one pass algorithm  smallwood   sondik        policy iteration pomdps  sondik 
       however  pomdp planning method used long    
require initial belief pomdp state  which would correspond initializing
momdp state w      computes optimal policy every
possible belief  recently developed exact methods  e g   cassandra  littman 
zhang        kaelbling  littman  cassandra         meet conditions
could thus employed  approximate point based pomdp methods  spaan   vlassis 
      pineau  gordon    thrun        meet conditions         could
adapted compute approximate convex hull  choosing prior distribution
weights could sample  online pomdp planning methods  ross  pineau 
paquet    chaib draa        applicable plan given belief 
converting pomdp thus allows use pomdp methods
solving momdps linear f   however  approach inefficient
exploit characteristics distinguish momdps general pomdps  i e  
part state  s    known observations give information
s    example  methods compute policies trees  e g    kaelbling et al        
exploit fact deterministic policies stationary functions state
needed momdps linear f   furthermore  mentioned before  general infinite
horizon pomdps undecidable  momdps fact possible compute
ccs m
ds   exactly 
reasons  researchers developed specialized planning methods
setting  viswanathan  aggarwal  nair        propose linear programming approach
episodic momdps  wakuta togawa        propose policy iteration approach
three phases  first phase uses policy iteration narrow set
possibly optimal policies  second phase uses linear programs check optimality 
since necessarily give definitive answer  third phase uses another linear
program handle undetermined solutions left second phase 
barrett narayanan        propose convex hull value iteration  chvi   computes ch m
   every state  chvi extends conventional value iteration storing
ds
set vectors  q  s  a  state action pair  representing convex hull policies involving action  sets vectors correspond q values single objective
setting  contain optimal q values possible w  backup operation
performed  q hulls next state propagated back s  possible next

state   possible actions considered  i e  union convex hulls q  s    
taken   weighted probability occurring taking action state s 
procedure similar witness algorithm  kaelbling et al         pomdps 
lizotte et al         propose value iteration approach finite horizon setting
computes different value function timestep  addition  uses piecewise
linear spline representation value functions  authors prove offers asymptotic
time space complexity improvements representation used chvi
enables application algorithm momdps continuous states  however 
  

fia survey multi objective sequential decision making

algorithm applicable problems two objectives  limitation addressed
authors subsequent work  lizotte  bowling    murphy        extends
algorithm arbitrary number objectives provides detailed implementation
case three objectives 
    multiple policy planning monotonically increasing scalarization
functions
section  consider planning momdps monotonically increasing f   discussed section      stochastic policies allowed  mixture policies deterministic
stationary policies sufficient  therefore  focus case deterministic
policies allowed consider methods compute p cs m
   include
non stationary policies  distinction p f  m
 

p
cs m

  usually
made literature 
linear case  scalarizing every w obtaining p cs m
  singleobjective methods problematic  infinitely many w consider but  unlike
linear case  additional difficulty scalarized returns may longer
additive  make single objective methods inapplicable 
daellenbach kluyver        present algorithm multi objective routing tasks
 essentially deterministic momdps   approach uses dynamic programming conjunction augmented state space find non pareto dominated policies iteratively 
number iterations equals maximum number steps route  algorithm finds undominated sub policies parallel  authors use two alternative explicit
scalarization functions  call weighted minsum weighted minmax operators  first  values solutions translated   objective  new value
becomes fractional difference optimal values objective across
solutions  then  value objective multiplied positive weight  finally 
either minimum sum  minsum  minimum maximal value  minmax  
new weighted fractional differences chosen scalarization  note
scalarization functions monotonically increasing objectives  optimal value
objective individually depend scalarization function 
white        extends work proposing dynamic programming method
approximately solves infinite horizon momdps  repeatedly backing according multi objective version bellman equation  since policies
non stationary  size pareto front grows rapidly number backups applied 
however  white notes number need large acceptable approximations reached  nonetheless  approach feasible small momdps 
wiering de jong        address difficulty dynamic programming method
called con modp deterministic momdps computes optimal stationary policies 
con modp works enforcing consistency dp updates  policy consistent
suggests action timesteps given state  inconsistent policy
inconsistent one state action pair  con modp makes consistent forcing
current action taken time current state visited  inconsistency runs
deeper  policy discarded 
  

firoijers  vamplew  whiteson   dazeley

contrast  gong        proposes linear programming approach finds paretofront stationary policies  however  authors note  approach suitable
small momdps number constraints decision variables
linear program increase rapidly state space grows 
mentioned section        one way cope intractably large pareto fronts
compute instead  approximate pareto front  much smaller  chatterjee
et al         propose linear programming method computes  approximate front
infinite horizon momdp  chatterjee        propose analogous algorithm
average reward setting  cases  stationary stochastic policies shown
sufficient 
another way improve scalability setting give planning whole
state space instead plan on line agents current state  using monte carlo tree
search approach  kocsis   szepesvari         approaches  proven
successful  e g   game go  gelly   silver         increasingly popular
single objective mdps  wang sebag        propose monte carlo tree search method
deterministic momdps  single objective tree search methods typically optimistically
explore tree selecting actions maximize upper confidence bound
value estimates  multi objective variant same  respect scalar
multi objective value function whose definition based hypervolume indicator induced proposed action together set pareto optimal policies computed
far  hypervolume indicator  zitzler  thiele  laumanns  fonseca    da fonseca       
measures hypervolume pareto dominated set points  since pareto
front maximizes hypervolume indicator  optimistic action selection strategy focuses
tree search branches likely compliment existing archive 

   learning momdps
methods reviewed section   assume model transition reward
dynamics momdp known  cases model directly available 
multi objective reinforcement learning  morl  used instead 
one way carry morl take model based approach  i e   use agents
interaction environment learn model transition reward function
momdp apply multi objective planning methods described
section    though approach seems well suited morl  papers
considered it   e g   lizotte et al                discuss opportunities future work
model based morl section      instead  work morl focused
model free methods  model transition reward function never explicitly
learned 
section  survey key morl approaches  majority
methods single policy setting  multiple policy methods developed  first glance  may seem multiple policy methods unlikely effective
learning setting  since finding policies would increase sample costs 
computational costs  former typically much scarcer resource  however  modelbased methods obviate issue  enough samples gathered learn
useful model  finding policies optimal weights requires computation  model  

fia survey multi objective sequential decision making

free methods practical multiple policy setting employ off policy
learning  sutton   barto        precup  sutton    dasgupta         makes possible learn one policy using data gathered another  way  policies
multiple weight settings optimized using data 
    single policy learning methods
known weights scenario  morl algorithm aims learn single policy
optimal given weights  discussed section      linear scalarization
equivalent learning optimal policy single objective mdp standard
temporal difference  td  methods  sutton        q learning  watkins       
easily applied 
however  even though specialized methods needed address setting 
nonetheless commonly studied setting morl  linear scalarization
uniform weights  i e   elements w equal  forms basis work karlsson
        ferreira  bianchi  ribeiro         aissani  beldjilali  trentesaux       
shabani        amongst others  non uniform weights used authors
castelletti et al          guo et al         perez et al          majority
work uses td methods  work on line  although castelletti et al         extend off line
fitted q iteration  ernst  geurts    wehenkel        multiple objectives 
cases  change made underlying rl algorithm that  rather
scalarizing reward function learning scalar value function resulting
single objective mdp  vector valued value function learned original momdp
scalarized selecting actions  argument approach
values individual objectives may easier learn scalarized value  particularly
function approximation employed  tesauro et al          example  function
approximator ignore state variables irrelevant objective  reducing
size state space thereby speeding learning 
discussed section        linear scalarization may appropriate scenarios  vamplew  yearwood  dazeley  berry        demonstrate empirically
practical consequences morl  therefore  morl methods work
nonlinear scalarization functions substantial importance  unfortunately  illustrated
section        coping setting especially challenging  since algorithms
td methods based bellman equation inherently incompatible
nonlinear scalarization functions due non additive nature scalarized returns 
four main classes single policy morl methods using non linear scalarization
arisen  differ deal issue  first class simply applies td
methods without modification  approaches either resign heuristics guaranteed converge impose restrictions environment ensure
convergence  second class modifies either td algorithm state representation
issue non additive returns avoided  third class uses td methods
learn multiple policies using linear scalarization different values w 
forms stochastic non stationary meta policy optimal respect
nonlinear scalarization  fourth class uses policy search methods 
  

firoijers  vamplew  whiteson   dazeley

make use bellman equation hence directly applied combination
nonlinear scalarizations 
first class includes methods model problem multi agent system 
one agent per objective  agent learns recommends actions basis
return objective  global switch selects winning agent  whose recommended action followed current state  examples include simple winner takes all
approach agent whose recommended action highest q value selected 
sophisticated approaches w learning  humphrys        selected
action one incur loss followed  one key weakness
approaches pointed russell zimdars         allow
selection actions that  optimal single objective  offer good compromise
multiple objectives  another key weakness that  since actions selected
different timesteps may recommended different agents  resulting behavior corresponds policy combines elements learned agent  combination
may optimal even single objective  i e   may pareto dominated perform
arbitrarily poorly 
td used directly nonlinear scalarization functions allow
consideration actions  optimal regards individual
objectives  scalarization functions based fuzzy logic proposed problems
discrete actions zhao  chen  hu        problems continuous
actions lin chung         widely cited approach nonlinear scalarization
gabor  kalmar  szepesvari         designed tasks constraints
must satisfied objectives  lexicographic ordering objectives defined
threshold value specified objectives except last  state action values
objective exceed corresponding threshold clamped threshold value
prior applying lexicographic ordering  thus  thresholded lexicographic ordering
 tlo  approach scalarization maximizes performance last objective subject
meeting constraints objectives specified thresholds 
methods combining td nonlinear scalarization may converge suitable
policy certain conditions  converge suboptimal policy even fail
converge conditions  example  issabekov vamplew        demonstrate empirically tlo fail converge suitable policy episodic tasks
constrained objective receives non zero rewards timestep end
episode  general  methods based combination td nonlinear scalarization
must regarded heuristic nature  applicable restricted classes problems 
second class avoids problems caused non additive scalarized returns modifying either td algorithm state representation  knowledge  two approaches
proposed geibel        address limitations tlo members
class  require reward accumulated objective current episode
stored  first algorithm  local decision making based scalarized value
sum cumulative reward current state action values  eliminates
problem non additive returns  yields policy non stationary respect
observed state  meaning algorithm may converge  second approach augments state representation cumulative reward  approach converges
correct policy learns slowly  due increase size state space 
  

fia survey multi objective sequential decision making

third class uses td methods learn policies based linear scalarizations 
policy selection mechanism based nonlinear scalarization used form
meta policy base policies  multiple directions reinforcement learning
 mdrl  algorithm mannor shimkin              uses approach
context on line learning non episodic tasks  user specifies target region within
long term average reward lie  initial active policy chosen arbitrarily
followed average reward moves outside target region agent
specified reference state  point  direction current average reward
vector closest point target set calculated  policy whose direction best
matches target direction selected active policy  way  average reward
steered towards users specified target region  underlying base policies
utilize linear scalarization  nature policy selection mechanism means
overall non stationary policy formed base policies optimal nonlinear
scalarization specified users defined target set  vamplew et al         suggest
similar approach episodic tasks  td used first learn policies optimal
linear scalarization range different w  stochastic mixture policy
constructed optimal regards nonlinear scalarization 
fourth class uses policy search algorithms directly learn policy without learning value function  single policy morl  research policy search approaches
focused policy gradient methods  sutton  mcallester  singh    mansour        kohl  
stone        kober   peters         methods  policy iteratively adjusted
direction gradient value respect parameters  usually probability
distributions actions per state  policy  shelton        proposes algorithm
first learns optimal policy individual objective  used base policies form initial mixture policy stochastically selects base policy start
episode  hill climbing method based weighted convex combination
normalized objective gradients iteratively improves mixture policy  approach
directly fit taxonomy returns never scalarized  instead 
weights used find step direction relative current policy parameters 
practical perspective  behavior akin single policy rl using nonlinear
scalarization function  converges single pareto optimal policy need lie
convex hull  uchibe doya        propose policy gradient method
morl called constrained policy gradient rl  cpgrl  uses gradient projection technique find policies whose average reward satisfies constraints one
objectives  sheltons approach  cpgrl learns stochastic policies works
nonlinear scalarization functions 
    multiple policy learning linear scalarization functions
unknown weights decision support scenarios  f linear  morl algorithms
aim learn ccs possible policies  simple inefficient approach used
castelletti et al         run td multiple times different values w 
simplest case  runs conducted sequentially gradually build approximate
ccs  natarajan tadepalli        showed approach made efficient
reusing policies learned earlier runs similar w  show
  

firoijers  vamplew  whiteson   dazeley

improves greatly sample costs learning policy w similar already
visited previous runs  however  many samples typically still required good
approximate ccs obtained 
sophisticated approach approximating convex coverage set learn multiple policies parallel  several algorithms proposed achieve within
td learning framework  approach hiraoka  yoshida  mishima        similar
chvi planning algorithm barrett narayanan         see section     
learns parallel optimal value function w  using convex hull representation 
approach prone infinite growth number vertices convex hull polygons 
threshold margin applied hull representations iteration  eliminating points contribute little hulls hypervolume  hiraoka et al         present
algorithm adapt margins learning improve efficiency  note many
parameters must tuned effective performance  mukai  kuroe  iima        present
similar extension chvi learning context  address problematic growth
number values stored pruning vectors q value update  vector
selected random set vectors stored given state action pair
others lying within threshold distance deleted 
approaches hiraoka et al         mukai et al         designed
on line learning  contrast  multi objective fitted q iteration  mofqi   castelletti 
pianosi    restelli              off line approach learning multiple policies  mofqi
multi objective extension fitted q iteration  fqi  algorithm  ernst et al        
uses combination historical data single step transition dynamics
environment  initial function approximator  q learning update rule construct
dataset maps state action pairs expected return  dataset used
train improved function approximator process repeats values
function approximator converge  mofqi provides computationally efficient extension
fqi multiple objectives including w input function approximator
constructing expanded training data set containing training instances randomly
generated ws  since learned function generalizes across weight space addition
state action space  used construct policy w 
discussed section      lizotte et al         lizotte et al         describe valueiteration algorithm find convex hull policies finite horizon tasks  note
method applied learning context estimating model state transition
probabilities immediate rewards basis experience environment 
approach demonstrated task analyzing randomized drug trial data producing
estimates historical data gathered clinical trials 
    multiple policy learning monotonically increasing scalarization
functions
f nonlinear  morl algorithms unknown weights decision support scenarios aim learn pcs  linear scalarization case  simplest approach
run single objective algorithms multiple times varying w  shelton        demonstrates approach policy gradient algorithm  vamplew et al        
tlo method gabor et al          approach however  requires
  

fia survey multi objective sequential decision making

f explicitly known learning algorithm  may undesirable decision
support scenario 
knowledge  currently methods learning multiple policies
nonlinear f using value function approach  might seem possible adapt convex
hull methods chvi using pareto dominance operators place convex hull
calculations  straightforward  scalarized values policies
certain state non additive  cannot restrict stationary policies want
find deterministic pareto optimal policies  as mentioned section         however 
bellman equation chvi work  additivity  resulting sufficiency
deterministic policies  required  discuss options developing multiple policy learning
methods nonlinear f sections         
given extensive research multi objective evolutionary algorithms  moeas 
 coello coello  lamont    van veldhuizen        tan  khor  lee    sathikannan       
drugan   thierens        evolutionary methods rl  whiteson         surprisingly little work evolutionary approaches morl  methods populationbased  well suited approximating pareto fronts  would thus seem natural
fit f nonlinear  knowledge  handa      b  first apply moeas
morl  extending estimation distribution  eda  evolutionary algorithms handle multiple objectives  eda rl  handa      a  uses conditional random fields  crf 
represent probabilistic policies  initial set policies used generate set
episodes  best episodes set selected crfs likely produce trajectories generated  policies formed crfs constitute
next generation  handa      b  extends eda rl momdps using paretodominance based fitness metric select best episodes 
soh demiris        apply moeas morl  policies represented
stochastic finite state controllers  sfsc  optimized using two different moeas 
nsga   standard evolutionary algorithm  mcma  eda  use sfscs gives
rise large search space  necessitating addition local search operator  local
search generates random w  uses scalarize rewards  performs gradient based
search sfsc  empirical comparisons multi objective variants three pomdp
benchmarks demonstrate evolutionary methods generally superior purely
local search approach  local search combined evolution usually outperforms
purely evolutionary methods  one papers directly consider partially
observable momdps 

   momdp applications
multi objective methods planning learning employed wide range
applications  simulation real world settings  section  survey
applications  sake brevity  list comprehensive instead aims
provide illustrative range examples  first  discuss use multi objective
methods specific applications  second  discuss research identified broader
classes problems multi objective methods play useful role 
  

firoijers  vamplew  whiteson   dazeley

    specific applications
important factor driving interest multi objective decision making increasing
social political emphasis environmental concerns  more  decisions must
made trade economic  social  environmental objectives  reflected
fact substantial proportion applications multi objective methods
environmental component 
perhaps extensively researched application water reservoir control problem considered castelletti et al          castelletti  pianosi  soncini sessa        
castelletti et al               castelletti  pianosi  restelli         general
task find control policy releasing water dam balancing multiple uses
reservoir  including hydroelectric production flood mitigation  management
hydroelectric power production examined shabani         another environmental application forest management balance economic benefits
timber harvesting environmental aesthetic objectives  demonstrated
simulation gong        bone dragicevic        
several researchers considered environmentally motivated applications concerning management energy consumption  saves system developed kwak
et al         controls various aspects commercial building  lighting  heating  airconditioning  computer systems  provide suitable trade off energy consumption comfort buildings occupants  simulation results indicate
saves reduce energy consumption approximately     compared manual control
system  maintaining slightly improving occupant comfort  tesauro et al 
       liu et al         consider problem controlling computing server 
objectives minimizing response time user requests power consumption 
guo et al         apply morl develop broker agent electricity market 
broker sets caps group agents sit hierarchy manage energy
consumption device level  must balance energy cost system stability 
shelton        examines application morl developing broker agents 
however  case agents task financial rather environmental  acting
market maker sets buy sell prices resources market  aim balance
objectives maximizing profit minimizing spread  the difference buy
sell prices  lead larger volume trades    
computing communications applications widely considered  perez
et al         apply morl allocation resources jobs cloud computing scenario  objectives maximizing system responsiveness  utilization resources 
fairness amongst different classes user  comsa et al         consider maximize
system throughput ensure user equity context long term evolution mobile
communications packet scheduling protocol  tong brown        use constraint based
scalarization address tasks call access control routing broadband multimedia network  system aims maximize profit  a function throughput 
satisfying constraints quality service metrics  capacity constraints fairness constraints   uses methods similar gabor et al          zheng  li  qiu  gong
    sheltons model market directly model trading volume  spread used proxy
volume 

  

fia survey multi objective sequential decision making

       use constrained morl methods make routing decisions cognitive radio
network  aiming minimize average transmission delay maintaining acceptably
low packet loss rate 
industrial mechanical control  important application single objective mdp
methods  explored momdp researchers  aoki  kimura  kobayashi
       apply distributed rl control sewage flow system  exploiting systems hierarchical structure find solution minimizes violation stock levels node
flow system  smoothing variation flow source  aissani et al        
apply morl maintenance scheduling within manufacturing plant minimize time
taken complete maintenance tasks machine downtime  aissani  beldjilali 
trentesaux        build work applying simulation real petroleum refinery demonstrating ability adapt unscheduled corrective maintenance required
due equipment failures  control wet clutch heavy duty transmission systems
examined van vaerenbergh et al          twin objectives minimizing
engagement time  making transition smooth 
robotics popular application momdps  though work far
simulation rather real robots  maravall de lope        consider control
two limbed brachiating robot  objectives moving desired direction
avoiding collisions     nojima  kojima  kubota        attempt balance
objectives progress target collision avoidance  agent makes use
predefined behavioral modules target tracing  collision avoidance  wall following 
morl used dynamically adjust weighting modules  meisner       
identifies social robots promising application momdp methods  behavior
inherently multi objective must carry task without causing anxiety
discomfort humans 
morl applied control traffic infrastructure  yang wen
       apply control freeway on ramps vehicle management systems  aiming
maximize throughput equity freeway system  multiple agents
shared policies used  action selection occurring via negotiation agents 
similarly  dusparic cahill        apply morl control traffic lights intersections
urban environment minimize waiting time two different classes vehicles  yin 
duan  li  zhang        houli  zhiheng  yi        apply morl traffic
light control  novelty approach lies considering different objectives based
current state road system  minimizing vehicle stops prioritized traffic
free flowing  minimizing waiting time emphasized system medium load 
minimizing queue length intersections targeted system congested 
lizotte et al               consider medical application  prescribing appropriate
drug regime patient achieve acceptable trade off drugs effectiveness severity side effects  system learns multiple policies based
    many robotic applications may ideal avoid collisions completely  environments
may possible  e g   presence moving obstacles whose velocity faster
robot  difficult predict  may case humans human controlled vehicles 
reducing likelihood impact collisions may reasonable attempting
find collision free policy  see example holenstein badreddin        pervez ryu
       

  

firoijers  vamplew  whiteson   dazeley

static data produced randomized controlled drug trials  selection best
treatment specific patient made doctor based patients individual circumstances  application excellent example problem stochastic
approaches mixture policies inappropriate  policy maximizes symptom relief side effects one patient minimizes side effects
symptom relief next patient may appear give excellent results averaged
across episodes  however  experience individual patient likely regarded
undesirable 
    applications within broader planning learning tasks
addition specific applications discussed above  several authors identified
general classes tasks multi objective sequential decision making applied 
      probabilistic risk aware planning
cheng  subrahmanian  westerberg        argue decision making uncertainty inherently multi objective nature  even single reward
considered  such profit   environmental uncertainty means expected value
alone insufficient support good decision making  decision maker must consider
variance return  similarly  bryce        states probabilistic planning
inherently multi objective due need optimize cost probability
success plan  criticizes approaches either aggregate factors bound one
optimize other  arguing favor explicitly multi objective methods 
aptly named probabilistic planning multi objective  paper bryce  cushing 
kambhampati        demonstrates might achieved  describing method based
multi objective dynamic programming belief states  multi objective extension
looping ao  search algorithm find set pareto optimal plans  recent work
kolobov  mausam  weld        teichteil konigsbuch      b  examine extension stochastic shortest path  ssp  methods problems dead end states exist 
ssp methods assume least one policy exists guaranteed reach goal 
presence dead ends policy exists  authors propose algorithms
aim maximize probability reaching goal minimize cost
paths found goal 
bryce        notes probabilistic plan fails environment enters non goal
absorbing state  hence  multi objective probabilistic planning strong parallels
research risk aware rl carried geibel        geibel wysotzki        
add second reward signal indicating transition environment error
state  defourny  ernst  wehenkel        provide useful insights incorporation risk awareness mdp methods  review range criteria proposed
constraining risk  note many nonlinear produce non additive
scalarized returns incompatible local decision making methods based
bellman equation  recommend custom risk control requirements
mostly enforced heuristically  altering policy optimization procedures checking
compliance policies initial requirements  multi policy momdp methods treating risk additional objective would satisfy requirement  iden  

fia survey multi objective sequential decision making

tified coverage set  risk aware metric used select best policy 
however  measures risk may expressed directly discounted cumulative
rewards  example  agent may wish minimize variance expected return particular reward signal rather discounted cumulative value  methods
based multi objective probabilistic model checking  courcoubetis   yannakakis       
forejt  kwiatkowska  norman  parker    qu        forejt  kwiatkowska    parker       
teichteil konigsbuch      a   evaluate whether system modelled mdp satisfies multiple  possibly conflicting  properties  may suitable tasks 
      multi agent systems
use mdps within multi agent systems widely explored  bosoniu  babuska 
  schutter         several authors proposed approaches strongly related
momdps  multi agent system  agent objective  effective
overall performance must consider actions affect agents 
agents completely self interested  problem framed momdp
treating effects agents additional objectives  example  mouaddib       
uses multi objective dynamic programming facilitate cooperation multiple agents
whose underlying goals may conflicting  state action pair  agent stores
three values  local utility  gain agents receive  penalty inflicts
agents  policy agent established converting vector values
regret ratios applying leximin ordering ratios 
dusparic cahill        compare application morl multi agent tasks
multi agent methods evolutionary ant colony algorithms  dusparic
cahill        extend w learning algorithm humphrys         agent learns
local policies  one objectives  remote policies  one
local policy neighboring agents   timestep  local policies
active remote policies agent nominate actions  winning action selected
combining action values across nominating policies  weighting term applied
values remote policies determine level cooperation agent offers neighbor 
experimental results urban traffic control simulator show substantial improvement
level cooperation non zero  work similar schneider  wong 
moore  riedmiller         addresses use multiple agents distributed
network power distribution grid  aim maximize global reward
formed combination agents local reward  demonstrate
agent focuses local reward  policies learned may maximize global
reward  performance improved agent perform linearly scalarized
learning using local reward rewards neighboring agents 
      multi objective optimization using reinforcement learning
reinforcement learning primarily applied sequential decision making tasks dynamic
environment  however employed control search mechanisms static optimization tasks scheduling  carchrae   beck         multi objective optimization
static tasks design well established field and  majority work
  

firoijers  vamplew  whiteson   dazeley

employed mathematical evolutionary approaches  coello coello et al         
authors explored application reinforcement learning contexts 
mariano morales            b      a  investigate use rl methods  ant q
q learning  search mechanism optimization multi objective design tasks 
values decision variables considered current state  actions defined
alter values variables  multiple agents explore state space parallel 
agents divided families  family focuses single objective 
end episode  final states found agent evaluated  undominated solutions kept archive agents discovered solutions rewarded 
increasing likelihood similar policies followed future  method
shown work small number test problems evolutionary multi objective
optimization literature  liao  wu  jiang        apply rl search static control
settings power generation system objectives reducing fuel usage ensuring voltage stability  propose rl algorithm formulated specifically tasks
high dimensional state spaces  compare performance evolutionary
multi objective algorithm  finding rl method discovers fronts
accurate better distributed  improving speed search 
note effectively apply rl multi objective optimization  assumptions usually made nature environment  example  liao et al         require
action increases decreases value precisely one state variable  result 
methods likely limited applicability general morl problems
described earlier 

   future work
section  enumerate possibilities future research multi objective
planning learning 
    model based methods
mentioned section    little work model based approaches
morl  given breadth planning methods momdps  could employed
model based morl methods subroutines  surprising  knowledge 
work area lizotte et al                model momdps
transition probabilities reward function derived historical data 
spline based multi objective value iteration approach applied model  general 
learning models seems negligibly harder single objective setting  since
estimates reward function learned separately  problem learning
transition function  generally considered hard part model learning  identical
single objective setting  especially multiple policy scenarios  model based approaches
morl could greatly reduce sample costs  model learned  entire
ccs pcs computed off line  without requiring additional samples 
   

fia survey multi objective sequential decision making

    learning multiple policies monotonically increasing scalarization
functions using value functions
mentioned section      aware methods use value function
approach learn multiple policies pcs  stochastic policies permitted 
problem easier learn ccs m
ds    use either mixture policies
 vamplew et al         stationary randomizations  wakuta        policies
ccs  see section         however  deterministic policies permitted 
problem difficult  one option could use finite horizon approximation
infinite horizon problem  planning backwards planning horizon  expected
reward timesteps go approximates infinite horizon value better better
  mentioned section      similar approaches used pomdp
setting  another way find good approximations non stationary policies could
learn stationary policies  perhaps extending con mdp  wiering   de jong       
learning setting   prefix timesteps non stationary policy 
    many objective sequential decision making
majority research reviewed article  theoretical applied  deals
momdps objectives  mirrors state early evolutionary multiobjective research  focused almost exclusively problems two three
objectives  however  last decade growing interest evolutionary
methods so called many objective problems  least four sometimes
fifty objectives  ishibuchi  tsukamoto    nojima         research
shown many algorithms perform well objectives scale poorly
number objectives  necessitating special algorithms many objective setting 
many objective mdps received little consideration far  numerous real world control problems naturally modeled way  example 
fleming et al         point many objective control problems commonly arise
engineering  give example jet engine control system eight objectives 
many objective problems considered evolutionary computation  seems likely
least methods explored far scale poorly number objectives  example  multi policy momdp planning algorithm described lizotte
et al         limited problems two objectives 
key challenge posed many objective problems number undominated
solutions typically grows exponentially number objectives  particularly
problematic multiple policy momdp methods  fleming et al         note one
effective approaches used many objective evolutionary computation
incorporate user preferences restrict search space small region interest 
particular  recommend interactive preference articulation user interactively steers system towards desirable solution optimization  vamplew
et al         raise possibility incorporating approach morl 
aware research actually done so 
   

firoijers  vamplew  whiteson   dazeley

     

     

b

c


     


figure    momdp two objectives four states 
    expectation scalarized return
section    defined scalarized value vw  s  result applying scalarization function f multi objective value v  s  according w  i e   vw  s    f  v  s   w  
since v  s  expectation  means scalarization function applied
expectation computed  i e  
vw  s    f  v  s   w    f  e 


x

k rk     s    s   w  

k  

formulation  refer scalarization expected return  ser 
standard literature  however  option  possible define
vw  s  expectation scalarized return  esr  
vw  s    e f  


x

k rk   w      s    s 

k  

definition used critically affect policies preferred  example 
consider following momdp  illustrated figure    four states  a  b  c 
d  two objectives  agent starts state two possible actions  a  transits
state b c  probability      a  transits state probability   
actions lead        reward  states b  c one action 
leads deterministic reward        b         c         d 
scalarization function multiplies two objectives together  thus  ser 
vw  s    v   s v   s  
esr 
vw  s    e 


x
k  

k rk 


x
k  

   


k rk      s    s  

fia survey multi objective sequential decision making

rki reward i th objective timestep k  w needed example
since f involves constants      a    a     a    a    multi objective
values v   a                         v   a                   
ser  leads scalarized values v    a                v    a   
         consequently   preferred  esr  however  v    a     
v    a             thus   preferred 
intuitively  ser formulation appropriate policy used many times
return accumulates across episodes  e g   user using policy
time  then  scalarizing expected reward makes sense   preferable
expectation accumulate return objectives  however  policy
used times return accumulate across episodes  e g  
episode conducted different user  esr formulation appropriate 
case  expected return scalarization interest   preferable
  always yield zero scalarized return given episode 
knowledge  literature momdps employs esr formulation 
even though many real world scenarios seems appropriate 
example  medical application lizotte et al         mentioned section   
patient gets one episode treat illness  thus clearly interested
maximizing esr  ser  thus  believe developing methods momdps
esr formulation critical direction future research 

   conclusions
article presented survey algorithms designed sequential decision making problems multiple objectives 
order make explicit circumstances special methods needed
solve multi objective problems  identified three distinct scenarios converting
problem single objective one impossible  infeasible  undesirable  well
providing motivation need multi objective methods  scenarios represent
three main ways methods applied practice 
proposed taxonomy classifies multi objective methods according applicable scenario  scalarization function  which projects multi objective values scalar
ones   type policies considered  showed factors determine
nature optimal solution  single policy  coverage set  convex
pareto   taxonomy based utility based approach  sees scalarization
function part utility  thus part problem definition  contrasts
so called axiomatic approach  usually assumes pareto front appropriate
solution  showed utility based approach used justify choice
solution set  following line thought  observed  observation    computing
pareto front often necessary  many cases convex coverage set
deterministic stationary policies sufficient 
using taxonomy  surveyed literature multi objective methods planning
learning  interesting observation learning methods use modelfree rather model based approach  identifying latter understudied class
   

firoijers  vamplew  whiteson   dazeley

methods  another part taxonomy yet widely studied learning
case monotonically increasing scalarization functions 
discussed key applications momdp methods motivation importance
methods  applications identified diverse range fields including environmental management  financial markets  information communications technology 
control industrial processes  robotic systems traffic infrastructure  addition connections identified multi objective sequential decision making broad
areas research probabilistic planning model checking  multi agent systems
general multi objective optimization 
finally  outlined several opportunities future work  include understudied
areas  model based methods  learning monotonically increasing scalarization settings 
many objective sequential decision making   reformulation objective
momdps expectation scalarized return particularly important
optimize policy executed once 

acknowledgments
would thank matthijs spaan  frans oliehoek  matthijs snel  marie d  manner
samy sa  well anonymous reviewers  valuable feedback  work
supported netherlands organisation scientific research  nwo   decisiontheoretic control network capacity allocation problems                project 

references
aberdeen  d   thiebaux  s     zhang  l          decision theoretic military operations
planning  proc  icaps  vol      pp         
aissani  n   beldjilali  b     trentesaux  d          efficient effective reactive scheduling manufacturing system using sarsa multi objective agents  mosim     th
conference internationale de modelisation et simulation  pp         
aissani  n   beldjilali  b     trentesaux  d          dynamic scheduling maintenance
tasks pretroleum industry  reinforcement approach  engineering applications
artificial intelligence               
altman  e          constrained markov decision processes  chapman hall crc 
london 
aoki  k   kimura  h     kobayashi  s          distributed reinforcement learning using
bi directional decision making multi criteria control multi stage flow systems 
 th conference intelligent autonomous systems  vol           pp         
barrett  l     narayanan  s          learning optimal policies multiple criteria 
proceedings   th international conference machine learning  pp       
new york  ny  usa  acm 
becker  r   zilberstein  s   lesser  v     goldman  c  v          transition independent
decentralized markov decision processes  proc   nd intl joint conf 
autonomous agents   multi agent systems 
   

fia survey multi objective sequential decision making

bellman  r  e       a   markov decision process  journal mathematical mech     
       
bellman  r       b   dynamic programming  princeton university press 
bhattacharya  b   lobbrecht  a  h     solomantine  d  p          neural networks reinforcement learning control water systems  journal water resources planning
management                  
bone  c     dragicevic  s          gis intelligent agents multiobjective natural
resource allocation  reinforcement learning approach  transactions gis         
       
bosoniu  l   babuska  r     schutter  b  d          comprehensive survey multiagent
reinforcement learning  ieee transactions systems  man  cybernetics   part
c  applications reviews                 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  journal artificial intelligence research 
        
brazdil  t   brozek  v   chatterjee  k   forejt  v     kucera  a          two views
multiple mean payoff objectives markov decision processes  corr  abs           
bryce  d          value s  probabilistic plans  workshop reality check
planning scheduling uncertainty  icaps    
bryce  d   cushing  w     kambhampati  s          probabilistic planning multiobjective   technical report         arizona state university 
carchrae  t     beck  j  c          applying machine learning low knowledge control
optimization algorithms  computational intelligence                 
cassandra  a   littman  m  l     zhang  n  l          incremental pruning  simple  fast 
exact method partially observable markov decision processes  proceedings
thirteenth conference uncertainty artificial intelligence  pp       
castelletti  a   pianosi  f     restelli  m          multiobjective reinforcement learning
approach water resources systems operation  pareto frontier approximation
single run  water resources research 
castelletti  a   corani  g   rizzolli  a   soncini sessa  r     weber  e          reinforcement learning operational management water system  ifac workshop
modeling control environmental issues  pp         
castelletti  a   galelli  s   restelli  m     soncini sessa  r          tree based reinforcement learning optimal water reservoir operation  water resources research 
    w       
castelletti  a   pianosi  f     restelli  m          multi objective fitted q iteration  pareto
frontier approximation one single run  international conference networking 
sensing control  pp         
castelletti  a   pianosi  f     restelli  m          tree based fitted q iteration multiobjective markov decision processes  ieee world congress computational
intelligence 
   

firoijers  vamplew  whiteson   dazeley

castelletti  a   pianosi  f     soncini sessa  r          water reservoir control economic  social environmental constraints  automatica               
chatterjee  k          markov decision processes multiple long run average objectives 
fsttcs  vol  lncs       pp         
chatterjee  k   majumdar  r     henzinger  t  a          markov decision processes
multiple objectives  proceedings   rd annual conference theoretical
aspects computer science  stacs    pp          berlin  heidelberg  springerverlag 
cheng  l   subrahmanian  e     westerberg  a          multiobjective decision processes
uncertainty  applications  formulations solution strategies  industrial
engineering chemistry research                   
clemen  r  t          making hard decisions  introduction decision analysis   
edition   south western college pub 
coello coello  c  a   lamont  g  b     van veldhuizen  d  a          evolutionary algorithms solving multi objective problems  kluwer academic publishers 
comsa  i   aydin  m   zhang  s   kuonen  p     wagen  j  f          multi objective resource scheduling lte networks using reinforcement learning  international journal
distributed systems technologies              
courcoubetis  c     yannakakis  m          markov decision processes regular events 
ieee transactions automatic control                    
crites  r  h     barto  a  g          improving elevator performance using reinforcement
learning  touretzky  d  s   mozer  m  c     hasselmo  m  e   eds    advances
neural information processing systems    pp            mit press 
daellenbach  h  g     kluyver  c  a  d          note multiple objective dynamic
programming  journal operational research society             
defourny  b   ernst  d     wehenkel  l          risk aware decision making dynamic
programming  nips      workshop model uncertainty risk rl 
diehl  m     haimes  y  y          influence diagrams multiple objectives tradeoff analysis  systems  man cybernetics  part a  systems humans  ieee
transactions on                 
drugan  m  m     thierens  d          stochastic pareto local search  pareto neighbourhood
exploration perturbation strategies  journal heuristics                 
dusparic  i     cahill  v          distributed w learning  multi policy optimization selforganizing systems  third ieee international conference self adaptive
self organizing systems  pp       
dusparic  i     cahill  v          multi policy optimization self organizing systems 
soar       lncs       pp         
dyer  j  s   fishburn  p  c   steuer  r  e   wallenius  j     zionts  s          multiple criteria decision making  multiattribute utility theory  next ten years  management
science                 
   

fia survey multi objective sequential decision making

ernst  d   geurts  p     wehenkel  l          tree based batch mode reinforcement learning 
journal machine learning research            
ernst  d   glavic  m     wehenkel  l          power systems stability control  reinforcement learning framework  ieee transactions power systems                 
feinberg  e  a     shwartz  a          constrained markov decision models weighted
discounted rewards  mathematics operations research                 
ferreira  l   bianchi  r     ribeiro  c          multi agent multi objective reinforcement
learning using heuristically accelerated reinforcement learning       brazilian
robotics symposium latin american robotics symposium  pp       
fleming  p   purshouse  r     lygoe  r          many objective optimization  engineering design perspective  evolutionary multi criterion optimization  lecture notes
computer science  vol        pp       
forejt  v   kwiatkowska  m   norman  g   parker  d     qu  h          quantitative
multi objective verification probabilistic systems  tools algorithms
construction analysis systems  pp          springer berlin heidelberg 
forejt  v   kwiatkowska  m     parker  d          pareto curves probabilistic model
checking  automated technology verification analysis  pp         
springer berlin heidelberg 
furnkranz  j   hullermeier  e   cheng  w     park  s  h          preference based reinforcement learning  formal framework policy iteration algorithm  machine
learning                   
gabor  z   kalmar  z     szepesvari  c          multi criteria reinforcement learning 
fifteenth international conference machine learning  pp         
geibel  p     wysotzki  f          risk sensitive reinforcement learning applied control
constraints  journal artificial intelligence research            
geibel  p          reinforcement learning bounded risk  proceeding   th
international conference machine learning  pp         
geibel  p          reinforcement learning mdps constraints  european conference machine learning  vol        pp         
gelly  s     silver  d          monte carlo tree search rapid action value estimation
computer go  artificial intelligence                     
gong  p          multiobjective dynamic programming forest resource management 
forest ecology management           
guo  y   zeman  a     li  r          reinforcement learning approach setting multiobjective goals energy demand management  international journal agent technologies systems              
handa  h       a   eda rl  estimation distribution algorithms reinforcement learning problems  acm sigevo genetic evolutionary computation conference 
pp         
   

firoijers  vamplew  whiteson   dazeley

handa  h       b   solving multi objective reinforcement learning problems eda rl acquisition various strategies  proceedings ninth internatonal conference
intelligent sysems design applications  pp         
hiraoka  k   yoshida  m     mishima  t          parallel reinforcement learning weighted
multi criteria model adaptive margin  cognitive neurodynamics          
holenstein  a  a     badreddin  e          collision avoidance behavior based mobile
robot design  robotics automation        proceedings        ieee international conference on  pp          ieee 
houli  d   zhiheng  l     yi  z          multiobjective reinforcement learning traffic
signal control using vehicular ad hoc network  eurasip journal advances
signal processing 
howard  r  a          dynamic programming markov decision processes  mit press 
humphrys  m          action selection methods using reinforcement learning  proceedings fourth international conference simulation adaptive behavior  pp 
       
ishibuchi  h   tsukamoto  n     nojima  y          evolutionary many objective optimisation  short review  ieee congress evolutionary computation  pp           
issabekov  r     vamplew  p          empirical comparison two common multiobjective reinforcement learning algorithms  ai        th australasian joint
conference artificial intelligence  pp         
kaelbling  l  p   littman  m  l     cassandra  a  r          planning acting
partially observable stochastic domains  artificial intelligence             
karlsson  j          learning solve multiple goals  ph d  thesis  university rochester 
kober  j     peters  j          policy search motor primitives robotics  machine
learning             
kober  j     peters  j          reinforcement learning robotics  survey  wiering 
m     otterlo  m   eds    reinforcement learning  vol     adaptation  learning 
optimization  pp          springer berlin heidelberg 
kocsis  l     szepesvari  c          bandit based monte carlo planning    th european
conference machine learning  pp          springer 
kohl  n     stone  p          policy gradient reinforcement learning fast quadrupedal
locomotion  proceedings ieee international conference robotics
automation  pp           
kolobov  a   mausam    weld  d  s          theory goal oriented mdps dead
ends  proceedings twenty eighth conference uncertainty artificial
intelligence 
kwak  j   varakantham  p   maheswarn  r   tambe  m   jazizadeh  f   kavulya  g   klein 
l   becerik gerber  b   hayes  t     wood  w          saves  sustainable multiagent application conserve building energy considering occupants    th international conference autonomous agents multiagent systems  pp       
   

fia survey multi objective sequential decision making

liao  h   wu  q     jiang  l          multi objective optimization reinforcement learning
power system dispatch voltage stability  innovative smart grid technologies
conference europe 
lin  c  t     chung  i  f          reinforcement neuro fuzzy combiner multiobjective
control  ieee transactions systems  man cyberbetics   part b             
    
liu  c   xu  x     hu  d          multiobjective reinforcement learning  comprehensive
overview  systems  man  cybernetics  part c  applications reviews  ieee
transactions on  pp           
liu  w   tan  y     qiu  q          enhanced q learning algorithm dynamic power
management performance constraints  date    pp         
lizotte  d  j   bowling  m     murphy  s  a          efficient reinforcement learning
multiple reward functions randomized clinical trial analysis    th international
conference machine learning  pp         
lizotte  d  j   bowling  m     murphy  s  a          linear fitted q iteration multiple
reward functions  journal machine learning research               
madani  o   hanks  s     condon  a          undecidability probabilistic planning
infinite horizon partially observable markov decision problems  proceedings
national conference artificial intelligence  aaai   pp         
mannor  s     shimkin  n          steering approach multi criteria reinforcement
learning  neural information processing systems  pp           
mannor  s     shimkin  n          geometric approach multi criterion reinforcement
learning  journal machine learning research            
maravall  d     de lope  j          reinforcement learning method dynamic obstacle
avoidance robotic mechanisms  computational intelligent systems applied
research  proceedings  th international flins conference  pp          singapore  world scientific 
mariano  c     morales  e          moaq ant q algorithm multiple objective
optimization problems  gecco     proceedings genetic evolutionary
computation conference  pp         
mariano  c     morales  e       a   new approach solution multiple objective
optimization problems based reinforcement learning  advances artificial
intelligence  international joint conference   th ibero american conference ai 
  th brazilian symposium  springer 
mariano  c     morales  e       b   new distributed reinforcement learning algorithm
multiple objective optimisation problems  lecture notes ai vol       proceedings mexican international conference artficial intelligence  pp         
springer 
meisner  e  m          learning controllers human robot interaction  ph d  thesis 
rensselaer polytechnic institute 
   

firoijers  vamplew  whiteson   dazeley

mouaddib  a  i          collective multi objective planning  proceedings ieee
workshop distributed intelligent systems  collective intelligence applications  dis     pp        washington  dc  usa  ieee computer society 
mukai  y   kuroe  y     iima  h          multi objective reinforcement learning method
acquiring pareto optimal policies simultaneously  ieee international conference systems  man cybernetics  pp           
natarajan  s     tadepalli  p          dynamic preferences multi criteria reinforcement
learning  international conference machine learning  pp         
nojima  y   kojima  f     kubota  n          local episode based learning multiobjective behavior coordination mobile robot dynamic environments 
  th ieee international conference fuzzy systems  vol     pp         
ogryczak  w   perny  p     weng  p          minimizing ordered weighted regrets
multiobjective markov decision processes   nd international conference
algorithmic decision theory  pp         
ong  s  c   png  s  w   hsu  d     lee  w  s          planning uncertainty robotic
tasks mixed observability  international journal robotics research         
         
pareto  v          manuel deconomie politique  giard  paris 
peek  n  b          explicit temporal models decisiontheoretic planning clinical
management  artificial intelligence medicine                 
perez  j   germain renaud  c   kegl  b     loomis  c          responsive elastic computing  international conference autonomic computing  pp       
perny  p     weng  p          finding compromise solutions multiobjective markov
decision processes  ecai multidisciplinary workshop advances preference
handling  pp       
perny  p   weng  p   goldsmith  j     hanna  j  p          approximation lorenz optimal
solutions multiobjective markov decision processes  workshops twentyseventh aaai conference artificial intelligence 
pervez  a     ryu  j          safe physical human robot interaction past  present
future  journal mechanical science technology                 
pineau  j   gordon  g     thrun  s          anytime point based approximations large
pomdps  journal artificial intelligence research                 
precup  d   sutton  r  s     dasgupta  s          off policy temporal difference learning
function approximation  proceedings   th international conference
machine learning  pp         
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley   sons  inc 
roijers  d  m   whiteson  s     oliehoek  f  a          computing convex coverage sets
multi objective coordination graphs  adt       proceedings third international conference algorithmic decision theory  appear 
   

fia survey multi objective sequential decision making

ross  s   pineau  j   paquet  s     chaib draa  b          online planning algorithms
pomdps  journal artificial intelligence research             
russell  s     zimdars  a  l          q decomposition reinforcement learning agents 
proceedings   th international conference machine learning  pp         
schneider  j   wong  w  k   moore  a     riedmiller  m          distributed value functions  proceedings   th international conference machine learning  pp 
        san francisco  ca  morgan kaufmann 
shabani  n          incorporating flood control rule curves columbia river hydroelectric system multireservoir reinforcement learning optimization model  masters
thesis  university british columbia 
shelton  c  r          importance sampling reinforcement learning multiple objectives  ai technical report           mit 
shortreed  s   laber  e   lizotte  d   stroup  t   pineau  j     murphy  s          informing
sequential clinical decision making reinforcement learning  empirical study 
machine learning             
smallwood  r     sondik  e          optimal control partially observable markov
processes finite horizon  operations research                   
soh  h     demiris  y          evolving policies multi reward partially observable
markov decision processes  mr pomdps   gecco   proceedings   th
annual conference genetic evolutionary computation  pp         
sondik  e          optimal control partially observable processes finite horizon 
ph d  thesis  stanford university  stanford  california 
sondik  e          optimal control partially observable markov processes
infinite horizon  discounted costs  operations research                 
spaan  m     vlassis  n          perseus  randomized point based value iteration
pomdps  journal artificial intelligence research                 
stewart  t  j          critical survey status multiple criteria decision making
theory practice  omega                 
sutton  r  s          learning predict methods temporal differences  machine
learning             
sutton  r  s     barto  a  g          introduction reinforcement learning   st edition  
mit press  cambridge  ma  usa 
sutton  r   mcallester  d   singh  s     mansour  y          policy gradient methods
reinforcement learning function approximation  nips  pp           
szita  i          reinforcement learning games  wiering  m     otterlo  m   eds   
reinforcement learning  vol     adaptation  learning  optimization  pp     
     springer berlin heidelberg 
tan  k  c   khor  e  f   lee  t  h     sathikannan  r          evolutionary algorithm
advanced goal priority specification multi objective optimization  journal
artificial intelligence research             
   

firoijers  vamplew  whiteson   dazeley

teichteil konigsbuch  f       a   path constrained markov decision processes  bridging
gap  proceedings twentieth european conference artificial intelligence 
teichteil konigsbuch  f       b   stochastic safest shortest path problems  proceedings twenty sixth aaai conference artificial intelligence 
tesauro  g   das  r   chan  h   kephart  j  o   lefurgy  c   levine  d  w     rawson  f 
        managing power consumption performance computing systems using
reinforcement learning  neural information processing systems 
tong  h     brown  t  x          reinforcement learning call admission control
routing quality service constraints multimedia networks  machine learning             
uchibe  e     doya  k          constrained reinforcement learning intrinsic
extrinsic rewards  pp          theory novel applications machine learning 
i tech  vienna  austria 
vamplew  p   dazeley  r   barker  e     kelarev  a          constructing stochastic mixture
policies episodic multiobjective reinforcement learning tasks  ai      nd
australasian conference artificial intelligence  pp         
vamplew  p   dazeley  r   berry  a   dekker  e     issabekov  r          empirical evaluation methods multiobjective reinforcement learning algorithms  machine learning 
               
vamplew  p   yearwood  j   dazeley  r     berry  a          limitations scalarisation multi objective reinforcement learning pareto fronts  ai      st
australasian joint conference artificial intelligence  pp          springer 
van otterlo  m     wiering  m          reinforcement learning markov decision processes  reinforcement learning  state art  chap     pp       springer 
van vaerenbergh  k   rodriguez  a   gagliolo  m   vrancx  p   nowe  a   stoev  j  
goossens  s   pinte  g     symens  w          improving wet clutch engagement
reinforcement learning  international joint conference neural networks 
ijcnn      
vira  c     haimes  y  y          multiobjective decision making  theory methodology 
no     north holland 
viswanathan  b   aggarwal  v  v     nair  k  p  k          multiple criteria markov
decision processes  tims studies management science            
wakuta  k     togawa  k          solution procedures markov decision processes  optimization  journal mathematical programming operations research         
     
wakuta  k          note structure value spaces vector valued markov decision
processes   mathematical methods operations research               
wang  w     sebag  m          hypervolume indicator dominance reward based multiobjective monte carlo tree search  machine learning      
watkins  c  j  c  h          learning delayed rewards  ph d  thesis  cambridge
university 
   

fia survey multi objective sequential decision making

white  c  c     kim  k  m          solution procedures solving vector criterion markov
decision processes  large scale systems            
white  d          multi objective infinite horizon discounted markov decision processes 
journal mathematical analysis applications                  
whiteson  s          evolutionary computation reinforcement learning  wiering 
m  a     van otterlo  m   eds    reinforcement learning  state art  chap     
pp          springer  berlin 
wiering  m     de jong  e          computing optimal stationary policies multiobjective markov decision processes  ieee international symposium approximate dynamic programming reinforcement learning  pp          ieee 
yang  z     wen  k          multi objective optimization freeway traffic flow via
fuzzy reinforcement learning method   rd international conference advanced
computer theory engineering  vol     pp         
yin  s   duan  h   li  z     zhang  y          multi objective reinforcement learning
traffic signal coordinate control   th world conference transport research 
zeleny  m     cochrane  j  l          multiple criteria decision making  vol      mcgrawhill new york 
zhao  y   chen  q     hu  w          multi objective reinforcement learning algorithm
mosdmp unknown environment  proceedings  th world congress
intelligent control automation  pp           
zheng  k   li  h   qiu  r  c     gong  s          multi objective reinforcement learning
based routing cognitive radio networks  walking random maze  international
conference computing  networking communications  pp         
zitzler  e   thiele  l   laumanns  m   fonseca  c  m     da fonseca  v  g          performance assessment multiobjective optimizers  analysis review  evolutionary
computation  ieee transactions on                

   


