journal of artificial intelligence research                  

submitted        published      

unsupervised sub tree alignment
for tree to tree translation
tong xiao
jingbo zhu

xiaotong mail neu edu cn
zhujingbo mail neu edu cn

college of information science and engineering
northeastern university
no       wenhua road  heping district
shenyang  china

abstract
this article presents a probabilistic sub tree alignment model and its application to
tree to tree machine translation  unlike previous work  we do not resort to surface heuristics or expensive annotated data  but instead derive an unsupervised model to infer the
syntactic correspondence between two languages  more importantly  the developed model
is syntactically motivated and does not rely on word alignments  as a by product  our
model outputs a sub tree alignment matrix encoding a large number of diverse alignments
between syntactic structures  from which machine translation systems can efficiently extract translation rules that are often filtered out due to the errors in   best alignment 
experimental results show that the proposed approach outperforms three state of the art
baseline approaches in both alignment accuracy and grammar quality  when applied to
machine translation  our approach yields a      bleu improvement and a      ter reduction on the nist machine translation evaluation corpora  with tree binarization and
fuzzy decoding  it even outperforms a state of the art hierarchical phrase based system 

   introduction
recent years have witnessed increasing interest in syntax based methods for many artificial intelligence  ai  and natural language processing  nlp  applications ranging from
text summarization to machine translation  mt   in particular  syntax based models have
been intensively investigated in statistical machine translation  smt   approaches include
string to tree mt  galley  hopkins  knight    marcu        galley  graehl  knight  marcu 
deneefe  wang    thayer         tree to string mt  liu  liu    lin        huang  kevin 
  joshi        and tree to tree mt  eisner        zhang  jiang  aw  li  tan    li       
liu  lu    liu      a  chiang         all of which train on tree string tree tree pairs and
seek to model the translation equivalency relations learned from parsed data  as a part of
the focus on syntax based mt  tree to tree models that use synchronous context free grammars or synchronous tree substitution grammars have received growing interest  showing
very promising results on several well established evaluation tasks  zhang et al         liu
et al       a  chiang         for example  recent studies  chiang        have demonstrated
that modern tree to tree systems can significantly outperform the hierarchical phrase based
counterpart in large scale chinese english and arabic english translation 
in tree to tree mt  the translation problem can be broadly regarded as transformation
from a source language syntax tree to a target language syntax tree  to model this process 
c
    
ai access foundation  all rights reserved 

fixiao   zhu

most tree to tree systems resort to the general framework of synchronous grammars  where
a pair of trees is generated with derivations of synchronous grammar rules  or translation
rules   in such a model  the goal of translation is to build the underlying derivations for
all pairs of trees and output the target string encoded in the most likely derivation  figure
  shows an intuitive example to illustrate the generation process of a tree pair using a
sample grammar  where both the source and target language sentences are associated with
the phrase structure trees generated using automatic parsers  
previous work has shown that the acquisition of good translation rules is one of the essential factors contributing to the success of syntax based systems  deneefe  knight  wang 
  marcu         to date  several research groups have addressed the issue of rule acquisition and designed effective algorithms to extract high coverage grammars from bilingual
parsed data  zhang et al         liu et al       a  chiang         despite their differences
in detailed modeling  all these approaches rely on syntactic alignments that align tree nodes
in the syntactic parse tree in one language to tree nodes in the other  and these alignments
could be employed by standard tree to tree rule extraction algorithms  liu et al       a 
chiang        
while current tree to tree models heavily depend on syntactic alignments between two
languages  all these alignments are induced indirectly from word alignments and tree to tree
systems are very sensitive to the word alignment behavior  unfortunately  word alignments
are in general far from perfect from the viewpoint of syntactic alignment  fossum  knight 
  abney         in some cases  even one spurious word alignment can prevent a large
number of desirable rules from extraction  for example  figure   a  shows some tree totree translation rules extracted using the word alignment produced by giza    this
alignment incorrectly aligns the source word   a past tense marker in chinese  to the
target word the  this spurious word alignment produces an incorrect rule as   
dt the  and blocks the extraction of more high level syntactic transfer rules  such as
ip nn  vp     s np  vp    
obviously  a more desirable solution is to directly infer node correspondences from
the source and target parse trees  namely sub tree alignment  as syntactic parse trees can
explain the underlying structure of sentences well  performing alignment in sub tree level can
make more benefits from the high level structural information and syntactic categorization 
for example  consider the alignment in figure   b   it links up the nodes in the two parse
trees  in chinese and english   rather than aligning them in word level  in this example  it
is very confident to align the vp sub tree  spanning     in the source tree
to the vp sub tree  spanning have drastically fallen  in the target tree   we therefore
   in a phrase structure tree  the leaf nodes are words of the sentence  the internal tree nodes followed by
leaf nodes are labeled with part of speech  pos  tags  while other tree nodes are labeled with syntactic
categories defined in treebanks  see appendix for meanings of the pos tags and syntactic categories used
in this work   in nlp  many well developed parsers are available for automatic parsing  also  several
good quality phrase structure treebanks across languages can be used to train parsing models  such as
the penn english and chinese treebanks  marcus  santorini    marcinkiewicz        xue  xia  chiou 
  palmer         note that in addition to phrase structure syntax  there are other popular formalisms
 e g   dependency syntax  can be used in syntax based mt  but the discussion on different formalisms
of syntactic parsing is beyond the scope of this article  we instead focus on tree to tree mt based on
phrase structure trees throughout this work 
   both chinese and english follow the subject verb object structure  the verb phrases in a chinese
sentence are frequently aligned to the verb phrases in its english translation 

   

fiunsupervised sub tree alignment for tree to tree translation

np

vp

np

vp

prp

vbd

he

was

target language side
 english 

s

vp
vbn

pp

satisfied
pp
in

np

with

r 

r 

dt

nns

the

answers

r 


 ta 


 huida 

p

nn


 biaoshi 


 manyi 

vv

nn

pp

pn

pp

vp

np

vp

np

vp

source language side
 chinese 

r 

 dui 

ip

synchronous grammar used
id
r 
r 
r 
r 

source language side
np pn   
pp p   nn   
vp pp  vp vv   nn    
ip np  vp   

target language side
np prp he  
pp in with  np dt the  nns answers   
vp vbd was  vp vbn satisfied  pp    
s np  vp   

figure    example derivation of tree to tree translation rules  all the rules are represented
as aligned pairs of tree fragments  linked with dotted lines   the subscripts on
both language sides of the grammar rules indicate the alignments of frontier nonterminals  on each language side of the derivation  the round head lines link up
the frontier non terminals that are rewritten during translation 
know that the child nodes in the source language vp are likely to be aligned with the child
nodes in the target language vp  this means that once the two vps are aligned  their
children should not be aligned outside the vp sub tree structure  i e   we can prevent the
alignment between the chinese tree node as and the english tree node dt due to its
inconsistency with the vp vp alignment  in this case  as is correctly aligned to vbp 
   

fixiao   zhu

s

s
np
dt

np

vp

nns

vbp

dt

advp

the imports have

rb

vp

nns

vbp

advp

the imports have

vbn

rb

drastically fallen

drastically fallen








vv

as

ad

nn

vbn




vp



vv

as

ad

nn

vp



vp
vp

ip

ip

 minimal  rules extracted

 minimal  rules extracted
r 

as    dt the 

r 

ad    rb drastically 

r 

nn    nns imports 

r 

vv    vbn fallen 

r 

ad    rb drastically 

r 

as    vbp have 

r 

vv    vbn fallen 

r 

nn    np dt the  nns imports  

r 

ip nn  vp ad  vp vv  as      

r 

vp ad  vp vv  as     
vp vbp  advp rb  vbn    

s np dt  nns    vp vbp have  advp rb  vbn     

r 

 a  word alignment and extracted rules

ip nn  vp     s np  vp   

 b  sub tree alignment and extracted rules

figure    tree to tree translation rules extracted via word alignment  a  or sub tree alignment  b   the dashed lines represent word alignment links  and the dotted lines
represent sub tree alignment  or node alignment  links 
as a result  the bad rule as    dt the  is ruled out  and a few more desirable rules
are extracted using the sub tree alignment  including the desirable rules that are blocked
in figure   a   
actually  researchers have been aware of the sub tree alignment problem and tried to
explore solutions  tinsley  zhechev  hearne    way        sun  zhang    tan      b 
    a   for example  they proposed to judge whether two nodes should be aligned or not  in
their work  the alignment confidence is first calculated using lexical translation probabilities
or classifiers trained on labeled data  and then the final alignment is determined according to
node level alignment score  however  the inference of sub tree alignment in these approaches
relies on heuristic algorithms  and their models are essentially not optimized within a unified
probabilistic framework 
moreover  when the alignment result is applied to tree to tree translation  most systems
suffer from another problem that translation rules are extracted using the   best alignment
only  zhang et al         liu et al       a  chiang         this problem significantly affects
   

fiunsupervised sub tree alignment for tree to tree translation

the rule set coverage rate due to alignment errors  a simple solution to this issue is to use
k best alignments instead  however  k best alignments often have few variations and many
redundancies  most of them differ in only a few alignment links  it is obviously inefficient
to extract rules from those similar alignments 
in this article we address the sub tree alignment issue in a principled way and investigate
methods to effectively apply the sub tree alignment result to tree to tree mt  in particular 
 we develop an unsupervised approach to learning a probabilistic sub tree alignment
model from bi lingual parsed data 
 we investigate different methods for integrating sub tree alignment to tree to tree
machine translation  specifically  we develop a sub tree alignment matrix encoding
an exponentially large number of diverse sub tree alignments  and extract multiple
alternative translation rules using alignment posteriors from the sub tree alignment
matrix 
the advantages of our approach are three fold  first  our approach does not rely on
heuristic algorithms or labeled data  second  the developed sub tree alignment model has
the same structure as the model used in mt  i e   both are based on synchronous tree
substitution grammars  it means that mt systems can directly make benefits from the subtree alignment model  especially for rule extraction and mt parameter estimation  third 
by accessing the sub tree alignment matrix which encodes a large number of alignments 
we can efficiently obtain rules that are often filtered out due to the errors within the  best k best alignment result  we experiment with our approach in chinese english subtree alignment and translation tasks  for sub tree alignment  it significantly outperforms
three state of the art baselines  for machine translation  our approach obtains significant
improvements for a tree to tree system in both rule quality and translation quality  for
example  it yields a      bleu improvement and a      ter reduction on the nist mt
evaluation corpora  finally  our system even outperforms a state of the art hierarchical
phrase based system when equipped with tree binarization  wang  knight    marcu      b 
and fuzzy decoding  chiang        techniques 
the rest of the article is structured as follows  section   briefly introduces the subtree alignment task  section   describes our unsupervised approach to sub tree alignment 
then  section   investigates effective methods for applying our alignment model to tree totree translation  then  section   presents experimental evaluation of our approach  after
reviewing the related work in section    some interesting issues are discussed in section   
finally  the article is concluded with a summary in section   

   problem statement
in general  sub tree alignment can be defined as a task that we find an alignment from the
nodes in a tree to the nodes of another tree   while we restrict ourselves to machine translation in this article  sub tree alignment is actually not a task that must be tightly coupled
with specific applications  for example  in addition to machine translation  there are other
   in this work term tree refers to a data structure that can be defined recursively as a collection of nodes
starting at a root node  each node has a list of edges pointing to nodes  or its children   with the
constraint that no edge is duplicated or points to the root  knuth        

   

fixiao   zhu

nlp tasks which can make benefits from sub tree alignment  including sentence simplification  cohn   lapata        woodsend   lapata         paraphrasing  das   smith 
       question answering  wang  smith    mitamura      a   and parser adaptation and
projection  smith   eisner        
ideally  we would like a sub tree alignment system that is language independent and
application independent  given a parallel corpus of training examples  we should be able
to learn an alignment model and use it to infer the syntactic correspondence for any tree
pairs  broadly speaking  any alignments between paired linguistic tree structures can be
regarded as instances of sub tree alignment  for example  the alignment can be performed
between dependency trees  eisner        nakazawa   kurohashi        or phrase structure
trees  tinsley et al         sun et al       b  
although the sub tree alignment problem includes a number of tasks that seek alignments between syntactic tree structures  we are particularly interested in aligning the tree
nodes of phrase structure trees in this work  we focus on phrase structure sub tree alignment because     phrase structure parsing is one of the most popular syntactic analysis
formalisms  several state of the art full parsing models tools have been developed for many
languages     phrase structure trees are the basis of many successful syntax based mt systems  while other alternatives  such as dependency trees  can also benefit mt systems 
the constituency based models are of interest to a relatively larger portion of the mt community and show state of the art performance in recent tree to tree systems  zhang et al  
      liu et al       a  chiang        
in natural language processing  a phrase structure parse tree is an ordered and rooted
tree  it represents the syntactic structure of a sentence according to some phrase structure grammars  or constituency grammars  which describe the way words combine to form
phrases and sentences  chiswell   hodges         generally  phrase structure parse trees
distinguish between terminal and non terminal nodes  the leaf nodes are labeled by terminal categories  or words   while the internal nodes are labeled by non terminal categories of
the grammar  or phrasal categories   for example  in the english parse tree in figure   b  
imports is a terminal  while nodes np and nns are two non terminals indicating
the noun phrase and the plural form of nouns respectively   in the following description
and experiments  we take the penn treebank for the standard of tree annotation  here we
choose the penn treebank because it is one of the most popular tree annotated corpora
used in syntactic parsing and of good quality and quantity for several languages  such as
chinese and english 
based on the above definition  a sub tree alignment can be defined as the alignments
between non terminals in the source and target language  phrase structure  parse trees  
more formally  given a source language parse tree s and a target language parse tree t   a
sub tree alignment  denoted as a s  t   or a for short  is a set of node to node links between
s and t   for any node pair  u  v  in  s  t    a good alignment should follow three criteria
 tinsley et al         
   u  or v  can only be aligned once  indicating   to   alignment  
   note that the non terminals that are always followed by leaf nodes are also called pre terminals and
labeled by part of speech tags  e g   the nns node is followed by the terminal node imports and
thus is a pre terminal 
   in contrast  a word alignment can be regarded as the alignments between terminals in two languages 

   

fiunsupervised sub tree alignment for tree to tree translation

   if u is aligned to v  the descendants of u can only be aligned to the descendants of v 
   if u is aligned to v  the ancestors of u can only be aligned to the ancestors of v 
such criteria prevent from aligning constituents that cross each other  this property
is very similar to that of some bi parsing formalisms  such as synchronous context free
grammars and synchronous tree substitution grammars  its advantage is that it enables the
use of powerful synchronous grammars in modeling the sub tree alignment problem  as is
shown in the very next section  based on the above constraints we can take synchronous
tree substitution grammars as the basis of our proposed model 
according to tinsley et al s        work  the alignments satisfying the above criteria
are called well formed alignments  an alignment is ill formed when it violates any of these
criteria  in this work we focus on the well formed alignments  hence the sub tree alignment
task can be stated as  given a pair of parse trees  s  t    we search for the most likely wellformed alignment between s and t
a   arg max p a   s  t  

   

a s t  

where  s  t   is the set of well formed alignments  and p a   s  t   can be viewed as an
alignment model which predicts the probability for every alignment a given s and t  
in what follows  we describe our approach to sub tree alignment for tree to tree translation  including the alignment model  the training and inference methods  and the effective
use of our model in tree to tree mt systems 

   unsupervised sub tree alignment
in this section we present our unsupervised sub tree alignment model  we first define
the base model of sub tree alignment in the framework of synchronous tree substitution
grammars  and then describe the model parameterization  training and inference methods 
    base model
the fundamental question of sub tree alignment is how to define the correspondence between
nodes of the source language parse tree and nodes of the target language parse tree  here we
address the issue using synchronous tree substitution grammars  stsgs  which have been
widely adopted to model the transformation process between source and target language
parse trees in mt  zhang et al         liu et al       a  chiang         in the general
framework of stsgs  chiang   knight         it is assumed that a pair of source and
target parse trees can be simultaneously generated using a derivation of stsg rules  or
tree to tree transfer rules   for example  the grammar in figure   is an stsg and the
rules in it can be used to generate a pair of sentences  more formally  an stsg is a system
hns   nt   ws   wt   i  where ns and nt are sets of non terminals in the source and target
languages  ws and wt are sets of terminals  or words  in the source and target languages 
 is a finite set of productions  each production is an stsg rewrite rule  denoted as r  for
a pair of source and target language non terminals  snt   tnt   
hsnt   tnt i  hsr   tr   r i
   

fixiao   zhu

where sr is a source language tree fragment  whose frontier nodes are either words in ws or
non terminals in ns  labeled by x   tr is the corresponding target language tree fragment 
and r is a set of   to   alignments that connect the frontier non terminals of sr to the
frontier non terminals of tr   for example  for r  in figure   a   we have
snt   ip
tnt   s
sr   ip nn x vp ad x vp vv x as x   
tr   s np dt x nns x  vp vbp have  advp rb x vbn x   
r                       
note that the non terminals on the left hand side of the rule are actually the roots of
the corresponding tree fragments on the right hand side  this means that the rule contains
exactly the same information no matter whether the root nodes  snt   tnt   are explicitly
represented or not  so in the following parts of this article we use hsr   tr   r i for a simpler representation of stsg rules  beyond this  stsg rules can be written in a more
compact form where the alignment r is encoded in the numbers assigned to the frontier
non terminals of sr and tr   for example  in figures   and    the subscripts on both language
sides of the stsg rules indicate the aligned pairs of frontier non terminals 
in the stsg model  frontier non terminals are also called substitution nodes  when
applying stsgs  we can rewrite an aligned pair of substitution nodes with the tree fragment
pair encoded in an stsg rule  the only constraint in this operation is that the labels of
the substituted non terminals must match the root labels of the rewrite rules  for example 
the round head lines in figure   show the substitution operations used in a derivation 
by using stsg rules  we can parse any tree pair and generate the corresponding derivations  the generation process is trivial  we start with the pair of root symbols and repeatedly rewrite pairs of non terminal symbols using stsg rules  for example  for the tree pair
in figure   b   we start with the root labels of the source and target language parse trees
 the superscript indicates the node index in the tree 
h ip      s    i
then we apply rule r   
ip    s   

 h ip nn    vp       s np    vp      i
r 

ip    s   

where  represents the operation that rewrites the aligned node pair ip    and
r 

s    with r   denoted as ip     s       this process proceeds by repeatedly rewriting the
remaining frontier non terminals until we get the complete source and target language trees 
like so 
   

fiunsupervised sub tree alignment for tree to tree translation

nn    np   


r 

vp    vp   


r 

h ip nn   vp       s np dt the  nns imports   vp      i
h ip nn   vp ad

   

   

vp vv

as        

s np dt the  nns imports   vp vbp
ad    rb   


r 

h ip nn   vp ad   vp vv

   

s np dt the  nns imports   vp vbp
   

vv

   

advp rb

   

vbn        i

as        

   

advp rb drastically  vbn        i

   

vbn

 h ip nn   vp ad   vp vv   as        
r 

s np dt the  nns imports   vp vbp

   

advp rb drastically  vbn fallen     i
as    vbp   


r 

h ip nn   vp ad   vp vv   as      
s np dt the  nns imports   vp vbp have 
advp rb drastically  vbn fallen     i

in the above process  each rewrite rule indicates a node alignment  more importantly 
derivations from this model have two very nice properties  first  for each node u in the
source language  or target language  parse tree  there is at most one node of the targetlanguage  or source language  parse tree which is aligned with u  second  the hierarchical
structure behind the alignment avoids links between constituents that cross each other 
consequently  for any well formed sub tree alignment a  we can always find a derivation d
that encodes the alignment a  it means that the sub tree alignment problem is essentially
the same as the problem of finding the most likely stsg derivation  thus the sub tree
alignment task  see equation      can be restated as finding the most likely derivation for
a given pair of parse trees 
to model the derivation probability  we follow the formulation adopted in statistical
word alignment  brown  pietra  pietra    mercer        vogel  ney    tillmann        
the transformation from a source language tree s to a target language tree t is described
by the following equation 
x
p t   s   
p  t  d s 
   
dd s t  

where d s  t   is the set of all derivations transforming s into t  say  aligning the nodes
of s to the nodes of t    p  t  d s  is the probability of transforming from s to t using a
derivation d  d s  t    and  is the parameters of the model  here we use the notation
p    to express the dependence of the model on the parameters  in general  the optimal
value of  is learned from parsed parallel data by some training criteria  for example  in
the context of unsupervised learning  we can optimize the model parameters by maximizing
the probability of the observed data  known as maximum likelihood training  
given a set of optimal parameters   the best sub tree alignment for  s  t   is determined
p  t d s 
by choosing a derivation for which p  d   s  t   is greatest  since p  d   s  t     p  t  s 


   

fixiao   zhu

and p  t   s  is a constant for given  s  t   and   finding the best derivation d is the same
as finding a derivation so as to make p  t  d   s  as large as possible  hence we reach the
fundamental equation of sub tree alignment 
d   arg max p  t  d   s 

   

dd s t  

the above formulation implies three fundamental issues of sub tree alignment  including
modeling of derivation probability  i e   p  t  d s    learning of model parameters  i e    
and finding the best alignment given the learned model  i e   the arg max operation   in
the following parts of this section  we describe our solutions to these issues 
    parameterization
in the simplest case  our alignment model has one parameter for each instance of derivation 
however  this model would have an unmanageable set of parameters since the number of
derivations is exponential in the length of the input sentences  here we choose a simple
solution to this issue which decomposes the base model into a product of trainable submodels  we start with an assumption that rules are conditionally independent for the given
source language parse tree s  then the probability p t  d   s  can be defined as a product
of rule probabilities  for conciseness  we will drop the subscript  from now on  
y
p t  d   s  
p r   s 
   
rd

nevertheless complex tree to tree mappings still result in an extremely large number of
rules  which causes both the computational problem and the degenerate analysis of the
data   to control the number of parameters at a reasonable level  we further decompose
the rule probability into simpler probability factors under independence assumptions 
first we assume that the generation of a rule r is independent of the input tree s  when
conditioned on the source language side of the rule  that is 
p r   s   p r   sr  

   

note that this is a strong assumption that the generation of a synchronous grammar
rule depends only on its source language side  it is similar to those used in statistical
modeling of machine translation  brown et al         koehn  och    marcu        galley
et al         chiang        where the generation of atomic alignment translation units are
conditioned on the associated source language words or tree fragments  rather than the
whole input sentence or tree  in smt  the independence assumptions based on phrases or
translation rules are generally used to decompose the parallel corpus into manageable units
for parameter estimation  as they have been successfully used in most of modern smt
systems  we adopt a similar assumption here to ease the parameter estimation process of
our model 
then we further decompose p r   sr   with additional assumptions  since r   hsr   tr   r i 
p r   sr   can be written into another form using the chain rule 
   here degenerate analysis refers to the case where using models that are too complex results in overfitting
and a poor generalization ability on unseen data 

   

fiunsupervised sub tree alignment for tree to tree translation

p r   sr     p sr   tr   r   sr  
  p r   sr   tr    p tr   sr  

   

equation     indicates two sub models  including a reordering model of frontier nonterminals p r   sr   tr    and a tree fragment translation model p tr   sr   
to model p r   sr   tr    we view frontier non terminal reordering as a problem of aligning
the elements between two vectors of non terminals  let vnt   be a function that returns
the vector of leaf non terminals for a given tree fragment  r defines a   to   alignment
between the non terminals in vnt sr   and vnt tr    for example  for r  in figure   a   the
frontier non terminal vectors of sr  and tr  are 
vnt sr       nn  ad  vv  as 
vnt tr       dt  nns  rb  vbn 
then r                         indicates an alignment between vnt sr    and vnt tr     say 
nn is aligned to nns  ad is aligned to rb and so on  here we opt for a simple model
for selecting r   it models the non terminal reordering probability on the condition of the
frontier non terminal vectors on both language sides  as follows 
p r   tr   sr    preorder  r   vnt sr    vnt tr   

   

we then turn to the problem of modeling the tree fragment translation p tr   sr    i e  
the second sub model defined in equation       we define that a tree fragment  consists of
two parts  words lex    i e   terminals of   and a tree structure tree   without lexicons
involved  for example  for r  in figure   a   the target language tree fragment contains
two elements lex tr    and tree tr    
lex tr      have
tree tr      s np dt x nns x  vp vbp advp rb x vbn x   
let root   be a function that returns the root for a given tree fragment  we can write
p tr   sr   as 
p tr   sr     p lex tr    tree tr     sr  
  p root tr     sr   
p tree tr     root tr    sr   
p lex tr     tree tr    root tr    sr  

   

it is worth noting that equation     is not an approximation  here we just choose
one of the many ways in which p tr   sr   can be written as the product of a series of
   the reordering model defined here ensures that arbitrary   to   alignments can be handled  but it might
result in a very large model with sparse parameter distributions if big tree fragments are involved  in
considering this issue  we choose several pruning methods for better control of rule size in our sub tree
alignment system  see section       for the pruning settings in this work 

   

fixiao   zhu

conditional probabilities  we simply assert this equation that when generating a targetlanguage tree fragment for a source language tree fragment  first we can choose the root
symbol of the target language tree fragment given the source language tree fragment  in
probability of p root tr     sr     then we can choose the tree structure of the target language
tree fragment given its root symbol and the source language tree fragment  in probability of
p tree tr     root tr    sr     then we can choose the target language terminals associated with
the tree fragment given the target language tree structure  target language root symbol and
source language tree fragment  in probability of p lex tr     tree tr    root tr    sr    
another note is that equation     actually does not reduce the model complexity  for
example  p lex tr     tree tr    root tr    sr   essentially indicates all the combinations of source
and target language tree fragments  a simpler model is required for a feasible solution
for parameter estimation  to do this  we introduce additional assumptions to relax the
conditions of these probabilities and reduce the number of parameters to a reasonable level 
   p root tr     sr   depends only on root sr    i e  
p root tr     sr    pnt  root tr     root sr   

   

this assumption implies the node correspondence between the source and targetlanguage parse trees 
   p tree tr     root tr    sr   depends only on root tr    i e  
p tree tr     root tr    sr    ptree  tree tr     root tr   

    

the second assumption results in a monolingual model of generating target language
tree structures  where the generation of a tree fragment is only conditioned on its
root  it can be viewed as an analogy to the generative model used in standard tsgs 
   p lex tr     tree tr    root tr    sr   only depends on source words lex sr    i e  
p lex tr     tree tr    root tr    sr    plex  lex tr     lex sr   

    

this allows us to directly model the terminal correspondence between the two languages 
then  we substitute equations          into equation      and get
p tr   sr    pnt  root tr     root sr    
ptree  tree tr     root tr    
plex  lex tr     lex sr   
by using equations          and       equation     can be finally written as 
   

    

fiunsupervised sub tree alignment for tree to tree translation

id

rule

probability

r 

ad    rb drastically 

pnt  rb   ad   plex  drastically    

r 

vv    vbn fallen 

pnt  vbn   vv   plex  fallen    

r 

as    vbp have 

pnt  vbp   as   plex  have    

r 

nn   

pnt  np   nn   plex  the imports    

np dt the  nns imports  

ptree  np dt nns    np 

vp ad  vp vv  as     

pnt  vp   vp   ptree  vp vbp advp rb vbn     vp 

vp vbp  advp rb  vbn    

preorder                   ad  vv  as    vbp  rb  vbn  

ip nn  vp     s np  vp   

pnt  s   ip   ptree  s np vp    s 

r 
r 

preorder              nn  vp    np  vp  

table    rule probabilities for the sample derivation d    r    r    r    r    r    r    in figure   b 
p t  d   s  

y

pnt  root tr     root sr    

rd

ptree  tree tr     root tr    
plex  lex tr     lex sr    
preorder  r   vnt sr    vnt tr   

    

this is a simplified model for the generative story described in this section  it takes
the rule generation probability as a product of four probability factors  pnt    is the nonterminal mapping probability  which roughly captures the syntactic correspondence of subtrees between the two languages  ptree    is the probability of generating the tree structure
of t   plex    is the probability of the terminal mappings between the two language sides of
a rule  and preorder    is the probability of the frontier non terminal reordering encoded in
a rule  see table   for rule probabilities of a sample derivation 
in our model all parameters are assumed to be multinomial distributions  the calculation of pnt     ptree    and preorder    is straightforward  they can be directly used
without any further decompositions and assumptions  to calculate plex     we choose the
form adopted in the popular models of word alignment  och   ney        thayer  ettelaie  knight  marcu  munteanu  och    tipu         where the probability is defined as a
product of word based translation probabilities 
l
m
y
  x
plex  t     tl   s     sm    plength  l   m 
pw  ti   sj  
m
i  

    

j  

where ti is a target word  and sj is a source word  plength    is used to control the number of
target words produced from a given number of source words  pw    is the word translation
probability  this sub model is in principle doing something rather similar to conventional
word based translation tables  as in ibm models  brown et al         
    node deletion and insertion
word  or sub tree  deletion insertion is common in real world alignment and translation
tasks  to add flexibility to modeling this problem  we allow for the production of empty
   

fixiao   zhu

sub trees on either source or target language side of a rule in our model  more formally  for
a rule whose target language side is an empty sub tree  its probability is defined as 
p r   s   pnt  root     root sr    
ptree  tree     root    
plex  lex     lex sr    
preorder     vnt sr    vnt   

    

where  is a special symbol that indicates nothing  factors pnt  root     root sr    and
plex  lex     lex sr    model the deletion probability at different levels of a tree fragment 
ptree  tree     root    is the probability of generating an empty tree fragment  factor
preorder     vnt sr    vnt    regards  as a special reordering pattern  which aligns all
frontier non terminals on the source side to a virtual node null  obviously  the values of
ptree  tree     root    and preorder     vnt sr    vnt    are both simply   
similarly  for a rule whose source side is an empty sub tree  its probability is defined as 
p r   s   pnt  root tr     root    
ptree  tree tr     root tr    
plex  lex tr     lex    
preorder     vnt    vnt tr   

    

where the value of preorder     vnt    vnt tr    is also   
it is worthwhile to note that word deletion and insertion problems are very important
in mt in spite of relatively less discussion in recent studies on tree to tree translation 
what we are doing here is actually an analogy to the null alignment used in ibm models
 brown et al          for word phrase based models  removing some words from alignment
can leave more space for correctly aligning other words in the sentence   this is even more
necessary for    to    sub tree alignment because the alignment has to respect the syntactic
constraints on both language sides  e g   sub tree alignments are not allowed to break the
constraints imposed by neighbouring parts of the tree  in some cases  we cannot obtain a
correct   to   alignment for the tree pair due to only one or two bad nodes which are not
necessarily to be aligned with a valid node in the counterpart tree  instead  some nodes
can be skipped in alignment and thus do not impose bad constraints to other parts of
the tree if node deletion insertion is allowed  this is especially true when we align sentence
pairs with very flat tree structures or free translations  in this work we found that node
deletion and insertion operations were necessary to achieve satisfactory sub tree alignment
result  we therefore used them in our implementation by default 
    training
we now turn to the training problem  as discussed in section      we focus on unsupervised
learning of model parameters  that is  the optimal values of parameters are estimated given
   note that current phrase based approaches  koehn et al         och   ney        allow null aligned
words to appear at the boundary of a phrase  which can be viewed as a way of implicit modeling of the
word insertion deletion problem

   

fiunsupervised sub tree alignment for tree to tree translation

a collection of tree pairs without any annotation of sub tree level alignment  in this work we
choose two approaches for estimating parameters of the sub tree alignment model  including
the maximum likelihood estimation  mle  approach and the bayesian approach 
      maximum likelihood training
mle is one of the most popular methods of parameter estimation for statistical models  its
basic idea is that  given a model and a set of parameters  the mle method selects the values
of parameters to generate a distribution that gives the highest probability to the observed
data  mle is a general approach to parameter estimation which has been widely adopted in
many ai and nlp tasks  such as part of speech tagging  in the case of sub tree alignment 
mle can be simply described as finding the optimal values of parameters that lead to the
maximum probability of aligning the tree nodes from a source language parse tree to a
target language parse tree  more formally  given a set of tree pairs   s    t           sn   tn    
the objective of the mle based training is defined to be 
   arg max


n
y

x

p  ti   d   si  

    

i   dd si  ti  

we choose here expectation maximization  em  dempster  laird    rubin        as
the algorithm to solve the above optimization problem  basically the em algorithm is an
iterative training method for finding the maximum likelihood estimates of model parameters 
where it is assumed that the observed data depends on some latent variables  the algorithm
performs by iteratively calling two sub routines  namely the expectation  e  step and the
maximization  m  step  in the e step  it calculates the expected value of the likelihood
function associated with the parameters and observed data  with respect to the distribution
of latent variables given the observed data and current estimates of parameters  in the
m step  it seeks for the parameters that maximize the expected likelihood found in the
e step 
when applying the em algorithm to our case  we can view the input pairs of parse
trees   s    t           sn   tn    as observed data  the underlying derivations of rules as latent
variables  and the distributions pnt     ptree     preorder    and plex     i e   plength    and
pw     as unknown parameters  see figure   for the pseudo code of the training algorithm
for pnt     denoted as tnt  snt    because this algorithm is directly applicable to the estimation
of all parameters in our model  we skip the description of learning the remaining parameters
here  for a more detailed description of the em based training of all model parameters we
refer the reader to the appendix 
in the algorithm  snt and tnt represent a source language non terminal symbol and a
target language non terminal symbol  u and v represent a source language tree node and
a target language tree node  ec   represents the expected count of the given variable 
p k   t  d   s  represents the derivation probability based on the parameters obtained in
the k th round  in each em iteration  the e step of the algorithm accumulates the expected
count over all pairs of parse trees  then  the m step finds the maximum likelihood estimate
using this quantity  the only nontrivial part in this algorithm is the computation of the
expected count in the e step  roughly speaking  the physical meaning of the right hand
side of line   is the relative probability that a derivation contains rule r  with root node
   

fixiao   zhu

   function trainmodelwithem    s    t           sn   tn    
   
   set  tnt  snt     an initial model
   for k     to k    do
  
foreach non terminal symbol pair  snt   tnt   do
  
ec tnt  snt      
   e step 
  
foreach tree pair  s  t   in sequence   s    t           sn   tn    do
  
foreach node pair  u  v  with symbol pair  tnt   snt   in  s  t   do
  
foreach rule r rooted
p at  u  v  do
p

is rooted at  u v  
  
ec tnt  snt       d  rd  r p
 
d  p  k   t d  s 
   m step 
   
foreach non terminal symbol pair  snt   tnt   do

   
   

 k   

tnt  snt  

p

 k   t d s 

ec tnt  snt  
ec t   s  

t nt

nt

nt

 k 

return  tnt  snt  
figure    the em based training algorithm  for pnt    

p
pair  u  v    the numerator d  rd  r is rooted at  u v  p k   t  d   s  is the probability sum
p
over all the derivations that involve r   while the denominator d  p k   t  d    s  is the
overall probability of the alignment from s to t   however  the brute force computation of
expected counts is very inefficient because it requires the sum over all possible derivations
whose number is exponential in the length of the input sentences 
in this work we use the bilingual version of the inside and outside probabilities  manning
  schutze        to avoid the naive enumeration of all possible derivations in computing
various probabilities  the inside probability of  u  v   denoted as  u  v   measures how
likely we generate the sub tree pair inside the node pair  u  v   the outside probability
 denoted as  u  v   is a dual of the inside probability  it measures how likely we generate
the remaining parts of the tree pair  s  t   from the start symbols  like the formulation
used in monolingual parsing  manning   schutze          u  v  and  u  v  can be defined
using the following recursive forms 


x
y
 u  v   
p r   s  
 p  q 
    
r root r   u v 

 u  v   

x

 p q yield r 



 root r    p r   s  

r  u v yield r 

y

 p  q 



    

 p q yield r 
  p q    u v 

where root r  is the abbreviation of the node pair  root sr    root tr     and yield r  is the
set of the aligned frontier non terminal pairs yielded by r  based on the above recursive
definitions  both  u  v  and  u  v  can be efficiently computed with dynamic programming 
by using the inside and outside probabilities  it is easy to address the computation
problem mentioned above  let  u  v  denote the probability that the tree node u is aligned
with the tree node v  this probability can be expressed in an inside outside fashion 
   

fiunsupervised sub tree alignment for tree to tree translation

 u  v   

x

p t  d   s 

d   u v  is
aligned in d

   u  v    u  v 

    

in this way  the overall alignment probability from s to t  i e   the denominator of the
right hand side of line    can be simply written as 
x

p t  d   s     root s   root t      root s   root t   

    

d

for the numerator of the right hand side of line    let us view it in another angle  in the
e step of the algorithm  the expected count is accumulated over rules whose root is  u  v  
as all the rules rooted at  u  v  indicate the same node alignment between u and v  lines
    in principle imply the probability of all derivations aligning u to v  or more precisely
the node alignment probability of  u  v   this probability can be written in a very simple
form using the inside and outside probabilities 
x
x
p t  d   s     u  v 
r  r is rooted d  rd
at  u v 

   u  v    u  v 

    

together with the result in equation       the e step can be efficiently implemented by
replacing lines     with the following equation
ec tnt  snt      

 u  v    u  v 
 root s   root t      root s   root t   

    

where  snt   tnt   is the symbol pair of  u  v   note that for each  snt   tnt    the e step step
 u v  u v 
increases ec tnt  snt   by the sum of  root s  root t
   root s  root t    over all node pairs  u  v 
whose symbols are snt and tnt   this means that if  snt   tnt   is aligned at different positions
in the input tree pair  the above method considers the alignment of  snt   tnt   for multiple
times and updates ec tnt  snt   accordingly 
it is also worth noting that there are several methods for initializing the model parameters before the em style training begins  for example  the model can be initialized by
uniform or random distributions  in this work we initialize all parameters in our sub tree
alignment model by the model obtained using the word alignment result  this is a standard way adopted in many unsupervised models where a simpler model is used for a good
starting point in the training process  it is very helpful when the optimization procedure is
sensitive to the initial setting of model parameters  e g   em for non convex objective functions   in our experiments we found that using the giza   word alignment for parameter
initialization resulted in better performance and fewer iterations for convergence than the
uniform initial distributions  as the word alignment can be obtained in an unsupervised
manner  it does not change the training condition of our approach  thus we chose this
method for initializing the model parameters in our implementation 
   

fixiao   zhu

      the bayesian approach
while mle is one of the standard approaches to training unsupervised models  it is well
known for its tendency to overfit the data  the overfitting problem becomes more severe
for complex models since they have more parameters and fit the training data better  in
the case of stsgs  this is likely to result in the degenerate analysis of the data  i e   rare
and big rules dominate the ml solution of stsgs  while they are considered to be noisy
and generalize poorly on the unseen data  cohn   blunsom        liu   gildea        
a natural solution to this problem is to incorporate constraints or proper priors into the
training process  here we take the bayesian approach as an alternative solution for the
training problem 
unlike mle  the bayesian approach does not plug a single optimum point estimate of
the parameter into the distribution of a data point  but instead account for any uncertainty
in the value of the parameter  in bayesian models  the parameters are assumed to be
drawn from some probability distributions or priors  the parameters of these extra prior
distributions are called hyperparameters  and are denoted by   as the parameters of
our model are viewed mathematically as multinomials  we choose dirichlet distributions
 ferguson        for the prior over model parameters  the advantage of using dirichlet
distributions is that they are conjugate to multinomial distributions and inference with
such priors is easier 
following the previous description  we use  to denote the model parameters which are
multinomial with outcomes          k   i e   k is the probability of outcome k           k   
from this multinomial distribution we sample a set of outcomes  x         xn   with probability
p xi   k    k   as the dirichlet prior is a distribution over multinomials  each sample
from the prior is actually a set of parameter values   therefore the distribution can be
modeled as 
xi     multinomial  

    

     dirichlet  

    

here equation      means that xi is distributed according to a multinomial with parameters
  similarly  equation      can be read as  is distributed according to a dirichlet distribution with parameters                k   is the hyperparameter vector corresponding to the
outcomes  in this work we use a symmetric dirichlet prior   i e            k share the same
value   and use  to represent the single hyperparameter instead of the hyperparameter
vector 
using this model  we can compute the conditional distribution of a new observation
xn   given previous observations  x         xn   and the hyperparameter   as follows 
z
p xn     x         xn       p xn     x         xn     p      d
    
the big advantage of the bayesian approach is to introduce a prior distribution over
the unknown parameters of the model  which is meant to capture the knowledge and beliefs
about the model before seeing the data  neal         it is especially important in our case
where we need a bias towards some preferred situations  for example  we expect that
our model can favor high frequency rules and dislike rare and big rules  this goal can be
   

fiunsupervised sub tree alignment for tree to tree translation

easily achieved by using the bayesian approach and an appropriate choice of priors  say  a
dirichlet prior with a low concentration parameter   however  the introduction of priors
generally makes it intractable to estimate the posterior analytically  in practical systems
based on the bayesian approach  a widely used solution is to use approximate methods to
seek a compromise between exact inference and computational resources  in this work we
choose variational bayes for approximate inference  variational bayes is a good method
that preserves the benefits of introducing the prior but with a tractable inference procedure
 attias        beal         it has been successfully applied to several nlp related models 
such as hidden markov models  hmms  and ibm models  beal        riley   gildea 
       one more good thing is that variational bayes can be seen as an extension of the
em algorithm and resembles the usual forms used in em  the resulting procedure looks a lot
like the em algorithm with a modified m step  which is very convenient for implementation 
here we follow the approach presented in previous work  beal        riley   gildea       
where variational bayesian algorithms are applied to similar tasks  all we need is only a
very slight change to the m step of the original em algorithm presented in section       
in the original em algorithm  see figure     the m step normalizes the expected counts
collected in the e step as standard mle  the variational bayesian version of the m step
slightly modifies this formula and performs an inexact normalization by passing counts
through function f  x    exp  x   

tnt  snt  

f  ec tnt  snt      
p
f   t   ec t nt  snt       

    

nt

where  x  is the digamma function  johnson         it has an approximate effect of
subtracting     from its argument  the choice of  controls the behavior of this estimation 
when  is set to a low value  it performs estimation as a way of anti smoothing  as
about     is subtracted from the rule counts  small counts corresponding to rare events are
penalized heavily  while large counts corresponding to frequent events are not be affected
very much  for example  low values of  make equation      favor the non terminal pairs
which are aligned frequently and distrust the non terminal pairs which are aligned rarely 
in this way  the variational bayesian method could control the overfitting caused by abusing
rare events  on the other hand  a larger  can be used when smoothing is required 
the above method is applicable to training all the parameters in our model  it only
requires a replacement of the m step in figure   with the variational bayesian m step  as
in equation        in our implementation  after the variational bayes based training  we
perform an additional round of normalization without variational bayes to normalize rule
probabilities to sum to one  
   the additional normalization process makes the posterior probabilities directly comparable with those
obtained in other training methods  such as the em based training  note that here we convert the result
of bayesian inference into some probability distributions for a good explanation of various probability
factors in our model  on the other hand  this technical trick results in a pseudo bayesian procedure
that is not doing bayesian inference exactly though it shows good results in our empirical study  one
can remove the additional round of normalization for a pure bayesian approach  but all these changes
do not affect the overall pipeline of our approach  from a practical standpoint  

   

fixiao   zhu

   function decode
 s  t  

  
         getinsideoutsideprobabilities  s  t  
  
foreach node u in s in bottom up order do
  
foreach node v in t in bottom up order do
  
 u  v     u  v    u  v 
  
foreach tree fragment sr rooted at u do
  
foreach tree fragment tr rooted at v do
  
foreach frontier non terminal alignment a between sr and ts do
  
r   createrule s
q r   tr   a 
   
score   p r   s    p q yield r  p d p  q  
   
if score   p d u  v   then
   
d u  v    createderivation r   d p  q     p  q   yield r   
    return  d       
    function getinsideoutsideprobabilities  s  t  
    foreach node u in s in bottom up order do
   
foreach node v in t in bottom up order do
   
set  u  v  according to equation     
    foreach node u in s in top down order do
   
foreach node v in t in top down order do
   
set  u  v  according to equation     
    return         
figure    decoding algorithm of the proposed sub tree alignment model for both   best
and posterior based outputs
    decoding
inference with our model is straightforward  the simplest case is inferring the   best subtree alignment  given a set of learned parameters  we first visit every node pair  u  v  in
a bottom up fashion  and compute the posterior probability of aligning the sub tree pair
rooting at  u  v   this procedure is the same as the dynamic program used in the trainer 
we then select the derivation with the maximum sub tree alignment probability for the
input tree pair  also  we can generate a list of k best derivations in a similar manner 
in addition to the   best k best output  our model is able to output the alignment
posterior probability for every pair of tree nodes  to do this  we only need to record the
probability  u  v  for each node pair after we obtain the inside and outside probabilities 
note that outputting alignment posterior probabilities is also commonly used in statistical
word and phrasal aligners  it provides a flexible way of making use of the alignment result
for the downstream components  such as the rule extraction system  as is presented in the
very next sections  tree to tree mt systems can make great benefits from the posteriorbased alignment output  which results in a very effective rule extraction method as well as
better translation results 
figure   depicts the pseudo code of the decoding algorithm for both   best and posteriorbased outputs  in this algorithm  d x  y  is a data structure that records the best derivation rooted at  x  y    x  y    x  y  and  x  y  are data structures that record the inside
probabilities  output probabilities and alignment posterior probabilities  respectively  cre   

fiunsupervised sub tree alignment for tree to tree translation

aterule   creates a rule with a pair of tree fragments  sr   tr   and a frontier non terminal
alignment a  and calculates the rule probability  createderivation   builds a derivation
using the input rules  for output  we can access the   best alignment by traversing from
d root s   root t     and access the alignment posterior through    
given a pair of trees  s  t    the outer two loops of the algorithm iterates over each pair
of nodes in the two trees  resulting in time complexity of o  s    t    where      represents the
     where n
size of the input tree  generating all pairs of tree fragments requires o ntree
tree
is the maximum number of tree fragments given a tree node  computing the alignment
between  sr   tr   requires o l   where l is the maximum number of leaf non terminals in a
 
rule  therefore the time complexity of this algorithm is o  s    t    ntree
 l    quadratic
in the size of the input trees  note that the actual time complexity of this algorithm could
be very high if all potential alignments are considered  for example  ntree is generally an
exponential function of the depth of the input tree fragment  and a very deep tree could
results in an extremely large space of alignments  to make practical sub tree alignment
systems  pruning techniques are taken into account in this work  for example  in our
implementation  we restrict the depth of tree fragment to a reasonable number  see section
        in addition  as is commonly used in phrasal alignment and related tasks  we consider
word alignments in pruning and discard the sub tree alignments which violate a certain
number of word alignments  for example  we throw away the sub tree alignments if there
are more than two word alignment links outside the spans covered by the aligned sub trees 

   applying sub tree alignment to tree to tree translation
once sub tree alignment is obtained  current tree to tree systems can directly learn translation rules from node aligned tree pairs  in this section we investigate methods of applying
sub tree alignment to tree to tree rule extraction 
    rule extraction using   best k best sub tree alignments
to data  several methods have been developed for tree to tree rule extraction  zhang et al  
      liu et al       a  chiang         the most popular of these is the ghkm like
method which extends the idea of extracting syntactic translation rules from string tree pairs
 galley et al          in ghkm like extraction  we first compute the set of the minimallysized translation rules that can explain the mappings between the source language tree
and the target language tree while respecting the alignment and reordering between the
two languages  larger rules are then learned by composing two or more minimal rules  for
example  in figure   b   r  and r  are two minimal rules extracted according to the sub tree
alignment  we can compose these rules to form a larger rule  like this 
ip nn   vp     s np dt the  nns imports   vp   
in this work we use the tree to tree version of ghkm like extraction which is described
in liu et al s      a  work  see figure   a  for the pseudo code of rule extraction with
  best sub tree alignment  we choose this method because it has been widely used in
most tree to tree systems  note that rule extraction in tree to tree translation is generally
not restricted to be performed on the   best sub tree alignment result  the ghkm like
   

fixiao   zhu

   function onebestextract  s  t   a 
  
foreach node u in s do
  
foreach node v in t do
  
foreach tree fragment pair  sr   tr  
  
rooted at  u  v  do
  
a   onetoonealign sr   tr   a 
  
if a is not empty then
  
r   createrule sr   tr   a 
  
rules add r 
  
return rules
    function onetoonealign sr   tr   a 
    if frontier non terminals of  sr   tr   have
   
  to   alignments in a then
   
return frontier alignment of  sr   tr  
    else
   
return 

   function matrixextract  s  t   m  
  
foreach node u in s do
  
foreach node v in t do
  
if isextractable   u  v    m   do
  
next loop
  
foreach tree fragment pair  sr   tr  
  
rooted at  u  v  do
  
foreach frontier alignment a
  
between  sr   tr   do
  
if isextractable a  m   then
  
r   createrule sr   tr   a 
   
rules add r 
    return rules
    function isextractable a  m  
    foreach alignment  p  q  in a do
   
if probability of  p  q  in m   pmin then
   
return false
    return true

 a    best extraction

 b  matrix based extraction

figure    the   best and matrix based rule extraction algorithms
extraction method can be employed when a list of k best sub tree alignments is provided 
in k best extraction we only need to repeat the procedure of   best extraction for each
sub tree alignment in the k best list 
    rule extraction using sub tree alignment matrices
previous work has pointed out that current mt systems suffer from error propagation due
to the alignment errors made within   best alignment  venugopal  zollmann  smith   
stephan         as sub tree alignment is an early stage step in the training pipeline  the
errors in   best alignment are likely to be propagated to translation rule extraction and
parameter estimation of the translation model  though this problem can be alleviated by
using k best alignments  the limited scope of k best alignments still results in inefficient
learning of translation rules  for example  our preliminary experiment shows that       of
the extracted rules are redundant when     best alignments are involved 
here we instead present a simple but efficient method  namely matrix based rule extraction  in this method  we use the posterior based output of our aligner and represent the
sub tree alignment with a compact structure   call it sub tree alignment matrix or alignment
matrix for short  liu  xia  xiao    liu      b  de gispert  pino    byrne        
see figure   a  for two example sub tree alignment matrices made from a pair of sentence segments  in the matrices  each entry is indexed by a pair of source and target nodes 
the score in an entry is the posterior probability of the alignment between the corresponding node pair  i e   the  u  v  probability defined in equation       this probability is
straightforwardly accessible in the output of the inference algorithm described in section
     in principle  u  v  can be viewed a measure of sub tree alignment confidence  a higher
value indicates a more confident alignment between the two nodes  in this way we can
   

fihave

rb

   

vbn

drastically



 

fallen





vv   

as   

ad   

  

as   

  

  

vp   

  

ad   

  

  

  

  

  

  

vp   

  

  

vv   

  

as   

  vv   
 

vb
n 

  

vb
p    
ad
vp  
  
rb  

  

ad   
vp   

vp   
vp   

vp   

 

   

vp  

advp   

vb

vbp   

vp  

  

vp   

p    
ad
vp  
  
rb  
  
vb
n    

unsupervised sub tree alignment for tree to tree translation

  

   fixed alignment

   possible alignment

matrix      best alignment

matrix    posterior

 a  sub tree alignment matrices for a sample sub tree pair
minimal rules
extracted from matrix    posterior 
r 
ad    rb drastically 
r 
vv    vbn fallen 
r 
as    vbp have 
r 
vp ad  vp vv  as     
vp vbp  advp rb  vbn    
r   vp vv   as     vbn fallen 
r   vp ad  vp     vp vbp  advp   

minimal rules
extracted from matrix      best 
r  ad    rb drastically 
r  vv    vbn fallen 
r  as    vbp have 
r  vp ad  vp vv  as     
vp vbp  advp rb  vbn    

   
 b  rules extracted using   best alignment and alignment posterior
figure    matrix based representation of sub tree alignment and sample rules extracted 
matrix   shows the case of   best sub tree alignment  and matrix   shows the
case of sub tree alignment posterior 
access all possible sub tree alignments  with different probabilities   rather than a limited
number of them 
we then extract rules using the sub tree alignment matrix  the method is simple  we
collect the rules associated with each entry from the matrix  the core algorithm of this
method is in essential the same as that used in   best k best extraction  the only difference
from   best k best extraction is that the matrix based method considers all possible node
pairs for extraction  rather than visiting some of them only  see figure   b  for the pseudocode of the sub tree alignment matrix based rule extraction algorithm  where m represents
a sub tree alignment matrix for the pair of trees  s  t    compared to extracting rules from
k best alignments  this method can efficiently obtain additional rules whose extraction is
blocked in k best extraction  for example  on the right side of figure   b  two new rules
r   and r   are extracted  which cannot be obtained from the   best alignment result  to
prevent the extraction of a great number of noisy rules with low alignment probabilities  we
   

fixiao   zhu

prune away those rules whose alignment probabilities are below a pre specified threshold 
more formally  given a pair of nodes  u  v   rule extraction can be executed at  u  v  only
when it satisfies 
 u  v 
  pmin
    
 root s   root t   
this expression measures the relative probability of alignment  u  v  with respect to the
sum of probabilities over all possible derivations  pmin is an empirical threshold to control
how often rules to be pruned  a larger pmin means more rules are thrown away   in this
work  it is set to     by default  therefore  all those entries with a zero score in figure
  a   denoted as a dot  are excluded in rule extraction 
however  discarding rules with relatively low probabilities in turn results in an incompleteness problem  that is  the extracted rules might be unable to transform a given source
parse tree  even in the training set  nonetheless  this problem is not very severe in our
case  in our experiments we observed that most parse tree pairs  over      in the training
corpus could be recovered by the extracted rules when pmin chose its default value  and
the contribution to translation accuracy from those low confidence rules was very limited
 generally less than     bleu points  
another note on sub tree alignment matrix based extraction  the advantage of this
method is that it follows the general and well developed framework of syntax based mt 
i e   word syntactic alignment   rule extraction parameter estimation   mt decoding  all
we need is to replace the rule extraction component with our sub tree alignment matrixbased system  while preserve other components of the pipeline  this means that we can still
use some heuristics to obtain additional useful rules from the result of sub tree alignment
matrix based extraction  such as rule composing  galley et al         and spmt extraction
 marcu  wang  echihabi    knight         also  the posterior probability encoded in the
matrix can be used for better estimation of various mt oriented features   
note that the basis of our approach is the stsg model  and all rules in the sub tree
alignment model resemble the general forms of the translation rules used in tree to tree mt
systems  so  as an alternative but simple way of rule induction  we can directly infer translation rules from our sub tree alignment model and take the corresponding rule probabilities
as features of the translation model for mt decoding  however  in tree to tree mt this
method suffers from several problems  first  our sub tree alignment model requires computation of all possible aligned tree fragments  which results in very high time complexity
for both training and decoding procedures  as a result  aggressive pruning has to be used
for a reasonable size of search space  e g   we consider only relatively small tree fragments
in our implementation for acceptable running speed  as a side effect  many relatively large
rules  e g   composed rules and spmt rules  are absent in our sub tree alignment model 
while they are available if we use the traditional alignment   extraction heuristics pipeline 
from an engineering standpoint  it is not efficient to directly infer translation rules from
the sub tree alignment model  compared to inferring rules using a pruned and fixed subtree alignment matrix plus some heuristics  second  the rule probability and optimization
objective for sub tree alignment is very different from those used in mt systems  for example  we use a generative model and a maximum likelihood bayesian approach in sub tree
    see section     for more detailed discussion of the parameter estimation issue 

   

fiunsupervised sub tree alignment for tree to tree translation

alignment  but use a discriminative model and minimum error rate training in mt  many
features employed in the mt decoder are not considered in our sub tree alignment model 
all the above issues might lead to unsatisfactory mt performance  as shown in our experiments  see section         directly inferring translation rules from the sub tree alignment
model does not achieve promising results 
    learning features for machine translation
in previous work on syntax based mt  it is proved that syntax based systems can make great
benefits from mt oriented features  even some of them are not necessarily well explained
in the syntactic parsing viewpoint  e g   phrase based translation probabilities   however 
most of these features are not available in the word sub tree alignment model  instead we
need to learn these features using an additional step of parameter estimation for mt  to
do this  we follow the commonly used framework which estimates values of various mtoriented features on the extracted rule set using mle  this procedure is simple  once all
translation rules are extracted  we obtain the maximum likelihood  or relative frequency 
estimate of parameters according to the definition of each feature function 
however  in traditional tree to tree systems each rule extracted from a tree pair has
a count of unit one  which is then used to calculate the values of various features  such
a approach might enlarge the influence of noisy rules extracted from sub tree alignment
matrices  e g   a rule with a high alignment probability has an equal weight as a rule with
a low alignment probability  and thus has a unreasonably large impact on mt systems  a
more desired solution is that a rule extracted from a derivation having a low probability is
penalized accordingly in feature learning  motivated by this idea  we use fractional counts
to estimate the appearance for each rule  mi   huang         given a node pair  u  v  in
 s  t    the alignment probability of a rule r rooted at  u  v  is defined to be  denoted as
 r  u  v   
x
 r  u  v   
p t  d   s 
    
dd s t  
 rd

where  r  u  v  is regarded as the probability sum over all derivations involving r at  u  v  
also  we can rewrite equation      in an inside outside fashion 
y
 r  u  v     u  v  
 p  q   p r   s 
    
 p q yield r 

then we define the probability that r is involved in the derivations of  s  t   as 
x
 r   
 r  u  v 

    

u v

equation      is the sum of probabilities of r over all node pairs  this means that the
rule probability can be considered for multiple times if some particular derivations contain
r more than once  by using  r   the fractional count of r is defined to be 
c r   

 r 
 root s   root t   

   

    

fixiao   zhu

equation      reflects the probability how likely r is involved in a derivation given a
pair of trees  for a set of bilingual parse trees  c r  can be accumulated over each tree pair 
obviously  c r  can be used to estimate the parameters of the mt model  that is  once
translation rules are weighted  the parameter estimation procedure can proceed as usual 
but with weight counts  in this work c r  is employed to learn five features used in the
mt decoder  including the bi directional phrase based conditional translation probabilities
 marcu et al         and three syntax based conditional probabilities  mi   huang        
let    be a function that returns the sequence of frontier nodes for an input tree fragment 
these probabilities can be computed by the following equations 
p
  
r     sr      sr   tr      tr   c r  
p
pphrase  tr   sr    
    
 
r    sr     sr   c r  
p
  
r     sr      sr   tr      tr   c r  
p
pphrase  sr   tr    
    
 
r    t      tr   c r  
r

c r 

p r   root r    

p

p r   sr    

p

r   root r    root r  c r

c r 
r   sr   sr

p r   tr    

c r   

c r 
p

r   tr   tr

c r   

  

    
    
    

   experiments
for evaluation  we first experimented with our approach on a chinese english sub tree
alignment task  then tested its effectiveness in a state of the art tree to tree mt system 
    baselines
three unsupervised sub tree alignment methods were chosen as baselines in our experiments 
 wordalign     wordalign   is based on a ghkm like method  galley et al        
that uses word alignments to infer syntactic correspondences  in our implementation 
both the giza   toolkit and the grow diag final and method were used to obtain
the symmetric word alignment from sentence pairs  the sub tree alignments were
then heuristically induced by selecting those node correspondences that are consistent
with the word alignment result  i e   sub tree alignments that do not violate any word
alignments   we chose this method because it has been widely adopted in modern
tree to tree systems 
 wordalign     the second baseline is essentially the same as wordalign    the only
difference from wordalign   is that we improved the word alignment system using
link deletion techniques  fossum et al          the basic idea is to delete harmful
alignment links from an initial word alignment result  e g   deleting the link between
 and the in figure   a    in our experiments we only considered the most likely
deletion between the top    most common chinese words  including         
   

fiunsupervised sub tree alignment for tree to tree translation

             and the top    most common english words  including  the 
of  and  to  in  a  is  that  for  on   
 heuristicalgin  heuristicalgin is a re implementation of the approach proposed in
tinsley et al s        work  in this method the alignment confidence of every node
pair is first computed with lexical translation probabilities  and then used to obtain
node correspondences via a heuristic algorithm  because this method does not require
a training process and has been successfully adopted in several translation tasks  such
as french english translation  it was chosen as another baseline for comparison 
    experimental setup
the settings of our experiments are described as follows 
      data preparation
our bilingual corpus consists of      million sentence pairs    as mentioned above  we
used giza   and the grow diag final and heuristics to generate   best k best word
alignments  which were then used as our baseline word alignment results  the parse trees in
both chinese and english were generated using the berkeley parser    a publicly available
corpus was used to evaluate the sub tree alignment result    it consists of     node aligned
sentence pairs  with gold standard parse trees on both language sides  in ldc    e  
which is also included in our bilingual data  this corpus was divided into two parts  a
held out set used for finding an appropriate setting of hyperparameters     sentences in
articles           and a test set used for evaluating the sub tree alignment systems     
sentences in articles           for mt experiments  a   gram language model was trained
on the xinhua portion of the gigaword corpus in addition to the english part of the ldc
bilingual training data    we used the nist      mt evaluation corpus as our development
set      sentences  and the newswire portion of the nist           mt evaluation corpora
as our test set        sentences  
      sub tree alignment
all parameters of the sub tree alignment model were initialized with the add one smoothing
on the rule set extracted using word alignments  i e   the wordalign   baseline   then  the
model was trained on the parse trees of the bilingual corpus using the em algorithm or
the variational bayes  vb  approach  in our implementation of the vb based training  all
hyperparameters are assumed to share the same value    this leads to a setting of        
    ldc category  ldc    e    ldc    t    ldc    e    ldc    t    ldc    e    ldc    e   
ldc    e    ldc    e    ldc    e   and ldc    t    see http   www ldc upenn edu  for more
details 
    note that for the ldc    e   corpus we reused the gold standard parse trees provided in the chinese
and english treebanks 
    available from http   www nlplab com resources nodealigned bitreebank html
    ldc category of the english gigaword corpus  ldc    t  
    although we could adopt different hyperparameters for finer control over the priors of model parameters  we found that setting those hyperparameters to the same value could also lead to satisfactory
performance 

   

fixiao   zhu

which is an optimal value on the held out set  by default  we trained our model for   em or
variational em iterations  to speed up the training process and further avoid degenerate
analysis caused by too large rules  we restricted ourselves to rules with reasonable sizes rules with at most five frontier non terminals and depth of three  for rules having more
than five frontier non terminals  we only considered the tree fragments of depth one but did
not restrict the number of frontier non terminals involved  that is  for flat tree structures 
we only used the associated height one tree fragments  besides  we discarded the sub tree
alignment between every node pair whose terminals are aligned outside the corresponding
spans for more than two times in wordalign   
      machine translation
we used the niutrans open source toolkit  xiao  zhu  zhang    li        to build our
tree to tree mt system  for rule extraction  we used an extension of the ghkm method
to extract minimal tree to tree transformation rules  liu et al       a  and obtained larger
rules by composing two or three minimal rules  galley et al          we used a cky style
decoder with cube pruning  huang   chiang        and beam search to decode chinese
sentences  by default the beam size was set to     in addition to the features described in
equations            we also used several other features in our mt system  including the
  gram language model  the rule number bonus  the target length bonus and two binary
features   lexicalized rule and low frequency rule  marcu et al          these features are
combined in a log linear fashion and optimized using minimum error rate training  mert 
och        
    results
in the following part of this section  we present our experimental results  including evaluations of sub tree alignments  extracted rules  and mt systems  also  we show results of
several improved methods for the effective use of our approach in tree to tree mt 
      evaluation of alignments
first we evaluated the alignment quality of various sub tree alignment approaches in terms
of precision  p   recall  r  and f   score    see table   for results of the three baseline
systems and our sub tree alignment system  by those measures  our vb based system
significantly improves both the overall recall and f   score  slightly degrading in precision
compared to wordalign      also  the vb based training outperforms the em based counterpart due to the priors introduced into the learning process  the interesting observation
here is that  though the em training of our model suffers from the degenerate analysis of the
data  it does not show extremely bad results in our experiment  this phenomenon is due
to our restriction on the size of tree fragment in training  as described in section        we
restricted the translation rules to those reasonable size tree fragments in several ways  e g  
    let predicted be the number of alignments in system output  correct be the number of correct alignments
in system output  gold be the number of alignments in gold standard  the measure of precision  recall
 
 precisionrecall
correct
and f  score can be defined as  precision   predicted
  recall   correct
and f       
 
gold
   precision recall
here  is a parameter that controls the preference for recall  i e         or precision  i e            
in most nlp tasks  is set to    indicating equal weights of recall and precision 

   

fiunsupervised sub tree alignment for tree to tree translation

entry
overall
np  np
nn  nn
vp  vp
pu   
ip  s
pu   
np  nn
np  pp
nn  nns
nr  nnp
nn  np
pp  pp
nn  jj
p  in
qp  np

wordalign  
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

wordalign  
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

heuristicalgin
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

ours  em 
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

ours  vb 
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

table    evaluation results of sub tree alignment for our system and the baselines  all
measures are reported in percentage 
we set a parameter of maximum depth   while such constraints reduce the number of rules
involved in training  it prevents the use of rare and large rules  our result here indicates
the fact that the tree fragment size constraint is actually not only important for efficiency
but also crucial for learning  as discussed in previous work  without these constraints or
imposing a proper prior  the solution of em is degenerate  marcu   wong        denero 
gillick  zhang    klein        
in addition  table   shows the result for the    most common types of sub tree alignment  as expected  the vb based system achieves the best f   score in most cases  more
interestingly  it is observed that our approach obtains significantly better performance in
handling the pp  prepositional phrase  alignment that seems to be a difficult problem for
baselines due to the unclear boundary indicators in aligning the pp structures  we attribute
this to the better use of syntactic information on both language sides in our model  which
are generally ignored in traditional models based on surface heuristics and word alignments 
      evaluation of extracted rules
we then applied the sub tree alignment result to our tree to tree system to study the impact
of sub tree alignment on mt  as discussed in section    rule extraction is a downstream
component of sub tree alignment in the current tree to tree mt pipeline  we therefore
chose to evaluate the quality of the rules obtained from various sub tree alignment results 
to determine the goodness of extracted grammars  we computed the rule precision  recall 
and f   scores for our approach and the baseline approaches on the same test set used in
the    best  alignment quality evaluation  to make a gold standard grammar  we chose
the method used in fossum et al s        work where the grammar was automatically
generated from the manually annotated alignment result  that is  the rules extracted using
the annotated sub tree alignments were regarded as the gold standard in computing various
evaluation scores  table   shows the evaluation result for the grammars extracted with
   

fimatrix

 best

xiao   zhu

entry
wordalign  
wordalign  
heuristicalgin
ours  em 
ours  vb 
ours  vb   pmin
ours  vb   pmin
ours  vb   pmin
ours  vb   pmin

       
       
       
       

rule p
    
    
    
    
    
    
    
    
    

rule r
    
    
    
    
    
    
    
    
    

rule f  
    
    
    
    
    
    
    
    
    

table    evaluation results of rules obtained from various sub tree alignment approaches 
all measures are reported in percentage 
different sub tree alignment approaches  we see that the improvements persist when our
sub tree alignments are employed in translation rule extraction  our vb based approach
produces grammars with a higher rule f   score than all three of the baselines 
in addition to the   best extraction  we studied how rule extraction behaves under the
sub tree alignment matrix based extraction method  table   also shows the result of the
sub tree matrix based extraction method with different choices of the pruning parameter
pmin   we see that smaller values of pmin result in grammars with higher rule recall  also 
better rule f   scores can be achieved by adjusting pmin and seeking a good balance between
rule precision and rule recall   e g   pmin       or      
the above scores are informative as a measure of grammar quality  but we also investigated some of the differences in the rule sets obtained from our model compared to the
baseline approaches  following levenberg  dyer  and blunsoms        method  figure  
shows the most probable rules  frequency     obtained from our bilingual corpus using
the vb based alignment approach that do not appear in the model from the wordalign  
alignment and vice versa  we asked two annotators of sub tree alignment to estimate the
rule quality based on the syntactic correspondence and adequacy of frontier node sequence
between the two languages sides  a rule was labeled as good only if both judges considered it to be of good quality  from the figure  we see that eight of the top    rules extracted
using our approach but absent in the wordalign   grammar are good rules  in contrast 
only four of the top    rules in the baseline model are of good quality in a sense of human
preference  furthermore  we examined the top     most probable rules that appear in the
two grammars individually  again  the top     rules extracted using our proposed model
are of better quality  it results in    good rules  by contrast  only     of the top ranking
rules induced using the wordalign   alignment are good translation rules 
      evaluation of translations
we also evaluated the translations generated by the mt system for different sub tree alignment approaches  since the vb based training shows the best performance in the previous
experiments  we chose it as a default setting of our approach in the following experiments 
table   shows the evaluation result where the translation quality is estimated using caseinsensitive ibm version bleu   papineni  roukos  ward    zhu        and ter  snover 
   

fiunsupervised sub tree alignment for tree to tree translation

the
  
  
  
 
  
  
  
  
  
  

top    highest probability rules  for mt  in our approach but absent in wordalign  
np dnp  nn     vp advp  vp vbd improved   
np pu    np cd   nn    pu     np dt the  cd two  nns sessions  
np np qp  np nn     np     np x  np cd  np nns represents    
np nn   np     vp vb desire  nnp   
np pu   np nr  nn    pu     np    np nnp  nn independence      
np vp  dec   np nn   nn    
 np adjp adjp  jj ideological   nn struggle  
np np qp  np    np adjp jj    np nn     
 np np dt the  jj important  nn thinking   in of  sbar whnp  s    
np ip  deg   np nn   nn      np adjp adjp  jj practical  
nn significance  
np np pu   np qp  nn    pu    np adjp  nn    
 np np dt the  jj  nn idea   pp in of  np dt the  cd  nns represents    
np pu   nn   pu     vp vbg joining  np dt the  nn    

the top    highest probability rules  for mt  in wordalign   but absent in our approach
 
lcp qp  lc     adjp jj   
  
np dnp  nn     vp advp  vbp changes  
  
np nn   nn     np cd three  nns links  x   
 
np dnp ip  dec    np nn      np adjp  nn significance  
 
vp advp  vp vv  np nn   nn     
 vp advp  vp vp vv    np dt the  jj mass  nn     
 
ip np  vp vv   np nn   nn       np np nns    pp in for  np    
 
np vp  deg   nn     adjp jj   
 
np np pu   nt  pu    np nn    nr   
 np np prp  his   qp cd    nn speech  
  
vp vp advp  vp vv   cc   vv     np   
 vp advp  vp vp vb strengthen  cc and  vb improve   np    
    np np pu   nn   pu    np   
 np np    np nn taiwan  nn independence       nns   

figure    the top    highest probability rules built from the proposed sub tree alignment
approach that are not in the wordalign   baseline grammar  and the top    rules
in the wordalign   baseline grammar that are not obtained using the proposed
sub tree alignment approach      a good translation rule 
dorr  schwartz  makhoul  micciula    weischedel         and the significance test is performed using the bootstrap resampling method  koehn         moreover  the efficiency of
rule extraction is reported in terms of rule set size extraction time  for comparison  we
also report the result of rule extraction using word alignment matrices  liu et al       b 
on wordalign   and wordalign   
table   indicates that our approach outperforms the baselines by the bleu and ter
measures for both   best and    best extraction  in addition  the matrix based method is
much more efficient than the k best method  for example  compared with    best extraction  extracting rules from sub tree alignment matrices is   times more efficient  however 
when all rules are counted as unit one in parameter estimation of the translation model 
using alignment matrices does not show significant bleu improvements or ter reductions in comparison with the    best counterpart  see rows marked with unitcount   this
is because many of those additionally extracted rules are not utilized in real translation 
for example  we observed that only      of the rules used in generating the final    best 
   

fixiao   zhu

entry
wordalign      best 
wordalign      best 
heuristicalgin    best 
wordalign       best 
wordalign       best 
heuristicalgin     best 
wordalign    matrix 
wordalign    matrix 
ours    best   unitcount 
ours     best   unitcount 
ours  matrix   unitcount 
ours    best   posterior 
ours     best   posterior 
ours  matrix   posterior 

dev

test

bleu     ter   

bleu     ter   

    
    
    
    
    
    
     
     
     
     
     
     
     
      

    
    
    
    
     
    
     
     
     
     
      
     
      
      

    
    
    
    
    
    
    
    
    
    
     
     
      
      

    
    
    
    
    
    
     
     
    
     
     
     
      
      

rule set
size
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m

efficiency
 rule sec 

    
    
    
   
   
   
    
    
    
   
    
    
   
    

table    evaluation of translations for different alignment approaches  for bleu  higher
is better  for ter  lower is better  unitcount means that we take each rule
occurrence as unit one in parameter estimation  and posterior means that we use
rule posterior probabilities as fractional counts in parameter estimation    or   
  significantly better than all three of the   best baselines  p        or       
translations were indeed extracted from the alignments that were not seen in the    best
alignments  it thus indicates the fact that naively increasing the number of rules might not
be effective for improving the translation quality 
the last three rows in table   show the result of using alignment posterior probabilities
in parameter estimation  i e   the method described in section       we see that alignment
posterior probabilities are very helpful in improving translation quality because the system
can weight more on those rules with more confidence  entries with unitcount vs  entries
with posterior    by using sub tree alignment matrices in rule extraction and alignment
posterior probabilities in parameter estimation  our approach finally achieves a      bleu
improvement and a      ter reduction over the    best case of the baselines  it even
outperforms the word alignment matrix based counterpart by      bleu points and     
ter points  both are significant at p         
further  the effectiveness of the proposed approach is demonstrated in terms of bleu
and ter scores under the same rule set size  figure   compares our approach and the
baseline approaches in different numbers of unique rules extracted    clearly  in the same
number of unique rules  the proposed sub tree alignment approach leads to better translations than those of the baselines 
      the impact of alignment and grammar quality on mt performance
the above experiments demonstrate the effectiveness of the proposed approach in terms
of different measures individually  the next natural question is how sub tree alignment
    to do this  we adjusted pmin to obtain grammars with different sizes for our approach  for other
approaches  we used different k best lists for rule extraction 

   

fiunsupervised sub tree alignment for tree to tree translation

  

    ter   

bleu    

  

  

  

heuristicalgin
wordalign  
wordalign  
ours

  

heuristicalgin
wordalign  
wordalign  
ours
  

  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

rule set size  million 

rule set size  million 

figure    bleu and   ter against rule set size
and rule extraction affect the translation quality  the study of this issue is very important
when we optimize the upstream systems of mt decoding and select appropriate evaluation
metrics for a good prediction of mt performance 
we therefore carried out another set of experiments which compares the translation
quality in different sub tree alignment and rule extraction settings  to generate diverse
sub tree alignment and rule extraction results  we varied the values of  and pmin for
sub tree alignment and rule extraction respectively  in this way  we obtained ensembles of
sub tree alignments and grammars with different precision and recall scores    we chose
f  score as the evaluation metric for both the sub tree alignment system and the rule
extraction system  instead of fixing  to be    we varied the  value from     to    since
the parameter  can control the bias towards precision or recall  choosing different values for
 is very helpful in seeking a good tradeoff between precision and recall  then we can find
an appropriate evaluation measure of sub tree alignment and rule extraction for predicting
mt performance well 
figures   and    plot the f  scores as measures of mt performance for sub tree alignment and rule extraction  from figure     we see that the rule f   score correlates best
with the translation quality measures  which indicates that the mt system prefers rule
recall biased metrics  this agrees with our observation in figure   that the mt system
can make benefits from more rules  on the other hand  the curves in figure   show a
better correlation between sub tree alignment f   f   score and translation quality measures  implying a preference for relatively higher sub tree alignment recall  this result is
reasonable because in our framework more node alignment links can result in more aligned
tree fragments  or rules  extracted  a high recall sub tree alignment generally results in a
big grammar with high rule recall  and thus better bleu and ter results  we also com    for example  a larger value of  generally results in higher alignment precision  while a small value
prefers higher alignment recall  for rule extraction  a larger value of pmin generally leads to a grammar
with higher rule precision  while choosing a smaller pmin can generate a grammar with higher rule recall 

   

fixiao   zhu

  

sub tree alignment f    

sub tree alignment f    

  

  

  

f     
f     
f     
f     
f     

  

  
    

  

    

  

    

  

  

f     
f     
f     
f     
f     

  

  

  

  

bleu    

  

  

    ter   

  

  

  

  

rule f    

rule f    

figure    bleu and   ter against sub tree alignment f  measure

  
  

f     
f     
f     
f     
f     

  
  
  

    

  

  
  

f     
f     
f     
f     
f     

  
  

    

    

bleu    

  

    

  

    ter   

figure     bleu and   ter against rule f  measure

puted the pearsons correlation coefficients between sub tree alignment rule f   score and
bleu ter  for sub tree alignment f    its correlation coefficients with bleu and ter
are       and        respectively  for rule f    its correlation coefficients with bleu and
ter are       and        respectively  both of them show good correlations with the translation quality measures  another interesting observation here is that the mt performance
is more sensitive to the change of rule f  score than to the change of sub tree alignment
f  score  this may lie in that rule extraction is a direct upstream step of decoding and
impacts more on the output of mt systems  in contrast  sub tree alignment is a front end
step of the mt pipeline and has an indirect effect on the actual translation process 
   

fiunsupervised sub tree alignment for tree to tree translation

system
hierarchical phrase based
tree   best word alignment  wordalign   
to
word alignment matrix
tree sub tree alignment matrix

dev

test

bleu     ter   

bleu     ter   

    
    
    
     

    
    
    
      

    
     
     
      

    
    
     
      

table    mt evaluation results of rules obtained from various alignment approaches  for
bleu  higher is better  for ter  lower is better    or      significantly better
than the hierarchical phrase based baseline  p        or       
      further improvements
previous work has pointed out that the straightforward implementation of tree to tree mt
suffers from the problems of too few rules and too few derivations in either rule extraction or
decoding process  chiang         to further advance the tree to tree system and compare
it with the state of the art  we employed tree binarization  wang et al       b  and fuzzy
decoding  chiang        in our system  as our alignment approach can be equipped with
the general framework of tree to tree translation  it is trivial to conduct another set of
experiments to investigate the effectiveness of our approach in a stronger system    table
  shows the bleu and ter scores of our system enhanced with the above methods    for
comparison  we also report the result of a state of the art mt system that implements the
hierarchical phrase based model  chiang        and our tree to tree system that extracts
rules using the word alignment matrices  liu et al       b   table   indicates the superiority
of our approach when tree binarization and fuzzy decoding are involved  it significantly
outperforms the hierarchical phrase based system       bleu points and      ter points 
and the tree to tree system based on the word alignment matrices       bleu points and
     ter points  
as discussed in section    the transfer rules in our sub tree alignment model resembles
the general form of stsgs which can be directly used in mt  instead of resorting to an
explicit step of rule extraction  we can use the rules in the sub tree alignment model for
mt decoding  i e   sub tree alignment is cast as a grammar induction step  we therefore
built another system which directly acquires translation rules from the sub tree alignment
step  to do this  we only need to output all the rules from the derivation forest generated
by our alignment model  all rule probabilities can be obtained using the inside and output
probabilities  and pruning is performed by throwing away rules whose probability is below
pmin   in addition to rule probability  we reused the n gram language model  the rule number
bonus  the target length bonus  the lexicalized rule and low frequency rule indicators in our
base tree to tree system as additional features for a fair comparison  to obtain a good and
reasonable result  we employed both fuzzy decoding and tree binarizaiton in the experiment 
    we did not choose this setting in the previous experiments because our gold standard alignment annotation was on the penn treebank style trees only  it was difficult to evaluate the alignment and grammar
quality on binarized trees due to the lack of benchmark data  in our experiments we first conducted the
experiments on each individual tasks  see sections               and studied their correlations in a simple
but reasonable setting for a consistent result of sub tree alignment and mt  see section         then we
investigated the effectiveness of our approach on an advanced tree to tree system  see section        
    in our implementation all parse trees were binarized in a head out fashion 

   

fixiao   zhu

entry
baseline  explicit rule extraction 
rules from the sub tree alignment model
rules from the sub tree alignment model   mert
baseline   sub tree alignment features

dev

test

bleu     ter   

bleu     ter   

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

table    mt evaluation results of obtaining rules from the sub tree alignment model and
obtaining rules with the traditional rule extraction pipeline
table   compares the results of sub tree alignment matrix based rule extraction and inducing rules from the alignment model  row   vs  row     unfortunately  straightforwardly
inferring rules and their probabilities from the sub tree alignment model underperforms the
baseline  this might be attributed to several reasons  first  due to the large derivation
space  we cannot enumerate all relatively large tree fragments in the sub tree alignment
step  instead we only access the tree fragments with limited depths  by contrast  our baseline system extracts basic rules using sub tree alignment matrices and then obtains more
large rules with some heuristics  e g   rule composing   the additional rules obtained in
our baseline framework of rule extraction are in general very useful to modern syntax based
systems  galley et al         marcu et al         deneefe et al          second  the rule
probability in our sub tree alignment model is defined as a product of probability factors
for a good generation story  however  mt systems usually use features which are not required to form a generative model  such as the features shown in equations            in
consequence  many well developed features can be used by the baseline system but they
are not available in our sub tree alignment model  third  our sub tree alignment model
is trained by maximizing the likelihood or other criteria  which is not consistent with that
adopted by the mt system  i e   minimizing an evaluation related error rate function   to
further study these issues  we improved the system in two ways  first  we treated all four
of the probability factors in the sub tree alignment model  see equation       as different
features in the mt decoder  and tuned their weights using mert  row   in table   shows
that this method achieves better results than the system employing unweighted probability
factors  however  its performance is still worse than that of the baseline  which indicates
that the mt oriented features and rule extraction heuristics are crucial to the success of
our tree to tree system  finally we added the probabilistic factors of our sub tree alignment
model to the baseline system as additional features  as shown in the last row of table   
the enhanced system yields modest bleu improvements over the baseline  but no ter
improvement is achieved  all the above results give us two interesting messages      rule
extraction heuristics  mt oriented features and objectives of learning are key factors contributing to a good tree to tree system     and it is better to use our sub tree alignment
model as an upstream module of rule extraction and decoding  rather than using it as a
simple step for grammar induction 
the last issue we investigate here is whether our sub tree alignment model can make
benefits from labeled data  although we focus on unsupervised learning in this work 
the proposed model does not require a strictly unsupervised condition  instead it can be
enhanced with the use of labeled data  the idea is simple  we combine the probability
factors of our sub tree alignment model in a log linear weighted fashion  it means that
   

fiunsupervised sub tree alignment for tree to tree translation

entry
unweighted
weighted  weights are learned on labeled data 

dev

test

bleu     ter   

bleu     ter   

    
    

    
    

    
    

    
    

table    comparison of the unweighted and weighted sub tree alignment models
all probability factors of our sub tree alignment model are taken as real valued feature
functions  and their feature weights can be learned on labeled data by supervised methods 
in this way  our unweighted generative model  i e   each factor has a weight of one  is
transformed into a weighted model  i e   each factor has an individual weight   note that
this weighted model has almost the same form as those used in smt systems  the only
difference from the smt model is that no language model is needed because the targetlanguage side is fixed in the sub tree alignment step  to avoid bias towards too few or
too many rules  we also added the rule number as an additional feature to our new model 
for training and test  we divided our node aligned gold standard data into two parts   the
first     sentences were selected for weight training  and the remaining     sentences were
selected for testing the system  to learn feature weights in a supervised manner  we chose
mert which is one of the most powerful tools for training log linear models  the error
function used in mert is defined by one minus sub tree alignment f   score 
on the     sentence test data  with the tree annotation of the penn treebanks   the
weighted model achieves an alignment f   score of       and a rule f   score of       
respectively  this result is better than that of the unweighed  and unsupervised  model
which obtains an alignment f   score of       and a rule f   score of       on the same
data set  finally we tested the mt performance in our best setting  i e   sub tree alignment
matrix based rule extraction   tree binarization   fuzzy decoding     table   shows that
the weighted sub tree alignment model leads to a better bleu score on the tuning set but
does not show promising improvements on the test data  as the size of our labeled corpus
is small  we expect better results if more labeled data is available  also it is worth studying
more sophisticated supervised methods to learn better weights  such as the kernel based
methods  sun et al       b   as supervised semi supervised learning is not the focus of
this work  we leave these interesting issues to future investigations 

   related work
syntax based approaches have been widely adopted in machine translation over the last
ten years  many successful syntactic mt systems have been developed and shown good
results on several translation tasks  eisner        galley et al               liu et al  
      huang et al         zhang et al         liu et al       a  chiang         despite
differences in modeling and implementation details  all these models require an alignment
step to acquire the syntactic correspondence between the source and target languages  as
is standard in smt  most syntax based mt systems use word alignments to infer syntactic
alignments from string tree tree tree pairs  however  word alignments are generally not of
good quality from the viewpoint of syntactic alignment  it makes more sense to directly
    as we did not have sub tree aligned data for binarized trees  we reused the weights learned on the penn
treebank style trees for all four of the probability factors in our sub tree alignment model 

   

fixiao   zhu

induce sub tree level alignments from pairs of sentences with the syntactic information on
either language side or both  this is especially true for tree to tree mt where what we
actually need is the alignment between sub trees in two languages  rather than the surface
alignment of words  there are several lines of work that address the syntactic alignment
problem and make better use of various alignment results for tree to tree translation 
    word and sub tree alignment for machine translation
the earliest efforts in syntactic alignment focus on enhancing word alignment models with
syntactic information  to date  several research groups  fraser   marcu        denero
  klein        may   knight        fossum et al         haghighi  blitzer  denero   
klein        burkett  blitzer    klein        riesa  irvine    marcu        have proposed
syntax augmented models to advance their word alignment systems  although these models
achieved promising improvements  they still address the alignment problem in word level 
as discussed in section    such methods might not be desirable choices for learning the
correspondence between tree nodes in two languages  as an alternative and more straightforward solution  researchers tried to infer sub tree level alignments for pairs of syntactic
trees  for example  imamura         groves  hearne  and way         and tinsley et al 
       defined several scoring functions to measure the similarity between the source and
target sub trees  and aligned the tree nodes with greedy algorithms  their approaches 
though simple to implement  are not derived in a principled way  for example  all these
models do not have an explicit optimization procedure  as in a general framework of statistical learning  instead  the model parameters are obtained using additional alignment
models or lexicons  in another line of research  sun et al       a      b  attempted to address the sub tree alignment problem with supervised semi supervised models  they used
tree kernels and various syntactic features to advance their sub tree alignment system and
showed promising results on chinese english translation tasks  however  this approach still
relies on heuristic algorithms for inferring node correspondences between two parse trees 
beyond this  to train the tree kernels  their approach requires additional labeled data which
is generally very expensive to build  unlike these studies  we derive our sub tree model in a
principled way and develop an unsupervised sub tree alignment framework for tree to tree
mt 
    unsupervised syntactic alignment
there are also previous studies that do not resort to labeled data for sub tree alignment 
the earliest of these was eisners        work  he designed an unsupervised approach
to modeling the sub tree alignment problem in the stsg formalism  however  since no
detailed derivation and model decomposition is provided  this model is computationally
expensive  even difficult to be applied to current tree to tree systems where complex tree
structures are involved  gildea        also applied stsgs to tree to tree tree to string
alignment  he developed a loosely tree based alignment method to address the issue of
parse tree isomorphism in bitext  but his work only targets the word alignment rather
than modern syntactic mt systems  recently nakazawa and kurohashi        proposed
a bayesian approach for sub tree alignment between dependency trees  and tested it in a
japanese english mt system  actually their model has much in common with the model
   

fiunsupervised sub tree alignment for tree to tree translation

presented in this work  for example  they both apply unsupervised learning methods and
bayesian models to sub tree alignment  on the other hand  the two studies differ in some
important aspects  first  nakazawa and kurohashi        restricted themselves to sub tree
alignment between dependency trees  which is very different from aligning tree nodes in
phrase structure trees  since phrase structure trees involve more complex structures and
syntactic categories  the alignment problem in phrase structure trees are relatively more
difficult than the dependency based counterpart  second  our model makes benefits from
the recent advances in stsgs and is directly applicable to current state of the art tree totree systems 
another related work to what is presented here is pauls  klein  chiang  and knights
       work  they factored a node to string alignment model over components that each
generates a target side of a synchronous rule from a source side  moreover  the probability of
a rule fragment was factored into a lexical and structural component in their work  actually 
their model and our proposed model are two variants on a theme  but there appear to
be obvious differences between them  first  our focus is sub tree alignment for tree to tree
translation  while pauls et al         addressed the alignment issue for tree to string stringto tree translation  in our model  we parse both language sides independently  rather than
parsing only one side and projecting syntactic categories  as a result  inference is faster
in this work since we do not need to consider all possible parse trees of the unparsed side
during alignment  second  the permutation model presented in this work is more general
in order to handle non itg trees  third  we investigate methods for the effective use of
sub tree alignment in mt  in particular  we present a rule extraction approach to obtaining
additional translation rules using sub tree alignment posteriors  rather than learning rules
only from the   best sub tree alignment 
    rule extraction using various alignment results
in machine translation  word and syntactic alignments are used to extract translation rules
or phrases  in the traditional pipeline of rule and phrase extraction  only the   best alignment result is considered  which suffers from the limited scope of the single alignment  to
efficiently obtain diverse alignment parsing results  packed data structures were adopted to
improve   best pipeline mt systems in recent years  mi   huang        liu et al       a 
zhang  zhang  li  aw    tan         for example  liu et al       b  and de gispert et al 
       used alignment posterior probabilities for phrase or hierarchical phrase extraction 
the development of sub tree alignment matrices is actually motivated by a similar idea
from word alignment matrices  the difference from the above work is that we use sub tree
on both language sides to infer alignment posterior probabilities  while these probabilities
are calculated in word phrase level in previous work  liu et al       b  de gispert et al  
       moreover  to our knowledge  the effectiveness of sub tree alignment matrix has not
been systematically studied in the case of tree to tree translation 
note that the approach presented in this work is also doing something similar to synchronous grammar induction  for example  our model results in an stsg which has the
same formalism as that used in mt  recent studies on bayesian models  blunsom  cohn 
dyer    osborne        cohn   blunsom        levenberg et al         have shown very
promising results in directly learning synchronous grammars from bilingual data for hierar   

fixiao   zhu

chical phrase based and string to tree systems  rather than extracting synchronous grammar
rules based on an explicit word syntactic alignment step  however it is rare to see related
work on tree to tree mt  in principle this article is different from previous work on synchronous grammar induction  for example  the aim of this work is to learn a sub tree
alignment model  which can be applied to many potential applications except mt  such as
sentence compression for paraphrasing and test summarization  jing        cohn   lapata 
       unlike synchronous grammar induction where the alignment is implicitly encoded
in the learning process  we treat sub tree alignment as a separate task  this eases the
development and tuning of the alignment system because we actually do not resort to mt
systems which are slow and difficult to optimize  another advantage is that our approach
can make benefits from more compact models  rather than those used in mt where a great
number of rules are involved  take our implementation as instance  the alignment model
is learned on a relatively small set of grammar rules  rules with limited depths   while the
mt system accesses a much larger grammar where many additional rules are involved by
rule composing  such a method can result in an efficient alignment system and is likely to
alleviate the degenerate analysis of the data  with no cost of degrading in mt performance 

   discussion
the underlying assumption of our proposed model is that   to   sub tree alignments can
be achieved based on the constraints imposed by neighboring parts of the tree  see section
    this makes sense in the standpoint of linguistically motivated models  yet it in turn
faces a problem that the constraints make it difficult to align some sentences trees correctly 
particularly if they are free translations  there are several reasons that can explain why
our approach works nice with tree to tree mt and why it does not suffer greatly from
the structure divergence between languages  first  our model is flexible and allows for
node deletion insertion in alignment  it means that in some levels of the tree it is not
necessary to require that every node is aligned to a valid node in the other language side 
instead nodes can be dropped as needed  the advantage of such a method is that if
we cannot confidently align a node to any node in the counterpart tree  we can align it
to a virtual node and do not enforce bad constraints to aligning other parts of the tree 
this is very useful when there are very flat tree structures or partial translations which
are not syntactically well formed  second  the main purpose of our approach is to infer
sub tree alignment probabilities which can be used in pruning sub tree alignment matrices
and extracting rules for mt systems  though   to   alignment is required for training the
sub tree alignment model  what we actually do is to access a large number of alignment
alternatives for rule extraction  even if some of them cannot appear in the same derivation
due to our alignment constraints  third  our model can work with any phrase structure
trees  instead of the penn treebank style trees which are difficult for alignment in some
cases  our sub tree alignment system works well with binarized trees and shows promising
improvements over various baselines  note that tree binarization is a very effective method
to alleviate the structure divergence problem  especially for chinese english translation 
also  it might be interesting to investigate other methods for dealing with differences in
syntactic structures between languages  such as the forest based methods  mi   huang 
      liu et al       a  
   

fiunsupervised sub tree alignment for tree to tree translation

another note on our approach  if implemented naively  the speed of the sub tree alignment system will be very slow since our model needs the calculation of the alignment probability over all pairs of tree fragments  fortunately  with the thought of computation  several
optimizations can make the system much more efficient in practice  first  as is described in
this work  pruning methods can be employed to restrict the number of tree fragments to a
reasonable level  in our experiments  the system with pruning achieved a speed of       
sentence second on a single core of the intel xeon      ghz cpu  another way for speed
improvement is parallel processing  a very good property of the em style algorithms is
that the e step can be easily implemented in parallel computation environment  what we
need is to divide the training data set into a number of smaller parts  and then run the
inside outside algorithm on these parts in parallel  i e   a map procedure   the expected
counts of model parameters can be accumulated over the results of all the parts  i e   a
reduce procedure   then the m step can be performed as usual  in our implementation
we used    threads for parallel training  the running time of one training iteration over
our   million sentence corpus was about       hours  note that further system speed up
can be expected if more powerful distributed infrastructures are available  e g   clusters  
hadoop   and it is not difficult to scale up our approach to handle millions of sentence pairs
using the current training framework 

   conclusions
we have proposed an unsupervised probabilistic sub tree alignment approach for tree totree translation  by factoring the alignment model over several components  the resulting
model can be easily learned using the em algorithm and the variational bayesian approach 
also  we have investigated different ways of applying the proposed model to tree to tree
translation  in particular  we developed a sub tree alignment matrix which encodes an
exponentially large number of alignments  from this representation of sub tree alignment 
desirable rules can be extracted more efficiently than using the k best sub tree alignment
result  our experiments showed that the proposed model achieved significant improvements
in both alignment quality and grammar quality over several baselines  on the nist chineseenglish evaluation corpora  it achieved a      bleu improvement and a      ter reduction
on top of a state of the art tree to tree system  the improved mt system even significantly
outperformed a state of the art hierarchical phrase based system when equipped with tree
binarization and fuzzy decoding 

acknowledgments
this work was supported in part by the national science foundation of china  grants
         and            the natural science foundation for the youth of china  grant
           the china postdoctoral science foundation  grant     m         the specialized research fund for the doctoral program of higher education  grant                 
and the fundamental research funds for the central universities  grant n            the
authors would like to thank the anonymous reviewers for their pertinent and insightful comments  keh yih su for his great help in improving the early version of this article  ji ma for
   

fixiao   zhu

helpful discussions  and chunliang zhang and tongran liu for language refinement  the
corresponding author of this article is jingbo zhu 

appendix a  part of speech tags and phrase structure labels
in this work the annotation of pos tagging and phrase structure parsing follows the standard defined in the penn english and chinese treebanks  marcus et al         xue et al  
       see tables      for lists of pos tags and constituent labels used in the example
trees in this article 
pos tag
ad
as
nn
nr
p
pn
pu
vv

description
adverb
aspect particle
noun  except proper nouns and temporal nouns 
proper noun
preposition
pronoun
punctuation
verb  except stative verbs  copulas  and the main
verbs of    and  

table    chinese pos tags used in the examples
pos tag
dt
in
jj
nnp
nns
prp
rb
vbd
vbn
vbp
 
 

description
determiner
preposition
adjective
proper noun  singular 
noun  plural 
personal pronoun
adverb
verb  past tense 
verb  past participle 
verb  non  rd person singular present 
comma
period

table    english pos tags used in the examples
syntactic label
ip
np
pp
qp
vp

description
single clause
noun phrase
preposition phrase
quantity phrase
verb phrase

table     chinese constituent labels used in the examples

   

fiunsupervised sub tree alignment for tree to tree translation

syntactic label
advp
np
pp
s
vp

description
adverb phrase
noun phrase
preposition phrase
sentence
verb phrase

table     english constituent labels used in the examples
distribution
pnt   
ptree   
preorder   

notation
tnt  snt
ttree  tnt
tvnt  svnt

plength   
pw   

l m
tw  sw

description
snt and tnt are source and target language non terminal symbols
ttree is a target language tree fragment
svnt and tvnt are vectors of non terminal symbols in source
and target languages
m and l are numbers of source and target terminals  or words 
sw and tw are source and target terminals  or words 

table     notations of model parameters

appendix b  em based training for the sub tree alignment model
as described in section    the proposed sub tree alignment model has five types of parameters  including the non terminal mapping probability pnt     the target language treefragment generation probability ptree     the frontier non terminal reordering probability
preorder     the word number probability plength    and the word mapping probability pw    
for convenience we use a new set of notations to denote these model parameters in the following description  see table    for a symbol list 
here we follow the same framework of the em based training as that described in figure
   see figure    for a complete version of the em algorithm for all parameters of the model 
in this algorithm  ec   represents the expected count of the input variable   x   x  is
a     function that returns   if variable x takes a value of x    otherwise    k   r  u  v 
and   k   s  t   are the rule probability  see equation       and the probability of the subtree alignment between s and t  see equation        where k indicates that all these
probabilities are calculated based on the parameters in the k th iteration  tree    vnt  
and lex   are the functions that return the tree structure  frontier non terminal vector  and
terminal sequence of the input tree fragment  respectively  see section      
the basic idea of the e step is that we check each rule r  given a pair of tree nodes u
 r u v 
and v  and update ec   by its relative probability  root s  root t
     this can be applied
to the update rules for the parameters tnt  snt   ttree  tnt   tvnt  svnt and l m  see lines       
the only exception is tw  sw   as defined in equation       pw  ti   sj   is not a direct
product factor pinstead we use the sum over all terminals of the source language treefragment  i e   m
j   pw  ti   sj     here we follow the result of ibm model   and make the
p lex s   
update magnitude proportional to pw  ti   sj    j      r pw  ti   sj      we refer the reader
to brown et al s        work for detailed derivation of the expected count in ibm model
   it is also worth noting that the above algorithm performs parameter update based on
different choices of  u  v  and r in the e step  this means that if a rule instance is involved
in a particular derivation for more than one time  e g   the same tree fragment appears at
   

fixiao   zhu

   function trainmodelwithem    s    t           sn   tn    
   
   
   
   
   
   initialize  tnt  snt   ttree  tnt   tvnt  svnt   l m   tw  sw  
   for k     to k    do
  
set ec       for all model parameters
   e step 
  
foreach tree pair  s  t   in sequence   s    t           sn   tn    do
  
foreach node pair  u  v  in  s  t   do
  
foreach rule r rooted at  u  v  do
  k   r u v  snt  u  tnt  v 
  k   root s  root t   

  

ec tnt  snt  

  

  

ec ttree  tnt      

  k   r u v  tnt  v  ttree  tree tr   
  k   root s  root t   

   

ec tvnt  svnt      

  k   r u v  svnt  vnt sr    tvnt  vnt tr   
  k   root s  root t   

   

ec l m  

  k   r u v  m  lex sr     l  lex tr    
  k   root s  root t   

   

foreach word pair  sj   ti   in position  j  i  of  lex sr    lex  tr    do

  

 k 
p
 ti  sj  
 sw  sj   tw  ti  
r p k   t  s  
w
i j 
j     

  k   r u v  p lex sw  

   

ec tw  sw      

  k   root s  root t   

    m step 
   
   
   
   
   
   
   
   
   
   
   

foreach non terminal symbol pair  snt   tnt   do
 k   

tnt  snt

 

ec tnt  snt  
p

t nt



ec t 

nt  snt

foreach target non terminal symbol tnt and tree fragment structure ttree do
ec ttree  tnt  

 k   

ttree  tnt  

p

t tree



ec t 

tree  tnt

foreach pair of non terminal symbol vectors  svnt   tvnt   do
ec tvnt  svnt  

 k   

tvnt  svnt  

p

t vnt



ec t 

vnt  svnt

foreach pair of word numbers  m  l  do
 k   

l m

 

ec l m  
p

l 

ec l   m



foreach pair of words  sw   tw   do
 k   

tw  sw

 k 

 

ec tw  sw  
p

t w

 k 

ec t 



w  sw

 k 

 k 

 k 

return  tnt  snt   ttree  tnt   tvnt  svnt   l m   tw  sw  
figure     the em based training algorithm for all model parameters

different positions   the update of the corresponding parameters would be carried out for
multiple times 
another note on the em algorithm  the expected counts of all the parameters can be
efficiently calculated using the inside and outside probabilities according to lines      and
   

fiunsupervised sub tree alignment for tree to tree translation

    but for some parameters there are more efficient ways  for example  as is discussed in
section        the expected count of tnt  snt can be obtained without checking each individual
rule  that is  we can omit the loop for r in this case  this technique can be considered for
further speed up of the sub tree alignment system 

references
attias  h          a variational bayesian framework for graphical models  in solla  s  a  
leen  t  k     k   m   eds    advances in neural information processing systems    
pp          mit press 
beal  m  j          variational algorithms for approximate bayesian inference  masters
thesis  university college london 
blunsom  p   cohn  t   dyer  c     osborne  m          a gibbs sampler for phrasal
synchronous grammar induction  in proceedings of the joint conference of the   th
annual meeting of the acl and the  th international joint conference on natural
language processing of the afnlp  acl ijcnlp   pp          suntec  singapore 
brown  p  e   pietra  s  a  d   pietra  v  j  d     mercer  r  l          the mathematics
of statistical machine translation  parameter estimation  computational linguistics 
           
burkett  d   blitzer  j     klein  d          joint parsing and alignment with weakly synchronized grammars  in human language technologies  the      annual conference of the north american chapter of the association for computational linguistics
 hlt naacl   pp          los angeles  california  usa 
chiang  d          a hierarchical phrase based model for statistical machine translation 
in proceedings of the   rd annual meeting of the association for computational linguistics  acl   pp          ann arbor  michigan  usa 
chiang  d          hierarchical phrase based translation  computational linguistics     
     
chiang  d          learning to translate with source and target syntax  in proceedings
of the   th annual meeting of the association for computational linguistics  acl  
pp            uppsala  sweden 
chiang  d     knight  k          an introduction to synchronous grammars  in tutorials of
the   st international conference on computational linguistics and the   th annual
meeting of the association for computational linguistics  coling acl  
chiswell  i     hodges  w          mathematical logic  oxford university press 
cohn  t     blunsom  p          a bayesian model of syntax directed tree to string grammar induction  in proceedings of the      conference on empirical methods in natural
language processing  emnlp   pp          singapore 
cohn  t     lapata  m          sentence compression as tree transduction  journal of
artificial intelligence research             
das  d     smith  n  a          paraphrase identification as probabilistic quasi synchronous
recognition  in proceedings of the joint conference of the   th annual meeting of the
   

fixiao   zhu

acl and the  th international joint conference on natural language processing of
the afnlp  acl ijcnlp   pp          suntec  singapore 
de gispert  a   pino  j     byrne  w          hierarchical phrase based translation grammars extracted from alignment posterior probabilities  in proceedings of the     
conference on empirical methods in natural language processing  emnlp   pp 
        cambridge  ma  usa 
dempster  a   laird  n     rubin  d          maximum likelihood from incomplete data via
the em algorithm  journal of the royal statistical society  series b  methodological  
        
deneefe  s   knight  k   wang  w     marcu  d          what can syntax based mt
learn from phrase based mt   in proceedings of the      joint conference on empirical methods in natural language processing and computational natural language
learning  emnlp conll   pp          prague  czech republic 
denero  j   gillick  d   zhang  j     klein  d          why generative phrase models underperform surface heuristics  in proceedings on the workshop on statistical machine
translation  wmt   pp        new york city  usa 
denero  j     klein  d          tailoring word alignments to syntactic machine translation  in proceedings of the   th annual meeting of the association of computational
linguistics  acl   pp        prague  czech republic 
eisner  j          learning non isomorphic tree mappings for machine translation  in the
companion volume to the proceedings of   st annual meeting of the association for
computational linguistics  acl   pp          sapporo  japan 
ferguson  t  s          a bayesian analysis of some nonparametric problems  the annals
of statistics            
fossum  v   knight  k     abney  s          using syntax to improve word alignment
precision for syntax based machine translation  in proceedings of the third workshop
on statistical machine translation  wmt   pp        columbus  ohio  usa 
fraser  a     marcu  d          getting the structure right for word alignment  leaf  in
proceedings of the      joint conference on empirical methods in natural language
processing and computational natural language learning  emnlp conll   pp    
    prague  czech republic 
galley  m   graehl  j   knight  k   marcu  d   deneefe  s   wang  w     thayer  i         
scalable inference and training of context rich syntactic translation models  in proceedings of the   st international conference on computational linguistics and the
  th annual meeting of the association for computational linguistics  colingacl   pp          sydney  australia 
galley  m   hopkins  m   knight  k     marcu  d          whats in a translation rule   in
susan dumais  d  m     roukos  s   eds    proceedings of the      human language
technology conference of the north american chapter of the association for computational linguistics  hlt naacl   pp          boston  massachusetts  usa 
   

fiunsupervised sub tree alignment for tree to tree translation

gildea  d          loosely tree based alignment for machine translation  in proceedings of
the   st annual meeting of the association for computational linguistics  acl   pp 
      sapporo  japan 
groves  d   hearne  m     way  a          robust sub sentential alignment of phrasestructure trees  in proceedings of the   th international conference on computational
linguistics  coling   pp            geneva  switzerland 
haghighi  a   blitzer  j   denero  j     klein  d          better word alignments with
supervised itg models  in proceedings of the joint conference of the   th annual
meeting of the acl and the  th international joint conference on natural language
processing of the afnlp  acl ijcnlp   pp          suntec  singapore 
huang  l     chiang  d          better k best parsing  in proceedings of the ninth international workshop on parsing technology  iwpt   pp        vancouver  british
columbia  canada 
huang  l   kevin  k     joshi  a          statistical syntax directed translation with
extended domain of locality  in proceedings of the  th conference of the association for
machine translation in the americas  amta   pp        cambridge  massachusetts 
usa 
imamura  k          hierarchical phrase alignment harmonized with parsing  in proceedings
of the  th nlp pacific rim symposium  pp         
jing  h          sentence reduction for automatic text summarization  in proceedings of
the  th applied natural language processing conference  pp         
johnson  m          why doesnt em find good hmm pos taggers   in proceedings of
the      joint conference on empirical methods in natural language processing and
computational natural language learning  emnlp conll   pp          prague 
czech republic 
knuth  d          the art of computer programming  fundamental algorithms  addisonwesley 
koehn  p          statistical significance tests for machine translation evaluation  in lin 
d     wu  d   eds    proceedings of the      conference on empirical methods in
natural language processing  emnlp   pp          barcelona  spain 
koehn  p   och  f     marcu  d          statistical phrase based translation  in proceedings of the      human language technology conference of the north american
chapter of the association for computational linguistics  hlt naacl   pp       
edmonton  canada 
levenberg  a   dyer  c     blunsom  p          a bayesian model for learning scfgs with
discontiguous rules  in proceedings of the      joint conference on empirical methods in natural language processing and computational natural language learning
 emnlp conll   pp          jeju island  korea 
liu  d     gildea  d          bayesian learning of phrasal tree to string templates  in proceedings of the      conference on empirical methods in natural language processing
 emnlp   pp            singapore 
   

fixiao   zhu

liu  y   liu  q     lin  s          tree to string alignment template for statistical machine
translation  in proceedings of the   st international conference on computational
linguistics and the   th annual meeting of the association for computational linguistics  coling acl   pp          sydney  australia 
liu  y   lu  y     liu  q       a   improving tree to tree translation with packed forests 
in proceedings of the joint conference of the   th annual meeting of the acl and
the  th international joint conference on natural language processing of the afnlp
 acl ijcnlp   pp          suntec  singapore 
liu  y   xia  t   xiao  x     liu  q       b   weighted alignment matrices for statistical
machine translation  in proceedings of the      conference on empirical methods in
natural language processing  emnlp   pp            singapore 
manning  c  d     schutze  h          foundations of statistical natural language processing  the mit press 
marcu  d   wang  w   echihabi  a     knight  k          spmt  statistical machine translation with syntactified target language phrases  in proceedings of the      conference
on empirical methods in natural language processing  emnlp   pp        sydney 
australia 
marcu  d     wong  d          a phrase based joint probability model for statistical machine translation  in proceedings of the      conference on empirical methods in
natural language processing  emnlp   pp         
marcus  m  p   santorini  b     marcinkiewicz  m  a          building a large annotated
corpus of english  the penn treebank  computational linguistics             
may  j     knight  k          syntactic re alignment models for machine translation 
in proceedings of the      joint conference on empirical methods in natural language processing and computational natural language learning  emnlp conll  
pp          prague  czech republic 
mi  h     huang  l          forest based translation rule extraction  in proceedings of the
     conference on empirical methods in natural language processing  emnlp  
pp          honolulu  hawaii  usa 
nakazawa  t     kurohashi  s          bayesian subtree alignment model based on dependency trees  in proceedings of  th international joint conference on natural language
processing  ijcnlp   pp          chiang mai  thailand 
neal  r          philosophy of bayesian inference  http   www cs toronto edu radford 
res bayes ex html 
och  f          minimum error rate training in statistical machine translation  in proceedings of the   st annual meeting of the association for computational linguistics
 acl   pp          sapporo  japan 
och  f     ney  h          the alignment template approach to statistical machine translation  computational linguistics             
papineni  k   roukos  s   ward  t     zhu  w          bleu  a method for automatic
evaluation of machine translation  in proceedings of the   th annual meeting of the
   

fiunsupervised sub tree alignment for tree to tree translation

association for computational linguistics  acl   pp          philadelphia  pennsylvania  usa 
pauls  a   klein  d   chiang  d     knight  k          unsupervised syntactic alignment
with inversion transduction grammars  in proceedings of human language technologies  the      annual conference of the north american chapter of the association
for computational linguistics  hlt naacl   pp          los angeles  california 
usa 
riesa  j   irvine  a     marcu  d          feature rich language independent syntax based
alignment for statistical machine translation  in proceedings of the      conference
on empirical methods in natural language processing  emnlp   pp          edinburgh  scotland  uk 
riley  d     gildea  d          improving the performance of giza   using variational
bayes  tech  rep   university of rochester 
smith  d  a     eisner  j          parser adaptation and projection with quasi synchronous
grammar features  in proceedings of the      conference on empirical methods in
natural language processing  emnlp   pp          singapore 
snover  m   dorr  b   schwartz  r   makhoul  j   micciula  l     weischedel  r         
a study of translation error rate with targeted human annotation  tech  rep 
lamp tr     cs tr      umiacs tr          university of maryland  college
park and bbn technologies 
sun  j   zhang  m     tan  c  l       a   discriminative induction of sub tree alignment
using limited labeled data  in proceedings of the   rd international conference on
computational linguistics  coling   pp            beijing  china 
sun  j   zhang  m     tan  c  l       b   exploring syntactic structural features for sub tree
alignment using bilingual tree kernels  in proceedings of the   th annual meeting of
the association for computational linguistics  acl   pp          uppsala  sweden 
thayer  i   ettelaie  e   knight  k   marcu  d   munteanu  d   och  f     tipu  q         
the isi usc mt system  in proceedings of international workshop on spoken language
translation       pp       
tinsley  j   zhechev  v   hearne  m     way  a          robust language pair independent
sub tree alignment  in proceedings of machine translation summit xi  pp         
copenhagen  denmark 
venugopal  a   zollmann  a   smith  n  a     stephan  v          wider pipelines  n best
alignments and parses in mt training  in proceedings of the eighth conference of the
association for machine translation in the americas  amta   pp         
vogel  s   ney  h     tillmann  c          hmm based word alignment in statistical translation  in proceedings of the   rd international conference on computational linguistics  coling   pp         
wang  m   smith  n  a     mitamura  t       a   what is the jeopardy model  a quasisynchronous grammar for qa  in proceedings of the      joint conference on empirical methods in natural language processing and computational natural language
learning  emnlp conll   pp        prague  czech republic 
   

fixiao   zhu

wang  w   knight  k     marcu  d       b   binarizing syntax trees to improve syntaxbased machine translation accuracy  in proceedings of the      joint conference
on empirical methods in natural language processing and computational natural
language learning  emnlp conll   pp          prague  czech republic 
woodsend  k     lapata  m          learning to simplify sentences with quasi synchronous
grammar and integer programming  in proceedings of the      conference on empirical methods in natural language processing  emnlp   pp          edinburgh 
scotland  uk 
xiao  t   zhu  j   zhang  h     li  q          niutrans  an open source toolkit for phrasebased and syntax based machine translation  in proceedings of the   th annual meeting of the association for computational linguistics system demonstrations  acl  
pp        jeju island  korea 
xue  n   xia  f   chiou  f  d     palmer  m          the penn chinese treebank  phrase
structure annotation of a large corpus  natural language engineering             
zhang  h   zhang  m   li  h   aw  a     tan  c  l          forest based tree sequence to
string translation model  in proceedings of the joint conference of the   th annual
meeting of the acl and the  th international joint conference on natural language
processing of the afnlp  acl ijcnlp   pp          suntec  singapore 
zhang  m   jiang  h   aw  a   li  h   tan  c  l     li  s          a tree sequence alignmentbased tree to tree translation model  in proceedings of the   th annual meeting
of the association for computational linguistics  human language techonologies
 acl hlt   pp          columbus  ohio  usa 

   

fi