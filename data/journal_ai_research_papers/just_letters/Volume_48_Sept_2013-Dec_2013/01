journal artificial intelligence research                

submitted        published      

learning optimal bayesian networks 
shortest path perspective
changhe yuan

changhe yuan qc cuny edu

department computer science
queens college city university new york
queens  ny       usa

brandon malone

brandon malone cs helsinki fi

department computer science
helsinki institute information technology
fin       university helsinki  finland

abstract
paper  learning bayesian network structure optimizes scoring function
given dataset viewed shortest path problem implicit state space search
graph  perspective highlights importance two research issues  development
search strategies solving shortest path problem  design heuristic functions guiding search  paper introduces several techniques addressing
issues  one a  search algorithm learns optimal bayesian network structure
searching promising part solution space  others mainly
two heuristic functions  first heuristic function represents simple relaxation
acyclicity constraint bayesian network  although admissible consistent  heuristic may introduce much relaxation result loose bound  second heuristic
function reduces amount relaxation avoiding directed cycles within groups
variables  empirical results show methods constitute promising approach
learning optimal bayesian network structures 

   introduction
bayesian networks graphical models represent uncertain relations
random variables domain compactly intuitively  bayesian network directed
acyclic graph nodes represent random variables  arcs lack
represent dependence conditional independence relations variables 
relations quantified set conditional probability distributions  one
variable conditioning parents  overall  bayesian network represents joint
probability distribution variables 
applying bayesian networks real world problems typically requires building graphical
representations problems  one popular approach use score based methods
find high scoring structures given dataset  cooper   herskovits        heckerman 
       score based learning shown np hard  however  chickering        
due complexity  early research area mainly focused developing approximation algorithms greedy hill climbing approaches  heckerman        bouckaert 
      chickering        friedman  nachman    peer         unfortunately solutions
found methods unknown quality  recent years  several exact learning algoc
    
ai access foundation  rights reserved 

fiyuan   malone

rithms developed based dynamic programming  koivisto   sood        ott 
imoto    miyano        silander   myllymaki        singh   moore         branch
bound  de campos   ji         integer linear programming  cussens        jaakkola 
sontag  globerson    meila        hemmecke  lindner    studeny         methods
guaranteed find optimal solutions able finish successfully  however 
efficiency scalability leave much room improvement 
paper  view problem learning bayesian network structure optimizes scoring function given dataset shortest path problem  idea
represent solution space learning problem implicit state space search graph 
shortest path start goal nodes graph corresponds
optimal bayesian network  perspective highlights importance two orthogonal
research issues  development search strategies solving shortest path problem 
design admissible heuristic functions guiding search  present several
techniques address issues  firstly  a  search algorithm developed learn
optimal bayesian network focusing searching promising parts solution
space  secondly  two heuristic functions introduced guide search  tightness
heuristic determines efficiency search algorithm  first heuristic represents simple relaxation acyclicity constraint bayesian networks
variable chooses optimal parents independently  result  heuristic estimate may
contain many directed cycles result loose bound  second heuristic  named
k cycle conflict heuristic  based form relaxation tightens bound
avoiding directed cycles within groups variables  finally  traversing
search graph  need calculate cost arc visited  corresponds
selecting optimal parents variable candidate set  present two data
structures storing querying costs candidate parent sets  one set
full exponential size data structures called parent graphs stored hash tables
answer query constant time  sparse representation parent
graph stores optimal parent sets improve space efficiency 
empirically evaluated a  algorithm empowered different combinations
heuristic functions parent graph representations set uci machine learning
datasets  results show even simple heuristic full parent graph representation  a  often achieve better efficiency and or scalability existing approaches
learning optimal bayesian networks  k cycle conflict heuristic sparse parent
graph representation enabled algorithm achieve even greater efficiency
scalability  results indicate proposed methods constitute promising approach
learning optimal bayesian network structures 
remainder paper structured follows  section   reviews problem
learning optimal bayesian networks reviews related work  section   introduces
shortest path perspective learning problem  formulation search graph
discussed detail  section   introduces two data structures developed compute
store optimal parent sets pairs variables candidate sets  data structures used query cost arc search graph  section   presents
a  search algorithm  developed two heuristic functions guiding algorithm
studied theoretical properties  section   presents empirical results evaluating
algorithm several existing approaches  finally  section   concludes paper 
  

filearning optimal bayesian networks

   background
first provide brief summary related work learning bayesian networks 
    learning bayesian network structures
bayesian network directed acyclic graph  dag  g represents joint probability
distribution set random variables v    x    x         xn    directed arc xi
xj represents dependence two variables  say xi parent xj  
use paj stand parent set xj   dependence relation xj paj
quantified using conditional probability distribution  p  xj  paj    joint probability
distribution represented g factorized product
q conditional probability
distributions network  i e   p  x         xn     ni   p  xi  pai    addition
compact representation  bayesian networks provide principled approaches solving
various inference tasks  including belief updating  probable explanation  maximum
posteriori assignment  pearl         relevant explanation  yuan  liu  lu    lim 
      yuan  lim    littman      a  yuan  lim    lu      b  
given dataset    d         dn    data point di vector values
variables v  learning bayesian network task finding network structure
best fits d  work  assume variable discrete finite number
possible values  data point missing values 
roughly three main approaches learning problem  score based learning 
constraint based learning  hybrid methods  score based learning methods evaluate
quality bayesian network structures using scoring function selects one
best score  cooper   herskovits        heckerman         methods basically
formulate learning problem combinatorial optimization problem  work well
datasets many variables  may fail find optimal solutions large
datasets  discuss approach detail next section 
approach take  constraint based learning methods typically use statistical testings
identify conditional independence relations data build bayesian network
structure best fits independence relations  pearl        spirtes  glymour   
scheines        cheng  greiner  kelly  bell    liu        de campos   huete        xie  
geng         constraint based methods mostly rely results local statistical testings 
often scale large datasets  however  sensitive accuracy
statistical testings may work well insufficient noisy data 
comparison  score based methods work well even datasets relatively data
points  hybrid methods aim integrate advantages previous two approaches
use combinations constraint based and or score based methods solving learning
problem  dash   druzdzel        acid   de campos        tsamardinos  brown    aliferis 
      perrier  imoto    miyano         one popular strategy use constraint based
learning create skeleton graph use score based learning find high scoring
network structure subgraph skeleton  tsamardinos et al         perrier et al  
       work  consider bayesian model averaging methods aim
estimate posterior probabilities structural features edges rather model
selection  heckerman        friedman   koller        dash   cooper        
  

fiyuan   malone

    score based learning
score based learning methods rely scoring function score    evaluating quality
bayesian network structure  search strategy used find structure g optimizes
score  therefore  score based methods two major elements  scoring functions
search strategies 
      scoring functions
many scoring functions used measure quality network structure 
bayesian scoring functions define posterior probability distribution
network structures conditioning data  structure highest
posterior probability presumably best structure  scoring functions best
represented bayesian dirichlet score  bd   heckerman  geiger    chickering       
variations  e g   k   cooper   herskovits         bayesian dirichlet score
score equivalence  bde   heckerman et al          bayesian dirichlet score score
equivalence uniform priors  bdeu   buntine         scoring functions often
form trading goodness fit structure data complexity
structure  goodness fit measured likelihood structure given
data amount information compressed structure data 
scoring functions belonging category include minimum description length  mdl 
 or equivalently bayesian information criterion  bic   rissanen        suzuki        lam
  bacchus         akaike information criterion  aic   akaike        bozdogan        
 factorized  normalized maximum likelihood function  nml fnml   silander  roos  kontkanen    myllymaki         mutual information tests score  mit   de campos 
       scoring functions decomposable  is  score network
decomposed sum node scores  heckerman        
optimal structure g may unique multiple bayesian network structures may share optimal score    two network structures said belong
equivalence class  chickering        represent set probability distributions possible parameterizations  score equivalent scoring functions assign
score structures equivalence class  scoring functions
score equivalent 
mainly use mdl score work  let ri number states xi   npai
number data points consistent pai   pai   nxi  pai number data
points constrained xi   xi   mdl defined follows  lam   bacchus        

dl g   

x

dl xi  pai   



   often use optimal instead optimal throughout paper 

  

   

filearning optimal bayesian networks


log n
k xi  pai   
 
x
nxi  pai
h xi  pai    
nxi  pai log
 
npai
xi  pai

k xi  pai      ri   
rl  
xl pai

dl xi  pai     h xi  pai    

   
   
   

goal find bayesian network minimum mdl score  however 
methods means restricted mdl  decomposable scoring function 
bic  bdeu  fnml  used instead without affecting search strategy 
demonstrate that  test bdeu experimental section  one slight difference
mdl scoring functions latter scores need maximized
order find optimal solution  rather straightforward translate
maximization minimization problems simply changing sign scores  also 
sometimes use costs refer scores  represent distances
nodes search graph 
      local search strategies
given n variables  o n n n     directed acyclic graphs  dags   size
solution space grows exponentially number variables  surprising
score based structure learning shown np hard  chickering         due
complexity  early research focused mainly developing approximation algorithms  heckerman        bouckaert         popular search strategies used include greedy hill
climbing  stochastic search  genetic algorithm  etc  
greedy hill climbing methods typically begin initial network  e g   empty
network randomly generated structure  repeatedly apply single edge operations 
including addition  deletion  reversal  finding locally optimal network  extensions approach include tabu search random restarts  glover         limiting
number parents parameters variable  friedman et al          searching
space equivalence classes  chickering         searching space variable
orderings  teyssier   koller         searching constraints extracted
data  tsamardinos et al          optimal reinsertion algorithm  or   moore   wong 
      adds different operator  variable removed network  optimal parents
selected  variable reinserted network parents 
parents selected ensure new network still valid bayesian network 
stochastic search methods markov chain monte carlo simulated annealing
applied find high scoring structure  heckerman        de campos  
puerta        myers  laskey    levitt         methods explore solution space
using non deterministic transitions neighboring network structures favoring
better solutions  stochastic moves used hope escape local optima find
better solutions 
optimization methods genetic algorithms  hsu  guo  perry    stilson 
      larranaga  kuijpers  murga    yurramendi        ant colony optimization meth  

fiyuan   malone

ods  de campos  fernndez luna  gmez    puerta        daly   shen       
applied learning bayesian network structures well  unlike previous methods
work one solution time  population based methods maintain set candidate solutions throughout search  step  create next generation
solutions randomly reassembling current solutions genetic algorithms 
generating new solutions based information collected incumbent solutions
ant colony optimization  hope obtain increasingly better populations solutions
eventually find good network structure 
local search methods quite robust face large learning problems
many variables  however  guarantee find optimal solution  worse 
quality solutions typically unknown 
      optimal search strategies
recently multiple exact algorithms developed learning optimal bayesian networks  several dynamic programming algorithms proposed based observation
bayesian network least one leaf  ott et al         singh   moore        
leaf variable child variables bayesian network  order find optimal
bayesian network set variables v  sufficient find best leaf  leaf
choice x  best possible bayesian network constructed letting x choose optimal
parent set pax v  x  letting v  x  form optimal subnetwork 
best leaf choice one minimizes sum score x  pax   score v  x  
scoring function score     formally  have 
score v    min  score v    x     bestscore x  v    x    
xv

   


bestscore x  v    x    

min
score x  pax   
pax v  x 

   

given recurrence relation  dynamic programming algorithm works follows  first finds optimal structures single variables  trivial  starting
base cases  algorithm builds optimal subnetworks increasingly larger variable
sets optimal network found v  dynamic programming algorithms
find optimal bayesian network o n n   time space  koivisto   sood        ott
et al         silander   myllymaki        singh   moore         recent algorithms
improved memory complexity either trading longer running times reduced memory consumption  parviainen   koivisto        taking advantage layered structure
present within dynamic programming lattice  malone  yuan    hansen      b  malone 
yuan  hansen    bridges      a  
branch bound algorithm  bb  proposed de campos ji       
learning bayesian networks  algorithm first creates cyclic graph allowing
variable obtain optimal parents variables  best first search strategy
used break cycles removing one edge time  algorithm uses
approximation algorithm estimate initial upper bound solution pruning 
algorithm occasionally expands worst nodes search frontier hope find
  

filearning optimal bayesian networks

figure    order graph four variables 
better networks update upper bound  completion  algorithm finds optimal
network structure subgraph initial cyclic graph  algorithm ran
memory finding solution  switch using depth first search strategy
find suboptimal solution 
integer linear programming  ilp  used learn optimal bayesian network
structures  cussens        jaakkola et al          learning problem cast integer
linear program polytope exponential number facets  outer bound
approximation polytope solved  solution relaxed problem
integral  guaranteed optimal structure  otherwise  cutting planes branch
bound algorithms subsequently applied find optimal structure  recently
similar method proposed find optimal structure searching space
equivalence classes  hemmecke et al         
several methods considered optimal constraints enforce
network structure  example  optimal parents selected variable  k 
finds optimal network structure particular variable ordering  cooper   herskovits 
       methods developed  ordyniak   szeider        kojima  perrier  imoto   
miyano        find optimal network structure must subgraph given super
graph 

   shortest path perspective
section introduces shortest path perspective problem learning bayesian
network structure given dataset 
    order graph
state space graph learning bayesian networks basically hasse diagram containing
subsets variables domain  figure   visualizes state space graph
learning problem four variables  top most node empty set layer
  

fiyuan   malone

  start search node  bottom most node complete set layer n
goal node  n number variables domain  arc u u  x 
represents generating successor node adding new variable  x  existing set
variables u  u called predecessor u  x   cost arc equal score
selecting optimal parent set x u  i e   bestscore x  u   example  arc
 x    x     x    x    x    cost equal bestscore x     x    x      node layer
ni successors many ways add new variable  predecessors
many leaf choices  define expanding node u generating successors
nodes u 
search graph thus defined  path start node goal node defined
sequence nodes arc nodes next node
sequence  path corresponds ordering variables order
appearance  example  path traversing nodes    x      x    x      x    x    x    
 x    x    x    x    stands variable ordering x    x    x    x    call
search graph order graph  cost path defined sum costs
arcs path  shortest path path minimum total cost
order graph 
given shortest path  reconstruct bayesian network structure noting
arc path encodes choice optimal parents one variables
preceding variables  complete path represents ordering
variables  therefore  putting together optimal parent choices generates valid
bayesian network  construction  bayesian network structure optimal 
    finding shortest path
various methods applied solve shortest path problem  dynamic programming
considered evaluate order graph using top sweep order graph  silander
  myllymaki        malone et al       b   layer layer  dynamic programming finds
optimal subnetwork variables contained node order graph based
results previous layers  example  three ways construct bayesian
network node  x    x    x     using  x    x    subnetwork x  leaf  using
 x    x    subnetwork x  leaf  using  x    x    subnetwork x 
leaf  top down sweep makes sure optimal subnetworks already found
 x    x      x    x      x    x     need select optimal parents
leaves identify leaf produces optimal network  x    x    x    
evaluation reaches node last layer  shortest path and  equivalently  optimal
bayesian network found global variable set 
drawback dynamic programming approach need compute
bestscore    candidate parent sets variable  n variables 
 n nodes order graph   n  parent scores computed
variable  totally n n  scores  number variables increases  computing storing
order parent graphs quickly becomes infeasible 
paper  propose apply a  algorithm  hart  nilsson    raphael       
solve shortest path problem  a  uses heuristic function evaluate quality
search nodes expand promising search node search step 
  

filearning optimal bayesian networks

guidance heuristic functions  a  needs explore part search
graph finding optimal solution  however  comparison dynamic programming 
a  overhead calculating heuristic values maintaining priority queue 
actual relative performance dynamic programming a  thus depends
efficiency calculating heuristic values tightness values  felzenszwalb
  mcallester        klein   manning        

   finding optimal parent sets
introducing algorithm solving shortest path problem  first discuss
obtain cost bestscore x  u  arc u u  x  visit
order graph  recall arc involves selecting optimal parents variable
candidate set  need consider subsets candidate set finding subset
best score  section  introduce two data structures related methods
computing storing optimal parent sets scores pairs variable candidate
parent set 
exact algorithms learning bayesian network structures need calculate
optimal parent sets scores  present reasonable approach calculation
paper  note  however  approach applicable algorithms  vice versa 
    parent graph
use data structure called parent graph compute costs arcs order graph 
variable parent graph  parent graph variable x hasse diagram
consisting subsets variables v    x   node u stores optimal parent
set pax u minimizes score x  pa x   well bestscore x  u  itself 
example  figure   b  shows sample parent graph x  contains best scores
subsets  x    x    x     obtain figure   b   however  first need calculate
preliminary graph figure   a  contains raw score subset u parent
set x    i e   score x    u   equation   shows  scores calculated based
counts particular instantiations parent child variables 
use ad tree  moore   lee        collect counts dataset
compute scores  ad tree unbalanced tree structure contains two types
nodes  ad tree nodes varying nodes  ad tree node stores number data points
consistent particular variable instantiation  varying node used instantiate
state variable  full ad tree stores counts data points consistent
partial instantiations variables  sample ad tree two variables shown
figure    n variables states each  number ad tree nodes ad tree
 d   n   grows even faster size order parent graph  moore lee       
described sparse ad tree significantly reduces space complexity  readers
referred paper details  pseudo code assumes sparse ad tree
used 
given ad tree  ready calculate raw scores score x       figure   a  
exponential number scores parent graph  however  parent
sets possibly optimal bayesian network  certain parent sets discarded
without ever calculating values according following theorems tian        
  

fiyuan   malone

figure    sample parent graph variable x     a  raw scores score x      
parent sets  first line node gives parent set  second
line gives score using set parents x     b  optimal
scores bestscore x       candidate parent set  second line
node gives optimal score using subset variables first line
parents x     c  optimal parent sets scores  pruned parent
sets shown gray  parent set pruned predecessors
better score 

x     
x     
c     
vary
v
x 

vary
v
x 

x     
x     

x     
x     

x     
x     

x     
x     

c     

c     

c     

c     

vary
x 

vary
x 

x     
x     

x     
x     

x     
x     

x     
x     

c     

c  

c     

c     

figure    ad tree 
use theorems compute necessary mdl scores  scoring functions
bdeu similar pruning rules  de campos   ji         algorithm   provides
pseudo code calculating raw scores 
theorem   optimal bayesian network based mdl scoring function 
 n
variable log  log
n   parents  n number data points 
  

filearning optimal bayesian networks

algorithm   score calculation algorithm
input  ad sparse ad tree input data  v input variables 
output  score x  u  pair x v u v    x 
   function calculatemdlscores ad  v 
  
xi v
  
calculatescores xi   ad 
  
end
   end function
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

function calculatescores xi  ad 
 n
k   log  log
prune due theorem  
n  
u u v    x    u     k
parent sets size k
prune f alse
u
k xi  u    score xi   u    y       
prune true
prune due theorem  
break
end
end
prune     true
score xi   u  log  n k xi  u 
complexity term
instantiation xi   u xi   u
log likelihood term
cf amily getcount  xi   u ad 
cp arents getcount u  ad 
score xi   u  score xi   u    cf amily log cf amily
score xi   u  score xi   u    cf amily log cp arents
end
end
end
end
end function

theorem   let u two candidate parent sets x  u s  k xi  s 
dl xi  u       supersets cannot possibly optimal parent sets
x 
computing raw scores  compute parent graph according following
theorem appeared many earlier papers  e g   see work teyssier
koller         de campos ji         theorem simply means parent set
optimal subset better score 
theorem   let u two candidate parent sets x u s 
score x  u  score x  s   optimal parent set x candidate set 
  

fiyuan   malone

algorithm   computing parent graphs
input  necessary score x  u   x v u v    x 
output  full parent graphs containing bestscore x  u 
   function calculatefullparentgraphs v  score       
  
x v
  
layer   n
propagate best scores graph
  
u u v    x    u     layer
  
calculatebestscore x  u  score       
  
end
  
end
  
end
   end function
   
   
   
   
   
   
   
   

function calculatebestscore x  u  score       
bestscore x  u  score x  u 
u
bestscore x  u    y      bestscore x  u 
bestscore x  u  bestscore x  u    y   
end
end
end function

function getbestscore x  u 
   
return bestscore x  u 
    end function

propagate best scores

query bestscore x  u 

   

therefore  generate successor node u y   u parent graph x 
check whether score x  u  y    smaller bestscore x  u   so  let parent
graph node u y   record optimal parent set  otherwise bestscore x  u 
smaller  propagate optimal parent set u u y    propagation 
must following  teyssier   koller        
theorem   let u two candidate parent sets x u s  must
bestscore x  s  bestscore x  u  
pseudo code propagating scores computing parent graph outlined
algorithm    figure   b  shows parent graph optimal scores propagating
best scores top bottom 
search order graph  whenever visit new arc u u  x  
find score looking parent graph variable x  example  need find
optimal parents x   x    x     look node  x    x    x  parent graph
find optimal parent set score  make look ups efficient  use hash
tables organize parent graphs query answered constant time 
  

filearning optimal bayesian networks

parentsx 
scoresx 

 x    x   
 

 x   
 

 x   
 

  
  

table    sorted scores parent sets x  pruning parent sets
possibly optimal 
parentsx 
 
parentsx
x 
x 
parentsx 
 
parentsx
x 

 x    x   
 
 
 

 x   
 
 
 

 x   
 
 
 

  
 
 
 

table    parentsx  xi   bit vectors x      line xi indicates corresponding parent set includes variable xi     indicates otherwise  note
that  pruning  none optimal parent sets include x   

    sparse parent graphs
full parent graph variable x exhaustively enumerates subsets v    x 
stores bestscore x  u  subsets  naively  approach requires storing
n n  scores parent sets  silander   myllymaki         theorem    however 
number optimal parent sets often far smaller full size  figure   b  shows
optimal parent set may shared several candidate parent sets  full parent
graph representation allocate space repetitive information candidate sets 
resulting waste time space 
address limitations  introduce sparse representation parent graphs
related scanning techniques querying optimal parent sets  full parent
graphs  begin calculating pruning scores described last section  due
theorems      parent sets pruned without evaluated 
therefore  create full parent graphs  also  instead creating
hasse diagrams  sort optimal parent scores variable x list 
maintain parallel list stores associated optimal parent sets  call sorted
lists scoresx parentsx   table   shows sorted lists optimal scores
parent graph figure   b   essence  allows us store efficiently process
scores figure   c  
find optimal parent set x candidate set u  simply scan
list x starting beginning  soon find first parent set subset
u  find optimal parent score bestscore x  u   trivially true due
following theorem 
theorem   first subset u parentsx optimal parent set x u 
scanning lists find optimal parent sets inefficient done properly 
since scanning arc visited order graph  inefficiency
scanning large impact search algorithm 
  

fiyuan   malone

parentsx 
validx 
 
parentsx
x 
new
validx 

 x    x   
 
 
 

 x   
 
 
 

 x   
 
 
 

  
 
 
 

table    result performing bitwise operation exclude parent sets
include x      validx  bit vector means parent set
include x  used selecting optimal parents  first set bit
indicates best possible score parent set 

parentsx 
validx 
 
parentsx
x 
new
validx 

 x    x   
 
 
 

 x   
 
 
 

 x   
 
 
 

  
 
 
 

table    result performing bitwise operation exclude parent sets
include either x  x      validnew
x  bit vector means parent
set includes neither x  x    initial validx  bit vector already excluded
x    finding validnew
x  required excluding x   

ensure efficiency  propose following scanning technique  variable
x  first initialize working bit vector length kscoresx k called validx  s 
indicates parent scores scoresx usable  then  create n   bit vectors
length kscoresx k  one variable v    x   bit vector variable
denoted parentsyx contains  s parent sets contain  s others 
table   shows bit vectors example table    then  exclude variable
candidate parent  perform bit operation validnew
validx   parentsyx   new
x
validx bit vector contains  s parent sets subsets v    y   
first set bit corresponds bestscore x  v    y     table   shows example excluding
x  set possible parents x    first set bit new bit vector
corresponds bestscore x    v    x      want exclude x  candidate
parent  new bit vector last step becomes current bit vector step 
 
bit operation applied  validnew
validx   parentsx
x
x    first set
bit result corresponds bestscore x    v    x    x      table   demonstrates
operation  also  important note exclude one variable time  example 
if  excluding x    wanted exclude x  rather x    could take validnew

x
 
validx   parentsx
 

operations

described


createsparseparentgraph

x
getbestscore functions algorithm   
pruning duplicate scores  sparse representation requires much less
memory storing possible parent sets scores  long kscores x k  
c n    n     requires less memory memory efficient dynamic programming
algorithm  malone et al       b  
experimentally  show kscoresx k almost
  

filearning optimal bayesian networks

algorithm   sparse parent graph algorithms
input  necessary score x  u   x v u v    x 
output  sparse parent graphs containing optimal parent sets scores
   function createsparseparentgraph x  score       
  
x v
  
scorest   parentst sort score x    
sort scores  preferring low cardinality
  
scoresx   parentsx
initialize possibly optimal scores
  
     scorest  
  
prune f alse
  
j      scoresx  
check better subset pattern exists
  
contains parentst i   parentsx  j   scoresx  i  scorest  i 
  
prune true
   
break
   
end
   
end
   
prune     true
   
append scoresx   parentsx parentst i   parentst  i 
   
end
   
end
   
     scoresx  
set bit vectors efficient querying
   
parentsx  i 
   
set parentsyx  i  
   
end
   
end
   
end
    end function
   
   
   
   
   
   
   
   

function getbestscore x  u 
valid allscoresx
v   u
valid valid  parentsyx
end
f sb f irstsetbit valid 
return scoresx  f sb 
end function

query bestscore x  u 

return first score set bit

always smaller c n    n    several orders magnitude  approach offers
 usually substantial  memory savings compared previous best approaches 
sparse representation extra benefit improving time efficiency well 
full representation  create complete exponential size parent graphs 
even though many nodes parent graph share optimal parent choices 
sparse representation  avoid creating nodes  makes creating sparse
parent graphs much efficient 
  

fiyuan   malone

   a  search algorithm
ready tackle shortest path problem order graph  section
presents search algorithm well two admissible heuristic functions guiding
algorithm 
    algorithm
apply well known state space search method  a  algorithm  hart et al         
solve shortest path problem order graph  main idea algorithm
use evaluation function f measure quality search nodes always expand
one lowest f cost exploration order graph  node u 
f  u  decomposed sum exact past cost  g u   estimated future cost 
h u   g u  cost measures shortest distance start node u 
h u  cost estimates far away u goal node  therefore  f cost provides
estimated total cost best possible path passes u 
a  uses open list  usually priority queue  store search frontier 
closed list store expanded nodes  initially open list contains start node 
closed list empty  search step  node lowest f  cost
open list  say u  selected expansion generate successor nodes  expanding
u  however  need first check whether goal node  yes  shortest path
goal found  construct bayesian network path terminate
search 
u goal  expand generate successor nodes  successor
considers one possible way adding new variable  say x  leaf existing
subnetwork variables u    u  x   g cost calculated
sum g cost u cost arc u s  arc cost well
optimal parent set pax x u retrieved xs parent graph  h cost
computed heuristic function describe shortly  record
following information    g cost  h cost  x  pax  
clear order graph multiple paths node 
perform duplicate detection see whether node representing set variables
already generated before  check duplicates  search space blows
order graph size  n order tree size n   first check whether
duplicate already exists closed list  so  check whether duplicate
better g cost s  yes  discard immediately  represents worse path 
otherwise  remove duplicate closed list  place open list 
happens found better path lower g cost  reopen node future
search 
duplicate found closed list  need check open list 
duplicate found  simply add open list  otherwise  compare
g costs duplicate s  duplicate lower g cost  discarded 
otherwise  replace duplicate s  again  lower g cost means better
path found 
   delay calculation h duplicate detection avoid unnecessary calculations
nodes pruned 

  

filearning optimal bayesian networks

algorithm   a  search algorithm
input  full sparse parent graphs containing bestscore x  u 
output  optimal bayesian network g
   function main d 
  
start
  
score start    p
  
push open  start  v bestscore y  v    y   
  
 isempty open 
  
u pop open 
  
u goal
shortest path found
  
print the best score   score v  
  
g construct network shortest path
   
return g
   
end
   
put closed  u 
   
x v   u
generate successors
   
g bestscore x  u    score u 
   
contains closed  u  x  
closed list dd
   
g   score u  x  
reopen node
   
delete closed  u  x  
   
push  open  u  x   g   h 
   
score u  x   g
   
end
   
else
   
contains open  u  x     g   score u  x   open list dd
   
update open  u  x   g   h 
   
score u  x   g
   
end
   
end
   
end
   
end
    end function

successor nodes generated  place node u closed
list  indicates node already expanded  expanding top node open
list called one search step  a  algorithm performs step repeatedly goal
node selected expansion  moment shortest path start state
goal state found 
shortest path found  reconstruct optimal bayesian network
structure starting goal node tracing back shortest path reaching
start node  since node path stores leaf variable optimal parent set 
putting optimal parent sets together generates valid bayesian network structure 
pseudo code a  algorithm shown algorithm   
  

fiyuan   malone

    simple heuristic function
a  algorithm provides different theoretical guarantees depending properties
heuristic function h  function h admissible h cost never greater
true cost goal  words  optimistic  given admissible heuristic
function  a  algorithm guaranteed find shortest path goal node
selected expansion  pearl         let u node order graph  first consider
following simple heuristic function h 
definition  
h u   

x

bestscore x  v  x   

   

xv u

heuristic function allows remaining variable choose optimal parents
variables  design reflects principle exact cost relaxed problem
used admissible bound original problem  pearl         case 
original problem learn bayesian network directed acyclic graph  equation  
relaxes problem ignoring acyclicity constraint  directed cyclic graphs
allowed  heuristic function easily proven admissible following theorem 
proofs theorems paper found appendix a 
theorem   h admissible 
turns h even nicer property  heuristic function consistent if 
node u successor s  h u  h s    c u  s   c u  s  stands cost
arc u s  given consistent heuristic  f cost monotonically non decreasing
following path order graph  result  f cost node less
equal f cost goal node  follows immediately consistent heuristic
guaranteed admissible  consistent heuristic  a  algorithm guaranteed
find shortest path node u u selected expansion  duplicate
found closed list  duplicate must optimal g cost  new node
discarded immediately  show following simple heuristic equation  
consistent 
theorem   h consistent 
heuristic may seem expensive compute requires computing bestscore x  v 
 x   variable x  however  scores easily found querying parent
graphs stored array repeated use  takes linear time calculate
heuristic start node  subsequent computation h  however  takes constant
time simply subtract best score newly added variable
heuristic value parent node 
    improved admissible heuristic
simple heuristic function defined equation    referred hsimple hereafter  relaxes
acyclicity constraint bayesian networks completely  result  hsimple may introduce
many directed cycles result loose bound  introduce another heuristic
section tighten heuristic  first use toy example motivate new heuristic 
describe two specific approaches computing heuristic 
  

filearning optimal bayesian networks

x 

x 

x 

x 

figure    directed graph representing heuristic estimate start search node 

      motivating example
hsimple   heuristic estimate start node order graph allows variable
choose optimal parents variables  suppose optimal parent sets
x    x    x    x   x    x    x      x    x      x      x    x    respectively  parent
choices shown directed graph figure    since acyclicity constraint
ignored  directed cycles introduced  e g   x  x    however  know
final solution cannot cycles  three cases possible x  x        x 
parent x   so x  cannot parent x         x  parent x        neither
true  based theorem    third case cannot provide better value
first two cases one variables must fewer candidate parents 
         unclear one better  take minimum
get lower bound  consider case      delete arc x  x  rule
x  parent x    let x  rechoose optimal parents remaining
variables  x    x     is  must check parent sets including x    deletion
arc alone cannot produce new bound best parent set x 
 x    x    necessarily  x     total bound x  x  computed summing
together original bound x  new bound x    call total bound
b    case     handled similarly  call total bound b    joint cost
x  x    c x    x     must optimistic  compute minimum b  b   
effectively considered possible ways break cycle obtained tighter
heuristic value  new heuristic clearly admissible  still allow cycles among
variables 
often  hsimple introduces multiple cycles heuristic estimate  figure  
cycle x  x    cycle shares x  earlier cycle x  x   
say cycles overlap  one way break cycles set parent set x 
 x     however  introduces new cycle x  x    described detail
shortly  partition variables exclusive groups break cycles within
group  example  x  x  different groups  break cycle 
  

fiyuan   malone

      k cycle conflict heuristic
idea generalized compute joint cost variable group
size k avoiding cycles within group  node u order graph 
calculate heuristic value partitioning variables v   u several exclusive
groups sum costs together  name resulting technique k cycle conflict
heuristic  note simple heuristic hsimple special case new heuristic 
simply contains costs individual variables  k    
new heuristic application additive pattern database technique  felner 
korf    hanan         pattern databases  culberson   schaeffer        approach
computing admissible heuristic problem solving relaxed problem  consider
   puzzle problem     square tiles numbered      randomly placed  
  box one position left empty  configuration tiles called state 
goal slide tiles one time destination configuration  tile slide
empty position beside position     puzzle relaxed
contain tiles     tiles removed  relaxation  multiple
states original problem map one state abstract state space relaxed
problem share positions remaining tiles  abstract state called
pattern  cost pattern equal smallest cost sliding remaining
tiles destination positions  cost provides lower bound state
original state space maps pattern  costs patterns stored
pattern database 
relax problem different ways obtain multiple pattern databases 
solutions several relaxed problems independent  problems said
exclusive     puzzle  relax contain tiles       relaxation
solved independently previous one share puzzle
movements  concrete state original state space  positions tiles    
map pattern first pattern database  positions tiles      map
different pattern second pattern database  costs patterns added
together obtain admissible heuristic  hence name additive pattern databases 
learning problem  pattern defined group variables  cost
optimal joint cost variables avoiding directed cycles them 
decomposability scoring function implies costs two exclusive patterns
added together obtain admissible heuristic 
explicitly break cycles computing cost pattern 
following theorem offers straightforward approach so 
theorem   cost pattern u  c u   equal shortest distance v   u
goal node order graph 
consider example figure    cost pattern  x    x    equal
shortest distance  x    x    goal order graph figure   
furthermore  difference c u  sum simple heuristic values
variables u indicates amount improvement brought avoiding cycles within
pattern  differential score  called h   thus used quality measure
ordering patterns choosing patterns likely result tighter
heuristic 
  

filearning optimal bayesian networks

      dynamic k cycle conflict heuristic
two slightly different versions k cycle conflict heuristic  first version
named dynamic k cycle conflict heuristic  compute costs groups variables
size k store single pattern database  according theorem   
heuristic computed finding shortest distances nodes
last k layers order graph goal 
compute heuristic using breadth first search backward search
order graph k layers  search starts goal node expands order
graph backward layer layer  reverse arc u  x  u cost arc
u u  x   i e   bestscore x  u   reverse g cost u updated whenever new
path lower cost found  breadth first search ensures node u obtain
exact reverse g cost previous layer expanded  g cost cost pattern
v   u  compute differential score  h   pattern time 
pattern better differential score subset patterns
discarded  pruning significantly reduce size pattern database improve
query efficiency  algorithm computing dynamic k cycle conflict heuristic
shown algorithm   
heuristic created  calculate heuristic value search node
follows  node u  partition remaining variables v   u set exclusive
patterns  sum costs together heuristic value  since prune superset
patterns  always find partition  however  potentially many ways
partition  ideally want find one highest total cost  represents
tightest heuristic value  problem finding optimal partition formulated
maximum weighted matching problem  felner et al          k      define
undirected graph vertex represents variable  edge two
variables represents pattern containing variables weight equal
cost pattern  goal select set edges graph two
edges share vertex total weight edges maximized  matching problem
solved o n    time  n number vertices  papadimitriou   steiglitz 
      
k      add hyperedges matching graph connecting
k vertices represent larger patterns  goal becomes select set edges
hyperedges maximize total weight  however  three dimensional higher order
maximum weighted matching problem np hard  garey   johnson         means
solve np hard problem calculating heuristic value 
alleviate potential inefficiency  greedily select patterns based quality 
consider node u unsearched variables v   u  choose pattern highest
differential cost patterns subsets v   u  repeat step
remaining variables variables covered  total cost chosen patterns
used heuristic value u  hdynamic function algorithm   gives pseudocode
computing heuristic value 
dynamic k cycle conflict heuristic introduced example dynamically partitioned pattern database  felner et al         patterns dynamically
selected search algorithm  refer dynamic pattern database short 
  

fiyuan   malone

algorithm   dynamic k cycle conflict heuristic
input  full sparse parent graphs containing bestscore x  u 
output  pattern database p patterns size k
   function createdynamicpd k 
  
p d   v   
  
h  v   
  
l     k
perform bfs k levels
  
u p dl 
  
expand u  l 
  
checksave u 
  
p d v   u  p dl   u 
  
end
   
end
   
x p   save
remove superset patterns improvement
   
delete p d x 
   
end
   
sort p   h  
sort patterns decreasing costs
    end function
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

function expand u  l 
x u
g p dl   u    bestscore x  u    x  
g   p dl  u    x   p dl  u    x   g
end
end function

duplicate detection

function checksave u 
p
h  u  g v u bestscore y  v    y   
x v   u
check improvement subset patterns
h  u    h  u  x   save u 
end
end function
function hdynamic  u 
h 
ru
p
r
rr s
h h   p d s 
end
end
return h
end function

calculate heuristic value u

greedily find best subset pattern r

  

filearning optimal bayesian networks

potential drawback dynamic pattern databases that  even greedy
method  computing heuristic value still much expensive simple heuristic
equation    consequently  search time longer even though tighter pattern
database heuristic results pruning fewer expanded nodes 
      static k cycle conflict heuristic
address inefficiency dynamic pattern database computing heuristic values 
introduce another version named static k cycle conflict heuristic based statically
partitioned pattern database technique  felner et al          idea partition
variables several static exclusive groups  create separate pattern database
group  consider problem variables  x         x     divide variables
two groups   x         x     x         x     group  say  x         x     create
pattern database contains costs subsets  x         x    store
hash table  refer heuristic static pattern database short s
create static pattern databases follows  static grouping v   vi   need
compute pattern database group vi resembles order graph containing
subsets vi   use breadth first search create graph starting
node
vi   cost arc u x  u graph equal bestscore x    j  i vj  u  
means variables groups valid candidate parents  ensure
efficient retrieval  static pattern databases stored hashtables  nothing pruned
them  algorithm   gives pseudocode creating static pattern databases 
much simpler use static pattern databases compute heuristic value  consider
search node  x    x    x     unsearched variables  x    x    x    x    x     simply
divide variables two patterns  x    x     x    x    x    according static
grouping  look respective pattern databases  sum costs together
heuristic value  moreover  since search step processes one variable 
one pattern affected requires new score lookup  therefore  heuristic value
calculated incrementally  hstatic function algorithm   provides pseudocode
naively calculating heuristic value 
      properties k cycle conflict heuristic
versions k cycle conflict heuristic remain admissible  although avoid
cycles within pattern  cannot prevent cycles across different patterns  following theorem proves result 
theorem   k cycle conflict heuristic admissible 
understanding consistency new heuristic slightly complex  first
look static pattern database involve selecting patterns dynamically 
following theorem shows static pattern database still consistent 
theorem    static pattern database version k cycle conflict heuristic remains
consistent 
dynamic pattern database  search step needs solve maximum weighted
matching problem select set patterns compute heuristic value 
  

fiyuan   malone

algorithm   static k cycle conflict heuristics

input  full sparse parent graphs containing bestscore x  u   vi partition v
output  full pattern database p vi
   function createstaticpd vi  
  
p d i     
  
l     fivi
perform bfs vi

  
u p dl 
  
expand u  l  vi  
 u 
  
p  u  p dl 
  
end
  
end
   end function
   
   
   
   
   
   
   
   
   
   
   
   
   

function expand u  l  vi  
x vi   u

 u    bestscore x  u
g p dl 
j  i vj  


g   p dl  u x  p dl  u x  g
end
end function
function hstatic  u 
h 
vi v
h h   p  u vi  
end
return h
end function

duplicate detection

sum p separately

following  show dynamic k cycle conflict heuristic consistent closely
following theorem     work edelkamp schrodl        
theorem    dynamic pattern database version k cycle conflict heuristic remains consistent 
however  theorem assumes use shortest distances nodes
abstract space  use greedy method solve maximum weighted
matching problem  longer guarantee find shortest paths  result 
may lose consistency property dynamic pattern database  thus necessary
a  reopen duplicate node closed list better path found 

   experiments
evaluated a  search algorithm set benchmark datasets uci repository  bache   lichman         datasets    variables         data
points  discretized variables two states using mean values deleted
  

filearning optimal bayesian networks

    e   
    e   

full

largest layer

sparse

    e   
    e   

size

    e   
    e   
    e   
    e   
    e   
    e   
    e   

figure    number parent sets scores stored full parent graphs
 full   largest layer parent graphs memory efficient dynamic programming  largest layer   sparse representation  sparse  

data points missing values  a  search algorithm implemented java   
compared algorithm branch bound  bb    de campos   ji         dynamic programming  dp    silander   myllymaki         integer linear programming
 gobnilp  algorithms   cussens         used latest versions software
source code time experiments well default parameter settings 
version     gobnilp       scip  bb dp calculate mdl 
use bic score  uses equivalent calculation mdl  results confirmed
algorithms found bayesian networks either belong
equivalence class  experiments performed      ghz intel xeon   gb
ram running suse linux enterprise server version    
    full vs sparse parent graphs
first evaluated memory savings made possible sparse parent graphs comparison full parent graphs  particular  compared maximum number
scores stored variables algorithm  typical dynamic programming algorithm stores scores possible parent sets variables 
memory efficient dynamic programming  malone et al       b  stores possible parent
sets one layer parent graphs variables  size largest layer
   software package source code named urlearning  you learning  implementing a 
algorithm downloaded http   url cs qc cuny edu software urlearning html 
   http   www ecse rpi edu cvrl structlearning html
   http   b course hiit fi bene
   http   www cs york ac uk aig sw gobnilp 

  

fiyuan   malone

parent graphs indication space requirement  sparse representation
stores optimal parent sets variables 
figure   shows memory savings sparse representation benchmark
datasets  clear number optimal parent scores stored sparse representation typically several orders magnitude smaller full representation 
furthermore  due theorem    increasing number data points increases maximum number candidate parents  therefore  number candidate parent sets increases
number data points increases  however  many new parent sets pruned
sparse representation theorem    number variables affects
number candidate parent sets  consequently  number optimal parent scores
increases function number data points number variables 
results show  amount pruning data dependent  though  easily predictable 
practice  find number data points affect number unique scores much
number variables 
    pattern database heuristics
new pattern database heuristic two versions  static dynamic pattern databases 
parameterized different ways  tested various parameterizations
new heuristics a  algorithm two datasets named autos flag  chose
two datasets large enough number variables better
demonstrate effect pattern database heuristics  dynamic pattern database 
varied k      static pattern databases  tried groupings            
autos dataset groupings               flag dataset  obtained
groupings simply dividing variables datasets several consecutive blocks 
results based sparse parent graphs shown figure    show
results full parent graphs a  ran memory datasets
full parent graphs used  sparse representations  a  achieved much better
scalability  able solve autos heuristic flag
best heuristics using sparse parent graphs  hereafter experiments results
assume use sparse parent graphs 
also  pattern database heuristics improved efficiency scalability a  significantly  a  either simple heuristic static pattern database grouping
        ran memory flag dataset  pattern database heuristics enabled a  finish successfully  dynamic pattern database k     helped reduce
number expanded nodes significantly datasets  setting k     helped even
more  however  increasing k   resulted increased search time  sometimes
even increased number expanded nodes  not shown   believe larger k always
results better pattern database  occasional increase expanded nodes
greedy strategy used choose patterns fully utilize better heuristic 
longer search time understandable though  less efficient compute
heuristic value larger pattern databases  inefficiency gradually overtook
benefit  therefore  k     seems best parametrization dynamic pattern
database general  static pattern databases  able test much larger
  

filearning optimal bayesian networks

    e   

running time

size pattern database

    e   

    e   
    e   
    e   
    e   

   
   
   
   
   
   
   
   
   
  
 

autos

    e   

running time

size pattern database

    e   

    e   
    e   
    e   
    e   

   
   
   
   
   
   
   
   
   
  
 

x

x

f lag
figure    comparison a  enhanced different heuristics  hsimple   hdynamic k  
         hstatic groupings             autos dataset
groupings               flag dataset   size pattern database
means number patterns stored  running time means search time
 in seconds  using indicated pattern database strategy  x means
memory 

groups need enumerate groups certain size  results
suggest fewer larger groups tend result tighter heuristic 
sizes static pattern databases typically much larger dynamic
pattern databases  however  time needed create pattern databases still negligible comparison search time cases  thus cost effective try compute
larger affordable size static pattern databases achieve better search efficiency 
results show best static pattern databases typically helped a  achieve better
efficiency dynamic pattern databases  even number expanded nodes
larger  reason calculating heuristic values much efficient
using static pattern databases 
  

fiyuan   malone

     
bb scoring

dp scoring

a  scoring

scoring time

    

   

  

 

   

figure    comparison scoring time bb  dp  a  algorithms  label
x axis consists dataset name  number variables  number
data points 

    a  simple heuristic
first tested a  hsimple heuristic  competing algorithm roughly two
phases  computing optimal parent sets scores  scoring phase  searching bayesian
network structure  searching phase   therefore compare algorithms based two
parts running time  scoring time search time  figure   shows scoring times
bb  dp  a   gobnilp included assumes optimal scores
provided input  label horizontal axis shows dataset  number
variables  number data points  results show ad tree method used
a  algorithm seems efficient approach computing parent scores 
scoring part dp often order magnitude slower others 
result somewhat misleading  however  scoring searching parts dp
tightly integrated algorithms  result  work dp done
scoring part  little work left search  show shortly  search time
dp typically short 
figure   a  reports search time algorithms  benchmark
datasets difficult algorithms take long even fail find optimal
solutions  therefore terminate algorithm early runs       seconds
dataset  results show bb succeeded two datasets  voting
hepatitis  within time limit  datasets  a  algorithm several orders
magnitude faster bb  major difference a  bb formulation
search space  bb searches space directed cyclic graphs  a  always
maintains directed acyclic graph search  results indicate better
search space directed acyclic graphs 
results show search time needed dp algorithm often shorter
a   explained earlier  reason heavy lifting dp done
  

filearning optimal bayesian networks

     
bb

dp

gobnilp

a 

search time

    

   

  

 

x x

x

x

x

x

x

x

x x

x

 a 
     

total running time

dp total time

a  total time

    

   

  

 

 b 
figure    comparison  a  search time  in seconds  bb  dp  gobnilp  a 
 b  total running time dp a   x means corresponding
algorithm finish within time limit        seconds  ran memory
case a  
 
scoring part  add scoring search time together  shown figure   b  
a  several times faster dp datasets except adult voting  again 
gobnilp left search part   main difference a 
dp a  explores part order graph  dynamic programming fully
evaluates graph  however  step a  search algorithm overhead
cost computing heuristic function maintaining priority queue  one step
  

fiyuan   malone

a  expensive similar dynamic programming step  pruning
outweigh overhead  a  slower dynamic programming  adult
voting large number data points  makes pruning technique
theorem   less effective  although dp algorithm perform pruning  due
simplicity  algorithm highly streamlined optimized performing
calculations  dp algorithm faster a  search two
datasets  however  a  algorithm efficient dp datasets 
datasets  number data points large comparison number
variables  pruning significantly outweighs overhead a   example 
a  runs faster mushroom dataset comparing total running time even though
mushroom       data points 
comparison gobnilp a  shows advantages  a  able find optimal bayesian networks datasets well within
time limit  gobnilp failed learn optimal bayesian networks three datasets 
including letter  image  mushroom  reason gobnilp formulates
learning problem integer linear program whose variables correspond optimal
parent sets variables  even though datasets many variables 
many optimal parent sets  integer programs many variables
solvable within time limit  hand  results show gobnilp
quite efficient many datasets  even though dataset may many
variables  gobnilp solve efficiently long number optimal parent sets
small  much efficient a  datasets hepatitis heart  although
opposite true datasets adult statlog 
    a  pattern database heuristics
since static pattern databases seem work better dynamic pattern databases
cases  tested a  static pattern database  a  sp  a   dp  gobnilp
datasets used figure   well several larger datasets  used simple
static grouping n  n  datasets  n number variables 
results bb excluded solve additional dataset  results
shown figure   
benefits brought pattern databases a  rather obvious 
datasets a  able finish  a  sp typically order magnitude
faster  addition  a  sp able solve three larger datasets  sensor  autos  flag 
a  failed them  running time datasets pretty short 
indicates memory consumption parent graphs reduced  a 
able use memory order graph solve search problems rather
easily 
dp able solve one dataset  autos  a  able solve 
somewhat surprising given a  pruning capability  explanation a 
stores search information ram  fail ram exhausted  dp
algorithm described silander myllymaki        stores intermediate results
computer files hard disks  able scale larger datasets a  
  

filearning optimal bayesian networks

    

search time

dp

gobnilp

a 

a   sp

   

  

 

x

x

x

xxx

x

x xx x x

figure    comparison search time  in seconds  dp  gobnilp  a   a  sp 
x means corresponding algorithm finish within time
limit        seconds  ran memory case a  

gobnilp able solve autos  horse  flag  failed sensors  sensors
dataset        data points  number optimal parent sets large  almost
    shown figure    gobnilp begins difficulty solving datasets
       optimal parent scores particular computing environment  again 
gobnilp quite efficient datasets able solve autos flag 
algorithm solve horse dataset  figure    clear
reason number optimal parent sets small dataset 
    pruning a 
gain insight performance a   looked amount pruning
a  different layers order graph  plot figure    detailed numbers
expanded nodes versus numbers unexpanded nodes layer order
graph two datasets  mushroom parkinsons  use datasets
largest datasets solved a  a  sp  manifest different
pruning behaviors  top two figures show results a  simple heuristic 
bottom two show a  sp algorithm 
mushroom  plain a  needed expand small portion search nodes
layer  indicates heuristic function quite tight dataset  effective
pruning started early  th layer  parkinsons  however  plain a 
successful pruning nodes  first    layers  heuristic function appeared
loose  a  expand nodes layers  heuristic function became
tighter latter layers enabled a  prune increasing percentage search
nodes  help pattern database heuristic  however  a  sp helped prune many
  

fi    e   
expanded

    e   

unexpanded

expandedvsunexpandednodes

expandedvsunexpandednodes

yuan   malone

    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

 

 

 

 

  

     
layer

  

  

  

    e   
expanded

    e   
    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

  

 a  a  mushroom

 

 

 

 

        
layer

  

  

  

  

  

  

  

 b  a  parkinsons
    e   

    e   
expanded

unexpanded

expandedvsunexpandednodes

expandedvsunexpandednodes

unexpanded

    e   
    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

 

 

 

 

     
layer

  

  

  

  

  

 c  a  sp mushroom

expanded

unexpanded

    e   
    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

 

 

 

 

        
layer

  

 d  a  sp parkinsons

figure     number expanded unexpanded nodes a  layer order
graph mushroom parkinsons using different heuristics 

search nodes parkinsons  pruning became effective early  th layer 
a  sp helped prune nodes mushroom  although benefit clear
a  already quite effective dataset 
    factors affecting learning difficulty
several factors may affect difficulty dataset bayesian network learning
algorithms  including number variables  number data points  number
optimal parent sets  analyzed correlation factors search
times algorithms  replaced occurrence time       order
make analysis possible  we caution though may results underestimation  
figure    shows results  excluded results bb finished two
datasets  dp  a   a  sp  important factor determining efficiency
number variables  correlations search time numbers
variables greater       however  seems negative correlation
search time number data points  intuitively  increasing number
data points make dataset difficult  explanation preexisting negative correlation number data points number variables
datasets tested  analysis shows correlation      
  

filearning optimal bayesian networks

 

variables

data records

optimal parent sets

   

correlation

   
   
   
 
   

dp

gobnilp

a 

a   sp

   
   

figure     correlation search time algorithms several factors
may affect difficulty learning problem  including number
variables  number data points dataset  number optimal
parent sets 

since search time strong positive correlation number variables 
seemingly negative correlation search time number data points
becomes less surprising 
comparison  efficiency gobnilp affected number optimal
parent sets  correlation high close      also  positive correlation
number data points efficiency  because  explained earlier 
data points often leads optimal parent sets  finally  correlation
number variables almost zero  means difficulty dataset gobnilp
determined number variables 
insights quite important  provide guideline choosing suitable
algorithm given characteristic dataset  many optimal parent sets
many variables  a  better algorithm  way around true  gobnilp
better 
    effect scoring functions
analyses far based mainly mdl score  decomposable scoring
functions used a  algorithm  correctness search strategies
heuristic functions affected scoring function  however  different scoring
functions may different properties  example  theorem   property mdl
score  cannot use pruning technique scoring functions  consequently 
number optimal parent sets  tightness heuristic  practical performance
various algorithms may affected 
verify hypothesis  tested bdeu scoring function  heckerman       
equivalent sample size set      since scoring phase common
exact algorithms  focus experiment comparing number optimal parent
sets resulted scoring functions  search time a  sp gobnilp
  

fiyuan   malone

optimal ps  mdl

optimal ps  bdeu

size

        
       
      
     
    
   
  
 

 a 
     
gobnilp  mdl

gobnilp  bdeu

a   mdl

a   bdeu

search time

    

   

  

 

xx

xx

x

xx

xx

x

 b 
figure     comparison  a  number optimal parent sets   b  search time
a  sp gobnilp various datasets two scoring functions  mdl
bdeu 

datasets  horse flag included optimal parent sets
unavailable  figure    shows results 
main observation number optimal parent sets differ mdl
bdeu  bdeu score tends allow larger parent sets mdl results
larger number optimal parent sets datasets  difference around
order magnitude datasets imports autos 
comparison search time shows a  sp affected much gobnilp  increase number optimal parent sets  efficiency finding
optimal parent set affected  a  sp slowed slightly
datasets  significant change mushroom dataset  took a  sp  
seconds solve dataset using mdl      seconds using bdeu  comparison 
gobnilp affected much more  able solve datasets imports autos effi  

filearning optimal bayesian networks

ciently using mdl  failed solve within   hours using bdeu  remained
unable solve letter  image  mushroom  sensors within time limit 

   discussions conclusions
paper presents shortest path perspective problem learning optimal bayesian
networks optimize given scoring function  uses implicit order graph represent
solution space learning problem shortest path start
goal nodes graph corresponds optimal bayesian network  perspective
highlights importance two orthogonal directions research  one direction
develop search algorithms solving shortest path problem  main contribution
made line a  algorithm solving shortest path problem learning
optimal bayesian network  guided heuristic functions  a  algorithm focuses
searching promising parts solution space finding optimal bayesian
network 
second equally important research direction development search heuristics 
introduced two admissible heuristics shortest path problem  first heuristic
estimates future cost completely relaxing acyclicity constraint bayesian networks  shown admissible consistent  second heuristic 
k cycle conflict heuristic  developed based additive pattern database technique 
unlike simple heuristic variable allowed choose optimal parents independently  new heuristic tightens estimation enforcing acyclicity constraint
within small groups variables  two specific approaches computing
new heuristic  one approach named dynamic k cycle conflict heuristic computes costs
groups variables size k  search  dynamically partition
remaining variables exclusive patterns calculating heuristic value 
approach named static k cycle conflict heuristic partitions variables several static
exclusive groups  computes separate pattern database group  sum
costs static pattern databases obtain admissible heuristic  heuristics
remain admissible consistent  although consistency dynamic k cycle conflict
may sacrificed due greedy method used select patterns 
tested a  algorithm empowered different search heuristics set uci
machine learning datasets  results show pattern database heuristics
contributed significant improvements efficiency scalability a  algorithm  results show a  algorithm typically efficient dynamic
programming shares similar formulation  comparison gobnilp  integer
programming algorithm  a  less sensitive number optimal parent sets  number
data points  scoring functions  sensitive number variables
datasets  advantages  believe methods represent promising approach
learning optimal bayesian network structures 
exact algorithms learning optimal bayesian networks still limited relatively
small problems  scaling learning needed  e g   incorporating domain
expert knowledge learning  means approximation methods still useful
domains many variables  nevertheless  exact algorithms valuable
serve basis evaluate different approximation methods
  

fiyuan   malone

quality assurance  also  promising research direction develop algorithms
best properties approximation exact algorithms  is 
find good solutions quickly and  given enough resources  converge optimal
solution  malone   yuan        

acknowledgments
research supported nsf grants iis          eps          iis        
academy finland  finnish centre excellence computational inference research
coin           part research previously presented ijcai     yuan 
malone    wu        uai     yuan   malone        

appendix a  proofs
following proofs theorems paper 
a   proof theorem  
proof  note optimal parent set x u subset u 
subset best score  sorting unique parent scores makes sure
first found subset must satisfy requirements stated theorem 

a   proof theorem  
proof  heuristic function h clearly admissible  allows remaining variable
choose optimal parents variables v  chosen parent set must
superset parent set variable optimal directed acyclic graph
consisting remaining variables  due theorem    heuristic results lower
bound cost 

a   proof theorem  
proof  successor node u  let   u 
x
h u   
bestscore x  v  x  
xv u



x

bestscore x  v  x  

xv u x  y

 bestscore y  u 
  h s    c u  s  
inequality holds fewer variables used select optimal parents   hence 
h consistent 

a   proof theorem  
proof  theorem proven noting avoiding cycles variables
u equivalent finding optimal ordering variables best joint score 
  

filearning optimal bayesian networks

different paths v   u goal node correspond different orderings
variables  among shortest path hence corresponds optimal ordering 
a   proof theorem  
proof  node u  assume remaining variables v   u partitioned exclusive
sets v         vp   decomposability scoring function  h u   
p
p
c vi    computing c vi    allow directed cycles within vi  
i  

variables v   vi valid candidate parents  however  cost pattern  c vi   
must optimal definition pattern databases  argument used
proof theorem    h u  cost cannot worse total cost v   u  is 
cost optimal directed acyclic graph consisting variables  with u allowable
parents also   otherwise  simply arrange variables patterns
order optimal directed acyclic graph get cost  therefore  heuristic
still admissible 
note previous argument relies optimality pattern costs 
patterns chosen  greedy strategy used dynamic pattern database
affects patterns selected  therefore  theorem holds dynamic
static pattern databases 

a   proof theorem   
proof  recall using static pattern databases node partitions v   vi  
heuristic value node u follows 
h u   

x

c  v   u  vi   



 v  u  vi pattern ith static pattern database  then  successor
node u  let   u  without lost generality  let  v   u  vj   heuristic
value node
h s   

x

c  v   u  vi     c  v   u   vj    y     

i  j

also  cost u
c u  s    bestscore y  u  
definition pattern database  know c  v u vj   best possible
joint score variables pattern u searched  therefore 
c  v   u  vj   c v   u  vj    y      bestscore y   i  j vi    vj    v   u  
c  v   u   vj    y       bestscore y  u  
last inequality holds u  i  j vi    vj    v   u    following
immediately follows 
h u  h s    c u  s  
  

fiyuan   malone

hence  static k cycle conflict heuristic consistent 

a   proof theorem   
proof  heuristic values calculated dynamic pattern database considered shortest distances nodes abstract space  abstract space consists
set nodes  i e   subsets v  however  additional arcs added
node nodes k additional variables 
consider shortest path p two nodes u goal v original solution
space  path remains valid path  may longer shortest path u
v additional arcs 
let g  u  v  shortest distance u v abstract space 
successor node u  must following 
g  u  v  g  u  s    g  s  v  

   

now  recall g  u  v  g  s  v  heuristic values original solution
space  g  u  s  equal arc cost c u  s  original space  therefore
following 
h u  c u  s    h s  
   
hence  dynamic k cycle conflict heuristic consistent 



references
acid  s     de campos  l  m          hybrid methodology learning belief networks 
benedict  international journal approximate reasoning                 
akaike  h          information theory extension maximum likelihood principle 
proceedings second international symposium information theory  pp 
       
bache  k     lichman  m 
http   archive ics uci edu ml 

       

uci

machine

learning

repository 

bouckaert  r  r          properties bayesian belief network learning algorithms 
proceedings tenth conference uncertainty artificial intelligence  pp 
        seattle  wa  morgan kaufmann 
bozdogan  h          model selection akaikes information criterion  aic   general
theory analytical extensions  psychometrika             
buntine  w          theory refinement bayesian networks  proceedings seventh
conference        uncertainty artificial intelligence  pp        san francisco 
ca  usa  morgan kaufmann publishers inc 
cheng  j   greiner  r   kelly  j   bell  d     liu  w          learning bayesian networks
data  information theory based approach  artificial intelligence            
     
  

filearning optimal bayesian networks

chickering  d          transformational characterization equivalent bayesian network
structures  proceedings   th annual conference uncertainty artificial
intelligence  uai      pp        san francisco  ca  morgan kaufmann publishers 
chickering  d  m          learning bayesian networks np complete  learning
data  artificial intelligence statistics v  pp          springer verlag 
chickering  d  m          learning equivalence classes bayesian network structures 
journal machine learning research            
cooper  g  f     herskovits  e          bayesian method induction probabilistic
networks data  machine learning            
culberson  j  c     schaeffer  j          pattern databases  computational intelligence 
           
cussens  j          bayesian network learning cutting planes  proceedings
twenty seventh conference annual conference uncertainty artificial intelligence  uai      pp          corvallis  oregon  auai press 
daly  r     shen  q          learning bayesian network equivalence classes ant colony
optimization  journal artificial intelligence research             
dash  d     cooper  g          model averaging prediction discrete bayesian
networks  journal machine learning research              
dash  d  h     druzdzel  m  j          hybrid anytime algorithm construction
causal models sparse data  proceedings fifteenth annual conference
uncertainty artificial intelligence  uai     pp          san francisco  ca 
morgan kaufmann publishers  inc 
de campos  c  p     ji  q          efficient learning bayesian networks using constraints 
journal machine learning research             
de campos  c  p     ji  q          properties bayesian dirichlet scores learn bayesian
network structures  fox  m     poole  d   eds    aaai  pp          aaai press 
de campos  l  m          scoring function learning bayesian networks based
mutual information conditional independence tests  journal machine learning
research              
de campos  l  m   fernndez luna  j  m   gmez  j  a     puerta  j  m          ant colony
optimization learning bayesian networks  international journal approximate
reasoning                 
de campos  l  m     huete  j  f          new approach learning belief networks
using independence criteria  international journal approximate reasoning         
      
  

fiyuan   malone

de campos  l  m     puerta  j  m          stochastic local algorithms learning belief
networks  searching space orderings  benferhat  s     besnard  p 
 eds    ecsqaru  vol       lecture notes computer science  pp         
springer 
edelkamp  s     schrodl  s          heuristic search   theory applications  morgan
kaufmann 
felner  a   korf  r     hanan  s          additive pattern database heuristics  journal
artificial intelligence research             
felzenszwalb  p  f     mcallester  d  a          generalized a  architecture  journal
artificial intelligence research             
friedman  n     koller  d          bayesian network structure  bayesian
approach structure discovery bayesian networks  machine learning           
      
friedman  n   nachman  i     peer  d          learning bayesian network structure
massive datasets  sparse candidate algorithm  laskey  k  b     prade  h 
 eds    proceedings fifteenth conference conference uncertainty artificial
intelligence  uai      pp          morgan kaufmann 
garey  m  r     johnson  d  s          computers intractability  guide
theory np completeness  w  h  freeman   co   new york  ny  usa 
glover  f          tabu search  tutorial  interfaces               
hart  p  e   nilsson  n  j     raphael  b          formal basis heuristic determination minimum cost paths  ieee trans  systems science cybernetics        
       
heckerman  d   geiger  d     chickering  d  m          learning bayesian networks 
combination knowledge statistical data  machine learning             
heckerman  d          tutorial learning bayesian networks  holmes  d     jain 
l   eds    innovations bayesian networks  vol      studies computational
intelligence  pp        springer berlin   heidelberg 
hemmecke  r   lindner  s     studeny  m          characteristic imsets learning
bayesian network structure  international journal approximate reasoning         
         
hsu  w  h   guo  h   perry  b  b     stilson  j  a          permutation genetic algorithm
variable ordering learning bayesian networks data  langdon  w  b  
cant paz  e   mathias  k  e   roy  r   davis  d   poli  r   balakrishnan  k   honavar 
v   rudolph  g   wegener  j   bull  l   potter  m  a   schultz  a  c   miller  j  f  
burke  e  k     jonoska  n   eds    gecco  pp          morgan kaufmann 
  

filearning optimal bayesian networks

jaakkola  t   sontag  d   globerson  a     meila  m          learning bayesian network
structure using lp relaxations  proceedings   th international conference
artificial intelligence statistics  aistats   pp          chia laguna resort 
sardinia  italy 
klein  d     manning  c  d          a  parsing  fast exact viterbi parse selection 
proceedings human language conference north american association
computational linguistics  hlt naacl   pp         
koivisto  m     sood  k          exact bayesian structure discovery bayesian networks 
journal machine learning research            
kojima  k   perrier  e   imoto  s     miyano  s          optimal search clustered
structural constraint learning bayesian network structure  journal machine
learning research             
lam  w     bacchus  f          learning bayesian belief networks  approach based
mdl principle  computational intelligence             
larranaga  p   kuijpers  c  m  h   murga  r  h     yurramendi  y          learning
bayesian network structures searching best ordering genetic algorithms  ieee transactions systems  man  cybernetics  part a             
    
malone  b     yuan  c          evaluating anytime algorithms learning optimal bayesian
networks  proceedings   th conference uncertainty artificial intelligence  uai      pp          seattle  washington 
malone  b   yuan  c   hansen  e     bridges  s       a   improving scalability optimal bayesian network learning frontier breadth first branch bound search 
proceedings   th conference uncertainty artificial intelligence  uai     
pp          barcelona  catalonia  spain 
malone  b   yuan  c     hansen  e  a       b   memory efficient dynamic programming
learning optimal bayesian networks  proceedings   th aaai conference
artificial intelligence  aaai      pp            san francisco  ca 
moore  a     lee  m  s          cached sufficient statistics efficient machine learning
large datasets  journal artificial intelligence research          
moore  a     wong  w  k          optimal reinsertion  new search operator accelerated accurate bayesian network structure learning  international
conference machine learning  pp         
myers  j  w   laskey  k  b     levitt  t  s          learning bayesian networks
incomplete data stochastic search algorithms  laskey  k  b     prade  h 
 eds    proceedings fifteenth conference conference uncertainty artificial
intelligence  uai      pp          morgan kaufmann 
  

fiyuan   malone

ordyniak  s     szeider  s          algorithms complexity results exact bayesian
structure learning  gruwald  p     spirtes  p   eds    proceedings   th
conference conference uncertainty artificial intelligence  uai      pp     
     auai press 
ott  s   imoto  s     miyano  s          finding optimal models small gene networks 
pacific symposium biocomputing  pp         
papadimitriou  c  h     steiglitz  k          combinatorial optimization  algorithms
complexity  prentice hall  inc   upper saddle river  nj  usa 
parviainen  p     koivisto  m          exact structure discovery bayesian networks
less space  proceedings twenty fifth conference uncertainty artificial
intelligence  montreal  quebec  canada  auai press 
pearl  j          heuristics  intelligent search strategies computer problem solving 
addison wesley longman publishing co   inc   boston  ma  usa 
pearl  j          probabilistic reasoning intelligent systems  networks plausible inference  morgan kaufmann publishers inc 
perrier  e   imoto  s     miyano  s          finding optimal bayesian network given
super structure  journal machine learning research              
rissanen  j          modeling shortest data description  automatica             
silander  t     myllymaki  p          simple approach finding globally optimal bayesian network structure  proceedings   nd annual conference
uncertainty artificial intelligence  uai      pp          auai press 
silander  t   roos  t   kontkanen  p     myllymaki  p          factorized normalized
maximum likelihood criterion learning bayesian network structures  proceedings
 th european workshop probabilistic graphical models  pgm      pp     
    
singh  a     moore  a  w          finding optimal bayesian networks dynamic programming  tech  rep  cmu cald         carnegie mellon university 
spirtes  p   glymour  c     scheines  r          causation  prediction  search  second
edition   mit press 
suzuki  j          learning bayesian belief networks based minimum description
length principle  efficient algorithm using b b technique  international
conference machine learning  pp         
teyssier  m     koller  d          ordering based search  simple effective algorithm
learning bayesian networks  proceedings twenty first annual conference
uncertainty artificial intelligence  uai      pp          auai press 
  

filearning optimal bayesian networks

tian  j          branch and bound algorithm mdl learning bayesian networks 
uai     proceedings   th conference uncertainty artificial intelligence 
pp          san francisco  ca  usa  morgan kaufmann publishers inc 
tsamardinos  i   brown  l     aliferis  c          max min hill climbing bayesian
network structure learning algorithm  machine learning           
xie  x     geng  z          recursive method structural learning directed acyclic
graphs  journal machine learning research            
yuan  c   lim  h     littman  m  l       a   relevant explanation  computational
complexity approximation methods  annals mathematics artificial intelligence             
yuan  c   lim  h     lu  t  c       b   relevant explanation bayesian networks 
journal artificial intelligence research  jair              
yuan  c   liu  x   lu  t  c     lim  h          relevant explanation  properties 
algorithms  evaluations  proceedings   th conference uncertainty
artificial intelligence  uai      pp          montreal  canada 
yuan  c     malone  b          improved admissible heuristic learning optimal
bayesian networks  proceedings   th conference uncertainty artificial
intelligence  uai      pp          catalina island  ca 
yuan  c   malone  b     wu  x          learning optimal bayesian networks using a 
search  proceedings   nd international joint conference artificial intelligence  ijcai      pp            helsinki  finland 

  


