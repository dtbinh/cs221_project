journal of artificial intelligence research                

submitted        published      

learning optimal bayesian networks 
a shortest path perspective
changhe yuan

changhe yuan qc cuny edu

department of computer science
queens college city university of new york
queens  ny       usa

brandon malone

brandon malone cs helsinki fi

department of computer science
helsinki institute for information technology
fin       university of helsinki  finland

abstract
in this paper  learning a bayesian network structure that optimizes a scoring function
for a given dataset is viewed as a shortest path problem in an implicit state space search
graph  this perspective highlights the importance of two research issues  the development
of search strategies for solving the shortest path problem  and the design of heuristic functions for guiding the search  this paper introduces several techniques for addressing the
issues  one is an a  search algorithm that learns an optimal bayesian network structure
by only searching the most promising part of the solution space  the others are mainly
two heuristic functions  the first heuristic function represents a simple relaxation of the
acyclicity constraint of a bayesian network  although admissible and consistent  the heuristic may introduce too much relaxation and result in a loose bound  the second heuristic
function reduces the amount of relaxation by avoiding directed cycles within some groups
of variables  empirical results show that these methods constitute a promising approach
to learning optimal bayesian network structures 

   introduction
bayesian networks are graphical models that represent uncertain relations between the
random variables in a domain compactly and intuitively  a bayesian network is a directed
acyclic graph in which nodes represent random variables  and the arcs or lack of them
represent the dependence conditional independence relations between the variables  the
relations are further quantified by a set of conditional probability distributions  one for
each variable conditioning on its parents  overall  a bayesian network represents a joint
probability distribution over the variables 
applying bayesian networks to real world problems typically requires building graphical
representations of the problems  one popular approach is to use score based methods to
find high scoring structures for a given dataset  cooper   herskovits        heckerman 
       score based learning has been shown to be np hard  however  chickering        
due to the complexity  early research in this area mainly focused on developing approximation algorithms such as greedy hill climbing approaches  heckerman        bouckaert 
      chickering        friedman  nachman    peer         unfortunately the solutions
found by these methods have unknown quality  in recent years  several exact learning algoc
    
ai access foundation  all rights reserved 

fiyuan   malone

rithms have been developed based on dynamic programming  koivisto   sood        ott 
imoto    miyano        silander   myllymaki        singh   moore         branch and
bound  de campos   ji         and integer linear programming  cussens        jaakkola 
sontag  globerson    meila        hemmecke  lindner    studeny         these methods
are guaranteed to find optimal solutions when able to finish successfully  however  their
efficiency and scalability leave much room for improvement 
in this paper  we view the problem of learning a bayesian network structure that optimizes a scoring function for a given dataset as a shortest path problem  the idea is to
represent the solution space of a learning problem as an implicit state space search graph 
such that the shortest path between the start and goal nodes in the graph corresponds to
an optimal bayesian network  this perspective highlights the importance of two orthogonal
research issues  the development of search strategies for solving the shortest path problem 
and the design of admissible heuristic functions for guiding the search  we present several
techniques to address these issues  firstly  an a  search algorithm is developed to learn an
optimal bayesian network by focusing on searching the most promising parts of the solution
space  secondly  two heuristic functions are introduced to guide the search  the tightness
of the heuristic determines the efficiency of the search algorithm  the first heuristic represents a simple relaxation of the acyclicity constraint of bayesian networks such that each
variable chooses optimal parents independently  as a result  the heuristic estimate may
contain many directed cycles and result in a loose bound  the second heuristic  named
k cycle conflict heuristic  is based on the same form of relaxation but tightens the bound
by avoiding directed cycles within some groups of variables  finally  when traversing the
search graph  we need to calculate the cost for each arc being visited  which corresponds
to selecting optimal parents for a variable out of a candidate set  we present two data
structures for storing and querying the costs of all candidate parent sets  one is a set of
full exponential size data structures called parent graphs that are stored as hash tables and
can answer each query in constant time  the other is a sparse representation of the parent
graph which only stores optimal parent sets to improve the space efficiency 
we empirically evaluated the a  algorithm empowered with different combinations of
the heuristic functions and parent graph representations on a set of uci machine learning
datasets  the results show that even with the simple heuristic and full parent graph representation  a  can often achieve better efficiency and or scalability than existing approaches
for learning optimal bayesian networks  the k cycle conflict heuristic and the sparse parent
graph representation further enabled the algorithm to achieve even greater efficiency and
scalability  the results indicate that our proposed methods constitute a promising approach
to learning optimal bayesian network structures 
the remainder of the paper is structured as follows  section   reviews the problem
of learning optimal bayesian networks and reviews related work  section   introduces the
shortest path perspective of the learning problem  the formulation of the search graph is
discussed in detail  section   introduces two data structures that we developed to compute
and store optimal parent sets for all pairs of variables and candidate sets  the data structures are used to query the cost of each arc in the search graph  section   presents the
a  search algorithm  we developed two heuristic functions for guiding the algorithm and
studied their theoretical properties  section   presents empirical results for evaluating our
algorithm against several existing approaches  finally  section   concludes the paper 
  

filearning optimal bayesian networks

   background
we first provide a brief summary of related work on learning bayesian networks 
    learning bayesian network structures
a bayesian network is a directed acyclic graph  dag  g that represents a joint probability
distribution over a set of random variables v    x    x         xn    a directed arc from xi to
xj represents the dependence between the two variables  we say xi is a parent of xj   we
use paj to stand for the parent set of xj   the dependence relation between xj and paj are
quantified using a conditional probability distribution  p  xj  paj    the joint probability
distribution represented by g is factorized as the product
q of all the conditional probability
distributions in the network  i e   p  x         xn     ni   p  xi  pai    in addition to the
compact representation  bayesian networks also provide principled approaches to solving
various inference tasks  including belief updating  most probable explanation  maximum a
posteriori assignment  pearl         and most relevant explanation  yuan  liu  lu    lim 
      yuan  lim    littman      a  yuan  lim    lu      b  
given a dataset d    d         dn    where each data point di is a vector of values over
variables v  learning a bayesian network is the task of finding a network structure that
best fits d  in this work  we assume that each variable is discrete with a finite number of
possible values  and no data point has missing values 
there are roughly three main approaches to the learning problem  score based learning 
constraint based learning  and hybrid methods  score based learning methods evaluate the
quality of bayesian network structures using a scoring function and selects the one that has
the best score  cooper   herskovits        heckerman         these methods basically
formulate the learning problem as a combinatorial optimization problem  they work well
for datasets with not too many variables  but may fail to find optimal solutions for large
datasets  we will discuss this approach in more detail in the next section  as it is the
approach we take  constraint based learning methods typically use statistical testings to
identify conditional independence relations from the data and build a bayesian network
structure that best fits those independence relations  pearl        spirtes  glymour   
scheines        cheng  greiner  kelly  bell    liu        de campos   huete        xie  
geng         constraint based methods mostly rely on results of local statistical testings 
so they can often scale to large datasets  however  they are sensitive to the accuracy of
the statistical testings and may not work well when there are insufficient or noisy data 
in comparison  score based methods work well even for datasets with relatively few data
points  hybrid methods aim to integrate the advantages of the previous two approaches and
use combinations of constraint based and or score based methods for solving the learning
problem  dash   druzdzel        acid   de campos        tsamardinos  brown    aliferis 
      perrier  imoto    miyano         one popular strategy is to use constraint based
learning to create a skeleton graph and then use score based learning to find a high scoring
network structure that is a subgraph of the skeleton  tsamardinos et al         perrier et al  
       in this work  we do not consider bayesian model averaging methods which aim to
estimate the posterior probabilities of structural features such as edges rather than model
selection  heckerman        friedman   koller        dash   cooper        
  

fiyuan   malone

    score based learning
score based learning methods rely on a scoring function score    in evaluating the quality of
a bayesian network structure  a search strategy is used to find a structure g that optimizes
the score  therefore  score based methods have two major elements  scoring functions and
search strategies 
      scoring functions
many scoring functions can be used to measure the quality of a network structure  some
of them are bayesian scoring functions which define a posterior probability distribution
over the network structures conditioning on the data  and the structure with the highest
posterior probability is presumably the best structure  these scoring functions are best
represented by the bayesian dirichlet score  bd   heckerman  geiger    chickering       
and its variations  e g   k   cooper   herskovits         bayesian dirichlet score with
score equivalence  bde   heckerman et al          and bayesian dirichlet score with score
equivalence and uniform priors  bdeu   buntine         other scoring functions often have
the form of trading off the goodness of fit of a structure to the data and the complexity of
the structure  the goodness of fit is measured by the likelihood of the structure given the
data or the amount of information that can be compressed into a structure from the data 
scoring functions belonging to this category include minimum description length  mdl 
 or equivalently bayesian information criterion  bic   rissanen        suzuki        lam
  bacchus         akaike information criterion  aic   akaike        bozdogan        
 factorized  normalized maximum likelihood function  nml fnml   silander  roos  kontkanen    myllymaki         and the mutual information tests score  mit   de campos 
       all of these scoring functions are decomposable  that is  the score of a network can
be decomposed into a sum of node scores  heckerman        
the optimal structure g may not be unique because multiple bayesian network structures may share the same optimal score    two network structures are said to belong to the
same equivalence class  chickering        if they represent the same set of probability distributions with all possible parameterizations  score equivalent scoring functions assign the
same score to structures in the same equivalence class  most of the above scoring functions
are score equivalent 
we mainly use the mdl score in this work  let ri be the number of states of xi   npai
be the number of data points consistent with pai   pai   and nxi  pai be the number of data
points further constrained by xi   xi   mdl is defined as follows  lam   bacchus        

m dl g   

x

m dl xi  pai   

i

   that is why we often use an optimal instead of the optimal throughout this paper 

  

   

filearning optimal bayesian networks

where
log n
k xi  pai   
 
x
nxi  pai
h xi  pai     
nxi  pai log
 
npai
xi  pai
y
k xi  pai      ri    
rl  
xl pai

m dl xi  pai     h xi  pai    

   
   
   

the goal is then to find a bayesian network that has the minimum mdl score  however 
our methods are by no means restricted to mdl  any other decomposable scoring function 
such as bic  bdeu  or fnml  can be used instead without affecting the search strategy 
to demonstrate that  we will test bdeu in the experimental section  one slight difference
between mdl and the other scoring functions is that the latter scores need to be maximized
in order to find an optimal solution  but it is rather straightforward to translate between
maximization and minimization problems by simply changing the sign of the scores  also 
we sometimes use costs to refer to the scores  as they also represent distances between the
nodes in our search graph 
      local search strategies
given n variables  there are o n n n     directed acyclic graphs  dags   the size of
the solution space grows exponentially in the number of variables  it is not surprising that
score based structure learning has been shown to be np hard  chickering         due to the
complexity  early research focused mainly on developing approximation algorithms  heckerman        bouckaert         popular search strategies that were used include greedy hill
climbing  stochastic search  genetic algorithm  etc  
greedy hill climbing methods typically begin with an initial network  e g   an empty
network or a randomly generated structure  and repeatedly apply single edge operations 
including addition  deletion  and reversal  until finding a locally optimal network  extensions to this approach include tabu search with random restarts  glover         limiting
the number of parents or parameters for each variable  friedman et al          searching
in the space of equivalence classes  chickering         searching in the space of variable
orderings  teyssier   koller         and searching under the constraints extracted from
data  tsamardinos et al          the optimal reinsertion algorithm  or   moore   wong 
      adds a different operator  a variable is removed from the network  its optimal parents
are selected  and the variable is then reinserted into the network with those parents  the
parents are selected to ensure the new network is still a valid bayesian network 
stochastic search methods such as markov chain monte carlo and simulated annealing
have also been applied to find a high scoring structure  heckerman        de campos  
puerta        myers  laskey    levitt         these methods explore the solution space
using non deterministic transitions between neighboring network structures while favoring
better solutions  the stochastic moves are used in hope to escape local optima and find
better solutions 
other optimization methods such as genetic algorithms  hsu  guo  perry    stilson 
      larranaga  kuijpers  murga    yurramendi        and ant colony optimization meth  

fiyuan   malone

ods  de campos  fernndez luna  gmez    puerta        daly   shen        have been
applied to learning bayesian network structures as well  unlike the previous methods which
work with one solution at a time  these population based methods maintain a set of candidate solutions throughout their search  at each step  they create the next generation
of solutions randomly by reassembling the current solutions as in genetic algorithms  or
generating the new solutions based on information collected from incumbent solutions as in
ant colony optimization  the hope is to obtain increasingly better populations of solutions
and eventually find a good network structure 
these local search methods are quite robust in the face of large learning problems with
many variables  however  they do not guarantee to find an optimal solution  what is worse 
the quality of their solutions is typically unknown 
      optimal search strategies
recently multiple exact algorithms have been developed for learning optimal bayesian networks  several dynamic programming algorithms are proposed based on the observation
that a bayesian network has at least one leaf  ott et al         singh   moore         a
leaf is a variable with no child variables in a bayesian network  in order to find an optimal
bayesian network for a set of variables v  it is sufficient to find the best leaf  for any leaf
choice x  the best possible bayesian network is constructed by letting x choose an optimal
parent set pax from v  x  and letting v  x  form an optimal subnetwork  then the
best leaf choice is the one that minimizes the sum of score x  pax   and score v  x  
for a scoring function score     more formally  we have 
score v    min  score v    x     bestscore x  v    x    
xv

   

where
bestscore x  v    x    

min
score x  pax   
pax v  x 

   

given the above recurrence relation  a dynamic programming algorithm works as follows  it first finds optimal structures for single variables  which is trivial  starting with
these base cases  the algorithm builds optimal subnetworks for increasingly larger variable
sets until an optimal network is found for v  the dynamic programming algorithms can
find an optimal bayesian network in o n n   time and space  koivisto   sood        ott
et al         silander   myllymaki        singh   moore         recent algorithms have
improved the memory complexity by either trading longer running times for reduced memory consumption  parviainen   koivisto        or taking advantage of the layered structure
present within the dynamic programming lattice  malone  yuan    hansen      b  malone 
yuan  hansen    bridges      a  
a branch and bound algorithm  bb  was proposed by de campos and ji        for
learning bayesian networks  the algorithm first creates a cyclic graph by allowing each
variable to obtain optimal parents from all the other variables  a best first search strategy
is then used to break the cycles by removing one edge at a time  the algorithm uses an
approximation algorithm to estimate an initial upper bound solution for pruning  the
algorithm also occasionally expands the worst nodes in the search frontier in hope to find
  

filearning optimal bayesian networks

figure    an order graph of four variables 
better networks to update the upper bound  at completion  the algorithm finds an optimal
network structure that is a subgraph of the initial cyclic graph  if the algorithm ran out of
memory before finding the solution  it will switch to using a depth first search strategy to
find a suboptimal solution 
integer linear programming  ilp  has also been used to learn optimal bayesian network
structures  cussens        jaakkola et al          the learning problem is cast as an integer
linear program over a polytope with an exponential number of facets  an outer bound
approximation to the polytope is then solved  if the solution of the relaxed problem is
integral  it is guaranteed to be the optimal structure  otherwise  cutting planes and branch
and bound algorithms are subsequently applied to find the optimal structure  recently a
similar method has been proposed to find an optimal structure by searching in the space of
equivalence classes  hemmecke et al         
several other methods can be considered optimal under the constraints that they enforce
on the network structure  for example  if optimal parents are selected for each variable  k 
finds an optimal network structure for a particular variable ordering  cooper   herskovits 
       the methods developed in  ordyniak   szeider        kojima  perrier  imoto   
miyano        find an optimal network structure that must be a subgraph of a given super
graph 

   a shortest path perspective
this section introduces a shortest path perspective of the problem of learning a bayesian
network structure for a given dataset 
    order graph
the state space graph for learning bayesian networks is basically a hasse diagram containing
all of the subsets of the variables in a domain  figure   visualizes the state space graph
for a learning problem with four variables  the top most node with the empty set at layer
  

fiyuan   malone

  is the start search node  and the bottom most node with the complete set at layer n is
the goal node  where n is the number of variables in a domain  an arc from u to u   x 
represents generating a successor node by adding a new variable  x  to an existing set of
variables u  u is called a predecessor of u  x   the cost of the arc is equal to the score of
selecting an optimal parent set for x out of u  i e   bestscore x  u   for example  the arc
 x    x      x    x    x    has a cost equal to bestscore x     x    x      each node at layer
i has ni successors as there are this many ways to add a new variable  and i predecessors as
there are this many leaf choices  we define expanding a node u as generating all successors
nodes of u 
with the search graph thus defined  a path from the start node to the goal node is defined
as a sequence of nodes such that there is an arc from each of the nodes to the next node
in the sequence  each path also corresponds to an ordering of the variables in the order of
their appearance  for example  the path traversing nodes    x      x    x      x    x    x    
 x    x    x    x    stands for the variable ordering x    x    x    x    that is why we also call
the search graph an order graph  the cost of a path is defined as the sum of the costs of
all the arcs on the path  the shortest path is then the path with the minimum total cost in
the order graph 
given the shortest path  we can reconstruct a bayesian network structure by noting
that each arc on the path encodes the choice of optimal parents for one of the variables
out of the preceding variables  and the complete path represents an ordering of all the
variables  therefore  putting together all the optimal parent choices generates a valid
bayesian network  by construction  the bayesian network structure is optimal 
    finding the shortest path
various methods can be applied to solve the shortest path problem  dynamic programming
is considered to evaluate the order graph using a top down sweep of the order graph  silander
  myllymaki        malone et al       b   layer by layer  dynamic programming finds an
optimal subnetwork for the variables contained in each node of the order graph based on
results from the previous layers  for example  there are three ways to construct a bayesian
network for node  x    x    x     using  x    x    as the subnetwork and x  as the leaf  using
 x    x    as the subnetwork and x  as the leaf  or using  x    x    as the subnetwork and x 
as the leaf  the top down sweep makes sure that optimal subnetworks are already found
for  x    x      x    x     and  x    x     we only need to select optimal parents for the
leaves and identify the leaf that produces the optimal network for  x    x    x     once the
evaluation reaches the node in the last layer  a shortest path and  equivalently  an optimal
bayesian network are found for the global variable set 
a drawback of the dynamic programming approach is its need to compute all the
bestscore    of all candidate parent sets for each variable  for n variables  there are
 n nodes in the order graph  and there are also  n  parent scores to be computed for each
variable  totally n n  scores  as the number of variables increases  computing and storing
the order and parent graphs quickly becomes infeasible 
in this paper  we propose to apply the a  algorithm  hart  nilsson    raphael       
to solve the shortest path problem  a  uses the heuristic function to evaluate the quality of
search nodes and only expand the most promising search node at each search step  because
  

filearning optimal bayesian networks

of the guidance of the heuristic functions  a  only needs to explore part of the search
graph in finding the optimal solution  however  in comparison to dynamic programming 
a  has the overhead of calculating heuristic values and maintaining a priority queue  the
actual relative performance between dynamic programming and a  thus depends on the
efficiency in calculating the heuristic values and the tightness of these values  felzenszwalb
  mcallester        klein   manning        

   finding optimal parent sets
before introducing our algorithm for solving the shortest path problem  we first discuss how
to obtain the cost bestscore x  u  for each arc u  u   x  that we will visit in the
order graph  recall that each arc involves selecting optimal parents for a variable from a
candidate set  we need to consider all subsets of the candidate set in finding the subset with
the best score  in this section  we introduce two data structures and related methods for
computing and storing optimal parent sets and scores for all pairs of variable and candidate
parent set 
all exact algorithms for learning bayesian network structures need to calculate the
optimal parent sets and scores  we present a reasonable approach to the calculation in this
paper  note  however  our approach is applicable to other algorithms  and vice versa 
    parent graph
we use a data structure called parent graph to compute costs for the arcs of the order graph 
each variable has its own parent graph  the parent graph for variable x is a hasse diagram
consisting of all subsets of the variables in v    x   each node u stores the optimal parent
set pax out of u which minimizes score x  pa x   as well as bestscore x  u  itself  for
example  figure   b  shows a sample parent graph for x  that contains the best scores of
all subsets of  x    x    x     to obtain figure   b   however  we first need to calculate the
preliminary graph in figure   a  that contains the raw score of each subset u as the parent
set of x    i e   score x    u   as equation   shows  these scores can be calculated based on
the counts for particular instantiations of the parent and child variables 
we use an ad tree  moore   lee        to collect all the counts from a dataset and
compute the scores  an ad tree is an unbalanced tree structure that contains two types of
nodes  ad tree nodes and varying nodes  an ad tree node stores the number of data points
consistent with a particular variable instantiation  a varying node is used to instantiate the
state of a variable  a full ad tree stores counts of data points that are consistent with
all partial instantiations of the variables  a sample ad tree for two variables are shown in
figure    for n variables with d states each  the number of ad tree nodes in an ad tree is
 d   n   it grows even faster than the size of an order or parent graph  moore and lee       
also described a sparse ad tree which significantly reduces the space complexity  readers
are referred to that paper for more details  our pseudo code assumes a sparse ad tree is
used 
given an ad tree  we are ready to calculate the raw scores score x       for figure   a  
there is an exponential number of scores in each parent graph  however  not all parent
sets can possibly be in the optimal bayesian network  certain parent sets can be discarded
without ever calculating their values according to the following theorems by tian        
  

fiyuan   malone

figure    a sample parent graph for variable x     a  the raw scores score x       for all
the parent sets  the first line in each node gives the parent set  and the second
line gives the score of using all of that set as the parents for x     b  the optimal
scores bestscore x       for each candidate parent set  the second line in each
node gives the optimal score using some subset of the variables in the first line as
parents for x     c  the optimal parent sets and their scores  the pruned parent
sets are shown in gray  a parent set is pruned if any of its predecessors has a
better score 

x     
x     
c     
vary
v
x 

vary
v
x 

x     
x     

x     
x     

x     
x     

x     
x     

c     

c     

c     

c     

vary
x 

vary
x 

x     
x     

x     
x     

x     
x     

x     
x     

c     

c  

c     

c     

figure    an ad tree 
we use these theorems to compute only the necessary mdl scores  other scoring functions
such as bdeu also have similar pruning rules  de campos   ji         algorithm   provides
the pseudo code for calculating the raw scores 
theorem   in an optimal bayesian network based on the mdl scoring function  each
 n
variable has at most log  log
n   parents  where n is the number of data points 
  

filearning optimal bayesian networks

algorithm   score calculation algorithm
input  ad  sparse ad tree of input data  v  input variables 
output  score x  u  for each pair of x  v and u  v    x 
   function calculatemdlscores ad  v 
  
for each xi  v do
  
calculatescores xi   ad 
  
end for
   end function
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

function calculatescores xi  ad 
 n
for k    to log  log
 prune due to theorem  
n   do
for each u such that u  v    x    u     k do
 all parent sets of size k
prune  f alse
for each y  u do
if k xi  u    score xi   u    y        then
prune  true
 prune due to theorem  
break
end if
end for
if prune     true then
score xi   u   log  n k xi  u 
 complexity term
for each instantiation xi   u of xi   u do
 log likelihood term
cf amily  getcount  xi    u ad 
cp arents  getcount u  ad 
score xi   u  score xi   u    cf amily  log cf amily
score xi   u  score xi   u    cf amily  log cp arents
end for
end if
end for
end for
end function

theorem   let u and s be two candidate parent sets for x  u  s  and k xi  s  
m dl xi  u       then s and all supersets of s cannot possibly be optimal parent sets for
x 
after computing the raw scores  we compute the parent graph according to the following
theorem which has appeared in many earlier papers  e g   see the work of teyssier and
koller         and de campos and ji         the theorem simply means that a parent set
is not optimal when a subset has a better score 
theorem   let u and s be two candidate parent sets for x such that u  s  and
score x  u   score x  s   then s is not the optimal parent set of x for any candidate set 
  

fiyuan   malone

algorithm   computing parent graphs
input  all necessary score x  u   x  v u  v    x 
output  full parent graphs containing bestscore x  u 
   function calculatefullparentgraphs v  score       
  
for each x  v do
  
for layer    to n do
 propagate best scores down the graph
  
for each u such that u  v    x    u     layer do
  
calculatebestscore x  u  score       
  
end for
  
end for
  
end for
   end function
   
   
   
   
   
   
   
   

function calculatebestscore x  u  score       
bestscore x  u   score x  u 
for each y  u do
if bestscore x  u    y      bestscore x  u  then
bestscore x  u   bestscore x  u    y   
end if
end for
end function

function getbestscore x  u 
   
return bestscore x  u 
    end function

 propagate best scores

 query bestscore x  u 

   

therefore  when we generate a successor node u y   of u in the parent graph of x  we
check whether score x  u   y    is smaller than bestscore x  u   if so  we let the parent
graph node u y   record itself as the optimal parent set  otherwise if bestscore x  u  is
smaller  we propagate the optimal parent set in u to u y    because of such propagation 
we must have the following  teyssier   koller        
theorem   let u and s be two candidate parent sets for x such that u  s  we must
have bestscore x  s   bestscore x  u  
a pseudo code for propagating the scores and computing the parent graph is outlined in
algorithm    figure   b  shows the parent graph with the optimal scores after propagating
the best scores from top to bottom 
during the search of the order graph  whenever we visit a new arc u  u   x   we
find its score by looking up the parent graph of variable x  for example  if we need to find
optimal parents for x  out of  x    x     we look up the node  x    x    in x  s parent graph
to find the optimal parent set and its score  to make the look ups efficient  we use hash
tables to organize the parent graphs so that the query can be answered in constant time 
  

filearning optimal bayesian networks

parentsx 
scoresx 

 x    x   
 

 x   
 

 x   
 

  
  

table    sorted scores and parent sets for x  after pruning parent sets which are not
possibly optimal 
parentsx 
 
parentsx
x 
x 
parentsx 
 
parentsx
x 

 x    x   
 
 
 

 x   
 
 
 

 x   
 
 
 

  
 
 
 

table    the parentsx  xi   bit vectors for x    a   in line xi indicates that the corresponding parent set includes variable xi   while a   indicates otherwise  note
that  after pruning  none of the optimal parent sets include x   

    sparse parent graphs
the full parent graph for each variable x exhaustively enumerates all subsets of v    x 
and stores bestscore x  u  for all of those subsets  naively  this approach requires storing
n n  scores and parent sets  silander   myllymaki         because of theorem    however 
the number of optimal parent sets is often far smaller than the full size  figure   b  shows
that an optimal parent set may be shared by several candidate parent sets  the full parent
graph representation will allocate space for this repetitive information for all candidate sets 
resulting in waste of time and space 
to address these limitations  we introduce a sparse representation of the parent graphs
and related scanning techniques for querying optimal parent sets  as with the full parent
graphs  we begin by calculating and pruning scores as described in the last section  due
to theorems   and    some of the parent sets can be pruned without being evaluated 
therefore  we do not have to create the full parent graphs  also  instead of creating the
hasse diagrams  we sort all the optimal parent scores for each variable x in a list  and also
maintain a parallel list that stores the associated optimal parent sets  we call these sorted
lists scoresx and parentsx   table   shows the sorted lists for the optimal scores in the
parent graph in figure   b   in essence  this allows us to store and efficiently process only
the scores in figure   c  
to find the optimal parent set for x out of a candidate set u  we can simply scan the
list of x starting from the beginning  as soon as we find the first parent set that is a subset
of u  we find the optimal parent score bestscore x  u   this is trivially true due to the
following theorem 
theorem   the first subset of u in parentsx is the optimal parent set for x out of u 
scanning the lists to find optimal parent sets can be inefficient if not done properly 
since we have to do the scanning for each arc visited in the order graph  any inefficiency in
the scanning can have a large impact on the search algorithm 
  

fiyuan   malone

parentsx 
validx 
 
 parentsx
x 
new
validx 

 x    x   
 
 
 

 x   
 
 
 

 x   
 
 
 

  
 
 
 

table    the result of performing the bitwise operation to exclude all parent sets which
include x    a   in the validx  bit vector means that the parent set does not
include x  and can be used for selecting the optimal parents  the first set bit
indicates the best possible score and parent set 

parentsx 
validx 
 
 parentsx
x 
new
validx 

 x    x   
 
 
 

 x   
 
 
 

 x   
 
 
 

  
 
 
 

table    the result of performing the bitwise operation to exclude all parent sets which
include either x  or x    a   in the validnew
x  bit vector means that the parent
set includes neither x  nor x    the initial validx  bit vector had already excluded
x    so finding validnew
x  only required excluding x   

to ensure the efficiency  we propose the following scanning technique  for each variable
x  we first initialize a working bit vector of length kscoresx k called validx to be all  s  this
indicates that all the parent scores in scoresx are usable  then  we create n    bit vectors
also of length kscoresx k  one for each variable in v    x   the bit vector for variable y is
denoted as parentsyx and contains  s for all the parent sets that contain y and  s for others 
table   shows the bit vectors for the example in table    then  to exclude variable y as a
candidate parent  we perform the bit operation validnew
 validx    parentsyx   the new
x
validx bit vector now contains  s for all the parent sets that are subsets of v    y    the
first set bit corresponds to bestscore x  v    y     table   shows an example of excluding
x  from the set of possible parents for x    and the first set bit in the new bit vector
corresponds to bestscore x    v    x      if we further want to exclude x  as a candidate
parent  the new bit vector from the last step becomes the current bit vector for this step 
 
and the same bit operation is applied  validnew
 validx    parentsx
x
x    the first set
bit of the result corresponds to bestscore x    v    x    x      table   demonstrates this
operation  also  it is important to note that we exclude one variable at a time  for example 
if  after excluding x    we wanted to exclude x  rather than x    we could take validnew

x
 
validx    parentsx
 
these
operations
are
described
in
the
createsparseparentgraph
and
x
getbestscore functions in algorithm   
because of the pruning of duplicate scores  the sparse representation requires much less
memory than storing all the possible parent sets and scores  as long as kscores x k  
c n     n     it also requires less memory than the memory efficient dynamic programming
algorithm  malone et al       b  
experimentally  we show that kscoresx k is almost
  

filearning optimal bayesian networks

algorithm   sparse parent graph algorithms
input  all necessary score x  u   x  v u  v    x 
output  sparse parent graphs containing optimal parent sets and scores
   function createsparseparentgraph x  score       
  
for x  v do
  
scorest   parentst sort score x    
 sort scores  preferring low cardinality
  
scoresx   parentsx  
 initialize possibly optimal scores
  
for i       scorest   do
  
prune  f alse
  
for j       scoresx   do
 check if a better subset pattern exists
  
if contains parentst i   parentsx  j   scoresx  i   scorest  i  then
  
prune  true
   
break
   
end if
   
end for
   
if prune     true then
   
append scoresx   parentsx with parentst i   parentst  i 
   
end if
   
end for
   
for i       scoresx   do
 set bit vectors for efficient querying
   
for each y  parentsx  i  do
   
set parentsyx  i  
   
end for
   
end for
   
end for
    end function
   
   
   
   
   
   
   
   

function getbestscore x  u 
valid  allscoresx
for each y  v   u do
valid  valid   parentsyx
end for
f sb  f irstsetbit valid 
return scoresx  f sb 
end function

 query bestscore x  u 

 return the first score with a set bit

always smaller than c n     n    by several orders of magnitude  so this approach offers
 usually substantial  memory savings compared to previous best approaches 
the sparse representation has an extra benefit of improving the time efficiency as well 
with the full representation  we have to create the complete exponential size parent graphs 
even though many nodes in a parent graph share the same optimal parent choices  with the
sparse representation  we can avoid creating those nodes  which makes creating the sparse
parent graphs much more efficient 
  

fiyuan   malone

   an a  search algorithm
we are now ready to tackle the shortest path problem in the order graph  this section
presents our search algorithm as well as two admissible heuristic functions for guiding the
algorithm 
    the algorithm
we apply a well known state space search method  the a  algorithm  hart et al          to
solve the shortest path problem in the order graph  the main idea of the algorithm is to
use an evaluation function f to measure the quality of search nodes and always expand the
one that has the lowest f cost during the exploration of the order graph  for a node u 
f  u  is decomposed as the sum of an exact past cost  g u   and the estimated future cost 
h u   the g u  cost measures the shortest distance from the start node to u  while the
h u  cost estimates how far away u is from the goal node  therefore  the f cost provides
an estimated total cost of the best possible path which passes through u 
a  uses an open list  usually as a priority queue  to store the search frontier  and a
closed list to store the expanded nodes  initially the open list only contains the start node 
and the closed list is empty  at each search step  the node with the lowest f  cost from the
open list  say u  is selected for expansion to generate its successor nodes  before expanding
u  however  we need to first check whether it is the goal node  if yes  a shortest path to
the goal has been found  we can construct a bayesian network from the path and terminate
the search 
if u is not the goal  we expand it to generate the successor nodes  each successor
s considers one possible way of adding a new variable  say x  as a leaf to an existing
subnetwork over the variables in u  that is s   u   x   the g cost of s is calculated
as the sum of the g cost of u and the cost of the arc u  s  the arc cost as well as the
optimal parent set pax for x out of u are retrieved from xs parent graph  the h cost of
s is computed from a heuristic function which we will describe shortly  we record in s the
following information    g cost  h cost  x  and pax  
it is clear from the order graph that there are multiple paths to any node  we should
perform duplicate detection for s to see whether a node representing the same set of variables
has already been generated before  if we do not check for duplicates  the search space blows
up from an order graph with a size  n to an order tree with a size n   we first check whether
a duplicate already exists in the closed list  if so  we further check whether the duplicate
has a better g cost than s  if yes  we discard s immediately  as it represents a worse path 
otherwise  we remove the duplicate from the closed list  and place s in the open list  what
happens is we have found a better path with a lower g cost  so we reopen the node for future
search 
if no duplicate is found in the closed list  we also need to check the open list  if no
duplicate is found  we will simply add s to the open list  otherwise  we will compare the
g costs of the duplicate and s  if the duplicate has a lower g cost  s will be discarded 
otherwise  we will replace the duplicate with s  again  the lower g cost means a better
path is found 
   we can also delay the calculation of h until after duplicate detection to avoid unnecessary calculations
for nodes that will be pruned 

  

filearning optimal bayesian networks

algorithm   a  search algorithm
input  full or sparse parent graphs containing bestscore x  u 
output  an optimal bayesian network g
   function main d 
  
start  
  
score start     p
  
push open  start  y v bestscore y  v    y   
  
while  isempty open  do
  
u pop open 
  
if u is goal then
 a shortest path is found
  
print the best score is    score v  
  
g  construct a network from the shortest path
   
return g
   
end if
   
put closed  u 
   
for each x  v   u do
 generate successors
   
g  bestscore x  u    score u 
   
if contains closed  u   x   then
 closed list dd
   
if g   score u   x   then
 reopen node
   
delete closed  u   x  
   
push  open  u   x   g   h 
   
score u   x    g
   
end if
   
else
   
if contains open  u   x     g   score u   x   then open list dd
   
update open  u   x   g   h 
   
score u   x    g
   
end if
   
end if
   
end for
   
end while
    end function

after all the successor nodes have been generated  we will place node u in the closed
list  which indicates that node is already expanded  expanding the top node in the open
list is called one search step  the a  algorithm performs the step repeatedly until the goal
node is selected for expansion  at that moment a shortest path from the start state to the
goal state has been found 
once the shortest path is found  we can reconstruct the optimal bayesian network
structure by starting from the goal node and tracing back the shortest path until reaching
the start node  since each node on the path stores a leaf variable and its optimal parent set 
putting all the optimal parent sets together generates a valid bayesian network structure 
a pseudo code of the a  algorithm is shown in algorithm   
  

fiyuan   malone

    a simple heuristic function
the a  algorithm provides different theoretical guarantees depending on the properties of
the heuristic function h  the function h is admissible if the h cost is never greater than
the true cost to the goal  in other words  it is optimistic  given an admissible heuristic
function  the a  algorithm is guaranteed to find the shortest path once the goal node is
selected for expansion  pearl         let u be a node in the order graph  we first consider
the following simple heuristic function h 
definition  
h u   

x

bestscore x  v  x   

   

xv u

the heuristic function allows each remaining variable to choose optimal parents from all
the other variables  its design reflects the principle that the exact cost of a relaxed problem
can be used as an admissible bound for the original problem  pearl         in this case  the
original problem is to learn a bayesian network that is a directed acyclic graph  equation  
relaxes the problem by ignoring the acyclicity constraint  so all directed cyclic graphs are
allowed  the heuristic function is easily proven admissible in the following theorem  the
proofs of all the theorems in this paper can be found in appendix a 
theorem   h is admissible 
it turns out that h has an even nicer property  a heuristic function is consistent if  for
any node u and a successor s  h u   h s    c u  s   where c u  s  stands for the cost
of the arc u  s  given a consistent heuristic  the f cost is monotonically non decreasing
following any path in the order graph  as a result  the f cost of any node is less than or
equal to the f cost of the goal node  it follows immediately that a consistent heuristic is
guaranteed to be admissible  with a consistent heuristic  the a  algorithm is guaranteed
to find the shortest path to any node u once u is selected for expansion  if a duplicate is
found in the closed list  the duplicate must have the optimal g cost  so the new node can be
discarded immediately  we show in the following that the simple heuristic in equation   is
also consistent 
theorem   h is consistent 
the heuristic may seem expensive to compute as it requires computing bestscore x  v 
 x   for each variable x  however  these scores can be easily found by querying the parent
graphs and are stored in an array for repeated use  it takes linear time to calculate the
heuristic for the start node  any subsequent computation of h  however  only takes constant
time because we can simply subtract the best score of the newly added variable from the
heuristic value of the parent node 
    an improved admissible heuristic
the simple heuristic function defined in equation    referred to as hsimple hereafter  relaxes
the acyclicity constraint of bayesian networks completely  as a result  hsimple may introduce
many directed cycles and result in a loose bound  we introduce another heuristic in this
section to tighten the heuristic  we first use a toy example to motivate the new heuristic 
and then describe two specific approaches to computing the heuristic 
  

filearning optimal bayesian networks

x 

x 

x 

x 

figure    a directed graph representing the heuristic estimate for the start search node 

      a motivating example
with hsimple   the heuristic estimate of the start node in an order graph allows each variable
to choose optimal parents from all the other variables  suppose the optimal parent sets for
x    x    x    x  are  x    x    x      x    x      x      x    x    respectively  these parent
choices are shown as the directed graph in figure    since the acyclicity constraint is
ignored  directed cycles are introduced  e g   between x  and x    however  we know the
final solution cannot have cycles  three cases are possible between x  and x        x  is a
parent of x   so x  cannot be a parent of x         x  is a parent of x    or     neither of
the above is true  based on theorem    the third case cannot provide a better value than
the first two cases because one of the variables must have fewer candidate parents 
between     and      it is unclear which one is better  so we take the minimum of them
to get a lower bound  consider case      we have to delete the arc x   x  to rule out
x  as a parent of x    then we have to let x  rechoose optimal parents from the remaining
variables  x    x     that is  we must check all parent sets not including x    the deletion
of the arc alone cannot produce the new bound because the best parent set for x  out of
 x    x    is not necessarily  x     the total bound of x  and x  is computed by summing
together the original bound of x  and the new bound of x    we call this total bound
b    case     is handled similarly  we call that total bound b    because the joint cost for
x  and x    c x    x     must be optimistic  we compute it as the minimum of b  and b   
effectively we have considered all possible ways to break the cycle and obtained a tighter
heuristic value  the new heuristic is clearly admissible  as we still allow cycles among other
variables 
often  hsimple introduces multiple cycles into a heuristic estimate  figure   also has a
cycle between x  and x    this cycle shares x  with the earlier cycle between x  and x   
we say the cycles overlap  one way to break both cycles is to set the parent set of x  to be
 x     however  it introduces a new cycle between x  and x    as described in more detail
shortly  we partition the variables into exclusive groups and only break cycles within each
group  in this example  if x  and x  are in different groups  we do not break the cycle 
  

fiyuan   malone

      the k cycle conflict heuristic
the above idea can be generalized to compute the joint cost for any variable group with
size up to k by avoiding cycles within the group  then for any node u in the order graph 
we calculate its heuristic value by partitioning the variables v   u into several exclusive
groups and sum their costs together  we name the resulting technique the k cycle conflict
heuristic  note that the simple heuristic hsimple is a special case of this new heuristic  as it
simply contains costs for the individual variables  k    
the new heuristic is an application of the additive pattern database technique  felner 
korf    hanan         pattern databases  culberson   schaeffer        is an approach to
computing an admissible heuristic for a problem by solving a relaxed problem  consider
the    puzzle problem     square tiles numbered from   to    are randomly placed in a  
by   box with one position left empty  each such configuration of the tiles is called a state 
the goal is to slide the tiles one at a time into a destination configuration  a tile can slide
into the empty position only if it is beside that position  the    puzzle can be relaxed to
only contain the tiles     with the other tiles removed  because of the relaxation  multiple
states of the original problem map to one state in the abstract state space of the relaxed
problem as they share the positions of the remaining tiles  each abstract state is called
a pattern  the cost of the pattern is equal to the smallest cost for sliding the remaining
tiles into their destination positions  the cost provides a lower bound for any state in the
original state space which maps to that pattern  the costs of all patterns are stored in a
pattern database 
we can relax a problem in different ways and obtain multiple pattern databases  if
the solutions to several relaxed problems are independent  the problems are said to be
exclusive  for the    puzzle  we can also relax it to only contain tiles       this relaxation
can be solved independently from the previous one because they do not share any puzzle
movements  for any concrete state in the original state space  the positions of tiles    
map it to a pattern in the first pattern database  and the positions of tiles      map it to a
different pattern in the second pattern database  the costs of these patterns can be added
together to obtain an admissible heuristic  hence the name additive pattern databases 
for our learning problem  a pattern is defined as a group of variables  and its cost is
the optimal joint cost of these variables while avoiding directed cycles between them  the
decomposability of the scoring function implies that the costs of two exclusive patterns can
be added together to obtain an admissible heuristic 
we do not have to explicitly break cycles in computing the cost of a pattern  the
following theorem offers a straightforward approach to doing so 
theorem   the cost of the pattern u  c u   is equal to the shortest distance from v   u
to the goal node in the order graph 
again consider the example in figure    the cost of pattern  x    x    is equal to the
shortest distance between  x    x    and the goal in the order graph in figure   
furthermore  the difference between c u  and the sum of the simple heuristic values of
all variables in u indicates the amount of improvement brought by avoiding cycles within
the pattern  the differential score  called h   can thus be used as a quality measure for
ordering the patterns and for choosing patterns that are more likely to result in a tighter
heuristic 
  

filearning optimal bayesian networks

      dynamic k cycle conflict heuristic
there are two slightly different versions of the k cycle conflict heuristic  in the first version
named dynamic k cycle conflict heuristic  we compute the costs for all groups of variables
with size up to k and store them in a single pattern database  according to theorem   
this heuristic can be computed by finding the shortest distances between all the nodes in
the last k layers of the order graph and the goal 
we compute the heuristic by using a breadth first search to do a backward search in
the order graph for k layers  the search starts from the goal node and expands the order
graph backward layer by layer  a reverse arc u   x   u has the same cost as the arc
u  u   x   i e   bestscore x  u   the reverse g cost of u is updated whenever a new
path with a lower cost is found  breadth first search ensures that node u will obtain its
exact reverse g cost once the previous layer is expanded  the g cost is the cost of the pattern
v   u  we also compute the differential score  h   for each pattern at the same time  a
pattern which does not have a better differential score than any of its subset patterns will be
discarded  the pruning can significantly reduce the size of a pattern database and improve
its query efficiency  the algorithm for computing the dynamic k cycle conflict heuristic is
shown in algorithm   
once the heuristic is created  we can calculate the heuristic value for each search node
as follows  for node u  we partition the remaining variables v   u into a set of exclusive
patterns  and sum their costs together as the heuristic value  since we only prune superset
patterns  we can always find such a partition  however  there are potentially many ways of
partition  ideally we want to find the one with the highest total cost  which represents the
tightest heuristic value  the problem of finding the optimal partition can be formulated
as maximum weighted matching problem  felner et al          for k      we can define an
undirected graph in which each vertex represents a variable  and each edge between two
variables represents the pattern containing the same variables and has a weight equal to
the cost of the pattern  the goal is to select a set of edges from the graph so that no two
edges share a vertex and the total weight of the edges is maximized  the matching problem
can be solved in o n    time  where n is the number of vertices  papadimitriou   steiglitz 
      
for k      we have to add hyperedges to the matching graph for connecting up to
k vertices to represent larger patterns  the goal becomes to select a set of edges and
hyperedges to maximize the total weight  however  the three dimensional or higher order
maximum weighted matching problem is np hard  garey   johnson         that means
we have to solve an np hard problem when calculating each heuristic value 
to alleviate the potential inefficiency  we greedily select patterns based on their quality 
consider node u with unsearched variables v   u  we choose the pattern with the highest
differential cost from all the patterns that are subsets of v   u  we repeat this step for the
remaining variables until all the variables are covered  the total cost of the chosen patterns
is used as the heuristic value for u  the hdynamic function of algorithm   gives pseudocode
for computing the heuristic value 
the dynamic k cycle conflict heuristic introduced above is an example of the dynamically partitioned pattern database  felner et al         because the patterns are dynamically
selected during the search algorithm  we refer to it as dynamic pattern database for short 
  

fiyuan   malone

algorithm   dynamic k cycle conflict heuristic
input  full or sparse parent graphs containing all bestscore x  u 
output  a pattern database p d with patterns up to size k
   function createdynamicpd k 
  
p d   v    
  
h  v    
  
for l      k do
 perform bfs for k levels
  
for each u  p dl  do
  
expand u  l 
  
checksave u 
  
p d v   u   p dl   u 
  
end for
   
end for
   
for each x  p d   save do
 remove superset patterns with no improvement
   
delete p d x 
   
end for
   
sort p d   h  
 sort patterns in decreasing costs
    end function
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

function expand u  l 
for each x  u do
g  p dl   u    bestscore x  u    x  
if g   p dl  u    x   then p dl  u    x    g
end for
end function

 duplicate detection

function checksave u 
p
h  u   g  y v u bestscore y  v    y   
for each x  v   u do
 check improvement over subset patterns
if h  u    h  u   x   then save u 
end for
end function
function hdynamic  u 
h 
ru
for each s  p d do
if s  r then
rr s
h  h   p d s 
end if
end for
return h
end function

 calculate heuristic value for u

 greedily find best subset pattern of r

  

filearning optimal bayesian networks

a potential drawback of dynamic pattern databases is that  even with the greedy
method  computing a heuristic value is still much more expensive than the simple heuristic
in equation    consequently  the search time can be longer even though the tighter pattern
database heuristic results in more pruning and fewer expanded nodes 
      static k cycle conflict heuristic
to address the inefficiency of dynamic pattern database in computing heuristic values  we
introduce another version named static k cycle conflict heuristic based on the statically
partitioned pattern database technique  felner et al          the idea is to partition the
variables into several static exclusive groups  and create a separate pattern database for
each group  consider a problem with variables  x         x     we divide the variables into
two groups   x         x    and  x         x     for each group  say  x         x     we create a
pattern database that contains the costs of all subsets of  x         x    and store them as a
hash table  we refer to this heuristic as the static pattern database for short s
we create static pattern databases as follows  for a static grouping v   i vi   we need
to compute a pattern database for each group vi that resembles an order graph containing
all subsets of vi   we use a breadth first search to create the graph starting s
from the node
vi   the cost for an arc u x   u in this graph is equal to bestscore x    j  i vj  u  
which means that the variables in the other groups are valid candidate parents  to ensure
efficient retrieval  these static pattern databases are stored as hashtables  nothing is pruned
from them  algorithm   gives pseudocode for creating static pattern databases 
it is much simpler to use static pattern databases to compute a heuristic value  consider
the search node  x    x    x     the unsearched variables are  x    x    x    x    x     we simply
divide these variables into two patterns  x    x    and  x    x    x    according to the static
grouping  look them up in the respective pattern databases  and sum the costs together
as the heuristic value  moreover  since each search step just processes one variable  only
one pattern is affected and requires a new score lookup  therefore  the heuristic value can
be calculated incrementally  the hstatic function of algorithm   provides pseudocode for
naively calculating this heuristic value 
      properties of the k cycle conflict heuristic
both versions of the k cycle conflict heuristic remain admissible  although they can avoid
cycles within each pattern  they cannot prevent cycles across different patterns  the following theorem proves the result 
theorem   the k cycle conflict heuristic is admissible 
understanding the consistency of the new heuristic is slightly more complex  we first
look at the static pattern database as it does not involve selecting patterns dynamically 
the following theorem shows that the static pattern database is still consistent 
theorem    the static pattern database version of the k cycle conflict heuristic remains
consistent 
in the dynamic pattern database  each search step needs to solve a maximum weighted
matching problem and select a set of patterns to compute the heuristic value  in the
  

fiyuan   malone

algorithm   static k cycle conflict heuristics
s
input  full or sparse parent graphs containing bestscore x  u   i vi  a partition of v
output  a full pattern database p d i for each vi
   function createstaticpd vi  
  
p d i       fi fi
  
for l      fivi fi do
 perform bfs over vi
i
  
for each u  p dl  do
  
expand u  l  vi  
i  u 
  
p d i  u   p dl 
  
end for
  
end for
   end function
   
   
   
   
   
   
   
   
   
   
   
   
   

function expand u  l  vi  
for each x  vi   u do
s
i  u    bestscore x  u
g  p dl 
j  i vj  
i
i
if g   p dl  u  x  then p dl  u  x   g
end for
end function
function hstatic  u 
h 
for each vi  v do
h  h   p d i  u  vi  
end for
return h
end function

 duplicate detection

 sum over each p d i separately

following  we show that the dynamic k cycle conflict heuristic is also consistent by closely
following that of theorem     in the work of edelkamp and schrodl        
theorem    the dynamic pattern database version of the k cycle conflict heuristic remains consistent 
however  the above theorem assumes the use of the shortest distances between the nodes
in the abstract space  because we use a greedy method to solve the maximum weighted
matching problem  we can no longer guarantee to find the shortest paths  as a result  we
may lose the consistency property of the dynamic pattern database  it is thus necessary for
a  to reopen a duplicate node in the closed list if a better path is found 

   experiments
we evaluated the a  search algorithm on a set of benchmark datasets from the uci repository  bache   lichman         the datasets have up to    variables and         data
points  we discretized all variables into two states using the mean values and deleted all
  

filearning optimal bayesian networks

    e   
    e   

full

largest layer

sparse

    e   
    e   

size

    e   
    e   
    e   
    e   
    e   
    e   
    e   

figure    the number of parent sets and their scores stored in the full parent graphs
 full   the largest layer of the parent graphs in memory efficient dynamic programming  largest layer   and the sparse representation  sparse  

the data points with missing values  our a  search algorithm is implemented in java    we
compared our algorithm against the branch and bound  bb    de campos   ji         dynamic programming  dp    silander   myllymaki         and integer linear programming
 gobnilp  algorithms   cussens         we used the latest versions of these software or
source code at the time of the experiments as well as their default parameter settings  it
was version     for gobnilp and       for scip  bb and dp do not calculate mdl  but
they use the bic score  which uses an equivalent calculation as mdl  our results confirmed
that the algorithms found bayesian networks that either are the same or belong to the same
equivalence class  the experiments were performed on a      ghz intel xeon with   gb
of ram and running suse linux enterprise server version    
    full vs sparse parent graphs
we first evaluated the memory savings made possible by the sparse parent graphs in comparison to the full parent graphs  in particular  we compared the maximum number of
scores that have to be stored for all variables at once by each algorithm  a typical dynamic programming algorithm stores scores for all possible parent sets of all variables  the
memory efficient dynamic programming  malone et al       b  stores all possible parent
sets only in one layer of the parent graphs for all variables  so the size of the largest layer of
   a software package with source code named urlearning  you are learning  implementing the a 
algorithm can be downloaded at http   url cs qc cuny edu software urlearning html 
   http   www ecse rpi edu cvrl structlearning html
   http   b course hiit fi bene
   http   www cs york ac uk aig sw gobnilp 

  

fiyuan   malone

all parent graphs is an indication of its space requirement  the sparse representation only
stores the optimal parent sets for all variables 
figure   shows the memory savings by the sparse representation on the benchmark
datasets  it is clear that the number of optimal parent scores stored by the sparse representation is typically several orders of magnitude smaller than the full representation 
furthermore  due to theorem    increasing the number of data points increases the maximum number of candidate parents  therefore  the number of candidate parent sets increases
as the number of data points increases  however  many of the new parent sets are pruned
in the sparse representation because of theorem    the number of variables also affects
the number of candidate parent sets  consequently  the number of optimal parent scores
increases as a function of the number of data points and the number of variables  as the
results show  the amount of pruning is data dependent  though  and not easily predictable 
in practice  we find the number of data points to affect the number of unique scores much
more than the number of variables 
    pattern database heuristics
the new pattern database heuristic has two versions  static and dynamic pattern databases 
each of them can be parameterized in different ways  we tested various parameterizations
of the new heuristics on the a  algorithm on two datasets named autos and flag  we chose
these two datasets because they have a large enough number of variables and can better
demonstrate the effect of pattern database heuristics  for the dynamic pattern database  we
varied k from   to    for the static pattern databases  we tried groupings       and       for
the autos dataset and groupings         and       for the flag dataset  we obtained the
groupings by simply dividing the variables in the datasets into several consecutive blocks 
the results based on the sparse parent graphs are shown in figure    we did not show
the results of full parent graphs because a  ran out of memory on both datasets when
full parent graphs were used  with the sparse representations  a  achieved much better
scalability  and was able to solve both autos with any heuristic and flag with some of the
best heuristics when using sparse parent graphs  hereafter our experiments and results
assume the use of sparse parent graphs 
also  the pattern database heuristics improved the efficiency and scalability of a  significantly  a  with either the simple heuristic or the static pattern database with grouping
        ran out of memory on the flag dataset  the other pattern database heuristics enabled a  to finish successfully  the dynamic pattern database with k     helped to reduce
the number of expanded nodes significantly on both datasets  setting k     helped even
more  however  further increasing k to   resulted in increased search time  and sometimes
even an increased number of expanded nodes  not shown   we believe that a larger k always
results in a better pattern database  the occasional increase in expanded nodes is because
the greedy strategy we used to choose patterns did not fully utilize the better heuristic  the
longer search time is more understandable though  because it is less efficient to compute
a heuristic value in larger pattern databases  and the inefficiency gradually overtook the
benefit  therefore  k     seems to be the best parametrization for the dynamic pattern
database in general  for the static pattern databases  we were able to test much larger
  

filearning optimal bayesian networks

    e   

running time

size of pattern database

    e   

    e   
    e   
    e   
    e   

   
   
   
   
   
   
   
   
   
  
 

autos

    e   

running time

size of pattern database

    e   

    e   
    e   
    e   
    e   

   
   
   
   
   
   
   
   
   
  
 

x

x

f lag
figure    a comparison of a  enhanced with different heuristics  hsimple   hdynamic with k  
      and    and hstatic with groupings       and       for the autos dataset and
groupings         and       for the flag dataset   size of pattern database
means the number of patterns stored  running time means the search time
 in seconds  using the indicated pattern database strategy  an x means out of
memory 

groups because we do not need to enumerate all groups up to a certain size  the results
suggest that fewer larger groups tend to result in tighter heuristic 
the sizes of the static pattern databases are typically much larger than the dynamic
pattern databases  however  the time needed to create the pattern databases is still negligible in comparison to the search time in all cases  it is thus cost effective to try to compute
larger but affordable size static pattern databases to achieve better search efficiency  our
results show that the best static pattern databases typically helped a  to achieve better
efficiency than the dynamic pattern databases  even when the number of expanded nodes
is larger  the reason is that calculating the heuristic values is much more efficient when
using static pattern databases 
  

fiyuan   malone

     
bb scoring

dp scoring

a  scoring

scoring time

    

   

  

 

   

figure    a comparison of the scoring time of the bb  dp  and a  algorithms  each label
of the x axis consists of a dataset name  the number of variables  and the number
of data points 

    a  with the simple heuristic
we first tested a  with the hsimple heuristic  each competing algorithm has roughly two
phases  computing optimal parent sets scores  scoring phase  and searching for a bayesian
network structure  searching phase   we therefore compare the algorithms based on two
parts of running time  scoring time and search time  figure   shows the scoring times
of bb  dp  and a   gobnilp was not included because it assumes the optimal scores
are provided as input  each label in the horizontal axis shows a dataset  the number of
variables  and the number of data points  the results show that the ad tree method used
in our a  algorithm seems to be the most efficient approach to computing the parent scores 
the scoring part of dp is often more than an order of magnitude slower than others  this
result is somewhat misleading  however  the scoring and searching parts of dp are more
tightly integrated than the other algorithms  as a result  most of the work in dp is done in
the scoring part  little work is left for the search  as we will show shortly  the search time
of dp is typically very short 
figure   a  reports the search time of all the algorithms  some of the benchmark
datasets are so difficult that some algorithms take too long or even fail to find the optimal
solutions  we therefore terminate an algorithm early if it runs for more than       seconds
on a dataset  the results show that bb only succeeded on two of the datasets  voting and
hepatitis  within the time limit  on both datasets  the a  algorithm is several orders of
magnitude faster than bb  the major difference between a  and bb is the formulation
of the search space  bb searches in the space of directed cyclic graphs  while a  always
maintains a directed acyclic graph during the search  the results indicate that it is better
to search in the space of directed acyclic graphs 
the results also show that the search time needed by the dp algorithm is often shorter
than a   as we explained earlier  the reason is that all the heavy lifting in dp is done in
  

filearning optimal bayesian networks

     
bb

dp

gobnilp

a 

search time

    

   

  

 

x x

x

x

x

x

x

x

x x

x

 a 
     

total running time

dp total time

a  total time

    

   

  

 

 b 
figure    a comparison of the  a  search time  in seconds  for bb  dp  gobnilp  and a 
and  b  total running time for dp and a   an x means that the corresponding
algorithm did not finish within the time limit        seconds  or ran out of memory
in the case of a  
 
the scoring part  if we add the scoring and search time together  as shown in figure   b  
a  is several times faster than dp on all the datasets except adult and voting  again 
gobnilp is left out because it only has the search part   the main difference between a 
and dp is that a  only explores part of the order graph  while dynamic programming fully
evaluates the graph  however  each step of the a  search algorithm has some overhead
cost for computing the heuristic function and maintaining a priority queue  one step
  

fiyuan   malone

of a  is more expensive than a similar dynamic programming step  if the pruning does
not outweigh its overhead  a  can be slower than dynamic programming  both adult
and voting have a large number of data points  which makes the pruning technique in
theorem   less effective  although the dp algorithm does not perform any pruning  due
to its simplicity  the algorithm can be highly streamlined and optimized in performing all
its calculations  that is why the dp algorithm was faster than a  search on these two
datasets  however  our a  algorithm was more efficient than dp on all the other datasets 
for these datasets  the number of data points is not that large in comparison to the number
of variables  the pruning significantly outweighs the overhead of a   as an example 
a  runs faster on the mushroom dataset when comparing total running time even though
mushroom has over       data points 
the comparison between gobnilp and a  shows that they each has its own advantages  a  was able to find optimal bayesian networks for all the datasets well within the
time limit  gobnilp failed to learn optimal bayesian networks for three of the datasets 
including letter  image  and mushroom  the reason is that gobnilp formulates the
learning problem as an integer linear program whose variables correspond to the optimal
parent sets of all variables  even though these datasets do not have many variables  they
have many optimal parent sets  so the integer programs for them have too many variables to
be solvable within the time limit  on the other hand  the results also show that gobnilp
was quite efficient on many of the other datasets  even though a dataset may have many
variables  gobnilp can solve it efficiently as long as the number of optimal parent sets is
small  it is much more efficient than a  on datasets such as hepatitis and heart  although
the opposite is true on datasets such as adult and statlog 
    a  with pattern database heuristics
since static pattern databases seem to work better than dynamic pattern databases in most
cases  we tested a  with static pattern database  a  sp  against a   dp  and gobnilp
on all the datasets used in figure   as well as several larger datasets  we used the simple
static grouping of  n     n   for all the datasets  where n is the number of variables  the
results of bb are excluded because it did not solve any additional dataset  the results are
shown in figure   
the benefits brought by the pattern databases for a  are rather obvious  for the
datasets on which a  was able to finish  a  sp was typically up to an order of magnitude
faster  in addition  a  sp was able to solve three larger datasets  sensor  autos  and flag 
while a  failed on all of them  the running time on each of those datasets is pretty short 
which indicates that once the memory consumption of the parent graphs was reduced  a 
was able to use more memory for the order graph and solve the search problems rather
easily 
dp was able to solve one more dataset  autos  which a  was not able to solve  it is
somewhat surprising given that a  has pruning capability  the explanation is that a 
stores all search information in ram  so it will fail once the ram is exhausted  the dp
algorithm described by silander and myllymaki        stores its intermediate results as
computer files on hard disks  so it was able to scale to larger datasets than a  
  

filearning optimal bayesian networks

    

search time

dp

gobnilp

a 

a   sp

   

  

 

x

x

x

xxx

x

x xx x x

figure    a comparison of the search time  in seconds  for dp  gobnilp  a   and a  sp 
an x means that the corresponding algorithm did not finish within the time
limit        seconds  or ran out of memory in the case of a  

gobnilp was able to solve autos  horse  and flag  but failed on sensors  the sensors
dataset has        data points  the number of optimal parent sets is too large  almost
    as shown in figure    gobnilp begins to have difficulty solving datasets with more
than        optimal parent scores in our particular computing environment  but again 
gobnilp is quite efficient for datasets that it was able to solve such as autos and flag 
it is the only algorithm that can solve the horse dataset  from figure    it is clear that
the reason is the number of optimal parent sets is small for this dataset 
    pruning by a 
to gain more insight on the performance of a   we also looked at the amount of pruning
by a  in different layers of an order graph  we plot in figure    the detailed numbers
of expanded nodes versus the numbers of unexpanded nodes at each layer of the order
graph for two datasets  mushroom and parkinsons  we use these datasets because they are
the largest datasets that can be solved by both a  and a  sp  but they manifest different
pruning behaviors  the top two figures show the results for the a  with the simple heuristic 
and the bottom two show the a  sp algorithm 
on mushroom  the plain a  only needed to expand a small portion of the search nodes in
each layer  which indicates the heuristic function is quite tight on this dataset  the effective
pruning started as early as in the  th layer  for parkinsons  however  the plain a  was not
as successful in pruning the nodes  in the first    layers  the heuristic function appeared to
be too loose  a  had to expand most nodes in these layers  the heuristic function became
tighter for the latter layers and enabled a  to prune an increasing percentage of the search
nodes  with the help of pattern database heuristic  however  a  sp helped prune many
  

fi    e   
expanded

    e   

unexpanded

expandedvsunexpandednodes

expandedvsunexpandednodes

yuan   malone

    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

 

 

 

 

  

     
layer

  

  

  

    e   
expanded

    e   
    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

  

 a  a  on mushroom

 

 

 

 

        
layer

  

  

  

  

  

  

  

 b  a  on parkinsons
    e   

    e   
expanded

unexpanded

expandedvsunexpandednodes

expandedvsunexpandednodes

unexpanded

    e   
    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

 

 

 

 

     
layer

  

  

  

  

  

 c  a  sp on mushroom

expanded

unexpanded

    e   
    e   
    e   
    e   
    e   
    e   
    e   
    e   
 

 

 

 

 

        
layer

  

 d  a  sp on parkinsons

figure     the number of expanded and unexpanded nodes by a  at each layer of the order
graph on mushroom and parkinsons when using different heuristics 

more search nodes on parkinsons  the pruning became effective as early as in the  th layer 
the a  sp also helped prune more nodes on mushroom  although the benefit is not as clear
because a  was already quite effective on this dataset 
    factors affecting learning difficulty
several factors may affect the difficulty of a dataset for the bayesian network learning
algorithms  including the number of variables  the number of data points  and the number
of optimal parent sets  we analyzed the correlation between those factors and the search
times of the algorithms  we replaced each occurrence of out of time with       in order
to make the analysis possible  we caution though that it may results in underestimation  
figure    shows the results  we excluded the results of bb because it only finished on two
datasets  for dp  a   and a  sp  the most important factor in determining their efficiency
is the number of variables  as the correlations between their search time and the numbers
of variables were all greater than       however  there seems to be a negative correlation
between their search time with the number of data points  intuitively  increasing the number
of data points should make a dataset more difficult  the explanation is that there is preexisting negative correlation between the number of data points and the number of variables
for the datasets we tested  our analysis shows that the correlation between them is      
  

filearning optimal bayesian networks

 

variables

data records

optimal parent sets

   

correlation

   
   
   
 
   

dp

gobnilp

a 

a   sp

   
   

figure     the correlation between the search time of the algorithms and several factors
that may affect the difficulty of a learning problem  including the number of
variables  the number of data points in a dataset  and the number of optimal
parent sets 

since the search time has a strong positive correlation with the number of variables  the
seemingly negative correlation between the search time and the number of data points
becomes less surprising 
in comparison  the efficiency of gobnilp is most affected by the number of optimal
parent sets  their correlation is as high as close to      also  there is a positive correlation
between the number of data points and its efficiency  it is because  as we explained earlier 
more data points often leads to more optimal parent sets  finally  the correlation with the
number of variables is almost zero  which means the difficulty of a dataset for gobnilp
is not determined by the number of variables 
these insights are quite important  as they provide a guideline for choosing a suitable
algorithm given the characteristic of a dataset  if there are many optimal parent sets but
not many variables  a  is the better algorithm  if the other way around is true  gobnilp
is better 
    effect of scoring functions
our analyses so far are based mainly on the mdl score  other decomposable scoring
functions can also be used in the a  algorithm  as the correctness of the search strategies
and heuristic functions are not affected by the scoring function  however  different scoring
functions may have different properties  for example  theorem   is a property of the mdl
score  we cannot use this pruning technique for other scoring functions  consequently  the
number of optimal parent sets  the tightness of the heuristic  and the practical performance
of various algorithms may be affected 
to verify the hypothesis  we also tested the bdeu scoring function  heckerman       
with the equivalent sample size set to be      since the scoring phase is common for all
exact algorithms  we focus this experiment on comparing the number of optimal parent
sets resulted from the scoring functions  and the search time by a  sp and gobnilp
  

fiyuan   malone

optimal ps  mdl

optimal ps  bdeu

size

        
       
      
     
    
   
  
 

 a 
     
gobnilp  mdl

gobnilp  bdeu

a   mdl

a   bdeu

search time

    

   

  

 

xx

xx

x

xx

xx

x

 b 
figure     a comparison of  a  the number of optimal parent sets  and  b  the search time
by a  sp and gobnilp on various datasets for two scoring functions  mdl
and bdeu 

on the datasets  horse and flag were not included because their optimal parent sets were
unavailable  figure    shows the results 
the main observation is that the number of optimal parent sets does differ for mdl
and bdeu  bdeu score tends to allow for larger parent sets than mdl and results in a
larger number of optimal parent sets for most of the datasets  the difference was around
an order of magnitude on datasets such as imports and autos 
the comparison on the search time shows that a  sp is not affected as much as gobnilp  because of the increase in the number of optimal parent sets  the efficiency in finding
an optimal parent set is affected  but a  sp was only slowed down slightly on most of the
datasets  the only significant change is on the mushroom dataset  it took a  sp about  
seconds to solve the dataset when using mdl  but     seconds using bdeu  in comparison 
gobnilp was affected much more  it was able to solve datasets imports and autos effi  

filearning optimal bayesian networks

ciently when using mdl  but failed to solve them within   hours using bdeu  it remained
unable to solve letter  image  mushroom  and sensors within the time limit 

   discussions and conclusions
this paper presents a shortest path perspective of the problem of learning optimal bayesian
networks that optimize a given scoring function  it uses an implicit order graph to represent
the solution space of the learning problem such that the shortest path between the start
and goal nodes in the graph corresponds to an optimal bayesian network  this perspective
highlights the importance of two orthogonal directions of research  one direction is to
develop search algorithms for solving the shortest path problem  the main contribution
we made on this line is an a  algorithm for solving the shortest path problem in learning
an optimal bayesian network  guided by heuristic functions  the a  algorithm focuses on
searching the most promising parts of the solution space in finding the optimal bayesian
network 
the second equally important research direction is the development of search heuristics 
we introduced two admissible heuristics for the shortest path problem  the first heuristic
estimates the future cost by completely relaxing the acyclicity constraint of bayesian networks  it is shown to be not only admissible but also consistent  the second heuristic  the
k cycle conflict heuristic  is developed based on the additive pattern database technique 
unlike the simple heuristic in which each variable is allowed to choose optimal parents independently  the new heuristic tightens the estimation by enforcing the acyclicity constraint
within some small groups of variables  there are two specific approaches to computing the
new heuristic  one approach named dynamic k cycle conflict heuristic computes the costs
for all groups of variables with size up to k  during the search  we dynamically partition
remaining variables into exclusive patterns in calculating the heuristic value  the other
approach named static k cycle conflict heuristic partitions the variables into several static
exclusive groups  and computes a separate pattern database for each group  we can sum
the costs of the static pattern databases to obtain an admissible heuristic  both heuristics
remain admissible and consistent  although the consistency of the dynamic k cycle conflict
may be sacrificed due to a greedy method we used to select the patterns 
we tested the a  algorithm empowered with different search heuristics on a set of uci
machine learning datasets  the results show that both the pattern database heuristics
contributed to significant improvements in the efficiency and scalability of the a  algorithm  the results also show that our a  algorithm is typically more efficient than dynamic
programming that shares a similar formulation  in comparison to gobnilp  an integer
programming algorithm  a  is less sensitive to the number of optimal parent sets  number
of data points  or scoring functions  but is more sensitive to the number of variables in the
datasets  with those advantages  we believe our methods represent a promising approach
to learning optimal bayesian network structures 
exact algorithms for learning optimal bayesian networks are still limited to relatively
small problems  further scaling up the learning is needed  e g   by incorporating domain or
expert knowledge in the learning  it also means that approximation methods are still useful
in domains with many variables  nevertheless  the exact algorithms are valuable because
they can serve as the basis to evaluate different approximation methods so that we have
  

fiyuan   malone

some quality assurance  also  it is a promising research direction to develop algorithms
that have the best properties of both approximation and exact algorithms  that is  they
can find good solutions quickly and  if given enough resources  can converge to an optimal
solution  malone   yuan        

acknowledgments
this research was supported by nsf grants iis          eps          iis         and
the academy of finland  finnish centre of excellence in computational inference research
coin           part of this research has previously been presented in ijcai     yuan 
malone    wu        and uai     yuan   malone        

appendix a  proofs
the following are the proofs of the theorems in this paper 
a   proof of theorem  
proof  note that the optimal parent set for x out of u has to be a subset of u  and the
subset has to have the best score  sorting all the unique parent scores makes sure that the
first found subset must satisfy both requirements stated in the theorem 

a   proof of theorem  
proof  heuristic function h is clearly admissible  because it allows each remaining variable
to choose optimal parents from all the other variables in v  the chosen parent set must
be a superset of the parent set for the same variable in the optimal directed acyclic graph
consisting of the remaining variables  due to theorem    the heuristic results in a lower
bound cost 

a   proof of theorem  
proof  for any successor node s of u  let y  s   u  we have
x
h u   
bestscore x  v  x  
xv u



x

bestscore x  v  x  

xv u x  y

 bestscore y  u 
  h s    c u  s  
the inequality holds because fewer variables are used to select optimal parents for y   hence 
h is consistent 

a   proof of theorem  
proof  the theorem can be proven by noting that avoiding cycles between the variables
in u is equivalent to finding an optimal ordering of the variables with the best joint score 
  

filearning optimal bayesian networks

the different paths from v   u to the goal node correspond to the different orderings of
the variables  among which the shortest path hence corresponds to the optimal ordering  
a   proof of theorem  
proof  for node u  assume the remaining variables v   u are partitioned into exclusive
sets v         vp   because of the decomposability of the scoring function  we have h u   
p
p
c vi    when computing c vi    we do not allow directed cycles within vi   all the
i  

variables in v   vi are valid candidate parents  however  the cost of each pattern  c vi   
must be optimal by the definition of pattern databases  by the same argument used in the
proof of theorem    the h u  cost cannot be worse than the total cost of v   u  that is  the
cost of the optimal directed acyclic graph consisting of these variables  with u as allowable
parents also   otherwise  we can simply arrange the variables in the patterns in the same
order as in the optimal directed acyclic graph to get the same cost  therefore  the heuristic
is still admissible 
note that the previous argument only relies on the optimality of the pattern costs  not
on which patterns are chosen  the greedy strategy used in dynamic pattern database only
affects which patterns are selected  therefore  this theorem holds for both dynamic and
static pattern databases 

a   proof of theorem   
proof  recall that using static pattern databases with node partitions v   i vi   the
heuristic value for a node u is as follows 
h u   

x

c  v   u   vi   

i

where  v  u  vi is the pattern in the ith static pattern database  then  for any successor
node s of u  let y  s   u  without lost of generality  let y   v   u   vj   the heuristic
value for node s is then
h s   

x

c  v   u   vi     c  v   u    vj    y     

i  j

also  the cost between u and s is
c u  s    bestscore y  u  
from the definition of pattern database  we know that c  v u vj   is the best possible
joint score for the variables in the pattern after u are searched  therefore  we have
c  v   u   vj    c v   u   vj    y      bestscore y   i  j vi     vj    v   u  
 c  v   u    vj    y       bestscore y  u  
the last inequality holds because u   i  j vi     vj    v   u    the following then
immediately follows 
h u   h s    c u  s  
  

fiyuan   malone

hence  the static k cycle conflict heuristic is consistent 

a   proof of theorem   
proof  the heuristic values calculated from the dynamic pattern database can be considered as shortest distances between nodes in an abstract space  the abstract space consists
of the same set of nodes  i e   all subsets of v  however  additional arcs are added between
a node and nodes with up to k additional variables 
consider a shortest path p between any two nodes u and goal v in the original solution
space  the path remains a valid path  but may no longer be the shortest path between u
and v because of the additional arcs 
let g  u  v  be the shortest distance between u and v in the abstract space  for any
successor node s of u  we must have the following 
g  u  v   g  u  s    g  s  v  

   

now  recall that g  u  v  and g  s  v  are the heuristic values for the original solution
space  and g  u  s  is equal to the arc cost c u  s  in the original space  we therefore have
the following 
h u   c u  s    h s  
   
hence  the dynamic k cycle conflict heuristic is consistent 



references
acid  s     de campos  l  m          a hybrid methodology for learning belief networks 
benedict  international journal of approximate reasoning                 
akaike  h          information theory and an extension of the maximum likelihood principle 
in proceedings of the second international symposium on information theory  pp 
       
bache  k     lichman  m 
http   archive ics uci edu ml 

       

uci

machine

learning

repository 

bouckaert  r  r          properties of bayesian belief network learning algorithms  in
proceedings of the tenth conference on uncertainty in artificial intelligence  pp 
        seattle  wa  morgan kaufmann 
bozdogan  h          model selection and akaikes information criterion  aic   the general
theory and its analytical extensions  psychometrika             
buntine  w          theory refinement on bayesian networks  in proceedings of the seventh
conference        on uncertainty in artificial intelligence  pp        san francisco 
ca  usa  morgan kaufmann publishers inc 
cheng  j   greiner  r   kelly  j   bell  d     liu  w          learning bayesian networks
from data  an information theory based approach  artificial intelligence            
     
  

filearning optimal bayesian networks

chickering  d          a transformational characterization of equivalent bayesian network
structures  in proceedings of the   th annual conference on uncertainty in artificial
intelligence  uai      pp        san francisco  ca  morgan kaufmann publishers 
chickering  d  m          learning bayesian networks is np complete  in learning from
data  artificial intelligence and statistics v  pp          springer verlag 
chickering  d  m          learning equivalence classes of bayesian network structures 
journal of machine learning research            
cooper  g  f     herskovits  e          a bayesian method for the induction of probabilistic
networks from data  machine learning            
culberson  j  c     schaeffer  j          pattern databases  computational intelligence 
           
cussens  j          bayesian network learning with cutting planes  in proceedings of the
twenty seventh conference annual conference on uncertainty in artificial intelligence  uai      pp          corvallis  oregon  auai press 
daly  r     shen  q          learning bayesian network equivalence classes with ant colony
optimization  journal of artificial intelligence research             
dash  d     cooper  g          model averaging for prediction with discrete bayesian
networks  journal of machine learning research              
dash  d  h     druzdzel  m  j          a hybrid anytime algorithm for the construction
of causal models from sparse data  in proceedings of the fifteenth annual conference
on uncertainty in artificial intelligence  uai     pp          san francisco  ca 
morgan kaufmann publishers  inc 
de campos  c  p     ji  q          efficient learning of bayesian networks using constraints 
journal of machine learning research             
de campos  c  p     ji  q          properties of bayesian dirichlet scores to learn bayesian
network structures  in fox  m     poole  d   eds    aaai  pp          aaai press 
de campos  l  m          a scoring function for learning bayesian networks based on
mutual information and conditional independence tests  journal of machine learning
research              
de campos  l  m   fernndez luna  j  m   gmez  j  a     puerta  j  m          ant colony
optimization for learning bayesian networks  international journal of approximate
reasoning                 
de campos  l  m     huete  j  f          a new approach for learning belief networks
using independence criteria  international journal of approximate reasoning         
       
  

fiyuan   malone

de campos  l  m     puerta  j  m          stochastic local algorithms for learning belief
networks  searching in the space of the orderings  in benferhat  s     besnard  p 
 eds    ecsqaru  vol       of lecture notes in computer science  pp         
springer 
edelkamp  s     schrodl  s          heuristic search   theory and applications  morgan
kaufmann 
felner  a   korf  r     hanan  s          additive pattern database heuristics  journal of
artificial intelligence research             
felzenszwalb  p  f     mcallester  d  a          the generalized a  architecture  journal
of artificial intelligence research             
friedman  n     koller  d          being bayesian about network structure  a bayesian
approach to structure discovery in bayesian networks  machine learning           
      
friedman  n   nachman  i     peer  d          learning bayesian network structure from
massive datasets  the sparse candidate algorithm  in laskey  k  b     prade  h 
 eds    proceedings of the fifteenth conference conference on uncertainty in artificial
intelligence  uai      pp          morgan kaufmann 
garey  m  r     johnson  d  s          computers and intractability  a guide to the
theory of np completeness  w  h  freeman   co   new york  ny  usa 
glover  f          tabu search  a tutorial  interfaces               
hart  p  e   nilsson  n  j     raphael  b          a formal basis for the heuristic determination of minimum cost paths  ieee trans  systems science and cybernetics        
       
heckerman  d   geiger  d     chickering  d  m          learning bayesian networks  the
combination of knowledge and statistical data  machine learning             
heckerman  d          a tutorial on learning with bayesian networks  in holmes  d     jain 
l   eds    innovations in bayesian networks  vol      of studies in computational
intelligence  pp        springer berlin   heidelberg 
hemmecke  r   lindner  s     studeny  m          characteristic imsets for learning
bayesian network structure  international journal of approximate reasoning         
         
hsu  w  h   guo  h   perry  b  b     stilson  j  a          a permutation genetic algorithm
for variable ordering in learning bayesian networks from data  in langdon  w  b  
cant paz  e   mathias  k  e   roy  r   davis  d   poli  r   balakrishnan  k   honavar 
v   rudolph  g   wegener  j   bull  l   potter  m  a   schultz  a  c   miller  j  f  
burke  e  k     jonoska  n   eds    gecco  pp          morgan kaufmann 
  

filearning optimal bayesian networks

jaakkola  t   sontag  d   globerson  a     meila  m          learning bayesian network
structure using lp relaxations  in proceedings of the   th international conference on
artificial intelligence and statistics  aistats   pp          chia laguna resort 
sardinia  italy 
klein  d     manning  c  d          a  parsing  fast exact viterbi parse selection  in
proceedings of the human language conference and the north american association
for computational linguistics  hlt naacl   pp         
koivisto  m     sood  k          exact bayesian structure discovery in bayesian networks 
journal of machine learning research            
kojima  k   perrier  e   imoto  s     miyano  s          optimal search on clustered
structural constraint for learning bayesian network structure  journal of machine
learning research             
lam  w     bacchus  f          learning bayesian belief networks  an approach based on
the mdl principle  computational intelligence             
larranaga  p   kuijpers  c  m  h   murga  r  h     yurramendi  y          learning
bayesian network structures by searching for the best ordering with genetic algorithms  ieee transactions on systems  man  and cybernetics  part a             
    
malone  b     yuan  c          evaluating anytime algorithms for learning optimal bayesian
networks  in proceedings of the   th conference on uncertainty in artificial intelligence  uai      pp          seattle  washington 
malone  b   yuan  c   hansen  e     bridges  s       a   improving the scalability of optimal bayesian network learning with frontier breadth first branch and bound search  in
proceedings of the   th conference on uncertainty in artificial intelligence  uai     
pp          barcelona  catalonia  spain 
malone  b   yuan  c     hansen  e  a       b   memory efficient dynamic programming
for learning optimal bayesian networks  in proceedings of the   th aaai conference
on artificial intelligence  aaai      pp            san francisco  ca 
moore  a     lee  m  s          cached sufficient statistics for efficient machine learning
with large datasets  journal of artificial intelligence research          
moore  a     wong  w  k          optimal reinsertion  a new search operator for accelerated and more accurate bayesian network structure learning  in international
conference on machine learning  pp         
myers  j  w   laskey  k  b     levitt  t  s          learning bayesian networks from
incomplete data with stochastic search algorithms  in laskey  k  b     prade  h 
 eds    proceedings of the fifteenth conference conference on uncertainty in artificial
intelligence  uai      pp          morgan kaufmann 
  

fiyuan   malone

ordyniak  s     szeider  s          algorithms and complexity results for exact bayesian
structure learning  in gruwald  p     spirtes  p   eds    proceedings of the   th
conference conference on uncertainty in artificial intelligence  uai      pp     
     auai press 
ott  s   imoto  s     miyano  s          finding optimal models for small gene networks 
in pacific symposium on biocomputing  pp         
papadimitriou  c  h     steiglitz  k          combinatorial optimization  algorithms and
complexity  prentice hall  inc   upper saddle river  nj  usa 
parviainen  p     koivisto  m          exact structure discovery in bayesian networks with
less space  in proceedings of the twenty fifth conference on uncertainty in artificial
intelligence  montreal  quebec  canada  auai press 
pearl  j          heuristics  intelligent search strategies for computer problem solving 
addison wesley longman publishing co   inc   boston  ma  usa 
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann publishers inc 
perrier  e   imoto  s     miyano  s          finding optimal bayesian network given a
super structure  journal of machine learning research              
rissanen  j          modeling by shortest data description  automatica             
silander  t     myllymaki  p          a simple approach for finding the globally optimal bayesian network structure  in proceedings of the   nd annual conference on
uncertainty in artificial intelligence  uai      pp          auai press 
silander  t   roos  t   kontkanen  p     myllymaki  p          factorized normalized
maximum likelihood criterion for learning bayesian network structures  in proceedings
of the  th european workshop on probabilistic graphical models  pgm      pp     
    
singh  a     moore  a  w          finding optimal bayesian networks by dynamic programming  tech  rep  cmu cald         carnegie mellon university 
spirtes  p   glymour  c     scheines  r          causation  prediction  and search  second
edition   the mit press 
suzuki  j          learning bayesian belief networks based on the minimum description
length principle  an efficient algorithm using the b b technique  in international
conference on machine learning  pp         
teyssier  m     koller  d          ordering based search  a simple and effective algorithm
for learning bayesian networks  in proceedings of the twenty first annual conference
on uncertainty in artificial intelligence  uai      pp          auai press 
  

filearning optimal bayesian networks

tian  j          a branch and bound algorithm for mdl learning bayesian networks  in
uai     proceedings of the   th conference on uncertainty in artificial intelligence 
pp          san francisco  ca  usa  morgan kaufmann publishers inc 
tsamardinos  i   brown  l     aliferis  c          the max min hill climbing bayesian
network structure learning algorithm  machine learning           
xie  x     geng  z          a recursive method for structural learning of directed acyclic
graphs  journal of machine learning research            
yuan  c   lim  h     littman  m  l       a   most relevant explanation  computational
complexity and approximation methods  annals of mathematics and artificial intelligence             
yuan  c   lim  h     lu  t  c       b   most relevant explanation in bayesian networks 
journal of artificial intelligence research  jair              
yuan  c   liu  x   lu  t  c     lim  h          most relevant explanation  properties 
algorithms  and evaluations  in proceedings of   th conference on uncertainty in
artificial intelligence  uai      pp          montreal  canada 
yuan  c     malone  b          an improved admissible heuristic for learning optimal
bayesian networks  in proceedings of the   th conference on uncertainty in artificial
intelligence  uai      pp          catalina island  ca 
yuan  c   malone  b     wu  x          learning optimal bayesian networks using a 
search  in proceedings of the   nd international joint conference on artificial intelligence  ijcai      pp            helsinki  finland 

  

fi