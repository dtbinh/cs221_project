journal artificial intelligence research                  

submitted        published      

unsupervised sub tree alignment
tree to tree translation
tong xiao
jingbo zhu

xiaotong mail neu edu cn
zhujingbo mail neu edu cn

college information science engineering
northeastern university
      wenhua road  heping district
shenyang  china

abstract
article presents probabilistic sub tree alignment model application
tree to tree machine translation  unlike previous work  resort surface heuristics expensive annotated data  instead derive unsupervised model infer
syntactic correspondence two languages  importantly  developed model
syntactically motivated rely word alignments  by product 
model outputs sub tree alignment matrix encoding large number diverse alignments
syntactic structures  machine translation systems efficiently extract translation rules often filtered due errors   best alignment 
experimental results show proposed approach outperforms three state of the art
baseline approaches alignment accuracy grammar quality  applied
machine translation  approach yields      bleu improvement      ter reduction nist machine translation evaluation corpora  tree binarization
fuzzy decoding  even outperforms state of the art hierarchical phrase based system 

   introduction
recent years witnessed increasing interest syntax based methods many artificial intelligence  ai  natural language processing  nlp  applications ranging
text summarization machine translation  mt   particular  syntax based models
intensively investigated statistical machine translation  smt   approaches include
string to tree mt  galley  hopkins  knight    marcu        galley  graehl  knight  marcu 
deneefe  wang    thayer         tree to string mt  liu  liu    lin        huang  kevin 
  joshi        tree to tree mt  eisner        zhang  jiang  aw  li  tan    li       
liu  lu    liu      a  chiang         train tree string tree tree pairs
seek model translation equivalency relations learned parsed data  part
focus syntax based mt  tree to tree models use synchronous context free grammars synchronous tree substitution grammars received growing interest  showing
promising results several well established evaluation tasks  zhang et al         liu
et al       a  chiang         example  recent studies  chiang        demonstrated
modern tree to tree systems significantly outperform hierarchical phrase based
counterpart large scale chinese english arabic english translation 
tree to tree mt  translation problem broadly regarded transformation
source language syntax tree target language syntax tree  model process 
c
    
ai access foundation  rights reserved 

fixiao   zhu

tree to tree systems resort general framework synchronous grammars 
pair trees generated derivations synchronous grammar rules  or translation
rules   model  goal translation build underlying derivations
pairs trees output target string encoded likely derivation  figure
  shows intuitive example illustrate generation process tree pair using
sample grammar  source target language sentences associated
phrase structure trees generated using automatic parsers  
previous work shown acquisition good translation rules one essential factors contributing success syntax based systems  deneefe  knight  wang 
  marcu         date  several research groups addressed issue rule acquisition designed effective algorithms extract high coverage grammars bilingual
parsed data  zhang et al         liu et al       a  chiang         despite differences
detailed modeling  approaches rely syntactic alignments align tree nodes
syntactic parse tree one language tree nodes other  alignments
could employed standard tree to tree rule extraction algorithms  liu et al       a 
chiang        
current tree to tree models heavily depend syntactic alignments two
languages  alignments induced indirectly word alignments tree to tree
systems sensitive word alignment behavior  unfortunately  word alignments
general far perfect viewpoint syntactic alignment  fossum  knight 
  abney         cases  even one spurious word alignment prevent large
number desirable rules extraction  example  figure   a  shows tree totree translation rules extracted using word alignment produced giza   
alignment incorrectly aligns source word  a past tense marker chinese 
target word the  spurious word alignment produces incorrect rule as  
dt the  blocks extraction high level syntactic transfer rules 
ip nn  vp    s np  vp    
obviously  desirable solution directly infer node correspondences
source target parse trees  namely sub tree alignment  syntactic parse trees
explain underlying structure sentences well  performing alignment sub tree level
make benefits high level structural information syntactic categorization 
example  consider alignment figure   b   links nodes two parse
trees  in chinese english   rather aligning word level  example 
confident align vp sub tree  spanning   source tree
vp sub tree  spanning drastically fallen  target tree   therefore
   phrase structure tree  leaf nodes words sentence  internal tree nodes followed
leaf nodes labeled part of speech  pos  tags  tree nodes labeled syntactic
categories defined treebanks  see appendix meanings pos tags syntactic categories used
work   nlp  many well developed parsers available automatic parsing  also  several
good quality phrase structure treebanks across languages used train parsing models 
penn english chinese treebanks  marcus  santorini    marcinkiewicz        xue  xia  chiou 
  palmer         note addition phrase structure syntax  popular formalisms
 e g   dependency syntax  used syntax based mt  discussion different formalisms
syntactic parsing beyond scope article  instead focus tree to tree mt based
phrase structure trees throughout work 
   chinese english follow subject verb object structure  verb phrases chinese
sentence frequently aligned verb phrases english translation 

   

fiunsupervised sub tree alignment tree to tree translation

np

vp

np

vp

prp

vbd





target language side
 english 



vp
vbn

pp

satisfied
pp


np



r 

r 

dt

nns



answers

r 


 ta 


 huida 

p

nn


 biaoshi 


 manyi 

vv

nn

pp

pn

pp

vp

np

vp

np

vp

source language side
 chinese 

r 

 dui 

ip

synchronous grammar used
id
r 
r 
r 
r 

source language side
np pn   
pp p   nn   
vp pp  vp vv   nn    
ip np  vp   

target language side
np prp he  
pp in with  np dt the  nns answers   
vp vbd was  vp vbn satisfied  pp    
s np  vp   

figure    example derivation tree to tree translation rules  rules represented
aligned pairs tree fragments  linked dotted lines   subscripts
language sides grammar rules indicate alignments frontier nonterminals  language side derivation  round head lines link
frontier non terminals rewritten translation 
know child nodes source language vp likely aligned child
nodes target language vp  means two vps aligned 
children aligned outside vp sub tree structure  i e   prevent
alignment chinese tree node english tree node dt due
inconsistency vp vp alignment  case  correctly aligned vbp 
   

fixiao   zhu




np
dt

np

vp

nns

vbp

dt

advp

imports

rb

vp

nns

vbp

advp

imports

vbn

rb

drastically fallen

drastically fallen








vv



ad

nn

vbn




vp



vv



ad

nn

vp



vp
vp

ip

ip

 minimal  rules extracted

 minimal  rules extracted
r 

as   dt the 

r 

ad   rb drastically 

r 

nn   nns imports 

r 

vv   vbn fallen 

r 

ad   rb drastically 

r 

as   vbp have 

r 

vv   vbn fallen 

r 

nn   np dt the  nns imports  

r 

ip nn  vp ad  vp vv  as     

r 

vp ad  vp vv  as    
vp vbp  advp rb  vbn    

s np dt  nns    vp vbp have  advp rb  vbn     

r 

 a  word alignment extracted rules

ip nn  vp    s np  vp   

 b  sub tree alignment extracted rules

figure    tree to tree translation rules extracted via word alignment  a  sub tree alignment  b   dashed lines represent word alignment links  dotted lines
represent sub tree alignment  or node alignment  links 
result  bad rule as   dt the  ruled out  desirable rules
extracted using sub tree alignment  including desirable rules blocked
figure   a   
actually  researchers aware sub tree alignment problem tried
explore solutions  tinsley  zhechev  hearne    way        sun  zhang    tan      b 
    a   example  proposed judge whether two nodes aligned not 
work  alignment confidence first calculated using lexical translation probabilities
classifiers trained labeled data  final alignment determined according
node level alignment score  however  inference sub tree alignment approaches
relies heuristic algorithms  models essentially optimized within unified
probabilistic framework 
moreover  alignment result applied tree to tree translation  systems
suffer another problem translation rules extracted using   best alignment
 zhang et al         liu et al       a  chiang         problem significantly affects
   

fiunsupervised sub tree alignment tree to tree translation

rule set coverage rate due alignment errors  simple solution issue use
k best alignments instead  however  k best alignments often variations many
redundancies  differ alignment links  obviously inefficient
extract rules similar alignments 
article address sub tree alignment issue principled way investigate
methods effectively apply sub tree alignment result tree to tree mt  particular 
develop unsupervised approach learning probabilistic sub tree alignment
model bi lingual parsed data 
investigate different methods integrating sub tree alignment tree to tree
machine translation  specifically  develop sub tree alignment matrix encoding
exponentially large number diverse sub tree alignments  extract multiple
alternative translation rules using alignment posteriors sub tree alignment
matrix 
advantages approach three fold  first  approach rely
heuristic algorithms labeled data  second  developed sub tree alignment model
structure model used mt  i e   based synchronous tree
substitution grammars  means mt systems directly make benefits subtree alignment model  especially rule extraction mt parameter estimation  third 
accessing sub tree alignment matrix encodes large number alignments 
efficiently obtain rules often filtered due errors within  best k best alignment result  experiment approach chinese english subtree alignment translation tasks  sub tree alignment  significantly outperforms
three state of the art baselines  machine translation  approach obtains significant
improvements tree to tree system rule quality translation quality 
example  yields      bleu improvement      ter reduction nist mt
evaluation corpora  finally  system even outperforms state of the art hierarchical
phrase based system equipped tree binarization  wang  knight    marcu      b 
fuzzy decoding  chiang        techniques 
rest article structured follows  section   briefly introduces subtree alignment task  section   describes unsupervised approach sub tree alignment 
then  section   investigates effective methods applying alignment model tree totree translation  then  section   presents experimental evaluation approach 
reviewing related work section    interesting issues discussed section   
finally  article concluded summary section   

   problem statement
general  sub tree alignment defined task find alignment
nodes tree nodes another tree   restrict machine translation article  sub tree alignment actually task must tightly coupled
specific applications  example  addition machine translation 
   work term tree refers data structure defined recursively collection nodes
starting root node  node list edges pointing nodes  or children  
constraint edge duplicated points root  knuth        

   

fixiao   zhu

nlp tasks make benefits sub tree alignment  including sentence simplification  cohn   lapata        woodsend   lapata         paraphrasing  das   smith 
       question answering  wang  smith    mitamura      a   parser adaptation
projection  smith   eisner        
ideally  would sub tree alignment system language independent
application independent  given parallel corpus training examples  able
learn alignment model use infer syntactic correspondence tree
pairs  broadly speaking  alignments paired linguistic tree structures
regarded instances sub tree alignment  example  alignment performed
dependency trees  eisner        nakazawa   kurohashi        phrase structure
trees  tinsley et al         sun et al       b  
although sub tree alignment problem includes number tasks seek alignments syntactic tree structures  particularly interested aligning tree
nodes phrase structure trees work  focus phrase structure sub tree alignment because     phrase structure parsing one popular syntactic analysis
formalisms  several state of the art full parsing models tools developed many
languages     phrase structure trees basis many successful syntax based mt systems  alternatives  dependency trees  benefit mt systems 
constituency based models interest relatively larger portion mt community show state of the art performance recent tree to tree systems  zhang et al  
      liu et al       a  chiang        
natural language processing  phrase structure parse tree ordered rooted
tree  represents syntactic structure sentence according phrase structure grammars  or constituency grammars  describe way words combine form
phrases sentences  chiswell   hodges         generally  phrase structure parse trees
distinguish terminal non terminal nodes  leaf nodes labeled terminal categories  or words   internal nodes labeled non terminal categories
grammar  or phrasal categories   example  english parse tree figure   b  
imports terminal  nodes np nns two non terminals indicating
noun phrase plural form nouns respectively   following description
experiments  take penn treebank standard tree annotation 
choose penn treebank one popular tree annotated corpora
used syntactic parsing good quality quantity several languages 
chinese english 
based definition  sub tree alignment defined alignments
non terminals source target language  phrase structure  parse trees  
formally  given source language parse tree target language parse tree  
sub tree alignment  denoted a s    short  set node to node links
  node pair  u  v   s     good alignment follow three criteria
 tinsley et al         
   u  or v  aligned  indicating   to   alignment  
   note non terminals always followed leaf nodes called pre terminals
labeled part of speech tags  e g   nns node followed terminal node imports
thus pre terminal 
   contrast  word alignment regarded alignments terminals two languages 

   

fiunsupervised sub tree alignment tree to tree translation

   u aligned v  descendants u aligned descendants v 
   u aligned v  ancestors u aligned ancestors v 
criteria prevent aligning constituents cross other  property
similar bi parsing formalisms  synchronous context free
grammars synchronous tree substitution grammars  advantage enables
use powerful synchronous grammars modeling sub tree alignment problem 
shown next section  based constraints take synchronous
tree substitution grammars basis proposed model 
according tinsley et al s        work  alignments satisfying criteria
called well formed alignments  alignment ill formed violates
criteria  work focus well formed alignments  hence sub tree alignment
task stated as  given pair parse trees  s     search likely wellformed alignment
  arg max p a   s   

   

a s t  

 s    set well formed alignments  p a   s    viewed
alignment model predicts probability every alignment given  
follows  describe approach sub tree alignment tree to tree translation  including alignment model  training inference methods  effective
use model tree to tree mt systems 

   unsupervised sub tree alignment
section present unsupervised sub tree alignment model  first define
base model sub tree alignment framework synchronous tree substitution
grammars  describe model parameterization  training inference methods 
    base model
fundamental question sub tree alignment define correspondence
nodes source language parse tree nodes target language parse tree 
address issue using synchronous tree substitution grammars  stsgs 
widely adopted model transformation process source target language
parse trees mt  zhang et al         liu et al       a  chiang         general
framework stsgs  chiang   knight         assumed pair source
target parse trees simultaneously generated using derivation stsg rules  or
tree to tree transfer rules   example  grammar figure   stsg
rules used generate pair sentences  formally  stsg system
hns   nt   ws   wt   i  ns nt sets non terminals source target
languages  ws wt sets terminals  or words  source target languages 
finite set productions  production stsg rewrite rule  denoted r 
pair source target language non terminals  snt   tnt   
hsnt   tnt hsr   tr   r
   

fixiao   zhu

sr source language tree fragment  whose frontier nodes either words ws
non terminals ns  labeled x   tr corresponding target language tree fragment 
r set   to   alignments connect frontier non terminals sr
frontier non terminals tr   example  r  figure   a  
snt   ip
tnt  
sr   ip nn x vp ad x vp vv x as x   
tr   s np dt x nns x  vp vbp have  advp rb x vbn x   
r                       
note non terminals left hand side rule actually roots
corresponding tree fragments right hand side  means rule contains
exactly information matter whether root nodes  snt   tnt   explicitly
represented not  following parts article use hsr   tr   r simpler representation stsg rules  beyond this  stsg rules written
compact form alignment r encoded numbers assigned frontier
non terminals sr tr   example  figures      subscripts language
sides stsg rules indicate aligned pairs frontier non terminals 
stsg model  frontier non terminals called substitution nodes 
applying stsgs  rewrite aligned pair substitution nodes tree fragment
pair encoded stsg rule  constraint operation labels
substituted non terminals must match root labels rewrite rules  example 
round head lines figure   show substitution operations used derivation 
using stsg rules  parse tree pair generate corresponding derivations  generation process trivial  start pair root symbols repeatedly rewrite pairs non terminal symbols using stsg rules  example  tree pair
figure   b   start root labels source target language parse trees
 the superscript indicates node index tree 
h ip      s   
apply rule r   
ip    s   

h ip nn    vp       s np    vp     
r 

ip    s   

represents operation rewrites aligned node pair ip   
r 

s    r   denoted ip    s       process proceeds repeatedly rewriting
remaining frontier non terminals get complete source target language trees 
so 
   

fiunsupervised sub tree alignment tree to tree translation

nn    np   


r 

vp    vp   


r 

h ip nn   vp       s np dt the  nns imports   vp     
h ip nn   vp ad

   

   

vp vv

as        

s np dt the  nns imports   vp vbp
ad    rb   


r 

h ip nn   vp ad   vp vv

   

s np dt the  nns imports   vp vbp
   

vv

   

advp rb

   

vbn       

as        

   

advp rb drastically  vbn       

   

vbn

h ip nn   vp ad   vp vv   as        
r 

s np dt the  nns imports   vp vbp

   

advp rb drastically  vbn fallen    
as    vbp   


r 

h ip nn   vp ad   vp vv   as      
s np dt the  nns imports   vp vbp have 
advp rb drastically  vbn fallen    

process  rewrite rule indicates node alignment  importantly 
derivations model two nice properties  first  node u
source language  or target language  parse tree  one node targetlanguage  or source language  parse tree aligned u  second  hierarchical
structure behind alignment avoids links constituents cross other 
consequently  well formed sub tree alignment a  always find derivation
encodes alignment a  means sub tree alignment problem essentially
problem finding likely stsg derivation  thus sub tree
alignment task  see equation      restated finding likely derivation
given pair parse trees 
model derivation probability  follow formulation adopted statistical
word alignment  brown  pietra  pietra    mercer        vogel  ney    tillmann        
transformation source language tree target language tree described
following equation 
x
p t   s   
p  t  d s 
   
dd s t  

d s    set derivations transforming  say  aligning nodes
nodes    p  t  d s  probability transforming using
derivation d s     parameters model  use notation
p    express dependence model parameters  general  optimal
value learned parsed parallel data training criteria  example 
context unsupervised learning  optimize model parameters maximizing
probability observed data  known maximum likelihood training  
given set optimal parameters   best sub tree alignment  s    determined
p  t d s 
choosing derivation p  d   s    greatest  since p  d   s      p  t  s 


   

fixiao   zhu

p  t   s  constant given  s      finding best derivation
finding derivation make p  t    s  large possible  hence reach
fundamental equation sub tree alignment 
  arg max p  t    s 

   

dd s t  

formulation implies three fundamental issues sub tree alignment  including
modeling derivation probability  i e   p  t  d s    learning model parameters  i e    
finding best alignment given learned model  i e   arg max operation  
following parts section  describe solutions issues 
    parameterization
simplest case  alignment model one parameter instance derivation 
however  model would unmanageable set parameters since number
derivations exponential length input sentences  choose simple
solution issue decomposes base model product trainable submodels  start assumption rules conditionally independent given
source language parse tree s  probability p t    s  defined product
rule probabilities  for conciseness  drop subscript on  

p t    s 
p r   s 
   
rd

nevertheless complex tree to tree mappings still result extremely large number
rules  causes computational problem degenerate analysis
data   control number parameters reasonable level  decompose
rule probability simpler probability factors independence assumptions 
first assume generation rule r independent input tree s 
conditioned source language side rule  is 
p r   s  p r   sr  

   

note strong assumption generation synchronous grammar
rule depends source language side  similar used statistical
modeling machine translation  brown et al         koehn  och    marcu        galley
et al         chiang        generation atomic alignment translation units
conditioned associated source language words tree fragments  rather
whole input sentence tree  smt  independence assumptions based phrases
translation rules generally used decompose parallel corpus manageable units
parameter estimation  successfully used modern smt
systems  adopt similar assumption ease parameter estimation process
model 
decompose p r   sr   additional assumptions  since r   hsr   tr   r i 
p r   sr   written another form using chain rule 
   degenerate analysis refers case using models complex results overfitting
poor generalization ability unseen data 

   

fiunsupervised sub tree alignment tree to tree translation

p r   sr     p sr   tr   r   sr  
  p r   sr   tr   p tr   sr  

   

equation     indicates two sub models  including reordering model frontier nonterminals p r   sr   tr    tree fragment translation model p tr   sr   
model p r   sr   tr    view frontier non terminal reordering problem aligning
elements two vectors non terminals  let vnt   function returns
vector leaf non terminals given tree fragment  r defines   to   alignment
non terminals vnt sr   vnt tr    example  r  figure   a  
frontier non terminal vectors sr  tr  are 
vnt sr       nn  ad  vv  as 
vnt tr       dt  nns  rb  vbn 
r                         indicates alignment vnt sr    vnt tr     say 
nn aligned nns  ad aligned rb on  opt simple model
selecting r   models non terminal reordering probability condition
frontier non terminal vectors language sides  follows 
p r   tr   sr   preorder  r   vnt sr    vnt tr   

   

turn problem modeling tree fragment translation p tr   sr    i e  
second sub model defined equation       define tree fragment consists
two parts  words lex    i e   terminals   tree structure tree   without lexicons
involved  example  r  figure   a   target language tree fragment contains
two elements lex tr    tree tr    
lex tr     
tree tr      s np dt x nns x  vp vbp advp rb x vbn x   
let root   function returns root given tree fragment  write
p tr   sr   as 
p tr   sr     p lex tr    tree tr     sr  
  p root tr     sr  
p tree tr     root tr    sr  
p lex tr     tree tr    root tr    sr  

   

worth noting equation     approximation  choose
one many ways p tr   sr   written product series
   reordering model defined ensures arbitrary   to   alignments handled  might
result large model sparse parameter distributions big tree fragments involved 
considering issue  choose several pruning methods better control rule size sub tree
alignment system  see section       pruning settings work 

   

fixiao   zhu

conditional probabilities  simply assert equation generating targetlanguage tree fragment source language tree fragment  first choose root
symbol target language tree fragment given source language tree fragment  in
probability p root tr     sr     choose tree structure target language
tree fragment given root symbol source language tree fragment  in probability
p tree tr     root tr    sr     choose target language terminals associated
tree fragment given target language tree structure  target language root symbol
source language tree fragment  in probability p lex tr     tree tr    root tr    sr    
another note equation     actually reduce model complexity 
example  p lex tr     tree tr    root tr    sr   essentially indicates combinations source
target language tree fragments  simpler model required feasible solution
parameter estimation  this  introduce additional assumptions relax
conditions probabilities reduce number parameters reasonable level 
   p root tr     sr   depends root sr    i e  
p root tr     sr   pnt  root tr     root sr   

   

assumption implies node correspondence source targetlanguage parse trees 
   p tree tr     root tr    sr   depends root tr    i e  
p tree tr     root tr    sr   ptree  tree tr     root tr   

    

second assumption results monolingual model generating target language
tree structures  generation tree fragment conditioned
root  viewed analogy generative model used standard tsgs 
   p lex tr     tree tr    root tr    sr   depends source words lex sr    i e  
p lex tr     tree tr    root tr    sr   plex  lex tr     lex sr   

    

allows us directly model terminal correspondence two languages 
then  substitute equations          equation      get
p tr   sr   pnt  root tr     root sr   
ptree  tree tr     root tr   
plex  lex tr     lex sr   
using equations                equation     finally written as 
   

    

fiunsupervised sub tree alignment tree to tree translation

id

rule

probability

r 

ad   rb drastically 

pnt  rb   ad  plex  drastically    

r 

vv   vbn fallen 

pnt  vbn   vv  plex  fallen    

r 

as   vbp have 

pnt  vbp   as  plex  have    

r 

nn  

pnt  np   nn  plex  the imports    

np dt the  nns imports  

ptree  np dt nns    np 

vp ad  vp vv  as    

pnt  vp   vp  ptree  vp vbp advp rb vbn     vp 

vp vbp  advp rb  vbn    

preorder                   ad  vv  as    vbp  rb  vbn  

ip nn  vp    s np  vp   

pnt  s   ip  ptree  s np vp    s 

r 
r 

preorder              nn  vp    np  vp  

table    rule probabilities sample derivation    r    r    r    r    r    r    figure   b 
p t    s 



pnt  root tr     root sr   

rd

ptree  tree tr     root tr   
plex  lex tr     lex sr   
preorder  r   vnt sr    vnt tr   

    

simplified model generative story described section  takes
rule generation probability product four probability factors  pnt    nonterminal mapping probability  roughly captures syntactic correspondence subtrees two languages  ptree    probability generating tree structure
  plex    probability terminal mappings two language sides
rule  preorder    probability frontier non terminal reordering encoded
rule  see table   rule probabilities sample derivation 
model parameters assumed multinomial distributions  calculation pnt     ptree    preorder    straightforward  directly used
without decompositions assumptions  calculate plex     choose
form adopted popular models word alignment  och   ney        thayer  ettelaie  knight  marcu  munteanu  och    tipu         probability defined
product word based translation probabilities 
l


  x
plex  t     tl   s     sm   plength  l   m 
pw  ti   sj  

i  

    

j  

ti target word  sj source word  plength    used control number
target words produced given number source words  pw    word translation
probability  sub model principle something rather similar conventional
word based translation tables  ibm models  brown et al         
    node deletion insertion
word  or sub tree  deletion insertion common real world alignment translation
tasks  add flexibility modeling problem  allow production empty
   

fixiao   zhu

sub trees either source target language side rule model  formally 
rule whose target language side empty sub tree  probability defined as 
p r   s  pnt  root     root sr   
ptree  tree     root   
plex  lex     lex sr   
preorder     vnt sr    vnt   

    

special symbol indicates nothing  factors pnt  root     root sr   
plex  lex     lex sr    model deletion probability different levels tree fragment 
ptree  tree     root    probability generating empty tree fragment  factor
preorder     vnt sr    vnt    regards special reordering pattern aligns
frontier non terminals source side virtual node null  obviously  values
ptree  tree     root    preorder     vnt sr    vnt    simply   
similarly  rule whose source side empty sub tree  probability defined as 
p r   s  pnt  root tr     root   
ptree  tree tr     root tr   
plex  lex tr     lex   
preorder     vnt    vnt tr   

    

value preorder     vnt    vnt tr      
worthwhile note word deletion insertion problems important
mt spite relatively less discussion recent studies tree to tree translation 
actually analogy null alignment used ibm models
 brown et al          word phrase based models  removing words alignment
leave space correctly aligning words sentence   even
necessary    to    sub tree alignment alignment respect syntactic
constraints language sides  e g   sub tree alignments allowed break
constraints imposed neighbouring parts tree  cases  cannot obtain
correct   to   alignment tree pair due one two bad nodes
necessarily aligned valid node counterpart tree  instead  nodes
skipped alignment thus impose bad constraints parts
tree node deletion insertion allowed  especially true align sentence
pairs flat tree structures free translations  work found node
deletion insertion operations necessary achieve satisfactory sub tree alignment
result  therefore used implementation default 
    training
turn training problem  discussed section      focus unsupervised
learning model parameters  is  optimal values parameters estimated given
   note current phrase based approaches  koehn et al         och   ney        allow null aligned
words appear boundary phrase  viewed way implicit modeling
word insertion deletion problem

   

fiunsupervised sub tree alignment tree to tree translation

collection tree pairs without annotation sub tree level alignment  work
choose two approaches estimating parameters sub tree alignment model  including
maximum likelihood estimation  mle  approach bayesian approach 
      maximum likelihood training
mle one popular methods parameter estimation statistical models 
basic idea that  given model set parameters  mle method selects values
parameters generate distribution gives highest probability observed
data  mle general approach parameter estimation widely adopted
many ai nlp tasks  part of speech tagging  case sub tree alignment 
mle simply described finding optimal values parameters lead
maximum probability aligning tree nodes source language parse tree
target language parse tree  formally  given set tree pairs   s    t           sn   tn    
objective mle based training defined be 
  arg max


n


x

p  ti     si  

    

i   dd si  ti  

choose expectation maximization  em  dempster  laird    rubin       
algorithm solve optimization problem  basically em algorithm
iterative training method finding maximum likelihood estimates model parameters 
assumed observed data depends latent variables  algorithm
performs iteratively calling two sub routines  namely expectation  e  step
maximization  m  step  e step  calculates expected value likelihood
function associated parameters observed data  respect distribution
latent variables given observed data current estimates parameters 
m step  seeks parameters maximize expected likelihood found
e step 
applying em algorithm case  view input pairs parse
trees   s    t           sn   tn    observed data  underlying derivations rules latent
variables  distributions pnt     ptree     preorder    plex     i e   plength   
pw     unknown parameters  see figure   pseudo code training algorithm
pnt     denoted tnt  snt    algorithm directly applicable estimation
parameters model  skip description learning remaining parameters
here  detailed description em based training model parameters
refer reader appendix 
algorithm  snt tnt represent source language non terminal symbol
target language non terminal symbol  u v represent source language tree node
target language tree node  ec   represents expected count given variable 
p k   t    s  represents derivation probability based parameters obtained
k th round  em iteration  e step algorithm accumulates expected
count pairs parse trees  then  m step finds maximum likelihood estimate
using quantity  nontrivial part algorithm computation
expected count e step  roughly speaking  physical meaning right hand
side line   relative probability derivation contains rule r  with root node
   

fixiao   zhu

   function trainmodelwithem    s    t           sn   tn    
   
   set  tnt  snt     initial model
   k     k  
  
foreach non terminal symbol pair  snt   tnt  
  
ec tnt  snt      
   e step 
  
foreach tree pair  s    sequence   s    t           sn   tn   
  
foreach node pair  u  v  symbol pair  tnt   snt    s   
  
foreach rule r rooted
p  u  v 
p

rooted  u v 
  
ec tnt  snt       d  rd r p
 
d  p  k   t d  s 
   m step 
   
foreach non terminal symbol pair  snt   tnt  

   
   

 k   

tnt  snt  

p

 k   t d s 

ec tnt  snt  
ec t   s  

t nt

nt

nt

 k 

return  tnt  snt  
figure    em based training algorithm  for pnt    

p
pair  u  v    numerator d  rd r rooted  u v  p k   t    s  probability sum
p
derivations involve r   denominator d  p k   t  d    s 
overall probability alignment   however  brute force computation
expected counts inefficient requires sum possible derivations
whose number exponential length input sentences 
work use bilingual version inside outside probabilities  manning
  schutze        avoid naive enumeration possible derivations computing
various probabilities  inside probability  u  v   denoted  u  v   measures
likely generate sub tree pair inside node pair  u  v   outside probability
 denoted  u  v   dual inside probability  measures likely generate
remaining parts tree pair  s    start symbols  formulation
used monolingual parsing  manning   schutze          u  v   u  v  defined
using following recursive forms 


x

 u  v   
p r   s 
 p  q 
    
r root r   u v 

 u  v   

x

 p q yield r 



 root r   p r   s 

r  u v yield r 



 p  q 



    

 p q yield r 
 p q    u v 

root r  abbreviation node pair  root sr    root tr     yield r 
set aligned frontier non terminal pairs yielded r  based recursive
definitions   u  v   u  v  efficiently computed dynamic programming 
using inside outside probabilities  easy address computation
problem mentioned above  let  u  v  denote probability tree node u aligned
tree node v  probability expressed inside outside fashion 
   

fiunsupervised sub tree alignment tree to tree translation

 u  v   

x

p t    s 

d   u v 
aligned

   u  v   u  v 

    

way  overall alignment probability  i e   denominator
right hand side line    simply written as 
x

p t    s     root s   root t     root s   root t   

    



numerator right hand side line    let us view another angle 
e step algorithm  expected count accumulated rules whose root  u  v  
rules rooted  u  v  indicate node alignment u v  lines
    principle imply probability derivations aligning u v  precisely
node alignment probability  u  v   probability written simple
form using inside outside probabilities 
x
x
p t    s     u  v 
r  r rooted d  rd
 u v 

   u  v   u  v 

    

together result equation       e step efficiently implemented
replacing lines     following equation
ec tnt  snt      

 u  v   u  v 
 root s   root t     root s   root t   

    

 snt   tnt   symbol pair  u  v   note  snt   tnt    e step step
 u v  u v 
increases ec tnt  snt   sum  root s  root t
   root s  root t    node pairs  u  v 
whose symbols snt tnt   means  snt   tnt   aligned different positions
input tree pair  method considers alignment  snt   tnt   multiple
times updates ec tnt  snt   accordingly 
worth noting several methods initializing model parameters em style training begins  example  model initialized
uniform random distributions  work initialize parameters sub tree
alignment model model obtained using word alignment result  standard way adopted many unsupervised models simpler model used good
starting point training process  helpful optimization procedure
sensitive initial setting model parameters  e g   em non convex objective functions   experiments found using giza   word alignment parameter
initialization resulted better performance fewer iterations convergence
uniform initial distributions  word alignment obtained unsupervised
manner  change training condition approach  thus chose
method initializing model parameters implementation 
   

fixiao   zhu

      bayesian approach
mle one standard approaches training unsupervised models  well
known tendency overfit data  overfitting problem becomes severe
complex models since parameters fit training data better 
case stsgs  likely result degenerate analysis data  i e   rare
big rules dominate ml solution stsgs  considered noisy
generalize poorly unseen data  cohn   blunsom        liu   gildea        
natural solution problem incorporate constraints proper priors
training process  take bayesian approach alternative solution
training problem 
unlike mle  bayesian approach plug single optimum point estimate
parameter distribution data point  instead account uncertainty
value parameter  bayesian models  parameters assumed
drawn probability distributions priors  parameters extra prior
distributions called hyperparameters  denoted   parameters
model viewed mathematically multinomials  choose dirichlet distributions
 ferguson        prior model parameters  advantage using dirichlet
distributions conjugate multinomial distributions inference
priors easier 
following previous description  use denote model parameters
multinomial outcomes          k   i e   k probability outcome k          k   
multinomial distribution sample set outcomes  x         xn   probability
p xi   k    k   dirichlet prior distribution multinomials  sample
prior actually set parameter values   therefore distribution
modeled as 
xi   multinomial  

    

  dirichlet  

    

equation      means xi distributed according multinomial parameters
  similarly  equation      read distributed according dirichlet distribution parameters               k   hyperparameter vector corresponding
outcomes  work use symmetric dirichlet prior   i e            k share
value   use represent single hyperparameter instead hyperparameter
vector 
using model  compute conditional distribution new observation
xn   given previous observations  x         xn   hyperparameter   follows 
z
p xn     x         xn       p xn     x         xn     p     
    
big advantage bayesian approach introduce prior distribution
unknown parameters model  meant capture knowledge beliefs
model seeing data  neal         especially important case
need bias towards preferred situations  example  expect
model favor high frequency rules dislike rare big rules  goal
   

fiunsupervised sub tree alignment tree to tree translation

easily achieved using bayesian approach appropriate choice priors  say 
dirichlet prior low concentration parameter   however  introduction priors
generally makes intractable estimate posterior analytically  practical systems
based bayesian approach  widely used solution use approximate methods
seek compromise exact inference computational resources  work
choose variational bayes approximate inference  variational bayes good method
preserves benefits introducing prior tractable inference procedure
 attias        beal         successfully applied several nlp related models 
hidden markov models  hmms  ibm models  beal        riley   gildea 
       one good thing variational bayes seen extension
em algorithm resembles usual forms used em  resulting procedure looks lot
em algorithm modified m step  convenient implementation 
follow approach presented previous work  beal        riley   gildea       
variational bayesian algorithms applied similar tasks  need
slight change m step original em algorithm presented section       
original em algorithm  see figure     m step normalizes expected counts
collected e step standard mle  variational bayesian version m step
slightly modifies formula performs inexact normalization passing counts
function f  x    exp  x   

tnt  snt  

f  ec tnt  snt      
p
f   t   ec t nt  snt       

    

nt

 x  digamma function  johnson         approximate effect
subtracting     argument  choice controls behavior estimation 
set low value  performs estimation way anti smoothing 
    subtracted rule counts  small counts corresponding rare events
penalized heavily  large counts corresponding frequent events affected
much  example  low values make equation      favor non terminal pairs
aligned frequently distrust non terminal pairs aligned rarely 
way  variational bayesian method could control overfitting caused abusing
rare events  hand  larger used smoothing required 
method applicable training parameters model 
requires replacement m step figure   variational bayesian m step  as
equation        implementation  variational bayes based training 
perform additional round normalization without variational bayes normalize rule
probabilities sum one  
   additional normalization process makes posterior probabilities directly comparable
obtained training methods  em based training  note convert result
bayesian inference probability distributions good explanation various probability
factors model  hand  technical trick results pseudo bayesian procedure
bayesian inference exactly though shows good results empirical study  one
remove additional round normalization pure bayesian approach  changes
affect overall pipeline approach  from practical standpoint  

   

fixiao   zhu

   function decode
 s   

  
         getinsideoutsideprobabilities  s   
  
foreach node u bottom up order
  
foreach node v bottom up order
  
 u  v     u  v   u  v 
  
foreach tree fragment sr rooted u
  
foreach tree fragment tr rooted v
  
foreach frontier non terminal alignment sr ts
  
r   createrule s
q r   tr   a 
   
score   p r   s   p q yield r  p d p  q  
   
score   p d u  v  
   
d u  v    createderivation r   d p  q     p  q  yield r   
    return  d       
    function getinsideoutsideprobabilities  s   
    foreach node u bottom up order
   
foreach node v bottom up order
   
set  u  v  according equation     
    foreach node u top down order
   
foreach node v top down order
   
set  u  v  according equation     
    return         
figure    decoding algorithm proposed sub tree alignment model   best
posterior based outputs
    decoding
inference model straightforward  simplest case inferring   best subtree alignment  given set learned parameters  first visit every node pair  u  v 
bottom up fashion  compute posterior probability aligning sub tree pair
rooting  u  v   procedure dynamic program used trainer 
select derivation maximum sub tree alignment probability
input tree pair  also  generate list k best derivations similar manner 
addition   best k best output  model able output alignment
posterior probability every pair tree nodes  this  need record
probability  u  v  node pair obtain inside outside probabilities 
note outputting alignment posterior probabilities commonly used statistical
word phrasal aligners  provides flexible way making use alignment result
downstream components  rule extraction system  presented
next sections  tree to tree mt systems make great benefits posteriorbased alignment output  results effective rule extraction method well
better translation results 
figure   depicts pseudo code decoding algorithm   best posteriorbased outputs  algorithm  d x  y  data structure records best derivation rooted  x  y    x  y    x  y   x  y  data structures record inside
probabilities  output probabilities alignment posterior probabilities  respectively  cre   

fiunsupervised sub tree alignment tree to tree translation

aterule   creates rule pair tree fragments  sr   tr   frontier non terminal
alignment a  calculates rule probability  createderivation   builds derivation
using input rules  output  access   best alignment traversing
d root s   root t     access alignment posterior    
given pair trees  s     outer two loops algorithm iterates pair
nodes two trees  resulting time complexity o  s   t        represents
     n
size input tree  generating pairs tree fragments requires o ntree
tree
maximum number tree fragments given tree node  computing alignment
 sr   tr   requires o l   l maximum number leaf non terminals
 
rule  therefore time complexity algorithm o  s   t   ntree
l    quadratic
size input trees  note actual time complexity algorithm could
high potential alignments considered  example  ntree generally
exponential function depth input tree fragment  deep tree could
results extremely large space alignments  make practical sub tree alignment
systems  pruning techniques taken account work  example 
implementation  restrict depth tree fragment reasonable number  see section
        addition  commonly used phrasal alignment related tasks  consider
word alignments pruning discard sub tree alignments violate certain
number word alignments  example  throw away sub tree alignments
two word alignment links outside spans covered aligned sub trees 

   applying sub tree alignment tree to tree translation
sub tree alignment obtained  current tree to tree systems directly learn translation rules node aligned tree pairs  section investigate methods applying
sub tree alignment tree to tree rule extraction 
    rule extraction using   best k best sub tree alignments
data  several methods developed tree to tree rule extraction  zhang et al  
      liu et al       a  chiang         popular ghkm like
method extends idea extracting syntactic translation rules string tree pairs
 galley et al          ghkm like extraction  first compute set minimallysized translation rules explain mappings source language tree
target language tree respecting alignment reordering
two languages  larger rules learned composing two minimal rules 
example  figure   b   r  r  two minimal rules extracted according sub tree
alignment  compose rules form larger rule  this 
ip nn   vp    s np dt the  nns imports   vp   
work use tree to tree version ghkm like extraction described
liu et al s      a  work  see figure   a  pseudo code rule extraction
  best sub tree alignment  choose method widely used
tree to tree systems  note rule extraction tree to tree translation generally
restricted performed   best sub tree alignment result  ghkm like
   

fixiao   zhu

   function onebestextract  s    a 
  
foreach node u
  
foreach node v
  
foreach tree fragment pair  sr   tr  
  
rooted  u  v 
  
  onetoonealign sr   tr   a 
  
empty
  
r   createrule sr   tr   a 
  
rules add r 
  
return rules
    function onetoonealign sr   tr   a 
    frontier non terminals  sr   tr  
   
  to   alignments
   
return frontier alignment  sr   tr  
    else
   
return

   function matrixextract  s     
  
foreach node u
  
foreach node v
  
isextractable   u  v     
  
next loop
  
foreach tree fragment pair  sr   tr  
  
rooted  u  v 
  
foreach frontier alignment
  
 sr   tr  
  
isextractable a   
  
r   createrule sr   tr   a 
   
rules add r 
    return rules
    function isextractable a   
    foreach alignment  p  q 
   
probability  p  q    pmin
   
return false
    return true

 a    best extraction

 b  matrix based extraction

figure      best matrix based rule extraction algorithms
extraction method employed list k best sub tree alignments provided 
k best extraction need repeat procedure   best extraction
sub tree alignment k best list 
    rule extraction using sub tree alignment matrices
previous work pointed current mt systems suffer error propagation due
alignment errors made within   best alignment  venugopal  zollmann  smith   
stephan         sub tree alignment early stage step training pipeline 
errors   best alignment likely propagated translation rule extraction
parameter estimation translation model  though problem alleviated
using k best alignments  limited scope k best alignments still results inefficient
learning translation rules  example  preliminary experiment shows      
extracted rules redundant     best alignments involved 
instead present simple efficient method  namely matrix based rule extraction  method  use posterior based output aligner represent
sub tree alignment compact structure   call sub tree alignment matrix alignment
matrix short  liu  xia  xiao    liu      b  de gispert  pino    byrne        
see figure   a  two example sub tree alignment matrices made pair sentence segments  matrices  entry indexed pair source target nodes 
score entry posterior probability alignment corresponding node pair  i e    u  v  probability defined equation       probability
straightforwardly accessible output inference algorithm described section
     principle  u  v  viewed measure sub tree alignment confidence  higher
value indicates confident alignment two nodes  way
   

fihave

rb

   

vbn

drastically



 

fallen





vv   

as   

ad   

  

as   

  

  

vp   

  

ad   

  

  

  

  

  

  

vp   

  

  

vv   

  

as   

  vv   
 

vb
n 

  

vb
p    
ad
vp  
  
rb  

  

ad   
vp   

vp   
vp   

vp   

 

   

vp  

advp   

vb

vbp   

vp  

  

vp   

p    
ad
vp  
  
rb  
  
vb
n    

unsupervised sub tree alignment tree to tree translation

  

  fixed alignment

  possible alignment

matrix      best alignment

matrix    posterior

 a  sub tree alignment matrices sample sub tree pair
minimal rules
extracted matrix    posterior 
r 
ad   rb drastically 
r 
vv   vbn fallen 
r 
as   vbp have 
r 
vp ad  vp vv  as    
vp vbp  advp rb  vbn    
r   vp vv   as    vbn fallen 
r   vp ad  vp    vp vbp  advp   

minimal rules
extracted matrix      best 
r  ad   rb drastically 
r  vv   vbn fallen 
r  as   vbp have 
r  vp ad  vp vv  as    
vp vbp  advp rb  vbn    

   
 b  rules extracted using   best alignment alignment posterior
figure    matrix based representation sub tree alignment sample rules extracted 
matrix   shows case   best sub tree alignment  matrix   shows
case sub tree alignment posterior 
access possible sub tree alignments  with different probabilities   rather limited
number them 
extract rules using sub tree alignment matrix  method simple 
collect rules associated entry matrix  core algorithm
method essential used   best k best extraction  difference
  best k best extraction matrix based method considers possible node
pairs extraction  rather visiting only  see figure   b  pseudocode sub tree alignment matrix based rule extraction algorithm  represents
sub tree alignment matrix pair trees  s     compared extracting rules
k best alignments  method efficiently obtain additional rules whose extraction
blocked k best extraction  example  right side figure   b  two new rules
r   r   extracted  cannot obtained   best alignment result 
prevent extraction great number noisy rules low alignment probabilities 
   

fixiao   zhu

prune away rules whose alignment probabilities pre specified threshold 
formally  given pair nodes  u  v   rule extraction executed  u  v 
satisfies 
 u  v 
  pmin
    
 root s   root t   
expression measures relative probability alignment  u  v  respect
sum probabilities possible derivations  pmin empirical threshold control
often rules pruned  a larger pmin means rules thrown away  
work  set     default  therefore  entries zero score figure
  a   denoted dot  excluded rule extraction 
however  discarding rules relatively low probabilities turn results incompleteness problem  is  extracted rules might unable transform given source
parse tree  even training set  nonetheless  problem severe
case  experiments observed parse tree pairs  over      training
corpus could recovered extracted rules pmin chose default value 
contribution translation accuracy low confidence rules limited
 generally less     bleu points  
another note sub tree alignment matrix based extraction  advantage
method follows general well developed framework syntax based mt 
i e   word syntactic alignment   rule extraction parameter estimation   mt decoding 
need replace rule extraction component sub tree alignment matrixbased system  preserve components pipeline  means still
use heuristics obtain additional useful rules result sub tree alignment
matrix based extraction  rule composing  galley et al         spmt extraction
 marcu  wang  echihabi    knight         also  posterior probability encoded
matrix used better estimation various mt oriented features   
note basis approach stsg model  rules sub tree
alignment model resemble general forms translation rules used tree to tree mt
systems  so  alternative simple way rule induction  directly infer translation rules sub tree alignment model take corresponding rule probabilities
features translation model mt decoding  however  tree to tree mt
method suffers several problems  first  sub tree alignment model requires computation possible aligned tree fragments  results high time complexity
training decoding procedures  result  aggressive pruning used
reasonable size search space  e g   consider relatively small tree fragments
implementation acceptable running speed  side effect  many relatively large
rules  e g   composed rules spmt rules  absent sub tree alignment model 
available use traditional alignment   extraction heuristics pipeline 
engineering standpoint  efficient directly infer translation rules
sub tree alignment model  compared inferring rules using pruned fixed subtree alignment matrix plus heuristics  second  rule probability optimization
objective sub tree alignment different used mt systems  example  use generative model maximum likelihood bayesian approach sub tree
    see section     detailed discussion parameter estimation issue 

   

fiunsupervised sub tree alignment tree to tree translation

alignment  use discriminative model minimum error rate training mt  many
features employed mt decoder considered sub tree alignment model 
issues might lead unsatisfactory mt performance  shown experiments  see section         directly inferring translation rules sub tree alignment
model achieve promising results 
    learning features machine translation
previous work syntax based mt  proved syntax based systems make great
benefits mt oriented features  even necessarily well explained
syntactic parsing viewpoint  e g   phrase based translation probabilities   however 
features available word sub tree alignment model  instead
need learn features using additional step parameter estimation mt 
this  follow commonly used framework estimates values various mtoriented features extracted rule set using mle  procedure simple 
translation rules extracted  obtain maximum likelihood  or relative frequency 
estimate parameters according definition feature function 
however  traditional tree to tree systems rule extracted tree pair
count unit one  used calculate values various features 
approach might enlarge influence noisy rules extracted sub tree alignment
matrices  e g   rule high alignment probability equal weight rule
low alignment probability  thus unreasonably large impact mt systems 
desired solution rule extracted derivation low probability
penalized accordingly feature learning  motivated idea  use fractional counts
estimate appearance rule  mi   huang         given node pair  u  v 
 s     alignment probability rule r rooted  u  v  defined  denoted
 r  u  v   
x
 r  u  v   
p t    s 
    
dd s t  
rd

 r  u  v  regarded probability sum derivations involving r  u  v  
also  rewrite equation      inside outside fashion 

 r  u  v     u  v 
 p  q  p r   s 
    
 p q yield r 

define probability r involved derivations  s    as 
x
 r   
 r  u  v 

    

u v

equation      sum probabilities r node pairs  means
rule probability considered multiple times particular derivations contain
r once  using  r   fractional count r defined be 
c r   

 r 
 root s   root t   

   

    

fixiao   zhu

equation      reflects probability likely r involved derivation given
pair trees  set bilingual parse trees  c r  accumulated tree pair 
obviously  c r  used estimate parameters mt model  is 
translation rules weighted  parameter estimation procedure proceed usual 
weight counts  work c r  employed learn five features used
mt decoder  including bi directional phrase based conditional translation probabilities
 marcu et al         three syntax based conditional probabilities  mi   huang        
let    function returns sequence frontier nodes input tree fragment 
probabilities computed following equations 
p
  
r     sr      sr   tr      tr   c r  
p
pphrase  tr   sr    
    
 
r    sr     sr   c r  
p
  
r     sr      sr   tr      tr   c r  
p
pphrase  sr   tr    
    
 
r    t      tr   c r  
r

c r 

p r   root r    

p

p r   sr    

p

r   root r    root r  c r

c r 
r   sr   sr

p r   tr    

c r   

c r 
p

r   tr   tr

c r   

  

    
    
    

   experiments
evaluation  first experimented approach chinese english sub tree
alignment task  tested effectiveness state of the art tree to tree mt system 
    baselines
three unsupervised sub tree alignment methods chosen baselines experiments 
wordalign     wordalign   based ghkm like method  galley et al        
uses word alignments infer syntactic correspondences  implementation 
giza   toolkit grow diag final and method used obtain
symmetric word alignment sentence pairs  sub tree alignments
heuristically induced selecting node correspondences consistent
word alignment result  i e   sub tree alignments violate word
alignments   chose method widely adopted modern
tree to tree systems 
wordalign     second baseline essentially wordalign   
difference wordalign   improved word alignment system using
link deletion techniques  fossum et al          basic idea delete harmful
alignment links initial word alignment result  e g   deleting link
figure   a    experiments considered likely
deletion top    common chinese words  including         
   

fiunsupervised sub tree alignment tree to tree translation

             top    common english words  including  the 
of  and  to  in  a  is  that  for  on   
heuristicalgin  heuristicalgin re implementation approach proposed
tinsley et al s        work  method alignment confidence every node
pair first computed lexical translation probabilities  used obtain
node correspondences via heuristic algorithm  method require
training process successfully adopted several translation tasks 
french english translation  chosen another baseline comparison 
    experimental setup
settings experiments described follows 
      data preparation
bilingual corpus consists      million sentence pairs    mentioned above 
used giza   grow diag final and heuristics generate   best k best word
alignments  used baseline word alignment results  parse trees
chinese english generated using berkeley parser    publicly available
corpus used evaluate sub tree alignment result    consists     node aligned
sentence pairs  with gold standard parse trees language sides  ldc    e  
included bilingual data  corpus divided two parts 
held out set used finding appropriate setting hyperparameters     sentences
articles           test set used evaluating sub tree alignment systems     
sentences articles           mt experiments    gram language model trained
xinhua portion gigaword corpus addition english part ldc
bilingual training data    used nist      mt evaluation corpus development
set      sentences  newswire portion nist           mt evaluation corpora
test set        sentences  
      sub tree alignment
parameters sub tree alignment model initialized add one smoothing
rule set extracted using word alignments  i e   wordalign   baseline   then 
model trained parse trees bilingual corpus using em algorithm
variational bayes  vb  approach  implementation vb based training 
hyperparameters assumed share value    leads setting       
    ldc category  ldc    e    ldc    t    ldc    e    ldc    t    ldc    e    ldc    e   
ldc    e    ldc    e    ldc    e   ldc    t    see http   www ldc upenn edu 
details 
    note ldc    e   corpus reused gold standard parse trees provided chinese
english treebanks 
    available http   www nlplab com resources nodealigned bitreebank html
    ldc category english gigaword corpus  ldc    t  
    although could adopt different hyperparameters finer control priors model parameters  found setting hyperparameters value could lead satisfactory
performance 

   

fixiao   zhu

optimal value held out set  default  trained model   em
variational em iterations  speed up training process avoid degenerate
analysis caused large rules  restricted rules reasonable sizes rules five frontier non terminals depth three  rules
five frontier non terminals  considered tree fragments depth one
restrict number frontier non terminals involved  is  flat tree structures 
used associated height one tree fragments  besides  discarded sub tree
alignment every node pair whose terminals aligned outside corresponding
spans two times wordalign   
      machine translation
used niutrans open source toolkit  xiao  zhu  zhang    li        build
tree to tree mt system  rule extraction  used extension ghkm method
extract minimal tree to tree transformation rules  liu et al       a  obtained larger
rules composing two three minimal rules  galley et al          used cky style
decoder cube pruning  huang   chiang        beam search decode chinese
sentences  default beam size set     addition features described
equations            used several features mt system  including
  gram language model  rule number bonus  target length bonus two binary
features   lexicalized rule low frequency rule  marcu et al          features
combined log linear fashion optimized using minimum error rate training  mert 
och        
    results
following part section  present experimental results  including evaluations sub tree alignments  extracted rules  mt systems  also  show results
several improved methods effective use approach tree to tree mt 
      evaluation alignments
first evaluated alignment quality various sub tree alignment approaches terms
precision  p   recall  r  f   score    see table   results three baseline
systems sub tree alignment system  measures  vb based system
significantly improves overall recall f   score  slightly degrading precision
compared wordalign      also  vb based training outperforms em based counterpart due priors introduced learning process  interesting observation
that  though em training model suffers degenerate analysis
data  show extremely bad results experiment  phenomenon due
restriction size tree fragment training  described section       
restricted translation rules reasonable size tree fragments several ways  e g  
    let predicted number alignments system output  correct number correct alignments
system output  gold number alignments gold standard  measure precision  recall
 
 precisionrecall
correct
f  score defined as  precision   predicted
  recall   correct
f       
 
gold
  precision recall
parameter controls preference recall  i e        precision  i e          
nlp tasks set    indicating equal weights recall precision 

   

fiunsupervised sub tree alignment tree to tree translation

entry
overall
np np
nn nn
vp vp
pu  
ip
pu  
np nn
np pp
nn nns
nr nnp
nn np
pp pp
nn jj
p
qp np

wordalign  
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

wordalign  
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

heuristicalgin
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

 em 
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

 vb 
p
r
f  
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              
              

table    evaluation results sub tree alignment system baselines 
measures reported percentage 
set parameter maximum depth   constraints reduce number rules
involved training  prevents use rare large rules  result indicates
fact tree fragment size constraint actually important efficiency
crucial learning  discussed previous work  without constraints
imposing proper prior  solution em degenerate  marcu   wong        denero 
gillick  zhang    klein        
addition  table   shows result    common types sub tree alignment  expected  vb based system achieves best f   score cases 
interestingly  observed approach obtains significantly better performance
handling pp  prepositional phrase  alignment seems difficult problem
baselines due unclear boundary indicators aligning pp structures  attribute
better use syntactic information language sides model 
generally ignored traditional models based surface heuristics word alignments 
      evaluation extracted rules
applied sub tree alignment result tree to tree system study impact
sub tree alignment mt  discussed section    rule extraction downstream
component sub tree alignment current tree to tree mt pipeline  therefore
chose evaluate quality rules obtained various sub tree alignment results 
determine goodness extracted grammars  computed rule precision  recall 
f   scores approach baseline approaches test set used
   best  alignment quality evaluation  make gold standard grammar  chose
method used fossum et al s        work grammar automatically
generated manually annotated alignment result  is  rules extracted using
annotated sub tree alignments regarded gold standard computing various
evaluation scores  table   shows evaluation result grammars extracted
   

fimatrix

 best

xiao   zhu

entry
wordalign  
wordalign  
heuristicalgin
 em 
 vb 
 vb   pmin
 vb   pmin
 vb   pmin
 vb   pmin

       
       
       
       

rule p
    
    
    
    
    
    
    
    
    

rule r
    
    
    
    
    
    
    
    
    

rule f  
    
    
    
    
    
    
    
    
    

table    evaluation results rules obtained various sub tree alignment approaches 
measures reported percentage 
different sub tree alignment approaches  see improvements persist
sub tree alignments employed translation rule extraction  vb based approach
produces grammars higher rule f   score three baselines 
addition   best extraction  studied rule extraction behaves
sub tree alignment matrix based extraction method  table   shows result
sub tree matrix based extraction method different choices pruning parameter
pmin   see smaller values pmin result grammars higher rule recall  also 
better rule f   scores achieved adjusting pmin seeking good balance
rule precision rule recall   e g   pmin            
scores informative measure grammar quality  investigated differences rule sets obtained model compared
baseline approaches  following levenberg  dyer  blunsoms        method  figure  
shows probable rules  frequency    obtained bilingual corpus using
vb based alignment approach appear model wordalign  
alignment vice versa  asked two annotators sub tree alignment estimate
rule quality based syntactic correspondence adequacy frontier node sequence
two languages sides  rule labeled good judges considered good quality  figure  see eight top    rules extracted
using approach absent wordalign   grammar good rules  contrast 
four top    rules baseline model good quality sense human
preference  furthermore  examined top     probable rules appear
two grammars individually  again  top     rules extracted using proposed model
better quality  results    good rules  contrast      top ranking
rules induced using wordalign   alignment good translation rules 
      evaluation translations
evaluated translations generated mt system different sub tree alignment approaches  since vb based training shows best performance previous
experiments  chose default setting approach following experiments 
table   shows evaluation result translation quality estimated using caseinsensitive ibm version bleu   papineni  roukos  ward    zhu        ter  snover 
   

fiunsupervised sub tree alignment tree to tree translation


  
  
  
 
  
  
  
  
  
  

top    highest probability rules  for mt  approach absent wordalign  
np dnp  nn    vp advp  vp vbd improved   
np pu    np cd   nn    pu    np dt the  cd two  nns sessions  
np np qp  np nn     np    np x  np cd  np nns represents    
np nn   np    vp vb desire  nnp   
np pu   np nr  nn    pu    np    np nnp  nn independence      
np vp  dec   np nn   nn    
np adjp adjp  jj ideological   nn struggle  
np np qp  np    np adjp jj    np nn     
np np dt the  jj important  nn thinking   in of  sbar whnp  s    
np ip  deg   np nn   nn     np adjp adjp  jj practical  
nn significance  
np np pu   np qp  nn    pu    np adjp  nn    
np np dt the  jj  nn idea   pp in of  np dt the  cd  nns represents    
np pu   nn   pu    vp vbg joining  np dt the  nn    

top    highest probability rules  for mt  wordalign   absent approach
 
lcp qp  lc    adjp jj   
  
np dnp  nn    vp advp  vbp changes  
  
np nn   nn    np cd three  nns links  x   
 
np dnp ip  dec    np nn     np adjp  nn significance  
 
vp advp  vp vv  np nn   nn     
vp advp  vp vp vv    np dt the  jj mass  nn     
 
ip np  vp vv   np nn   nn      np np nns    pp in for  np    
 
np vp  deg   nn    adjp jj   
 
np np pu   nt  pu    np nn    nr   
np np prp  his   qp cd    nn speech  
  
vp vp advp  vp vv   cc   vv     np   
vp advp  vp vp vb strengthen  cc and  vb improve   np    
    np np pu   nn   pu    np   
np np    np nn taiwan  nn independence       nns   

figure    top    highest probability rules built proposed sub tree alignment
approach wordalign   baseline grammar  top    rules
wordalign   baseline grammar obtained using proposed
sub tree alignment approach      good translation rule 
dorr  schwartz  makhoul  micciula    weischedel         significance test performed using bootstrap resampling method  koehn         moreover  efficiency
rule extraction reported terms rule set size extraction time  comparison 
report result rule extraction using word alignment matrices  liu et al       b 
wordalign   wordalign   
table   indicates approach outperforms baselines bleu ter
measures   best    best extraction  addition  matrix based method
much efficient k best method  example  compared    best extraction  extracting rules sub tree alignment matrices   times efficient  however 
rules counted unit one parameter estimation translation model 
using alignment matrices show significant bleu improvements ter reductions comparison    best counterpart  see rows marked unitcount  
many additionally extracted rules utilized real translation 
example  observed      rules used generating final    best 
   

fixiao   zhu

entry
wordalign      best 
wordalign      best 
heuristicalgin    best 
wordalign       best 
wordalign       best 
heuristicalgin     best 
wordalign    matrix 
wordalign    matrix 
   best   unitcount 
    best   unitcount 
 matrix   unitcount 
   best   posterior 
    best   posterior 
 matrix   posterior 

dev

test

bleu     ter   

bleu     ter   

    
    
    
    
    
    
     
     
     
     
     
     
     
      

    
    
    
    
     
    
     
     
     
     
      
     
      
      

    
    
    
    
    
    
    
    
    
    
     
     
      
      

    
    
    
    
    
    
     
     
    
     
     
     
      
      

rule set
size
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m
    m

efficiency
 rule sec 

    
    
    
   
   
   
    
    
    
   
    
    
   
    

table    evaluation translations different alignment approaches  bleu  higher
better  ter  lower better  unitcount means take rule
occurrence unit one parameter estimation  posterior means use
rule posterior probabilities fractional counts parameter estimation      
  significantly better three   best baselines  p              
translations indeed extracted alignments seen    best
alignments  thus indicates fact naively increasing number rules might
effective improving translation quality 
last three rows table   show result using alignment posterior probabilities
parameter estimation  i e   method described section       see alignment
posterior probabilities helpful improving translation quality system
weight rules confidence  entries unitcount vs  entries
posterior    using sub tree alignment matrices rule extraction alignment
posterior probabilities parameter estimation  approach finally achieves      bleu
improvement      ter reduction    best case baselines  even
outperforms word alignment matrix based counterpart      bleu points     
ter points  both significant p         
further  effectiveness proposed approach demonstrated terms bleu
ter scores rule set size  figure   compares approach
baseline approaches different numbers unique rules extracted    clearly 
number unique rules  proposed sub tree alignment approach leads better translations baselines 
      impact alignment grammar quality mt performance
experiments demonstrate effectiveness proposed approach terms
different measures individually  next natural question sub tree alignment
    this  adjusted pmin obtain grammars different sizes approach 
approaches  used different k best lists rule extraction 

   

fiunsupervised sub tree alignment tree to tree translation

  

    ter   

bleu    

  

  

  

heuristicalgin
wordalign  
wordalign  


  

heuristicalgin
wordalign  
wordalign  

  

  
  

  

  

  

  

  

  

  

  

  

  

  

  

  

rule set size  million 

rule set size  million 

figure    bleu   ter rule set size
rule extraction affect translation quality  study issue important
optimize upstream systems mt decoding select appropriate evaluation
metrics good prediction mt performance 
therefore carried another set experiments compares translation
quality different sub tree alignment rule extraction settings  generate diverse
sub tree alignment rule extraction results  varied values pmin
sub tree alignment rule extraction respectively  way  obtained ensembles
sub tree alignments grammars different precision recall scores    chose
f  score evaluation metric sub tree alignment system rule
extraction system  instead fixing    varied value        since
parameter control bias towards precision recall  choosing different values
helpful seeking good tradeoff precision recall  find
appropriate evaluation measure sub tree alignment rule extraction predicting
mt performance well 
figures      plot f  scores measures mt performance sub tree alignment rule extraction  figure     see rule f   score correlates best
translation quality measures  indicates mt system prefers rule
recall biased metrics  agrees observation figure   mt system
make benefits rules  hand  curves figure   show
better correlation sub tree alignment f   f   score translation quality measures  implying preference relatively higher sub tree alignment recall  result
reasonable framework node alignment links result aligned
tree fragments  or rules  extracted  high recall sub tree alignment generally results
big grammar high rule recall  thus better bleu ter results  com    example  larger value generally results higher alignment precision  small value
prefers higher alignment recall  rule extraction  larger value pmin generally leads grammar
higher rule precision  choosing smaller pmin generate grammar higher rule recall 

   

fixiao   zhu

  

sub tree alignment f    

sub tree alignment f    

  

  

  

f     
f     
f     
f     
f     

  

  
    

  

    

  

    

  

  

f     
f     
f     
f     
f     

  

  

  

  

bleu    

  

  

    ter   

  

  

  

  

rule f    

rule f    

figure    bleu   ter sub tree alignment f  measure

  
  

f     
f     
f     
f     
f     

  
  
  

    

  

  
  

f     
f     
f     
f     
f     

  
  

    

    

bleu    

  

    

  

    ter   

figure     bleu   ter rule f  measure

puted pearsons correlation coefficients sub tree alignment rule f   score
bleu ter  sub tree alignment f    correlation coefficients bleu ter
             respectively  rule f    correlation coefficients bleu
ter              respectively  show good correlations translation quality measures  another interesting observation mt performance
sensitive change rule f  score change sub tree alignment
f  score  may lie rule extraction direct upstream step decoding
impacts output mt systems  contrast  sub tree alignment front end
step mt pipeline indirect effect actual translation process 
   

fiunsupervised sub tree alignment tree to tree translation

system
hierarchical phrase based
tree   best word alignment  wordalign   

word alignment matrix
tree sub tree alignment matrix

dev

test

bleu     ter   

bleu     ter   

    
    
    
     

    
    
    
      

    
     
     
      

    
    
     
      

table    mt evaluation results rules obtained various alignment approaches 
bleu  higher better  ter  lower better         significantly better
hierarchical phrase based baseline  p              
      improvements
previous work pointed straightforward implementation tree to tree mt
suffers problems rules derivations either rule extraction
decoding process  chiang         advance tree to tree system compare
state of the art  employed tree binarization  wang et al       b  fuzzy
decoding  chiang        system  alignment approach equipped
general framework tree to tree translation  trivial conduct another set
experiments investigate effectiveness approach stronger system    table
  shows bleu ter scores system enhanced methods   
comparison  report result state of the art mt system implements
hierarchical phrase based model  chiang        tree to tree system extracts
rules using word alignment matrices  liu et al       b   table   indicates superiority
approach tree binarization fuzzy decoding involved  significantly
outperforms hierarchical phrase based system       bleu points      ter points 
tree to tree system based word alignment matrices       bleu points
     ter points  
discussed section    transfer rules sub tree alignment model resembles
general form stsgs directly used mt  instead resorting
explicit step rule extraction  use rules sub tree alignment model
mt decoding  i e   sub tree alignment cast grammar induction step  therefore
built another system directly acquires translation rules sub tree alignment
step  this  need output rules derivation forest generated
alignment model  rule probabilities obtained using inside output
probabilities  pruning performed throwing away rules whose probability
pmin   addition rule probability  reused n gram language model  rule number
bonus  target length bonus  lexicalized rule low frequency rule indicators
base tree to tree system additional features fair comparison  obtain good
reasonable result  employed fuzzy decoding tree binarizaiton experiment 
    choose setting previous experiments gold standard alignment annotation penn treebank style trees only  difficult evaluate alignment grammar
quality binarized trees due lack benchmark data  experiments first conducted
experiments individual tasks  see sections               studied correlations simple
reasonable setting consistent result sub tree alignment mt  see section        
investigated effectiveness approach advanced tree to tree system  see section        
    implementation parse trees binarized head out fashion 

   

fixiao   zhu

entry
baseline  explicit rule extraction 
rules sub tree alignment model
rules sub tree alignment model   mert
baseline   sub tree alignment features

dev

test

bleu     ter   

bleu     ter   

    
    
    
    

    
    
    
    

    
    
    
    

    
    
    
    

table    mt evaluation results obtaining rules sub tree alignment model
obtaining rules traditional rule extraction pipeline
table   compares results sub tree alignment matrix based rule extraction inducing rules alignment model  row   vs  row     unfortunately  straightforwardly
inferring rules probabilities sub tree alignment model underperforms
baseline  might attributed several reasons  first  due large derivation
space  cannot enumerate relatively large tree fragments sub tree alignment
step  instead access tree fragments limited depths  contrast  baseline system extracts basic rules using sub tree alignment matrices obtains
large rules heuristics  e g   rule composing   additional rules obtained
baseline framework rule extraction general useful modern syntax based
systems  galley et al         marcu et al         deneefe et al          second  rule
probability sub tree alignment model defined product probability factors
good generation story  however  mt systems usually use features required form generative model  features shown equations           
consequence  many well developed features used baseline system
available sub tree alignment model  third  sub tree alignment model
trained maximizing likelihood criteria  consistent
adopted mt system  i e   minimizing evaluation related error rate function  
study issues  improved system two ways  first  treated four
probability factors sub tree alignment model  see equation       different
features mt decoder  tuned weights using mert  row   table   shows
method achieves better results system employing unweighted probability
factors  however  performance still worse baseline  indicates
mt oriented features rule extraction heuristics crucial success
tree to tree system  finally added probabilistic factors sub tree alignment
model baseline system additional features  shown last row table   
enhanced system yields modest bleu improvements baseline  ter
improvement achieved  results give us two interesting messages      rule
extraction heuristics  mt oriented features objectives learning key factors contributing good tree to tree system     better use sub tree alignment
model upstream module rule extraction decoding  rather using
simple step grammar induction 
last issue investigate whether sub tree alignment model make
benefits labeled data  although focus unsupervised learning work 
proposed model require strictly unsupervised condition  instead
enhanced use labeled data  idea simple  combine probability
factors sub tree alignment model log linear weighted fashion  means
   

fiunsupervised sub tree alignment tree to tree translation

entry
unweighted
weighted  weights learned labeled data 

dev

test

bleu     ter   

bleu     ter   

    
    

    
    

    
    

    
    

table    comparison unweighted weighted sub tree alignment models
probability factors sub tree alignment model taken real valued feature
functions  feature weights learned labeled data supervised methods 
way  unweighted generative model  i e   factor weight one 
transformed weighted model  i e   factor individual weight   note
weighted model almost form used smt systems 
difference smt model language model needed targetlanguage side fixed sub tree alignment step  avoid bias towards
many rules  added rule number additional feature new model 
training test  divided node aligned gold standard data two parts  
first     sentences selected weight training  remaining     sentences
selected testing system  learn feature weights supervised manner  chose
mert one powerful tools training log linear models  error
function used mert defined one minus sub tree alignment f   score 
    sentence test data  with tree annotation penn treebanks  
weighted model achieves alignment f   score       rule f   score       
respectively  result better unweighed  and unsupervised  model
obtains alignment f   score       rule f   score      
data set  finally tested mt performance best setting  i e   sub tree alignment
matrix based rule extraction   tree binarization   fuzzy decoding     table   shows
weighted sub tree alignment model leads better bleu score tuning set
show promising improvements test data  size labeled corpus
small  expect better results labeled data available  worth studying
sophisticated supervised methods learn better weights  kernel based
methods  sun et al       b   supervised semi supervised learning focus
work  leave interesting issues future investigations 

   related work
syntax based approaches widely adopted machine translation last
ten years  many successful syntactic mt systems developed shown good
results several translation tasks  eisner        galley et al               liu et al  
      huang et al         zhang et al         liu et al       a  chiang         despite
differences modeling implementation details  models require alignment
step acquire syntactic correspondence source target languages 
standard smt  syntax based mt systems use word alignments infer syntactic
alignments string tree tree tree pairs  however  word alignments generally
good quality viewpoint syntactic alignment  makes sense directly
    sub tree aligned data binarized trees  reused weights learned penn
treebank style trees four probability factors sub tree alignment model 

   

fixiao   zhu

induce sub tree level alignments pairs sentences syntactic information
either language side both  especially true tree to tree mt
actually need alignment sub trees two languages  rather surface
alignment words  several lines work address syntactic alignment
problem make better use various alignment results tree to tree translation 
    word sub tree alignment machine translation
earliest efforts syntactic alignment focus enhancing word alignment models
syntactic information  date  several research groups  fraser   marcu        denero
  klein        may   knight        fossum et al         haghighi  blitzer  denero   
klein        burkett  blitzer    klein        riesa  irvine    marcu        proposed
syntax augmented models advance word alignment systems  although models
achieved promising improvements  still address alignment problem word level 
discussed section    methods might desirable choices learning
correspondence tree nodes two languages  alternative straightforward solution  researchers tried infer sub tree level alignments pairs syntactic
trees  example  imamura         groves  hearne  way         tinsley et al 
       defined several scoring functions measure similarity source
target sub trees  aligned tree nodes greedy algorithms  approaches 
though simple implement  derived principled way  example 
models explicit optimization procedure  general framework statistical learning  instead  model parameters obtained using additional alignment
models lexicons  another line research  sun et al       a      b  attempted address sub tree alignment problem supervised semi supervised models  used
tree kernels various syntactic features advance sub tree alignment system
showed promising results chinese english translation tasks  however  approach still
relies heuristic algorithms inferring node correspondences two parse trees 
beyond this  train tree kernels  approach requires additional labeled data
generally expensive build  unlike studies  derive sub tree model
principled way develop unsupervised sub tree alignment framework tree to tree
mt 
    unsupervised syntactic alignment
previous studies resort labeled data sub tree alignment 
earliest eisners        work  designed unsupervised approach
modeling sub tree alignment problem stsg formalism  however  since
detailed derivation model decomposition provided  model computationally
expensive  even difficult applied current tree to tree systems complex tree
structures involved  gildea        applied stsgs tree to tree tree to string
alignment  developed loosely tree based alignment method address issue
parse tree isomorphism bitext  work targets word alignment rather
modern syntactic mt systems  recently nakazawa kurohashi        proposed
bayesian approach sub tree alignment dependency trees  tested
japanese english mt system  actually model much common model
   

fiunsupervised sub tree alignment tree to tree translation

presented work  example  apply unsupervised learning methods
bayesian models sub tree alignment  hand  two studies differ
important aspects  first  nakazawa kurohashi        restricted sub tree
alignment dependency trees  different aligning tree nodes
phrase structure trees  since phrase structure trees involve complex structures
syntactic categories  alignment problem phrase structure trees relatively
difficult dependency based counterpart  second  model makes benefits
recent advances stsgs directly applicable current state of the art tree totree systems 
another related work presented pauls  klein  chiang  knights
       work  factored node to string alignment model components
generates target side synchronous rule source side  moreover  probability
rule fragment factored lexical structural component work  actually 
model proposed model two variants theme  appear
obvious differences them  first  focus sub tree alignment tree to tree
translation  pauls et al         addressed alignment issue tree to string stringto tree translation  model  parse language sides independently  rather
parsing one side projecting syntactic categories  result  inference faster
work since need consider possible parse trees unparsed side
alignment  second  permutation model presented work general
order handle non itg trees  third  investigate methods effective use
sub tree alignment mt  particular  present rule extraction approach obtaining
additional translation rules using sub tree alignment posteriors  rather learning rules
  best sub tree alignment 
    rule extraction using various alignment results
machine translation  word syntactic alignments used extract translation rules
phrases  traditional pipeline rule phrase extraction    best alignment result considered  suffers limited scope single alignment 
efficiently obtain diverse alignment parsing results  packed data structures adopted
improve   best pipeline mt systems recent years  mi   huang        liu et al       a 
zhang  zhang  li  aw    tan         example  liu et al       b  de gispert et al 
       used alignment posterior probabilities phrase hierarchical phrase extraction 
development sub tree alignment matrices actually motivated similar idea
word alignment matrices  difference work use sub tree
language sides infer alignment posterior probabilities  probabilities
calculated word phrase level previous work  liu et al       b  de gispert et al  
       moreover  knowledge  effectiveness sub tree alignment matrix
systematically studied case tree to tree translation 
note approach presented work something similar synchronous grammar induction  example  model results stsg
formalism used mt  recent studies bayesian models  blunsom  cohn 
dyer    osborne        cohn   blunsom        levenberg et al         shown
promising results directly learning synchronous grammars bilingual data hierar   

fixiao   zhu

chical phrase based string to tree systems  rather extracting synchronous grammar
rules based explicit word syntactic alignment step  however rare see related
work tree to tree mt  principle article different previous work synchronous grammar induction  example  aim work learn sub tree
alignment model  applied many potential applications except mt 
sentence compression paraphrasing test summarization  jing        cohn   lapata 
       unlike synchronous grammar induction alignment implicitly encoded
learning process  treat sub tree alignment separate task  eases
development tuning alignment system actually resort mt
systems slow difficult optimize  another advantage approach
make benefits compact models  rather used mt great
number rules involved  take implementation instance  alignment model
learned relatively small set grammar rules  rules limited depths  
mt system accesses much larger grammar many additional rules involved
rule composing  method result efficient alignment system likely
alleviate degenerate analysis data  cost degrading mt performance 

   discussion
underlying assumption proposed model   to   sub tree alignments
achieved based constraints imposed neighboring parts tree  see section
    makes sense standpoint linguistically motivated models  yet turn
faces problem constraints make difficult align sentences trees correctly 
particularly free translations  several reasons explain
approach works nice tree to tree mt suffer greatly
structure divergence languages  first  model flexible allows
node deletion insertion alignment  means levels tree
necessary require every node aligned valid node language side 
instead nodes dropped needed  advantage method
cannot confidently align node node counterpart tree  align
virtual node enforce bad constraints aligning parts tree 
useful flat tree structures partial translations
syntactically well formed  second  main purpose approach infer
sub tree alignment probabilities used pruning sub tree alignment matrices
extracting rules mt systems  though   to   alignment required training
sub tree alignment model  actually access large number alignment
alternatives rule extraction  even cannot appear derivation
due alignment constraints  third  model work phrase structure
trees  instead penn treebank style trees difficult alignment
cases  sub tree alignment system works well binarized trees shows promising
improvements various baselines  note tree binarization effective method
alleviate structure divergence problem  especially chinese english translation 
also  might interesting investigate methods dealing differences
syntactic structures languages  forest based methods  mi   huang 
      liu et al       a  
   

fiunsupervised sub tree alignment tree to tree translation

another note approach  implemented naively  speed sub tree alignment system slow since model needs calculation alignment probability pairs tree fragments  fortunately  thought computation  several
optimizations make system much efficient practice  first  described
work  pruning methods employed restrict number tree fragments
reasonable level  experiments  system pruning achieved speed       
sentence second single core intel xeon      ghz cpu  another way speed
improvement parallel processing  good property em style algorithms
e step easily implemented parallel computation environment 
need divide training data set number smaller parts  run
inside outside algorithm parts parallel  i e   map procedure   expected
counts model parameters accumulated results parts  i e  
reduce procedure   m step performed usual  implementation
used    threads parallel training  running time one training iteration
  million sentence corpus       hours  note system speed up
expected powerful distributed infrastructures available  e g   clusters  
hadoop   difficult scale approach handle millions sentence pairs
using current training framework 

   conclusions
proposed unsupervised probabilistic sub tree alignment approach tree totree translation  factoring alignment model several components  resulting
model easily learned using em algorithm variational bayesian approach 
also  investigated different ways applying proposed model tree to tree
translation  particular  developed sub tree alignment matrix encodes
exponentially large number alignments  representation sub tree alignment 
desirable rules extracted efficiently using k best sub tree alignment
result  experiments showed proposed model achieved significant improvements
alignment quality grammar quality several baselines  nist chineseenglish evaluation corpora  achieved      bleu improvement      ter reduction
top state of the art tree to tree system  improved mt system even significantly
outperformed state of the art hierarchical phrase based system equipped tree
binarization fuzzy decoding 

acknowledgments
work supported part national science foundation china  grants
                    natural science foundation youth china  grant
           china postdoctoral science foundation  grant     m         specialized research fund doctoral program higher education  grant                 
fundamental research funds central universities  grant n           
authors would thank anonymous reviewers pertinent insightful comments  keh yih su great help improving early version article  ji
   

fixiao   zhu

helpful discussions  chunliang zhang tongran liu language refinement 
corresponding author article jingbo zhu 

appendix a  part of speech tags phrase structure labels
work annotation pos tagging phrase structure parsing follows standard defined penn english chinese treebanks  marcus et al         xue et al  
       see tables      lists pos tags constituent labels used example
trees article 
pos tag
ad

nn
nr
p
pn
pu
vv

description
adverb
aspect particle
noun  except proper nouns temporal nouns 
proper noun
preposition
pronoun
punctuation
verb  except stative verbs  copulas  main
verbs    

table    chinese pos tags used examples
pos tag
dt

jj
nnp
nns
prp
rb
vbd
vbn
vbp
 
 

description
determiner
preposition
adjective
proper noun  singular 
noun  plural 
personal pronoun
adverb
verb  past tense 
verb  past participle 
verb  non  rd person singular present 
comma
period

table    english pos tags used examples
syntactic label
ip
np
pp
qp
vp

description
single clause
noun phrase
preposition phrase
quantity phrase
verb phrase

table     chinese constituent labels used examples

   

fiunsupervised sub tree alignment tree to tree translation

syntactic label
advp
np
pp

vp

description
adverb phrase
noun phrase
preposition phrase
sentence
verb phrase

table     english constituent labels used examples
distribution
pnt   
ptree   
preorder   

notation
tnt  snt
ttree  tnt
tvnt  svnt

plength   
pw   

l m
tw  sw

description
snt tnt source target language non terminal symbols
ttree target language tree fragment
svnt tvnt vectors non terminal symbols source
target languages
l numbers source target terminals  or words 
sw tw source target terminals  or words 

table     notations model parameters

appendix b  em based training sub tree alignment model
described section    proposed sub tree alignment model five types parameters  including non terminal mapping probability pnt     target language treefragment generation probability ptree     frontier non terminal reordering probability
preorder     word number probability plength    word mapping probability pw    
convenience use new set notations denote model parameters following description  see table    symbol list 
follow framework em based training described figure
   see figure    complete version em algorithm parameters model 
algorithm  ec   represents expected count input variable   x   x 
    function returns   variable x takes value x    otherwise   k   r  u  v 
 k   s    rule probability  see equation       probability subtree alignment  see equation        k indicates
probabilities calculated based parameters k th iteration  tree    vnt  
lex   functions return tree structure  frontier non terminal vector 
terminal sequence input tree fragment  respectively  see section      
basic idea e step check rule r  given pair tree nodes u
 r u v 
v  update ec   relative probability  root s  root t
     applied
update rules parameters tnt  snt   ttree  tnt   tvnt  svnt l m  see lines       
exception tw  sw   defined equation       pw  ti   sj   direct
product factor pinstead use sum terminals source language treefragment  i e  
j   pw  ti   sj     follow result ibm model   make
p lex s   
update magnitude proportional pw  ti   sj    j      r pw  ti   sj      refer reader
brown et al s        work detailed derivation expected count ibm model
   worth noting algorithm performs parameter update based
different choices  u  v  r e step  means rule instance involved
particular derivation one time  e g   tree fragment appears
   

fixiao   zhu

   function trainmodelwithem    s    t           sn   tn    
   
   
   
   
   
   initialize  tnt  snt   ttree  tnt   tvnt  svnt   l m   tw  sw  
   k     k  
  
set ec       model parameters
   e step 
  
foreach tree pair  s    sequence   s    t           sn   tn   
  
foreach node pair  u  v   s   
  
foreach rule r rooted  u  v 
 k   r u v  snt  u  tnt  v 
 k   root s  root t   

  

ec tnt  snt  

  

  

ec ttree  tnt      

 k   r u v  tnt  v  ttree  tree tr   
 k   root s  root t   

   

ec tvnt  svnt      

 k   r u v  svnt  vnt sr    tvnt  vnt tr   
 k   root s  root t   

   

ec l m  

 k   r u v  m  lex sr     l  lex tr    
 k   root s  root t   

   

foreach word pair  sj   ti   position  j  i   lex sr    lex  tr   

  

 k 
p
 ti  sj  
 sw  sj   tw  ti  
r p k   t  s  
w
j 
j     

 k   r u v  p lex sw  

   

ec tw  sw      

 k   root s  root t   

    m step 
   
   
   
   
   
   
   
   
   
   
   

foreach non terminal symbol pair  snt   tnt  
 k   

tnt  snt

 

ec tnt  snt  
p

t nt



ec t 

nt  snt

foreach target non terminal symbol tnt tree fragment structure ttree
ec ttree  tnt  

 k   

ttree  tnt  

p

t tree



ec t 

tree  tnt

foreach pair non terminal symbol vectors  svnt   tvnt  
ec tvnt  svnt  

 k   

tvnt  svnt  

p

t vnt



ec t 

vnt  svnt

foreach pair word numbers  m  l 
 k   

l m

 

ec l m  
p

l 

ec l   m



foreach pair words  sw   tw  
 k   

tw  sw

 k 

 

ec tw  sw  
p

t w

 k 

ec t 



w  sw

 k 

 k 

 k 

return  tnt  snt   ttree  tnt   tvnt  svnt   l m   tw  sw  
figure     em based training algorithm model parameters

different positions   update corresponding parameters would carried
multiple times 
another note em algorithm  expected counts parameters
efficiently calculated using inside outside probabilities according lines     
   

fiunsupervised sub tree alignment tree to tree translation

    parameters efficient ways  example  discussed
section        expected count tnt  snt obtained without checking individual
rule  is  omit loop r case  technique considered
speed up sub tree alignment system 

references
attias  h          variational bayesian framework graphical models  solla  s  a  
leen  t  k     k   m   eds    advances neural information processing systems    
pp          mit press 
beal  m  j          variational algorithms approximate bayesian inference  masters
thesis  university college london 
blunsom  p   cohn  t   dyer  c     osborne  m          gibbs sampler phrasal
synchronous grammar induction  proceedings joint conference   th
annual meeting acl  th international joint conference natural
language processing afnlp  acl ijcnlp   pp          suntec  singapore 
brown  p  e   pietra  s  a  d   pietra  v  j  d     mercer  r  l          mathematics
statistical machine translation  parameter estimation  computational linguistics 
           
burkett  d   blitzer  j     klein  d          joint parsing alignment weakly synchronized grammars  human language technologies       annual conference north american chapter association computational linguistics
 hlt naacl   pp          los angeles  california  usa 
chiang  d          hierarchical phrase based model statistical machine translation 
proceedings   rd annual meeting association computational linguistics  acl   pp          ann arbor  michigan  usa 
chiang  d          hierarchical phrase based translation  computational linguistics     
     
chiang  d          learning translate source target syntax  proceedings
  th annual meeting association computational linguistics  acl  
pp            uppsala  sweden 
chiang  d     knight  k          introduction synchronous grammars  tutorials
  st international conference computational linguistics   th annual
meeting association computational linguistics  coling acl  
chiswell  i     hodges  w          mathematical logic  oxford university press 
cohn  t     blunsom  p          bayesian model syntax directed tree string grammar induction  proceedings      conference empirical methods natural
language processing  emnlp   pp          singapore 
cohn  t     lapata  m          sentence compression tree transduction  journal
artificial intelligence research             
das  d     smith  n  a          paraphrase identification probabilistic quasi synchronous
recognition  proceedings joint conference   th annual meeting
   

fixiao   zhu

acl  th international joint conference natural language processing
afnlp  acl ijcnlp   pp          suntec  singapore 
de gispert  a   pino  j     byrne  w          hierarchical phrase based translation grammars extracted alignment posterior probabilities  proceedings     
conference empirical methods natural language processing  emnlp   pp 
        cambridge  ma  usa 
dempster  a   laird  n     rubin  d          maximum likelihood incomplete data via
em algorithm  journal royal statistical society  series b  methodological  
        
deneefe  s   knight  k   wang  w     marcu  d          syntax based mt
learn phrase based mt   proceedings      joint conference empirical methods natural language processing computational natural language
learning  emnlp conll   pp          prague  czech republic 
denero  j   gillick  d   zhang  j     klein  d          generative phrase models underperform surface heuristics  proceedings workshop statistical machine
translation  wmt   pp        new york city  usa 
denero  j     klein  d          tailoring word alignments syntactic machine translation  proceedings   th annual meeting association computational
linguistics  acl   pp        prague  czech republic 
eisner  j          learning non isomorphic tree mappings machine translation 
companion volume proceedings   st annual meeting association
computational linguistics  acl   pp          sapporo  japan 
ferguson  t  s          bayesian analysis nonparametric problems  annals
statistics            
fossum  v   knight  k     abney  s          using syntax improve word alignment
precision syntax based machine translation  proceedings third workshop
statistical machine translation  wmt   pp        columbus  ohio  usa 
fraser  a     marcu  d          getting structure right word alignment  leaf 
proceedings      joint conference empirical methods natural language
processing computational natural language learning  emnlp conll   pp    
    prague  czech republic 
galley  m   graehl  j   knight  k   marcu  d   deneefe  s   wang  w     thayer  i         
scalable inference training context rich syntactic translation models  proceedings   st international conference computational linguistics
  th annual meeting association computational linguistics  colingacl   pp          sydney  australia 
galley  m   hopkins  m   knight  k     marcu  d          whats translation rule  
susan dumais  d  m     roukos  s   eds    proceedings      human language
technology conference north american chapter association computational linguistics  hlt naacl   pp          boston  massachusetts  usa 
   

fiunsupervised sub tree alignment tree to tree translation

gildea  d          loosely tree based alignment machine translation  proceedings
  st annual meeting association computational linguistics  acl   pp 
      sapporo  japan 
groves  d   hearne  m     way  a          robust sub sentential alignment phrasestructure trees  proceedings   th international conference computational
linguistics  coling   pp            geneva  switzerland 
haghighi  a   blitzer  j   denero  j     klein  d          better word alignments
supervised itg models  proceedings joint conference   th annual
meeting acl  th international joint conference natural language
processing afnlp  acl ijcnlp   pp          suntec  singapore 
huang  l     chiang  d          better k best parsing  proceedings ninth international workshop parsing technology  iwpt   pp        vancouver  british
columbia  canada 
huang  l   kevin  k     joshi  a          statistical syntax directed translation
extended domain locality  proceedings  th conference association
machine translation americas  amta   pp        cambridge  massachusetts 
usa 
imamura  k          hierarchical phrase alignment harmonized parsing  proceedings
 th nlp pacific rim symposium  pp         
jing  h          sentence reduction automatic text summarization  proceedings
 th applied natural language processing conference  pp         
johnson  m          doesnt em find good hmm pos taggers   proceedings
     joint conference empirical methods natural language processing
computational natural language learning  emnlp conll   pp          prague 
czech republic 
knuth  d          art computer programming  fundamental algorithms  addisonwesley 
koehn  p          statistical significance tests machine translation evaluation  lin 
d     wu  d   eds    proceedings      conference empirical methods
natural language processing  emnlp   pp          barcelona  spain 
koehn  p   och  f     marcu  d          statistical phrase based translation  proceedings      human language technology conference north american
chapter association computational linguistics  hlt naacl   pp       
edmonton  canada 
levenberg  a   dyer  c     blunsom  p          bayesian model learning scfgs
discontiguous rules  proceedings      joint conference empirical methods natural language processing computational natural language learning
 emnlp conll   pp          jeju island  korea 
liu  d     gildea  d          bayesian learning phrasal tree to string templates  proceedings      conference empirical methods natural language processing
 emnlp   pp            singapore 
   

fixiao   zhu

liu  y   liu  q     lin  s          tree to string alignment template statistical machine
translation  proceedings   st international conference computational
linguistics   th annual meeting association computational linguistics  coling acl   pp          sydney  australia 
liu  y   lu  y     liu  q       a   improving tree to tree translation packed forests 
proceedings joint conference   th annual meeting acl
 th international joint conference natural language processing afnlp
 acl ijcnlp   pp          suntec  singapore 
liu  y   xia  t   xiao  x     liu  q       b   weighted alignment matrices statistical
machine translation  proceedings      conference empirical methods
natural language processing  emnlp   pp            singapore 
manning  c  d     schutze  h          foundations statistical natural language processing  mit press 
marcu  d   wang  w   echihabi  a     knight  k          spmt  statistical machine translation syntactified target language phrases  proceedings      conference
empirical methods natural language processing  emnlp   pp        sydney 
australia 
marcu  d     wong  d          phrase based joint probability model statistical machine translation  proceedings      conference empirical methods
natural language processing  emnlp   pp         
marcus  m  p   santorini  b     marcinkiewicz  m  a          building large annotated
corpus english  penn treebank  computational linguistics             
may  j     knight  k          syntactic re alignment models machine translation 
proceedings      joint conference empirical methods natural language processing computational natural language learning  emnlp conll  
pp          prague  czech republic 
mi  h     huang  l          forest based translation rule extraction  proceedings
     conference empirical methods natural language processing  emnlp  
pp          honolulu  hawaii  usa 
nakazawa  t     kurohashi  s          bayesian subtree alignment model based dependency trees  proceedings  th international joint conference natural language
processing  ijcnlp   pp          chiang mai  thailand 
neal  r          philosophy bayesian inference  http   www cs toronto edu radford 
res bayes ex html 
och  f          minimum error rate training statistical machine translation  proceedings   st annual meeting association computational linguistics
 acl   pp          sapporo  japan 
och  f     ney  h          alignment template approach statistical machine translation  computational linguistics             
papineni  k   roukos  s   ward  t     zhu  w          bleu  method automatic
evaluation machine translation  proceedings   th annual meeting
   

fiunsupervised sub tree alignment tree to tree translation

association computational linguistics  acl   pp          philadelphia  pennsylvania  usa 
pauls  a   klein  d   chiang  d     knight  k          unsupervised syntactic alignment
inversion transduction grammars  proceedings human language technologies       annual conference north american chapter association
computational linguistics  hlt naacl   pp          los angeles  california 
usa 
riesa  j   irvine  a     marcu  d          feature rich language independent syntax based
alignment statistical machine translation  proceedings      conference
empirical methods natural language processing  emnlp   pp          edinburgh  scotland  uk 
riley  d     gildea  d          improving performance giza   using variational
bayes  tech  rep   university rochester 
smith  d  a     eisner  j          parser adaptation projection quasi synchronous
grammar features  proceedings      conference empirical methods
natural language processing  emnlp   pp          singapore 
snover  m   dorr  b   schwartz  r   makhoul  j   micciula  l     weischedel  r         
study translation error rate targeted human annotation  tech  rep 
lamp tr     cs tr      umiacs tr          university maryland  college
park bbn technologies 
sun  j   zhang  m     tan  c  l       a   discriminative induction sub tree alignment
using limited labeled data  proceedings   rd international conference
computational linguistics  coling   pp            beijing  china 
sun  j   zhang  m     tan  c  l       b   exploring syntactic structural features sub tree
alignment using bilingual tree kernels  proceedings   th annual meeting
association computational linguistics  acl   pp          uppsala  sweden 
thayer  i   ettelaie  e   knight  k   marcu  d   munteanu  d   och  f     tipu  q         
isi usc mt system  proceedings international workshop spoken language
translation       pp       
tinsley  j   zhechev  v   hearne  m     way  a          robust language pair independent
sub tree alignment  proceedings machine translation summit xi  pp         
copenhagen  denmark 
venugopal  a   zollmann  a   smith  n  a     stephan  v          wider pipelines  n best
alignments parses mt training  proceedings eighth conference
association machine translation americas  amta   pp         
vogel  s   ney  h     tillmann  c          hmm based word alignment statistical translation  proceedings   rd international conference computational linguistics  coling   pp         
wang  m   smith  n  a     mitamura  t       a   jeopardy model  quasisynchronous grammar qa  proceedings      joint conference empirical methods natural language processing computational natural language
learning  emnlp conll   pp        prague  czech republic 
   

fixiao   zhu

wang  w   knight  k     marcu  d       b   binarizing syntax trees improve syntaxbased machine translation accuracy  proceedings      joint conference
empirical methods natural language processing computational natural
language learning  emnlp conll   pp          prague  czech republic 
woodsend  k     lapata  m          learning simplify sentences quasi synchronous
grammar integer programming  proceedings      conference empirical methods natural language processing  emnlp   pp          edinburgh 
scotland  uk 
xiao  t   zhu  j   zhang  h     li  q          niutrans  open source toolkit phrasebased syntax based machine translation  proceedings   th annual meeting association computational linguistics system demonstrations  acl  
pp        jeju island  korea 
xue  n   xia  f   chiou  f  d     palmer  m          penn chinese treebank  phrase
structure annotation large corpus  natural language engineering             
zhang  h   zhang  m   li  h   aw  a     tan  c  l          forest based tree sequence
string translation model  proceedings joint conference   th annual
meeting acl  th international joint conference natural language
processing afnlp  acl ijcnlp   pp          suntec  singapore 
zhang  m   jiang  h   aw  a   li  h   tan  c  l     li  s          tree sequence alignmentbased tree to tree translation model  proceedings   th annual meeting
association computational linguistics  human language techonologies
 acl hlt   pp          columbus  ohio  usa 

   


