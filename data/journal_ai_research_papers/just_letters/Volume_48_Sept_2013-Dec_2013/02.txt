journal of artificial intelligence research                 

submitted       published      

a survey of multi objective sequential decision making
diederik m  roijers

d m roijers uva nl

informatics institute
university of amsterdam
amsterdam  the netherlands

peter vamplew

p vamplew ballarat edu au

school of science 
information technology and engineering
university of ballarat
ballarat  victoria  australia

shimon whiteson

s a whiteson uva nl

informatics institute
university of amsterdam
amsterdam  the netherlands

richard dazeley

r dazeley ballarat edu au

school of science 
information technology and engineering
university of ballarat
ballarat  victoria  australia

abstract
sequential decision making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision theoretic planning and learning 
which has largely focused on single objective settings  this article surveys algorithms designed for sequential decision making problems with multiple objectives  though there is
a growing body of literature on this subject  little of it makes explicit under what circumstances special methods are needed to solve multi objective problems  therefore  we
identify three distinct scenarios in which converting such a problem to a single objective
one is impossible  infeasible  or undesirable  furthermore  we propose a taxonomy that
classifies multi objective methods according to the applicable scenario  the nature of the
scalarization function  which projects multi objective values to scalar ones   and the type
of policies considered  we show how these factors determine the nature of an optimal solution  which can be a single policy  a convex hull  or a pareto front  using this taxonomy 
we survey the literature on multi objective methods for planning and learning  finally  we
discuss key applications of such methods and outline opportunities for future work 

   introduction
sequential decision problems  commonly modeled as markov decision processes  mdps 
 bellman      a   occur in a range of real world tasks such as robot control  kober  
peters         game playing  szita         clinical management of patients  peek        
military planning  aberdeen  thiebaux    zhang         and control of elevators  crites
  barto         power systems  ernst  glavic    wehenkel         and water supplies
 bhattacharya  lobbrecht    solomantine         therefore  the development of algorithms
c
    
ai access foundation  all rights reserved 

firoijers  vamplew  whiteson   dazeley

for automatically solving such problems  either by planning given a model of the mdp  e g  
via dynamic programming methods  bellman      b  or by learning through interaction
with an unknown mdp  e g   via temporal difference methods  sutton   barto         is
an important challenge in artificial intelligence 
in most research on these topics  the desirability or undesirability of actions and their
effects are codified in a single  scalar reward function  typically  the objective of the
autonomous agent interacting with the mdp is then to maximize the expected  possibly
discounted  sum of these rewards over time  in many tasks  a scalar reward function is the
most natural  e g   a financial trading agent could be rewarded based on the monetary gain
or loss in its holdings over the most recent time period  however  there are also many tasks
that are more naturally described in terms of multiple  possibly conflicting objectives  e g  
a traffic control system should minimize latency and maximize throughput  an autonomous
vehicle should minimize both travel time and fuel costs  multi objective problems have
been widely examined in many areas of decision making  zeleny   cochrane        vira  
haimes        stewart        diehl   haimes        roijers  whiteson    oliehoek       
and there is a growing  albeit fragmented  literature addressing multi objective decisionmaking in sequential settings 
in this article  we present a survey of the algorithms that have been devised for such
settings  we begin in section   by formalizing the problem as a multi objective mdp
 momdp   then  in section    we motivate the multi objective perspective on decisionmaking  little of the existing literature on multi objective algorithms makes explicit why a
multi objective approach is beneficial and  crucially  which cases cannot be trivially reduced
to a single objective problem and solved with standard algorithms  to address this  we
describe three motivating scenarios for multi objective algorithms 
then  in section    we present a novel taxonomy that organizes multi objective problems
in terms of their underlying assumptions and the nature of the resulting solutions  a key
difficulty with the existing literature is that authors have considered many different types
of problems  often without making explicit the assumptions involved  how these differ from
those of other authors  or the scope of applicability of the resulting methods  our taxonomy
aims to fill this void 
sections   and   survey momdp planning and learning methods  respectively  organizing them according to the taxonomy and identifying some key differences between the
approaches examined in the planning and learning areas  section   surveys applications
of these methods  covering both specific applications and more general classes of problems
where momdp methods can be applied  section   discusses future directions for the field
based on gaps in the literature identified in sections   and    and section   concludes 

   background
a finite single objective markov decision process  mdp  is a tuple hs  a  t  r    i where 
 s is a finite set of states 
 a is a finite set of actions 
 t   s  a  s         is a transition function specifying  for each state  action  and
next state  the probability of that next state occurring 
  

fia survey of multi objective sequential decision making

 r   s  a  s   is a reward function  specifying  for each state  action  and next
state  the expected immediate reward 
    s         is a probability distribution over initial states  and
          is a discount factor specifying the relative importance of immediate rewards 
the goal of an agent that acts in this environment is to maximize the expected return
rt   which is some function of the rewards received from timestep t and onwards  typically 
the return is additive  boutilier  dean    hanks         i e   it is a sum of these rewards  in
an infinite horizon mdp  the return is typically an infinite sum  with each term discounted
according to  

x
 k rt k    
rt  
k  

where rt is the reward obtained at time t  the parameter  thus quantifies the relative
importance of short term and long term rewards 
in contrast  in a finite horizon mdp  the return is typically an undiscounted finite sum 
i e   after a certain number of timesteps  the process terminates and no more reward can be
obtained  while single  and multi objective methods have been developed for finite horizon 
discounted infinite horizon  and average reward settings  puterman         for the sake of
brevity we formalize only infinite horizon discounted reward mdps in this article  
an agents policy  determines which actions it selects at each timestep  in the broadest
sense  a policy can condition on everything that is known to the agent  a state indepedent
value function v  specifies the expected return when following  from the initial state 
v    e r      

   

if the policy is stationary  i e   it conditions only on the current state  then it can be
formalized as    s  a          it specifies  for each state and action  the probability of
taking that action in that state  we can then specify the state value function of a policy  
v   s    e rt     st   s  
for all t when st   s  the bellman equation restates this expectation recursively for
stationary policies 
x
x
t  s  a  s   r s  a  s     v   s    
v   s   
 s  a 
a

s

note that the bellman equation  which forms the heart of most standard solution algorithms
such as dynamic programming  bellman      b  and temporal difference methods  sutton
  barto         explicitly relies on the assumption of additive returns  this is important
because  as we explain in section        some multi objective settings can interfere with
this additivity property  making planning and learning methods that rely on the bellman
equation inapplicable 
   for formalizations of the other settings  see for example the overview by van otterlo and wiering        

  

firoijers  vamplew  whiteson   dazeley

state value functions induce a partial ordering over policies  i e    is better than or
equal to   if and only if its value is greater for all states 


     s  v   s   v   s  
a special case of a stationary policy is a deterministic stationary policy  in which one
action is chosen with probability   for every state  a deterministic stationary policy can be
seen as a mapping from states to actions     s  a  for single objective mdps  there is
always at least one optimal policy   i e             that is stationary and deterministic 
theorem    for any additive infinite horizon single objective mdp  there exists a deterministic stationary optimal policy  see e g   howard        boutilier et al         
if more than one optimal policy exists  they share the same value function  known as
the optimal value function v   s    max v   s   the bellman optimality equation defines
the optimal value function recursively 
x
t  s  a  s   r s  a  s     v   s    
v   s    max
a

s

note that  because it maximizes over actions  this equation makes use of the fact that there
is an optimal deterministic stationary policy  because an optimal policy maximizes the
value for every state  such a policy is optimal regardless of the initial state distribution  
however  the state independent value  equation    may very well be different for different
initial state distributions  using   the state value function can be translated back into the
state independent value function  equation    
x
v  
 s v   s  
ss

a multi objective mdp  momdp   is an mdp in which the reward function r   s 
a  s  n describes a vector of n rewards  one for each objective  instead of a scalar 
similarly  a value function v in an momdp specifies the expected cumulative discounted
reward vector 

x
 k rk       
   
v   e 
k  

where rt is the vector of rewards received at time t  the only difference between the single
objective value  equation    and the multi objective value  equation    of a policy is that
the return  and the underlying sum of rewards  is now a vector rather than a scalar  for
stationary policies  we can also define the multi objective value of a state 
v  s    e 


x

 k rt k       st   s  

   

k  

in a single objective mdp  state value functions impose only a partial ordering because

policies are compared at different states  e g   it is possible that v   s    v   s  but v   s    
   multi objective mdps should not be confused with mixed observability mdps  ong  png  hsu    lee 
       which are also sometimes abbreviated with momdp 

  

fia survey of multi objective sequential decision making



v   s    but for a given state  the ordering is complete  i e   v   s  must be greater than 

equal to  or less than v   s   the same is true of state independent value functions 
in contrast  in an momdp  the presence of multiple objectives means that the value
function v  s  for a state s is a vector of expected cumulative rewards instead of a scalar 
such value functions supply only a partial ordering  even for a given state  for example 


it is possible that  for some state s  vi  s    vi  s  but vj  s    vj  s   similarly  for


state independent value functions  it may be that vi   vi but vj   vj   consequently 
unlike in an mdp  we can no longer determine which values are optimal without additional
information about how to prioritize the objectives  such information can be provided in
the form of a scalarization function  which we discuss in the following sections 
though not the focus of this article  there are also momdp variants in which constraints
are specified on some objectives  see e g   feinberg   shwartz        altman         the
goal of the agent is then to maximize the regular objectives while meeting the constraints
on the other objectives  constrained objectives are fundamentally different from regular
objectives because they are explicitly prioritized over the regular objectives  i e   any policy
that fails to meet a constraint is inferior to any policy that meets all constraints  regardless
of how well the policies maximize the regular objectives 

   motivating scenarios
while the momdp setting has received considerable attention  it is not immediately obvious why it is a useful addition to the standard mdp or why specialized algorithms for
it are needed  in fact  some researchers argue that modeling problems as explicitly multiobjective is not necessary  and that a scalar reward function is adequate for all sequential
decision making tasks  the most direct formulation of this perspective is suttons reward
hypothesis  which states that all of what we mean by goals and purposes can be well
thought of as maximization of the expected value of the cumulative sum of a received scalar
signal  reward   
this view does not imply that multi objective problems do not exist  indeed  that
would be a difficult claim  since it is so easy to think of problems that naturally possess
multiple objectives  instead  the implication of the reward hypothesis is that the resulting
momdps can always be converted into single objective mdps with additive returns  such
a conversion process would involve two steps  the first step is to specify a scalarization
function 
definition    a scalarization function f   is a function that projects the multi objective
value v to a scalar value 
vw  s    f  v  s   w  
where w is a weight vector parameterizing f  
for example  f may compute a linear combination of the values  in which case each element
of w quantifies the relative importance of the corresponding objective  this setting is discussed further in section         the second step is to define a single objective mdp with
   http   rlai cs ualberta ca rlai rewardhypothesis html

  

firoijers  vamplew  whiteson   dazeley

figure    the three motivating scenarios for momdps   a  the unknown weights scenario 
 b  the decision support scenario   c  the known weights scenario 

additive returns such that  for all  and s  the expected return equals the scalarized value
vw  s  
though it rarely  if ever  makes the issue explicit  all research on momdps rests on the
premise that there exist tasks for which one or both of these conversion steps is impossible 
infeasible  or undesirable  in this section  we discuss three scenarios in which this can occur
 see figure    
the first scenario  which we call the unknown weights scenario  figure  a   occurs when
w is unknown at the moment when planning or learning must occur  consider for example
a public transport system that aims to minimize both latency  i e   the time that commuters
need to reach their destinations  and pollution costs  in addition  assume that the resulting
momdp can be scalarized by converting each objective into monetary cost  economists
can compute the cost of lost productivity due to commuting and pollution incurs a tax that
must be paid in pollution credits purchased at a given price  assume also that those credits
are traded on an open market and therefore the price constantly fluctuates  if the transport
system is complex  it may be infeasible to compute a new plan every day given the latest
prices  in such a scenario  it can be preferable to use a multi objective planning method
that computes a set of policies such that  for any price  one of those policies is optimal
 see the planning or learning phase in figure  a   while doing so is more computationally
expensive than computing a single optimal policy for a given price  it needs to be done
only once and can be done in advance  when more computational resources are available 
then  when it is time to select a policy  the current weights  i e   the price of the pollution
  

fia survey of multi objective sequential decision making

credits  are used to determine the best policy from the set  the selection phase   finally 
the selected policy is employed in the task  the execution phase  
in the unknown weights scenario  scalarization is impossible before planning or learning
but trivial once a policy actually needs to be used because w is known by that time 
in contrast  in the second scenario  which we call the decision support scenario  figure
 b   scalarization is infeasible throughout the entire decision making process because of the
difficulty of specifying w  or even f   for example  economists may not be able to accurately
compute the cost of lost productivity due to commuting  the user may also have fuzzy
preferences that defy meaningful quantification  for example  if the transport system could
be made more efficient by building a new train line that obstructs a beautiful view  then a
human designer may not be able to quantify the loss of beauty  the difficulty of specifying
the exact scalarization is especially apparent when the designer is not a single person but
a committee or legislative body whose members have different preferences and agendas 
in such a system  the momdp method is used to calculate an optimal solution set with
respect to the known constraints about f and w  as figure  b shows  the decision support
scenario proceeds similarly to the unknown weights scenario except that  in the selection
phase  the user or users select a policy from the set according to their arbitrary preferences 
rather than explicit scalarization according to given weights 
in all these cases  one can still argue that scalarization before planning or learning is
possible in principle  for example  the loss of beauty can be quantified by measuring the
resulting drop in housing prices in neighborhoods that previously enjoyed an unobstructed
view  however  the difficulty with scalarization is not only that doing so may be impractical
but  more importantly  that it forces the users to express their preferences in a way that may
be inconvenient and unnatural  this is because selecting w requires weighing hypothetical
trade offs  which can be much harder than choosing from a set of actual alternatives  this
is a well understood phenomenon in the field of decision analysis  clemen         where
the standard workflow involves presenting alternatives before soliciting preferences  that
is why subfields of decision analysis such as multiple criteria decision making and multiattribute utility theory focus on multiple objectives  dyer  fishburn  steuer  wallenius   
zionts         for the same reasons  algorithms for momdps can provide critical decision
support  rather than forcing the users to specify w in advance  these algorithms just
prune policies that would not be optimal for any w  then  they offer the users a range of
alternatives from which they can select according to preferences whose relative importance
is not easily quantified 
in the third scenario  which we call the known weights scenario  figure  c   we assume
that w is known at the time of planning or learning and thus scalarization is both possible
and feasible  however  it may be undesirable because of the difficulty of the second step
in the conversion  in particular  if f is nonlinear  then the resulting single objective mdp
may not have additive returns  see section         as a result  the optimal policy may be
non stationary  see section        or stochastic  see section         which cannot occur in
single objective  additive  infinite horizon mdps  see theorem     consequently  the mdp
can be difficult to solve  as standard methods are not applicable  converting the mdp to
one with additive returns may not help either as it can cause a blowup in the state space 
  

firoijers  vamplew  whiteson   dazeley

which also leaves the problem intractable   therefore  even though scalarization is possible
when w is known  it may still be preferable to use methods specially designed for momdps
rather than to convert the problem to a single objective mdp  in contrast to the unknown
weights and the decision support scenarios  in the known weights scenario  the momdp
method only produces one policy  which is then executed  i e   there is no separate selection
phase  as shown in figure  c 
note that figure   assumes an off line scenario  planning or learning occurs only once 
before execution  however  multi objective methods can also be employed in on line settings
in which planning or learning are interleaved with execution  in the on line version of the
unknown weights scenario  the weights are better characterized as dynamic  rather than
unknown  in an on line scenario  the agent must already have seen weights in all timesteps
t     since this is a prerequisite for execution in timesteps            t     however  if the
weights change over time  the agent may not yet know the weights that will be used in
timestep t when it is in the planning or learning phase of that timestep 

   problem taxonomy
so far  we have described the momdp formalism and proposed three motivating scenarios
for it  in this section  we discuss what constitutes an optimal solution  unfortunately  there
is no simple answer to this question  as it depends on several critical factors  therefore 
we propose a problem taxonomy  shown in table    that categorizes momdps according
to these factors and describes the nature of an optimal solution in each category  our
taxonomy is based on what we call the utility based approach  in contrast to many other
multi objective papers that follow an axiomatic approach to optimality in momdps 
the utility based approach rests on the following premise  before the execution phases
of the scenarios in section    one policy is selected by collapsing the value vector of a policy
to a scalar utility  using the scalarization function  the application of the scalarization
function may be implicit and hidden  e g   it may be embedded in the thought process of
the user  but it nonetheless occurs  the scalarization function is part of the notion of utility 
i e   what the agent should maximize  therefore  if we find a set with an optimal solution
for each possible weight setting of the scalarization function  we have solved the momdp 
the utility based approach derives the optimal solution set from the assumptions that are
made about the scalarization function  which policies the user allows  and whether we need
one or multiple policies 
by contrast  the axiomatic approach begins with the axiom that the optimal solution set
is the pareto front  see section          this approach is limiting because  as we demonstrate
in this section  there are some settings for which other solution concepts are more suitable 
thus  we take a utility based approach because it makes it possible to derive the solution
concept  rather than just assuming it  when the pareto front is in fact the correct solution
   since non additive returns can depend on the agents entire history  the immediate reward function in
the converted mdp may also depend on that history and thus the state representation in the converted
mdp must be augmented to include it 
   for an example of an axiomatic approach to multi objective reinforcement learning  see the survey by
liu  xu  and hu        

  

fia survey of multi objective sequential decision making

single policy
 known weights 
deterministic
linear
scalarization

multiple policies
 unknown weights or decision support 

stochastic

one deterministic stationary
policy    

monotonically one
increasing
deterministic
scalarization
non stationary
policy    

deterministic

stochastic

convex coverage set of
deterministic stationary policies
   

one mixture
policy of two
or more
deterministic
stationary
policies    

pareto
coverage set of
deterministic
non stationary
policies    

convex
coverage set of
deterministic
stationary
policies    

table    the momdp problem taxonomy showing the critical factors in the problem and
the nature of the resulting optimal solution  the columns describe whether the
problem necessitates a single policy or multiple ones  and whether those policies
must be deterministic  by specification  or are allowed to be stochastic  the rows
describe whether the scalarization function is a linear combination of the rewards
or  whether this cannot be assumed and the scalarization function is merely a
monotonically increasing function of them  the contents of each cell describe
what an optimal solution for the given setting looks like 

concept  the utility based approach provides a justification for it  when it is not  it allows
for a more appropriate solution concept to be derived instead 
our taxonomy categorizes problem classes based on the assumptions about the scalarization function  which policies the user allows  and whether one or multiple policies are
required  we show that this leads different solution concepts  underscoring the importance
of carefully considering the choice of solution concept based on all the available information 
we discuss the three factors that constitute our taxonomy in the following order  in
section      we discuss the first factor  whether one or multiple policies are sought  a choice
that follows directly from which motivating scenario is applicable  the known weights
scenario  figure  c  implies a single policy approach while the unknown weights and decision
support scenarios  figure  a and  b  imply a multiple policy approach  in section      we
discuss the second factor  whether the scalarization function is a linear combination of the
rewards or merely a monotonically increasing function of them  in section      we discuss
the third factor  whether stochastic or only deterministic policies are permitted 
the goal of the taxonomy is to cover most research on momdps while remaining simple
and intuitive  however  due to the diversity of research on momdps  some research does
not fit neatly in our taxonomy  we note these discrepancies when discussing such research
in sections   and   
  

firoijers  vamplew  whiteson   dazeley

    single versus multiple policies
following the approach of vamplew et al          we first distinguish problems in which
only one policy is sought from ones in which multiple policies are sought  which case holds
depends on which of the three motivating scenarios discussed in section   applies 
in the unknown weights and decision support scenarios  the solution to an momdp
consists of multiple policies  though these two scenarios are conceptually quite different 
from an algorithmic perspective they are identical  the reason is that they are both characterized by a strict separation of the decision making process into two phases  the planning
or learning phase and the execution phase  though in on line settings  the agent may go
back and forth between the two  
in the planning or learning phase  w is unavailable  consequently  the planning or learning algorithm must return not a single policy but a set of policies  and the corresponding
multi objective values   this set should not contain any policies that are suboptimal for all
scalarizations  i e  we are only interested in undominated policies 
definition    for an momdp m and a scalarization function f   the set of undominated
policies  u  m    is the subset of all possible policies m for m for which there exists a w
for which the scalarized value is maximal 


u  m           m  w    m   vw  vw   

   

u  m   is sufficient to solve m  i e   for each w  it contains a policy with the optimal
scalarized value  however  it may contain redundant policies that  while optimal for some
weights  are not the only optimal policy in the set for w  such policies can be removed
while still ensuring the set contains an optimal policy for all w  in fact  in order to solve
m  we need only a subset of the undominated policies such that  for any possible w  at
least one policy in the set is optimal  this is sometimes called a coverage set  cs   becker 
zilberstein  lesser    goldman        
definition    for an momdp m and a scalarization function f   a set cs m   is a
coverage set if it is a subset of u  m   and if  for every w  it contains a policy with maximal
scalarized value  i e   if 


m
m
m

m


cs     u       w      cs           vw  vw  
   
note that u  m   is automatically a coverage set  however  while u  m   is unique  cs m  
need not be  when there are multiple policies with the same value  u  m   contains all of
them  while a coverage set need contain only one  in addition  for a given cs m    there

may exist a policy   
  cs m   for which v is different from v for all   cs m  
but which has the same scalarized value as a   cs m   for all w at which   is optimal 
in contrast to single objective mdps  in momdps whether or not a policy is in a cs m  
can depend on the initial state distribution   it is thus important to accurately specify 
when formulating an momdp 
ideally  an momdp algorithm should find the smallest cs m    however  doing so
might be harder than just finding one smaller than u  m    in section      we specialize
the coverage set for two classes of scalarization functions 
  

fia survey of multi objective sequential decision making

in the execution phase  a single policy is chosen from the set returned in the planning or
learning phase and executed  in the unknown weights scenario  we assume that w is revealed
after planning or learning is complete but before execution begins  selecting a policy then
requires only maximizing over the scalarized value of each policy in the returned set 
    argmax vw  
cs m  

in the decision support scenario  this set is manually inspected by the user s   who select a
policy for execution informally  making an implicit trade off between the objectives 
in the known weights scenario  w is known before planning or learning begins  therefore 
returning multiple policies is unnecessary  however  as mentioned in section   and discussed
further in section        scalarization can yield a single objective mdp that is difficult to
solve 
    linear versus monotonically increasing scalarization functions
the second critical factor affecting what constitutes an optimal solution to an momdp is
the nature of the scalarization function  in this section  we discuss two types of scalarization
function  those that are linear combinations of the rewards and those that are merely
monotonically increasing functions of them 
      linear scalarization functions
a common assumption about the scalarization function  e g   natarajan   tadepalli       
barrett   narayanan         is that f is linear  i e   it computes the weighted sum of the
values for each objective 
definition    a linear scalarization function computes the inner product of a weight vector
w and a value vector v
vw   w  v  
   
each element of w specifies how much one unit of value for the corresponding objective
contributes to the scalarized value  the elements of the weight vector w are all positive real
numbers and constrained to sum to   
linear scalarization functions are a simple and intuitive way to scalarize  one common
situation in which they are applicable is when rewards can be easily translated into monetary
value  for example  consider a mining task in which different policies yield different expected
quantities of various minerals  if the prices per kilo of those minerals fluctuate daily  then
the task can be formulated as an momdp  with each objective corresponding to a different
mineral  each element of v then reflects the expected number of kilos of that mineral
that are mined under  and the scalarized value vw corresponds to the monetary value
of everything that is mined  vw can be computed only when w  corresponding to the
 normalized  current price per kilo of each mineral  becomes known 
in the single policy setting  where w is known  the presence of multiple objectives poses
no difficulties given a linear f   instead  f can simply be applied to each reward vector in the
  

firoijers  vamplew  whiteson   dazeley

momdp  because the inner product computed by f distributes over addition  the result is
a single objective mdp with additive returns  in the infinite horizon setting this leads to 
vw   w  v   w  e 


x

 k rt k       e 


x

 k  w  rt k      

   

k  

k  

since this single objective mdp has additive returns  it can be solved with standard methods  yielding a single policy  as reflected in the box labeled     in table    due to theorem
   a determinstic stationary policy suffices  however  a multi objective approach can still
be preferable in this case  e g   v may be easier to estimate than vw in large or continuous
momdps where function approximation is required  see section      
in the multiple policy setting  however  we do not know w during planning or learning
and therefore want to find a coverage set  if f is linear  then u  m    which is automatically
a coverage set  consists of the convex hull  substituting equation   in the definition of the
undominated set  definition     we obtain the definition of the convex hull 
definition    for an momdp m  the convex hull  ch  is the subset of m for which
there exists a w for which the linearly scalarized value is maximal 


ch m           m  w    m   w  v  w  v   

   

figure  a illustrates the concept of a convex hull for stationary deterministic policies  each
point in the plot represents the multi objective value of a given policy for a two objective
momdp  the axes represent the reward dimensions  the convex hull is shown as a set
of filled circles  connected by lines that form a convex surface   given a linear f   the
scalarized value of each policy is a linear function of the weights  this is illustrated in
figure  b  where the x axis represents the weight for dimension    w         w      and
the y axis the scalarized value of the policies  to select a policy  we need only know the
values of the convex hull policies  which form the upper surface of the scalarized value 
as illustrated by the black solid lines  and correspond to the three convex hull policies in
figure  a  the upper surface forms a piecewise linear and convex function  such functions
are also well known from the literature on partially observable markov decision processes
 pomdps   whose relationship to momdps we discuss in section     
like any u  m    ch m   can contain superfluous policies  however  we can also define
the convex coverage set  ccs  as the specification of the coverage set when f is linear  this
is reflected in box     in table    we explain why the policies in this set are deterministic
and stationary in section        
definition    for an momdp m  a set ccs m   is a convex coverage set if it is a
subset of ch m   and if  for every w  it contains a policy whose linearly scalarized value
is maximal  i e   if 



ccs m    ch m     w      ccs m        m   w  v  w  v      
   note that the term convex hull has a slightly different meaning in the multi objective literature than
its standard geometric definition  in geometry  the convex hull of a finite set s of points in euclidean
space is the minimal subset of s so that each of the other points in s can be expressed as a convex
combination of the points in the convex hull  in a multi objective setting  we are only interested in a
particular subset of the geometric convex hull  those points of which its convex combinations are strictly
bigger  in all dimensions  than any other point in s  i e   all points that are optimal for some weight 

  

fia survey of multi objective sequential decision making

 a 

 b 

figure    example of the convex hull and pareto front  each point in  a  represents the
multi objective value of a given policy and each line in  b  represents the linearly
scalarized value of a policy across values of w  the convex hull is shown as black
filled circles in  a   and black lines in  b   the pareto front consists of all filled
points  circles and squares  in  a   and both the dashed and solid black lines in
 b   the unfilled points in  a   grey lines in  b   are dominated 

for deterministic stationary policies  the difference between ch m   and ccs m   may
often be small  therefore  the terms are often used interchangeably  however  in the case
of non stationary or stochastic policies  the difference is quite significant  as the ch can
contain infinitely many policies  while it is possible to construct a finite ccs  as we show
in section       
      monotonically increasing scalarization functions
while linear scalarization functions are intuitive and simple  they are not always adequate
for expressing the users preferences  for example  suppose in the mining task mentioned
above  there are two minerals that can be mined and only three policies are available   
sends the mining equipment to a location where only the first mineral can be mined    to
a location where only the second mineral can be mined  and   to a location where both
minerals can be mined  suppose the owner of the equipment prefers     e g   because it at
least partially appeases clients with different interests  however  it may be the case that 
because the location corresponding to   has fewer minerals  the convex hull contains only
  and     thus  the owners preference of   implies that he or she  implicitly or explicitly 
employs a nonlinear scalarization function 
here  we consider the case in which f can be nonlinear  and corresponds to a common
notion of the relationship between reward and utility  this class of possibly nonlinear scalarizations are the strictly monotonically increasing scalarization functions  these functions
adhere to the constraint that if a policy is changed in such a way that its value increases in
  

firoijers  vamplew  whiteson   dazeley

one or more of the objectives  without decreasing in any other objectives  then the scalarized
value also increases 
definition    a scalarization function f is strictly monotonically increasing if 






 i  vi  vi  i  vi   vi     w  vw   vw   

    

linear scalarization functions  with non zero positive weights  are included in this class
of functions  the condition on the left hand side of equation    is more commonly known
as pareto dominance  pareto        
definition    a policy  pareto dominates another policy   when its value is at least as
high in all objectives and strictly higher in at least one objective 






v p v  i  vi  vi  i  vi   vi  

    

demanding that f is strictly monotonically increasing is quite a minimal constraint  as it
requires only that  all other things being equal  getting more reward for a certain objective
is always better  in fact  it is difficult to think of any f that violates this constraint without
employing a highly unnatural notion of reward  
three observations are in order about strictly monotonically increasing scalarization
functions and the related concept of pareto dominance  first  unlike in the linear case  we
do not necessarily know the exact shape of f   instead  we know only that it belongs to a
particular class of functions  the solution concept that follows thus applies to any strictly
monotonically increasing f   in cases where stronger assumptions about f can be made 
more specific solution concepts are possible  however  except for linearity  we are not aware
of any such properties of f that have been exploited in solving momdps 
second  the notions of optimality introduced in section       are no longer appropriate 
the reason is that  even though the vector valued returns are still additive  equation     the
scalarized returns may not be because f may no longer be linear  as an example  consider
the well known tchebycheff scalarization function  perny   weng          
x
 v   p  w     max wi  pi  vi    
wi  pi  vi   
    
i    n

i    n

where p is an optimistic reference point  w are weights  and  is an arbitrarily small positive
constant greater than    note that the sum on the righthand side is what makes the function
strictly monotonically increasing  now  if p          p         r            r           
k
w              and       then f  v   w      but e  
k    f  rt k     w              
                this loss of additivity of the scalarized returns when applying a nonlinear
f has important consequences for which methods can be applied  as we show in section       
third  we can still identify and prune policies that are not optimal for any w for any
strictly monotonically increasing f   even though it may be nonlinear  consider the three
   in addition  if f is not strictly monotonically increasing and no other assumptions are made  then no
policies can be pruned from the coverage set  thus  computing the value of every policy in this coverage
set  which is required by the selection phase  is likely to be intractable 
   our definition differs slightly from that of perny and weng         it is multiplied by   to express
maximization instead of minimization  for the sake of consistency with the rest of this article 

  

fia survey of multi objective sequential decision making

labeled policies in figure  a  note that figure  b does not apply  because the scalarization
function is no longer linear   b has a higher value than a in one objective  but a lower
value in the other  we therefore cannot tell whether a or b ought to be preferred without
knowing w  however  c has a lower value than a in both objectives  and thus a paretodominates c  a p c  because f is strictly monotonically increasing  the scalarized value
of a is greater than that of c for all w and thus we can discard c 
for now  we defer a full discussion of what constitutes an optimal solution for an
momdp with a strictly monotonically increasing scalarization function  i e   boxes        
in table    because this depends  not only on whether the single or multiple policy setting
applies  but also on whether only deterministic or also stochastic policies are considered 
which is addressed in section     
however  we can already observe that  given any strictly monotonically increasing f   we
can use the pareto front as a set of viable policies  the pareto front consists of all policies
that are not pareto dominated 
definition    for an momdp m  the pareto front is the set of all policies that are not
pareto dominated by any other policy in m  


p f  m           m      m    v p v   

    

note that p f  m   is not the set of undominated policies u  m   for all specific strictly
monotonically increasing f   we have already seen that for the special case of linear f  
u  m     ch m    which is a subset of p f  m     for example  in figure    the pareto
front consists of the convex hull plus b   however  for any strictly monotonically increasing
f   we know that a policy that is not in the p f  m   is dominated with respect to f   i e  
   p f  m       u  m    this is because  for strictly monotonically increasing f and

  p f  m    there cannot exist a w for which  is optimal  since by definition there exists

a   such that v p v and  since f is strictly monotonically increasing  this implies

that vw   vw  
however  if we know only that f is strictly monotonically increasing  we cannot settle
for a subset of p f  m   either  because there exist strictly monotonically increasing f for
which u  m     p f  m    perny and weng        show that u  m     p f  m   for the
tchebycheff function  equation      which is strictly monotonically increasing  therefore 
we cannot discard policies from the p f  m   and retain an undominated set u  m   for all
strictly monotonically increasing f  
a pareto coverage set  pcs  of minimal size can be constructed by retaining only one
policy of the policies with identical vector values in the p f  m    we can formally define
the pcs as follows 
definition     for an momdp m  a set p cs m   is a pareto coverage set if it is a subset
of p f  m   and if  for every policy    m   it contains a policy that either dominates  
or has equal value to     i e   if 




p cs m    p f  m        m       p cs m     v p v  v   v         
again  for deterministic stationary policies the difference between a p cs m   and p f  m  
may be minor  note that p f  m   is automatically a p cs m    most papers in the
literature therefore take p f  m   as the solution 
  

firoijers  vamplew  whiteson   dazeley

we can also slightly relax the constraint on f   without having to change which policies
are in the p cs m    specifically  we can define a monotonically increasing scalarization

function as a function for which the following property holds   i  vi  vi     w  vw 

vw    this relaxation influences the set of undominated policies  while policies that are not
in the p f  m   are always dominated under a strictly monotonically increasing f   they need
not be under any monotonically increasing f   consider for example f  v   w       which
is monotonically increasing but not strictly monotonically increasing  for this function
there are no dominated policies  as every policy has the same scalarized value  however 
because the scalarized value of a policy     p f  m   cannot be greater than the scalarized
function of a policy   p cs m    we can use the p cs m   for  non strict  monotonically
increasing f   therefore  in this article  we focus on monotonically increasing f   as this is
the broader class of functions 
because the p f  m    and even a p cs m    may be prohibitively large and contain
many policies whose values differ by negligible amounts  chatterjee et al         and brazdil
et al         introduce a slack parameter   and use this to define an  approximate pareto
front  p f  m    p f  m   contains all values of policies such that for every possible policy

   m there is a policy   p f  m   such that i vi  s      vi  s   by weakening
the requirements for domination  this approach yields a smaller set that can be calculated
more efficiently 
another option for finding a smaller set than p f  m   is making additional assumptions
about the scalarization function  for example  perny  weng  goldsmith  and hanna       
introduce the notion of fairness between objectives  leading to lorentz optimality  the
additional assumption is that if the sum of the values over all objectives stays the same 
making the difference between two objectives smaller yields a higher scalarized value  this is
of course a strong assumption that does not apply as broadly as pareto optimality  however 
when it does apply  it can help reduce the size of the optimal solution set 
    deterministic versus stochastic policies
the third critical factor affecting what constitutes an optimal solution to an momdp is
whether only deterministic polices are considered or stochastic ones are also allowed  while
in most applications there is no reason to exclude stochastic policies a priori  there can be
cases when stochastic policies are clearly undesirable or even unethical  for example  if the
policy determines the clinical treatment of a patient  e g   as in work of lizotte  bowling 
and murphy        and shortreed  laber  lizotte  stroup  pineau  and murphy        
then flipping a coin to determine the course of action may be inappropriate  we denote the
m
set of deterministic policies m
d and the set of stationary policies s   both sets are subsets
m
m
m
m
of all policies  d    s     finally the set of policies that are both deterministic
m
m
and stationary is the intersection of both these sets  denoted m
ds   d  s  
in single objective mdps  this factor is not critical because  due to theorem    we can
restrict our search to deterministic stationary policies  i e  the optimal attainable value

v    however  the
is attainable with a deterministic stationary policy  maxm v    max
m


 ds

situation is more complex in momdps  in this section  we discuss how the focus on
stochastic or deterministic policies affects each setting considered in our taxonomy 
  

fia survey of multi objective sequential decision making

      deterministic and stochastic policies with linear scalarization
functions
when f is linear  a result similar to theorem   holds for momdps due to the following
corollary 
m
corollary    for an momdp m  any ccs m
ds   is also a ccs    

proof  if f is linear  we can translate the momdp to a single objective mdp  for each
possible w  this is done by treating the inner product of the reward vector and w as the
new rewards  and leaving the rest of the problem as is  since the inner product distributes
over addition  the scalarized returns remain additive  equation     thus  for every w
there exists a translation to a single objective mdp  for which an optimal deterministic and
stationary policy must exist  due to theorem    hence  for each w there exists an optimal
deterministic stationary policy  therefore  there exists a   ccs m
ds   that  is optimal

m
m
for that w  consequently  there cannot exist a      ds such that w  v   w  v
m
and thus ccs m
ds   is also a ccs    
any ccs m
ds   is thus sufficient for solving momdps with linear f   even when stochastic and non stationary policies are allowed  this is reflected in box     in table    it also
applies to box     since the optimal policy in that case is just a member of this ccs m
ds   
i e   the one that is best for the given known w 
unfortunately  no result analogous to corollary   holds for momdps with monotonically increasing f   in the rest of this section  we discuss why this is so and the consequences
for the nature of an optimal momdp solution for boxes         in table   
      multiple deterministic policies with monotonically increasing
scalarization functions
in the multiple policy setting when only deterministic policies are allowed and f is nonlinear 
non stationary policies may be better than the best stationary ones 
theorem    in infinite horizon momdps  deterministic non stationary policies can paretodominate deterministic stationary policies that are undominated by other deterministic stationary policies  white        
to see why  consider the following momdp  denoted m   adapted from an example by
white         there is only one state and three actions a    a    and a    which yield rewards
                and         respectively  if we allow only deterministic stationary policies 
then there are three possible policies            m 
ds   each corresponding to always taking
one of the actions  all of which are pareto optimal  these policies have the following
state independent values  equation     v                  v                  and
v                        however  if we now consider the set of possibly non stationary
m 
m 
policies m 
d  including non stationary ones   we can construct a policy ns  d   ds
that alternates between a  and a    starting with a    and whose value is vns         
                   consequently  ns p   when        and thus we cannot restrict our
  

firoijers  vamplew  whiteson   dazeley

attention to stationary policies   consequently  in the multiple deterministic policies case
with monotonically increasing f   we need to find a p cs m
d    which includes non stationary
policies  as shown in box     of table   
in addition to having to consider a broader class of policies  another consequence is
that defining a policy indirectly via the value function is no longer possible  in standard
single objective methods  the optimal policy can be found by doing local action selection
with respect to the value function  i e   for every state  the policy selects the action that
maximizes the expected value  however  for local selection to yield a non stationary policy  the value function must also be non stationary  i e   it must condition on the current
timestep  while this is standard in the finite horizon setting  where a different value function is computed for each timestep  it is not possible in the infinite horizon setting  we
discuss how to address this difficulty in sections   and   
      multiple stochastic policies with monotonically increasing
scalarization functions
in the multiple policy setting where stochastic non stationary policies  i e   the full set m  
are allowed  we again cannot consider only deterministic stationary policies  however  we
can employ stochastic stationary policies instead of deterministic non stationary ones  in
particular  we can employ a mixture policy  vamplew  dazeley  barker    kelarev       
m that takes a set of n deterministic
policies  and selects the i th policy from this set  i
p
with probability pi   where n
p
 
  
this leads to values that are a linear combination
i   i
of the values of the constituent policies  in our previous example  we can replace ns by a
policy m that chooses   with probability p  and   otherwise  resulting in the following
values 


 p       p   
m
 
 
v   p  v       p   v  
 
 
 
 
fortunately  it is not necessary to explicitly represent an entire p cs m   explicitly 
instead  it is sufficient to compute a ccs m
ds    the necessary stochastic policies to
create a p cs m   can then be easily constructed by making mixture policies from those
policies on the ccs m
ds   
corollary    in an infinite horizon discounted momdp  an infinite set of mixture policies
pm can be constructed from policies that are on a ccs m
ds    such that this set pm   is a
m
p cs     vamplew et al         
proof  we can construct a policy with any value vector on the convex surface  e g   the
   thereblack lines in figure  a  by mixing policies on a ccs m
ds    e g   the black dots 
fore  we can always construct a mixture policy that dominates a policy with a value under
this surface  e g   b  we can show by contradiction that there cannot be any policy above
   white        shows this in an infinite horizon discounted setting  but the arguments hold also for the
finite horizon and average reward settings 
    note that we should always mix policies that are adjacent  the line between any pair of the policies
we mix should be on the convex surface  e g  mixing the policy represented by the leftmost black dot
in figure  a and the policy represented by the rightmost black dot does not lead to optimal policies  as
the line connecting these two points is under the convex surface 

  

fia survey of multi objective sequential decision making

the convex surface  if there was  it would be optimal for some w if f was linear  consequently  by corollary    there would be a deterministic stationary policy with at least
equal value  but since the convex surface spans the values on the ccs m
ds    this leads
to a contradiction  thus  no policy can pareto dominate a mixture policy on the convex
surface 
thanks to corollary    it is sufficient to compute a ccs m
ds   to solve an momdp  as
reflected in box     of table    a surprising consequence of this fact  which to our knowledge
is not made explicit in the literature  is that pareto optimality  though the most common
solution concept associated with multi objective problems  is actually only necessary in one
specific problem setting 
observation    the multiple policy setting when f is monotonically increasing and only
deterministic policies are considered  box     of table     requires computing a pareto coverage set  when either f is linear or stochastic policies are allowed  a ccs m
ds   suffices 
wakuta        proves the sufficiency of a ccs m
ds   for monotonically increasing
scalarizations with multiple stochastic policies  box     of table    in infinite horizon
momdps  but in a different way  instead of the mixture policies in corollary    he uses stationary randomizations over deterministic stationary policies  wakuta and togawa       
provide a similar proof for the average reward case 
note that  while it is common to consider non stationary or stochastic policies when f
is nonlinear  such policies typically condition only on the current state  or the current state
and time  not the agents reward history  however  in this setting  policies that condition
on that reward history can dominate those that do not  for example  suppose there are two
objectives which can take only positive values and f simply selects the smaller of the two  i e  
f  v   w    mini vi   suppose also that  in a given state  two actions are available  which
yields rewards of        and        respectively  finally  suppose that the agent can arrive at
that state with one of two reward histories  whose discounted sums are either        or        
a policy that conditions on these discounted reward histories can outperform policies that
do not  i e   the optimal policy selects the action yielding        when the reward history sums
to        and the action yielding        when the reward history sums to         so  while for
single objective mdps the markov property and additive returns are sufficient to restrict our
attention to policies that ignore history  in the multi objective case  the scalarized returns
are no longer additive and therefore the optimal policy can depend on the history  examples
of methods that exploit this fact are the steering approach  mannor   shimkin        and
the reward augmented state thresholded lexicographic ordering method by geibel        
which are discussed in section     
      single deterministic and stochastic policies with monotonically
increasing scalarization functions
all that remains to address is the single policy setting with monotonically increasing f  
the nature of the optimal solution in this case follows directly from the reasoning given for
the multiple policy setting 
if only deterministic policies are considered  then the single policy that is sought may
be non stationary  as reflected in box     of table    for the reasons elucidated by whites
  

firoijers  vamplew  whiteson   dazeley

example  again  it is hard to define such a non stationary policy by local action selection 
due to the risk of circular dependencies in the q values 
if stochastic policies are allowed  then the optimal policy may be stochastic  but this
can be represented as a mixture policy of two or more deterministic stationary policies  as
reflected in box     of table    for the same reasons given in corollary    in both cases 
policies can potentially benefit from conditioning on the reward history 

   planning in momdps
in this section  we survey some key approaches to planning in momdps  i e   computing
an optimal policy or the coverage set of undominated policies given a complete model of
the momdp  following the taxonomy presented in section    we first consider single policy
methods and then turn to multiple policy methods for linear and monotonically increasing
scalarization functions 
    single policy planning
in the known weights scenario  w is known before planning begins  and so only a single
policy  optimal for w  must be discovered  since the momdp can be transformed to a
single objective mdp when f is linear  see section         we focus here on single policy
planning for nonlinear f  
as discussed in section        nonlinear f can cause the scalarized return to be nonadditive  consequently  single objective dynamic programming and linear programming
methods  which exploit the assumption of additive returns by employing the bellman equation  are not applicable  however  different linear programming formulations for singlepolicy planning in momdps are possible  a key feature of such methods is that they
can produce stochastic policies  which  as discussed in section    can be optimal when the
scalarization function is nonlinear  while we are not aware of any single policy planning
methods that work for arbitrary nonlinear f   methods have been developed for two special
cases  in particular  perny and weng        propose a linear programming method for
momdps scalarized using the tchebycheff function mentioned in section        because
the tchebycheff function always has a w for which any pareto optimal policy is optimal 
this approach can find any  single  policy on the pareto front  in addition  ogryczak  perny 
and weng        propose an analogous method for the ordered weighted regret metric  this
metric calculates the regret of each objective with respect to an estimated ideal reference
point  sorts these into descending order  and calculates a weighted sum in which the weights
are also in descending order 
other researchers have proposed single policy methods for momdps with constraints 
feinberg and shwartz        consider momdps with one regular objective and m objectives with inequality constraints  they show that if a feasible policy exists for this setting 
it can be deterministic and stationary after some finite number of timesteps n and that 
prior to timestep n   at most m random actions must be performed  they call this a  m  n  
policy  show that all pareto optimal values can be achieved by  m  n   policies  and propose
a linear programming algorithm that finds  approximate policies for this setting  more
general momdps with constraints have also been considered  in particular  altman       
proposes several linear programming approaches for such settings 
  

fia survey of multi objective sequential decision making

furnkranz  hullermeier  cheng  and park        propose a framework for mdps with
qualitative reward signals  which are related to momdps but do not fit neatly in our
taxonomy  qualitative reward signals indicate a preference between policies or actions
without directly ascribing a numeric value to them  since such preferences induce a partial
ordering between policies  the policy iteration method the authors propose for this setting
may be applicable to momdps with nonlinear f   as pareto dominance also induces partial
orderings  however  the authors note that multi objective tasks generally do have numeric
feedback that can be exploited  thus  they suggest that quantitative momdps can be
viewed as a subset of preference based mdps  and as such methods designed specifically
for momdps may be more efficient than general preference based methods 
    multiple policy planning with linear scalarization functions
in the multiple policy setting with linear f   we seek a ccs m
ds    note however  the
distinction between the convex hull and a convex coverage set is usually not made in the
literature 
one might argue that explicitly multi objective methods are not necessary in this setting  because one could repeatedly run single objective methods to obtain a ccs m
ds   
however  since there are infinitely many possible w  it is not obvious that all possible values for w can be covered  it might be possible to devise a way to run the single objective
methods a finite number times and still guarantee that a ccs m
ds   is produced  however 
this would be a nontrivial result and the corresponding algorithm would in essence be a
multi objective method that happens to use single objective methods as subroutines 
one approach that has been attempted to find a minimally sized ccs m
d    i e   a convex
coverage set of deterministic but not necessarily stationary policies  originally proposed by
white and kim         is to translate the momdp into a partially observable markov
decision process  pomdp   sondik         an intuitive way to think about this translation
is to imagine that there is in fact only one true objective but the agent is unaware which of
the objectives in the momdp it is  this is modeled in the pomdp by defining the state
as a tuple hs    s  i where s  is the state in the momdp and s            n  indicates which is
the true objective  the observations thus identify s  exactly but give no information about
s    note that this translation from momdps to pomdps is one way only  not every
pomdp can be translated to an equivalent momdp 
typically  an agent interacting with a pomdp maintains a belief  i e   a probability
distribution over states  in a pomdp derived from an momdp  this belief can be decomposed into a belief about s  and a belief about s    the former is degenerative because s  is
known  the latter is a vector of size n in which the i th element specifies the probability
that the i th objective is the true one  this vector is analogous to w for a linear f   in
fact  this is the reason why figure  b resembles the piecewise linear value functions often
depicted for pomdps  the only difference is whether the x axis is interpreted as w or as a
belief 
white and kim        show that  in the finite horizon case  the solution for every belief
is exactly the solution for each w  and that the solutions for the resulting pomdp are
exactly those for the original momdp  the infinite horizon case is more difficult because
infinite horizon pomdps are undecidable  madani  hanks    condon         however 
  

firoijers  vamplew  whiteson   dazeley

for a sufficiently large horizon  the solution to a finite horizon pomdp can be used as an
approximate solution to an infinite horizon momdp 
to solve the resulting pomdp  white and kim        propose a combination of sondiks
one pass algorithm  smallwood   sondik        and policy iteration for pomdps  sondik 
       however  any pomdp planning method can be used as long as it     does not
require an initial belief about the pomdp state  which would correspond to initializing
not only the momdp state but also w  and     computes the optimal policy for every
possible belief  more recently developed exact methods  e g   cassandra  littman  and
zhang        and kaelbling  littman  and cassandra         meet these conditions and
could thus be employed  approximate point based pomdp methods  spaan   vlassis 
      pineau  gordon    thrun        do not meet conditions     and     but could be
adapted to compute an approximate convex hull  by choosing a prior distribution for the
weights from which they could sample  online pomdp planning methods  ross  pineau 
paquet    chaib draa        are not applicable because they plan only for a given belief 
converting to a pomdp thus allows the use of some but not all pomdp methods for
solving momdps with linear f   however  this approach can be inefficient because it does
not exploit the characteristics that distinguish such momdps from general pomdps  i e  
that part of the state  s    is known and that no observations give any information about
s    for example  methods that compute policies trees  e g    kaelbling et al         do not
exploit the fact that only deterministic policies that are stationary functions of the state
are needed for momdps with linear f   furthermore  as mentioned before  general infinite
horizon pomdps are undecidable  but for momdps it is in fact possible to compute the
ccs m
ds   exactly 
for these reasons  researchers have also developed specialized planning methods for this
setting  viswanathan  aggarwal  and nair        propose a linear programming approach
for episodic momdps  wakuta and togawa        propose a policy iteration approach
that has three phases  the first phase uses policy iteration to narrow down the set of
possibly optimal policies  the second phase uses linear programs to check for optimality 
since this does not necessarily give a definitive answer  the third phase uses another linear
program to handle any undetermined solutions left after the second phase 
barrett and narayanan        propose convex hull value iteration  chvi   which computes the ch m
   in every state  chvi extends conventional value iteration by storing a
 ds
set of vectors  q  s  a  for each state action pair  representing the convex hull of policies involving that action  these sets of vectors correspond to the q values in the single objective
setting  they contain the optimal q values for all possible w  when a backup operation is
performed  the q hulls at the next state s are propagated back to s  for each possible next
s 
state s   all possible actions a are considered  i e  the union of convex hulls a q  s   a   is
taken   and weighted by the probability of s occurring when taking action a in state s  this
procedure is very similar to the witness algorithm  kaelbling et al         for pomdps 
lizotte et al         propose a value iteration approach for the finite horizon setting
that computes a different value function for each timestep  in addition  it uses a piecewise
linear spline representation of the value functions  the authors prove this offers asymptotic
time and space complexity improvements over the representation used by chvi and also
enables application of this algorithm to momdps with continuous states  however  the
  

fia survey of multi objective sequential decision making

algorithm is only applicable to problems with two objectives  this limitation is addressed
in the authors subsequent work  lizotte  bowling    murphy        which extends the
algorithm to an arbitrary number of objectives and provides a detailed implementation for
the case of three objectives 
    multiple policy planning with monotonically increasing scalarization
functions
in this section  we consider planning in momdps with monotonically increasing f   as discussed in section      when stochastic policies are allowed  mixture policies of deterministic
stationary policies are sufficient  therefore  we focus on the case when only deterministic
policies are allowed and consider methods that compute a p cs m
d    which can include
non stationary policies  the distinction between the p f  m
 
and
p
cs m
d
d   is usually not
made in the literature 
as in the linear case  scalarizing for every w and obtaining a p cs m
d   by the singleobjective methods is problematic  again there are infinitely many w to consider but  unlike
the linear case  there is the additional difficulty that the scalarized returns may no longer
be additive  which can make single objective methods inapplicable 
daellenbach and kluyver        present an algorithm for multi objective routing tasks
 essentially deterministic momdps   their approach uses dynamic programming in conjunction with an augmented state space to find all non pareto dominated policies iteratively 
where the number of iterations equals the maximum number of steps in the route  the algorithm finds undominated sub policies in parallel  the authors use two alternative explicit
scalarization functions  which they call the weighted minsum and weighted minmax operators  first  the values of all solutions are translated   for each objective  the new value
becomes the fractional difference between the optimal values for that objective across all
solutions  then  the value for that objective is multiplied by a positive weight  finally 
either the minimum of the sum  minsum  or the minimum of the maximal value  minmax  
of these new weighted fractional differences is chosen as the scalarization  note that both
scalarization functions are monotonically increasing in all objectives  as the optimal value
for each objective individually does not depend on the scalarization function 
white        extends this work by proposing a dynamic programming method that
approximately solves infinite horizon momdps  it does so by repeatedly backing up according to a multi objective version of the bellman equation  since the policies can be
non stationary  the size of the pareto front grows rapidly in the number of backups applied 
however  white notes that this number need not be too large before acceptable approximations are reached  nonetheless  this approach is feasible only for small momdps 
wiering and de jong        address this difficulty with a dynamic programming method
called con modp for deterministic momdps that computes optimal stationary policies 
con modp works by enforcing consistency during dp updates  a policy is consistent if
it suggests the same action at all timesteps for a given state  if an inconsistent policy is
inconsistent only in one state action pair  con modp makes it consistent by forcing the
current action to be taken each time the current state is visited  if the inconsistency runs
deeper  the policy is discarded 
  

firoijers  vamplew  whiteson   dazeley

by contrast  gong        proposes a linear programming approach that finds the paretofront of stationary policies  however  as the authors note  this approach is also suitable
only to small momdps because the number of constraints and decision variables in the
linear program increase rapidly as the state space grows 
as mentioned in section        one way to cope with intractably large pareto fronts is
to compute instead an  approximate pareto front  which can be much smaller  chatterjee
et al         propose a linear programming method that computes the  approximate front
for an infinite horizon momdp  while chatterjee        propose an analogous algorithm
for the average reward setting  in both cases  stationary stochastic policies are shown to be
sufficient 
another way to improve scalability in this setting is to give up on planning for the whole
state space and instead plan on line for the agents current state  using a monte carlo tree
search approach  kocsis   szepesvari         such approaches  which have proven very
successful  e g   in the game of go  gelly   silver         are increasingly popular for
single objective mdps  wang and sebag        propose a monte carlo tree search method
for deterministic momdps  single objective tree search methods typically optimistically
explore the tree by selecting actions that maximize the upper confidence bound of their
value estimates  the multi objective variant does the same  but with respect to a scalar
multi objective value function whose definition is based on the hypervolume indicator induced by the proposed action together with the set of pareto optimal policies computed so
far  the hypervolume indicator  zitzler  thiele  laumanns  fonseca    da fonseca       
measures the hypervolume that is pareto dominated by a set of points  since the pareto
front maximizes the hypervolume indicator  this optimistic action selection strategy focuses
the tree search on the branches most likely to compliment the existing archive 

   learning in momdps
the methods reviewed in section   assume that a model of the transition and reward
dynamics of the momdp are known  in cases where such a model is not directly available 
multi objective reinforcement learning  morl  can be used instead 
one way to carry out morl is to take a model based approach  i e   use the agents
interaction with the environment to learn a model of the transition and reward function of
the momdp and then apply multi objective planning methods such as those described in
section    though such an approach seems well suited to morl  only a few papers have
considered it   e g   lizotte et al                we discuss opportunities for future work
in model based morl in section      instead  most of the work in morl has focused on
model free methods  where a model of the transition and reward function is never explicitly
learned 
in this section  we survey some key morl approaches  while the majority of these
methods are for the single policy setting  multiple policy methods have also been developed  at first glance  it may seem that multiple policy methods are unlikely to be effective
in the learning setting  since finding more policies would increase sample costs  not just
computational costs  and the former is typically a much scarcer resource  however  modelbased methods can obviate this issue  once enough samples have been gathered to learn a
useful model  finding policies optimal for more weights requires only computation  model  

fia survey of multi objective sequential decision making

free methods can also be practical for the multiple policy setting if they employ off policy
learning  sutton   barto        precup  sutton    dasgupta         which makes it possible to learn about one policy using data gathered by another  in this way  policies for
multiple weight settings can be optimized using the same data 
    single policy learning methods
in the known weights scenario  a morl algorithm aims to learn a single policy that is
optimal for the given weights  as discussed in section      under linear scalarization this
is equivalent to learning the optimal policy for a single objective mdp and so standard
temporal difference  td  methods  sutton        such as q learning  watkins        can
easily be applied 
however  even though no specialized methods are needed to address this setting  it
is nonetheless the most commonly studied setting for morl  linear scalarization with
uniform weights  i e   all the elements of w are equal  forms the basis of the work of karlsson
        ferreira  bianchi  and ribeiro         aissani  beldjilali  and trentesaux        and
shabani        amongst others  while non uniform weights have been used by authors such
as castelletti et al          guo et al         and perez et al          the majority of this
work uses td methods  which work on line  although castelletti et al         extend off line
fitted q iteration  ernst  geurts    wehenkel        to multiple objectives 
in most cases  the only change made to the underlying rl algorithm is that  rather than
scalarizing the reward function and then learning a scalar value function in the resulting
single objective mdp  a vector valued value function is learned in the original momdp and
then scalarized only when selecting actions  the argument for this approach is that the
values of individual objectives may be easier to learn than the scalarized value  particularly
when function approximation is employed  tesauro et al          for example  each function
approximator can ignore any state variables that are irrelevant to its objective  reducing
the size of the state space and thereby speeding learning 
as discussed in section        linear scalarization may not be appropriate for some scenarios  vamplew  yearwood  dazeley  and berry        demonstrate empirically that this
can have practical consequences for morl  therefore  morl methods that can work with
nonlinear scalarization functions are of substantial importance  unfortunately  as illustrated
in section        coping with this setting is especially challenging  since algorithms such as
td methods that are based on the bellman equation are inherently incompatible with
nonlinear scalarization functions due to the non additive nature of the scalarized returns 
four main classes of single policy morl methods using non linear scalarization have
arisen  which differ in how they deal with this issue  the first class simply applies td
methods without modification  these approaches either resign themselves to being heuristics that are not guaranteed to converge or impose restrictions on the environment to ensure
convergence  the second class modifies either the td algorithm or the state representation
such that the issue of non additive returns is avoided  the third class uses td methods
to learn multiple policies using linear scalarization with different values for w  and then
forms a stochastic or non stationary meta policy from them that is optimal with respect
to a nonlinear scalarization  the fourth class uses policy search methods  which do not
  

firoijers  vamplew  whiteson   dazeley

make use of the bellman equation and hence can be directly applied in combination with
nonlinear scalarizations 
the first class includes methods that model the problem as a multi agent system  with
one agent per objective  each agent learns and recommends actions on the basis of the
return for its own objective  a global switch then selects a winning agent  whose recommended action is followed for the current state  examples include a simple winner takes all
approach in which the agent whose recommended action has the highest q value is selected 
or more sophisticated approaches such as w learning  humphrys        where the selected
action is the one that will incur the most loss if it is not followed  one key weakness of
such approaches was pointed out by russell and zimdars         they do not allow for the
selection of actions that  while not optimal for any single objective  offer a good compromise
between multiple objectives  another key weakness is that  since the actions selected at
different timesteps may be recommended by different agents  the resulting behavior corresponds to a policy that combines elements of those learned by each agent  this combination
may not be optimal even for a single objective  i e   it may be pareto dominated and perform
arbitrarily poorly 
td has also been used directly with nonlinear scalarization functions that do allow for
the consideration of all actions  not just those which are optimal with regards to individual
objectives  scalarization functions based on fuzzy logic have been proposed for problems
with discrete actions by zhao  chen  and hu        and for problems with continuous
actions by lin and chung         a widely cited approach to nonlinear scalarization is
that of gabor  kalmar  and szepesvari         which is designed for tasks where constraints
must be satisfied for some objectives  a lexicographic ordering of the objectives is defined
and a threshold value is specified for all objectives except the last  state action values for
each objective that exceed the corresponding threshold are clamped to that threshold value
prior to applying the lexicographic ordering  thus  this thresholded lexicographic ordering
 tlo  approach to scalarization maximizes performance on the last objective subject to
meeting constraints on the other objectives as specified by the thresholds 
while methods combining td with nonlinear scalarization may converge to a suitable
policy under certain conditions  they can also converge to a suboptimal policy or even fail
to converge under other conditions  for example  issabekov and vamplew        demonstrate empirically that tlo can fail to converge to a suitable policy for episodic tasks if a
constrained objective receives non zero rewards at any timestep other than the end of the
episode  in general  methods based on the combination of td and nonlinear scalarization
must be regarded as heuristic in nature  or applicable only to restricted classes of problems 
the second class avoids the problems caused by non additive scalarized returns by modifying either the td algorithm or the state representation  to our knowledge  two approaches
proposed by geibel        to address the limitations of tlo are the only members of this
class  both require that the reward accumulated for each objective over the current episode
be stored  in the first algorithm  local decision making is based on the scalarized value of
the sum of the cumulative reward and the current state action values  this eliminates the
problem of non additive returns  but yields a policy that is non stationary with respect to
the observed state  meaning the algorithm may not converge  the second approach augments the state representation with the cumulative reward  this approach converges to the
correct policy but learns slowly  due to the increase in the size of the state space 
  

fia survey of multi objective sequential decision making

the third class uses td methods only to learn policies based on linear scalarizations 
a policy selection mechanism based on a nonlinear scalarization is then used to form a
meta policy from these base policies  the multiple directions reinforcement learning
 mdrl  algorithm of mannor and shimkin              uses such an approach in the
context of on line learning for non episodic tasks  the user specifies a target region within
which the long term average reward should lie  an initial active policy is chosen arbitrarily
and followed until the average reward moves outside of the target region and the agent is
in a specified reference state  at this point  the direction from the current average reward
vector to the closest point of the target set is calculated  and the policy whose direction best
matches this target direction is selected as the active policy  in this way  the average reward
is steered towards the users specified target region  while the underlying base policies
utilize linear scalarization  the nature of the policy selection mechanism means that the
overall non stationary policy formed from these base policies is optimal for the nonlinear
scalarization specified by the users defined target set  vamplew et al         suggest a
similar approach for episodic tasks  with td used first to learn policies that are optimal
under linear scalarization for a range of different w  before a stochastic mixture policy is
constructed that is optimal with regards to a nonlinear scalarization 
the fourth class uses policy search algorithms that directly learn a policy without learning a value function  for single policy morl  research on policy search approaches has
focused on policy gradient methods  sutton  mcallester  singh    mansour        kohl  
stone        kober   peters         in such methods  a policy is iteratively adjusted in the
direction of the gradient of the value with respect to the parameters  usually probability
distributions over actions per state  of a policy  shelton        proposes an algorithm that
first learns the optimal policy for each individual objective  these are used as base policies to form an initial mixture policy that stochastically selects a base policy at the start
of each episode  a hill climbing method based on a weighted convex combination of the
normalized objective gradients iteratively improves the mixture policy  this approach does
not directly fit our taxonomy because the returns themselves are never scalarized  instead 
the weights are used to find a step direction relative to the current policy parameters  from
a practical perspective  its behavior is akin to that of single policy rl using a nonlinear
scalarization function  as it converges to a single pareto optimal policy that need not lie
on the convex hull  uchibe and doya        also propose a policy gradient method for
morl called constrained policy gradient rl  cpgrl  which uses a gradient projection technique to find policies whose average reward satisfies constraints on one or more of
the objectives  like sheltons approach  cpgrl learns stochastic policies and works with
nonlinear scalarization functions 
    multiple policy learning with linear scalarization functions
in the unknown weights and decision support scenarios  if f is linear  then morl algorithms
aim to learn a ccs of the possible policies  a simple but inefficient approach used by
castelletti et al         is to run td multiple times with different values of w  in the
simplest case  the runs are conducted sequentially to gradually build up an approximate
ccs  natarajan and tadepalli        showed that this approach can be made more efficient
by reusing the policies learned on the earlier runs for the most similar w  they show that
  

firoijers  vamplew  whiteson   dazeley

this improves greatly on sample costs when learning a policy for w similar to those already
visited in previous runs  however  many samples are typically still required before a good
approximate ccs is obtained 
a more sophisticated approach to approximating a convex coverage set is to learn multiple policies in parallel  several algorithms have been proposed to achieve this within a
td learning framework  the approach of hiraoka  yoshida  and mishima        is similar
to the chvi planning algorithm of barrett and narayanan         see section      in that
it learns in parallel the optimal value function for all w  using a convex hull representation 
this approach is prone to infinite growth in the number of vertices in convex hull polygons 
and so a threshold margin is applied to the hull representations on each iteration  eliminating points that contribute little to the hulls hypervolume  hiraoka et al         present an
algorithm to adapt the margins during learning to improve efficiency  but note that many
parameters must be tuned for effective performance  mukai  kuroe  and iima        present
a similar extension of chvi to a learning context  they address the problematic growth
in the number of values stored by pruning vectors after each q value update  a vector is
selected at random from the set of vectors stored for the given state action pair and all
others lying within a threshold distance of it are deleted 
the approaches of both hiraoka et al         and mukai et al         are designed for
on line learning  by contrast  multi objective fitted q iteration  mofqi   castelletti 
pianosi    restelli              is an off line approach to learning multiple policies  mofqi
is a multi objective extension of the fitted q iteration  fqi  algorithm  ernst et al        
which uses a combination of historical data about the single step transition dynamics of the
environment  an initial function approximator  and the q learning update rule to construct
a dataset that maps state action pairs to their expected return  this dataset is then used
to train an improved function approximator and the process repeats until the values of the
function approximator converge  mofqi provides a computationally efficient extension of
fqi to multiple objectives by including w in the input to the function approximator and
constructing an expanded training data set containing training instances with randomly
generated ws  since the learned function generalizes across weight space in addition to
state action space  it can be used to construct a policy for any w 
as discussed in section      lizotte et al         and lizotte et al         describe a valueiteration algorithm to find the convex hull of policies for finite horizon tasks  they note that
this method can be applied in a learning context by estimating a model of the state transition
probabilities and immediate rewards on the basis of experience of the environment  this
approach is demonstrated for the task of analyzing randomized drug trial data by producing
these estimates from the historical data gathered during the clinical trials 
    multiple policy learning with monotonically increasing scalarization
functions
if f is nonlinear  then morl algorithms for the unknown weights and decision support scenarios should aim to learn a pcs  as in the linear scalarization case  the simplest approach
is to run single objective algorithms multiple times with varying w  shelton        demonstrates this approach with a policy gradient algorithm  while vamplew et al         do the
same with the tlo method of gabor et al          this approach however  requires that
  

fia survey of multi objective sequential decision making

f is explicitly known to the learning algorithm  which may be undesirable in the decision
support scenario 
to our knowledge  there are currently no methods for learning multiple policies with
nonlinear f using a value function approach  while it might seem possible to adapt convex
hull methods such as chvi by using pareto dominance operators in place of convex hull
calculations  doing so is not straightforward  because the scalarized values of policies in a
certain state are non additive  we cannot restrict ourselves to stationary policies if we want
to find all deterministic pareto optimal policies  as mentioned in section         however 
for the bellman equation from chvi to work  additivity  and the resulting sufficiency of
deterministic policies  is required  we discuss options for developing multiple policy learning
methods for nonlinear f in sections     and     
given the extensive research on both multi objective evolutionary algorithms  moeas 
 coello coello  lamont    van veldhuizen        tan  khor  lee    sathikannan       
drugan   thierens        and evolutionary methods for rl  whiteson         there is surprisingly little work on evolutionary approaches to morl  as these methods are populationbased  they are well suited to approximating pareto fronts  and would thus seem a natural
fit when f is nonlinear  to our knowledge  handa      b  was the first to apply moeas
to morl  by extending estimation of distribution  eda  evolutionary algorithms to handle multiple objectives  eda rl  handa      a  uses conditional random fields  crf 
to represent probabilistic policies  an initial set of policies are used to generate a set of
episodes  the best episodes from this set are selected and crfs that are likely to produce these trajectories are generated  the policies formed from these crfs then constitute
the next generation  handa      b  extends eda rl to momdps by using a paretodominance based fitness metric to select the best episodes 
soh and demiris        also apply moeas to morl  policies are represented as
stochastic finite state controllers  sfsc  and are optimized using two different moeas 
nsga   a standard evolutionary algorithm  and mcma  an eda  the use of sfscs gives
rise to a large search space  necessitating the addition of a local search operator  the local
search generates a random w  uses it to scalarize the rewards  and performs gradient based
search on the sfsc  empirical comparisons on multi objective variants of three pomdp
benchmarks demonstrate that the evolutionary methods are generally superior to the purely
local search approach  and that local search combined with evolution usually outperforms
the purely evolutionary methods  this is one of very few papers to directly consider partially
observable momdps 

   momdp applications
multi objective methods for planning and learning have been employed in a wide range
of applications  both in simulation and real world settings  in this section  we survey
these applications  for the sake of brevity  this list is not comprehensive but instead aims
to provide an illustrative range of examples  first  we discuss the use of multi objective
methods in specific applications  second  we discuss research that has identified broader
classes of problems in which multi objective methods can play a useful role 
  

firoijers  vamplew  whiteson   dazeley

    specific applications
an important factor driving interest in multi objective decision making is the increasing
social and political emphasis on environmental concerns  more and more  decisions must
be made that trade off economic  social  and environmental objectives  this is reflected in
the fact that a substantial proportion of applications of multi objective methods have an
environmental component 
perhaps the most extensively researched application is the water reservoir control problem considered by castelletti et al          castelletti  pianosi  and soncini sessa        
castelletti et al               and castelletti  pianosi  and restelli         the general
task is to find a control policy for releasing water from a dam while balancing multiple uses
of the reservoir  including hydroelectric production and flood mitigation  management of
hydroelectric power production has also been examined by shabani         another environmental application is that of forest management to balance the economic benefits of
timber harvesting with environmental or aesthetic objectives  which has been demonstrated
in simulation by both gong        and bone and dragicevic        
several researchers have also considered environmentally motivated applications concerning the management of energy consumption  the saves system developed by kwak
et al         controls various aspects of a commercial building  lighting  heating  airconditioning  and computer systems  to provide a suitable trade off between energy consumption and the comfort of the buildings occupants  simulation results indicate that
saves can reduce energy consumption approximately     compared to a manual control
system  while maintaining or slightly improving occupant comfort  both tesauro et al 
       and liu et al         consider the problem of controlling a computing server  with
the objectives of minimizing both response time to user requests and power consumption 
guo et al         apply morl to develop a broker agent in the electricity market  the
broker sets caps for a group of agents that sit below it in a hierarchy and manage energy
consumption at a device level  and must balance energy cost and system stability 
shelton        also examines the application of morl to developing broker agents 
however  in this case the agents task is financial rather than environmental  acting as a
market maker that sets buy and sell prices for resources in a market  the aim is to balance
the objectives of maximizing profit and minimizing spread  the difference between the buy
and sell prices  as that will lead to a larger volume of trades    
computing and communications applications have also been widely considered  perez
et al         apply morl to the allocation of resources to jobs in a cloud computing scenario  with the objectives of maximizing system responsiveness  utilization of resources  and
fairness amongst different classes of user  comsa et al         consider how to maximize
system throughput and ensure user equity in the context of a long term evolution mobile
communications packet scheduling protocol  tong and brown        use constraint based
scalarization to address the tasks of call access control and routing in a broadband multimedia network  their system aims to maximize profit  a function of throughput  while
satisfying constraints on quality of service metrics  capacity constraints and fairness constraints   and uses methods similar to that of gabor et al          zheng  li  qiu  and gong
    sheltons model of the market does not directly model trading volume  and so spread is used as a proxy
for volume 

  

fia survey of multi objective sequential decision making

       also use constrained morl methods to make routing decisions in a cognitive radio
network  aiming to minimize average transmission delay while maintaining an acceptably
low packet loss rate 
industrial and mechanical control  an important application for single objective mdp
methods  has also been explored by momdp researchers  aoki  kimura  and kobayashi
       apply distributed rl to control a sewage flow system  exploiting the systems hierarchical structure to find a solution that minimizes violation of stock levels at each node
in the flow system  while smoothing variation in flow at the source  aissani et al        
apply morl to maintenance scheduling within a manufacturing plant to minimize the time
taken to complete all maintenance tasks and machine downtime  aissani  beldjilali  and
trentesaux        build on this work by applying it to a simulation of a real petroleum refinery and demonstrating the ability to adapt to unscheduled corrective maintenance required
due to equipment failures  the control of a wet clutch in heavy duty transmission systems
is examined by van vaerenbergh et al          there are twin objectives of minimizing
engagement time  while also making the transition smooth 
robotics are also a popular application for momdps  though most work so far has been
in simulation rather than on real robots  maravall and de lope        consider the control
of a two limbed brachiating robot  with the objectives of moving in a desired direction
while avoiding collisions     nojima  kojima  and kubota        also attempt to balance
the objectives of progress to a target and collision avoidance  their agent makes use of
predefined behavioral modules for target tracing  collision avoidance  and wall following 
with morl used to dynamically adjust the weighting of the modules  meisner       
identifies social robots as a promising application of momdp methods  their behavior is
inherently multi objective because they must carry out a task without causing anxiety or
discomfort for humans 
morl has also been applied to the control of traffic infrastructure  yang and wen
       apply it to the control of freeway on ramps and vehicle management systems  aiming
to maximize both the throughput and equity of a freeway system  multiple agents with
shared policies are used  with action selection occurring via negotiation between agents 
similarly  dusparic and cahill        apply morl to control traffic lights at intersections
in an urban environment to minimize waiting time of two different classes of vehicles  yin 
duan  li  and zhang        and houli  zhiheng  and yi        also apply morl to traffic
light control  the novelty of their approach lies in considering different objectives based
on the current state of the road system  minimizing vehicle stops is prioritized when traffic
is free flowing  minimizing waiting time is emphasized when the system is at medium load 
and minimizing queue length at intersections is targeted when the system is congested 
lizotte et al               consider a medical application  prescribing an appropriate
drug regime for a patient so as to achieve an acceptable trade off between the drugs effectiveness and the severity of its side effects  their system learns multiple policies based on
    in many robotic applications it may be ideal to avoid collisions completely  but in some environments
this may not be possible  e g   in the presence of moving obstacles whose velocity is both faster than that
of the robot  and difficult to predict  such as may be the case for humans or human controlled vehicles 
and so reducing both the likelihood and impact of collisions may be more reasonable than attempting
to find a collision free policy  see for example holenstein and badreddin        and pervez and ryu
       

  

firoijers  vamplew  whiteson   dazeley

static data produced during randomized controlled drug trials  the selection of the best
treatment for a specific patient is then made by a doctor based on that patients individual circumstances  this application is an excellent example of a problem where stochastic
approaches like mixture policies are inappropriate  a policy that maximizes both symptom relief and also side effects for one patient and then minimizes side effects but also
symptom relief for the next patient may appear to give excellent results when averaged
across episodes  however  the experience of each individual patient will likely be regarded
as undesirable 
    applications within broader planning and learning tasks
in addition to the specific applications discussed above  several authors have identified more
general classes of tasks in which multi objective sequential decision making can be applied 
      probabilistic and risk aware planning
cheng  subrahmanian  and westerberg        argue that decision making under uncertainty is inherently multi objective in nature  even if there is only a single reward to be
considered  such as profit   the environmental uncertainty means that the expected value
alone is insufficient to support good decision making  the decision maker must also consider
the variance of the return  similarly  bryce        states that probabilistic planning is
inherently multi objective due to the need to optimize both the cost and the probability of
success of the plan  he criticizes approaches that either aggregate these factors or bound one
and then optimize the other  arguing in favor of explicitly multi objective methods  the
aptly named probabilistic planning is multi objective  paper by bryce  cushing  and
kambhampati        demonstrates how this might be achieved  describing a method based
on multi objective dynamic programming over belief states  and a multi objective extension
of the looping ao  search algorithm to find the set of pareto optimal plans  recent work
by kolobov  mausam  and weld        and teichteil konigsbuch      b  examine the extension of stochastic shortest path  ssp  methods to problems where dead end states exist 
ssp methods assume that at least one policy exists which is guaranteed to reach the goal 
in the presence of dead ends no such policy exists  and so the authors propose algorithms
which aim to both maximize the probability of reaching the goal and minimize the cost of
the paths found to that goal 
bryce        notes that a probabilistic plan fails when the environment enters a non goal
absorbing state  hence  multi objective probabilistic planning has strong parallels with the
research into risk aware rl carried out by geibel        and geibel and wysotzki        
which add a second reward signal indicating the transition of the environment into an error
state  defourny  ernst  and wehenkel        also provide useful insights into the incorporation of risk awareness into mdp methods  they review a range of criteria proposed
for constraining risk  and note that many of these are nonlinear and produce non additive
scalarized returns that are incompatible with the local decision making of methods based
on the bellman equation  they recommend that custom risk control requirements should
be mostly enforced heuristically  by altering policy optimization procedures and checking
the compliance of the policies with the initial requirements  multi policy momdp methods treating risk as an additional objective would satisfy this requirement  having iden  

fia survey of multi objective sequential decision making

tified the coverage set  any risk aware metric can then be used to select the best policy 
however  some measures of risk may not be expressed directly as discounted cumulative
rewards  for example  an agent may wish to minimize the variance in the expected return for a particular reward signal rather than its discounted cumulative value  methods
based on multi objective probabilistic model checking  courcoubetis   yannakakis       
forejt  kwiatkowska  norman  parker    qu        forejt  kwiatkowska    parker       
teichteil konigsbuch      a   which evaluate whether a system modelled as an mdp satisfies multiple  possibly conflicting  properties  may be suitable for such tasks 
      multi agent systems
the use of mdps within multi agent systems has been widely explored  bosoniu  babuska 
  schutter         and several authors have proposed approaches that are strongly related
to momdps  in a multi agent system  each agent has its own objective  but for effective
overall performance must also consider how its actions will affect the other agents  if the
agents are not completely self interested  then this problem can be framed as an momdp by
treating the effects on other agents as additional objectives  for example  mouaddib       
uses multi objective dynamic programming to facilitate cooperation between multiple agents
whose underlying goals may be conflicting  for each state action pair  each agent stores
three values  its local utility  the gain other agents receive  and the penalty it inflicts on
other agents  the policy for each agent is then established by converting these vector values
to regret ratios and applying a leximin ordering to these ratios 
dusparic and cahill        compare the application of morl to multi agent tasks with
other multi agent methods such as evolutionary and ant colony algorithms  dusparic and
cahill        extend the w learning algorithm of humphrys         each agent learns
both local policies  one for each of its own objectives  and remote policies  one for each
local policy of each of its neighboring agents   at each timestep  all local policies and all
active remote policies of each agent nominate actions  and a winning action is selected by
combining action values across all nominating policies  a weighting term is applied to the
values of remote policies to determine the level of cooperation each agent offers its neighbor 
experimental results in an urban traffic control simulator show substantial improvement
when the level of cooperation is non zero  this work is similar to that of schneider  wong 
moore  and riedmiller         which addresses the use of multiple agents in a distributed
network such as a power distribution grid  where the aim is to maximize a global reward
formed from a combination of each agents local reward  they demonstrate that if each
agent focuses only on its own local reward  the policies learned may not maximize the global
reward  and that performance is improved by having each agent perform linearly scalarized
learning using both its own local reward and the rewards of its neighboring agents 
      multi objective optimization using reinforcement learning
reinforcement learning is primarily applied to sequential decision making tasks in a dynamic
environment  however it can also be employed to control search mechanisms for static optimization tasks such as scheduling  carchrae   beck         multi objective optimization
for static tasks such as design is a well established field and  while the majority of this work
  

firoijers  vamplew  whiteson   dazeley

has employed mathematical or evolutionary approaches  coello coello et al          a few
authors have explored the application of reinforcement learning in such contexts 
mariano and morales            b      a  investigate the use of rl methods  ant q
and q learning  as a search mechanism for optimization of multi objective design tasks  the
values of the decision variables are considered as the current state  and actions are defined
that alter the values of those variables  multiple agents explore the state space in parallel 
agents are divided into families  where each family focuses on a single objective  at the
end of an episode  the final states found by each agent are evaluated  undominated solutions are kept in an archive and the agents that discovered those solutions are rewarded 
increasing the likelihood of similar policies being followed in the future  the method is
shown to work on a small number of test problems from the evolutionary multi objective
optimization literature  liao  wu  and jiang        apply rl to search for static control
settings for a power generation system with the objectives of reducing fuel usage and ensuring voltage stability  they propose an rl algorithm that is formulated specifically for tasks
with high dimensional state spaces  and compare its performance against an evolutionary
multi objective algorithm  finding that the rl method discovers fronts that are both more
accurate and better distributed  while also improving the speed of search 
note that to effectively apply rl to multi objective optimization  assumptions are usually made about the nature of the environment  for example  liao et al         require
that each action increases or decreases the value of precisely one state variable  as a result 
these methods are likely to have limited applicability to the more general morl problems
described earlier 

   future work
in this section  we enumerate some of the possibilities for future research in multi objective
planning and learning 
    model based methods
as mentioned in section    there has been very little work on model based approaches to
morl  given the breadth of planning methods for momdps  which could be employed in
model based morl methods as subroutines  this is surprising  to our knowledge  the only
work in this area is that of lizotte et al                where a model of the momdps
transition probabilities and reward function is derived from historical data  and then a
spline based multi objective value iteration approach is applied to that model  in general 
learning such models seems only negligibly harder than in the single objective setting  since
estimates of each reward function can just be learned separately  the problem of learning
the transition function  generally considered the hard part of model learning  is identical to
the single objective setting  especially in multiple policy scenarios  model based approaches
to morl could greatly reduce sample costs  once the model has been learned  an entire
ccs or pcs can be computed off line  without requiring additional samples 
   

fia survey of multi objective sequential decision making

    learning multiple policies with monotonically increasing scalarization
functions using value functions
as mentioned in section      we are not aware of any methods that use a value function
approach to learn multiple policies on the pcs  when stochastic policies are permitted 
the problem is easier because we can learn ccs m
ds    and use either mixture policies
 vamplew et al         or stationary randomizations  wakuta        of the policies on
this ccs  see section         however  when only deterministic policies are permitted  the
problem is more difficult  one option could be to use a finite horizon approximation to the
infinite horizon problem  by planning backwards from the planning horizon  the expected
reward of t timesteps to go approximates the infinite horizon value better and better as
t    as mentioned in section      similar approaches have been used in the pomdp
setting  another way to find good approximations to non stationary policies could be to
learn stationary policies  perhaps by extending con mdp  wiering   de jong        to
the learning setting   and prefix them by t timesteps of non stationary policy 
    many objective sequential decision making
the majority of the research reviewed in this article  both theoretical and applied  deals with
momdps with only a few objectives  this mirrors the state of early evolutionary multiobjective research  which focused almost exclusively on problems with two or at most three
objectives  however  over the last decade there has been growing interest in evolutionary
methods for so called many objective problems  which have at least four and sometimes
more than fifty objectives  ishibuchi  tsukamoto    nojima         this research has
shown that many algorithms that perform well for a few objectives scale poorly in the
number of objectives  necessitating special algorithms for the many objective setting 
while many objective mdps have received little consideration so far  there are numerous real world control problems that can be naturally modeled in this way  for example 
fleming et al         point out that many objective control problems commonly arise in
engineering  and give the example of a jet engine control system with eight objectives  as
with the many objective problems considered in evolutionary computation  it seems likely
that at least some of the methods explored so far will scale poorly in the number of objectives  for example  the multi policy momdp planning algorithm described by lizotte
et al         is limited to problems with two objectives 
a key challenge posed by many objective problems is that the number of undominated
solutions typically grows exponentially in the number of objectives  this is particularly
problematic for multiple policy momdp methods  fleming et al         note that one
of the most effective approaches used in many objective evolutionary computation is to
incorporate user preferences to restrict the search space to a small region of interest  in
particular  they recommend interactive preference articulation in which the user interactively steers the system towards a desirable solution during optimization  while vamplew
et al         raise the possibility of incorporating such an approach into morl  we are not
aware of any research that has actually done so 
   

firoijers  vamplew  whiteson   dazeley

     

     

b

c
a

     

d
figure    an momdp with two objectives and four states 
    expectation of scalarized return
in section    we defined the scalarized value vw  s  to be the result of applying the scalarization function f to the multi objective value v  s  according to w  i e   vw  s    f  v  s   w  
since v  s  is itself an expectation  this means that the scalarization function is applied
after the expectation is computed  i e  
vw  s    f  v  s   w    f  e 


x

 k rk     s    s   w  

k  

this formulation  which we refer to as the scalarization of the expected return  ser  is
standard in the literature  however  it is not the only option  it is also possible to define
vw  s  as the expectation of the scalarized return  esr  
vw  s    e f  


x

 k rk   w      s    s 

k  

which definition is used can critically affect which policies are preferred  for example 
consider the following momdp  illustrated in figure    there are four states  a  b  c  and
d  and two objectives  the agent starts in state a and has two possible actions  a  transits
to state b or c  each with probability of      and a  transits to state d with probability   
both actions lead to a        reward  in states b  c and d there is only one action  which
leads to a deterministic reward of        for b         for c  and        for d 
the scalarization function just multiplies the two objectives together  thus  under ser 
vw  s    v   s v   s  
and under esr 
vw  s    e 


x
k  

 k rk 


 x
k  

   


 k rk      s    s  

fia survey of multi objective sequential decision making

where rki is the reward for the i th objective on timestep k  w is not needed in this example
since f involves no constants   if    a    a  and    a    a    then the multi objective
values are v   a                           and v   a                     
under ser  this leads to scalarized values of v    a                 and v    a   
          and consequently   is preferred  under esr  however  we have v    a     
and v    a              and thus   is preferred 
intuitively  the ser formulation is appropriate when the policy will be used many times
and return accumulates across episodes  e g   because the same user is using the policy each
time  then  scalarizing the expected reward makes sense and   is preferable because in
expectation it will accumulate more return in both objectives  however  if the policy will
only be used a few times or the return does not accumulate across episodes  e g   because
each episode is conducted for a different user  then the esr formulation is more appropriate 
in this case  the expected return before scalarization is not of interest and   is preferable
because   will always yield zero scalarized return on any given episode 
to our knowledge  there is no literature on momdps that employs the esr formulation 
even though there are many real world scenarios in which it seems more appropriate  for
example  in the medical application of lizotte et al         mentioned in section    each
patient gets only one episode to treat his or her illness  and is thus clearly interested in
maximizing esr  not ser  thus  we believe that developing methods for momdps under
the esr formulation is a critical direction for future research 

   conclusions
this article presented a survey of algorithms designed for sequential decision making problems with multiple objectives 
in order to make explicit under what circumstances special methods are needed to
solve multi objective problems  we identified three distinct scenarios in which converting
such a problem to a single objective one is impossible  infeasible  or undesirable  as well as
providing motivation for the need for multi objective methods  these scenarios also represent
the three main ways these methods are applied in practice 
we proposed a taxonomy that classifies multi objective methods according to the applicable scenario  the scalarization function  which projects multi objective values to scalar
ones   and the type of policies considered  we showed how these factors determine the
nature of an optimal solution  which can be a single policy  or a coverage set  convex or
pareto   our taxonomy is based on a utility based approach  which sees the scalarization
function as part of the utility  and thus part of the problem definition  this contrasts with
the so called axiomatic approach  which usually assumes the pareto front is the appropriate
solution  we showed that the utility based approach can be used to justify the choice for
a solution set  following this line of thought  we observed  observation    that computing
the pareto front is often not necessary  and that in many cases a convex coverage set of
deterministic stationary policies is sufficient 
using our taxonomy  we surveyed the literature on multi objective methods for planning
and learning  an interesting observation is that most of the learning methods use a modelfree rather than a model based approach  identifying the latter as an understudied class of
   

firoijers  vamplew  whiteson   dazeley

methods  another part of the taxonomy which has not yet been widely studied is learning
in the case of monotonically increasing scalarization functions 
we discussed key applications of momdp methods as motivation for the importance
of such methods  applications were identified in a diverse range of fields including environmental management  financial markets  information and communications technology  and
control of industrial processes  robotic systems and traffic infrastructure  in addition connections were identified between multi objective sequential decision making and other broad
areas of research such as probabilistic planning and model checking  multi agent systems
and more general multi objective optimization 
finally  we outlined several opportunities for future work  which include understudied
areas  model based methods  learning in monotonically increasing scalarization settings 
and many objective sequential decision making   and a reformulation of the objective for
momdps  the expectation of scalarized return  which is particularly important to
optimize when a policy can be executed only once 

acknowledgments
we would like to thank matthijs spaan  frans oliehoek  matthijs snel  marie d  manner
and samy sa  as well as the anonymous reviewers  for their valuable feedback  this work
is supported by the netherlands organisation for scientific research  nwo   decisiontheoretic control for network capacity allocation problems                project 

references
aberdeen  d   thiebaux  s     zhang  l          decision theoretic military operations
planning  in proc  icaps  vol      pp         
aissani  n   beldjilali  b     trentesaux  d          efficient and effective reactive scheduling of manufacturing system using sarsa multi objective agents  in mosim     th
conference internationale de modelisation et simulation  pp         
aissani  n   beldjilali  b     trentesaux  d          dynamic scheduling of maintenance
tasks in the pretroleum industry  a reinforcement approach  engineering applications
of artificial intelligence               
altman  e          constrained markov decision processes  chapman and hall crc 
london 
aoki  k   kimura  h     kobayashi  s          distributed reinforcement learning using
bi directional decision making for multi criteria control of multi stage flow systems 
in the  th conference on intelligent autonomous systems  vol           pp         
barrett  l     narayanan  s          learning all optimal policies with multiple criteria 
in proceedings of the   th international conference on machine learning  pp       
new york  ny  usa  acm 
becker  r   zilberstein  s   lesser  v     goldman  c  v          transition independent
decentralized markov decision processes  in proc  of the  nd intl joint conf  on
autonomous agents   multi agent systems 
   

fia survey of multi objective sequential decision making

bellman  r  e       a   a markov decision process  journal of mathematical mech     
       
bellman  r       b   dynamic programming  princeton university press 
bhattacharya  b   lobbrecht  a  h     solomantine  d  p          neural networks and reinforcement learning in control of water systems  journal of water resources planning
and management                  
bone  c     dragicevic  s          gis and intelligent agents for multiobjective natural
resource allocation  a reinforcement learning approach  transactions in gis         
       
bosoniu  l   babuska  r     schutter  b  d          a comprehensive survey of multiagent
reinforcement learning  ieee transactions on systems  man  and cybernetics   part
c  applications and reviews                 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research 
        
brazdil  t   brozek  v   chatterjee  k   forejt  v     kucera  a          two views on
multiple mean payoff objectives in markov decision processes  corr  abs           
bryce  d          the value s  of probabilistic plans  in workshop on a reality check for
planning and scheduling under uncertainty  icaps    
bryce  d   cushing  w     kambhampati  s          probabilistic planning is multiobjective   technical report         arizona state university 
carchrae  t     beck  j  c          applying machine learning to low knowledge control of
optimization algorithms  computational intelligence                 
cassandra  a   littman  m  l     zhang  n  l          incremental pruning  a simple  fast 
exact method for partially observable markov decision processes  in proceedings of
the thirteenth conference on uncertainty in artificial intelligence  pp       
castelletti  a   pianosi  f     restelli  m          a multiobjective reinforcement learning
approach to water resources systems operation  pareto frontier approximation in a
single run  water resources research 
castelletti  a   corani  g   rizzolli  a   soncini sessa  r     weber  e          reinforcement learning in the operational management of a water system  in ifac workshop
on modeling and control in environmental issues  pp         
castelletti  a   galelli  s   restelli  m     soncini sessa  r          tree based reinforcement learning for optimal water reservoir operation  water resources research 
    w       
castelletti  a   pianosi  f     restelli  m          multi objective fitted q iteration  pareto
frontier approximation in one single run  in international conference on networking 
sensing and control  pp         
castelletti  a   pianosi  f     restelli  m          tree based fitted q iteration for multiobjective markov decision processes  in ieee world congress on computational
intelligence 
   

firoijers  vamplew  whiteson   dazeley

castelletti  a   pianosi  f     soncini sessa  r          water reservoir control under economic  social and environmental constraints  automatica               
chatterjee  k          markov decision processes with multiple long run average objectives 
in fsttcs  vol  lncs       pp         
chatterjee  k   majumdar  r     henzinger  t  a          markov decision processes with
multiple objectives  in proceedings of the   rd annual conference on theoretical
aspects of computer science  stacs    pp          berlin  heidelberg  springerverlag 
cheng  l   subrahmanian  e     westerberg  a          multiobjective decision processes
under uncertainty  applications  formulations and solution strategies  industrial and
engineering chemistry research                   
clemen  r  t          making hard decisions  an introduction to decision analysis   
edition   south western college pub 
coello coello  c  a   lamont  g  b     van veldhuizen  d  a          evolutionary algorithms for solving multi objective problems  kluwer academic publishers 
comsa  i   aydin  m   zhang  s   kuonen  p     wagen  j  f          multi objective resource scheduling in lte networks using reinforcement learning  international journal
of distributed systems and technologies              
courcoubetis  c     yannakakis  m          markov decision processes and regular events 
ieee transactions on automatic control                    
crites  r  h     barto  a  g          improving elevator performance using reinforcement
learning  in touretzky  d  s   mozer  m  c     hasselmo  m  e   eds    advances in
neural information processing systems    pp            mit press 
daellenbach  h  g     kluyver  c  a  d          note on multiple objective dynamic
programming  journal of the operational research society             
defourny  b   ernst  d     wehenkel  l          risk aware decision making and dynamic
programming  in nips      workshop on model uncertainty and risk in rl 
diehl  m     haimes  y  y          influence diagrams with multiple objectives and tradeoff analysis  systems  man and cybernetics  part a  systems and humans  ieee
transactions on                 
drugan  m  m     thierens  d          stochastic pareto local search  pareto neighbourhood
exploration and perturbation strategies  journal of heuristics                 
dusparic  i     cahill  v          distributed w learning  multi policy optimization in selforganizing systems  in third ieee international conference on self adaptive and
self organizing systems  pp       
dusparic  i     cahill  v          multi policy optimization in self organizing systems  in
soar       lncs       pp         
dyer  j  s   fishburn  p  c   steuer  r  e   wallenius  j     zionts  s          multiple criteria decision making  multiattribute utility theory  the next ten years  management
science                 
   

fia survey of multi objective sequential decision making

ernst  d   geurts  p     wehenkel  l          tree based batch mode reinforcement learning 
journal of machine learning research            
ernst  d   glavic  m     wehenkel  l          power systems stability control  reinforcement learning framework  ieee transactions on power systems                 
feinberg  e  a     shwartz  a          constrained markov decision models with weighted
discounted rewards  mathematics of operations research                 
ferreira  l   bianchi  r     ribeiro  c          multi agent multi objective reinforcement
learning using heuristically accelerated reinforcement learning  in      brazilian
robotics symposium and latin american robotics symposium  pp       
fleming  p   purshouse  r     lygoe  r          many objective optimization  an engineering design perspective  in evolutionary multi criterion optimization  lecture notes
in computer science  vol        pp       
forejt  v   kwiatkowska  m   norman  g   parker  d     qu  h          quantitative
multi objective verification for probabilistic systems  in tools and algorithms for the
construction and analysis of systems  pp          springer berlin heidelberg 
forejt  v   kwiatkowska  m     parker  d          pareto curves for probabilistic model
checking  in automated technology for verification and analysis  pp         
springer berlin heidelberg 
furnkranz  j   hullermeier  e   cheng  w     park  s  h          preference based reinforcement learning  a formal framework and a policy iteration algorithm  machine
learning                   
gabor  z   kalmar  z     szepesvari  c          multi criteria reinforcement learning  in
the fifteenth international conference on machine learning  pp         
geibel  p     wysotzki  f          risk sensitive reinforcement learning applied to control
under constraints  journal of artificial intelligence research            
geibel  p          reinforcement learning with bounded risk  in proceeding of the   th
international conference on machine learning  pp         
geibel  p          reinforcement learning for mdps with constraints  in european conference on machine learning  vol        pp         
gelly  s     silver  d          monte carlo tree search and rapid action value estimation in
computer go  artificial intelligence                     
gong  p          multiobjective dynamic programming for forest resource management 
forest ecology and management           
guo  y   zeman  a     li  r          a reinforcement learning approach to setting multiobjective goals for energy demand management  international journal of agent technologies and systems              
handa  h       a   eda rl  estimation of distribution algorithms for reinforcement learning problems  in acm sigevo genetic and evolutionary computation conference 
pp         
   

firoijers  vamplew  whiteson   dazeley

handa  h       b   solving multi objective reinforcement learning problems by eda rl acquisition of various strategies  in proceedings of the ninth internatonal conference
on intelligent sysems design and applications  pp         
hiraoka  k   yoshida  m     mishima  t          parallel reinforcement learning for weighted
multi criteria model with adaptive margin  cognitive neurodynamics          
holenstein  a  a     badreddin  e          collision avoidance in a behavior based mobile
robot design  in robotics and automation        proceedings        ieee international conference on  pp          ieee 
houli  d   zhiheng  l     yi  z          multiobjective reinforcement learning for traffic
signal control using vehicular ad hoc network  eurasip journal on advances in
signal processing 
howard  r  a          dynamic programming and markov decision processes  mit press 
humphrys  m          action selection methods using reinforcement learning  in proceedings of the fourth international conference on simulation of adaptive behavior  pp 
       
ishibuchi  h   tsukamoto  n     nojima  y          evolutionary many objective optimisation  a short review  in ieee congress on evolutionary computation  pp           
issabekov  r     vamplew  p          an empirical comparison of two common multiobjective reinforcement learning algorithms  in ai      the   th australasian joint
conference on artificial intelligence  pp         
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in
partially observable stochastic domains  artificial intelligence             
karlsson  j          learning to solve multiple goals  ph d  thesis  university of rochester 
kober  j     peters  j          policy search for motor primitives in robotics  machine
learning             
kober  j     peters  j          reinforcement learning in robotics  a survey  in wiering 
m     otterlo  m   eds    reinforcement learning  vol     of adaptation  learning 
and optimization  pp          springer berlin heidelberg 
kocsis  l     szepesvari  c          bandit based monte carlo planning  in   th european
conference on machine learning  pp          springer 
kohl  n     stone  p          policy gradient reinforcement learning for fast quadrupedal
locomotion  in proceedings of the ieee international conference on robotics and
automation  pp           
kolobov  a   mausam    weld  d  s          a theory of goal oriented mdps with dead
ends  in proceedings of the twenty eighth conference on uncertainty in artificial
intelligence 
kwak  j   varakantham  p   maheswarn  r   tambe  m   jazizadeh  f   kavulya  g   klein 
l   becerik gerber  b   hayes  t     wood  w          saves  a sustainable multiagent application to conserve building energy considering occupants  in   th international conference on autonomous agents and multiagent systems  pp       
   

fia survey of multi objective sequential decision making

liao  h   wu  q     jiang  l          multi objective optimization by reinforcement learning
for power system dispatch and voltage stability  in innovative smart grid technologies
conference europe 
lin  c  t     chung  i  f          a reinforcement neuro fuzzy combiner for multiobjective
control  ieee transactions on systems  man and cyberbetics   part b             
    
liu  c   xu  x     hu  d          multiobjective reinforcement learning  a comprehensive
overview  systems  man  and cybernetics  part c  applications and reviews  ieee
transactions on  pp           
liu  w   tan  y     qiu  q          enhanced q learning algorithm for dynamic power
management with performance constraints  in date    pp         
lizotte  d  j   bowling  m     murphy  s  a          efficient reinforcement learning with
multiple reward functions for randomized clinical trial analysis  in   th international
conference on machine learning  pp         
lizotte  d  j   bowling  m     murphy  s  a          linear fitted q iteration with multiple
reward functions  journal of machine learning research               
madani  o   hanks  s     condon  a          on the undecidability of probabilistic planning
and infinite horizon partially observable markov decision problems  in proceedings of
the national conference on artificial intelligence  aaai   pp         
mannor  s     shimkin  n          the steering approach for multi criteria reinforcement
learning  in neural information processing systems  pp           
mannor  s     shimkin  n          a geometric approach to multi criterion reinforcement
learning  journal of machine learning research            
maravall  d     de lope  j          a reinforcement learning method for dynamic obstacle
avoidance in robotic mechanisms  in computational intelligent systems for applied
research  proceedings of the  th international flins conference  pp          singapore  world scientific 
mariano  c     morales  e          moaq an ant q algorithm for multiple objective
optimization problems  in gecco     proceedings of the genetic and evolutionary
computation conference  pp         
mariano  c     morales  e       a   a new approach for the solution of multiple objective
optimization problems based on reinforcement learning  in advances in artificial
intelligence  international joint conference   th ibero american conference on ai 
  th brazilian symposium  springer 
mariano  c     morales  e       b   a new distributed reinforcement learning algorithm for
multiple objective optimisation problems  in lecture notes in ai vol       proceedings of the mexican international conference on artficial intelligence  pp         
springer 
meisner  e  m          learning controllers for human robot interaction  ph d  thesis 
rensselaer polytechnic institute 
   

firoijers  vamplew  whiteson   dazeley

mouaddib  a  i          collective multi objective planning  in proceedings of the ieee
workshop on distributed intelligent systems  collective intelligence and its applications  dis     pp        washington  dc  usa  ieee computer society 
mukai  y   kuroe  y     iima  h          multi objective reinforcement learning method
for acquiring all pareto optimal policies simultaneously  in ieee international conference on systems  man and cybernetics  pp           
natarajan  s     tadepalli  p          dynamic preferences in multi criteria reinforcement
learning  in international conference on machine learning  pp         
nojima  y   kojima  f     kubota  n          local episode based learning of multiobjective behavior coordination for a mobile robot in dynamic environments  in the
  th ieee international conference on fuzzy systems  vol     pp         
ogryczak  w   perny  p     weng  p          on minimizing ordered weighted regrets
in multiobjective markov decision processes  in  nd international conference on
algorithmic decision theory  pp         
ong  s  c   png  s  w   hsu  d     lee  w  s          planning under uncertainty for robotic
tasks with mixed observability  the international journal of robotics research         
         
pareto  v          manuel deconomie politique  giard  paris 
peek  n  b          explicit temporal models for decisiontheoretic planning of clinical
management  artificial intelligence in medicine                 
perez  j   germain renaud  c   kegl  b     loomis  c          responsive elastic computing  in international conference on autonomic computing  pp       
perny  p     weng  p          on finding compromise solutions in multiobjective markov
decision processes  in ecai multidisciplinary workshop on advances in preference
handling  pp       
perny  p   weng  p   goldsmith  j     hanna  j  p          approximation of lorenz optimal
solutions in multiobjective markov decision processes  in workshops at the twentyseventh aaai conference on artificial intelligence 
pervez  a     ryu  j          safe physical human robot interaction past  present and
future  journal of mechanical science and technology                 
pineau  j   gordon  g     thrun  s          anytime point based approximations for large
pomdps  journal of artificial intelligence research                 
precup  d   sutton  r  s     dasgupta  s          off policy temporal difference learning
with function approximation  in proceedings of the   th international conference on
machine learning  pp         
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley   sons  inc 
roijers  d  m   whiteson  s     oliehoek  f  a          computing convex coverage sets
for multi objective coordination graphs  in adt       proceedings of the third international conference on algorithmic decision theory  to appear 
   

fia survey of multi objective sequential decision making

ross  s   pineau  j   paquet  s     chaib draa  b          online planning algorithms for
pomdps  journal of artificial intelligence research             
russell  s     zimdars  a  l          q decomposition for reinforcement learning agents  in
proceedings of the   th international conference on machine learning  pp         
schneider  j   wong  w  k   moore  a     riedmiller  m          distributed value functions  in proceedings of the   th international conference on machine learning  pp 
        san francisco  ca  morgan kaufmann 
shabani  n          incorporating flood control rule curves of the columbia river hydroelectric system in a multireservoir reinforcement learning optimization model  masters
thesis  university of british columbia 
shelton  c  r          importance sampling for reinforcement learning with multiple objectives  ai technical report           mit 
shortreed  s   laber  e   lizotte  d   stroup  t   pineau  j     murphy  s          informing
sequential clinical decision making through reinforcement learning  an empirical study 
machine learning             
smallwood  r     sondik  e          the optimal control of partially observable markov
processes over a finite horizon  operations research                   
soh  h     demiris  y          evolving policies for multi reward partially observable
markov decision processes  mr pomdps   in gecco   proceedings of the   th
annual conference on genetic and evolutionary computation  pp         
sondik  e          the optimal control of partially observable processes over a finite horizon 
ph d  thesis  stanford university  stanford  california 
sondik  e          the optimal control of partially observable markov processes over the
infinite horizon  discounted costs  operations research                 
spaan  m     vlassis  n          perseus  randomized point based value iteration for
pomdps  journal of artificial intelligence research                 
stewart  t  j          a critical survey on the status of multiple criteria decision making
theory and practice  omega                 
sutton  r  s          learning to predict by the methods of temporal differences  machine
learning             
sutton  r  s     barto  a  g          introduction to reinforcement learning   st edition  
mit press  cambridge  ma  usa 
sutton  r   mcallester  d   singh  s     mansour  y          policy gradient methods for
reinforcement learning with function approximation  in nips  pp           
szita  i          reinforcement learning in games  in wiering  m     otterlo  m   eds   
reinforcement learning  vol     of adaptation  learning  and optimization  pp     
     springer berlin heidelberg 
tan  k  c   khor  e  f   lee  t  h     sathikannan  r          an evolutionary algorithm
with advanced goal and priority specification for multi objective optimization  journal
of artificial intelligence research             
   

firoijers  vamplew  whiteson   dazeley

teichteil konigsbuch  f       a   path constrained markov decision processes  bridging the
gap  in proceedings of the twentieth european conference on artificial intelligence 
teichteil konigsbuch  f       b   stochastic safest and shortest path problems  in proceedings of the twenty sixth aaai conference on artificial intelligence 
tesauro  g   das  r   chan  h   kephart  j  o   lefurgy  c   levine  d  w     rawson  f 
        managing power consumption and performance of computing systems using
reinforcement learning  in neural information processing systems 
tong  h     brown  t  x          reinforcement learning for call admission control and
routing under quality of service constraints in multimedia networks  machine learning             
uchibe  e     doya  k          constrained reinforcement learning from intrinsic and
extrinsic rewards  pp          theory and novel applications of machine learning 
i tech  vienna  austria 
vamplew  p   dazeley  r   barker  e     kelarev  a          constructing stochastic mixture
policies for episodic multiobjective reinforcement learning tasks  in ai    the   nd
australasian conference on artificial intelligence  pp         
vamplew  p   dazeley  r   berry  a   dekker  e     issabekov  r          empirical evaluation methods for multiobjective reinforcement learning algorithms  machine learning 
               
vamplew  p   yearwood  j   dazeley  r     berry  a          on the limitations of scalarisation for multi objective reinforcement learning of pareto fronts  in ai    the   st
australasian joint conference on artificial intelligence  pp          springer 
van otterlo  m     wiering  m          reinforcement learning and markov decision processes  in reinforcement learning  state of the art  chap     pp       springer 
van vaerenbergh  k   rodriguez  a   gagliolo  m   vrancx  p   nowe  a   stoev  j  
goossens  s   pinte  g     symens  w          improving wet clutch engagement
with reinforcement learning  in international joint conference on neural networks 
ijcnn      
vira  c     haimes  y  y          multiobjective decision making  theory and methodology 
no     north holland 
viswanathan  b   aggarwal  v  v     nair  k  p  k          multiple criteria markov
decision processes  tims studies management science            
wakuta  k     togawa  k          solution procedures for markov decision processes  optimization  a journal of mathematical programming and operations research         
     
wakuta  k          a note on the structure of value spaces in vector valued markov decision
processes   mathematical methods of operations research               
wang  w     sebag  m          hypervolume indicator and dominance reward based multiobjective monte carlo tree search  machine learning      
watkins  c  j  c  h          learning from delayed rewards  ph d  thesis  cambridge
university 
   

fia survey of multi objective sequential decision making

white  c  c     kim  k  m          solution procedures for solving vector criterion markov
decision processes  large scale systems            
white  d          multi objective infinite horizon discounted markov decision processes 
journal of mathematical analysis and applications                   
whiteson  s          evolutionary computation for reinforcement learning  in wiering 
m  a     van otterlo  m   eds    reinforcement learning  state of the art  chap     
pp          springer  berlin 
wiering  m     de jong  e          computing optimal stationary policies for multiobjective markov decision processes  in ieee international symposium on approximate dynamic programming and reinforcement learning  pp          ieee 
yang  z     wen  k          multi objective optimization of freeway traffic flow via a
fuzzy reinforcement learning method  in  rd international conference on advanced
computer theory and engineering  vol     pp         
yin  s   duan  h   li  z     zhang  y          multi objective reinforcement learning for
traffic signal coordinate control  in  th world conference on transport research 
zeleny  m     cochrane  j  l          multiple criteria decision making  vol      mcgrawhill new york 
zhao  y   chen  q     hu  w          multi objective reinforcement learning algorithm for
mosdmp in unknown environment  in proceedings of the  th world congress on
intelligent control and automation  pp           
zheng  k   li  h   qiu  r  c     gong  s          multi objective reinforcement learning
based routing in cognitive radio networks  walking in a random maze  in international
conference on computing  networking and communications  pp         
zitzler  e   thiele  l   laumanns  m   fonseca  c  m     da fonseca  v  g          performance assessment of multiobjective optimizers  an analysis and review  evolutionary
computation  ieee transactions on                

   

fi