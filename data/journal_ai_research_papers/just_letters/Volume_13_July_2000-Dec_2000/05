journal artificial intelligence research                  

submitted        published      

hierarchical reinforcement learning maxq value
function decomposition
thomas g  dietterich

department computer science  oregon state university
corvallis       

abstract

tgd cs orst edu

paper presents new approach hierarchical reinforcement learning based decomposing target markov decision process  mdp  hierarchy smaller mdps
decomposing value function target mdp additive combination
value functions smaller mdps  decomposition  known maxq decomposition  procedural semantics as subroutine hierarchy and declarative
semantics as representation value function hierarchical policy  maxq unifies
extends previous work hierarchical reinforcement learning singh  kaelbling 
dayan hinton  based assumption programmer identify useful
subgoals define subtasks achieve subgoals  defining subgoals 
programmer constrains set policies need considered reinforcement
learning  maxq value function decomposition represent value function
policy consistent given hierarchy  decomposition creates opportunities exploit state abstractions  individual mdps within hierarchy
ignore large parts state space  important practical application
method  paper defines maxq hierarchy  proves formal results representational power  establishes five conditions safe use state abstractions  paper
presents online model free learning algorithm  maxq q  proves converges
probability   kind locally optimal policy known recursively optimal policy 
even presence five kinds state abstraction  paper evaluates maxq
representation maxq q series experiments three domains shows
experimentally maxq q  with state abstractions  converges recursively optimal
policy much faster q learning  fact maxq learns representation
value function important benefit  makes possible compute execute
improved  non hierarchical policy via procedure similar policy improvement
step policy iteration  paper demonstrates effectiveness non hierarchical
execution experimentally  finally  paper concludes comparison related work
discussion design tradeoffs hierarchical reinforcement learning 

c      ai access foundation morgan kaufmann publishers  rights reserved 

fidietterich

   introduction
area reinforcement learning  bertsekas   tsitsiklis        sutton   barto       
studies methods agent learn optimal near optimal plans interacting
directly external environment  basic methods reinforcement learning
based classical dynamic programming algorithms developed late
    s  bellman        howard         however  reinforcement learning methods offer two
important advantages classical dynamic programming  first  methods online 
permits focus attention parts state space important
ignore rest space  second  methods employ function approximation algorithms  e g   neural networks  represent knowledge  allows
generalize across state space learning time scales much better 
despite recent advances reinforcement learning  still many shortcomings 
biggest lack fully satisfactory method incorporating hierarchies
reinforcement learning algorithms  research classical planning shown hierarchical methods hierarchical task networks  currie   tate         macro actions
 fikes  hart    nilsson        korf         state abstraction methods  sacerdoti       
knoblock        provide exponential reductions computational cost finding
good plans  however  basic algorithms probabilistic planning reinforcement learning   at  methods they treat state space one huge search space 
means paths start state goal state long 
length paths determines cost learning planning  information
future rewards must propagated backward along paths 
many researchers  singh        lin        kaelbling        dayan   hinton       
hauskrecht  et al         parr   russell        sutton  precup    singh        experimented different methods hierarchical reinforcement learning hierarchical
probabilistic planning  research explored many different points design space
hierarchical methods  several systems designed specific situations 
lack crisp definitions main approaches clear understanding relative
merits different methods 
paper formalizes clarifies one approach attempts understand
compares techniques  approach  called maxq method  provides
hierarchical decomposition given reinforcement learning problem set subproblems  simultaneously provides decomposition value function given
problem set value functions subproblems  hence  declarative
semantics  as value function decomposition  procedural semantics  as subroutine
hierarchy  
decomposition subproblems many advantages  first  policies learned
subproblems shared  reused  multiple parent tasks  second  value functions
learned subproblems shared  subproblem reused new task 
learning overall value function new task accelerated  third  state abstractions applied  overall value function represented compactly
sum separate terms depends subset state variables 
compact representation value function require less data learn  hence 
learning faster 
   

fimaxq hierarchical reinforcement learning

previous research shows several important design decisions must
made constructing hierarchical reinforcement learning system  provide
overview results paper  let us review issues see maxq
method approaches them 
first issue specify subtasks  hierarchical reinforcement learning involves
breaking target markov decision problem hierarchy subproblems subtasks 
three general approaches defining subtasks  one approach define
subtask terms fixed policy provided programmer  or
learned separate process    option  method sutton  precup  singh
       takes approach  second approach define subtask terms nondeterministic finite state controller  hierarchy abstract machines  ham  method
parr russell        takes approach  method permits programmer
provide  partial policy  constrains set permitted actions point 
specify complete policy subtask  third approach define
subtask terms termination predicate local reward function  define
means subtask completed final reward completing
subtask  maxq method described paper follows approach  building
upon previous work singh         kaelbling         dayan hinton         dean
lin        
advantage  option  partial policy approaches subtask
defined terms amount effort course action rather terms
achieving particular goal condition  however   option  approach  at least
simple form described paper   requires programmer provide complete policies
subtasks  dicult programming task real world problems 
hand  termination predicate method requires programmer guess relative
desirability different states subtask might terminate 
dicult  although dean lin show guesses revised automatically
learning algorithm 
potential drawback hierarchical methods learned policy may
suboptimal  hierarchy constrains set possible policies considered 
constraints poorly chosen  resulting policy suboptimal  nonetheless 
learning algorithms developed  option  partial policy approaches
guarantee learned policy best possible policy consistent
constraints 
termination predicate method suffers additional source suboptimality 
learning algorithm described paper converges form local optimality
call recursive optimality  means policy subtask locally optimal
given policies children  might exist better hierarchical policies
policy subtask must locally suboptimal overall policy optimal 
example  subtask buying milk might performed suboptimally  at distant
store  larger problem involves buying film  at store   problem
avoided careful definition termination predicates local reward functions 
added burden programmer   it interesting note problem
recursive optimality noticed previously  previous work
   

fidietterich

focused subtasks single terminal state  cases  problem
arise  
second design issue whether employ state abstractions within subtasks 
subtask employs state abstraction ignores aspects state environment 
example  many robot navigation problems  choices route take
reach goal location independent robot currently carrying 
exceptions  state abstraction explored previously  see maxq
method creates many opportunities exploit state abstraction  abstractions
huge impact accelerating learning  see important
design tradeoff  successful use state abstraction requires subtasks defined
terms termination predicates rather using option partial policy methods 
maxq method must employ termination predicates  despite problems
create 
third design issue concerns non hierarchical  execution  learned hierarchical policy  kaelbling        first point value function learned
hierarchical policy could evaluated incrementally yield potentially much
better non hierarchical policy  dietterich        sutton  et al         generalized
show arbitrary subroutines could executed non hierarchically yield improved
policies  however  order support non hierarchical execution  extra learning
required  ordinarily  hierarchical reinforcement learning  states learning
required higher levels hierarchy states one subroutines could terminate  plus possible initial states   support non hierarchical
execution  learning required states  and levels hierarchy   general 
requires additional exploration well additional computation memory 
consequence hierarchical decomposition value function  maxq method
able support either form execution  see many problems
improvement non hierarchical execution worth added cost 
fourth final issue form learning algorithm employ  important advantage reinforcement learning algorithms typically operate online 
however  finding online algorithms work general hierarchical reinforcement learning
dicult  particularly within termination predicate family methods  singh s
method relied subtask unique terminal state  kaelbling employed mix
online batch algorithms train hierarchy  work within  options  framework usually assumes policies subproblems given need
learned all  best previous online algorithms hamq q learning algorithm
parr russell  for partial policy method  feudal q algorithm dayan
hinton  unfortunately  hamq method requires   attening  hierarchy 
several undesirable consequences  feudal q algorithm tailored specific kind
problem  converge well defined optimal policy 
paper  present general algorithm  called maxq q  fully online learning
hierarchical value function  algorithm enables subtasks within hierarchy
learned simultaneously online  show experimentally theoretically
algorithm converges recursively optimal policy  show substantially
faster   at   i e   non hierarchical  q learning state abstractions employed 
   

fimaxq hierarchical reinforcement learning

remainder paper organized follows  introducing notation
section    define maxq value function decomposition section   illustrate
simple example markov decision problem  section   presents analytically
tractable version maxq q learning algorithm called maxq   algorithm
proves convergence recursively optimal policy  shows extend maxq  produce maxq q algorithm  shows extend theorem similarly 
section   takes issue state abstraction formalizes series five conditions
state abstractions safely incorporated maxq representation 
state abstraction give rise hierarchical credit assignment problem  paper
brie discusses one solution problem  finally  section   presents experiments
three example domains  experiments give idea generality maxq
representation  provide results relative importance temporal state
abstractions importance non hierarchical execution  paper concludes
discussion design issues brie described above  particular 
addresses tradeoff method defining subtasks  via termination predicates 
ability exploit state abstractions 
readers may disappointed maxq provides way learning structure hierarchy  philosophy developing maxq  which share
reinforcement learning researchers  notably parr russell  draw inspiration
development belief networks  pearl         belief networks first introduced
formalism knowledge engineer would describe structure networks domain experts would provide necessary probability estimates  subsequently 
methods developed learning probability values directly observational data 
recently  several methods developed learning structure belief
networks data  dependence knowledge engineer reduced 
paper  likewise require programmer provide structure
hierarchy  programmer need make several important design decisions 
see maxq representation much computer program 
rely programmer design modules indicate permissible
ways modules invoke other  learning algorithms fill
 implementations  module way overall program work well 
believe approach provide practical tool solving large real world
mdps  believe help us understand structure hierarchical learning
algorithms  hope subsequent research able automate
work currently requiring programmer do 

   formal definitions
begin introducing definitions markov decision problems semi markov decision problems 

    markov decision problems
employ standard definition markov decision problems  also known markov
decision processes   paper  restrict attention situations agent
   

fidietterich

interacting fully observable stochastic environment  situation modeled
markov decision problem  mdp  hs  a  p  r  p  defined follows 
  finite set states environment  point time  agent
observe complete state environment 
a  finite set actions  technically  set available actions depends
current state s  suppress dependence notation 
p   action   performed  environment makes probabilistic transition current state resulting state s  according probability
distribution p  s  js  a  
r  similarly  action performed environment makes transition
s    agent receives real valued  possibly stochastic  reward r whose
expected value r s  js  a   simplify notation  customary treat
reward given time action initiated  even though may
general depend s  well a 
p    starting state distribution  mdp initialized  state
probability p   s  
policy    mapping states actions tells action    s  perform
environment state s 
consider two settings  episodic infinite horizon 
episodic setting  rewards finite least one zero cost absorbing
terminal state  absorbing terminal state state actions lead back
state probability   zero reward  technical reasons  consider
problems deterministic policies  proper  that is  deterministic policies
non zero probability reaching terminal state started arbitrary state 
 we believe condition relaxed  verified formally  
episodic setting  goal agent find policy maximizes expected
cumulative reward  special case rewards non positive  problems
referred stochastic shortest path problems  rewards viewed
costs  i e   lengths   policy attempts move agent along path minimum
expected cost 
infinite horizon setting  rewards finite  addition  discount
factor   agent s goal find policy minimizes infinite discounted sum
future rewards 
value function v policy function tells  state s 
expected cumulative reward executing policy starting state s  let rt
random variable tells reward agent receives time step following
policy   define value function episodic setting
v  s    e frt   rt     rt     jst   s  g  
discounted setting  value function


n



v  s    e rt   rt       rt     fifi st   s   
   

fimaxq hierarchical reinforcement learning

see equation reduces previous one      however  infinitehorizon mdps sum may converge     
value function satisfies bellman equation fixed policy 

v  s   

x

s 

p  s  js   s   r s  js   s     v  s     




quantity right hand side called backed up value performing action
state s  possible successor state s    computes reward would received
value resulting state weights according probability
ending s   
optimal value function v value function simultaneously maximizes
expected cumulative reward states     bellman        proved unique
solution known bellman equation 

v  s    max


x

s 

p  s  js  a  r s  js  a    v  s     




   

may many optimal policies achieve value  policy chooses
achieve maximum right hand side equation optimal policy 
denote optimal policy   note optimal policies  greedy 
respect backed up value available actions 
closely related value function so called action value function  q function
 watkins         function  q  s  a   gives expected cumulative reward performing action state following policy thereafter  q function satisfies
bellman equation 

q  s  a   

x

s 

p  s  js  a  r s  js  a    q  s     s      




optimal action value function written q  s  a   satisfies equation
x
q  s  a    p  s  js  a 

s 





r s  js  a    max q s    a   
a 

 

   

note policy greedy respect q optimal policy  may
many optimal policies they differ break ties actions
identical q values 
action order  denoted    total order actions within mdp  is   
anti symmetric  transitive relation   a    a    true iff a  strictly preferred
a    ordered greedy policy    greedy policy breaks ties using    example 
suppose two best actions state a  a    q s  a      q s  a    
  a    a     ordered greedy policy   choose a       s    a    note
although may many optimal policies given mdp  ordered greedy policy 
    unique 
   

fidietterich

    semi markov decision processes

order introduce prove properties maxq decomposition 
need consider simple generalization mdps the semi markov decision process 
discrete time semi markov decision process  smdp  generalization markov
decision process actions take variable amount time complete 
particular  let random variable n denote number time steps action takes
executed state s  extend state transition probability function
joint distribution result states s  number time steps n action
performed state s  p  s    n js  a   similarly  expected reward changed
r s    n js  a   
straightforward modify bellman equation define value function
fixed policy
h

x
v  s    p  s   n js   s   r s    n js   s     n v  s     
s   n

change expected value right hand side taken respect
s  n   raised power n ect variable amount time
may elapse executing action a 
note expectation linear operator  write bellman
equations sum expected reward performing action expected value
resulting state s    example  rewrite equation
x
   
v  s    r s   s     p  s    n js   s   n v  s    
s   n

r s   s   expected reward performing action  s  state s  expectation taken respect s  n  
results given paper generalized apply discrete time semimarkov decision processes  consequence whenever paper talks
executing primitive action  could easily talk executing hand coded openloop  subroutine   subroutines would learned  could execution
interrupted discussed section    many applications  e g   robot
control limited sensors   open loop controllers useful  e g   hide partialobservability   example  see kalmar  szepesvari  a  lorincz        
note episodic case  difference mdp semi markov
decision process  discount factor    therefore neither optimal policy
optimal value function depend amount time action takes 

    reinforcement learning algorithms

reinforcement learning algorithm algorithm tries construct optimal policy
unknown mdp  algorithm given access unknown mdp via following
   formalization slightly different standard formulation smdps  separates
p  s js  a  f  tjs  a   f cumulative distribution function probability
terminate time units  real valued rather integer valued  case  important
consider joint distribution s  n   need consider actions arbitrary
real valued durations 

   

fimaxq hierarchical reinforcement learning

reinforcement learning protocol  time step t  algorithm told current state
mdp set actions a s  executable state 
algorithm chooses action   a s   mdp executes action  which causes
move state s   returns real valued reward r  absorbing terminal state 
set actions a s  contains special action reset  causes mdp move
one initial states  drawn according p   
paper  make use two well known learning algorithms  q learning
 watkins        watkins   dayan        sarsa     rummery   niranjan        
apply algorithms case action value function q s  a  represented
table one entry pair state action  every entry table
initialized arbitrarily 
q learning  algorithm observed s  chosen a  received r  observed s   
performs following update 

qt  s  a          fft  qt    s  a    fft  r   max
q  s    a     
a  t  
fft learning rate parameter 
jaakkola  jordan singh        bertsekas tsitsiklis        prove
agent follows  exploration policy  tries every action every state infinitely often



x
x
lim

 
 

lim
ff t    
   

  
t  
t  

t  

qt converges optimal action value function q probability    proof
holds settings discussed paper  episodic infinite horizon  
sarsa    algorithm similar  observing s  choosing a  observing r 
observing s    choosing a    algorithm performs following update 

qt  s  a          fft  qt    s  a    fft  r   qt    s    a     
fft learning rate parameter  key difference q value chosen
action a    q s    a     appears right hand side place q learning uses
q value best action  singh  et al         provide two important convergence results 
first  fixed policy employed choose actions  sarsa    converge
value function policy provided fft decreases according equations      second 
so called glie policy employed choose actions  sarsa    converge value
function optimal policy  provided fft decreases according equations     
glie policy defined follows 

definition   glie  greedy limit infinite exploration  policy policy

satisfying

   action executed infinitely often every state visited infinitely often 
   limit  policy greedy respect q value function probability
  
   

fidietterich

 

r

g

 
 
 
 
 

b
 

 

 

 

figure    taxi domain 

   maxq value function decomposition
center maxq method hierarchical reinforcement learning maxq
value function decomposition  maxq describes decompose overall value function
policy collection value functions individual subtasks  and subsubtasks 
recursively  

    motivating example

make discussion concrete  let us consider following simple example  figure  
shows   by   grid world inhabited taxi agent  four specially designated
locations world  marked r ed   b lue   g reen   y ellow   taxi problem
episodic  episode  taxi starts randomly chosen square 
passenger one four locations  chosen randomly   passenger wishes
transported one four locations  also chosen randomly   taxi must go
passenger s location  the  source    pick passenger  go destination location
 the  destination    put passenger there   to keep things uniform  taxi
must pick drop passenger even he she already located destination  
episode ends passenger deposited destination location 
six primitive actions domain   a  four navigation actions move
taxi one square north  south  east  west   b  pickup action   c  putdown action 
reward    action additional reward     successfully
delivering passenger  reward     taxi attempts execute
putdown pickup actions illegally  navigation action would cause taxi hit
wall  action no op  usual reward    
simplify examples throughout section  make six primitive actions deterministic  later  make actions stochastic order create greater
challenge learning algorithms 
seek policy maximizes total reward per episode      possible
states     squares    locations passenger  counting four starting locations
taxi     destinations 
task simple hierarchical structure two main sub tasks 
get passenger deliver passenger  subtasks turn involves
   

fimaxq hierarchical reinforcement learning

subtask navigating one four locations performing pickup putdown
action 
task illustrates need support temporal abstraction  state abstraction 
subtask sharing  temporal abstraction obvious for example  process navigating passenger s location picking passenger temporally extended
action take different numbers steps complete depending distance
target  top level policy  get passenger  deliver passenger  expressed
simply temporal abstractions employed 
need state abstraction perhaps less obvious  consider subtask getting
passenger  subtask solved  destination passenger
completely irrelevant it cannot affect nagivation pickup decisions  perhaps
importantly  navigating target location  either source destination
location passenger   target location important  fact
cases taxi carrying passenger cases irrelevant 
finally  support subtask sharing critical  system could learn solve
navigation subtask once  solution could shared  get passenger 
 deliver passenger  subtasks  show maxq method provides
value function representation learning algorithm supports temporal abstraction 
state abstraction  subtask sharing 
construct maxq decomposition taxi problem  must identify set
individual subtasks believe important solving overall task 
case  let us define following four tasks 
navigate t   subtask  goal move taxi current location
one four target locations  indicated formal parameter t 
get  subtask  goal move taxi current location
passenger s current location pick passenger 
put  goal subtask move taxi current location
passenger s destination location drop passenger 
root  whole taxi task 
subtasks defined subgoal  subtask terminates
subgoal achieved 
defining subtasks  must indicate subtask subtasks
primitive actions employ reach goal  example  navigate t  subtask
use four primitive actions north  south  east  west  get subtask
use navigate subtask pickup primitive action  on 
information summarized directed acyclic graph called task
graph  shown figure    graph  node corresponds subtask
primitive action  edge corresponds potential way one subtask
 call  one child tasks  notation formal actual  e g   t source  tells formal
parameter bound actual parameter 
suppose subtasks  write policy  e g   computer
program  achieve subtask  refer policy subtask  subroutine   view parent subroutine invoking child subroutine via ordinary
   

fidietterich

root

get

put
t source

pickup

t destination

navigate t 

north

south

east

putdown

west

figure    task graph taxi problem 
subroutine call and return semantics  policy subtask  gives
us overall policy taxi mdp  root subtask executes policy calling
subroutines policies get put subtasks  get policy calls subroutines
navigate t  subtask pickup primitive action  on  call
collection policies hierarchical policy  hierarchical policy  subroutine executes
enters terminal state subtask 

    definitions

let us formalize discussion far 
maxq decomposition takes given mdp decomposes finite set
subtasks fm    m            mn g convention m  root subtask  i e   solving
m  solves entire original mdp   
definition   unparameterized subtask three tuple  hti  ai   r i i  defined follows 
   ti termination predicate partitions set active states  si   set
terminal states  ti   policy subtask mi executed current
state si   if  time subtask mi executed  mdp enters
state ti   mi terminates immediately  even still executing subtask  see
below  
   ai set actions performed achieve subtask mi   actions
either primitive actions a  set primitive actions mdp 
subtasks  denote indexes i  refer
actions  children  subtask i  sets ai define directed graph
subtasks m            mn   graph may contain cycles  stated another way 
subtask invoke recursively either directly indirectly 
child subtask mj formal parameters  interpreted subtask
occurred multiple times ai   one occurrence possible tuple actual
   

fimaxq hierarchical reinforcement learning

values could bound formal parameters  set actions ai may differ
one state another one set actual parameter values another 
technically  ai function actual parameters  however  suppress
dependence notation 
   r   s    pseudo reward function  specifies  deterministic  pseudo reward
transition terminal state s    ti   pseudo reward tells desirable
terminal states subtask  typically employed give goal
terminal states pseudo reward   non goal terminal states negative
reward  definition  pseudo reward r   s  zero non terminal states
s  pseudo reward used learning  mentioned
section   
primitive action primitive subtask maxq decomposition
always executable  always terminates immediately execution 
pseudo reward function uniformly zero 

subtask formal parameters  possible binding actual values
formal parameters specifies distinct subtask  think values formal
parameters part  name  subtask  practice  course  implement
parameterized subtask parameterizing various components task  b specifies
actual parameter values task mi   define parameterized termination
predicate ti  s  b  parameterized pseudo reward function r   s    b   simplify notation
rest paper  usually omit parameter bindings  however 
noted parameter subtask takes large number possible values 
equivalent creating large number different subtasks  need
learned  create large number candidate actions parent task 
make learning problem dicult parent task well 

definition   hierarchical policy    set containing policy subtasks
problem    f            n g 
subtask policy takes state returns name primitive action
execute name subroutine  and bindings formal parameters  invoke 
terminology sutton  precup  singh         subtask policy deterministic
 option   probability terminating state  which denote  s    
  si       ti  
parameterized task  policy must parameterized well takes
state bindings formal parameters returns chosen action bindings
 if any  formal parameters 
table   gives pseudo code description procedure executing hierarchical
policy  hierarchical policy executed using stack discipline  similar ordinary
programming languages  let kt denote contents pushdown stack time t 
subroutine invoked  name actual parameters pushed onto stack 
subroutine terminates  name actual parameters popped stack 
notice  line     subroutine stack terminates  subroutines
   

fidietterich

table    pseudo code execution hierarchical policy 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  

procedure executehierarchicalpolicy  

st state world time
kt state execution stack time
let      kt   empty stack  observe st
push     nil  onto stack kt  invoke root task parameters 

repeat
top kt   primitive action

let  i       top kt  
name  current  subroutine 
gives parameter bindings
let  a  fa       s    
action fa gives parameter bindings chosen policy
push  a  fa   onto stack kt
end   
let  a  nil     pop kt  primitive action top stack 
execute primitive action a  observe st    receive reward r st  jst   a 
subtask kt terminated st  
let   terminated subtask highest  closest root  stack 
top kt       pop kt 
pop kt 
kt      kt resulting execution stack 
kt   empty
end executehierarchicalpolicy

immediately aborted  control returns subroutine invoked
terminated subroutine 
sometimes useful think contents stack additional part
state space problem  hence  hierarchical policy implicitly defines mapping
current state st current stack contents kt primitive action a  action
executed  yields resulting state st   resulting stack contents kt    
added state information stack  hierarchical policy non markovian
respect original mdp 
hierarchical policy maps states stack contents k actions 
value function hierarchical policy must assign values combinations states
stack contents k  

definition   hierarchical value function  denoted v  hs  k i   gives expected cumu 

lative reward following hierarchical policy starting state stack contents
k 

hierarchical value function exactly learned ron parr s      b  hamq
algorithm  discuss below  however  paper  focus learning
projected value functions subtasks m            mn hierarchy 
   

fimaxq hierarchical reinforcement learning

definition   projected value function hierarchical policy subtask mi  denoted
v  i  s   expected cumulative reward executing  and policies descendents
mi   starting state mi terminates 
purpose maxq value function decomposition decompose v     s   the
projected value function root task  terms projected value functions v  i  s 
subtasks maxq decomposition 

    decomposition projected value function

defined hierarchical policy projected value function  show
value function decomposed hierarchically  decomposition based
following theorem 

theorem   given task graph tasks m            mn hierarchical policy  
subtask mi defines semi markov decision process states si   actions ai   probability
transition function pi  s    n js  a   expected reward function r s  a    v  a  s  
v  a  s  projected value function child task state s  primitive
action 
v  a  s  defined expected immediate reward executing s  v  a  s   
p
 
 
s  p  s js  a r s js  a  
proof  consider subroutines descendents task mi task graph 

subroutines executing fixed policies  specified hierarchical policy
   probability transition function pi  s    n js  a  well defined  stationary distribution
child subroutine a  set states si set actions ai obvious 
interesting part theorem fact expected reward function r s  a 
smdp projected value function child task  
see this  let us write value v  i  s  

v  i  s    e frt   rt       rt     jst   s  g
   
sum continues subroutine task mi enters state ti  
let us suppose first action chosen subroutine a  subroutine
invoked  executes number steps n terminates state s  according
pi  s    n js  a   rewrite equation    
v  i  s    e

 

nx
  
u  

u rt u

 

 
x

u n




t u fifi

ur

st   s 

 

   

first summation right hand side equation     discounted sum rewards
executing subroutine starting state terminates  words  v  a  s  
projected value function child task   second term right hand side
equation value s  current task i  v  i  s     discounted n  
s  current state subroutine terminates  write form
bellman equation 
x
   
v  i  s    v  i  s   s    pi  s    n js   s   n v  i  s   
s   n

   

fidietterich

form equation      bellman equation smdp 
first term expected reward r s   s    q e d 
obtain hierarchical decomposition projected value function  let us switch
action value  or q  representation  first  need extend q notation
handle task hierarchy  let q  i  s  a  expected cumulative reward subtask
mi performing action state following hierarchical policy subtask
mi terminates  action may either primitive action child subtask 
notation  re state equation     follows 

q  i  s  a    v  a  s   

x

s   n

pi  s    n js  a  n q  i  s     s     

   

right most term equation expected discounted reward completing task

mi executing action state s  term depends i  s  a 
summation marginalizes away dependence s  n   let us define c  i  s  a 
equal term 

definition   completion function  c  i  s  a   expected discounted cumulative

reward completing subtask mi invoking subroutine subtask state s 
reward discounted back point time begins execution 

c  i  s  a   

x

s   n

pi  s   n js  a  n q  i  s     s    

   

definition  express q function recursively

q  i  s  a    v  a  s    c  i  s  a  

    

finally  re express definition v  i  s 

v  i  s   

 

 i  s   s  
q
composite
p
 
 
s  p  s js  i r s js  i  primitive

    

refer equations                 decomposition equations
maxq hierarchy fixed hierarchical policy   equations recursively decompose
projected value function root  v     s  projected value functions
individual subtasks  m            mn individual completion functions c  j  s  a 
j              n  fundamental quantities must stored represent value
function decomposition c values non primitive subtasks v values
primitive actions 
make easier programmers design debug maxq decompositions 
developed graphical representation call maxq graph  maxq graph
taxi domain shown figure    graph contains two kinds nodes  max nodes
q nodes  max nodes correspond subtasks task decomposition there
one max node primitive action one max node subtask  including
root  task  primitive max node stores value v  i  s   q nodes correspond
actions available subtask  q node parent task i  state
   

fimaxq hierarchical reinforcement learning

maxroot

qpickup

qget

qput

maxget

maxput

qnavigateforput

qnavigateforget

t source

qputdown

t destination

pickup

putdown

maxnavigate t 

qnorth t 

qeast t 

qsouth t 

qwest t 

north

east

south

west

figure    maxq graph taxi domain 
subtask stores value c  i  s  a   children node unordered that
is  order drawn figure   imply anything order
executed  indeed  child action may executed multiple times
parent subtask completed 
addition storing information  max nodes q nodes viewed performing parts computation described decomposition equations  specifically 
max node viewed computing projected value function v  i  s 
subtask  primitive max nodes  information stored node  composite
max nodes  information obtained  asking  q node corresponding  s  
q node parent task child task viewed computing value
q  i  s  a    asking  child task projected value function v  a  s 
adding completion function c  i  s  a  
   

fidietterich

example  consider situation shown figure    denote s   
suppose passenger r wishes go b  let hierarchical policy
evaluating optimal policy denoted  we omit superscript   reduce
clutter notation   value state     cost  
unit move taxi r    unit pickup passenger    units move taxi b 
  unit putdown passenger  total    units  a reward      
passenger delivered  agent gets reward      net value     
figure   shows maxq hierarchy computes value  compute value
v  root  s     maxroot consults policy finds root  s   get  hence   asks 
q node  qget compute q  root  s    get   completion cost root task
performing get  c  root  s    get       cost   units deliver
customer  for net reward              completing get subtask  however 
reward completing get  must ask maxget estimate expected
reward performing get itself 
policy maxget dictates s    navigate subroutine invoked
bound r  maxget consults q node  qnavigateforget compute expected
reward  qnavigateforget knows completing navigate r  task  one action
 the pickup  required complete get  c  maxget  s    navigate r        
asks maxnavigate r  compute expected reward performing navigate
location r 
policy maxnavigate chooses north action  maxnavigate asks qnorth
compute value  qnorth looks completion cost  finds c  navigate  s    north 
   i e   navigate task completed performing north action   consults
maxnorth determine expected cost performing north action itself 
maxnorth primitive action  looks expected reward     
series recursive computations conclude follows 

q  navigate r   s    north          
v  navigate r   s        
q  get  s    navigate r            
    perform navigate plus    complete get 
v  get  s       
q  root  s   get           
    perform get plus    complete root task collect final reward  
end result value v  root  s    decomposed sum
c terms plus expected reward chosen primitive action 

v  root  s      v  north  s      c  navigate r   s    north   
c  get  s    navigate r     c  root  s   get 
                  
    
   

fimaxq hierarchical reinforcement learning

  
maxroot
  
  

qget

qput

  
maxget

maxput
  

qpickup

qnavigateforput

qnavigateforget

qputdown

  
  
pickup

putdown

maxnavigate t 
  
 

qnorth t 

qeast t 

qsouth t 

qwest t 

east

south

west

  
north

figure    computing value state using maxq hierarchy  c value
q node shown left node  numbers show values
returned graph 
general  maxq value function decomposition form

v     s    v  am   s    c  am     s              c  a    s  a      c     s  a         
a    a             path  max nodes chosen hierarchical policy going
root primitive leaf node  summarized graphically figure   
summarize presentation section following theorem 

theorem   let   fi               ng hierarchical policy defined given maxq
graph subtasks m            mn   let     root node graph 
exist values c  i  s  a   for internal max nodes  v  i  s   for primitive  leaf max
   

fidietterich



v   x  s 

xxxxx




x
v  a   s 
p
pppp
 

 
v  am     s 

   

zz

zz


v  am   s  c  am     s   
r 

r 

r 

r 

r 

c  a    s  a   
     

r 

r 

c     s  a   

r   r   r   r   r  

figure    maxq decomposition  r            r   denote sequence rewards received
primitive actions times               
nodes  v     s   as computed decomposition equations                 
expected discounted cumulative reward following policy starting state s 

proof  proof induction number levels task graph 

level i  compute values c  i  s   s    or v  i  s   primitive  according
decomposition equations  apply decomposition equations compute
q  i  s   s   apply equation     theorem   conclude q  i  s   s   gives
value function level i       obtain value function entire
hierarchical policy  q  e  d 
important note representation theorem mention pseudoreward function  pseudo reward used learning  theorem
captures representational power maxq decomposition  address
question whether learning algorithm find given policy 
subject next section 

   learning algorithm maxq decomposition
section presents central contributions paper  first  discuss optimality criteria employed hierarchical reinforcement learning  introduce
maxq   learning algorithm  learn value functions  and policies  maxq
hierarchies pseudo rewards  i e   pseudo rewards zero  
central theoretical result paper maxq   converges recursively optimal
policy given maxq hierarchy  followed brief discussion ways
accelerating maxq   learning  section concludes description maxq q
learning algorithm  handles non zero pseudo reward functions 
   

fimaxq hierarchical reinforcement learning

    two kinds optimality

order develop learning algorithm maxq decomposition  must consider
exactly hoping achieve  course  mdp   would find
optimal policy   however  maxq method  and hierarchical reinforcement
learning general   programmer imposes hierarchy problem  hierarchy
constrains space possible policies may possible represent
optimal policy value function 
maxq method  constraints take two forms  first  within subtask 
possible primitive actions may permitted  example  taxi task 
navigate t   north  south  east  west actions available the pickup
putdown actions allowed  second  consider max node mj child nodes
fmj           mjk g  policy learned mj must involve executing learned policies
child nodes  policy child node mji executed  run enters
state tji   hence  policy learned mj must pass subset
terminal state sets ftj           tjk g 
ham method shares two constraints addition  imposes
partial policy node  policy subtask mi must deterministic
refinement given non deterministic initial policy node i 
 option  approach  policy even constrained  approach 
two non primitive levels hierarchy  subtasks lower level  i e  
whose children primitive actions  given complete policies programmer 
hence  learned policy upper level must constructed  concatenating 
given lower level policies order 
purpose imposing constraints policy incorporate prior knowledge
thereby reduce size space must searched find good policy 
however  constraints may make impossible learn optimal policy 
can t learn optimal policy  next best target would learn best
policy consistent  i e   represented by  given hierarchy 
 

 

definition   hierarchically optimal policy mdp policy achieves
highest cumulative reward among policies consistent given hierarchy 

parr      b  proves hamq learning algorithm converges probability  
hierarchically optimal policy  similarly  given fixed set options  sutton  precup 
singh        prove smdp learning algorithm converges hierarchically
optimal value function  incidentally  show primitive actions
made available  trivial  options  smdp method converges optimal
policy  however  case  hard say anything formal options speed
learning process  may fact hinder  hauskrecht et al         
maxq decomposition represent value function hierarchical
policy  could easily construct modified version hamq algorithm apply
learn hierarchically optimal policies maxq hierarchy  however  decided
pursue even weaker form optimality  reasons become clear proceed 
form optimality called recursive optimality 
   

fidietterich

maxroot

g

qexit

qgotogoal

maxexit

maxgotogoal

 

 

qexitnorth

qexitsouth

qexiteast

north

qnorthg

south

qsouthg

qeastg

east

figure    simple mdp  left  associated maxq graph  right   policy shown
left diagram recursively optimal hierarchically optimal  shaded
cells indicate points locally optimal policy globally optimal 

definition   recursively optimal policy markov decision process maxq
decomposition fm            mk g hierarchical policy   f            k g

subtask mi   corresponding policy optimal smdp defined set states
si   set actions ai   state transition probability function p  s    n js  a  
reward function given sum original reward function r s  js  a  pseudoreward function r   s    

note state transition probability distribution  p  s    n js  a  subtask mi
defined locally optimal policies fj g subtasks descendents mi
maxq graph  hence  recursive optimality kind local optimality
policy node optimal given policies children 
reason seek recursive optimality rather hierarchical optimality recursive optimality makes possible solve subtask without reference context
executed  context free property makes easier share re use
subtasks  turn essential successful use state abstraction 
proceed describe learning algorithm recursive optimality  let us see
recursive optimality differs hierarchical optimality 
easy construct examples policies recursively optimal hierarchically optimal  and vice versa   consider simple maze problem associated
maxq graph shown figures    suppose robot starts somewhere left room 
must reach goal g right room  robot three actions  north  south 
east  actions deterministic  robot receives reward    move 
let us define two subtasks 
   

fimaxq hierarchical reinforcement learning

exit  task terminates robot exits left room  set pseudo 

reward function r    two terminal states  i e   two states indicated
  s  
gotogoal  task terminates robot reaches goal g 
arrows figure   show locally optimal policy within room  arrows
left seek exit left room shortest path  specified
set pseudo reward function    arrows right follow shortest
path goal  fine  however  resulting policy neither hierarchically optimal
optimal 
exists hierarchical policy would always exit left room upper
door  maxq value function decomposition represent value function
policy  policy would locally optimal  because  example  states
 shaded  region would follow shortest path doorway   hence 
example illustrates recursively optimal policy hierarchically optimal
hierarchically optimal policy recursively optimal 
consider moment  see way fix problem  value
upper starred state optimal hierarchical policy    value lower
starred state     hence  changed r  values  instead zero  
recursively optimal policy would hierarchically optimal  and globally optimal  
words  programmer guess right values terminal states
subtask  recursively optimal policy hierarchically optimal 
basic idea first pointed dean lin         describe algorithm
makes initial guesses values starred states updates
guesses based computed values starred states resulting recursivelyoptimal policy  proved converge hierarchically optimal policy 
drawback method requires repeated solution resulting hierarchical
learning problem  always yield speedup solving original 
problem 
parr      a  proposed interesting approach constructs set different r  functions computes recursively optimal policy subtask 
method chooses r  functions way hierarchically optimal policy
approximated desired degree  unfortunately  method quite expensive 
relies solving series linear programming problems requires time
polynomial several parameters  including number states jsi j within subtask 
discussion suggests while  principle  possible learn good values
pseudo reward function  practice  must rely programmer specify single
pseudo reward function  r    subtask  programmer wishes consider small
number alternative pseudo reward functions  handled defining small
number subtasks identical except r  functions  permitting
learning algorithm choose one gives best recursively optimal policy 
experiments  employed following simplified approach defining
r    subtask mi  define two predicates  termination predicate  ti  
goal predicate  gi   goal predicate defines subset terminal states  goal
states   pseudo reward    terminal states fixed constant
   

fidietterich

pseudo reward  e g         set always better terminate goal state
non goal state  problems tested maxq method 
worked well 
experiments maxq  found easy make mistakes
defining ti gi   goal defined carefully  easy create set subtasks
lead infinite looping  example  consider problem figure    suppose
permit fourth action  west  mdp let us define termination goal
predicates right hand room satisfied iff either robot reaches goal
exits room  natural definition  since quite similar definition
left hand room  however  resulting locally optimal policy room
attempt move nearest three locations  goal  upper door 
lower door  easily see states near goal  policies
constructed maxroot loop forever  first trying leave left room
entering right room  trying leave right room entering left room 
problem easily fixed defining goal predicate gi right room true
robot reaches goal g  avoiding  undesired termination  bugs
hard complex domains 
worst case  possible programmer specify pseudo rewards
recursively optimal policy made arbitrarily worse hierarchically optimal
policy  example  suppose change original mdp figure   state
immediately left upper doorway gives large negative reward  l whenever
robot visits square  rewards everywhere else     hierarchicallyoptimal policy exits room lower door  suppose programmer chosen
instead force robot exit upper door  e g   assigning pseudo reward
   l leaving via lower door   case  recursively optimal policy leave
upper door suffer large  l penalty  making l arbitrarily large 
make difference hierarchically optimal policy recursively optimal
policy arbitrarily large 

    maxq   learning algorithm
understanding recursively optimal policies  present two learning
algorithms  first one  called maxq    applies case pseudo reward
function r  always zero  first prove convergence properties show
extended give second algorithm  maxq q  works general
pseudo reward functions 
table   gives pseudo code maxq    maxq   recursive function executes
current exploration policy starting max node state s  performs actions
reaches terminal state  point returns count total number primitive
actions executed  execute action  maxq   calls recursively
 line     recursive call returns  updates value completion function
node i  uses count number primitive actions appropriately discount
value resulting state s    leaf nodes  maxq   updates estimated one step
expected reward  v  i  s   value fft  i   learning rate  parameter
gradually decreased zero limit 
   

fimaxq hierarchical reinforcement learning

table    maxq   learning algorithm 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  

function maxq   maxnode i  state s 
primitive maxnode
execute i  receive r  observe result state s 
vt  i  s          fft  i   vt  i  s    fft  i  rt
return  
else
let count    
ti  s  false
choose action according current exploration policy x i  s 
let n   maxq   a   s   recursive call 
observe result state
ct  i  s  a          fft  i   ct  i  s  a    fft  i  n vt  i  s   
  

  

count    count   n
   s 

end
return count

end maxq  

   main program
initialize v  i  s  c  i  s  j   arbitrarily
maxq   root node    starting state s   

three things must specified order make algorithm description
complete 
first  keep pseudo code readable  table   show  ancestor termination  handled  recall action  termination predicates
subroutines calling stack checked  termination predicate one
satisfied  calling stack unwound highest terminated subroutine  cases  c values updated subroutines interrupted
except follows  subroutine invoked subroutine j   j  s termination condition
satisfied  subroutine update value c  i  s  j   
second  must specify compute vt  i  s    line     since stored
max node  computed following modified versions decomposition
equations 
 

maxa qt  i  s  a  composite
    
vt  i  s 
primitive
qt i  s  a    vt a  s    ct  i  s  a  
    
equations ect two important changes compared equations           
first  equation       vt  i  s  defined terms q value best action a  rather
action chosen fixed hierarchical policy  second  superscripts 
current value function  vt  i  s   based fixed hierarchical policy  
compute vt  i  s  using equations  must perform complete search
paths maxq graph starting node ending leaf nodes  table  

vt  i  s   

   

fidietterich

table    pseudo code greedy execution maxq graph 
function evaluatemaxnode i  s 
 
 
 
 
 
 
 

primitive max node
return hvt  i  s   ii
else
j   ai  
let hvt  j  s   aj   evaluatemaxnode j  s 
let j hg   argmaxj vt j  s    ct  i  s  j  
return hvt  j hg   s   ajhg
end    evaluatemaxnode

gives pseudo code recursive function  evaluatemaxnode  implements depthfirst search  addition returning vt  i  s   evaluatemaxnode returns action
leaf node achieves value  information needed maxq   
useful later consider non hierarchical execution learned recursivelyoptimal policy 
search computationally expensive  problem future research
develop ecient methods computing best path graph  one
approach perform best first search use bounds values within subtrees
prune useless paths maxq graph  better approach would make
computation incremental  state environment changes 
nodes whose values changed result state change re considered 
possible develop ecient bottom up method similar rete algorithm  and
successors  used soar architecture  forgy        tambe   rosenbloom 
      
third thing must specified complete definition maxq  
exploration policy  x   require x ordered glie policy 

definition   ordered glie policy glie policy  greedy limit infinite

exploration  converges limit ordered greedy policy  greedy policy
imposes arbitrary fixed order   available actions breaks ties favor
action appears earliest order 

need property order ensure maxq   converges uniquely defined
recursively optimal policy  fundamental problem recursive optimality
general  max node choice many different locally optimal policies given
policies adopted descendent nodes  different locally optimal policies
achieve locally optimal value function  give rise different probability transition functions p  s    n js  i   result semi markov decision
problems defined next level node maxq graph differ depending
various locally optimal policies chosen node i  differences may
lead better worse policies higher levels maxq graph  even though make
difference inside subtask i  practice  designer maxq graph need
design pseudo reward function subtask ensure locally optimal policies
   

fimaxq hierarchical reinforcement learning

equally valuable parent subroutine  carry formal analysis 
rely arbitrary tie breaking mechanism   establish fixed ordering
max nodes maxq graph  e g   left to right depth first numbering   break ties
favor lowest numbered action  defines unique policy max node 
consequently  induction  defines unique policy entire maxq graph  let
us call policy r   use r subscript denote recursively optimal quantities
ordered greedy policy  hence  corresponding value function vr   cr
qr denote corresponding completion function action value function  prove
maxq   algorithm converges r  

theorem   let   hs  a  p  r  p  either episodic mdp deterministic

policies proper discounted infinite horizon mdp discount factor   let h
maxq graph defined subtasks fm            mk g pseudo reward function
r   s    zero s   let fft  i      sequence constants max node



x
x
lim

 

 
 
 

lim
ff t  i     
    

  
  
t  

t  

let x  i  s  ordered glie policy node state assume
immediate rewards bounded  probability    algorithm maxq   converges
r   unique recursively optimal policy consistent h x 

proof  proof follows argument similar introduced prove convergence

q learning sarsa     bertsekas   tsitsiklis        jaakkola et al         
employ following result stochastic approximation theory  state without
proof 

lemma    proposition     bertsekas tsitsiklis        consider iteration
rt    i          fft  i  rt  i    fft  i   urt   i    wt  i    ut i   
let ft   fr   i           rt  i   w   i           wt    i   ff   i           fft  i    ig entire history
iteration 


 a  fft  i    satisfy conditions     
 b  every t  noise terms wt  i  satisfy e  wt  i jft      
 c  given norm jj jj rn   exist constants b e  wt   i jft  
  b jjrtjj   
 d  exists vector r   positive vector   scalar          
t 

jjurt   rjj jjrt   rjj

   alternatively  could break ties using stochastic policy chose randomly among tied
actions 

   

fidietterich

 e  exists nonnegative random sequence converges zero probability
 
jut  i j t jjrt jj     
rt converges r probability    notation jj jj denotes weighted maximum
norm
ja i j  
jjajj   max
 i 

structure proof theorem   inductive  starting leaves
maxq graph working toward root  employ different time clock
node count number update steps performed maxq   node 
variable always refer time clock current node i 
prove base case primitive max node  note line   maxq  
standard stochastic approximation algorithm computing expected reward
performing action state s  therefore converges conditions given
above 
prove recursive case  consider composite max node child node j   let
pt  s   n js  j   transition probability distribution performing child action j state
time  i e   following exploration policy descendent nodes node j   
inductive assumption  maxq   applied j converge  unique  recursively optimal value function vr  j  s  probability    furthermore  maxq  
following ordered glie policy j descendents  converge executing greedy policy respect value functions  pt  s    n js  j   converge
pr  s    n js  j    unique transition probability function executing child j
locally optimal policy r   remains shown update assignment c
 line    maxq   algorithm  converges optimal cr function probability
  
prove this  apply lemma    identify x lemma
state action pair  s  a   vector rt completion cost table ct  i  s  a 
s  fixed update steps  vector r optimal completion cost
cr  i  s  a   again  fixed i   define mapping u
 uc   i  s  a   

x

s 





   
   
pr  s    n js  a  n max
   c  i        vr  a     


c update mdp mi assuming descendent value functions 
vr  a  s   transition probabilities  pr s    n js  a   converged 
apply lemma  must first express c update formula form
update rule lemma  let state results performing state s  line
   written

ct    i  s  a    

     fft  i   ct  i  s  a    fft  i  n





max ct  i  s  a      vt  a    s  
a 

        fft  i   ct  i  s  a    fft  i    uct   i  s  a    wt  i  s  a    ut  i  s  a  
   

fimaxq hierarchical reinforcement learning







wt  i  s  a    n max
 c  i  s  a      vt  a    s    
a 
x

s   n

ut  i  s  a   

x

s   n
x

s   n









pt  s    n js  a  n max
 c  i  s    a      vt  a    s    
a 

   
   
pt  s    n js  a  n max
   ct  i        vt  a     




 



pr  s   n js  a  n max
 c  i  s    a      vr  a    s    
a 

wt  i  s  a  difference update node using single sample
point drawn according pt  s    n js  a  update using full distribution
pt  s   n js  a   value ut  i  s  a  captures difference update using
current probability transitions pt  s    n js  a  current value functions children
vt  a   s    update using optimal probability transitions pr  s    n js  a 
optimal values children vr  a    s    
verify conditions lemma   
condition  a  assumed conditions theorem fft  s  a    fft  i  
condition  b  satisfied sampled pt  s    n js  a   expected value
difference zero 
condition  c  follows directly fact jct  i  s  a j jvt  i  s j bounded 
show bounded episodic case discounted case
follows  episodic case  assumed policies proper  hence  trajectories
terminate finite time finite total reward  discounted case  infinite sum
future rewards bounded one step rewards bounded  values c v
computed temporal averages cumulative rewards received finite number
 bounded  updates  hence  means  variances  maximum values
bounded 
condition  d  condition u weighted max norm pseudo contraction 
derive starting weighted max norm q learning  well known
q weighted max norm pseudo contraction  bertsekas   tsitsiklis       
episodic case deterministic policies proper  and discount factor     
infinite horizon discounted case  with       is  exists positive
vector scalar           t 

jjtqt   qjj jjqt   qjj  

    

operator
 tq  s  a   

x

s   n

p  s   n js  a  n  r s  js  a    max
q s    a     
a 

show derive pseudo contraction c update operator u  
plan show first express u operator learning c terms operator
updating q values  replace tq pseudo contraction equation q
   

fidietterich

learning uc   show u weighted max norm pseudo contraction
weights  
recall eqn       q i  s  a    c  i  s  a    v  a  s   furthermore  u operator
performs updates using optimal value functions child nodes  write
qt  i  s  a    ct  i  s  a    v  a  s   children node converged 
q function version bellman equation mdp mi written

q i  s  a   

x

s   n

pr s    n js  a  n  vr  a  s    max
q i  s    a     
a 

noted before  vr  a  s  plays role immediate reward function mi  
therefore  node i  operator rewritten
 tq  i  s  a   

x

s   n

pr  s  js  a  n  vr a  s    max
q i  s    a     
a 

replace q i  s  a  c  i  s  a    vr  a  s   obtain
 tq  i  s  a   

x

s   n

pr  s   n js  a  n  vr  a  s    max
 c  i  s    a      vr  a    s      
a 

note vr  a  s  depend s  n   move outside expectation
obtain
 tq  i  s  a    vr  a  s   

x

s   n

pr  s    n js  a  n  max
 c  i  s    a      vr  a    s     
a 

  vr  a  s     uc   i  s  a 

abusing notation slightly  express vector form tq i    vr   uc  i  
similarly  write qt  i  s  a    ct  i  s  a   vr  a  s  vector form qt  i    ct  i   vr  
substitute two formulas max norm pseudo contraction formula
  eqn       obtain

jjvr   uct  i     cr i    vr jj jjvr   ct  i     cr i    vr jj  
thus  u weighted max norm pseudo contraction 

jjuct  i    cr i jj jjct  i    cr i jj  
condition  d  satisfied 
finally  easy verify  e   important condition  assumption 
ordered glie policies child nodes converge probability   locally optimal
policies children  therefore pt  s    n js  a  converges pr  s    n js  a  s   n  s 
probability   vt  a  s  converges probability   vr  a  s  child
actions a  therefore  jut j converges zero probability    trivially construct
sequence   jut j bounds convergence 

jut  s  a j  jjct  s  a jj      
   

fimaxq hierarchical reinforcement learning

verified conditions lemma    conclude ct  i  converges
cr i  probability    induction  conclude holds nodes
maxq including root node  value function represented maxq graph
converges unique value function recursively optimal policy r   q e d 
important aspect theorem proves q learning take
place levels maxq hierarchy simultaneously the higher levels need
wait lower levels converged begin learning  necessary
lower levels eventually converge  locally  optimal policies 

    techniques speeding maxq  

algorithm maxq   extended accelerate learning higher nodes graph
technique call  all states updating   action chosen max node
state s  execution move environment sequence states
  s           sn   sn      s   subroutines markovian  resulting
state s  would reached started executing action state s    s   
state including sn   hence  execute version line    maxq  
intermediate states shown replacement pseudo code 
  a
j   n
  b
ct  i  sj   a          fft  i   ct i  sj   a    fft  i  n  j maxa qt  i  s    a   
  c
end   
implementation  composite action executed maxq    constructs
linked list sequence primitive states visited  list returned
composite action terminates  parent max node process state
list shown above  parent max node concatenates state lists receives
children passes parent terminates  experiments paper
employ all states updating 
kaelbling        introduced related  powerful  method accelerating hierarchical reinforcement learning calls  all goals updating   understand
method  suppose primitive action  several composite tasks could
invoked primitive action  all goals updating  whenever primitive action
executed  equivalent line    maxq   applied every composite task could
invoked primitive action  sutton  precup  singh        prove
composite tasks converge optimal q values all goals updating  furthermore  point exploration policy employed choosing primitive
actions different policies subtasks learned 
straightforward implement simple form all goals updating within maxq
hierarchy case composite tasks invoke primitive actions  whenever one
primitive actions executed state s  update c  i  s  a  value parent
tasks invoke a 
however  additional care required implement all goals updating non primitive
actions  suppose executing exploration policy  following sequence world
states actions obtained  s    a    s            ak     sk     ak   sk     let j composite task terminated state sk     let sk n  ak n           ak     ak sequence
actions could executed subtask j children  words  suppose
 

  

   

  

 

 

fidietterich

possible  parse  state action sequence terms series subroutine calls
returns one invocation subtask j   possible parent task invokes j  
update value c  i  sk n   j    course  order updates useful 
exploration policy must ordered glie policy converge recursively
optimal policy subtask j descendents  cannot follow arbitrary exploration
policy  would produce accurate samples result states drawn according
p  s    n js  j    hence  unlike simple case described sutton  precup  singh 
exploration policy cannot different policies subtasks learned 
although considerably reduces usefulness all goals updating 
completely eliminate it  simple way implementing non primitive all goals updating
would perform maxq q learning usual  whenever subtask j invoked
state returned  could update value c  i  s  j   potential calling subtasks
i  implemented this  however  complexity involved identifying
possible actual parameters potential calling subroutines 

    maxq q learning algorithm
shown convergence maxq    let us design learning algorithm
work arbitrary pseudo reward functions  r   s     could add pseudoreward maxq    would effect changing mdp
different reward function  pseudo rewards  contaminate  values
completion functions computed hierarchy  resulting learned policy
recursively optimal original mdp 
problem solved learning one completion function use  inside 
max node separate completion function use  outside  max node  quantities used  inside  node written tilde  r    c    q    quantities used
 outside  node written without tilde 
 outside  completion function  c  i  s  a  completion function
discussing far paper  computes expected reward completing task
mi performing action state following learned policy mi  
computed without reference r    completion function used parent
tasks compute v  i  s   expected reward performing action starting state s 
second completion function c   i  s  a  completion function use
 inside  node order discover locally optimal policy task mi   function
incorporate rewards  real  reward function  r s  js  a  
pseudo reward function  r   s     used evaluatemaxnode line  
choose best action j hg execute  note  however  evaluatemaxnode still
return  external  value vt  j hg   s  chosen action 
employ two different update rules learn two completion functions 
c  function learned using update rule similar q learning rule line   
maxq    c function learned using update rule similar sarsa    
purpose learn value function policy discovered optimizing c   
pseudo code resulting algorithm  maxq q shown table   
key step lines        line     maxq q first updates c  using value
greedy action    resulting state  update includes pseudo reward r   
   

fimaxq hierarchical reinforcement learning

table    maxq q learning algorithm 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  

function maxq q maxnode i  state s 
let seq      sequence states visited executing
primitive maxnode
execute i  receive r  observe result state s 
vt  i  s          fft  i   vt  i  s    fft  i  rt
push onto beginning seq
else
let count    
ti  s  false
choose action according current exploration policy x i  s 
let childseq   maxq q a s   childseq sequence states visited
  

executing action a   in reverse order 
observe result state s 
let   argmaxa  c t  i  s    a      vt  a    s    
let n    
childseq
c t    i  s  a          fft  i   c t  i  s  a    fft  i  n  r   s      c t  i  s        vt a   s  
ct    i  s  a          fft  i   ct  i  s  a    fft  i  n  ct  i  s        vt a   s    
n    n    
end   
append
childseq onto front seq
   s 
end   
end    else
return seq
end maxq q
 

line     maxq q updates c using greedy action   even would
greedy action according  uncontaminated  value function  update 
course  include pseudo reward function 
important note whereever vt  a  s  appears pseudo code  refers
 uncontaminated  value function state executing max node a 
computed recursively exactly way maxq   
finally  note pseudo code incorporates all states updating  call
maxq q returns list states visited execution 
updates lines       performed states  list states
ordered most recent first  states updated starting last state visited
working backward starting state  helps speed algorithm 
maxq q converged  resulting recursively optimal policy computed
node choosing action maximizes q   i  s  a    c   i  s  a   v  a  s   breaking
ties according fixed ordering established ordered glie policy  
reason gave name  max nodes  nodes represent subtasks  and
learned policies  within maxq graph  q node j parent node stores
c   i  s  j   c  i  s  j    computes q   i  s  j   q i  s  j   invoking child
max node j   max node takes maximum q values computes either
v  i  s  computes best action  using q   
   

fidietterich

corollary   conditions theorem    maxq q converges unique

recursively optimal policy mdp defined maxq graph h   pseudo reward functions
r    ordered glie exploration policy x 
proof  argument identical to  tedious than  proof theorem   
proof convergence c  values identical original proof c values 
relies proving convergence  new  c values well  follows
weighted max norm pseudo contraction argument  q e d 

   state abstraction

many reasons introduce hierarchical reinforcement learning  perhaps
important reason create opportunities state abstraction  introduced
simple taxi problem figure    pointed within subtask  ignore
certain aspects state space  example  performing maxnavigate t  
taxi make navigation decisions regardless whether passenger
taxi  purpose section formalize conditions safe
introduce state abstractions show convergence proofs maxq q
extended prove convergence presence state abstraction  specifically 
identify five conditions permit  safe  introduction state abstractions 
throughout section  use taxi problem running example 
see five conditions permit us reduce number distinct values
must stored order represent maxq value function decomposition 
establish starting point  let us compute number values must stored
taxi problem without state abstraction 
maxq representation must tables c functions internal
nodes v functions leaves  first  six leaf nodes  store v  i  s  
must store     values node      states     locations    possible
destinations passenger    possible current locations passenger  the four
special locations inside taxi itself   second  root node  two children 
requires              values  third  maxget maxput nodes   
actions each  one requires      values  total       finally  maxnavigate t  
four actions  must consider target parameter t  take
four possible values  hence  effectively      combinations states values
action       total values must represented  total  therefore  maxq
representation requires        separate quantities represent value function 
place number perspective  consider q learning representation must
store separate value six primitive actions     possible states 
total       values  hence  see without state abstraction  maxq
representation requires four times memory q table 

    five conditions permit state abstraction

introduce five conditions permit introduction state abstractions 
condition  give definition prove lemma states condition satisfied  value function corresponding class policies
   

fimaxq hierarchical reinforcement learning

represented abstractly  i e   abstract versions v c functions   condition  provide rules identifying condition satisfied
give examples taxi domain 
begin introducing definitions notation 

definition    let mdp h maxq graph defined   suppose

state written vector values set state variables  max
node i  suppose state variables partitioned two sets xi yi   let
function projects state onto values variables xi   h combined
called state abstracted maxq graph 

cases state variables partitioned  often write    x  y 
mean state represented vector values state variables x
vector values state variables   similarly  sometimes write
p  x    y   n jx  y  a   v  a  x  y   r   x    y    place p  s   n js  a   v  a  s   r   s    
respectively 

definition     abstract policy  abstract hierarchical policy mdp state 

abstracted maxq graph h associated abstraction functions   hierarchical policy
policy  corresponding subtask mi   satisfies condition two
states s  s   s       s      s       s      when stochastic policy 
exploration policy  interpreted mean probability distributions
choosing actions states  

order maxq q converge presence state abstractions  require
times  instantaneous  exploration policy abstract hierarchical policy 
one way achieve construct exploration policy uses information relevant state variables deciding action perform  boltzmann exploration based  state abstracted  q values   greedy exploration  counter based
exploration based abstracted states abstract exploration policies  counter based
exploration based full state space abstract exploration policy 
introduced notation  let us describe analyze five abstraction conditions  identified three different kinds conditions
abstractions introduced  first kind involves eliminating irrelevant variables
within subtask maxq graph  form abstraction  nodes toward
leaves maxq graph tend relevant variables  nodes higher
graph relevant variables  hence  kind abstraction useful
lower levels maxq graph 
second kind abstraction arises  funnel  actions  macro actions
move environment large number initial states small number
resulting states  completion cost subtasks represented using number
values proportional number resulting states  funnel actions tend appear higher
maxq graph  form abstraction useful near root graph 
third kind abstraction arises structure maxq graph itself 
exploits fact large parts state space subtask may reachable
termination conditions ancestors maxq graph 
   

fidietterich

begin describing two abstraction conditions first type  present
two conditions second type  finally  describe one condition third type 
      condition    max node irrelevance

first condition arises set state variables irrelevant max node 

definition     max node irrelevance  let mi max node maxq graph h

mdp   set state variables irrelevant node state variables
partitioned two sets x stationary abstract hierarchical
policy executed descendents i  following two properties hold 

state transition probability distribution p  s   n js  a  node factored
product two distributions 

p  x    y    n jx  y  a    p  y jx  y  a  p  x    n jx  a  

    

y  give values variables   x x  give values
variables x  

pair states s     x  y    s     x  y     s      s     x 
child action a  v  a  s      v  a  s    r   s      r   s    

note two conditions must hold stationary abstract policies executed
descendents subtask i  discuss rather strong
requirements satisfied practice  first  however  prove conditions
sucient permit c v tables represented using state abstractions 

lemma   let mdp full state maxq graph h   suppose state vari 

ables yi irrelevant max node i  let  s    x associated abstraction function
projects onto remaining relevant variables xi   let abstract hierarchical
policy  action value function q node represented compactly 
one value completion function c  i  s  j   equivalence class states
share values relevant variables 
specifically q  i  s  j   computed follows 

q  i  s  j     v  j   s     c  i   s   j  


c  i  x  j    

x

x   n

p  x    n jx  j   n  v   x     x      r   x      c  i  x     x      

v  j     x      v  j     x    y     r   x      r   x    y      x     x  y    arbitrary
value y  irrelevant state variables yi  
   

fimaxq hierarchical reinforcement learning

proof  define new mdp i mi   node follows 
states  x   fx j i s    x    g 
actions  a 
transition probabilities  p  x    n jx  a 
reward function  v  a  x    r  i x  

abstract policy  decisions states  s    x
x  therefore  well defined policy  mi    action value function
 mi   unique solution following bellman equation 
x
q  i  x  j     v  j  x    p  x    n jx  j   n  r  i x      q  i  x     x     
    
x   n

compare bellman equation mi  
x
q  i  s  j     v  j  s    p  s   n js  j   n  r i  s     q  i  s     s     
s   n

    

note v  j  s    v  j   s     v  j  x  r   s      r    s       r   x     furthermore  know distribution p factored separate distributions yi
xi   hence  rewrite     
x
x
q  i  s  j     v  j  x    p  y  jx  y  j   p  x    n jx  j   n  r   x      q  i  s     s     
y 

x   n

right most sum depend y    sum y  evaluates   
eliminated give
x
q  i  s  j     v  j  x    p  x    n jx  j   n  r  i x      q  i  s     s      
    
x   n

finally  note equations           identical except expressions
q values  since solution bellman equation unique  must conclude
q  i  s  j     q  i   s   j   
rewrite right hand side obtain
q  i  s  j     v  j   s     c  i   s   j   

x
c  i  x  j     p  x   n jx  j   n  v   x     x      r   x      c  i  x     x      

q e d 

x   n

course primarily interested able discover represent optimal
policy node i  following corollary shows optimal policy abstract
policy  hence  represented abstractly 
   

fidietterich

corollary   consider conditions lemma    change ab 

stract hierarchical policy executed descendents node i  node
i  let   ordering actions  optimal ordered policy   node
abstract policy  action value function represented abstractly 

proof  define policy   optimal ordered policy abstract mdp

 m    let q  i  x  j   corresponding optimal action value function 
argument given above  q solution optimal bellman equation
original mdp  means policy   defined    s      s   optimal

ordered policy  construction  abstract policy  q e d 
stated  max node irrelevance condition appears quite dicult satisfy  since
requires state transition probability distribution factor x components
possible abstract hierarchical policies  however  practice  condition often
satisfied 
example  let us consider navigate t  subtask  source destination
passenger irrelevant achievement subtask  policy successfully completes subtask value function regardless source
destination locations passenger  abstracting away passenger source destination  obtain huge savings space  instead requiring      values represent
c functions task  require     values    actions     locations    possible
values t  
advantages form abstraction similar obtained boutilier 
dearden goldszmidt        belief network models actions exploited
simplify value iteration stochastic planning  indeed  one way understanding
conditions definition    express form decision diagram  shown
figure    diagram shows irrelevant variables affect rewards
either directly indirectly  therefore  affect either value function
optimal policy 
one rule noticing cases abstraction condition holds examine
subgraph rooted given max node i  set state variables irrelevant leaf
state transition probabilities reward functions pseudo reward functions
termination conditions subgraph  variables satisfy max node
irrelevance condition 

lemma   let mdp associated maxq graph h   let max node

h   let xi yi partition state variables   set state variables yi
irrelevant node
primitive leaf node descendent i 
p  x    y  jx  y  a    p  y  jx  y  a p  x  jx  a 
r x   y  jx  y  a    r x  jx  a  

internal node j equal node descendent   r  j  x    y    
r j  x   termination predicate tj  x    y    true iff tj  x   
   

fimaxq hierarchical reinforcement learning

j

v

x

x





figure    dynamic decision diagram represents conditions definition    
probabilistic nodes x represent state variables time t  nodes
x     represent state variables later time   n   square action
node j chosen child subroutine  utility node v represents value
function v  j  x  child action  note x may uence    
cannot affect x     therefore  cannot affect v  

proof  must show abstract hierarchical policy give rise smdp

node whose transition probability distribution factors whose reward function depends
xi   definition  abstract hierarchical policy choose actions based
upon information xi   primitive probability transition functions factor
independent component xi since termination conditions nodes
based variables xi   probability transition function pi  x    y    n jx  y  a 
must factor pi  y  jx  y  a  pi  x    n jx  a   similarly  reward functions
v  j  x  y  must equal v  j  x   rewards received within subtree  either
leaves pseudo rewards  depend variables xi   therefore 
variables yi irrelevant max node i  q e d 
taxi task  primitive navigation actions  north  south  east  west
depend location taxi location passenger  pseudoreward function termination condition maxnavigate t  node depend
location taxi  and parameter t   hence  lemma applies  passenger
source destination irrelevant maxnavigate node 
      condition    leaf irrelevance

second abstraction condition describes situations apply state abstractions leaf nodes maxq graph  leaf nodes  obtain stronger result
lemma   using slightly weaker definition irrelevance 
   

fidietterich

definition     leaf irrelevance  set state variables irrelevant primitive
action maxq graph states expected value reward function 
x
v  a  s    p  s  js  a r s  js  a 
s 

depend values state variables   words 
pair states s  s  differ values variables  
x

s  

p  s   js    a r s   js    a   

x

s  

p  s   js    a r s   js    a  

condition satisfied leaf a  following lemma shows
represent value function v  a  s  compactly 

lemma   let mdp full state maxq graph h   suppose state vari 

ables irrelevant leaf node a  let  s    x associated abstraction function
projects onto remaining relevant variables x   represent v  a  s 
state abstracted value function v  a   s     v  a  x  

proof  according definition leaf irrelevance  two states differ

irrelevant state variables value v  a  s   hence  represent
unique value v  a  x   q e d 
two rules finding cases leaf irrelevance applies  first rule shows
probability distribution factors  leaf irrelevance 

lemma   suppose probability transition function primitive action a  p  s js  a   factors p  x    y  jx  y  a    p  y  jx  y  a p  x  jx  a  reward function satisfies r s  js  a   
r x jx  a   variables irrelevant leaf node a 
proof  plug definition v  a  s  simplify 
x
v  a  s   
p  s  js  a r s  js  a 
 
 
 

s 

x

x   y 
x

y 

x

x 

p  y  jx  y  a p  x  jx  a r x  jx  a 

p  y  jx  y  a 

x

x 

p  x  jx  a r x  jx  a 

p  x  jx  a r x  jx  a 

hence  expected reward action depends variables x
variables   q e d 
second rule shows reward function primitive action constant 
apply state abstractions even p  s  js  a  factor 

lemma   suppose r s js  a   the reward function action mdp   always equal

constant ra   entire state irrelevant primitive action a 
   

fimaxq hierarchical reinforcement learning

proof 
v  a  s   

x

s 

p  s  js  a r s  js  a 

x

 
p  s  js  a ra
 

  ra  
depend s  entire state irrelevant primitive action a  q e d 
lemma satisfied four leaf nodes north  south  east  west taxi
task  one step reward constant       hence  instead requiring     
values store v functions  need   values one action  similarly 
expected rewards pickup putdown actions require   values  depending
whether corresponding actions legal illegal  hence  together  require  
values  instead      values 
      condition    result distribution irrelevance

consider condition results  funnel  actions 

definition     result distribution irrelevance   set state variables yj irrelevant result distribution action j if  abstract policies executed node j
descendents maxq hierarchy  following holds  pairs states s 
s  differ values state variables yj  

p  s    n js    j     p  s    n js    j  
s  n  

condition satisfied subtask j   c value parent task
represented compactly 

lemma   let mdp full state maxq graph h   suppose set

state variables yj irrelevant result distribution action j   child max
node i  let ij associated abstraction function  ij  s    x  define
abstract completion cost function c  i  ij  s   j   states s 

c  i  s  j     c  i  ij  s   j   

proof  completion function fixed policy defined follows 
x
c  i  s  j     p  s    n js  j   n q  i  s    
s   n

    

consider two states s  s    ij  s      ij  s      x  result distribution irrelevance  transition probability distributions same  hence 
right hand sides      value  conclude

c  i  s    j     c  i  s    j   
   

fidietterich

therefore  define abstract completion function  c  i  x  j   represent quantity  q e d 
undiscounted cumulative reward problems  definition result distribution irrelevance weakened eliminate n   number steps  needed
pairs states s  s  differ irrelevant state variables 
p  s  js    j     p  s  js   j    for s     undiscounted case  lemma   still holds
revised definition 
might appear result distribution irrelevance condition would rarely satisfied  often find cases condition true  consider  example  get
subroutine taxi task  matter location taxi state s  taxi
passenger s starting location get finishes executing  i e  
taxi completed picking passenger   hence  starting location
irrelevant resulting location taxi  p  s  js    get    p  s  js    get 
states s  s  differ taxi s location 
note  however  maximizing discounted reward  taxi s location would
irrelevant  probability get terminate exactly n steps would
depend location taxi  could differ states s  s    different values
n produce different amounts discounting       hence  cannot ignore
taxi location representing completion function get 
undiscounted case  applying lemma    represent c  root  s  get 
using    distinct values     equivalence classes states    source locations
times   destination locations   much less     quantities unabstracted
representation 
note although state variables may irrelevant result distribution
subtask j   may important within subtask j   taxi task  location
taxi critical representing value v  get  s   irrelevant result state
distribution get  therefore irrelevant representing c  root  s  get   hence 
maxq decomposition essential obtaining benefits result distribution irrelevance 
 funnel  actions arise many hierarchical reinforcement learning problems  example  abstract actions move robot doorway move car onto entrance
ramp freeway property  result distribution irrelevance condition
applicable situations long undiscounted setting 
      condition    termination

fourth condition closely related  funnel  property  applies subtask
guaranteed cause parent task terminate goal state  sense  subtask
funneling environment set states described goal predicate
parent task 

lemma    termination   let mi task maxq graph states
goal predicate gi  s  true  pseudo reward function r   s       suppose
child task state hierarchical policies  

  s  pi  s   n js  a        gi s    
   

fimaxq hierarchical reinforcement learning

 i e   every possible state s  results applying make goal predicate 
gi   true  
policy executed node i  completion cost c  i  s  a  zero
need explicitly represented 

proof  action executed state s  guaranteed result state s 

gi  s  true  definition  goal states satisfy termination predicate ti  s  
task terminate  gi s  true  terminal pseudo reward zero 
hence  completion function always zero  q e d 
example  taxi task  states taxi holding passenger 
put subroutine succeed result goal terminal state root 
termination predicate put  i e   passenger destination location 
implies goal condition root  which same   means c  root  s  put 
uniformly zero  states put terminated 
easy detect cases termination condition satisfied  need
compare termination predicate ta subtask goal predicate gi parent
task  first implies second  termination lemma satisfied 
      condition    shielding

shielding condition arises structure maxq graph 
lemma    shielding   let mi task maxq graph state
paths root graph node mi subtask j  possibly equal i 
whose termination predicate tj  s  true  q nodes mi need represent
c values state s 
proof  order task executed state s  must exist path ancestors
task leading root graph ancestor tasks
terminated  condition lemma guarantees false  hence task
cannot executed state s  therefore  c values need represented  q e d 
termination condition  shielding condition verified analyzing
structure maxq graph identifying nodes whose ancestor tasks terminated 
taxi domain  simple example arises put task  terminated
states passenger taxi  means need
represent c  root  s  put  states  result that  combined
termination condition above  need explicitly represent completion function
put all 
      dicussion

applying five abstraction conditions  obtain following  safe  state abstractions taxi task 
north  south  east  west  terminal nodes require one quantity each 
total four values   leaf irrelevance  
   

fidietterich

pickup putdown require   values  legal illegal states   total four 
 leaf irrelevance  

qnorth t   qsouth t   qeast t   qwest t  require     values  four values
   locations    max node irrelevance  

qnavigateforget requires   values  for four possible source locations    the passenger destination max node irrelevant maxget  taxi starting location
result distribution irrelevant navigate action  

qpickup requires     possible values    possible source locations    possible taxi
locations   passenger destination max node irrelevant maxget  

qget requires    possible values    source locations    destination locations    result
distribution irrelevance  

qnavigateforput requires   values  for four possible destination locations  

 the passenger source destination max node irrelevant maxput  taxi
location result distribution irrelevant navigate action  

qputdown requires     possible values     taxi locations    possible destination locations    passenger source max node irrelevant maxput  

qput requires   values   termination shielding  
gives total     distinct values  much less      values required
q learning  hence  see applying state abstractions  maxq
representation give much compact representation value function 
key thing note state abstractions  value function decomposed sum terms single term depends entire state mdp 
even though value function whole depend entire state mdp 
example  consider state described figures      there  showed
value state s  passenger r  destination b  taxi      
decomposed

v  root  s      v  north  s      c  navigate r   s    north   
c  get  s    navigate r     c  root  s    get 
state abstractions  see term right hand side depends
subset features 

v  north  s   constant
c  navigate r   s    north  depends taxi location passenger s source
location 

c  get  s   navigate r   depends source location 
c  root  s    get  depends passenger s source destination 
   

fimaxq hierarchical reinforcement learning

without maxq decomposition  features irrelevant  value function depends entire state 
prior knowledge required part programmer order identify
state abstractions  suces know qualitative constraints one step
reward functions  one step transition probabilities  termination predicates  goal
predicates  pseudo reward functions within maxq graph  specifically  max
node irrelevance leaf irrelevance conditions require simple analysis one step
transition function reward pseudo reward functions  opportunities apply
result distribution irrelevance condition found identifying  funnel  effects
result definitions termination conditions operators  similarly 
shielding termination conditions require analysis termination predicates
various subtasks  hence  applying five conditions introduce state abstractions
straightforward process  model one step transition reward functions
learned  abstraction conditions checked see satisfied 

    convergence maxq q state abstraction

shown state abstractions safely introduced maxq value
function decomposition five conditions described above  however  conditions guarantee value function fixed abstract hierarchical policy
represented they show recursively optimal policies represented 
show maxq q learning algorithm find recursively optimal policy
forced use state abstractions  goal section prove two
results   a  ordered recursively optimal policy abstract policy  and  hence 
represented using state abstractions   b  maxq q converge
policy applied maxq graph safe state abstractions 

lemma    let mdp full state maxq graph h abstract state maxq
graph  h   abstractions satisfy five conditions given above  let  
ordering actions maxq graph  following statements true 
unique ordered recursively optimal policy r defined   h     abstract policy  i e   depends relevant state variables node  see
definition     
c v functions  h   represent projected value function r 

proof  five abstraction lemmas tell us ordered recursively optimal policy
abstract  c v functions  h   represent value function  hence 
heart lemma first claim  last two forms abstraction  shielding
termination  place restrictions abstract policies  ignore
proof 
proof induction levels maxq graph  starting leaves 
base case  let us consider max node whose children primitive actions 
case  policies executed within children max node  hence variables
yi irrelevant node i  apply abstraction lemmas represent
value function policy node i not abstract policies  consequently  value
   

fidietterich

function optimal policy node represented  property

q i  s    a    q  i  s    a 
    
states s  s   s       s    
let us impose action ordering   compute optimal ordered policy  consider
two actions a  a    a    a     i e     prefers a     suppose
 tie  q function state s  values

q  i  s    a      q  i  s    a   
two actions maximize q state  optimal ordered
policy must choose a    states s   s       s    
established      q values same  hence  tie exist
a  a    hence  optimal ordered policy must make choice
states  hence  optimal ordered policy node abstract policy 
let us turn recursive case max node i  make inductive assumption
ordered recursively optimal policy abstract within descendent nodes consider
locally optimal policy node i  set state variables irrelevant
node i  corollary   tells us q  i  s    j     q  i  s    j   states s  s 
i s      s     similarly  set variables irrelevant result distribution
particular action j   lemma   tells us thing  hence  ordering
argument given above  ordered optimal policy node must abstract  induction 
proves lemma  q e d 
lemma  established combination mdp   abstract
maxq graph h   action ordering defines unique recursively optimal ordered abstract policy  ready prove maxq q converge policy 

theorem   let   hs  a  p  r  p  either episodic mdp deterministic

policies proper discounted infinite horizon mdp discount factor      let h
unabstracted maxq graph defined subtasks fm            mk g pseudo reward
functions r   s     let  h   state abstracted maxq graph defined applying state
abstractions node h five conditions given above  let x  i   s  
abstract ordered glie exploration policy node state whose decisions
depend  relevant  state variables node i  let r unique recursivelyoptimal hierarchical policy defined x     r    probability    algorithm
maxq q applied  h   converges r provided learning rates fft  i  satisfy
equation      one step rewards bounded 

proof  rather repeating entire proof maxq q  describe

must change state abstraction  last two forms state abstraction refer states
whose values inferred structure maxq graph  therefore
need represented all  since values updated maxq q 
ignore them  consider first three forms state abstraction turn 
begin considering primitive leaf nodes  let leaf node let set
state variables leaf irrelevant a  let s     x  y    s     x  y    two states
   

fimaxq hierarchical reinforcement learning

differ values   leaf irrelevance  probability transitions
p  s   js    a  p  s   js    a  need same  expected reward performing
states must same  maxq q visits abstract state x 
 know  value y  part state abstracted away  nonetheless 
draws sample according p  s  jx  y  a   receives reward r s  jx  y  a   updates
estimate v  a  x   line   maxq q   let pt  y  probability maxq q
visiting  x  y  given unabstracted part state x  line   maxq q
computing stochastic approximation
x

s   n y

write

x



pt  y pt  s    n jx  y  a r s  jx  y  a  

pt  y 

x

s   n

pt  s    n jx  y  a r s  jx  y  a  

according leaf irrelevance  inner sum value states
 s    x  call value r   x   gives
x



pt  y r   x  

equal r   x  distribution pt  y   hence  maxq q converges leaf
irrelevance abstractions 
let us turn two forms abstraction apply internal nodes  max node
irrelevance result distribution irrelevance  consider smdp defined node
abstracted maxq graph time maxq q  would ordinary smdp
transition probability function pt  x    n jx  a  reward function vt  a  x    r   x   
except maxq q draws samples state transitions  drawn according
distribution pt  s    n js  a  original state space  prove theorem  must
show drawing  s    n   according second distribution equivalent drawing
 x    n   according first distribution 
max node irrelevance  know abstract policies applied node
descendents  transition probability distribution factors

p  s    n js  a    p  y  jx  y  a p  x    n jx  a  
exploration policy abstract policy  pt  s    n js  a  factors way 
means yi components state cannot affect xi components  hence 
sampling pt  s    n js  a  discarding yi values gives samples pt  x    n jx  a  
therefore  maxq q converge max node irrelevance abstractions 
finally  consider result distribution irrelevance  let j child node i  suppose
yj set state variables irrelevant result distribution j  
smdp node wishes draw sample pt  x    n jx  j     know 
current value y  irrelevant part current state  however 
matter  result distribution irrelevance means possible values y 
pt  x    y   n jx  y  j   same  hence  maxq q converge result distribution
irrelevance abstractions 
   

fidietterich

three cases  maxq q converge locally optimal ordered policy
node maxq graph  lemma     produces locally optimal ordered
policy unabstracted smdp node i  hence  induction  maxq q converge
unique ordered recursively optimal policy r defined maxq q h   mdp  
ordered exploration policy x   q e d 

    hierarchical credit assignment problem

still situations would introduce state abstractions
five properties described permit them  consider following
modification taxi problem  suppose taxi fuel tank time
taxi moves one square  costs one unit fuel  taxi runs fuel
delivering passenger destination  receives reward      trial
ends  fortunately  filling station taxi execute fillup action fill
fuel tank 
solve modified problem using maxq hierarchy  introduce another
subtask  refuel  goal moving taxi filling station filling
tank  maxrefuel child maxroot  invokes navigate t   with bound
location filling station  move taxi filling station 
introduction fuel possibility might run fuel means
must include current amount fuel feature representing every c value
 for internal nodes  v value  for leaf nodes  throughout maxq graph 
unfortunate  intuition tells us amount fuel uence
decisions inside navigate t  subtask  is  either taxi enough
fuel reach target  in case  chosen navigation actions depend
fuel   else taxi enough fuel  hence  fail reach regardless
navigation actions taken  words  navigate t  subtask
need worry amount fuel  even enough fuel 
action navigate t  take get fuel  instead  top level subtasks
monitoring amount fuel deciding whether go refuel  go pick
passenger  go deliver passenger 
given intuition  natural try abstracting away  amount remaining
fuel  within navigate t  subtask  however  doesn t work  taxi
runs fuel     reward given  qnorth  qsouth  qeast  qwest nodes
cannot  explain  reward received that is  consistent way
setting c tables predict negative reward occur  c
values ignore amount fuel tank  stated formally  diculty
max node irrelevance condition satisfied one step reward function
r s js  a  actions depends amount fuel 
call hierarchical credit assignment problem  fundamental issue
maxq decomposition information rewards stored leaf nodes
hierarchy  would separate basic rewards received navigation
 i e      action  reward received exhausting fuel        make
reward leaves depend location taxi  max node irrelevance
condition satisfied 
   

fimaxq hierarchical reinforcement learning

one way programmer manually decompose reward function

indicate nodes hierarchy  receive  reward  let r s  js  a   
p
 
 
r i  js  a  decomposition reward function  r i  js  a  specifies
part reward must handled max node i  modified taxi problem 
example  decompose reward leaf nodes receive original
penalties  out of fuel rewards must handled maxroot  lines      
maxq q algorithm easily modified include r i  s  js  a  
domains  believe easy designer hierarchy decompose
reward function  straightforward problems studied 
however  interesting problem future research develop algorithm
solve hierarchical credit assignment problem autonomously 

   non hierarchical execution maxq hierarchy

point paper  focused exclusively representing learning
hierarchical policies  however  often optimal policy mdp strictly hierarchical  kaelbling        first introduced idea deriving non hierarchical policy
value function hierarchical policy  section  exploit maxq decomposition
generalize ideas apply recursively levels hierarchy 
describe two methods non hierarchical execution 
first method based dynamic programming algorithm known policy
iteration  policy iteration algorithm starts initial policy     repeats
following two steps policy converges  policy evaluation step  computes
value function v k current policy k   then  policy improvement step 
computes new policy  k   according rule

k   s     argmax


x

s 

p  s  js  a  r s  js  a    v k  s     

    

howard        proved k optimal policy  k   guaranteed
improvement  note order apply method  need know transition
probability distribution p  s  js  a  reward function r s  js  a  
know p  s  js  a  r s  js  a   use maxq representation value
function perform one step policy iteration  start hierarchical policy
represent value function using maxq hierarchy  e g   could learned via
maxq q   then  perform one step policy improvement applying equation     
using v     s     computed maxq hierarchy  compute v  s    

corollary   let g  s    argmaxa ps  p  s js  a  r s  js  a    v     s    v     s 
value function computed maxq hierarchy primitive action  then 
optimal policy  g strictly better least one state  

proof  direct consequence howard s policy improvement theorem  q e d 
unfortunately  can t iterate policy improvement process  new policy 

g unlikely hierarchical policy  i e   unlikely representable
   

fidietterich

table    procedure executing one step greedy policy 
procedure executehgpolicy s 
 
repeat
 
let hv     s   ai    evaluatemaxnode    s 
 
 

execute primitive action
let resulting state
end    executehgpolicy

terms local policies node maxq graph   nonetheless  one step policy
improvement give significant improvements 
approach non hierarchical execution ignores internal structure maxq
graph  effect  maxq hierarchy viewed way represent v  any
representation would give one step improved policy g  
second approach non hierarchical execution borrows idea q learning 
one great beauties q representation value functions compute
one step policy improvement without knowing p  s  js  a   simply taking new policy
g  s     argmaxa q s  a   gives us one step greedy policy
computed using one step lookahead  maxq decomposition  perform
policy improvement steps levels hierarchy 
already defined function need  table   presented function
evaluatemaxnode  which  given current state s  conducts search along paths
given max node leaves maxq graph finds path
best value  i e   maximum sum c values along path  plus v value
leaf   equivalent computing best action greedily level
maxq graph  addition  evaluatemaxnode returns primitive action end
best path  action would first primitive action executed
learned hierarchical policy executed starting current state s  second method
non hierarchical execution maxq graph call evaluatemaxnode
state  execute primitive action returned  pseudo code shown
table   
call policy computed executehgpolicy hierarchical greedy policy 
denote hg   superscript   indicates computing greedy
action time step  following theorem shows give better policy
original  hierarchical policy 

theorem   let g maxq graph representing value function hierarchical policy

 i e   terms c  i  s  j    computed i  s  j    let v hg     s  value
computed executehgpolicy  line     let hg resulting policy  define
v hg value function hg  states s  case
v  s  v hg     s  v hg  s  
    

proof   sketch  left inequality equation      satisfied construction line  
evaluatemaxnode  see this  consider original hierarchical policy   
   

fimaxq hierarchical reinforcement learning

viewed choosing  path  maxq graph running root one
leaf nodes  v     s  sum c values along chosen path  plus
v value leaf node   contrast  evaluatemaxnode performs traversal
paths maxq graph finds best path  is  path largest
sum c  and leaf v   values  hence  v hg     s  must least large v     s  
establish right inequality  note construction v hg     s  value function
policy  call hg   chooses one action greedily level maxq graph
 recursively   follows thereafter  consequence fact line
  evaluatemaxnode c right hand side  c represents cost
 completing  subroutine following   following other  greedier  policy 
 in table    c written ct    however  execute executehgpolicy  and
hence  execute hg    opportunity improve upon hg time step 
hence  v hg     s  underestimate actual value hg   q e d 
note theorem works one direction  says find state
v hg     s    v  s   greedy policy  hg   strictly better  
however  could optimal policy yet structure maxq
graph prevents us considering action  either primitive composite  would
improve   hence  unlike policy improvement theorem howard  where primitive
actions always eligible chosen   guarantee suboptimal 
hierarchically greedy policy strict improvement 
contrast  perform one step policy improvement discussed start
section  corollary   guarantees improve policy  see
general  neither two methods non hierarchical execution always better
other  nonetheless  first method operates level individual primitive
actions  able produce large improvements policy  contrast 
hierarchical greedy method obtain large improvements policy changing
actions  i e   subroutines  chosen near root hierarchy  hence  general 
hierarchical greedy execution probably better method   of course  value functions
methods could computed  one better estimated value could
executed  
sutton  et al         simultaneously developed closely related method nonhierarchical execution macros  method equivalent executehgpolicy
special case maxq hierarchy one level subtasks  interesting
aspect executehgpolicy permits greedy improvements levels
tree uence action chosen 
care must taken applying theorem   maxq hierarchy whose c values
learned via maxq q  online algorithm  maxq q correctly learned values states nodes maxq graph  example 
taxi problem  value c  put  s  qputdown  learned well except
four special locations r  g  b  y  put subtask cannot
executed passenger taxi  usually means get
completed  taxi passenger s source location  exploration  children put tried states  putdown usually fail  and receive negative
reward   whereas navigate eventually succeed  perhaps lengthy exploration 
   

fidietterich

take taxi destination location  all states updating  values
c  put  s  navigate t   learned states along path
passenger s destination  c values putdown action learned
passenger s source destination locations  hence  train maxq representation using hierarchical execution  as maxq q   switch hierarchically greedy
execution  results quite bad  particular  need introduce hierarchicallygreedy execution early enough exploration policy still actively exploring   in
theory  glie exploration policy never ceases explore  practice  want find
good policy quickly  asymptotically  
course alternative would use hierarchically greedy execution
beginning learning  however  remember higher nodes maxq hierarchy
need obtain samples p  s    n js  a  child action a  hierarchical greedy
execution interrupts child reached terminal state  i e   state
along way  another subtask appears better evaluatemaxnode   samples
cannot obtained  hence  important begin purely hierarchical execution
training  make transition greedy execution point 
approach taken implement maxq q way
specify number primitive actions l taken hierarchically hierarchical execution  interrupted  control returns top level  where new action
chosen greedily   start l set large  execution completely
hierarchical when child action invoked  committed execute action
terminates  however  gradually  reduce l becomes    point
hierarchical greedy execution  time reaches   time
boltzmann exploration cools temperature      which exploration effectively
halted   experimental results show  generally gives excellent results
little added exploration cost 

   experimental evaluation maxq method
performed series experiments maxq method three goals
mind   a  understand expressive power value function decomposition   b 
characterize behavior maxq q learning algorithm   c  assess relative
importance temporal abstraction  state abstraction  non hierarchical execution 
section  describe experiments present results 

    fickle taxi task
first experiments performed modified version taxi task  version
incorporates two changes task described section      first  four
navigation actions noisy  probability     moves intended direction 
probability     instead moves right  of intended direction 
probability     moves left  purpose change create realistic
dicult challenge learning algorithms  second change
taxi picked passenger moved one square away passenger s source
location  passenger changes destination location probability     
   

fimaxq hierarchical reinforcement learning

purpose change create situation optimal policy hierarchical
policy effectiveness non hierarchical execution measured 
compared four different configurations learning algorithm   a  q learning 
 b  maxq q learning without form state abstraction   c  maxq q learning
state abstraction   d  maxq q learning state abstraction greedy execution 
configurations controlled many parameters  include following   a 
initial values q c functions   b  learning rate  we employed fixed
learning rate    c  cooling schedule boltzmann exploration  the glie policy
employed    d  non hierarchical execution  schedule decreasing l  number
steps consecutive hierarchical execution  optimized settings separately
configuration goal matching exceeding  with primitive training
actions possible  best policy could code hand  boltzmann exploration 
established initial temperature cooling rate  separate temperature
maintained max node maxq graph  temperature reduced
multiplying cooling rate time subtask terminates goal state 
process optimizing parameter settings algorithm time consuming 
q learning maxq q  critical parameter schedule
cooling temperature boltzmann exploration  cooled rapidly 
algorithms converge suboptimal policy  case  tested nine different
cooling rates  choose different cooling rates various subtasks  started
using fixed policies  e g   either random hand coded  subtasks except subtasks
closest leaves  then  chosen schedules subtasks  allowed
parent tasks learn policies tuned cooling rates  on  one
nice effect method cooling temperature subtask terminates
naturally causes subtasks higher maxq graph cool slowly  meant
good results could often obtained using cooling rate max
nodes 
choice learning rate easier  since determined primarily degree
stochasticity environment  tested three four different rates
configuration  initial values q c functions set based knowledge
problems no experiments required 
took care tuning parameters experiments one would
normally take real application  wanted ensure method
compared best possible conditions  general form results  particularly
speed learning  wide ranges cooling rate learning rate
parameter settings 
following parameters selected based tuning experiments  q
learning  initial q values       states  learning rate       boltzmann exploration
initial temperature    cooling rate          we use initial values
end       signature  debugging detect weight modified  
maxq q learning without state abstraction  used initial values        learning rate       boltzmann exploration initial temperature    cooling
rates        maxroot maxput         maxget         maxnavigate 
   

fidietterich

   
maxq abstract

mean cumulative reward

 
maxq
abstract 
greedy

    

maxq
abstract

    

flat q

    

    

     
 

     

     

     
     
      
primitive actions

      

      

figure    comparison performance hierarchical maxq q learning  without state abstractions  state abstractions  state abstractions combined
hierarchical greedy evaluation  q learning 
maxq q learning state abstraction  used initial values        learning
rate       boltzmann exploration initial temperature    cooling rates
       maxroot         maxput         maxget         maxnavigate 
maxq q learning non hierarchical execution  used settings
state abstraction  addition  initialized l     decreased   
trial reached       trials  execution completely greedy 
figure   shows averaged results     training runs  training run involves
performing repeated trials convergence  different trials execute different
numbers primitive actions  plotted number primitive actions
horizontal axis rather number trials 
first thing note forms maxq learning better initial performance
q learning  constraints introduced maxq hierarchy 
example  agent executing navigate subtask  never attempt pickup
putdown passenger  actions available navigate  similarly 
agent never attempt putdown passenger first picked passenger
 and vice versa  termination conditions get put subtasks 
second thing notice without state abstractions  maxq q learning actually takes longer converge  flat q curve crosses maxq no abstraction
   

fimaxq hierarchical reinforcement learning

curve  shows without state abstraction  cost learning huge number
parameters maxq representation really worth benefits  suspect
consequence model free nature maxq q algorithm  maxq decomposition represents information redundantly  example  cost performing
put subtask computed c  root  s  get  v  put  s   model based
algorithm could compute learned model  maxq q must learn
separately experience 
third thing notice state abstractions  maxq q converges
quickly hierarchically optimal policy  seen clearly figure   
focuses range reward values neighborhood optimal policy 
see maxq abstractions attains hierarchically optimal policy
approximately        steps  whereas q learning requires roughly twice long reach
level  however  q learning  course  continue onward reach optimal
performance  whereas maxq hierarchy  best hierarchical policy slow
respond  fickle  behavior passenger he she changes destination 
last thing notice greedy execution  maxq policy able
attain optimal performance  execution becomes  more greedy  
temporary drop performance  maxq q must learn c values new regions
state space visited recursively optimal policy  despite
drop performance  greedy maxq q recovers rapidly reaches hierarchically optimal
performance faster purely hierarchical maxq q learning  hence  added
cost in terms exploration for introducing greedy execution 
experiment presents evidence favor three claims  first  hierarchical reinforcement learning much faster q learning  second  state abstraction
required maxq q learning good performance  third  non hierarchical
execution produce significant improvements performance little added
exploration cost 

    kaelbling s hdg method
second task consider simple maze task introduced leslie kaelbling
       shown figure     trial task  agent starts randomlychosen state must move randomly chosen goal state using usual north  south 
east  west operators  we employed deterministic operators   small cost
move  agent must minimize undiscounted sum costs 
goal state     different locations  actually    
different mdps  kaelbling s hdg method starts choosing arbitrary set landmark
states defining voronoi partition state space based manhattan distances
landmarks  i e   two states belong voronoi cell iff
nearest landmark   method defines one subtask landmark l  subtask
move state current voronoi cell neighboring voronoi cell
landmark l  optimal policies subtasks computed 
hdg policies subtasks  solve abstract markov decision
problem moving landmark state landmark state using subtask
solutions macro actions  subroutines   computes value function mdp 
   

fidietterich

  

maxq abstract greedy

mean cumulative reward

 

optimal policy
flat q
hier optimal policy

 
maxq abstract

maxq abstract

  

   

   
 

     

      

      
      
primitive actions

      

      

figure    close up view previous figure  figure shows two horizontal lines
indicating optimal performance hierarchically optimal performance
domain  make figure readable  applied     step moving
average data points  which average     runs  
finally  possible destination location g within voronoi cell landmark l 
hdg method computes optimal policy getting l g 
combining subtasks  hdg method construct good approximation
optimal policy follows  addition value functions discussed above 
agent maintains two functions  nl s   name landmark nearest state s 
n  l   list landmarks cells immediate neighbors cell l 
combining these  agent build list state current landmark
landmarks neighboring cells  landmark  agent computes sum
three terms 
 t   expected cost reaching landmark 
 t   expected cost moving landmark landmark goal cell 
 t   expected cost moving goal cell landmark goal state 
note terms  t    t   exact estimates  term  t   computed using
landmark subtasks subroutines  means corresponding path must pass
intermediate landmark states rather going directly goal landmark 
   

fimaxq hierarchical reinforcement learning

  
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

  

figure     kaelbling s    by    navigation task  circled state landmark state 
heavy lines show boundaries voronoi cells  episode 
start state goal state chosen random  figure  start state
shown black square  goal state shown black hexagon 

hence  term  t   typically overestimate required distance   also note  t  
choices intermediate landmarks  need explicitly
included computation best action agent enters cell containing
goal  
given information  agent chooses move toward best landmarks
 unless agent already goal voronoi cell  case agent moves toward
goal state   example  figure     term  t   cost reaching landmark
row    column       term  t   cost getting row    column  
landmark row   column    by going one landmark another   case 
best landmark to landmark path go directly row   column   row   column
   hence  term  t      term  t   cost getting row   column   goal 
   sum                 comparison  optimal path
length   
kaelbling s experiments  employed variation q learning learn terms  t  
 t    computed  t   regular intervals via floyd warshall all sources
shortest paths algorithm 
figure    shows maxq approach solving problem  overall task root 
takes one argument g  specifies goal cell  three subtasks 
   

fidietterich

maxroot g 
gl nl g 

qgotogoallmk gl 

qgotogoal g 

maxgotogoallmk gl 

qgotolmk l gl 

maxgotolmk l 

qnorthlmk l 

qsouthlmk l 

maxgotogoal g 

qeastlmk l 

north

qwestlmk l 

south

qnorthg g 

east

qsouthg g 

qeastg g 

qwestg g 

west

figure     maxq graph hdg navigation task 

gotogoallmk  go landmark nearest goal location  termination
predicate subtask true agent reaches landmark nearest
goal  goal predicate termination predicate 

gotolmk l   go landmark l  termination predicate true either  a 

agent reaches landmark l  b  agent outside region defined
voronoi cell l neighboring voronoi cells  n  l   goal predicate
subtask true condition  a  

gotogoal g   go goal location g  termination predicate subtask

true either agent goal location agent outside voronoi
cell nl g  contains g  goal predicate subtask true agent
goal location 
   

fimaxq hierarchical reinforcement learning

maxq decomposition essentially kaelbling s method  somewhat
redundant  consider state agent inside voronoi cell goal
g  states  hdg decomposes value function three terms  t     t     t   
similarly  maxq decomposes three terms 
v  gotolmk l   s  a  cost getting landmark l  represented sum
v  a  s  c  gotolmk l   s  a  
c  gotogoallmk gl   s  maxgotolmk l   cost getting landmark l
landmark gl nearest goal 
c  root  s  gotogoallmk gl   cost getting goal location reaching gl
 i e   cost completing root task reaching gl  
agent inside goal voronoi cell  hdg maxq store
essentially information  hdg stores q gotogoal g   s  a   maxq breaks
two terms  c  gotogoal g   s  a  v  a  s  sums two quantities
compute q value 
note maxq decomposition stores information twice specifically 
cost getting goal landmark gl goal stored c  root  s  gotogoallmk gl  
c  gotogoal g   s  a    v  a  s  
let us compare amount memory required q learning  hdg  maxq 
    locations    possible actions      possible goal states  q learning
must store        values 
compute quantity  t    hdg must store   q values  for four actions 
state respect landmark landmarks n  nl s    gives
total       values must stored 
compute quantity  t    hdg must store  landmark  information
shortest path every landmark     landmarks  consider landmark
row    column      neighboring landmarks constitute five macro actions
agent perform move another landmark  nearest landmark
goal cell could    landmarks  gives total    q values
must stored  similar computations    landmarks give total     values
must stored 
finally  compute quantity  t    hdg must store information  square inside
voronoi cell  get squares inside voronoi
cell  requires       values 
hence  grand total hdg        huge savings q learning 
let s consider maxq hierarchy without state abstractions 
v  a  s   expected reward primitive action state 
    states   primitive actions  requires     values  however 
reward constant       apply leaf irrelevance store single value 
c  gotolmk l   s  a   one four primitive actions  requires
amount space  t   kaelbling s representation indeed  combined
v  a  s   represents exactly information  t    requires       values 
state abstractions applied 
   

fidietterich

c  gotogoallmk gl   s  gotolmk l    cost completing gotogoallmk

task going landmark l  primitive actions deterministic 
gotolmk l  always terminate location l  hence  need store
pair l gl  exactly kaelbling s quantity  t   
requires     values  however  primitive actions stochastic as
kaelbling s original paper then must store value possible
terminal state gotolmk action  actions could terminate
target landmark l one states bordering set voronoi cells
neighbors cell l  requires       values  kaelbling stores
values  t    effectively making assumption gotolmk l 
never fail reach landmark l  approximation introduce
maxq representation choice state abstraction node 

c  gotogoal  s  a   cost completing gotogoal task executing one
primitive actions a  quantity  t   hdg representation 
requires amount space        values 

c  root  s  gotogoallmk   cost reaching goal reached

landmark nearest goal  maxq must represent combinations
goal landmarks goals  requires     values  note values
values c  gotogoal g   s  a    v  a  s  primitive actions 
means maxq representation stores information twice  whereas
hdg representation stores  as term  t    

c  root  s  gotogoal   cost completing root task exe 

cuted gotogoal task  primitive action deterministic  always zero 
gotogoal reached goal  hence  apply termination
condition store values all  however  primitive actions stochastic  must store value possible state borders voronoi cell
contains goal  requires    different values  again  kaelbling s hdg
representation value function  ignoring probability gotogoal
terminate non goal state  maxq exact representation value
function  ignore possibility   incorrectly  apply termination
condition case  maxq representation becomes function approximation 

stochastic case  without state abstractions  maxq representation requires
       values  safe state abstractions  requires        values  approximations employed kaelbling  or equivalently  primitive actions deterministic  
maxq representation state abstractions requires       values  numbers
summarized table    see that  unsafe state abstractions  maxq
representation requires slightly space hdg representation  because
redundancy storing c  root  s  gotogoallmk  
example shows hdg task  start fully general formulation provided maxq impose assumptions obtain method similar
hdg  maxq formulation guarantees value function hierarchical
policy represented exactly  assumptions introduce approximations
   

fimaxq hierarchical reinforcement learning

table    comparison number values must stored represent value
function using hdg maxq methods 
hdg maxq
hdg maxq maxq maxq
item item
values abs safe abs unsafe abs
v  a  s 
 
   
 
 
 t   c  gotolmk l   s  a 
           
     
     
 t   c  gotogoallmk  s  gotolmk l  
         
     
   
 t   c  gotogoal g   s  a 
           
     
     
c  root  s  gotogoallmk 
 
   
   
   
c  root  s  gotogoal 
 
  
  
 
total number values required
                   
     
value function representation  might useful general design methodology
building application specific hierarchical representations  long term goal develop
methods new application require inventing new set techniques  instead  off the shelf tools  e g   based maxq  could specialized imposing
assumptions state abstractions produce ecient special purpose systems 
one important contributions hdg method introduced
form non hierarchical execution  soon agent crosses one voronoi cell
another  current subtask reaching landmark cell  interrupted  
agent recomputes  current target landmark   effect  until
reaches goal voronoi cell   agent always aiming landmark outside
current voronoi cell  hence  although agent  aims for  sequence landmark states 
typically visit many states way goal  states provide
convenient set intermediate targets  taking  shortcuts   hdg compensates
fact that  general  overestimated cost getting goal 
computed value function based policy agent goes one landmark
another 
effect obtained hierarchical greedy execution maxq graph  which
directly inspired hdg method   note storing nl  nearest landmark 
function  kaelbing s hdg method detect eciently current subtask
interrupted  technique works navigation problems space
distance metric  contrast  executehgpolicy performs kind  polling  
checks primitive action whether interrupt current subroutine
invoke new one  important goal future research maxq find general
purpose mechanism avoiding unnecessary  polling  that is  mechanism
discover eciently evaluable interrupt conditions 
figure    shows results experiments hdg using maxq q learning algorithm  employed following parameters  flat q learning  initial values
       learning rate      initial temperature     cooling rate        
maxq q without state abstractions  initial values          learning rate      initial
   

fidietterich

 
flat q

maxq  
abstract

   

mean cumulative reward

   

maxq abstract

   
   
    
    
    
 

      

      

      
      
primitive actions

 e   

   e   

   e   

figure     comparison flat q learning maxq q learning without state
abstraction   average     runs  
temperature     cooling rates        maxroot         maxgotogoallmk 
       maxgotogoal         maxgotolmk  maxq q state abstractions 
initial values          learning rate      initial temperature     cooling rates
       maxroot         maxgotogoal         maxgotogoallmk        
maxgotolmk  hierarchical greedy execution introduced starting      primitive actions per trial  reducing every trial   actions       trials 
execution completely greedy 
figure confirms observations made experiments fickle taxi task 
without state abstractions  maxq q converges much slowly q learning 
state abstractions  converges roughly three times fast  figure    shows close up
view figure    allows us compare differences final levels performance
methods  here  see maxq q state abstractions able
reach quality hand coded hierarchical policy presumably even exploration
would required achieve this  whereas state abstractions  maxq q able
slightly better hand coded policy  hierarchical greedy execution  maxq q
able reach goal using one fewer action  average so approaches
performance best hierarchical greedy policy  as computed value iteration   notice
however  best performance obtained hierarchical greedy execution
best recursively optimal policy cannot match optimal performance  hence  flat q
   

fimaxq hierarchical reinforcement learning

  
optimal policy

mean cumulative reward

hierarchical greedy optimal policy
maxq abstract   greedy
maxq   abstract

  

   

flat q

hierarchical hand coded policy

maxq abstract

   

   

 

      

      

      
      
primitive actions

 e   

   e   

   e   

figure     expanded view comparing flat q learning maxq q learning
without state abstraction without hierarchical greedy execution 
 average     runs  
learning achieves policy reaches goal state  average  one fewer
primitive action  finally notice taxi domain  added exploration
cost shifting greedy execution 
kaelbling s hdg work recently extended generalized moore  baird
kaelbling        sparse mdp overall task get given
start state desired goal state  key success approach
landmark subtask guaranteed terminate single resulting state  makes
possible identify sequence good intermediate landmark states assemble
policy visits sequence  moore  baird kaelbling show construct
hierarchy landmarks  the  airport  hierarchy  makes planning process ecient 
note subtask terminate single state  as general mdps  
airport method would work  would combinatorial explosion
potential intermediate states would need considered 

    parr russell  hierarchies abstract machines

     b  dissertation work  ron parr considered approach hierarchical reinforcement learning programmer encodes prior knowledge form hierarchy
finite state controllers called ham  hierarchy abstract machines   hierarchy
   

fidietterich

intersection
vertical hallway
horizontal hallway
goal

figure     parr s maze problem  on left   start state upper left corner 
states lower right hand room terminal states  smaller diagram
right shows hallway intersection structure maze 
executed using procedure call and return discipline  provides partial policy
task  policy partial machine include non deterministic  choice 
machine states  machine lists several options action specify
one chosen  programmer puts  choice  states point
he she know action performed  given partial policy  parr s
goal find best policy making choices choice states  words 
goal learn hierarchical value function v  hs  mi   state  of external
environment  contains internal state hierarchy  i e   contents
procedure call stack values current machine states machines
appearing stack   key observation necessary learn value
function choice states hs  mi  parr s algorithm learn decomposition value
function  instead    attens  hierarchy create new markov decision problem
choice states hs  mi  hence  hierarchical primarily sense programmer
structures prior knowledge hierarchically  advantage parr s method
find optimal hierarchical policy subject constraints provided programmer 
disadvantage method cannot executed  non hierarchically  produce
better policy 
parr illustrated work using maze shown figure     maze large scale
structure  as series hallways intersections   small scale structure  a series
obstacles must avoided order move hallways intersections  
   

fimaxq hierarchical reinforcement learning

trial  agent starts top left corner  must move state
bottom right corner room  agent usual four primitive actions  north  south 
east  west  actions stochastic  probability      succeed 
probability     action move  left  probability     action
move  right  instead  e g   north action move east probability    
west probability       action would collide wall obstacle 
effect 
maze structured series  rooms   containing    by    block states
 and various obstacles   rooms parts  hallways   connected
two rooms opposite sides  rooms  intersections   two
hallways meet 
test representational power maxq hierarchy  want see well
represent prior knowledge parr able represent using ham  begin
describing parr s ham maze task  present maxq hierarchy
captures much prior knowledge  
parr s top level machine  mroot  consists loop single choice state
chooses among four possible child machines  mgo east   mgo south   mgo west  
mgo north   loop terminates agent reaches goal state  mroot
invoke particular machine hallway specified direction  hence 
start state  consider mgo south  mgo east  
mgo d  machine begins executing agent intersection  first
thing tries exit intersection hallway specified direction d 
attempts traverse hallway reaches another intersection  first
invoking mexitintersection d  machine  machine returns  invokes
mexithallway d  machine  machine returns  mgo returns 
mexitintersection mexithallway machines identical except termination conditions  machines consist loop one choice state chooses among
four possible subroutines  simplify description  suppose mgo east  chosen mexitintersection east   four possible subroutines msniff  east  north  
msniff  east  south   mback east  north   mback east  south  
msniff  d  p  machine always moves direction encounters wall  either
part obstacle part walls maze   moves perpendicular
direction p reaches end wall  wall  end  two ways  either
agent trapped corner walls directions p else
longer wall direction d  first case  msniff machine terminates  second
case  resumes moving direction d 
mback d  p  machine moves one step backwards  in direction opposite d 
moves five steps direction p  moves may may succeed 
actions stochastic may walls blocking way  actions carried
case  mback machine returns 
msniff mback machines terminate reach end hall
end intersection 
   author thanks ron parr providing details ham task 

   

fidietterich

finite state controllers define highly constrained partial policy  mback 
msniff  mgo machines contain choice states all  choice points
mroot  must choose direction move  mexitintersection
mexithall  must decide call msniff  call mback 
 perpendicular  direction tell machines try cannot move forward 

maxroot

go d 
r room
maxgo d r 

qexitinter d r 

qexithall d r 

maxexitinter d r 

maxexithall d r 

qsniffei d p 

qbackei d p 

qsniffeh d p 
x x

qbackeh d p 
x x
y y

y y
maxsniff d p 

maxback d p x y 

qfollowwall d p 

qtowall d 

qbackone d 

qperpthree p 

maxfollowwall d p 

maxtowall d 

maxbackone d 

maxperpthree p 

d p

d d

qmovefw d 

d inv d 

qmovetw d 

qmovebo d 

d p
qmovep  d 

maxmove d 

figure     maxq graph parr s maze task 
figure    shows maxq graph encodes similar set constraints policy 
subtasks defined follows 
   

fimaxq hierarchical reinforcement learning

root  exactly mroot machine  must choose direction

invoke go  terminates agent enters terminal state 
goal condition  of course  

go d  r    go direction leaving room r   parameter r bound identi 

fication number corresponding current    by     room  agent
located  go terminates agent enters room end hallway
direction leaves desired hallway  e g   wrong direction  
goal condition go satisfied agent reaches desired intersection 

exitinter d  r   terminates agent exited room r  goal condition
agent exit room r direction d 

exithall d  r   terminates agent exited current hall  into

intersection   goal condition agent entered desired intersection
direction d 

sniff  d  r   encodes subtask equivalent msniff machine  however 

sniff must two child subtasks  towall followwall  simply internal
states msniff  necessary  subtask maxq framework cannot

contain internal state  whereas finite state controller ham representation
contain many internal states necessary  particular  one state
moving forward another state following wall sideways 

towall d   equivalent one part msniff  terminates

wall  front  agent direction d  goal condition
termination condition 

followwall d  p   equivalent part msniff  moves direction

p wall direction ends  or stuck corner walls
directions p   goal condition termination condition 

back d  p  x  y   attempts encode information mback machine 

case maxq hierarchy cannot capture information 

mback simply executes sequence   primitive actions  one step back  five steps
direction p   this  mback must   internal states  maxq
allow  instead  back subtask subgoal moving agent least

one square backwards least   squares direction p  order determine
whether achieved subgoal  must remember x position
started execute  bound parameters back  back terminates
achieves desired change position runs walls prevent
achieving subgoal  goal condition termination condition 

backone d  x  y   moves agent one step backwards  in direction opposite

d  needs starting x position order tell succeeded 
terminates moved least one unit direction wall
direction  goal condition termination condition 
   

fidietterich

perpthree p  x  y   moves agent three steps direction p  needs

starting x positions order tell succeeded  terminates
moved least three units direction p wall direction 
goal condition termination condition 

move d    parameterized primitive  action  executes one primitive move
direction terminates immediately 

this  see three major differences maxq representation ham representation  first  ham finite state controller contain
internal states  convert maxq subtask graph  must make separate
subtask internal state ham  second  ham terminate based
 amount effort   e g   performing   actions   whereas maxq subtask must terminate
based change state world  impossible define maxq subtask performs k steps terminate regardless effects steps  i e  
without adding kind  counter  state mdp   third  dicult
formulate termination conditions maxq subtasks ham machines 
example  ham  necessary specify mexithallway machine terminates entered different intersection one mgo executed 
however  important maxq method  maxq  subtask learns
value function policy independent parent tasks  example  without
requirement enter different intersection  learning algorithms maxq
always prefer maxexithall take one step backward return room
go action started  because much easier terminal state reach  
problem arise ham approach  policy learned subtask
depends whole   attened  hierarchy machines  returning state
go action started help solve overall problem reaching goal state
lower right corner 
construct maxq graph problem  introduced three programming
tricks   a  binding parameters aspects current state  in order serve kind
 local memory  subtask began executing    b  parameterized
primitive action  in order able pass parameter value specifies primitive
action perform    c  employing  inheritance termination conditions  that is 
subtask maxq graph  but others paper  inherits termination
conditions ancestor tasks  hence  agent middle executing towall
action leaves intersection  towall subroutine terminates exitinter
termination condition satisfied  behavior similar standard behavior
maxq  ordinarily  ancestor task terminates  descendent tasks forced
return without updating c values  inheritance termination conditions 
hand  descendent tasks forced terminate  updating c
values  words  termination condition child task logical disjuntion
termination conditions ancestors  plus termination condition  
inheritance made easier write maxq graph  parents need
pass children information necessary children define
complete termination goal predicates 
   

fimaxq hierarchical reinforcement learning

essentially opportunities state abstraction task 
irrelevant features state  opportunities apply shielding
termination properties  however  particular  exithall d  guaranteed cause
parent task  maxgo d   terminate  require stored c values 
many states subtasks terminated  e g   go east  state
wall east side room   c values need stored 
nonetheless  even applying state elimination conditions  maxq representation task requires much space representation  exact
computation dicult  applying maxq q learning  maxq representation
required        values  whereas q learning requires fewer        values  parr
states method requires       values 
test relative effectiveness maxq representation  compare maxq q
learning q learning  large negative values states
acquire  particularly early phases learning   unable get boltzmann
exploration work well one bad experience would cause action receive
low q value  would never tried again  hence  experimented
 greedy exploration counter based exploration   greedy exploration policy
ordered  abstract glie policy random action chosen probability  
gradually decreased time  counter based exploration policy keeps track
many times action executed state s  choose action state
s  selects action executed fewest times actions
executed times  switches greedy execution  hence  genuine glie
policy  parr employed counter based exploration policies experiments task 
domains  conducted several experimental runs  e g   testing boltzmann   greedy  counter based exploration  determine best parameters
algorithm  flat q learning  chose following parameters  learning rate       greedy exploration initial value      decreased       successful
execution max node  initial q values           maxq q learning  chose
following parameters  counter based exploration       learning rate equal
reciprocal number times action performed  initial values c
values selected carefully provide underestimates true c values  example 
initial values qexitinter          worst case  completing
exitinter task  takes    steps complete subsequent exithall task hence 
complete go parent task  performance quite sensitive initial c values 
potential drawback maxq approach 
figure    plots results  see maxq q learning converges   
times faster flat q learning  know whether maxq q converged
recursively optimal policy  comparison  show performance hierarchical
policy coded hand  hand coded policy  used knowledge contextual
information choose operators  policy surely better best recursively
optimal policy  hamq learning converge policy equal slightly better
hand coded policy 
experiment demonstrates maxq representation capture most but
all of prior knowledge represented hamq hierarchy 
   

fidietterich

    
    

mean reward per trial

    

hand coded hierarchical policy

    
    
    

maxq q learning

flat q learning

    
    
    
 

 e   

 e   

 e   
primitive steps

 e   

 e   

 e   

figure     comparison flat q learning maxq q learning parr maze task 
shows maxq representation requires much care design goal
conditions subtasks 

    domains
addition three domains discussed above  developed maxq graphs
singh s          ag task   treasure hunter task described tadepalli dietterich
        dayan hinton s        feudal q learning task  tasks
easily naturally placed maxq framework indeed  fit easily
parr russell maze task 
maxq able exactly duplicate singh s work decomposition value
function while using exactly amount space represent value function 
maxq duplicate results tadepalli dietterich however 
maxq explanation based method  considerably slower requires substantially space represent value function 
feudal q task  maxq able give better performance feudal q learning 
reason feudal q learning  subroutine makes decisions using q
function learned level hierarchy that is  without information
estimated costs actions descendents  contrast  maxq value function
decomposition permits max node make decisions based sum completion
function  c  i  s  j    costs estimated descendents  v  j  s   course  maxq
   

fimaxq hierarchical reinforcement learning

supports non hierarchical execution  possible feudal q 
learn value function decomposition 

   discussion

concluding paper  wish discuss two issues   a  design tradeoffs hierarchical reinforcement learning  b  methods automatically learning  or least
improving  maxq hierarchies 

    design tradeoffs hierarchical reinforcement learning

introduction paper  discussed four issues concerning design hierarchical reinforcement learning architectures   a  method defining subtasks   b 
use state abstraction   c  non hierarchical execution   d  design learning algorithms  subsection  want highlight tradeoff first two
issues 
maxq defines subtasks using termination predicate ti pseudo reward function
r    least two drawbacks method  first  hard programmer define ti r  correctly  since essentially requires guessing value function
optimal policy mdp states subtask terminates  second 
leads us seek recursively optimal policy rather hierarchically optimal policy 
recursively optimal policies may much worse hierarchically optimal ones 
may giving substantial performance 
however  return two drawbacks  maxq obtains important benefit 
policies value functions subtasks become context free  words 
depend parent tasks larger context invoked 
understand point  consider mdp shown figure    clear
optimal policy exiting left hand room  the exit subtask  depends location
goal  top right hand room  agent prefer
exit via upper door  whereas bottom right hand room  agent
prefer exit lower door  however  define subtask exiting
left hand room using pseudo reward zero doors  obtain policy
optimal either case  policy re use cases  furthermore 
policy depend location goal  hence  apply max node
irrelevance solve exit subtask using location robot ignore
location goal 
example shows obtain benefits subtask reuse state abstraction define subtask using termination predicate pseudo reward
function  termination predicate pseudo reward function provide barrier
prevents  communication  value information exit subtask context 
compare parr s ham method  hamq algorithm finds best policy
consistent hierarchy  achieve this  must permit information propagate
 into  exit subtask  i e   exit finite state controller  environment 
means state reached leaving exit subtask different
values depending location goal  different values propagate
back exit subtask  represent different values  exit subtask must know
   

fidietterich

location goal  short  achieve hierarchically optimal policy within exit
subtask  must  in general  represent value function using entire state space  state
abstractions cannot employed without losing hierarchical optimality 
see  therefore  direct tradeoff achieving hierarchical
optimality employing state abstractions  methods hierarchical optimality
freedom defining subtasks  e g   using partial policies  ham approach  
cannot  safely  employ state abstractions within subtasks  general  cannot
reuse solution one subtask multiple contexts  methods recursive optimality 
hand  must define subtasks using method  such pseudo reward functions
maxq fixed policies options framework  isolates subtask
context  return  apply state abstraction learned policy
reused many contexts  where less optimal  
interesting iterative method described dean lin       
viewed method moving along tradeoff  dean lin method 
programmer makes initial guess values terminal states subtask
 i e   doorways figure     based initial guess  locally optimal policies
subtasks computed  locally optimal policy parent task
computed while holding subtask policies fixed  i e   treating options  
point  algorithm computed recursively optimal solution original
problem  given initial guesses  instead solving various subproblems sequentially
via oine algorithm dean lin suggested  could use maxq q learning
algorithm 
method dean lin stop here  instead  computes new values
terminal states subtask based learned value function entire
problem  allows update  guesses  values terminal states 
entire solution process repeated obtain new recursively optimal solution 
based new guesses  prove process iterated indefinitely 
converge hierarchically optimal policy  provided  course  state abstractions
used within subtasks  
suggests extension maxq q learning adapts r  values online 
time subtask terminates  could update r  function based computed value
terminated state  precise  j subtask i  j terminates
state s    update r  j  s    equal v   i  s      maxa  q   i  s    a     however 
work r  j  s    represented using full state s   subtask j employing state
abstractions  x    s   r  j  x    need average value v   i  s    
average taken states s  x     s     weighted probability
visiting states   easily accomplished performing stochastic approximation
update form
r j  x           fft  r  j  x      fftv   i  s   
time subtask j terminates  algorithm could expected converge
best hierarchical policy consistent given state abstractions 
suggests problems  may worthwhile first learn recursively
optimal policy using aggressive state abstractions use learned value
function initialize maxq representation detailed representation
states  progressive refinements state space could guided monitoring
   

fimaxq hierarchical reinforcement learning

degree values v   i  x    vary abstract state x    large
variance  means state abstractions failing make important distinctions
values states  refined 
kinds adaptive algorithms take longer converge basic
maxq method described paper  tasks agent must solve many times
lifetime  worthwhile learning algorithms provide initial useful
solution gradually improve solution optimal  important goal
future research find methods diagnosing repairing errors  or sub optimalities 
initial hierarchy ultimately optimal policy discovered 

    automated discovery abstractions

approach taken paper rely upon programmer design
maxq hierarchy including termination conditions  pseudo reward functions  state
abstractions  results paper  particularly concerning state abstraction  suggest
ways might able automate construction hierarchy 
main purpose hierarchy create opportunities subtask sharing
state abstraction  actually closely related  order subtask shared
two different regions state space  must case value function
two different regions identical except additive offset  maxq framework 
additive offset would difference c values parent task  one way
find reusable subtasks would look regions state space value function
exhibits additive offsets 
second way would search structure one step probability transition
function p  s  js  a   subtask useful enables state abstractions max
node irrelevance  formulate problem identifying region
state space that  conditioned region  p  s  js  a  factors according
equation     top down divide and conquer algorithm similar decision tree algorithms
might able this 
third way would search funnel actions looking bottlenecks state
space policies must travel  would useful discovering cases
result distribution irrelevance 
ways  dicult kinds state abstractions discover
arbitrary subgoals introduced constrain policy  and sacrifice optimality  
example  could algorithm automatically decide impose landmarks onto
hdg task  perhaps detecting large region state space without bottlenecks
variations reward function 
problem discovering hierarchies important challenge future 
least paper provided guidelines constitute good state abstractions 
serve objective functions guiding automated search abstractions 

   concluding remarks

paper introduced new representation value function hierarchical reinforcement learning the maxq value function decomposition  proved
maxq decomposition represent value function hierarchical policy
   

fidietterich

finite horizon undiscounted  cumulative reward criterion infinite horizon
discounted reward criterion  representation supports subtask sharing re use  overall value function decomposed value functions individual subtasks 
paper introduced learning algorithm  maxq q learning  proved
converges probability   recursively optimal policy  paper argued although
recursive optimality weaker either hierarchical optimality global optimality 
important form optimality permits subtask learn locally optimal
policy ignoring behavior ancestors maxq graph  increases
opportunities subtask sharing state abstraction 
shown maxq decomposition creates opportunities state abstraction  identified set five properties  max node irrelevance  leaf irrelevance 
result distribution irrelevance  shielding  termination  allow us ignore large
parts state space within subtasks  proved maxq q still converges
presence forms state abstraction  showed experimentally state abstraction important practice successful application maxq q learning at
least taxi hdg tasks 
paper presented two different methods deriving improved non hierarchical policies maxq value function representation  formalized conditions
methods improve hierarchical policy  paper verified
experimentally non hierarchical execution gives improved performance fickle
taxi task  where achieves optimal performance  hdg task  where gives
substantial improvement  
finally  paper argued tradeoff governing design hierarchical
reinforcement learning methods  one end design spectrum  context free 
methods maxq q learning  provide good support state abstraction
subtask sharing learn recursively optimal policies  end
spectrum  context sensitive  methods hamq  options framework 
early work dean lin  methods discover hierarchically optimal
policies  or  cases  globally optimal policies   drawback cannot
easily exploit state abstractions share subtasks  great speedups
enabled state abstraction  paper argued context free approach
preferred and relaxed needed obtain improved policies 

acknowledgements
author gratefully acknowledges support national science foundation
grant number iri          oce naval research grant number n                air force oce scientific research grant number f                
spanish government program estancias de investigadores extranjeros en
regimen de a no sabatico en espa na  addition  author indebted many colleagues
helping develop clarify ideas paper including valentina zubek  leslie
kaelbling  bill langford  wes pinchot  rich sutton  prasad tadepalli  sebastian thrun 
particularly want thank eric chown encouraging study feudal reinforcement
learning  ron parr providing details ham machines  sebastian thrun
encouraging write single comprehensive paper  thank andrew moore
   

fimaxq hierarchical reinforcement learning

 the action editor   valentina zubek  two sets anonymous reviewers previous
drafts paper suggestions careful reading  improved paper
immeasurably 

references

bellman  r  e          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
belmont  ma 
boutilier  c   dearden  r     goldszmidt  m          exploiting structure policy construction  proceedings fourteenth international joint conference artificial
intelligence  pp            
currie  k     tate  a          o plan  open planning architecture  artificial intelligence                
dayan  p     hinton  g          feudal reinforcement learning  advances neural
information processing systems     pp           morgan kaufmann  san francisco 
ca 
dean  t     lin  s  h          decomposition techniques planning stochastic domains 
tech  rep  cs        department computer science  brown university  providence 
rhode island 
dietterich  t  g          maxq method hierarchical reinforcement learning 
fifteenth international conference machine learning  pp           morgan kaufmann 
fikes  r  e   hart  p  e     nilsson  n  j          learning executing generalized robot
plans  artificial intelligence             
forgy  c  l          rete  fast algorithm many pattern many object pattern
match problem  artificial intelligence                
hauskrecht  m   meuleau  n   kaelbling  l  p   dean  t     boutilier  c          hierarchical
solution markov decision processes using macro actions  proceedings
fourteenth annual conference uncertainty artificial intelligence  uai      pp 
        san francisco  ca  morgan kaufmann publishers 
howard  r  a          dynamic programming markov processes  mit press  cambridge  ma 
jaakkola  t   jordan  m  i     singh  s  p          convergence stochastic iterative
dynamic programming algorithms  neural computation                   
kaelbling  l  p          hierarchical reinforcement learning  preliminary results  proceedings tenth international conference machine learning  pp          san
francisco  ca  morgan kaufmann 
   

fidietterich

kalmar  z   szepesvari  c     lorincz  a          module based reinforcement learning
real robot  machine learning            
knoblock  c  a          learning abstraction hierarchies problem solving  proceedings
eighth national conference artificial intelligence  pp          boston  ma 
aaai press 
korf  r  e          macro operators  weak method learning  artificial intelligence 
              
lin  l  j          reinforcement learning robots using neural networks  ph d  thesis 
carnegie mellon university  department computer science  pittsburgh  pa 
moore  a  w   baird  l     kaelbling  l  p          multi value functions  ecient automatic action hierarchies multiple goal mdps  proceedings international joint conference artificial intelligence  pp            san francisco  morgan kaufmann 
parr  r       a   flexible decomposition algorithms weakly coupled markov decision
problems  proceedings fourteenth annual conference uncertainty
artificial intelligence  uai      pp          san francisco  ca  morgan kaufmann
publishers 
parr  r       b   hierarchical control learning markov decision processes  ph d 
thesis  university california  berkeley  california 
parr  r     russell  s          reinforcement learning hierarchies machines  advances neural information processing systems  vol      pp            cambridge 
ma  mit press 
pearl  j          probabilistic inference intelligent systems  networks plausible inference  morgan kaufmann  san mateo  ca 
rummery  g  a     niranjan  m          online q learning using connectionist systems 
tech  rep  cued finfeng tr      cambridge university engineering department  cambridge  england 
sacerdoti  e  d          planning hierarchy abstraction spaces  artificial intelligence 
               
singh  s   jaakkola  t   littman  m  l     szepesvari  c          convergence results
single step on policy reinforcement learning algorithms  tech  rep   university
colorado  department computer science  boulder  co  appear machine
learning 
singh  s  p          transfer learning composing solutions elemental sequential
tasks  machine learning             
sutton  r  s   singh  s   precup  d     ravindran  b          improved switching among
temporally abstract actions  advances neural information processing systems 
vol      pp             mit press 
   

fimaxq hierarchical reinforcement learning

sutton  r     barto  a  g          introduction reinforcement learning  mit press 
cambridge  ma 
sutton  r  s   precup  d     singh  s          mdps semi mdps  learning 
planning  representing knowledge multiple temporal scales  tech  rep   university massachusetts  department computer information sciences  amherst 
ma  appear artificial intelligence 
tadepalli  p     dietterich  t  g          hierarchical explanation based reinforcement
learning  proceedings fourteenth international conference machine
learning  pp          san francisco  ca  morgan kaufmann 
tambe  m     rosenbloom  p  s          investigating production system representations
non combinatorial match  artificial intelligence                  
watkins  c  j  c  h          learning delayed rewards  ph d  thesis  king s college 
oxford   to reprinted mit press   
watkins  c  j     dayan  p          technical note q learning  machine learning         

   


