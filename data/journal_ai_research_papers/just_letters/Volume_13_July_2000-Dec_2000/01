journal artificial intelligence research                

submitted       published     

value function approximations partially observable
markov decision processes

milos hauskrecht

milos cs brown edu

computer science department  brown university
box       brown university  providence  ri        usa

abstract

partially observable markov decision processes  pomdps  provide elegant mathematical framework modeling complex decision planning problems stochastic
domains states system observable indirectly  via set imperfect
noisy observations  modeling advantage pomdps  however  comes price  
exact methods solving computationally expensive thus applicable
practice simple problems  focus ecient approximation  heuristic 
methods attempt alleviate computational problem trade accuracy
speed  two objectives here  first  survey various approximation methods 
analyze properties relations provide new insights differences 
second  present number new approximation methods novel refinements existing techniques  theoretical results supported experiments problem
agent navigation domain 
   introduction

making decisions dynamic environments requires careful evaluation cost benefits immediate action choices may future 
evaluation becomes harder effects actions stochastic  must pursue evaluate many possible outcomes parallel  typically  problem becomes
complex look future  situation becomes even worse
outcomes observe imperfect unreliable indicators underlying process
special actions needed obtain reliable information  unfortunately  many
real world decision problems fall category 
consider  example  problem patient management  patient comes
hospital initial set complaints  rarely allow physician  decisionmaker  diagnose underlying disease certainty  number disease options
generally remain open initial evaluation  physician multiple choices
managing patient  he she choose nothing  wait see   order additional tests
learn patient state disease  proceed radical treatment
 e g  surgery   making right decision easy task  disease patient suffers
progress time may become worse window opportunity particular
effective treatment missed  hand  selection wrong treatment may
make patient s condition worse  may prevent applying correct treatment later 
result treatment typically non deterministic outcomes possible 
addition  treatment investigative choices come different costs  thus 
c      ai access foundation morgan kaufmann publishers  rights reserved 

fihauskrecht

course patient management  decision maker must carefully evaluate costs
benefits current future choices  well interaction ordering 
decision problems similar characteristics   complex temporal cost benefit tradeoffs 
stochasticity  partial observability underlying controlled process   include robot
navigation  target tracking  machine mantainance replacement  like 
sequential decision problems modeled markov decision processes  mdps 
 bellman        howard        puterman        boutilier  dean    hanks       
extensions  model choice problems similar patient management partially
observable markov decision process  pomdp   drake        astrom        sondik       
lovejoy      b   pomdp represents two sources uncertainty  stochasticity
underlying controlled process  e g  disease dynamics patient management problem  
imperfect observability states via set noisy observations  e g  symptoms 
findings  results tests   addition  lets us model uniform way control
information gathering  investigative  actions  well effects cost benefit tradeoffs  partial observability ability model reason information gathering
actions main features distinguish pomdp widely known fully
observable markov decision process  bellman        howard        
although useful modeling perspective  pomdps disadvantage hard solve  papadimitriou   tsitsiklis        littman        mundhenk  goldsmith 
lusena    allender        madani  hanks    condon         optimal  optimal solutions obtained practice problems low complexity  challenging goal
research area exploit additional structural properties domain and or suitable
approximations  heuristics  used obtain good solutions eciently 
focus heuristic approximation methods  particular approximations based
value functions  important research issues area design new ecient
algorithms  well better understanding existing techniques relations 
advantages disadvantages  paper address issues  first 
survey various value function approximations  analyze properties relations
provide insights differences  second  present number new methods
novel refinements existing techniques  theoretical results findings
supported empirically problem agent navigation domain 
   partially observable markov decision processes

partially observable markov decision process  pomdp  describes stochastic control
process partially observable  hidden  states  formally  corresponds tuple
 s  a    t  o  r  set states  set actions  set observations 
           set transition probabilities describe dynamic behavior
modeled environment             set observation probabilities
describe relationships among observations  states actions  r     ir
denotes reward model assigns rewards state transitions models payoffs associated transitions  instances definition pomdp includes
priori probability distribution set initial states  

  

fivalue function approximations pomdps

o 

t 



ot

t 

t  

st

a 

t 

t  



t 

r



figure    part uence diagram describing pomdp model  rectangles correspond
decision nodes  actions   circles random variables  states  diamonds
reward nodes  links represent dependencies among components  st     ot
rt denote state  action  observation reward time t  note action
time depends past observations actions  states 

    objective function
given pomdp  goal construct control policy maximizes objective  value 
function  objective function combines partial  stepwise  rewards multiple steps
using various kinds decision models  typically  models cumulative based
expectations  two models frequently used practice 

finite horizon model maximize e  ptt   rt    rt reward obtained
time t 

infinite horizon discounted model maximize e  p t   rt       
    discount factor 

note pomdps cumulative decision models provide rich language modeling
various control objectives  example  one easily model goal achievement tasks  a
specific goal must reached  giving large reward transition state
zero smaller rewards transitions 
paper focus primarily discounted infinite horizon model  however 
results easily applied finite horizon case 

    information state
pomdp process states hidden cannot observe making
decision next action  thus  action choices based information available us quantities derived information  illustrated
uence diagram figure    action time depends previous
observations actions  states  quantities summarizing information called
information states  complete information states represent trivial case 
  

fihauskrecht

t  



st

t  



t  

t  





rt

rt

figure    uence diagram pomdp information states corresponding
information state mdp  information states  it it     represented
double circled nodes  action choice  rectangle  depends current
information state 

definition    complete information state   complete information state time  denoted itc   consists of 




prior belief b  states time   
complete history actions observations fo    a    o    a      ot         ot g starting time     

sequence information states defines controlled markov process call
information state markov decision process information state mdp  policy
information state mdp defined terms control function     mapping
information state space actions  new information state  it   deterministic function
previous state  it      last action  at     new observation  ot   

   it     ot       
    update function mapping information state space  observations
actions back information space   easy see one always convert
original pomdp information state mdp using complete information states 
relation components two models sketch reduction
pomdp information state mdp  shown figure   
    bellman equations pomdps
information state mdp infinite horizon discounted case fully observable
mdp satisfies standard fixed point  bellman  equation 
 

 

x
v  i     max  i  a    p  i   ji  a v  i      
a a
i 

   

   paper  denotes generic update function  thus use symbol even information
state space different 
  

fivalue function approximations pomdps

p

here  v  i   denotes optimal value function maximizing e    
t   rt   state    i  a 
expected one step reward equals
x
xx
 i  a     s  a p  sji    
r s  a  s   p  s  js  a p  sji   
s s
s  s   

 s  a  denotes expected one step reward state action a 
since next information state      i  o  a  deterministic function previous
information state   action a  observation o  equation   rewritten
compactly summing possible observations  
v  i     max
a a

 

x
s s

 s  a p  sji    

x
o 

 

p  oji  a v    i  o  a    

   

optimal policy  control function      selects value maximizing action
 

 

x
x
 i     arg max
 s  a p  sji     p  oji  a v    i  o  a    
a a s s
o 

   

value control functions expressed terms action value functions
 q functions 
v  i     max q  i  a 
 i     arg max q  i  a  
a a
a a
x
x

q  i  a     s  a p  sji     p  oji  a v    i  o  a   
   
s s
o 
q function corresponds expected reward chosing fixed action  a  first
step acting optimally afterwards 
      sufficient statistics

derive equations     implicitly used complete information states  however 
remarked earlier  information available decision maker summarized
quantities  call sucient information states  states must preserve
necessary information content markov property information state
decision process 

definition    sucient information state process   let information state space
    update function defining information process  
 it         ot    process sucient regard optimal control when 
time step t  satisfies
p  st jit     p  st jitc  
p  ot jit           p  ot jitc         
itc itc   complete information states 
easy see equations       complete information states must hold
sucient information states  key benefit sucient statistics often
  

fihauskrecht

easier manipulate store  since unlike complete histories  may expand
time  example  standard pomdp model sucient work belief states
assign probabilities every possible process state  astrom          case
bellman equation reduces to 
 

v  b    max
a a

x
s s

 s  a b s   

xx
o  s s

 

p  ojs  a b s v    b  o  a    

   

next step belief state b 
x
b   s     b  o  a  s    fip  ojs  a 
p  sja  s   b s    
 
 s

    p  ojb  a  normalizing constant  defines belief state mdp
special case continuous state mdp  belief state mdps primary focus
investigation paper 
      value function mappings properties

bellman equation   belief state mdp rewritten value function
mapping form  let v space real valued bounded functions v     ir defined
belief information space   let h   b   ir defined

h b  a  v    

x

s s

 s  a b s   

xx

o  s s

p  ojs  a b s v    b  o  a   

defining value function mapping h   v   v  hv   b    maxa a h b  a  v   
bellman equation   information states written v   hv   well
known h  for mdps  isotone mapping contraction
supremum norm  see  heyman   sobel        puterman         

definition   mapping h isotone  v  u

  v v u implies hv hu  

definition   let k k supremum norm  mapping h contraction
supremum norm  v  u   v   khv hu k kv u k holds       
    value iteration
optimal value function  equation    approximation computed using dynamic programming techniques  simplest approach value iteration  bellman 
      shown figure    case  optimal value function v determined
limit performing sequence value iteration steps vi   hvi     vi
ith approximation value function  ith value function    sequence estimates
   models belief states sucient include pomdps observation action channel
lags  see hauskrecht         
   note update v   hv   applied solve finite horizon problem
standard way  difference v stands i steps to go value function v  represents
value function  rewards  end states 






  

fivalue function approximations pomdps

value iteration  p omdp    
initialize v b    
repeat
v  v 
update v hv   b    
supb j v  b  v    b  j
return v 
figure    value iteration procedure 
converges unique fixed point solution direct consequence banach s
theorem contraction mappings  see  example  puterman         
practice  stop iteration well reaches limit solution  stopping
criterion use algorithm  figure    examines maximum difference value
functions obtained two consecutive steps   so called bellman error  puterman       
littman         algorithm stops quantity falls threshold  
accuracy approximate solution  ith value function  regard v expressed
terms bellman error  

theorem   let   supb jvi  b  vi    b j   kvi vi   k magnitude bellman
error  kvi v k   kvi   v k   hold 
then  obtain approximation v precision bellman error fall
      
      piecewise linear convex approximations value function

major diculty applying value iteration  or dynamic programming  beliefstate mdps belief space infinite need compute update vi   hvi  
it  poses following threats  value function ith step may
representable finite means and or computable finite number steps 
address problem sondik  sondik        smallwood   sondik        showed
one guarantee computability ith value function well finite description
belief state mdp considering piecewise linear convex representations
value function estimates  see figure     particular  sondik showed piecewise
linear convex representation vi     vi   hvi   computable remains piecewise
linear convex 

theorem    piecewise linear convex functions   let v  initial value function
piecewise linear convex  ith value function obtained finite
number update steps belief state mdp finite  piecewise linear convex 
equal to 
x
vi  b    max b s ffi  s  
ffi   s s

b ffi vectors size js j finite set vectors  linear functions  ffi  
  

fihauskrecht

vi  b 

 

 

b s   

figure    piecewise linear convex function pomdp two process states
fs    s g  note b s       b s    holds belief state 

key part proof express update ith value function
terms linear functions   defining vi    
 
 x

vi  b    max  
a a

s s

 s  a b s   

x

max

o  ffi    



 
x x
  s   s s s

 

 
 

p  s    ojs  a b s  ffi    s      

   

leads piecewise linear convex value function vi represented
finite set linear functions ffi   one linear function every combination actions
j
permutations ffi   vectors size jj  let w    a  fo    ffji     g  fo    ffji     g  fojj   ffi j j g 
combination  linear function corresponding defined
xx
ffw
p  s    ojs  a ffji    s    
   
 s     s  a   
o  s   s


theorem   basis dynamic programming algorithm finding optimal
solution finite horizon models value iteration algorithm finding nearoptimal approximations v discounted  infinite horizon model  note  however 
result imply piecewise linearity optimal  fixed point  solution v  
      algorithms computing value function updates

key part value iteration algorithm computation value function updates
vi   hvi     assume ith value function vi represented finite number linear
segments  ff vectors   total number possible linear functions jajj   jjj  one
every combination actions permutations ffi   vectors size jj 
enumerated o jajjs j  j   jjj   time  however  complete set linear functions
rarely needed  linear functions dominated others omission
change resulting piecewise linear convex function  illustrated
figure   

  

fivalue function approximations pomdps

vi  b 

redundant linear
function
 

 

b s   

figure    redundant linear function  function dominate regions
belief space excluded 

linear function eliminated without changing resulting value function
solution called redundant  conversely  linear function singlehandedly achieves
optimal value least one point belief space called useful  
sake computational eciency important make size linear
function set small possible  keep useful linear functions  value iteration steps 
two main approaches computing useful linear functions  first approach
based generate and test paradigm due sondik        monahan        
idea enumerate possible linear functions first  test usefulness
linear functions set prune redundant vectors  recent extensions
method interleave generate test stages early pruning set partially
constructed linear functions  zhang   liu      a  cassandra  littman    zhang       
zhang   lee        
second approach builds sondik s idea computing useful linear function
single belief state  sondik        smallwood   sondik         done eciently 
key problem locate belief points seed useful linear functions
different methods address problem differently  methods implement idea
sondik s one  two pass algorithms  sondik         cheng s methods  cheng        
witness algorithm  kaelbling  littman    cassandra        littman        cassandra 
      
      limitations complexity

major diculty solving belief state mdp complexity piecewise
linear convex function grow extremely fast number update steps 
specifically  size linear function set defining function grow exponentially  in
number observations  single update step  then  assuming initial
value function
linear  number linear functions defining ith value function
o jajjj     


   defining redundant useful linear functions assume linear function duplicates 
i e  one copy linear function kept set  


  

fihauskrecht

potential growth size linear function set bad news 
remarked earlier  piecewise linear convex value function usually less complex
worst case many linear functions pruned away updates  however 
turned task identifying useful linear functions computationally
intractable well  littman         means one faces potential
super exponential growth number useful linear functions  ineciencies
related identification vectors  significant drawback makes
exact methods applicable relatively simple problems 
analysis suggests solving pomdp problem intrinsically hard
task  indeed  finding optimal solution finite horizon problem pspace hard
 papadimitriou   tsitsiklis         finding optimal solution discounted infinitehorizon criterion even harder  corresponding decision problem shown
undecidable  madani et al          thus optimal solution may computable 
      structural refinements basic algorithm

standard pomdp model uses state space full transition reward matrices 
however  practice  problems often exhibit structure represented
compactly  example  using graphical models  pearl        lauritzen         often
dynamic belief networks  dean   kanazawa        kjaerulff        dynamic uence
diagrams  howard   matheson        tatman   schachter          many ways
take advantage problem structure modify improve exact algorithms 
example  refinement basic monahan algorithm compact transition reward
models studied boutilier poole         hybrid framework combines
mdp pomdp problem solving techniques take advantage perfectly partially observable components model subsequent value function decomposition
proposed hauskrecht                     similar approach perfect information
region  subset states  containing actual underlying state discussed
zhang liu      b      a   finally  casta non        yost        explore techniques
solving large pomdps consist set smaller  resource coupled otherwise
independent pomdps 

    extracting control strategy
value iteration allow us compute ith approximation value function vi   however 
ulimate goal find optimal control strategy     close approximation 
thus focus problem extraction control strategies results
value iteration 
      lookahead design

simplest way define control function     value function vi via
greedy one step lookahead 
 

 b    arg max
a a

x
s s

 s  a b s   

x
o 

 

p  ojb  a vi    b  o  a    

   see survey boutilier  dean hanks        different ways represent structured mdps 
  

fivalue function approximations pomdps

vi  b 
a 
a 

a 
a 

 

b

 

b s   

figure    direct control design  every linear function defining vi associated
action  action selected linear function  or q function  maximal 
vi represents ith approximation optimal value function  question
arises good resulting controller really is   following theorem  puterman       
williams   baird        littman        relates accuracy  lookahead  controller
bellman error 

theorem   let   kvi vi   k magnitude bellman error  let vila
expected reward lookahead controller designed vi   kvila v k     
bound used construct value iteration routine yields lookahead
strategy minimum required precision  result extended kstep lookahead design straightforward way  k steps  error bound becomes
kvila k  v k        
k

      direct design

extract control action via lookahead essentially requires computing one full update 
obviously  lead unwanted delays reaction times  general  speed
response remembering using additional information  particular  every linear
function defining vi associated choice action  see equation     action
byproduct methods computing linear functions extra computation required
find it  action corresponding best linear function selected directly
belief state  idea illustrated figure   
bound accuracy direct controller infinite horizon case
derived terms magnitude bellman error 

theorem   let   kvi vi   k magnitude bellman error  let vidr
expected reward direct controller designed vi   kvidr v k     
direct action choice closely related notion action value function  or
q function   analogously equation    ith q function satisfies
vi  b    max qi  b  a  
a a

   note control action extracted via lookahead v optimal  i      steps to go
finite horizon model  main difference v optimal value function steps go 




  

fihauskrecht

a 

o 
o 

a 

o   

 

o 

o   

a 

o    o 

 

o 
a 

a 

o 

o 

a 

o    o 
a 

figure    policy graph  finite state machine  obtained two value iteration steps 
nodes correspond linear functions  or states finite state machine 
links dependencies linear functions  transitions states   every
linear function  node  associated action  ensure policy
applied infinite horizon problem  add cycle last state
 dashed line  

qi  b  a    r b  a   

x
o 

p  ojb  a vi      b  a  o   

perspective  direct strategy selects action best  maximum  qfunction given belief state  
      finite state machine design

complex refinement technique remember  every linear function
vi   action choice choice linear function previous
step observations  see equation     idea applied
recursively linear functions previous steps  obtain relatively complex
dependency structure relating linear functions vi   vi     v    observations actions
represents control strategy  kaelbling et al         
see this  model structure graphical terms  figure     different nodes
represent linear functions  actions associated nodes correspond optimizing actions 
links emanating nodes correspond different observations  successor nodes correspond linear functions paired observations  graphs called policy graphs
 kaelbling et al         littman        cassandra         one interpretation dependency structure represents collection finite state machines  fsms  many
possible initial states implement pomdp controller  nodes correspond states
controller  actions controls  outputs   links transitions conditioned inputs
   williams baird        give results relating accuracy direct q function controller
bellman error q functions 

  

fivalue function approximations pomdps

 observations   start state fsm controller chosen greedily selecting
linear function  controller state  optimizing value initial belief state 
advantage finite state machine representation strategy
first steps works observations directly  belief state updates needed 
contrasts two policy models  lookahead direct models   must keep
track current belief state update time order extract appropriate
control  drawback approach fsm controller limited steps
correspond number value iteration steps performed  however  infinitehorizon model controller expected run infinite number steps  one way
remedy deficiency extend fsm structure create cycles let us
visit controller states repeatedly  example  adding cycle transition end state
fsm controller figure    dashed line  ensures controller applicable
infinite horizon problem 

    policy iteration
alternative method finding solution discounted infinite horizon problem
policy iteration  howard        sondik         policy iteration searches policy space
gradually improves current control policy one belief states  method
consists two steps performed iteratively 




policy evaluation  computes expected value current policy 
policy improvement  improves current policy 

saw section      many ways represent control policy
pomdp  restrict attention finite state machine model observations
correspond inputs actions outputs  platzman        hansen      b  kaelbling
et al          
      finite state machine controller

finite state machine  fsm  controller c    m    a        pomdp described
set memory states controller  set observations  inputs    set
actions  outputs  a  transition function     mapping states fsm
next memory states given observation  output function     mapping
memory states actions  function   i    selects initial memory state given
initial information state  initial information state corresponds either prior
posterior belief state time t  depending availability initial observation 
      policy evaluation

first step policy iteration policy evaluation  important property
fsm model value function specific fsm strategy computed
eciently number controller states   key ecient computability
   policy iteration algorithm policies defined regions belief space described
first sondik        
  

fihauskrecht

x 
o 
o 

a 

a 

x 

o 


o 
a 

o 

o 

x 

 

a 

o 

x 

figure    example four state fsm policy  nodes represent states  links transitions states  conditioned observations   every memory state
associated control action  output  

fact value function executing fsm strategy memory state x
linear  platzman         

theorem   let c finite state machine controller set memory states  
value function applying c memory state x     v c  x  b   linear  value
functions x   found solving system linear equations js jjm j
variables 
illustrate main idea example  assume fsm controller four memory
states fx    x    x    x  g  figure    stochastic process two hidden states  
fs    s g  value policy augmented state space satisfies system
linear equations
v  x    s       s     x      
v  x    s       s     x      
v  x    s       s     x      



v  x    s       s     x      

xx

o  s s

xx

o  s s

xx

o  s s

xx
o  s s

p  o  sjs     x    v   x    o   s 
p  o  sjs     x    v   x    o   s 
p  o  sjs     x    v   x    o   s 
p  o  sjs     x    v   x    o   s  

 x  action executed x  x  o  state one transits
seeing input  observation  o  assuming start policy memory state x   
value policy is 
x
v c  x    b    v  x    s b s  
s s
   idea linearity ecient computability value functions fixed fsm based strategy
addressed recently different contexts number researchers  littman        cassandra 
      hauskrecht        hansen      b  kaelbling et al          however  origins idea
traced earlier work platzman        
  

fivalue function approximations pomdps

thus value function linear computed eciently solving system
linear equations 
since general fsm controller start memory state  always
choose initial memory state greedily  maximizing expected value result 
case optimal choice function defined as 
 b    arg max v c  x  b  
x m
value fsm policy c belief state b is 

v c  b    max v c  x  b    v c    b   b  
x m

note resulting value function strategy c piecewise linear convex
represents expected rewards following c   since strategy perform better
optimal strategy  v c v must hold 
      policy improvement

policy iteration method  searching space controllers  starts arbitrary initial policy improves gradually refining finite state machine  fsm  description 
particular  one keeps modifying structure controller adding removing controller states  memory  transitions  let c c   old new fsm controller 
improvement step must satisfy
 

v c  b  v c  b  b    

 b   v c    b    v c  b  
guarantee improvement  hansen      a      b  proposed policy iteration algorithm relies exact value function updates obtain new improved policy structure    basic idea improvement based observation one switch
back forth fsm policy description piecewise linear convex
representation value function  particular 

value function fsm policy piecewise linear convex every linear
function describing corresponds memory state controller 

individual linear functions comprising new value function update
viewed new memory states fsm policy  described section       

allows us improve policy adding new memory states corresponding linear
functions new value function obtained exact update  technique
refined removing linear functions  memory states  whenever fully
dominated one linear functions 
    policy iteration algorithm exploits exact value function updates works policies defined
belief space used earlier sondik        

  

fihauskrecht

b    

o 

o 

a 

b

a 

b    

figure    two step decision tree  rectangles correspond decision nodes  moves
decision maker  circles chance nodes  moves environment  
black rectangles represent leaves tree  reward specific path
associated every leaf tree  decision nodes associated
information states obtained following action observation choices along
path root tree  example  b    belief state obtained
performing action a  initial belief state b observing observation o   

    forward  decision tree  methods
methods discussed far assume prior knowledge initial belief state treat
belief states equally likely  however  initial state known fixed  methods
often modified take advantage fact  example  finite horizon
problem  finite number belief states reached given initial state 
case often easier enumerate possible histories  sequences actions
observations  represent problem using stochastic decision trees  raiffa        
example two step decision tree shown figure   
algorithm solving stochastic decision tree basically mimics value function
updates  restricted situations reached initial belief state 
key diculty number possible trajectories grows exponentially
horizon interest 
      combining dynamic programming decision tree techniques

solve pomdp fixed initial belief state  apply two strategies  one constructs decision tree first solves it  solves problem backward
fashion via dynamic programming  unfortunately  techniques inecient  one
suffering exponential growth decision tree size  super exponential
growth value function complexity  however  two techniques combined
  

fivalue function approximations pomdps

way least partially eliminates disadvantages  idea based fact
two techniques work solution two different sides  one forward
backward  complexity worsens gradually  solution
compute complete kth value function using dynamic programming  value iteration 
cover remaining steps forward decision tree expansion 
various modifications idea possible  example  one often replace
exact dynamic programming two ecient approximations providing upper
lower bounds value function  decision tree must expanded
bounds sucient determine optimal action choice  number search
techniques developed ai literature  korf        combined branch and bound
pruning  satia   lave        applied type problem  several researchers
experimented solve pomdps  washington        hauskrecht       
hansen      b   methods applicable problem based monte carlo
sampling  kearns  mansour    ng        mcallester   singh        real time dynamic
programming  barto  bradtke    singh        dearden   boutilier        bonet   geffner 
      
      classical planning framework

pomdp problems fixed initial belief states solutions closely related
work classical planning extensions handle stochastic partially observable
domains  particularly work buridan c buridan planners  kushmerick 
hanks    weld        draper  hanks    weld         objective planners
maximize probability reaching goal state  however  task similar
discounted reward task terms complexity  since discounted reward model
converted goal achievement model introducing absorbing state  condon 
      
   heuristic approximations

key obstacle wider application pomdp framework computational
complexity pomdp problems  particular  finding optimal solution finitehorizon case pspace hard  papadimitriou   tsitsiklis        discounted infinitehorizon case may even computable  madani et al          one approach
problems approximate solution  precision  unfortunately  even
remains intractable general pomdps cannot approximated eciently  burago 
rougemont    slissenko        lusena  goldsmith    mundhenk        madani et al  
       reason simple problems solved optimally
near optimally practice 
alleviate complexity problem  research pomdp area focused various
heuristic methods  or approximations without error parameter  ecient   
heuristic methods focus here  thus  referring approximations  mean
heuristics  unless specifically stated otherwise 
    quality heuristic approximation tested using bellman error  requires one exact
update step  however  heuristic methods per se contain precision parameter 

  

fihauskrecht

many approximation methods combinations divided two often
closely related classes  value function approximations policy approximations 

    value function approximations
main idea value function approximation approach approximate optimal
value function v     ir function vb     ir defined information
space  typically  new function lower complexity  recall optimal nearoptimal value function may consist large set linear functions  easier compute
exact solution  approximations often formulated dynamic programming
problems expressed terms approximate value function updates hb   thus 
understand differences advantages various approximations exact methods 
often sucient analyze compare update rules 
      value function bounds

although heuristic approximations guaranteed precision  many cases
able say whether overestimate underestimate optimal value function 
information bounds used multiple ways  example  upper  lowerbounds help narrowing range optimal value function  elimination
suboptimal actions subsequent speed ups exact methods  alternatively  one
use knowledge value function bounds determine accuracy controller
generated based one bounds  see section         also  instances  lower
bound alone sucient guarantee control choice always achieves expected
reward least high one given bound  section        
bound property different methods determined examining updates
bound relations 

definition    upper bound   let h exact value function mapping hb apb   b   hv   b  holds
proximation  say hb upper bounds h v  hv
every b    
analogous definition constructed lower bound 
      convergence approximate value iteration

let hb value function mapping representing approximate update  approximate value iteration computes ith value function vbi   hb vbi     fixed point
solution vc   hb vb close approximation would represent intended output
approximation routine  main problem iteration method general
converge unique multiple solutions  diverge  oscillate  depending hb
initial function vb    therefore  unique convergence cannot guaranteed arbitrary
mapping hb convergence specific approximation method must proved 

definition    convergence hb    value iteration hb converges value function v  limn   hb n v    exists 

  

fivalue function approximations pomdps

definition    unique convergence hb    value iteration converges uniquely v
every v   v   limn   hb n v   exists pairs v  u   v   limn   hb n v    
limn   hb n u   
sucient condition unique convergence show hb contraction 
contraction bound properties hb combined  additional conditions 
show convergence iterative approximation method bound  address
issue present theorem comparing fixed point solutions two value function mappings 
theorem   let h  h  two value function mappings defined v  v 
   h    h  contractions fixed points v    v   
   v    v  h  v  h  v    v   

   h  isotone mapping 
v  v  holds 

note theorem require v  v  cover space value
functions  example  v  cover possible value functions belief state mdp 
v  restricted space piecewise linear convex value functions 
gives us exibility design iterative approximation algorithms computing
value function bounds  analogous theorem holds lower bound 
      control

approximation value function available  used generate
control strategy  general  control solutions correspond options presented section
    include lookahead  direct  q function  finite state machine designs 
drawback control strategies based heuristic approximations
precision guarantee  one way find accuracy strategies one exact
update value function approximation adopt result theorems    
bellman error  alternative solution problem bound accuracy
controllers using upper  lower bound approximations optimal value
function  illustrate approach  present prove  in appendix  following
theorem relates quality bounds quality lookahead controller 

theorem   let vbu vbl upper lower bounds optimal value function
discounted infinite horizon problem  let   supb jvbu  b  vbl  b j   kvbu vbl k
maximum bound difference  expected reward lookahead controller vb la  
constructed either vbu vbl   satisfies kvb la v k          
    policy approximation
alternative value function approximation policy approximation  shown earlier 
strategy  controller  pomdp represented using finite state machine  fsm 
model  policy iteration searches space possible policies  fsms  optimal near optimal solution  space usually enormous  bottleneck
  

fihauskrecht

method  thus  instead searching complete policy space  restrict attention
subspace believe contain optimal solution good approximation  memoryless policies  platzman        white   scherer        littman        singh 
jaakkola    jordan         policies based truncated histories  platzman        white  
scherer        mccallum         finite state controllers fixed number memory
states  platzman        hauskrecht        hansen      a      b  examples
policy space restriction  following consider finite state machine model
 see section         quite general  models viewed special cases 
states fsm policy model represent memory controller and  general 
summarize information past activities observations  thus  best viewed
approximations information states  feature states  transition model
controller    approximates update function information state mdp
    output function fsm    approximates control function    mapping
information states actions  important property model  shown section
       value function fixed controller fixed initial memory state
obtained eciently solving system linear equations  platzman        
apply policy approximation approach first need decide     restrict
space policies     judge policy quality 
restriction frequently used consider controllers fixed number
states  say k  structural restrictions narrowing space policies
restrict either output function  choice actions different controller states  
transitions current next states  general  heuristic domain related
insight may help selecting right biases 
two different policies yield value functions better different regions
belief space  thus  order decide policy best  need define
importance different regions combinations  multiple solutions this 
example  platzman        considers worst case measure optimizes worst
 minimal  value initial belief states  let c space fsm controllers satisfying
given restrictions  quality policy worst case measure is 
max min max v c  x  b  
c  c b i x m
c

another option consider distribution initial belief states maximize
expectation value function values  however  common objective choose
policy leads best value single initial belief state b   
max max v c  x  b    
c  c x m
c

finding optimal policy case reduces combinatorial optimization problem 
unfortunately  trivial cases  even problem computationally intractable 
example  problem finding optimal policy memoryless case  only current observations considered  np hard  littman         thus  various heuristics
typically applied alleviate diculty  littman        

  

fivalue function approximations pomdps

valuefunction
approximations

gridbased linear
function methods
section    

fully observable mdp
approximations
section    

fast informed bound
approximations
section    

fixed strategy
approximations
section    

curvefitting
approximations
section    

gridbased value interpolation
extrapolation methods
section    

unobservable mdp
approximations
section    

figure     value function approximation methods 
      randomized policies

restricting space policies simplify policy optimization problem 
hand  simultaneously give opportunity find best optimal policy  replacing best restricted policy  point  considered deterministic
policies fixed number internal controller states  is  policies deterministic
output transition functions  however  finding best deterministic policy always best option  randomized policies  randomized output transition functions 
usually lead far better performance  application randomized  or stochastic 
policies pomdps introduced platzman         essentially  deterministic
policy represented randomized policy single action transition 
best randomized policy worse best deterministic policy  difference control performance two policies shows often cases number
states controller relatively small compared optimal strategy 
advantage stochastic policies space larger parameters
policy continuous  therefore problem finding optimal stochastic policy
becomes non linear optimization problem variety optimization methods
applied solve it  example gradient based approach  see meuleau et al         
   value function approximation methods

section discuss depth value function approximation methods  focus approximations belief information space    survey known techniques 
include number new methods modifications existing methods  figure   
summarizes methods covered  describe methods means update rules
    alternative value function approximations may work complete histories past actions observations  approximation methods used white scherer        example 

  

fihauskrecht

  

  

  

  

  

  

  

  

  

  

 

 

 

 

 

 

 

 

 

 

moves

sensors

figure     test example  maze navigation problem  maze   
implement  simplifies analysis theoretical comparison  focus following properties  complexity dynamic programming  value iteration  updates 
complexity value functions method uses  ability methods bound
exact update  convergence value iteration approximate update rules 
control performance related controllers  results theoretical analysis illustrated empirically problem agent navigation domain  addition  use
agent navigation problem illustrate give intuitions characteristics
methods theoretical underpinning  thus  results generalized
problems used rank different methods 
agent navigation problem

maze   maze navigation problem    states  six actions eight observations 
maze  figure     consists    partially connected rooms  states  robot
operates collects rewards  robot move four directions  north  south  east
west  check presence walls using sensors  but  neither  move 
actions sensor inputs perfect  robot end moving unintended
directions  robot moves unintended direction probability          
neighboring directions   move wall keeps robot
position  investigative actions help robot navigate activating sensor inputs  two
investigative actions allow robot check inputs  presence wall  northsouth east west directions  sensor accuracy detecting walls      two wall
case  e g  north south wall       one wall case  north south      
no wall case  smaller probabilities wrong perceptions 
control objective maximize expected discounted rewards discount
factor      small reward given every action leading bumping wall
   points move   points investigative action   one large reward     
points  given achieving special target room  indicated circle figure 
recognizing performing one move actions  collecting
reward  robot placed random new start position 
although maze   problem moderate complexity regard size
state  action observation spaces  exact solution beyond reach current
exact methods  exact methods tried problem include witness algorithm
 kaelbling et al          incremental pruning algorithm  cassandra et al          
    many thanks anthony cassandra running algorithms 
  

fivalue function approximations pomdps

vmdp

 
vmdp
 s    

 
vmdp
 s    

vqmdp

q  mdp s    a    

q  mdp s    a   

q  mdp s    a   

q  mdp s    a   

 
vpomdp
 

 
vpomdp
 

b s   

 

 a 

 

b s   

 b 

figure     approximations based fully observable version two state pomdp
 with states s    s      a  mdp approximation   b  qmdp approximation 
values extreme points belief space solutions fully observable
mdp 
policy iteration fsm model  hansen      b   main obstacle preventing
algorithms obtaining optimal close to optimal solution complexity
value function  the number linear functions needed describe it  subsequent
running times memory problems 

    approximations fully observable mdp
perhaps simplest way approximate value function pomdp assume
states process fully observable  astrom        lovejoy         case
optimal value function v pomdp approximated as 
vb  b   

x

s s

 s  
b s vmdp

   

 s  optimal value function state fully observable version
vmdp
process  refer approximation mdp approximation  idea
approximation illustrated figure   a  resulting value function linear
fully defined values extreme points belief simplex  correspond
optimal values fully observable case  main advantage approximation
fully observable mdp  fomdp  solved eciently finitehorizon problem discounted infinite horizon problems    update step  fully
observable  mdp is 
 
 

vimdp
 s  a   
    s    max
 

x
s   s

 
 

p  s  js  a vimdp  s      

    solution finite state fully observable mdp discounted infinite horizon criterion
found eciently formulating equivalent linear programming task  bertsekas       

  

fihauskrecht

      mdp approximation

mdp approximation approach  equation    described terms valuefunction updates belief space mdp  although step strictly speaking redundant
here  simplifies analysis comparison approach approximations 
let vbi linear value function described vector ffmdp
corresponding values

vimdp  s    states s       i     th value function vbi  

vbi    b   

x
s s

 

b s  max   s  a   
a a

   hmdp vbi   b  

x
s   s

 

p  s  js  a ffmdp
 s    


vbi   described linear function components
mdp
ffmdp
i    s    vi    s    max


 

 s  a   

x
s s

 

p  s  js  a ffmdp
 s     


mdp based rule hmdp rewritten general form starts
arbitrary piecewise linear convex value function vi   represented set linear
functions  

vbi    b   

x
s s

 
 

b s  max   s  a   
a a

x
s   s

 
 

p  s  js  a  max ffi  s      
ffi  



application hmdp mapping always leads linear value function 
update easy compute takes o jajjs j    j jjs j  time  reduces o jajjs j   
time mdp based updates strung together  remarked earlier  optimal
solution infinite horizon  discounted problem solved eciently via linear
programming 
update mdp approximation upper bounds exact update  is  h vbi
hmdp vbi   show property later theorem    covers cases  intuition
cannot get better solution less information  thus fully observable
mdp must upper bound partially observable case 
      approximation q functions  qmdp 

variant approximation based fully observable mdp uses q functions  littman 
cassandra    kaelbling        
x
vb  b    max b s qmdp  s  a  
a a s s

x
 s   
qmdp  s  a     s  a   
p  s  js  a vmdp
s   s
optimal action value function  q function  fully observable mdp  qmdp
approximation vb piecewise linear convex jaj linear functions  corresponding
  

fivalue function approximations pomdps

one action  figure   b   qmdp update rule  for belief state mdp  vbi
linear functions ffki   is 

vbi    b    max

x

a a s s

 

b s    s  a   

   hqmdp vbi   b  

x
s   s

 

p  s  js  a  max ffi  s    
ffi  



hqmdp generates value function jaj linear functions  time complexity
update mdp approximation case   o jajjs j    j jjs j   reduces
o jajjs j    time qmdp updates used  hqmdp contraction mapping
fixed point solution found solving corresponding fully observable mdp 
qmdp update upper bounds exact update  bound tighter
mdp update  is  h vbi hqmdp vbi hmdp vbi   prove later theorem   
inequalities hold fixed point solutions  through theorem    
illustrate difference quality bounds mdp approximation
qmdp method  use maze   navigation problem  measure quality
bound use mean value function values  since belief states equally important
assume uniformly distributed  approximate measure using
average values fixed set n        belief points  points set
selected uniformly random beginning  set chosen  fixed
remained tests  here later   figure    shows results
experiment  include results fast informed bound method presented
next section    figure    shows running times methods  methods
implemented common lisp run sun ultra   workstation 
      control

mdp qmdp value function approximations used construct controllers based one step lookahead  addition  qmdp approximation suitable
direct control strategy  selects action corresponding best  highest
value  q function  thus  method special case q function approach discussed
section          advantage direct qmdp method faster
lookahead designs  hand  lookahead tends improve control performance 
shown figure     compares control performance different controllers
maze   problem 
quality policy b   preference towards particular initial belief state 
measured mean value function values b uniformly distributed initial
belief states  approximate measure using average discounted rewards

    confidence interval limits probability level      range              respective
average scores holds bound experiments paper  relatively small
include graphs 
    pointed littman et al          instances  direct qmdp controller never selects
investigative actions  is  actions try gain information underlying process
state  note  however  observation true general qmdp based controller
direct action selection may select investigative actions  even though fully observable version
problem investigative actions never chosen 
  

fihauskrecht

bound quality
mdp
approximation

qmdp
approximation

running times

fast informed
bound

  

   

  
time  sec 

score

   
   

  
  
  

  
  

  

 
mdp
approximation

  

qmdp
approximation

fast informed
bound

figure     comparison mdp  qmdp fast informed bound approximations 
bound quality  left   running times  right   bound quality score
average value approximation set      belief points  chosen uniformly random   methods upper bound optimal value function 
ip bound quality graph longer bars indicate better approximations 
     control trajectories obtained fixed set n        initial belief states  selected
uniformly random beginning   trajectories obtained simulation
   steps long   
validate comparison along averaged performance scores  must show
scores result randomness methods indeed statistically
significantly different  rely pairwise significance tests    summarize
obtained results  score differences                 two methods  here
later paper  sucient reject method lower score
better performer significance levels                  respectively    error bars
figure    ect critical score difference significance level      
figure    shows average reaction times different controllers
experiments  results show clear dominance direct qmdp controller 
need lookahead order extract action  compared two mdpbased controllers 

    fast informed bound method
mdp qmdp approaches ignore partial observability use fully
observable mdp surrogate  improve approximations account  at least
    length trajectories     steps  maze   problem chosen ensure estimates
 discounted  cumulative rewards far actual rewards infinite number steps 
    alternative way compare two methods compute confidence limits scores inspect
overlaps  however  case  ability distinguish two methods reduced due
uctuations scores different initializations  maze    confidence interval limits probability
level      range            respective average scores  covers control experiments
later  pairwise tests eliminate dependency examining differences individual values
thus improve discriminative power 
    critical score differences listed cover worst case combination  thus  may pairs
smaller difference would suce 
  

fivalue function approximations pomdps

reaction times

control performance
     

  

lookahead
    

lookahead

lookahead

time  sec 

score

  
  

direct
  

lookahead

direct

     
    

  

     

  

 
mdp
approximation

direct

fast informed
bound

qmdp
approximation

lookahead

lookahead

mdp
approximation

direct

qmdp
approximation

fast informed
bound

figure     comparison control performance mdp  qmdp fast informed bound
methods  quality control  left   reaction times  right   quality of control
score average discounted rewards      control trajectories obtained
fixed set      initial belief states  selected uniformly random  
error bars show critical score difference value        two methods become statistically different significance level      
degree  partial observability propose new method   fast informed bound
method  let vbi piecewise linear convex value function represented set linear
functions   new update defined
 
 x

xx

 
 

x

vbi    b    max    s  a b s   
max
p  s    ojs  a b s ffi  s    
a a s s

 
 
o  s s
 s
 
 


 

 x



x

x

  max   b s    s  a   
max
a a s s
o    s   s
   hf ib vbi   b  




 
 
p  s    ojs  a ffi  s     

fast informed bound update obtained exact update following
derivation 
 
 x

x

xx

 
 

x

 
 

 h vbi   b    max    s  a b s   
max
p  s    ojs  a b s ffi  s    
a a s s

 
o 
s   s s s


 
 x

max
 s  a b s   
a a  
s s

x

 

xx



max

o  s s ffi   s   s

x

x

p  s    ojs  a b s ffi  s    
 

  max b s    s  a   
max
p  s    ojs  a ffi  s    
a a s s

 
o 
s   s


x



  max b s ffai    s 
a a s s
   hf ib vbi   b  

value function vbi     hf ib vbi one obtains update piecewise linear
convex consists jaj different linear functions  corresponding one
  

fihauskrecht

action

ffai    s     s  a   

x

x
max
p  s    ojs  a ffi  s    

 
o 
s   s




hf ib update ecient computed o jajjs j  jjj j  time  method
always outputs jaj linear functions  computation done o jaj  js j  jj  time 
many hf ib updates strung together  significant complexity reduction
compared exact approach  latter lead function consisting jajj jjj
linear functions  exponential number observations worst case
takes o jajjs j  j jjj  time 
hf ib updates polynomial complexity one find approximation
finite horizon case eciently  open issue remains problem finding solution
infinite horizon discounted case complexity  address establish
following theorem 

theorem   solution fast informed bound approximation found solving
mdp js jjajjj states  jaj actions discount factor  
full proof theorem deferred appendix  key part proof
construction equivalent mdp js jjajjj states representing hf ib updates 
since finite state mdp solved linear program conversion  fixed point
solution fast informed bound update computable eciently 
      fast informed bound versus fully observable mdp approximations

fast informed update upper bounds exact update tighter mdp
qmdp approximation updates 

theorem   let vbi corresponds piecewise linear convex value function defined
linear functions  h vbi hf ib vbi hqmdp vbi hmdp vbi  
key trick deriving result swap max sum operators  the
proof appendix  thus obtain upper bound inequalities
subsequent reduction complexity update rules compared exact update 
shown figure     umdp approximation  included figure    
discussed later section      thus  difference among methods boils
simple mathematical manipulations  note inequality relations derived
updates hold fixed point solutions  through theorem    
figure   a illustrates improvement bound mdp based approximations
maze   problem  note  however  improvement paid increased
running time complexity  figure   b  
      control

fast informed bound always outputs piecewise linear convex function  one
linear function per action  allows us build pomdp controller selects action
associated best  highest value  linear function directly  figure    compares
control performance direct lookahead controllers mdp qmdp
controllers  see fast informed bound leads tighter bounds
  

fivalue function approximations pomdps

umdp update 


v       b     ax b             ax
aa






exact update 


v       b     ax b            
aa


p          b    

 

ax

 



fast informed bound update 



v       b     ax b            
aa



ax




p           ax      







mdp approx  update 


v       b     b     ax        
aa




p                  



 

 


  s   



p            b          




qmdp approx  update 


v       b     ax b            











 s


p           ax      



figure     relations exact update umdp  fast informed bound 
qmdp mdp updates 
improved control average  however  stress currently theoretical
underpinning observation thus may true belief states
problem 
      extensions fast informed bound method

main idea fast informed bound method select best linear function
every observation every current state separately  differs exact update
seek linear function gives best result every observation
combination states  however  observe great deal middle ground
two extremes  indeed  one design update rule chooses optimal
 maximal  linear functions disjoint sets states separately  illustrate idea 
assume partitioning   fs    s      sm g state space   new update is 

vbi    b    max
a a

 

x
s s

 s  a b s   

x

 
  max

x x

o  ffi   s s  s   s

p  s    ojs  a b s ffi  s    

x x
max
p  s    ojs  a b s ffi  s       
  s s s   s
 




max

x x

ffi   s s s   s


  
 
p  s    ojs  a b s ffi  s     

easy see update upper bounds exact update  exploration
approach various partitioning heuristics remains interesting open research issue 
  

fihauskrecht

    approximation unobservable mdp
mdp approximation assumes full observability pomdp states obtain simpler
ecient updates  extreme discard observations available
decision maker  mdp observations called unobservable mdp  umdp 
one may choose value function solution alternative approximation 
find solution unobservable mdp  derive corresponding update
rule  humdp   similarly update partially observable case  humdp preserves
piecewise linearity convexity value function contraction  update
equals 
 
 x

 
 

xx

vbi    b    max    s  a b s    max
p  s  js  a b s ffi  s    
a a s s
  s s s   s
   humdp vbi   b  




set linear functions describing vbi   vbi   remains piecewise linear convex
consists j jjaj linear functions  contrast exact update 
number possible vectors next step grow exponentially number
observations leads jajj jjj possible vectors  time complexity update
o jajjs j  j j   thus  starting vb  one linear function  running time complexity
k updates bounded o jajk js j     problem finding optimal solution
unobservable mdp remains intractable  finite horizon case np hard burago et al  
       discounted infinite horizon case undecidable  madani et al          thus 
usually useful approximation 
update humdp lower bounds exact update  intuitive result ecting
fact one cannot better less information  provide insight
two updates related  following derivation  proves bound
property elegant way 
 
 x

x

 
 

xx

 h vbi   b    max    s  a b s   
max
p  s    ojs  a b s ffi  s    
a a s s

 
o 
s   s s s
 
 


 x

 s  a b s    max
max
a a  
ff 
s s

 
 x







xxx

o  s s s   s

xx

 

p  s    ojs  a b s ffi  s    
 
 

  max    s  a b s    max
p  s  js  a b s ffi  s    
a a s s
  s s s   s
   humdp vbi   b  




see difference exact umdp updates max
sum next step observations exchanged  causes choice vectors
humdp become independent observations  sum max operations
exchanged  observations marginalized out  recall idea swaps leads
number approximation updates  see figure    summary 
  

fivalue function approximations pomdps

    fixed strategy approximations
finite state machine  fsm  model used primarily define control strategy 
strategy require belief state updates since directly maps sequences observations
sequences actions  value function fsm strategy piecewise linear convex
found eciently number memory states  section        
policy iteration policy approximation contexts value function specific strategy
used quantify goodness policy first place  value function alone
used substitute optimal value function  case  value function
 defined belief space  equals
v c  b    max v c  x  b  
x m

p

v c  x  b    s s v c  x  s b s  obtained solving set js jjm j linear equations
 section         remarked earlier  value fixed strategy lower bounds
optimal value function  v c v  
simplify comparison fixed strategy approximation approximations 
rewrite solution terms fixed strategy updates
 
 x

 
 

xxx

vbi    b    max    s   x  b s   
p  o  s  js   x  b s ffi   x  o   s      
x m s s
o  s s s   s
 
 
 x

 

xx

 

 

  max   b s    s   x    
p  o  s  js   x  ffi   x  o   s     
x m s s
o  s   s
   hf sm vbi   b  

value function vbi piecewise linear convex consists jm j linear functions
ffi  x      infinite horizon discounted case ffi  x  s  represents ith approximation
v c  x  s   note update applied finite horizon case straightforward
way 
      quality control

assume fsm strategy would use substitute optimal
control policy  three different ways use extract control 
first simply execute strategy represented fsm  need
update belief states case  second possibility choose linear functions
corresponding different memory states associated actions repeatedly every
step  refer controller direct  dr  controller  approach requires
updating belief states every step  hand control performance
worse fsm control  final strategy discards information
actions extracts policy using value function vb  b  one step lookahead 
method  la  requires belief state updates lookaheads leads worst
reactive time  dr  however  strategy guaranteed worse fsm
controller  following theorem relates performances three controllers 
  

fihauskrecht

control performance

reaction times

  

     
  

  

time  sec 

score

    

  

     
    

  

     

  

 

      
dr controller

fsm controller

fsm controller

la controller

dr controller

la controller

figure     comparison three different controllers  fsm  dr la  maze  
problem collection one action policies  control quality  left  response time  right   error bars control performance graph indicate
critical score difference two methods become statistically different
significance level      

theorem    let cf sm fsm controller  let cdr cla direct
one step lookahead controllers constructed based cf sm   v c  b  v c  b 
v c  b  v c  b  hold belief states b    
though prove direct controller lookahead controller
always better underlying fsm controller  see appendix full proof
theorem   cannot show similar property first two controllers initial
belief states  however  lookahead approach typically tends dominate  ecting
usual trade off control quality response time  illustrate trade off
running maze   example collection jaj one action policies  generating
sequence action  control quality response time results shown figure
    see controller based fsm fastest three 
worst terms control quality  hand  direct controller slower  it needs
update belief states every step  delivers better control  finally  lookahead
controller slowest best control performance 
f sm

f sm

dr

la

      selecting fsm model

quality fixed strategy approximation depends strongly fsm model used 
model provided priori constructed automatically  techniques automatic
construction fsm policies correspond search problem either complete
restricted space policies examined find optimal near optimal policy
space  search process equivalent policy approximations policy iteration
techniques discussed earlier sections         

  

fivalue function approximations pomdps

    grid based approximations value interpolation extrapolation
value function continuous belief space approximated finite set grid
points g interpolation extrapolation rule estimates value arbitrary
point belief space relying points grid associated values 
definition    interpolation extrapolation rule  let f     ir real valued function
defined information space   g   fbg    bg    bgk g set grid points g  
f bg    f  bg       bg    f  bg         bgk   f  bgk  g set point value pairs  function rg  
 i ir jgj   ir estimates f point information space using
values associated grid points called interpolation extrapolation rule 
main advantage interpolation extrapolation model estimating true value
function requires us compute value updates finite set grid points
g  let vbi approximation ith value function  approximation
 i     th value function vbi   obtained
vbi    b    rg  b  gi     
values associated every grid point bgj   g  and included gi     are 

 i    bgj      h vbi   bgj     max
a a

 

 b  a   

x
 

p  ojb  a vbi    bgj   o  a  

 

 

   

grid based update described terms value function mapping hg  
vbi     hg vbi   complexity update o jgjjajjs j  jjceval  rg   jgj  
ceval  rg   jgj  computational cost evaluating interpolation extrapolation rule
rg jgj grid points  show later  section         instances  need
evaluate interpolation extrapolation rule every step eliminated 
      family convex rules

number possible interpolation extrapolation rules enormous  focus
set convex rules relatively small important subset interpolationextrapolation rules   

definition    convex rule  let f function defined space   g   fbg    bg    bgk g
set grid points  g   f bg    f  bg       bg    f  bg         bgk   f  bgk   g set pointvalue pairs  rule rg estimating f using g called convex every b    
value fb b  is 
fb b    rg  b  g    
  bj   every j        jgj 

jgj
x
bj f  bj   

j   

pjgj

b
j    j

    

    note convex rules used work special case averagers introduced gordon        
difference minor  definition averager includes constant  independent grid points
values  added convex combination 
  

fihauskrecht

key property convex rules corresponding grid based update hg
contraction max norm  gordon         thus  approximate value iteration based
hg converges unique fixed point solution  addition  hg based convex rules
isotone 
      examples convex rules

family convex rules includes approaches commonly used practice 
nearest neighbor  kernel regression  linear point interpolations many others 
take  example  nearest neighbor approach  function belief point b
estimated using value grid point closest terms distance metric
defined belief space  then  point b  exactly one nonzero parameter
bj     k b bgj km k b bgi km holds           k 
zero  assuming euclidean distance metric  nearest neighbor approach leads
piecewise constant approximation  regions equal values correspond regions
common nearest grid point 
nearest neighbor estimates function value taking account one
grid point value  kernel regression expands upon using grid points 
adds weights contributions  values  according distance target
point  example  assuming gaussian kernels  weight grid point bgj

bj   exp kb

bg
k m      
j

p
normalizing constant ensuring jjg  j bj     parameter
attens narrows weight functions  euclidean metric  kernel regression
rule leads smooth approximation function 
linear point interpolations subclass convex rules addition constraints
definition   satisfy
jgj
x
b   bj bgj  
j   

is  belief point b convex combination grid points corresponding coecients  optimal value function pomdp convex 
new constraint sucient prove upper bound property approximation 
general  many different linear point interpolations given grid  challenging problem find rule best approximation  discuss issues
section       
      conversion grid based mdp

assume would find approximation value function using gridbased convex rule grid based update  equation     view process
process finding sequence values     bgj        bgj       i  bgj    grid points
bgj   g  show instances sequence values computed without
applying interpolation extrapolation rule every step  cases  problem
  

fivalue function approximations pomdps

converted fully observable mdp states corresponding grid points g   
call mdp grid based mdp 

theorem    let g finite set grid points rg convex rule parameters bj fixed  values   bgj   bgj   g found solving fully
observable mdp jgj states discount factor  
proof grid point bgj write 
 

 i    bgj     max  bgj   a   
a a

 
 

x
o 

x

p  ojbgj   a vbig    bgj   a  o  

 

  
jgj
 
x
g   
p  ojbgj   a    o a
 
 
b
j k k  
 

  max   bgj   a   
a a
o 
k  
 
 
  
jgj
 h
 

x
x
  max    bgj   a     gi  bgk  
p  ojbgj   a o a
j k  
a a
k  
o 
p

g g
denoting   o  p  ojbj   a g o a
j k   p  bk jbj   a   construct fully observable
mdp problem states corresponding grid points g discount factor  
update step equals 

 
 

 i    bgj     max   bgj   a   
a a

jgj
x
k  

 
 

p  bgk jbgj   a  gi  bgk     

p
prerequisite   bj   every j        jgj jjg  j bj     guarantees
p  bgk jbgj   a  interpreted true probabilities  thus  one compute values   bgj  
solving equivalent fully observable mdp   
      solving grid based approximations

idea converting grid based approximation grid based mdp basis
simple powerful approximation algorithm  brie y  key find
parameters  transition probabilities rewards  new mdp model solve
it  process relatively easy parameters used interpolate extrapolate
value non grid point fixed  the assumption theorem      case 
determine parameters new mdp eciently one step  grid set g 
nearest neighbor kernel regression examples rules property  note
leads polynomial time algorithms finding values grid points  recall
mdp solved eciently finite discounted  infinite horizon criteria  
problem solving grid based approximation arises parameters
used interpolation extrapolation fixed subject optimization
itself  happens  example  multiple ways interpolating value
    note similar result proved independently gordon        
  

fihauskrecht

point belief space would find best interpolation  leading
best values  grid points g  case  corresponding  optimal 
grid based mdp cannot found single step iterative approximation  solving
sequence grid based mdps  usually needed  worst case complexity problem
remains open question 
      constructing grids

issue touched far selection grids  multiple ways
select grids  divide two classes   regular non regular grids 
regular grids  lovejoy      a  partition belief space evenly equal size regions   
main advantage regular grids simplicity locate grid points
neighborhood belief point  disadvantage regular grids
restricted specific number points  increase grid resolution paid
exponential increase grid size  example  sequence regular grids
   dimensional belief space  corresponds pomdp    states  consists         
                   grid points    prevents one using method higher
grid resolutions problems larger state spaces 
non regular grids unrestricted thus provide exibility grid resolution must increased adaptively  hand  due irregularities  methods
locating grid points adjacent arbitrary belief point usually complex
compared regular grids 
      linear point interpolation

fact optimal value function v convex belief state mdps used
show approximation based linear point interpolation always upper bounds
exact solution  lovejoy      a         neither kernel regression nearest neighbor
guarantee us bound 

theorem     upper bound property grid based point interpolation update   let vbi
convex value function  h vbi hg vbi  
upper bound property hg update convex value functions follows directly
jensen s inequality  convergence upper bound follows theorem   
note point interpolation update imposes additional constraint choice
grid points  particular  easy see valid grid must include extreme points belief simplex  extreme points correspond                          
    regular grids used lovejoy      a  based freudenthal triangulation  eaves         essentially  idea used partition evenly n dimensional subspace ir   fact 
ane transform allows us map isomorphically grid points belief space grid points
n dimensional space  lovejoy      a  
    number points regular grid sequence given  lovejoy      a  
n

  js j    
 
jgj    m
  js j    

        grid refinement parameter 

  

fivalue function approximations pomdps

etc    without extreme points one would unable cover whole belief space via
interpolation  nearest neighbor kernel regression impose restrictions grid 
      finding best interpolation

general  multiple ways interpolate point belief space  objective
find best interpolation  is  one leads tightest upper bound
optimal value function 
let b belief point f bj   f  bj   jbj   gg set grid value pairs  best
interpolation point b is 
jgj
x
fb b    min j f  bj  
j   
p
       jgj  jjg  j j

p
subject   j   j
     b   jjg  j j bgj  
linear optimization problem  although solved polynomial time
 using linear programming techniques   computational cost still relatively
large  especially considering fact optimization must repeated many times 
alleviate problem seek ecient ways finding interpolation  sacrificing
optimality 
one way find  suboptimal  interpolation quickly apply regular grids proposed
lovejoy      a   case value belief point approximated using
convex combination grid points closest it  approximation leads piecewise linear
convex value functions  interpolations fixed here  problem finding
approximation converted equivalent grid based mdp solved
finite state mdp  however  pointed previous section  regular grids must use
specific number grid points increase resolution grid paid
exponential increase grid size  feature makes method less attractive
problem large state space need achieve high grid resolution   
present work focus non regular  or arbitrary  grids  propose interpolation approach searches limited space interpolations guaranteed run
time linear size grid  idea approach interpolate point
b belief space dimension js j set grid points consists arbitrary
grid point b    g js j   extreme points belief simplex  coecients
interpolation found eciently search best interpolation  let
b    g grid point defining one interpolation  value point b satisfies
 

bb
vbi  b    min
  vi  b  
b  g

vbib  value interpolation grid point b    figure    illustrates
resulting approximation  function characterized  sawtooth  shape 
uenced choice interpolating set 
find best value function solution close approximation apply value
iteration procedure search best interpolation every update step 
    one solution problem may use adaptive regular grids grid resolution increased
parts belief space  leave idea future work 
  

fihauskrecht

v b 

v b 
 

v b 

  b
 

b 

b 

b 

b     b s  
 

figure     value function approximation based linear time interpolation approach
 a two dimensional case   interpolating sets restricted single internal
point belief space 
drawback approach interpolations may remain unchanged many
update steps  thus slowing solution process  alternative approach solve
sequence grid based mdps instead  particular  every stage find best
 minimum value  interpolations belief points reachable grid points one step  fix
coecients interpolations  s   construct grid based mdp solve  exactly
approximately   process repeated improvement  or improvement
larger threshold  seen values different grid points 
      improving grids adaptively

quality approximation  bound  depends strongly points used grid 
objective provide good approximation smallest possible set grid
points  however  task impossible achieve  since cannot known advance
 before solving  belief points pick  way address problem build grids
incrementally  starting small set grid points adding others adaptively 
places greater chance improvement  key part approach
heuristic choosing grid points added next 
one heuristic method developed attempts maximize improvements bound
values via stochastic simulations  method builds fact every interpolation
grid must include extreme points  otherwise cannot cover entire belief space  
extreme points values affect grid points  try improve
values first place  general  value grid point b improves
precise values used successor belief points  is  belief states correspond
 b    o  choice observation o  current optimal action choice b 
incorporating points grid makes larger improvement value
initial grid point b likely  assuming initial point extreme point 
heuristic tends improve value point  naturally  one proceed
selection incorporating successor points first level successors
grid well  forth 
  

fivalue function approximations pomdps

generate new grid points  g  vb g  
set gnew   fg
extreme points b
repeat b    g   gnew
n

p
set   arg maxa  b  a    o  p  ojb  a vb g    b  a  o  
select observation according p  ojb   
update b    b    o 
add b gnew
return gnew
figure     procedure generating additional grid points based bound improvement heuristic 

bound quality
mdp

fast interpolation
qmdp
informed regular grid

interpolation
adaptive grid

interpolation
random grid

   
                                     

   

score

  
   

  

  

           

                   

  

  

figure     improvement upper bound quality grid based point interpolations
based adaptive grid method  method compared randomly
refined grid regular grid     points  upper bound approximations  the mdp  qmdp fast informed bound methods  included
comparison 
capture idea  generate new grid points via simulation  starting one
extremes belief simplex continuing belief point currently
grid reached  algorithm implements bound improvement heuristic
expands current grid g set js j new grid points relying current
value function approximation vb g shown figure    
figure    illustrates performance  bound quality  adaptive grid method
maze   problem  use combination adaptive grids linear time
interpolation approach  method gradually expands grid    point increments
    grid points  figure    shows performance random grid method
  

fihauskrecht

running times
    
    
    
   

time  sec 

    
    

   

    
   

   

    
   

   

    

       

   

    

   

   

   
    

    

     

     

   

   

   
   
     

   

 
mdp qmdp fast
informed

interpolation
regular grid

interpolation
adaptive grid

interpolation
random grid

figure     running times grid based point interpolation methods  methods tested include adaptive grid  random grid  regular grid     grid
points  running times adaptive grid cumulative  ecting dependencies higher grid resolutions lower level resolutions  running
time results mdp  qmdp  fast informed bound approximations
shown comparison 
new points grid selected iniformly random  results    grid point increments
shown   addition  figure gives results regular grid interpolation  based
lovejoy      a       belief points upper bound methods  mdp 
qmdp fast informed bound approximations 
see dramatic improvement quality bound adaptive method 
contrast this  uniformly sampled grid  random grid approach  hardly changes
bound  two reasons this      uniformly sampled grid points likely
concentrated center belief simplex      transition matrix maze  
problem relatively sparse  belief points one obtains extreme points one
step boundary simplex  since grid points center simplex
never used interpolate belief states reachable extremes one step cannot
improve values extremes bound change 
one drawback adaptive method running time  for every grid size need
solve sequence grid based mdps   figure    compares running times different
methods maze   problem  grid expansion adaptive method depends
value function obtained previous steps  plot cumulative running times 
see relatively large increase running time  especially larger grid sizes  ecting
trade off bound quality running time  however  note
adaptive grid method performs quite well initial steps     grid
points outperforms regular grid  with     points  bound quality 
finally  note heuristic approaches constructing adaptive grids point
interpolation possible  example  different approach refines grid ex 

  

fivalue function approximations pomdps

control performance
  

score

  

   

  

   
          

  
  

   
  

   
   

   

  

  

  

fast
interpolation interpolation
mdp qmdp informed  regular grid   adaptive grid 

interpolation
 random grid 

nearest neighbor nearest neighbor
 adaptive grid 
 random grid 

figure     control performance lookahead controllers based grid based point interpolation nearest neighbor methods varying grid sizes  results
compared mdp  qmdp fast informed bound controllers 
amining differences values current grid points recently proposed brafman
       
      control

value functions obtained different grid based methods define variety controllers  figure    compares performances lookahead controllers based point interpolation
nearest neighbor methods  run two versions approaches  one adaptive grid  random grid  show results obtained            
grid points  addition  compare performances interpolation regular
grids  with     grid points   mdp  qmdp fast informed bound approaches 
overall  performance interpolation extrapolation techniques tested
maze   problem bit disappointing  particular  better scores achieved
simpler qmdp fast informed bound methods  see that  although heuristics
improved bound quality approximations  lead similar improvement
qmdp fast informed bound methods terms control  result
shows bad bound  in terms absolute values  always imply bad control
performance  main reason control performance uenced mostly
relative rather absolute value function values  or  words  shape
function   interpolation extrapolation techniques use  except regular grid
interpolation  approximate value function functions piecewise linear
convex  interpolations based linear time interpolation technique
sawtooth shaped function  nearest neighbor leads piecewise constant function 
allow match shape optimal function correctly 
factor affects performance large sensitivity methods selection grid
points  documented  example  comparison heuristic random grids 

  

fihauskrecht

tests focused lookahead controllers only  however  alternative way
define controller grid based interpolation extrapolation methods use q function
approximations instead value functions  either direct lookahead designs    qfunction approximations found solving grid based mdp  keeping
values  functions  different actions separate end 

    approximations value functions using curve fitting  least squares
fit 
alternative way approximate function continuous space use curve fitting
techniques  approach relies predefined parametric model value function
set values associated finite set  grid  belief points g  approach
similar interpolation extrapolation techniques relies set belief value
pairs  difference curve fitting  instead remembering belief value pairs 
tries summarize terms given parametric function model  strategy seeks
best possible match model parameters observed point values  best
match defined using various criteria  often least squares fit criterion 
objective minimize
error f    

 x
 y
  j j

f  bj      

bj yj correspond belief point associated value  index j ranges
points sample set g 
      combining dynamic programming least squares fit

least squares approximation function used construct dynamic programming
algorithm update step  vbi     hlsf vbi   approach two steps  first 
obtain new values set sample points g 

 i    b     h vbi   b    max
a a

 

x
s s

 s  a b s   

xx
o  s s

 
b
p  ojs  a b s vi    b  a  o    

second  fit parameters value function model vbi   using new sample value pairs
square error cost function  complexity update o jgjjajjs j  jjceval  vbi   
cfit  vbi     jgj   time  ceval vbi   computational cost evaluating vbi
cfit  vbi     jgj  cost fitting parameters vbi   jgj belief value pairs 
advantage approximation based least squares fit requires us
compute updates finite set belief states  drawback approach
that  combined value iteration method  lead instability and or
divergence  shown mdps several researchers  bertsekas        boyan
  moore        baird        tsitsiklis   roy        
    similar qmdp method  allows lookahead greedy designs  fact  qmdp
viewed special case grid based method q function approximations  grid
points correspond extremes belief simplex 
  

fivalue function approximations pomdps

      on line version least squares fit

problem finding set parameters best fit solved available
optimization procedure  includes on line  or instance based  version gradient
descent method  corresponds well known delta rule  rumelhart  hinton   
williams        
let f denote parametric value function belief space adjustable weights
w   fw    w      wk g  on line update weight wi computed as 

wi

wi ffi  f  bj   yj  

 f
j  
 wi b
j

ffi learning constant  bj yj last seen point value  note
gradient descent method requires function differentiable regard
adjustable weights 
solve discounted infinite horizon problem  stochastic  on line  version
least squares fit combined either parallel  synchronous  incremental  gaussseidel  point updates  first case  value function previous step fixed
new value function computed scratch using set belief point samples
values computed one step expansion  parameters stabilized  by
attenuating learning rates   newly acquired function fixed  process proceeds
another iteration  incremental version  single value function model
time updated used compute new values sampled points  littman et al        
parr russell        implement approach using asynchronous reinforcement
learning backups sample points updated next obtained via stochastic
simulation  stress versions subject threat instability divergence 
remarked above 
      parametric function models

apply least squares approach must first select appropriate value function
model  examples simple convex functions linear quadratic functions 
complex models possible well 
one interesting relatively simple approach based least squares approximation linear action value functions  q functions   littman et al         
value function vbi   approximated piecewise linear convex combination qb i  
functions 
vbi    b    max qb i    b  a  
a a
qb i    b  a  least squares fit linear function set sample points g 
values points g obtained

 ai    b     b  a   

x

o 

p  ojb  a vbi    b  o  a   

method leads approximation jaj linear functions coecients
functions found eciently solving set linear equations  recall two
approximations  the qmdp fast informed bound approximations  work
  

fihauskrecht

jaj linear functions  main differences methods qmdp

fast informed bound methods update linear functions directly  guarantee upper
bounds unique convergence 
sophisticated parametric model convex function softmax model  parr
  russell        
 

vb  b     

 
x x

ff 

s s

 k    
ff s b s   

k

 

set linear functions adaptive parameters fit k  temperature  parameter provides better fit underlying piecewise linear convex function
larger values  function represents soft approximation piecewise linear convex
function  parameter k smoothing approximation 
      control

tested control performance least squares approach linear q function
model  littman et al         softmax model  parr   russell         softmax
model varied number linear functions  trying cases       linear functions
respectively  first set experiments used parallel  synchronous  updates
samples fixed set     belief points  applied stochastic gradient descent techniques
find best fit cases  tested control performance value function
approximations obtained           updates  starting qmdp solution 
second set experiments  applied incremental stochastic update scheme
gauss seidel style updates  results method acquired every grid point
updated     times  learning rates decreasing linearly range    
       started qmdp solution  results lookahead controllers
summarized figure     shows control performance direct q function
controller and  comparison  results qmdp method 
linear q function model performed well results lookahead design
better results qmdp method  difference quite apparent
direct approaches  general  good performance method attributed
choice function model let us match shape optimal value function
reasonably well  contrast  softmax models  with       linear functions 
perform expected  probably softmax model linear functions
updated every sample point  leads situations multiple linear functions
try track belief point update  circumstances hard capture
structure optimal value function accurately  negative feature
effects on line changes linear functions added softmax approximation 
thus could bias incremental update schemes  ideal case  would identify
one vector responsible specific belief point update  modify  vector 
linear q function approach avoids problem always updating single linear
function  corresponding action  

  

fivalue function approximations pomdps

control performance
  

  
lookahead

     
iter iter    stoch
iter

     
iter iter

        
iter iter iter

score

  

  

stoch

  
iter stoch

  
   iter
iter

  
iter

stoch

direct

  

  
qmdp
approximation

linear q function
lookahead

linear q function
direct

softmax
    linear functions 

softmax
    linear functions 

figure     control performance least squares fit methods  models tested include  linear
q function model  with direct lookahead control  softmax models       linear functions  lookahead control only   value functions
obtained           synchronous updates value functions obtained
incremental stochastic update scheme used define different
controllers  comparison  include results two qmdp controllers 

    grid based approximations linear function updates
alternative grid based approximation method constructed applying sondik s
approach computing derivatives  linear functions  points grid  lovejoy      a 
       let vbi piecewise linear convex function described set linear functions  
new linear function belief point b action computed eciently
 smallwood   sondik        littman       
ffb a
i    s     s  a   

xx
o  s   s

p  s    ojs  a ffi b a o   s    

 b  a  o  indexes linear function ffi set linear functions
maximizes expression
 
x x

s   s s s

    


 defining vbi  

 

p  s    ojs  a b s  ffi  s   

fixed combination b  a  o  optimizing function b acquired choosing
vector best overall value action vectors  is  assuming bi  
set candidate linear functions  resulting functions satisfies
  arg max x ffb  s b s  
ffb i  
i  
        s s
collection linear functions obtained set belief points combined
piecewise linear convex value function  idea behind number exact
b


b


  

fihauskrecht

v b 
 

v b 
v b 
new linear function

 

b

 

b s   

figure     incremental version grid based linear function method  piecewise
linear lower bound improved new linear function computed belief
point b using sondik s method 
algorithms  see section         however  exact case  set points cover
linear functions defining new value function must located first  hard task
itself  contrast  approximation method uses incomplete set belief points
fixed least easy locate  example via random heuristic selection  use
hgl denote value function mapping grid approach 
advantage grid based method leads ecient updates 
time complexity update polynomial equals o jgjjajjs j  jj   yields set
jgj linear functions  compared jajj jjj possible functions exact update 
since set grid points incomplete  resulting approximation lower bounds
value function one would obtain performing exact update  lovejoy      a  

theorem     lower bound property grid based linear function update   let vbi
piecewise linear value function g set grid points used compute linear function
updates  hgl vbi h vbi  
      incremental linear function approach

drawback grid based linear function method hgl contraction
discounted infinite horizon case  therefore value iteration method based
mapping may converge  lovejoy      a   remedy problem  propose
incremental version grid based linear function method  idea refinement
prevent instability gradually improving piecewise linear convex lower bound
value function 
assume vbi v convex piecewise linear lower bound optimal value
function defined linear function set   let ffb linear function point b
computed vbi using sondik s method  one construct new improved
value function vbi   vbi simply adding new linear function ffb   is 
i       ffb   idea incremental update  illustrated figure     similar
incremental methods used cheng        lovejoy         method
  

fivalue function approximations pomdps

running times

bound quality

    

  
 

    

  

  

 

score

  
  
  
  

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

time  sec 

 

  

 

 

    

  

 

 
 
    

 

 
 

 

 
 

   

 

 

 

  
    

     

 

 

 

 

  
standard approach

qmdp

incremental approach

fast
informed

standard approach

incremental approach

figure     bound quality running times standard incremental version
grid based linear function method fixed    point grid  cumulative
running times  including previous update cycles  shown methods 
running times qmdp fast informed bound methods included
comparison 
extended handle set grid points g straightforward way  note
adding one new linear functions   previous linear functions may
become redundant removed value function  techniques redundancy
checking applied exact approaches  monahan        eagle        
incremental refinement stable converges fixed set grid points 
price paid feature linear function set grow size iteration
steps  although growth linear number iterations  compared
potentially exponential growth exact methods  linear function set describing
piecewise linear approximation become huge  thus  practice usually stop
incremental updates well method converges  question remains open
complexity  hardness  problem finding fixed point solution fixed set
grid points g 
figure    illustrates trade offs involved applying incremental updates
compared standard fixed grid approach maze   problem  use
grid    points techniques initial value function  results     
update cycles shown  see incremental method longer running times
standard method  since number linear functions grow every update 
hand  bound quality incremental method improves rapidly
never become worse update steps 
      minimum expected reward

incremental method improves lower bound value function  value function  say vbi   used create controller  with either lookahead direct action
choice   general case  cannot say anything performance quality
controllers regard vbi   however  certain conditions performance
controllers guaranteed never fall vbi   following theorem  proved
appendix  establishes conditions 

theorem    let vbi value function obtained via incremental linear function method 
starting vb    corresponds fixed strategy c    let cla i cdr i two
  

fihauskrecht

controllers based vbi   lookahead controller direct action controller  v c  
vc
respective value functions  vbi v c
vbi v c
hold 
note property holds incremental version exact value iteration 
is  lookahead direct controllers perform worse vi obtained
incremental updates v  corresponding fsm controller c   
la i

dr i

la i

dr i

      selecting grid points

incremental version grid based linear function approximation exible
works arbitrary grid    moreover  grid need fixed changed
line  thus  problem finding grids reduces problem selecting belief points
updated next  one apply various strategies this  example  one use
fixed set grid points update repeatedly  one select belief points line
using various heuristics 
incremental linear function method guarantees value function always
improved  all linear functions previous steps kept unless found redundant  
quality new linear function  to added next  depends strongly quality
linear functions obtained previous steps  therefore  objective select order
points better chances larger improvement  designed two heuristic
strategies selecting ordering belief points 
first strategy attempts optimize updates extreme points belief simplex
ordering heuristically  idea heuristic based fact states
higher expected rewards  e g  designated goal states  backpropagate effects
 rewards  locally  therefore  desirable states neighborhood highest
reward state updated first  distant ones later  apply idea order
extreme points belief simplex  relying current estimate value function
identify highest expected reward states pomdp model determine
neighbor states 
second strategy based idea stochastic simulation  strategy generates
sequence belief points likely reached  fixed  initial belief point 
points sequence used reverse order generate updates  intent
heuristic  maximize  improvement value function initial fixed
point  run heuristic  need find initial belief point set initial belief
points  address problem  use first heuristic allows us order
extreme points belief simplex  points used initial beliefs
simulation part  thus  two tier strategy  top level strategy orders extremes
belief simplex  lower level strategy applies stochastic simulation generate
sequence belief states likely reachable specific extreme point 
tested order heuristics two tier heuristics maze   problem 
compared two simple point selection strategies  fixed grid strategy 
set    grid points updated repeatedly  random grid strategy 
points always chosen uniformly random  figure    shows bound quality
    restriction grid points must included grid  required
example linear point interpolation scheme  use extreme points belief
simplex 
  

fivalue function approximations pomdps

bound quality
  
  

score

  
 

  

            
   
 

 

 

 
     

      
 

 

 

 

        
   

 

 

 

 

  

        
     

  
  
  
fixed grid

random grid

order heuristic

  tier heuristic

figure     improvements bound quality incremental linear function method
four different grid selection heuristics  cycle includes    grid point
updates 
methods    update cycles  each cycle consists    grid point updates 
maze   problem  see differences quality value function approximations
different strategies  even simple ones  relatively small  note
observed similar results problems  maze   
relatively small improvement heuristics explained fact
every new linear function uences larger portion belief space thus method
less sensitive choice specific point    however  another plausible explanation heuristics good accurate heuristics combinations
heuristics could constructed  ecient strategies locating grid points used
exact methods  e g  witness algorithm  kaelbling et al         cheng s methods  cheng        potentially applied problem  remains open area
research 
      control

grid based linear function approach leads piecewise linear convex approximation  every linear function comes natural action choice lets us choose
action greedily  thus run lookahead direct controllers  figure   
compares performance four different controllers fixed grid    points  combining standard incremental updates lookahead direct greedy control   
     update cycles  results  see figure     illustrate trade offs
computational time obtaining solution quality  see incremental
approach lookahead controller design tend improve control performance 
prices paid worse running reaction times  respectively 
    small sensitivity incremental method selection grid points would suggest one
could  many instances  replace exact updates simpler point selection strategies  could
increase speed exact value iteration methods  at least initial stages   suffer
ineciencies associated locating complete set grid points updated every step  however 
issue needs investigated 
  

fihauskrecht

control performance
  

 

  

  

  
 

lookahead

lookahead

 

  

 

  

 

 
 

 

score

  
direct
  

direct

  

  
qmdp

fast
informed

direct
standard

lookahead
standard

direct
incremental

lookahead
incremental

figure     control performance four different controllers based grid based linear function updates         update cycles    point grid  controllers represent combinations two update strategies  standard incremental  two action extraction techniques  direct lookahead   running
times two update strategies presented figure     comparison include performances qmdp fast informed bound
methods  with direct lookahead designs  
control performance
  
 

score

 

  

  

 

 

  

 

 

  
 

 

  

  

  

  

  
qmdp

fast
informed

fixed grid

random grid

order heuristic

  tier heuristic

figure     control performances lookahead controllers based incremental linearfunction approach different point selection heuristics         improvement cycles  comparison  scores qmdp fast informed
bound approximations shown well 
figure    illustrates effect point selection heuristics control  compare
results lookahead control only  using approximations obtained         improvement cycles  each cycle consists    grid point updates   test results show that 

  

fivalue function approximations pomdps

bound quality  big differences among various heuristics  suggesting
small sensitivity control selection grid points 

    summary value function approximations
heuristic value function approximations methods allow us replace hard to compute exact
methods trade solution quality speed  numerous methods employ  different properties different trade offs quality versus speed  tables  
  summarize main theoretical properties approximation methods covered
paper  majority methods polynomial complexity least ecient  polynomial  bellman updates  makes good candidates complex
pomdp problems reach exact methods 
methods heuristic approximations give solutions
guaranteed precision  despite fact proved solutions methods
worse others terms value function quality  see figure      one
main contributions paper  however  currently minimal theoretical results
relating methods terms control performance  exception results
fsm controllers fsm based approximations  key observation
quality control  lookahead control  important approximate shape
 derivatives  value function correctly  illustrated empirically gridbased interpolation extrapolation methods section       based non convex
value functions  main challenges find ways analyzing comparing
control performance different approximations theoretically identify classes
pomdps certain methods dominate others 
finally  note list methods complete value function approximation methods refinements existing methods possible  example  white
scherer        investigate methods based truncated histories lead upper
lower bound estimates value function complete information states  complete
histories   also  additional restrictions methods change properties
generic method  example  possible additional assumptions
able ensure convergence least squares fit approximation 
   conclusions

pomdps offers elegant mathematical framework representing decision processes
stochastic partially observable domains  despite modeling advantages  however 
pomdp problems hard solve exactly  thus  complexity problem solvingprocedures becomes key aspect sucessful application model real world
problems  even expense optimality  recent complexity results
approximability pomdp problems encouraging  lusena et al         madani
et al          focus heuristic approximations  particular approximations value
functions 

  

fihauskrecht

method
mdp approximation
qmdp approximation
fast informed bound
umdp approximation
fixed strategy method
grid based interpolation extrapolation
nearest neighbor
kernel regression
linear point interpolation
curve fitting  least squares fit 
linear q function
grid based linear function method
incremental version  start lower bound 

bound
upper
upper
upper
lower
lower
upper
lower
lower

isotonicity

p
p
p
p
p
 p
p
p

contraction

 p

  

p
p
p
p
p
 p
p
p

table    properties different value function approximation methods  bound property 
isotonicity contraction property underlying mappings       
    although incremental version grid based linear function method
contraction always converges 
method
mdp approximation
qmdp approximation
fast informed bound
umdp approximation
fixed strategy method
grid based interpolation extrapolation
nearest neighbor
kernel regression
linear point interpolation
fixed interpolation
best interpolation
curve fitting  least squares fit 
linear q function
grid based linear function method
incremental version

finite horizon
p
p
p
np hard
p
varies
p
p
p
p
p
varies
p
p
na

discounted infinite horizon
p
p
p
undecidable
p
na
p
p
varies
p
 
na
na
na
 

table    complexity value function approximation methods finite horizon problem
discounted infinite horizon problem  objective discounted infinitehorizon case find corresponding fixed point solution  complexity
results take account  addition components pomdps 
approximation specific parameters  e g   size grid g grid based methods    indicates open instances na methods applicable one
problems  e g  possible divergence  

  

fivalue function approximations pomdps

    contributions
paper surveys new known value function approximation methods solving pomdps 
focus primarily theoretical analysis comparison methods  findings results supported experimentally problem moderate size agent
navigation domain  analyze methods different perspectives  computational complexity  capability bound optimal value function  convergence properties
iterative implementations  quality derived controllers  analysis includes new
theoretical results  deriving properties individual approximations  relations
exact methods  general  relations trade offs among different methods
well understood  provide new insights issues analyzing
corresponding updates  example  showed differences among exact 
mdp  qmdp  fast informed bound  umdp methods boil simple
mathematical manipulations subsequent effect value function approximation  allowed us determine relations among different methods terms quality
respective value functions one main results paper 
presented number new methods heuristic refinements existing
techniques  primary contributions area include fast informed bound  gridbased point interpolation methods  including adaptive grid approaches based stochastic sampling   incremental linear function method  showed
instances solutions obtained eciently converting original approximation equivalent finite state mdp  example  grid based approximations
convex rules often solved via conversion grid based mdp  in grid points
correspond new states   leading polynomial complexity algorithm finite discounted infinite horizon cases  section         result dramatically
improve run time performance grid based approaches  similar conversion
equivalent finite state mdp  allowing polynomial time solution discounted
infinite horizon problem  shown fast informed bound method  section      
    challenges future directions
work pomdps approximations far complete  complexity results
remain open  particular  complexity grid based approach seeking best interpolation  complexity finding fixed point solution incremental version
grid based linear function method  another interesting issue needs investigation convergence value iteration least squares approximation  although
method unstable general case  possible certain restrictions
converge 
paper use single pomdp problem  maze    support theoretical
findings illustrate intuitions  therefore  results supported theoretically  related mostly control  cannot generalized used rank different methods 
since performance may vary problems  general  area pomdps
pomdp approximations suffers shortage larger scale experimental work
multiple problems different complexities broad range methods  experimental
work especially needed study compare different methods regard control
quality  main reason theoretical results relating
  

fihauskrecht

control performance  studies help focus theoretical exploration discovering
interesting cases possibly identifying classes problems certain approximations less suitable  preliminary experimental results show
significant differences control performance among different methods
may suitable approximate control policies  example  grid based
nearest neighbor approach piecewise constant approximation typically inferior
outperformed simpler  and ecient  value function methods 
present work focused heuristic approximation methods  investigated general   at  pomdps take advantage additional structural refinements 
however  real world problems usually offer structure exploited devise
new algorithms perhaps lead speed ups  possible
restricted versions pomdps  with additional structural assumptions  solved
approximated eciently  even though general complexity results pomdps approximations encouraging  papadimitriou   tsitsiklis        littman       
mundhenk et al         lusena et al         madani et al          challenge
identify models allow ecient solutions time interesting enough
point application 
finally  number interesting issues arise move problems large state 
action  observation spaces  here  complexity value function updates
belief state updates becomes issue  general  partial observability hidden
process states allow us factor decompose belief states  and updates  
even transitions great deal structure represented compactly 
promising directions deal issues include various monte carlo approaches  isard
  blake        kanazawa  koller    russell        doucet        kearns et al          
methods approximating belief states via decomposition  boyen   koller              
combination two approaches  mcallester   singh        
acknowledgements

anthony cassandra  thomas dean  leslie kaelbling  william long  peter szolovits
anonymous reviewers provided valuable feedback comments work  research
supported grant ro  lm       grant  t  lm      national library
medicine  dod advanced research project agency  arpa  contract number
n         m      darpa rome labs planning initiative grant f                
appendix a  theorems proofs

a   convergence bound
theorem   let h  h  two value function mappings defined v  v  s t 
   h    h  contractions fixed points v    v   
   v    v  h  v  h  v    v   

   h  isotone mapping 
v  v  holds 
  

fivalue function approximations pomdps

proof applying h  condition   expanding result condition  
get  h   v  h  v  h  v    v    repeating get limit v  h n v 
h   v  h v  h v    v    proves result   
a   accuracy lookahead controller based bounds
theorem   let vbu vbl upper lower bounds optimal value function
discounted infinite horizon problem  let   supb jvbu  b  vbl  b j   kvbu vbl k
maximum bound difference  expected reward lookahead controller vb la  
constructed either vbu vbl   satisfies kvb la v k          

proof let vb denotes either upper lower bound approximation v h la
value function mapping corresponding lookahead policy vb   note  since
lookahead policy always optimizes actions regard vb   h vb   h la vb must hold 
error vb la bounded using triangle inequality

kvb la v k kvb la vb k   kvb v k 
first component satisfies 

kvb la vb k   kh la vb la vb k
kh la vb la h vb k   kh vb vb k
  kh la vb la h la vb k   kh vb vb k
kvb la vb k  
inequality  kh vb vb k follows isotonicity h fact vb either
upper lower bound  rearranging inequalities  obtain  kvb la vb k         
bound second term kvb v k trivial 
    
therefore  kvb la v k                   
     
a   mdp  qmdp fast informed bounds
theorem   solution fast informed bound approximation found solving
mdp js jjajjj states  jaj actions discount factor  
proof let ffai linear function action defining vbi   let ffi  s  a  denote parameters
function  parameters vbi   satisfy 
ffi    s  a     s  a   
let

x

x
 
   
max
   a   p  s   ojs  a ffi  s     

o 
 s

ffi    s  a  o    max
 

x

  s   

  

p  s    ojs  a ffi  s    a    

fihauskrecht

now  rewrite ffi    s  a  o  every s  a  as 
 
 x

 

x

ffi    s  a  o    max
p  s    ojs  a    s    a     
a   a  s   s
o   
 
 
  x
 
  max
a   a    

 

 

  
 
ffi  s    a    o     

p  s    ojs  a  s    a        

x x

o    s   s

 s

  
 
p  s    ojs  a ffi  s    a    o     

equations define mdp state space   action space discount
factor   thus  solution fast informed bound update found solving
equivalent finite state mdp   

theorem   let vbi corresponds piecewise linear convex value function defined
linear functions  h vbi hf ib vbi hqmdp vbi hmdp vbi  
proof
 
 x

x



 
 

xx

max    s  a b s   
max
p  s    ojs  a b s ffi  s    
a a s s

 
 
o 
 s s s
   hvi   b 


max
a a

x
s s

 

b s    s  a   

   hf ib vi   b 

max
a a

x
s s

b s    s  a   



s s

a a

 

   hmdp vbi   b 

 

p  s    ojs  a ffi  s    
 

x
s   s

 

b s  max   s  a   

max

x

o  ffi   s   s

 

   hqmdp vbi   b 
x

x



p  s  js  a  max ffi  s    
ffi  



 

x
s   s

p  s  js  a  max ffi  s    
ffi  



a   fixed strategy approximations
theorem    let cf sm fsm controller  let cdr cla direct
one step lookahead controllers constructed based cf sm   v c  b  v c  b 
v c  b  v c  b  hold belief states b    
proof value function fsm controller cf sm satisfies 
f sm

f sm

la

vc

f sm



 b    max v  x  b    v    b   b 
x m

v  x  b     b   x    

x
o 

p  ojb   x  v   x  o    b   x   o   
  

dr

fivalue function approximations pomdps

direct controller cdr selects action greedily every step  is  always
chooses according  b    arg maxx m v  x  b   lookahead controller cla selects
action based v  x  b  one step away 

la  b    arg max
a a

 

 

x

 
 b  a    p  ojb  a  max
   m v  x    b  a  o    
x
o 

expanding value function cf sm one step get 

vc

f sm

 b    max v  x  b 
x m
 

 

x

  max  b   x     p  ojb   x  v   x  o    b   x   o  
x m
o 
   b     b     

 b     b     
 

x

o 

x

o 

   

p  ojb     b   v   x  o    b     b    o  

p  ojb     b    max
v  x     b     b    o  
 
x  m

x

   

 

 
max
 b  a    p  ojb  a  max
   m v  x    b  a  o  
a a
x
o 
x
la
 
la
   b   b     p  ojb  la  b   max
   m v  x    b   b   o  
x
o 

   

iteratively expanding maxx   m v  x         expression   substituing improved
 higher value  expressions     back obtain value functions direct
lookahead controllers   expansions   lead value direct controller
expansions   value lookahead controller   thus v c
vc
c
c
v
v must hold  note  however  action choices  b  la b 
expressions     different leading different next step belief states
subsequently different expansion sequences  therefore  result imply
v dr  b  v la  b  b      
f sm

f sm

dr

la

a   grid based linear function method
theorem    let vbi value function obtained via incremental linear function method 
starting vb    corresponds fixed strategy c    let cla i cdr i two
controllers based vbi   lookahead controller direct action controller  v c  
vc
respective value functions  vbi v c
vbi v c
hold 
proof initializing method value function fsm controller c   
incremental updates interpreted additions new states fsm controller  a
new linear function corresponds new state fsm   let ci controller
step i  v c
  vbi holds inequalities follow theorem      
la i

dr i

la i

f sm i

  

dr i

fihauskrecht

references

astrom  k  j          optimal control markov decision processes incomplete state
estimation  journal mathematical analysis applications              
baird  l  c          residual algorithms  reinforcement learning function approximation  proceedings twelfth international conference machine learning 
pp        
barto  a  g   bradtke  s  j     singh  s  p          learning act using real time dynamic
programming  artificial intelligence             
bellman  r  e          dynamic programming  princeton university press  princeton  nj 
bertsekas  d  p          counter example temporal differences learning  neural computation             
bertsekas  d  p          dynamic programming optimal control  athena scientific 
bonet  b     geffner  h          learning sorting classification pomdps 
proceedings fifteenth international conference machine learning 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  artificial intelligence           
boutilier  c     poole  d          exploiting structure policy construction  proceedings
thirteenth national conference artificial intelligence  pp            
boyan  j  a     moore  a  a          generalization reinforcement learning  safely
approximating value function  advances neural information processing
systems    mit press 
boyen  x     koller  d          tractable inference complex stochastic processes 
proceedings fourteenth conference uncertainty artificial intelligence  pp 
      
boyen  x     koller  d          exploiting architecture dynamic systems  proceedings sixteenth national conference artificial intelligence  pp          
brafman  r  i          heuristic variable grid solution method pomdps  proceedings fourteenth national conference artificial intelligence  pp          
burago  d   rougemont  m  d     slissenko  a          complexity partially
observed markov decision processes  theoretical computer science               
cassandra  a  r          exact approximate algorithms partially observable markov
decision processes  ph d  thesis  brown university 
cassandra  a  r   littman  m  l     zhang  n  l          incremental pruning  simple 
fast  exact algorithm partially observable markov decision processes  proceedings
thirteenth conference uncertainty artificial intelligence  pp        
  

fivalue function approximations pomdps

casta non  d          approximate dynamic programming sensor management 
proceedings conference decision control 
cheng  h  t          algorithms partially observable markov decision processes  ph d 
thesis  university british columbia 
condon  a          complexity stochastic games  information computation 
            
dean  t     kanazawa  k          model reasoning persistence causation 
computational intelligence             
dearden  r     boutilier  c          abstraction approximate decision theoretic planning  artificial intelligence              
doucet  a          sequential simulation based methods bayesian filtering  tech 
rep  cued f infeng tr      department engineering  cambridge university 
drake  a          observation markov process noisy channel  ph d  thesis 
massachusetts institute technology 
draper  d   hanks  s     weld  d          probabilistic planning information gathering
contingent execution  proceedings second international conference
ai planning systems  pp        
eagle  j  n          optimal search moving target search path constrained 
operations research                
eaves  b          course triangulations soving differential equations deformations  springer verlag  berlin 
gordon  g  j          stable function approximation dynamic programming  proceedings twelfth international conference machine learning 
hansen  e       a   improved policy iteration algorithm partially observable mdps 
advances neural information processing systems     mit press 
hansen  e       b   solving pomdps searching policy space  proceedings
fourteenth conference uncertainty artificial intelligence  pp          
hauskrecht  m          planning control stochastic domains imperfect information  ph d  thesis  massachusetts institute technology 
hauskrecht  m     fraser  h          planning medical therapy using partially observable
markov decision processes  proceedings ninth international workshop
principles diagnosis  dx      pp          
hauskrecht  m     fraser  h          planning treatment ischemic heart disease
partially observable markov decision processes  artificial intelligence medicine     
        
  

fihauskrecht

heyman  d     sobel  m          stochastic methods operations research  stochastic
optimization  mcgraw hill 
howard  r  a          dynamic programming markov processes  mit press  cambridge 
howard  r  a     matheson  j          uence diagrams  principles applications
decision analysis    
isard  m     blake  a          contour tracking stochastic propagation conditional
density  proccedings europian conference computer vision  pp          
kaelbling  l  p   littman  m  l     cassandra  a  r          planning acting
partially observable stochastic domains  artificial intelligence              
kanazawa  k   koller  d     russell  s  j          stochastic simulation algorithms
dynamic probabilistic networks  proceedings eleventh conference uncertainty artificial intelligence  pp          
kearns  m   mansour  y     ng  a  y          sparse sampling algorithm near
optimal planning large markov decision processes  proceedings sixteenth
international joint conference artificial intelligence  pp            
kjaerulff  u          computational scheme reasoning dynamic probabilistic networks  proceedings eighth conference uncertainty artificial intelligence  pp          
korf  r          depth first iterative deepening  optimal admissible tree search  artificial
intelligence             
kushmerick  n   hanks  s     weld  d          algorithm probabilistic planning 
artificial intelligence              
lauritzen  s  l          graphical models  clarendon press 
littman  m  l          memoryless policies  theoretical limitations practical results 
cliff  d   husbands  p   meyer  j     wilson  s   eds    animals animats    proceedings third international conference simulation adaptive
behavior  mit press  cambridge 
littman  m  l          algorithms sequential decision making  ph d  thesis  brown
university 
littman  m  l   cassandra  a  r     kaelbling  l  p          learning policies partially
observable environments  scaling up  proceedings twelfth international
conference machine learning  pp          
lovejoy  w  s       a   computationally feasible bounds partially observed markov
decision processes  operations research              
  

fivalue function approximations pomdps

lovejoy  w  s       b   survey algorithmic methods partially observed markov
decision processes  annals operations research            
lovejoy  w  s          suboptimal policies bounds parameter adaptive decision
processes  operations research              
lusena  c   goldsmith  j     mundhenk  m          nonapproximability results markov
decision processes  tech  rep   university kentucky 
madani  o   hanks  s     condon  a          undecidability probabilistic planning
infinite horizon partially observable markov decision processes  proceedings
sixteenth national conference artificial intelligence 
mcallester  d     singh  s  p          approximate planning factored pomdps using
belief state simplification  proceedings fifteenth conference uncertainty
artificial intelligence  pp          
mccallum  r          instance based utile distinctions reinforcement learning
hidden state  proceedings twelfth international conference machine
learning 
monahan  g  e          survey partially observable markov decision processes  theory 
models  algorithms  management science           
mundhenk  m   goldsmith  j   lusena  c     allender  e          encyclopaedia complexity results finite horizon markov decision process problems  tech  rep   cs
dept tr         university kentucky 
papadimitriou  c  h     tsitsiklis  j  n          complexity markov decision processes  mathematics operations research              
parr  r     russell  s          approximating optimal policies partially observable
stochastic domains  proceedings fourteenth international joint conference
artificial intelligence  pp            
pearl  j          probabilistic reasoning intelligent systems  morgan kaufman 
platzman  l  k          finite memory estimation control finite probabilistic systems 
ph d  thesis  massachusetts institute technology 
platzman  l  k          feasible computational approach infinite horizon partiallyobserved markov decision problems  tech  rep   georgia institute technology 
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley  new york 
raiffa  h          decision analysis  introductory lectures choices uncertainty 
addison wesley 
rumelhart  d   hinton  g  e     williams  r  j          learning internal representations
error propagation  parallel distributed processing  pp          
  

fihauskrecht

satia  j     lave  r          markovian decision processes probabilistic observation
states  management science           
singh  s  p   jaakkola  t     jordan  m  i          learning without state estimation
partially observable markovian decision processes  proceedings eleventh
international conference machine learning  pp          
smallwood  r  d     sondik  e  j          optimal control partially observable
processes finite horizon  operations research                
sondik  e  j          optimal control partially observable markov decision processes 
ph d  thesis  stanford university 
sondik  e  j          optimal control partially observable processes infinite
horizon  discounted costs  operations research              
tatman  j     schachter  r  d          dynamic programming uence diagrams 
ieee transactions systems  man cybernetics              
tsitsiklis  j  n     roy  b  v          feature based methods large scale dynamic
programming  machine learning            
washington  r          incremental markov model planning  proceedings eight
ieee international conference tools artificial intelligence  pp        
white  c  c     scherer  w  t          finite memory suboptimal design partially
observed markov decision processes  operations research              
williams  r  j     baird  l  c          tight performance bounds greedy policies based
imperfect value functions  proceedings tenth yale workshop adaptive
learning systems yale university 
yost  k  a          solution large scale allocation problems partially observable
outcomes  ph d  thesis  naval postgraduate school  monterey  ca 
zhang  n  l     lee  s  s          planning partially observable markov decision
processes  advances exact solution method  proceedings fourteenth conference uncertainty artificial intelligence  pp          
zhang  n  l     liu  w       a   model approximation scheme planning partially
observable stochastic domains  journal artificial intelligence research             
zhang  n  l     liu  w       b   region based approximations planning stochastic
domains  proceedings thirteenth conference uncertainty artificial
intelligence  pp          

  


