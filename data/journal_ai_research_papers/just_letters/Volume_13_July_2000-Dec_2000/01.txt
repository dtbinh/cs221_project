journal of artificial intelligence research                

submitted       published     

value function approximations for partially observable
markov decision processes

milos hauskrecht

milos cs brown edu

computer science department  brown university
box       brown university  providence  ri        usa

abstract

partially observable markov decision processes  pomdps  provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic
domains in which states of the system are observable only indirectly  via a set of imperfect
or noisy observations  the modeling advantage of pomdps  however  comes at a price  
exact methods for solving them are computationally very expensive and thus applicable
in practice only to very simple problems  we focus on ecient approximation  heuristic 
methods that attempt to alleviate the computational problem and trade off accuracy for
speed  we have two objectives here  first  we survey various approximation methods 
analyze their properties and relations and provide some new insights into their differences 
second  we present a number of new approximation methods and novel refinements of existing techniques  the theoretical results are supported by experiments on a problem from
the agent navigation domain 
   introduction

making decisions in dynamic environments requires careful evaluation of the cost and benefits not only of the immediate action but also of choices we may have in the future  this
evaluation becomes harder when the effects of actions are stochastic  so that we must pursue and evaluate many possible outcomes in parallel  typically  the problem becomes more
complex the further we look into the future  the situation becomes even worse when the
outcomes we can observe are imperfect or unreliable indicators of the underlying process
and special actions are needed to obtain more reliable information  unfortunately  many
real world decision problems fall into this category 
consider  for example  a problem of patient management  the patient comes to the
hospital with an initial set of complaints  only rarely do these allow the physician  decisionmaker  to diagnose the underlying disease with certainty  so that a number of disease options
generally remain open after the initial evaluation  the physician has multiple choices in
managing the patient  he she can choose to do nothing  wait and see   order additional tests
and learn more about the patient state and disease  or proceed to a more radical treatment
 e g  surgery   making the right decision is not an easy task  the disease the patient suffers
can progress over time and may become worse if the window of opportunity for a particular
effective treatment is missed  on the other hand  selection of the wrong treatment may
make the patient s condition worse  or may prevent applying the correct treatment later 
the result of the treatment is typically non deterministic and more outcomes are possible 
in addition  both treatment and investigative choices come with different costs  thus  in
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fihauskrecht

a course of patient management  the decision maker must carefully evaluate the costs and
benefits of both current and future choices  as well as their interaction and ordering  other
decision problems with similar characteristics   complex temporal cost benefit tradeoffs 
stochasticity  and partial observability of the underlying controlled process   include robot
navigation  target tracking  machine mantainance and replacement  and the like 
sequential decision problems can be modeled as markov decision processes  mdps 
 bellman        howard        puterman        boutilier  dean    hanks        and their
extensions  the model of choice for problems similar to patient management is the partially
observable markov decision process  pomdp   drake        astrom        sondik       
lovejoy      b   the pomdp represents two sources of uncertainty  stochasticity of the
underlying controlled process  e g  disease dynamics in the patient management problem  
and imperfect observability of its states via a set of noisy observations  e g  symptoms 
findings  results of tests   in addition  it lets us model in a uniform way both control and
information gathering  investigative  actions  as well as their effects and cost benefit tradeoffs  partial observability and the ability to model and reason with information gathering
actions are the main features that distinguish the pomdp from the widely known fully
observable markov decision process  bellman        howard        
although useful from the modeling perspective  pomdps have the disadvantage of being hard to solve  papadimitriou   tsitsiklis        littman        mundhenk  goldsmith 
lusena    allender        madani  hanks    condon         and optimal or  optimal solutions can be obtained in practice only for problems of low complexity  a challenging goal in
this research area is to exploit additional structural properties of the domain and or suitable
approximations  heuristics  that can be used to obtain good solutions more eciently 
we focus here on heuristic approximation methods  in particular approximations based
on value functions  important research issues in this area are the design of new and ecient
algorithms  as well as a better understanding of the existing techniques and their relations 
advantages and disadvantages  in this paper we address both of these issues  first  we
survey various value function approximations  analyze their properties and relations and
provide some insights into their differences  second  we present a number of new methods
and novel refinements of existing techniques  the theoretical results and findings are also
supported empirically on a problem from the agent navigation domain 
   partially observable markov decision processes

a partially observable markov decision process  pomdp  describes a stochastic control
process with partially observable  hidden  states  formally  it corresponds to a tuple
 s  a    t  o  r  where s is a set of states  a is a set of actions   is a set of observations 
t   s  a  s          is a set of transition probabilities that describe the dynamic behavior
of the modeled environment  o   s  a            is a set of observation probabilities that
describe the relationships among observations  states and actions  and r   s  a  s   ir
denotes a reward model that assigns rewards to state transitions and models payoffs associated with such transitions  in some instances the definition of a pomdp also includes an
a priori probability distribution over the set of initial states s  

  

fivalue function approximations for pomdps

o 

o t 

o

ot

t 

o t  

st

a 

a t 

s t  

at

a t 

r

t

figure    part of the inuence diagram describing a pomdp model  rectangles correspond
to decision nodes  actions   circles to random variables  states  and diamonds to
reward nodes  links represent the dependencies among the components  st   at   ot
and rt denote state  action  observation and reward at time t  note that an action
at time t depends only on past observations and actions  not on states 

    objective function
given a pomdp  the goal is to construct a control policy that maximizes an objective  value 
function  the objective function combines partial  stepwise  rewards over multiple steps
using various kinds of decision models  typically  the models are cumulative and based on
expectations  two models are frequently used in practice 

 a finite horizon model in which we maximize e  ptt   rt    where rt is a reward obtained
at time t 

 an infinite horizon discounted model in which we maximize e  p t    t rt    where    
     is a discount factor 

note that pomdps and cumulative decision models provide a rich language for modeling
various control objectives  for example  one can easily model goal achievement tasks  a
specific goal must be reached  by giving a large reward for a transition to that state and
zero or smaller rewards for other transitions 
in this paper we focus primarily on discounted infinite horizon model  however  the
results can be easily applied also to the finite horizon case 

    information state
in a pomdp the process states are hidden and we cannot observe them while making a
decision about the next action  thus  our action choices are based only on the information available to us or on quantities derived from that information  this is illustrated in
the inuence diagram in figure    where the action at time t depends only on previous
observations and actions  not on states  quantities summarizing all information are called
information states  complete information states represent a trivial case 
  

fihauskrecht

i t  

it

st

i t  

it

o t  

s t  

at

at

rt

rt

figure    inuence diagram for a pomdp with information states and corresponding
information state mdp  information states  it and it     are represented by
double circled nodes  an action choice  rectangle  depends only on the current
information state 

definition    complete information state   the complete information state at time t  denoted itc   consists of 




a prior belief b  on states in s at time   
a complete history of actions and observations fo    a    o    a         ot     at     ot g starting from time t     

a sequence of information states defines a controlled markov process that we call an
information state markov decision process or information state mdp  the policy for the
information state mdp is defined in terms of a control function    i   a mapping
information state space to actions  the new information state  it   is a deterministic function
of the previous state  it      the last action  at     and the new observation  ot   

it     it     ot   at     
   i    a   i is the update function mapping the information state space  observations
and actions back to the information space   it is easy to see that one can always convert
the original pomdp into the information state mdp by using complete information states 
the relation between the components of the two models and a sketch of a reduction of a
pomdp to an information state mdp  are shown in figure   
    bellman equations for pomdps
an information state mdp for the infinite horizon discounted case is like a fully observable
mdp and satisfies the standard fixed point  bellman  equation 
 

 

x
v   i     max  i  a     p  i   ji  a v   i      
a a
i 

   

   in this paper   denotes the generic update function  thus we use the same symbol even if the information
state space is different 
  

fivalue function approximations for pomdps

p
t
here  v   i   denotes the optimal value function maximizing e    
t    rt   for state i    i  a 
is the expected one step reward and equals
x
xx
 i  a     s  a p  sji    
r s  a  s   p  s  js  a p  sji   
s s
s  s s    s

 s  a  denotes an expected one step reward for state s and action a 
since the next information state i       i  o  a  is a deterministic function of the previous
information state i   action a  and the observation o  the equation   can be rewritten more
compactly by summing over all possible observations  
v   i     max
a a

 

x
s s

 s  a p  sji     

x
o 

 

p  oji  a v     i  o  a    

   

the optimal policy  control function     i   a selects the value maximizing action
 

 

x
x
  i     arg max
 s  a p  sji      p  oji  a v     i  o  a    
a a s s
o 

   

the value and control functions can be also expressed in terms of action value functions
 q functions 
v   i     max q  i  a 
  i     arg max q  i  a  
a a
a a
x
x

q  i  a     s  a p  sji      p  oji  a v     i  o  a   
   
s s
o 
a q function corresponds to the expected reward for chosing a fixed action  a  in the first
step and acting optimally afterwards 
      sufficient statistics

to derive equations     we implicitly used complete information states  however  as
remarked earlier  the information available to the decision maker can be also summarized
by other quantities  we call them sucient information states  such states must preserve
the necessary information content and also the markov property of the information state
decision process 

definition    sucient information state process   let i be an information state space
and    i  a     i be an update function defining an information process it  
  it     at     ot    the process is sucient with regard to the optimal control when  for any
time step t  it satisfies
p  st jit     p  st jitc  
p  ot jit     at       p  ot jitc     at     
where itc and itc   are complete information states 
it is easy to see that equations       for complete information states must hold also for
sucient information states  the key benefit of sucient statistics is that they are often
  

fihauskrecht

easier to manipulate and store  since unlike complete histories  they may not expand with
time  for example  in the standard pomdp model it is sucient to work with belief states
that assign probabilities to every possible process state  astrom          in this case the
bellman equation reduces to 
 

v  b    max
a a

x
s s

 s  a b s    

xx
o  s s

 

p  ojs  a b s v    b  o  a    

   

where the next step belief state b  is
x
b   s      b  o  a  s    fip  ojs  a 
p  sja  s   b s    
 
s  s

fi     p  ojb  a  is a normalizing constant  this defines a belief state mdp which is a
special case of a continuous state mdp  belief state mdps are also the primary focus of
our investigation in this paper 
      value function mappings and their properties

the bellman equation   for the belief state mdp can be also rewritten in the value function
mapping form  let v be a space of real valued bounded functions v   i   ir defined on
the belief information space i   and let h   i  a  b   ir be defined as

h b  a  v    

x

s s

 s  a b s    

xx

o  s s

p  ojs  a b s v    b  o  a   

now by defining the value function mapping h   v   v as  hv   b    maxa a h b  a  v   
the bellman equation   for all information states can be written as v    hv    it is well
known that h  for mdps  is an isotone mapping and that it is a contraction under the
supremum norm  see  heyman   sobel        puterman         

definition   the mapping h is isotone  if v  u

  v and v  u implies hv  hu  

definition   let k k be a supremum norm  the mapping h is a contraction under the
supremum norm  if for all v  u   v   khv hu k  fi kv u k holds for some    fi     
    value iteration
the optimal value function  equation    or its approximation can be computed using dynamic programming techniques  the simplest approach is the value iteration  bellman 
      shown in figure    in this case  the optimal value function v  can be determined
in the limit by performing a sequence of value iteration steps vi   hvi     where vi is the
ith approximation of the value function  ith value function    the sequence of estimates
   models in which belief states are not sucient include pomdps with observation and action channel
lags  see hauskrecht         
   we note that the same update v   hv   can be applied to solve the finite horizon problem in a
standard way  the difference is that v now stands for the i steps to go value function and v  represents
the value function  rewards  for end states 
i

i

i

  

fivalue function approximations for pomdps

value iteration  p omdp    
initialize v for all b   i  
repeat
v  v 
update v hv   for all b   i  
until supb j v  b  v    b  j 
return v 
figure    value iteration procedure 
converges to the unique fixed point solution which is the direct consequence of banach s
theorem for contraction mappings  see  for example  puterman         
in practice  we stop the iteration well before it reaches the limit solution  the stopping
criterion we use in our algorithm  figure    examines the maximum difference between value
functions obtained in two consecutive steps   the so called bellman error  puterman       
littman         the algorithm stops when this quantity falls below the threshold   the
accuracy of the approximate solution  ith value function  with regard to v  can be expressed
in terms of the bellman error  

theorem   let    supb jvi  b  vi    b j   kvi vi   k be the magnitude of the bellman
error  then kvi v  k    and kvi   v  k      hold 
then  to obtain the approximation of v  with precision  the bellman error should fall
below        
      piecewise linear and convex approximations of the value function

the major diculty in applying the value iteration  or dynamic programming  to beliefstate mdps is that the belief space is infinite and we need to compute an update vi   hvi  
for all of it  this poses the following threats  the value function for the ith step may not
be representable by finite means and or computable in a finite number of steps 
to address this problem sondik  sondik        smallwood   sondik        showed that
one can guarantee the computability of the ith value function as well as its finite description
for a belief state mdp by considering only piecewise linear and convex representations of
value function estimates  see figure     in particular  sondik showed that for a piecewise
linear and convex representation of vi     vi   hvi   is computable and remains piecewise
linear and convex 

theorem    piecewise linear and convex functions   let v  be an initial value function
that is piecewise linear and convex  then the ith value function obtained after a finite
number of update steps for a belief state mdp is also finite  piecewise linear and convex 
and is equal to 
x
vi  b    max b s ffi  s  
ffi   i s s

where b and ffi are vectors of size js j and i is a finite set of vectors  linear functions  ffi  
  

fihauskrecht

vi  b 

 

 

b s   

figure    a piecewise linear and convex function for a pomdp with two process states
fs    s g  note that b s       b s    holds for any belief state 

the key part of the proof is that we can express the update for the ith value function
in terms of linear functions i   defining vi    
 
 x

vi  b    max  
a a

s s

 s  a b s    

x

max

o  ffi    

i

 
x x
  s   s s s

 

 
 

p  s    ojs  a b s  ffi    s      

   

this leads to a piecewise linear and convex value function vi that can be represented by
a finite set of linear functions ffi   one linear function for every combination of actions and
j
permutations of ffi   vectors of size jj  let w    a  fo    ffji     g  fo    ffji     g     fojj   ffi j j g 
be such a combination  then the linear function corresponding to it is defined as
xx
ffw
p  s    ojs  a ffji    s    
   
i  s     s  a    
o  s   s
o

theorem   is the basis of the dynamic programming algorithm for finding the optimal
solution for the finite horizon models and the value iteration algorithm for finding nearoptimal approximations of v  for the discounted  infinite horizon model  note  however 
that this result does not imply piecewise linearity of the optimal  fixed point  solution v   
      algorithms for computing value function updates

the key part of the value iteration algorithm is the computation of value function updates
vi   hvi     assume an ith value function vi that is represented by a finite number of linear
segments  ff vectors   the total number of all its possible linear functions is jajj i   jjj  one
for every combination of actions and permutations of ffi   vectors of size jj  and they can
be enumerated in o jajjs j  j i   jjj   time  however  the complete set of linear functions
is rarely needed  some of the linear functions are dominated by others and their omission
does not change the resulting piecewise linear and convex function  this is illustrated in
figure   

  

fivalue function approximations for pomdps

vi  b 

redundant linear
function
 

 

b s   

figure    redundant linear function  the function does not dominate in any of the regions
of the belief space and can be excluded 

a linear function that can be eliminated without changing the resulting value function
solution is called redundant  conversely  a linear function that singlehandedly achieves the
optimal value for at least one point of the belief space is called useful  
for the sake of computational eciency it is important to make the size of the linear
function set as small as possible  keep only useful linear functions  over value iteration steps 
there are two main approaches for computing useful linear functions  the first approach is
based on a generate and test paradigm and is due to sondik        and monahan        
the idea here is to enumerate all possible linear functions first  then test the usefulness
of linear functions in the set and prune all redundant vectors  recent extensions of the
method interleave the generate and test stages and do early pruning on a set of partially
constructed linear functions  zhang   liu      a  cassandra  littman    zhang       
zhang   lee        
the second approach builds on sondik s idea of computing a useful linear function for a
single belief state  sondik        smallwood   sondik         which can be done eciently 
the key problem here is to locate all belief points that seed useful linear functions and
different methods address this problem differently  methods that implement this idea are
sondik s one  and two pass algorithms  sondik         cheng s methods  cheng         and
the witness algorithm  kaelbling  littman    cassandra        littman        cassandra 
      
      limitations and complexity

the major diculty in solving a belief state mdp is that the complexity of a piecewise
linear and convex function can grow extremely fast with the number of update steps  more
specifically  the size of a linear function set defining the function can grow exponentially  in
the number of observations  during a single update step  then  assuming that the initial
value function
is linear  the number of linear functions defining the ith value function is
o jajjj     
i

   in defining redundant and useful linear functions we assume that there are no linear function duplicates 
i e  only one copy of the same linear function is kept in the set  
i

  

fihauskrecht

the potential growth of the size of the linear function set is not the only bad news  as
remarked earlier  a piecewise linear convex value function is usually less complex than the
worst case because many linear functions can be pruned away during updates  however 
it turned out that the task of identifying all useful linear functions is computationally
intractable as well  littman         this means that one faces not only the potential
super exponential growth of the number of useful linear functions  but also ineciencies
related to the identification of such vectors  this is a significant drawback that makes the
exact methods applicable only to relatively simple problems 
the above analysis suggests that solving a pomdp problem is an intrinsically hard
task  indeed  finding the optimal solution for the finite horizon problem is pspace hard
 papadimitriou   tsitsiklis         finding the optimal solution for the discounted infinitehorizon criterion is even harder  the corresponding decision problem has been shown to be
undecidable  madani et al          and thus the optimal solution may not be computable 
      structural refinements of the basic algorithm

the standard pomdp model uses a at state space and full transition and reward matrices 
however  in practice  problems often exhibit more structure and can be represented more
compactly  for example  using graphical models  pearl        lauritzen         most often
dynamic belief networks  dean   kanazawa        kjaerulff        or dynamic inuence
diagrams  howard   matheson        tatman   schachter          there are many ways
to take advantage of the problem structure to modify and improve exact algorithms  for
example  a refinement of the basic monahan algorithm to compact transition and reward
models has been studied by boutilier and poole         a hybrid framework that combines
mdp pomdp problem solving techniques to take advantage of perfectly and partially observable components of the model and the subsequent value function decomposition was
proposed by hauskrecht                     a similar approach with perfect information
about a region  subset of states  containing the actual underlying state was discussed by
zhang and liu      b      a   finally  casta non        and yost        explore techniques
for solving large pomdps that consist of a set of smaller  resource coupled but otherwise
independent pomdps 

    extracting control strategy
value iteration allow us to compute an ith approximation of the value function vi   however 
our ulimate goal is to find the optimal control strategy    i   a or its close approximation 
thus our focus here is on the problem of extraction of control strategies from the results of
value iteration 
      lookahead design

the simplest way to define the control function    i   a from the value function vi is via
greedy one step lookahead 
 

 b    arg max
a a

x
s s

 s  a b s    

x
o 

 

p  ojb  a vi    b  o  a    

   see the survey by boutilier  dean and hanks        for different ways to represent structured mdps 
  

fivalue function approximations for pomdps

vi  b 
a 
a 

a 
a 

 

b

 

b s   

figure    direct control design  every linear function defining vi is associated with an
action  the action is selected if its linear function  or q function  is maximal 
as vi represents only the ith approximation of the optimal value function  the question
arises how good the resulting controller really is   the following theorem  puterman       
williams   baird        littman        relates the accuracy of the  lookahead  controller
and the bellman error 

theorem   let    kvi vi   k be the magnitude of the bellman error  let vila be the
expected reward for the lookahead controller designed for vi   then kvila v  k      
the bound can be used to construct the value iteration routine that yields a lookahead
strategy with a minimum required precision  the result can be also extended to the kstep lookahead design in a straightforward way  with k steps  the error bound becomes
kvila k  v  k        
k

      direct design

to extract the control action via lookahead essentially requires computing one full update 
obviously  this can lead to unwanted delays in reaction times  in general  we can speed up
the response by remembering and using additional information  in particular  every linear
function defining vi is associated with the choice of action  see equation     the action is a
byproduct of methods for computing linear functions and no extra computation is required
to find it  then the action corresponding to the best linear function can be selected directly
for any belief state  the idea is illustrated in figure   
the bound on the accuracy of the direct controller for the infinite horizon case can be
once again derived in terms of the magnitude of the bellman error 

theorem   let    kvi vi   k be the magnitude of the bellman error  let vidr be an
expected reward for the direct controller designed for vi   then kvidr v  k      
the direct action choice is closely related to the notion of action value function  or
q function   analogously to equation    the ith q function satisfies
vi  b    max qi  b  a  
a a

   note that the control action extracted via lookahead from v is optimal for  i      steps to go and the
finite horizon model  the main difference here is that v is the optimal value function for i steps to go 
i

i

  

fihauskrecht

a 

o 
o 

a 

o    o

 

o 

o    o

a 

o    o 

 

o 
a 

a 

o 

o 

a 

o    o 
a 

figure    a policy graph  finite state machine  obtained after two value iteration steps 
nodes correspond to linear functions  or states of the finite state machine  and
links to dependencies between linear functions  transitions between states   every
linear function  node  is associated with an action  to ensure that the policy can
be also applied to the infinite horizon problem  we add a cycle to the last state
 dashed line  

qi  b  a    r b  a    

x
o 

p  ojb  a vi      b  a  o   

from this perspective  the direct strategy selects the action with the best  maximum  qfunction for a given belief state  
      finite state machine design

a more complex refinement of the above technique is to remember  for every linear function
in vi   not only the action choice but also the choice of a linear function for the previous
step and to do this for all observations  see equation     as the same idea can be applied
recursively to the linear functions for all previous steps  we can obtain a relatively complex
dependency structure relating linear functions in vi   vi        v    observations and actions
that itself represents a control strategy  kaelbling et al         
to see this  we model the structure in graphical terms  figure     here different nodes
represent linear functions  actions associated with nodes correspond to optimizing actions 
links emanating from nodes correspond to different observations  and successor nodes correspond to linear functions paired with observations  such graphs are also called policy graphs
 kaelbling et al         littman        cassandra         one interpretation of the dependency structure is that it represents a collection of finite state machines  fsms  with many
possible initial states that implement a pomdp controller  nodes correspond to states of
the controller  actions to controls  outputs   and links to transitions conditioned on inputs
   williams and baird        also give results relating the accuracy of the direct q function controller to
the bellman error of q functions 

  

fivalue function approximations for pomdps

 observations   the start state of the fsm controller is chosen greedily by selecting the
linear function  controller state  optimizing the value of an initial belief state 
the advantage of the finite state machine representation of the strategy is that for the
first i steps it works with observations directly  belief state updates are not needed  this
contrasts with the other two policy models  lookahead and direct models   which must keep
track of the current belief state and update it over time in order to extract appropriate
control  the drawback of the approach is that the fsm controller is limited to i steps
that correspond to the number of value iteration steps performed  however  in the infinitehorizon model the controller is expected to run for an infinite number of steps  one way
to remedy this deficiency is to extend the fsm structure and to create cycles that let us
visit controller states repeatedly  for example  adding a cycle transition to the end state of
the fsm controller in figure    dashed line  ensures that the controller is also applicable
to the infinite horizon problem 

    policy iteration
an alternative method for finding the solution for the discounted infinite horizon problem
is policy iteration  howard        sondik         policy iteration searches the policy space
and gradually improves the current control policy for one or more belief states  the method
consists of two steps performed iteratively 




policy evaluation  computes expected value for the current policy 
policy improvement  improves the current policy 

as we saw in section      there are many ways to represent a control policy for a
pomdp  here we restrict attention to a finite state machine model in which observations
correspond to inputs and actions to outputs  platzman        hansen      b  kaelbling
et al          
      finite state machine controller

a finite state machine  fsm  controller c    m    a        for a pomdp is described
by a set of memory states m of the controller  a set of observations  inputs    a set of
actions  outputs  a  a transition function    m     m mapping states of the fsm to
next memory states given the observation  and an output function    m   a mapping
memory states to actions  a function   i    m selects the initial memory state given
the initial information state  the initial information state corresponds either to a prior or
a posterior belief state at time t  depending on the availability of an initial observation 
      policy evaluation

the first step of the policy iteration is policy evaluation  the most important property
of the fsm model is that the value function for a specific fsm strategy can be computed
eciently in the number of controller states m   the key to ecient computability is the
   a policy iteration algorithm in which policies are defined over the regions of the belief space was described
first by sondik        
  

fihauskrecht

x 
o 
o 

a 

a 

x 

o 
o

o 
a 

o 

o 

x 

 

a 

o 

x 

figure    an example of a four state fsm policy  nodes represent states  links transitions between states  conditioned on observations   every memory state has an
associated control action  output  

fact that the value function for executing an fsm strategy from some memory state x is
linear  platzman         

theorem   let c be a finite state machine controller with a set of memory states m  
the value function for applying c from a memory state x   m   v c  x  b   is linear  value
functions for all x   m can be found by solving a system of linear equations with js jjm j
variables 
we illustrate the main idea by an example  assume an fsm controller with four memory
states fx    x    x    x  g  as in figure    and a stochastic process with two hidden states s  
fs    s g  the value of the policy for an augmented state space s  m satisfies a system of
linear equations
v  x    s       s     x       
v  x    s       s     x       
v  x    s       s     x       



v  x    s       s     x       

xx

o  s s

xx

o  s s

xx

o  s s

xx
o  s s

p  o  sjs     x    v   x    o   s 
p  o  sjs     x    v   x    o   s 
p  o  sjs     x    v   x    o   s 
p  o  sjs     x    v   x    o   s  

where  x  is the action executed in x and  x  o  is the state to which one transits after
seeing an input  observation  o  assuming we start the policy from the memory state x   
the value of the policy is 
x
v c  x    b    v  x    s b s  
s s
   the idea of linearity and ecient computability of the value functions for a fixed fsm based strategy
has been addressed recently in different contexts by a number of researchers  littman        cassandra 
      hauskrecht        hansen      b  kaelbling et al          however  the origins of the idea can be
traced to the earlier work by platzman        
  

fivalue function approximations for pomdps

thus the value function is linear and can be computed eciently by solving a system of
linear equations 
since in general the fsm controller can start from any memory state  we can always
choose the initial memory state greedily  maximizing the expected value of the result  in
such a case the optimal choice function is defined as 
 b    arg max v c  x  b  
x m
and the value for the fsm policy c and belief state b is 

v c  b    max v c  x  b    v c    b   b  
x m

note that the resulting value function for the strategy c is piecewise linear and convex and
represents expected rewards for following c   since no strategy can perform better that the
optimal strategy  v c  v  must hold 
      policy improvement

the policy iteration method  searching the space of controllers  starts from an arbitrary initial policy and improves it gradually by refining its finite state machine  fsm  description 
in particular  one keeps modifying the structure of the controller by adding or removing controller states  memory  and transitions  let c and c   be an old and a new fsm controller 
in the improvement step we must satisfy
 

v c  b   v c  b  for all b   i  

 b   i such that v c    b    v c  b  
to guarantee the improvement  hansen      a      b  proposed a policy iteration algorithm that relies on exact value function updates to obtain a new improved policy structure    the basic idea of the improvement is based on the observation that one can switch
back and forth between the fsm policy description and the piecewise linear and convex
representation of a value function  in particular 

 the value function for an fsm policy is piecewise linear and convex and every linear
function describing it corresponds to a memory state of a controller 

 individual linear functions comprising the new value function after an update can be
viewed as new memory states of an fsm policy  as described in section       

this allows us to improve the policy by adding new memory states corresponding to linear
functions of the new value function obtained after the exact update  the technique can be
refined by removing some of the linear functions  memory states  whenever they are fully
dominated by one of the other linear functions 
    a policy iteration algorithm that exploits exact value function updates but works with policies defined
over the belief space was used earlier by sondik        

  

fihauskrecht

b    

o 

o 

a 

b

a 

b    

figure    a two step decision tree  rectangles correspond to the decision nodes  moves
of the decision maker  and circles to chance nodes  moves of the environment  
black rectangles represent leaves of the tree  the reward for a specific path
is associated with every leaf of the tree  decision nodes are associated with
information states obtained by following action and observation choices along the
path from the root of the tree  for example  b    is a belief state obtained by
performing action a  from the initial belief state b and observing observation o   

    forward  decision tree  methods
the methods discussed so far assume no prior knowledge of the initial belief state and treat
all belief states as equally likely  however  if the initial state is known and fixed  methods
can often be modified to take advantage of this fact  for example  for the finite horizon
problem  only a finite number of belief states can be reached from a given initial state  in
this case it is very often easier to enumerate all possible histories  sequences of actions and
observations  and represent the problem using stochastic decision trees  raiffa         an
example of a two step decision tree is shown in figure   
the algorithm for solving the stochastic decision tree basically mimics value function
updates  but is restricted only to situations that can be reached from the initial belief state 
the key diculty here is that the number of all possible trajectories grows exponentially
with the horizon of interest 
      combining dynamic programming and decision tree techniques

to solve a pomdp for a fixed initial belief state  we can apply two strategies  one constructs the decision tree first and then solves it  the other solves the problem in a backward
fashion via dynamic programming  unfortunately  both these techniques are inecient  one
suffering from exponential growth in the decision tree size  the other from super exponential
growth in the value function complexity  however  the two techniques can be combined in
  

fivalue function approximations for pomdps

a way that at least partially eliminates their disadvantages  the idea is based on the fact
that the two techniques work on the solution from two different sides  one forward and the
other backward  and the complexity for each of them worsens gradually  then the solution
is to compute the complete kth value function using dynamic programming  value iteration 
and cover the remaining steps by forward decision tree expansion 
various modifications of the above idea are possible  for example  one can often replace
exact dynamic programming with two more ecient approximations providing upper and
lower bounds of the value function  then the decision tree must be expanded only when
the bounds are not sucient to determine the optimal action choice  a number of search
techniques developed in the ai literature  korf        combined with branch and bound
pruning  satia   lave        can be applied to this type of problem  several researchers
have experimented with them to solve pomdps  washington        hauskrecht       
hansen      b   other methods applicable to this problem are based on monte carlo
sampling  kearns  mansour    ng        mcallester   singh        and real time dynamic
programming  barto  bradtke    singh        dearden   boutilier        bonet   geffner 
      
      classical planning framework

pomdp problems with fixed initial belief states and their solutions are closely related to
work in classical planning and its extensions to handle stochastic and partially observable
domains  particularly the work on buridan and c buridan planners  kushmerick 
hanks    weld        draper  hanks    weld         the objective of these planners is
to maximize the probability of reaching some goal state  however  this task is similar to
the discounted reward task in terms of complexity  since a discounted reward model can
be converted into a goal achievement model by introducing an absorbing state  condon 
      
   heuristic approximations

the key obstacle to wider application of the pomdp framework is the computational
complexity of pomdp problems  in particular  finding the optimal solution for the finitehorizon case is pspace hard  papadimitriou   tsitsiklis        and the discounted infinitehorizon case may not even be computable  madani et al          one approach to such
problems is to approximate the solution to some  precision  unfortunately  even this
remains intractable and in general pomdps cannot be approximated eciently  burago 
rougemont    slissenko        lusena  goldsmith    mundhenk        madani et al  
       this is also the reason why only very simple problems can be solved optimally or
near optimally in practice 
to alleviate the complexity problem  research in the pomdp area has focused on various
heuristic methods  or approximations without the error parameter  that are more ecient   
heuristic methods are also our focus here  thus  when referring to approximations  we mean
heuristics  unless specifically stated otherwise 
    the quality of a heuristic approximation can be tested using the bellman error  which requires one exact
update step  however  heuristic methods per se do not contain a precision parameter 

  

fihauskrecht

the many approximation methods and their combinations can be divided into two often
very closely related classes  value function approximations and policy approximations 

    value function approximations
the main idea of the value function approximation approach is to approximate the optimal
value function v   i   ir with a function vb   i   ir defined over the same information
space  typically  the new function is of lower complexity  recall that the optimal or nearoptimal value function may consist of a large set of linear functions  and is easier to compute
than the exact solution  approximations can be often formulated as dynamic programming
problems and can be expressed in terms of approximate value function updates hb   thus 
to understand the differences and advantages of various approximations and exact methods 
it is often sucient to analyze and compare their update rules 
      value function bounds

although heuristic approximations have no guaranteed precision  in many cases we are
able to say whether they overestimate or underestimate the optimal value function  the
information on bounds can be used in multiple ways  for example  upper  and lowerbounds can help in narrowing the range of the optimal value function  elimination of some
of the suboptimal actions and subsequent speed ups of exact methods  alternatively  one
can use knowledge of both value function bounds to determine the accuracy of a controller
generated based on one of the bounds  see section         also  in some instances  a lower
bound alone is sucient to guarantee the control choice that always achieves an expected
reward at least as high as the one given by that bound  section        
the bound property of different methods can be determined by examining the updates
and their bound relations 

definition    upper bound   let h be the exact value function mapping and hb its apb   b    hv   b  holds
proximation  we say that hb upper bounds h for some v when  hv
for every b   i  
an analogous definition can be constructed for the lower bound 
      convergence of approximate value iteration

let hb be a value function mapping representing an approximate update  then the approximate value iteration computes the ith value function as vbi   hb vbi     the fixed point
solution vc   hb vb  or its close approximation would then represent the intended output of
the approximation routine  the main problem with the iteration method is that in general
it can converge to unique or multiple solutions  diverge  or oscillate  depending on hb and
the initial function vb    therefore  unique convergence cannot be guaranteed for an arbitrary
mapping hb and the convergence of a specific approximation method must be proved 

definition    convergence of hb    the value iteration with hb converges for a value function v  when limn   hb n v    exists 

  

fivalue function approximations for pomdps

definition    unique convergence of hb    the value iteration converges uniquely for v
when for every v   v   limn   hb n v   exists and for all pairs v  u   v   limn   hb n v    
limn   hb n u   
a sucient condition for the unique convergence is to show that hb be a contraction  the
contraction and the bound properties of hb can be combined  under additional conditions  to
show the convergence of the iterative approximation method to the bound  to address this
issue we present a theorem comparing fixed point solutions of two value function mappings 
theorem   let h  and h  be two value function mappings defined on v  and v  such that
   h    h  are contractions with fixed points v    v   
   v    v  and h  v   h  v    v   

   h  is an isotone mapping 
then v   v  holds 

note that this theorem does not require that v  and v  cover the same space of value
functions  for example  v  can cover all possible value functions of a belief state mdp 
while v  can be restricted to a space of piecewise linear and convex value functions  this
gives us some exibility in the design of iterative approximation algorithms for computing
value function bounds  an analogous theorem also holds for the lower bound 
      control

once the approximation of the value function is available  it can be used to generate a
control strategy  in general  control solutions correspond to options presented in section
    and include lookahead  direct  q function  and finite state machine designs 
a drawback of control strategies based on heuristic approximations is that they have
no precision guarantee  one way to find the accuracy of such strategies is to do one exact
update of the value function approximation and adopt the result of theorems   and   for
the bellman error  an alternative solution to this problem is to bound the accuracy of
such controllers using the upper  and the lower bound approximations of the optimal value
function  to illustrate this approach  we present and prove  in the appendix  the following
theorem that relates the quality of bounds to the quality of a lookahead controller 

theorem   let vbu and vbl be upper and lower bounds of the optimal value function for
the discounted infinite horizon problem  let    supb jvbu  b  vbl  b j   kvbu vbl k be
the maximum bound difference  then the expected reward for a lookahead controller vb la  
constructed for either vbu or vbl   satisfies kvb la v  k           
    policy approximation
an alternative to value function approximation is policy approximation  as shown earlier 
a strategy  controller  for a pomdp can be represented using a finite state machine  fsm 
model  the policy iteration searches the space of all possible policies  fsms  for the optimal or near optimal solution  this space is usually enormous  which is the bottleneck of the
  

fihauskrecht

method  thus  instead of searching the complete policy space  we can restrict our attention
only to its subspace that we believe to contain the optimal solution or a good approximation  memoryless policies  platzman        white   scherer        littman        singh 
jaakkola    jordan         policies based on truncated histories  platzman        white  
scherer        mccallum         or finite state controllers with a fixed number of memory
states  platzman        hauskrecht        hansen      a      b  are all examples of a
policy space restriction  in the following we consider only the finite state machine model
 see section         which is quite general  other models can be viewed as its special cases 
states of an fsm policy model represent the memory of the controller and  in general 
summarize information about past activities and observations  thus  they are best viewed
as approximations of the information states  or as feature states  the transition model of
the controller    then approximates the update function of the information state mdp
    and the output function of an fsm    approximates the control function    mapping
information states to actions  the important property of the model  as shown section
       is that the value function for a fixed controller and fixed initial memory state can be
obtained eciently by solving a system of linear equations  platzman        
to apply the policy approximation approach we first need to decide     how to restrict
a space of policies and     how to judge the policy quality 
a restriction frequently used is to consider only controllers with a fixed number of
states  say k  other structural restrictions further narrowing the space of policies can
restrict either the output function  choice of actions at different controller states   or the
transitions between the current and next states  in general  any heuristic or domain related
insight may help in selecting the right biases 
two different policies can yield value functions that are better in different regions of
the belief space  thus  in order to decide which policy is the best  we need to define the
importance of different regions and their combinations  there are multiple solutions to this 
for example  platzman        considers the worst case measure and optimizes the worst
 minimal  value for all initial belief states  let c be a space of fsm controllers satisfying
given restrictions  then the quality of a policy under the worst case measure is 
max min max v c  x  b  
c  c b i x m
c

another option is to consider a distribution over all initial belief states and maximize the
expectation of their value function values  however  the most common objective is to choose
the policy that leads to the best value for a single initial belief state b   
max max v c  x  b    
c  c x m
c

finding the optimal policy for this case reduces to a combinatorial optimization problem 
unfortunately  for all but trivial cases  even this problem is computationally intractable 
for example  the problem of finding the optimal policy for a memoryless case  only current observations are considered  is np hard  littman         thus  various heuristics are
typically applied to alleviate this diculty  littman        

  

fivalue function approximations for pomdps

valuefunction
approximations

gridbased linear
function methods
section    

fully observable mdp
approximations
section    

fast informed bound
approximations
section    

fixed strategy
approximations
section    

curvefitting
approximations
section    

gridbased value interpolation
extrapolation methods
section    

unobservable mdp
approximations
section    

figure     value function approximation methods 
      randomized policies

by restricting the space of policies we simplify the policy optimization problem  on the
other hand  we simultaneously give up an opportunity to find the best optimal policy  replacing it with the best restricted policy  up to this point  we have considered only deterministic
policies with a fixed number of internal controller states  that is  policies with deterministic
output and transition functions  however  finding the best deterministic policy is not always the best option  randomized policies  with randomized output and transition functions 
usually lead to the far better performance  the application of randomized  or stochastic 
policies to pomdps was introduced by platzman         essentially  any deterministic
policy can be represented as a randomized policy with a single action and transition  so
that the best randomized policy is no worse than the best deterministic policy  the difference in control performance of two policies shows up most often in cases when the number
of states of the controller is relatively small compared to that in the optimal strategy 
the advantage of stochastic policies is that their space is larger and parameters of
the policy are continuous  therefore the problem of finding the optimal stochastic policy
becomes a non linear optimization problem and a variety of optimization methods can be
applied to solve it  an example is the gradient based approach  see meuleau et al         
   value function approximation methods

in this section we discuss in more depth value function approximation methods  we focus on approximations with belief information space    we survey known techniques  but
also include a number of new methods and modifications of existing methods  figure   
summarizes the methods covered  we describe the methods by means of update rules they
    alternative value function approximations may work with complete histories of past actions and observations  approximation methods used by white and scherer        are an example 

  

fihauskrecht

  

  

  

  

  

  

  

  

  

  

 

 

 

 

 

 

 

 

 

 

moves

sensors

figure     test example  the maze navigation problem  maze   
implement  which simplifies their analysis and theoretical comparison  we focus on the following properties  the complexity of the dynamic programming  value iteration  updates 
the complexity of value functions each method uses  the ability of methods to bound the
exact update  the convergence of value iteration with approximate update rules  and the
control performance of related controllers  the results of the theoretical analysis are illustrated empirically on a problem from the agent navigation domain  in addition  we use the
agent navigation problem to illustrate and give some intuitions on other characteristics of
methods with no theoretical underpinning  thus  these results should not be generalized
to other problems or used to rank different methods 
agent navigation problem

maze   is a maze navigation problem with    states  six actions and eight observations 
the maze  figure     consists of    partially connected rooms  states  in which a robot
operates and collects rewards  the robot can move in four directions  north  south  east
and west  and can check for the presence of walls using its sensors  but  neither  move 
actions nor sensor inputs are perfect  so that the robot can end up moving in unintended
directions  the robot moves in an unintended direction with probability of           for
each of the neighboring directions   a move into the wall keeps the robot in the same
position  investigative actions help the robot to navigate by activating sensor inputs  two
such investigative actions allow the robot to check inputs  presence of a wall  in the northsouth and east west directions  sensor accuracy in detecting walls is      for a two wall
case  e g  both north and south wall       for a one wall case  north or south  and      for
a no wall case  with smaller probabilities for wrong perceptions 
the control objective is to maximize the expected discounted rewards with a discount
factor of      a small reward is given for every action not leading to bumping into the wall
   points for a move and   points for an investigative action   and one large reward     
points  is given for achieving the special target room  indicated by the circle in the figure 
and recognizing it by performing one of the move actions  after doing that and collecting
the reward  the robot is placed at random in a new start position 
although the maze   problem is of only moderate complexity with regard to the size
of state  action and observation spaces  its exact solution is beyond the reach of current
exact methods  the exact methods tried on the problem include the witness algorithm
 kaelbling et al          the incremental pruning algorithm  cassandra et al           and
    many thanks to anthony cassandra for running these algorithms 
  

fivalue function approximations for pomdps

vmdp

 
vmdp
 s    

 
vmdp
 s    

vqmdp

q  mdp s    a    

q  mdp s    a   

q  mdp s    a   

q  mdp s    a   

 
vpomdp
 

 
vpomdp
 

b s   

 

 a 

 

b s   

 b 

figure     approximations based on the fully observable version of a two state pomdp
 with states s    s      a  the mdp approximation   b  the qmdp approximation 
values at extreme points of the belief space are solutions of the fully observable
mdp 
policy iteration with an fsm model  hansen      b   the main obstacle preventing these
algorithms from obtaining the optimal or close to optimal solution was the complexity of
the value function  the number of linear functions needed to describe it  and subsequent
running times and memory problems 

    approximations with fully observable mdp
perhaps the simplest way to approximate the value function for a pomdp is to assume
that states of the process are fully observable  astrom        lovejoy         in that case
the optimal value function v  for a pomdp can be approximated as 
vb  b   

x

s s

  s  
b s vmdp

   

  s  is the optimal value function for state s for the fully observable version of
where vmdp
the process  we refer to this approximation as to the mdp approximation  the idea of
the approximation is illustrated in figure   a  the resulting value function is linear and
is fully defined by values at extreme points of the belief simplex  these correspond to the
optimal values for the fully observable case  the main advantage of the approximation
is that the fully observable mdp  fomdp  can be solved eciently for both the finitehorizon problem and discounted infinite horizon problems    the update step for the  fully
observable  mdp is 
 
 

vimdp
 s  a    
    s    max
a  

x
s   s

 
 

p  s  js  a vimdp  s      

    the solution for the finite state fully observable mdp and discounted infinite horizon criterion can be
found eciently by formulating an equivalent linear programming task  bertsekas       

  

fihauskrecht

      mdp approximation

the mdp approximation approach  equation    can be also described in terms of valuefunction updates for the belief space mdp  although this step is strictly speaking redundant
here  it simplifies the analysis and comparison of this approach to other approximations 
let vbi be a linear value function described by a vector ffmdp
corresponding to values
i
of vimdp  s    for all states s    s   then the  i     th value function vbi   is

vbi    b   

x
s s

 

b s  max   s  a    
a a

   hmdp vbi   b  

x
s   s

 

p  s  js  a ffmdp
 s    
i

vbi   is described by a linear function with components
mdp
ffmdp
i    s    vi    s    max
a

 

 s  a    

x
s s

 

p  s  js  a ffmdp
 s     
i

the mdp based rule hmdp can be also rewritten in a more general form that starts from
an arbitrary piecewise linear and convex value function vi   represented by a set of linear
functions i  

vbi    b   

x
s s

 
 

b s  max   s  a    
a a

x
s   s

 
 

p  s  js  a  max ffi  s      
ffi  

i

the application of the hmdp mapping always leads to a linear value function  the
update is easy to compute and takes o jajjs j    j i jjs j  time  this reduces to o jajjs j   
time when only mdp based updates are strung together  as remarked earlier  the optimal
solution for the infinite horizon  discounted problem can be solved eciently via linear
programming 
the update for the mdp approximation upper bounds the exact update  that is  h vbi 
hmdp vbi   we show this property later in theorem    which covers more cases  the intuition
is that we cannot get a better solution with less information  and thus the fully observable
mdp must upper bound the partially observable case 
      approximation with q functions  qmdp 

a variant of the approximation based on the fully observable mdp uses q functions  littman 
cassandra    kaelbling        
x
vb  b    max b s qmdp  s  a  
a a s s
where
x
  s   
qmdp  s  a     s  a    
p  s  js  a vmdp
s   s
is the optimal action value function  q function  for the fully observable mdp  the qmdp
approximation vb is piecewise linear and convex with jaj linear functions  each corresponding
  

fivalue function approximations for pomdps

to one action  figure   b   the qmdp update rule  for the belief state mdp  for vbi with
linear functions ffki   i is 

vbi    b    max

x

a a s s

 

b s    s  a    

   hqmdp vbi   b  

x
s   s

 

p  s  js  a  max ffi  s    
ffi  

i

hqmdp generates a value function with jaj linear functions  the time complexity of the
update is the same as for the mdp approximation case   o jajjs j    j i jjs j   which reduces
to o jajjs j    time when only qmdp updates are used  hqmdp is a contraction mapping
and its fixed point solution can be found by solving the corresponding fully observable mdp 
the qmdp update upper bounds the exact update  the bound is tighter than the
mdp update  that is  h vbi  hqmdp vbi  hmdp vbi   as we prove later in theorem    the
same inequalities hold for both fixed point solutions  through theorem    
to illustrate the difference in the quality of bounds for the mdp approximation and
the qmdp method  we use our maze   navigation problem  to measure the quality of a
bound we use the mean of value function values  since all belief states are equally important
we assume that they are uniformly distributed  we approximate this measure using the
average of values for a fixed set of n        belief points  the points in the set were
selected uniformly at random at the beginning  once the set was chosen  it was fixed
and remained the same for all tests  here and later   figure    shows the results of the
experiment  we include also results for the fast informed bound method that is presented in
the next section    figure    also shows the running times of the methods  the methods
were implemented in common lisp and run on sun ultra   workstation 
      control

the mdp and the qmdp value function approximations can be used to construct controllers based on one step lookahead  in addition  the qmdp approximation is also suitable
for the direct control strategy  which selects an action corresponding to the best  highest
value  q function  thus  the method is a special case of the q function approach discussed
in section          the advantage of the direct qmdp method is that it is faster than both
lookahead designs  on other the hand  lookahead tends to improve the control performance 
this is shown in figure     which compares the control performance of different controllers
on the maze   problem 
the quality of a policy b   with no preference towards a particular initial belief state  can
be measured by the mean of value function values for b and uniformly distributed initial
belief states  we approximate this measure using the average of discounted rewards for

    the confidence interval limits for probability level      range in              from their respective
average scores and this holds for all bound experiments in the paper  as these are relatively small we
do not include them in our graphs 
    as pointed out by littman et al          in some instances  the direct qmdp controller never selects
investigative actions  that is  actions that try to gain more information about the underlying process
state  note  however  that this observation is not true in general and the qmdp based controller with
direct action selection may select investigative actions  even though in the fully observable version of the
problem investigative actions are never chosen 
  

fihauskrecht

bound quality
mdp
approximation

qmdp
approximation

running times

fast informed
bound

  

   

  
time  sec 

score

   
   

  
  
  

  
  

  

 
mdp
approximation

  

qmdp
approximation

fast informed
bound

figure     comparison of the mdp  qmdp and fast informed bound approximations 
bound quality  left   running times  right   the bound quality score is the
average value of the approximation for the set of      belief points  chosen uniformly at random   as the methods upper bound the optimal value function  we
ip the bound quality graph so that longer bars indicate better approximations 
     control trajectories obtained for the fixed set of n        initial belief states  selected
uniformly at random at the beginning   the trajectories were obtained through simulation
and were    steps long   
to validate the comparison along the averaged performance scores  we must show that
these scores are not the result of randomness and that methods are indeed statistically
significantly different  to do this we rely on pairwise significance tests    to summarize the
obtained results  the score differences of            and      between any two methods  here
and also later in the paper  are sucient to reject the method with a lower score being
the better performer at significance levels            and       respectively    error bars in
figure    reect the critical score difference for the significance level      
figure    also shows the average reaction times for different controllers during these
experiments  the results show the clear dominance of the direct qmdp controller  which
need not do a lookahead in order to extract an action  compared to the other two mdpbased controllers 

    fast informed bound method
both the mdp and the qmdp approaches ignore partial observability and use the fully
observable mdp as a surrogate  to improve these approximations and account  at least to
    the length of the trajectories     steps  for the maze   problem was chosen to ensure that our estimates
of  discounted  cumulative rewards are not far from the actual rewards for an infinite number of steps 
    an alternative way to compare two methods is to compute confidence limits for their scores and inspect
their overlaps  however  in this case  the ability to distinguish two methods can be reduced due to
uctuations of scores for different initializations  for maze    confidence interval limits for probability
level      range in            from their respective average scores  this covers all control experiments
here and later  pairwise tests eliminate the dependency by examining the differences of individual values
and thus improve the discriminative power 
    the critical score differences listed cover the worst case combination  thus  there may be some pairs for
which the smaller difference would suce 
  

fivalue function approximations for pomdps

reaction times

control performance
     

  

lookahead
    

lookahead

lookahead

time  sec 

score

  
  

direct
  

lookahead

direct

     
    

  

     

  

 
mdp
approximation

direct

fast informed
bound

qmdp
approximation

lookahead

lookahead

mdp
approximation

direct

qmdp
approximation

fast informed
bound

figure     comparison of control performance of the mdp  qmdp and fast informed bound
methods  quality of control  left   reaction times  right   the quality of control
score is the average of discounted rewards for      control trajectories obtained
for the fixed set of      initial belief states  selected uniformly at random  
error bars show the critical score difference value        at which any two methods become statistically different at significance level      
some degree  for partial observability we propose a new method   the fast informed bound
method  let vbi be a piecewise linear and convex value function represented by a set of linear
functions i   the new update is defined as
 
 x

xx

 
 

x

vbi    b    max    s  a b s    
max
p  s    ojs  a b s ffi  s    
a a s s
ff
 
 
o  s s
s  s
 
 
i

 

 x

i

x

x

  max   b s    s  a    
max
a a s s
o  ff   s   s
   hf ib vbi   b  
i

i

 
 
p  s    ojs  a ffi  s     

the fast informed bound update can be obtained from the exact update by the following
derivation 
 
 x

x

xx

 
 

x

 
 

 h vbi   b    max    s  a b s    
max
p  s    ojs  a b s ffi  s    
a a s s
ff
 
o 
s   s s s
i

 
 x

 max
 s  a b s    
a a  
s s

x

 

xx

i

max

o  s s ffi   i s   s

x

x

p  s    ojs  a b s ffi  s    
 

  max b s    s  a    
max
p  s    ojs  a ffi  s    
a a s s
ff
 
o 
s   s
i

x

i

  max b s ffai    s 
a a s s
   hf ib vbi   b  

the value function vbi     hf ib vbi one obtains after an update is piecewise linear and
convex and consists of at most jaj different linear functions  each corresponding to one
  

fihauskrecht

action

ffai    s     s  a    

x

x
max
p  s    ojs  a ffi  s    
ff
 
o 
s   s
i

i

the hf ib update is ecient and can be computed in o jajjs j  jjj i j  time  as the method
always outputs jaj linear functions  the computation can be done in o jaj  js j  jj  time 
when many hf ib updates are strung together  this is a significant complexity reduction
compared to the exact approach  the latter can lead to a function consisting of jajj i jjj
linear functions  which is exponential in the number of observations and in the worst case
takes o jajjs j  j i jjj  time 
as hf ib updates are of polynomial complexity one can find the approximation for the
finite horizon case eciently  the open issue remains the problem of finding the solution
for the infinite horizon discounted case and its complexity  to address it we establish the
following theorem 

theorem   a solution for the fast informed bound approximation can be found by solving
an mdp with js jjajjj states  jaj actions and the same discount factor   
the full proof of the theorem is deferred to the appendix  the key part of the proof
is the construction of an equivalent mdp with js jjajjj states representing hf ib updates 
since a finite state mdp can be solved through linear program conversion  the fixed point
solution for the fast informed bound update is computable eciently 
      fast informed bound versus fully observable mdp approximations

the fast informed update upper bounds the exact update and is tighter than both the mdp
and the qmdp approximation updates 

theorem   let vbi corresponds to a piecewise linear convex value function defined by i
linear functions  then h vbi  hf ib vbi  hqmdp vbi  hmdp vbi  
the key trick in deriving the above result is to swap max and sum operators  the
proof is in the appendix  and thus obtain both to the upper bound inequalities and the
subsequent reduction in the complexity of update rules compared to the exact update 
this is also shown in figure     the umdp approximation  also included in figure    
is discussed later in section      thus  the difference among the methods boils down to
simple mathematical manipulations  note that the same inequality relations as derived for
updates hold also for their fixed point solutions  through theorem    
figure   a illustrates the improvement of the bound over mdp based approximations
on the maze   problem  note  however  that this improvement is paid for by the increased
running time complexity  figure   b  
      control

the fast informed bound always outputs a piecewise linear and convex function  with one
linear function per action  this allows us to build a pomdp controller that selects an action
associated with the best  highest value  linear function directly  figure    compares the
control performance of the direct and the lookahead controllers to the mdp and the qmdp
controllers  we see that the fast informed bound leads not only to tighter bounds but also
  

fivalue function approximations for pomdps

umdp update 


v i       b     m ax   b   s      s   a      m ax 
aa
 i  i s s

 s s



exact update 


v i       b     m ax   b   s      s   a     
aa
 s s

 p   s     s   a  b   s  

s   s

 m ax  

o   i  i s  s s   s



fast informed bound update 



v i       b     m ax   b   s       s   a     
aa

 s s

 m ax 




p   s     s   a   m ax  i   s      
 i  i






mdp approx  update 


v i       b      b   s   m ax     s   a     
aa
s s



p   s     o   s   a   i   s      


 i  i s s
 

s   s


  s    



p   s     o   s   a  b   s   i   s     


o 

qmdp approx  update 


v i       b     m ax   b   s       s   a     
a a
s
s




i



s  s


p   s     s   a   m ax  i   s     
 i  i


figure     relations between the exact update and the umdp  the fast informed bound 
the qmdp and the mdp updates 
to improved control on average  however  we stress that currently there is no theoretical
underpinning for this observation and thus it may not be true for all belief states and any
problem 
      extensions of the fast informed bound method

the main idea of the fast informed bound method is to select the best linear function for
every observation and every current state separately  this differs from the exact update
where we seek a linear function that gives the best result for every observation and the
combination of all states  however  we observe that there is a great deal of middle ground
between these two extremes  indeed  one can design an update rule that chooses optimal
 maximal  linear functions for disjoint sets of states separately  to illustrate this idea 
assume a partitioning s   fs    s         sm g of the state space s   the new update for s is 

vbi    b    max
a a

 

x
s s

 s  a b s    

x

 
  max

x x

o  ffi   i s s  s   s

p  s    ojs  a b s ffi  s    

x x
max
p  s    ojs  a b s ffi  s          
ff   s s s   s
 
i

i

max

x x

ffi   i s s s   s
m

  
 
p  s    ojs  a b s ffi  s     

it is easy to see that the update upper bounds the exact update  exploration of this
approach and various partitioning heuristics remains an interesting open research issue 
  

fihauskrecht

    approximation with unobservable mdp
the mdp approximation assumes full observability of pomdp states to obtain simpler
and more ecient updates  the other extreme is to discard all observations available to
the decision maker  an mdp with no observations is called unobservable mdp  umdp 
and one may choose its value function solution as an alternative approximation 
to find the solution for the unobservable mdp  we derive the corresponding update
rule  humdp   similarly to the update for the partially observable case  humdp preserves
piecewise linearity and convexity of the value function and is a contraction  the update
equals 
 
 x

 
 

xx

vbi    b    max    s  a b s     max
p  s  js  a b s ffi  s    
a a s s
ff   s s s   s
   humdp vbi   b  
i

i

where i is a set of linear functions describing vbi   vbi   remains piecewise linear and convex
and it consists of at most j i jjaj linear functions  this is in contrast to the exact update 
where the number of possible vectors in the next step can grow exponentially in the number
of observations and leads to jajj i jjj possible vectors  the time complexity of the update is
o jajjs j  j i j   thus  starting from vb  with one linear function  the running time complexity
for k updates is bounded by o jajk js j     the problem of finding the optimal solution for the
unobservable mdp remains intractable  the finite horizon case is np hard burago et al  
       and the discounted infinite horizon case is undecidable  madani et al          thus 
it is usually not very useful approximation 
the update humdp lower bounds the exact update  an intuitive result reecting the
fact that one cannot do better with less information  to provide some insight into how
the two updates are related  we do the following derivation  which also proves the bound
property in an elegant way 
 
 x

x

 
 

xx

 h vbi   b    max    s  a b s    
max
p  s    ojs  a b s ffi  s    
a a s s
ff
 
o 
s   s s s
 
 
i

 x

 s  a b s     max
 max
a a  
ff 
s s

 
 x

i

i

i

xxx

o  s s s   s

xx

 

p  s    ojs  a b s ffi  s    
 
 

  max    s  a b s     max
p  s  js  a b s ffi  s    
a a s s
ff   s s s   s
   humdp vbi   b  
i

i

we see that the difference between the exact and umdp updates is that the max and
the sum over next step observations are exchanged  this causes the choice of ff vectors in
humdp to become independent of the observations  once the sum and max operations are
exchanged  the observations can be marginalized out  recall that the idea of swaps leads
to a number of approximation updates  see figure    for their summary 
  

fivalue function approximations for pomdps

    fixed strategy approximations
a finite state machine  fsm  model is used primarily to define a control strategy  such a
strategy does not require belief state updates since it directly maps sequences of observations
to sequences of actions  the value function of an fsm strategy is piecewise linear and convex
and can be found eciently in the number of memory states  section         while in the
policy iteration and policy approximation contexts the value function for a specific strategy
is used to quantify the goodness of the policy in the first place  the value function alone can
be also used as a substitute for the optimal value function  in this case  the value function
 defined over the belief space  equals
v c  b    max v c  x  b  
x m

p

where v c  x  b    s s v c  x  s b s  is obtained by solving a set of js jjm j linear equations
 section         as remarked earlier  the value for the fixed strategy lower bounds the
optimal value function  that is v c  v   
to simplify the comparison of the fixed strategy approximation to other approximations 
we can rewrite its solution also in terms of fixed strategy updates
 
 x

 
 

xxx

vbi    b    max    s   x  b s    
p  o  s  js   x  b s ffi   x  o   s      
x m s s
o  s s s   s
 
 
 x

 

xx

 

 

  max   b s    s   x     
p  o  s  js   x  ffi   x  o   s     
x m s s
o  s   s
   hf sm vbi   b  

the value function vbi is piecewise linear and convex and consists of jm j linear functions
ffi  x      for the infinite horizon discounted case ffi  x  s  represents the ith approximation of
v c  x  s   note that the update can be applied to the finite horizon case in a straightforward
way 
      quality of control

assume we have an fsm strategy and would like to use it as a substitute for the optimal
control policy  there are three different ways in which we can use it to extract the control 
the first is to simply execute the strategy represented by the fsm  there is no need
to update belief states in this case  the second possibility is to choose linear functions
corresponding to different memory states and their associated actions repeatedly in every
step  we refer to such a controller as a direct  dr  controller  this approach requires
updating of belief states in every step  on the other hand its control performance is no
worse than that of the fsm control  the final strategy discards all the information about
actions and extracts the policy by using the value function vb  b  and one step lookahead 
this method  la  requires both belief state updates and lookaheads and leads to the worst
reactive time  like dr  however  this strategy is guaranteed to be no worse than the fsm
controller  the following theorem relates the performances of the three controllers 
  

fihauskrecht

control performance

reaction times

  

     
  

  

time  sec 

score

    

  

     
    

  

     

  

 

      
dr controller

fsm controller

fsm controller

la controller

dr controller

la controller

figure     comparison of three different controllers  fsm  dr and la  for the maze  
problem and a collection of one action policies  control quality  left  and response time  right   error bars in the control performance graph indicate the
critical score difference at which any two methods become statistically different
at significance level      

theorem    let cf sm be an fsm controller  let cdr and cla be the direct and the
one step lookahead controllers constructed based on cf sm   then v c  b   v c  b  and
v c  b   v c  b  hold for all belief states b   i  
though we can prove that both the direct controller and the lookahead controller are
always better than the underlying fsm controller  see appendix for the full proof of the
theorem   we cannot show the similar property between the first two controllers for all initial
belief states  however  the lookahead approach typically tends to dominate  reecting the
usual trade off between control quality and response time  we illustrate this trade off on
our running maze   example and a collection of jaj one action policies  each generating a
sequence of the same action  control quality and response time results are shown in figure
    we see that the controller based on the fsm is the fastest of the three  but it is also the
worst in terms of control quality  on the other hand  the direct controller is slower  it needs
to update belief states in every step  but delivers better control  finally  the lookahead
controller is the slowest and has the best control performance 
f sm

f sm

dr

la

      selecting the fsm model

the quality of a fixed strategy approximation depends strongly on the fsm model used 
the model can be provided a priori or constructed automatically  techniques for automatic
construction of fsm policies correspond to a search problem in which either the complete or
a restricted space of policies is examined to find the optimal or the near optimal policy for
such a space  the search process is equivalent to policy approximations or policy iteration
techniques discussed earlier in sections     and     

  

fivalue function approximations for pomdps

    grid based approximations with value interpolation extrapolation
a value function over a continuous belief space can be approximated by a finite set of grid
points g and an interpolation extrapolation rule that estimates the value of an arbitrary
point of the belief space by relying only on the points of the grid and their associated values 
definition    interpolation extrapolation rule  let f   i   ir be a real valued function
defined over the information space i   g   fbg    bg       bgk g be a set of grid points and  g  
f bg    f  bg       bg    f  bg            bgk   f  bgk  g be a set of point value pairs  a function rg  
i   i  ir jgj   ir that estimates f at any point of the information space i using only
values associated with grid points is called an interpolation extrapolation rule 
the main advantage of an interpolation extrapolation model in estimating the true value
function is that it requires us to compute value updates only for a finite set of grid points
g  let vbi be the approximation of the ith value function  then the approximation for the
 i     th value function vbi   can be obtained as
vbi    b    rg  b   gi     
where values associated with every grid point bgj   g  and included in  gi     are 

 i    bgj      h vbi   bgj     max
a a

 

 b  a    

x
o  

p  ojb  a vbi    bgj   o  a  

 

 

   

the grid based update can also be described in terms of a value function mapping hg  
vbi     hg vbi   the complexity of such an update is o jgjjajjs j  jjceval  rg   jgj   where
ceval  rg   jgj  is the computational cost of evaluating the interpolation extrapolation rule
rg for jgj grid points  we show later  section         that in some instances  the need to
evaluate the interpolation extrapolation rule in every step can be eliminated 
      a family of convex rules

the number of all possible interpolation extrapolation rules is enormous  we focus on a
set of convex rules that is a relatively small but very important subset of interpolationextrapolation rules   

definition    convex rule  let f be some function defined over the space i   g   fbg    bg       bgk g
be a set of grid points  and  g   f bg    f  bg       bg    f  bg            bgk   f  bgk   g be a set of pointvalue pairs  the rule rg for estimating f using  g is called convex when for every b   i  
the value fb b  is 
fb b    rg  b   g    
such that    bj    for every j           jgj  and

jgj
x
bj f  bj   

j   

pjgj

b
j    j

    

    we note that convex rules used in our work are a special case of averagers introduced by gordon        
the difference is minor  the definition of an averager includes a constant  independent of grid points and
their values  that is added to the convex combination 
  

fihauskrecht

the key property of convex rules is that their corresponding grid based update hg is a
contraction in the max norm  gordon         thus  the approximate value iteration based
on hg converges to the unique fixed point solution  in addition  hg based on convex rules
is isotone 
      examples of convex rules

the family of convex rules includes approaches that are very commonly used in practice 
like nearest neighbor  kernel regression  linear point interpolations and many others 
take  for example  the nearest neighbor approach  the function for a belief point b is
estimated using the value at the grid point closest to it in terms of some distance metric m
defined over the belief space  then  for any point b  there is exactly one nonzero parameter
bj     such that k b bgj km k b bgi km holds for all i              k  all other s are
zero  assuming the euclidean distance metric  the nearest neighbor approach leads to a
piecewise constant approximation  in which regions with equal values correspond to regions
with a common nearest grid point 
the nearest neighbor estimates the function value by taking into an account only one
grid point and its value  kernel regression expands upon this by using more grid points  it
adds up and weights their contributions  values  according to their distance from the target
point  for example  assuming gaussian kernels  the weight for a grid point bgj is

bj   fi exp kb

bg
k m      
j

p
where fi is a normalizing constant ensuring that jjg  j bj     and  is a parameter that
attens or narrows weight functions  for the euclidean metric  the above kernel regression
rule leads to a smooth approximation of the function 
linear point interpolations are a subclass of convex rules that in addition to constraints
in definition   satisfy
jgj
x
b   bj bgj  
j   

that is  a belief point b is a convex combination of grid points and the s are the corresponding coecients  because the optimal value function for the pomdp is convex  the
new constraint is sucient to prove the upper bound property of the approximation  in
general  there can be many different linear point interpolations for a given grid  a challenging problem here is to find the rule with the best approximation  we discuss these issues
in section       
      conversion to a grid based mdp

assume that we would like to find the approximation of the value function using our gridbased convex rule and grid based update  equation     we can view this process also as
a process of finding a sequence of values     bgj        bgj          i  bgj       for all grid points
bgj   g  we show that in some instances the sequence of values can be computed without
applying an interpolation extrapolation rule in every step  in such cases  the problem can
  

fivalue function approximations for pomdps

be converted into a fully observable mdp with states corresponding to grid points g    we
call this mdp a grid based mdp 

theorem    let g be a finite set of grid points and rg be a convex rule such that parameters bj are fixed  then the values of   bgj   for all bgj   g can be found by solving a fully
observable mdp with jgj states and the same discount factor   
proof for any grid point bgj we can write 
 

 i    bgj     max  bgj   a    
a a

 
 

x
o 

x

p  ojbgj   a vbig    bgj   a  o  

 

  
jgj
 
x
g   
p  ojbgj   a    o a
 
 
b
j k i k  
 

  max   bgj   a    
a a
o 
k  
 
 
  
jgj
 h
 
i
x
x
  max    bgj   a      gi  bgk  
p  ojbgj   a o a
j k  
a a
k  
o 
p

g g
now denoting   o  p  ojbj   a g o a
j k   as p  bk jbj   a   we can construct a fully observable
mdp problem with states corresponding to grid points g and the same discount factor   
the update step equals 

 
 

 i    bgj     max   bgj   a    
a a

jgj
x
k  

 
 

p  bgk jbgj   a  gi  bgk     

p
the prerequisite    bj    for every j           jgj and jjg  j bj     guarantees that
p  bgk jbgj   a  can be interpreted as true probabilities  thus  one can compute values   bgj  
by solving the equivalent fully observable mdp   
      solving grid based approximations

the idea of converting a grid based approximation into a grid based mdp is a basis of
our simple but very powerful approximation algorithm  briey  the key here is to find
the parameters  transition probabilities and rewards  of a new mdp model and then solve
it  this process is relatively easy if the parameters  used to interpolate extrapolate the
value of a non grid point are fixed  the assumption of theorem      in such a case  we
can determine parameters of the new mdp eciently in one step  for any grid set g  the
nearest neighbor or the kernel regression are examples of rules with this property  note that
this leads to polynomial time algorithms for finding values for all grid points  recall that an
mdp can be solved eciently for both finite and discounted  infinite horizon criteria  
the problem in solving grid based approximation arises only when the parameters 
used in the interpolation extrapolation are not fixed and are subject to the optimization
itself  this happens  for example  when there are multiple ways of interpolating a value
    we note that a similar result has been also proved independently by gordon        
  

fihauskrecht

at some point of the belief space and we would like to find the best interpolation  leading
to the best values  for all grid points in g  in such a case  the corresponding  optimal 
grid based mdp cannot be found in a single step and iterative approximation  solving a
sequence of grid based mdps  is usually needed  the worst case complexity of this problem
remains an open question 
      constructing grids

an issue we have not touched on so far is the selection of grids  there are multiple ways to
select grids  we divide them into two classes   regular and non regular grids 
regular grids  lovejoy      a  partition the belief space evenly into equal size regions   
the main advantage of regular grids is the simplicity with which we can locate grid points
in the neighborhood of any belief point  the disadvantage of regular grids is that they
are restricted to a specific number of points  and any increase in grid resolution is paid for
in an exponential increase in the grid size  for example  a sequence of regular grids for a
   dimensional belief space  corresponds to a pomdp with    states  consists of         
                      grid points    this prevents one from using the method with higher
grid resolutions for problems with larger state spaces 
non regular grids are unrestricted and thus provide for more exibility when grid resolution must be increased adaptively  on the other hand  due to irregularities  methods for
locating grid points adjacent to an arbitrary belief point are usually more complex when
compared to regular grids 
      linear point interpolation

the fact that the optimal value function v  is convex for a belief state mdps can be used
to show that the approximation based on linear point interpolation always upper bounds
the exact solution  lovejoy      a         neither kernel regression nor nearest neighbor
can guarantee us any bound 

theorem     upper bound property of a grid based point interpolation update   let vbi be
a convex value function  then h vbi  hg vbi  
the upper bound property of hg update for convex value functions follows directly
from jensen s inequality  the convergence to an upper bound follows from theorem   
note that the point interpolation update imposes an additional constraint on the choice
of grid points  in particular  it is easy to see that any valid grid must also include extreme points of the belief simplex  extreme points correspond to                                
    regular grids used by lovejoy      a  are based on freudenthal triangulation  eaves         essentially  this is the same idea as used to partition evenly the n dimensional subspace of ir   in fact  an
ane transform allows us to map isomorphically grid points in the belief space to grid points in the
n dimensional space  lovejoy      a  
    the number of points in the regular grid sequence is given by  lovejoy      a  
n

  js j    
 
jgj    m
m   js j    

where m            is a grid refinement parameter 

  

fivalue function approximations for pomdps

etc    without extreme points one would be unable to cover the whole belief space via
interpolation  nearest neighbor and kernel regression impose no restrictions on the grid 
      finding the best interpolation

in a general  there are multiple ways to interpolate a point of a belief space  our objective
is to find the best interpolation  that is  the one that leads to the tightest upper bound of
the optimal value function 
let b be a belief point and f bj   f  bj   jbj   gg a set of grid value pairs  then the best
interpolation for point b is 
jgj
x
fb b    min j f  bj  
 j   
p
          jgj  jjg  j j

p
subject to    j    for all j
     and b   jjg  j j bgj  
this is a linear optimization problem  although it can be solved in polynomial time
 using linear programming techniques   the computational cost of doing this is still relatively
large  especially considering the fact that the optimization must be repeated many times 
to alleviate this problem we seek more ecient ways of finding the interpolation  sacrificing
the optimality 
one way to find a  suboptimal  interpolation quickly is to apply regular grids proposed
by lovejoy      a   in this case the value at a belief point is approximated using the
convex combination of grid points closest to it  the approximation leads to piecewise linear
and convex value functions  as all interpolations are fixed here  the problem of finding
the approximation can be converted into an equivalent grid based mdp and solved as a
finite state mdp  however  as pointed in the previous section  the regular grids must use a
specific number of grid points and any increase in the resolution of a grid is paid for by an
exponential increase in the grid size  this feature makes the method less attractive when
we have a problem with a large state space and we need to achieve high grid resolution   
in the present work we focus on non regular  or arbitrary  grids  we propose an interpolation approach that searches a limited space of interpolations and is guaranteed to run
in time linear in the size of the grid  the idea of the approach is to interpolate a point
b of a belief space of dimension js j with a set of grid points that consists of an arbitrary
grid point b    g and js j   extreme points of the belief simplex  the coecients of this
interpolation can be found eciently and we search for the best such interpolation  let
b    g be a grid point defining one such interpolation  then the value at point b satisfies
 

bb
vbi  b    min
  vi  b  
b  g

where vbib  is the value of the interpolation for the grid point b    figure    illustrates the
resulting approximation  the function is characterized by its  sawtooth  shape  which is
inuenced by the choice of the interpolating set 
to find the best value function solution or its close approximation we can apply a value
iteration procedure in which we search for the best interpolation after every update step 
    one solution to this problem may be to use adaptive regular grids in which grid resolution is increased
only in some parts of the belief space  we leave this idea for future work 
  

fihauskrecht

v b 

v b 
 

v b 

  b
 

b 

b 

b 

b     b s  
 

figure     value function approximation based on the linear time interpolation approach
 a two dimensional case   interpolating sets are restricted to a single internal
point of the belief space 
the drawback of this approach is that interpolations may remain unchanged for many
update steps  thus slowing down the solution process  an alternative approach is to solve
a sequence of grid based mdps instead  in particular  at every stage we find the best
 minimum value  interpolations for all belief points reachable from grid points in one step  fix
coecients of these interpolations  s   construct a grid based mdp and solve it  exactly or
approximately   this process is repeated until no further improvement  or no improvement
larger than some threshold  is seen in values at different grid points 
      improving grids adaptively

the quality of an approximation  bound  depends strongly on the points used in the grid 
our objective is to provide a good approximation with the smallest possible set of grid
points  however  this task is impossible to achieve  since it cannot be known in advance
 before solving  what belief points to pick  a way to address this problem is to build grids
incrementally  starting from a small set of grid points and adding others adaptively  but
only in places with a greater chance of improvement  the key part of this approach is a
heuristic for choosing grid points to be added next 
one heuristic method we have developed attempts to maximize improvements in bound
values via stochastic simulations  the method builds on the fact that every interpolation
grid must also include extreme points  otherwise we cannot cover the entire belief space  
as extreme points and their values affect the other grid points  we try to improve their
values in the first place  in general  a value at any grid point b improves more the more
precise values are used for its successor belief points  that is  belief states that correspond
to   b  a   o  for a choice of observation o  a is the current optimal action choice for b 
incorporating such points into the grid then makes a larger improvement in the value at
the initial grid point b more likely  assuming that our initial point is an extreme point  we
have a heuristic that tends to improve a value for that point  naturally  one can proceed
further with this selection by incorporating the successor points for the first level successors
into the grid as well  and so forth 
  

fivalue function approximations for pomdps

generate new grid points  g  vb g  
set gnew   fg
for all extreme points b do
repeat until b    g   gnew
n
o
p
set a   arg maxa  b  a     o  p  ojb  a vb g    b  a  o  
select observation o according to p  ojb  a  
update b     b  a   o 
add b into gnew
return gnew
figure     procedure for generating additional grid points based on our bound improvement heuristic 

bound quality
mdp

fast interpolation
qmdp
informed regular grid

interpolation
adaptive grid

interpolation
random grid

   
                                     

   

score

  
   

  

  

           

                   

  

  

figure     improvement in the upper bound quality for grid based point interpolations
based on the adaptive grid method  the method is compared to randomly
refined grid and the regular grid with     points  other upper bound approximations  the mdp  qmdp and fast informed bound methods  are included for
comparison 
to capture this idea  we generate new grid points via simulation  starting from one
of the extremes of the belief simplex and continuing until a belief point not currently in
the grid is reached  an algorithm that implements the bound improvement heuristic and
expands the current grid g with a set of js j new grid points while relying on the current
value function approximation vb g is shown in figure    
figure    illustrates the performance  bound quality  of our adaptive grid method on
the maze   problem  here we use the combination of adaptive grids with our linear time
interpolation approach  the method gradually expands the grid in    point increments up to
    grid points  figure    also shows the performance of the random grid method in which
  

fihauskrecht

running times
    
    
    
   

time  sec 

    
    

   

    
   

   

    
   

   

    

       

   

    

   

   

   
    

    

     

     

   

   

   
   
     

   

 
mdp qmdp fast
informed

interpolation
regular grid

interpolation
adaptive grid

interpolation
random grid

figure     running times of grid based point interpolation methods  methods tested include the adaptive grid  the random grid  and the regular grid with     grid
points  running times for the adaptive grid are cumulative  reecting the dependencies of higher grid resolutions on the lower level resolutions  the running
time results for the mdp  qmdp  and fast informed bound approximations are
shown for comparison 
new points of the grid are selected iniformly at random  results for    grid point increments
are shown   in addition  the figure gives results for the regular grid interpolation  based
on lovejoy      a   with     belief points and other upper bound methods  the mdp  the
qmdp and the fast informed bound approximations 
we see a dramatic improvement in the quality of the bound for the adaptive method 
in contrast to this  the uniformly sampled grid  random grid approach  hardly changes the
bound  there are two reasons for this      uniformly sampled grid points are more likely to
be concentrated in the center of the belief simplex      the transition matrix for the maze  
problem is relatively sparse  the belief points one obtains from the extreme points in one
step are on the boundary of the simplex  since grid points in the center of the simplex
are never used to interpolate belief states reachable from extremes in one step they cannot
improve values at extremes and the bound does not change 
one drawback of the adaptive method is its running time  for every grid size we need
to solve a sequence of grid based mdps   figure    compares running times of different
methods on the maze   problem  as grid expansion of the adaptive method depends on
the value function obtained for previous steps  we plot its cumulative running times  we
see a relatively large increase in running time  especially for larger grid sizes  reecting
the trade off between the bound quality and running time  however  we note that the
adaptive grid method performs quite well in the initial few steps  and with only    grid
points outperforms the regular grid  with     points  in bound quality 
finally  we note that other heuristic approaches to constructing adaptive grids for point
interpolation are possible  for example  a different approach that refines the grid by ex 

  

fivalue function approximations for pomdps

control performance
  

score

  

   

  

   
          

  
  

   
  

   
   

   

  

  

  

fast
interpolation interpolation
mdp qmdp informed  regular grid   adaptive grid 

interpolation
 random grid 

nearest neighbor nearest neighbor
 adaptive grid 
 random grid 

figure     control performance of lookahead controllers based on grid based point interpolation and nearest neighbor methods and varying grid sizes  the results are
compared to the mdp  the qmdp and the fast informed bound controllers 
amining differences in values at current grid points has recently been proposed by brafman
       
      control

value functions obtained for different grid based methods define a variety of controllers  figure    compares the performances of lookahead controllers based on the point interpolation
and nearest neighbor methods  we run two versions of both approaches  one with the adaptive grid  the other with the random grid  and we show results obtained for         and    
grid points  in addition  we compare their performances to the interpolation with regular
grids  with     grid points   the mdp  the qmdp and the fast informed bound approaches 
overall  the performance of the interpolation extrapolation techniques we tested on
the maze   problem was a bit disappointing  in particular  better scores were achieved
by the simpler qmdp and fast informed bound methods  we see that  although heuristics
improved the bound quality of approximations  they did not lead to the similar improvement
over the qmdp and the fast informed bound methods in terms of control  this result
shows that a bad bound  in terms of absolute values  does not always imply bad control
performance  the main reason for this is that the control performance is inuenced mostly
by relative rather than absolute value function values  or  in other words  by the shape
of the function   all interpolation extrapolation techniques we use  except regular grid
interpolation  approximate the value function with functions that are not piecewise linear
and convex  the interpolations are based on the linear time interpolation technique with a
sawtooth shaped function  and the nearest neighbor leads to a piecewise constant function 
this does not allow them to match the shape of the optimal function correctly  the other
factor that affects the performance is a large sensitivity of methods to the selection of grid
points  as documented  for example  by the comparison of heuristic and random grids 

  

fihauskrecht

in the above tests we focused on lookahead controllers only  however  an alternative way
to define a controller for grid based interpolation extrapolation methods is to use q function
approximations instead of value functions  and either direct or lookahead designs    qfunction approximations can be found by solving the same grid based mdp  and by keeping
values  functions  for different actions separate at the end 

    approximations of value functions using curve fitting  least squares
fit 
an alternative way to approximate a function over a continuous space is to use curve fitting
techniques  this approach relies on a predefined parametric model of the value function
and a set of values associated with a finite set of  grid  belief points g  the approach
is similar to interpolation extrapolation techniques in that it relies on a set of belief value
pairs  the difference is that the curve fitting  instead of remembering all belief value pairs 
tries to summarize them in terms of a given parametric function model  the strategy seeks
the best possible match between model parameters and observed point values  the best
match can be defined using various criteria  most often the least squares fit criterion  where
the objective is to minimize
error f    

 x
 y
  j j

f  bj      

here bj and yj correspond to the belief point and its associated value  the index j ranges
over all points of the sample set g 
      combining dynamic programming and least squares fit

the least squares approximation of a function can be used to construct a dynamic programming
algorithm with an update step  vbi     hlsf vbi   the approach has two steps  first  we
obtain new values for a set of sample points g 

 i    b     h vbi   b    max
a a

 

x
s s

 s  a b s    

xx
o  s s

 
b
p  ojs  a b s vi    b  a  o    

second  we fit the parameters of the value function model vbi   using new sample value pairs
and the square error cost function  the complexity of the update is o jgjjajjs j  jjceval  vbi   
cfit  vbi     jgj   time  where ceval vbi   is the computational cost of evaluating vbi and
cfit  vbi     jgj  is the cost of fitting parameters of vbi   to jgj belief value pairs 
the advantage of the approximation based on the least squares fit is that it requires us
to compute updates only for the finite set of belief states  the drawback of the approach
is that  when combined with the value iteration method  it can lead to instability and or
divergence  this has been shown for mdps by several researchers  bertsekas        boyan
  moore        baird        tsitsiklis   roy        
    this is similar to the qmdp method  which allows both lookahead and greedy designs  in fact  qmdp
can be viewed as a special case of the grid based method with q function approximations  where grid
points correspond to extremes of the belief simplex 
  

fivalue function approximations for pomdps

      on line version of the least squares fit

the problem of finding a set of parameters with the best fit can be solved by any available
optimization procedure  this includes the on line  or instance based  version of the gradient
descent method  which corresponds to the well known delta rule  rumelhart  hinton   
williams        
let f denote a parametric value function over the belief space with adjustable weights
w   fw    w         wk g  then the on line update for a weight wi is computed as 

wi

wi ffi  f  bj   yj  

 f
j  
 wi b
j

where ffi is a learning constant  and bj and yj are the last seen point and its value  note
that the gradient descent method requires the function to be differentiable with regard to
adjustable weights 
to solve the discounted infinite horizon problem  the stochastic  on line  version of a
least squares fit can be combined with either parallel  synchronous  or incremental  gaussseidel  point updates  in the first case  the value function from the previous step is fixed
and a new value function is computed from scratch using a set of belief point samples and
their values computed through one step expansion  once the parameters are stabilized  by
attenuating learning rates   the newly acquired function is fixed  and the process proceeds
with another iteration  in the incremental version  a single value function model is at the
same time updated and used to compute new values at sampled points  littman et al        
and parr and russell        implement this approach using asynchronous reinforcement
learning backups in which sample points to be updated next are obtained via stochastic
simulation  we stress that all versions are subject to the threat of instability and divergence 
as remarked above 
      parametric function models

to apply the least squares approach we must first select an appropriate value function
model  examples of simple convex functions are linear or quadratic functions  but more
complex models are possible as well 
one interesting and relatively simple approach is based on the least squares approximation of linear action value functions  q functions   littman et al          here the
value function vbi   is approximated as a piecewise linear and convex combination of qb i  
functions 
vbi    b    max qb i    b  a  
a a
where qb i    b  a  is the least squares fit of a linear function for a set of sample points g 
values at points in g are obtained as

 ai    b     b  a    

x

o 

p  ojb  a vbi    b  o  a   

the method leads to an approximation with jaj linear functions and the coecients of these
functions can be found eciently by solving a set of linear equations  recall that other two
approximations  the qmdp and the fast informed bound approximations  also work with
  

fihauskrecht

jaj linear functions  the main differences between the methods are that the qmdp and

fast informed bound methods update linear functions directly  and they guarantee upper
bounds and unique convergence 
a more sophisticated parametric model of a convex function is the softmax model  parr
  russell        
 

vb  b     

 
x x

ff 

s s

 k    
ff s b s   

k

 

where is the set of linear functions ff with adaptive parameters to fit and k is a  temperature  parameter that provides a better fit to the underlying piecewise linear convex function
for larger values  the function represents a soft approximation of a piecewise linear convex
function  with the parameter k smoothing the approximation 
      control

we tested the control performance of the least squares approach on the linear q function
model  littman et al         and the softmax model  parr   russell         for the softmax
model we varied the number of linear functions  trying cases with    and    linear functions
respectively  in the first set of experiments we used parallel  synchronous  updates and
samples at a fixed set of     belief points  we applied stochastic gradient descent techniques
to find the best fit in both cases  we tested the control performance for value function
approximations obtained after        and    updates  starting from the qmdp solution  in
the second set of experiments  we applied the incremental stochastic update scheme with
gauss seidel style updates  the results for this method were acquired after every grid point
was updated     times  with learning rates decreasing linearly in the range between     and
       again we started from the qmdp solution  the results for lookahead controllers are
summarized in figure     which also shows the control performance of the direct q function
controller and  for comparison  the results for the qmdp method 
the linear q function model performed very well and the results for the lookahead design
were better than the results for the qmdp method  the difference was quite apparent for
direct approaches  in general  the good performance of the method can be attributed to
the choice of a function model that let us match the shape of the optimal value function
reasonably well  in contrast  the softmax models  with    and    linear functions  did not
perform as expected  this is probably because in the softmax model all linear functions are
updated for every sample point  this leads to situations in which multiple linear functions
try to track a belief point during its update  under these circumstances it is hard to capture
the structure of the optimal value function accurately  the other negative feature is that
the effects of on line changes of all linear functions are added in the softmax approximation 
and thus could bias incremental update schemes  in the ideal case  we would like to identify
one vector ff responsible for a specific belief point and update  modify  only that vector 
the linear q function approach avoids this problem by always updating only a single linear
function  corresponding to an action  

  

fivalue function approximations for pomdps

control performance
  

  
lookahead

     
iter iter    stoch
iter

     
iter iter

        
iter iter iter

score

  

  

stoch

  
iter stoch

  
   iter
iter

  
iter

stoch

direct

  

  
qmdp
approximation

linear q function
lookahead

linear q function
direct

softmax
    linear functions 

softmax
    linear functions 

figure     control performance of least squares fit methods  models tested include  linear
q function model  with both direct and lookahead control  and softmax models with    and    linear functions  lookahead control only   value functions
obtained after        and    synchronous updates and value functions obtained
through the incremental stochastic update scheme are used to define different
controllers  for comparison  we also include results for two qmdp controllers 

    grid based approximations with linear function updates
an alternative grid based approximation method can be constructed by applying sondik s
approach for computing derivatives  linear functions  to points of the grid  lovejoy      a 
       let vbi be a piecewise linear convex function described by a set of linear functions i  
then a new linear function for a belief point b and an action a can be computed eciently
as  smallwood   sondik        littman       
ffb a
i    s     s  a    

xx
o  s   s

p  s    ojs  a ffi b a o   s    

where  b  a  o  indexes a linear function ffi in a set of linear functions
maximizes the expression
 
x x

s   s s s

    
i

 defining vbi   that

 

p  s    ojs  a b s  ffi  s   

for a fixed combination of b  a  o  the optimizing function for b is then acquired by choosing
the vector with the best overall value from all action vectors  that is  assuming bi   is a
set of all candidate linear functions  the resulting functions satisfies
   arg max x ffb  s b s  
ffb i  
i  
ff         s s
a collection of linear functions obtained for a set of belief points can be combined into
a piecewise linear and convex value function  this is the idea behind a number of exact
b
i

b
i

  

fihauskrecht

v b 
 

v b 
v b 
new linear function

 

b

 

b s   

figure     an incremental version of the grid based linear function method  the piecewise
linear lower bound is improved by a new linear function computed for a belief
point b using sondik s method 
algorithms  see section         however  in the exact case  a set of points that cover all
linear functions defining the new value function must be located first  which is a hard task
in itself  in contrast  the approximation method uses an incomplete set of belief points that
are fixed or at least easy to locate  for example via random or heuristic selection  we use
hgl to denote the value function mapping for the grid approach 
the advantage of the grid based method is that it leads to more ecient updates  the
time complexity of the update is polynomial and equals o jgjjajjs j  jj   it yields a set of
jgj linear functions  compared to jajj i jjj possible functions for the exact update 
since the set of grid points is incomplete  the resulting approximation lower bounds the
value function one would obtain by performing the exact update  lovejoy      a  

theorem     lower bound property of the grid based linear function update   let vbi be a
piecewise linear value function and g a set of grid points used to compute linear function
updates  then hgl vbi  h vbi  
      incremental linear function approach

the drawback of the grid based linear function method is that hgl is not a contraction
for the discounted infinite horizon case  and therefore the value iteration method based on
the mapping may not converge  lovejoy      a   to remedy this problem  we propose an
incremental version of the grid based linear function method  the idea of this refinement is
to prevent instability by gradually improving the piecewise linear and convex lower bound
of the value function 
assume that vbi  v  is a convex piecewise linear lower bound of the optimal value
function defined by a linear function set i   and let ffb be a linear function for a point b
that is computed from vbi using sondik s method  then one can construct a new improved
value function vbi    vbi by simply adding the new linear function ffb into i   that is 
i     i   ffb   the idea of the incremental update  illustrated in figure     is similar
to incremental methods used by cheng        and lovejoy         the method can be
  

fivalue function approximations for pomdps

running times

bound quality

    

  
 

    

  

  

 

score

  
  
  
  

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

time  sec 

 

  

 

 

    

  

 

 
 
    

 

 
 

 

 
 

   

 

 

 

  
    

     

 

 

 

 

  
standard approach

qmdp

incremental approach

fast
informed

standard approach

incremental approach

figure     bound quality and running times of the standard and incremental version of
the grid based linear function method for the fixed    point grid  cumulative
running times  including all previous update cycles  are shown for both methods 
running times of the qmdp and the fast informed bound methods are included
for comparison 
extended to handle a set of grid points g in a straightforward way  note also that after
adding one or more new linear functions to i   some of the previous linear functions may
become redundant and can be removed from the value function  techniques for redundancy
checking are the same as are applied in the exact approaches  monahan        eagle        
the incremental refinement is stable and converges for a fixed set of grid points  the
price paid for this feature is that the linear function set i can grow in size over the iteration
steps  although the growth is at most linear in the number of iterations  compared to
the potentially exponential growth of exact methods  the linear function set describing
the piecewise linear approximation can become huge  thus  in practice we usually stop
incremental updates well before the method converges  the question that remains open is
the complexity  hardness  of the problem of finding the fixed point solution for a fixed set
of grid points g 
figure    illustrates some of the trade offs involved in applying incremental updates
compared to the standard fixed grid approach on the maze   problem  we use the same
grid of    points for both techniques and the same initial value function  results for     
update cycles are shown  we see that the incremental method has longer running times
than the standard method  since the number of linear functions can grow after every update 
on the other hand  the bound quality of the incremental method improves more rapidly
and it can never become worse after more update steps 
      minimum expected reward

the incremental method improves the lower bound of the value function  the value function  say vbi   can be used to create a controller  with either the lookahead or direct action
choice   in the general case  we cannot say anything about the performance quality of
such controllers with regard to vbi   however  under certain conditions the performance of
both controllers is guaranteed never to fall below vbi   the following theorem  proved in the
appendix  establishes these conditions 

theorem    let vbi be a value function obtained via the incremental linear function method 
starting from vb    which corresponds to some fixed strategy c    let cla i and cdr i be two
  

fihauskrecht

controllers based on vbi   the lookahead controller and the direct action controller  and v c  
vc
be their respective value functions  then vbi  v c
and vbi  v c
hold 
we note that the same property holds for the incremental version of exact value iteration 
that is  both the lookahead and the direct controllers perform no worse than vi obtained
after i incremental updates from some v  corresponding to a fsm controller c   
la i

dr i

la i

dr i

      selecting grid points

the incremental version of the grid based linear function approximation is exible and
works for an arbitrary grid    moreover  the grid need not be fixed and can be changed on
line  thus  the problem of finding grids reduces to the problem of selecting belief points to
be updated next  one can apply various strategies to do this  for example  one can use a
fixed set of grid points and update them repeatedly  or one can select belief points on line
using various heuristics 
the incremental linear function method guarantees that the value function is always
improved  all linear functions from previous steps are kept unless found to be redundant  
the quality of a new linear function  to be added next  depends strongly on the quality of
linear functions obtained in previous steps  therefore  our objective is to select and order
points with better chances of larger improvement  to do this we have designed two heuristic
strategies for selecting and ordering belief points 
the first strategy attempts to optimize updates at extreme points of the belief simplex
by ordering them heuristically  the idea of the heuristic is based on the fact that states
with higher expected rewards  e g  some designated goal states  backpropagate their effects
 rewards  locally  therefore  it is desirable that states in the neighborhood of the highest
reward state be updated first  and the distant ones later  we apply this idea to order
extreme points of the belief simplex  relying on the current estimate of the value function
to identify the highest expected reward states and on a pomdp model to determine the
neighbor states 
the second strategy is based on the idea of stochastic simulation  the strategy generates
a sequence of belief points more likely to be reached from some  fixed  initial belief point 
the points of the sequence are then used in reverse order to generate updates  the intent
of this heuristic is to  maximize  the improvement of the value function at the initial fixed
point  to run this heuristic  we need to find an initial belief point or a set of initial belief
points  to address this problem  we use the first heuristic that allows us to order the
extreme points of the belief simplex  these points are then used as initial beliefs for the
simulation part  thus  we have a two tier strategy  the top level strategy orders extremes
of the belief simplex  and the lower level strategy applies stochastic simulation to generate
a sequence of belief states more likely reachable from a specific extreme point 
we tested the order heuristics and the two tier heuristics on our maze   problem  and
compared them also to two simple point selection strategies  the fixed grid strategy  in
which a set of    grid points was updated repeatedly  and the random grid strategy  in
which points were always chosen uniformly at random  figure    shows the bound quality
    there is no restriction on the grid points that must be included in the grid  such as was required for
example in the linear point interpolation scheme  which had to use all extreme points of the belief
simplex 
  

fivalue function approximations for pomdps

bound quality
  
  

score

  
 

  

            
   
 

 

 

 
     

      
 

 

 

 

        
   

 

 

 

 

  

        
     

  
  
  
fixed grid

random grid

order heuristic

  tier heuristic

figure     improvements in the bound quality for the incremental linear function method
and four different grid selection heuristics  each cycle includes    grid point
updates 
of the methods for    update cycles  each cycle consists of    grid point updates  on the
maze   problem  we see that the differences in the quality of value function approximations
for different strategies  even the very simple ones  are relatively small  we note that we
observed similar results also for other problems  not just maze   
the relatively small improvement of our heuristics can be explained by the fact that
every new linear function inuences a larger portion of the belief space and thus the method
should be less sensitive to a choice of a specific point    however  another plausible explanation is that our heuristics were not very good and more accurate heuristics or combinations
of heuristics could be constructed  ecient strategies for locating grid points used in some
of the exact methods  e g  the witness algorithm  kaelbling et al         or cheng s methods  cheng        can potentially be applied to this problem  this remains an open area
of research 
      control

the grid based linear function approach leads to a piecewise linear and convex approximation  every linear function comes with a natural action choice that lets us choose the
action greedily  thus we can run both the lookahead and the direct controllers  figure   
compares the performance of four different controllers for the fixed grid of    points  combining standard and incremental updates with lookahead and direct greedy control after   
  and    update cycles  the results  see also figure     illustrate the trade offs between the
computational time of obtaining the solution and its quality  we see that the incremental
approach and the lookahead controller design tend to improve the control performance  the
prices paid are worse running and reaction times  respectively 
    the small sensitivity of the incremental method to the selection of grid points would suggest that one
could  in many instances  replace exact updates with simpler point selection strategies  this could
increase the speed of exact value iteration methods  at least in their initial stages   which suffer from
ineciencies associated with locating a complete set of grid points to be updated in every step  however 
this issue needs to be investigated 
  

fihauskrecht

control performance
  

 

  

  

  
 

lookahead

lookahead

 

  

 

  

 

 
 

 

score

  
direct
  

direct

  

  
qmdp

fast
informed

direct
standard

lookahead
standard

direct
incremental

lookahead
incremental

figure     control performance of four different controllers based on grid based linear function updates after      and    update cycles for the same    point grid  controllers represent combinations of two update strategies  standard and incremental  and two action extraction techniques  direct and lookahead   running
times for the two update strategies were presented in figure     for comparison we include also performances of the qmdp and the fast informed bound
methods  with both direct and lookahead designs  
control performance
  
 

score

 

  

  

 

 

  

 

 

  
 

 

  

  

  

  

  
qmdp

fast
informed

fixed grid

random grid

order heuristic

  tier heuristic

figure     control performances of lookahead controllers based on the incremental linearfunction approach and different point selection heuristics after      and    improvement cycles  for comparison  scores for the qmdp and the fast informed
bound approximations are shown as well 
figure    illustrates the effect of point selection heuristics on control  we compare the
results for lookahead control only  using approximations obtained after      and    improvement cycles  each cycle consists of    grid point updates   the test results show that  as

  

fivalue function approximations for pomdps

for the bound quality  there are no big differences among various heuristics  suggesting a
small sensitivity of control to the selection of grid points 

    summary of value function approximations
heuristic value function approximations methods allow us to replace hard to compute exact
methods and trade off solution quality for speed  there are numerous methods we can employ  each with different properties and different trade offs of quality versus speed  tables  
and   summarize main theoretical properties of the approximation methods covered in this
paper  the majority of these methods are of polynomial complexity or at least have ecient  polynomial  bellman updates  this makes them good candidates for more complex
pomdp problems that are out of reach of exact methods 
all of the methods are heuristic approximations in that they do not give solutions of a
guaranteed precision  despite this fact we proved that solutions of some of the methods are
no worse than others in terms of value function quality  see figure      this was one of the
main contributions of the paper  however  there are currently minimal theoretical results
relating these methods in terms of control performance  the exception are some results
for fsm controllers and fsm based approximations  the key observation here is that for
the quality of control  lookahead control  it is more important to approximate the shape
 derivatives  of the value function correctly  this is also illustrated empirically on gridbased interpolation extrapolation methods in section       that are based on non convex
value functions  the main challenges here are to find ways of analyzing and comparing
control performance of different approximations also theoretically and to identify classes of
pomdps for which certain methods dominate the others 
finally  we note that the list of methods is not complete and other value function approximation methods or the refinements of existing methods are possible  for example  white
and scherer        investigate methods based on truncated histories that lead to upper
and lower bound estimates of the value function for complete information states  complete
histories   also  additional restrictions on some of the methods can change the properties
of a more generic method  for example  it is possible that under additional assumptions
we will be able to ensure convergence of the least squares fit approximation 
   conclusions

pomdps offers an elegant mathematical framework for representing decision processes
in stochastic partially observable domains  despite their modeling advantages  however 
pomdp problems are hard to solve exactly  thus  the complexity of problem solvingprocedures becomes the key aspect in the sucessful application of the model to real world
problems  even at the expense of the optimality  as recent complexity results for the
approximability of pomdp problems are not encouraging  lusena et al         madani
et al          we focus on heuristic approximations  in particular approximations of value
functions 

  

fihauskrecht

method
mdp approximation
qmdp approximation
fast informed bound
umdp approximation
fixed strategy method
grid based interpolation extrapolation
nearest neighbor
kernel regression
linear point interpolation
curve fitting  least squares fit 
linear q function
grid based linear function method
incremental version  start from a lower bound 

bound
upper
upper
upper
lower
lower
upper
lower
lower

isotonicity

p
p
p
p
p
 p
p
p

contraction

 p

  

p
p
p
p
p
 p
p
p

table    properties of different value function approximation methods  bound property 
isotonicity and contraction property of the underlying mappings for         
    although incremental version of the grid based linear function method is not
a contraction it always converges 
method
mdp approximation
qmdp approximation
fast informed bound
umdp approximation
fixed strategy method
grid based interpolation extrapolation
nearest neighbor
kernel regression
linear point interpolation
fixed interpolation
best interpolation
curve fitting  least squares fit 
linear q function
grid based linear function method
incremental version

finite horizon
p
p
p
np hard
p
varies
p
p
p
p
p
varies
p
p
na

discounted infinite horizon
p
p
p
undecidable
p
na
p
p
varies
p
 
na
na
na
 

table    complexity of value function approximation methods for finite horizon problem
and discounted infinite horizon problem  the objective for the discounted infinitehorizon case is to find the corresponding fixed point solution  the complexity
results take into account  in addition to components of pomdps  also all other
approximation specific parameters  e g   the size of the grid g in grid based methods    indicates open instances and na methods that are not applicable to one
of the problems  e g  because of possible divergence  

  

fivalue function approximations for pomdps

    contributions
the paper surveys new and known value function approximation methods for solving pomdps 
we focus primarily on the theoretical analysis and comparison of the methods  with findings and results supported experimentally on a problem of moderate size from the agent
navigation domain  we analyze the methods from different perspectives  their computational complexity  capability to bound the optimal value function  convergence properties of
iterative implementations  and the quality of derived controllers  the analysis includes new
theoretical results  deriving the properties of individual approximations  and their relations
to exact methods  in general  the relations between and trade offs among different methods
are not well understood  we provide some new insights on these issues by analyzing their
corresponding updates  for example  we showed that the differences among the exact  the
mdp  the qmdp  the fast informed bound  and the umdp methods boil down to simple
mathematical manipulations and their subsequent effect on the value function approximation  this allowed us to determine relations among different methods in terms of quality of
their respective value functions which is one of the main results of the paper 
we also presented a number of new methods and heuristic refinements of some existing
techniques  the primary contributions in this area include the fast informed bound  gridbased point interpolation methods  including adaptive grid approaches based on stochastic sampling   and the incremental linear function method  we also showed that in some
instances the solutions can be obtained more eciently by converting the original approximation into an equivalent finite state mdp  for example  grid based approximations with
convex rules can be often solved via conversion into a grid based mdp  in which grid points
correspond to new states   leading to the polynomial complexity algorithm for both the finite and the discounted infinite horizon cases  section         this result can dramatically
improve the run time performance of the grid based approaches  a similar conversion to
the equivalent finite state mdp  allowing a polynomial time solution for the discounted
infinite horizon problem  was shown for the fast informed bound method  section      
    challenges and future directions
work on pomdps and their approximations is far from complete  some complexity results
remain open  in particular  the complexity of the grid based approach seeking the best interpolation  or the complexity of finding the fixed point solution for the incremental version
of the grid based linear function method  another interesting issue that needs more investigation is the convergence of value iteration with least squares approximation  although
the method can be unstable in the general case  it is possible that under certain restrictions
it will converge 
in the paper we use a single pomdp problem  maze    only to support theoretical
findings or to illustrate some intuitions  therefore  the results not supported theoretically  related mostly to control  cannot be generalized and used to rank different methods 
since their performance may vary on other problems  in general  the area of pomdps
and pomdp approximations suffers from a shortage of larger scale experimental work with
multiple problems of different complexities and a broad range of methods  experimental
work is especially needed to study and compare different methods with regard to control
quality  the main reason for this is that there are only few theoretical results relating the
  

fihauskrecht

control performance  these studies should help focus theoretical exploration by discovering
interesting cases and possibly identifying classes of problems for which certain approximations are more or less suitable  our preliminary experimental results show that there are
significant differences in control performance among different methods and that not all of
them may be suitable to approximate the control policies  for example  the grid based
nearest neighbor approach with piecewise constant approximation is typically inferior to
and outperformed by other simpler  and more ecient  value function methods 
the present work focused on heuristic approximation methods  we investigated general  at  pomdps and did not take advantage of any additional structural refinements 
however  real world problems usually offer more structure that can be exploited to devise
new algorithms and perhaps lead to further speed ups  it is also possible that some of the
restricted versions of pomdps  with additional structural assumptions  can be solved or
approximated eciently  even though the general complexity results for pomdps or their approximations are not very encouraging  papadimitriou   tsitsiklis        littman       
mundhenk et al         lusena et al         madani et al          a challenge here is to
identify models that allow ecient solutions and are at the same time interesting enough
from the point of application 
finally  a number of interesting issues arise when we move to problems with large state 
action  and observation spaces  here  the complexity of not only value function updates
but also belief state updates becomes an issue  in general  partial observability of hidden
process states does not allow us to factor and decompose belief states  and their updates  
even when transitions have a great deal of structure and can be represented very compactly 
promising directions to deal with these issues include various monte carlo approaches  isard
  blake        kanazawa  koller    russell        doucet        kearns et al          
methods for approximating belief states via decomposition  boyen   koller              
or a combination of the two approaches  mcallester   singh        
acknowledgements

anthony cassandra  thomas dean  leslie kaelbling  william long  peter szolovits and
anonymous reviewers provided valuable feedback and comments on this work  this research
was supported by grant ro  lm       and grant  t  lm      from the national library
of medicine  by dod advanced research project agency  arpa  under contract number
n         m      and darpa rome labs planning initiative grant f                
appendix a  theorems and proofs

a   convergence to the bound
theorem   let h  and h  be two value function mappings defined on v  and v  s t 
   h    h  are contractions with fixed points v    v   
   v    v  and h  v   h  v    v   

   h  is an isotone mapping 
then v   v  holds 
  

fivalue function approximations for pomdps

proof by applying h  to condition   and expanding the result with condition   again we
get  h   v   h  v   h  v    v    repeating this we get in the limit v       h n v  
   h   v   h v   h v    v    which proves the result   
a   accuracy of a lookahead controller based on bounds
theorem   let vbu and vbl be upper and lower bounds of the optimal value function for
the discounted infinite horizon problem  let    supb jvbu  b  vbl  b j   kvbu vbl k be
the maximum bound difference  then the expected reward for a lookahead controller vb la  
constructed for either vbu or vbl   satisfies kvb la v  k           

proof let vb denotes either an upper or lower bound approximation of v  and h la be the
value function mapping corresponding to the lookahead policy for vb   note  that since the
lookahead policy always optimizes its actions with regard to vb   h vb   h la vb must hold 
the error of vb la can be bounded using the triangle inequality

kvb la v  k  kvb la vb k   kvb v k 
the first component satisfies 

kvb la vb k   kh la vb la vb k
 kh la vb la h vb k   kh vb vb k
  kh la vb la h la vb k   kh vb vb k
  kvb la vb k   
the inequality  kh vb vb k   follows from the isotonicity of h and the fact that vb is either
an upper or a lower bound  rearranging the inequalities  we obtain  kvb la vb k           
the bound on the second term kvb v  k   is trivial 
     
therefore  kvb la v  k                      
     
a   mdp  qmdp and the fast informed bounds
theorem   a solution for the fast informed bound approximation can be found by solving
an mdp with js jjajjj states  jaj actions and the same discount factor   
proof let ffai be a linear function for action a defining vbi   let ffi  s  a  denote parameters
of the function  the parameters of vbi   satisfy 
ffi    s  a     s  a    
let

x

x
 
   
max
   a   p  s   ojs  a ffi  s   a   
a
o 
s  s

ffi    s  a  o    max
 

x

a   a s    s

  

p  s    ojs  a ffi  s    a    

fihauskrecht

now  we can rewrite ffi    s  a  o  for every s  a  o as 
 
 x

 

x

ffi    s  a  o    max
p  s    ojs  a    s    a      
a   a  s   s
o   
 
 
  x
 
  max
a   a    

 

 

  
 
ffi  s    a    o     

p  s    ojs  a  s    a         

x x

o    s   s

s  s

  
 
p  s    ojs  a ffi  s    a    o     

these equations define an mdp with state space s  a    action space a and discount
factor    thus  a solution for the fast informed bound update can be found by solving an
equivalent finite state mdp   

theorem   let vbi corresponds to a piecewise linear convex value function defined by
linear functions  then h vbi  hf ib vbi  hqmdp vbi  hmdp vbi  
proof
 
 x

x

i

 
 

xx

max    s  a b s    
max
p  s    ojs  a b s ffi  s    
a a s s
ff
 
 
o 
s  s s s
   hvi   b 
i

 max
a a

x
s s

 

b s    s  a    

   hf ib vi   b 

 max
a a

x
s s

b s    s  a    



s s

a a

 

   hmdp vbi   b 

 

p  s    ojs  a ffi  s    
 

x
s   s

 

b s  max   s  a    

max

x

o  ffi   i s   s

 

   hqmdp vbi   b 
x

x

i

p  s  js  a  max ffi  s    
ffi  

i

 

x
s   s

p  s  js  a  max ffi  s    
ffi  

i

a   fixed strategy approximations
theorem    let cf sm be an fsm controller  let cdr and cla be the direct and the
one step lookahead controllers constructed based on cf sm   then v c  b   v c  b  and
v c  b   v c  b  hold for all belief states b   i  
proof the value function for the fsm controller cf sm satisfies 
f sm

f sm

la

vc

f sm

where

 b    max v  x  b    v    b   b 
x m

v  x  b     b   x     

x
o 

p  ojb   x  v   x  o     b   x   o   
  

dr

fivalue function approximations for pomdps

the direct controller cdr selects the action greedily in every step  that is  it always
chooses according to  b    arg maxx m v  x  b   the lookahead controller cla selects the
action based on v  x  b  one step away 

la  b    arg max
a a

 

 

x

 
 b  a     p  ojb  a  max
   m v  x     b  a  o    
x
o 

by expanding the value function for cf sm for one step we get 

vc

f sm

 b    max v  x  b 
x m
 

 

x

  max  b   x      p  ojb   x  v   x  o     b   x   o  
x m
o 
   b     b      

  b     b      
 

x

o 

x

o 

   

p  ojb     b   v   x  o     b     b    o  

p  ojb     b    max
v  x      b     b    o  
 
x  m

x

   

 

 
 max
 b  a     p  ojb  a  max
   m v  x     b  a  o  
a a
x
o 
x
la
 
la
   b    b      p  ojb  la  b   max
   m v  x     b    b   o  
x
o 

   

iteratively expanding maxx   m v  x     in   and   with expression   and substituing improved
 higher value  expressions   and   back we obtain value functions for both the direct and
the lookahead controllers   expansions of   lead to the value for the direct controller
and expansions of   to the value for the lookahead controller   thus v c
 vc
c
c
and v
 v must hold  note  however  that action choices  b  and la b 
in expressions   and   can be different leading to different next step belief states and
subsequently to different expansion sequences  therefore  the above result does not imply
that v dr  b   v la  b  for all b   i    
f sm

f sm

dr

la

a   grid based linear function method
theorem    let vbi be a value function obtained via the incremental linear function method 
starting from vb    which corresponds to some fixed strategy c    let cla i and cdr i be two
controllers based on vbi   the lookahead controller and the direct action controller  and v c  
vc
be their respective value functions  then vbi  v c
and vbi  v c
hold 
proof by initializing the method with a value function for some fsm controller c    the
incremental updates can be interpreted as additions of new states to the fsm controller  a
new linear function corresponds to a new state of the fsm   let ci be a controller after
step i  then v c
  vbi holds and the inequalities follow from theorem      
la i

dr i

la i

f sm i

  

dr i

fihauskrecht

references

astrom  k  j          optimal control of markov decision processes with incomplete state
estimation  journal of mathematical analysis and applications              
baird  l  c          residual algorithms  reinforcement learning with function approximation  in proceedings of the twelfth international conference on machine learning 
pp        
barto  a  g   bradtke  s  j     singh  s  p          learning to act using real time dynamic
programming  artificial intelligence             
bellman  r  e          dynamic programming  princeton university press  princeton  nj 
bertsekas  d  p          a counter example to temporal differences learning  neural computation             
bertsekas  d  p          dynamic programming and optimal control  athena scientific 
bonet  b     geffner  h          learning sorting and classification with pomdps  in
proceedings of the fifteenth international conference on machine learning 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  artificial intelligence           
boutilier  c     poole  d          exploiting structure in policy construction  in proceedings
of the thirteenth national conference on artificial intelligence  pp            
boyan  j  a     moore  a  a          generalization in reinforcement learning  safely
approximating the value function  in advances in neural information processing
systems    mit press 
boyen  x     koller  d          tractable inference for complex stochastic processes  in
proceedings of the fourteenth conference on uncertainty in artificial intelligence  pp 
      
boyen  x     koller  d          exploiting the architecture of dynamic systems  in proceedings of the sixteenth national conference on artificial intelligence  pp          
brafman  r  i          a heuristic variable grid solution method for pomdps  in proceedings of the fourteenth national conference on artificial intelligence  pp          
burago  d   rougemont  m  d     slissenko  a          on the complexity of partially
observed markov decision processes  theoretical computer science               
cassandra  a  r          exact and approximate algorithms for partially observable markov
decision processes  ph d  thesis  brown university 
cassandra  a  r   littman  m  l     zhang  n  l          incremental pruning  a simple 
fast  exact algorithm for partially observable markov decision processes  in proceedings
of the thirteenth conference on uncertainty in artificial intelligence  pp        
  

fivalue function approximations for pomdps

casta non  d          approximate dynamic programming for sensor management  in
proceedings of conference on decision and control 
cheng  h  t          algorithms for partially observable markov decision processes  ph d 
thesis  university of british columbia 
condon  a          the complexity of stochastic games  information and computation 
            
dean  t     kanazawa  k          a model for reasoning about persistence and causation 
computational intelligence             
dearden  r     boutilier  c          abstraction and approximate decision theoretic planning  artificial intelligence              
doucet  a          on sequential simulation based methods for bayesian filtering  tech 
rep  cued f infeng tr      department of engineering  cambridge university 
drake  a          observation of a markov process through a noisy channel  ph d  thesis 
massachusetts institute of technology 
draper  d   hanks  s     weld  d          probabilistic planning with information gathering
and contingent execution  in proceedings of the second international conference on
ai planning systems  pp        
eagle  j  n          the optimal search for a moving target when search path is constrained 
operations research                
eaves  b          a course in triangulations for soving differential equations with deformations  springer verlag  berlin 
gordon  g  j          stable function approximation in dynamic programming  in proceedings of the twelfth international conference on machine learning 
hansen  e       a   an improved policy iteration algorithm for partially observable mdps 
in advances in neural information processing systems     mit press 
hansen  e       b   solving pomdps by searching in policy space  in proceedings of the
fourteenth conference on uncertainty in artificial intelligence  pp          
hauskrecht  m          planning and control in stochastic domains with imperfect information  ph d  thesis  massachusetts institute of technology 
hauskrecht  m     fraser  h          planning medical therapy using partially observable
markov decision processes  in proceedings of the ninth international workshop on
principles of diagnosis  dx      pp          
hauskrecht  m     fraser  h          planning treatment of ischemic heart disease with
partially observable markov decision processes  artificial intelligence in medicine     
        
  

fihauskrecht

heyman  d     sobel  m          stochastic methods in operations research  stochastic
optimization  mcgraw hill 
howard  r  a          dynamic programming and markov processes  mit press  cambridge 
howard  r  a     matheson  j          inuence diagrams  principles and applications of
decision analysis    
isard  m     blake  a          contour tracking by stochastic propagation of conditional
density  in proccedings of europian conference on computer vision  pp          
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in
partially observable stochastic domains  artificial intelligence              
kanazawa  k   koller  d     russell  s  j          stochastic simulation algorithms for
dynamic probabilistic networks  in proceedings of the eleventh conference on uncertainty in artificial intelligence  pp          
kearns  m   mansour  y     ng  a  y          a sparse sampling algorithm for near
optimal planning in large markov decision processes  in proceedings of the sixteenth
international joint conference on artificial intelligence  pp            
kjaerulff  u          a computational scheme for reasoning in dynamic probabilistic networks  in proceedings of the eighth conference on uncertainty in artificial intelligence  pp          
korf  r          depth first iterative deepening  an optimal admissible tree search  artificial
intelligence             
kushmerick  n   hanks  s     weld  d          an algorithm for probabilistic planning 
artificial intelligence              
lauritzen  s  l          graphical models  clarendon press 
littman  m  l          memoryless policies  theoretical limitations and practical results 
in cliff  d   husbands  p   meyer  j     wilson  s   eds    from animals to animats    proceedings of the third international conference on simulation of adaptive
behavior  mit press  cambridge 
littman  m  l          algorithms for sequential decision making  ph d  thesis  brown
university 
littman  m  l   cassandra  a  r     kaelbling  l  p          learning policies for partially
observable environments  scaling up  in proceedings of the twelfth international
conference on machine learning  pp          
lovejoy  w  s       a   computationally feasible bounds for partially observed markov
decision processes  operations research              
  

fivalue function approximations for pomdps

lovejoy  w  s       b   a survey of algorithmic methods for partially observed markov
decision processes  annals of operations research            
lovejoy  w  s          suboptimal policies with bounds for parameter adaptive decision
processes  operations research              
lusena  c   goldsmith  j     mundhenk  m          nonapproximability results for markov
decision processes  tech  rep   university of kentucky 
madani  o   hanks  s     condon  a          on the undecidability of probabilistic planning
and infinite horizon partially observable markov decision processes  in proceedings of
the sixteenth national conference on artificial intelligence 
mcallester  d     singh  s  p          approximate planning for factored pomdps using
belief state simplification  in proceedings of the fifteenth conference on uncertainty
in artificial intelligence  pp          
mccallum  r          instance based utile distinctions for reinforcement learning with
hidden state  in proceedings of the twelfth international conference on machine
learning 
monahan  g  e          a survey of partially observable markov decision processes  theory 
models  and algorithms  management science           
mundhenk  m   goldsmith  j   lusena  c     allender  e          encyclopaedia of complexity results for finite horizon markov decision process problems  tech  rep   cs
dept tr         university of kentucky 
papadimitriou  c  h     tsitsiklis  j  n          the complexity of markov decision processes  mathematics of operations research              
parr  r     russell  s          approximating optimal policies for partially observable
stochastic domains  in proceedings of the fourteenth international joint conference
on artificial intelligence  pp            
pearl  j          probabilistic reasoning in intelligent systems  morgan kaufman 
platzman  l  k          finite memory estimation and control of finite probabilistic systems 
ph d  thesis  massachusetts institute of technology 
platzman  l  k          a feasible computational approach to infinite horizon partiallyobserved markov decision problems  tech  rep   georgia institute of technology 
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley  new york 
raiffa  h          decision analysis  introductory lectures on choices under uncertainty 
addison wesley 
rumelhart  d   hinton  g  e     williams  r  j          learning internal representations
by error propagation  in parallel distributed processing  pp          
  

fihauskrecht

satia  j     lave  r          markovian decision processes with probabilistic observation
of states  management science           
singh  s  p   jaakkola  t     jordan  m  i          learning without state estimation
in partially observable markovian decision processes  in proceedings of the eleventh
international conference on machine learning  pp          
smallwood  r  d     sondik  e  j          the optimal control of partially observable
processes over a finite horizon  operations research                
sondik  e  j          the optimal control of partially observable markov decision processes 
ph d  thesis  stanford university 
sondik  e  j          the optimal control of partially observable processes over the infinite
horizon  discounted costs  operations research              
tatman  j     schachter  r  d          dynamic programming and inuence diagrams 
ieee transactions on systems  man and cybernetics              
tsitsiklis  j  n     roy  b  v          feature based methods for large scale dynamic
programming  machine learning            
washington  r          incremental markov model planning  in proceedings of the eight
ieee international conference on tools with artificial intelligence  pp        
white  c  c     scherer  w  t          finite memory suboptimal design for partially
observed markov decision processes  operations research              
williams  r  j     baird  l  c          tight performance bounds on greedy policies based
on imperfect value functions  in proceedings of the tenth yale workshop on adaptive
and learning systems yale university 
yost  k  a          solution of large scale allocation problems with partially observable
outcomes  ph d  thesis  naval postgraduate school  monterey  ca 
zhang  n  l     lee  s  s          planning with partially observable markov decision
processes  advances in exact solution method  in proceedings of the fourteenth conference on uncertainty in artificial intelligence  pp          
zhang  n  l     liu  w       a   a model approximation scheme for planning in partially
observable stochastic domains  journal of artificial intelligence research             
zhang  n  l     liu  w       b   region based approximations for planning in stochastic
domains  in proceedings of the thirteenth conference on uncertainty in artificial
intelligence  pp          

  

fi