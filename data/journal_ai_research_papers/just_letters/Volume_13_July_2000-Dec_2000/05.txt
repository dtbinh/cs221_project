journal of artificial intelligence research                  

submitted        published      

hierarchical reinforcement learning with the maxq value
function decomposition
thomas g  dietterich

department of computer science  oregon state university
corvallis  or      

abstract

tgd cs orst edu

this paper presents a new approach to hierarchical reinforcement learning based on decomposing the target markov decision process  mdp  into a hierarchy of smaller mdps
and decomposing the value function of the target mdp into an additive combination of the
value functions of the smaller mdps  the decomposition  known as the maxq decomposition  has both a procedural semantics as a subroutine hierarchy and a declarative
semantics as a representation of the value function of a hierarchical policy  maxq unifies
and extends previous work on hierarchical reinforcement learning by singh  kaelbling  and
dayan and hinton  it is based on the assumption that the programmer can identify useful
subgoals and define subtasks that achieve these subgoals  by defining such subgoals  the
programmer constrains the set of policies that need to be considered during reinforcement
learning  the maxq value function decomposition can represent the value function of any
policy that is consistent with the given hierarchy  the decomposition also creates opportunities to exploit state abstractions  so that individual mdps within the hierarchy can
ignore large parts of the state space  this is important for the practical application of the
method  this paper defines the maxq hierarchy  proves formal results on its representational power  and establishes five conditions for the safe use of state abstractions  the paper
presents an online model free learning algorithm  maxq q  and proves that it converges
with probability   to a kind of locally optimal policy known as a recursively optimal policy 
even in the presence of the five kinds of state abstraction  the paper evaluates the maxq
representation and maxq q through a series of experiments in three domains and shows
experimentally that maxq q  with state abstractions  converges to a recursively optimal
policy much faster than at q learning  the fact that maxq learns a representation of
the value function has an important benefit  it makes it possible to compute and execute
an improved  non hierarchical policy via a procedure similar to the policy improvement
step of policy iteration  the paper demonstrates the effectiveness of this non hierarchical
execution experimentally  finally  the paper concludes with a comparison to related work
and a discussion of the design tradeoffs in hierarchical reinforcement learning 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fidietterich

   introduction
the area of reinforcement learning  bertsekas   tsitsiklis        sutton   barto       
studies methods by which an agent can learn optimal or near optimal plans by interacting
directly with the external environment  the basic methods in reinforcement learning are
based on the classical dynamic programming algorithms that were developed in the late
    s  bellman        howard         however  reinforcement learning methods offer two
important advantages over classical dynamic programming  first  the methods are online 
this permits them to focus their attention on the parts of the state space that are important
and to ignore the rest of the space  second  the methods can employ function approximation algorithms  e g   neural networks  to represent their knowledge  this allows them to
generalize across the state space so that the learning time scales much better 
despite recent advances in reinforcement learning  there are still many shortcomings 
the biggest of these is the lack of a fully satisfactory method for incorporating hierarchies
into reinforcement learning algorithms  research in classical planning has shown that hierarchical methods such as hierarchical task networks  currie   tate         macro actions
 fikes  hart    nilsson        korf         and state abstraction methods  sacerdoti       
knoblock        can provide exponential reductions in the computational cost of finding
good plans  however  all of the basic algorithms for probabilistic planning and reinforcement learning are  at  methods they treat the state space as one huge at search space 
this means that the paths from the start state to the goal state are very long  and the
length of these paths determines the cost of learning and planning  because information
about future rewards must be propagated backward along these paths 
many researchers  singh        lin        kaelbling        dayan   hinton       
hauskrecht  et al         parr   russell        sutton  precup    singh        have experimented with different methods of hierarchical reinforcement learning and hierarchical
probabilistic planning  this research has explored many different points in the design space
of hierarchical methods  but several of these systems were designed for specific situations 
we lack crisp definitions of the main approaches and a clear understanding of the relative
merits of the different methods 
this paper formalizes and clarifies one approach and attempts to understand how it
compares with the other techniques  the approach  called the maxq method  provides a
hierarchical decomposition of the given reinforcement learning problem into a set of subproblems  it simultaneously provides a decomposition of the value function for the given
problem into a set of value functions for the subproblems  hence  it has both a declarative
semantics  as a value function decomposition  and a procedural semantics  as a subroutine
hierarchy  
the decomposition into subproblems has many advantages  first  policies learned in
subproblems can be shared  reused  for multiple parent tasks  second  the value functions
learned in subproblems can be shared  so when the subproblem is reused in a new task 
learning of the overall value function for the new task is accelerated  third  if state abstractions can be applied  then the overall value function can be represented compactly as
the sum of separate terms that each depends on only a subset of the state variables  this
more compact representation of the value function will require less data to learn  and hence 
learning will be faster 
   

fimaxq hierarchical reinforcement learning

previous research shows that there are several important design decisions that must
be made when constructing a hierarchical reinforcement learning system  to provide an
overview of the results in this paper  let us review these issues and see how the maxq
method approaches each of them 
the first issue is how to specify subtasks  hierarchical reinforcement learning involves
breaking the target markov decision problem into a hierarchy of subproblems or subtasks 
there are three general approaches to defining these subtasks  one approach is to define
each subtask in terms of a fixed policy that is provided by the programmer  or that has
been learned in some separate process   the  option  method of sutton  precup  and singh
       takes this approach  the second approach is to define each subtask in terms of a nondeterministic finite state controller  the hierarchy of abstract machines  ham  method
of parr and russell        takes this approach  this method permits the programmer to
provide a  partial policy  that constrains the set of permitted actions at each point  but
does not specify a complete policy for each subtask  the third approach is to define each
subtask in terms of a termination predicate and a local reward function  these define what
it means for the subtask to be completed and what the final reward should be for completing
the subtask  the maxq method described in this paper follows this approach  building
upon previous work by singh         kaelbling         dayan and hinton         and dean
and lin        
an advantage of the  option  and partial policy approaches is that the subtask can
be defined in terms of an amount of effort or a course of action rather than in terms of
achieving a particular goal condition  however  the  option  approach  at least in the
simple form described in this paper   requires the programmer to provide complete policies
for the subtasks  which can be a dicult programming task in real world problems  on the
other hand  the termination predicate method requires the programmer to guess the relative
desirability of the different states in which the subtask might terminate  this can also be
dicult  although dean and lin show how these guesses can be revised automatically by
the learning algorithm 
a potential drawback of all hierarchical methods is that the learned policy may be
suboptimal  the hierarchy constrains the set of possible policies that can be considered  if
these constraints are poorly chosen  the resulting policy will be suboptimal  nonetheless  the
learning algorithms that have been developed for the  option  and partial policy approaches
guarantee that the learned policy will be the best possible policy consistent with these
constraints 
the termination predicate method suffers from an additional source of suboptimality 
the learning algorithm described in this paper converges to a form of local optimality that
we call recursive optimality  this means that the policy of each subtask is locally optimal
given the policies of its children  but there might exist better hierarchical policies where
the policy for a subtask must be locally suboptimal so that the overall policy is optimal 
for example  a subtask of buying milk might be performed suboptimally  at a more distant
store  because the larger problem also involves buying film  at the same store   this problem
can be avoided by careful definition of termination predicates and local reward functions 
but this is an added burden on the programmer   it is interesting to note that this problem
of recursive optimality has not been noticed previously  this is because previous work
   

fidietterich

focused on subtasks with a single terminal state  and in such cases  the problem does not
arise  
the second design issue is whether to employ state abstractions within subtasks  a
subtask employs state abstraction if it ignores some aspects of the state of the environment 
for example  in many robot navigation problems  choices about what route to take to
reach a goal location are independent of what the robot is currently carrying  with few
exceptions  state abstraction has not been explored previously  we will see that the maxq
method creates many opportunities to exploit state abstraction  and that these abstractions
can have a huge impact in accelerating learning  we will also see that there is an important
design tradeoff  the successful use of state abstraction requires that subtasks be defined
in terms of termination predicates rather than using the option or partial policy methods 
this is why the maxq method must employ termination predicates  despite the problems
that this can create 
the third design issue concerns the non hierarchical  execution  of a learned hierarchical policy  kaelbling        was the first to point out that a value function learned
from a hierarchical policy could be evaluated incrementally to yield a potentially much
better non hierarchical policy  dietterich        and sutton  et al         generalized this
to show how arbitrary subroutines could be executed non hierarchically to yield improved
policies  however  in order to support this non hierarchical execution  extra learning is
required  ordinarily  in hierarchical reinforcement learning  the only states where learning
is required at the higher levels of the hierarchy are states where one or more of the subroutines could terminate  plus all possible initial states   but to support non hierarchical
execution  learning is required in all states  and at all levels of the hierarchy   in general 
this requires additional exploration as well as additional computation and memory  as a
consequence of the hierarchical decomposition of the value function  the maxq method
is able to support either form of execution  and we will see that there are many problems
where the improvement from non hierarchical execution is worth the added cost 
the fourth and final issue is what form of learning algorithm to employ  an important advantage of reinforcement learning algorithms is that they typically operate online 
however  finding online algorithms that work for general hierarchical reinforcement learning
has been dicult  particularly within the termination predicate family of methods  singh s
method relied on each subtask having a unique terminal state  kaelbling employed a mix of
online and batch algorithms to train her hierarchy  and work within the  options  framework usually assumes that the policies for the subproblems are given and do not need to be
learned at all  the best previous online algorithms are the hamq q learning algorithm of
parr and russell  for the partial policy method  and the feudal q algorithm of dayan and
hinton  unfortunately  the hamq method requires  attening  the hierarchy  and this has
several undesirable consequences  the feudal q algorithm is tailored to a specific kind of
problem  and it does not converge to any well defined optimal policy 
in this paper  we present a general algorithm  called maxq q  for fully online learning
of a hierarchical value function  this algorithm enables all subtasks within the hierarchy to
be learned simultaneously and online  we show experimentally and theoretically that the
algorithm converges to a recursively optimal policy  we also show that it is substantially
faster than  at   i e   non hierarchical  q learning when state abstractions are employed 
   

fimaxq hierarchical reinforcement learning

the remainder of this paper is organized as follows  after introducing our notation in
section    we define the maxq value function decomposition in section   and illustrate
it with a simple example markov decision problem  section   presents an analytically
tractable version of the maxq q learning algorithm called the maxq   algorithm and
proves its convergence to a recursively optimal policy  it then shows how to extend maxq  to produce the maxq q algorithm  and shows how to extend the theorem similarly 
section   takes up the issue of state abstraction and formalizes a series of five conditions
under which state abstractions can be safely incorporated into the maxq representation 
state abstraction can give rise to a hierarchical credit assignment problem  and the paper
briey discusses one solution to this problem  finally  section   presents experiments with
three example domains  these experiments give some idea of the generality of the maxq
representation  they also provide results on the relative importance of temporal and state
abstractions and on the importance of non hierarchical execution  the paper concludes with
further discussion of the design issues that were briey described above  and in particular  it
addresses the tradeoff between the method of defining subtasks  via termination predicates 
and the ability to exploit state abstractions 
some readers may be disappointed that maxq provides no way of learning the structure of the hierarchy  our philosophy in developing maxq  which we share with other
reinforcement learning researchers  notably parr and russell  has been to draw inspiration
from the development of belief networks  pearl         belief networks were first introduced
as a formalism in which the knowledge engineer would describe the structure of the networks and domain experts would provide the necessary probability estimates  subsequently 
methods were developed for learning the probability values directly from observational data 
most recently  several methods have been developed for learning the structure of the belief
networks from data  so that the dependence on the knowledge engineer is reduced 
in this paper  we will likewise require that the programmer provide the structure of the
hierarchy  the programmer will also need to make several important design decisions  we
will see below that a maxq representation is very much like a computer program  and
we will rely on the programmer to design each of the modules and indicate the permissible
ways in which the modules can invoke each other  our learning algorithms will fill in
 implementations  of each module in such a way that the overall program will work well 
we believe that this approach will provide a practical tool for solving large real world
mdps  we also believe that it will help us understand the structure of hierarchical learning
algorithms  it is our hope that subsequent research will be able to automate most of the
work that we are currently requiring the programmer to do 

   formal definitions
we begin by introducing definitions for markov decision problems and semi markov decision problems 

    markov decision problems
we employ the standard definition for markov decision problems  also known as markov
decision processes   in this paper  we restrict our attention to situations in which an agent
   

fidietterich

is interacting with a fully observable stochastic environment  this situation can be modeled
as a markov decision problem  mdp  hs  a  p  r  p  i defined as follows 
 s   the finite set of states of the environment  at each point in time  the agent can
observe the complete state of the environment 
 a  a finite set of actions  technically  the set of available actions depends on the
current state s  but we will suppress this dependence in our notation 
 p   when an action a   a is performed  the environment makes a probabilistic transition from its current state s to a resulting state s  according to the probability
distribution p  s  js  a  
 r  similarly  when action a is performed and the environment makes its transition
from s to s    the agent receives a real valued  possibly stochastic  reward r whose
expected value is r s  js  a   to simplify the notation  it is customary to treat this
reward as being given at the time that action a is initiated  even though it may in
general depend on s  as well as on s and a 
 p    the starting state distribution  when the mdp is initialized  it is in state s with
probability p   s  
a policy    is a mapping from states to actions that tells what action a    s  to perform
when the environment is in state s 
we will consider two settings  episodic and infinite horizon 
in the episodic setting  all rewards are finite and there is at least one zero cost absorbing
terminal state  an absorbing terminal state is a state in which all actions lead back to the
same state with probability   and zero reward  for technical reasons  we will only consider
problems where all deterministic policies are  proper  that is  all deterministic policies
have a non zero probability of reaching a terminal state when started in an arbitrary state 
 we believe this condition can be relaxed  but we have not verified this formally   in the
episodic setting  the goal of the agent is to find a policy that maximizes the expected
cumulative reward  in the special case where all rewards are non positive  these problems
are referred to as stochastic shortest path problems  because the rewards can be viewed as
costs  i e   lengths   and the policy attempts to move the agent along the path of minimum
expected cost 
in the infinite horizon setting  all rewards are also finite  in addition  there is a discount
factor    and the agent s goal is to find a policy that minimizes the infinite discounted sum
of future rewards 
the value function v  for policy  is a function that tells  for each state s  what the
expected cumulative reward will be of executing policy  starting in state s  let rt be a
random variable that tells the reward that the agent receives at time step t while following
policy   we can define the value function in the episodic setting as
v   s    e frt   rt     rt        jst   s  g  
in the discounted setting  the value function is
fi

n

o

v   s    e rt   rt        rt       fifi st   s    
   

fimaxq hierarchical reinforcement learning

we can see that this equation reduces to the previous one when       however  in infinitehorizon mdps this sum may not converge when      
the value function satisfies the bellman equation for a fixed policy 

v   s   

x

s 

p  s  js   s   r s  js   s     v   s     




the quantity on the right hand side is called the backed up value of performing action a in
state s  for each possible successor state s    it computes the reward that would be received
and the value of the resulting state and then weights those according to the probability of
ending up in s   
the optimal value function v  is the value function that simultaneously maximizes the
expected cumulative reward in all states s   s   bellman        proved that it is the unique
solution to what is now known as the bellman equation 

v   s    max
a

x

s 

p  s  js  a  r s  js  a    v   s     




   

there may be many optimal policies that achieve this value  any policy that chooses a
in s to achieve the maximum on the right hand side of this equation is an optimal policy 
we will denote an optimal policy by    note that all optimal policies are  greedy  with
respect to the backed up value of the available actions 
closely related to the value function is the so called action value function  or q function
 watkins         this function  q  s  a   gives the expected cumulative reward of performing action a in state s and then following policy  thereafter  the q function also satisfies
a bellman equation 

q  s  a   

x

s 

p  s  js  a  r s  js  a    q  s     s      




the optimal action value function is written q  s  a   and it satisfies the equation
x
q  s  a    p  s  js  a 

s 





r s  js  a     max q s    a   
a 

 

   

note that any policy that is greedy with respect to q is an optimal policy  there may be
many such optimal policies they differ only in how they break ties between actions with
identical q values 
an action order  denoted    is a total order over the actions within an mdp  that is   
is an anti symmetric  transitive relation such that   a    a    is true iff a  is strictly preferred
to a    an ordered greedy policy    is a greedy policy that breaks ties using    for example 
suppose that the two best actions at state s are a  and a    that q s  a      q s  a     and
that   a    a     then the ordered greedy policy   will choose a       s    a    note that
although there may be many optimal policies for a given mdp  the ordered greedy policy 
    is unique 
   

fidietterich

    semi markov decision processes

in order to introduce and prove some of the properties of the maxq decomposition  we
need to consider a simple generalization of mdps the semi markov decision process 
a discrete time semi markov decision process  smdp  is a generalization of the markov
decision process in which the actions can take a variable amount of time to complete  in
particular  let the random variable n denote the number of time steps that action a takes
when it is executed in state s  we can extend the state transition probability function to
be the joint distribution of the result states s  and the number of time steps n when action
a is performed in state s  p  s    n js  a   similarly  the expected reward can be changed to
be r s    n js  a   
it is straightforward to modify the bellman equation to define the value function for a
fixed policy  as
h
i
x
v   s    p  s   n js   s   r s    n js   s      n v   s     
s   n

the only change is that the expected value on the right hand side is taken with respect to
both s  and n   and  is raised to the power n to reect the variable amount of time that
may elapse while executing action a 
note that because expectation is a linear operator  we can write each of these bellman
equations as the sum of the expected reward for performing action a and the expected value
of the resulting state s    for example  we can rewrite the equation above as
x
   
v   s    r s   s     p  s    n js   s   n v   s    
s   n

where r s   s   is the expected reward of performing action  s  in state s  and the expectation is taken with respect to s  and n  
all of the results given in this paper can be generalized to apply to discrete time semimarkov decision processes  a consequence of this is that whenever this paper talks of
executing a primitive action  it could just as easily talk of executing a hand coded openloop  subroutine   these subroutines would not be learned  and nor could their execution
be interrupted as discussed below in section    but in many applications  e g   robot
control with limited sensors   open loop controllers can be very useful  e g   to hide partialobservability   for an example  see kalmar  szepesvari  and a  lorincz        
note that for the episodic case  there is no difference between a mdp and a semi markov
decision process  because the discount factor  is    and therefore neither the optimal policy
nor the optimal value function depend on the amount of time each action takes 

    reinforcement learning algorithms

a reinforcement learning algorithm is an algorithm that tries to construct an optimal policy
for an unknown mdp  the algorithm is given access to the unknown mdp via the following
   this formalization is slightly different from the standard formulation of smdps  which separates
p  s js  a  and f  tjs  a   where f is the cumulative distribution function for the probability that a will
terminate in t time units  and t is real valued rather than integer valued  in our case  it is important
to consider the joint distribution of s  and n   but we do not need to consider actions with arbitrary
real valued durations 

   

fimaxq hierarchical reinforcement learning

reinforcement learning protocol  at each time step t  the algorithm is told the current state
s of the mdp and the set of actions a s   a that are executable in that state  the
algorithm chooses an action a   a s   and the mdp executes this action  which causes it to
move to state s   and returns a real valued reward r  if s is an absorbing terminal state  the
set of actions a s  contains only the special action reset  which causes the mdp to move
to one of its initial states  drawn according to p   
in this paper  we will make use of two well known learning algorithms  q learning
 watkins        watkins   dayan        and sarsa     rummery   niranjan         we
will apply these algorithms to the case where the action value function q s  a  is represented
as a table with one entry for each pair of state and action  every entry of the table is
initialized arbitrarily 
in q learning  after the algorithm has observed s  chosen a  received r  and observed s   
it performs the following update 

qt  s  a          fft  qt    s  a    fft  r    max
q  s    a     
a  t  
where fft is a learning rate parameter 
jaakkola  jordan and singh        and bertsekas and tsitsiklis        prove that if the
agent follows an  exploration policy  that tries every action in every state infinitely often
and if
t
t
x
x
lim
ff
 
 
and
lim
ff t    
   
t
t   
t  
t  

t  

then qt converges to the optimal action value function q with probability    their proof
holds in both settings discussed in this paper  episodic and infinite horizon  
the sarsa    algorithm is very similar  after observing s  choosing a  observing r 
observing s    and choosing a    the algorithm performs the following update 

qt  s  a          fft  qt    s  a    fft  r   qt    s    a     
where fft is a learning rate parameter  the key difference is that the q value of the chosen
action a    q s    a     appears on the right hand side in the place where q learning uses the
q value of the best action  singh  et al         provide two important convergence results 
first  if a fixed policy  is employed to choose actions  sarsa    will converge to the
value function of that policy provided fft decreases according to equations      second  if a
so called glie policy is employed to choose actions  sarsa    will converge to the value
function of the optimal policy  provided again that fft decreases according to equations     
a glie policy is defined as follows 

definition   a glie  greedy in the limit with infinite exploration  policy is any policy

satisfying

   each action is executed infinitely often in every state that is visited infinitely often 
   in the limit  the policy is greedy with respect to the q value function with probability
  
   

fidietterich

 

r

g

 
 
 
  y
 

b
 

 

 

 

figure    the taxi domain 

   the maxq value function decomposition
at the center of the maxq method for hierarchical reinforcement learning is the maxq
value function decomposition  maxq describes how to decompose the overall value function
for a policy into a collection of value functions for individual subtasks  and subsubtasks 
recursively  

    a motivating example

to make the discussion concrete  let us consider the following simple example  figure  
shows a   by   grid world inhabited by a taxi agent  there are four specially designated
locations in this world  marked as r ed   b lue   g reen   and y ellow   the taxi problem
is episodic  in each episode  the taxi starts in a randomly chosen square  there is a
passenger at one of the four locations  chosen randomly   and that passenger wishes to be
transported to one of the four locations  also chosen randomly   the taxi must go to the
passenger s location  the  source    pick up the passenger  go to the destination location
 the  destination    and put down the passenger there   to keep things uniform  the taxi
must pick up and drop off the passenger even if he she is already located at the destination  
the episode ends when the passenger is deposited at the destination location 
there are six primitive actions in this domain   a  four navigation actions that move the
taxi one square north  south  east  or west   b  a pickup action  and  c  a putdown action 
there is a reward of    for each action and an additional reward of     for successfully
delivering the passenger  there is a reward of     if the taxi attempts to execute the
putdown or pickup actions illegally  if a navigation action would cause the taxi to hit a
wall  the action is a no op  and there is only the usual reward of    
to simplify the examples throughout this section  we will make the six primitive actions deterministic  later  we will make the actions stochastic in order to create a greater
challenge for our learning algorithms 
we seek a policy that maximizes the total reward per episode  there are     possible
states     squares    locations for the passenger  counting the four starting locations and
the taxi   and   destinations 
this task has a simple hierarchical structure in which there are two main sub tasks 
get the passenger and deliver the passenger  each of these subtasks in turn involves the
   

fimaxq hierarchical reinforcement learning

subtask of navigating to one of the four locations and then performing a pickup or putdown
action 
this task illustrates the need to support temporal abstraction  state abstraction  and
subtask sharing  the temporal abstraction is obvious for example  the process of navigating to the passenger s location and picking up the passenger is a temporally extended
action that can take different numbers of steps to complete depending on the distance to
the target  the top level policy  get passenger  deliver passenger  can be expressed very
simply if these temporal abstractions can be employed 
the need for state abstraction is perhaps less obvious  consider the subtask of getting
the passenger  while this subtask is being solved  the destination of the passenger is
completely irrelevant it cannot affect any of the nagivation or pickup decisions  perhaps
more importantly  when navigating to a target location  either the source or destination
location of the passenger   only the target location is important  the fact that in some
cases the taxi is carrying the passenger and in other cases it is not is irrelevant 
finally  support for subtask sharing is critical  if the system could learn how to solve the
navigation subtask once  then the solution could be shared by both the  get the passenger 
and  deliver the passenger  subtasks  we will show below that the maxq method provides
a value function representation and learning algorithm that supports temporal abstraction 
state abstraction  and subtask sharing 
to construct a maxq decomposition for the taxi problem  we must identify a set of
individual subtasks that we believe will be important for solving the overall task  in this
case  let us define the following four tasks 
 navigate t   in this subtask  the goal is to move the taxi from its current location to
one of the four target locations  which will be indicated by the formal parameter t 
 get  in this subtask  the goal is to move the taxi from its current location to the
passenger s current location and pick up the passenger 
 put  the goal of this subtask is to move the taxi from the current location to the
passenger s destination location and drop off the passenger 
 root  this is the whole taxi task 
each of these subtasks is defined by a subgoal  and each subtask terminates when the
subgoal is achieved 
after defining these subtasks  we must indicate for each subtask which other subtasks or
primitive actions it should employ to reach its goal  for example  the navigate t  subtask
should use the four primitive actions north  south  east  and west  the get subtask should
use the navigate subtask and the pickup primitive action  and so on 
all of this information can be summarized by a directed acyclic graph called the task
graph  which is shown in figure    in this graph  each node corresponds to a subtask or a
primitive action  and each edge corresponds to a potential way in which one subtask can
 call  one of its child tasks  the notation formal actual  e g   t source  tells how a formal
parameter is to be bound to an actual parameter 
now suppose that for each of these subtasks  we write a policy  e g   as a computer
program  to achieve the subtask  we will refer to the policy for a subtask as a  subroutine   and we can view the parent subroutine as invoking the child subroutine via ordinary
   

fidietterich

root

get

put
t source

pickup

t destination

navigate t 

north

south

east

putdown

west

figure    a task graph for the taxi problem 
subroutine call and return semantics  if we have a policy for each subtask  then this gives
us an overall policy for the taxi mdp  the root subtask executes its policy by calling
subroutines that are policies for the get and put subtasks  the get policy calls subroutines
for the navigate t  subtask and the pickup primitive action  and so on  we will call this
collection of policies a hierarchical policy  in a hierarchical policy  each subroutine executes
until it enters a terminal state for its subtask 

    definitions

let us formalize the discussion so far 
the maxq decomposition takes a given mdp m and decomposes it into a finite set of
subtasks fm    m            mn g with the convention that m  is the root subtask  i e   solving
m  solves the entire original mdp m   
definition   an unparameterized subtask is a three tuple  hti  ai   r i i  defined as follows 
   ti is a termination predicate that partitions s into a set of active states  si   and a set
of terminal states  ti   the policy for subtask mi can only be executed if the current
state s is in si   if  at any time that subtask mi is being executed  the mdp enters a
state in ti   then mi terminates immediately  even if it is still executing a subtask  see
below  
   ai is a set of actions that can be performed to achieve subtask mi   these actions can
either be primitive actions from a  the set of primitive actions for the mdp  or they
can be other subtasks  which we will denote by their indexes i  we will refer to these
actions as the  children  of subtask i  the sets ai define a directed graph over the
subtasks m            mn   and this graph may not contain any cycles  stated another way 
no subtask can invoke itself recursively either directly or indirectly 
if a child subtask mj has formal parameters  then this is interpreted as if the subtask
occurred multiple times in ai   with one occurrence for each possible tuple of actual
   

fimaxq hierarchical reinforcement learning

values that could be bound to the formal parameters  the set of actions ai may differ
from one state to another and from one set of actual parameter values to another  so
technically  ai is a function of s and the actual parameters  however  we will suppress
this dependence in our notation 
   r  i  s    is the pseudo reward function  which specifies a  deterministic  pseudo reward
for each transition to a terminal state s    ti   this pseudo reward tells how desirable
each of the terminal states is for this subtask  it is typically employed to give goal
terminal states a pseudo reward of   and any non goal terminal states a negative
reward  by definition  the pseudo reward r  i  s  is also zero for all non terminal states
s  the pseudo reward is only used during learning  so it will not be mentioned further
until section   
each primitive action a from m is a primitive subtask in the maxq decomposition
such that a is always executable  it always terminates immediately after execution  and its
pseudo reward function is uniformly zero 

if a subtask has formal parameters  then each possible binding of actual values to the
formal parameters specifies a distinct subtask  we can think of the values of the formal
parameters as being part of the  name  of the subtask  in practice  of course  we implement
a parameterized subtask by parameterizing the various components of the task  if b specifies
the actual parameter values for task mi   then we can define a parameterized termination
predicate ti  s  b  and a parameterized pseudo reward function r  i  s    b   to simplify notation
in the rest of the paper  we will usually omit these parameter bindings  however  it should
be noted that if a parameter of a subtask takes on a large number of possible values  this
is equivalent to creating a large number of different subtasks  each of which will need to be
learned  it will also create a large number of candidate actions for the parent task  which
will make the learning problem more dicult for the parent task as well 

definition   a hierarchical policy    is a set containing a policy for each of the subtasks
in the problem     f            n g 
each subtask policy i takes a state and returns the name of a primitive action to
execute or the name of a subroutine  and bindings for its formal parameters  to invoke  in
the terminology of sutton  precup  and singh         a subtask policy is a deterministic
 option   and its probability of terminating in state s  which they denote by fi  s   is   if
s   si   and   if s   ti  
in a parameterized task  the policy must be parameterized as well so that  takes a
state and the bindings of formal parameters and returns a chosen action and the bindings
 if any  of its formal parameters 
table   gives a pseudo code description of the procedure for executing a hierarchical
policy  the hierarchical policy is executed using a stack discipline  similar to ordinary
programming languages  let kt denote the contents of the pushdown stack at time t 
when a subroutine is invoked  its name and actual parameters are pushed onto the stack 
when a subroutine terminates  its name and actual parameters are popped off the stack 
notice  line     that if any subroutine on the stack terminates  then all subroutines below
   

fidietterich

table    pseudo code for execution of a hierarchical policy 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  

procedure executehierarchicalpolicy  

st is the state of the world at time t
kt is the state of the execution stack at time t
let t      kt   empty stack  observe st
push     nil  onto stack kt  invoke the root task with no parameters 

repeat
while top kt   is not a primitive action

let  i  fi      top kt   where
i is the name of the  current  subroutine  and
fi gives the parameter bindings for i
let  a  fa      i  s  fi    where
a is the action and fa gives the parameter bindings chosen by policy i
push  a  fa   onto the stack kt
end    while
let  a  nil     pop kt  be the primitive action on the top of the stack 
execute primitive action a  observe st    and receive reward r st  jst   a 
if any subtask on kt is terminated in st   then
let m   be the terminated subtask that is highest  closest to the root  on the stack 
while top kt     m   do pop kt 
pop kt 
kt      kt is the resulting execution stack 
until kt   is empty
end executehierarchicalpolicy

it are immediately aborted  and control returns to the subroutine that had invoked the
terminated subroutine 
it is sometimes useful to think of the contents of the stack as being an additional part of
the state space for the problem  hence  a hierarchical policy implicitly defines a mapping
from the current state st and current stack contents kt to a primitive action a  this action is
executed  and this yields a resulting state st   and a resulting stack contents kt     because
of the added state information in the stack  the hierarchical policy is non markovian with
respect to the original mdp 
because a hierarchical policy maps from states s and stack contents k to actions  the
value function for a hierarchical policy must assign values to combinations of states s and
stack contents k  

definition   a hierarchical value function  denoted v   hs  k i   gives the expected cumu 

lative reward of following the hierarchical policy  starting in state s with stack contents
k 

this hierarchical value function is exactly what is learned by ron parr s      b  hamq
algorithm  which we will discuss below  however  in this paper  we will focus on learning
only the projected value functions of each of the subtasks m            mn in the hierarchy 
   

fimaxq hierarchical reinforcement learning

definition   the projected value function of hierarchical policy  on subtask mi  denoted
v   i  s   is the expected cumulative reward of executing i  and the policies of all descendents
of mi   starting in state s until mi terminates 
the purpose of the maxq value function decomposition is to decompose v     s   the
projected value function of the root task  in terms of the projected value functions v  i  s 
of all of the subtasks in the maxq decomposition 

    decomposition of the projected value function

now that we have defined a hierarchical policy and its projected value function  we can show
how that value function can be decomposed hierarchically  the decomposition is based on
the following theorem 

theorem   given a task graph over tasks m            mn and a hierarchical policy   each
subtask mi defines a semi markov decision process with states si   actions ai   probability
transition function pi  s    n js  a   and expected reward function r s  a    v   a  s   where
v   a  s  is the projected value function for child task ma in state s  if a is a primitive
action 
v   a  s  is defined as the expected immediate reward of executing a in s  v   a  s   
p
 
 
s  p  s js  a r s js  a  
proof  consider all of the subroutines that are descendents of task mi in the task graph 

because all of these subroutines are executing fixed policies  specified by hierarchical policy
   the probability transition function pi  s    n js  a  is a well defined  stationary distribution
for each child subroutine a  the set of states si and the set of actions ai are obvious  the
interesting part of this theorem is the fact that the expected reward function r s  a  of the
smdp is the projected value function of the child task ma  
to see this  let us write out the value of v   i  s  

v   i  s    e frt   rt        rt        jst   s  g
   
this sum continues until the subroutine for task mi enters a state in ti  
now let us suppose that the first action chosen by i is a subroutine a  this subroutine
is invoked  and it executes for a number of steps n and terminates in state s  according to
pi  s    n js  a   we can rewrite equation     as
v   i  s    e

 

nx
  
u  

 u rt u

 

 
x

u n

fi
fi
fi
t u fifi

ur

st   s  

 

   

the first summation on the right hand side of equation     is the discounted sum of rewards
for executing subroutine a starting in state s until it terminates  in other words  it is v   a  s  
the projected value function for the child task ma   the second term on the right hand side
of the equation is the value of s  for the current task i  v   i  s     discounted by  n   where
s  is the current state when subroutine a terminates  we can write this in the form of a
bellman equation 
x
   
v   i  s    v   i  s   s    pi  s    n js  i  s   n v   i  s   
s   n

   

fidietterich

this has the same form as equation      which is the bellman equation for an smdp  where
the first term is the expected reward r s   s    q e d 
to obtain a hierarchical decomposition of the projected value function  let us switch
to the action value  or q  representation  first  we need to extend the q notation to
handle the task hierarchy  let q  i  s  a  be the expected cumulative reward for subtask
mi of performing action a in state s and then following hierarchical policy  until subtask
mi terminates  action a may be either a primitive action or a child subtask  with this
notation  we can re state equation     as follows 

q  i  s  a    v   a  s   

x

s   n

pi  s    n js  a  n q  i  s     s     

   

the right most term in this equation is the expected discounted reward of completing task

mi after executing action a in state s  this term only depends on i  s  and a  because the
summation marginalizes away the dependence on s  and n   let us define c   i  s  a  to be
equal to this term 

definition   the completion function  c   i  s  a   is the expected discounted cumulative

reward of completing subtask mi after invoking the subroutine for subtask ma in state s 
the reward is discounted back to the point in time where a begins execution 

c   i  s  a   

x

s   n

pi  s   n js  a  n q  i  s     s    

   

with this definition  we can express the q function recursively as

q  i  s  a    v   a  s    c   i  s  a  

    

finally  we can re express the definition for v   i  s  as

v   i  s   

 

  i  s  i  s  
q
if i is composite
p
 
 
s  p  s js  i r s js  i  if i is primitive

    

we will refer to equations            and      as the decomposition equations for the
maxq hierarchy under a fixed hierarchical policy   these equations recursively decompose
the projected value function for the root  v      s  into the projected value functions for
the individual subtasks  m            mn and the individual completion functions c   j  s  a 
for j              n  the fundamental quantities that must be stored to represent the value
function decomposition are just the c values for all non primitive subtasks and the v values
for all primitive actions 
to make it easier for programmers to design and debug maxq decompositions  we have
developed a graphical representation that we call the maxq graph  a maxq graph for the
taxi domain is shown in figure    the graph contains two kinds of nodes  max nodes and
q nodes  the max nodes correspond to the subtasks in the task decomposition there is
one max node for each primitive action and one max node for each subtask  including the
root  task  each primitive max node i stores the value of v   i  s   the q nodes correspond
to the actions that are available for each subtask  each q node for parent task i  state s
   

fimaxq hierarchical reinforcement learning

maxroot

qpickup

qget

qput

maxget

maxput

qnavigateforput

qnavigateforget

t source

qputdown

t destination

pickup

putdown

maxnavigate t 

qnorth t 

qeast t 

qsouth t 

qwest t 

north

east

south

west

figure    a maxq graph for the taxi domain 
and subtask a stores the value of c   i  s  a   the children of any node are unordered that
is  the order in which they are drawn in figure   does not imply anything about the order in
which they will be executed  indeed  a child action may be executed multiple times before
its parent subtask is completed 
in addition to storing information  the max nodes and q nodes can be viewed as performing parts of the computation described by the decomposition equations  specifically 
each max node i can be viewed as computing the projected value function v   i  s  for its
subtask  for primitive max nodes  this information is stored in the node  for composite
max nodes  this information is obtained by  asking  the q node corresponding to i  s  
each q node with parent task i and child task a can be viewed as computing the value of
q  i  s  a   it does this by  asking  its child task a for its projected value function v   a  s 
and then adding its completion function c   i  s  a  
   

fidietterich

as an example  consider the situation shown in figure    which we will denote by s   
suppose that the passenger is at r and wishes to go to b  let the hierarchical policy we
are evaluating be an optimal policy denoted by   we will omit the superscript   to reduce
the clutter of the notation   the value of this state under  is     because it will cost  
unit to move the taxi to r    unit to pickup the passenger    units to move the taxi to b 
and   unit to putdown the passenger  for a total of    units  a reward of       when the
passenger is delivered  the agent gets a reward of      so the net value is     
figure   shows how the maxq hierarchy computes this value  to compute the value
v   root  s     maxroot consults its policy and finds that root  s   is get  hence  it  asks 
the q node  qget to compute q  root  s    get   the completion cost for the root task
after performing a get  c   root  s    get   is     because it will cost   units to deliver the
customer  for a net reward of              after completing the get subtask  however  this
is just the reward after completing the get  so it must ask maxget to estimate the expected
reward of performing the get itself 
the policy for maxget dictates that in s    the navigate subroutine should be invoked with
t bound to r  so maxget consults the q node  qnavigateforget to compute the expected
reward  qnavigateforget knows that after completing the navigate r  task  one more action
 the pickup  will be required to complete the get  so c   maxget  s    navigate r        
it then asks maxnavigate r  to compute the expected reward of performing a navigate to
location r 
the policy for maxnavigate chooses the north action  so maxnavigate asks qnorth to
compute the value  qnorth looks up its completion cost  and finds that c   navigate  s    north 
is    i e   the navigate task will be completed after performing the north action   it consults
maxnorth to determine the expected cost of performing the north action itself  because
maxnorth is a primitive action  it looks up its expected reward  which is    
now this series of recursive computations can conclude as follows 

 q  navigate r   s    north          
 v   navigate r   s        
 q  get  s    navigate r            
    to perform the navigate plus    to complete the get 
 v   get  s       
 q  root  s   get           
    to perform the get plus    to complete the root task and collect the final reward  
the end result of all of this is that the value of v   root  s    is decomposed into a sum
of c terms plus the expected reward of the chosen primitive action 

v   root  s      v   north  s      c   navigate r   s    north   
c   get  s    navigate r     c   root  s   get 
                  
    
   

fimaxq hierarchical reinforcement learning

  
maxroot
  
  

qget

qput

  
maxget

maxput
  

qpickup

qnavigateforput

qnavigateforget

qputdown

  
  
pickup

putdown

maxnavigate t 
  
 

qnorth t 

qeast t 

qsouth t 

qwest t 

east

south

west

  
north

figure    computing the value of a state using the maxq hierarchy  the c value of each
q node is shown to the left of the node  all other numbers show the values being
returned up the graph 
in general  the maxq value function decomposition has the form

v      s    v   am   s    c   am     s  am             c   a    s  a      c      s  a         
where a    a            am is the  path  of max nodes chosen by the hierarchical policy going
from the root down to a primitive leaf node  this is summarized graphically in figure   
we can summarize the presentation of this section by the following theorem 

theorem   let    fi  i              ng be a hierarchical policy defined for a given maxq
graph with subtasks m            mn   and let i     be the root node of the graph  then there
exist values for c   i  s  a   for internal max nodes  and v   i  s   for primitive  leaf max
   

fidietterich



v   x  s 

xxxxx




x
v   a   s 
p
 pppp
 

 
v   am     s 

   

zz

zz


v   am   s  c   am     s  am  
r 

r 

r 

r 

r 

c   a    s  a   
     

r 

r 

c      s  a   

r   r   r   r   r  

figure    the maxq decomposition  r            r   denote the sequence of rewards received
from primitive actions at times               
nodes  such that v      s   as computed by the decomposition equations            and      
is the expected discounted cumulative reward of following policy  starting in state s 

proof  the proof is by induction on the number of levels in the task graph  at each

level i  we compute values for c   i  s   s    or v   i  s   if i is primitive  according to the
decomposition equations  we can apply the decomposition equations again to compute
q  i  s   s   and apply equation     and theorem   to conclude that q  i  s   s   gives
the value function for level i  when i      we obtain the value function for the entire
hierarchical policy  q  e  d 
it is important to note that this representation theorem does not mention the pseudoreward function  because the pseudo reward is used only during learning  this theorem
captures the representational power of the maxq decomposition  but it does not address
the question of whether there is a learning algorithm that can find a given policy  that is
the subject of the next section 

   a learning algorithm for the maxq decomposition
this section presents the central contributions of the paper  first  we discuss what optimality criteria should be employed in hierarchical reinforcement learning  then we introduce
the maxq   learning algorithm  which can learn value functions  and policies  for maxq
hierarchies in which there are no pseudo rewards  i e   the pseudo rewards are zero   the
central theoretical result of the paper is that maxq   converges to a recursively optimal
policy for the given maxq hierarchy  this is followed by a brief discussion of ways of
accelerating maxq   learning  the section concludes with a description of the maxq q
learning algorithm  which handles non zero pseudo reward functions 
   

fimaxq hierarchical reinforcement learning

    two kinds of optimality

in order to develop a learning algorithm for the maxq decomposition  we must consider
exactly what we are hoping to achieve  of course  for any mdp m   we would like to find
an optimal policy    however  in the maxq method  and in hierarchical reinforcement
learning in general   the programmer imposes a hierarchy on the problem  this hierarchy
constrains the space of possible policies so that it may not be possible to represent the
optimal policy or its value function 
in the maxq method  the constraints take two forms  first  within a subtask  only
some of the possible primitive actions may be permitted  for example  in the taxi task 
during a navigate t   only the north  south  east  and west actions are available the pickup
and putdown actions are not allowed  second  consider a max node mj with child nodes
fmj           mjk g  the policy learned for mj must involve executing the learned policies of
these child nodes  when the policy for child node mji is executed  it will run until it enters
a state in tji   hence  any policy learned for mj must pass through some subset of these
terminal state sets ftj           tjk g 
the ham method shares these same two constraints and in addition  it imposes a
partial policy on each node  so that the policy for any subtask mi must be a deterministic
refinement of the given non deterministic initial policy for node i 
in the  option  approach  the policy is even further constrained  in this approach  there
are only two non primitive levels in the hierarchy  and the subtasks at the lower level  i e  
whose children are all primitive actions  are given complete policies by the programmer 
hence  any learned policy at the upper level must be constructed by  concatenating  the
given lower level policies in some order 
the purpose of imposing these constraints on the policy is to incorporate prior knowledge
and thereby reduce the size of the space that must be searched to find a good policy 
however  these constraints may make it impossible to learn the optimal policy 
if we can t learn the optimal policy  the next best target would be to learn the best
policy that is consistent with  i e   can be represented by  the given hierarchy 
 

 

definition   a hierarchically optimal policy for mdp m is a policy that achieves the
highest cumulative reward among all policies consistent with the given hierarchy 

parr      b  proves that his hamq learning algorithm converges with probability  
to a hierarchically optimal policy  similarly  given a fixed set of options  sutton  precup 
and singh        prove that their smdp learning algorithm converges to a hierarchically
optimal value function  incidentally  they also show that if the primitive actions are also
made available as  trivial  options  then their smdp method converges to the optimal
policy  however  in this case  it is hard to say anything formal about how the options speed
the learning process  they may in fact hinder it  hauskrecht et al         
because the maxq decomposition can represent the value function of any hierarchical
policy  we could easily construct a modified version of the hamq algorithm and apply it
to learn hierarchically optimal policies for the maxq hierarchy  however  we decided to
pursue an even weaker form of optimality  for reasons that will become clear as we proceed 
this form of optimality is called recursive optimality 
   

fidietterich

maxroot

g

qexit

qgotogoal

maxexit

maxgotogoal

 

 

qexitnorth

qexitsouth

qexiteast

north

qnorthg

south

qsouthg

qeastg

east

figure    a simple mdp  left  and its associated maxq graph  right   the policy shown in
the left diagram is recursively optimal but not hierarchically optimal  the shaded
cells indicate points where the locally optimal policy is not globally optimal 

definition   a recursively optimal policy for markov decision process m with maxq
decomposition fm            mk g is a hierarchical policy    f            k g such that for each

subtask mi   the corresponding policy i is optimal for the smdp defined by the set of states
si   the set of actions ai   the state transition probability function p   s    n js  a   and the
reward function given by the sum of the original reward function r s  js  a  and the pseudoreward function r  i  s    

note that the state transition probability distribution  p   s    n js  a  for subtask mi is
defined by the locally optimal policies fj g of all subtasks that are descendents of mi in
the maxq graph  hence  recursive optimality is a kind of local optimality in which the
policy at each node is optimal given the policies of its children 
the reason to seek recursive optimality rather than hierarchical optimality is that recursive optimality makes it possible to solve each subtask without reference to the context
in which it is executed  this context free property makes it easier to share and re use
subtasks  it will also turn out to be essential for the successful use of state abstraction 
before we proceed to describe our learning algorithm for recursive optimality  let us see
how recursive optimality differs from hierarchical optimality 
it is easy to construct examples of policies that are recursively optimal but not hierarchically optimal  and vice versa   consider the simple maze problem and its associated
maxq graph shown in figures    suppose a robot starts somewhere in the left room  and
it must reach the goal g in the right room  the robot has three actions  north  south  and
east  and these actions are deterministic  the robot receives a reward of    for each move 
let us define two subtasks 
   

fimaxq hierarchical reinforcement learning

 exit  this task terminates when the robot exits the left room  we can set the pseudo 

reward function r  to be   for the two terminal states  i e   the two states indicated
by   s  
 gotogoal  this task terminates when the robot reaches the goal g 
the arrows in figure   show the locally optimal policy within each room  the arrows
on the left seek to exit the left room by the shortest path  because this is what we specified
when we set the pseudo reward function to    the arrows on the right follow the shortest
path to the goal  which is fine  however  the resulting policy is neither hierarchically optimal
nor optimal 
there exists a hierarchical policy that would always exit the left room by the upper
door  the maxq value function decomposition can represent the value function of this
policy  but such a policy would not be locally optimal  because  for example  the states
in the  shaded  region would not follow the shortest path to a doorway   hence  this
example illustrates both a recursively optimal policy that is not hierarchically optimal and
a hierarchically optimal policy that is not recursively optimal 
if we consider for a moment  we can see a way to fix this problem  the value of the
upper starred state under the optimal hierarchical policy is    and the value of the lower
starred state is     hence  if we changed r  to have these values  instead of being zero  
then the recursively optimal policy would be hierarchically optimal  and globally optimal  
in other words  if the programmer can guess the right values for the terminal states of a
subtask  then the recursively optimal policy will be hierarchically optimal 
this basic idea was first pointed out by dean and lin         they describe an algorithm
that makes initial guesses for the values of these starred states and then updates those
guesses based on the computed values of the starred states under the resulting recursivelyoptimal policy  they proved that this will converge to a hierarchically optimal policy  the
drawback of their method is that it requires repeated solution of the resulting hierarchical
learning problem  and this does not always yield a speedup over just solving the original 
at problem 
parr      a  proposed an interesting approach that constructs a set of different r  functions and computes the recursively optimal policy under each of them for each subtask  his
method chooses the r  functions in such a way that the hierarchically optimal policy can be
approximated to any desired degree  unfortunately  the method is quite expensive  because
it relies on solving a series of linear programming problems each of which requires time
polynomial in several parameters  including the number of states jsi j within the subtask 
this discussion suggests that while  in principle  it is possible to learn good values for
the pseudo reward function  in practice  we must rely on the programmer to specify a single
pseudo reward function  r    for each subtask  if the programmer wishes to consider a small
number of alternative pseudo reward functions  they can be handled by defining a small
number of subtasks that are identical except for their r  functions  and permitting the
learning algorithm to choose the one that gives the best recursively optimal policy 
in our experiments  we have employed the following simplified approach to defining
r    for each subtask mi  we define two predicates  the termination predicate  ti   and a
goal predicate  gi   the goal predicate defines a subset of the terminal states that are  goal
states   and these have a pseudo reward of    all other terminal states have a fixed constant
   

fidietterich

pseudo reward  e g         that is set so that it is always better to terminate in a goal state
than in a non goal state  for the problems on which we have tested the maxq method 
this worked very well 
in our experiments with maxq  we have found that it is easy to make mistakes in
defining ti and gi   if the goal is not defined carefully  it is easy to create a set of subtasks
that lead to infinite looping  for example  consider again the problem in figure    suppose
we permit a fourth action  west  in the mdp and let us define the termination and goal
predicates for the right hand room to be satisfied iff either the robot reaches the goal or it
exits the room  this is a very natural definition  since it is quite similar to the definition
for the left hand room  however  the resulting locally optimal policy for this room will
attempt to move to the nearest of these three locations  the goal  the upper door  or the
lower door  we can easily see that for all but a few states near the goal  the only policies
that can be constructed by maxroot will loop forever  first trying to leave the left room by
entering the right room  and then trying to leave the right room by entering the left room 
this problem is easily fixed by defining the goal predicate gi for the right room to be true
if and only if the robot reaches the goal g  but avoiding such  undesired termination  bugs
can be hard in more complex domains 
in the worst case  it is possible for the programmer to specify pseudo rewards such that
the recursively optimal policy can be made arbitrarily worse than the hierarchically optimal
policy  for example  suppose that we change the original mdp in figure   so that the state
immediately to the left of the upper doorway gives a large negative reward  l whenever
the robot visits that square  because rewards everywhere else are     the hierarchicallyoptimal policy exits the room by the lower door  but suppose the programmer has chosen
instead to force the robot to exit by the upper door  e g   by assigning a pseudo reward of
   l for leaving via the lower door   in this case  the recursively optimal policy will leave
by the upper door and suffer the large  l penalty  by making l arbitrarily large  we can
make the difference between the hierarchically optimal policy and the recursively optimal
policy arbitrarily large 

    the maxq   learning algorithm
now that we have an understanding of recursively optimal policies  we present two learning
algorithms  the first one  called maxq    applies only in the case when the pseudo reward
function r  is always zero  we will first prove its convergence properties and then show
how it can be extended to give the second algorithm  maxq q  which works with general
pseudo reward functions 
table   gives pseudo code for maxq    maxq   is a recursive function that executes
the current exploration policy starting at max node i in state s  it performs actions until it
reaches a terminal state  at which point it returns a count of the total number of primitive
actions that have been executed  to execute an action  maxq   calls itself recursively
 line     when the recursive call returns  it updates the value of the completion function
for node i  it uses the count of the number of primitive actions to appropriately discount
the value of the resulting state s    at leaf nodes  maxq   updates the estimated one step
expected reward  v  i  s   the value fft  i  is a  learning rate  parameter that should be
gradually decreased to zero in the limit 
   

fimaxq hierarchical reinforcement learning

table    the maxq   learning algorithm 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  

function maxq   maxnode i  state s 
if i is a primitive maxnode
execute i  receive r  and observe result state s 
vt  i  s          fft  i    vt  i  s    fft  i   rt
return  
else
let count    
while ti  s  is false do
choose an action a according to the current exploration policy x i  s 
let n   maxq   a   s   recursive call 
observe result state s
ct  i  s  a          fft  i    ct  i  s  a    fft  i    n vt  i  s   
  

  

count    count   n
s    s 

end
return count

end maxq  

   main program
initialize v  i  s  and c  i  s  j   arbitrarily
maxq   root node    starting state s   

there are three things that must be specified in order to make this algorithm description
complete 
first  to keep the pseudo code readable  table   does not show how  ancestor termination  is handled  recall that after each action  the termination predicates of all of the
subroutines on the calling stack are checked  if the termination predicate of any one of
these is satisfied  then the calling stack is unwound up to the highest terminated subroutine  in such cases  no c values are updated in any of the subroutines that were interrupted
except as follows  if subroutine i had invoked subroutine j   and j  s termination condition
is satisfied  then subroutine i can update the value of c  i  s  j   
second  we must specify how to compute vt  i  s    in line     since it is not stored in
the max node  it is computed by the following modified versions of the decomposition
equations 
 

maxa qt  i  s  a  if i is composite
    
vt  i  s 
if i is primitive
qt i  s  a    vt a  s    ct  i  s  a  
    
these equations reect two important changes compared with equations      and      
first  in equation       vt  i  s  is defined in terms of the q value of the best action a  rather
than of the action chosen by a fixed hierarchical policy  second  there are no  superscripts 
because the current value function  vt  i  s   is not based on a fixed hierarchical policy  
to compute vt  i  s  using these equations  we must perform a complete search of all
paths through the maxq graph starting at node i and ending at the leaf nodes  table  

vt  i  s   

   

fidietterich

table    pseudo code for greedy execution of the maxq graph 
function evaluatemaxnode i  s 
 
 
 
 
 
 
 

if i is a primitive max node
return hvt  i  s   ii
else
for each j   ai  
let hvt  j  s   aj i   evaluatemaxnode j  s 
let j hg   argmaxj vt j  s    ct  i  s  j  
return hvt  j hg   s   ajhg i
end    evaluatemaxnode

gives pseudo code for a recursive function  evaluatemaxnode  that implements a depthfirst search  in addition to returning vt  i  s   evaluatemaxnode also returns the action
at the leaf node that achieves this value  this information is not needed for maxq    but it
will be useful later when we consider non hierarchical execution of the learned recursivelyoptimal policy 
this search can be computationally expensive  and a problem for future research is
to develop more ecient methods for computing the best path through the graph  one
approach is to perform a best first search and use bounds on the values within subtrees to
prune useless paths through the maxq graph  a better approach would be to make the
computation incremental  so that when the state of the environment changes  only those
nodes whose values have changed as a result of the state change are re considered  it should
be possible to develop an ecient bottom up method similar to the rete algorithm  and
its successors  that is used in the soar architecture  forgy        tambe   rosenbloom 
      
the third thing that must be specified to complete our definition of maxq   is the
exploration policy  x   we require that x be an ordered glie policy 

definition   an ordered glie policy is a glie policy  greedy in the limit with infinite

exploration  that converges in the limit to an ordered greedy policy  which is a greedy policy
that imposes an arbitrary fixed order   on the available actions and breaks ties in favor of
the action a that appears earliest in that order 

we need this property in order to ensure that maxq   converges to a uniquely defined
recursively optimal policy  a fundamental problem with recursive optimality is that in
general  each max node i will have a choice of many different locally optimal policies given
the policies adopted by its descendent nodes  these different locally optimal policies will
all achieve the same locally optimal value function  but they can give rise to different probability transition functions p  s    n js  i   the result will be that the semi markov decision
problems defined at the next level above node i in the maxq graph will differ depending
on which of these various locally optimal policies is chosen by node i  these differences may
lead to better or worse policies at higher levels of the maxq graph  even though they make
no difference inside subtask i  in practice  the designer of the maxq graph will need to
design the pseudo reward function for subtask i to ensure that all locally optimal policies
   

fimaxq hierarchical reinforcement learning

are equally valuable for the parent subroutine  but to carry out our formal analysis  we will
just rely on an arbitrary tie breaking mechanism   if we establish a fixed ordering over the
max nodes in the maxq graph  e g   a left to right depth first numbering   and break ties
in favor of the lowest numbered action  then this defines a unique policy at each max node 
and consequently  by induction  it defines a unique policy for the entire maxq graph  let
us call this policy r   we will use the r subscript to denote recursively optimal quantities
under an ordered greedy policy  hence  the corresponding value function is vr   and cr and
qr denote the corresponding completion function and action value function  we now prove
that the maxq   algorithm converges to r  

theorem   let m   hs  a  p  r  p  i be either an episodic mdp for which all deterministic

policies are proper or a discounted infinite horizon mdp with discount factor    let h be
a maxq graph defined over subtasks fm            mk g such that the pseudo reward function
r  i  s    is zero for all i and s   let fft  i      be a sequence of constants for each max node i
such that
t
t
x
x
lim
ff
 
i
 
 
 
and
lim
ff t  i     
    
t
t   
t   
t  

t  

let x  i  s  be an ordered glie policy at each node i and state s and assume that the
immediate rewards are bounded  then with probability    algorithm maxq   converges to
r   the unique recursively optimal policy for m consistent with h and x 

proof  the proof follows an argument similar to those introduced to prove the convergence

of q learning and sarsa     bertsekas   tsitsiklis        jaakkola et al          we will
employ the following result from stochastic approximation theory  which we state without
proof 

lemma    proposition     from bertsekas and tsitsiklis        consider the iteration
rt    i          fft  i  rt  i    fft  i   urt   i    wt  i    ut i   
let ft   fr   i           rt  i   w   i           wt    i   ff   i           fft  i    ig be the entire history of the
iteration 
if

 a  the fft  i     satisfy conditions     
 b  for every i and t  the noise terms wt  i  satisfy e  wt  i jft      
 c  given any norm jj  jj on rn   there exist constants a and b such that e  wt   i jft   
a   b jjrtjj   
 d  there exists a vector r   a positive vector    and a scalar fi           such that for all
t 

jjurt   rjj  fi jjrt   rjj

   alternatively  we could break ties by using a stochastic policy that chose randomly among the tied
actions 

   

fidietterich

 e  there exists a nonnegative random sequence t that converges to zero with probability
  and is such that for all t
jut  i j  t jjrt jj     
then rt converges to r with probability    the notation jj  jj denotes a weighted maximum
norm
ja i j  
jjajj   max
i   i 

the structure of the proof of theorem   will be inductive  starting at the leaves of the
maxq graph and working toward the root  we will employ a different time clock at each
node i to count the number of update steps performed by maxq   at that node  the
variable t will always refer to the time clock of the current node i 
to prove the base case for any primitive max node  we note that line   of maxq   is
just the standard stochastic approximation algorithm for computing the expected reward
for performing action a in state s  and therefore it converges under the conditions given
above 
to prove the recursive case  consider any composite max node i with child node j   let
pt  s   n js  j   be the transition probability distribution for performing child action j in state
s at time t  i e   while following the exploration policy in all descendent nodes of node j   
by the inductive assumption  maxq   applied to j will converge to the  unique  recursively optimal value function vr  j  s  with probability    furthermore  because maxq  
is following an ordered glie policy for j and its descendents  they will all converge to executing a greedy policy with respect to their value functions  so pt  s    n js  j   will converge
to pr  s    n js  j    the unique transition probability function for executing child j under the
locally optimal policy r   what remains to be shown is that the update assignment for c
 line    of the maxq   algorithm  converges to the optimal cr function with probability
  
to prove this  we will apply lemma    we will identify the x in the lemma with a
state action pair  s  a   the vector rt will be the completion cost table ct  i  s  a  for all
s  a and fixed i after t update steps  the vector r will be the optimal completion cost
cr  i  s  a   again  for fixed i   define the mapping u to be
 uc   i  s  a   

x

s 





   
    
pr  s    n js  a  n max
   c  i  s   a     vr  a   s   
a

this is a c update under the mdp mi assuming that all descendent value functions 
vr  a  s   and transition probabilities  pr s    n js  a   have converged 
to apply the lemma  we must first express the c update formula in the form of the
update rule in the lemma  let s be the state that results from performing a in state s  line
   can be written

ct    i  s  a    

     fft  i    ct  i  s  a    fft  i    n





max ct  i  s  a      vt  a    s  
a 

        fft  i    ct  i  s  a    fft  i     uct   i  s  a    wt  i  s  a    ut  i  s  a  
   

fimaxq hierarchical reinforcement learning

where





wt  i  s  a     n max
 c  i  s  a      vt  a    s    
a  t
x

s   n

ut  i  s  a   

x

s   n
x

s   n









pt  s    n js  a  n max
 c  i  s    a      vt  a    s    
a  t

   
   
pt  s    n js  a  n max
   ct  i  s   a     vt  a   s   


a

 



pr  s   n js  a  n max
 c  i  s    a      vr  a    s    
a  t

here wt  i  s  a  is the difference between doing an update at node i using the single sample
point s drawn according to pt  s    n js  a  and doing an update using the full distribution
pt  s   n js  a   the value of ut  i  s  a  captures the difference between doing an update using
the current probability transitions pt  s    n js  a  and current value functions of the children
vt  a   s    and doing an update using the optimal probability transitions pr  s    n js  a  and
the optimal values of the children vr  a    s    
we now verify the conditions of lemma   
condition  a  is assumed in the conditions of the theorem with fft  s  a    fft  i  
condition  b  is satisfied because s is sampled from pt  s    n js  a   so the expected value
of the difference is zero 
condition  c  follows directly from the fact that jct  i  s  a j and jvt  i  s j are bounded 
we can show that these are bounded for both the episodic case and the discounted case as
follows  in the episodic case  we have assumed all policies are proper  hence  all trajectories
terminate in finite time with a finite total reward  in the discounted case  the infinite sum
of future rewards is bounded if the one step rewards are bounded  the values of c and v
are computed as temporal averages of the cumulative rewards received over a finite number
of  bounded  updates  and hence  their means  variances  and maximum values are all
bounded 
condition  d  is the condition that u is a weighted max norm pseudo contraction  we
can derive this by starting with the weighted max norm for q learning  it is well known
that q is a weighted max norm pseudo contraction  bertsekas   tsitsiklis        in both
the episodic case where all deterministic policies are proper  and the discount factor      
and in the infinite horizon discounted case  with        that is  there exists a positive
vector  and a scalar fi           such that for all t 

jjtqt   qjj  fi jjqt   qjj  

    

where t is the operator
 tq  s  a   

x

s   n

p  s   n js  a  n  r s  js  a    max
q s    a     
a 

now we will show how to derive the pseudo contraction for the c update operator u   our
plan is to show first how to express the u operator for learning c in terms of the t operator
for updating q values  then we will replace tq in the pseudo contraction equation for q
   

fidietterich

learning with uc   and show that u is a weighted max norm pseudo contraction under the
same weights  and the same fi  
recall from eqn       that q i  s  a    c  i  s  a    v  a  s   furthermore  the u operator
performs its updates using the optimal value functions of the child nodes  so we can write
this as qt  i  s  a    ct  i  s  a    v   a  s   now once the children of node i have converged 
the q function version of the bellman equation for mdp mi can be written as

q i  s  a   

x

s   n

pr s    n js  a  n  vr  a  s    max
q i  s    a     
a 

as we have noted before  vr  a  s  plays the role of the immediate reward function for mi  
therefore  for node i  the t operator can be rewritten as
 tq  i  s  a   

x

s   n

pr  s  js  a  n  vr a  s    max
q i  s    a     
a 

now we replace q i  s  a  by c  i  s  a    vr  a  s   and obtain
 tq  i  s  a   

x

s   n

pr  s   n js  a  n  vr  a  s    max
 c  i  s    a      vr  a    s      
a 

note that vr  a  s  does not depend on s  or n   so we can move it outside the expectation
and obtain
 tq  i  s  a    vr  a  s   

x

s   n

pr  s    n js  a  n  max
 c  i  s    a      vr  a    s     
a 

  vr  a  s     uc   i  s  a 

abusing notation slightly  we will express this in vector form as tq i    vr   uc  i  
similarly  we can write qt  i  s  a    ct  i  s  a   vr  a  s  in vector form as qt  i    ct  i   vr  
now we can substitute these two formulas into the max norm pseudo contraction formula
for t   eqn       to obtain

jjvr   uct  i     cr i    vr jj  fi jjvr   ct  i     cr i    vr jj  
thus  u is a weighted max norm pseudo contraction 

jjuct  i    cr i jj  fi jjct  i    cr i jj  
and condition  d  is satisfied 
finally  it is easy to verify  e   the most important condition  by assumption  the
ordered glie policies in the child nodes converge with probability   to locally optimal
policies for the children  therefore pt  s    n js  a  converges to pr  s    n js  a  for all s   n  s 
and a with probability   and vt  a  s  converges with probability   to vr  a  s  for all child
actions a  therefore  jut j converges to zero with probability    we can trivially construct a
sequence t   jut j that bounds this convergence  so

jut  s  a j  t  t  jjct  s  a jj      
   

fimaxq hierarchical reinforcement learning

we have verified all of the conditions of lemma    so we can conclude that ct  i  converges
to cr i  with probability    by induction  we can conclude that this holds for all nodes in
the maxq including the root node  so the value function represented by the maxq graph
converges to the unique value function of the recursively optimal policy r   q e d 
the most important aspect of this theorem is that it proves that q learning can take
place at all levels of the maxq hierarchy simultaneously the higher levels do not need to
wait until the lower levels have converged before they begin learning  all that is necessary
is that the lower levels eventually converge to their  locally  optimal policies 

    techniques for speeding up maxq  

algorithm maxq   can be extended to accelerate learning in the higher nodes of the graph
by a technique that we call  all states updating   when an action a is chosen for max node
i in state s  the execution of a will move the environment through a sequence of states
s   s           sn   sn      s   because all of our subroutines are markovian  the same resulting
state s  would have been reached if we had started executing action a in state s    or s    or
any state up to and including sn   hence  we can execute a version of line    in maxq  
for each of these intermediate states as shown in this replacement pseudo code 
  a
for j from   to n do
  b
ct  i  sj   a          fft  i    ct i  sj   a    fft  i    n  j maxa qt  i  s    a   
  c
end    for
in our implementation  as each composite action is executed by maxq    it constructs
a linked list of the sequence of primitive states that were visited  this list is returned when
the composite action terminates  the parent max node can then process each state in this
list as shown above  the parent max node concatenates the state lists that it receives from
its children and passes them to its parent when it terminates  all experiments in this paper
employ all states updating 
kaelbling        introduced a related  but more powerful  method for accelerating hierarchical reinforcement learning that she calls  all goals updating   to understand this
method  suppose that for each primitive action  there are several composite tasks that could
have invoked that primitive action  in all goals updating  whenever a primitive action is
executed  the equivalent of line    of maxq   is applied in every composite task that could
have invoked that primitive action  sutton  precup  and singh        prove that each of
the composite tasks will converge to the optimal q values under all goals updating  furthermore  they point out that the exploration policy employed for choosing the primitive
actions can be different from the policies of any of the subtasks being learned 
it is straightforward to implement a simple form of all goals updating within the maxq
hierarchy for the case where composite tasks invoke primitive actions  whenever one of the
primitive actions a is executed in state s  we can update the c  i  s  a  value for all parent
tasks i that can invoke a 
however  additional care is required to implement all goals updating for non primitive
actions  suppose that by executing the exploration policy  the following sequence of world
states and actions has been obtained  s    a    s            ak     sk     ak   sk     let j be a composite task that is terminated in state sk     and let sk n  ak n           ak     ak be a sequence of
actions that could have been executed by subtask j and its children  in other words  suppose
 

  

   

  

 

 

fidietterich

it is possible to  parse  this state action sequence in terms of a series of subroutine calls and
returns for one invocation of subtask j   then for each possible parent task i that invokes j  
we can update the value of c  i  sk n   j    of course  in order for these updates to be useful 
the exploration policy must be an ordered glie policy that will converge to the recursively
optimal policy for subtask j and its descendents  we cannot follow an arbitrary exploration
policy  because this would not produce accurate samples of result states drawn according to
p   s    n js  j    hence  unlike the simple case described by sutton  precup  and singh  the
exploration policy cannot be different from the policies of the subtasks being learned 
although this considerably reduces the usefulness of all goals updating  it does not
completely eliminate it  a simple way of implementing non primitive all goals updating
would be to perform maxq q learning as usual  but whenever a subtask j was invoked in
state s and returned  we could update the value of c  i  s  j   for all potential calling subtasks
i  we have not implemented this  however  because of the complexity involved in identifying
the possible actual parameters of the potential calling subroutines 

    the maxq q learning algorithm
now that we have shown the convergence of maxq    let us design a learning algorithm that
can work with arbitrary pseudo reward functions  r  i  s     we could just add the pseudoreward into maxq    but this would have the effect of changing the mdp m to have
a different reward function  the pseudo rewards  contaminate  the values of all of the
completion functions computed in the hierarchy  the resulting learned policy will not be
recursively optimal for the original mdp 
this problem can be solved by learning one completion function for use  inside  each
max node and a separate completion function for use  outside  the max node  the quantities used  inside  a node will be written with a tilde  r    c    and q    the quantities used
 outside  a node will be written without the tilde 
the  outside  completion function  c  i  s  a  is the completion function that we have
been discussing so far in this paper  it computes the expected reward for completing task
mi after performing action a in state s and then following the learned policy for mi   it is
computed without any reference to r  i   this completion function will be used by parent
tasks to compute v  i  s   the expected reward for performing action i starting in state s 
the second completion function c   i  s  a  is a completion function that we will use only
 inside  node i in order to discover the locally optimal policy for task mi   this function
will incorporate rewards both from the  real  reward function  r s  js  a   and from the
pseudo reward function  r  i  s     it will also be used by evaluatemaxnode in line   to
choose the best action j hg to execute  note  however  that evaluatemaxnode will still
return the  external  value vt  j hg   s  of this chosen action 
we will employ two different update rules to learn these two completion functions  the
c  function will be learned using an update rule similar to the q learning rule in line    of
maxq    but the c function will be learned using an update rule similar to sarsa    
its purpose is to learn the value function for the policy that is discovered by optimizing c   
pseudo code for the resulting algorithm  maxq q is shown in table   
the key step is at lines    and     in line     maxq q first updates c  using the value
of the greedy action  a   in the resulting state  this update includes the pseudo reward r  i  
   

fimaxq hierarchical reinforcement learning

table    the maxq q learning algorithm 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
  

function maxq q maxnode i  state s 
let seq      be the sequence of states visited while executing i
if i is a primitive maxnode
execute i  receive r  and observe result state s 
vt  i  s          fft  i    vt  i  s    fft  i   rt
push s onto the beginning of seq
else
let count    
while ti  s  is false do
choose an action a according to the current exploration policy x i  s 
let childseq   maxq q a s   where childseq is the sequence of states visited
  

while executing action a   in reverse order 
observe result state s 
let a   argmaxa  c t  i  s    a      vt  a    s    
let n    
for each s in childseq do
c t    i  s  a          fft  i    c t  i  s  a    fft  i    n  r  i  s      c t  i  s    a     vt a   s  
ct    i  s  a          fft  i    ct  i  s  a    fft  i    n  ct  i  s    a     vt a   s    
n    n    
end    for
append
childseq onto the front of seq
s    s 
end    while
end    else
return seq
end maxq q
 

then in line     maxq q updates c using this same greedy action a   even if this would
not be the greedy action according to the  uncontaminated  value function  this update 
of course  does not include the pseudo reward function 
it is important to note that whereever vt  a  s  appears in this pseudo code  it refers to
the  uncontaminated  value function of state s when executing the max node a  this is
computed recursively in exactly the same way as in maxq   
finally  note that the pseudo code also incorporates all states updating  so each call
to maxq q returns a list of all of the states that were visited during its execution  and
the updates of lines    and    are performed for each of those states  the list of states is
ordered most recent first  so the states are updated starting with the last state visited and
working backward to the starting state  which helps speed up the algorithm 
when maxq q has converged  the resulting recursively optimal policy is computed at
each node by choosing the action a that maximizes q   i  s  a    c   i  s  a   v  a  s   breaking
ties according to the fixed ordering established by the ordered glie policy   it is for this
reason that we gave the name  max nodes  to the nodes that represent subtasks  and
learned policies  within the maxq graph  each q node j with parent node i stores both
c   i  s  j   and c  i  s  j    and it computes both q   i  s  j   and q i  s  j   by invoking its child
max node j   each max node i takes the maximum of these q values and computes either
v  i  s  or computes the best action  a using q   
   

fidietterich

corollary   under the same conditions as theorem    maxq q converges to the unique

recursively optimal policy for mdp m defined by maxq graph h   pseudo reward functions
r    and ordered glie exploration policy x 
proof  the argument is identical to  but more tedious than  the proof of theorem    the
proof of convergence of the c  values is identical to the original proof for the c values  but
it relies on proving convergence of the  new  c values as well  which follows from the same
weighted max norm pseudo contraction argument  q e d 

   state abstraction

there are many reasons to introduce hierarchical reinforcement learning  but perhaps the
most important reason is to create opportunities for state abstraction  when we introduced
the simple taxi problem in figure    we pointed out that within each subtask  we can ignore
certain aspects of the state space  for example  while performing a maxnavigate t   the
taxi should make the same navigation decisions regardless of whether the passenger is in
the taxi  the purpose of this section is to formalize the conditions under which it is safe
to introduce such state abstractions and to show how the convergence proofs for maxq q
can be extended to prove convergence in the presence of state abstraction  specifically  we
will identify five conditions that permit the  safe  introduction of state abstractions 
throughout this section  we will use the taxi problem as a running example  and we will
see how each of the five conditions will permit us to reduce the number of distinct values
that must be stored in order to represent the maxq value function decomposition  to
establish a starting point  let us compute the number of values that must be stored for the
taxi problem without any state abstraction 
the maxq representation must have tables for each of the c functions at the internal
nodes and the v functions at the leaves  first  at the six leaf nodes  to store v  i  s   we
must store     values at each node  because there are     states     locations    possible
destinations for the passenger  and   possible current locations for the passenger  the four
special locations and inside the taxi itself   second  at the root node  there are two children 
which requires               values  third  at the maxget and maxput nodes  we have  
actions each  so each one requires      values  for a total of       finally  at maxnavigate t  
we have four actions  but now we must also consider the target parameter t  which can take
four possible values  hence  there are effectively      combinations of states and t values for
each action  or      total values that must be represented  in total  therefore  the maxq
representation requires        separate quantities to represent the value function 
to place this number in perspective  consider that a at q learning representation must
store a separate value for each of the six primitive actions in each of the     possible states 
for a total of       values  hence  we can see that without state abstraction  the maxq
representation requires more than four times the memory of a at q table 

    five conditions that permit state abstraction

we now introduce five conditions that permit the introduction of state abstractions  for
each condition  we give a definition and then prove a lemma which states that if the condition is satisfied  then the value function for some corresponding class of policies can be
   

fimaxq hierarchical reinforcement learning

represented abstractly  i e   by abstract versions of the v and c functions   for each condition  we then provide some rules for identifying when that condition can be satisfied and
give examples from the taxi domain 
we begin by introducing some definitions and notation 

definition    let m be a mdp and h be a maxq graph defined over m   suppose that

each state s can be written as a vector of values of a set of state variables  at each max
node i  suppose the state variables are partitioned into two sets xi and yi   and let i be a
function that projects a state s onto only the values of the variables in xi   then h combined
with i is called a state abstracted maxq graph 

in cases where the state variables can be partitioned  we will often write s    x  y 
to mean that a state s is represented by a vector of values for the state variables in x
and a vector of values for the state variables in y   similarly  we will sometimes write
p  x    y   n jx  y  a   v  a  x  y   and r  a  x    y    in place of p  s   n js  a   v  a  s   and r  a  s    
respectively 

definition     abstract policy  an abstract hierarchical policy for mdp m with state 

abstracted maxq graph h and associated abstraction functions i   is a hierarchical policy
in which each policy i  corresponding to subtask mi   satisfies the condition that for any two
states s  and s  such that i  s      i  s     i  s      i  s      when i is a stochastic policy 
such as an exploration policy  this is interpreted to mean that the probability distributions
for choosing actions are the same in both states  

in order for maxq q to converge in the presence of state abstractions  we will require
that at all times t its  instantaneous  exploration policy is an abstract hierarchical policy 
one way to achieve this is to construct the exploration policy so that it only uses information from the relevant state variables in deciding what action to perform  boltzmann exploration based on the  state abstracted  q values   greedy exploration  and counter based
exploration based on abstracted states are all abstract exploration policies  counter based
exploration based on the full state space is not an abstract exploration policy 
now that we have introduced our notation  let us describe and analyze the five abstraction conditions  we have identified three different kinds of conditions under which
abstractions can be introduced  the first kind involves eliminating irrelevant variables
within a subtask of the maxq graph  under this form of abstraction  nodes toward the
leaves of the maxq graph tend to have very few relevant variables  and nodes higher in
the graph have more relevant variables  hence  this kind of abstraction is most useful at
the lower levels of the maxq graph 
the second kind of abstraction arises from  funnel  actions  these are macro actions
that move the environment from some large number of initial states to a small number of
resulting states  the completion cost of such subtasks can be represented using a number of
values proportional to the number of resulting states  funnel actions tend to appear higher
in the maxq graph  so this form of abstraction is most useful near the root of the graph 
the third kind of abstraction arises from the structure of the maxq graph itself  it
exploits the fact that large parts of the state space for a subtask may not be reachable
because of the termination conditions of its ancestors in the maxq graph 
   

fidietterich

we begin by describing two abstraction conditions of the first type  then we will present
two conditions of the second type  and finally  we describe one condition of the third type 
      condition    max node irrelevance

the first condition arises when a set of state variables is irrelevant to a max node 

definition     max node irrelevance  let mi be a max node in a maxq graph h

for mdp m   a set of state variables y is irrelevant for node i if the state variables of m
can be partitioned into two sets x and y such that for any stationary abstract hierarchical
policy  executed by the descendents of i  the following two properties hold 

 the state transition probability distribution p   s   n js  a  at node i can be factored into
the product of two distributions 

p   x    y    n jx  y  a    p   y jx  y  a   p   x    n jx  a  

    

where y and y  give values for the variables in y   and x and x  give values for the
variables in x  

 for any pair of states s     x  y    and s     x  y    such that  s      s     x  and
any child action a  v   a  s      v   a  s    and r  i  s      r  i  s    

note that the two conditions must hold for all stationary abstract policies  executed
by all of the descendents of the subtask i  we will discuss below how these rather strong
requirements can be satisfied in practice  first  however  we prove that these conditions are
sucient to permit the c and v tables to be represented using state abstractions 

lemma   let m be an mdp with full state maxq graph h   and suppose that state vari 

ables yi are irrelevant for max node i  let i  s    x be the associated abstraction function
that projects s onto the remaining relevant variables xi   let  be any abstract hierarchical
policy  then the action value function q at node i can be represented compactly  with only
one value of the completion function c   i  s  j   for each equivalence class of states s that
share the same values on the relevant variables 
specifically q  i  s  j   can be computed as follows 

q  i  s  j     v   j  i  s     c   i  i  s   j  
where

c   i  x  j    

x

x   n

p   x    n jx  j     n  v    x     x      r  i  x      c   i  x     x      

where v   j     x      v   j     x    y     r  i  x      r  i  x    y     and  x     x  y    for some arbitrary
value y  for the irrelevant state variables yi  
   

fimaxq hierarchical reinforcement learning

proof  define a new mdp i mi   at node i as follows 
 states  x   fx j i s    x  for some s   s g 
 actions  a 
 transition probabilities  p   x    n jx  a 
 reward function  v   a  x    r  i x  

because  is an abstract policy  its decisions are the same for all states s such that i  s    x
for some x  therefore  it is also a well defined policy over i  mi    the action value function
for  over i  mi   is the unique solution to the following bellman equation 
x
q  i  x  j     v   j  x    p   x    n jx  j     n  r  i x      q  i  x     x     
    
x   n

compare this to the bellman equation over mi  
x
q  i  s  j     v   j  s    p   s   n js  j     n  r i  s     q  i  s     s     
s   n

    

and note that v   j  s    v   j   s     v   j  x  and r  i  s      r  i   s       r  i  x     furthermore  we know that the distribution p  can be factored into separate distributions for yi
and xi   hence  we can rewrite      as
x
x
q  i  s  j     v   j  x    p  y  jx  y  j   p   x    n jx  j     n  r  i  x      q  i  s     s     
y 

x   n

the right most sum does not depend on y or y    so the sum over y  evaluates to    and can
be eliminated to give
x
q  i  s  j     v   j  x    p   x    n jx  j     n  r  i x      q  i  s     s      
    
x   n

finally  note that equations      and      are identical except for the expressions for
the q values  since the solution to the bellman equation is unique  we must conclude that
q  i  s  j     q  i   s   j   
we can rewrite the right hand side to obtain
q  i  s  j     v   j   s     c   i   s   j   
where
x
c   i  x  j     p  x   n jx  j     n  v    x     x      r  i  x      c   i  x     x      

q e d 

x   n

of course we are primarily interested in being able to discover and represent the optimal
policy at each node i  the following corollary shows that the optimal policy is an abstract
policy  and hence  that it can be represented abstractly 
   

fidietterich

corollary   consider the same conditions as lemma    but with the change that the ab 

stract hierarchical policy  is executed only by the descendents of node i  but not by node
i  let   be an ordering over actions  then the optimal ordered policy   at node i is an
abstract policy  and its action value function can be represented abstractly 

proof  define the policy   to be the optimal ordered policy over the abstract mdp

 m    and let q  i  x  j   be the corresponding optimal action value function  then by the
same argument given above  q is also a solution to the optimal bellman equation for the
original mdp  this means that the policy   defined by    s       s   is an optimal

ordered policy  and by construction  it is an abstract policy  q e d 
as stated  the max node irrelevance condition appears quite dicult to satisfy  since it
requires that the state transition probability distribution factor into x and y components
for all possible abstract hierarchical policies  however  in practice  this condition is often
satisfied 
for example  let us consider the navigate t  subtask  the source and destination of
the passenger are irrelevant to the achievement of this subtask  any policy that successfully completes this subtask will have the same value function regardless of the source and
destination locations of the passenger  by abstracting away the passenger source and destination  we obtain a huge savings in space  instead of requiring      values to represent
the c functions for this task  we require only     values    actions     locations    possible
values for t  
the advantages of this form of abstraction are similar to those obtained by boutilier 
dearden and goldszmidt        in which belief network models of actions are exploited
to simplify value iteration in stochastic planning  indeed  one way of understanding the
conditions of definition    is to express them in the form of a decision diagram  as shown
in figure    the diagram shows that the irrelevant variables y do not affect the rewards
either directly or indirectly  and therefore  they do not affect either the value function or
the optimal policy 
one rule for noticing cases where this abstraction condition holds is to examine the
subgraph rooted at the given max node i  if a set of state variables is irrelevant to the leaf
state transition probabilities and reward functions and also to all pseudo reward functions
and termination conditions in the subgraph  then those variables satisfy the max node
irrelevance condition 

lemma   let m be an mdp with associated maxq graph h   and let i be a max node in

h   let xi and yi be a partition of the state variables for m   a set of state variables yi is
irrelevant to node i if
 for each primitive leaf node a that is a descendent of i 
p  x    y  jx  y  a    p  y  jx  y  a p  x  jx  a  and
r x   y  jx  y  a    r x  jx  a  

 for each internal node j that is equal to node i or is a descendent of i   r  j  x    y    
r j  x   and the termination predicate tj  x    y    is true iff tj  x   
   

fimaxq hierarchical reinforcement learning

j

v

x

x

y

y

figure    a dynamic decision diagram that represents the conditions of definition     the
probabilistic nodes x and y represent the state variables at time t  and the nodes
x   and y   represent the state variables at a later time t   n   the square action
node j is the chosen child subroutine  and the utility node v represents the value
function v  j  x  of that child action  note that while x may inuence y     y
cannot affect x     and therefore  it cannot affect v  

proof  we must show that any abstract hierarchical policy will give rise to an smdp at

node i whose transition probability distribution factors and whose reward function depends
only on xi   by definition  any abstract hierarchical policy will choose actions based only
upon information in xi   because the primitive probability transition functions factor into
an independent component for xi and since the termination conditions at all nodes below i
are based only on the variables in xi   the probability transition function pi  x    y    n jx  y  a 
must also factor into pi  y  jx  y  a  and pi  x    n jx  a   similarly  all of the reward functions
v  j  x  y  must be equal to v  j  x   because all rewards received within the subtree  either
at the leaves or through pseudo rewards  depend only on the variables in xi   therefore 
the variables in yi are irrelevant for max node i  q e d 
in the taxi task  the primitive navigation actions  north  south  east  and west only
depend on the location of the taxi and not on the location of the passenger  the pseudoreward function and termination condition for the maxnavigate t  node only depend on the
location of the taxi  and the parameter t   hence  this lemma applies  and the passenger
source and destination are irrelevant for the maxnavigate node 
      condition    leaf irrelevance

the second abstraction condition describes situations under which we can apply state abstractions to leaf nodes of the maxq graph  for leaf nodes  we can obtain a stronger result
than lemma   by using a slightly weaker definition of irrelevance 
   

fidietterich

definition     leaf irrelevance  a set of state variables y is irrelevant for a primitive
action a of a maxq graph if for all states s the expected value of the reward function 
x
v  a  s    p  s  js  a r s  js  a 
s 

does not depend on any of the values of the state variables in y   in other words  for any
pair of states s  and s  that differ only in their values for the variables in y  
x

s  

p  s   js    a r s   js    a   

x

s  

p  s   js    a r s   js    a  

if this condition is satisfied at leaf a  then the following lemma shows that we can
represent its value function v  a  s  compactly 

lemma   let m be an mdp with full state maxq graph h   and suppose that state vari 

ables y are irrelevant for leaf node a  let  s    x be the associated abstraction function
that projects s onto the remaining relevant variables x   then we can represent v  a  s  for
any state s by an abstracted value function v  a   s     v  a  x  

proof  according to the definition of leaf irrelevance  any two states that differ only on

the irrelevant state variables have the same value for v  a  s   hence  we can represent this
unique value by v  a  x   q e d 
here are two rules for finding cases where leaf irrelevance applies  the first rule shows
that if the probability distribution factors  then we have leaf irrelevance 

lemma   suppose the probability transition function for primitive action a  p  s js  a   factors as p  x    y  jx  y  a    p  y  jx  y  a p  x  jx  a  and the reward function satisfies r s  js  a   
r x jx  a   then the variables in y are irrelevant to the leaf node a 
proof  plug in to the definition of v  a  s  and simplify 
x
v  a  s   
p  s  js  a r s  js  a 
 
 
 

s 

x

x   y 
x

y 

x

x 

p  y  jx  y  a p  x  jx  a r x  jx  a 

p  y  jx  y  a 

x

x 

p  x  jx  a r x  jx  a 

p  x  jx  a r x  jx  a 

hence  the expected reward for the action a depends only on the variables in x and not on
the variables in y   q e d 
the second rule shows that if the reward function for a primitive action is constant 
then we can apply state abstractions even if p  s  js  a  does not factor 

lemma   suppose r s js  a   the reward function for action a in mdp m   is always equal

to a constant ra   then the entire state s is irrelevant to the primitive action a 
   

fimaxq hierarchical reinforcement learning

proof 
v  a  s   

x

s 

p  s  js  a r s  js  a 

x

 
p  s  js  a ra
 
s
  ra  
this does not depend on s  so the entire state is irrelevant to the primitive action a  q e d 
this lemma is satisfied by the four leaf nodes north  south  east  and west in the taxi
task  because their one step reward is a constant       hence  instead of requiring     
values to store the v functions  we only need   values one for each action  similarly  the
expected rewards of the pickup and putdown actions each require only   values  depending
on whether the corresponding actions are legal or illegal  hence  together  they require  
values  instead of      values 
      condition    result distribution irrelevance

now we consider a condition that results from  funnel  actions 

definition     result distribution irrelevance   a set of state variables yj is irrelevant for the result distribution of action j if  for all abstract policies  executed by node j
and its descendents in the maxq hierarchy  the following holds  for all pairs of states s 
and s  that differ only in their values for the state variables in yj  

p   s    n js    j     p   s    n js    j  
for all s  and n  

if this condition is satisfied for subtask j   then the c value of its parent task i can be
represented compactly 

lemma   let m be an mdp with full state maxq graph h   and suppose that the set of

state variables yj is irrelevant to the result distribution of action j   which is a child of max
node i  let ij be the associated abstraction function  ij  s    x  then we can define an
abstract completion cost function c   i  ij  s   j   such that for all states s 

c   i  s  j     c   i  ij  s   j   

proof  the completion function for fixed policy  is defined as follows 
x
c   i  s  j     p  s    n js  j     n q  i  s    
s   n

    

consider any two states s  and s    such that ij  s      ij  s      x  under result distribution irrelevance  their transition probability distributions are the same  hence  the
right hand sides of      have the same value  and we can conclude that

c   i  s    j     c   i  s    j   
   

fidietterich

therefore  we can define an abstract completion function  c   i  x  j   to represent this quantity  q e d 
in undiscounted cumulative reward problems  the definition of result distribution irrelevance can be weakened to eliminate n   the number of steps  all that is needed is
that for all pairs of states s  and s  that differ only in the irrelevant state variables 
p   s  js    j     p   s  js   j    for all s     in the undiscounted case  lemma   still holds under
this revised definition 
it might appear that the result distribution irrelevance condition would rarely be satisfied  but we often find cases where the condition is true  consider  for example  the get
subroutine for the taxi task  no matter what location the taxi has in state s  the taxi
will be at the passenger s starting location when the get finishes executing  i e   because
the taxi will have just completed picking up the passenger   hence  the starting location
is irrelevant to the resulting location of the taxi  and p  s  js    get    p  s  js    get  for all
states s  and s  that differ only in the taxi s location 
note  however  that if we were maximizing discounted reward  the taxi s location would
not be irrelevant  because the probability that get will terminate in exactly n steps would
depend on the location of the taxi  which could differ in states s  and s    different values
of n will produce different amounts of discounting in       and hence  we cannot ignore the
taxi location when representing the completion function for get 
but in the undiscounted case  by applying lemma    we can represent c  root  s  get 
using    distinct values  because there are    equivalence classes of states    source locations
times   destination locations   this is much less than the     quantities in the unabstracted
representation 
note that although state variables y may be irrelevant to the result distribution of a
subtask j   they may be important within subtask j   in the taxi task  the location of the
taxi is critical for representing the value of v  get  s   but it is irrelevant to the result state
distribution for get  and therefore it is irrelevant for representing c  root  s  get   hence  the
maxq decomposition is essential for obtaining the benefits of result distribution irrelevance 
 funnel  actions arise in many hierarchical reinforcement learning problems  for example  abstract actions that move a robot to a doorway or that move a car onto the entrance
ramp of a freeway have this property  the result distribution irrelevance condition is
applicable in all such situations as long as we are in the undiscounted setting 
      condition    termination

the fourth condition is closely related to the  funnel  property  it applies when a subtask
is guaranteed to cause its parent task to terminate in a goal state  in a sense  the subtask
is funneling the environment into the set of states described by the goal predicate of the
parent task 

lemma    termination   let mi be a task in a maxq graph such that for all states s
where the goal predicate gi  s  is true  the pseudo reward function r  i  s       suppose there
is a child task a and state s such that for all hierarchical policies  

  s  pi  s   n js  a        gi s    
   

fimaxq hierarchical reinforcement learning

 i e   every possible state s  that results from applying a in s will make the goal predicate 
gi   true  
then for any policy executed at node i  the completion cost c  i  s  a  is zero and does
not need to be explicitly represented 

proof  when action a is executed in state s  it is guaranteed to result in a state s  such

that gi  s  is true  by definition  goal states also satisfy the termination predicate ti  s   so
task i will terminate  because gi s  is true  the terminal pseudo reward will be zero  and
hence  the completion function will always be zero  q e d 
for example  in the taxi task  in all states where the taxi is holding the passenger  the
put subroutine will succeed and result in a goal terminal state for root  this is because the
termination predicate for put  i e   that the passenger is at his or her destination location 
implies the goal condition for root  which is the same   this means that c  root  s  put  is
uniformly zero  for all states s where put is not terminated 
it is easy to detect cases where the termination condition is satisfied  we only need to
compare the termination predicate ta of a subtask with the goal predicate gi of the parent
task  if the first implies the second  then the termination lemma is satisfied 
      condition    shielding

the shielding condition arises from the structure of the maxq graph 
lemma    shielding   let mi be a task in a maxq graph and s be a state such that in
all paths from the root of the graph down to node mi there is a subtask j  possibly equal to i 
whose termination predicate tj  s  is true  then the q nodes of mi do not need to represent
c values for state s 
proof  in order for task i to be executed in state s  there must exist some path of ancestors
of task i leading up to the root of the graph such that all of those ancestor tasks are not
terminated  the condition of the lemma guarantees that this is false  and hence that task
i cannot be executed in state s  therefore  no c values need to be represented  q e d 
as with the termination condition  the shielding condition can be verified by analyzing
the structure of the maxq graph and identifying nodes whose ancestor tasks are terminated 
in the taxi domain  a simple example of this arises in the put task  which is terminated
in all states where the passenger is not in the taxi  this means that we do not need
to represent c  root  s  put  in these states  the result is that  when combined with the
termination condition above  we do not need to explicitly represent the completion function
for put at all 
      dicussion

by applying these five abstraction conditions  we obtain the following  safe  state abstractions for the taxi task 
 north  south  east  and west  these terminal nodes require one quantity each  for a
total of four values   leaf irrelevance  
   

fidietterich

 pickup and putdown each require   values  legal and illegal states   for a total of four 
 leaf irrelevance  

 qnorth t   qsouth t   qeast t   and qwest t  each require     values  four values for
t and    locations    max node irrelevance  

 qnavigateforget requires   values  for the four possible source locations    the passenger destination is max node irrelevant for maxget  and the taxi starting location
is result distribution irrelevant for the navigate action  

 qpickup requires     possible values    possible source locations and    possible taxi
locations   passenger destination is max node irrelevant to maxget  

 qget requires    possible values    source locations    destination locations    result
distribution irrelevance  

 qnavigateforput requires only   values  for the four possible destination locations  

 the passenger source and destination are max node irrelevant to maxput  the taxi
location is result distribution irrelevant for the navigate action  

 qputdown requires     possible values     taxi locations    possible destination locations    passenger source is max node irrelevant for maxput  

 qput requires   values   termination and shielding  
this gives a total of     distinct values  which is much less than the      values required
by at q learning  hence  we can see that by applying state abstractions  the maxq
representation can give a much more compact representation of the value function 
a key thing to note is that with these state abstractions  the value function is decomposed into a sum of terms such that no single term depends on the entire state of the mdp 
even though the value function as a whole does depend on the entire state of the mdp  for
example  consider again the state described in figures   and    there  we showed that the
value of a state s  with the passenger at r  the destination at b  and the taxi at       can
be decomposed as

v  root  s      v  north  s      c  navigate r   s    north   
c  get  s    navigate r     c  root  s    get 
with state abstractions  we can see that each term on the right hand side only depends on
a subset of the features 

 v  north  s   is a constant
 c  navigate r   s    north  depends only on the taxi location and the passenger s source
location 

 c  get  s   navigate r   depends only on the source location 
 c  root  s    get  depends only on the passenger s source and destination 
   

fimaxq hierarchical reinforcement learning

without the maxq decomposition  no features are irrelevant  and the value function depends on the entire state 
what prior knowledge is required on the part of a programmer in order to identify
these state abstractions  it suces to know some qualitative constraints on the one step
reward functions  the one step transition probabilities  and termination predicates  goal
predicates  and pseudo reward functions within the maxq graph  specifically  the max
node irrelevance and leaf irrelevance conditions require simple analysis of the one step
transition function and the reward and pseudo reward functions  opportunities to apply
the result distribution irrelevance condition can be found by identifying  funnel  effects
that result from the definitions of the termination conditions for operators  similarly  the
shielding and termination conditions only require analysis of the termination predicates of
the various subtasks  hence  applying these five conditions to introduce state abstractions is
a straightforward process  and once a model of the one step transition and reward functions
has been learned  the abstraction conditions can be checked to see if they are satisfied 

    convergence of maxq q with state abstraction

we have shown that state abstractions can be safely introduced into the maxq value
function decomposition under the five conditions described above  however  these conditions only guarantee that the value function of any fixed abstract hierarchical policy can be
represented they do not show that recursively optimal policies can be represented  nor do
they show that the maxq q learning algorithm will find a recursively optimal policy when
it is forced to use these state abstractions  the goal of this section is to prove these two
results   a  that the ordered recursively optimal policy is an abstract policy  and  hence 
can be represented using state abstractions  and  b  that maxq q will converge to this
policy when applied to a maxq graph with safe state abstractions 

lemma    let m be an mdp with full state maxq graph h and abstract state maxq
graph  h   where the abstractions satisfy the five conditions given above  let   be an
ordering over all actions in the maxq graph  then the following statements are true 
 the unique ordered recursively optimal policy r defined by m   h   and   is an abstract policy  i e   it depends only on the relevant state variables at each node  see
definition     
 the c and v functions in  h   can represent the projected value function of r 

proof  the five abstraction lemmas tell us that if the ordered recursively optimal policy
is abstract  then the c and v functions of  h   can represent its value function  hence 
the heart of this lemma is the first claim  the last two forms of abstraction  shielding and
termination  do not place any restrictions on abstract policies  so we ignore them in this
proof 
the proof is by induction on the levels of the maxq graph  starting at the leaves  as
a base case  let us consider a max node i all of whose children are primitive actions  in this
case  there are no policies executed within the children of the max node  hence if variables
yi are irrelevant for node i  then we can apply our abstraction lemmas to represent the
value function of any policy at node i not just abstract policies  consequently  the value
   

fidietterich

function of any optimal policy for node i can be represented  and it will have the property
that
q i  s    a    q  i  s    a 
    
for any states s  and s  such that i  s      i  s    
now let us impose the action ordering   to compute the optimal ordered policy  consider
two actions a  and a  such that   a    a     i e     prefers a     and suppose that there is a
 tie  in the q function at state s  such that the values

q  i  s    a      q  i  s    a   
and they are the only two actions that maximize q in this state  then the optimal ordered
policy must choose a    now in all other states s  such that i  s      i  s     we have just
established in      that the q values will be the same  hence  the same tie will exist
between a  and a    and hence  the optimal ordered policy must make the same choice in all
such states  hence  the optimal ordered policy for node i is an abstract policy 
now let us turn to the recursive case at max node i  make the inductive assumption that
the ordered recursively optimal policy is abstract within all descendent nodes and consider
the locally optimal policy at node i  if y is a set of state variables that are irrelevant to
node i  corollary   tells us that q  i  s    j     q  i  s    j   for all states s  and s  such that
i s     i  s     similarly  if y is a set of variables irrelevant to the result distribution of
a particular action j   then lemma   tells us the same thing  hence  by the same ordering
argument given above  the ordered optimal policy at node i must be abstract  by induction 
this proves the lemma  q e d 
with this lemma  we have established that the combination of an mdp m   an abstract
maxq graph h   and an action ordering defines a unique recursively optimal ordered abstract policy  we are now ready to prove that maxq q will converge to this policy 

theorem   let m   hs  a  p  r  p  i be either an episodic mdp for which all deterministic

policies are proper or a discounted infinite horizon mdp with discount factor       let h
be an unabstracted maxq graph defined over subtasks fm            mk g with pseudo reward
functions r  i  s     let  h   be a state abstracted maxq graph defined by applying state
abstractions i to each node i of h under the five conditions given above  let x  i  i  s  
be an abstract ordered glie exploration policy at each node i and state s whose decisions
depend only on the  relevant  state variables at each node i  let r be the unique recursivelyoptimal hierarchical policy defined by x   m   and r    then with probability    algorithm
maxq q applied to  h   converges to r provided that the learning rates fft  i  satisfy
equation      and the one step rewards are bounded 

proof  rather than repeating the entire proof for maxq q  we will only describe what

must change under state abstraction  the last two forms of state abstraction refer to states
whose values can be inferred from the structure of the maxq graph  and therefore do not
need to be represented at all  since these values are not updated by maxq q  we can
ignore them  we will now consider the first three forms of state abstraction in turn 
we begin by considering primitive leaf nodes  let a be a leaf node and let y be a set of
state variables that are leaf irrelevant for a  let s     x  y    and s     x  y    be two states
   

fimaxq hierarchical reinforcement learning

that differ only in their values for y   under leaf irrelevance  the probability transitions
p  s   js    a  and p  s   js    a  need not be the same  but the expected reward of performing a
in both states must be the same  when maxq q visits an abstract state x  it does not
 know  the value of y  the part of the state that has been abstracted away  nonetheless 
it draws a sample according to p  s  jx  y  a   receives a reward r s  jx  y  a   and updates
its estimate of v  a  x   line   of maxq q   let pt  y  be the probability that maxq q is
visiting  x  y  given that the unabstracted part of the state is x  then line   of maxq q
is computing a stochastic approximation to
x

s   n y

we can write this as

x

y

pt  y pt  s    n jx  y  a r s  jx  y  a  

pt  y 

x

s   n

pt  s    n jx  y  a r s  jx  y  a  

according to leaf irrelevance  the inner sum has the same value for all states s such that
 s    x  call this value r   x   this gives
x

y

pt  y r   x  

which is equal to r   x  for any distribution pt  y   hence  maxq q converges under leaf
irrelevance abstractions 
now let us turn to the two forms of abstraction that apply to internal nodes  max node
irrelevance and result distribution irrelevance  consider the smdp defined at each node i
of the abstracted maxq graph at time t during maxq q  this would be an ordinary smdp
with transition probability function pt  x    n jx  a  and reward function vt  a  x    r  i  x   
except that when maxq q draws samples of state transitions  they are drawn according to
the distribution pt  s    n js  a  over the original state space  to prove the theorem  we must
show that drawing  s    n   according to this second distribution is equivalent to drawing
 x    n   according to the first distribution 
for max node irrelevance  we know that for all abstract policies applied to node i and
its descendents  the transition probability distribution factors as

p  s    n js  a    p  y  jx  y  a p  x    n jx  a  
because the exploration policy is an abstract policy  pt  s    n js  a  factors in this way  this
means that the yi components of the state cannot affect the xi components  and hence 
sampling from pt  s    n js  a  and discarding the yi values gives samples for pt  x    n jx  a  
therefore  maxq q will converge under max node irrelevance abstractions 
finally  consider result distribution irrelevance  let j be a child of node i  and suppose
yj is a set of state variables that are irrelevant to the result distribution of j   when
the smdp at node i wishes to draw a sample from pt  x    n jx  j    it does not  know 
the current value of y  the irrelevant part of the current state  however  this does not
matter  because result distribution irrelevance means that for all possible values of y 
pt  x    y   n jx  y  j   is the same  hence  maxq q will converge under result distribution
irrelevance abstractions 
   

fidietterich

in each of these three cases  maxq q will converge to a locally optimal ordered policy
at node i in the maxq graph  by lemma     this produces a locally optimal ordered
policy for the unabstracted smdp at node i  hence  by induction  maxq q will converge
to the unique ordered recursively optimal policy r defined by maxq q h   mdp m   and
ordered exploration policy x   q e d 

    the hierarchical credit assignment problem

there are still some situations where we would like to introduce state abstractions but
where the five properties described above do not permit them  consider the following
modification of the taxi problem  suppose that the taxi has a fuel tank and that each time
the taxi moves one square  it costs one unit of fuel  if the taxi runs out of fuel before
delivering the passenger to his or her destination  it receives a reward of      and the trial
ends  fortunately  there is a filling station where the taxi can execute a fillup action to fill
the fuel tank 
to solve this modified problem using the maxq hierarchy  we can introduce another
subtask  refuel  which has the goal of moving the taxi to the filling station and filling the
tank  maxrefuel is a child of maxroot  and it invokes navigate t   with t bound to the
location of the filling station  to move the taxi to the filling station 
the introduction of fuel and the possibility that we might run out of fuel means that
we must include the current amount of fuel as a feature in representing every c value
 for internal nodes  and v value  for leaf nodes  throughout the maxq graph  this is
unfortunate  because our intuition tells us that the amount of fuel should have no inuence
on our decisions inside the navigate t  subtask  that is  either the taxi will have enough
fuel to reach the target t  in which case  the chosen navigation actions do not depend on the
fuel   or else the taxi will not have enough fuel  and hence  it will fail to reach t regardless
of what navigation actions are taken  in other words  the navigate t  subtask should not
need to worry about the amount of fuel  because even if there is not enough fuel  there is
no action that navigate t  can take to get more fuel  instead  it is the top level subtasks
that should be monitoring the amount of fuel and deciding whether to go refuel  to go pick
up the passenger  or to go deliver the passenger 
given this intuition  it is natural to try abstracting away the  amount of remaining
fuel  within the navigate t  subtask  however  this doesn t work  because when the taxi
runs out of fuel and a     reward is given  the qnorth  qsouth  qeast  and qwest nodes
cannot  explain  why this reward was received that is  they have no consistent way of
setting their c tables to predict when this negative reward will occur  because their c
values ignore the amount of fuel in the tank  stated more formally  the diculty is that
the max node irrelevance condition is not satisfied because the one step reward function
r s js  a  for these actions depends on the amount of fuel 
we call this the hierarchical credit assignment problem  the fundamental issue here is
that in the maxq decomposition all information about rewards is stored in the leaf nodes
of the hierarchy  we would like to separate out the basic rewards received for navigation
 i e      for each action  from the reward received for exhausting fuel        if we make the
reward at the leaves only depend on the location of the taxi  then the max node irrelevance
condition will be satisfied 
   

fimaxq hierarchical reinforcement learning

one way to do this is to have the programmer manually decompose the reward function
and
indicate which nodes in the hierarchy will  receive  each reward  let r s  js  a   
p
 
 
i r i  s js  a  be a decomposition of the reward function  such that r i  s js  a  specifies
that part of the reward that must be handled by max node i  in the modified taxi problem 
for example  we can decompose the reward so that the leaf nodes receive all of the original
penalties  but the out of fuel rewards must be handled by maxroot  lines    and    of the
maxq q algorithm are easily modified to include r i  s  js  a  
in most domains  we believe it will be easy for the designer of the hierarchy to decompose
the reward function  it has been straightforward in all of the problems we have studied 
however  an interesting problem for future research is to develop an algorithm that can
solve the hierarchical credit assignment problem autonomously 

   non hierarchical execution of the maxq hierarchy

up to this point in the paper  we have focused exclusively on representing and learning
hierarchical policies  however  often the optimal policy for a mdp is not strictly hierarchical  kaelbling        first introduced the idea of deriving a non hierarchical policy from the
value function of a hierarchical policy  in this section  we exploit the maxq decomposition
to generalize her ideas and apply them recursively at all levels of the hierarchy  we will
describe two methods for non hierarchical execution 
the first method is based on the dynamic programming algorithm known as policy
iteration  the policy iteration algorithm starts with an initial policy     it then repeats
the following two steps until the policy converges  in the policy evaluation step  it computes
the value function v k of the current policy k   then  in the policy improvement step  it
computes a new policy  k   according to the rule

k   s     argmax
a

x

s 

p  s  js  a  r s  js  a    v k  s     

    

howard        proved that if k is not an optimal policy  then k   is guaranteed to be
an improvement  note that in order to apply this method  we need to know the transition
probability distribution p  s  js  a  and the reward function r s  js  a  
if we know p  s  js  a  and r s  js  a   we can use the maxq representation of the value
function to perform one step of policy iteration  we start with a hierarchical policy  and
represent its value function using the maxq hierarchy  e g    could have been learned via
maxq q   then  we can perform one step of policy improvement by applying equation     
using v      s     computed by the maxq hierarchy  to compute v   s    

corollary   let g  s    argmaxa ps  p  s js  a  r s  js  a    v      s    where v      s  is
the value function computed by the maxq hierarchy and a is a primitive action  then  if
 was not an optimal policy  g is strictly better for at least one state in s  

proof  this is a direct consequence of howard s policy improvement theorem  q e d 
unfortunately  we can t iterate this policy improvement process  because the new policy 

g is very unlikely to be a hierarchical policy  i e   it is unlikely to be representable in
   

fidietterich

table    the procedure for executing the one step greedy policy 
procedure executehgpolicy s 
 
repeat
 
let hv     s   ai    evaluatemaxnode    s 
 
 

execute primitive action a
let s be the resulting state
end    executehgpolicy

terms of local policies for each node of the maxq graph   nonetheless  one step of policy
improvement can give very significant improvements 
this approach to non hierarchical execution ignores the internal structure of the maxq
graph  in effect  the maxq hierarchy is just viewed as a way to represent v   any other
representation would give the same one step improved policy g  
the second approach to non hierarchical execution borrows an idea from q learning 
one of the great beauties of the q representation for value functions is that we can compute
one step of policy improvement without knowing p  s  js  a   simply by taking the new policy
to be g  s     argmaxa q s  a   this gives us the same one step greedy policy as we
computed above using one step lookahead  with the maxq decomposition  we can perform
these policy improvement steps at all levels of the hierarchy 
we have already defined the function that we need  in table   we presented the function
evaluatemaxnode  which  given the current state s  conducts a search along all paths
from a given max node i to the leaves of the maxq graph and finds the path with the
best value  i e   with the maximum sum of c values along the path  plus the v value at
the leaf   this is equivalent to computing the best action greedily at each level of the
maxq graph  in addition  evaluatemaxnode returns the primitive action a at the end
of this best path  this action a would be the first primitive action to be executed if the
learned hierarchical policy were executed starting in the current state s  our second method
for non hierarchical execution of the maxq graph is to call evaluatemaxnode in each
state  and execute the primitive action a that is returned  the pseudo code is shown in
table   
we will call the policy computed by executehgpolicy the hierarchical greedy policy 
and denote it hg   where the superscript   indicates that we are computing the greedy
action at each time step  the following theorem shows that this can give a better policy
than the original  hierarchical policy 

theorem   let g be a maxq graph representing the value function of hierarchical policy

  i e   in terms of c   i  s  j    computed for all i  s  and j    let v hg     s  be the value
computed by executehgpolicy  line     and let hg be the resulting policy  define
v hg to be the value function of hg  then for all states s  it is the case that
v   s   v hg     s   v hg  s  
    

proof   sketch  the left inequality in equation      is satisfied by construction by line  
of evaluatemaxnode  to see this  consider that the original hierarchical policy    can
   

fimaxq hierarchical reinforcement learning

be viewed as choosing a  path  through the maxq graph running from the root to one of
the leaf nodes  and v      s  is the sum of the c  values along this chosen path  plus the
v  value at the leaf node   in contrast  evaluatemaxnode performs a traversal of all
paths through the maxq graph and finds the best path  that is  the path with the largest
sum of c   and leaf v    values  hence  v hg     s  must be at least as large as v      s  
to establish the right inequality  note that by construction v hg     s  is the value function
of a policy  call it hg   that chooses one action greedily at each level of the maxq graph
 recursively   and then follows  thereafter  this is a consequence of the fact that line
  of evaluatemaxnode has c  on its right hand side  and c  represents the cost of
 completing  each subroutine by following   not by following some other  greedier  policy 
 in table    c  is written as ct    however  when we execute executehgpolicy  and
hence  execute hg    we have an opportunity to improve upon  and hg at each time step 
hence  v hg     s  is an underestimate of the actual value of hg   q e d 
note that this theorem only works in one direction  it says that if we can find a state
where v hg     s    v   s   then the greedy policy  hg   will be strictly better than  
however  it could be that  is not an optimal policy and yet the structure of the maxq
graph prevents us from considering an action  either primitive or composite  that would
improve   hence  unlike the policy improvement theorem of howard  where all primitive
actions are always eligible to be chosen   we do not have a guarantee that if  is suboptimal 
then the hierarchically greedy policy is a strict improvement 
in contrast  if we perform one step policy improvement as discussed at the start of this
section  corollary   guarantees that we will improve the policy  so we can see that in
general  neither of these two methods for non hierarchical execution is always better than
the other  nonetheless  the first method only operates at the level of individual primitive
actions  so it is not able to produce very large improvements in the policy  in contrast  the
hierarchical greedy method can obtain very large improvements in the policy by changing
which actions  i e   subroutines  are chosen near the root of the hierarchy  hence  in general 
hierarchical greedy execution is probably the better method   of course  the value functions
of both methods could be computed  and the one with the better estimated value could be
executed  
sutton  et al         have simultaneously developed a closely related method for nonhierarchical execution of macros  their method is equivalent to executehgpolicy for
the special case where the maxq hierarchy has only one level of subtasks  the interesting
aspect of executehgpolicy is that it permits greedy improvements at all levels of the
tree to inuence which action is chosen 
some care must be taken in applying theorem   to a maxq hierarchy whose c values
have been learned via maxq q  being an online algorithm  maxq q will not have correctly learned the values of all states at all nodes of the maxq graph  for example  in the
taxi problem  the value of c  put  s  qputdown  will not have been learned very well except
at the four special locations r  g  b  and y  this is because the put subtask cannot be
executed until the passenger is in the taxi  and this usually means that a get has just been
completed  so the taxi is at the passenger s source location  during exploration  both children of put will be tried in such states  the putdown will usually fail  and receive a negative
reward   whereas the navigate will eventually succeed  perhaps after lengthy exploration 
   

fidietterich

and take the taxi to the destination location  now because of all states updating  the values
for c  put  s  navigate t   will have been learned at all of the states along the path to the
passenger s destination  but the c values for the putdown action will only be learned for
the passenger s source and destination locations  hence  if we train the maxq representation using hierarchical execution  as in maxq q   and then switch to hierarchically greedy
execution  the results will be quite bad  in particular  we need to introduce hierarchicallygreedy execution early enough so that the exploration policy is still actively exploring   in
theory  a glie exploration policy never ceases to explore  but in practice  we want to find
a good policy quickly  not just asymptotically  
of course an alternative would be to use hierarchically greedy execution from the very
beginning of learning  however  remember that the higher nodes in the maxq hierarchy
need to obtain samples of p  s    n js  a  for each child action a  if the hierarchical greedy
execution interrupts child a before it has reached a terminal state  i e   because at some state
along the way  another subtask appears better to evaluatemaxnode   then these samples
cannot be obtained  hence  it is important to begin with purely hierarchical execution
during training  and make a transition to greedy execution at some point 
the approach we have taken is to implement maxq q in such a way that we can
specify a number of primitive actions l that can be taken hierarchically before the hierarchical execution is  interrupted  and control returns to the top level  where a new action
can be chosen greedily   we start with l set very large  so that execution is completely
hierarchical when a child action is invoked  we are committed to execute that action until
it terminates  however  gradually  we reduce l until it becomes    at which point we have
hierarchical greedy execution  we time this so that it reaches   at about the same time our
boltzmann exploration cools to a temperature of      which is where exploration effectively
has halted   as the experimental results will show  this generally gives excellent results
with very little added exploration cost 

   experimental evaluation of the maxq method
we have performed a series of experiments with the maxq method with three goals in
mind   a  to understand the expressive power of the value function decomposition   b  to
characterize the behavior of the maxq q learning algorithm  and  c  to assess the relative
importance of temporal abstraction  state abstraction  and non hierarchical execution  in
this section  we describe these experiments and present the results 

    the fickle taxi task
our first experiments were performed on a modified version of the taxi task  this version
incorporates two changes to the task described in section      first  each of the four
navigation actions is noisy  so that with probability     it moves in the intended direction 
but with probability     it instead moves to the right  of the intended direction  and with
probability     it moves to the left  the purpose of this change is to create a more realistic
and more dicult challenge for the learning algorithms  the second change is that after the
taxi has picked up the passenger and moved one square away from the passenger s source
location  the passenger changes his or her destination location with probability      the
   

fimaxq hierarchical reinforcement learning

purpose of this change is to create a situation where the optimal policy is not a hierarchical
policy so that the effectiveness of non hierarchical execution can be measured 
we compared four different configurations of the learning algorithm   a  at q learning 
 b  maxq q learning without any form of state abstraction   c  maxq q learning with
state abstraction  and  d  maxq q learning with state abstraction and greedy execution 
these configurations are controlled by many parameters  these include the following   a 
the initial values of the q and c functions   b  the learning rate  we employed a fixed
learning rate    c  the cooling schedule for boltzmann exploration  the glie policy that we
employed   and  d  for non hierarchical execution  the schedule for decreasing l  the number
of steps of consecutive hierarchical execution  we optimized these settings separately for
each configuration with the goal of matching or exceeding  with as few primitive training
actions as possible  the best policy that we could code by hand  for boltzmann exploration 
we established an initial temperature and then a cooling rate  a separate temperature is
maintained for each max node in the maxq graph  and its temperature is reduced by
multiplying by the cooling rate each time that subtask terminates in a goal state 
the process of optimizing the parameter settings for each algorithm is time consuming 
both for at q learning and for maxq q  the most critical parameter is the schedule
for cooling the temperature of boltzmann exploration  if this is cooled too rapidly  then
the algorithms will converge to a suboptimal policy  in each case  we tested nine different
cooling rates  to choose the different cooling rates for the various subtasks  we started by
using fixed policies  e g   either random or hand coded  for all subtasks except the subtasks
closest to the leaves  then  once we had chosen schedules for those subtasks  we allowed
their parent tasks to learn their policies while we tuned their cooling rates  and so on  one
nice effect of our method of cooling the temperature only when a subtask terminates is that
it naturally causes the subtasks higher in the maxq graph to cool more slowly  this meant
that good results could often be obtained just by using the same cooling rate for all max
nodes 
the choice of learning rate is easier  since it is determined primarily by the degree
of stochasticity in the environment  we only tested three or four different rates for each
configuration  the initial values for the q and c functions were set based on our knowledge
of the problems no experiments were required 
we took more care in tuning these parameters for these experiments than one would
normally take in a real application  because we wanted to ensure that each method was
compared under the best possible conditions  the general form of the results  particularly
the speed of learning  is the same for wide ranges of the cooling rate and learning rate
parameter settings 
the following parameters were selected based on the tuning experiments  for at q
learning  initial q values of       in all states  learning rate       and boltzmann exploration
with an initial temperature of    and a cooling rate of          we use initial values that
end in      as a  signature  during debugging to detect when a weight has been modified  
for maxq q learning without state abstraction  we used initial values of        a learning rate of       and boltzmann exploration with an initial temperature of    and cooling
rates of        at maxroot and maxput         at maxget  and        at maxnavigate 
   

fidietterich

   
maxq abstract

mean cumulative reward

 
maxq
abstract 
greedy

    

maxq
no abstract

    

flat q

    

    

     
 

     

     

     
     
      
primitive actions

      

      

figure    comparison of performance of hierarchical maxq q learning  without state abstractions  with state abstractions  and with state abstractions combined with
hierarchical greedy evaluation  to at q learning 
for maxq q learning with state abstraction  we used initial values of        a learning
rate of       and boltzmann exploration with an initial temperature of    and cooling rates
of        at maxroot         at maxput         at maxget  and        at maxnavigate 
for maxq q learning with non hierarchical execution  we used the same settings as
with state abstraction  in addition  we initialized l to     and decreased it by    with each
trial until it reached    so after    trials  execution was completely greedy 
figure   shows the averaged results of     training runs  each training run involves
performing repeated trials until convergence  because the different trials execute different
numbers of primitive actions  we have just plotted the number of primitive actions on the
horizontal axis rather than the number of trials 
the first thing to note is that all forms of maxq learning have better initial performance
than at q learning  this is because of the constraints introduced by the maxq hierarchy 
for example  while the agent is executing a navigate subtask  it will never attempt to pickup
or putdown the passenger  because those actions are not available to navigate  similarly  the
agent will never attempt to putdown the passenger until it has first picked up the passenger
 and vice versa  because of the termination conditions of the get and put subtasks 
the second thing to notice is that without state abstractions  maxq q learning actually takes longer to converge  so that the flat q curve crosses the maxq no abstraction
   

fimaxq hierarchical reinforcement learning

curve  this shows that without state abstraction  the cost of learning the huge number
of parameters in the maxq representation is not really worth the benefits  we suspect
this is a consequence of the model free nature of the maxq q algorithm  the maxq decomposition represents some information redundantly  for example  the cost of performing
a put subtask is computed both as c  root  s  get  and also as v  put  s   a model based
algorithm could compute both of these from a learned model  but maxq q must learn
each of them separately from experience 
the third thing to notice is that with state abstractions  maxq q converges very
quickly to a hierarchically optimal policy  this can be seen more clearly in figure    which
focuses on the range of reward values in the neighborhood of the optimal policy  here
we can see that maxq with abstractions attains the hierarchically optimal policy after
approximately        steps  whereas at q learning requires roughly twice as long to reach
the same level  however  at q learning  of course  can continue onward and reach optimal
performance  whereas with the maxq hierarchy  the best hierarchical policy is slow to
respond to the  fickle  behavior of the passenger when he she changes the destination 
the last thing to notice is that with greedy execution  the maxq policy is also able
to attain optimal performance  but as the execution becomes  more greedy   there is a
temporary drop in performance  because maxq q must learn c values in new regions
of the state space that were not visited by the recursively optimal policy  despite this
drop in performance  greedy maxq q recovers rapidly and reaches hierarchically optimal
performance faster than purely hierarchical maxq q learning  hence  there is no added
cost in terms of exploration for introducing greedy execution 
this experiment presents evidence in favor of three claims  first  that hierarchical reinforcement learning can be much faster than at q learning  second  that state abstraction
is required by maxq q learning for good performance  and third  that non hierarchical
execution can produce significant improvements in performance with little or no added
exploration cost 

    kaelbling s hdg method
the second task that we will consider is a simple maze task introduced by leslie kaelbling
       and shown in figure     in each trial of this task  the agent starts in a randomlychosen state and must move to a randomly chosen goal state using the usual north  south 
east  and west operators  we employed deterministic operators   there is a small cost for
each move  and the agent must minimize the undiscounted sum of these costs 
because the goal state can be in any of     different locations  there are actually    
different mdps  kaelbling s hdg method starts by choosing an arbitrary set of landmark
states and defining a voronoi partition of the state space based on the manhattan distances
to these landmarks  i e   two states belong to the same voronoi cell iff they have the same
nearest landmark   the method then defines one subtask for each landmark l  the subtask
is to move from any state in the current voronoi cell or in any neighboring voronoi cell to
the landmark l  optimal policies for these subtasks are then computed 
once hdg has the policies for these subtasks  it can solve the abstract markov decision
problem of moving from each landmark state to any other landmark state using the subtask
solutions as macro actions  subroutines   so it computes a value function for this mdp 
   

fidietterich

  

maxq abstract greedy

mean cumulative reward

 

optimal policy
flat q
hier optimal policy

 
maxq abstract

maxq no abstract

  

   

   
 

     

      

      
      
primitive actions

      

      

figure    close up view of the previous figure  this figure also shows two horizontal lines
indicating optimal performance and hierarchically optimal performance in this
domain  to make this figure more readable  we have applied a     step moving
average to the data points  which are themselves the average of     runs  
finally  for each possible destination location g within a voronoi cell for landmark l  the
hdg method computes the optimal policy of getting from l to g 
by combining these subtasks  the hdg method can construct a good approximation
to the optimal policy as follows  in addition to the value functions discussed above  the
agent maintains two other functions  nl s   the name of the landmark nearest to state s 
and n  l   a list of the landmarks of the cells that are immediate neighbors of cell l  by
combining these  the agent can build a list for each state s of the current landmark and the
landmarks of the neighboring cells  for each such landmark  the agent computes the sum
of three terms 
 t   the expected cost of reaching that landmark 
 t   the expected cost of moving from that landmark to the landmark in the goal cell  and
 t   the expected cost of moving from the goal cell landmark to the goal state 
note that while terms  t   and  t   can be exact estimates  term  t   is computed using
the landmark subtasks as subroutines  this means that the corresponding path must pass
through the intermediate landmark states rather than going directly to the goal landmark 
   

fimaxq hierarchical reinforcement learning

  
 
 
 
 
 
 
 
 
 
 

 

 

 

 

 

 

 

 

  

figure     kaelbling s    by    navigation task  each circled state is a landmark state 
and the heavy lines show the boundaries of the voronoi cells  in each episode  a
start state and a goal state are chosen at random  in this figure  the start state
is shown by the black square  and the goal state is shown by the black hexagon 

hence  term  t   is typically an overestimate of the required distance   also note that  t  
is the same for all choices of the intermediate landmarks  so it does not need to be explicitly
included in the computation of the best action until the agent enters the cell containing the
goal  
given this information  the agent then chooses to move toward the best of the landmarks
 unless the agent is already in the goal voronoi cell  in which case the agent moves toward
the goal state   for example  in figure     term  t   is the cost of reaching the landmark
in row    column    which is    term  t   is the cost of getting from row    column   to
the landmark at row   column    by going from one landmark to another   in this case 
the best landmark to landmark path is to go directly from row   column   to row   column
   hence  term  t   is    term  t   is the cost of getting from row   column   to the goal 
which is    the sum of these is                 for comparison  the optimal path has
length   
in kaelbling s experiments  she employed a variation of q learning to learn terms  t  
and  t    and she computed  t   at regular intervals via the floyd warshall all sources
shortest paths algorithm 
figure    shows a maxq approach to solving this problem  the overall task root 
takes one argument g  which specifies the goal cell  there are three subtasks 
   

fidietterich

maxroot g 
gl nl g 

qgotogoallmk gl 

qgotogoal g 

maxgotogoallmk gl 

qgotolmk l gl 

maxgotolmk l 

qnorthlmk l 

qsouthlmk l 

maxgotogoal g 

qeastlmk l 

north

qwestlmk l 

south

qnorthg g 

east

qsouthg g 

qeastg g 

qwestg g 

west

figure     a maxq graph for the hdg navigation task 

 gotogoallmk  go to the landmark nearest to the goal location  the termination
predicate for this subtask is true if the agent reaches the landmark nearest to the
goal  the goal predicate is the same as the termination predicate 

 gotolmk l   go to landmark l  the termination predicate for this is true if either  a 

the agent reaches landmark l or  b  the agent is outside of the region defined by the
voronoi cell for l and the neighboring voronoi cells  n  l   the goal predicate for this
subtask is true only for condition  a  

 gotogoal g   go to the goal location g  the termination predicate for this subtask is

true if either the agent is in the goal location or the agent is outside of the voronoi
cell nl g  that contains g  the goal predicate for this subtask is true if the agent is
in the goal location 
   

fimaxq hierarchical reinforcement learning

the maxq decomposition is essentially the same as kaelbling s method  but somewhat
redundant  consider a state where the agent is not inside the same voronoi cell as the goal
g  in such states  hdg decomposes the value function into three terms  t     t    and  t   
similarly  maxq also decomposes it into these same three terms 
 v  gotolmk l   s  a  the cost of getting to landmark l  this is represented as the sum
of v  a  s  and c  gotolmk l   s  a  
 c  gotogoallmk gl   s  maxgotolmk l   the cost of getting from landmark l to the
landmark gl nearest the goal 
 c  root  s  gotogoallmk gl   the cost of getting to the goal location after reaching gl
 i e   the cost of completing the root task after reaching gl  
when the agent is inside the goal voronoi cell  then again hdg and maxq store
essentially the same information  hdg stores q gotogoal g   s  a   while maxq breaks
this into two terms  c  gotogoal g   s  a  and v  a  s  and then sums these two quantities to
compute the q value 
note that this maxq decomposition stores some information twice specifically  the
cost of getting from the goal landmark gl to the goal is stored both as c  root  s  gotogoallmk gl  
and as c  gotogoal g   s  a    v  a  s  
let us compare the amount of memory required by at q learning  hdg  and maxq 
there are     locations    possible actions  and     possible goal states  so at q learning
must store        values 
to compute quantity  t    hdg must store   q values  for the four actions  for each
state s with respect to its own landmark and the landmarks in n  nl s    this gives a
total of       values that must be stored 
to compute quantity  t    hdg must store  for each landmark  information on the
shortest path to every other landmark  there are    landmarks  consider the landmark at
row    column    it has   neighboring landmarks which constitute the five macro actions
that the agent can perform to move to another landmark  the nearest landmark to the
goal cell could be any of the other    landmarks  so this gives a total of    q values that
must be stored  similar computations for all    landmarks give a total of     values that
must be stored 
finally  to compute quantity  t    hdg must store information  for each square inside
each voronoi cell  about how to get to each of the other squares inside the same voronoi
cell  this requires       values 
hence  the grand total for hdg is        which is a huge savings over at q learning 
now let s consider the maxq hierarchy with and without state abstractions 
 v  a  s   this is the expected reward of each primitive action in each state  there are
    states and   primitive actions  so this requires     values  however  because the
reward is constant       we can apply leaf irrelevance to store only a single value 
 c  gotolmk l   s  a   where a is one of the four primitive actions  this requires the
same amount of space as  t   in kaelbling s representation indeed  combined with
v  a  s   this represents exactly the same information as  t    it requires       values 
no state abstractions can be applied 
   

fidietterich

 c  gotogoallmk gl   s  gotolmk l    this is the cost of completing the gotogoallmk

task after going to landmark l  if the primitive actions are deterministic  then
gotolmk l  will always terminate at location l  and hence  we only need to store
this for each pair of l and gl  this is exactly the same as kaelbling s quantity  t   
which requires     values  however  if the primitive actions are stochastic as they
were in kaelbling s original paper then we must store this value for each possible
terminal state of each gotolmk action  each of these actions could terminate at its
target landmark l or in one of the states bordering the set of voronoi cells that are
the neighbors of the cell for l  this requires       values  when kaelbling stores
values only for  t    she is effectively making the assumption that gotolmk l  will
never fail to reach landmark l  this is an approximation which we can introduce into
the maxq representation by our choice of state abstraction at this node 

 c  gotogoal  s  a   this is the cost of completing the gotogoal task after executing one
of the primitive actions a  this is the same as quantity  t   in the hdg representation 
and it requires the same amount of space        values 

 c  root  s  gotogoallmk   this is the cost of reaching the goal once we have reached

the landmark nearest the goal  maxq must represent this for all combinations of
goal landmarks and goals  this requires     values  note that these values are the
same as the values of c  gotogoal g   s  a    v  a  s  for each of the primitive actions 
this means that the maxq representation stores this information twice  whereas the
hdg representation only stores it once  as term  t    

 c  root  s  gotogoal   this is the cost of completing the root task after we have exe 

cuted the gotogoal task  if the primitive action are deterministic  this is always zero 
because gotogoal will have reached the goal  hence  we can apply the termination
condition and not store any values at all  however  if the primitive actions are stochastic  then we must store this value for each possible state that borders the voronoi cell
that contains the goal  this requires    different values  again  in kaelbling s hdg
representation of the value function  she is ignoring the probability that gotogoal will
terminate in a non goal state  because maxq is an exact representation of the value
function  it does not ignore this possibility  if we  incorrectly  apply the termination
condition in this case  the maxq representation becomes a function approximation 

in the stochastic case  without state abstractions  the maxq representation requires
       values  with safe state abstractions  it requires        values  with the approximations employed by kaelbling  or equivalently  if the primitive actions are deterministic  
the maxq representation with state abstractions requires       values  these numbers are
summarized in table    we can see that  with the unsafe state abstractions  the maxq
representation requires only slightly more space than the hdg representation  because of
the redundancy in storing c  root  s  gotogoallmk  
this example shows that for the hdg task  we can start with the fully general formulation provided by maxq and impose assumptions to obtain a method that is similar
to hdg  the maxq formulation guarantees that the value function of the hierarchical
policy will be represented exactly  the assumptions will introduce approximations into the
   

fimaxq hierarchical reinforcement learning

table    comparison of the number of values that must be stored to represent the value
function using the hdg and maxq methods 
hdg maxq
hdg maxq maxq maxq
item item
values no abs safe abs unsafe abs
v  a  s 
 
   
 
 
 t   c  gotolmk l   s  a 
           
     
     
 t   c  gotogoallmk  s  gotolmk l  
         
     
   
 t   c  gotogoal g   s  a 
           
     
     
c  root  s  gotogoallmk 
 
   
   
   
c  root  s  gotogoal 
 
  
  
 
total number of values required
                   
     
value function representation  this might be useful as a general design methodology for
building application specific hierarchical representations  our long term goal is to develop
such methods so that each new application does not require inventing a new set of techniques  instead  off the shelf tools  e g   based on maxq  could be specialized by imposing
assumptions and state abstractions to produce more ecient special purpose systems 
one of the most important contributions of the hdg method was that it introduced
a form of non hierarchical execution  as soon as the agent crosses from one voronoi cell
into another  the current subtask of reaching the landmark in that cell is  interrupted  
and the agent recomputes the  current target landmark   the effect of this is that  until
it reaches the goal voronoi cell   the agent is always aiming for a landmark outside of its
current voronoi cell  hence  although the agent  aims for  a sequence of landmark states  it
typically does not visit many of these states on its way to the goal  the states just provide
a convenient set of intermediate targets  by taking these  shortcuts   hdg compensates
for the fact that  in general  it has overestimated the cost of getting to the goal  because its
computed value function is based on a policy where the agent goes from one landmark to
another 
the same effect is obtained by hierarchical greedy execution of the maxq graph  which
was directly inspired by the hdg method   note that by storing the nl  nearest landmark 
function  kaelbing s hdg method can detect very eciently when the current subtask
should be interrupted  this technique only works for navigation problems in a space with
a distance metric  in contrast  executehgpolicy performs a kind of  polling   because
it checks after each primitive action whether it should interrupt the current subroutine and
invoke a new one  an important goal for future research on maxq is to find a general
purpose mechanism for avoiding unnecessary  polling  that is  a mechanism that can
discover eciently evaluable interrupt conditions 
figure    shows the results of our experiments with hdg using the maxq q learning algorithm  we employed the following parameters  for flat q learning  initial values
of        a learning rate of      initial temperature of     and cooling rate of         for
maxq q without state abstractions  initial values of          learning rate of      initial
   

fidietterich

 
flat q

maxq  
abstract

   

mean cumulative reward

   

maxq no abstract

   
   
    
    
    
 

      

      

      
      
primitive actions

 e   

   e   

   e   

figure     comparison of flat q learning with maxq q learning with and without state
abstraction   average of     runs  
temperature of     and cooling rates of        for maxroot         for maxgotogoallmk 
       for maxgotogoal  and        for maxgotolmk  for maxq q with state abstractions 
initial values of          learning rate of      initial temperature of     and cooling rates of
       for maxroot         for maxgotogoal         for maxgotogoallmk  and        for
maxgotolmk  hierarchical greedy execution was introduced by starting with      primitive actions per trial  and reducing this every trial by   actions  so that after      trials 
execution is completely greedy 
the figure confirms the observations made in our experiments with the fickle taxi task 
without state abstractions  maxq q converges much more slowly than at q learning 
with state abstractions  it converges roughly three times as fast  figure    shows a close up
view of figure    that allows us to compare the differences in the final levels of performance
of the methods  here  we can see that maxq q with no state abstractions was not able to
reach the quality of our hand coded hierarchical policy presumably even more exploration
would be required to achieve this  whereas with state abstractions  maxq q is able to do
slightly better than our hand coded policy  with hierarchical greedy execution  maxq q
is able to reach the goal using one fewer action  on the average so that it approaches the
performance of the best hierarchical greedy policy  as computed by value iteration   notice
however  that the best performance that can be obtained by hierarchical greedy execution
of the best recursively optimal policy cannot match optimal performance  hence  flat q
   

fimaxq hierarchical reinforcement learning

  
optimal policy

mean cumulative reward

hierarchical greedy optimal policy
maxq abstract   greedy
maxq   abstract

  

   

flat q

hierarchical hand coded policy

maxq no abstract

   

   

 

      

      

      
      
primitive actions

 e   

   e   

   e   

figure     expanded view comparing flat q learning with maxq q learning with and
without state abstraction and with and without hierarchical greedy execution 
 average of     runs  
learning achieves a policy that reaches the goal state  on the average  with about one fewer
primitive action  finally notice that as in the taxi domain  there was no added exploration
cost for shifting to greedy execution 
kaelbling s hdg work has recently been extended and generalized by moore  baird
and kaelbling        to any sparse mdp where the overall task is to get from any given
start state to any desired goal state  the key to the success of their approach is that each
landmark subtask is guaranteed to terminate in a single resulting state  this makes it
possible to identify a sequence of good intermediate landmark states and then assemble a
policy that visits them in sequence  moore  baird and kaelbling show how to construct a
hierarchy of landmarks  the  airport  hierarchy  that makes this planning process ecient 
note that if each subtask did not terminate in a single state  as in general mdps   then
the airport method would not work  because there would be a combinatorial explosion of
potential intermediate states that would need to be considered 

    parr and russell  hierarchies of abstract machines

in his      b  dissertation work  ron parr considered an approach to hierarchical reinforcement learning in which the programmer encodes prior knowledge in the form of a hierarchy
of finite state controllers called a ham  hierarchy of abstract machines   the hierarchy
   

fidietterich

intersection
vertical hallway
horizontal hallway
goal

figure     parr s maze problem  on left   the start state is in the upper left corner  and
all states in the lower right hand room are terminal states  the smaller diagram
on the right shows the hallway and intersection structure of the maze 
is executed using a procedure call and return discipline  and it provides a partial policy for
the task  the policy is partial because each machine can include non deterministic  choice 
machine states  in which the machine lists several options for action but does not specify
which one should be chosen  the programmer puts  choice  states at any point where
he she does not know what action should be performed  given this partial policy  parr s
goal is to find the best policy for making choices in the choice states  in other words  his
goal is to learn a hierarchical value function v  hs  mi   where s is a state  of the external
environment  and m contains all of the internal state of the hierarchy  i e   the contents
of the procedure call stack and the values of the current machine states for all machines
appearing in the stack   a key observation is that it is only necessary to learn this value
function at choice states hs  mi  parr s algorithm does not learn a decomposition of the value
function  instead  it  attens  the hierarchy to create a new markov decision problem over
the choice states hs  mi  hence  it is hierarchical primarily in the sense that the programmer
structures the prior knowledge hierarchically  an advantage of this is that parr s method
can find the optimal hierarchical policy subject to constraints provided by the programmer 
a disadvantage is that the method cannot be executed  non hierarchically  to produce a
better policy 
parr illustrated his work using the maze shown in figure     this maze has a large scale
structure  as a series of hallways and intersections   and a small scale structure  a series of
obstacles that must be avoided in order to move through the hallways and intersections  
   

fimaxq hierarchical reinforcement learning

in each trial  the agent starts in the top left corner  and it must move to any state in the
bottom right corner room  the agent has the usual four primitive actions  north  south 
east  and west  the actions are stochastic  with probability      they succeed  but with
probability     the action will move to the  left  and with probability     the action will
move to the  right  instead  e g   a north action will move east with probability     and
west with probability       if an action would collide with a wall or an obstacle  it has no
effect 
the maze is structured as a series of  rooms   each containing a    by    block of states
 and various obstacles   some rooms are parts of  hallways   because they are connected
to two other rooms on opposite sides  other rooms are  intersections   where two or more
hallways meet 
to test the representational power of the maxq hierarchy  we want to see how well it
can represent the prior knowledge that parr is able to represent using the ham  we begin
by describing parr s ham for his maze task  and then we will present a maxq hierarchy
that captures much of the same prior knowledge  
parr s top level machine  mroot  consists of a loop with a single choice state that
chooses among four possible child machines  mgo east   mgo south   mgo west   and
mgo north   the loop terminates when the agent reaches a goal state  mroot will only
invoke a particular machine if there is a hallway in the specified direction  hence  in the
start state  it will only consider mgo south  and mgo east  
the mgo d  machine begins executing when the agent is in an intersection  so the first
thing it tries to do is to exit the intersection into a hallway in the specified direction d  then
it attempts to traverse the hallway until it reaches another intersection  it does this by first
invoking an mexitintersection d  machine  when that machine returns  it then invokes an
mexithallway d  machine  when that machine returns  mgo also returns 
the mexitintersection and mexithallway machines are identical except for their termination conditions  both machines consist of a loop with one choice state that chooses among
four possible subroutines  to simplify their description  suppose that mgo east  has chosen mexitintersection east   then the four possible subroutines are msniff  east  north  
msniff  east  south   mback east  north   and mback east  south  
the msniff  d  p  machine always moves in direction d until it encounters a wall  either
part of an obstacle or part of the walls of the maze   then it moves in perpendicular
direction p until it reaches the end of the wall  a wall can  end  in two ways  either the
agent is now trapped in a corner with walls in both directions d and p or else there is no
longer a wall in direction d  in the first case  the msniff machine terminates  in the second
case  it resumes moving in direction d 
the mback d  p  machine moves one step backwards  in the direction opposite from d 
and then moves five steps in direction p  these moves may or may not succeed  because the
actions are stochastic and there may be walls blocking the way  but the actions are carried
out in any case  and then the mback machine returns 
the msniff and mback machines also terminate if they reach the end of a hall or the
end of an intersection 
   the author thanks ron parr for providing the details of the ham for this task 

   

fidietterich

these finite state controllers define a highly constrained partial policy  the mback 
msniff  and mgo machines contain no choice states at all  the only choice points are
in mroot  which must choose the direction in which to move  and in mexitintersection
and mexithall  which must decide when to call msniff  when to call mback  and which
 perpendicular  direction to tell these machines to try when they cannot move forward 

maxroot

go d 
r room
maxgo d r 

qexitinter d r 

qexithall d r 

maxexitinter d r 

maxexithall d r 

qsniffei d p 

qbackei d p 

qsniffeh d p 
x x

qbackeh d p 
x x
y y

y y
maxsniff d p 

maxback d p x y 

qfollowwall d p 

qtowall d 

qbackone d 

qperpthree p 

maxfollowwall d p 

maxtowall d 

maxbackone d 

maxperpthree p 

d p

d d

qmovefw d 

d inv d 

qmovetw d 

qmovebo d 

d p
qmovep  d 

maxmove d 

figure     maxq graph for parr s maze task 
figure    shows a maxq graph that encodes a similar set of constraints on the policy 
the subtasks are defined as follows 
   

fimaxq hierarchical reinforcement learning

 root  this is exactly the same as the mroot machine  it must choose a direction d

and invoke go  it terminates when the agent enters a terminal state  this is also its
goal condition  of course  

 go d  r    go in direction d leaving room r   the parameter r is bound to an identi 

fication number corresponding to the current    by     room  in which the agent is
located  go terminates when the agent enters the room at the end of the hallway in
direction d or when it leaves the desired hallway  e g   in the wrong direction   the
goal condition for go is satisfied only if the agent reaches the desired intersection 

 exitinter d  r   this terminates when the agent has exited room r  the goal condition
is that the agent exit room r in direction d 

 exithall d  r   this terminates when the agent has exited the current hall  into some

intersection   the goal condition is that the agent has entered the desired intersection
in direction d 

 sniff  d  r   this encodes a subtask that is equivalent to the msniff machine  however 

sniff must have two child subtasks  towall and followwall  that were simply internal
states of msniff  this is necessary  because a subtask in the maxq framework cannot

contain any internal state  whereas a finite state controller in the ham representation
can contain as many internal states as necessary  in particular  it can have one state
for when it is moving forward and another state for when it is following a wall sideways 

 towall d   this is equivalent to one part of msniff  it terminates when there is a

wall in  front  of the agent in direction d  the goal condition is the same as the
termination condition 

 followwall d  p   this is equivalent to the other part of msniff  it moves in direction

p until the wall in direction d ends  or until it is stuck in a corner with walls in both
directions d and p   the goal condition is the same as the termination condition 

 back d  p  x  y   this attempts to encode the same information as the mback machine 

but this is a case where the maxq hierarchy cannot capture the same information 

mback simply executes a sequence of   primitive actions  one step back  five steps in
direction p   but to do this  mback must have   internal states  which maxq does
not allow  instead  the back subtask has the subgoal of moving the agent at least

one square backwards and at least   squares in the direction p  in order to determine
whether it has achieved this subgoal  it must remember the x and y position where
it started to execute  so these are bound as parameters to back  back terminates if
it achieves the desired change in position or if it runs into walls that prevent it from
achieving the subgoal  the goal condition is the same as the termination condition 

 backone d  x  y   this moves the agent one step backwards  in the direction opposite

to d  it needs the starting x and y position in order to tell when it has succeeded  it
terminates if it has moved at least one unit in direction d or if there is a wall in this
direction  its goal condition is the same as its termination condition 
   

fidietterich

 perpthree p  x  y   this moves the agent three steps in the direction p  it needs the

starting x and y positions in order to tell when it has succeeded  it terminates when it
has moved at least three units in the direction p or if there is a wall in that direction 
the goal condition is the same as the termination condition 

 move d   this is a  parameterized primitive  action  it executes one primitive move
in direction d and terminates immediately 

from this  we can see that there are three major differences between the maxq representation and the ham representation  first  a ham finite state controller can contain
internal states  to convert them into a maxq subtask graph  we must make a separate
subtask for each internal state in the ham  second  a ham can terminate based on an
 amount of effort   e g   performing   actions   whereas a maxq subtask must terminate
based on some change in the state of the world  it is impossible to define a maxq subtask that performs k steps and then terminate regardless of the effects of those steps  i e  
without adding some kind of  counter  to the state of the mdp   third  it is more dicult
to formulate the termination conditions for maxq subtasks than for ham machines  for
example  in the ham  it was not necessary to specify that the mexithallway machine terminates when it has entered a different intersection than the one where the mgo was executed 
however  this is important for the maxq method  because in maxq  each subtask learns
its own value function and policy independent of its parent tasks  for example  without
the requirement to enter a different intersection  the learning algorithms for maxq will
always prefer to have maxexithall take one step backward and return to the room in which
the go action was started  because that is a much easier terminal state to reach   this
problem does not arise in the ham approach  because the policy learned for a subtask
depends on the whole  attened  hierarchy of machines  and returning to the state where
the go action was started does not help solve the overall problem of reaching the goal state
in the lower right corner 
to construct the maxq graph for this problem  we have introduced three programming
tricks   a  binding parameters to aspects of the current state  in order to serve as a kind
of  local memory  for where the subtask began executing    b  having a parameterized
primitive action  in order to be able to pass a parameter value that specifies which primitive
action to perform   and  c  employing  inheritance of termination conditions  that is  each
subtask in this maxq graph  but not the others in this paper  inherits the termination
conditions of all its ancestor tasks  hence  if the agent is in the middle of executing a towall
action when it leaves an intersection  the towall subroutine terminates because the exitinter
termination condition is satisfied  this behavior is very similar to the standard behavior of
maxq  ordinarily  when an ancestor task terminates  all of its descendent tasks are forced
to return without updating their c values  with inheritance of termination conditions  on
the other hand  the descendent tasks are forced to terminate  but after updating their c
values  in other words  the termination condition of each child task is the logical disjuntion
of all of the termination conditions of its ancestors  plus its own termination condition  
this inheritance made it easier to write the maxq graph  because the parents did not need
to pass down to their children all of the information necessary for the children to define the
complete termination and goal predicates 
   

fimaxq hierarchical reinforcement learning

there are essentially no opportunities for state abstraction in this task  because there
are no irrelevant features of the state  there are some opportunities to apply the shielding
and termination properties  however  in particular  exithall d  is guaranteed to cause its
parent task  maxgo d   to terminate  so it does not require any stored c values  there are
many states where some subtasks are terminated  e g   go east  in any state where there
is a wall on the east side of the room   and so no c values need to be stored 
nonetheless  even after applying the state elimination conditions  the maxq representation for this task requires much more space than a at representation  an exact
computation is dicult  but after applying maxq q learning  the maxq representation
required        values  whereas at q learning requires fewer than        values  parr
states that his method requires only       values 
to test the relative effectiveness of the maxq representation  we compare maxq q
learning with at q learning  because of the very large negative values that some states
acquire  particularly during the early phases of learning   we were unable to get boltzmann
exploration to work well one very bad experience would cause an action to receive such
a low q value  that it would never be tried again  hence  we experimented with both
 greedy exploration and counter based exploration  the  greedy exploration policy is an
ordered  abstract glie policy in which a random action is chosen with probability   and 
is gradually decreased over time  the counter based exploration policy keeps track of how
many times each action a has been executed in each state s  to choose an action in state
s  it selects the action that has been executed the fewest times until all actions have been
executed t times  then it switches to greedy execution  hence  it is not a genuine glie
policy  parr employed counter based exploration policies in his experiments with this task 
as in the other domains  we conducted several experimental runs  e g   testing boltzmann   greedy  and counter based exploration  to determine the best parameters for each
algorithm  for flat q learning  we chose the following parameters  learning rate       greedy exploration with initial value for  of       decreased by       after each successful
execution of a max node  and initial q values of           for maxq q learning  we chose
the following parameters  counter based exploration with t       learning rate equal to the
reciprocal of the number of times an action had been performed  and initial values for the c
values selected carefully to provide underestimates of the true c values  for example  the
initial values for qexitinter were          because in the worst case  after completing an
exitinter task  it takes about    steps to complete the subsequent exithall task and hence 
complete the go parent task  performance was quite sensitive to these initial c values 
which is a potential drawback of the maxq approach 
figure    plots the results  we can see that maxq q learning converges about   
times faster than flat q learning  we do not know whether maxq q has converged to a
recursively optimal policy  for comparison  we also show the performance of a hierarchical
policy that we coded by hand  but in our hand coded policy  we used knowledge of contextual
information to choose operators  so this policy is surely better than the best recursively
optimal policy  hamq learning should converge to a policy equal to or slightly better than
our hand coded policy 
this experiment demonstrates that the maxq representation can capture most but
not all of the prior knowledge that can be represented by the hamq hierarchy  it also
   

fidietterich

    
    

mean reward per trial

    

hand coded hierarchical policy

    
    
    

maxq q learning

flat q learning

    
    
    
 

 e   

 e   

 e   
primitive steps

 e   

 e   

 e   

figure     comparison of flat q learning and maxq q learning in the parr maze task 
shows that the maxq representation requires much more care in the design of the goal
conditions for the subtasks 

    other domains
in addition to the three domains discussed above  we have developed maxq graphs for
singh s         ag task   the treasure hunter task described by tadepalli and dietterich
        and dayan and hinton s        feudal q learning task  all of these tasks can be
easily and naturally placed into the maxq framework indeed  all of them fit more easily
than the parr and russell maze task 
maxq is able to exactly duplicate singh s work and his decomposition of the value
function while using exactly the same amount of space to represent the value function 
maxq can also duplicate the results from tadepalli and dietterich however  because
maxq is not an explanation based method  it is considerably slower and requires substantially more space to represent the value function 
in the feudal q task  maxq is able to give better performance than feudal q learning 
the reason is that in feudal q learning  each subroutine makes decisions using only a q
function learned at its own level of the hierarchy that is  without information about the
estimated costs of the actions of its descendents  in contrast  the maxq value function
decomposition permits each max node to make decisions based on the sum of its completion
function  c  i  s  j    and the costs estimated by its descendents  v  j  s   of course  maxq
   

fimaxq hierarchical reinforcement learning

also supports non hierarchical execution  which is not possible for feudal q  because it does
not learn a value function decomposition 

   discussion

before concluding this paper  we wish to discuss two issues   a  design tradeoffs in hierarchical reinforcement learning and  b  methods for automatically learning  or at least
improving  maxq hierarchies 

    design tradeoffs in hierarchical reinforcement learning

in the introduction to this paper  we discussed four issues concerning the design of hierarchical reinforcement learning architectures   a  the method for defining subtasks   b  the
use of state abstraction   c  non hierarchical execution  and  d  the design of learning algorithms  in this subsection  we want to highlight a tradeoff between the first two of these
issues 
maxq defines subtasks using a termination predicate ti and a pseudo reward function
r    there are at least two drawbacks of this method  first  it can be hard for the programmer to define ti and r  correctly  since this essentially requires guessing the value function
of the optimal policy for the mdp at all states where the subtask terminates  second  it
leads us to seek a recursively optimal policy rather than a hierarchically optimal policy 
recursively optimal policies may be much worse than hierarchically optimal ones  so we
may be giving up substantial performance 
however  in return for these two drawbacks  maxq obtains a very important benefit 
the policies and value functions for subtasks become context free  in other words  they
do not depend on their parent tasks or the larger context in which they are invoked  to
understand this point  consider again the mdp shown in figure    it is clear that the
optimal policy for exiting the left hand room  the exit subtask  depends on the location
of the goal  if it is at the top of the right hand room  then the agent should prefer to
exit via the upper door  whereas if it is at the bottom of the right hand room  the agent
should prefer to exit by the lower door  however  if we define the subtask of exiting the
left hand room using a pseudo reward of zero for both doors  then we obtain a policy that
is not optimal in either case  but a policy that we can re use in both cases  furthermore 
this policy does not depend on the location of the goal  hence  we can apply max node
irrelevance to solve the exit subtask using only the location of the robot and ignore the
location of the goal 
this example shows that we obtain the benefits of subtask reuse and state abstraction because we define the subtask using a termination predicate and a pseudo reward
function  the termination predicate and pseudo reward function provide a barrier that
prevents  communication  of value information between the exit subtask and its context 
compare this to parr s ham method  the hamq algorithm finds the best policy
consistent with the hierarchy  to achieve this  it must permit information to propagate
 into  the exit subtask  i e   the exit finite state controller  from its environment  but
this means that if any state that is reached after leaving the exit subtask has different
values depending on the location of the goal  then these different values will propagate
back into the exit subtask  to represent these different values  the exit subtask must know
   

fidietterich

the location of the goal  in short  to achieve a hierarchically optimal policy within the exit
subtask  we must  in general  represent its value function using the entire state space  state
abstractions cannot be employed without losing hierarchical optimality 
we can see  therefore  that there is a direct tradeoff between achieving hierarchical
optimality and employing state abstractions  methods for hierarchical optimality have more
freedom in defining subtasks  e g   using partial policies  as in the ham approach   but
they cannot  safely  employ state abstractions within subtasks  and in general  they cannot
reuse the solution of one subtask in multiple contexts  methods for recursive optimality  on
the other hand  must define subtasks using some method  such as pseudo reward functions
for maxq or fixed policies for the options framework  that isolates the subtask from its
context  but in return  they can apply state abstraction and the learned policy can be
reused in many contexts  where it will be more or less optimal  
it is interesting that the iterative method described by dean and lin        can be
viewed as a method for moving along this tradeoff  in the dean and lin method  the
programmer makes an initial guess for the values of the terminal states of each subtask
 i e   the doorways in figure     based on this initial guess  the locally optimal policies
for the subtasks are computed  then the locally optimal policy for the parent task is
computed while holding the subtask policies fixed  i e   treating them as options   at
this point  their algorithm has computed the recursively optimal solution to the original
problem  given the initial guesses  instead of solving the various subproblems sequentially
via an oine algorithm as dean and lin suggested  we could use the maxq q learning
algorithm 
but the method of dean and lin does not stop here  instead  it computes new values
of the terminal states of each subtask based on the learned value function for the entire
problem  this allows it to update its  guesses  for the values of the terminal states  the
entire solution process can now be repeated to obtain a new recursively optimal solution 
based on the new guesses  they prove that if this process is iterated indefinitely  it will
converge to the hierarchically optimal policy  provided  of course  that no state abstractions
are used within the subtasks  
this suggests an extension to maxq q learning that adapts the r  values online  each
time a subtask terminates  we could update the r  function based on the computed value
of the terminated state  to be precise  if j is a subtask of i  then when j terminates in
state s    we should update r  j  s    to be equal to v   i  s      maxa  q   i  s    a     however  this
will only work if r  j  s    is represented using the full state s   if subtask j is employing state
abstractions  x    s   then r  j  x    will need to be the average value of v   i  s     where
the average is taken over all states s  such that x     s     weighted by the probability of
visiting those states   this is easily accomplished by performing a stochastic approximation
update of the form
r j  x           fft  r  j  x      fftv   i  s   
each time subtask j terminates  such an algorithm could be expected to converge to the
best hierarchical policy consistent with the given state abstractions 
this also suggests that in some problems  it may be worthwhile to first learn a recursively
optimal policy using very aggressive state abstractions and then use the learned value
function to initialize a maxq representation with a more detailed representation of the
states  these progressive refinements of the state space could be guided by monitoring the
   

fimaxq hierarchical reinforcement learning

degree to which the values of v   i  x    vary for each abstract state x    if they have a large
variance  this means that the state abstractions are failing to make important distinctions
in the values of the states  and they should be refined 
both of these kinds of adaptive algorithms will take longer to converge than the basic
maxq method described in this paper  but for tasks that an agent must solve many times
in its lifetime  it is worthwhile to have learning algorithms that provide an initial useful
solution but then gradually improve that solution until it is optimal  an important goal for
future research is to find methods for diagnosing and repairing errors  or sub optimalities 
in the initial hierarchy so that ultimately the optimal policy will be discovered 

    automated discovery of abstractions

the approach taken in this paper has been to rely upon the programmer to design the
maxq hierarchy including the termination conditions  pseudo reward functions  and state
abstractions  but the results of this paper  particularly concerning state abstraction  suggest
ways in which we might be able to automate the construction of the hierarchy 
the main purpose of the hierarchy is to create opportunities for subtask sharing and
state abstraction  these are actually very closely related  in order for a subtask to be shared
in two different regions of the state space  it must be the case that the value function in those
two different regions is identical except for an additive offset  in the maxq framework 
that additive offset would be the difference in the c values of the parent task  so one way to
find reusable subtasks would be to look for regions of state space where the value function
exhibits these additive offsets 
a second way would be to search for structure in the one step probability transition
function p  s  js  a   a subtask will be useful if it enables state abstractions such as max
node irrelevance  we can formulate this as the problem of identifying some region of
state space such that  conditioned on being in that region  p  s  js  a  factors according to
equation     a top down divide and conquer algorithm similar to decision tree algorithms
might be able to do this 
a third way would be to search for funnel actions by looking for bottlenecks in the state
space through which all policies must travel  this would be useful for discovering cases of
result distribution irrelevance 
in some ways  the most dicult kinds of state abstractions to discover are those in
which arbitrary subgoals are introduced to constrain the policy  and sacrifice optimality  
for example  how could an algorithm automatically decide to impose landmarks onto the
hdg task  perhaps by detecting a large region of state space without bottlenecks or
variations in the reward function 
the problem of discovering hierarchies is an important challenge for the future  but at
least this paper has provided some guidelines for what constitute good state abstractions 
and these can serve as objective functions for guiding the automated search for abstractions 

   concluding remarks

this paper has introduced a new representation for the value function in hierarchical reinforcement learning the maxq value function decomposition  we have proved that the
maxq decomposition can represent the value function of any hierarchical policy under
   

fidietterich

both the finite horizon undiscounted  cumulative reward criterion and the infinite horizon
discounted reward criterion  this representation supports subtask sharing and re use  because the overall value function is decomposed into value functions for individual subtasks 
the paper introduced a learning algorithm  maxq q learning  and proved that it
converges with probability   to a recursively optimal policy  the paper argued that although
recursive optimality is weaker than either hierarchical optimality or global optimality  it is
an important form of optimality because it permits each subtask to learn a locally optimal
policy while ignoring the behavior of its ancestors in the maxq graph  this increases the
opportunities for subtask sharing and state abstraction 
we have shown that the maxq decomposition creates opportunities for state abstraction  and we identified a set of five properties  max node irrelevance  leaf irrelevance 
result distribution irrelevance  shielding  and termination  that allow us to ignore large
parts of the state space within subtasks  we proved that maxq q still converges in the
presence of these forms of state abstraction  and we showed experimentally that state abstraction is important in practice for the successful application of maxq q learning at
least in the taxi and hdg tasks 
the paper presented two different methods for deriving improved non hierarchical policies from the maxq value function representation  and it has formalized the conditions
under which these methods can improve over the hierarchical policy  the paper verified
experimentally that non hierarchical execution gives improved performance in the fickle
taxi task  where it achieves optimal performance  and in the hdg task  where it gives a
substantial improvement  
finally  the paper has argued that there is a tradeoff governing the design of hierarchical
reinforcement learning methods  at one end of the design spectrum are  context free 
methods such as maxq q learning  they provide good support for state abstraction and
subtask sharing but they can only learn recursively optimal policies  at the other end
of the spectrum are  context sensitive  methods such as hamq  the options framework 
and the early work of dean and lin  these methods can discover hierarchically optimal
policies  or  in some cases  globally optimal policies   but their drawback is that they cannot
easily exploit state abstractions or share subtasks  because of the great speedups that are
enabled by state abstraction  this paper has argued that the context free approach is to be
preferred and that it can be relaxed as needed to obtain improved policies 

acknowledgements
the author gratefully acknowledges the support of the national science foundation under
grant number iri          the oce of naval research under grant number n                the air force oce of scientific research under grant number f                 and
the spanish government under their program of estancias de investigadores extranjeros en
regimen de a no sabatico en espa na  in addition  the author is indebted to many colleagues
for helping develop and clarify the ideas in this paper including valentina zubek  leslie
kaelbling  bill langford  wes pinchot  rich sutton  prasad tadepalli  and sebastian thrun 
i particularly want to thank eric chown for encouraging me to study feudal reinforcement
learning  ron parr for providing the details of his ham machines  and sebastian thrun
for encouraging me to write a single comprehensive paper  i also thank andrew moore
   

fimaxq hierarchical reinforcement learning

 the action editor   valentina zubek  and the two sets of anonymous reviewers of previous
drafts of this paper for their suggestions and careful reading  which have improved the paper
immeasurably 

references

bellman  r  e          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
belmont  ma 
boutilier  c   dearden  r     goldszmidt  m          exploiting structure in policy construction  in proceedings of the fourteenth international joint conference on artificial
intelligence  pp            
currie  k     tate  a          o plan  the open planning architecture  artificial intelligence                
dayan  p     hinton  g          feudal reinforcement learning  in advances in neural
information processing systems     pp           morgan kaufmann  san francisco 
ca 
dean  t     lin  s  h          decomposition techniques for planning in stochastic domains 
tech  rep  cs        department of computer science  brown university  providence 
rhode island 
dietterich  t  g          the maxq method for hierarchical reinforcement learning  in
fifteenth international conference on machine learning  pp           morgan kaufmann 
fikes  r  e   hart  p  e     nilsson  n  j          learning and executing generalized robot
plans  artificial intelligence             
forgy  c  l          rete  a fast algorithm for the many pattern many object pattern
match problem  artificial intelligence                
hauskrecht  m   meuleau  n   kaelbling  l  p   dean  t     boutilier  c          hierarchical
solution of markov decision processes using macro actions  in proceedings of the
fourteenth annual conference on uncertainty in artificial intelligence  uai      pp 
        san francisco  ca  morgan kaufmann publishers 
howard  r  a          dynamic programming and markov processes  mit press  cambridge  ma 
jaakkola  t   jordan  m  i     singh  s  p          on the convergence of stochastic iterative
dynamic programming algorithms  neural computation                   
kaelbling  l  p          hierarchical reinforcement learning  preliminary results  in proceedings of the tenth international conference on machine learning  pp          san
francisco  ca  morgan kaufmann 
   

fidietterich

kalmar  z   szepesvari  c     lorincz  a          module based reinforcement learning for
a real robot  machine learning            
knoblock  c  a          learning abstraction hierarchies for problem solving  in proceedings
of the eighth national conference on artificial intelligence  pp          boston  ma 
aaai press 
korf  r  e          macro operators  a weak method for learning  artificial intelligence 
              
lin  l  j          reinforcement learning for robots using neural networks  ph d  thesis 
carnegie mellon university  department of computer science  pittsburgh  pa 
moore  a  w   baird  l     kaelbling  l  p          multi value functions  ecient automatic action hierarchies for multiple goal mdps  in proceedings of the international joint conference on artificial intelligence  pp            san francisco  morgan kaufmann 
parr  r       a   flexible decomposition algorithms for weakly coupled markov decision
problems  in proceedings of the fourteenth annual conference on uncertainty in
artificial intelligence  uai      pp          san francisco  ca  morgan kaufmann
publishers 
parr  r       b   hierarchical control and learning for markov decision processes  ph d 
thesis  university of california  berkeley  california 
parr  r     russell  s          reinforcement learning with hierarchies of machines  in advances in neural information processing systems  vol      pp            cambridge 
ma  mit press 
pearl  j          probabilistic inference in intelligent systems  networks of plausible inference  morgan kaufmann  san mateo  ca 
rummery  g  a     niranjan  m          online q learning using connectionist systems 
tech  rep  cued finfeng tr      cambridge university engineering department  cambridge  england 
sacerdoti  e  d          planning in a hierarchy of abstraction spaces  artificial intelligence 
               
singh  s   jaakkola  t   littman  m  l     szepesvari  c          convergence results
for single step on policy reinforcement learning algorithms  tech  rep   university of
colorado  department of computer science  boulder  co  to appear in machine
learning 
singh  s  p          transfer of learning by composing solutions of elemental sequential
tasks  machine learning             
sutton  r  s   singh  s   precup  d     ravindran  b          improved switching among
temporally abstract actions  in advances in neural information processing systems 
vol      pp             mit press 
   

fimaxq hierarchical reinforcement learning

sutton  r     barto  a  g          introduction to reinforcement learning  mit press 
cambridge  ma 
sutton  r  s   precup  d     singh  s          between mdps and semi mdps  learning 
planning  and representing knowledge at multiple temporal scales  tech  rep   university of massachusetts  department of computer and information sciences  amherst 
ma  to appear in artificial intelligence 
tadepalli  p     dietterich  t  g          hierarchical explanation based reinforcement
learning  in proceedings of the fourteenth international conference on machine
learning  pp          san francisco  ca  morgan kaufmann 
tambe  m     rosenbloom  p  s          investigating production system representations
for non combinatorial match  artificial intelligence                  
watkins  c  j  c  h          learning from delayed rewards  ph d  thesis  king s college 
oxford   to be reprinted by mit press   
watkins  c  j     dayan  p          technical note q learning  machine learning         

   

fi