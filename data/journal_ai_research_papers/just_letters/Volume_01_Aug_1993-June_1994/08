journal artificial intelligence research                 

submitted        published     

learning past tense english verbs 
symbolic pattern associator vs  connectionist models
charles x  ling

ling csd uwo ca

department computer science
university western ontario
london  ontario  canada n a  b 

abstract

learning past tense english verbs   seemingly minor aspect language acquisition   generated heated debates since       become landmark task
testing adequacy cognitive modeling  several artificial neural networks  anns 
implemented  challenge better symbolic models posed 
paper  present general purpose symbolic pattern associator  spa  based upon
decision tree learning algorithm id   conduct extensive head to head comparisons
generalization ability ann models spa different representations  conclude spa generalizes past tense unseen verbs better
ann models wide margin  offer insights case 
discuss new default strategy decision tree learning algorithms 

   introduction
learning past tense english verbs  seemingly minor aspect language acquisition 
generated heated debates since first connectionist implementation       rumelhart   mcclelland         based results  rumelhart mcclelland claimed
use acquisition human knowledge language best formulated ann
 artificial neural network  models without symbol processing postulates existence
explicit symbolic representation rules  since then  learning past tense become landmark task testing adequacy cognitive modeling  years
number criticisms connectionist modeling appeared  pinker   prince        lachter  
bever        prasada   pinker        ling  cherwenka    marinov         criticisms
centered mainly upon issues high error rates low reliability experimental results  inappropriateness training testing procedures   hidden  features
representation network architecture facilitate learning  well opaque
knowledge representation networks  several subsequent attempts improving
original results new ann models made  plunkett   marchman        cottrell   plunkett        macwhinney   leinbach        daugherty   seidenberg        
notably  macwhinney leinbach        constructed multilayer neural network
backpropagation  bp   attempted answer early criticisms  hand 
supporters symbolic approach believe symbol structures parse trees 
propositions  etc   rules manipulations  critical cognitive level 
connectionist approach may provide account neural structures
traditional symbol processing cognitive architecture implemented  fodor
  pylyshyn         pinker        prasada pinker        argue proper

c      ai access foundation morgan kaufmann publishers  rights reserved 

filing

accounting regular verbs dependent upon production rules  irregular
past tense ections may generalized ann like associative memory 
proper way debating adequacy symbolic connectionist modeling
contrasting competitive implementations  thus  symbolic implementation needed
compared ann models  is  fact  challenge posed macwhinney
leinbach         assert symbolic methods would work well
model  section titled  is better symbolic model   claim 
approach provided even accurate
characterization learning process  might still forced reject
connectionist approach  despite successes  proper way debating conceptualizations contrasting competitive implementations 
present case  would need symbolic implementation could contrasted
current implementation   macwhinney   leinbach        page     
paper  present general purpose symbolic pattern associator  spa  based
upon symbolic decision tree learning algorithm id   quinlan         shown
 ling   marinov        spa s results much psychologically realistic
ann models compared human subjects  issue predictive accuracy 
macwhinney leinbach        report important results model unseen
regular verbs  reply criticism  macwhinney        re implemented ann
model  claimed raw generalization power close spa 
believed case systems learn data set 
good reason equivalent performance two
models        two computationally powerful systems given
set input data  extract every bit data regularity input 
without processing  much blood squeezed
turnip  systems  spa ann  extracted
could   macwhinney        page     
show case  obviously reasons one learning
algorithm outperforms another  otherwise study different learning algorithms   
occam s razor principle   preferring simplest hypothesis complex
ones   creates preference biases learning algorithms  preference bias preference
order among competitive hypotheses hypothesis space  different learning algorithms 
however  employ different ways measuring simplicity  thus concepts bias
different  well learning program generalizes depends upon degree
regularity data fits bias  study compare raw generalization
ability symbolic ann models task learning past tense english
verbs  perform extensive head to head comparisons ann spa  show
effects different representations encodings generalization abilities 
experimental results demonstrate clearly
   distributed representation  feature connectionists advocating 
lead better generalization compared symbolic representation  arbitrary error correcting codes proper length 
   

filearning past tense  symbolic vs connectionist models

   anns cannot learn identity mapping preserves verb stem past
tense well spa can 
   new representation suggested macwhinney        improves predictive accuracy spa ann  spa still outperforms ann models 
   sum  spa generalizes past tense unseen verbs better ann models
wide margin 
section   discuss reasons spa better learning model
task english past tense acquisition  results support view many
rule governed cognitive processes better modeled symbolic  rather connectionist  systems 

   review previous work
section  review brie two main connectionist models learning past
tenses english verbs  subsequent criticisms 

    rumelhart mcclelland s model

rumelhart mcclelland s model based simple perceptron based pattern associator interfaced input output encoding decoding network allows model
associate verb stems past tenses using special wickelphone wickelfeature
phoneme representation format  learning algorithm classical perceptron convergence procedure  training testing sets mutually disjoint experiments 
errors made model training process broadly follow u shaped learning curve stages acquisition english past tense exhibited young children 
testing sample consists     unseen  low frequency verbs     irregular    regular 
randomly chosen  testing sample results     error rate
irregulars  regulars fare better       error rate  thus  overall error rate
whole testing sample          wrong ambiguous past tense forms    tested 
rumelhart mcclelland        claim outcome experiment disconfirms
view exist explicit  though inaccessible  rules underlie human knowledge
language 

    macwhinney leinbach s model

macwhinney leinbach        report new connectionist model learning
past tenses english verbs  claim results new simulation far
superior rumelhart mcclelland s results  answer criticisms aimed earlier model  major departure rumelhart mcclelland s
model wickelphone wickelfeature representational format replaced
unibet  macwhinney        phoneme representational system allows assignment single alphabetic numerical letter total    phonemes  macwhinney
leinbach use special templates code phoneme position
word  actual input network created coding individual phonemes sets
   

filing

phonetic features way similar coding wickelphones wickelfeatures  cf
section       network two layers      hidden  units fully connected adjacent
layers  number arrived trial error  addition  network
special purpose set connections copy input units directly onto output units 
altogether       regular irregular english verbs selected experiment
       used training       regular     irregular      low
frequency irregular verbs used testing  macwhinney   leinbach        page      
training network takes        epochs  end training still    errors
irregular pasts  macwhinney leinbach believe allow network
run several additional days give additional hidden unit resources  probably
reach complete convergence  macwhinney   leinbach        page      
testing error rate reported based small biased test sample    unseen
irregular verbs       predicted incorrectly  test model
unseen regular verbs   unfortunately  test similar set    regulars  
 macwhinney   leinbach        page      

    criticism connectionist models

previous current criticisms connectionist models learning past tenses
english verbs center mainly several issues  issues summarized
following subsections 

      error rates

error rate producing past tenses  unseen  test verbs high
ann models  important tests carried macwhinney leinbach
       model  experimental results indicate neither model reaches level
adult competence  addition  relatively large numbers errors psychologically
realistic since humans rarely make them 

      training testing procedures

rumelhart mcclelland s model  macwhinney leinbach s model 
generalization ability measured one training testing sample  further  testing
sets randomly chosen  small  accuracy testing irregular
verbs vary greatly depending upon particular set testing verbs chosen  thus
multiple runs large testing samples necessary assess true generalization
ability learning model  therefore  results previous connectionist models
reliable  section    set reliable testing procedure compare connectionist
models symbolic approach  previous connectionist simulations
criticized crude training processes  for example  sudden increase regular
verbs training set   create behavior u shaped learning curves 

      data representation network architecture

past criticisms connectionist models aimed datarepresentation formats employed simulations  lachter bever        pointed
   

filearning past tense  symbolic vs connectionist models

results achieved rumelhart mcclelland s model would impossible without use several trics  the representations crucially supposes 
introduced adoption wickelphone wickelfeature representational format 
macwhinney leinbach claim improved upon earlier connectionist
model getting rid wickelphone wickelfeature representation format  thus
responded many criticisms format entailed  however  macwhinney leinbach introduce several trics data representation format 
example  instead coding predecessor successor phonemes wickelphones  introduce special templates code positional information  means
network learn associate patterns phoneme positions within predetermined consonant vowel pattern  further  use restrictive templates gets rid many english
verbs fit chosen template  may bias model favour shorter
verbs  predominantly anglo saxon origin  longer verbs  predominantly composite latin french origin  another trics introduced phonetic feature
encoding  a distributed representation   clear phonetic features front 
centre  back  high  etc  chosen  represent finer grained  microfeatures 
help capture regularities english past tenses  section      show
straightforward symbolic representation leads better generalization
carefully engineered distributed representation  undermines claimed advantages
distributed representation connectionist models 

      knowledge representation integration acquired knowledge
pinker prince         lachter bever        point rumelhart
mcclelland try model acquisition production past tense isolation
rest english morphological system  rumelhart mcclelland  well
macwhinney leinbach  assume acquisition process establishes direct
mapping phonetic representation stem phonetic representation
past tense form  direct mapping collapses well established distinctions
lexical item vs  phoneme string  morphological category vs  morpheme  simply
remaining level phonetic patterns  impossible express new categorical
information first order  predicate function variable  format  one inherent deficits
connectionist implementations thing variable verb
stem  hence way model attain knowledge one could
add sux stem get past tense  pinker   prince        page       since
acquired knowledge networks large weight matrix  usually opaque
human observer  unclear phonological levels processing connectionist
models carry integrated morphological  lexical  syntactical level
processing  neither rumelhart mcclelland macwhinney leinbach address
issue  contrast anns whose internal representations entirely opaque 
spa represent acquired knowledge form production rules  allow
processing  resulting higher level categories verb stem voiced
consonants  linguistically realistic production rules using new categories regular
verbs  associative templates irregular verbs  ling   marinov        
   

filing

   symbolic pattern associator

take macwhinney leinbach s challenge better symbolic model learning
past tense english verbs  present general purpose symbolic pattern associator
 spa   generalize past tense unseen verbs much accurately
connectionist models section  model symbolic several reasons  first 
input output representation learning program set phoneme symbols 
basic elements governing past tense ection  second  learning
program operates phoneme symbols directly  acquired knowledge
represented form production rules using phoneme symbols well  third 
production rules phonological level easily generalized firstorder rules use abstract  high level symbolic categories morphemes
verb stem  ling   marinov         contrast  connectionist models operate
distributed representation  phonetic feature vectors   acquired knowledge
embedded large weight matrix  therefore hard see knowledge
generalized abstract representations categories 

    architecture symbolic pattern associator

spa based c     quinlan        improved implementation id 
learning algorithm  cf   quinlan          id  program inducing classification rules
form decision trees set classified examples  uses information gain ratio
criterion selecting attributes roots subtrees  divide and conquer strategy
recursively applied building subtrees remaining examples training set
belong single concept  class   leaf labeled concept  information
gain guides greedy heuristic search locally relevant discriminating attribute
maximally reduces entropy  randomness  divided set examples 
use heuristic usually results building small decision trees instead larger ones
fit training data 
task learn classify set different patterns single class several
mutually exclusive categories  id  shown comparable neural networks
 i e   within    range predictive accuracy  many real world learning tasks
 cf   shavlik  mooney    towell        feng  king  sutherland    henery        ripley 
      weiss   kulikowski          however  task classify set  input  patterns
 output  patterns many attributes  id  cannot applied directly  reason
id  treats different output patterns mutually exclusive classes  number
classes would exponentially large and  importantly  generalization individual
output attributes within output patterns would lost 
turn id  similar n to   classification system general purpose n to m
symbolic pattern associators  spa applies id  output attributes combines
individual decision trees  forest   set trees  similar approach proposed
dealing distributed  binary  encoding multiclass learning tasks nettalk
 english text to speech mapping   dietterich  hild    bakiri         tree takes
input set attributes input patterns  used determine value
   spa programs relevant datasets obtained anonymously ftp csd uwo ca
pub spa   

   

filearning past tense  symbolic vs connectionist models

one attribute output pattern  specifically  pair input attributes    n  
output attributes      m   represented as 

        n             m

spa build total decision trees  one output attribute  i   
m  taking input attributes         n per tree  trees built  spa
use jointly determine output pattern            m input pattern
        n 

important feature spa explicit knowledge representation  decision trees
output attributes easily transformed propositional production rules  quinlan 
       since entities rules symbols semantic meanings  acquired
knowledge often comprehensible human observer  addition  processing
integration rules yield high level knowledge  e g   rules using verb stems 
 ling   marinov         another feature spa trees different output
attributes contain identical components  branches subtrees   ling   marinov        
components similar roles hidden units anns since shared
decision trees one output attribute  identical components
viewed high level concepts feature combinations created learning program 

    default strategies

interesting research issue decision tree learning algorithms handle default
class  default class class assigned leaves training examples
classified into  call leaves empty leaves  happens attributes
many different values  training set relatively small  cases 
tree construction  branches explored attributes  testing
examples fall empty leaves  default strategy needed assign classes
empty leaves 
easier understanding  use spelling form verbs subsection explain
different default strategies work   in actual learning experiment verbs
represented phonetic form   use consecutive left to right alphabetic representation 
verb stems past tenses small training set represented follows 
a f f o r d                  
e a t                        
l a u n c h                  
l e a v e                    

  
  
  
  

a f f o r d e d              
a t e                        
l a u n c h e d              
l e f t                      

used filler empty space  left hand    columns input patterns
stems verbs  right hand    columns output patterns
corresponding correct past tense forms 
discussed     decision trees constructed  one output attribute 
decision tree first output attribute constructed  see figure    a  
following   examples 
a f f o r d                     
e a t                           
l a u n c h                      l

   

filing

l e a v e                        l

last column classification first output attribute  however  many
branches  such     c figure    a   explored  since training example
attribute value  testing pattern first input attribute equal c 
class assigned to  id  uses majority default  is  popular
class whole subtree   assigned empty leaves  example above 
either class l chosen since   training examples  however 
clearly right strategy task since verb create would output
l       a        incorrect  unlikely small training set
variations attribute values  majority default strategy id  appropriate
task 

 


e

 
l



c

z

   passthrough

  



 
a  

a  

l  

c  

o  

z  

d   
p

x n indicates n examples
classified leaf labelled x 
x    boxed  indicates empty leaves 

t   

figure     a  passthrough default

   majority

r

d  

l

d  

k

t  

 b  various default

applications verb past tense learning  new default heuristic   passthrough
  may suitable  is  classification empty leaf
attribute value branch  example  using passthrough default strategy 
create output c        passthrough strategy gives decision trees first order
avor  since production rules empty leaves represented attribute   x
class   x x unused attribute values  passthrough domaindependent heuristic strategy class labels may nothing
attribute values applications 
applying passthrough strategy alone  however  adequate every output
attribute  endings regular past tenses identical input
patterns  irregular verbs may vowel consonant changes middle
verbs  cases  majority default may suitable passthrough 
order choose right default strategy   majority passthrough   decision
made based upon training data corresponding subtree  spa first determines
majority class  counts number examples subtrees belong
class  counts number examples subtrees coincide
   

filearning past tense  symbolic vs connectionist models

passthrough strategy  two numbers compared  default strategy employed
examples chosen  instance  example  see figure    a   
majority class l  or a    instances  however    examples coinciding
passthrough default  two l one a  thus passthrough strategy takes over 
assigns empty leaves level  empty attribute branch c would assigned
class c  note default strategy empty leaves attribute x depends upon
training examples falling subtree rooted x   localized method ensures
related objects uence calculating default classes  result  spa
adapt default strategy best suited different levels decision trees 
example  figure    b   two different default strategies used different levels
tree  use spa adaptive default strategy throughout remainder
paper  note new default strategy trics data representation 
rather  represents bias learning program  learning algorithm default
strategy independent data representation  effect different data representations
generalization discussed sections                passthrough strategy
imposed anns well adding set copy connections input units
twin output units  see section     detail 

    comparisons default strategies id   spa  ann
default strategy neural networks tend take generalizing default classes
compared id  spa  conducted several experiments determine neural
networks  default strategy  assume domain one attribute x
may take values a  b  c  d  class one a  b  c  d  training
examples attribute values a  b  c   reserved testing default
class  training set contains multiple copies example form certain
majority class  table   shows two sets training testing examples used test
compare default strategies id   spa neural networks 
data set  
data set  
training examples
training examples
values x class   copies values x class   copies


  

c
  
b
b
 
b
b
 
c
c
 
c
c
 
testing example
testing example

 
 

 
 
table    two data sets testing default strategies various methods 
classification testing examples id  spa quite easy decide  since
id  takes majority default  output class  with    training examples 
first data set  c  with    training examples  second data set  spa 
number examples using passthrough    first data set     second
   

filing

data set  therefore  passthrough strategy wins first case output class
d  majority strategy wins second case output class c 
neural networks  various coding methods used represent values attribute x   dense coding  used    represent a     b     c   
d  tried standard one per class encoding  real number encoding      a 
    b      c     d   networks trained using hidden units
possible case  found cases classification testing example stable  varies different random seeds initialize networks  table
  summarises experimental results  anns  various classifications obtained   
different random seeds listed first ones occurring frequently  seems
neural networks consistent default strategy 
neither majority default id  passthrough default spa  may
explain connectionist models cannot generalize unseen regular verbs well even
training set contains regular verbs  see section       networks diculty
 or underconstrained  generalizing identity mapping copies attributes
verb stems past tenses 
classification testing example
data set   data set  
id 

c
spa

c
ann  dense coding
b  c
b
ann  one per class
b  c 
c  b
ann  real numbers
c 
d  c
table    default strategies id   spa ann two data sets 

   head to head comparisons symbolic ann models

section  perform series extensive head to head comparisons using several
different representations encoding methods  demonstrate spa generalizes
past tense unseen verbs better ann models wide margin 

    format data

verb set came macwhinney s original list verbs  set contains
     stem past tense pairs  learning based upon phonological unibet representation  macwhinney         different phonemes represented different
alphabetic numerical letters  total    phonemes  source file transferred
standard format pairs input output patterns  example  verbs
table   represented pairs input output patterns  verb stem    past tense  
  b   n d   n
  
  b   n d   n d
i k s e l   r e t    i k s e l   r e t i d

   

filearning past tense  symbolic vs connectionist models

  r   z      r o z
b i k   m    b i k e m

see table    the original verb set available online appendix     keep one
form past tense among multiple past tenses  such hang hanged hang hung 
data set  addition  homophones exist original data set  consequently 
noise  contradictory data input pattern different output
patterns  training testing examples  note information whether
verb regular irregular provided training testing processes 
base  stem 
unibet
b base
    irregular
spelling form phonetic form d  past tense     regular
abandon
 b nd n
b
 
abandoned
 b nd nd

 
benefit
ben fit
b
 
benefited
ben fitid

 
arise
 r z
b
 
arose
 roz

 
become
bik m
b
 
became
bikem

 
      
table    source file macwhinney leinbach 

    experiment setup

guarantee unbiased reliable comparison results  use training testing samples
randomly drawn several independent runs  spa ann provided
sets training testing examples run  allows us achieve reliable
estimate inductive generalization capabilities model task 
neural network program used package called xerion  developed
university toronto  several sophisticated search mechanisms
standard steepest gradient descent method momentum  found training
conjugate gradient method much faster standard backpropagation
algorithm  using conjugate gradient method avoids need search proper
settings parameters learning rate  however  need determine
proper number hidden units  experiments anns  first tried various
numbers hidden units chose one produced best predictive accuracy
trial run  use network number hidden units actual runs 
spa  hand  parameters adjust 
one major difference implementation anns spa spa take
 symbolic  phoneme letters directly anns normally encode phoneme letter
binary bits   of course  spa apply binary representation   studied
various binary encoding methods compared results spa using symbolic letter
   

filing

representation  since outputs neural networks real numbers  need decode
network outputs back phoneme letters  used standard method decoding 
phoneme letter minimal real number hamming distance  smallest angle 
network outputs chosen  see binary encoding affects generalization 
spa trained binary representation  since spa s outputs
binary  decoding process may tie several phoneme letters  case  one
chosen randomly  ects probability correct decoding level
phoneme letters  phoneme letters decoded  one letters
incorrect  whole pattern counted incorrect word level 

    templated  distributed representation

set experiments conducted using distributed representation suggested
macwhinney leinbach         according macwhinney leinbach  output
left justified template format cccvvcccvvcccvvccc  c stands
consonant v vowel space holders  input two components  left justified
template format input  right justified template format
vvccc  example  verb bet  represented unibet coding bet  shown
template format follows   blank phoneme  
input
bet
template 
output
bet
template 

b  e t            
cccvvcccvvcccvvccc
 left justified 

 e  t
vvccc
 right justified 

b  e t            
cccvvcccvvcccvvccc
 left justified 

specific distributed representation   set  binary  phonetic features   used
encode phoneme letters connectionist networks  vowel  v
templates  encoded   phonetic features  front  centre  back  high  low  middle  round 
diphthong  consonant  c templates     phonetic features
 voiced  labial  dental  palatal  velar  nasal  liquid  trill  fricative interdental   note
two feature sets vowels consonants identical  templates
needed order decode right type phoneme letters outputs
network 
experimental comparison  decided use right justified template
 vvccc  since information redundant  therefore  used left justified
template  cccvvcccvvcccvvccc  input output   the whole verb set
templated phoneme representation available online appendix    contains
     pairs verb stems past tenses fit template   ease implementation 
added two extra features always assigned   vowel phonetic feature
set  therefore  vowels consonants encoded    binary bits  ann
thus             input bits     output bits  found one layer    
hidden units  same macwhinney        model  reached highest predictive accuracy
trial run  see figure   network architecture used 
   

filearning past tense  symbolic vs connectionist models

     output units 

c

   

   

   

   

      

   

   

   

c

c

v

v

cccvvcccvv

c

c

c

 full connection two layers 
      

     hidden units 

      

 full connection two layers 

c

   

   

   

   

      

   

   

   

c

c

v

v

cccvvcccvv

c

c

c

     input units 

figure    architecture network used experiment 
spa trained tested data sets phoneme letters directly 
is     decision trees built phoneme letters output templates 
see phonetic feature encoding affects generalization  trained spa
distributed representation   binary bit patterns     input bits
    output bits   exactly ann simulation  addition  see
 symbolic  encoding works ann  train another neural network  with    
hidden units   one per class  encoding  is  phoneme letter  total    
   phoneme letters plus one blank  encoded    bits  one phoneme letter 
used     verb pairs  including regular irregular verbs  training
testing sets  sampling done randomly without replacement  training testing
sets disjoint  three runs spa ann conducted  spa ann
trained tested data set run  training reached      accuracy
spa around     ann 
testing accuracy novel verbs produced interesting results  ann model
spa using distributed representation similar accuracy  ann
slightly better  may well caused binary outputs spa suppress
fine differences prediction  hand  spa using phoneme letters directly
produces much higher accuracy testing  spa outperforms neural networks  with
either distributed one per class representations     percentage points  testing
results ann spa found table    findings clearly indicate
spa using symbolic representation leads much better generalization ann models 

    learning regular verbs

predicting past tense unseen verb  either regular irregular 
easy task  irregular verbs learned rote traditionally thought since
   

filing

distributed representation
ann    correct
spa    correct
reg irrg comb reg irrg comb
                             
                           
                            
                             

symbolic representation
ann    correct
spa    correct
reg irrg comb reg irrg comb
                             
                             
                             
                             

table    comparisons testing accuracy spa ann distributed symbolic
representations 
children adults occasionally extend irregular ection irregular sounding regular
verbs pseudo verbs  such cleef   cleft   prasada   pinker         similar
novel verb cluster irregular verbs similar phonological patterns 
likely prediction irregular past tense form  pinker        prasada
pinker        argue regular past tenses governed rules  irregulars may
generated associated memory graded effect irregular past tense
generalization  would interesting  therefore  compare spa ann
past tense generalization regular verbs only  spa ann use same 
position specific  representation  learning regular past tenses would require learning different
suxes  different positions  learn identity mapping copies verb stem
past tenses verbs different lengths 
used templated representation previous section  training
testing sets contained regular verbs  samples drawn randomly without
replacement  maximize size testing sets  testing sets simply consisted
regular verbs sampled training sets  training testing sets
used following methods compared  see effect adaptive
default strategy  as discussed section      generalization  spa majority
default adaptive default tested  ann models similar
used previous section  except     one layer hidden units  turned
best predictive accuracy test run   passthrough default strategy
imposed neural networks adding set copy connections connect directly
input units twin output units  macwhinney leinbach        used
copy connections simulation  therefore tested networks copy
connection see generalization would improved well 
results predictive accuracy spa anns one run
randomly sampled training testing sets summarized table    see 
spa adaptive default strategy  combines majority passthrough
default  outperforms spa majority default strategy used id  
   phonological form three different suxes regular verbs  verb stem ends
 unibet phonetic representations   sux id  example  extend   extended  in
spelling form   verb stem ends unvoiced consonant  sux t  example  talk
  talked  verb stem ends voiced consonant vowel  sux d  example 
solve   solved 

   

filearning past tense  symbolic vs connectionist models

anns copy connections generalize better ones without  however  even
ann models copy connections lower predictive accuracy spa  majority   addition  differences predictive accuracy larger smaller sets
training examples  smaller training sets make difference testing accuracy
evident  training set contains      patterns  out        testing accuracy
becomes similar  would approach asymptotically      larger training sets 
upon examination  errors made ann models occur identity mapping
 i e   strange phoneme change drop   verb stems cannot preserved past
tense phonemes previously seen training examples  contradicts
findings prasada pinker         show native english speakers generate
regular sux adding past tenses equally well unfamiliar sounding verb stems  as long
verb stems sound close irregular verbs   indicates bias
ann learning algorithms suitable type task  see discussion
section   
training
percent correct testing
size
spa  adaptive  spa  majority  ann  copy con   ann  normal 
  
    
    
    
   
   
    
    
    
    
   
    
    
    
    
   
    
    
    
    
    
    
    
    
    
table    predictive accuracy learning past tense regular verbs

    error correcting codes
dietterich bakiri        reported increase predictive accuracy errorcorrecting codes large hamming distances used encode values attributes 
codes larger hamming distance  d  allow correcting fewer d  
bits errors  thus  learning programs allowed make mistakes bit level
without outputs misinterpreted word level 
wanted find performances spa anns improved errorcorrecting codes encoding    phonemes  chose error correcting codes ranging
ones small hamming distance ones large hamming distance  using
bhc codes  see dietterich bakiri          number attributes
phoneme large  data representation changed slightly experiment 
instead    phoneme holders templates    consecutive  left to right phoneme holders
used  verbs stems past tenses   phonemes removed
training testing sets   the whole verb set representation available online
appendix    contains      pairs verb stems past tenses whose lengths shorter
    spa ann take exactly training testing sets  contains    
pairs verb stems past tenses  error correcting codes encoding phoneme
   

filing

letter  still  training networks    bit longer error correcting codes takes long
run  there            input attributes     output attributes   therefore 
two runs        bit codes conducted  consistent dietterich bakiri
       s findings  found testing accuracy generally increases hamming
distance increases  however  observed testing accuracy decreases
slightly codes become long  accuracy using    bit codes  with hamming
distance     reaches maximum value          quite close accuracy
        spa using direct phoneme letter representation  seems trade off
tolerance errors large hamming distance diculty learning
longer codes  addition  found testing accuracy anns lower one
spa    bit     bit error correcting codes  results summarized
table   
ann
hamming distance correct bit level correct word level
   bit codes
  
     
     
   bit codes
  
     
     
spa
hamming distance correct bit level correct word level
   bit codes
  
     
     
   bit codes
  
     
     
   bit codes
  
     
     
    bit codes
  
     
     
table    comparisons testing accuracy spa anns error correcting codes
results previous two subsections undermine advantages
distributed representation anns  unique feature advocated connectionists 
demonstrated that  task  distributed representation actually allow
adequate generalization  spa using direct symbolic phoneme letters spa
error correcting codes outperform anns distributed representation wide margin 
however  neither phoneme symbols bits error correcting codes encode  implicitly
explicitly  micro features distributed representation  may
distributed representation used optimally designed  nevertheless  straightforward
symbolic format requires little representation engineering compared distributed
representation anns 

    right justified  isolated sux representation

macwhinney leinbach        report important results predictive accuracy model unseen regular verbs  reply  macwhinney       
paper  ling   marinov         macwhinney re implemented ann model  new
implementation        verb stem past tense pairs training set  among
     regular     irregular  training took       epochs  reached     
correct regulars     irregulars  testing set consisted    regulars   
irregulars  percent correct testing epoch           regulars    
irregulars  combined       testing set  macwhinney claimed raw
   

filearning past tense  symbolic vs connectionist models

generalization power ann model close spa  believes
case simply systems trained data set 
realize  via private communication  new representation used macwhinney s
recent implementation plays critical role improved performance  macwhinney s
new representation  input  for verb stems  coded right justified template
cccvvcccvvcccvvccc  output contains two parts  right justified template
one input  coda form vvccc  rightjustified template output used represent past tense without including
sux regular verbs  sux regular past tense always stays coda 
isolated main  right justified templates  irregular past tense 
coda left empty  example  input output templated patterns past tense
verbs table   represented as 
input
 right justified 
cccvvcccvvcccvvccc
     b    nd   n  
b  e n    f  i t  
          r    z  
     b  i k    m  

output
 right justified 
cccvvcccvvcccvvccc
     b    nd   n  
b  e n    f  i t  
          r  o z  
     b  i k  e m  

 suffix only 
vvccc
  d    for abandon abandoned 
i d    for benefit benefited 
       for arise arose 
       for become became 

data representation clearly facilitates learning  regular verbs  output
patterns always identical input patterns  addition  verb ending phoneme
letters always appear fixed positions  i e   right vvccc section
input template  due right justified  templated representation  furthermore  sux
always occupies coda  isolated right justified templates 
performed series experiments see much improvement could accomplish using new representation macwhinney s recent ann model
left justified representation discussed section      spa  with averaged predictive
accuracy        outperforms macwhinney s recent ann implementation  with predictive accuracy        wide margin  addition  predictive accuracy
improved average       left justified representation      
right justified  isolated sux one  see results table   

   general discussion conclusions

two factors contribute generalization ability learning program  first
data representation  bias learning program  arriving
right  optimal  representation dicult task  argued prasada pinker
        regular verbs represented coarse grain terms verb stem
suxes  irregular verbs finer grain terms phonological properties 
admittedly  spa works uniformly level phoneme letters  anns do  however 
spa produces simple production rules use phoneme letters directly 
rules generalized first order rules new representations stems
voiced consonants used across board rule learning
modules  ling   marinov         one major advantages ann models 
   

filing

predictive accuracy right justified  isolated sux representation
spa
macwhinney s ann model
training testing training testing
training testing
       
        
        
run  
    
    
run  
    
    
run  
    
    
average
    
    
      one run 
table    comparisons testing accuracy spa ann  with right justified  isolated
sux representation 
seems quite conceivable children acquire high level concepts stems
voiced consonants learning noun plurals  verb past tense  verb third person
singular  comparative adjectives  on  large weight matrix result
learning  hard see knowledge generalized ann models
shared modules 
even exactly data representation  exist learning tasks
symbolic methods spa generalize categorically better anns  converse true  fact ects different inductive biases different learning
algorithms  occam s razor principle   preferring simplest hypothesis
complex ones   creates preference bias  preference choosing certain hypotheses
others hypothesis space  however  different learning algorithms choose different hypotheses use different measurements simplicity  example  among
possible decision trees fit training examples  id  spa induce simple decision
trees instead complicated ones  simple decision trees converted small sets
production rules  well learning algorithm generalizes depends upon degree
underlying regularities target concept fit bias  words 
underlying regularities represented compactly format hypotheses produced
learning algorithm  data generalized well  even small set training
examples  otherwise  underlying regularities large hypothesis 
algorithm looking compact ones  as per occam s razor principle   hypotheses inferred accurate  learning algorithm searches hypotheses larger
necessary  i e   use occam s razor principle  normally  underconstrained   know  based training examples only  many
competitive hypotheses large size inferred 
describe bias learning algorithm looking training examples
different classes separated n dimensional hyperspace n number
attributes  decision node decision tree forms hyperplane described
linear function x   a  hyperplanes perpendicular axis 
partial space hyperplanes extend within subregion formed
hyperplanes parents  nodes  likewise  hidden units threshold function
anns viewed forming hyperplanes hyperspace  however  unlike ones
decision trees  need perpendicular axis  full space
   

filearning past tense  symbolic vs connectionist models

hyperplanes extend whole space  id  applied concepts fit
ann s bias  especially hyperplanes perpendicular axis  many
zigzag hyperplanes perpendicular axes would needed separate different
classes examples  hence  large decision tree would needed  fit
id  s bias  similarly  ann learning algorithms applied concepts fit id  s
bias  especially hyperplanes form many separated  partial space regions  many
hidden units may needed regions 
another major difference anns id  anns larger variation
weaker bias  cf   geman  bienenstock    doursat         id   many
boolean functions  e g   linearly separable functions  fit small network  e g   one
hidden units  small decision tree  sometimes attributed
claimed versatility exibility anns  learn  but necessarily predict reliably well  many functions  symbolic methods brittle  however  belief
humans versatile  learning algorithm large variation 
rather set strong biased learning algorithms  somehow
search bias space add new members set new learning tasks  symbolic learning algorithms clear semantic components explicit representation 
thus easily construct strong based algorithms motivated various specific
learning tasks  adaptive default strategy spa example 
hand  still largely know effectively strengthen bias anns many
specific tasks  such identity mapping  k term dnf  etc    techniques  such
adding copy connections weight decaying  exist  exact effects biasing
towards classes functions clear 
analyses  ling   marinov         underlying regularities governing
ection past tense english verbs form small set production rules
phoneme letters  especially regular verbs  rules either identity
rules sux adding rules  example  decision trees converted set
precedence ordered production rules complicated rules  rules conditions  listed first  example  using consecutive  left to right phonetic representation 
typical sux adding rule verb stems   phoneme letters  such talk   talked  is 
    k           
is  fourth input phoneme k fifth blank  i e   verb
ending  fifth output phoneme t  hand  identity mapping rules
one condition  typical identity rule looks like 
    l       l
fact  passthrough default strategy allows identity mapping rules represented simple first order format 
    x       x
x phoneme  clearly  knowledge forming regular past tenses
thus expressed simple  conjunctive rules fit bias spa  id   
therefore  spa much better generalization ability ann models 
conclude  demonstrated  via extensive head to head comparisons 
spa realistic better generalization capacity anns learning
past tense english verbs  argued symbolic decision tree production rule
learning algorithms outperform anns  because  first  domain seems
   

filing

governed compact set rules  thus fits bias symbolic learning algorithm 
second  spa directly manipulates representation better anns  i e  
symbolic phoneme letters vs  distributed representation   third  spa able
derive high level concepts used throughout english morphology  results support
view many high level  rule governed cognitive tasks better modeled
symbolic  rather connectionist  systems 

acknowledgements
gratefully thank steve pinker constant encouragement  marin marinov  steve
cherwenka huaqing zeng discussions help implementing spa 
thank brian macwhinney providing verb data used simulation  discussions
tom dietterich  dave touretzky brian macwhinney  well comments
reviewers  helpful  research conducted support nserc
research grant computing facilities department 

references

cottrell  g     plunkett  k          using recurrent net learn past tense 
proceedings cognitive science society conference 
daugherty  k     seidenberg  m          beyond rules exceptions  connectionist
modeling approach ectional morphology  lima  s   ed    reality
linguistic rules  john benjamins 
dietterich  t     bakiri  g          error correcting output codes  general method
improving multiclass inductive learning programs  aaai     proceedings ninth
national conference artificial intelligence  
dietterich  t   hild  h     bakiri  g          comparative study id  backpropagation english text to speech mapping  proceedings  th international
conference machine learning  morgan kaufmann 
feng  c   king  r   sutherland  a     henery  r          comparison symbolic  statistical neural network classifiers  manuscript  department computer science 
university ottawa 
fodor  j     pylyshyn  z          connectionism cognitive architecture  critical
analysis  pinker  s     mehler  j   eds    connections symbols  pp         
cambridge  ma  mit press 
geman  s   bienenstock  e     doursat  r          neural networks bias variance
dilemma  neural computation            
lachter  j     bever  t          relation linguistic structure associative
theories language learning   constructive critique connectionist learning
models  pinker  s     mehler  j   eds    connections symbols  pp            
cambridge  ma  mit press 
   

filearning past tense  symbolic vs connectionist models

ling  x   cherwenka  s     marinov  m          symbolic model learning past
tenses english verbs  proceedings ijcai     thirteenth international conference artificial intelligence   pp             morgan kaufmann publishers 
ling  x     marinov  m          answering connectionist challenge  symbolic model
learning past tense english verbs  cognition                  
macwhinney  b          childes project  tools analyzing talk  hillsdale  nj 
erlbaum 
macwhinney  b          connections symbols  closing gap  cognition         
        
macwhinney  b     leinbach  j          implementations conceptualizations  revising verb model  cognition                
pinker  s          rules language  science                 
pinker  s     prince  a          language connectionism  analysis parallel
distributed processing model language acquisition  pinker  s     mehler  j 
 eds    connections symbols  pp            cambridge  ma  mit press 
plunkett  k     marchman  v          u shaped learning frequency effects multilayered perceptron  implications child language acquisition  cognition          
    
prasada  s     pinker  s          generalization regular irregular morphological
patterns  language cognitive processes                
quinlan  j          induction decision trees  machine learning                  
quinlan  j          c    programs machine learning  morgan kaufmann  san mateo 
ca 
ripley  b          statistical aspects neural networks  invited lectures semstat
 seminaire europeen de statistique  sandbjerg  denmark        april       
rumelhart  d     mcclelland  j          learning past tenses english verbs 
rumelhart  d   mcclelland  j     pdp research group  eds    parallel distributed processing vol    pp             cambridge  ma  mit press 
shavlik  j   mooney  r     towell  g          symbolic neural learning algorithms 
experimental comparison  machine learning                   
weiss  s     kulikowski  c          computer systems learn  classification prediction methods statistics  neural networks  machine learning  expert systems 
morgan kaufmann  san mateo  ca 

   


