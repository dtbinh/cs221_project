journal of artificial intelligence research                 

submitted        published     

learning the past tense of english verbs 
the symbolic pattern associator vs  connectionist models
charles x  ling

ling csd uwo ca

department of computer science
the university of western ontario
london  ontario  canada n a  b 

abstract

learning the past tense of english verbs   a seemingly minor aspect of language acquisition   has generated heated debates since       and has become a landmark task
for testing the adequacy of cognitive modeling  several artificial neural networks  anns 
have been implemented  and a challenge for better symbolic models has been posed  in
this paper  we present a general purpose symbolic pattern associator  spa  based upon
the decision tree learning algorithm id   we conduct extensive head to head comparisons
on the generalization ability between ann models and the spa under different representations  we conclude that the spa generalizes the past tense of unseen verbs better than
ann models by a wide margin  and we offer insights as to why this should be the case 
we also discuss a new default strategy for decision tree learning algorithms 

   introduction
learning the past tense of english verbs  a seemingly minor aspect of language acquisition 
has generated heated debates since the first connectionist implementation in       rumelhart   mcclelland         based on their results  rumelhart and mcclelland claimed that
the use and acquisition of human knowledge of language can best be formulated by ann
 artificial neural network  models without symbol processing that postulates the existence
of explicit symbolic representation and rules  since then  learning the past tense has become a landmark task for testing the adequacy of cognitive modeling  over the years a
number of criticisms of connectionist modeling appeared  pinker   prince        lachter  
bever        prasada   pinker        ling  cherwenka    marinov         these criticisms
centered mainly upon the issues of high error rates and low reliability of the experimental results  the inappropriateness of the training and testing procedures   hidden  features of the
representation and the network architecture that facilitate learning  as well as the opaque
knowledge representation of the networks  several subsequent attempts at improving the
original results with new ann models have been made  plunkett   marchman        cottrell   plunkett        macwhinney   leinbach        daugherty   seidenberg        
most notably  macwhinney and leinbach        constructed a multilayer neural network
with backpropagation  bp   and attempted to answer early criticisms  on the other hand 
supporters of the symbolic approach believe that symbol structures such as parse trees 
propositions  etc   and the rules for their manipulations  are critical at the cognitive level 
while the connectionist approach may only provide an account of the neural structures
in which the traditional symbol processing cognitive architecture is implemented  fodor
  pylyshyn         pinker        and prasada and pinker        argue that a proper

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

filing

accounting for regular verbs should be dependent upon production rules  while irregular
past tense inections may be generalized by ann like associative memory 
the proper way of debating the adequacy of symbolic and connectionist modeling is by
contrasting competitive implementations  thus  a symbolic implementation is needed that
can be compared with the ann models  this is  in fact  a challenge posed by macwhinney
and leinbach         who assert that no symbolic methods would work as well as their own
model  in a section titled  is there a better symbolic model   they claim 
if there were some other approach that provided an even more accurate
characterization of the learning process  we might still be forced to reject the
connectionist approach  despite its successes  the proper way of debating conceptualizations is by contrasting competitive implementations  to do this in the
present case  we would need a symbolic implementation that could be contrasted
with the current implementation   macwhinney   leinbach        page     
in this paper  we present a general purpose symbolic pattern associator  spa  based
upon the symbolic decision tree learning algorithm id   quinlan         we have shown
 ling   marinov        that the spa s results are much more psychologically realistic than
ann models when compared with human subjects  on the issue of the predictive accuracy 
macwhinney and leinbach        did not report important results of their model on unseen
regular verbs  to reply to our criticism  macwhinney        re implemented the ann
model  and claimed that its raw generalization power is very close to that of our spa  he
believed that this should be the case because both systems learn from the same data set 
there is a very good reason for the equivalent performance of these two
models        when two computationally powerful systems are given the same
set of input data  they both extract every bit of data regularity from that input 
without any further processing  there is only so much blood that can be squeezed
out of a turnip  and each of our systems  spa and ann  extracted what they
could   macwhinney        page     
we will show that this is not the case  obviously there are reasons why one learning
algorithm outperforms another  otherwise why do we study different learning algorithms   
the occam s razor principle   preferring the simplest hypothesis over more complex
ones   creates preference biases for learning algorithms  a preference bias is a preference
order among competitive hypotheses in the hypothesis space  different learning algorithms 
however  employ different ways of measuring simplicity  and thus concepts that they bias
to are different  how well a learning program generalizes depends upon the degree to which
the regularity of the data fits with its bias  we study and compare the raw generalization
ability of symbolic and ann models on the task of learning the past tense of english
verbs  we perform extensive head to head comparisons between ann and spa  and show
the effects of different representations and encodings on their generalization abilities  our
experimental results demonstrate clearly that
   the distributed representation  a feature that connectionists have been advocating 
does not lead to better generalization when compared with the symbolic representation  or with arbitrary error correcting codes of a proper length 
   

filearning the past tense  symbolic vs connectionist models

   anns cannot learn the identity mapping that preserves the verb stem in the past
tense as well as the spa can 
   a new representation suggested by macwhinney        improves the predictive accuracy of both spa and ann  but spa still outperforms ann models 
   in sum  the spa generalizes the past tense of unseen verbs better than ann models
by a wide margin 
in section   we discuss reasons as to why the spa is a better learning model for the
the task of english past tense acquisition  our results support the view that many such
rule governed cognitive processes should be better modeled by symbolic  rather than connectionist  systems 

   review of previous work
in this section  we review briey the two main connectionist models of learning the past
tenses of english verbs  and the subsequent criticisms 

    rumelhart and mcclelland s model

rumelhart and mcclelland s model is based on a simple perceptron based pattern associator interfaced with an input output encoding decoding network which allows the model
to associate verb stems with their past tenses using a special wickelphone wickelfeature
phoneme representation format  the learning algorithm is the classical perceptron convergence procedure  the training and the testing sets are mutually disjoint in the experiments 
the errors made by the model during the training process broadly follow the u shaped learning curve in the stages of acquisition of the english past tense exhibited by young children 
the testing sample consists of     unseen  low frequency verbs     irregular and    regular 
that are not randomly chosen  the testing sample results have a     error rate for the
irregulars  the regulars fare better with a       error rate  thus  the overall error rate for
the whole testing sample is          wrong or ambiguous past tense forms out of    tested 
rumelhart and mcclelland        claim that the outcome of their experiment disconfirms
the view that there exist explicit  though inaccessible  rules that underlie human knowledge
of language 

    macwhinney and leinbach s model

macwhinney and leinbach        report a new connectionist model on the learning of the
past tenses of english verbs  they claim that the results from the new simulation are far
superior to rumelhart and mcclelland s results  and that they can answer most of the criticisms aimed at the earlier model  the major departure from rumelhart and mcclelland s
model is that the wickelphone wickelfeature representational format is replaced with the
unibet  macwhinney        phoneme representational system which allows the assignment of a single alphabetic numerical letter to each of the total    phonemes  macwhinney
and leinbach use special templates with which to code each phoneme and its position in a
word  the actual input to the network is created by coding the individual phonemes as sets
   

filing

of phonetic features in a way similar to the coding of wickelphones as wickelfeatures  cf
section       the network has two layers of      hidden  units fully connected to adjacent
layers  this number was arrived at through trial and error  in addition  the network has a
special purpose set of connections that copy the input units directly onto the output units 
altogether       regular and irregular english verbs are selected for the experiment
       of them are used for training       regular and     irregular   but only    low
frequency irregular verbs are used for testing  macwhinney   leinbach        page      
training the network takes        epochs  at the end of training there still are    errors
on the irregular pasts  macwhinney and leinbach believe that if they allow the network
to run for several additional days and give it additional hidden unit resources  it probably
can reach complete convergence  macwhinney   leinbach        page       the only
testing error rate reported is based on a very small and biased test sample of    unseen
irregular verbs    out of    are predicted incorrectly  they do not test their model on any
of the unseen regular verbs   unfortunately  we did not test a similar set of    regulars  
 macwhinney   leinbach        page      

    criticism of the connectionist models

previous and current criticisms of the connectionist models of learning the past tenses of
english verbs center mainly on several issues  each of these issues is summarized in the
following subsections 

      error rates

the error rate in producing the past tenses of the  unseen  test verbs is very high in
both ann models  and important tests were not carried out in macwhinney and leinbach
       model  the experimental results indicate that neither model reaches the level of
adult competence  in addition  relatively large numbers of the errors are not psychologically
realistic since humans rarely make them 

      training and testing procedures

in both rumelhart and mcclelland s model  and macwhinney and leinbach s model  the
generalization ability is measured on only one training testing sample  further  the testing
sets are not randomly chosen  and they are very small  the accuracy in testing irregular
verbs can vary greatly depending upon the particular set of testing verbs chosen  and thus
multiple runs with large testing samples are necessary to assess the true generalization
ability of a learning model  therefore  the results of the previous connectionist models are
not reliable  in section    we set up a reliable testing procedure to compare connectionist
models with our symbolic approach  previous connectionist simulations have also been
criticized for their crude training processes  for example  the sudden increase of regular
verbs in the training set   which create such behavior as the u shaped learning curves 

      data representation and network architecture

most of the past criticisms of the connectionist models have been aimed at the datarepresentation formats employed in the simulations  lachter and bever        pointed
   

filearning the past tense  symbolic vs connectionist models

out that the results achieved by rumelhart and mcclelland s model would have been impossible without the use of several trics  the representations it crucially supposes 
introduced with the adoption of the wickelphone wickelfeature representational format 
macwhinney and leinbach claim that they have improved upon the earlier connectionist
model by getting rid of the wickelphone wickelfeature representation format  and thus
to have responded to the many criticisms that this format entailed  however  macwhinney and leinbach also introduce several trics in their data representation format  for
example  instead of coding predecessor and successor phonemes as wickelphones  they introduce special templates with which to code positional information  this means that the
network will learn to associate patterns of phoneme positions within a predetermined consonant vowel pattern  further  the use of restrictive templates gets rid of many english
verbs that do not fit the chosen template  this may bias the model in favour of shorter
verbs  predominantly of anglo saxon origin  and against longer verbs  predominantly composite or of latin and french origin  another trics introduced is the phonetic feature
encoding  a distributed representation   it is not clear why phonetic features such as front 
centre  back  high  etc  are chosen  do they represent finer grained  microfeatures  that
help to capture the regularities in english past tenses  in section      we will show that
the straightforward symbolic representation leads to better generalization than does the
carefully engineered distributed representation  this undermines the claimed advantages of
the distributed representation of connectionist models 

      knowledge representation and integration of acquired knowledge
pinker and prince         and lachter and bever        point out that rumelhart and
mcclelland try to model the acquisition of the production of the past tense in isolation
from the rest of the english morphological system  rumelhart and mcclelland  as well
as macwhinney and leinbach  assume that the acquisition process establishes a direct
mapping from the phonetic representation of the stem to the phonetic representation of
the past tense form  this direct mapping collapses some well established distinctions such
as lexical item vs  phoneme string  and morphological category vs  morpheme  simply
remaining at the level of phonetic patterns  it is impossible to express new categorical
information in first order  predicate function variable  format  one of the inherent deficits
of the connectionist implementations is that there is no such thing as a variable for verb
stem  and hence there is no way for the model to attain the knowledge that one could
add sux to a stem to get its past tense  pinker   prince        page       since the
acquired knowledge in such networks is a large weight matrix  which usually is opaque to the
human observer  it is unclear how the phonological levels processing that the connectionist
models carry out can be integrated with the morphological  lexical  and syntactical level
of processing  neither rumelhart and mcclelland nor macwhinney and leinbach address
this issue  in contrast to anns whose internal representations are entirely opaque  the
spa can represent the acquired knowledge in the form of production rules  and allow for
further processing  resulting in higher level categories such as the verb stem and the voiced
consonants  linguistically realistic production rules using these new categories for regular
verbs  and associative templates for irregular verbs  ling   marinov        
   

filing

   the symbolic pattern associator

we take up macwhinney and leinbach s challenge for a better symbolic model for learning
the past tense of english verbs  and present a general purpose symbolic pattern associator
 spa   that can generalize the past tense of unseen verbs much more accurately than
connectionist models in this section  our model is symbolic for several reasons  first 
the input output representation of the learning program is a set of phoneme symbols 
which are the basic elements governing the past tense inection  second  the learning
program operates on those phoneme symbols directly  and the acquired knowledge can be
represented in the form of production rules using those phoneme symbols as well  third 
those production rules at the phonological level can easily be further generalized into firstorder rules that use more abstract  high level symbolic categories such as morphemes and
the verb stem  ling   marinov         in contrast  the connectionist models operate
on a distributed representation  phonetic feature vectors   and the acquired knowledge is
embedded in a large weight matrix  it is therefore hard to see how this knowledge can be
further generalized into more abstract representations and categories 

    the architecture of the symbolic pattern associator

the spa is based on c     quinlan        which is an improved implementation of the id 
learning algorithm  cf   quinlan          id  is a program for inducing classification rules in
the form of decision trees from a set of classified examples  it uses information gain ratio as
a criterion for selecting attributes as roots of the subtrees  the divide and conquer strategy
is recursively applied in building subtrees until all remaining examples in the training set
belong to a single concept  class   then a leaf is labeled as that concept  the information
gain guides a greedy heuristic search for the locally most relevant or discriminating attribute
that maximally reduces the entropy  randomness  in the divided set of the examples  the
use of this heuristic usually results in building small decision trees instead of larger ones
that also fit the training data 
if the task is to learn to classify a set of different patterns into a single class of several
mutually exclusive categories  id  has been shown to be comparable with neural networks
 i e   within about    range on the predictive accuracy  on many real world learning tasks
 cf   shavlik  mooney    towell        feng  king  sutherland    henery        ripley 
      weiss   kulikowski          however  if the task is to classify a set of  input  patterns
into  output  patterns of many attributes  id  cannot be applied directly  the reason is
that if id  treats the different output patterns as mutually exclusive classes  the number of
classes would be exponentially large and  more importantly  any generalization of individual
output attributes within the output patterns would be lost 
to turn id  or any similar n to   classification system into general purpose n to m
symbolic pattern associators  the spa applies id  on all output attributes and combines
individual decision trees into a  forest   or set of trees  a similar approach was proposed for
dealing with the distributed  binary  encoding in multiclass learning tasks such as nettalk
 english text to speech mapping   dietterich  hild    bakiri         each tree takes as
input the set of all attributes in the input patterns  and is used to determine the value of
   the spa programs and relevant datasets can be obtained anonymously from ftp csd uwo ca under
pub spa   

   

filearning the past tense  symbolic vs connectionist models

one attribute in its output pattern  more specifically  if a pair of input attributes    to n  
and output attributes     to  m   is represented as 

        n             m

then the spa will build a total of m decision trees  one for each output attribute  i    
i  m  taking all input attributes         n per tree  once all of m trees are built  the spa
can use them jointly to determine the output pattern            m from any input pattern
        n 

an important feature of the spa is explicit knowledge representation  decision trees for
output attributes can easily be transformed into propositional production rules  quinlan 
       since entities of these rules are symbols with semantic meanings  the acquired
knowledge often is comprehensible to the human observer  in addition  further processing
and integration of these rules can yield high level knowledge  e g   rules using verb stems 
 ling   marinov         another feature of the spa is that the trees for different output
attributes contain identical components  branches and subtrees   ling   marinov        
these components have similar roles as hidden units in anns since they are shared in the
decision trees of more than one output attribute  these identical components can also be
viewed as high level concepts or feature combinations created by the learning program 

    default strategies

an interesting research issue is how decision tree learning algorithms handle the default
class  a default class is the class to be assigned to leaves which no training examples are
classified into  we call these leaves empty leaves  this happens when the attributes have
many different values  or when the training set is relatively small  in these cases  during the
tree construction  only a few branches are explored for some attributes  when the testing
examples fall into the empty leaves  a default strategy is needed to assign classes to those
empty leaves 
for easier understanding  we use the spelling form of verbs in this subsection to explain
how different default strategies work   in the actual learning experiment the verbs are
represented in phonetic form   if we use consecutive left to right alphabetic representation 
the verb stems and their past tenses of a small training set can be represented as follows 
a f f o r d                  
e a t                        
l a u n c h                  
l e a v e                    

  
  
  
  

a f f o r d e d              
a t e                        
l a u n c h e d              
l e f t                      

where is used as a filler for empty space  the left hand    columns are the input patterns
for the stems of the verbs  the right hand    columns are the output patterns for their
corresponding correct past tense forms 
as we have discussed     decision trees will be constructed  one for each output attribute 
the decision tree for the first output attribute can be constructed  see figure    a   from
the following   examples 
a f f o r d                      a
e a t                            a
l a u n c h                      l

   

filing

l e a v e                        l

where the last column is the classification of the first output attribute  however  many
other branches  such as     c in figure    a   are not explored  since no training example
has that attribute value  if a testing pattern has its first input attribute equal to c  what
class should it be assigned to  id  uses the majority default  that is  the most popular
class in the whole subtree under   is assigned to the empty leaves  in the example above 
either class a or l will be chosen since they each have   training examples  however  this is
clearly not the right strategy for this task since a verb such as create would be output as
l       or a        which is incorrect  because it is unlikely for a small training set to have all
variations of attribute values  the majority default strategy of id  is not appropriate for
this task 

 
a

e

 
l

a

c

z

   passthrough

  

d

 
a  

a  

l  

c  

o  

z  

d   
p

x n indicates that there are n examples
classified in the leaf labelled as x 
x    boxed  indicates the empty leaves 

t   

figure     a  passthrough default

   majority

r

d  

l

d  

k

t  

 b  various default

for applications such as verb past tense learning  a new default heuristic   passthrough
  may be more suitable  that is  the classification of an empty leaf should be the same
as the attribute value of that branch  for example  using the passthrough default strategy 
create will be output as c        the passthrough strategy gives decision trees some first order
avor  since the production rules for empty leaves can be represented as if attribute   x
then class   x where x can be any unused attribute values  passthrough is a domaindependent heuristic strategy because the class labels may have nothing to do with the
attribute values in other applications 
applying the passthrough strategy alone  however  is not adequate for every output
attribute  the endings of the regular past tenses are not identical to any of the input
patterns  and the irregular verbs may have vowel and consonant changes in the middle of
the verbs  in these cases  the majority default may be more suitable than the passthrough 
in order to choose the right default strategy   majority or passthrough   a decision is
made based upon the training data in the corresponding subtree  the spa first determines
the majority class  and counts the number of examples from all subtrees that belong to
this class  it then counts the number of examples in the subtrees that coincide with the
   

filearning the past tense  symbolic vs connectionist models

passthrough strategy  these two numbers are compared  and the default strategy employed
by more examples is chosen  for instance  in the example above  see figure    a    the
majority class is l  or a  having   instances  however  there are   examples coinciding with
the passthrough default  two l and one a  thus the passthrough strategy takes over  and
assigns all empty leaves at this level  the empty attribute branch c would then be assigned
the class c  note that the default strategy for empty leaves of attribute x depends upon
training examples falling into the subtree rooted at x   this localized method ensures that
only related objects have an inuence on calculating default classes  as a result  the spa
can adapt the default strategy that is best suited at different levels of the decision trees  for
example  in figure    b   two different default strategies are used at different levels in the
same tree  we use the spa with the adaptive default strategy throughout the remainder of
this paper  note that the new default strategy is not a trics in the data representation 
rather  it represents a bias of the learning program  any learning algorithm has a default
strategy independent of the data representation  the effect of different data representations
on generalization is discussed in sections           and      the passthrough strategy can
be imposed on anns as well by adding a set of copy connections between the input units
and the twin output units  see section     for detail 

    comparisons of default strategies of id   spa  and ann
which default strategy do neural networks tend to take in generalizing default classes
when compared with id  and spa  we conducted several experiments to determine neural
networks  default strategy  we assume that the domain has only one attribute x which
may take values a  b  c  and d  the class also can be one of the a  b  c  and d  the training
examples have attribute values a  b  and c but not d   it is reserved for testing the default
class  the training set contains multiple copies of the same example to form a certain
majority class  table   shows two sets of training testing examples that we used to test
and compare default strategies of id   spa and neural networks 
data set  
data set  
training examples
training examples
values of x class   of copies values of x class   of copies
a
a
  
a
c
  
b
b
 
b
b
 
c
c
 
c
c
 
testing example
testing example
d
 
 
d
 
 
table    two data sets for testing default strategies of various methods 
the classification of the testing examples by id  and spa is quite easy to decide  since
id  takes only the majority default  the output class is a  with    training examples  for
the first data set  and c  with    training examples  for the second data set  for spa  the
number of examples using passthrough is    for the first data set  and    for the second
   

filing

data set  therefore  the passthrough strategy wins in the first case with the output class
d  and the majority strategy wins in the second case with the output class c 
for neural networks  various coding methods were used to represent values of the attribute x   in the dense coding  we used    to represent a     for b     for c and    for
d  we also tried the standard one per class encoding  and real number encoding      for a 
    for b      for c and     for d   the networks were trained using as few hidden units as
possible in each case  we found that in most cases the classification of the testing example is not stable  it varies with different random seeds that initialize the networks  table
  summarises the experimental results  for anns  various classifications obtained by   
different random seeds are listed with the first ones occurring most frequently  it seems
that not only do neural networks not have a consistent default strategy  but also that it
is neither the majority default as in id  nor the passthrough default as in spa  this may
explain why connectionist models cannot generalize unseen regular verbs well even when
the training set contains only regular verbs  see section       the networks have diculty
 or are underconstrained  in generalizing the identity mapping that copies the attributes of
the verb stems into the past tenses 
the classification for the testing example
data set   data set  
id 
a
c
spa
d
c
ann  dense coding
b  c
b
ann  one per class
b  c  a
c  b
ann  real numbers
c  d
d  c
table    default strategies of id   spa and ann on two data sets 

   head to head comparisons between symbolic and ann models

in this section  we perform a series of extensive head to head comparisons using several
different representations and encoding methods  and demonstrate that the spa generalizes
the past tense of unseen verbs better than ann models do by a wide margin 

    format of the data

our verb set came from macwhinney s original list of verbs  the set contains about
     stem past tense pairs  learning is based upon the phonological unibet representation  macwhinney         in which different phonemes are represented by different
alphabetic numerical letters  there is a total of    phonemes  the source file is transferred
into the standard format of pairs of input and output patterns  for example  the verbs in
table   are represented as pairs of input and output patterns  verb stem    past tense  
  b   n d   n
  
  b   n d   n d
i k s e l   r e t    i k s e l   r e t i d

   

filearning the past tense  symbolic vs connectionist models

  r   z      r o z
b i k   m    b i k e m

see table    the original verb set is available in online appendix     we keep only one
form of the past tense among multiple past tenses  such as hang hanged and hang hung 
in the data set  in addition  no homophones exist in the original data set  consequently 
there is no noise  contradictory data which have the same input pattern but different output
patterns  in the training and testing examples  note also that information as to whether
the verb is regular or irregular is not provided in training testing processes 
base  stem 
unibet
b base
    irregular
spelling form phonetic form d  past tense     regular
abandon
 b nd n
b
 
abandoned
 b nd nd
d
 
benefit
ben fit
b
 
benefited
ben fitid
d
 
arise
 r z
b
 
arose
 roz
d
 
become
bik m
b
 
became
bikem
d
 
      
table    source file from macwhinney and leinbach 

    experiment setup

to guarantee unbiased and reliable comparison results  we use training and testing samples
randomly drawn in several independent runs  both spa and ann are provided with the
same sets of training testing examples for each run  this allows us to achieve a reliable
estimate of the inductive generalization capabilities of each model on this task 
the neural network program we used is a package called xerion  which was developed
at the university of toronto  it has several more sophisticated search mechanisms than
the standard steepest gradient descent method with momentum  we found that training
with the conjugate gradient method is much faster than with the standard backpropagation
algorithm  using the conjugate gradient method also avoids the need to search for proper
settings of parameters such as the learning rate  however  we do need to determine the
proper number of hidden units  in the experiments with anns  we first tried various
numbers of hidden units and chose the one that produced the best predictive accuracy in
a trial run  and then use the network with that number of hidden units in the actual runs 
the spa  on the other hand  has no parameters to adjust 
one major difference in implementation between anns and spa is that spa can take
 symbolic  phoneme letters directly while anns normally encode each phoneme letter to
binary bits   of course  spa also can apply to the binary representation   we studied
various binary encoding methods and compared results with spa using symbolic letter
   

filing

representation  since outputs of neural networks are real numbers  we need to decode the
network outputs back to phoneme letters  we used the standard method of decoding  the
phoneme letter that has the minimal real number hamming distance  smallest angle  with
the network outputs was chosen  to see how binary encoding affects the generalization 
the spa was also trained with the binary representation  since the spa s outputs are
binary  the decoding process may tie with several phoneme letters  in this case  one of
them is chosen randomly  this reects the probability of the correct decoding at the level
of phoneme letters  when all of the phoneme letters are decoded  if one or more letters are
incorrect  the whole pattern is counted as incorrect at the word level 

    templated  distributed representation

this set of experiments was conducted using the distributed representation suggested by
macwhinney and leinbach         according to macwhinney and leinbach  the output is
a left justified template in the format of cccvvcccvvcccvvccc  where c stands for
consonant and v for vowel space holders  the input has two components  a left justified
template in the same format as the input  and a right justified template in the format of
vvccc  for example  the verb bet  represented in unibet coding as bet  is shown in the
template format as follows   is the blank phoneme  
input
bet
template 
output
bet
template 

b  e t            
cccvvcccvvcccvvccc
 left justified 

 e  t
vvccc
 right justified 

b  e t            
cccvvcccvvcccvvccc
 left justified 

a specific distributed representation   a set of  binary  phonetic features   is used
to encode all phoneme letters for the connectionist networks  each vowel  v in the above
templates  is encoded by   phonetic features  front  centre  back  high  low  middle  round 
and diphthong  and each consonant  c in the above templates  by    phonetic features
 voiced  labial  dental  palatal  velar  nasal  liquid  trill  fricative and interdental   note
that because the two feature sets of vowels and consonants are not identical  templates are
needed in order to decode the right type of the phoneme letters from the outputs of the
network 
in our experimental comparison  we decided not to use the right justified template
 vvccc  since this information is redundant  therefore  we used only the left justified
template  cccvvcccvvcccvvccc  in both input and output   the whole verb set
in the templated phoneme representation is available in online appendix    it contains
     pairs of verb stems and past tenses that fit the template   to ease implementation 
we added two extra features that always were assigned to   in the vowel phonetic feature
set  therefore  both vowels and consonants were encoded by    binary bits  the ann
thus had              input bits and     output bits  and we found that one layer of    
hidden units  same as macwhinney        model  reached the highest predictive accuracy
in a trial run  see figure   for the network architecture used 
   

filearning the past tense  symbolic vs connectionist models

     output units 

c

   

   

   

   

      

   

   

   

c

c

v

v

cccvvcccvv

c

c

c

 full connection between the two layers 
      

     hidden units 

      

 full connection between the two layers 

c

   

   

   

   

      

   

   

   

c

c

v

v

cccvvcccvv

c

c

c

     input units 

figure    the architecture of the network used in the experiment 
the spa was trained and tested on the same data sets but with phoneme letters directly 
that is     decision trees were built for each of the phoneme letters in the output templates 
to see how phonetic feature encoding affects the generalization  we also trained the spa
with the the same distributed representation   binary bit patterns of     input bits and
    output bits   exactly the same as those in the ann simulation  in addition  to see how
the  symbolic  encoding works in ann  we also train another neural network  with    
hidden units  with the  one per class  encoding  that is  each phoneme letter  total of    
   phoneme letters plus one for blank  is encoded by    bits  one for each phoneme letter 
we used     verb pairs  including both regular and irregular verbs  in the training and
testing sets  sampling was done randomly without replacement  and training and testing
sets were disjoint  three runs of spa and ann were conducted  and both spa and ann
were trained and tested on the same data set in each run  training reached      accuracy
for spa and around     for ann 
testing accuracy on novel verbs produced some interesting results  the ann model
and the spa using the distributed representation have very similar accuracy  with ann
slightly better  this may well be caused by the binary outputs of spa that suppress the
fine differences in prediction  on the other hand  the spa using phoneme letters directly
produces much higher accuracy on testing  the spa outperforms neural networks  with
either distributed or one per class representations  by    percentage points  the testing
results of ann and spa can be found in table    our findings clearly indicate that the
spa using symbolic representation leads to much better generalization than ann models 

    learning regular verbs

predicting the past tense of an unseen verb  which can be either regular or irregular  is
not an easy task  irregular verbs are not learned by rote as traditionally thought since
   

filing

distributed representation
ann    correct
spa    correct
reg irrg comb reg irrg comb
                             
                           
                            
                             

symbolic representation
ann    correct
spa    correct
reg irrg comb reg irrg comb
                             
                             
                             
                             

table    comparisons of testing accuracy of spa and ann with distributed and symbolic
representations 
children and adults occasionally extend irregular inection to irregular sounding regular
verbs or pseudo verbs  such as cleef   cleft   prasada   pinker         the more similar
the novel verb is to the cluster of irregular verbs with similar phonological patterns  the
more likely the prediction of an irregular past tense form  pinker        and prasada and
pinker        argue that regular past tenses are governed by rules  while irregulars may
be generated by the associated memory which has this graded effect of irregular past tense
generalization  it is would be interesting  therefore  to compare spa and ann on the
past tense generalization of regular verbs only  because both spa and ann use the same 
position specific  representation  learning regular past tenses would require learning different
suxes  at different positions  and to learn the identity mapping that copies the verb stem
to the past tenses for verbs of different lengths 
we used the same templated representation as in the previous section  but both training
and testing sets contained only regular verbs  again samples were drawn randomly without
replacement  to maximize the size of the testing sets  testing sets simply consisted of all
regular verbs that were not sampled in the training sets  the same training and testing sets
were used for each of the following methods compared  to see the effect of the adaptive
default strategy  as discussed in section      on generalization  the spa with the majority
default only and with the adaptive default were both tested  the ann models were similar
to those used in the previous section  except with     one layer hidden units  which turned
out to have the best predictive accuracy in a test run   the passthrough default strategy can
be imposed on neural networks by adding a set of copy connections that connect directly
from the input units to the twin output units  macwhinney and leinbach        used
such copy connections in their simulation  we therefore tested the networks with the copy
connection to see if generalization would be improved as well 
the results on the predictive accuracy of the spa and anns on one run with with
randomly sampled training and testing sets are summarized in table    as we can see 
the spa with the adaptive default strategy  which combines the majority and passthrough
default  outperforms the spa with only the majority default strategy used in id   the
   in phonological form there are three different suxes for regular verbs  when the verb stem ends with
t or d  unibet phonetic representations   then the sux is id  for example  extend   extended  in
spelling form   when the verb stem ends with a unvoiced consonant  the sux is t  for example  talk
  talked  when the verb stem ends with a voiced consonant or vowel  the sux is d  for example 
solve   solved 

   

filearning the past tense  symbolic vs connectionist models

anns with copy connections do generalize better than the ones without  however  even
ann models with copy connections have a lower predictive accuracy than the spa  majority   in addition  the differences in the predictive accuracy are larger with smaller sets
of training examples  smaller training sets make the difference in testing accuracy more
evident  when the training set contains      patterns  out of        the testing accuracy
becomes very similar  and would approach asymptotically to      with larger training sets 
upon examination  most of the errors made in ann models occur in the identity mapping
 i e   strange phoneme change and drop   the verb stems cannot be preserved in the past
tense if the phonemes are not previously seen in the training examples  this contradicts the
findings of prasada and pinker         which show that native english speakers generate
regular sux adding past tenses equally well with unfamiliar sounding verb stems  as long
as these verb stems do not sound close to irregular verbs   this also indicates that the bias
of the ann learning algorithms is not suitable to this type of task  see further discussion
in section   
training
percent correct on testing
size
spa  adaptive  spa  majority  ann  copy con   ann  normal 
  
    
    
    
   
   
    
    
    
    
   
    
    
    
    
   
    
    
    
    
    
    
    
    
    
table    predictive accuracy on learning the past tense of regular verbs

    error correcting codes
dietterich and bakiri        reported an increase in the predictive accuracy when errorcorrecting codes of large hamming distances are used to encode values of the attributes 
this is because codes with larger hamming distance  d  allow for correcting fewer than d  
bits of errors  thus  learning programs are allowed to make some mistakes at the bit level
without their outputs being misinterpreted at the word level 
we wanted to find if performances of the spa and anns are improved with the errorcorrecting codes encoding all of the    phonemes  we chose error correcting codes ranging
from ones with small hamming distance to ones with very large hamming distance  using
the bhc codes  see dietterich and bakiri          because the number of attributes for
each phoneme is too large  the data representation was changed slightly for this experiment 
instead of    phoneme holders with templates    consecutive  left to right phoneme holders
were used  verbs with stems or past tenses of more than   phonemes were removed from the
training testing sets   the whole verb set in the this representation is available in online
appendix    it contains      pairs of verb stems and past tenses whose lengths are shorter
than     both spa and ann take exactly the same training testing sets  each contains    
pairs of verb stems and past tenses  with the error correcting codes encoding each phoneme
   

filing

letter  still  training networks with    bit or longer error correcting codes takes too long to
run  there are             input attributes and     output attributes   therefore  only
two runs with     and    bit codes were conducted  consistent with dietterich and bakiri
       s findings  we found that the testing accuracy generally increases when the hamming
distance increases  however  we also observed that the testing accuracy decreases very
slightly when the codes become too long  the accuracy using    bit codes  with hamming
distance of     reaches the maximum value          which is quite close to the accuracy
        of spa using the direct phoneme letter representation  it seems there is a trade off
between tolerance of errors with large hamming distance and diculty in learning with
longer codes  in addition  we found the testing accuracy of anns to be lower than the one
of spa for both    bit  and    bit error correcting codes  the results are summarized in
table   
ann
hamming distance correct at bit level correct at word level
   bit codes
  
     
     
   bit codes
  
     
     
spa
hamming distance correct at bit level correct at word level
   bit codes
  
     
     
   bit codes
  
     
     
   bit codes
  
     
     
    bit codes
  
     
     
table    comparisons of the testing accuracy of spa and anns with error correcting codes
our results in this and the previous two subsections undermine the advantages of the
distributed representation of anns  a unique feature advocated by connectionists  we have
demonstrated that  in this task  the distributed representation actually does not allow for
adequate generalization  both spa using direct symbolic phoneme letters and spa with
error correcting codes outperform anns with distributed representation by a wide margin 
however  neither phoneme symbols nor bits in the error correcting codes encode  implicitly
or explicitly  any micro features as in the distributed representation  it may be that the
distributed representation used was not optimally designed  nevertheless  straightforward
symbolic format requires little representation engineering compared with the distributed
representation in anns 

    right justified  isolated sux representation

macwhinney and leinbach        did not report important results of the predictive accuracy of their model on unseen regular verbs  in his reply  macwhinney        to our
paper  ling   marinov         macwhinney re implemented the ann model  in his new
implementation        verb stem and past tense pairs were in the training set  among which
     were regular and     were irregular  training took       epochs  and reached     
correct on regulars and     on irregulars  the testing set consisted of    regulars and   
irregulars  the percent correct on testing at epoch       was     for regulars and     for
irregulars  with a combined       on the testing set  macwhinney claimed that the raw
   

filearning the past tense  symbolic vs connectionist models

generalization power of ann model is very close to that of our spa  he believes that this
should be the case simply because both systems were trained on the same data set 
we realize  via private communication  that a new representation used in macwhinney s
recent implementation plays a critical role in the improved performance  in macwhinney s
new representation  the input  for verb stems  is coded by the right justified template
cccvvcccvvcccvvccc  the output contains two parts  a right justified template
that is the same as the one in the input  and a coda in the form of vvccc  the rightjustified template in the output is used to represent the past tense without including the
sux for the regular verbs  the sux of the regular past tense always stays in the coda 
which is isolated from the main  right justified templates  for the irregular past tense  the
coda is left empty  for example  the input and output templated patterns for the past tense
of verbs in table   are represented as 
input
 right justified 
cccvvcccvvcccvvccc
     b    nd   n  
b  e n    f  i t  
          r    z  
     b  i k    m  

output
 right justified 
cccvvcccvvcccvvccc
     b    nd   n  
b  e n    f  i t  
          r  o z  
     b  i k  e m  

 suffix only 
vvccc
  d    for abandon abandoned 
i d    for benefit benefited 
       for arise arose 
       for become became 

such data representation clearly facilitates learning  for the regular verbs  the output
patterns are always identical to the input patterns  in addition  the verb ending phoneme
letters always appear at a few fixed positions  i e   the right most vvccc section in the
input template  due to the right justified  templated representation  furthermore  the sux
always occupies the coda  isolated from the right justified templates 
we performed a series of experiments to see how much improvement we could accomplish using the new representation over macwhinney s recent ann model and over the
left justified representation discussed in section      our spa  with an averaged predictive
accuracy of        outperforms macwhinney s recent ann implementation  with the predictive accuracy of        by a wide margin  in addition  the predictive accuracy is also
improved from an average of       from the left justified representation to       of the
right justified  isolated sux one  see results in table   

   general discussion and conclusions

two factors contribute to the generalization ability of a learning program  the first is
the data representation  and the other is the bias of the learning program  arriving at
the right  optimal  representation is a dicult task  as argued by prasada and pinker
        regular verbs should be represented in a coarse grain in terms of the verb stem
and suxes  while irregular verbs in a finer grain in terms of phonological properties 
admittedly  spa works uniformly at the level of phoneme letters  as anns do  however 
because spa produces simple production rules that use these phoneme letters directly  those
rules can be further generalized to first order rules with new representations such as stems
and the voiced consonants which can be used across the board in other such rule learning
modules  ling   marinov         this is one of the major advantages over ann models 
   

filing

predictive accuracy with right justified  isolated sux representation
spa
macwhinney s ann model
training testing training testing
training testing
       
        
        
run  
    
    
run  
    
    
run  
    
    
average
    
    
      one run 
table    comparisons of testing accuracy of spa and ann  with right justified  isolated
sux representation 
it seems quite conceivable that children acquire these high level concepts such as stems
and voiced consonants through learning noun plurals  verb past tense  verb third person
singular  comparative adjectives  and so on  with a large weight matrix as the result of
learning  it is hard to see how this knowledge can be further generalized in ann models
and shared in other modules 
even with exactly the same data representation  there exist some learning tasks that
symbolic methods such as the spa generalize categorically better than anns  the converse also is true  this fact reects the different inductive biases of the different learning
algorithms  the occam s razor principle   preferring the simplest hypothesis over more
complex ones   creates a preference bias  a preference of choosing certain hypotheses over
others in the hypothesis space  however  different learning algorithms choose different hypotheses because they use different measurements for simplicity  for example  among all
possible decision trees that fit the training examples  id  and spa induce simple decision
trees instead of complicated ones  simple decision trees can be converted to small sets of
production rules  how well a learning algorithm generalizes depends upon the degree to
which the underlying regularities of the target concept fit its bias  in other words  if the
underlying regularities can be represented compactly in the format of hypotheses produced
by the learning algorithm  the data can be generalized well  even with a small set of training
examples  otherwise  if the underlying regularities only have a large hypothesis  but the
algorithm is looking for compact ones  as per the occam s razor principle   the hypotheses inferred will not be accurate  a learning algorithm that searches for hypotheses larger
than necessary  i e   that does not use the occam s razor principle  is normally  underconstrained   it does not know  based on the training examples only  which of the many
competitive hypotheses of the large size should be inferred 
we also can describe the bias of a learning algorithm by looking at how training examples
of different classes are separated in the n dimensional hyperspace where n is the number
of attributes  a decision node in a decision tree forms a hyperplane as described by a
linear function such as x   a  not only are these hyperplanes perpendicular to the axis 
they are also partial space hyperplanes that extend only within the subregion formed by
the hyperplanes of their parents  nodes  likewise  hidden units with a threshold function in
anns can be viewed as forming hyperplanes in the hyperspace  however  unlike the ones
in the decision trees  they need not be perpendicular to any axis  and they are full space
   

filearning the past tense  symbolic vs connectionist models

hyperplanes that extend through the whole space  if id  is applied to the concepts that fit
ann s bias  especially if their hyperplanes are not perpendicular to any axis  then many
zigzag hyperplanes that are perpendicular to axes would be needed to separate different
classes of the examples  hence  a large decision tree would be needed  but this does not fit
id  s bias  similarly  if ann learning algorithms are applied to the concepts that fit id  s
bias  especially if their hyperplanes form many separated  partial space regions  then many
hidden units may be needed for these regions 
another major difference between anns and id  is that anns have a larger variation
and a weaker bias  cf   geman  bienenstock    doursat         than id   many more
boolean functions  e g   linearly separable functions  can fit a small network  e g   one with
no hidden units  than they can a small decision tree  this is sometimes attributed to the
claimed versatility and exibility of anns  they can learn  but not necessarily predict reliably well  many functions  while symbolic methods are brittle  however  it is my belief that
we humans are versatile  not because we have a learning algorithm with a large variation 
but rather because we have a set of strong biased learning algorithms  and we can somehow
search in the bias space and add new members into the set for the new learning tasks  symbolic learning algorithms have clear semantic components and explicit representation  and
thus we can more easily construct strong based algorithms motivated from various specific
learning tasks  the adaptive default strategy in the spa is such an example  on the other
hand  we still largely do not know how to effectively strengthen the bias of anns for many
specific tasks  such as the identity mapping  k term dnf  etc    some techniques  such
as adding copy connections and weight decaying  exist  but their exact effects on biasing
towards classes of functions are not clear 
from our analyses  ling   marinov         the underlying regularities governing the
inection of the past tense of english verbs do form a small set of production rules with
phoneme letters  this is especially so for regular verbs  all the rules are either identity
rules or the sux adding rules  for example  decision trees can be converted into a set of
precedence ordered production rules with more complicated rules  rules with more conditions  listed first  as an example  using consecutive  left to right phonetic representation  a
typical sux adding rule for verb stems with   phoneme letters  such as talk   talked  is 
if     k and       then      t
that is  if the fourth input phoneme is k and the fifth is blank  i e   if we are at a verb
ending  then the fifth output phoneme is t  on the other hand  the identity mapping rules
have only one condition  a typical identity rule looks like 
if     l  then      l
in fact  the passthrough default strategy allows all of the identity mapping rules to be represented in a simple first order format 
if     x  then      x
where x can be any phoneme  clearly  the knowledge of forming regular past tenses can
thus be expressed in simple  conjunctive rules which fit the bias of the spa  id    and
therefore  the spa has a much better generalization ability than the ann models 
to conclude  we have demonstrated  via extensive head to head comparisons  that the
spa has a more realistic and better generalization capacity than anns on learning the
past tense of english verbs  we have argued that symbolic decision tree production rule
learning algorithms should outperform anns  this is because  first  the domain seems to be
   

filing

governed by a compact set of rules  and thus fits the bias of our symbolic learning algorithm 
second  the spa directly manipulates on a representation better than anns do  i e   the
symbolic phoneme letters vs  the distributed representation   and third  the spa is able
to derive high level concepts used throughout english morphology  our results support the
view that many such high level  rule governed cognitive tasks should be better modeled by
symbolic  rather than connectionist  systems 

acknowledgements
i gratefully thank steve pinker for his constant encouragement  and marin marinov  steve
cherwenka and huaqing zeng for discussions and for help in implementing the spa  i
thank brian macwhinney for providing the verb data used in his simulation  discussions
with tom dietterich  dave touretzky and brian macwhinney  as well as comments from
reviewers  have been very helpful  the research is conducted with support from the nserc
research grant and computing facilities from our department 

references

cottrell  g     plunkett  k          using a recurrent net to learn the past tense  in
proceedings of the cognitive science society conference 
daugherty  k     seidenberg  m          beyond rules and exceptions  a connectionist
modeling approach to inectional morphology  in lima  s   ed    the reality of
linguistic rules  john benjamins 
dietterich  t     bakiri  g          error correcting output codes  a general method for
improving multiclass inductive learning programs  in aaai     proceedings of ninth
national conference on artificial intelligence  
dietterich  t   hild  h     bakiri  g          a comparative study of id  and backpropagation for english text to speech mapping  in proceedings of the  th international
conference on machine learning  morgan kaufmann 
feng  c   king  r   sutherland  a     henery  r          comparison of symbolic  statistical and neural network classifiers  manuscript  department of computer science 
university of ottawa 
fodor  j     pylyshyn  z          connectionism and cognitive architecture  a critical
analysis  in pinker  s     mehler  j   eds    connections and symbols  pp         
cambridge  ma  mit press 
geman  s   bienenstock  e     doursat  r          neural networks and the bias variance
dilemma  neural computation            
lachter  j     bever  t          the relation between linguistic structure and associative
theories of language learning   a constructive critique of some connectionist learning
models  in pinker  s     mehler  j   eds    connections and symbols  pp            
cambridge  ma  mit press 
   

filearning the past tense  symbolic vs connectionist models

ling  x   cherwenka  s     marinov  m          a symbolic model for learning the past
tenses of english verbs  in proceedings of ijcai     thirteenth international conference on artificial intelligence   pp             morgan kaufmann publishers 
ling  x     marinov  m          answering the connectionist challenge  a symbolic model
of learning the past tense of english verbs  cognition                  
macwhinney  b          the childes project  tools for analyzing talk  hillsdale  nj 
erlbaum 
macwhinney  b          connections and symbols  closing the gap  cognition         
        
macwhinney  b     leinbach  j          implementations are not conceptualizations  revising the verb model  cognition                
pinker  s          rules of language  science                 
pinker  s     prince  a          on language and connectionism  analysis of a parallel
distributed processing model of language acquisition  in pinker  s     mehler  j 
 eds    connections and symbols  pp            cambridge  ma  mit press 
plunkett  k     marchman  v          u shaped learning and frequency effects in a multilayered perceptron  implications for child language acquisition  cognition          
    
prasada  s     pinker  s          generalization of regular and irregular morphological
patterns  language and cognitive processes                
quinlan  j          induction of decision trees  machine learning                  
quinlan  j          c    programs for machine learning  morgan kaufmann  san mateo 
ca 
ripley  b          statistical aspects of neural networks  invited lectures for semstat
 seminaire europeen de statistique  sandbjerg  denmark        april       
rumelhart  d     mcclelland  j          on learning the past tenses of english verbs 
in rumelhart  d   mcclelland  j     the pdp research group  eds    parallel distributed processing vol    pp             cambridge  ma  mit press 
shavlik  j   mooney  r     towell  g          symbolic and neural learning algorithms  an
experimental comparison  machine learning                   
weiss  s     kulikowski  c          computer systems that learn  classification and prediction methods from statistics  neural networks  machine learning  and expert systems 
morgan kaufmann  san mateo  ca 

   

fi