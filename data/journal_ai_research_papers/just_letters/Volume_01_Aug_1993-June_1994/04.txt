journal of artificial intelligence research                

submitted       published      

the diculties of learning logic programs with cut
francesco bergadano

bergadan di unito it

daniele gunetti
umberto trinchero

gunetti di unito it
trincher di unito it

universita di catania  dipartimento di matematica 
via andrea doria          catania  italy
universita di torino  dipartimento di informatica 
corso svizzera            torino  italy

abstract

as real logic programmers normally use cut      an effective learning procedure for logic
programs should be able to deal with it  because the cut predicate has only a procedural
meaning  clauses containing cut cannot be learned using an extensional evaluation method 
as is done in most learning systems  on the other hand  searching a space of possible
programs  instead of a space of independent clauses  is unfeasible  an alternative solution
is to generate first a candidate base program which covers the positive examples  and then
make it consistent by inserting cut where appropriate  the problem of learning programs
with cut has not been investigated before and this seems to be a natural and reasonable
approach  we generalize this scheme and investigate the diculties that arise  some of the
major shortcomings are actually caused  in general  by the need for intensional evaluation 
as a conclusion  the analysis of this paper suggests  on precise and technical grounds  that
learning cut is dicult  and current induction techniques should probably be restricted to
purely declarative logic languages 

   introduction

much recent research in ai and machine learning is addressing the problem of learning
relations from examples  especially under the title of inductive logic programming  muggleton         one goal of this line of research  although certainly not the only one  is the
inductive synthesis of logic programs  more generally  we are interested in the construction
of program development tools based on machine learning techniques  such techniques now
include ecient algorithms for the induction of logical descriptions of recursive relations 
however  real logic programs contain features that are not purely logical  most notably the
cut     predicate  the problem of learning programs with cut has not been studied before
in inductive logic programming  and this paper analyzes the diculties involved 

    why learn programs with cut 

there are two main motivations for learning logic programs with cut 
   ilp should provide practical tools for developing logic programs  in the context of
some general program development methodology  e g    bergadano      b    as real
size logic programs normally contain cut  learning cut will be important for creating
an integrated software engineering framework 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fibergadano  gunetti    trinchero

   extensive use of cut can make programs sensibly shorter  and the diculty of learning
a given logic program is very much related to its length 
for both of these objectives  we need not only cuts that make the programs more
ecient without changing their input output behavior   green cuts    but also cuts that
eliminate some possible computed results   red cuts    red cuts are sometimes considered
bad programming style  but are often useful  moreover  only the red cuts are effective in
making programs shorter  green cuts are also important  and less controversial  once a
correct program has been inferred via inductive methods  it could be made more ecient
through the insertion of green cuts  either manually or by means of automated program
transformation techniques  lau   clement        

    why standard approaches cannot be used 
most machine learning algorithms generate rules or clauses one at a time and independently
of each other  if a rule is useful  it covers some positive example  and correct  it does not
cover any negative example   then it is added to the description or program which is being
generated  until all positive examples have been covered  this means that we are searching
a space of possible clauses  without backtracking  this is obviously a great advantage  as
programs are sets of clauses  and therefore the space of possible programs is exponentially
larger 
the one principle which allows this simplification of the problem is the extensional
evaluation of possible clauses  used to determine whether a clause c covers an example
e  the fact that a clause c covers an example e is then used as an approximation of the
fact that a logic program containing c derives e  consider  for instance  the clause c  
 p x y  ff   and suppose the example e is p a b   in order to see whether c covers e  the
extensionality principle makes us evaluate any literal in ff as true if and only if it matches
some given positive example  for instance  if ff   q x z    p z y   then the example p a b 
is extensionally covered iff there is a ground term c such that q a c  and p c b  are given
as positive examples  in particular  in order to obtain the truth value of p c b   we will
not need to call other clauses that were learned previously  for this reason  determining
whether c covers e only depends on c and on the positive examples  therefore  the learning
system will decide whether to accept c as part of the final program p independently of the
other clauses p will contain 
the extensionality principle is found in foil  quinlan        and its derivatives  but is
also used in bottom up methods such as golem  muggleton   feng         shapiro s mis
system  shapiro        uses it when refining clauses  although it does not when backtracing
inconsistencies  we have also used an extensional evaluation of clauses in the filp system
 bergadano   gunetti        
when learning programs with cut  clauses are no longer independent and their standalone extensional evaluation is meaningless  when a cut predicate is evaluated  other possible clauses for proving the same goal will be ignored  this changes the meaning of these
other clauses  even if a clause extensionally covers some example e  it may be the case that
the final program does not derive e  because some derivation paths have been eliminated
by the evaluation of a cut predicate 
  

fithe difficulties of learning logic programs with cut

however  an exhaustive search in a space of programs is prohibitive  learning methods 
even if based on extensionality  are often considered inecient if sucient prior information
is not available  searching for sets of clauses will be exponentially worse  this would amount
to a brute force enumeration of all possible logic programs containing cut  until a program
that is consistent with the given examples is found 

    is there an alternative method 

cut will only eliminate some computed results  i e   after adding cut to some program  it
may be the case that some example is no longer derived  this observation suggests a general
learning strategy  a base program p is induced with standard techniques  given the positive
and maybe some of the negative examples  then the remaining negative examples are ruled
out by inserting cut in some clause of p  obviously  after inserting cut  we must make sure
that the positive examples may still be derived 
given the present technology and the discussion above  this seems to be the only viable
path to a possible solution  using standard techniques  the base program p would be generated one clause at a time  so that the positive examples are extensionally covered  however 
we think this view is too restrictive  as there are programs which derive all given positive
examples  although they do not cover them extensionally  bergadano      a  deraedt 
lavrac    dzeroski         more generally  we consider traces of the positive examples 

definition   given a hypothesis space s of possible clauses  and an example e such that s
 

e  the set of clauses ts which is used during the derivation of e is called a trace for e 

we will use as a candidate base program p any subset of s which is the union of some
traces for the positive examples  if ps extensionally covers the positive examples  then it
will also be the union of such traces  but the converse is not always true  after a candidate
program has been generated  an attempt is made to insert cuts so that the negative examples
are not derived  if this is successful  we have a solution  otherwise  we backtrack to another
candidate base program  we will analyze the many problems inherent in learning cut with
this class of trace based learning methods  but  as we discuss later  section     the same
problems need to be faced in the more restrictive framework of extensional evaluation  in
other words  even if we choose to learn the base program p extensionally  and then we
try to make it consistent by using cut  the same computational problems would still arise 
the main difference is that standard approaches based on extensionality do not allow for
backtracking and do not guarantee that a correct solution is found  bergadano      a  
as far as computational complexity is concerned  trace based methods have a complexity
standing between the search in a space of independent clauses  for the extensional methods 
and the exhaustive search in a space of possible programs  we need the following 

definition   given a hypothesis space s  the depth of an example e is the maximum

number of clauses in s successfully used in the derivation of e 

for example  if we are in a list processing domain  and s only contains recursive calls of
the type  p  hjt           p t        then the depth of an example p l  is the length of l 
for practical program induction tasks  it is often the case that the depth of an example is
  

fibergadano  gunetti    trinchero

related to its complexity  and not to the hypothesis space s  if d is the maximum depth for
the given m positive examples  then the complexity of trace based methods is of the order
of js jmd  while extensional methods will just enumerate possible clauses with a complexity
which is linear in js j  and enumerating all possible programs is exponential in js j 

   a simple induction procedure
the trace based induction procedure we analyze here takes as input a finite set of clauses
s and a set of positive and negative examples e  and e  and tries to find a subset t of s
such that t derives all the positive examples and none of the negative examples  for every
positive example e    e   we assume that s is large enough to derive it  moreover  we
assume that all clauses in s are attened    if this is not the case  clauses are attened as a
preprocessing step 
we consider one possible proof for s   e   and we build an intermediate program t  s
containing a trace of the derivation  the same is done for the other positive examples  and
the corresponding traces t are merged  every time t is updated  it is checked against the
negative examples  if some of them are derived from t  cut     is inserted in the antecedents
of the clauses in t  so that a consistent program is found  if it exists  if this is not the case 
the procedure backtracks to a different proof for s   e   the algorithm can be informally
described as follows 
input  a set of clauses s
a set of positive examples e 
a set of negative examples es    atten s 
t  
for each positive example e    e 
find t   s such that t   sld e   backtracking point   
t t   t 
if t derives some negative example e  then trycut t e  
if trycut t e   fails then backtrack
output the clauses listed in t
trycut t e   
insert   somewhere in t  backtracking point    so that
   all previously covered positive examples are still derived from t  and
   t   sld e 

the complexity of adding cut somewhere in the trace t  so that the negative example eis no longer derived  obviously only depends on the size of t  but this size depends on the
depth of the positive examples  not on the size of the hypothesis space s  although more
   a clause is flattened if it does not contain any functional symbol  given an unattened clause  it is alway
possible to atten it  by turning functions into new predicates with an additional argument representing
the result of the function  and vice versa  rouveirol  in press  

  

fithe difficulties of learning logic programs with cut

clever ways of doing this can be devised  based on the particular example e   we propose a
simple enumerative technique in the implementation described in the appendix 

   example  simplifying a list

in this section we show an example of the use of the induction procedure to learn the logic
program  simplify    simplify takes as input a list whose members may be lists  and
transforms it into a  attened  list of single members  containing no repetitions and no
lists as members  this program appears as exercise number    in  coelho   cotta        
is composed of nine clauses  plus the clauses for append and member   six of them are
recursive  one is doubly recursive and cut is extensively used  even if simplify is a not a
very complex logic program  it is more complex than usual ilp test cases  for instance 
the quicksort and partition program  which is very often used  is composed of only five
clauses  plus those for append   and three of them are recursive  moreover  note that the
conciseness of simplify is essentially due to the extensive use of cut  without cut  this
program would be much longer  in general  the longer a logic program  the more dicult
to learn it 
as a consequence  we start with a relatively strong bias  suppose that the following
hypothesis space of n      possible clauses is defined by the user 



the clause  simplify l nl     atten l l    remove l  nl   
all clauses whose head is  atten x l   and whose body is composed of a conjunction
of any of the following literals 
head x h   tail x l    equal x  l  t    null t   null h   null l    equal x  l    
atten h x    atten l  x   
append x  x  l   assign x  l   assign x  l   list x l  



all clauses whose head is  remove il ol   and whose body is composed of a conjunction of any of the following literals 
cons x n ol   null il   assign    ol  
head il x   tail il l   member x l   remove l ol   remove l n  



the correct clauses for null  head  tail  equal  assign  member  append are given 
null     
head  hj   h  
tail   jt  t  
equal x x  
assign x x  
member x  xj    
member x   jt      member x t  
  

fibergadano  gunetti    trinchero

append    z z  
append  hjx  y  hjz      append x y z  
by using various kinds of constraints  the initial number of clauses can be strongly reduced 
possible constraints are the following 
 once an output is produced it must not be instantiated again  this means that any
variable cannot occur as output in the antecedent more than once 
 inputs must be used  all input variables in the head of a clause must also occur in its
antecedent 
 some conjunctions of literals are ruled out because they can never be true  e g 
null il  head il x  
by applying various combination of these constraints it is possible to strongly restrict the
initial hypothesis space  which is then given in input to the learning procedure  the set of
positive and negative examples used in the learning task is 
simplify pos       b a a        b a    remove pos  a a   a   
 simplify neg       b a a       x  not equal x  b a    
simplify neg   a b a       a  b a     remove neg  a a   a a   
note that we define some negative examples of simplify to be all the examples with
the same input of a given positive example and a different output  for instance simplify neg       b a a        a b    obviously  it is also possible to give negative examples as
normal ground literals  the learning procedure outputs the program for simplify reported
below  which turns out to be substantially equivalent to the one described in  coelho  
cotta         we have kept clauses unattened  
simplify l nl     atten l l    remove l  nl  
atten x l     equal x  l  t    null t      atten l  x    assign x  l  
atten x l     head x h   tail x l    null h      atten l  x    assign x  l  
atten x l     equal x  l        atten l  x    assign x  l  
atten x l     head x h   tail x l      
atten h x       atten l  x    append x  x  l  
atten x l     list x l  
remove il ol     head il x   tail il l   member x l      remove l ol  
remove il ol     head il x   tail il l   remove l n   cons x n ol  
remove il ol     null il   assign    ol  
the learning task takes about    seconds on our implementation  however  this is obtained
at some special conditions  which are thoroughly discussed in the next sections 
 all the constraints listed above are applied  so that the final hypothesis space is
reduced to less than one hundred clauses 
  

fithe difficulties of learning logic programs with cut





clauses in the hypothesis space are generated in the correct order  as they must appear
in the final program  moreover  literals in each clause are in the correct position  this
is important  since in a logic program with cut the relative position of clauses and
literals is significant  as a consequence  we can learn simplify without having to test
for different clause and literal orderings  see subsections     and      
we tell the learning procedure to use at most two cuts per clause  this seems to be
quite an intuitive constraint since  in fact  many classical logic programs have no more
than one cut per clause  see subsections     and      

   problems

experiments with the above induction procedure have shown that many problems arise when
learning logic programs containing cut  in the following  we analyze these problems  and
this is a major contribution of the present paper  as cut cannot be evaluated extensionally 
this analysis is general  and does not depend on the specific induction method adopted 
some possible partial solutions will be discussed in section   

    problem    intensional evaluation  backtracking and cut

the learning procedure of section   is very simple  but it can be inecient  however 
we believe this is common to every intensional method  because clauses cannot be learned
independently of one another  as a consequence  backtracking cannot be avoided and this
can have some impact on the complexity of the learning process  moreover  cut must be
added to every trace covering negative examples  if no constraints are in force  we can
range from only one cut in the whole trace to a cut between each two literals of each clause
in the trace  clearly  the number of possibilities is exponential in the number of literals in
the trace  fortunately  this number is usually much smaller than the size of the hypothesis
space  as it depends on the depth of the positive examples 
however  backtracking also has some advantages  in particular  it can be useful to search
for alternative solutions  these alternative programs can then be confronted on the basis of
any required characteristic  such as simplicity or eciency  for example  using backtracking
we discovered a version of simplify equivalent to the one given but without the cut predicate
between the two recursive calls of the fourth clause of flatten 

    problem    ordering of clauses in the trace

in a logic program containing cut  the mutual position of clauses is significant  and a different ordering can lead to a different  perhaps wrong  behavior of the program  for example 
the following program for intersection 

c   int x s  y     null x   null y  
c   int x s  y     head x h   tail x tail   member h s       int tail s  s   cons h s y  
c   int x s  y     head x h   tail x tail   int tail s  y  
behaves correctly only if c  comes before c   suppose the hypothesis space given in input
to the induction procedure consists of the same three clauses as above  but with c  before
  

fibergadano  gunetti    trinchero

c   if  int  a   a      is given as a negative example  then the learning task fails  because
clauses c  and c  derive that example 

in other words  learning a program containing cut means not only to learn a set of
clauses  but also a specific ordering for those clauses  in terms of our induction procedure
this means that for every trace t covering some negative example  we must check not only
every position for inserting cuts  but also every possible clause ordering in the trace  this
 generate and test  behavior is not dicult to implement  but it can dramatically decrease
the performance of the learning task  in the worst case all possible permutations must be
generated and checked  and this requires a time proportional to  md   for a trace of md
clauses   
the necessity to test for different permutations of clauses in a trace is a primary source
of ineciency when learning programs with cut  and probably the most dicult problem
to solve 

    problem    kinds of given examples

our induction procedure is only able to learn programs which are traces  i e  where every
clause in the program is used to derive at least one positive example  when learning definite
clauses  this is not a problem  because derivation is monotone  and for every program p 
complete and consistent w r t  the given examples  there is a program p p which is also
complete and consistent and is a trace   on the other hand  when learning clauses containing cut  it may happen that the only complete and consistent program s  in the hypothesis
space is neither a trace  nor contains it as a subset  this is because derivation is no longer
monotone and it can be the case that a negative example is derived by a set of clauses  but
not by a superset of them  as in the following simple example 
s   fsum a b c     a       m is a    sum m b n   c is n   
sum a b c     c is b g
sum pos         sum neg        
the two clauses in the hypothesis space represent a complete and consistent program for
the given examples  but our procedure is unable to learn it  observe that the negative
example is derived by the second clause  which is a trace for the positive example  but not
by the first and the second together 
this problem can be avoided if we require that  for every negative example  a corresponding positive example with the same input be given  in the above case  the example
required is sum pos          in this way  if a complete program exists in the hypothesis
space  then it is also a trace  and can be learned  then it can be made consistent using
cut  in order to rule out the derivation of negative examples  the constraint on positive
and negative examples seems to be quite intuitive  in fact  when writing a program  a
   it must be noted that if we are learning programs for two different predicates  of j and k clauses
respectively  that is  md   j  k   then we have to consider not  j  k   different programs  but only
j   k   we can do better if  inside a program  it is known that non recursive clauses have a fixed
position  and can be put before or after of all the recursive clauses 
   a learned program p is complete if it derives all the given positive examples  and it is consistent if it
does not derive any of the given negative examples

  

fithe difficulties of learning logic programs with cut

programmer usually thinks in terms of what a program should compute on given inputs 
and then tries to avoid wrong computations for those inputs 

    problem    ordering of given examples
when learning clauses with cut  even the order of the positive examples may be significant 
in the example above  if sum pos        comes after sum pos        then the learning task
fails to learn a correct program for sum  because it cannot find a program consistent w r t 
the first positive example and the negative one s  
in general  for a given set of m positive examples this problem can be remedied by
testing different example orderings  again  in the worst case k  different orderings of a set
of k positive examples must be checked  moreover  in some situations a favorable ordering
does not exist  consider the following hypothesis space 

c   int x y w     head x a   tail x b   notmember a y   int b y w  
c   int x y w     head x a   tail x b   notmember a y      int b y w  
c   int x y z     head x a   tail x b   int b y w   cons a w z  
c   int x y z     head x a   tail x b      int b y w   cons a w z  
c   int x y z     null z  
together with the set of examples 

e    int pos  a   b       
e    int pos  a   a   a   
e    int neg  a   b   a   
e    int neg  a   a       
our induction procedure will not be able to find a correct program for any ordering of the
two positive examples  even if such a program does exist   c  c  c     this program is the
union of two traces   c  c    which covers e    and  c  c    which covers e    both of these traces
are inconsistent  because the first covers e    and the second covers e    this problem can
be remedied only if all the positive examples are derived before the check against negative
examples is done 
however  in that case we have a further loss of eciency  because some inconsistent
traces are discarded only in the end  in other words  we would need to learn a program
covering all the positive examples  and then make it consistent by using cut and by reordering clauses  moreover  there can be no way to make a program consistent by using cut and
reorderings  as a consequence  all the time used to build that program is wasted  as an
example  suppose we are given the following hypothesis space 

c    int x y z     head x a   tail x b   int b y w   cons a w z  
c    int x y z     null x   null z  
c    int x y z     null z  
  

fibergadano  gunetti    trinchero

with the examples 

e    int pos  a   a   a   
e     int pos  a b   c      
e     int neg  a   b   a   
then we can learn the trace  c   c    from e   and the trace  c    from e     but  c   c   c    covers
e    and there is no way to make it consistent using cut or by reordering its clauses  in fact 
the first partial trace is responsible for this inconsistency  and hence the time used to learn
 c    is totally wasted 
here it is also possible to understand why we need attened clauses  consider the following program for intersection  which is equivalent to  c  c  c    but with the three clauses
unattened 

u    int  ajb  y w     notmember a y      int b y w  
u    int  ajb  y  ajw         int b y w  
u    int         
now  this program covers int neg  a   a       i e   u   u  u      int  a   a       in fact  clause
u  fails on this example because a is a member of  a   clause u  fails because the empty
list cannot be matched with  ajw   but clause u  succeeds because its arguments match
those of the negative example  as a consequence  this program would be rejected by the
induction procedure 
the problem is that  if we use unattened clauses  it may happen that a clause body is
not evaluated because an example does not match the head of the clause  as a consequence 
possible cuts in that clause are not evaluated and cannot inuence the behavior of the entire
program  in our example  the cut in clause u  has no effect because the output argument of
int  a   a      does not match  ajw   and the body of u  is not evaluated at all  then u  is
fired and the negative example is covered  in the attened version  clause c  fails only when
cons a        is reached  but at that point a cut is in force and clause c  cannot be activated 
note that program  u   u  u   behaves correctly on the query int  a   a  x   and gives x  a 
as the only output 

    problem    ordering of literals

even the relative position of literals and cut in a clause is significant  consider again the
correct program for intersection as above   c  c  c     but with c  modified by putting the
cons literal in front of the antecedent 

c    int x y z     cons a w z   head x a   tail x b   int b y w  
then  there is no way to get a correct program for intersection using this clause  to rule
out the negative example int neg  a   a      we must put a cut before the cons predicate 
in order to prevent the activation of c   but  then  some positive examples are no longer
covered  such as int pos  a          in fact  we have a wrong behavior every time clause c   is
   

fithe difficulties of learning logic programs with cut

called and fails  since it prevents the activation on c    in general  this problem cannot be
avoided even by reordering clauses  if we put c   after c  and c    then int neg  a   a      will
be covered  as a consequence  we should also test for every possible permutation of literals
in every clause of a candidate program 

   situations where learning cut is still practical
from the above analysis  learning cut appears to be dicult since  in general  a learning
procedure should be able to backtrack on the candidate base programs  e g   traces   on
the position of cut s  in the program  on the order of the clauses in the program  on the
order of literals in the clauses and on the order of given positive examples  however  we
have spotted some general conditions at which learning cut could still be practical  clearly 
these conditions cannot be a final solution to learning cut  but  if applicable  can alleviate
the computational problems of the task 

    small hypothesis space

first of all  a restricted hypothesis space is necessary  if clauses cannot be learned independently of one another  a small hypothesis space would help to limit the backtracking
required on candidate traces  problem     moreover  even the number of clauses in a trace
would be probably smaller  and hence also the number of different permutations and the
number of different positions for inserted cuts  problems   and     a small trace would also
have a slight positive impact on the need to test for different literal orderings in clauses
 problem    
in general  many kinds of constraints can be applied to keep a hypothesis space small 
such as ij determinism  muggleton   feng         rule sets and schemata  kietz   wrobel 
      bergadano   gunetti         determinations  russell         locality  cohen        
etc  in fact  some of these restrictions and others  such as those listed in section    are
available in the actual implementation of our procedure   see the appendix     moreover 
candidate recursive clauses must be designed so that no infinite chains of recursive calls
can take place  bergadano   gunetti         otherwise the learning task itself could be
non terminating   in general  the number of possible recursive calls must be kept small  in
order to avoid too much backtracking when searching for possible traces  however  general
constraints may not be sucient  the hypothesis space must be designed carefully from
the very beginning  and this can be dicult  in the example of learning simplify an initial
hypothesis space of  only       clauses was obtained specifying not only the set of required
predicates  but even the variables occurring in every literal 
if clauses cannot be learned independently  experiments have shown to us that a dramatic improvement of the learning task can be obtained by generating the clauses in the
hypothesis space so that recursive clauses  and in general more complex clauses  are taken
into consideration after the simpler and non recursive ones  since simpler and non recursive
clauses require less time to be evaluated  they will have a small impact on the learning time 
moreover  learning simpler clauses  i e  shorter  also alleviates problem   
   we found these constraints particularly useful  by using them we were often able to restrict a hypothesis
space of one order of magnitude without ruling out any possible solution 

   

fibergadano  gunetti    trinchero

finally  it must be noted that our induction procedure does not necessarily require that
the hypothesis space s of possible clauses be represented explicitly  the learning task could
start with an empty set s and an implicit description of the hypothesis space  for example
the one given in section    when a positive example cannot be derived from s  a new clause
is asked for to a clause generator and added to s  this step is repeated until the example
is derivable from the updated s  and then the learning task can proceed normally 

    simple examples

another improvement can be achieved by using examples that are as simple as possible 
in fact  each example which may involve a recursive call is potentially responsible for the
activation of all the corresponding clauses in the hypothesis space  the more complex the
example  the larger the number of consecutive recursive activations of clauses and the larger
the number of traces to be considered for backtracking  problem     for instance  to learn
the append relation  it may be sucient to use an example like append  a   b   a b   instead
of one like append  a b c d   b   a b c d b    since simple examples would probably require
a smaller number of different clauses to be derived  this would result in smaller traces 
alleviating the problem of permutation of clauses and literals in a trace  problems   and   
and decreasing the number of positions for cuts  problem    

    small number of examples

since a candidate program is formed by taking the union of partial traces learned for single
examples  if we want a small trace  problems   and    we must use as few examples as
possible  while still completely describing the required concept  in other words  we should
avoid redundant information  for example  if we want to learn the program for append  it
will be normally sucient to use only one of the two positive examples append  a   b   a b  
and append  c   d   c d    obviously it may happen that different examples are derived by
the same set of clauses  and in this case the final program does not change 
having to check for all possible orderings of a set of positive examples  a small number of
examples is also a solution to problem    fortunately  experiments have shown that normally
very few positive examples are needed to learn a program  and hence the corresponding
number of different orderings is  in any case  a small number  moreover  since in our
method a positive example is sucient to learn all the clauses necessary to derive it  most
of the time a complete program can be learned using only one well chosen example  if such
an example can be found  as in the case of the learning task of section    where only one
example of simplify and one of remove are given   the computational problem of testing
different example orderings is automatically solved 
however  it must be noted that  in general  a small number of examples may not be
sucient  except for very simple programs  in fact  if we want to learn logic programs
such as member  append  reverse and so on  then any example involving recursion will be
sucient  but for more complex programs the choice may not be trivial  for example  our
procedure is able to learn the quicksort  plus partition  program with only one  good 
example  but if one does not know how quicksort and partition work  it is likely that
she or he will provide an example allowing to learn only a partial description of partition 
this is particularly clear in the example of simplify   had we used the positive example
   

fithe difficulties of learning logic programs with cut

simplify pos       b a a     b a    which is very close to the one effectively used   the first clause
of flatten would not have been learned  in other words  to give few examples we must give
good examples  and often this is possible only by having in mind  at least partially and in
an informal way  the target program  moreover  for complex programs  good examples can
mean complex examples  and this is in contrast with the previous requirement  for further
studies of learning from good examples we refer the reader to the work of ling        and
aha  ling  matwin and lapointe        

    constrained positions for cut and literals

experiments have shown that it is not practical to allow the learning procedure to test all
possible positions of cut in a trace  even if we are able to keep the number of clauses in
a trace small  the user must be able to indicate the positions where a cut is allowed to
occur  e g   at the beginning of a clause body  or before a recursive call  in this case  many
alternative programs with cut are automatically ruled out and thus do not have to be tested
against the negative examples  it may also be useful to limit the maximum number of cuts
per clause or per trace  for example  most of the time one cut per clause can be sucient
to learn a correct program  in the actual implementation of our procedure  it is in fact
possible to specify the exact position of cut w r t  a literal or a group of literals within each
clause of the hypothesis space  when this information is known 
to eliminate the need to test for different ordering of literals  problem     we may also
impose a particular global order  which must be maintained in every clause of the hypothesis
space  however this requires a deep knowledge of the program we want  otherwise some
 or even all  solutions will be lost  moreover  this solution can be in contrast with a use of
constrained positions for cut  since a solution program for a particular literal ordering and
for particular positions for cuts may not exist 

   conclusion

our induction procedure is based on an intensional evaluation of clauses  since the cut
predicate has no declarative meaning  we believe that intensional evaluation of clauses
cannot be abandoned  independently of the kind of learning method adopted  this can
decrease the performance of the learning task  compared with extensional methods  which
examine clauses one at a time without backtracking  however  the computational problems
outlined in section   remain even if we choose to learn a complete program extensionally 
and then we try to make it consistent by inserting cut  the only difference is that we do
not have backtracking  problem     but the situation is probably worse  since extensional
methods can fail to learn a complete program even if it exists in the hypothesis space 
 bergadano      a  
even if the ability to learn clauses containing procedural predicates like cut seems to be
fundamental to learning  real  logic programs  in particular short and ecient programs 
many problems inuencing the complexity of the learning task must be faced  these include
the number and the relative ordering of clauses and literals in the hypothesis space  the kind
and the relative ordering of given examples  such problems seem to be related to the need
for an intensional evaluation of clauses in general  and not to the particular learning method
adopted  even just to alleviate these problems  it seems necessary to know a lot about the
   

fibergadano  gunetti    trinchero

target program  an alternative solution is simply to ignore some of the problems  that is 
avoid testing for different clause and or literal and or example orderings  clearly  in this
way the learning process can become feasible  but it can fail to find a solution even when
it exists  however  many ilp systems  such as foil  adopt such an  incomplete but fast 
approach  which is guided by heuristic information 
as a consequence  we view results presented in this paper as  at least partially  negative  the problems we raised appear computationally dicult  and suggest that attention
should be restricted to purely declarative logic languages  which are  in any case  suciently
expressive 

acknowledgements
this work was in part supported by bra esprit project      on inductive logic programming 

appendix a
the induction procedure of section   is written in c prolog  interpreted  and runs on a
sunsparcstation    we are planning to translate it in quintus prolog  this appendix
contains a simplified description of its implementation  as a preliminary step  in order to
record a trace of the clauses deriving a positive example e   every clause in the hypothesis
space  s must be numbered and modified by adding to its body two literals  the first
one  allowed n m  is used to activate only the clauses which must be checked against the
negative examples  the second one  marker n   is used to remember that clause number n
has been successfully used while deriving e   hence  in general  a clause in the hypothesis
space s takes the following form 

p  x          xm     allowed n m    marker n  
where  is the actual body of the clause  n is the number of the clause in the set and m is a
number used to deal with cuts  for every clause n  the one without cut is augmented with
allowed n     while those containing a cut somewhere in their body are augmented with
allowed n     allowed n          and so on  moreover  for every augmented clause as above 
a fact  alt n m    is inserted in s  in order to implement an enumeration mechanism 
a simplified  but running  version of the learning algorithm is reported below  in the
algorithm  the output  if any  is the variable trace containing the list of the  numbers of the 
clauses representing the learned program p  by using the backtracking mechanism of prolog 
more than one solution  trace  can be found  we assume the two predicates listpositive
and listnegative build a list of the given positive and negative examples  respectively 
consult file containing the set of clauses s  
   we assume clauses in the hypothesis space to be attened

   

fithe difficulties of learning logic programs with cut

allowed x    
marker x     assert trace x   
marker x     retract trace x       fail 
main    listpositive posexamplelist   tracer    posexamplelist trace  
tracer covered  examplejcdr  trace     example     backtracking point     
setof l trace l  trace   
notneg trace   examplejcovered  cdr  
tracer  examplejcovered  cdr trace  
tracer      trace     setof  i j  allowed i j  trace   asserta  marker x     true      
assertem     
assertem  ijcdr      alt i j   backassert allowed i j    assertem cdr  
prep t     retract allowed x      assertem t  
backassert x     assert x  
backassert x     retract x      fail 
resetallowed          
resetallowed       abolish allowed     assert allowed x        
notneg t covered remaining     listnegative     
notneg t covered remaining     listnegative negexamplelist  
asserta  marker x     true     
prep t      backtracking point     
trypos covered   trynegs negexamplelist  
resetallowed remaining  
retract  marker x     true     
notneg t covered remaining     resetallowed remaining  
retract  marker x     true         fail 
trypos  examplejcdr      example     trypos cdr  
trypos          
trynegs  examplejcdr      example   fail 
trynegs  examplejcdr      trynegs cdr  
trynegs          
actually  our complete implementation is more complex  also in order to achieve greater
eciency  the behavior of the learning task is quite simple  initially  the set s of clauses is
read into the prolog interpreter  together with the learning algorithm  then the learning
task can be started by calling the predicate main  a list of the positive examples is formed
   

fibergadano  gunetti    trinchero

and the tracer procedure is called on that list  for every positive example  tracer calls
the example itself  firing all the clauses in s that may be resolved against that example 
observe that  initially  an allowed x    predicate is asserted in the database  in this way
only clauses not containing a cut are allowed to be used  this is because clauses with cut are
employed only if some negative example is derived   then  a trace  if any  of  the numbers
associated to  the clauses successfully used in the derivation of that example is built  using
the setof predicate 
the trace is added to the traces found for the previous examples  and the result is
checked against the set of the negative examples calling the notneg procedure  if notneg
does not fail  i e  no negative examples are covered by this trace  then a new positive
example is taken into consideration  otherwise notneg modifies the trace with cut and
tests it again  if also this fails  backtracking occurs and a new trace for the current example
 and possibly for the previous ones  is searched for 
the notneg procedure works as follows  first  only the clauses in the trace are allowed
to be checked against the negative examples  by retracting the allowed x    clause and
asserting an allowed n    if the n th clause  without cut  is in the trace  this is done with
the prep and assertem predicates  then a list of the negative examples is formed and we
check if they can be derived from the clauses in the trace  if at least one negative example is
covered   i e   if trynegs fails  then we backtrack to the prep procedure  backtracking point
   where a clause of the trace is substituted with an equivalent one but with cut inserted
somewhere  or in a different position   if no correct program can be found in such a way
by trying all possible alternatives  i e  by using cut in all possible ways   notneg fails  and
backtracking to backtracking point   occurs  where another trace is searched for  otherwise 
all clauses in s without cut are reactivated by asserting again allowed x     and the next
positive example is considered  note that trypos is used in notneg to verify if a modified
trace still derives the set of positive examples derived initially  the possibility to substitute
clauses in the current trace with others having cut inserted somewhere is achieved through
the alt predicate in the assertem procedure  finally  note that this simplified version of
the learning procedure is not able to generate and test for different orderings of clauses in
a trace or for different ordering of literals in each clause  nor to use different orderings for
the set of positive examples 
in order to derive all the positive examples before the check against the negative ones
 see subsection       we must change the first clause of the tracer procedure into 
tracer  pos        posn    pos        posn  setof l trace l  t   notneg t  
the actual implementation of the above induction procedure is available through ftp  for
further information contact gunetti di unito it 

references

aha  d   ling  c   matwin  s     lapointe  s          learning singly recursive relations
from small datasets  in proceedings of the ijcai    workshop on ilp 
bergadano  f       a   inductive database relations  ieee transactions on data and
knowledge engineering        
   

fithe difficulties of learning logic programs with cut

bergadano  f       b   test case generation by means of learning techniques  in proceedings of acm sigsoft    
bergadano  f     gunetti  d          an interactive system to learn functional logic programs  in proceedings of ijcai    
coelho  h     cotta  j  c          prolog by example  how to learn teach and use it  berlin 
springer verlag 
cohen  w          rapid prototyping of ilp systems using explicit bias  in proceedings
of the ijcai    workshop on ilp 
deraedt  l   lavrac  n     dzeroski  s          multiple predicate learning  in proceedings
of ijcai    
kietz  j  u     wrobel  s          controlling the complexity of learning in logic through
syntactic and task oriented models  in muggleton  s   ed    inductive logic programming  london  academic press 
lau  k  k     clement  t   eds            logic program synthesis and transformation 
berlin  springer verlag 
ling  x  c          learning from good examples  in proceedings of ijcai    
muggleton  s   ed            inductive logic programming  london  academic press 
muggleton  s     feng  c          ecient induction of logic programs  in proceedings of
the first conference on algorithmic learning theory 
quinlan  r          learning logical definitions from relations  machine learning    
        
rouveirol  c   in press   flattening  a representation change for generalization  machine
learning 
russell  s          tree structured bias  in proceedings of aaai    
shapiro  e  y          algorithmic program debugging  cambridge  ca  mit press 

   

fi