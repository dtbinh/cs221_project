journal of artificial intelligence research                  

submitted       published     

towards adjustable autonomy for the real world
scerri isi edu
pynadath isi edu
tambe usc edu

paul scerri
david v  pynadath
milind tambe
information sciences institute and computer science department
university of southern california
     admiralty way  marina del rey  ca       usa

abstract

adjustable autonomy refers to entities dynamically varying their own autonomy  transferring decision making control to other entities  typically agents transferring control to
human users  in key situations  determining whether and when such transfers of control
should occur is arguably the fundamental research problem in adjustable autonomy  previous work has investigated various approaches to addressing this problem but has often
focused on individual agent human interactions  unfortunately  domains requiring collaboration between teams of agents and humans reveal two key shortcomings of these previous
approaches  first  these approaches use rigid one shot transfers of control that can result in
unacceptable coordination failures in multiagent settings  second  they ignore costs  e g  
in terms of time delays or effects on actions  to an agent s team due to such transfers ofcontrol 
to remedy these problems  this article presents a novel approach to adjustable autonomy  based on the notion of a transfer of control strategy  a transfer of control strategy
consists of a conditional sequence of two types of actions   i  actions to transfer decisionmaking control  e g   from an agent to a user or vice versa  and  ii  actions to change an
agent s pre specified coordination constraints with team members  aimed at minimizing
miscoordination costs  the goal is for high quality individual decisions to be made with
minimal disruption to the coordination of the team  we present a mathematical model
of transfer of control strategies  the model guides and informs the operationalization of
the strategies using markov decision processes  which select an optimal strategy  given an
uncertain environment and costs to the individuals and teams  the approach has been
carefully evaluated  including via its use in a real world  deployed multi agent system that
assists a research group in its daily activities 
  

introduction

exciting  emerging application areas ranging from intelligent homes  lesser et al          to
routine organizational coordination  pynadath et al          to electronic commerce  collins
et al       a   to long term space missions  dorais et al         utilize the decision making
skills of both agents and humans  these new applications have brought forth an increasing
interest in agents  adjustable autonomy  aa   i e   in entities dynamically adjusting their own
level of autonomy based on the situation  mulsiner   pell         many of these exciting
applications will not be deployed unless reliable aa reasoning is a central component  with
aa  an entity need not make all decisions autonomously  rather it can choose to reduce its
own autonomy and transfer decision making control to other users or agents  when doing so
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiscerri  pynadath   tambe
is expected to have some net benefit  dorais et al         barber  goel    martin      a 
hexmoor   kortenkamp        
a central problem in aa is to determine whether and when transfers of decision making
control should occur  a key challenge is to balance two potentially conicting goals  on the
one hand  to ensure that the highest quality decisions are made  an agent can transfer control to a human user  or another agent  whenever that user has superior decision making
expertise   on the other hand  interrupting a user has high costs and the user may be
unable to make and communicate a decision  thus such transfers of control should be minimized  previous work has examined several different techniques that attempt to balance
these two conicting goals and thus address the transfer of control problem  for example 
one technique suggests that decision making control should be transferred if the expected
utility of doing so is higher than the expected utility of making an autonomous decision
 horvitz  jacobs    hovel         a second technique uses uncertainty as the sole rationale
for deciding who should have control  forcing the agent to relinquish control to the user
whenever uncertainty is high  gunderson   martin         yet other techniques transfer
control to a user if an erroneous autonomous decision could cause significant harm  dorais
et al         or if the agent lacks the capability to make the decision  ferguson  allen   
miller        
unfortunately  these previous approaches to transfer of control reasoning and indeed
most previous work in aa  have focused on domains involving a single agent and a single
user  isolated from interactions with other entities  when applied to interacting teams of
agents and humans  where interaction between an agent and a human impacts the interaction with other entities  these techniques can lead to dramatic failures  in particular 
the presence of other entities as team members introduces a third goal of maintaining coordination  in addition to the two goals already mentioned above   which these previous
techniques fail to address  failures occur for two reasons  firstly  these previous techniques
ignore team related factors  such as costs to the team due to incorrect decisions or due to
delays in decisions during such transfers of control  secondly  and more importantly   these
techniques use one shot transfers of control  rigidly committing to one of two choices   i 
transfer control and wait for input  choice h   or  ii  act autonomously  choice a   however 
given interacting teams of agents and humans  either choice can lead to costly failures if the
entity with control fails to make or report a decision in a way that maintains coordination 
for instance  a human user might be unable to provide the required input due to a temporary communication failure  this may cause an agent to fail in its part of a joint action 
as this joint action may be dependent on the user s input  on the other hand  forcing a
less capable entity to make a decision simply to avoid miscoordination can lead to poor
decisions with significant consequences  indeed  as seen in section      when we applied a
rigid transfer of control decision making to a domain involving teams of agents and users 
it failed dramatically 
yet  many emerging applications do involve multiple agents and multiple humans acting
cooperatively towards joint goals  to address the shortcomings of previous aa work in such
domains  this article introduces the notion of a transfer of control strategy  a transfer ofcontrol strategy consists of a pre defined  conditional sequence of two types of actions   i 
   while the aa problem in general involves transferring control from one entity to another  in this paper 
we will typically focus on interactions involving autonomous agents and human users 

   

fitowards adjustable autonomy for the real world
actions to transfer decision making control  e g   from an agent to a user or vice versa  
 ii  actions to change an agent s pre specified coordination constraints with team members 
rearranging activities as needed  e g   reordering tasks to buy time to make the decision  
the agent executes such a strategy by performing the actions in order  transferring control to
the specified entity and changing coordination as required  until some point in time when the
entity currently in control exercises that control and makes the decision  thus  the previous
choices of h or a are just two of many different and possibly more complex transfer ofcontrol strategies  for instance  an adah strategy implies that an agent initially attempts
to make an autonomous decision  if the agent makes the decision autonomously the strategy
execution ends there  however  there is a chance that it is unable to make the decision
in a timely manner  perhaps because its computational resources are busy with higher
priority tasks  to avoid miscoordination the agent executes a d action which changes the
coordination constraints on the activity  for example  a d action could be to inform other
agents that the coordinated action will be delayed  thus incurring a cost of inconvenience
to others but buying more time to make the decision  if it still cannot make the decision  it
will eventually take action h   transferring decision making control to the user and waiting
for a response  in general  strategies can involve all available entities and contain many
actions to change coordination constraints  while such strategies may be useful in singleagent single human settings  they are particularly critical in general multiagent settings  as
discussed below 
transfer of control strategies provide a exible approach to aa in complex systems
with many actors  by enabling multiple transfers of control between two  or more  entities 
rather than rigidly committing to one entity  i e   a or h    a strategy attempts to provide the
highest quality decision  while avoiding coordination failures  in particular  in a multiagent
setting there is often uncertainty about whether an entity will make a decision and when it
will do so  e g   a user may fail to respond  an agent may not be able to make a decision as
expected or a communication channel may fail  a strategy addresses such uncertainty by
planning multiple transfers of control to cover for such contingencies  for instance  with the
adh strategy  an agent ultimately transfers control to a human to attempt to ensure that
some response will be provided in case the agent is unable to act  furthermore  explicit
coordination change actions  i e   d actions  reduce miscoordination effects  for a cost  while
better decisions are being made  finally  since the utility of transferring control or changing
coordination is dependent on the actions taken afterwards  the agent must plan a strategy
in advance to find the sequence of actions that maximizes team benefits  for example 
reacting to the current situation and repeatedly taking and giving control as in the strategy
adhadh       may be more costly than planning ahead  making a bigger coordination
change  and using a shorter adh strategy  we have developed a decision theoretic model
of such strategies  that allows the expected utility of a strategy to be calculated and  hence 
strategies to be compared 
thus  a key aa problem is to select the right strategy  i e   one that provides the
benefit of high quality decisions without risking significant costs in interrupting the user and
miscoordination with the team  furthermore  an agent must select the right strategy despite
significant uncertainty  markov decision processes  mdps   puterman        are a natural
choice for implementing such reasoning because they explicitly represent costs  benefits and
uncertainty as well as doing lookahead to examine the potential consequences of sequences
   

fiscerri  pynadath   tambe
of actions  in section    a general reward function is presented for an mdp that results in an
agent carefully balancing risks of incorrect autonomous decisions  potential miscoordination
and costs due to changing coordination between team members  detailed experiments were
performed on the mdp  the key results of which are as follows  as the relative importance
of central factors  such as the cost of miscoordination  was varied the resulting mdp policies
varied in a desirable way  i e   the agent made more decisions autonomously if the cost of
transferring control to other entities increased  other experiments reveal a phenomenon not
reported before in the literature  an agent may act more autonomously when coordination
change costs are either too low or too high  but in a  middle  range  the agent tends to act
less autonomously 
our research has been conducted in the context of a real world multi agent system 
called electric elves  e elves   chalupsky  gil  knoblock  lerman  oh  pynadath  russ 
  tambe        pynadath et al          that we have used for over six months at the
university of southern california  information sciences institute  the e elves assists a
group of researchers and a project assistant in their daily activities  providing an exciting
opportunity to test aa ideas in a real environment  individual user proxy agents called
friday  from robinson crusoe s servant friday  act in a team to assist with rescheduling
meetings  ordering meals  finding presenters and other day to day activities  over the course
of several months  mdp based aa reasoning was used around the clock in the e elves 
making many thousands of autonomy decisions  despite the unpredictability of the user s
behavior and the agent s limited sensing abilities  the mdp consistently made sensible
aa decisions  moreover  many times the agent performed several transfers of control to
cope with contingencies such as a user not responding  one lesson learned when actually
deploying the system was that sometimes users wished to inuence the aa reasoning  e g  
to ensure that control was transferred to them in particular circumstances  to allow users
to inuence the aa reasoning  safety constraints are introduced that allow users to prevent
agents from taking particular actions or ensuring that they do take particular actions 
these safety constraints provide guarantees on the behavior of the aa reasoning  making
the basic approach more generally applicable and  in particular  making it more applicable
to domains where mistakes have serious consequences 
the rest of this article is organized as follows  section   gives a detailed description of the
aa problem and presents the electric elves as a motivating example application  section
  presents a formal model of transfer of control strategies for aa   readers not interested
in the mathematical details may wish to skip over section     the operationalization of
the strategies via mdps is described in section    in section    the results of detailed
experiments are presented  section   looks at related work  including how earlier aa work
can be analyzed within the strategies framework  section   gives a summary of the article 
finally  section   outlines areas where the work could be extended to make it applicable to
more applications 
  

adjustable autonomy   the problem

the general aa problem has not been previously formally defined in the literature  particularly for a multiagent context  in the following  a formal definition of the problem is given
so as to clearly define the task for the aa reasoning  the team  which may consist entirely
   

fitowards adjustable autonomy for the real world
of agents or include humans  has some joint activity  ff  each entity in the team works
cooperatively on the joint activity  the agent  a  has a role    in the team  depending
on the specific task  some or all of the roles will need to be performed successfully in order
for the joint activity to succeed  the primary goal of the agent is the success of ff which
it pursues by performing   performing  requires that one or more non trivial decisions
are made  to make a decision  d  the agent can draw upon n other entities from a set
e   fe        en g  which typically includes the agent itself  each entity in e  e g   a human
user  is capable of making decision d  the entities in e are not necessarily part of the
team performing ff  different agents and users will have differing abilities to make decisions
due to available computational resources  access to relevant information  etc  coordination
constraints    exist between  and the roles of other members of the team  for example 
various roles might need to be executed simultaneously or in a certain order or with some
combined quality or total cost  a critical facet of the successful completion of the joint task
ff  given its jointness  is to ensure that coordination between team members is maintained 
i e    are not violated  thus  we can describe an aa problem instance with the tuple 
ha  ff      d  e i 
from an aa perspective  the agent can take two types of actions for a decision  d 
first  it can transfer control to an entity in e capable of making that decision  in general 
there are no restrictions on when  how often or for how long decision making control can
be transferred to a particular entity  typically  the agent can also transfer decision making
control to itself  in general  we assume that when the agent transfers control  it does not
have any guarantee on the exact time of response or exact quality of the decision made by
the entity to which control is transferred  in fact  in some cases it will not know whether
the entity will be able to make a decision at all or even whether the entity will know it has
decision making control  e g   if control was transferred via email  the agent may not know
if the user actually read the email 
the second type of action that an agent can take is to request changes in the coordination constraints    between team members  a coordination change gives the agent
the possibility of changing the requirements surrounding the decision to be made  e g   the
required timing  cost or quality of the decision  which may allow it to better fulfill its responsibilities  a coordination change might involve reordering or delaying tasks or it may
involve changing roles  or it may be a more dramatic change where the team pursues ff in
a completely different way  changing coordination has some cost  but it may be better
to incur this cost than violate coordination constraints  i e   incur miscoordination costs 
miscoordination between team members will occur for many reasons  e g   a constraint that
limits the total cost of a joint task might be violated if one team member incurs a higher
than expected cost and other team members do not reduce their costs  in this article  we
are primarily concerned with constraints related to the timing of roles  e g   ordering constraints or requirements on simultaneous execution  this in turn  usually requires that the
agent guards against delayed decisions although it can also require that a decision is not
made too soon 
thus  the aa problem for the agent  given a problem instance  ha  ff      d  e i  is to
choose the transfer of control or coordination change actions that maximizes the overall
expected utility of the team  in the remainder of this section we describe a concrete  real   

fiscerri  pynadath   tambe
world domain for aa  section      and an initial failed approach that motivates our solution
 section      

    the electric elves
this research was initiated in response to issues that arose in a real application and the
resulting approach was extensively tested in the day to day running of that application 
the electric elves  e elves  is a project at usc isi to deploy an agent organization in
support of the daily activities of a human organization  pynadath et al         chalupsky
et al          we believe this application to be fairly typical of future generation applications involving teams of agents and humans  the operation of a human organization
requires the performance of many everyday tasks to ensure coherence in organizational
activities  e g   monitoring the status of activities  gathering information and keeping everyone informed of changes in activities  teams of software agents can aid organizations
in accomplishing these tasks  facilitating coherent functioning and rapid  exible response
to crises  a number of underlying ai technologies support the e elves  e g   technologies
devoted to agent human interactions  agent coordination  accessing multiple heterogeneous
information sources  dynamic assignment of organizational tasks  and deriving information
about organization members  chalupsky et al          while these technologies are useful 
aa is fundamental to the effective integration of the e elves into the day to day running
of a real organization and  hence  is the focus of this paper 
the basic design of the e elves is shown in figure   a   each agent proxy is called
friday  after robinson crusoes  man servant friday  and acts on behalf of its user in the
agent team  the design of the friday proxies is discussed in detail in  tambe  pynadath 
chauvat  das    kaminka         where they are referred to as teamcore proxies  
currently  friday can perform several tasks for its user  if a user is delayed to a meeting 
friday can reschedule the meeting  informing other fridays  who in turn inform their users 
if there is a research presentation slot open  friday may respond to the invitation to present
on behalf of its user  friday can also order its user s meals  see figure   a   and track the
user s location  posting it on a web page  friday communicates with users using wireless
devices  such as personal digital assistants  palm viis  and wap enabled mobile phones 
and via user workstations  figure   b  shows a palm vii connected to a global positioning
service  gps  device  for tracking users  locations and enabling wireless communication
between friday and a user  each friday s team behavior is based on a teamwork model 
called steam  tambe         steam encodes and enforces the constraints between roles
that are required for the success of the joint activity  e g   meeting attendees should arrive at
a meeting simultaneously  when a role within the team needs to be filled  steam requires
that a team member is assigned responsibility for that role  to find the best suited person 
the team auctions off the role  allowing it to consider a combination of factors and assign
the best suited user  friday can bid on behalf of its user  indicating whether its user is
capable and or willing to fill a particular role  figure   b  shows a tool that allows users
to view auctions in progress and intervene if they so desire  in the auction in progress  jay
modi s friday has bid that jay is capable of giving the presentation  but is unwilling to do
so  paul scerri s agent has the highest bid and was eventually allocated the role 

   

fitowards adjustable autonomy for the real world

friday

friday

friday

friday

 a 

 b 

figure     a  overall e elves architecture  showing friday agents interacting with users 
 b palm vii for communicating with users and gps device for detecting their
location 

   

fiscerri  pynadath   tambe
aa is critical to the success of the e elves since  despite the range of sensing devices 
friday has considerable uncertainty about the user s intentions and even location  hence 
friday will not always have the appropriate information to make correct decisions  on the
other hand  while the user has the required information  friday cannot continually ask the
user for input  since such interruptions are disruptive and time consuming  there are four
decisions in the e elves to which aa reasoning is applied   i  whether a user will attend a
meeting on time   ii  whether to close an auction for a role   iii  whether the user is willing
to perform an open team role  and  iv  if and what to order for lunch  in this paper  we
focus on the aa reasoning for two of those decisions  whether a user will attend a meeting
on time and whether to close an auction for a role  the decision as to whether a user will
attend a meeting on time is the most often used and most dicult of the decisions friday
faces  we briey describe the decision to close an auction and later show how an insight
provided by the model of strategies led to a significant reduction in the amount of code
required to implement the aa reasoning for that decision  the decision to volunteer a user
for a meeting is similar to the earlier decisions  and omitted for brevity  the decision to order
lunch is currently implemented in a simpler fashion and is not  at least as yet  illustrative
of the full set of complexities 
a central decision for friday  which we describe in terms of our problem formulation 
ha  ff      d  e i  is whether its user will attend a meeting at the currently scheduled meeting time  in this case  friday is the agent  a  the joint activity  ff  is for the meeting
attendees to attend the meeting simultaneously  friday acts as proxy for its user  hence
its role    is to ensure that its user arrives at the currently scheduled meeting time  the
coordination constraint    between friday s role and the roles of other fridays is that they
occur simultaneously  i e   the users must attend at the currently scheduled time  if any
attendee arrives late  or not at all  the time of all the attendees is wasted  on the other
hand  delaying a meeting is disruptive to users  schedules  the decision  d  is whether the
user will attend the meeting or not and could be made by either friday or the user  i e  
e   fuser  fridayg  clearly  the user will be often better placed to make this decision 
however  if friday transfers control to the user for the decision  it must guard against miscoordination  i e   having the other attendees wait  while waiting for a user response  some
decisions are potentially costly  e g   incorrectly telling the other attendees that the user
will not attend  and friday should avoid taking them autonomously  to buy more time for
the user to make the decision or for itself to gather more information  friday could change
coordination constraints with a d action  friday has several different d actions at its disposal  including delaying the meeting by different lengths of time  as well as being able to
cancel the meeting entirely  the user can also request a d action  e g   via the dialog box in
figure   a   to buy more time to make it to the meeting  if the user decides a d is required 
friday is the conduit through which other fridays  and hence their users  are informed 
friday must select a sequence of actions  either transferring control to the user  delaying or
cancelling the meeting or autonomously announcing that the user will or will not attend 
to maximize the utility of the team 
the second aa decision that we look at is the decision to close an auction for an open
role and assign a user to that role   in this case  the joint activity  ff  is the group research
   there are also roles for submitting bids to the auction but the aa for those decisions is simpler  hence
we do not focus on them here 

   

fitowards adjustable autonomy for the real world

 a 

 b 

figure     a  friday transferring control to the user for a decision whether to order lunch 
 b  the e elves auction monitoring tool 
meeting and the role    is to be the auctioneer  users will not always submit bids for the
role immediately  in fact  the bids may be spread out over several days  or some users might
not bid at all  the specific decision  d  on which we focus is whether to close the auction
and assign the role or continue waiting for incoming bids  once individual team members
provide their bids  the auctioneer agent or human team leader decides on a presenter based
on that input  e   fuser  auctioneer agentg   the team expects a willing presenter to do
a high quality research presentation  which means the presenter will need some time to
prepare  thus  the coordination constraint   is that the most capable  willing user must
be allocated to the role with enough time to prepare the presentation  despite individually
responsible actions  the agent team may reach a highly undesirable decision  e g   assigning
the same user week after week  hence there is advantage in getting the human team leader s
input  the agent faces uncertainty  e g   will better bids come in    costs  i e   the later
the assignment  the less time the presenter has to prepare   and needs to consider the
possibility that the human team leader has some special preference about who should do
a presentation at some particular meeting  by transferring control  the agent allows the
human team leader to make an assignment  for this decision  a coordination change action 
d  would reschedule the research meeting  however  relative to the cost of cancelling the
meeting  the cost of rescheduling is too high for rescheduling to be a useful action 

    decision tree approach
one logical avenue of attack on the aa problem for the e elves was to apply an approach
used in a previously reported  successful meeting scheduling system  in particular cap
 mitchell  caruana  freitag  mcdermott    zabowski         like cap  friday learned
user preferences using c    decision tree learning  quinlan         friday recorded values
of a dozen carefully selected attributes and the user s preferred action  identified by asking
   

fiscerri  pynadath   tambe
the user  whenever it had to make a decision  friday used the data to learn a decision
tree that encoded its autonomous decision making  for aa  friday also asked if the user
wanted such decisions taken autonomously in the future  from these responses  friday
used c    to learn a second decision tree which encoded its rules for transferring control 
thus  if the second decision tree indicated that friday should act autonomously  it would
take the action suggested by the first decision tree  initial tests with the c    approach
were promising  tambe et al          but a key problem soon became apparent  when
friday encountered a decision for which it had learned to transfer control to the user  it
would wait indefinitely for the user to make the decision  even though this inaction caused
miscoordination with teammates  in particular  other team members would arrive at the
meeting location  waiting for a response from the user s friday  but they would end up
completely wasting their time as no response arrived  to address this problem  if a user
did not respond within a fixed time limit  five minutes   friday took an autonomous action 
although performance improved  when the resulting system was deployed      it led to
some dramatic failures  including 
   example    tambe s  a user  friday incorrectly cancelled a meeting with the division
director because friday over generalized from training examples 
   example    pynadath s  another user  friday incorrectly cancelled the group s weekly
research meeting when a time out forced the choice of an  incorrect  autonomous
action 
   example    a friday delayed a meeting almost    times  each time by   minutes 
it was correctly applying a learned rule but ignoring the nuisance to the rest of the
meeting participants 
   example    tambe s friday automatically volunteered him for a presentation  but he
was actually unwilling  again friday over generalized from a few examples and when
a timeout occurred it took an undesirable autonomous action 
clearly  in a team context  rigidly transferring control to one agent  user  failed  furthermore  using a time out that rigidly transferred control back to the agent  when it was
not capable of making a high quality decision  also failed  in particular  the agent needed
to better avoid taking risky decisions by explicitly considering their costs  example     or
take lower cost actions to delay meetings to buy the user more time to respond  example  
and     furthermore  as example   showed  the agent needed to plan ahead  to avoid taking
costly sequences of actions that could be replaced by a single less costly action  example
    in theory  using c    friday might have eventually been able to learn rules that would
successfully balance costs and deal with uncertainty and handle all the special cases and so
on  but a very large amount of training data would be required 
  

strategies for adjustable autonomy

to avoid rigid one shot transfers of control and allow team costs to be considered  we
introduce the notion of a transfer of control strategy  which is defined as follows 
   

fitowards adjustable autonomy for the real world
definition     a transfer of control strategy is a pre defined  conditional sequence of two

types of actions   i  actions to transfer decision making control  e g   from an agent to
a user or other agents  or vice versa  and  ii  actions to change an agent s pre specified
coordination constraints with team members  aimed at minimizing miscoordination costs 

the agent executes a transfer of control strategy by performing the specified actions in
sequence  transferring control to the specified entity and changing coordination as required 
until some point in time when the entity currently in control exercises that control and
makes the decision  considering multi step strategies allows an agent to exploit decisionmaking sources considered too risky to exploit without the possibility of retaking control 
for example  control could be transferred to a very capable but not always available decision
maker then taken back if the decision was not made before serious miscoordination occurred 
more complex strategies  potentially involving several coordination changes  give the agent
the option to try several decision making sources or to be more exible in getting input from
high quality decision makers  as a result  transfer of control strategies specifically allow an
agent to avoid costly errors  such as those enumerated in the previous section  
given an aa problem instance  ha  ff      d  e i   agent a can transfer decision making
control for a decision d to any entity ei   e   and we denote such a transfer of control
action with the symbol representing the entity  i e   transferring control to ei is denoted as
ei   when the agent transfers decision making control  it may stipulate a limit on the time
that it will wait for a response from that entity  to capture this additional stipulation 
we denote transfer of control actions with this time limit  e g   ei  t  represents that ei has
decision making control for a maximum time of t  such an action has two possible outcomes 
either ei responds before time t and makes the decision  or it does not respond and decision
d remains unmade at time t  in addition  the agent has some mechanism by which it
can change coordination constraints  denoted d  to change the expected timing of the
decision  the d action changes the coordination constraints    between team members 
the action has an associated value  dvalue   which specifies its magnitude  i e   how much
the d has alleviated the temporal pressure   and a cost  dcost   which specifies the price paid
for making the change  we can concatenate such actions to specify a complete transferof control strategy  for instance  the strategy h    a would specify that the agent first
relinquishes control and asks entity h  denoting the h uman user   if the user responds
with a decision within five minutes  then there is no need to go further  if not  then the
agent proceeds to the next transfer of control action in the sequence  in this example  this
next action  a  specifies that the agent itself make the decision and complete the task 
no further transfers of control occur in this case  we can define the space of all possible
strategies with the following regular expression 

s    e  r   e  r    d 

   

where  e  r  is all possible combinations of entity and maximum time 
for readability  we will frequently omit the time specifications from the transfer ofcontrol actions and instead write just the order in which the agent transfers control among
   in some domains  it may make sense to attempt to get input from more than one entity at once  hence
requiring strategies that have actions that might be executed in parallel  however  in this work  as a first
step  we do not consider such strategies  furthermore  they are not relevant for the domains at hand 

   

fiscerri  pynadath   tambe
the entities and executes ds  e g   we will often write ha instead of h    a   if time
specifications are omitted  we assume the transfers happen at the optimal times   i e  
the times that lead to highest expected utility  if we consider strategies with the same
sequence of actions but different timings to be the same strategy  the agent has o je jk  
possible strategies to select from  where k is the maximum length of the strategy and je j
is the number of entities  thus  the agent has a wide range of options  even if practical
considerations lead to a reasonable upper bound on k and je j  the agent must select the
strategy that maximizes the overall expected utility of ff 
in the rest of this section  we present a mathematical model of transfer of control strategies for aa and use that model to guide the search for a solution  moreover  the model
provides a tool for predicting the performance of various strategies  justifying their use
and explaining observed phenomena of their use  section     presents the model of aa
strategies in detail  section     reveals key properties of complex strategies  including dominance relationships among strategies  section     examines the e elves application in the
light of the model  to make specific predictions about some properties that a successful
aa approach reasoning for that application class will have  these predictions shape the
operationalization of strategies in section   

    a mathematical model of strategies
the transfer of control model presented in this section allows calculation of the expected
utility  eu  of individual strategies  thus allowing strategies to be compared  the calculation of a strategy s eu considers four elements  the likely relative quality of different
entities  decisions  the probability of getting a response from an entity at a particular time 
the cost of delaying a decision  and the costs and benefits of changing coordination constraints  while other parameters might also be modeled in a similar manner  our experience
with the e elves and other aa work suggests that these parameters are the critical ones
across a wide range of joint activities 
the first element of the model is the expected quality of an entity s decision  in general 
we capture the quality of an entity s decision at time t with the functions eq   feqde  t   
r   rg  the quality of a decision reects both the probability that the entity will make an
 appropriate  decision and the costs incurred if the decision is wrong  the expected quality
of a decision is calculated in a decision theoretic way  by multiplying the probability of each
outcome  i e   each decision  by the utility of that decision  i e   the cost or benefit of that
decision  for example  the higher the probability that the entity will make a mistake  the
lower the quality  even lower if the mistakes might be very costly  the quality of decision
an entity will make can vary over time as the information available to it changes or as it has
more time to  think   the second element of the model is the probability that an entity
will make a decision if control is transferred to it  the functions  p   fp e  t    r         g 
represent continuous probability distributions over the time that
the entity e will respond 
r t  ei
that is  the probability that ei will respond before time t  is   p   t dt 
the third element of the model is a representation of the cost of inappropriate timing
of a decision  in general  not making a decision until a particular point in time incurs some
   the best time to transfer control can be found  e g   by differentiating the expected utility equation in
section     and solving for   

   

fitowards adjustable autonomy for the real world
cost that is a function of both the time  t  and the coordination constraints    between
team members  as stated earlier  we focus on cases of constraint violations due to delays
in making decisions  thus  the cost is due to the violation of the constraints caused by
not making a decision until that point in time  we can write down a wait cost function  
w   f    t  which returns the cost of not making a decision until a particular point in time
given coordination constraints    this miscoordination cost is a fundamental aspect of our
model given our emphasis on multiagent domains  it is called a  wait cost  because it models
the miscoordination that arises while the team  waits  for some entity to make the ultimate
decision  in domains like e elves  the team incurs such wait costs in situations where  for
example  other meeting attendees have assembled in a meeting room at the time of the
meeting  but are kept waiting without any input or decision from friday  potentially because
it cannot provide a high quality decision  nor can it get any input from its user   notice
that different roles will lead to different wait cost functions  since delays in the performance
of different roles will have different effects on the team  we assume that there is some
point in time    after which no more costs accrue  i e   if t   then f    t    f      
at the deadline    the maximum cost due to inappropriate timing of a decision has been
incurred  finally  we assume that  in general  until   the wait cost function is nondecreasing  reecting the idea that bigger violations of constraints lead to higher wait costs 
the final element of the model is the coordination change action  d  which moves the agent
further away from the deadline and hence reduces the wait costs that are incurred  we
model the effect of the d by letting w be a function of t dvalue  rather than t  after
the d action and as having a fixed cost  dcost   incurred immediately upon its execution 
for example  in the e elves domain  suppose at the time of the meeting  friday delays the
meeting by    minutes  d action   then  in the following time period  it will incur the
relatively low cost of not making a decision    minutes before the meeting  t dvalue   
rather than the relatively high cost of not making the decision at the time of the meeting 
other  possibly more complex  models of a d action could also be used 
we use these four elements to compute the eu of an arbitrary strategy  s  the utility
derived from a decision being made at time t by the entity in control is the quality of the
entity s decision minus the costs incurred from waiting until t  i e   euedc  t    eqdec  t 
w  t   if a coordination change action has been taken it will also have an effect on utility 
until a coordination change of value dvalue is taken at some time   the incurred wait cost
is w     then  between  and t  the wait cost incurred is w  t dvalue   w   dvalue   
thus  if a d action has been taken at time  for cost dcost and with value dvalue   the
utility from a decision at time t  t     is  euedc  t    eqdec  t  w    w   dvalue    
w  t dvalue  dcost  to calculate the eu of an entire strategy  we multiply the response
probability mass function s value at each instant by the eu of receiving a response at that
instant  and then integrate over the products  hence  the eu for a strategy s given a
problem instance  ha  ff      d  e i   is 
z  
h
a ff  

 d e
i
eus
 
p  t euedc  t   dt
   
   
if a strategy involves several actions  we need to ensure that the probability of response
function and the wait cost calculation reect the control situation at that point in the
strategy  for example  if the user  h   has control at time t  p   t  should reect h s
   

fiscerri  pynadath   tambe

w    

euad   eqda    
eued
d
euea

 

 



z

 
z t

 

eueddea  

p  t    eqde  t 
p  t    eqde  t 

w  t   dt  
w  t   dt  

 
p  t    eqde  t 


z

 

z

t

r

   

w  d   dt    

p  t  dt   eqda  t  

w  t       

d
 
  p   t  eqe  t  w  t   dt  
d
 p  t  eqe  t  w      w   dvalue   w  t dvalue   dcost   dt  
r 
d
t p   t  eqa  t  w      w   dvalue   w  t dvalue   dcost   dt

   

rt

table    general aa eu equations for sample transfer of control strategies 
probability of responding at t  i e   p h  t     to this end  we can break the integral from
equation   into separate terms  with each term representing one segment of the strategy 
e g   for a strategy ua there would be one term for when u has control and another for
when a has control 
using this basic technique for writing down eu calculations  we can write down the
specific equations for arbitrary transfer of control strategies  equations     in table  
show the eu equations for the strategies a  e   ea and e dea respectively  the equations
assume that the agent  a  can make the decision instantaneously  or at least  with no delay
significant enough to affect the overall value of the decision   the equations are created by
writing down the integral for each of the segments of the strategy  as described above  t
is the time when the agent takes control from e   and  is the time at which the d occurs 
one can write down the equations for more complex strategies in the same way  notice
that these equations make no assumptions about the particular functions 
given that the eu of a strategy can be calculated  the aa problem for the agent reduces
to finding and following the transfer of control strategy that will maximize its eu  formally 
the agent s problem is 

axiom     for a problem ha  ff      d  e i   the agent must select s   s such that  s   
s  s     s  eusha ff   d e i  eush a ff   d e i

   

fitowards adjustable autonomy for the real world

 
 
  
   

w    

   

   

   
   p

figure    graph comparing the eu of two strategies  h da  solid line  and h  dashed line 
given a particular instantiation of the model with constant expected decisionmaking quality  exponentially rising wait costs  and markovian response probabilities  p is a parameter to the p  t  function  with higher p meaning longer
expected response time  w is a parameter to the w  t  function with higher w
meaning more rapidly accruing wait costs 

    dominance relationships among strategies
an agent could potentially find the strategy with the highest eu by examining each and
every strategy in s  computing its eu  and selecting the strategy with the highest value  for
example  consider the problem for domains with constant expected decision making quality 
exponentially rising wait costs  and markovian response probabilities  figure   shows a
graph of the eu of two strategies  h da and h   given this particular model instantiation 
notice that  for different response probabilities and rates of wait cost accrual  one strategy
outperforms the other  but neither strategy is dominant over the entire parameter space 
the eu of a strategy is also dependent on the timing of transfers of control  which in turn
depend on the relative quality of the entities  decision making  appendix i provides a more
detailed analysis 
fortunately  we do not have to evaluate and compare each and every candidate in an
exhaustive search to find the optimal strategy  we can instead use analytical methods
to draw general conclusions about the relative values of different candidate strategies  in
particular  we present three lemmas that show the domain level conditions under which
particular strategy types are superior to others  the lemmas also lead us to the  perhaps
surprising  conclusion that complex strategies are not necessarily superior to single shot
strategies  even in a multi agent context  in fact  no particular strategy dominates all other
strategies across all domains 
let us first consider the aa subproblem of whether an agent should ever take back
control from another entity  if we can show that  under certain conditions  an agent should
always eventually take back control  then our strategy selection process can ignore any
strategies where the agent does not do so  i e   any strategies not ending in a   the agent s
goal is to strike the right balance between not waiting indefinitely for a user response and not
   

fiscerri  pynadath   tambe
taking a risky autonomous action  informally  the agent reasons that it should eventually
make a decision if the expected cost of continued waiting exceeds the difference between the
user s decision quality and its own  more formally  the agent should eventually take back
decision making control iff  for some time t 
z 
p   t   w  t    dt  w  t    eqdu  t  eqda  t 
   
t

where the left hand side calculates the future expected wait costs and the right hand side
calculates the extra utility to be gained by getting a response from the user  this result
leads to the following general conclusion about strategies that end with giving control back
to the agent 
lemma    if s   s isra strategy ending with e   e   and s  is sa  then eusd    eusd iff
 e   e   t    such that t p  t   w  t    dt  w  t    eqde  t  eqda t 
lemma   says that if  at any point in time  the expected cost of indefinitely leaving
control in the hands of the user exceeds the difference in quality between the agent s and
user s decisions  then strategies which ultimately give the agent control dominate those
which do not  thus  if the rate of wait cost accrual increases or the difference in the
relative quality of the decision making abilities decreases or the user s probability of response
decreases  then strategies where the agent eventually takes back control will dominate  a
key consequence of the lemma  in the opposite direction  is that  if the rate that costs accrue
does not accelerate  and if the probability of response stays constant  i e   markovian   then
the agent should indefinitely leave control with the user  if the user had originally been
given control   since the expected wait cost will not change over time  hence  even if the
agent is faced with a situation with potentially high total wait costs  the optimal strategy
may be a one shot strategy of handing over control and waiting indefinitely  because the
expected future wait costs at each point in time are relatively low  thus  lemma   isolates
the condition under which we should consider appending an a transfer of control action to
our strategy 
we can perform a similar analysis to identify the conditions under which we should
include a d action in our strategy  the agent has incentive in changing coordination
constraints via a d action due to the additional time made available for getting a highquality response from an entity  however  the overall value of a d action depends on
a number of factors  e g   the cost of taking the d action and the timing of subsequent
transfers of control   we can calculate the expected value of a d by comparing the eu of a
strategy with and without a d  the d is useful if and only if the increased expected value
of the strategy with it is greater than its cost  dcost  
lemma    if sr  s has no d and s  is s with a d included at t then eusd    eusd iff
r
p   t   w  t  dt 
p  t   w  tjd  dt    dcost
we can illustrate the consequences of lemma   by considering the specific problem model
of appendix i  i e   p   t     exp t   w  t      exp t   eqde  t    c  and candidate strategies
d iff        exp         exp  dvalue    
ea and e da   in this case  euedda   euea
dcost  figure   plots the value of the d action as we vary the rate of wait cost accumulation 
w  and the parameter of the markovian response probability function  p  the graph shows
   

fitowards adjustable autonomy for the real world

value
    
    
    
    
 
     
       
w           

 
    
    p
    

figure    the value of d action in a particular model  p   t     exp
and eqde  t    c  

t  

w  t      exp t  

that the benefit from the d is highest when the probability of response is neither too low
nor too high  when the probability of response is low  the user is unlikely to respond 
even given the extra time  hence  the agent will have incurred dcost with no benefit  a
d also has little value when the probability of response is high  because the user will likely
respond shortly after the d  meaning that it has little effect  the effect of the d is on the
wait costs after the action is taken   overall  according to lemma    at those points where
the graph goes above dcost   the agent should include a d action  and  at all other points  it
should not  figure   demonstrates the value of a d action for a specific subclass of problem
domains  but we can extend our conclusion to the more general case as well  for instance 
while the specific model has exponential wait costs  in models where wait costs grow more
slowly  there will be fewer situations where lemma   s criterion holds  i e   where a d will
be useful   thus  lemma   allows us to again eliminate strategies from consideration  based
on the evaluation of its criterion in the particular domain of interest 
given lemma   s evaluation of adding a single d action to a strategy  it is natural to
ask whether a second  third  etc  d action would increase eu even further  in other words 
when a complex strategy is better than a simple one  is an even more complex strategy even
better  the answer is  not necessarily  

 k   n   w   w   p   p   eq   eq such that the optimal strategy
d actions 
informally  lemma   says that we cannot fix a single  optimal number of d actions 
because for every possible number of d actions  there is a potential domain  i e   combination
lemma   

has k

of a wait cost  response probability  and expected quality functions  for which that number
of d actions is justified by being optimal  consider a situation where the cost of a d was
a function of the number of ds to date  i e   the cost of the k th d is f  k     for example 
in the e elves  meeting case  the cost of delaying a meeting for the third time is much
higher than the cost of the first delay  since each delay is successively more annoying to
other meeting participants  hence  the test for the usefulness of the k th d in a strategy 
   

fiscerri  pynadath   tambe
given the specific model in appendix i  is 

 

exp  exp  t  
   
f  k       exp dvalue        exp t


depending on the nature of f  k    equation   can hold for any number of ds  so  for any
k   there will be some conditions for which a strategy with k ds is optimal  for instance 
in section      we show that the maximum length of the optimal strategy for a random
configuration of up to    entities is usually less than eight actions 
equation   illustrates how the value of an additional d can be limited by changing dcost  
but lemma   also shows us that other factors can affect the value of an additional d  for
example  even with a constant dcost   the value of an additional d depends on how many
other d actions the agent performs  figure   shows that the value of the d depends on the
rate at which wait costs accrue  if the rate of wait cost accrual accelerates over time  e g  
for the exponential model   a d action slows that acceleration  rendering a second d action
less useful  since the wait costs are now accruing more slowly   notice also that ds become
valueless after the deadline  when wait costs stop accruing 
taken together  lemmas     show that no particular transfer of control strategy dominates all others across all domains  moreover  very different strategies  from single shot
strategies to arbitrarily complex strategies  are appropriate for different situations  although
the range of situations where a particular transfer of control action provides benefit can be
quite narrow  since a strategy might have very low eu for some set of parameters  choosing
the wrong strategy can lead to very poor results  on the other hand  once we understand
the parameter configuration of an intended application domain  lemmas     provide useful
tools for focusing the search for an optimal transfer of control strategy  the lemmas can
be used off line to substantially reduce the space of strategies that need to be searched to
find the optimal strategy  however  in general there may be many strategies and finding
the optimal strategy may not be possible or feasible 

    model predictions for the e elves
in this section  we use the model to predict properties of a successful approach to aa in
the e elves  using approximate functions for the probability of response  wait cost  and
expected decision quality  we can calculate the eu of various strategies and determine the
types of strategies that are going to be useful  armed with this knowledge  we can predict
some key properties of a successful implementation 
a key feature of the e elves is that the user is mobile  as she moves around the environment  her probability of responding to requests for decisions changes drastically  e g   she is
most likely to respond when at her workstation  to calculate the eu of different strategies 
we need to know p  t   which means that we need to estimate the response probabilities
and model how they change as the user moves around  when friday communicates via a
workstation dialog box  the user will respond  on average  in five minutes  however  when
friday communicates via a palm pilot the average user response time is an hour  users
generally take longer to decide whether they want to present at a research meeting  taking
approximately two days on average  so  the function p  t  should have an average value
of   minutes when the user in her oce  an average of one hour when the user is contacted
via a palm pilot and an average of two days when the decision is whether to present at a
   

fitowards adjustable autonomy for the real world
research meeting  it is also necessary to estimate the relative quality of the user  eqdu  t  
and friday s decision making  eqda  t   we assume that the user s decision making eqdu  t 
is high with respect to friday s  eqda  t   the uncertainty about user intentions makes it
very hard for friday to consistently make correct decisions about the time at which the user
will arrive at meetings  although its sensors  e g   gps device  give some indication of the
user s location  when dealing with more important meetings  the cost of friday s errors
is higher  thus  in some cases  the decision making quality of the user and friday will be
similar  i e   equd  t   eqad  t   while in other cases  there will be an order of magnitude
difference  i e   equd  t       eqad  t   the wait cost function  w  t   will be much larger for
big meetings than small and increase rapidly as other attendees wait longer in the meeting
room  finally  the cost of delays  i e   dcost   can vary by about an order of magnitude  in
particular  the cost of rescheduling meetings varies greatly  e g   the cost of rescheduling
small informal meetings with colleagues is far less than rescheduling a full lecture room at
  pm friday 
the parameters laid out above show how parameters vary from decision to decision  for
a specific decision  we use markovian response probabilities  e g   when the user is in her
oce  the average response time is five minutes   exponentially increasing wait costs  and
constant decision making quality  though it changes from decision to decision  to calculate
the eu of interesting strategies  calculating the eu of different strategies using the values
for different parameters shown above allows us to draw the following conclusions  table  
in section     presents a quantitative illustration of these predictions  

 the strategy e should not be used  since for all combinations of user location and
meeting importance the eu of this strategy is very low 

 multiple strategies are required  since for different user locations and meeting importance different strategies are optimal 

 since quite different strategies are required when the user is in different locations  the
aa reasoning will need to change strategies when the user changes location 

 no strategy has a reasonable eu for all possible parameter instantiations  hence always
using the same strategy will occasionally cause dramatic failures 

 for most decisions  strategies will end with the agent taking a decision  since strategies
ending with the user in control generally have very low eu 

these predictions provide important guidance about a successful solution for aa in the
e elves  in particular  they make clear that the approach must exibly choose between
different strategies and adjust depending on the meeting type and user location 
section     described the unsuccessful c    approach to aa in e elves and identified
several reasons for the mistakes that occurred  in particular  rigidly transferring control to
one entity and ignoring potential team costs involved in an agent s decision were highlighted
as reasons for the dramatic mistakes in friday s autonomy reasoning  reviewing the c   
approach in the light of the notion of strategies  we see that friday learned one strategy and
stuck with that strategy  in particular  originally  friday would wait indefinitely for a user
response  i e   it would follow strategy e   if it had learned to transfer control  as shown later
   

fiscerri  pynadath   tambe
in table    this strategy has a very low eu  when a fixed length timeout was introduced 
friday would follow strategy e    a  such a strategy has high eu when equd  t   eqad  t 
but very low eu when equd  t       eqad  t   thus  the model explains a phenomenon
observed in practice 
on the other hand  we can use the model to understand that c    s failure in this case
does not mean that it will never be useful for aa  different strategies are only required
when certain parameters  like probability of response or wait cost  change significantly  in
applications where such parameters do not change dramatically from decision to decision 
one particular strategy may always be appropriate  for such applications  c    might learn
the right strategy just with a small amount of training data and perform acceptably well 
  

operationalizing strategies with mdps

we have formalized the problem of aa as the selection of the transfer of control strategy with the highest eu  we now need an operational mechanism that allows an agent to
perform that selection  one major conclusion from the previous section is that different
strategies dominate in different situations  and that applications such as e elves will require mechanism s  for selecting strategies in a situation sensitive fashion  in particular 
the mechanism must exibly change strategies as the situation changes  the required mechanism must also represent the utility function specified by our expected decision qualities 
eq  the costs of violating coordination constraints  w  and our coordination change cost 
dcost  finally  the mechanism must also represent the uncertainty of entity responses and
then look ahead over the possible responses  or lack thereof  that may occur in the future 
mdps are a natural means of performing the decision theoretic planning required to find
the best transfer of control strategy  mdp policies provide a mapping between the agent s
state and the optimal transfer of control strategy  by encoding the parameters of the model
of aa strategies into the mdp  the mdp effectively becomes a detailed implementation of
the model and  hence  assumes its properties  we can use standard algorithms  puterman 
      to find the optimal mdp policy and  hence  the optimal strategies to follow in each
state 
to simplify exposition  as well as to illustrate the generality of the resulting mdp  this
section describes the mapping from aa strategies to the mdp in four subsections  in
particular  section     provides a direct mapping of strategies to an abstract mdp  section
    fills in state features to enable a more concrete realization of the reward function  while
still maintaining a domain independent view  thus  the section completely defines a general
mdp for aa is potentially reusable across a broad class of domains  section     illustrates
an implemented instantiation of the mdp in e elves  section     addresses further practical
issues in operationalizing such mdps in domains such as e elves 

    abstract mdp representation of aa problem
our mdp representation s fundamental state features capture the state of control 

 controlling entity is the entity that currently has decision making control 
 ei  response is any response ei has made to the agent s requests for input 
   

fitowards adjustable autonomy for the real world
original state action
destination state
ectrl time
ectrl ei  response
time
ej
tk
ei
ei
yes
tk  
ej
tk
ei
ei
no
tk  
ei
tk
wait
ei
yes
tk  
ei
tk
wait
ei
no
tk  
ei
tk
d
ei
no
tk dvalue

probability
 
 

r tk   ei
tkr p   t dt
tk   ei
tk p   t dt
r tk  
ei
tkr p   t dt
tk   ei
tk p   t dt

 

table    transition probability function for aa mdp  ectrl is the controlling entity 



time is the current time  typically discretized and ranging from   to our deadline 
   i e   a set ft       t    t            tn   g 

if ei  response is not null or if time     then the agent is in a terminal state  in the former
case  the decision is the value of ei  response 
we can specify the set of actions for this mdp representation as   e  fd  waitg  the
set of actions subsumes the set of entities  e   since the agent can transfer decision making
control to any one of these entities  the d action is the coordination change action that
changes coordination constraints  as discussed earlier  the  wait  action puts off transferring control and making any autonomous decision  without changing coordination with the
team  the agent should reason that  wait  is the best action when  in time  the situation
is likely to change to put the agent in a position for an improved autonomous decision or
transfer of control  without significant harm  for example  in the e elves domain  at times
closer to a meeting  users can generally make more accurate determinations about whether
they will arrive on time  hence it is sometimes useful to wait when the meeting is a long
time off 
the transition probabilities  specified in table    represent the effects of the actions as
a distribution over their effects  i e   the ensuing state of the world   if  in a state with
time   tk   the agent chooses an action that transfers decision making control to an entity 
ei   other than the agent itself  the outcome is a state with controlling entity   ei and
time   tk     there are two possible outcomes for ei  response  either the entity responds
with a decision during this transition  producing a terminal state   or it does not  and we
derive the probability distribution over the two from p  the  wait  action has a similar
branch  except that the controlling entity remains unchanged  finally  the d action occurs
instantaneously  so there is no time for the controlling entity to respond  but the resulting
state effectively moves to an earlier time  e g   from tk to tk dvalue   
we can derive the reward function for this mdp in a straightforward fashion from
our strategy model  table   presents the complete specification of this reward function 
in transitions that take up time  i e   transferring control and not receiving a response
 table    row    or  wait   table    row     the agent incurs the wait cost of that interval 
in transitions where the agent performs d  the agent incurs the cost of that action  table   
row     in terminal states with a response from ei   the agent derives the expected quality of
that entity s decision  table    row     a policy that maximizes the reward that an agent
expects to receive according to this aa mdp model will correspond exactly to an optimal
   

fiscerri  pynadath   tambe
controlling entity time ei  response action
ej
tk
no
ei
ei
tk
no
wait
ei
tk
no
d
ei
tk
yes

reward

w  k      w  k 
w  k      w  k 
dcost
eqdei  tk  

table    reward function for aa mdp 
transfer of control strategy  note that this reward function is described in an abstract
fashion for example  it does not specify how to compute the agent s expected quality of
decision  eqad  t  

    mdp representation of aa problem within team context
we have now given a high level description of an mdp for implementing the notion of
transfer of control strategies for aa  the remainder of this section provides a more detailed
look at the mdp for a broad class of aa domains  including the e elves  where the agent
acts on behalf of a user who is filling a role    within the context of a team activity  ff 
the reward function compares the eu of different strategies  finding the optimal one for
the current state  to facilitate this calculation  we need to represent the parameters used
in the model  we introduce the following state features to capture the aspects of the aa
problem in a team context 

 team orig expect  is what the team originally expected of the fulfilling of  
 team expect  is the team s current expectations of what fulfilling the role  implies 
 agent expect  is the agent s  probabilistic  estimation for how  will be fulfilled 
  other ff attributes  encapsulate other aspects of the joint activity that are affected
by the decision 

when we add these more specific features to the generic aa state features already
presented  the overall state  within the mdp representation of a decision d  is a tuple 

hcontrolling entity  team orig expect   team expect   agent expect   ff status 
ei  response  time  other ff attributesi
for example  for a meeting scenario  team orig expect  could be  meet at  pm   teamexpect  could be  meet at     pm  after a user requested a delay  and agent expect  could
be  meet at     pm  if the agent believes its user will not make the rescheduled meeting 
the transition probability function for the aa mdp in a team context includes our
underlying aa transition probabilities from table    but it must also include probabilities
over these new state features  in particular  in addition to the temporal effect of the
d action described in section      there is the additional effect on the coordination of ff 
the d action changes the value of the team expect  feature  in a domain dependent but
   

fitowards adjustable autonomy for the real world
deterministic way   no other actions affect the team s expectations  the team orig expect 
feature does not change  we include it to simplify the definition of the reward function  the
transition probabilities over agent expect  and other ff specific features are domain specific 
we provide an example of such transition probabilities in section     
the final part of the mdp representation is the reward function  our team aa mdp
framework uses a reward function that breaks down the function from table   as follows 

r s  a    f  team orig expect  s   team expect   s   agent expect   s  
ff status  s   time s   a 
x
 
eqde  time s    e  response

   

e  e nfag

  f   k team orig expect   s  team expect   s  k 
   f    time s  
   f    k team expect  s  agent expect  s  k 
   f   ff status  s       f   a 

    

the first component of the reward function captures the value of getting a response from
a decision making entity other than the agent itself  notice that only one entity will actually
respond  so only one e  response will be non zero  this corresponds to the eqed  t  function
used in the model and the bottom row of table    the f  function reects the inherent
value of performing a role as the team originally expected  hence deterring the agent from
taking costly coordination changes unless they can gain some indirect value from doing
so  this corresponds to dcost from the mathematical model and the third row of table   
the f   corresponds to the second row of table    so it represents the wait cost function 
w  t   from the model  this component encourages the agent to keep other team members
informed of the role s status  e g   by making a decision or taking an explicit d action  
rather than causing them to wait without information  functions f   and f  represent
the quality of the agent s decision  represented by qad  t   the standard mdp algorithms
compute an expectation over the agent s reward  and an expectation over this quality will
produce the desired eqad  t  from the fourth row of table    the first quality function  f    
reects the value of keeping the team s understanding of how the role will be performed in
accordance with how the agent expects the user to actually perform the role  the agent
receives most reward when the role is performed exactly as the team expects  but because of
the uncertainty in the agent s expectation  errors are possible  f   represents the costs that
come with such errors  the second quality component  f    inuences overall reward based
on the successful completion of the joint activity  which encourages the agent to take actions
that maximize the likelihood that the joint activity succeeds  the desire to have the joint
task succeed is implicit in the mathematical model but must be explicitly represented in the
mdp  the component  f    augments the first row from table   to account for additional
costs of transfer of control actions  in particular  f  can be broken down further as follows 
 

f   a   

q e   if a   e
 
otherwise
   

    

fiscerri  pynadath   tambe
the function q e   represents the cost of transferring control to a particular entity  e g   the
cost of a wap phone message to a user  notice  that these detailed  domain specific costs
do not appear directly in the model 
given the mdp s state space  actions  transition probabilities  and reward function 
an agent can use value iteration to generate a policy p   s   that specifies the optimal
action in each state  puterman         the agent then executes the policy by taking the
action that the policy dictates in each and every state in which it finds itself  a policy
may include several transfers of control and coordination change actions  the particular
series of actions depends on the activities of the user  we can then interpret this policy as
a contingent combination of many transfer of control strategies  with the strategy to follow
chosen depending on the user s status  i e   agent expect   

    example  the e elves mdps
an example of an aa mdp is the generic delay mdp  which can be instantiated for any
meeting for which friday may act on behalf of its user  recall the decision  d  is whether
to let other meeting attendees wait for a user or to begin their meeting  the joint activity 
ff  is the meeting in which the agent has the role    of ensuring that its user attends the
meeting at the scheduled time  the coordination constraints    are that the attendees
arrive at the meeting location simultaneously and the effect of the d action is to delay or
cancel the meeting 
in the delay mdp s state representation  team orig expect  is originally scheduledmeeting time  since attendance at the originally scheduled meeting time is what the team
originally expects of the user and is the best possible outcome  team expect  is timerelative to meeting  which may increase if the meeting is delayed  ff status becomes statusof meeting  agent expect  is not represented explicitly  instead  user location is used as
an observable heuristic of when the user is likely to attend the meeting  for example  a
user who is away from the department shortly before a meeting should begin is unlikely to
be attending on time  if at all  with all the state features  the total state space contains
     states for each individual meeting  with the large number of states arising from a very
fine grained discretization of time 
the general reward function is mapped to the delay mdp reward function in the following way 
 

g n  ff  if n    
    
 
otherwise
where n is the number of times the meeting is rescheduled and g is a function that takes
into account factors like the number of meeting attendees  the size of the meeting delay and
the time until the originally scheduled meeting time  this function effectively forbids the
agent from ever performing   or more d actions 
in the delay mdp  the functions  f   and f     both correspond to the cost of making the
meeting attendees wait  so we can merge them into a single function  f    we expect that
such a consolidation is possible in similar domains where the team s expectations relate to
f   

   

fitowards adjustable autonomy for the real world
the temporal aspect of role performance 
 

f   

h late  ff  if late    
 
otherwise

    

where late is the difference between the scheduled meeting time and the time the user
arrives at the meeting room  late is probabilistically calculated by the mdp based on the
user s current location and a model of the user s behavior 
 
 
 

rff   ruser if the user attends
f      rff
if the meeting takes place  but the user does not attend
   
otherwise

    

the value  rff   models the inherent value of ff  while the value ruser models the user s
individual value to ff 
f  was given previously in equation     the cost of communicating with the user
depends on the medium which is used to communicate  for example  there is higher cost
to communicating via a wap phone than via a workstation dialog box 
when the users are asked for input  it is assumed that  if they respond  their response
will be  correct   i e   if a user says to delay the meeting by    minutes  we assume the
user will arrive on time for the re scheduled meeting  if the user is asked while in front of
his her workstation  a dialog like the one shown in figure   is popped up  allowing the user
to select the action to be taken  the expected quality of the agent s decision is calculated
by considering the agent s proposed decision and the possible outcomes of that decision 
for example  if the agent proposes delaying the meeting by    minutes  the calculation of
the decision quality includes the probability and benefits that the user will actually arrive
   minutes after the originally scheduled meeting time  the probability and costs that the
user arrives at the originally scheduled meeting time  etc 

 a 

 b 

figure     a  dialog box for delaying meetings   b  a small portion of the delay mdp 
the delay mdp also represents probabilities that a change in user location  e g   from
oce to meeting location  will occur in a given time interval  figure   b  shows a portion
   

fiscerri  pynadath   tambe
of the state space  showing only the user response  and user location features  a transition
labeled  delay n  corresponds to the action  delay by n minutes   the figure also shows
multiple transitions due to  ask   i e   transfer control to the user  and  wait  actions  where
the relative probability of each outcome is represented by the thickness of the arrow  other
state transitions correspond to uncertainty associated with a user s response  e g   when the
agent performs the  ask  action  the user may respond with specific information or may
not respond at all  leaving the agent to effectively  wait    one possible policy produced
by the delay mdp  for a subclass of meetings  specifies  ask  in state s  of figure   b 
 i e   the agent gives up some autonomy   if the world reaches state s   the policy specifies
 wait   however  if the agent then reaches state s   the policy chooses  delay      which
the agent then executes autonomously  in terms of strategies  this sequence of actions is
h d 
earlier  we described another aa decision in the e elves  namely whether to close an
auction for an open team role  here  we briey describe the key aspects of the mapping
of that decision to the mdp  the auction must be closed in time for the user to prepare
for the meeting  but with sucient time given for interested users to submit bids and for
the human team leader to choose a particular user  team orig expect   s  is that a highquality presenter be selected with enough time to prepare  there is no d action  hence
team expect   s    team orig expect   s   agent expect  s  is whether the agent believes it
has a high quality bid or believes such a bid will arrive in time for that user to be allocated
to the role  the agent s decision quality  eqda  t   is a function of the number of bids that
have been submitted and the quality of those bids  e g   if all team members have submitted
bids and one user s bid stands out  the agent can confidently choose that user to do the
presentation  thus  ff status is primarily the quality of the best bid so far and the difference
between the quality of that bid and the second best bid  the most critical component of
the reward function from equation    is the   component  which gives reward if the agent
fulfills the users  expectation of having a willing presenter do a high quality presentation 

    user specified constraints
the standard mdp algorithms provide the agent with optimal policies subject to the encoded probabilities and reward function  thus  if the agent designer has access to correct
models of the entities   e g   human users in the e elves  decision qualities and probabilities of response  then the agent will select the best possible transfer of control strategy 
however  it is possible that the entities themselves have more accurate information about
their own abilities than does the agent designer  to exploit this knowledge  an entity could
communicate its model of its quality of decision and probability of response directly to
the agent designer  unfortunately  the typical entity is unlikely to be able to express its
knowledge in the form of our mdp reward function and transition probabilities  an agent
could potentially learn this additional knowledge on its own through its interactions with
the entities in the domain  however  learning may require an arbitrarily large number of
such interactions  all of which will take place without the benefit of the entities  inside
knowledge 
as an alternative  we can provide a language of constraints that allows the entities to
directly and immediately communicate their inside information to the agent  our constraint
   

fitowards adjustable autonomy for the real world

figure    screenshot of the tool for entering constraints  the constraint displayed forbids
not transferring control  i e   forces transfer  five minutes before the meeting if the
teammates have previously been given information about the user s attendance
at the meeting 
language provides the entities a simple way to inform the agent of their specific properties
and needs  an entity can use a constraint to forbid the agent from entering specific states or
performing specific actions in specific states  such constraints can be directly communicated
by a user via the tool shown in figure    for instance  in the figure shown the user is
forbidding the agent from autonomous action five minutes before the meeting  we define
such forbidden action constraints to be a set  cfa   where each element constraint is a
boolean function  cfa   s  a  ft  f g  similarly  we define forbidden state constraints to
be a set  cfs   with elements  cfs   s  ft  f g  if a constraint returns t for a particular domain
element  either state or state action pair  as appropriate   then the constraint applies to the
given element  for example  a forbidden action constraint  cfa   forbids the action a from
being performed in state s if and only if cfa  s  a    t 
to provide probabilistic semantics  suitable for an mdp context  we first provide some
notation  denote the probability that the agent will ever arrive in state sf after following
 s jp    then  we define the semantics of
a policy  p   from an initial state si as pr si  
f
 s jp        the semantics given to a
a forbidden state constraint cfs as requiring pr si  
f
 s  p  s   ajp      
forbidden action constraint  cfa   is a bit more complex  requiring pr si 
f
f
 i e   cfa forbids the agent from entering state sf and then performing action a   in some
cases  an aggregation of constraints may forbid all actions in state sf   in this case  the
conjunction allows the agent to still satisfy all forbidden action constraints by avoiding sf
 i e   the state sf itself becomes forbidden   once a state  sf   becomes indirectly forbidden
in this fashion  any action that potentially leads the agent from an ancestor state into
sf likewise becomes forbidden  hence  the effect of forbidding constraints can propagate
backward through the state space  affecting state action pairs beyond those which cause
immediate violations 
   

fiscerri  pynadath   tambe
the forbidding constraints are powerful enough for the entity to communicate a wide
range of knowledge about their decision quality and probability of response to the agent 
for instance  some e elves users have forbidden their agents from rescheduling meetings
to lunch time  to do so  the users provide a feature specification of the states they want
to forbid  such as meeting time     pm  such a specification generates a forbidden state
constraint  cfs   that is true in any state  s  where meeting time     pm in s  this constraint
effectively forbids the agent from performing any d action that would result in a state where
meeting time    pm  similarly  some users have forbidden autonomous actions in certain
states by providing a specification of the actions they want to forbid  e g   action    ask  
this generates a forbidden action constraint  cfa   that is true for any state action pair 
 s  a   with a    ask   for example  a user might specify such a constraint for states
where they are in their oce  at the time of a meeting because they know that they will
always make decisions in that case  users can easily create more complicated constraints
by specifying values for multiple features  as well as by using comparison functions other
than    e g          
analogous to the forbidding constraints  we also introduce required state and requiredaction constraints  defined as sets  crs and cra   respectively  the interpretation provided
to the required state constraint is symmetric  but opposite to that of the forbidden state
 s jp        thus  from any state  the agent must eventually reach a
constraint  pr si  
f
 s  p  s   ajp       
required state  sf   similarly  for the required action constraint  pr si 
f
f
the users specify such constraints as they do for their forbidding counterparts  i e   by specifying the values of the relevant state features or action  as appropriate   in addition  the
requiring constraints also propagate backward  informally  the forbidden constraints focus
locally on specific states or actions  while the required constraints express global properties
over all states 
the resulting language allows the agent to exploit synergistic interactions between its
initial model of transfer of control strategies and entity specified constraints  for example 
a forbidden action constraint that prevents the agent from taking autonomous action in a
particular state is equivalent to the user specifying that the agent must transfer control to
the user in that state  in aa terms  the user instructs the agent not to consider any transferof control strategies that violate this constraint  to exploit this pruning of the strategy
space by the user  we have extended standard value iteration to also consider constraint
satisfaction when generating optimal strategies  appendix ii provides a description of a
novel algorithm that finds optimal policies while respecting user constraints  the appendix
also includes a proof of the algorithm s correctness 
   experimental results

this section presents experimental results aimed at validating the claims made in the previous sections  in particular  the experiments aim to show the utility of complex transfer ofcontrol strategies and the effectiveness of mdps as a technique for their operationalization 
section     details the use of the e elves in daily activities and section     discusses the
pros and cons of living and working with the assistance of fridays  section     shows some
characteristics of strategies in this type of domain  in particular  that different strategies
   

fitowards adjustable autonomy for the real world
are used in practice   finally  section     describes detailed experiments that illustrate
characteristics of the aa mdp 

    the e elves in daily use
the e elves system was heavily used by ten users in a research group at isi  between june
     and december        the friday agents ran continuously  around the clock  seven
days a week  the exact number of agents running varied over the period of execution  with
usually five to ten friday agents for individual users  a capability matcher  with proxy  
and an interest matcher  with proxy   occasionally  temporary friday agents operated on
behalf of special guests or other short term visitors 
daily counts of exchanged messages
no  of messages

   
   
   
   
   
  
 
jun jul aug sep oct nov dec
date

figure    number of daily coordination messages exchanged by proxies over a seven month
period 
figure   plots the number of daily messages exchanged by the fridays over seven months
 june through december         the size of the daily counts reects the large amount of
coordination necessary to manage various activities  while the high variability illustrates
the dynamic nature of the domain  note the low periods during vacations and final exams  
figure   a  illustrates the number of meetings monitored for each user  over the seven
months  nearly     meetings where monitored  some users had fewer than    meetings 
while others had over      most users had about     of their meetings delayed  this includes
regularly scheduled meetings that were cancelled  for instance due to travel   figure   b 
shows that usually     or more of delayed meetings were autonomously delayed  in this
graph  repeated delays of a single meeting are counted only once  the graphs show that the
   the user base of the system was greatly reduced after this period due to personnel relocations and
student graduations  but it remains in use with a smaller number of users 

   

fiuser delays vs  autonomous delays

meetings monitored vs  meetings delayed
   
   
   
   
   
   
   
  
 

   

number of meetings

ito

ramanan

tambe

nair

scerri

modi

pynadath

jungh

total delays
human delays

   

monitored
delayed

kulkarni

number of meetings

scerri  pynadath   tambe

   
  
  
  
  
 
 

users

 

 

 

 

 

 

 

users

 a 

 b 

figure     a  monitored vs  delayed meetings per user   b  meetings delayed autonomously
vs  by hand 
agents are acting autonomously in a large number of instances  but  equally importantly 
humans are also often intervening  indicating the critical importance of adjustable autonomy
in friday agents 
for a seven month period  the presenter for usc isi s teamcore research group
presentations was decided using auctions  table   shows a summary of the auction results 
column     date   shows the dates of the research presentations  column     no  of
bids   shows the total number of bids received before a decision  a key feature is that
auction decisions were made without all   users entering bids  in fact  in one case  only
  bids were received  column     best bid   shows the winning bid  a winner typically
bid           i e   indicating that the user it represents is both capable and willing to
do the presentation   a high quality bid  interestingly  the winner on july    made a
bid of           i e   not capable but willing  the team was able to settle on a winner
despite the bid not being the highest possible  illustrating its exibility  finally  columns
    winner   and     method   show the auction outcome  an  h  in column   indicates
the auction was decided by a human  an  a  indicates it was decided autonomously  in five
of the seven auctions  a user was automatically selected to be presenter  the two manual
assignments were due to exceptional circumstances in the group  e g   a first time visitor  
again illustrating the need for aa 
date
no  of bids best bid winner method
jul        
 
   
scerri
h
jul         
 
   
scerri
a
jul         
 
   
kulkarni
a
aug        
 
   
nair
a
aug        
 
   
tambe
a
sept         
 
  visitor
h
oct         
 
   
tambe
a
table    results for auctioning research presentation slot 
   

fitowards adjustable autonomy for the real world
    evaluating the pros and cons of e elves use
the general effectiveness of the e elves is shown by several observations  during the
e elves  operation  the group members exchanged very few email messages to announce
meeting delays  instead  fridays autonomously informed users of delays  thus reducing the
overhead of waiting for delayed members  second  the overhead of sending emails to recruit
and announce a presenter for research meetings was assumed by agent run auctions  third 
a web page  where friday agents post their users  location  was commonly used to avoid
the overhead of trying to track users down manually  fourth  mobile devices kept users
informed remotely of changes in their schedules  while also enabling them to remotely delay
meetings  volunteer for presentations  order meals  etc  users began relying on friday so
heavily to order lunch that one local  subway  restaurant owner even suggested         more
and more computers are getting to order food      so we might have to think about marketing
to them     notice that this daily use of the e elves by a number of different users occurred
only after the mdp implementation of aa replaced the unreliable c    implementation 
however  while the agents ensured that users spent less time on daily coordination  and
miscoordination   there was a price to be paid  one issue was that users felt they had
less privacy when their location was continually posted on the web and monitored by their
agent  another issue was the security of private information such as credit card numbers
used for ordering lunch  as users adjusted to having agents monitor their daily activities 
some users adjusted their own behavior around that of the agent  one example of such
behavior was some users preferring to be a minute or two early for a meeting lest their
agent decide they were late and delay the meeting  in general  since the agents never made
catastrophically bad decisions most users felt comfortable using their agent and frequently
took advantage of its services 
the most emphatic evidence of the success of the mdp approach is that  since replacing
the c    implementation  the agents have never repeated any of the catastrophic mistakes
enumerated in section      in particular  friday avoids errors such as error   from section
    by selecting a strategy with a single  large d action  because it has a higher eu than a
strategy with many small ds  e g   dddd   friday avoids error    because the large cost
associated with an erroneous cancel action significantly penalizes the eu of a cancellation 
friday instead chooses the higher eu strategy that first transfers control to a user before
taking such an action autonomously  friday avoids errors such as errors   and   by selecting
strategies in a situation sensitive manner  for instance  if the agent s decision making
quality is low  i e   high risk   then the agent can perform a coordination change action to
allow more time for user response or for the agent itself to get more information  in other
words  it exibly uses strategies like e dea  rather than always using the e    a strategy
discussed in section      this indicates that a reasonably appropriate strategy was chosen
in each situation  although the current agents do occasionally make mistakes  these errors
are typically on the order of transferring control to the user a few minutes earlier than may
be necessary  thus  the agents  decisions have been reasonable  though not always optimal  
   the inherent subjectivity in user feedback makes a determination of optimality dicult 

   

fiscerri  pynadath   tambe
    strategy evaluation
the previous section looked at the application of the mdp approach to the e elves but did
not address strategies in particular  in this section  we specifically examine strategies in the
e elves  we show that fridays did indeed follow strategies and that the strategies followed
were the ones predicted by the model  we also show how the model led to an insight that 
in turn  led to a dramatic simplification in one part of the implementation  finally  we show
that the use of strategies is not limited to the e elves application by showing empirically
that  for random configurations of entities  the optimal strategy will have more than one
transfer of control action in     of cases 
figure   shows a frequency distribution of the number of actions taken per meeting
 this graph omits  wait  actions   the number of actions taken for a meeting corresponds
to the length of the part of the strategy followed  the strategy may have been longer  but
a decision was made so the actions were not taken   the graph shows both that the mdp
followed complex strategies in the real world and that it followed different strategies at
different times  the graph bears out the model s predictions that different strategies would
be required of a good solution to the aa problem in the e elves domain 
table   shows the eu values computed by the model and the strategy selected by
the mdp  recall that the mdp explicitly models the users  movements between locations 
while the model assumes that the users do not move  hence  in order to do an accurate
comparison between the model and the mdp s results  we focus on only those cases when
the user s location does not change  i e   where the probability of response is constant  
these eu values were calculated using the parameter values set out in section      notice 
that the mdp will often perform ds before transferring control to buy time to reduce
uncertainty  the model is an abstraction of the domain  so such d actions  like changes
in user location  are not captured  except for a slight discrepancy in the first case the
match between the mdp s behavior and the model s predictions is exact  provided that we
ignore the d actions at the beginning of some mdp strategies  thus  despite the model
being considerably abstracted from the domain there is high correlation between the mdp
policies and the model s suggested strategies  moreover  general properties of the policies
that were predicted by the model were borne out exactly  in particular  recall that the model
predicted different strategies would be required  that strategy e would not be used  and that
generally strategies ending in a would be best   all properties of the mdp policies 
the model predicts that if parameters do not vary greatly then it is sucient to find
a single optimal strategy and follow that strategy in each situation  the mdp for the
decision to close an auction is an instance of this for the e elves  the same pattern of
behavior is followed every time an open role needs to be filled by the team  this consistency
arises because the wait cost is the same  since the meetings are the same  and because
the pattern of incoming bids is reasonably consistent  variations in individuals  behavior
cancel each other out when we look at the team as a whole   the model predicts that
when parameters do not change  we can find the optimal strategy for those parameters
and execute that strategy every time  however  since the mdp had worked effectively for
the meeting aa  an mdp was also chosen for implementing the auction aa  when it was
realized that the parameters do not vary greatly  we concluded the mdp could be replaced
with a simple implementation of the optimal strategy  to verify this hypothesis  we replaced
   

fitowards adjustable autonomy for the real world

no  of meetings

no  of actions per meeting
   
   
   
   
   
   
  
  
  
  
 
 

 

 
 
 
no  of actions

  

  

figure    the frequency distribution of the number of steps taken in an aa strategy for
the meeting scenario  if no actions were taken for a meeting  the meeting was
cancelled before friday started aa reasoning 

location
a
e
ea e da mdp
small meeting  active participant
oce
                     dde da
not   dept         e            ddea
  meet loc         e           
ea
large meeting  passive participant
oce
       e               ddea
not   dept         e           
ddea
  meet loc         e            
ea
table    eu values for the simple strategies as calculated from the model  the last column
shows the strategy actually followed by the mdp 

   

fiscerri  pynadath   tambe
date no  bids mdp ea
       
 
       
       
 
       
      
 
       
table    auction results  the  mdp  column shows the percentage of available auction
time remaining when the mdp chose to close the auction  the  ea  column
shows the percentage of available auction time remaining when the strategy ea 
with eqde  t  proportional to the number of bids received   no  bids  column  
would have closed the auction 

the general mdp code with three simple lines of code implementing the ea strategy  which
we determined to be optimal for the particular parameters of the problem  using log files
recorded during the actual auctions reported in  scerri  pynadath    tambe         we
experimentally verified that both the mdp and the ea strategy produced the same result 
table   shows the percentage of available auction time remaining  e g   if the auction was
opened four days before the role should be performed  closing the auction one day before
would correspond to      when the mdp version and the ea version of the code closed
the auction  the number of bids is used to estimate the agent s expected decision quality 
the timing of the auction closing is close  certainly within just a few hours  the result
is not precisely the same for the mdp and strategy implementations  because the mdp
implementation was more reactive to incoming bids than the strategy implementation 
to confirm that the need for strategies was not a phenomenon unique to the particular
settings of the e elves  an experiment was run with randomly generated configurations
of entities  the wait cost for each configuration increased exponentially  with the rate of
accrual varying from configuration to configuration  the configurations contained between
  and    entities  with randomly chosen markovian response probabilities and randomly
chosen  but constant  decision making quality  the cost and value of a d action was also
randomly selected  in each configuration  there was an agent that could respond instantly 
but with lower decision quality than any of the other entities  for each configuration  the
optimal transfer of control strategy was found  figure    a  shows the percentage of optimal
strategies  z axis  that were of each length  y axis  jopt  strat j    separated according to
the rate at which wait costs accrued  x axis   wait cost param    the figure shows that
if the rate at which the wait cost accrues is very low  most optimal strategies are of length
one  with the agent just handing control to the entity with the highest decision making
quality  when the rate of wait cost accrual is high  most strategies are of length two 
with the agent briey giving the best decision maker an opportunity to make the decision
but taking back control and acting before the wait costs became too high  for intermediate
values of the wait cost parameter  there was considerably more variation in the length of the
optimal strategy  figure    b  shows the percentage of optimal strategies for each length
when the wait cost parameter is       i e   a slice through figure    a    hence  strategies
often contained several transfers of control and several coordination changes  thus  this
experiment shows that complex transfer of control strategies are useful  not only in e elves 
   

fitowards adjustable autonomy for the real world
but in a range of domains  especially those in which wait costs are neither negligible nor
accruing too fast 
strategy lengths for w       
  

  of opt  strats 

  
  
  of opt  strats 

   
  
  
  
  
  
  
  
  
  
  

 

  
  
  

 

 opt  strat  

 

 

 

 

   

        
        
        
   
wait
cost
param
    

 
 
 

 a 

 

 

 
 
 opt  strat  

 

 

 

 b 

figure      a  percentage of optimal strategies having a certain length  broken down according to how fast wait costs are accruing   b  percentage of optimal strategies
having certain length for wait cost parameter        
thus  we have shown that the mdp produces strategies and that friday follows these
strategies in practice  moreover  the strategies followed are the ones predicted by the model 
of practical use  when we followed a prediction of the model  i e   that an mdp was not
required for auctions  we were able to substantially reduce the complexity of one part of the
system  finally  we showed that the need for strategies was not specifically a phenomenon
of the e elves domain 

    mdp experiments
experience using the mdp approach to aa in the e elves indicates that it is effective
at making reasonable aa decisions  however  in order to determine whether mdps are
a generally useful tool for aa reasoning  more systematic experiments are required  in
this section  we present such systematic experiments to determine important properties of
mdps for aa  the mdp reward function is designed to result in the optimal strategy being
followed in each state 
in each of the experiments  we vary one of the  parameters that are the weights of the
different factors in equation     the mdp is instantiated with each of a range of values
for the parameter and a policy produced for each value  in each case  the total policy is
defined over      states  the policy is analyzed to determine some basic properties of that
policy  in particular  we counted the number of states in which the policy specifies to ask 
to delay  to say the user is attending and to say the user is not attending  the statistics
show broadly how the policy changes as the parameters change  e g   whether friday gives
up autonomy more or less when the cost of a coordination change is increased  the first
aim of the experiments is to simply confirm that policies change in the desired and expected
way when parameters in the reward function are changed  for instance  if friday s expected
decision quality is increased  there should be more states where it makes an autonomous
   

fiscerri  pynadath   tambe
decision  secondly  from a practical perspective it is critical to understand how sensitive the
mdp policies are to small variations in parameters  because such sensitivity would mean
that any small variations in parameter values can significantly impact mdp performance 
finally  the experiments reveal some interesting phenomena 
the first experiment looks at the effect of the   parameter from equation     represented in the delay mdp implementation by the team repair cost  function g from equation
     on the policies produced by the delay mdp  this parameter determines how averse friday should be to changing coordination constraints  figure    shows how some properties
of the policy change as the team repair cost value is varied  the x axis gives the value of
the team repair cost  and the y axis gives the number of times that action appears in the
policy  figure    a  shows the number of times friday will ask the user for input  the
number of times it will transfer control exhibits an interesting phenomenon  the number
of asks has a maximum at an intermediate value for the parameter  for the low values 
friday can  confidently   i e   its decision quality is high  make decisions autonomously 
since the cost of errors is low  hence there is less value to relinquishing autonomy  for
very high team repair costs  friday can  confidently  decide autonomously not to make a
coordination change  it is in the intermediate region that friday is uncertain and needs
to call on the user s decision making more often  furthermore  as the cost of delaying the
meeting increases  friday will delay the meeting less  figure    b   and tell the team the
user is not attending more often  figure    d    by doing so  friday gives the user less time
to arrive at the meeting  choosing instead to just announce that the user is not attending 
essentially  friday s decision quality has become close enough to the user s decision quality
that asking the user is not worth the risk that they will not respond and the cost of asking
for their input  except for a jump between a value of zero and any non zero value  the
number of times friday says the user is attending does not change  figure    c    the
delay mdp in use in the e elves has the team repair cost parameter set at two  around
this value the policy changes little  hence slight changes in the parameter do not lead to
large changes in the policy 
in the second experiment  we vary the   parameter from equation     implemented
in the delay mdp by the variable team wait cost  function h from equation      this is
the factor that determines how heavily friday should weigh differences between how the
team expects the user will fulfill the role and how the user will actually fulfill the role  in
particular  it determines the cost of having other team members wait in the meeting room
for the user  figure    shows the changes to the policy when this parameter is varied  again
the x axis shows the value of the parameter and the y axis shows the number of times the
action appears in the policy   the graph of the number of times the agent asks in the
policy  figure    a    exhibits the same phenomena as when the   parameter was varied 
i e   increasing and then decreasing as the parameter increases  the graphs show that  as
the cost of teammates  time increases  friday acts autonomously more often  figure    bd    friday asks whenever the potential costs of asking are lower than the potential costs
of errors it makes   as the cost of time waiting for a user decision increases  the balance
tips towards acting  notice that the phenomenon of the number of asks increasing then
decreasing occurs in the same way that it did for the   parameter  however  it occurs for a
slightly different reason  in this case  when waiting costs are low  friday s decision making
quality is high so it acts autonomously  when the waiting costs are high  friday cannot
   

fitowards adjustable autonomy for the real world

number of delays in policy

  
  
  
  
  
  
  
  
  
  
  

  delays

  asks

number of asks in policy

 

 
 
 
 
 team repair cost  weight

   
   
   
   
   
  
  
  
  
  
  
  

  

 

 a 

number of not attending messages in policy
  not attending

  attending

   
   
   
   
   
   
   
   
   
  
  
 
 
 
 
 team repair cost  weight

  

 b 

number of attending messages in policy

 

 
 
 
 
 team repair cost  weight

  

  
  
  
  
  
  
  
 
 

 c 

 
 
 
 
 team repair cost  weight

 d 

figure     properties of the mdp policy as team repair cost is varied 

   

  

fiscerri  pynadath   tambe
afford the risk that the user will not respond quickly  so it again acts autonomously  despite
its decision quality being low   figure    b  shows that the number of delay actions taken
by friday increases  but only in states in which the meeting has already been delayed twice 
this indicates that the normally very expensive third delay of the same meeting starts to
become worthwhile if the cost of having teammates wait in the meeting room is very high 
in the delay mdp  a value of   is used for     the decision to transfer control  i e   ask 
is not particularly sensitive to changes in the parameter around this value again  slight
changes will not have a significant impact 
number of asks in policy

number of delays in policy

  
  delays

  asks

  
  
  

   
  
  
  
  
 

  
 

 

 

 

 

  

 

 cost of teammates time  weight

 

 

 

  

 b 

number of not attending messages in policy
  
  not attending

number of attending messages in policy
   
   
   
   
   
   
   
   
   
  
 

 

 cost of teammates time  weight

 a 

  attending

total
 st delay
 nd delay
 rd delay

   

  

 
 
 
 
  
 cost of teammates time  weight

  
  
  
  
 
 
 

 c 

 
 
 
 
  
 cost of teammates time  weight

 d 

figure     properties of the mdp policy as teammate time cost is varied   b  shows the
number of times the meeting is delayed in states where it has not yet been
delayed  where it has been delayed once already  and where it has been delayed
twice already 
in the third experiment  the value of the     the weight of the joint task  was varied
 figure      in the e elves  the value of the joint task includes the value of the user to the
meeting and the value of the meeting without the user  in this experiment  the value of the
   

fitowards adjustable autonomy for the real world
meeting without the user is varied  figure    shows how the policy changes as the value of
the meeting without the user changes  again the x axis shows the value of the parameter
and the y axis shows the number of times the action appears in the policy   these graphs
show significantly more instability than for the other  values  these large changes are a
result of the simultaneous change in both the utility of taking key actions and the expected
quality of friday s decision making  e g   the utility of saying the user is attending is much
higher if the meeting has very low value without that user  in the current delay mdp  this
value is set at       which is in a part of the graph that is very insensitive to small changes
of the parameter 
in the three experiments above  the specific e elves parameters were in regions of the
graph where small changes in the parameter do not lead to significant changes in the policy 
however  there were regions of the graphs where the policy did change dramatically for small
changes in a parameter  this indicates that in some domains  with parameters different to
those in e elves  the policies will be sensitive to small changes in the parameters 

   
   
   
   
   
  
  
  
  
 
   

number of delays in policy
   
   
  delays

  asks

number of asks in policy

  
  
  

  

  
  
  
 
joint activity weight

  
   

 

  

 a 

number of not attending messages in policy
  
  not attending

  attending

   
   
   
   
  

  
  
  
 
joint activity weight

 

 b 

number of attending messages in policy
   

   
   

  
  
  
 
joint activity weight

  
  
 
 
   

 

 c 

  

  
  
  
joint activity weight

 

 

 d 

figure     properties of the mdp policy as the importance of a successful joint task is
varied 
   

fiscerri  pynadath   tambe
the above experiments show three important properties of the mdp approach to aa 
first  changing the parameters of the reward function generally lead to the changes in the
policy that are expected and desired  second  while the value of the parameters inuenced
the policy  the effect on the aa reasoning was often reasonably small  suggesting that small
errors in the model should not affect users too greatly  finally  the interesting phenomena of
the number of asks reaching a peak at intermediate values of the parameters was revealed 
the three previous experiments have examined how the behavior of the mdp changes
as the parameters of the reward function are changed  in another experiment  a central
domain level parameter affecting the behavior of the mdp  i e   the probability of getting a
user response and the cost of getting that response  corresponding to f     is varied  figure
   shows how the number of times friday chooses to ask  y axis  varies with both the
expected time to get a user response  x axis  and the cost of doing so  each line on the
graph represents a different cost   the mdp performs as expected  choosing to ask more
often if the cost of doing so is low and or it is likely to get a prompt response  notice
that  if the cost is low enough  friday will sometimes choose to ask the user even if there
is a long expected response time  conversely  if the expected response time is suciently
high  friday will assume complete autonomy  this graph also shows that there is a distinct
change in the number of asks at some point  depending on the cost   but outside this change
point the graphs are relatively at  the key reason for the fairly rapid change in the number
of asks is that often the difference between the quality of friday s and the user s decision
making is in a fairly small range  as the mean response time increases  the expected wait
costs increase  eventually becoming high enough for friday to decide to act autonomously
instead of asking 

  asks

number of asks in policy
  
  
  
  
  
  
  
 
    

cost         
cost      
cost      

   
 
  
mean response time

   

figure     number of ask actions in policy as the mean response time  in minutes  is varied 
the x axis uses a logarithmic scale 
we conclude this section with a quantitative illustration of the impact constraints have
on strategy selection  in this experiment  we merged user specified constraints from all the
e elves users  resulting in a set of    distinct constraints  we started with an unconstrained
   

fitowards adjustable autonomy for the real world

figure      a  number of possible strategies  logarithmic    b  time required for strategy
generation 
instance of the delay mdp and added these constraints one at a time  counting the strategies
that satisfied the applied constraints  we then repeated these experiments on expanded
instances of the delay mdp  where we increased the initial state space by increasing the
frequency of decisions  i e   adding values to the time relative to meeting feature   this
expansion results in three new delay mdps  which are artificial  but are inuenced by the
real delay mdp  figure   a displays these results  on a logarithmic scale   where line a
corresponds to the original delay mdp       states   and lines b       states   c      
states   and d       states  correspond to the expanded instances  each data point is a
mean over five different orderings of constraint addition  for all four mdps  the constraints
substantially reduce the space of possible agent behaviors  for instance  in the original
delay mdp  applying all    constraints eliminated      of the      original states from
consideration  and reduced the mean number of viable actions per acceptable state from
      to        the end result is a     reduction in the size  log     of the strategy space 
on the other hand  constraints alone did not provide a complete strategy  since all of the
plots stay well above    even with all    constraints  since none of the individual users were
able willing to provide    constraints  we cannot expect anyone to add enough constraints
to completely specify an entire strategy  thus  the mdp representation and associated
policy selection algorithms are still far from redundant 
the constraints  elimination of behaviors also decreases the time required for strategy
selection  figure   b plots the total time for constraint propagation and value iteration over
the same four mdps as in figure   a  averaged over the same five constraint orderings  
each data point is also a mean over five separate iterations  for a total of    iterations
per data point  the values for the zero constraint case correspond to standard value iteration without constraints  the savings in value iteration over the restricted strategy space
dramatically outweigh the cost of pre propagating the additional constraints  in addition 
the savings increase with the size of the mdp  for the original delay mdp  a   there is
a     reduction in policy generation time  while for the largest mdp  d   there is a    
reduction  thus  the introduction of constraints can provide dramatic acceleration of the
agent s strategy selection 

   

fiscerri  pynadath   tambe
  

related work

we have discussed some related work in section    this section adds to that discussion 
in section      we examine two representative aa systems   where detailed experimental
results have been presented   and explain those results via our model  this illustrates the
potential applicability of our model to other systems  in section      we examine other aa
systems and other areas of related work  such as meta reasoning  conditional planning and
anytime algorithms 

    analyzing other aa work using the strategy model
goodrich  olsen  crandall  and palmer        report on tele operated teams of robots 
where both the user s high level reasoning and the robots  low level skills are required to
achieve some task  within this domain  they have examined the effect of user neglect on
robot performance  the idea of user neglect is similar to our idea of entities taking time
to make decisions  in this case  if the user  neglects  the robot  the joint task takes longer
to perform  in this domain  the coordination constraint is that user input must arrive so
that the robot can work out the low level actions it needs to perform  four control systems
were tested on the robot  each giving a different amount of autonomy to the robot  and the
performance was measured as user neglect was varied 
although quite distinct from the e elves system  mapping goodrich s team of robots
to our aa problem formulation provides some interesting insights  this system has the
interesting feature that the entity the robot can call on for a decision  i e   the user  is also
part of the team  changing the autonomy of the robot effectively changes the nature of
the coordination constraints between the user and robot  figure    shows the performance
 y axis  of the four control policies as the amount of user neglect was increased  x axis  
the experiments showed that higher robot autonomy allowed the operator to  neglect  the
robot more without as serious an impact on its performance 
the notion of transfer of control strategies can be used to qualitatively predict the same
behavior as was observed in practice  even though goodrich et al         did not use the
notion of strategies  the lowest autonomy control policy used by goodrich et al        
was a pure tele operation one  since the robot cannot resort to its own decision making 
we represent this control policy with a strategy u   i e   control indefinitely in the hands
of the user  the second control policy allows the user to specify waypoints and on board
intelligence works out the details of getting to the waypoints  since the robot has no highlevel decision making ability  the strategy is simply to give control to the user  however 
since the coordination between the robot and user is more abstract  i e   the coordination
constraints are looser  the wait cost function is less severe  also the human is giving less
detailed guidance than in the fully tele operated case  which is not as good according to
 goodrich et al           hence we use a lower value for the expected quality of the user
decision  we denote this approach uw p to distinguish it from the fully tele operated case 
the next control policy allows the robot to choose its own waypoints given that the user
inputs regions of interest  the robot can also accept waypoints from the user  the ability
for the robot to calculate waypoints is modeled as a d  since it effectively changes the
coordination between the entities  by removing the user s need to give waypoints  we model
this control policy as the strategy u du   the final control policy is full autonomy  i e   a 
   

fitowards adjustable autonomy for the real world
performance

a

udu
u wp

u

neglect

 a 
goodrich robot operation eu
  
  
eu

  
 
   
   
   
 

   

 b 

 
p

   

 

figure     goodrich at al s various control strategies plotted against neglect   a  experimental results  thinner lines represent control systems with more intelligence
and autonomy   b  results theoretically derived from model of strategies presented in this article  p is the parameter to the probability of response function  
robot decision making is inferior to that of the user  hence the robot s decision quality is less
than the user s  the graphs of the four strategies  plotted against the probability of response
parameter  getting smaller to the right  to match  neglect  in the goodrich et al graph  is
shown in figure     notice that the shape of the graph theoretically derived from our model 
shown in figure    b   is qualitatively the same as the shape of the experimentally derived
graph  figure    a   hence  the theory predicted qualitatively the same performance as
was found from experimentation 
a common assumption in earlier aa work has been that if any entity is asked for a
decision it will make that decision promptly  hence strategies handling the contingency
   

fiscerri  pynadath   tambe
of a lack of response have not been required  for example  horvitz s        work using
decision theory is aimed at developing general  theoretical models for aa reasoning for a
user at a workstation  a prototype system  called lookout  for helping users manage their
calendars has been implemented to test these ideas  horvitz         although such systems
are distinctly different from e elves  mapping them to our problem formulation allows us to
analyze the utility of the approaches across a range of domains without having to implement
the approach in those domains 
a critical difference between horvitz s work and our work is that lookout does not
address the possibility of not receiving a  timely  response  thus  complex strategies are
not required  in the typical case for lookout  the agent has three options  to take some
action  not to take the action  or to engage in dialog  the central factor inuencing the
decision is whether the user has a particular goal that the action would aid  i e   if the user
has the goal  then the action is useful  but if he she does not have the goal  the action is
disruptive  choosing to act or not to act corresponds to pursuing strategy a   choosing
to seek user input corresponds to strategy u   figure    a  shows a graph of the different
options plotted against the probability the user has the goal  corresponds to figure   in
horvitz          the agent s expected decision quality  eqda  t  is derived from equation
  in horvitz          in other words  horvitz s model performs more detailed calculations
of expected decision quality   our model then predicts the same selection of strategies as
horvitz does  i e   choosing strategy a when eqda  t  is low  u otherwise  assuming that
only those two strategies are available   however  our model further predicts something
that horvitz did not consider  i e   that if the rate at which wait costs accrue becomes
non negligible then the choice is not as simple  figure    b  shows how the eu of the two
strategies changes as the rate of wait costs accruing is increased  the fact that the optimal
strategy varies with wait cost suggests that horvitz s approach would not immediately be
appropriate for a domain where wait costs were non negligible  e g   it would need to be
modified in many multi agent settings 

    other approaches to aa
several different approaches have been taken to the core problem of whether and when to
transfer decision making control  for example  hexmoor examines how much time the agent
has to do aa reasoning  hexmoor         similarly  in the dynamic adaptive autonomy
framework  a group of agents allocates votes amongst themselves  hence defining the amount
of inuence each agent has over a decision and thus  by their definition  the autonomy of
the agent with respect to the decision  barber  martin    mckay      b   for the related
application of meeting scheduling cesta  collia  and d aloisi        have taken the approach
of providing powerful tools for users to constrain and monitor the behavior of their proxy
agents  but the agents do not explicitly reason about relinquishing control to the user 
while at least some of this work is done in a multiagent context  the possibility of multiple
transfers of control is not considered 
complementing our work  other researchers have focused on issues of architectures for
aa  for instance  an aa interface to the  t architecture  bonasso  firby  gat  kortenkamp 
   we consider choosing not to act an autonomous decision  hence categorize it in the same way as autonomous action

   

fitowards adjustable autonomy for the real world

horvitzs eu calculations with wait cost
 

eu

 
  
  
 

   
   
   
   
probability user has goal

 

 a 

horvitzs eu calculations with wait cost
   

eu

   
 
    
    
    
 

                          

 b 

w

figure     eu of different agent options  the solid  darkest  line shows the eu taking
an autonomous action  the dashed  medium dark  line shows the eu of autonomously deciding not to act and the dotted line shows the eu of transferring
control to the user   a  plotted against the probability of user having goal  no
wait cost   b  plotted against wait cost  fixed probability of user having goal 
miller    slack        has been implemented to solve human machine interaction problems
experienced in a number of nasa projects  brann  thurman    mitchell         the
experiences showed that interaction with the system was required all the way from the
deliberative layer through to detailed control of actuators  the aa controls at all layers
are encapsulated in what is referred to as the  t s fourth layer   the interaction layer
   

fiscerri  pynadath   tambe
 schreckenghost         a similar area where aa technology is required is for safety critical
intelligent software  such as for controlling nuclear power plants and oil refineries  musliner
  krebsbach         that work has resulted in a system called aegis  abnormal event
guidance and information system  that combines human and agent capabilities for rapid
reaction to emergencies in a petro chemical refining plant  aegis features a shared task
representation that both the users and the intelligent system can work with  goldman 
guerlain  miller    musliner         a key hypothesis of the work is that the model needs
to have multiple levels of abstraction so that the user can interact at the level they see fit 
interesting work by fong  thorpe  and baur        has extended the idea of tele operated
robotics by re defining the relationship between the robot and user as a collaborative one 
rather than the traditional master slave configuration  in particular  the robot treats the
human as a resource that can perform perceptual or cognitive functions that the robot
determines it cannot adequately perform  however  as yet the work has not looked at the
possibility that the user is not available to provide input when required  which would require
the robot perform more complex transfer of control reasoning 
while most previous work in aa has ignored complex strategies for aa  there is work
in other research fields that is potentially relevant  for example  the research issues addressed by fields such as mixed initiative decision making  collins  bilot  gini    mobasher 
    b   anytime algorithms  zilberstein         multi processor scheduling  stankovic  ramamritham    cheng         meta reasoning  russell   wefald         game theory  fudenberg   tirole         and contingency plans  draper  hanks    weld        peot  
smith        all have  at least superficial  similarities with the aa problem  however  it
turns out that the core assumptions and focus of these other research areas are different
enough that the algorithms developed in these related fields are not directly applicable to
the aa problem 
in mixed initiative decision making a human user is assumed to be continually available
 collins et al       b  ferguson   allen         negating any need for reasoning about the
likelihood of response  furthermore  there is often little or no time pressure or coordination
constraints  thus  while the basic problem of transferring control between a human and
agent is common to both mixed initiative decision making and aa  the assumptions are
quite different leading to distinct solutions  likewise  other related research fields make
distinctly different assumptions which lead to distinctly different solutions  for instance 
contingency planning  draper et al         peot   smith        deals with the problem of
creating plans to deal with critical developments in the environment  strategies are related
to contingency planning in that they are plans to deal with the specific contingency of an
entity not making a decision in a manner that maintains coordination  however  in contingency planning  the key diculty is in creating the plans  in contrast  in aa  creating
strategies is straightforward and the key diculty is choosing between those strategies  our
contribution is in recognizing the need for strategies in addressing the aa problem  instantiating such strategies via mdps  and the development of a general  domain independent
reward function that leads to an mdp choosing the optimal strategy for a particular situation 
similarly  another related research area is meta reasoning  russell   wefald        
meta reasoning work looks at online reasoning about computation  a type of meta reasoning 
most closely related to aa  chooses between sequences of computations with different ex   

fitowards adjustable autonomy for the real world
pected quality and running time  subject to the constraint that choosing the highest quality
sequence of computations is not possible  because it takes too long   russell   wefald 
       the idea is to treat computations as actions and  meta reason  about the eu of
doing certain combinations of computation and  base level  actions  the output of metareasoning is a sequence of computations that are executed in sequence  aa parallels metareasoning if we consider reasoning about transferring control to entities as reasoning about
selecting computations  i e   we think of entities as computations  however  in aa  the
aim is to have one entity make a high quality decision  while in meta reasoning  the aim is
for a sequence of computations to have some high quality  moreover  the meta reasoning
assumption that computations are guaranteed to return a timely result if executed  does
not apply in aa  finally  meta reasoning looks for a sequence of computations that use a
fixed amount of time  while aa reasons about trading off extra time for a better decision
 possibly buying time with a d action   thus  algorithms developed for meta reasoning are
not applicable to aa 
another research area with conceptual similarity to aa is the field of anytime algorithms  zilberstein         an anytime algorithm quickly finds an initial solution and then
incrementally tries to improve the solution until stopped  the aa problem is similar when
we assume that the agent itself can make an immediate decision  because the problem then
has the property that a solution is always available  an important property of an anytime
algorithm   however  this will not be the case in general  i e   the agent will not always have
an answer  furthermore  anytime algorithms do not generally need to deal with multiple 
distributed entities  nor do they have the opportunity to change coordination  i e   using a
d action  
multi processor scheduling looks at assigning tasks to nodes in order to meet certain
time constraints  stankovic et al          if entities are thought of as  nodes   then aa
is also about assigning tasks to nodes  in multiprocessor scheduling  the quality of the
computation performed on each of the nodes is usually assumed to be equal  i e   the nodes
are homogeneous  thus  reasoning that trades off quality and time is not required  as it is in
aa  moreover  deadlines are externally imposed for multi processor scheduling algorithms 
rather than being exibly reasoned about as in aa  multi processor scheduling algorithms
can sometimes deal with a node rejecting a task because it cannot fulfill the time constraints
or network failures  however  while the aa problem focuses on failure to get a response
as a central issue and load balancing as an auxiliary issue  multi processor scheduling has
the opposite focus  the difference in focus leads to algorithms being developed in the
multiprocessor scheduling community that are not well suited to aa  and vice versa  
  

conclusions

adjustable autonomy is critical to the success of real world agent systems because it allows
an agent to leverage the skills  resources and decision making abilities of other entities 
both human and agent  previous work has addressed aa in the context of single agent
and single human scenarios  but those solutions do not scale to increasingly complex multiagent systems  in particular  previous work used rigid  one shot transfers of control that
did not consider team costs and  more importantly  did not consider the possibility of costly

   

fiscerri  pynadath   tambe
miscoordination between team members  indeed  when we applied a rigid transfer of control
approach to a multi agent context  it failed dramatically 
this article makes three key contributions to enable the application of aa in more
complex multiagent domains  first  the article introduces the notion of a transfer of control
strategy  a transfer of control strategy consists of a conditional sequence of two types
of actions   i  actions to transfer decision making control and  ii  actions to change an
agent s pre specified coordination constraints with team members  aimed at minimizing
miscoordination costs  such strategies allow agents to plan sequences of transfer of control
actions  thus  a strategy allows the agent to transfer control to entities best able to make
decisions  buy more time for decisions to be made and still avoid miscoordination   even
if the entity to which control is transferred fails to make the decision  additionally  we
introduced the idea of changing coordination constraints as a mechanism for giving the
agent more opportunity to provide high quality decisions  and we showed that such changes
can  in some cases  be an effective way of increasing the team s expected utility 
the second contribution of this article is a mathematical model of aa strategies that
allows us to calculate the expected utility of such strategies  the model shows that while
complex strategies are indeed better than single shot strategies in some situations  they are
not always superior  in fact  our analysis showed that no particular strategy dominates
over the whole space of aa decisions  instead  different strategies are optimal in different
situations 
the third contribution of this article is the operationalization of the notion of transferof control strategies via markov decision processes and a general reward function that
leads the mdp to find optimal strategies in a multiagent context  the general  domainindependent reward function should allow our approach to potentially be applied to other
multi agent domains  we implemented  applied  and tested our mdp approach to aa reasoning in a real world application supporting researchers in their daily activities  daily use
showed the mdp approach to be effective at balancing the need to avoid risky autonomous
decisions and the potential for costly miscoordination  furthermore  detailed experiments
showed that the policies produced by the mdps have desirable properties  such as transferring control to the user less often when the probability of getting a timely response is low 
finally  practical experience with the system revealed that users require the ability to manipulate the aa reasoning of the agents  to this end  we introduced a constraint language
that allows the user to limit the range of behavior the mdp can exhibit  we presented an
algorithm for processing such constraints  and we showed it to have the desirable property
of reducing the time it takes to find optimal policies 
  

future work

the model of aa presented in this article is suciently rich to model a wide variety of
interesting applications  however  there are some key factors that are not modeled in the
current formulation that are required for some domains  one key issue is to allow an agent
to factor the aa reasoning of other agents into its own aa reasoning  for instance  in
the elves domain  if one agent is likely to decide to delay a meeting  another agent may
wait until that decision and avoid asking its user  conversely  if an agent about to take
back control of a decision knows another agent is going to continue waiting for user input 
   

fitowards adjustable autonomy for the real world
it might also continue to wait for input  such interactions will substantially increase the
complexity of the reasoning an agent needs to perform  in this article  we have assumed
that the agent is finding a transfer of control strategy for a single  isolated decision  in
general  there will be many decisions to be made at once and the agent will not be able to
ignore the interactions between those decisions  for example  transferring control of many
decisions to a user  reduces the probability of getting a prompt response to any of them 
reasoning about these interactions will add further complexity to the required reasoning of
the agent 
another focus of future work will be generalizing the aa decision making to allow other
types of constraints   not just coordination constraints   to be taken into account  this
would in turn require generalization of the concept of a d action to include other types
of stop gap actions and may lead to different types of strategies an agent could pursue 
additionally  transfer of control actions could be generalized to allow parts of a decision
to be transferred  e g   to allow input to be received from a user without transferring total
control to him her  or allow actions that could be performed collaboratively  similarly  if
actions were reversible  the agent could make the decision but allow the user to reverse
it  we hope that such generalizations would improve the applicability of our adjustable
autonomy research in more complex domains 
acknowledgments

this research was supported by darpa award no  f                 the effort is being
managed by air force research labs rome site  this article unifies  generalizes  and significantly extends approaches described in our previous conference papers  scerri et al        
scerri  pynadath    tambe        pynadath   tambe         we thank our colleagues 
especially  craig knoblock  yolanda gil  hans chalupsky and tom russ for collaborating
on the electric elves project  we would also like to thank the jair reviewers for their
useful comments 

   

fiscerri  pynadath   tambe
appendix a  an example instantiation of the model

in this appendix  we present a detailed look at one possible instantiation of the aa model 
we use that instantiation to calculate the eu of commonly used strategies and show how
that eu varies with parameters such as the rate of wait cost accrual and the time at which
transfers of control are performed  in this instantiation  the agent  a  has only one entity to
call on for a decision  i e   the user u    hence e   fa  u g  for w  t   we use the following
function 
 

 t t  
w  t       exp
exp  otherwise

    

the exponential wait cost function reects the idea that a big delay is much worse than
a small one  a polynomial or similar function could have also been used but an exponential
was used since it makes the mathematics cleaner  for the probability of response we use 
p  t     exp t   a markovian response probability reects an entity that is just as likely
to respond at the next point in time as they were at the previous point  for users moving around a dynamic environment  this turns out to be a reasonable approximation  the
entities  decision making quality is constant over time  in particular  eqda  t    ff and for
eqdu  t    fi   assuming constant decision making quality will not always be accurate in a
dynamic environment since information available to an entity may change  hence inuencing
their ability to make the decision  however  for decisions involving static facts or preferences
decision making quality will be relatively constant  the functions are a coarse approximation of a range of interesting applications  including the e elves  table   shows the resulting
instantiated equations for the simple strategies  for convenience we let         figures
   a  and  b  show graphically how the eu of the ea strategy varies along different axes  w
is the parameter to the wait cost function  higher w means faster accruing wait costs and
p is the parameter to the response probability function  higher p means faster response  
notice how the eu depends on the transfer time  t  as much as it does on fi  the user s
decision quality   figure    d  shows the value of a d  as discussed earlier  
figure    c  compares the eu of the e dea and e strategies  the more complex the
transfer of control strategy  i e   the more transfers of control it makes   the atter the
eu graph when plotted against wait cost  w  and response probability  p  parameters  in
particular  the fall off when the wait costs are high and the probability of response low is
not so dramatic for the more complex strategy 

appendix b  constraint propagation algorithm and its correctness

in section      we examined the need for user specified constraints in conjunction with
our mdp based approach to strategies  we must thus extend the standard mdp policy
evaluation algorithms to support the evaluation of strategies while accounting for both the
standard quantitative reward function and these new qualitative constraints  this appendix
provides the novel algorithm that we developed to evaluate strategies while accounting for
   

fitowards adjustable autonomy for the real world

 
   
 
   
 
 

  
  
  
 
 
  

   
   

w    

   

   

  p

 

  

 a 

t  

  

  

 b 
value
    
    
    
    
 
     

 
 
  
   

w    

   

 

  
  
  
beta
 

   

   
   p

       
w           

 c 

 
    
    p
    

 d 

figure     equation     i e   strategy ea plotted against  a     i e   w  the rate at which
wait costs accrue  and   i e   p the likelihood of response  and  b  t  transfer
time and beta  the user s decision quality    c  comparing strategies e dea and
e  dotted line is e     d  the value of a d 

   

fiscerri  pynadath   tambe


eued t   exp    

d t     exp
euea

t   

     exp



  

 
 fi


t  ff

fi 

    

 
 fi


    

eueddeat  
    
 
d
value
   exp       fi    exp        exp
 exp t  exp     


 dcost fi   exp t exp        exp   exp  dvalue    exp  exp t  
exp t  dcost ff     exp  exp   dvalue     exp  t dvalue     
table    instantiated aa eu equations for simple transfer of control strategies 
both  we also present a detailed proof that our algorithm s output is the correct strategy
 i e   the strategy with the highest expected utility  subject to the user specified constraints  
in the standard mdp value iteration algorithm  the value of a strategy in a particular
state is a single number  an expected utility u   with the addition of our two types of
constraints  this value is now a tuple hf  n  u i  f represents a strategy s ability to satisfy
the forbidding constraints  therefore  it is a boolean indicating whether the state is forbidden
or not  n represents a strategy s ability to satisfy the necessary constraints  therefore  it
is the set of requiring constraints that will be satisfied  as in traditional value iteration 
u is the expected reward  for instance  if the value of a state  v  s    htrue  fcrs g     i 
then executing the policy from state s will achieve an expected value of     and will satisfy
required state constraint crs   however  it is not guaranteed to satisfy any other requiredstate  nor any required action  constraints  in addition  s is forbidden  so there is a nonzero
probability of violating a forbidden action or forbidden state constraint  we do not record
which forbidding constraints the policy violates  since violating any one of them is equally
bad  we do have to record which requiring constraints the policy satisfies  since satisfying
all such constraints is preferable to satisfying only some of them  therefore  the size of the
value function grows linearly with the number of requiring constraints  but is independent
of the number of forbidding constraints 
following the form of standard value iteration  we initialize the value function over
states by considering the immediate value of the strategy in the given state  without any
lookahead  more precisely 

v    s 

 

 

c cfs

 

c s   fc   crs jc s g   rs  s 

    

thus  the state s is forbidden if any forbidden state constraints immediately apply  and
it satisfies those required state constraints that immediately apply  as in standard value
iteration  the expected utility is the value of the reward function in the state 
   

fitowards adjustable autonomy for the real world
in value iteration  we must define an updated value function v t   as a refinement
of the previous iteration s value function  v t   states become forbidden in v t   if they
violate any constraints directly or if any of their successors are forbidden according to v t  
states satisfy requirements if they satisfy them directly or if all of their successors satisfy
the requirement  to simplify the following expressions  we define s   to be the set of all
successors  fs    s jmssa      g  the following expression provides the precise definition of
this iterative step 
 

 
 
 
max
c s   
c s  a   
f   
a a c c
t
 
 
 
c cfa v  s   hf  n  u   i s   s  
fs
 
fc   crsjc s g   fc   cra jc s  a g   n   
v t  s    hf    n    u   i s   s  
 
x
rs  s    r s  a    mssa   u  
    
v t  s    hf    n    u   i s   s  
just as in standard value iteration  this iterative step specifies a maximization over all possible choices of action  however  with our two additional components to represent the value
of the strategy with respect to the constraints  we no longer have an obvious comparison
function to use when evaluating candidate actions  therefore  we perform the maximization
using the following preference ordering  where x  y means that y is preferable to x 
ht  n  u i  

f  n    u  ff ff
hf  n  u i  
f  n    n  uff 
hf  n  u i  f  n  u     u

v t    s 

in other words  satisfying a forbidden constraint takes highest priority  satisfying more
requiring constraints is second  and increasing expected value is last  we define the optimal
action  p  s   as the action  a  for which the final v  s  expression above is maximized 
despite the various set operations in equation     the time complexity of this iteration
step exceeds that of standard value iteration by only a linear factor  namely the number
of constraints  jcfs j   jcfa j   jcrsj   jcra j  the eciency derives from the fact that the
constraints are satisfied violated independently of each other  the determination of whether
a single constraint is satisfied violated requires no more time than that of standard value
iteration  hence the overall linear increase in time complexity 
because expected value has the lowest priority  we can separate the iterative step of
equation    into two phases  constraint propagation and value iteration  during the
constraint propagation phase  we compute only the first two components of our value function  hf  n  i  the value iteration phase computes the third component  h    u i  as in
standard value iteration  however  we can ignore any state action pairs that  according
to the results of constraint propagation  violate a forbidding constraint  ht  n  i  or requiring constraint  hf  n  crs   cra   i   because of the component wise independence of
equation     the two phase algorithm computes an identical value function as the original 
single phase version  over state action pairs that satisfy all constraints  
in the rest of this appendix we provide a proof of the correctness of the modified value
iteration policy  given a policy  p   constructed according to the above algorithm  we must
   

fiscerri  pynadath   tambe
show that an agent following p will obey the constraints specified by the user  if the agent
begins in some state  s   s   we must prove that it will satisfy all of its constraints if and only
if v  s    hf  cra   crs   u i  we prove the results for forbidding and requiring constraints
separately 

theorem   an agent following policy  p   with value function  v   generated as in section      from any state s   s will violate a forbidding constraint with probability zero if
and only if v  s    hf  n  u i  for some u and n   
proof  we prove the theorem by induction over subspaces of the states  classified by

how  close  they are to violating a forbidding constraint  more precisely  we partition the
state space  s   into subsets  sk   defined to contain all states that can violate a forbidding
constraint after a minimum of k state transitions  in other words  s  contains those states
that violate a forbidding constraint directly  s  contains those states that do not violate
any forbidding constraints themselves  but have a successor state  following the transition
probability function  p   that does  i e   a successor state in s     s  contains those states
that do not violate any forbidding constraints  nor have any successors that do  but who
have at least one successor state that has a successor state that does  i e   a successor state
in s     etc  there are at most js j nonempty subsets in this mutually exclusive sequence  to
make this partition exhaustive  the special subset  s    contains all states from which the
agent will never violate a forbidding constraint by following p   we first show  by induction
over k  that  s   sk     k  js j   v  s    ht  n  u i  as required by the theorem 
basis step  s    by definition  the agent will violate a forbidding constraint in s   s   
therefore  either  c   cfs such that c s    t or  c   cfa such that c s  p  s     t  so we
know  from equation     v  s    ht  n  u i 
inductive step  sk      k  js j   assume  as the induction hypothesis  that  s   
sk     v  s      ht  n     u   i  by the definition of sk   each state  s   sk   has at least one
successor state  s    sk     then  according to equation     v  s    ht  n  u i  because the
disjunction over s   must include s    for which f     t 
therefore  by induction  we know that for all s   sk     k  js j   v  s    ht  n  u i 
we now show that  s   s    v  s    hf  n  u i  we prove  by induction over t  that  for any
state  s   s   v t  s    hf  n  u i 
basis step  v      by definition  if s   
s    thereff cannot exist any c   cfs such that
c s    t  then  from equation     v    s    f  n     u    
inductive step  v t   t       assume  as the
 inductive
hypothesis  that  for any s    s   
ff
v t    s      hf  n     u   i  we know that v t  s    f  n t   u t if and only if all three disjunctions
in equation    are false  the first is false  as described in the basis step  the second term
is similarly false  since  by the definition of s   there cannot exist any c   cfa such that
c s  p  s     t  in evaluating the third term  we first note that s    s   in other words 
all of the successor states of s are also in s   if successor s    sk for some finite k  then
s   sk     since all of the successors are in s    we know  by the inductive hypothesis  that
the disjunction over v t   in all these successors
is fffalse  therefore  all three disjunctive


terms in equation    are false  so v t  s    f  n t   u t  
therefore  by induction  we know that for all s   s    v  s    hf  n  u i  by the definition
of the state partition  these two results prove the theorem as required   
   

fitowards adjustable autonomy for the real world
theorem   an agent following policy  p   with value function  v   generated as described
in section      from any state s   s will satisfy each and every requiring constraint with
probability one if and only if v  s    hf  cra   crs   u i  for some u and f   
proof sketch  the proof parallels that of theorem    but with a state partition  sk  
where k corresponds to the maximum number of transitions before satisfying a requiring
constraint  however  here  states in s  are those that violate the constraint  rather than

satisfy it  some cycles in the state space can prevent a guarantee of satisfying a requiring
constraint within any fixed number of transitions  although the probability of satisfaction
in the limit may be    in our current constraint semantics  we have decided that such a
situation fails to satisfy the constraint  and our algorithm behaves accordingly  such cycles
have no effect on the handling of forbidding constraints  where  as we saw for theorem   
we need consider only the minimum  length trajectory   
the proofs of the two theorems operate independently  so the policy specified action will
satisfy all constraints  if such an action exists  the precedence of forbidding constraints
over requiring ones has no effect on the optimal action in such states  however  if there
are conicting forbidding and requiring constraints in a state  then the preference ordering
causes the agent to choose a policy that satisfies the forbidding constraint and violates
a requiring constraint  the agent can make the opposite choice if we simply change the
preference ordering from section      regardless of the choice  from theorems   and   
the agent can use the value function  v   to identify the existence of any such violation and
notify the user of the violation and possible constraint conict 
references

barber  k   goel  a     martin  c       a   dynamic adaptive autonomy in multi agent
systems  journal of experimental and theoretical artificial intelligence              
    
barber  k  s   martin  c     mckay  r       b   a communication protocol supporting
dynamic autonomy agreements  in proceedings of pricai      workshop on teams
with adjustable autonomy  pp        melbourne  australia 
bonasso  r   firby  r   gat  e   kortenkamp  d   miller  d     slack  m          experiences with an architecture for intelligent reactive agents  journal of experimental
and theorectical artificial intelligence                 
brann  d   thurman  d     mitchell  c          human interaction with lights out automation  a field study  in proceedings of the      symposium on human interaction and
complex systems  pp           dayton  usa 
cesta  a   collia  m     d aloisi  d          tailorable interactive agents for scheduling
meetings  in lecture notes in ai  proceedings of aimsa     no        pp          
springer verlag 
chalupsky  h   gil  y   knoblock  c   lerman  k   oh  j   pynadath  d   russ  t     tambe 
m          electric elves  applying agent technology to support human organizations 
in international conference on innovative applications of ai  pp        
   

fiscerri  pynadath   tambe
collins  j   bilot  c   gini  m     mobasher  b       a   mixed initiative decision support
in agent based automated contracting  in proceedings of the international conference
on autonomous agents  agents       
collins  j   bilot  c   gini  m     mobasher  b       b   mixed initiative decision support
in agent based automated contracting  in proceedings of the international conference
on autonomous agents  agents        pp          
dorais  g   bonasso  r   kortenkamp  d   pell  b     schreckenghost  d          adjustable
autonomy for human centered autonomous systems on mars  in proceedings of the
first international conference of the mars society  pp          
draper  d   hanks  s     weld  d          probabilistic planning with information gathering
and contingent execution  in hammond  k   ed    proc  second international conference on artificial intelligence planning systems  pp         university of chicago 
illinois  aaai press 
ferguson  g   allen  j     miller  b          trains      towards a mixed initiative
planning assistant  in proceedings of the third conference on artificial intelligence
planning systems  pp        
ferguson  g     allen  j          trips   an intelligent integrated problem solving assistant  in proceedings of fifteenth national conference on artificial intelligence aaai     pp           madison  wi  usa 
fong  t   thorpe  c     baur  c          robot as partner  vehicle teleoperation with collaborative control  in workshop on multi robot systems  naval research laboratory 
washington  d c 
fudenberg  d     tirole  j          game theory  the mit press  cambridge  massachusetts 
goldman  r   guerlain  s   miller  c     musliner  d          integrated task representation for indirect interaction  in working notes of the aaai spring symposium on
computational models for mixed initiative interaction 
goodrich  m   olsen  d   crandall  j     palmer  t          experiments in adjustable
autonomy  in hexmoor  h   castelfranchi  c   falcone  r     cox  m   eds    proceedings of ijcai workshop on autonomy  delegation and control  interacting with
intelligent agents 
gunderson  j     martin  w          effects of uncertainty on variable autonomy in maintainance robots  in agents    workshop on autonomy control software  pp        
hexmoor  h          a cognitive model of situated autonomy  in proceedings of pricai      workshop on teams with adjustable autonomy  pp         melbourne  australia 
hexmoor  h     kortenkamp  d          introduction to autonomy control software  journal
of experiemental and theoretical artificial intelligence                  
horvitz  e          principles of mixed initiative user interfaces  in proceedings of acm
sigchi conference on human factors in computing systems  chi      pp          
pittsburgh  pa 
   

fitowards adjustable autonomy for the real world
horvitz  e   jacobs  a     hovel  d          attention sensitive alerting  in proceedings of
conference on uncertainty and artificial intelligence  uai      pp           stockholm  sweden 
lesser  v   atighetchi  m   benyo  b   horling  b   raja  a   vincent  r   wagner  t   xuan 
p     zhang  s          the umass intelligent home project  in proceedings of the
third annual conference on autonomous agents  pp           seattle  usa 
mitchell  t   caruana  r   freitag  d   mcdermott  j     zabowski  d          experience
with a learning personal assistant  communications of the acm                
mulsiner  d     pell  b          call for papers  aaai spring symposium on adjustable
autonomy  www aaai org 
musliner  d     krebsbach  k          adjustable autonomy in procedural control for
refineries  in aaai spring symposium on agents with adjustable autonomy  pp 
       stanford  california 
peot  m  a     smith  d  e          conditional nonlinear planning  in hendler  j   ed   
proc  first international conference on artificial intelligence planning systems  pp 
         college park  maryland  morgan kaufmann 
puterman  m  l          markov decision processes  john wiley   sons 
pynadath  d   tambe  m   arens  y   chalupsky  h   gil  y   knoblock  c   lee  h   lerman 
k   oh  j   kamachandran  s   rosenbloom  p     russ  t          electric elves 
immersing and agent organization in a human organization  in proceedings of the
aaai fall symposium on socially intelligent agents   the human in the loop 
pynadath  d     tambe  m          revisiting asimov s first law  a response to the call to
arms  in intelligent agents viii proceedings of the international workshop on agents 
theories  architectures and languages  atal     
quinlan  j  r          c     programs for machine learning  morgan kaufmann  san
mateo  ca 
russell  s  j     wefald  e          principles of metareasoning  in brachman  r  j  
levesque  h  j     reiter  r   eds    kr     principles of knowledge representation
and reasoning  pp           morgan kaufmann  san mateo  california 
scerri  p   pynadath  d     tambe  m          adjustable autonomy in real world multiagent environments  in proceedings of the fifth international conference on autonomous agents  agents      pp          
scerri  p   pynadath  d     tambe  m          why the elf acted autonomously  towards
a theory of adjustable autonomy  in first international joint conference on autonomous agents and multi agent systems  aamas     
schreckenghost  d          human interaction with control software supporting adjustable
autonomy  in musliner  d     pell  b   eds    agents with adjustable autonomy 
aaai      spring symposium series  pp          
stankovic  j   ramamritham  k     cheng  s          evaluation of a exible task scheduling algorithm for distributed hard real time system  ieee transactions on computers                     
   

fiscerri  pynadath   tambe
tambe  m          towards exible teamwork  journal of artificial intelligence research
 jair             
tambe  m   pynadath  d  v   chauvat  n   das  a     kaminka  g  a          adaptive
agent integration architectures for heterogeneous team members  in proceedings of
the international conference on multiagent systems  pp          
zilberstein  s          using anytime algorithms in intelligent systems  ai magazine         
      

   

fi