journal of artificial intelligence research                  

submitted       published      

specific to general learning for temporal events
with application to learning event definitions from video
alan fern
robert givan
jeffrey mark siskind

afern   purdue   edu
givan   purdue   edu
qobi   purdue   edu

school of electrical and computer engineering
purdue university  west lafayette  in       usa

abstract
we develop  analyze  and evaluate a novel  supervised  specific to general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video
sequences  first  we introduce a simple  propositional  temporal  event description language called
ama that is sufficiently expressive to represent many events yet sufficiently restrictive to support
learning  we then give algorithms  along with lower and upper complexity bounds  for the subsumption and generalization problems for ama formulas  we present a positive examplesonly
specific to general learning method based on these algorithms  we also present a polynomialtimecomputable syntactic subsumption test that implies semantic subsumption without being
equivalent to it  a generalization algorithm based on syntactic subsumption can be used in place of
semantic generalization to improve the asymptotic complexity of the resulting learning algorithm 
finally  we apply this algorithm to the task of learning relational event definitions from video and
show that it yields definitions that are competitive with hand coded ones 

   introduction
humans conceptualize the world in terms of objects and events  this is reflected in the fact that
we talk about the world using nouns and verbs  we perceive events taking place between objects 
we interact with the world by performing events on objects  and we reason about the effects that
actual and hypothetical events performed by us and others have on objects  we also learn new
object and event types from novel experience  in this paper  we present and evaluate novel implemented techniques that allow a computer to learn new event types from examples  we show results
from an application of these techniques to learning new event types from automatically constructed
relational  force dynamic descriptions of video sequences 
we wish the acquired knowledge of event types to support multiple modalities  humans can
observe someone faxing a letter for the first time and quickly be able to recognize future occurrences
of faxing  perform faxing  and reason about faxing  it thus appears likely that humans use and
learn event representations that are sufficiently general to support fast and efficient use in multiple
modalities  a long term goal of our research is to allow similar cross modal learning and use of
event representations  we intend the same learned representations to be used for vision  as described
in this paper   planning  something that we are beginning to investigate   and robotics  something
left to the future  
a crucial requirement for event representations is that they capture the invariants of an event
type  humans classify both picking up a cup off a table and picking up a dumbbell off the floor
as picking up  this suggests that human event representations are relational  we have an abstract

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fif ern   g ivan     s iskind

relational notion of picking up that is parameterized by the participant objects rather than distinct
propositional notions instantiated for specific objects  humans also classify an event as picking
up no matter whether the hand is moving slowly or quickly  horizontally or vertically  leftward or
rightward  or along a straight path or circuitous one  it appears that it is not the characteristics of
participant object motion that distinguish picking up from other event types  rather  it is the fact
that the object being picked up changes from being supported by resting on its initial location to
being supported by being grasped by the agent  this suggests that the primitive relations used to
build event representations are force dynamic  talmy        
another desirable property of event representations is that they be perspicuous  humans can
introspect and describe the defining characteristics of event types  such introspection is what allows us to create dictionaries  to support such introspection  we prefer a representation language
that allows such characteristics to be explicitly manifest in event definitions and not emergent consequences of distributed parameters as in neural networks or hidden markov models 
we develop a supervised learner for an event representation possessing these desired characteristics as follows  first  we present a simple  propositional  temporal logic called ama that is a
sublanguage of a variety of familiar temporal languages  e g  linear temporal logic  or ltl bacchus   kabanza        event logic siskind         this logic is expressive enough to describe a
variety of interesting temporal events  but restrictive enough to support an effective learner  as we
demonstrate below  we proceed to develop a specific to general learner for the ama logic by giving algorithms and complexity bounds for the subsumption and generalization problems involving
ama formulas  while we show that semantic subsumption is intractable  we provide a weaker syntactic notion of subsumption that implies semantic subsumption but can be checked in polynomial
time  our implemented learner is based upon this syntactic subsumption 
we next show means to adapt this  propositional  ama learner to learn relational concepts 
we evaluate the resulting relational learner in a complete system for learning force dynamic event
definitions from positive only training examples given as real video sequences  this is not the first
system to perform visual event recognition from video  we review prior work and compare it to
the current work later in the paper  in fact  two such prior systems have been built by one of the
authors  h oward  siskind   morris        learns to classify events from video using temporal 
relational representations  but these representations are not force dynamic  l eonard  siskind 
      classifies events from video using temporal  relational  force dynamic representations but
does not learn these representations  it uses a library of hand code representations  this work adds
a learning component to l eonard   essentially duplicating the performance of the hand coded
definitions automatically 
while we have demonstrated the utility of our learner in the visual eventlearning domain  we
note that there are many domains where interesting concepts take the form of structured temporal sequences of events  in machine planning  macro actions represent useful temporal patterns of
action  in computer security  typical application behavior  represented perhaps as temporal patterns of system calls  must be differentiated from compromised application behavior  and likewise
authorized user behavior from intrusive behavior  
in what follows  section   introduces our application domain of recognizing visual events and
provides an informal description of our system for learning event definitions from video  section  
introduces the ama language  syntax and semantics  and several concepts needed in our analysis
of the language  section   develops and analyzes algorithms for the subsumption and generalization
problems in the language  and introduces the more practical notion of syntactic subsumption  sec   

fil earning t emporal e vents

tion   extends the basic propositional learner to handle relational data and negation  and to control
exponential run time growth  section   presents our results on visual event learning  sections  
and   compare to related work and conclude 

   system overview
this section provides an overview of our system for learning to recognize visual events from video 
the aim is to provide an intuitive picture of our system before providing technical details  a formal
presentation of our event description language  algorithms  and both theoretical and empirical results appears in sections     we first introduce the application domain of visual event recognition
and the l eonard system  the event recognizer upon which our learner is built  second  we describe
how our positive only learner fits into the overall system  third  we informally introduce the ama
event description language that is used by our learner  finally  we give an informal presentation of
the learning algorithm 
    recognizing visual events
l eonard  siskind        is a system for recognizing visual events from video camera input
an example of a simple visual event is a hand picking up a block  this research was originally
motivated by the problem of adding a learning component to l eonardallowing l eonard to
learn to recognize an event by viewing example events of the same type  below  we give a high level
description of the l eonard system 
l eonard is a three stage pipeline depicted in figure    the raw input consists of a video frame
image sequence depicting events  first  a segmentation and tracking component transforms this
input into a polygon movie  a sequence of frames  each frame being a set of convex polygons placed
around the tracked objects in the video  figure  a shows a partial video sequence of a pick up event
that is overlaid with the corresponding polygon movie  next  a model reconstruction component
transforms the polygon movie into a force dynamic model  this model describes the changing
support  contact  and attachment relations between the tracked objects over time  constructing
this model is a somewhat involved process as described in siskind         figure  b shows a
visual depiction of the force dynamic model corresponding to the pick up event  finally  an eventrecognition component armed with a library of event definitions determines which events occurred
in the model and  accordingly  in the video  figure  c shows the text output and input of the
event recognizer for the pick up event  the first line corresponds to the output which indicates
the interval s  during which a pick up occurred  the remaining lines are the text encoding of the
event recognizer input  model reconstruction output   indicating the time intervals in which various
force dynamic relations are true in the video 
the event recognition component of l eonard represents event types with event logic formulas like the following simplified example  representing x picking up y off of z  

 

p ick u p  x  y  z      s upports  z  y     c ontacts  z  y      s upports  x  y     attached  x  y   

this formula asserts that an event of x picking up y off of z is defined as a sequence of two states
where z supports y by way of contact in the first state and x supports y by way of attachment in
the second state  s upports   c ontacts   and attached are primitive force dynamic relations 
this formula is a specific example of the more general class of ama formulas that we use in our
learning 
   

fif ern   g ivan     s iskind

image
sequence

segmentation
and tracking

polygonscene
sequence

model
reconstruction

training
models
event
labels

model
sequence

event
learner

event
classification

event
labels

learned event
definitions

figure    the upper boxes represent the three primary components of l eonards pipeline  the
lower box depicts the event learning component described in this paper  the input to the
learning component consists of training models of target events  e g   movies of pick up
events  along with event labels  e g   p ick u p  hand  red  green   and the output is an
event definition  e g   a temporal logic formula defining p ick u p  x  y  z    
    adding a learning component
prior to the work reported in this paper  the definitions in l eonard s event recognition library
were hand coded  here  we add a learning component to l eonard so that it can learn to recognize
events  figure   shows how the event learner fits into the overall system  the input to the event
learner consists of force dynamic models from the model reconstruction stage  along with event
labels  and its output consists of event definitions which are used by the event recognizer  we take
a supervised learning approach where the force dynamic model reconstruction process is applied
to training videos of a target event type  the resulting force dynamic models along with labels
indicating the target event type are then given to the learner which induces a candidate definition of
the event type 
for example  the input to our learner might consist of two models corresponding to two videos 
one of a hand picking up a red block off of a green block with label p ick u p  hand  red  green  and
one of a hand picking up a green block off of a red block with label p ick u p  hand  green  red the
output would be a candidate definition of p ick u p  x  y  z   that is applicable to previously unseen
pick up events  note that our learning component is positive only in the sense that when learning
a target event type it uses only positive training examples  where the target event occurs  and does
not use negative examples  where the target event does not occur   the positive only setting is of
interest as it appears that humans are able to learn many event definitions given primarily or only
positive examples  from a practical standpoint  a positive only learner removes the often difficult
task of collecting negative examples that are representative of what is not the event to be learned
 e g   what is a typical non pickup event   
the construction of our learner involves two primary design choices  first  we must choose an
event representation language to serve as the learners hypothesis space  i e   the space of event definitions it may output   second  we must design an algorithm for selecting a good event definition
from the hypothesis space given a set of training examples of an event type 
    the ama hypothesis space
the full event logic supported by l eonard is quite expressive  allowing the specification of a
wide variety of temporal patterns  formulas   to help support successful learning  we use a more
   

fil earning t emporal e vents

 a 

frame  

frame  

frame  

frame   

frame   

frame   

frame  

frame  

frame  

frame   

frame   

frame   

 b 

 pick up hand red green                   

 c 

 supported  red            
 supported  hand                       
 supports  red hand                       
 supports  hand red             
 supports  green red            
 supports  green hand            
 contacts  red green                     
 attached  red hand            
 attached  red green           

figure    l eonard recognizes a pick up event   a  frames from the raw video input with the automatically generated polygon movie overlaid   b  the same frames with a visual depiction
of the automatically generated force dynamic properties   c  the text input output of the
event classifier corresponding to the depicted movie  the top line is the output and the
remaining lines make up the input that encodes the changing force dynamic properties 
green represents the block on the table and red represents the block being picked up 

   

fif ern   g ivan     s iskind

restrictive subset of event logic  called ama  as our learners hypothesis space  this subset excludes
many practically useless formulas that may confuse the learner  while still retaining substantial
expressiveness  thus allowing us to represent and learn many useful event types  our restriction to
ama formulas is a form of syntactic learning bias 
the most basic ama formulas are called states which express constant properties of time intervals of arbitrary duration  for example  s upports  z  y     c ontacts  z  y   is a state which tells us
that z must support and be in contact with y   in general  a state can be the conjunction of any number
of primitive propositions  in this case force dynamic relations   using ama we can also describe
sequences of states  for example   s upports  z  y     c ontacts  z  y       s upports  x  y    
attached  x  y    is a sequence of two states  with the first state as given above and the second
state indicating that x must support and be attached to y   this formula is true whenever the first
state is true for some time interval  followed immediately by the second state being true for some
time interval meeting the first time interval  such sequences are called ma timelines since they
are the meets of ands  in general  ma timelines can contain any number of states  finally  we can
conjoin ma timelines to get ama formulas  ands of mas   for example  the ama formula

  s upports  z  y    c ontacts  z  y      s upports  x  y    attached  x  y     
  s upports  u  v    attached  u  v      s upports  w  v    c ontacts  w  v   
defines an event where two ma timelines must be true simultaneously over the same time interval 
using ama formulas we can represent events by listing various property sequences  ma timelines  
all of which must occur in parallel as an event unfolds  it is important to note  however  that the
transitions between states of different timelines in an ama formula can occur in any relation to one
another  for example  in the above ama formula  the transition between the two states of the first
timeline can occur before  after  or exactly at the transition between states of the second timeline 
an important assumption leveraged by our learner is that the primitive propositions used to construct states describe liquid properties  shoham         for our purposes  we say that a property is
liquid if when it holds over a time interval it holds over all of its subintervals  the force dynamic
properties produced by l eonard are liquide g   if a hand s upports a block over an interval
then clearly the hand supports the block over all subintervals  because primitive propositions are
liquid  properties described by states  conjunctions of primitives  are also liquid  however  properties described by ma and ama formulas are not  in general  liquid 
    specific to general learning from positive data
recall that the examples that we wish to classify and learn from are force dynamic models  which
can be thought of  and are derived from  movies depicting temporal events  also recall that our
learner outputs definitions from the ama hypothesis space  given an ama formula  we say that
it covers an example model if it is true in that model  for a particular target event type  such as
p ick u p    the ultimate goal is for the learner to output an ama formula that covers an example
model if and only if the model depicts an instance of the target event type  to understand our
learner  it is useful to define a generality relationship between ama formulas  we say that ama
formula    is more general  less specific  than ama formula    if and only if    covers every
example that    covers  and possibly more   
   in our formal analysis  we will use two different notions of generality  semantic and syntactic   in this section  we
ignore such distinctions  we note  however  that the algorithm we informally describe later in this section is based on
the syntactic notion of generality 

   

fil earning t emporal e vents

if the only learning goal is to find an ama formula that is consistent with a set of positiveonly training data  then one result can be the trivial solution of returning the formula that covers
all examples  rather than fix this problem by adding negative training examples  which will rule
out the trivial solution   we instead change the learning goal to be that of finding the least general
formula that covers all of the positive examples   this learning approach has been pursued for a
variety of different languages within the machine learning literature  including clausal first order
logic  plotkin         definite clauses  muggleton   feng         and description logic  cohen  
hirsh         it is important to choose an appropriate hypothesis space as a bias for this learning
approach or the hypothesis returned may simply be  or resemble  one of two extremes  either the
disjunction of the training examples or the universal hypothesis that covers all examples  in our
experiments  we have found that  with enough training data  the least general ama formula often
converges usefully 
we take a standard specific to general machine learning approach to finding the least general
ama formula that covers a set of positive examples  the approach relies on the computation of two
functions  the least general covering formula  lgcf  of an example model and the least general
generalization  lgg  of a set of ama formulas  the lgcf of an example model is the least general
ama formula that covers the example  intuitively  the lgcf is the ama formula that captures the
most information about the model  the lgg of any set of ama formulas is the least general ama
formula that is more general than each formula in the set  intuitively  the lgg of a formula set is
the ama formula that captures the largest amount of common information among the formulas 
viewed differently  the lgg of a formula set covers all of the examples covered by those formulas 
but covers as few other examples as possible  while remaining in ama   
the resulting specific to general learning approach proceeds as follows  first  use the lgcf
function to transform each positive training model into an ama formula  second  return the lgg
of the resulting formulas  the result represents the least general ama formula that covers all of
the positive training examples  thus  to specify our learner  all that remains is to provide algorithms for computing the lgcf and lgg for the ama language  below we informally describe
our algorithms for computing these functions  which are formally derived and analyzed in sections     and   
    computing the ama lgcf
to increase the readability of our presentation  in what follows  we dispense with presenting examples where the primitive properties are meaningfully named force dynamic relations  rather  our
examples will utilize abstract propositions such as a and b  in our current application  these propositions correspond exclusively to force dynamic properties  but may not for other applications  we
now demonstrate how our system computes the lgcf of an example model 
consider the following example model  fa         b         c         d         d       g   here 
we take each number                to represent a time interval of arbitrary  possibly varying with the
number  duration during which nothing changes  and then each fact p  i  j   indicates that proposition p is continuously true throughout the time intervals numbered i through j   this model can
be depicted graphically  as shown in figure    the top four lines in the figure indicate the time
   this avoids the need for negative examples and corresponds to finding the specific boundary of the version space
 mitchell        
   the existence and uniqueness of the lgcf and lgg defined here is a formal property of the hypothesis space and is
proven for ama in sections     and    respectively 

   

fif ern   g ivan     s iskind

 

 

a

 

 

a

a

b

b

 

 

b

b
c

d
a d

d

d

d

  a b d   a b   b d   b c d

figure    lgcf computation  the top four horizontal lines of the figure indicate the intervals over which the propositions a  b  c and d are true in the model given by
fa         b         c         d         d       g   the bottom line shows how the model
can be divided into intervals where no transitions occur  the lgcf is an ma timeline 
shown at the bottom of the figure  with a state for each of the no transition intervals  each
state simply contains the true propositions within the corresponding interval 
intervals over which each of the propositions a  b  c  and d are true in the model  the bottom line
in the figure shows how the model can be divided into five time intervals where no propositions
change truth value  this division is possible because of the assumption that our propositions are
liquid  this allows us  for example  to break up the time interval where a is true into three consecutive subintervals where a is true  after dividing the model into intervals with no transitions  we
compute the lgcf by simply treating each of those intervals as a state of an ma timeline  where
the states contain only those propositions that are true during the corresponding time interval  the
resulting five state ma timeline is shown at the bottom of the figure  we show later that this simple
computation returns the lgcf for any model  thus  we see that the lgcf of a model is always an
ma timeline 
    computing the ama lgg
we now describe our algorithm for computing the lgg of two ama formulasthe lgg of m
formulas can be computed via a sequence of m   pairwise lgg applications  as discussed later 
consider the two ma timelines       a   b   c    b   c   d   e and      a   b   e   a   e   d  
it is useful to consider the various ways in which both timelines can be true simultaneously along
an arbitrary time interval  to do this  we look at the various ways in which the two timelines
can be aligned along a time interval  figure  a shows one of the many possible alignments of
these timelines  we call such alignments interdigitationsin general  there are exponentially many
interdigitations  each one ordering the state transitions differently  note that an interdigitation is
allowed to constrain two transitions from different timelines to occur simultaneously  though this is
not depicted in the figure   
   thus  an interdigitation provides an ordering relation on transitions that need not be anti symmetric  but is reflexive 
transitive  and total 

   

fil earning t emporal e vents

 a 

a b e
 b 

a b c

b c d

a

e d

e

a b c a b c a b c b c d
a b e
a b  

a

e d

e d

a

  true  

d

e
e d
 

e

figure    generalizing the ma timelines  a   b   c    b   c   d   e and  a   b   e   a   e   d    a 
one of the exponentially many interdigitations of the two timelines   b  computing the
interdigitation generalization corresponding to the interdigitation from part  a   states are
formed by intersecting aligned states from the two timelines  the state true represents a
state with no propositions 

given an interdigitation of two timelines  it is easy to construct a new ma timeline that must be
true whenever either of the timelines is true  i e   to construct a generalization of the two timelines  
in figure  b  we give this construction for the interdigitation given in figure  a  the top two
horizontal lines in the figure correspond to the interdigitation  only here we have divided every state
on either timeline into two identical states  whenever a transition occurs during that state in the other
timeline  the resulting pair of timelines have only simultaneous transitions and can be viewed as
a sequence of state pairs  one from each timeline  the bottom horizontal line is then labeled by
an ma timeline with one state for each such state pair  with that state being the intersection of the
proposition sets in the state pair  here  true represents the empty set of propositions  and is a state
that is true anywhere 
we call the resulting timeline an interdigitation generalization  ig  of   and     it should be
clear that this ig will be true whenever either   or   are true  in particular  if   holds along a
time interval in a model  then there is a sequence of consecutive  meeting  subintervals where the
sequence of states in   are true  by construction  the ig can be aligned relative to   along the
interval so that when we view states as sets  the states in the ig are subsets of the corresponding
aligned state s  in     thus  the ig states are all true in the model under the alignment  showing
that the ig is true in the model 
in general  there are exponentially many igs of two input ma timelines  one for each possible
interdigitation between the two  clearly  since each ig is a generalization of the input timelines 
then so is the conjunction of all the igs  this conjunction is an ama formula that generalizes the
input ma timelines  in fact  we show later in the paper that this ama formula is the lgg of the
two timelines  below we show the conjunction of all the igs of   and   which serves as their
lgg 
   

fif ern   g ivan     s iskind

  a   b   b  e  true  e   
  a   b   b  true  e   
  a   b   b  true  true  e   
  a   b   b  true  e   
  a   b   b  true  d  e   
  a   b   true  true  e   
  a   b   true  e   
  a   b   true  d  e   
  a   b   a  true  true  e   
  a   b   a  true  e   
  a   b   a  true  d  e   
  a   b   a  d  e   
  a   b   a  true  d  e 
while this formula is an lgg  it contains redundant timelines that can be pruned  first  it is
clear that different igs can result in the same ma timelines  and we can remove all but one copy
of each timeline from the lgg  second  note that if a timeline   is more general than a timeline
  then      is equivalent to thus  we can prune away timelines that are generalizations of
others  later in the paper  we show how to efficiently test whether one timeline is more general
than another  after performing these pruning steps  we are left with only the first and next to last
timelines in the above formulathus    a   b   a  d  e      a   b   b  e  true  e  is an lgg of   and
   
we have demonstrated how to compute the lgg of pairs of ma timelines  we can use this
procedure to compute the lgg of pairs of ama formulas  given two ama formulas we compute
their lgg by simply conjoining the lggs of all pairs of timelines  one from each ama formula 
i e   the formula
m 
n
 
lgg i    j  
i j

is an lgg of the two ama formulas          m and            n   where the i and  j are
ma timelines 
we have now informally described the lgcf and lgg operations needed to carry out the
specific to general learning approach described above  in what follows  we more formally develop
these operations and analyze the theoretical properties of the corresponding problems  then discuss
the needed extensions to bring these  exponential  propositional  and negation free  operations to
practice 

   representing events with ama
here we present a formal account of the ama hypothesis space and an analytical development of the
algorithms needed for specific to general learning for ama  readers that are primarily interested in
a high level view of the algorithms and their empirical evaluation may wish to skip sections   and  
and instead proceed directly to sections   and    where we discuss several practical extensions to
the basic learner and then present our empirical evaluation 
we study a subset of an interval based logic called event logic  siskind        utilized by
l eonard for event recognition in video sequences  this logic is interval based in explicitly rep   

fil earning t emporal e vents

resenting each of the possible interval relationships given originally by allen        in his calculus
of interval relations  e g   overlaps  meets  during   event logic formulas allow the definition
of event types which can specify static properties of intervals directly and dynamic properties by
hierarchically relating sub intervals using the allen relations  in this paper  the formal syntax and
semantics of full event logic are needed only for proposition   and are given in appendix a 
here we restrict our attention to a much simpler subset of event logic we call ama  defined
below  we believe that our choice of event logic rather than first order logic  as well as our restriction
to the ama fragment of event logic  provide a useful learning bias by ruling out a large number of
practically useless concepts while maintaining substantial expressive power  the practical utility
of this bias is demonstrated via our empirical results in the visual eventrecognition application 
ama can also be seen as a restriction of ltl  bacchus   kabanza        to conjunction and
until  with similar motivations  below we present the syntax and semantics of ama along with
some of the key technical properties of ama that will be used throughout this paper 
    ama syntax and semantics
it is natural to describe temporal events by specifying a sequence of properties that must hold over
consecutive time intervals  for example  a hand picking up a block might become the block
is not supported by the hand and then the block is supported by the hand  we represent such
sequences with ma timelines    which are sequences of conjunctive state restrictions  intuitively  an
ma timeline is given by a sequence of propositional conjunctions  separated by semicolons  and is
taken to represent the set of events that temporally match the sequence of consecutive conjunctions 
an ama formula is then the conjunction of a number of ma timelines  representing events that
can be simultaneously viewed as satisfying each of the conjoined timelines  formally  the syntax of
ama formulas is given by 
state
ma
ama

    true j prop j prop   state
     state  j  state   ma
   may omit parens
    ma j ma   ama

where prop is any primitive proposition  sometimes called a primitive event type   we take this
grammar to formally define the terms ma timeline  ma formula  ama formula  and state  a k ma formula is an ma formula with at most k states  and a k  ama formula is an ama formula
all of whose ma timelines are k  ma timelines  we often treat states as proposition sets with
true the empty set and ama formulas as ma timeline sets  we may also treat ma formulas as
sets of statesit is important to note  however  that ma formulas may contain duplicate states 
and the duplication can be significant  for this reason  when treating ma timelines as sets  we
formally intend sets of state index pairs  where the index gives a states position in the formula  
we do not indicate this explicitly to avoid encumbering our notation  but the implicit index must be
remembered whenever handling duplicate states 
the semantics of ama formulas is defined in terms of temporal models  a temporal model
m   hm  i i over the set prop of propositions is a pair of a mapping m from the natural numbers
 representing time  to the truth assignments over prop  and a closed natural number interval i  
we note that siskind        gives a continuous time semantics for event logic where the models
   ma stands for meets and  an ma timeline being the meet of a sequence of conjunctively restricted intervals 

   

fif ern   g ivan     s iskind

are defined in terms of real valued time intervals  the temporal models defined here use discrete
natural number time indices  however  our results here still apply under the continuous time semantics   that semantics bounds the number of state changes in the continuous timeline to a countable number   it is important to note that the natural numbers in the domain of m are representing
time discretely  but that there is no prescribed unit of continuous time represented by each natural
number  instead  each number represents an arbitrarily long period of continuous time during which
nothing changed  similarly  the states in our ma timelines represent arbitrarily long periods of time
during which the conjunctive restriction given by the state holds  the satisfiability relation for ama
formulas is given as follows 




a state s is satisfied by a model hm  i i iff m  x  assigns p true for every x   i and p



an ama formula              n is satisfied by m iff each i is satisfied by m 

  s 

an ma timeline s    s            sn is satisfied by a model hm   t  t   i iff there exists some t  
in  t  t    such that hm   t  t    i satisfies s  and either hm   t     t   i or hm   t        t   i satisfies
s            sn  

the condition defining satisfaction for ma timelines may appear unintuitive at first due to the
fact that there are two ways that s            sn can be satisfied  the reason for this becomes clear by recalling that we are using the natural numbers to represent continuous time intervals  intuitively  from
a continuous time perspective  an ma timeline is satisfied if there are consecutive continuous time
intervals satisfying the sequence of consecutive states of the ma timeline  the transition between
consecutive states si and si   can occur either within an interval of constant truth assignment  that
happens to satisfy both states  or exactly at the boundary of two time intervals of constant truth
value  in the above definition  these cases correspond to s            sn being satisfied during the time
intervals  t     t    and  t        t    respectively 
when m satisfies  we say that m is a model of  or that  covers m  we say that ama   
subsumes ama    iff every model of    is a model of      written          and we say that   
properly subsumes      written           when we also have           alternatively  we may state
       by saying that    is more general  or less specific  than    or that    covers      siskind
       provides a method to determine whether a given model satisfies a given ama formula 
finally  it will be useful to associate a distinguished ma timeline to a model  the ma projection
of a model m   hm   i  j  i written as map m  is an ma timeline s    s            sj i where state sk
gives the true propositions in m  i   k   for    k  j i  intuitively  the ma projection gives
the sequence of propositional truth assignments from the beginning to the end of the model  later
we show that the ma projection of a model can be viewed as representing that model in a precise
sense 
the following two examples illustrate some basic behaviors of ama formulas 
example    stretchability   s    s    s    s    s    s            s    s    and s    s    s    s    s    s    s  are
all equivalent ma timelines  in general  ma timelines have the property that duplicating any state
results in a formula equivalent to the original formula  recall that  given a model hm  i i  we
view each truth assignment m  x  as representing a continuous time interval  this interval can
conceptually be divided into an arbitrary number of subintervals  thus if state s is satisfied by
hm   x  x i  then so is the state sequence s   s           s  
   

fil earning t emporal e vents

example    infinite descending chains   given propositions a and b   the ma timeline   
is subsumed by each of the formulas a  b   a  b   a  b   a  b   a  b   a  b           this is
intuitively clear when our semantics are viewed from a continuous time perspective  any interval
in which both a and b are true can be broken up into an arbitrary number of subintervals where
both a and b hold  this example illustrates that there can be infinite descending chains of ama
formulas where the entire chain subsumes a given formula  but no member is equivalent to the given
formula   in general  any ama formula involving only the propositions a and b will subsume  

 a   b  

    motivation for ama
ma timelines are a very natural way to capture stretchable sequences of state constraints  but
why consider the conjunction of such sequences  i e   ama  we have several reasons for this language enrichment  first of all  we show below that the ama least general generalization  lgg 
is uniquethis is not true for ma  second  and more informally  we argue that parallel conjunctive constraints can be important to learning efficiency  in particular  the space of ma formulas
of length k grows in size exponentially with k   making it difficult to induce long ma formulas 
however  finding several shorter ma timelines that each characterize part of a long sequence of
changes is exponentially easier   at least  the space to search is exponentially smaller   the ama
conjunction of these timelines places these shorter constraints simultaneously and often captures a
great deal of the concept structure  for this reason  we analyze ama as well as ma and  in our
empirical work  we consider k  ama 
the ama language is propositional  but our intended applications are relational  or first order 
including visual event recognition  later in this paper  we show that the propositional ama learning algorithms that we develop can be effectively applied in relational domains  our approach to
first order learning is distinctive in automatically constructing an object correspondence across examples  cf  lavrac  dzeroski    grobelnik        roth   yih         similarly  though ama
does not allow for negative state constraints  in section     we discuss how to extend our results to
incorporate negation into our learning algorithms  which is crucial in visual event recognition 
    conversion to first order clauses
we note that ama formulas can be translated in various ways into first order clauses  it is not
straightforward  however  to then use existing clausal generalization techniques for learning  in
particular  to capture the ama semantics in clauses  it appears necessary to define subsumption and
generalization relative to a background theory that restricts us to a continuous time first order
model space 
for example  consider the ama formulas     a   b and     a  b where a and b are
propositionsfrom example   we know that        now  consider a straightforward clausal
translation of these formulas giving c    a i     b  i   and c    a i      b  i      m eets  i    i     
i   s pan  i    i     where the i and ij are variables that represent time intervals  m eets indicates
that two time intervals meet each other  and s pan is a function that returns a time interval equal
to the union of its two time interval arguments  the meaning we intend to capture is for satisfying
assignments of i in c  and c  to indicate intervals over which   and   are satisfied  respectively 
it should be clear that  contrary to what we want  c    c   i e    j  c    c     since it is easy to
find unintended first order models that satisfy c    but not c    thus such a translation  and other
similar translations  do not capture the continuous time nature of the ama semantics 
   

fif ern   g ivan     s iskind

in order to capture the ama semantics in a clausal setting  one might define a first order theory
that restricts us to continuous time modelsfor example  allowing for the derivation if property b
holds over an interval  then that property also holds over all sub intervals  given such a theory  
we have that  j  c    c    as desired  however  it is well known that least general generalizations relative to such background theories need not exist  plotkin         so prior work on clausal
generalization does not simply subsume our results for the ama language 
we note that for a particular training set  it may be possible to compile a continuous time background theory  into a finite but adequate set of ground facts  relative to such ground theories 
clausal lggs are known to always exist and thus could be used for our application  however 
the only such compiling approaches that look promising to us require exploiting an analysis similar to the one given in this paperi e   understanding the ama generalization and subsumption
problem separately from clausal generalization and exploiting that understanding in compiling the
background theory  we have not pursued such compilations further 
even if we are given such a compilation procedure  there are other problems with using existing clausal generalization techniques for learning ama formulas  for the clausal translations of
ama we have found  the resulting generalizations typically fall outside of the  clausal translations
of formulas in the  ama language  so that the language bias of ama is lost  in preliminary empirical work in our video event recognition domain using clausal inductive logic programming  ilp 
systems  we found that the learner appeared to lack the necessary language bias to find effective
event definitions  while we believe that it would be possible to find ways to build this language bias
into ilp systems  we chose instead to define and learn within the desired language bias directly  by
defining the class of ama formulas  and studying the generalization operation on that class 
    basic concepts and properties of ama
we use the following convention in naming our results  propositions and theorems are the key
results of our work  with theorems being those results of the most technical difficulty  and lemmas
are technical results needed for the later proofs of propositions or theorems  we number all the
results in one sequence  regardless of type  proofs of theorems and propositions are provided in the
main textomitted proofs of lemmas are provided in the appendix 
we give pseudo code for our methods in a non deterministic style  in a non deterministic language functions can return more than one value non deterministically  either because they contain
non deterministic choice points  or because they call other non deterministic functions  since a nondeterministic function can return more than one possible value  depending on the choices made at
the choice points encountered  specifying such a function is a natural way to specify a richly structured set  if the function has no arguments  or relation  if the function has arguments   to actually
enumerate the values of the set  or the relation  once arguments are provided  one can simply use
a standard backtracking search over the different possible computations corresponding to different
choices at the choice points 
      s ubsumption

and

g eneralization

for

s tates

the most basic formulas we deal with are states  conjunctions of propositions   in our propositional
setting computing subsumption and generalization at the state level is straightforward  a state s 
subsumes s   s   s    iff s  is a subset of s    viewing states as sets of propositions  from this  we
derive that the intersection of states is the least general subsumer of those states and that the union
of states is likewise the most general subsumee 
   

fil earning t emporal e vents

      i nterdigitations
given a set of ma timelines  we need to consider the different ways in which a model could simultaneously satisfy the timelines in the set  at the start of such a model  i e   the first time point  
the initial state from each timeline must be satisfied  at some time point in the model  one or more
of the timelines can transition so that the second state in those timelines must be satisfied in place
of the initial state  while the initial state of the other timelines remains satisfied  after a sequence
of such transitions in subsets of the timelines  the final state of each timeline holds  each way of
choosing the transition sequence constitutes a different interdigitation of the timelines 
viewed differently  each model simultaneously satisfying the timelines induces a co occurrence
relation on tuples of timeline states  one from each timeline  identifying which tuples co occur at
some point in the model  we represent this concept formally as a set of tuples of co occurring states 
i e   a co occurrence relation  we sometimes think of this set of tuples as ordered by the sequence
of transitions  intuitively  the tuples in an interdigitation represent the maximal time intervals over
which no ma timeline has a transition  with those tuples giving the co occurring states for each
such time interval 
a relation r on x       xn is simultaneously consistent with orderings           n  if 
whenever r x            xn   and r x             x n    either xi i x i   for all i  or x i i xi   for all i  we say
r is piecewise total if the projection of r onto each component is totali e   every state in any xi
appears in r 
definition    interdigitation   an interdigitation i of a set f            n g of ma timelines is a cooccurrence relation over        n  viewing timelines as sets of states    that is piecewise total
and simultaneously consistent with the state orderings of each i   we say that two states s   i
and s    j for i    j co occur in i iff some tuple of i contains both s and s    we sometimes refer to
i as a sequence of tuples  meaning the sequence lexicographically ordered by the i state orderings 
we note that there are exponentially many interdigitations of even two ma timelines  relative to the
total number of states in the timelines   example   on page     shows an interdigitation of two ma
timelines  pseudo code for non deterministically generating an arbitrary interdigitation for a set of
ma timelines can be found in figure    given an interdigitation i of the timelines s    s            sm
and t    t            tn  and possibly others   the following basic properties of interdigitations are easily
verifiable 
   for i   j   if si and tk co occur in i then for all k  
  

  k  sj does not co occur with tk

 

in i  

i  s    t    and i  sm   tn   

we first use interdigitations to syntactically characterize subsumption between ma timelines 
definition    witnessing interdigitation   an interdigitation i of two ma timelines   and  
is a witness to      iff for every pair of co occurring states s      and s        we have that
s  is a subset of s   i e   s   s    
the following lemma and proposition establish the equivalence between witnessing interdigitations
and ma subsumption 
   recall  that  formally  ma timelines are viewed as sets of state index pairs  rather than just sets of states  we ignore
this distinction in our notation  for readability purposes  treating ma timelines as though no state is duplicated 

   

fif ern   g ivan     s iskind

  

an interdigitation  f                n g 

   input  ma timelines             n
   output  an interdigitation of f            n g

  
  

s     hhead              head n  i 
if for all    i  n  ji j    
then return hs  i 
 
t    fi such that ji j    g 
t       a non empty subset of  t     

  
  
  
  
  

for i      to n
if i   t   
then  i    rest i  
else  i    i  

  
   
   
   

return extend tuple  s    an interdigitation  f              n g   

   

figure    pseudo code for an interdigitation    which non deterministically computes an interdigitation for a set f            n g of ma timelines  the function head   returns the first
state in the timeline   rest   returns  with the first state removed  extend tuple x i  
extends a tuple i by adding a new first element x to form a longer tuple  a non emptysubset of s   non deterministically returns an arbitrary non empty subset of s  
lemma    for any ma timeline  and any model m  if m satisfies   then there is a witnessing
interdigitation for map m    
proposition    for ma timelines   and      

      

   iff there is an interdigitation that witnesses

proof  we show the backward direction by induction on the number of states n in timeline     if
n      then the existence of a witnessing interdigitation for      implies that every state in  
is a subset of the single state in     and thus that any model of   is a model of   so that       
now  suppose for induction that the backward direction of the theorem holds whenever   has n
or fewer states  given an arbitrary model m of an n     state   and an interdigitation w that
witnesses        we must show that m is also a model of   to conclude      as desired 
write   as s            sn   and   as t            tm   as a witnessing interdigitation  w must identify
some maximal prefix t            tm of   made up of states that co occur with s  and thus that are
subsets of s    since m   hm   t  t   i satisfies     by definition there must exist a t      t  t    such
that hm   t  t    i satisfies s   and thus t            tm   and hm  i   i satisfies s            sn   for i   equal to
either  t     t    or  t        t     in either case  it is straightforward to construct  from w   a witnessing
interdigitation for s            sn    tm              tm and use the induction hypothesis to then show that
hm  i   i must satisfy tm             tm   it follows that m satisfies   as desired 
for the forward direction  assume that        and let m be any model such that    
map m   it is clear that such an m exists and satisfies     it follows that m satisfies    
lemma   then implies that there is a witnessing interdigitation for map m     and thus for
        
 

 

 

 

   

fil earning t emporal e vents

      l east g eneral c overing f ormula
a logic can discriminate two models if it contains a formula that satisfies one but not the other  it
turns out that ama formulas can discriminate two models exactly when much richer internal positive event logic  ipel  formulas can do so  internal formulas are those that define event occurrence
only in terms of properties within the defining interval  that is  satisfaction by hm  i i depends only
on the proposition truth values given by m inside the interval i   positive formulas are those that
do not contain negation  appendix a gives the full syntax and semantics of ipel  which are used
only to state and prove lemma      the fact that ama can discriminate models as well as ipel
indicates that our restriction to ama formulas retains substantial expressive power and leads to
the following result which serves as the least general covering formula  lgcf  component of our
specific to general learning procedure  formally  an lgcf of model m within a formula language
l  e g  ama or ipel  is a formula in l that covers m such that no other covering formula in
l is strictly less general  intuitively  the lgcf of a model  if unique  is the most representative
formula of that model  our analysis uses the concept of model embedding  we say that model m
embeds model m  iff map m   map m    
lemma   

for any e

  ip el  if model m embeds a model that satisfies e   then m satisfies e  

proposition    the ma projection of a model is its lgcf for internal positive event logic  and
hence for ama   up to semantic equivalence 
proof  consider model m  we know that map m  covers m  so it remains to show that
map m  is the least general formula to do so  up to semantic equivalence 
let e be any ipel formula that covers m  let m  be any model that is covered by map m 
we want to show that e also covers m    we know  from lemma    that there is a witnessing
interdigitation for map m     map m   thus  by proposition    map m     map m 
showing that m  embeds m  combining these facts with lemma   it follows that e also covers
m  and hence map m   e    
proposition   tells us that  for ipel  the lgcf of a model exists  is unique  and is an ma
timeline  given this property  when an ama formula   covers all the ma timelines covered by
another ama formula      we have        thus  for the remainder of this paper  when considering
subsumption between formulas  we can abstract away from temporal models and deal instead with
ma timelines  proposition   also tells us that we can compute the lgcf of a model by constructing
the ma projection of that model  based on the definition of ma projection  it is straightforward to
derive an lgcf algorithm which runs in time polynomial in the size of the model    we note that
the ma projection may contain repeated states  in practice  we remove repeated states  since this
does not change the meaning of the resulting formula  as described in example    
      c ombining i nterdigitation

with

g eneralization

or

s pecialization

interdigitations are useful in analyzing both conjunctions and disjunctions of ma timelines  when
conjoining a set of timelines  any model of the conjunction induces an interdigitation of the timelines
such that co occurring states simultaneously hold in the model at some point  viewing states as
sets  the the states resulting from unioning co occurring states must hold   by constructing an
   we take the size of a model m   hm  i i to be the sum over x   i of the number of true propositions in m  x  

   

fif ern   g ivan     s iskind

interdigitation and taking the union of each tuple of co occurring states to get a sequence of states 
we get an ma timeline that forces the conjunction of the timelines to hold  we call such a sequence
an interdigitation specialization of the timelines  dually  an interdigitation generalization involving
intersections of states gives an ma timeline that holds whenever the disjunction of a set of timelines
holds 
definition    an interdigitation generalization  specialization  of a set  of ma timelines is an ma
timeline s            sm   such that  for some interdigitation i of  with m tuples  sj is the intersection
 respectively  union  of the components of the jth tuple of the sequence i   the set of interdigitation
generalizations  respectively  specializations  of  is called ig    respectively  is    
example    suppose that s    s    s    t    t    and t  are each sets of propositions  i e   states   consider the timelines s   s    s    s  and t   t    t    t    the relation

f hs   t  i   hs   t  i   hs   t  i   hs   t  i g
is an interdigitation of s and t in which states s  and s  co occur with t    and s  co occurs with
t  and t    the corresponding ig and is members are

s    t    s    t    s    t    s    t 
s    t    s    t    s    t    s    t 

  ig fs  t g 
  is fs  t g  

if t   s    t   s    t   s    and t   s    then the interdigitation witnesses s

 t 

each timeline in ig    dually  is    subsumes  is subsumed by  each timeline in this is
easily verified using proposition    for our complexity analyses  we note that the number of states
in any member of ig   or is   is bounded from below by the number of states in any of the
ma timelines in  and is bounded from above by the total number of states in all the ma timelines
in   the number of interdigitations of   and thus of members of ig   or is    is exponential in that same total number of states  the algorithms that we present later for computing lggs
require the computation of both ig    and is    here we give pseudo code to compute these
quantities  figure   gives pseudo code for the function an ig member that non deterministically
computes an arbitrary member of ig    an is member is the same  except that we replace intersection by union   given a set  of ma timelines we can compute ig   by executing all possible
deterministic computation paths of the function call an ig member    i e   computing the set of
results obtainable from the non deterministic function for all possible decisions at non deterministic
choice points 
we now give a useful lemma and a proposition concerning the relationships between conjunctions and disjunctions of ma concepts  the former being ama concepts   for convenience here 
we use disjunction on ma concepts  producing formulas outside of ama with the obvious interpretation 
lemma    given an ma formula  that subsumes each member of a set  of ma formulas   also
subsumes some member   of ig    dually  when  is subsumed by each member of   we have
that  is also subsumed by some member   of is    in each case  the length of   is bounded by
the size of  

   

fil earning t emporal e vents

an ig member  f                n g 

   input  ma timelines             n
   output  a member of ig f                n g 

return map  intersect tuple   an interdigitation  f            n g   
figure    pseudo code for an ig member  which non deterministically computes a member of
ig t   where t is a set of ma timelines  the function intersect tuple i   takes a tuple i
of sets as its argument and returns their intersection  the higher order function map f  i  
takes a function f and a tuple i as arguments and returns a tuple of the same length as i
obtained by applying f to each element of i and making a tuple of the results 
proposition   

the following hold 

    and to or  the conjunction of a set  of ma timelines equals the disjunction of the timelines
in is   
    or to and  the disjunction of a set  of ma timelines is subsumed by the conjunction of the
timelines in ig   
proof  to prove or to and  recall that  for any     and any     ig    we have that      
w
v
from this it is immediate that        ig     using a dual argument  we can show that
w
v
v
w
  is          it remains vto show that        isw     which is equivalent to showing
that any timeline subsumed by     is also subsumed by   is     by proposition     consider
v
any ma timeline  such that      this implies that each member of  subsumes   lemma
w
  then implies that there is some     is   such that       from this we get that     is   
as desired   
using and to or  we can now reduce ama subsumption to ma subsumption  with an exponential increase in the problem size 
proposition   
           

for ama

  

and

       

    if and only if for all     is     and    

proof  for the forward direction we show the contrapositive  assume there is a     is      and a
       such that w        thus  there is an ma timeline
 such that     but        this
w
tells us that     is       and that         thus   is            and by and to or we get
that          
for the backward direction assume that for all     is      and        that        this
w
tells us that for each     is       that       thus         is              

   subsumption and generalization
in this section we study subsumption and generalization of ama formulas  first  we give a
polynomial time algorithm for deciding subsumption between ma formulas and then show that
deciding subsumption for ama formulas is conp complete  second we give algorithms and complexity bounds for the construction of least general generalization  lgg  formulas based on our
   

fif ern   g ivan     s iskind

ma subsumes         
   input      s            sm and  
   output      

  t            tn

   if there is a path from v    to vm n in sg         then return true  for example 
 a 
 b 

 c 

create an array reachable i j   of boolean values  all false  for  
   j  n 
for i      to m  reachable i        true 
for j      to n  reachable    j      true 
for i      to m
for j      to n
reachable i  j       ti  sj     reachable i
reachable i  j
reachable i

if reachable m  n  then return true 

 i  m and

   j    
    
   j     

   otherwise  return false 
figure    pseudo code for the ma subsumption algorithm 
defined in the main text 

sg         is the subsumption graph

analysis of subsumption  including existence  uniqueness  lower upper bounds  and an algorithm for
the lgg on ama formulas  third  we introduce a polynomial timecomputable syntactic notion
of subsumption and an algorithm that computes the corresponding syntactic lgg that is exponentially faster than our semantic lgg algorithm  fourth  in section      we give a detailed example
showing the steps performed by our lgg algorithms to compute the semantic and syntactic lggs
of two ama formulas 
    subsumption
all our methods rely critically on a novel algorithm for deciding the subsumption question     
between ma formulas   and   in polynomial time  we note that merely searching the possible
interdigitations of   and   for a witnessing interdigitation provides an obvious decision procedure
for the subsumption questionhowever  there are  in general  exponentially many such interdigitations  we reduce the ma subsumption problem to finding a path in a graph on pairs of states
in        a polynomial time operation  pseudo code for the resulting ma subsumption algorithm is shown in figure    the main data structure used by the ma subsumption algorithm is the
subsumption graph 
definition    the subsumption graph of two ma timelines     s         sm and     t         tn
 written sg          is a directed
graph g   hv  e i with v   fvi j j    i  m     j  ng  

the  directed  edge set e equals hvi j   vi  j i j si  tj   si  tj   i  i   i      j  j    j      
 

 

 

 

to achieve a polynomial time bound one can simply use any polynomial time pathfinding algorithm  in our case the special structure of the subsumption graph can be exploited to determine if
   

fil earning t emporal e vents

the desired path exists in o  mn  time  as the example method shown in the pseudo code illustrates 
the following theorem asserts the correctness of the algorithm assuming a correct polynomial time
path finding method is used 
lemma    given ma timelines     s            sm and     t            tn   there is a witnessing
interdigitation for      iff there is a path in the subsumption graph sg         from v    to
vm n  
theorem   
mial time 

given ma timelines   and     ma subsumes         decides  

   in polyno 

proof  the algorithm clearly runs in polynomial time  lemma   tells us that line   of the algorithm
will return true iff there is a witnessing interdigitation  combining this with proposition   shows
that the algorithm returns true iff         
given this polynomial time algorithm for ma subsumption  proposition   immediately suggests
an exponential time algorithm for deciding ama subsumptionby computing ma subsumption
between the exponentially many is timelines of one formula and the timelines of the other formula 
our next theorem suggests that we cannot do any better than this in the worst casewe argue that
ama subsumption is conp complete by reduction from boolean satisfiability  readers uninterested
in the technical details of this argument may skip directly to section     
to develop a correspondence between boolean satisfiability problems  which include negation 
and ama formulas  which lack negation  we imagine that each boolean variable has two ama
propositions  one for true and one for false  in particular  given a boolean satisfiability problem
over n variables p            pn   we take the set propn to be the set containing  n ama propositions
truek and falsek for each k between   and n  we can now represent a truth assignment a to the pi
variables with an ama state sa given as follows 

sa   ftruei j    i  n  a pi     trueg   ffalsei j    i  n  a pi     falseg
as proposition   suggests  checking ama subsumption critically involves the exponentially
many interdigitation specializations of the timelines of one of the ama formulas  in our proof  we
design an ama formula whose interdigitation specializations can be seen to correspond to truth
assignments  to boolean variables  as shown in the following lemma 
lemma    

given some n  let   be the conjunction of the timelines
n
 
i  

f propn  truei  falsei  propn    propn  falsei  truei  propn g 

we have the following facts about truth assignments to the boolean variables p            pn  
   for any truth assignment a  propn   sa   propn is semantically equivalent to a member
of is    
   for each    is    there is a truth assignment a such that   propn   sa   propn  
   a truth assignment is a function mapping boolean variables to true or false 

   

fif ern   g ivan     s iskind

with this lemma in hand  we can now tackle the complexity of ama subsumption 
theorem    

deciding ama subsumption is conp complete 

proof  we first show that deciding the ama subsumption of    by    is in conp by providing
a polynomial length certificate for any no answer  this certificate for non subsumption is an
interdigitation of the timelines of    that yields a member of is      not subsumed by      such
a certificate can be checked in polynomial time  given such an interdigitation  the corresponding
member of is      can be computed in time polynomial in the size of      and we can then test
whether the resulting timeline is subsumed by each timeline in    using the polynomial time masubsumption algorithm  proposition   guarantees that         iff there is a timeline in is     
that is not subsumed by every timeline in      so that such a certificate will exist exactly when the
answer to a subsumption query is no 
to show conp hardness we reduce the problem of deciding the satisfiability of a   sat formula
s   c       cm to the problem of recognizing non subsumption between ama formulas  here 
each ci is  li     li     li     and each li j either a proposition p chosen from p   fp            pn g or
its negation  p  the idea of the reduction is to construct an ama formula   for which we view
the exponentially many members of is    as representing truth assignments  we then construct an
ma timeline  that we view as representing  s and show that s is satisfiable iff      
let   be as defined in lemma     let  be the formula s            sm   where

si  

ffalsej j li k   pj for some kg  
ftruej j li k    pj for some kg 

each si can be thought of as asserting not ci   we start by showing that if s is satisfiable
then       assume that s is satisfied via a truth assignment awe know from lemma   
that there is a     is    that is semantically equivalent to propn   sa   propn   we show that
propn   sa   propn is not subsumed by   to conclude      using proposition    as desired 
suppose for contradiction that propn   sa   propn is subsumed by then the state sa must be
subsumed by some state si in   consider the corresponding clause ci of s   since a satisfies s
we have that ci is satisfied and at least one of its literals li k must be true  assume that li k   pj  a
dual argument holds for li k    pj    then we have that si contains falsej while sa contains truej
but not falsej thus  we have that sa   si  since si   sa    contradicting our choice of i 
to complete the proof  we now assume that s is unsatisfiable and show that      using
proposition    we consider arbitrary   in is   we will show that      from lemma    we
know there is some truth assignment a such that    propn   sa   propn   since s is unsatisfiable
we know that some ci is not satisfied by a and hence  ci is satisfied by a  this implies that
each primitive proposition in si is in sa   let w be the following interdigitation between t  
propn   sa   propn and    s            sm  

fhpropn  s  i hpropn  s  i    hpropn  sii hsa  sii hpropn  sii hpropn  si  i    hpropn  smig

we see that in each tuple of co occurring states given above that the state from t is subsumed by
the state from   thus w is a witnessing interdigitation to propn   sa   propn    which then
holds by proposition  combining this with    propn   sa   propn we get that       
given this hardness result we later define a weaker polynomial timecomputable subsumption
notion for use in our learning algorithms 
   

fil earning t emporal e vents

    least general generalization 
an ama lgg of a set of ama formulas is an ama formula that is more general than each
formula in the set and not strictly more general than any other such formula  the existence of
an ama lgg is nontrivial as there can be infinite chains of increasingly specific formulas all of
which generalize given formulas  example   demonstrated such chains for an ma subsumee and
can be extended for ama subsumees  for example  each member of the chain p   q  p   q  p   q 
p   q  p   q  p   q        covers       p   q   q and      p    p   q   despite such complications 
the ama lgg does exist 
theorem     there is an lgg for any finite set  of ama formulas that is subsumed by all other
generalizations of  
proof  let be the set     is       let   be the conjunction of all the ma timelines that
generalize while having size no larger than   since there are only a finite number of primitive
propositions  there are only a finite number of such timelines  so   is well defined    we show that
  is a least general generalization of   first  note that each timeline in   generalizes and thus
  by proposition     so   must generalize   now  consider arbitrary generalization    of  
proposition   implies that    must generalize each formula in   lemma   then implies that each
timeline of    must subsume a timeline  that is no longer than the size of and that also subsumes
the timelines of   but then  must be a timeline of    by our choice of    so that every timeline of
   subsumes a timeline of    it follows that    subsumes    and that   is an lgg of  subsumed
by all other lggs of   as desired   
s

 

given that the ama lgg exists and is unique we now show how to compute it  our first step is to
strengthen or to and from proposition   to get an lgg for the ma sublanguage 
theorem     for a set  of ma formulas  the conjunction of all ma timelines in ig   is an ama
lgg of  
proof  let   be the specified conjunction  since each timeline of ig   subsumes all timelines
in     subsumes each member of   to show   is a least general such formula  consider an
ama formula    that also subsumes all members of   since each timeline of    must subsume all
members of   lemma   implies that each timeline of    subsumes a member of ig   and thus
each timeline of    subsumes    this implies          
we can now characterize the ama lgg using is and ig 
theorem    

s

ig     is     is an ama lgg of the set  of ama formulas 

proof  let    f              n g and e              n   we know that the ama lgg of 
must subsume e   or it would fail to subsume one of the  i   using and to or we can represent
w
w
e as a disjunction of ma timelines given by e     is                is  n     any ama
lgg must be a least general formula that subsumes e i e   an ama lgg of the set of ma
s
timelines fis   j    g  theorem    tells us that an lgg of these timelines is given by
s
ig  fis   j    g    
   there must be at least one such timeline  the timeline where the only state is true

   

fif ern   g ivan     s iskind

  
  
  
  
  
  
  
  
  

   
   
   
   
   

   

semantic lgg f                   m g 

   input  ama formulas               m
   output  lgg of f              m g

s    fg 
for i      to m
for each  in all values an is member   i   
if       s         
then s      f     s j     g 
s     s s       fg 
g    fg 
for each  in all values an ig member s   
if       g        
then g     f     g j      g 
g     g g      fg 
v

return  

g 

figure    pseudo code for computing the semantic ama lgg of a set of ama formulas 
theorem    leads directly to an algorithm for computing the ama lggfigure   gives
pseudo code for the computation  lines     of the pseudo code correspond to the computation
s
of fis   j    g  where timelines are not included in the set if they are subsumed by timelines
already in the set  which can be checked with the polynomial time ma subsumption algorithm  
this pruning  accomplished by the if test in line    often drastically reduces the size of the timeline set for which we perform the subsequent ig computationthe final result is not affected by
the pruning since the subsequent ig computation is a generalization step  the remainder of the
s
pseudo code corresponds to the computation of ig  fis   j    g  where we do not include
timelines in the final result that subsume some other timeline in the set  this pruning step  the if test
in line     is sound since when one timeline subsumes another  the conjunction of those timelines
is equivalent to the most specific one  section       traces the computations of this algorithm for an
example lgg calculation 
since the sizes of both is   and ig   are exponential in the sizes of their inputs  the code in
figure   is doubly exponential in the input size  we conjecture that we cannot do better than this 
but we have not yet proven a doubly exponential lower bound for the ama case  when the input
formulas are ma timelines the algorithm takes singly exponential time  since is fg     when
 is in ma  we now prove an exponential lower bound when the input formulas are in ma  again 
readers uninterested in the technical details of this proof can safely skip forward to section     
for this argument  we take the available primitive propositions to be those in the set fpi j j   
i  n     j  ng  and consider the ma timelines
and

    s     s             sn 
    s     s             s n  
   

where

fil earning t emporal e vents

and

si    pi          pi n
s j   p  j        pn j  

we will show that any ama lgg of   and   must contain an exponential number of timelines 
in particular  we will show that any ama lgg is equivalent to the conjunction of a subset of
ig f      g   and that certain timelines may not be omitted from such a subset 
lemma     any ama lgg   of a set
timelines from ig   with j   j  j j

 of ma timelines is equivalent to a conjunction    of

proof  lemma   implies that any timeline  in   must subsume some timeline     ig    but
then the conjunction    of such   must be equivalent to    since it clearly covers  and is covered
by the lgg    since    was formed by taking one timeline from ig   for each timeline in   
we have j   j  j j    we can complete our argument then by showing that exponentially many
timelines in ig f      g  cannot be omitted from such a conjunction while it remains an lgg 
notice that for any i  j we have that si   s j   pi j   this implies that any state in ig f      g 
contains exactly one proposition  since each such state is formed by intersecting a state from   and
    furthermore  the definition of interdigitation  applied here  implies the following two facts for
any timeline q    q            qm in ig f      g  
   q 

  p    and qm   pn n 

   for consecutive states qk
and not both i   i  and j

  pi j and qk     pi  j   i  is either i or i      j   is either j or j     
  j  
 

 

together these facts imply that any timeline in ig f      g  is a sequence of propositions starting
with p    and ending with pn n such that any consecutive propositions pi j   pi  j are different with
i  equal to i or i     and j   equal to j or j      we call a timeline in ig f      g  square if
and only if each pair of consecutive propositions pi j and pi  j have either i    i or j     j   the
following lemma implies that no square timeline can be omitted from the conjunction of timelines
in ig         if it is to remain an lgg of   and    
 

 

 

 

lemma     let   and   be as given above and let     ig f      g   for any
timelines are a subset of those in   that omits some square timeline  we have         
v

   whose

n     and hence is exponenthe number of square timelines in ig f      g  is equal to  n       
n    
tial in the size of   and     we have now completed the proof of the following result 

theorem    

the smallest lgg of two ma formulas can be exponentially large 

proof  by lemma     any ama lgg    of   and   is equivalent to a conjunction of the same
number of timelines chosen from ig f      g   however  by lemma     any such conjunction
n     timelines  and then so must      which must then be exponentially
must have at least  n       
n    
large   
conjecture    

the smallest lgg of two ama formulas can be doubly exponentially large 
   

fif ern   g ivan     s iskind

we now show that our lower bound on ama lgg complexity is not merely a consequence of
the existence of large ama lggs  even when there is a small lgg  it can be expensive to compute
due to the difficulty of testing ama subsumption 
theorem     determining whether a formula   is an ama lgg for two given ama formulas   
and    is co np hard  and is in co nexp  in the size of all three formulas together 
proof  to show co np hardness we use a straightforward reduction from ama subsumption  given
two ama formulas    and    we decide        by asking whether    is an ama lgg of   
and      clearly        iff    is an lgg of the two formulas 
to show the co nexp upper bound  note that we can check in exponential time whether      
and       using proposition   and the polynomial time ma subsumption algorithm  it remains
to show that we can check whether   is not the least subsumer  since theorem    shows that the
lgg of    and    is ig is        is        if   is not the lgg then     ig is        is       
thus  by proposition    if   is not a least subsumer  there must be timelines     is    and
    ig is        is       such that         we can then use exponentially long certificates
for no answers  each certificate is a pair of an interdigitation i  of   and an interdigitation i  of
is        is       such that the corresponding members     is    and     ig is        is      
have         given the pair of certificates i  and i      can be computed in polynomial time 
  can be computed in exponential time  and the subsumption between them can be checked in
polynomial time  relative to their size  which can be exponential   if   is the lgg then   
ig is        is        so that no such certificates will exist   
    syntactic subsumption and syntactic least general generalization 
given the intractability results for semantic ama subsumption  we now introduce a tractable generality notion  syntactic subsumption  and discuss the corresponding lgg problem  the use of
syntactic forms of generality for efficiency is familiar in ilp  muggleton   de raedt       
where  for example    subsumption is often used in place of the entailment generality relation 
unlike ama semantic subsumption  syntactic subsumption requires checking only polynomially
many ma subsumptions  each in polynomial time  via theorem    
definition    ama    is syntactically subsumed by ama     written   
timeline          there is an ma timeline        such that       

syn     iff for each ma

proposition     ama syntactic subsumption can be decided in polynomial time 
syntactic subsumption trivially implies semantic subsumptionhowever  the converse does not
hold in general  consider the ama formulas  a  b      b   a   and a  b   a where a and b are
primitive propositions  we have  a  b      b   a   a  b   a  however  we have neither a  b 
a  b   a nor b   a  a  b   a  so that a  b   a does not syntactically subsume  a  b      b   a  
syntactic subsumption fails to recognize constraints that are only derived from the interaction of
timelines within a formula 
syntactic least general generalization  a syntactic ama lgg is a syntactically least general
ama formula that syntactically subsumes the input ama formulas  here  least means that no
   

fil earning t emporal e vents

formula properly syntactically subsumed by a syntactic lgg can syntactically subsume the input
formulas  based on the hardness gap between syntactic and semantic ama subsumption  one might
conjecture that a similar gap exists between the syntactic and semantic lgg problems  proving such
a gap exists requires closing the gap between the lower and upper bounds on ama lgg shown in
theorem    in favor of the upper bound  as suggested by conjecture     while we cannot yet
show a hardness gap between semantic and syntactic lgg  we do give a syntactic lgg algorithm
that is exponentially more efficient than the best semantic lgg algorithm we have found  that of
theorem      first  we show that syntactic lggs exist and are unique up to mutual syntactic
subsumption  and hence up to semantic equivalence  
theorem     there exists a syntactic lgg for any ama formula set  that is syntactically subsumed by all syntactic generalizations of  
proof  let   be the conjunction of all the ma timelines that syntactically generalize  while
having size no larger than   as in the proof of theorem       is well defined  we show that
  is a syntactic lgg for   first  note that   syntactically generalizes  because each timeline
of   generalizes a timeline in every member of   by the choice of    now consider an arbitrary
syntactic generalization    of   by the definition of syntactic subsumption  each timeline  in
   must subsume some timeline ff in each member ff of   lemma   then implies that there is a
timeline   of size no larger than  that subsumes all the ff while being subsumed by   by our
choice of    the timeline   must be a timeline of    it follows then that    syntactically subsumes
   and that   is a syntactic lgg of  subsumed by all other syntactic generalizations of    
in general  we know that semantic and syntactic lggs are different  though clearly the syntactic
lgg is a semantic generalization and so must subsume the semantic lgg  for example   a  b    
 b   a   and a  b   a have a semantic lgg of a  b   a  as discussed above  but their syntactic lgg
is  a  b   true     true  b   a   which subsumes a  b   a but is not subsumed by a  b   a  even
so  for ma formulas 
proposition    

for ma  and ama     syn

  is equivalent to     

proof  the forward direction is immediate since we already know syntactic subsumption implies
semantic subsumption  for the reverse direction  note that     implies that each timeline of  
subsumes thus since  is a single timeline each timeline in   subsumes some timeline in 
which is the definition of syntactic subsumption   
proposition    

any syntactic ama lgg for an ma formula set  is also a semantic lgg for  

proof  now  consider a syntactic lgg   for   proposition    implies that   is a semantic
generalization of   consider any semantic lgg    of   we show that       to conclude that  
is a semantic lgg for   proposition    implies that    syntactically subsumes   it follows that
       syntactically subsumes   but         is syntactically subsumed by    which is a syntactic
lgg of it follows that        syntactically subsumes    or   would not be a least syntactic
generalization of   but then              which implies         as desired   
we note that the stronger result stating that a formula   is a syntactic lgg of a set  of ma formulas if and only if it is a semantic lgg of  is not an immediate consequence of our results above  at
   

fif ern   g ivan     s iskind

first examination  the strengthening appears trivial  given the equivalence of     and  syn  
for ma   however  being semantically least is not necessarily a stronger condition than being syntactically leastwe have not ruled out the possibility that a semantically least generalization   may
syntactically subsume another generalization that is semantically  but not syntactically  equivalent 
 this question is open  as we have not found an example of this phenomenon either  
proposition    together with theorem    have the nice consequence for our learning approach
that the syntactic lgg of two ama formulas is a semantic lgg of those formulas  as long as the
original formulas are themselves syntactic lggs of sets of ma timelines  because our learning approach starts with training examples that are converted to ma timelines using the lgcf operation 
the syntactic lggs computed  whether combining all the training examples at once  or incrementally computing syntactic lggs of parts of the training data  are always syntactic lggs of sets of
ma timelines and hence are also semantic lggs  in spite of the fact that syntactic subsumption is
weaker than semantic subsumption  we note  however  that the resulting semantic lggs may be
considerably larger than the smallest semantic lgg  which may not be a syntactic lgg at all  
using proposition     we now show that we cannot hope for a polynomial time syntactic lgg
algorithm 
theorem    

the smallest syntactic lgg of two ma formulas can be exponentially large 

proof  suppose there is always a syntactic lgg of two ma formulas that is not exponentially large 
since by proposition    each such formula is also a semantic lgg  there is always a semantic lgg
of two ma formulas that is not exponentially large  this contradicts theorem      
while this is discouraging  we have an algorithm for the syntactic lgg whose time complexity
matches this lower bound  unlike the semantic lgg case  where the best algorithm we have is
doubly exponential in the worst case  theorem    yields an exponential time method for computing
the semantic lgg of a set of ma timelines since for a timeline   is       we can simply
conjoin all the timelines of ig    given a set of ama formulas  the syntactic lgg algorithm uses
this method to compute the polynomially many semantic lggs of sets of timelines  one chosen
from each input formula  and conjoins all the results 
theorem    
              n  

the formula

    ig f            n g  is a syntactic lgg of the ama formulas

v

i

i

proof  let   be i   i ig f            n g   each timeline  of   must subsume each  i because
 is an output of ig on a set containing a timeline of  i thus   syntactically subsumes each  i  
to show that   is a syntactically least such formula  consider a    that syntactically subsumes every
 i   we show that   syn    to conclude  each timeline   in    subsumes a timeline ti    i  
for each i  by our assumption that  i syn      but then by lemma      must subsume a member
of ig ft            tn g and that member is a timeline of  so each timeline   of    subsumes a
timeline of    we conclude   syn      as desired   
v

this theorem yields an algorithm that computes a syntactic ama lgg in exponential time
pseudo code for this method is given in figure    the exponential time bound follows from the fact
that there are exponentially many ways to choose             m in line    and for each of these there
are exponentially many semantic lgg members in line    since the i are all ma timelines the
product of these two exponentials is still an exponential 
   

fil earning t emporal e vents

  
  
  
  
  
  

syntactic lgg f                   m g 

   input  ama formulas f              m g
   output  syntactic lgg of f              m g

g    fg 

for each h            m i            m

for each  in semantic lgg f            m g 

  
  
  
   

v

return  

if       g        
then g     f     g j      g 
g     g g      fg 

g 

figure    pseudo code that computes the syntactic ama lgg of a set of ama formulas 
the formula returned by the algorithm shown is actually a subset of the syntactic lgg given
by theorem     this subset is syntactically  and hence semantically  equivalent to the formula
specified by the theorem  but is possibly smaller due to the pruning achieved by the if statement in
lines     a timeline is pruned from the set if it is  semantically  subsumed by any other timeline in
the set  one timeline is kept from any semantically equivalent group of timelines  at random   this
pruning of timelines is sound  since a timeline is pruned from the output only if it subsumes some
other formula in the outputthis fact allows an easy argument that the pruned formula is syntactically equivalent to  i e  mutually syntactically subsumed by  the unpruned formula  section      
traces the computations of this algorithm for an example lgg calculation  we note that in our empirical evaluation discussed in section    there was no cost in terms of accuracy for using the more
efficient syntactic vs  semantic lgg  we know this because our learned definitions made errors in
the direction of being overly specificthus  since the semantic lgg is at least as specific as the
syntactic lgg there would be no advantage to using the semantic algorithm 
the method does an exponential amount of work even if the result is small  typically because
many timelines can be pruned from the output because they subsume what remains   it is still an
open question as to whether there is an output efficient algorithm for computing the syntactic ama
lggthis problem is in conp and we conjecture that it is conp complete  one route to settling
this question is to determine the output complexity of semantic lgg for ma input formulas  we
believe that problem also to be conp complete  but have not proven this  if that problem is in p 
there is an output efficient method for computing syntactic ama lgg based on theorem    
a summary of the algorithmic complexity results from this section can be found in table   in
the conclusions section of this paper 
    examples  least general generalization calculations
below we work through the details of a semantic and a syntactic lgg calculation  we consider the
ama formulas      a  b      b   a  and    a  b   a  for which the semantic lgg is a  b   a
and the syntactic lgg is  a  b   true     true  b   a  

   

fif ern   g ivan     s iskind

      s emantic lgg e xample
the first step in calculating the semantic lgg  according to the algorithm given in figure    is to
compute the interdigitation specializations of the input formulas  i e   is   and is      trivially 
we have that is        a  b   a  to calculate is     we must consider the possible interdigitations of    for which there are three 

f ha  b i   hb  b i   hb  ai g
f ha  b i   hb  ai g
f ha  b i   ha  ai   hb  ai g
each interdigitation leads to the corresponding member of is    by unioning  conjoining  the states
in each tuple  so is    is

f  a   b    b    a   b   
 a   b   
 a   b    a   a   b   g 
lines    of the semantic lgg algorithm compute the set s   which is equal to the union of the
timelines in is    and is    with all subsumed timelines removed  for our formulas  we see that
each timeline in is    is subsumed by thus  we have that s      a  b   a 
after computing s   the algorithm returns the conjunction of timelines in ig s    with redundant
timelines removed  i e   all subsuming timelines are removed   in our case  ig s     a  b   a 
trivially  as there is only one timeline in s   thus the algorithm correctly computes the semantic lgg
of   and  to be a  b   a 
      s yntactic lgg e xample
the syntactic lgg algorithm  shown in figure    computes a series of semantic lggs for ma
timeline sets  returning the conjunction of the results  after pruning   line   of the algorithm  cycles
through timeline tuples from the cross product of the input ama formulas  in our case the tuples
in     are t    ha  b   a  a  b i and t    ha  b   a  b   aifor each tuple  the algorithm
computes the semantic lgg of the tuples timelines 
the semantic lgg computation for each tuple uses the algorithm given in figure    but the
argument is always a set of ma timelines rather than ama formulas  for this reason  lines  
  are superfluous  as for an ma timeline     is           in the case of tuple t    lines   
of the algorithm just compute s   fa  b   a  a  b g  it remains to compute the interdigitationgeneralizations of s  i e   ig s     returning the conjunction of those timelines after pruning  lines
     in figure     the set of all interdigitations of s are 

f ha  ai   hb  ai   hb  b i   hb  ai g
f ha  ai   hb  b i   hb  ai g
f ha  ai   ha  b i   hb  b i   hb  ai g
f ha  ai   ha  b i   hb  ai g
f ha  ai   ha  b i   ha  ai   hb  ai g
by intersecting states in interdigitation tuples we get ig s   

f a  true  b   true  a  b   true  a  true  b   true  a  true  true  a  true  a  true g
   

fil earning t emporal e vents

since the timeline a  b   true is subsumed by all timelines in ig s    all other timelines will be
pruned  thus the semantic lgg algorithm returns a  b   true as the semantic lgg of the timelines
in t   
next the syntactic lgg algorithm computes the semantic lgg of the timelines in t    following
the same steps as for t    we find that the semantic lgg of the timelines in t  is true  b   a  since
a  b   true and true  b   a do not subsume one another  the set g computed by lines    of the
syntactic lgg algorithm is equal to f a  b   true  true  b   a g  thus  the algorithm computes the
syntactic lgg of  and   to be  a  b   true     true  b   a   note that  in this case  the syntactic
lgg is more general than the semantic lgg 

   practical extensions
we have implemented a specific to general ama learning algorithm based on the lgcf and syntactic lgg algorithms presented earlier  this implementation includes four practical extensions 
the first extension aims at controlling the exponential complexity by limiting the length of the
timelines we consider  second we describe an often more efficient lgg algorithm based on a
modified algorithm for computing pairwise lggs  the third extension deals with applying our
propositional algorithm to relational data  as is necessary for the application domain of visual event
recognition  fourth  we add negation into the ama language and show how to compute the corresponding lgcfs and lggs using our algorithms for ama  without negation   adding negation
into ama turns out to be crucial to achieving good performance in our experiments  we end this
section with a review of the overall complexity of our implemented system 
    k ama least general generalization
we have already indicated that our syntactic ama lgg algorithm takes exponential time relative
to the lengths of the timelines in the ama input formulas  this motivates restricting the ama
language to k  ama in practice  where formulas contain timelines with no more than k states 
as k is increased the algorithm is able to output increasingly specific formulas at the cost of an
exponential increase in computational time  in the visual eventrecognition experiments shown
later  as we increased k   the resulting formulas became overly specific before a computational bottleneck is reachedi e   for that application the best values of k were practically computable and the
ability to limit k provided a useful language bias 
we use a k  cover operator in order to limit our syntactic lgg algorithm to k  ama  a k  cover
of an ama formula is a syntactically least general k  ama formula that syntactically subsumes
the inputit is easy to show that a k  cover for a formula can be formed by conjoining all k  ma
timelines that syntactically subsume the formula  i e   that subsume any timeline in the formula   
figure    gives pseudo code for computing the k  cover of an ama formula  it can be shown that
this algorithm correctly computes a k  cover for any input ama formula  the algorithm calculates
the set of least general k  ma timelines that subsume each timeline in the inputthe resulting k  ma
formulas are conjoined and redundant timelines are pruned using a subsumption test  we note that
the k  cover of an ama formula may itself be exponentially larger than that formula  however  in
practice  we have found k  covers not to exhibit undue size growth 
given the k  cover algorithm we restrict our learner to k  ama as follows     compute the
k cover for each ama input formula     compute the syntactic ama lgg of the resulting kama formulas     return the k  cover of the resulting ama formula  the primary bottleneck of
   

fif ern   g ivan     s iskind

  
  
  
  
  
  
  
  
  
   
   

   
   
   
   
   
   
   
   

v

k cover k   im i  
v
   input  positive natural number k   ama formula  im i
v
   output  k  cover of  im i
g    fg 
for i      to m

   hp            pn i in all values a k partition  k  i   
t
t
      p               pn   
if       g        
then g     f     g j      g 
g     g g      fg 
v
return   g 
for each p

a k partition  k  s            sj  

   input  positive natural number k   ma timeline s            sj
   output  a tuple of  k sets of consecutive states that partitions s            sj

 k then return hfs g          fsj gi 
if k     then return hfs            sj gi 
l    a member of f              j k    g  
p    fs            sl g 
if j

return extend tuple  p    a k partition  k

   pick next block size
   construct next block

   sl             sj    

figure     pseudo code for non deterministically computing a k cover of an ama formula  along
with a non deterministic helper function for selecting a  k block partition of the states
of a timeline 

the original syntactic lgg algorithm is computing the exponentially large set of interdigitationgeneralizationsthe k  limited algorithm limits this complexity as it only computes interdigitationgeneralizations involving k  ma timelines 
    incremental pairwise lgg computation
our implemented learner computes the syntactic k ama lgg of ama formula setshowever 
it does not directly use the algorithm describe above  rather than compute the lgg of formula
sets via a single call to the above algorithm  it is typically more efficient to break the computation
into a sequence of pairwise lgg calculations  below we describe this approach and the potential
efficiency gains 
it is straightforward to show that for both syntactic and semantic subsumption we have that
lgg               m     lgg      lgg               m    where the  i are ama formulas  thus  by
recursively applying this transformation we can incrementally compute the lgg of m ama formulas via a sequence of m   pairwise lgg calculations  note that since the lgg operator is
   

fil earning t emporal e vents

commutative and associative the final result does not depend on the order in which we process the
formulas  we will refer to this incremental pairwise lgg strategy as the incremental approach and
to the strategy that makes a single call to the k ama lgg algorithm  passing in the entire formula
set  as the direct approach 
to simplify the discussion we will consider computing the lgg of an ma formula set the
argument can be extended easily to ama formulas  and hence to k ama   recall that the syntactic
lgg algorithm of figure   computes lgg   by conjoining timelines in ig   that do not subsume any of the others  eliminating subsuming timelines in a form of pruning  the incremental
approach applies this pruning step after each pair of input formulas is processedin contrast  the
direct approach must compute the interdigitation generalization of all the input formulas before any
pruning can happen  the resulting savings can be substantial  and typically more than compensates
for the extra effort spent checking for pruning  i e  testing subsumption between timelines as the
incremental lgg is computed   a formal approach to describing these savings can be constructed
s
s
based on the observation that both  ig f     g  ig fg    and  lgg        ig fg   
can be seen to compute the lgg of    f      g  but with the latter being possibly much cheaper
to compute due to pruning  that is  lgg         typically contains a much smaller number of
timelines than ig f      g  
based on the above observations our implemented system uses the incremental approach to
compute the lgg of a formula set  we now describe an optimization used in our system to speedup
the computation of pairwise lggs  compared to directly running the algorithm in figure    given a
pair of ama formulas                   m and                   n   let   be their syntactic
lgg obtained by running the algorithm in figure    the algorithm constructs   by computing
lggs of all ma timeline pairs  i e   lgg   i     j   for all i and j   and conjoining the results
while removing subsuming timelines  it turns out that we can often avoid computing many of these
ma lggs  to see this consider the case when there exists i and j such that   i    j   we know
lgg   i     j       j which tells us that that   j will be considered for inclusion into    it may
be pruned   furthermore we know that any other lgg involving   j will subsume   j and thus
will be pruned from    this shows that we need not compute any ma lggs involving   j   rather
we need only to consider adding   j when constructing   
the above observation leads to a modified algorithm  used in our system  for computing the
syntactic lgg of a pair of ama formulas  the new algorithm only computes lggs between
non subsuming timelines  given ama formulas    and      the modified algorithm proceeds as
follows     compute the subsumer set s   f      j         s t     g   f      j     
   s t     g     let ama            be the result of removing timelines from          that are
in s      let    be the syntactic lgg of     and     computed by running the algorithm in figure  
 if either   i is empty then    will be empty      let s   be the conjunction of timelines in s that do
not subsume any timeline in         return          s     this method avoids computing ma lggs
involving subsuming timelines  an exponential operation  at the cost of performing polynomially
many ma subsumption tests  a polynomial operation   we have noticed a significant advantage to
using this procedure in our experiments  in particular  the advantage tends to grow as we process
more training examples  this is due to the fact that as we incrementally process training examples
the resulting formulas become more generalthus  these more general formulas are likely to have
more subsuming timelines  in the best case when    syn     i e   all timelines in    are subsuming   we see that step   produces an empty formula and thus step    the expensive step  performs no
workin this case we return the set s      as desired 
   

fif ern   g ivan     s iskind

    relational data
l eonard produces relational models that involve objects and  force dynamic  relations between
those objects  thus event definitions include variables to allow generalization over objects  for
example  a definition for p ick u p  x  y  z   recognizes both p ick u p  hand  block  table  as well as
p ick u p  man  box  floor   despite the fact that our k  ama learning algorithm is propositional  we
are still able to use it to learn relational definitions 
we take a straightforward object correspondence approach to relational learning  we view the
models output by l eonard as containing relations applied to constants  since we  currently 
support only supervised learning  we have a set of distinct training examples for each event type 
there is an implicit correspondence between the objects filling the same role across the different training models for a given type  for example  models showing p ick u p  hand  block  table 
and p ick u p  man  box  floor  have implicit correspondences given by hhand  mani  hblock  boxi 
and htable  floori  we outline two relational learning methods that differ in how much objectcorrespondence information they require as part of the training data 
      c omplete o bject c orrespondence
this first approach assumes that a complete object correspondence is given  as input  along with
the training examples  given such information  we can propositionalize the training models by
replacing corresponding objects with unique constants  the propositionalized models are then given
to our propositional k  ama learning algorithm which returns a propositional k  ama formula  we
then lift this propositional formula by replacing each constant with a distinct variable  lavrac et al 
       has taken a similar approach 
      partial o bject c orrespondence
the above approach assumes complete object correspondence information  while it is sometimes
possible to provide all correspondences  for example  by color coding objects that fill identical
roles when recording training movies   such information is not always available  when only a
partial object correspondence  or even none at all  is available  we can automatically complete the
correspondence and apply the above technique 
for the moment  assume that we have an evaluation function that takes two relational models
and a candidate object correspondence  as input  and yields an evaluation of correspondence quality  given a set of training examples with missing object correspondences  we perform a greedy
search for the best set of object correspondence completions over the models  our method works
by storing a set p of propositionalized training examples  initially empty  and a set u of unpropositionalized training examples  initially the entire training set   for the first step  when p is empty  we
evaluate all pairs of examples from u   under all possible correspondences  select the pair that yields
the highest score  remove the examples involved in that pair from u   propositionalize them according to the best correspondence  and add them to p   for each subsequent step  we use the previously
computed values of all pairs of examples  one from u and one from p   under all possible correspondences  we then select the example from u and correspondence that yields the highest average
score relative to all models in p this example is removed from u   propositionalized according to
the winning correspondence  and added to p   for a fixed number of objects  the effort expended
here is polynomial in the size of the training set  however  if the number of objects b that appear in a
training example is allowed to grow  the number of correspondences that must be considered grows
   

fil earning t emporal e vents

as bb   for this reason  it is important that the events involved manipulate only a modest number of
objects 
our evaluation function is based on the intuition that object roles for visual events  as well as
events from other domains  can often be inferred by considering the changes between the initial
and final moments of an event  specifically  given two models and an object correspondence  we
first propositionalize the models according to the correspondence  next  we compute add and
delete lists for each model  the add list is the set of propositions that are true at the final
moment but not the initial moment  the delete list is the set of propositions that are true at the
initial moment but not the final moment  these add and delete lists are motivated by strips action
representations  fikes   nilsson         given such addi and deletei lists for models   and   
the evaluation function returns the sum of the cardinalities of add    add  and delete   
delete    this heuristic measures the similarity between the add and delete lists of the two
models  the intuition behind this heuristic is similar to the intuition behind the strips actiondescription languagei e   that most of the differences between the initial and final moments of an
event occurrence are related to the target event  and that event effects can be described by add and
delete lists  we have found that this evaluation function works well in the visual event domain 
note  that when full object correspondences are given to the learner  rather than automatically
extracted by the learner   the training examples are interpreted as specifying that the target event
took place as well as which objects filled the various event roles  e g   p ick u p  a b c    rather 
when no object correspondences are provided the training examples are interpreted as specifying the
existence of a target event occurrence but do not specify which objects fill the roles  i e   the training
example is labeled by p ick u p rather than p ick u p  a b c    accordingly  the rules learned when no
correspondences are provided only allow us to infer that a target event occurred and not which
objects filled the event roles  for example when object correspondences are manually provided the
learner might produce the rule 
 

   s upports  z  y    c ontacts  z  y   
p ick u p  x  y  z    
 s upports  x  y    attached  x  y  

 

whereas a learner that automatically extracts the correspondences would instead produce the rule 
 

   s upports  z  y    c ontacts  z  y   
p ick u p  
 s upports  x  y    attached  x  y  

 

its worth noting  however  that upon producing the second rule the availability of a single training
example with correspondence information allows the learner to determine the roles of the variables 
upon which it can output the first rule  thus  under the assumption that the learner can reliably
extract object correspondences  we need not label all training examples with correspondence information in order to obtain definitions that explicitly recognize object roles 
    negative information
the ama language does not allow negated propositions  negation  however  is sometimes necessary to adequately define an event type  in this section  we consider the language ama   which is a
superset of ama  with the addition of negated propositions  we first give the syntax and semantics
of ama   and extend ama syntactic subsumption to ama   next  we describe our approach to
   

fif ern   g ivan     s iskind

learning ama formulas using the above presented algorithms for ama  we show that our approach correctly computes the ama lgcf and the syntactic ama lgg  finally  we discuss
an alternative  related approach to adding negation designed to reduce the overfitting that appears to
result from the full consideration of negated propositions 
ama has the same syntax as ama  only with a new grammar for building states with negated
propositions 
literal
state

    true j prop j   prop
    literal j literal   state

where prop is any primitive proposition  the semantics of ama
for state satisfaction 

are the same as for ama except



a positive literal p  negative literal
true  false   for every x   i    

  p   is satisfied by model hm  i i iff m  x  assigns p



a state l         lm is satisfied by model hm  i i iff each literal li is satisfied by hm  i i 

subsumption  an important difference between ama and ama is that proposition    establishing the existence of witnessing interdigitations to ma subsumption  is no longer true for ma  
in other words  if we have two timelines         ama   such that        there need not be an
interdigitation that witnesses        to see this  consider the ama timelines 

     a   b   c   b  a  b   a   b      c 
    b  a  c  a  b  a     c  a  b
we can then argue 
   there is no interdigitation that witnesses        to see this  first show that  in any such
witness  the second and fourth states of    each just b  must interdigitate to align with
either the first and fifth  or the fifth and ninth states of    also  each just b   but in either
of these cases  the third state of   will interdigitate with states of   that do not subsume it 
   even so  we still have that        to see this  consider any model hm  i i that satisfies    
there must be an interval  i    i    within i such that hm   i    i   i satisfies the third state of    
that is the state a  we have two cases 
 a  the proposition c is true at some point in hm   i    i   i  then  one can verify that hm  i i
satisfies both   and   in the following alignment 

 
 

 
 

 a   b   c   b 
b 

a 
a  c  a 

b 
b 

 a   b      c 
a     c  a  b

    we note that it is important that we use the notation   p rather than just  p   in event logic  the formula  p
is satisfied by a model whenever p is false as some instant in the model  rather  event logic interprets   p as
indicating that p is never true in the model  as defined above   notice that the first form of negation does not yield a
liquid propertyi e    p can be true along an interval but not necessarily during all subintervals  the second form of
negation  however  does yield a liquid property provided that p is liquid  this is important to our learning algorithms 
since they all assume states are built from liquid properties 

   

fil earning t emporal e vents

 b  the proposition c is false everywhere in hm   i    i   i  then  one can verify that hm  i i
satisfies both   and   in the following alignment 

   
 a   b   c  
   
b  a  c  a 
it follows that       

b 
b 

a 
a     c  a 

b   a   b      c 
b

in light of such examples  we conjecture that it is computationally hard to compute ama
subsumption even between timelines  for this reason  we extend our definition of syntactic subsumption to ama in a way that provides a clearly tractable subsumption test analogous to that
discussed above for ama 
definition    ama    is syntactically subsumed by ama     written    syn      iff for
each timeline          there is a timeline        such that there is a witnessing interdigitation
for       
the difference between the definition here and the previous one for ama is that here we only need
to test for witnessing interdigitations between timelines rather than subsumption between timelines 
for ama formulas  we note that the new and old definition are equivalent  due to proposition    
however  for ama the new definition is weaker  and will result in more general lgg formulas  as
one might expect  ama syntactic subsumption implies semantic subsumption and can be tested
in polynomial time using the subsumption graph described in lemma   to test for witnesses 
learning  rather than design new lgcf and lgg algorithms to directly handle ama   we
instead compute these functions indirectly by applying our algorithms for ama to a transformed
problem  intuitively  we do this by adding new propositions to our models  i e   the training examples  that represent the proposition negations  assume that the training example models are over the
set of propositions p   fp            pn g  we introduce a new set p   fp            pn g of propositions
and use these to construct new training models over p   p by assigning true to pi at a time in a
model iff pi is false in the model at that time  after forming the new set of training models  each
with twice as many propositions as the original models  we compute the least general ama formula
that covers the new models  by computing the ama lgcfs and applying the syntactic ama lgg
algorithm   resulting in an ama formula   over the propositions p   p   finally we replace each pi
in   with   pi resulting in an ama formula    over propositions in p it turns out that under
syntactic subsumption    is the the least general ama formula that covers the original training
models 
we now show the correctness of the above transformational approach to computing the ama
lgcf and syntactic lgg  first  we introduce some notation  let m be the set of all models over
p   let m be the set of models over p   p   such that at any time  for each i  exactly one of pi
and pi is true  let t be the following mapping from m to m  for hm  i i   m  t  hm  i i  is the
unique hm     i i   m such that for all j   i and all i  m    j   assigns pi true iff m  j   assigns pi
true  notice that the inverse of t is a functional mapping from m to m  our approach to handling
negation using purely ama algorithms begins by applying t to the original training models  in
what follows  we consider ama formulas over the propositions in p   and ama formulas over
the propositions in p   p  
let f be a mapping from ama to ama where for     ama   f     is an ama formula
identical to   except that each   pi in   is replaced with pi   notice that the inverse of f is a func   

fif ern   g ivan     s iskind

tion from ama to ama and corresponds to the final step in our approach described above  the
following lemma shows that there is a one to one correspondence between satisfaction of ama
formulas by models in m and satisfaction of ama formulas by models in m 
lemma     for any model hm  i i   m and any     ama  
t  hm  i i  

  covers hm  i i

iff

f     covers

using this lemma  it is straightforward to show that our transformational approach computes the
ama lgcf under semantic subsumption  and hence under syntactic subsumption  
proposition    

for any

hm  i i   m  let  be the ama lgcf of the model t  hm  i i  
lgcf of hm  i i  up to equivalence 

f      is the unique ama

then 

proof  we know that  covers t  hm  i i   therefore by lemma    we know that f      covers
hm  i i  we now show that f     is the least general formula in ama that covers hm  i i  for
the sake of contradiction assume that some     ama covers hm  i i but that     f       it
follows that there is some model hm     i   i that is covered by f      but not by     by lemma   
we have that f      covers t  hm  i i  and since  is the unique ama lgcf of t  hm  i i   up to
equivalence  we have that   f       however  we also have that t  hm     i   i  is covered by 
but not by f      which gives a contradiction  thus  no such   can exist  it follows that  is an
ama lgcf  the uniqueness of the ama lgcf up to equivalence follows because ama is
closed under conjunction  so that if there were any two non equivalent lgcf formulas  they could
be conjoined to get an lgcf formula strictly less than one of them   
below we use the fact that the f operator preserves syntactic subsumption  in particular  given
two ma timelines         it is clear that any witnessing interdigitation of      can be trivially
converted into a witness for f       f       and vice versa   since syntactic subsumption is defined
in terms of witnessing interdigitations  it follows that for any           ama       syn      iff
 f       syn f         using this property  it is straightforward to show how to compute the syntactic
ama lgg using the syntactic ama lgg algorithm 
proposition    

for any ama

formulas

              m  

let

 

ff                f   m  g  then  f      is the unique syntactic ama

be the syntactic ama lgg of
lgg of f              m g 

proof  we know that for each i  f   i   syn  thus  since f   preserves syntactic subsumption 
we have that for each i   i syn f        this shows that f       is a generalization of the inputs 
we now show that f       is the least such formula  for the sake of contradiction assume that
f       is not least  it follows that there must be a      ama such that     syn f       and for
each i   i syn      combining this with the fact that f preserves syntactic subsumption  we get
that f        syn   and for each i  f   i    f        but this contradicts the fact that   is an lgg 
so we must have that f       is a syntactic ama lgg  as argued elsewhere  the uniqueness of
this lgg follows from the fact that ama is closed under conjunction   
these propositions ensure the correctness of our transformational approach to computing the
syntactic lgg within ama   for the case of semantic subsumption  the transformational approach
does not correctly compute the ama lgg  to see this  recall that above we have given two timelines         ama   such that        but there is no witnessing interdigitation  clearly under
   

fil earning t emporal e vents

semantic subsumption  the ama lgg of   and   is     however  the semantic ama lgg of
f      and f      is not f       the reason for this is that since there is no witness to f       f     
 and the f  i   are ma timelines   we know by proposition   that f        f       thus  f     
cannot be returned as the ama lgg  since it does not subsume both input formulasthis shows
that the transformational approach will not return     f    f        here  the transformational
approach will produce an ama formula that is more general than    
on the computational side  we note that  since the transformational approach doubles the number of propositions in the training data  algorithms specifically designed for ama may be more
efficient  such algorithms might leverage the special structure of the transformed examples that our
ama algorithms ignorein particular  that exactly one of pi or pi is true at any time 
boundary negation  in our experiments  we actually compare two methods for assigning truth
values to the pi propositions in the training data models  the first method  called full negation 
assigns truth values as described above  yielding the syntactically least general ama formula that
covers the examples  we found  however  that using full negation often results in learning overly
specific formulas  to help alleviate this problem  our second method places a bias on the use of
negation  our choice of bias is inspired by the idea that  often  much of the useful information for
characterizing an event type is in its pre  and post conditions  the second method  called boundary
negation  differs from full negation in that it only allows pi to be true in the initial and final moments
of a model  and then only if pi is false   pi must be false at all other times  that is  we only allow
informative negative information at the beginnings and ends of the training examples  we have
found that boundary negation provides a good trade off between no negation  i e   ama   which
often produces overly general results  and full negation  i e   ama    which often produces overly
specific and much more complicated results 
    overall complexity and scalability
we now review the overall complexity of our visual event learning component and discuss some
scalability issues  given a training set of temporal models  i e   a set of movies   our system does the
following     propositionalize the training models  translating negation as descried in section     
   compute the lgcf of each propositional model     compute the k  ama lgg of the lgcfs 
   return a lifted  variablized  version of the lgg  steps two and four require little computational
overhead  being linear in the sizes of the input and output respectively  steps one and three are
the computational bottlenecks of the systemthey encompass the inherent exponential complexity
arising from the relational and temporal problem structure 
step one  recall from section       that our system allows the user to annotate training examples with object correspondence information  our technique for propositionalizing the models was
shown to be exponential in the number of unannotated objects in a training example  thus  our
system requires that the number of objects be relatively small or that correspondence information
be given for all but a small number of objects  often the event class definitions we are interested
in do not involve a large number of objects  when this is true  in a controlled learning setting we
can manage the relational complexity by generating training examples with only a small number  or
zero  irrelevant objects  this is the case for all of the domains studied empirically in this paper 
in a less controlled setting  the number of unannotated objects may prohibit the use of our
correspondence techniquethere are at least three ways one might proceed  first  we can try to
   

fif ern   g ivan     s iskind

develop efficient domain specific techniques for filtering objects and finding correspondences  that
is  for a particular problem it may be possible to construct a simple filter that removes irrelevant
objects from consideration and then to find correspondences for any remaining objects  second  we
can provide the learning algorithm with a set of hand coded first order formulas  defining a set of
domain specific features  e g   in the spirit of roth   yih         these features can then be used
to propositionalize the training instances  third  we can draw upon ideas from relational learning to
design a truly first order version of the k  ama learning algorithm  for example  one could use
existing first order generalization algorithms to generalize relational state descriptions  effectively
this approach pushes the object correspondence problem into the k  ama learning algorithm rather
than treating it as a preprocessing step  since it is well known that computing first order lggs can
be intractable  plotkin         practical generalization algorithms retain tractability by constraining
the lggs in various ways  e g   muggleton   feng        morales        
step three  our system uses the ideas of section     to speedup the k  ama lgg computation
for a set of training data  nevertheless  the computational complexity is still exponential in k thus 
in practice we are restricted to using relatively small values of k   while this restriction did not limit
performance in our visual event experiments  we expect that it will limit the direct applicability
of our system to more complex problems  in particular  many event types of interest may not
be adequately represented via k  ama when k is small  such event types  however  often contain
significant hierarchical structurei e   they can be decomposed into a set of short sub event types 
an interesting research direction is to consider using our k  ama learner as a component of a
hierarchical learning systemthere it could be used to learn k  ama sub event types  we note
that our learner alone cannot be applied hierarchically because it requires liquid primitive events 
but learns non liquid composite event types  further work is required  and intended  to construct a
hierarchical learner based perhaps on non liquid ama learning 
finally  recall that to compute the lgg of m examples  our system uses a sequence of m  
pairwise lgg calculations  for a fixed k   each pairwise calculation takes polynomial time  however  since the size of a pairwise lgg can grow by at least a constant factor with respect to the
inputs  the worst case time complexity of computing the sequence of m   pairwise lggs is exponential in m  we expect that this worst case will primarily occur when the target event type does not
have a compact k  ama representationin which case a hierarchical approach as described above
is more appropriate  when there is a compact representation  our empirical experience indicates
that such growth does not occurin particular  each pairwise lgg tends to yield significant pruning  for such problems  reasonable assumptions about the amount of pruning   imply that the time
complexity of computing the sequence of m   pairwise lggs is polynomial in m 

   experiments
    data set
our data set contains examples of   different event types  pick up  put down  stack  unstack  move 
assemble  and disassemble  each of these involve a hand and two to three blocks  for a detailed
description and sample video sequences of these event types  see siskind         key frames from
sample video sequences of these event types are shown in figure     the results of segmentation 
    in particular  assume that the size of a pairwise k ama lgg is usually bounded by the sizes of the k covers of the
inputs 

   

fil earning t emporal e vents

tracking  and model reconstruction are overlaid on the video frames  we recorded    movies for
each of the   event classes resulting in a total of     movies comprising       frames    we
replaced one assemble movie  assemble left qobi      with a duplicate copy of another  assembleleft qobi     because of segmentation and tracking errors 
some of the event classes are hierarchical in that occurrences of events in one class contain occurrences of events in one or more simpler classes  for example  a movie depicting a
m ove  a  b  c  d  event  i e  a moves b from c to d  contains subintervals where p ick u p  a  b  c 
and p ut d own  a  b  d  events occur  in our experiments  when learning the definition of an event
class only the movies for that event class are used in training  we do not train on movies for other
event classes that may also depict an occurrence of the event class being learned as a subevent 
however  in evaluating the learned definitions  we wish to detect both the events that correspond to
an entire movie as well as subevents that correspond to portions of that movie  for example  given a
movie depicting a m ove  a  b  c  d  event  we wish to detect not only the m ove a  b  c  d  event but
also the p ick u p  a  b  c  and p ut d own  a  b  d  subevents as well  for each movie type in our data
set  we have a set of intended events and subevents that should be detected  if a definition does not
detect an intended event  we deem the error a false negative  if a definition detects an unintended
event  we deem the error a false positive  for example  if a movie depicts a m ove a  b  c  d  event 
the intended events are m ove a  b  c  d   p ick u p  a  b  c   and p ut d own  a  b  c   if the definition
for pick up detects the occurrence of p ick u p  c  b  a  and p ick u p  b  a  c   but not p ick u p  a  b  c  
it will be charged two false positives as well as one false negative  we evaluate our definitions in
terms of false positive and negative rates as describe below 
    experimental procedure
for each event type  we evaluate the k  ama learning algorithm using a leave one movie out crossvalidation technique with training set sampling  the parameters to our learning algorithm are k
and the degree d of negative information used  the value of d is either p  for positive propositions
only  bn  for boundary negation  or n  for full negation  the parameters to our evaluation procedure
include the target event type e and the training set size n   given this information  the evaluation
proceeds as follows  for each movie m  the held out movie  from the     movies  apply the k ama learning algorithm to a randomly drawn training sample of n movies from the    movies of
event type e  or    movies if m is one of the      use l eonard to detect all occurrences of the
learned event definition in m   based on e and the event type of m   record the number of false
positives and false negatives in m   as detected by l eonard   let fp and fn be the total number
of false positives and false negatives observed over all     held out movies respectively  repeat the
entire process of calculating fp and fn    times and record the averages as fp and fn   
since some event types occur more frequently in our data than others because simpler events
occur as subevents of more complex events but not vice versa  we do not report fp and fn directly 
instead  we normalize fp by dividing by the total number of times l eonard detected the target
event correctly or incorrectly within all     movies and we normalize fn by dividing by the total
    the source code and all of the data used for these experiments are available as online appendix    and also from
ftp   ftp ecn purdue edu qobi ama tar z 
    while we did not record the times for our experiments  the system is fast enough to give live demos when n     
and k     with boundary negation  giving the best results we show here  though we dont typically record    training
videos in a live demo for other reasons   some of the less favorable parameter settings  particularly k     and full
negation  can take a  real time  hour or so 

   

fif ern   g ivan     s iskind

pick up

put down

stack

unstack

move

assemble

disassemble

figure     key frames from sample videos of the   event types 

   

fil earning t emporal e vents

number of correct occurrences of the target event within all     movies  i e   the human assessment
of the number of occurrences of the target event   the normalized value of fp estimates the probability that the target event did not occur given that it was predicted to occur  while the normalized
value of fn estimates the probability that the event was not predicted to occur given that it did
occur 
    results
to evaluate our k  ama learning approach  we ran leave one movie out experiments  as described
above  for varying k   d   and n   the     example movies were recorded with color coded objects to
provide complete object correspondence information  we compared our learned event definitions to
the performance of two sets of hand coded definitions  the first set hd  of hand coded definitions
appeared in siskind         in response to subsequent deeper understanding of the behavior of
l eonard s model reconstruction methods  we manually revised these definitions to yield another
set hd  of hand coded definitions that gives a significantly better fn performance at some cost
in fp performance  appendix c gives the event definitions in hd  and hd  along with a set of
machine generated definitions  produced by the k  ama learning algorithm  given all training data
for k      and d   bn 
      o bject c orrespondence
to evaluate our algorithm for finding object correspondences  we ignored the correspondence information provided by color coding and applied the algorithm to all training models for each event
type  the algorithm selected the correct correspondence for all     training models  thus  for this
data set  the learning results when no correspondence information is given will be identical to those
where the correspondences are manually provided  except that  in the first case  the rules will not
specify particular object roles  as discussed in section        since our evaluation procedure uses
role information  the rest of our experiments use the manual correspondence information  provided
by color coding  rather than computing it 
while our correspondence technique was perfect in these experiments  it may not be suited to
some event types  furthermore  it is likely to produce more errors as noise levels increase  since
correspondence errors represent a form of noise and our learner makes no special provisions for
handling noise  the results are likely to be poor when such errors are common  for example  in the
worst case  it is possible for a single extremely noisy example to cause the the lgg to be trivial  i e  
the formula true   in such cases  we will be forced to improve the noise tolerance of our learner 
      varying k

the first three rows of table   show the fp and fn values for all   event types for k   f       g  
n       the maximum   and d   bn  similar trends were found for d   p and d   n  the
general trend is that  as k increases  fp decreases or remains the same and fn increases or remains
the same  such a trend is a consequence of our k  cover approach  this is because  as k increases 
the k  ama language contains strictly more formulas  thus for k    k    the k   cover of a formula
will never be more general than the k   cover  this strongly suggests  but does not prove  that fp
will be non increasing with k and fn will be non decreasing with k  
our results show that   ama is overly general for put down and assemble  i e  it gives high
fp  in contrast    ama achieves fp     for each event type  but pays a penalty in fn compared
   

fif ern   g ivan     s iskind

k d
  bn

pick up

put down

stack

unstack

move

assemble

disassemble

fp
fn

 
 

    
    

 
    

 
    

 
 

    
 

 
 

 

bn

fp
fn

 
 

 
   

 
    

 
    

 
    

 
    

 
    

 

bn

fp
fn

 
 

 
   

 
    

 
    

 
    

 
    

 
    

 

p

fp
fn

    
 

   
    

 
    

    
    

 
    

 
    

 
    

 

bn

fp
fn

 
 

 
   

 
    

 
    

 
    

 
    

 
    

 

n

fp
fn

 
    

 
    

 
    

 
    

 
    

 
   

 
   

hd 

fp
fn

    
    

    
    

 
    

 
    

 
    

 
   

 
   

hd 

fp
fn

    
   

    
    

 
    

 
    

 
   

 
    

 
   

table    fp and fn for learned definitions  varying both k and d   and for hand coded definitions 
to   ama  since   ama achieves fp      there is likely no advantage in moving to k  ama for
k      that is  the expected result is for fn to become larger  this effect is demonstrated for
  ama in the table 
      varying d

rows four through six of table   show fp and fn for all   event types for d   fp  bn  ng  n      
and k      similar trends were observed for other values of k   the general trend is that  as the
degree of negative information increases  the learned event definitions become more specific  in
other words  fp decreases and fn increases  this makes sense since  as more negative information
is added to the training models  more specific structure can be found in the data and exploited by
the k  ama formulas  we can see that  with d   p  the definitions for pick up and put down are
overly general  as they produce high fp  alternatively  with d   n  the learned definitions are
overly specific  giving fp      at the cost of high fn  in these experiments  as well as others  we
have found that d   bn yields the best of both worlds  fp     for all event types and lower fn
than achieved with d   n 
experiments not shown here have demonstrated that  without negation for pick up and put down 
we can increase k arbitrarily  in an attempt to specialize the learned definitions  and never significantly reduce fp  this indicates that negative information plays a particularly important role in
constructing definitions for these event types 

   

fil earning t emporal e vents

      c omparison

to

h and  c oded d efinitions

the bottom two rows of table   show the results for hd  and hd    we have not yet attempted to
automatically select the parameters for learning  i e  k and d    rather  here we focus on comparing
the hand coded definitions to the parameter set that we judged to be best performing across all event
types  we believe  however  that these parameters could be selected reliably using cross validation
techniques applied to a larger data set  in that case  the parameters would be selected on a perevent type basis and would likely result in an even more favorable comparison to the hand coded
definitions 
the results show that the learned definitions significantly outperform hd  on the current data
set  the hd  definitions were found to produce a large number of false negatives on the current
data set  notice that  although hd  produces significantly fewer false negatives for all event types 
it produces more false positives for pick up and put down  this is because the hand definitions
utilize pick up and put down as macros for defining the other events 
the performance of the learned definitions is competitive with the performance of hd    the
main differences in performance are   a  for pick up and put down  the learned and hd  definitions
achieve nearly the same fn but the learned definitions achieve fp     whereas hd  has significant
fp   b  for unstack and disassemble  the learned definitions perform moderately worse than hd 
with respect to fn  and  c  the learned definitions perform significantly better than hd  on assemble
events 
we conjecture that further manual revision could improve hd  to perform as well as  and perhaps better than  the learned definitions for every event class  nonetheless  we view this experiment
as promising  as it demonstrates that our learning technique is able to compete with  and sometimes
outperform  significant hand coding efforts by one of the authors 
      varying n
it is of practical interest to know how training set size affects our algorithms performance  for this
application  it is important that our method work well with fairly small data sets  as it can be tedious
to collect event data  table   shows the fn of our learning algorithm for each event type  as n is
reduced from    to    for these experiments  we used k     and d   bn  note that fp    
for all event types and all n and hence is not shown  we expect fn to increase as n is decreased 
since  with specific to general learning  more data yields more general definitions  generally  fn
is flat for n       increases slowly for      n       and increases abruptly for     n       we
also see that  for several event types  fn decreases slowly  as n is increased from    to     this
indicates that a larger data set might yield improved results for those event types 
      p erspicuity

of

l earned d efinitions

one motivation for using a logic based event representation is to support perspicuityin this respect
our results are mixed  we note that perspicuity is a fuzzy and subjective concept  realizing this 
we will say that an event definition is perspicuous if most humans with knowledge of the language
would find the definition to be natural  here  we do not assume the human has a detailed knowledge of the model reconstruction process that our learner is trying to fit  adding that assumption
would presumably make the definitions qualify as more perspicuous  as many of the complex features of the learned definitions appear in fact to be due to idiosyncrasies of the model reconstruction
process  in this sense  we are evaluating the perspicuity of the output of the entire system  not just
   

fif ern   g ivan     s iskind

of the learner itself  so that a key route to improving perspicuity in this sense would be to improve
the intuitive properties of the model reconstruction output without any change to the learner 
while the learned and hand coded definitions are similar with respect to accuracy  typically the
learned definitions are much less perspicuous  for our simplest event types  however  the learned
definitions are arguably perspicuous  below we look at this issue in more detail  appendix c gives
the hand coded definitions in hd  and hd  along with a set of machine generated definitions  the
learned definitions correspond to the output of our k  ama learner when run on all    training
movies from each event type with k     and d   bn  i e   our best performing configuration with
respect to accuracy  
perspicuous definitions  the p ick u p  x  y  z   and p ut d own  x  y  z   definitions are of particular interest here since short state sequences appear adequate for representing these event types
thus  we can hope for perspicuous   ama definitions  in fact  the hand coded definitions involve short sequences  consider the hand coded definitions of p ick u p x  y  z  the definitions
can roughly be viewed as   ma timelines of the form begin trans end    state begin asserts facts
that indicate y is on z and is not being held by x and end asserts facts that indicate y is being held by
x and is not on z   state trans is intended to model the fact that l eonards model reconstruction
process does not always handle the transition between begin and end smoothly  so the definition
begin end does not work well   we can make similar observations for p ut d own x  y  z   
figure    gives the learned   ama definitions of p ick u p  x  y  z   and p ut d own  x  y  z  
the definitions contain six and two   ma timelines respectively  since the definitions consists of
multiple parallel timelines  they may at first not seem perspicuous  however  a closer examination
reveals that  in each definition  there is a single timeline that is arguably perspicuouswe have
placed these perspicuous timelines at the beginning of each definition  the perspicuous timelines
have a natural begin trans end interpretation  in fact  they are practically equivalent to the definitions
of p ick u p  x  y  z   and p ut d own  x  y  z   in hd     
with this in mind  notice that the hd  definitions are overly general as indicated by significant
false positive rates  the learned definitions  however  yield no false positives without a significant
increase in false negatives  the learned definitions improve upon hd  by essentially specializing
the hd  definitions  i e   the perspicuous timelines  by conjoining them with the non perspicuous
timelines  while these non perspicuous timelines are often not intuitive  they capture patterns in the
events that help rule out non events  for example  in the learned definition of p ick u p  x  y  z   some
of the non perspicuous timelines indicate that attached  y  z   is true during the transition period
of the event  such an attachment relationship does not make intuitive sense  rather  it represents a
systematic error made by the model reconstruction process for pick up events 
in summary  we see that the learned definitions of p ick u p  x  y  z   and p ut d own  x  y  z   each
contain a perspicuous timeline and one or more non perspicuous timelines  the perspicuous timelines give an intuitive definition of the events  whereas the non perspicuous timelines capture nonintuitive aspects of the events and model reconstruction process that are important in practice  we
note that  for experienced users  the primary difficulty of hand coding definitions for l eonard is
    note that the event logic definition for p ick u p x  y  z   in hd  is written in a more compact form than   ma  but
this definition can be converted to   ma  and hence   ama   rather  hd  cannot be translated exactly to   ma
since it uses disjunctionit is the disjunction of two   ma timelines 
    the primary difference is that the hd  definitions contain more negated propositions  the learner only considers a
proposition and its negation if the proposition is true at some point during the training movies  many of the negated
propositions in hd  never appear positively  thus they are not included in the learned definitions 

   

fil earning t emporal e vents

to determining which non perspicuous properties must be included  typically this requires many
iterations of trial and error  our automated technique can relieve the user of this task  alternatively 
we could view the system as providing guidance for this task 
large definitions  the s tack  w  x  y  z   and u nstack  w  x  y  z   events are nearly identical
to put down and pick up respectively  the only difference is that now we are picking up from and
putting down onto a two block  rather than single block  tower  i e   composed of blocks y and z   
thus  here again we might expect there to be perspicuous   ama definitions  however  we see that
the learned definitions for s tack  w  x  y  z   and u nstack  w  x  y  z   in figures    and    involve
many more timelines than those for p ick u p  w  x  y   and p ut d own  w  x  y    accordingly  the
definitions are quite overwhelming and much less perspicuous 
despite the large number of timelines  these definitions have the same general structure as those
for pick up and put down  in particular  they each contain a distinguished perspicuous timeline 
placed at the beginning of each definition  that is conjoined with many non perspicuous timelines 
it is clear that  as above  the perspicuous timelines have a natural begin trans end interpretation
and  again  they are very similar to the definitions in hd     in this case  however  the definitions
in hd  are not overly general  committing no false positives   thus  here the inclusion of the
non perspicuous timelines has a detrimental effect since they unnecessarily specialize the definition
resulting in more false negatives 
we suspect that a primary reason for the large number of non perspicuous timelines relative
to the definitions of pick up and put down stems from the increased difficulty of constructing
force dynamic models  the inclusion of the two block tower in these examples causes the modelreconstruction process to produce more unintended results  particularly during the transition periods
of s tack and u nstack   the result is that often many unintuitive and physically incorrect patterns
involving the three blocks and the hand are produced during the transition period  the learner
captures these patterns roughly via the non perspicuous timelines  it is likely that generalizing the
definitions by including more training examples would filter out some of these timelines  making the
overall definition more perspicuous  alternatively  it is of interest to consider pruning the learned
definitions  a straightforward way to do this is to generate negative examples  then with these 
we could remove timelines  generalizing the definition  that do not contribute toward rejecting the
negative examples  it is unclear how to prune definitions without negative examples 
hierarchical events  m ove w  x  y  z    a ssemble  w  x  y  z    and d isassemble  w  x  y  z  
are inherently hierarchical  being composed of the four simpler event types  the hand coded definitions leverage this structure by utilizing the simpler definitions as macros  in this light  it should
be clear that  when viewed non hierarchically   as our learner does  these events involve relatively
long state sequences  thus    ama is not adequate for writing down perspicuous definitions  in
spite of this representational shortcoming  our learned   ama definitions perform quite well  this
performance supports one of our arguments for using ama from section      namely  given that it
is easier to find short rather than long sequences  a practical approach to finding definitions for long
events is to conjoin the short sequences within those events  examining the timelines of the learned
  ama definitions reveals what we might expect  each timeline captures an often understandable
property of the long event sequence  but the conjunction of those timelines cannot be considered
to be a perspicuous definition  a future direction is to utilize hierarchical learning techniques to
improve the perspicuity of our definitions while maintaining accuracy 
   

fif ern   g ivan     s iskind

n

pick up

put down

stack

unstack

move

assemble

disassemble

  
  
  
  
  
 

   
   
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

table    fn for k

     d   bn  and various values of n  

we note  however  that  at some level  the learned definition of m ove  w  x  y  z   given in figure    is perspicuous  in particular  the first   ma timeline is naturally interpreted as giving the
pre  and post conditions for a move action  that is  initially x is supported by y and the hand w is
empty and finally x is supported by z and the hand w is empty  thus  if all we care about is preand post conditions  we might consider this timeline to be perspicuous  the remaining timelines in
the definition capture pieces of the internal event structure such as facts indicating that x is moved
by the hand  a weaker case can be made for assemble and disassemble  the first timeline in each
of the learned definitions in figures    and    can be interpreted as giving pre  and post conditions 
however  in these cases  the pre post  conditions for assemble disassemble  are quite incomplete 
the incompleteness is due to the inclusion of examples where the model reconstruction process did
not properly handle the initial final  moments 

   related work
here we discuss two bodies of related work  first  we present previous work in visual event recognition and how it relates to our experiments here  second  we discuss previous approaches to learning
temporal patterns from positive data 
    visual event recognition
our system is unique in that it combines positive only learning with a temporal  relational  and
force dynamic representation to recognize events from real video  prior work has investigated various subsets of the features of our systembut  to date  no system has combined all of these pieces
together  incorporating any one of these pieces into a system is a significant endeavor  in this respect  there are no competing approaches to directly compare our system against  given this  the
following is a representative list of systems that have common features with ours  it is not meant to
be comprehensive and focuses on pointing out the primary differences between each of these systems and ours  as these primary differences actually render these systems only very loosely related
to ours 
borchardt        presents a representation for temporal  relational  force dynamic event definitions but these definitions are neither learned nor applied to video  regier        presents techniques for learning temporal event definitions but the learned definitions are neither relational  force
dynamic  nor applied to video  in addition the learning technique is not truly positive onlyrather 
it extracts implicit negative examples of an event type from positive examples of other event types 
   

fil earning t emporal e vents

yamoto  ohya  and ishii         brand and essa         siskind and morris         brand  oliver 
and pentland         and bobick and ivanov        present techniques for learning temporal event
definitions from video but the learned definitions are neither relational nor force dynamic  pinhanez
and bobick        and brand      a  present temporal  relational event definitions that recognize
events in video but these definitions are neither learned nor force dynamic  brand      b  and mann
and jepson        present techniques for analyzing force dynamics in video but neither formulate
event definitions nor apply these techniques to recognizing events or learning event definitions 
    learning temporal patterns
we divide this body of work into three main categories  temporal data mining  inductive logic
programming  and finite statemachine induction 
temporal data mining  the sequence mining literature contains many general to specific  levelwise  algorithms for finding frequent sequences  agrawal   srikant        mannila  toivonen 
  verkamo        kam   fu        cohen        hoppner         here we explore a specific togeneral approach  in this previous work  researchers have studied the problem of mining temporal
patterns using languages that are interpreted as placing constraints on partially or totally ordered
sets of time points  e g   sequential patterns  agrawal   srikant        and episodes  mannila et al  
       these languages place constraints on time points rather than time intervals as in our work
here  more recently there has been work on mining temporal patterns using interval based pattern
languages  kam   fu        cohen        hoppner        
though the languages and learning frameworks vary among these approaches  they share two
central features which distinguish them from our approach  first  they all typically have the goal
of finding all frequent patterns  formulas  within a temporal data setour approach is focused
on finding patterns with a frequency of one  covering all positive examples   our first learning
application of visual event recognition has not yet required us to find patterns with frequency less
than one  however  there are a number of ways in which we can extend our method in that direction
when it becomes necessary  e g   to deal with noisy training data   second  these approaches all
use standard general to specific level wise search techniques  whereas we chose to take a specificto general approach  one direction for future work is to develop a general to specific level wise
algorithm for finding frequent ma formulas and to compare it with our specific to general approach 
another direction is to design a level wise version of our specific to general algorithmwhere for
example  the results obtained for the k  ama lgg can be used to more efficiently calculate the
 k      ama lgg  whereas a level wise approach is conceptually straightforward in a general tospecific framework it is not so clear in the specific to general case  we are not familiar with other
temporal data mining systems that take a specific to general approach 
first order learning in section      we pointed out difficulties in using existing first order
clausal generalization techniques for learning ama formulas  in spite of these difficulties  it is still
possible to represent temporal events in first order logic  either with or without capturing the ama
semantics precisely  and to apply general purpose relational learning techniques  e g   inductive
logic programming  ilp   muggleton   de raedt         most ilp systems require both positive
and negative training examples and hence are not suitable for our current positive only framework 
exceptions include g olem  muggleton   feng         p rogol  muggleton         and c lau dien  de raedt   dehaspe         among others  while we have not performed a full evaluation
   

fif ern   g ivan     s iskind

inputs
ma
ama

subsumption
semantic
syntactic
p
p
conp complete p

semantic ama lgg
lower upper size
p
conp exp
conp nexp   exp 

syntactic ama lgg
lower upper size
p
conp exp
p
conp exp

table    complexity results summary  the lgg complexities are relative to input plus output size 
the size column reports the worst case smallest correct output size  the   indicates a
conjecture 
of these systems  our early experiments in the visual event recognition domain confirmed our belief
that horn clauses  lacking special handling of time  give a poor inductive bias  in particular  many of
the learned clauses find patterns that simply do not make sense from a temporal perspective and  in
turn  generalize poorly  we believe a reasonable alternative to our approach may be to incorporate
syntactic biases into ilp systems as done  for example  in cohen         dehaspe and de raedt
        klingspor  morik  and rieger         in this work  however  we chose to work directly in a
temporal logic representation 
finite state machines finally  we note there has been much theoretical and empirical research
into learning finite state machines  fsms   angluin        lang  pearlmutter    price         we
can view fsms as describing properties of strings  symbol sequences   in our case  however  we are
interested in describing sequences of propositional models rather than just sequences of symbols 
this suggests learning a type of factored fsm where the arcs are labeled by sets of propositions
rather than by single symbols  factored fsms may be a natural direction in which to extend the
expressiveness of our current language  for example by allowing repetition  we are not aware of
work concerned with learning factored fsms  however  it is likely that inspiration can be drawn
from symbol based fsm learning algorithms 

   conclusion
we have presented a simple logic for representing temporal events called ama and have shown
theoretical and empirical results for learning ama formulas  empirically  weve given the first
system for learning temporal  relational  force dynamic event definitions from positive only input
and we have applied that system to learn such definitions from real video input  the resulting
performance matches that of event definitions that are hand coded with substantial effort by human
domain experts  on the theoretical side  table   summarizes the upper and lower bounds that
we have shown for the subsumption and generalization problems associated with this logic  in
each case  we have provided a provably correct algorithm matching the upper bound shown  the
table also shows the worst case size that the smallest lgg could possibly take relative to the input
size  for both ama and ma inputs  the key results in this table are the polynomial time ma
subsumption and ama syntactic subsumption  the conp lower bound for ama subsumption  the
exponential size of lggs in the worst case  and the apparently lower complexity of syntactic ama
lgg versus semantic lgg  we described how to build a learner based on these results and applied
it to the visual event learning domain  to date  however  the definitions we learn are neither crossmodal nor perspicuous  and while the performance of the learned definitions matches that of hand   

fil earning t emporal e vents

coded ones  we wish to surpass hand coding  in the future  we intend to address cross modality by
applying our learning technique to the planning domain  we also believe that addressing perspicuity
will lead to improved performance 

acknowledgments
the authors wish to thank our anonymous reviewers for helping to improve this paper  this work
was supported in part by nsf grants         iis and         iis  an nsf graduate fellowship
for fern  and the center for education and research in information assurance and security at
purdue university  part of this work was performed while siskind was at nec research institute 
inc 

appendix a  internal positive event logic
here we give the syntax and semantics for an event logic called internal positive event logic
 ipel   this logic is used in the main text only to motivate our choice of a small subset of this
logic  ama  by showing  in proposition    that ama can define any set of models that ipel can
define 
an event type  i e   set of models  is said to be internal if whenever it contains any model
m   hm  i i  it also contains any model that agrees with m on truth assignments m  i  where i   i  
full event logic allows the definition of non internal events  for example  the formula        p
is satisfied by hm  i i when there is some interval i   entirely preceding i such that p is satisfied
by hm  i   i  thus   is not internal  the applications we are considering do not appear to require
non internal events  thus we currently only consider events that are internal 
call an event type positive if it contains the model m   hm        i where m     is the truth
assignment assigning all propositions the value true  a positive event type cannot require any proposition to be false at any point in time 
ipel is a fragment of full propositional event logic that can only describe positive internal
events  we conjecture  but have not yet proven  that all positive internal events representable in the
full event logic of siskind        can be represented by some ipel formula  formally  the syntax
of ipel formulas is given by

e     true j prop j e    e  j  r e  j e   r e   
 

where the ei are ipel formulas  prop is a primitive proposition  sometimes called a primitive event
type   r is a subset of the thirteen allen interval relations fs f d b m o   si fi di bi ai oi g  allen 
       and r  is a subset of the restricted set of allen relations fs f d  g  the semantics for each
allen relation is given in table    the difference between ipel syntax and that of full propositional
event logic is that event logic allows for a negation operator  and that  in full event logic  r  can
be any subset of all thirteen allen relations  the operators   and   used to define ama formulas
are merely abbreviations for the ipel operators  f g and  fmg respectively  so ama is a subset of
ipel  though a distinguished subset as indicated by proposition    
each of the thirteen allen interval relations are binary relations on the set of closed naturalnumber intervals  table   gives the definitions of these relations  defining  m    m    r  n    n    for
each allen relation r   satisfiability for ipel formulas can now be defined as follows 
   

fif ern   g ivan     s iskind

i 
 m    m   
 m    m   
 m    m   
 m    m   
 m    m   
 m    m   
 m    m   

relation
s
f
d
b
m
o
 

i 
 n    n   
 n    n   
 n    n   
 n    n   
 n    n   
 n    n   
 n    n   

english
starts
finishes
during
before
meets
overlaps
equals

definition
m    n  and m 
m   n  and m 
m   n  and m 

 n 
  n 
 n 

m   n 
m    n  or m        n 
m   n   m   n 
m    n  and m    n 

inverse
si
fi
di
bi
mi
oi
 

table    the thirteen allen relations  adapted to our semantics  

 true is satisfied by every model 
 prop is satisfied by model hm  i i iff m  x  assigns prop true for every x   i  
 e    e  is satisfied by a model m iff m satisfies e  or m satisfies e  
  re is satisfied by model hm  i i iff for some r   r there is an interval i   such that i   r i
and hm  i   i satisfies e  
 e   r e  is satisfied by model hm  i i iff for some r   r there exist intervals i  and i  such
that i  r i    s pan  i    i      i and both hm  i  i satisfies e  and hm  i  i satisfies e   
where prop is a primitive proposition  e and ei are ipel formulas  r is a set of allen relations  and
s pan  i    i    is the minimal interval that contains both i  and i    from this definition  it is easy to
show  by induction on the number of operators and connectives in a formula  that all ipel formulas
define internal events  one can also verify that the definition of satisfiability given earlier for ama
formulas corresponds to the one we give here 

appendix b  omitted proofs
lemma    for any ma timeline  and any model m  if m satisfies  then there is a witnessing
interdigitation for map m    
proof  assume that m   hm  i i satisfies the ma timeline    s            sn   and let    
map m   it is straightforward to argue  by induction on the length of   that there exists a mapping
v   from states of  to sub intervals of i   such that

 for any i   v    s   m  i  satisfies s 
 v   s   includes the initial time point of i  
 v   sn  includes the final time point of i   and
 for any i       n     we have v   si   meets v   si     see table    
   

fil earning t emporal e vents

let v be the relation between states s    and members i   i that is true when i   v    s   note
that the conditions on v   ensure that every s    and every i   i appear in some tuple in v  not
necessarily together   below we use v to construct a witnessing interdigitation w  
let r be the total  one to one  onto function from time points in i to corresponding states in    
noting that   has one state for each time point in i   as     map hm  i i   note that r preserves
ordering in that  when i  j   r i  is no later than r j   in     let w be the composition v  r of
the relations v and r 
we show that w is an interdigitation  we first show that each state from  or   appears in a
tuple in w   so w is piecewise total  states from  must appear  trivially  because each appears in a
tuple of v   and r is total  states from   appear because each i   i appears in a tuple of v   and r
is onto the states of    
it now suffices to show that for any states s before t from   w  s  s    and w  t  t    implies that
s  is no later than t  in     so that w is simultaneously consistent  the conditions defining v   above
imply that every number in i   v  s  is less than or equal to every j   v  t   the order preservation
property of r  noted above  then implies that every state s    v  r s  is no later than any state
t    v  r t  in     as desired  so w is an interdigitation 
we now argue that w witnesses      consider s    and t     such that w  s  t   by the
construction of w   there must be i   v    s  for which t is the ith state of     since     map m  
it follows that t is the set of true propositions in m  i   since i   v    s   we know that m  i  satisfies
s  it follows that s  t  and so t  s   

  ipel  if model m embeds a model that satisfies e then m satisfies e  
proof  consider the models m   hm  i i and m    hm     i   i such that m embeds m    let
   map m  and     map m     assume that e   ipel is satisfied by m    we will show that
e is also satisfied by m 
we know from the definition of embedding that     and thus there is a witnessing interdigitation w for     by proposition    we know there is a one to one correspondence between
numbers in i  i     and states of       and denote the state in       corresponding to i   i  i    i    
lemma    for any e

as si  ti    this correspondence allows us to naturally interpret w as a mapping v from subsets of
i   to subsets of i as follows  for i    i     v  i     equals the set of all i   i such that for some i    i    
si co occurs with ti in w   we will use the following properties of v  
 

 

   if i   is a sub interval of i     then v  i     is a sub interval of i  

   if i   is a sub interval of i     then hm  v  i    i embeds hm     i   i 

   if i   and i   are sub intervals of i     and r is an allen relation  then i   ri   iff v  i    rv  i     
   if i   and i   are sub intervals of i     then v  s pan  i     i        s pan  v  i      v  i      

  

v  i       i  

we sketch the proofs of these properties     use induction on the length of i     with the
definition of interdigitation     since v  i     is an interval  map hm  v  i    i  is well defined 
map hm  v  i    i   map hm     i   i  follows from the assumption that m embeds m       from
appendix a  we see that all allen relations are defined in terms of the  relation on the natural
   

fif ern   g ivan     s iskind

number endpoints of the intervals  we can show that v preserves   but not    on singleton sets
 i e   every member of v  fi  g  is  every member of v  fj   g  when i   j     and that v commutes with set union  it follows that v preserves the allen interval relations     use the fact that
v preserves  in the sense just argued  along with the fact that s pan  i     i     depends only on the
minimum and maximum numbers in i   and i        follows from the definition of interdigitation and
the construction of v  
we now use induction on the number of operators and connectives in e to prove that  if m 
satisfies e   then so must m  the base case is when e   prop  where prop is a primitive proposition 
or true  since m  satisfies e   we know that prop is true in all m    x    for x    i     since w witnesses
      we know that  if prop is true in m    x   then prop is true in all m  x   where x   v  x    
therefore  since v  i       i   prop is true for all m    x   where x   i   hence m  satisfies e  
for the inductive case  assume that the claim holds for ipel formulas with fewer than n operators and connectiveslet e    e  be two such formulas  when e   e    e    the claim trivially
holds  when e    r e    r must be a subset of the set of relations fs f d  g  notice that e can
be written as a disjunction of  r e  formulas  where r is a single allen relation from r  thus  it
suffices to handle the case where r is a single allen relation  suppose e    fsg e    since m 
satisfies e   there must be a sub interval i   of i   such that i   s i   and hm     i   i satisfies e    let
i    v  i      we know from the properties of v that v  i       i   and  hence  that i  s i   furthermore  we know that hm  i  i embeds hm     i   i  and  thus  by the inductive hypothesis  hm  i  i
satisfies e    combining these facts  we get that e is satisfied by m  similar arguments hold for
the remaining three allen relations  finally  consider the case when e   e   r e    where r can
be any set of allen relations  again  it suffices to handle the case when r is a single allen relation
r  since m  satisfies e   e   r e    we know that there are sub intervals i   and i   of i   such that
s pan  i     i       i     i   r i     hm     i   i satisfies e    and hm     i   i satisfies e    from these facts  and
the properties of v   it is easy to verify that m satisfies e    
lemma    given an ma formula  that subsumes each member of a set  of ma formulas  
also subsumes some member   of ig    dually  when  is subsumed by each member of   we
have that  is also subsumed by some member   of is    in each case  the length of   can be
bounded by the size of  
proof  we prove the result for ig    the proof for is   follows similar lines  let

  

f            ng     s           sm  and assume that for each    i  n  i    from proposition    for each i  there is a witnessing interdigitation wi for i    we will combine the wi

into an interdigitation of   and show that the corresponding member of ig   is subsumed by
  to construct an interdigitation of   first notice that  for each sj   each wi specifies a set of
states  possibly a single state but at least one  from i that all co occur with sj   furthermore  since
wi is an interdigitation  it is easy to show that this set of states corresponds to a consecutive subsequence of states from i let j i be the ma timeline corresponding to this subsequence  now
let j   fj i j    i  ng  and ffj be any interdigitation of j   we now take i to be the union of
all ffj   for    j  m  we show that i is an interdigitation of   since each state s appearing in 
must co occur with at least one state sj in  in at least one wi   s will be in at least one tuple of ffj  
and  hence  be in some tuple of i so i is piecewise total 
now  define the restriction i i j of i to components i and j   with i   j   to be the relation given
by taking the set of all pairs formed by shortening tuples of i by omitting all components except
   

fil earning t emporal e vents

the ith and the j th  likewise define ffi j
k for each k   to show i is an interdigitation  it now suffices
to show that each i i j is simultaneously consistent  consider states si and sj from timelines i and
j   respectively  such that i i j  si   sj    suppose that ti occurs after si in i  and for some tj   j  
i i j  ti   tj   holds  it suffices to show that sj is no later than tj in j   since i i j  si   sj   and i i j  ti   tj   
i j
 
 
we must have ffi j
k  si   sj   and ffk  ti   tj    respectively  for some k and k   we know k  k because
 
si is before ti in i and wi is simultaneously consistent  if k   k   then sj is no later than tj in j  
because ffk must be simultaneously consistent  being an interdigitation  otherwise  k   k     then sj
is no later than tj in j   as desired  because wj is simultaneously consistent  so i is simultaneously
consistent  and an interdigitation of  
let   be the member of ig   corresponding to i   we now show that      we know that
each state s      is the intersection of the states in a tuple of some ffj we say that s  derives from
ffj   consider the interdigitation i   between  and     where i    sj   s     for sj    and s        if and
only if s  derives from ffj   i   is piecewise total  as every tuple of i   derives from some ffj   and no ffj
is empty  i   is simultaneously consistent because tuples of i   deriving from later ffk must be later in
the lexicographic ordering of i   given the simultaneous consistency of the wk interdigitations used
to construct each ffj   finally  we know that sj subsumes  i e   is a subset of  each state in each tuple
of ffj   because each wk is a witnessing interdigitation to k    and  hence  subsumes  is a subset
of  the intersection of those states  therefore  if sj    co occurs with s      in i   we have that
s   sj   thus  i   is a witnessing interdigitation for      and by proposition   we have     
the size bound on   follows  since  as pointed out in the main text  the size of any member of
ig   is upper bounded by the number of states in    
 

lemma    given ma timelines     s            sm and     t            tn   there is a witnessing
interdigitation for      iff there is a path in the subsumption graph sg         from v    to
vm n  
proof  subsumption
graph sg         is equal to hv  e i with v   fvi j j     i  m     j  ng

and e   hvi j   vi  j i j si  tj   si  tj   i  i   i      j  j    j       note that there is a
correspondence between vertices and state tupleswith vertex vi j corresponding to hsi   tj i 
for the forward direction  assume that w is a witnessing interdigitation for        we
know that  if the states si and tj co occur in w   then si  tj since w witnesses        the
vertices corresponding to the tuples of w will be called co occurrence vertices  and satisfy the
first condition for belonging to some edge in e  that si  tj    it follows from the definition of
interdigitation that both v    and vm n are both co occurrence vertices  consider a co occurrence
vertex vi j not equal to vm n   and the lexicographically least co occurrence vertex vi  j after vi j
 ordering vertices
 by ordering
the pair of subscripts   we show that i  j   i    and j   satisfy the
ff
requirements for vi j   vi  j   e   if not  then either i    i     or j     j      if i    i      then
there can be no co occurrence vertex vi   j   contradicting that w is piecewise total  if j     j     
then since w is piecewise total  there must be a co occurrence vertex vi  j      but if i     i or
i     i    this contradicts the simultaneous consistency of w   and if i     i  this contradicts the
lexicographically least choice of vi  j   it follows that every co occurrence vertex but vm n has an
edge to another co occurrence vertex closer in manhattan distance to vm n   and thus that there is a
path from v    to vm n  
for the reverse direction assume there is a path of vertices in sg         from v    to vm n
given by  vi   j    vi   j            vir  js with i    j       ir   m  js   n  let w be the set of state
 

 

 

 

 

 

 

  

  

 

 

   

 

fif ern   g ivan     s iskind

tuples corresponding to the vertices along this path  w must be simultaneously consistent with the
i orderings because our directed edges are all non decreasing in the i orderings  w must be
piecewise total because no edge can cross more than one state transition in either   or     by the
edge set definition  so w is an interdigitation  finally  the definition of the edge set e ensures
that each tuple hsi   tj i in w has the property si  tj   so that w is a witnessing interdigitation for
       showing that        as desired   
lemma     given some n  let   be the conjunction of the timelines
n
 
i  

f propn  truei  falsei  propn    propn  falsei  truei  propn g 

we have the following facts about truth assignments to the boolean variables p            pn  
   for any truth assignment a  propn   sa   propn is semantically equivalent to a member
of is    
   for each    is    there is a truth assignment a such that   propn   sa   propn  
proof  to prove the first part of the lemma  we construct an interdigitation i of   such that the
corresponding member of is    is equivalent to propn   sa   propn   intuitively  we construct i
by ensuring that some tuple of i consists only of states of the form truek or falsek that agree with
the truth assignmentthe union of all the states in this tuple  taken by is    will equal sa   let
i   ft    t    t    t    t  g be an interdigitation of   with exactly five state tuples ti   we assign the
states of each timeline of   to the tuples as follows 
   for any k   such that    k




 n and a pk   is true 

for the timeline s    s    s    s    q  t ruek   f alsek   q  assign each state si to tuple ti  
and assign state s  to t  as well  and
for the timeline s     s     s     s     q  f alsek   t ruek   q  assign each state s i to tuple ti    
and state s   to tuple t  as well 

   for any k   such that    k  n and a pk   is false  assign states to tuples as in item   while
interchanging the roles of t ruek and f alsek  

it should be clear that i is piecewise total and simultaneously consistent with the state orderings
in    and so is an interdigitation  the union of the states in each of t    t    t    and t  is equal to
propn   since propn is included as a state in each of those tuples  furthermore  we see that the
union of the states in t  is equal to sa   thus  the member of is    corresponding to i is equal to
propn   propn   sa   propn   propn   which is semantically equivalent to propn   sa   propn   as
desired 
to prove the second part of the lemma  let  be any member of is     we first argue that
every state in  must contain either truek or falsek for each    k  n  for any k   since   contains propn   truek   falsek   propn   every member of is    must be subsumed by propn   truek  
falsek   propn   so   is subsumed by propn   truek   falsek   propn   but every state in propn  
truek   falsek   propn contains either truek or falsek   implying that so does   as desired 
   

fil earning t emporal e vents

next  we claim that for each    k  n  either   truek or   falsek i e   either all states
in  include truek   or all states in  include falsek  and possibly both   to prove this claim  assume 
for the sake of contradiction  that  for some k      truek and    falsek   combining this assumption with our first claim  we see there must be states s and s  in  such that s contains t ruek but
not f alsek   and s  contains f alsek but not t ruek   respectively  consider the interdigitation i of  
that corresponds to  as a member of is     we know that s and s  are each equal to the union of
states in tuples t and t     respectively  of i   t and t   must each include one state from each timeline
s    s    s    s    propn   truek   falsek   propn and s     s     s     s     propn   falsek   truek   propn  
clearly  since s does not include falsek   t includes the states s  and s     and likewise t   includes
the states s  and s     it follows that i is not simultaneously consistent with the state orderings in
s    s    s    s  and s     s     s     s     contradicting our choice of i as an interdigitation  this shows that
either   truek or   falsek  
define the truth assignment a such that for all    k  n  a pk   if and only if   truek  
since for each k     truek or   falsek   it follows that each state of  is subsumed by
sa   furthermore  since  begins and ends with propn   it is easy to give an interdigitation of
 and propn   sa   propn that witnesses   propn   sa   propn   thus  we have that  
propn   sa   propn    
lemma     let   and   be as given on page      in the proof of theorem     and let    
v
ig f      g   for any    whose timelines are a subset of those in   that omits some square
timeline  we have         
proof  since the timelines in    are a subset of the timelines in    we know that         it remains
to show that         we show this by constructing a timeline that is covered by      but not by   
let    s    s            s n   be a square timeline in   that is not included in      recall that each
si is a single proposition from the proposition set p   fpi j j    i  n     j  ng  and that 
for consecutive states si and si     if si   pi j   then si   is either pi   j or pi j      define a new
timeline    s    s            s n   with si    p si    we now show that      so that        and
that  for any   in   fg       so that        
for the sake of contradiction  assume that   then there must be a interdigitation w
witnessing     we show by induction on i that  for i     w  si   sj   implies j   i  for the
base case  when i      we know that s    s    since s    s    and so w  s    s    is false  since
w witnesses subsumption  for the inductive case  assume the claim holds for all i    i  and that
w  si   sj    we know that si   si   and thus i    j   because w is piecewise total  we must have
w  si     sj   for some j     and  by the induction hypothesis  we must have j     i    since w is
simultaneously consistent with the sk and sk state orderings  and i     i  we have j    j   it
follows that j   i as desired  given this claim  we see that s n   cannot co occur in w with any
state in   contradicting the fact that w is piecewise total  thus we have that     
let     s             s m be any timeline in   fg  we now construct an interdigitation that
witnesses       note that while  is assumed to be square    need not be  let j be the smallest
index where sj    s j  since s    s     p      and         we know that such a j must exist  and is
in the range    j  m  we use the index j to guide our construction of an interdigitation  let w
be an interdigitation of  and     with exactly the following co occurring states  i e   state tuples  
 

 

   for    i  j

   si   co occurs with s i  
   

fif ern   g ivan     s iskind

 i  m  sj co occurs with s i 
for j      i   n    si co occurs with s m  

   for j
  

it is easy to check that w is both piecewise total and simultaneously consistent with the state
orderings in  and   and so is an interdigitation  we now show that w witnesses     by
showing that all states in  are subsumed by the states they co occur with in w   for co occurring
states si   and s i corresponding to the first item above we have that s i   si this implies that s i
is contained in si     giving that si    s i   now consider co occurring states sj and s i from the
second item above  since  is square  choose k and l so that sj     pk l   we have that sj is either
pk   l or pk l    in addition  since sj     s j   we have that s j is either pk   l   pk l   or pk   l  
but that sj    s j   in any of these cases  we find that no state in   after s j can equal sj this follows
by noting that the proposition indices never decrease across the timeline        we therefore have
that  for i  j   sj  s i   finally  for co occurring states si and s m from item three above  we have
si  s m   since s m   pn n  which is in all states of   thus  we have shown that for all co occurring
states in w   the state from  is subsumed by the co occurring state in     therefore  w witnesses
      which implies that        
lemma     for any model hm  i i   m and any     ama  
t  hm  i i  

  covers hm  i i

iff

f     covers

proof  recall that m is the set of models over propositions in the set p   fp            pn g and that
we assume ama uses only primitive propositions from p  possibly negated   we also have the
set of propositions p   fp            pn g  and assume that formulas in ama use only propositions in
p   p and that m is the set of models over p   p   where for each i  exactly one of pi and pi is
true at any time  note that f     is in ama and that t  hm  i i  is in m  we prove the lemma via
straightforward induction on the structure of  proving the result for literals  then for states  then
for timelines  and finally for ama formulas 
to prove the result for literals  we consider two cases  the third case of true is trivial   first   
can be a single proposition pi   so that      f  pi     pi   consider any model hm  i i   m and let
hm     i i   t  hm  i i   the following relationships yield the desired result 

  covers hm  i i

iff
iff
iff

for each i   i   m  i  assigns pi true
for each i   i   m    i  assigns pi true
     pi covers t  hm  i i 

 by definition of satisfiability 
 by definition of t  
 by definition of satisfiability 

the second case is when   is a negated proposition   pi here  we get that      pi   let
hm  i i   m and hm     i i   t  hm  i i   the following relationships yield the desired result 

  covers hm  i i

iff
iff
iff

for each i   i   m  i  assigns pi false
for each i   i   m    i  assigns pi true
     pi covers t  hm  i i 

 by definition of satisfiability 
 by definition of t  
 by definition of satisfiability 

this proves the lemma for literals 
    note that if
pk   l    

 were not required to be square then it is possible for    to equal
 

sj

   

sj

i e   they could both equal

fil earning t emporal e vents

to prove the result for states  we use induction on the number k of literals in a state  the base
case is when k      the state is a single literal  and was proven above  now assume that the lemma
holds for states with k or fewer literals and let     l         lk   and hm  i i   m  from the
inductive assumption we know that    l       lk covers hm  i i iff f    covers t  hm  i i   from
our base case we also know that lk   covers hm  i i iff f  lk     covers t  hm  i i   from these facts
and the definition of satisfiability for states  we get that   covers hm  i i iff f      f  lk     covers
t  hm  i i   clearly f has the property that f      f  lk       f      showing that the lemma holds
for states 
to prove the result for timelines  we use induction on the number k of states in the timeline  the
base case is when k      the timeline is a single state  and was proven above  now assume that the
lemma holds for timelines with k or fewer states  let     s            sk   and hm   t  t   i   m with
hm      t  t   i   t  hm   t  t   i   we have the following relationships 

  covers hm   t  t   i

iff
iff
iff
iff

there exists some t      t  t     such that s  covers hm   t  t    i and
   s            sk   covers either hm   t     t   i or hm   t        t   i
there exists some t      t  t     such that f  s    covers hm      t  t    i and
f    covers either hm      t     t   i or hm      t        t   i
f  s     f    covers hm      t  t   i
f     covers hm      t  t   i

where the first iff follows from the definition of satisfiability  the second follows from our inductive
hypothesis  our base case  and the fact that for i   t  t    we have t  hm  i i    hm     i i  the third
follows from the definition of satisfiability  and the fourth follows from the fact that f  s     f     
f     
finally  we prove the result for ama formulas  by induction on the number k of timelines
in the formula  the base case is when k      the formula is a single timeline  and was proven
above  now assume that the lemma holds for ama formulas with with k or fewer timelines
and let              k   and hm  i i   m  from the inductive assumption  we know that
              k covers hm  i i iff f       covers t  hm  i i   from our base case  we also
know that k   covers hm  i i iff f  k     covers t  hm  i i   from these facts and the definition of
satisfiability  we get that   covers hm  i i iff f         f  k     covers t  hm  i i   clearly f has the
property that f         f  k       f      showing that the lemma holds for ama formulas  this
completes the proof   

appendix c  hand coded and learned definitions used in our experiments
below we give the two sets of hand coded definitions  hd  and hd    used in our experimental
evaluation  we also give a set of learned ama event definitions for the same seven event types  the
learned definitions correspond to the output of our k  ama learning algorithm  given all available
training examples     examples per event type   with k     and d   bn  all the event definitions
are written in event logic  where   p denotes the negation of proposition p 

   

fif ern   g ivan     s iskind

 

 

 

p ick u p  x  y  z  

 

p ut d own x  y  z  

 

s tack  w  x  y  z  

 

u nstack  w  x  y  z  

 

m ove w  x  y  z  
a ssemble w  x  y  z  
d isassemble w  x  y  z  

 

 

 

 
 
 
 
 
 

  x   y     z   x     z   y 
c
b s upported y       attached x  z   
b    
    c
c
b  
  attached x  y      s upports x  y  
 
 
c
b  
 
 
 
c
b  
 
 
s upports  z  y   
 
 
 
c
b  
 
 
 
 
 
c
b  
 
 
 
 
s upported x      attached y  z         
 
c
b  
 
 
 
 
c
b  
 
 
 
  s upports y  x      s upports y  z   
 
 
c
b  
 
 
 
b  
 
  s upports x  z       s upports z  x 
  c
c
b  
c
b
b     attached x  y     attached y  z     
    c
 
c
b  
attached x  y     s upports x  y   
 
 
c
b  
 
 
 
 
 
c
b  
 
  s upports z  y  
 
 
 
 
c
b  
 
 
 
 
 
c
b  
 
 
 
s
upported
 
x
 
 
 
 
a
ttached
 
y 
z
 
 
 
 
 
 
c
b  
 
 
 
 
 
 
a
   
 
 
s
upports
 
y 
x
 
 
 
 
s
upports
 
y 
z
 
 
 
 
 
 
 
 
  s upports x  z       s upports z  x 
 
 
  x   y     z   x     z   y 
c
b s upported y       attached x  z   
b    
    c
c
b  
a
ttached
 
x  y     s upports  x  y   
 
 
c
b  
 
 
c
 
b  
 
 
 
 
s
upports
 
z 
y
 
 
 
 
c
 
b  
 
 
 
 
c
b  
 
 
 
 
 
s
upported
 
x
 
 
 
 
a
ttached
 
y 
z
 
 
 
 
 
c
b  
 
 
 
 
 
c
b  
 
 
 
 
 
s
upports
 
y 
x
 
 
 
 
s
upports
 
y 
z
 
 
 
 
c
b  
 
 
 
b  
 
 
 
s
upports
 
x 
z
 
 
 
 
s
upports
 
z 
x
 
  c
c
b  
c
b
b     attached x  y     attached y  z     
    c
 
c
b  
a
ttached
 
x  y        s upports x  y   
 
 
 
 
  c
b  
   
 
c
b  
 
  s upports  z  y   
 
 
    c
b  
 
 
c
b  
 
    s upported x      attached y  z       
 
 
    c
b  
 
 
 
a
   
    s upports y  x      s upports y  z       
 
 
 
 
 
 
  s upports x  z       s upports z  x 
 
 
  z   w     z   x     z   y 
  p ut d own w  x  y     s upports z  y     
 attached z  y 


  z   w     z   x     z   y 
p ick u p  w  x  y     s upports z  y      attached z  y  
  y   z    p ick u p w  x  y   p ut d own w  x  z   
p ut d own w  y  z    f g s tack  w  x  y  z  
u nstack w  x  y  z    f g p ick u p  x  y  z  
 

 

figure     the hd  event logic definitions for all seven event types 

   

fil earning t emporal e vents

 

 
  x   y     z   x     z   y 
b
c
 y      attached  x  z   
b s upported
c
 
  c
b    
b  
c
a
ttached  x  y       s upports  x  y   
 
 
 
 
b  
c
   
 
 
 
b  
c
    s upports  z  y     c ontacts  z  y   
 
 
 
b  
c
 
 
 
 
 
b  
c
      s upported  x      attached  y  z       f  mg  
 
b  
c
 
 
 
 
 
b  
c
 
      s upports  y  x      s upports  y  z     
 
 
b  
c
 
 
 
p ick u p  x  y  z     b  
  c
 
b
c
s
upports  x  z       s upports  z  x 
 
 
 
b
 
c
b  
c
 
a
ttached
 
x 
y
 
 
s
upports
 
x 
y
 
 
 
b  
c
 
   
 
 
b  
c
 
 
 
 
s
upports
 
z 
y
 
 
 
 
 
b  
c
 
   
 
 
b  
c
 
 
 
 
 
s
upported
 
x
 
 
 
 
a
ttached
 
y 
z
 
 
 
 
b  
c
   
 
 
 
b  
c
 
 
 
 
 
   
a
 
 
s
upports
 
y 
x
 
 
 
 
s
upports
 
y 
z
 
 
 
 
 
 
 
 
  s upports x  z      s upports z  x 
 
 
  x   y     z   x     z   y 
c
b
 y      attached  x  z   
c
b s upported
 
  c
b    
c
b  
a
ttached  x  y     s upports  x  y   
 
 
c
b  
 
   
 
 
c
b  
 
 
s
upports
 
z 
y
 
 
 
 
 
 
 
c
b  
 
   
 
 
c
b  
 
 
 
 
s
upported
 
x
 
 
 
 
a
ttached
 
y 
z
 
 
 
 
 
 
f
  
m
g
c
b  
 
 
 
  c
   
b  
 
 
 
 
 
 
s
upports
 
y 
x
 
 
 
 
s
upports
 
y 
z
 
 
  b 
c
 
 
 
p ut d own  x  y  z     b  
  c
 
c
b
 
 
s
upports  x  z       s upports  z  x 
 
c
b
 
c
b  
 
 
 attached  x  y      s upports x  y  
 
c
b  
 
 
 
 
 
c
b  
 
    s upports  z  y     c ontacts  z  y   
 
 
c
b  
 
 
 
 
 
b  
c
 
      s upported  x      attached  y  z     
 
b  
c
 
 
 
 
 
b  
  c
 
 
    s upports  y  x      s upports  y  z     
a
   
 
 
 
 
 
 
  s upports x  z      s upports z  x 

figure     part i of the hd  event logic definitions 

   

fif ern   g ivan     s iskind

 

 

  w   x     y   w     y   x 
b   z   w     z   x     z   y  
c
b
c
b s upported  x      attached w  y   
c
b    
  c
 
b  
c
attached w  x    s upports  w  x  
 
b  
 
c
   
 
b  
 
 
c
 
 
s upports y  x  
 
b  
 
 
 
 
c
   
 
b  
 
 
c
s upports z  y     c ontacts z  y   
 
b  
 
 
 
 
c
   
 
b  
 
c
 
 
 
attached z  y   
 
f
mg  
b  
 
 
c
 
 
   
 
 
b  
c
 
 
 
s upported w      attached x  y     
 
b  
 
c
 
 
   
 
b  
c
 
 
  s upports x  w      s upports x  y  
 
b  
 
 
  c
b  
c
b
c
    s upports w  y       s upports y  w 
 
b  
c
  attached w  x      s upports w  x  
 
b  
c
 
 
b  
c
 
 
 
 
s upports y  x    c ontacts  y  x  
 
b  
c
 
 
 
 
 
b  
c
 
 
 
 
 
b  
c
  s upports z  y     c ontacts z  y   
 
 
 
 
 
b  
c
 
 
 
 
b  
c
 
      attached z  y   
 
 
 
b  
c
 
 
 
 
b  
c
      s upported w      attached x  y     
 
 
 
 
   
a
 
 
  s upports x  w      s upports x  y  
 
 
 
 
 
 
  s upports w  y      s upports y  w 
 
 
  w   x     y   w     y   x 
c
b   z   w     z   x     z   y  
c
b
c
b s upported x      attached w  y   
  c
b    
 
c
b  
  attached w  x      s upports w  x  
 
c
 
b  
 
c
 
 
b  
 
 
 
c
 
  s upports  y  x    c ontacts  y  x  
b  
 
 
 
 
c
 
 
b  
 
 
c
 
b  
 
    s upports  z  y     c ontacts  z  y   
 
 
c
 
 
b  
 
c
 
    attached z  y   
b  
   f mg  
 
 
 
c
 
 
b  
 
 
c
 
b  
      s upported w      attached x  y     
 
 
c
 
b  
 
 
  s upports x  w      s upports x  y  
 
c
 
b  
 
 
 
c
b
c
b
    s upports w  y       s upports y  w   
c
b  
attached w  x    s upports w  x  
 
c
 
b  
 
 
 
c
 
 
b  
 
c
 
 
b  
      s upports y  x  
 
 
 
c
 
 
b  
 
  s upports  z  y     c ontacts  z  y   
c
 
 
b  
 
 
 
 
c
 
 
b  
 
c
 
 
b  
      attached z  y   
 
 
 
c
 
 
b  
 
c
 
b  
      s upported w      attached x  y     
 
 
 
 
a
 
   
  s upports x  w      s upports x  y  
 
 
 
 
 
 
  s upports w  y      s upports y  w 
  y   z    p ick u p w  x  y   p ut d own w  x  z   
p ut d own w  y  z    f g s tack  w  x  y  z  
u nstack  w  x  y  z    f g p ick u p  x  y  z  
  

s tack w  x  y  z  

 

 

  

u nstack w  x  y  z  

m ove w  x  y  z  
a ssemble w  x  y  z  
d isassemble w  x  y  z  

 

 

 
 
 
 
 
 

 

 

figure     part ii of the hd  event logic definitions 

   

fil earning t emporal e vents

   
     
s upported  y     s upports  z  y   
 
 
 
 
 
b  
    c ontacts  y  z       s upports x  y   
    
 
 
b  
 
 
 
b  
 
  attached x  y      attached y  z  
 
b  
b
s upported y   
b    
    
 
b  
s upported  y     s upports  x  y   
 
 
 
b  
 
 
b  
 
 
 
attached x  y       s upports z  y   
 
 
 
b  
 
 
b
  c ontacts y  z       attached y  z  
b  
b   s upported y   
 
b  

  
 
b  
s upported y     attached x  y   
b
 
 
b  
attached y  z  
 
b  
 
 
 
b
b    s upported y     attached x  y     
b    s upported y     c ontacts  y  z       
b
b
b    s upported y     attached y  z         
b
b     s upported y     attached x  y   
   
b  
s upported  y     s upports  z  y   
 
 
b  
 
    c ontacts  y  z       s upports x  y   
b  
    
 
b  
b
 
a
ttached
 
x  y       attached y  z  
 
b  
 
 
b  
 
s
upported
 
y
 
 
s
upports
 
z 
y
  
 
 
 
 
b  
 
b    s upported y     attached x  y   
 
b  
b    s upported y     s upports  z  y     
 
 
b  
 
 
   s upported y     attached x  y     
b  
b    
   
b
s
upported
 
y     s upports  x  y   
b  
 
   
   
 
    attached x  y       s upports z  y   
 
 
 
 

 

 

 

 

p ick u p  x  y  z  

 

 

 

 

 

 

 

 

  c ontacts y  z       attached y  z  

p ut d own x  y  z  

 

 

 
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
a

   
     
s upported y     s upports x  y     attached x  y   
 
 
 
 
      s upports z  y       c ontacts y  z   
 
b  
    
 
b  
 
 
 
b  
  attached y  z  
b
 
b   s upported  y   
 
 
b  


 
 
 
b  
s upported y     s upports  z  y     c ontacts z  y   
 
b  
 
 
 
b  
 
s
upports
 
x  y      
a
ttached
 
x  y  
b   
 

b  
b
 s upported  y     attached x  y    
  
 
s upported  y     attached x  y     attached y  z    
 
 

 
 

 

 

 

s upported  y  

figure     the learned   ama definitions for p ick u p  x  y  z   and p ut d own  x  y  z   

   

 
c
c
c
c
c
c
c
c
c
c
c
c
a

fif ern   g ivan     s iskind

   
 
 
b  
b
b  
b  
b  
b  
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b  
b
b
b  
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b  
b
b
b  
b  
b
b  
b
b
b  
b
b  
b
b
b
b
b  
b  
b
b
b  
b
b  
b  
b  
b  
b
   
 
 

h

 

 

 

 

s upported y   attached w  x  s upports z  y   c ontacts y  z  
s upports x  y  
s upports y  x 
c ontacts x  y  
attached x  y  

  

    

    

    

i

 

 
 
 
 

 

c
c
i    c
s upported y     s upported x    s upports y  x    c ontacts x  y     c ontacts y  z   
 
  c
c
  s upports x  y      attached w  x      attached x  y       attached y  z 
c
c
 s upported y    attached w  x    
c
 s upported y    attached x  y    
 
c
c
 s upported y    s upported x    s upports y  x    c ontacts x  y  
 
c
c
 s upported y    attached w  x    
 s upported y    s upports x  y    attached w  x    attached x  y    attached y  z       c
c
c
 s upported y    s upported x s upports y  x  
 
c
c
 s upported y    attached w  x    
c
 s upported y    s upported x    s upports x  y    s upports y  x    attached w  x      
c
c
 s upported y    s upported x    s upports y  x  
 
c
c
 s upported y    attached w  x    s upports z  y    c ontacts y  z    
c
 s upported y    attached y  z    
 
c
c
 s upported y    s upported x    s upports y  x    c ontacts y  z    
c
c
 s upported y    attached w  x    s upports z  y    c ontacts y  z    
c
 s upported y    attached w  x    attached y  z    
 
c
c
 hs upported y    s upported x    s upports y  x  
i  
c
s upported y     attached w  x    s upports z  y     c ontacts y  z   
c
 
 
c
  s upports x  y      s upports y  x      c ontacts x  y      attached x  y 
c
 
c
 s upported y    attached w  x    
 
c
 s upported y    s upported x    s upports y  x  
c
 
c
 s upported y    attached w  x    
c
 s upported y    attached w  x    s upports z  y    c ontacts y  z      
c
c
 s upported y    s upported x  
c
 
c
 s upported y    attached w  x    
c
 s upported y    attached w  x    s upports z  y    s upported x      
c
c
 s upported y    s upported x  
c
 
c
 hs upported y    attached w  x    
i  
c
s upported y     c ontacts y  z     s upports z  y     s upported x  
c
 
 
c
  s upports x  y      attached x  y 
 
c
c
 s upported y    s upported x  
 
c
s upported y   
c
h
i  
c
s upported y     c ontacts y  z     s upports z  y     s upported x  
 
 
c
  s upports x  y      attached x  y      attached y  z 
c
 
c
 s upported y    s upported x    s upports y  x    
c
c
 s upported y    attached w  x    
c
 s upported y    c ontacts y  z    s upported x      
c
c
 s upported y    s upported x    s upported y x 
  c
 s upported y    attached w  x    
  c
c
 hs upported y    s upported x    s upports y  x    
i
 c
s upported y     s upported x    s upports y  x    c ontacts x  y     c ontacts y  z   
  c
c
  s upports x  y      attached w  x      attached x  y      attached y  z 
  c
s
upported y   
  c
h
i
 
  c
s upported y     s upported x    s upports y  x    s upports z  y   
c
 
c
c
ontacts x  y     c ontacts y  z  
h
i   a
s upported y     s upported x    s upports y  x    c ontacts x  y     c ontacts y  z   
 
 

 s upported y    
h

  s upports x  y      attached w  x      attached x  y      attached y  z 

figure     the learned   ama definition for s tack  w  x  y  z   

   

fil earning t emporal e vents

   
 
 
b  
 
b  
 
b  
 
b  
b
b  
b  
 
b  
b  
 
 
b  
b  
b
b  
b
b
b
b
b  
b
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b  
 
b  
 
b  
b  
b
b  
b  
 
b  
 
b  
b  
b
b  
 
b  
 
b  
b  
b
b  
b  
 
b  
 
b  
b
b  
b
b
b
b
b  
b
b  
 
b
b
b  
b  
b
b  
b
b
b
b
b  
b
b
b
b
b
b  
b  
b  
b
 
 
 

 

 

 

s upported x    s upported y     s upports y  x  
 
 
 
 
c ontacts x  y     c ontacts y  z       s upports w  x  
 
 
 
 
  s upports x  y      attached w  x      attached x  y 
 
 
  s upported x    s upported y    
 
 
s upported x    s upported y     attached w  x    s upports z  y   
 
 
 
  c ontacts y  z     attached w  x      s upports x  y   
   
 
 
   
 
  s upports y  x      c ontacts x  y  
 
 
  attached x  y      attached y  z 
 
 s upported x    s upported y    s upports y  x    
 s upported x    s upported y    attached w  x    attached y  z      
 s upported x    s upported y    attached w  x    c ontacts y  z    
 s upported x    s upported y    s upports y  x    c ontacts y  z    
 s upported x    s upported y    attached y  z    
 
 s upported x    s upported y    attached w  x    c ontacts y  z    
 s upported x    s upported y    s upports y  x    c ontacts x  y    
 s upported x    s upported y    s upports y  x    attached x  y      
 s upported x    s upported y    attached w  x    
 s upported x    s upported y    s upports y  x    
 s upported x    s upported y    c ontacts y  z      
 s upported x    s upported y    attached w  x  
 
 s upported x    s upported y    s upports y  x    
 
 
 
 
  s upported x    s upported y    attached w  x    
   
 
s upported x    s upported y     attached w  x    s upports z  y   
 
   
  c ontacts y  z     attached w  x      s upports x  y   
   
 
 
 
  s upports y  x      c ontacts x  y  
 
 
attached x  y       attached y  z  
 
 
 
   
s upported x    s upported y     s upports y  x  
 
 
 
  c ontacts x  y     c ontacts y  z   
   
 
 
    
  s upports w  x      s upports x  y  
 
  attached w  x      attached x  y 
 
 
 
 
 s upported x    s upported y    s upports y  x    
 
 
 s upported x    s upported y    attached w  x  
 s upported x    s upported y    s upports y  x    c ontacts y  z      
 s upported x    s upported y    s upports y  x    attached y  z      
 s upported x    s upported y    attached w  x  
 
 s upported x    s upported y    s upports y  x    

  
 
s upported x    s upported y     s upports y  x    attached y  z   
 
 
s upports x  y     attached w  x    attached x  y  
 
 
 s upported x    s upported y    attached w  x  
 
 s upported x    s upported y    
 s upported x    s upported y    s upports y  x    attached w  x      
 s upported x    s upported y    s upports w  x    attached w  x    
 s upported x    s upported y    s upports y  x    
 s upported x    s upported y    s upports w  x    attached w  x      
 s upported x    s upported y    attached w  x  
 
 s upported x    s upported y    s upports y  x    

  
 
s upported x    s upported y     c ontacts y  z   
 
  s upports x  y      attached x  y      attached y  z 
 
 
 s upported x    s upported y  

figure     the learned   ama definition for u nstack  w  x  y  z   

   

 
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
a

fif ern   g ivan     s iskind

   
 
 
 
b  
 
b  
 
b  
 
b  
b  
b
b  
b  
 
b  
 
b  
 
b  
 
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
 
 
 

 
 
 

s upported  x    s upports  y  x    c ontacts  y  x  
  s upports w  x      s upports z  x      c ontacts x  z  
  attached w  x      attached  y  x      attached  x  z 

 
 
 

s upported  x  
s upported  x    s upports  z  x    c ontacts  x  z   
 
 
   s upports  w  x      s upports  y  x      c ontacts  y  x  
  attached w  x      attached  y  x      attached  x  z 
 s upported  x    s upports  y  x      
 
 s upported  x    attached  w  x        
 
s upported  x 
 
 
s upported  x  
 
 s upported  x    attached  w  x    attached  x  z         
 
s upported  x 
 
 
 s upported  x    
 
 s upported  x    attached  x  z         
 s upported  x    c ontacts  x  z     
 
 
s upported  x  
 
 s upported  x    attached  w  x    s upports  w  x        
 
s upported  x 
 
 
s upported  x  
 
 s upported  x    attached  w  x    attached  y  x        
 
s upported  x 
 
 s upported  x    c ontacts  y  x      
 
 s upported  x    attached  y  x      
 
s upported  x 
 

figure     the learned   ama definition for m ove  w  x  y  z   

   

 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
a

fil earning t emporal e vents

   
 
 
 
b  
 
 
b  
b  
 
 
b  
b
b
b  
b  
 
b  
 
b  
 
b  
 
b  
 
b
b  
b  
b  
b  
 
b  
 
b
b
b  
b  
 
b  
 
b  
b  
b
b  
b  
b
b  
b  
b  
b
b  
b  
b
b  
b  
b  
b
b  
b  
b
b  
b  
b  
b
b  
b  
 
 
 

 

 

 

  s upported  x      s upports z  y      s upports y  x      
 
 
 
 
 
   c ontacts  x  y       c ontacts  z  y   
    
 
 
 
 
 
  attached w  x      attached  z  y 
 

true
 
 
 
 
 
 
 

 

s upported  x    s upported  y     s upports  z  y   
 
s upports  y  x    c ontacts  x  y   
 
c ontacts  z  y       attached  w  y  
  s upported  x      s upports z  y      s upports y  x  
  c ontacts x  y      c ontacts z  y  
  attached w  x      attached  z  y 

attached  w  y   
s upported  y  

 
 
 

true 

 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 s upported  y      attached  w  x      attached  z  y        
 
s upported  y  
 
 
true 
 
 s upported  y    attached  z  y        
 s upported  y    c ontacts  z  y    
true 
 s upported  y    s upports  z  y c ontacts  z  y    attached  w  x    
s upported  y  
 
 
true 
 
 s upported  y    attached  w  y attached  z  y      
 
s upported  y  
figure     the learned   ama definition for a ssemble  w  x  y  z   

   

 

 

 

 
 
 
 
 

 

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
a

fif ern   g ivan     s iskind

   
 
 
b  
 
b  
 
b  
 
b  
 
b  
b
b  
b  
b  
 
b  
 
b  
 
b  
 
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b
b
b  
b
b  
b  
b
 
 

 

 

 

s upported  x    s upported y     s upports y  x    s upports  z  y   
 
 
   
 
  c ontacts  x  y     c ontacts z  y       s upports w  x  
    
 
 
 
   
    s upports w  y       s upports x  y       attached x  w  
 
 
 
  attached w  y      attached x  y      attached z  y 
 
s upported y   
 
 
 
 
 
 
s upported  y       s upported x      s upports w  x  
 
 
 
 
    s upports z  y       s upports y  x      c ontacts x  y       
 
 
 
  c ontacts z  y      attached x  w      attached  z  y 
  s upported  x    s upported  y     
 

  
 
s upported x    s upported  y     s upports  w  x  
 
 
s upports z  y     c ontacts  z  y     attached x  w 
 
 
 
s upported y  
 


s upported x    s upported  y     s upports  z  y   
 
 
 
 
s upports y  x    c ontacts  x  y     c ontacts z  y  
 
  s upported  x    s upported  y     s upports  y  x    attached  x  y       
 
 
s upported y  
 
  s upported  x    s upported  y     s upports  y  x    c ontacts  z  y       
 


 
s upported x    s upported  y     s upports  x  y   
 
 
s upports y  z     attached x  y     attached z  y  
 
 
 
s upported y  
 
  s upported  x    s upported  y     s upports  y  x    
 
  

 
s upported x    s upported  y     s upports  x  y   
 
 
s upports y  z     attached x  y     attached z  y     attached x  w 
 
 
 
s upported y  
 
s upported y   
 
  s upported  y     attached  w  y     attached  z  y     
 
 
s upported y  
 
s upported y   
 
  s upported  y     s upports  w  y     attached  w  y     
 
s upported y  

 
 

 
 

 
 

 

 
 

 

 

 
 

 
 

figure     the learned   ama definition for d isassemble  w  x  y  z   

   

 
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
a

fil earning t emporal e vents

references
agrawal  r     srikant  r          mining sequential patterns  in proceedings of the eleventh
international conference on data engineering  pp      
allen  j  f          maintaining knowledge about temporal intervals  communications of the acm 
               
angluin  d          learning regular sets from queries and counterexamples  information and
computation            
bacchus  f     kabanza  f          using temporal logics to express search control knowledge for
planning  artificial intelligence             
bobick  a  f     ivanov  y  a          action recognition using probabilistic parsing  in proceedings of the ieee computer society conference on computer vision and pattern recognition 
pp          santa barbara  ca 
borchardt  g  c          event calculus  in proceedings of the ninth international joint conference
on artificial intelligence  pp          los angeles  ca 
brand  m       a   the inverse hollywood problem  from video to scripts and storyboards via
causal analysis  in proceedings of the fourteenth national conference on artificial intelligence  pp          providence  ri 
brand  m       b   physics based visual understanding  computer vision and image understanding                
brand  m     essa  i          causal analysis for visual gesture understanding  in proceedings of
the aaai fall symposium on computational models for integrating language and vision 
brand  m   oliver  n     pentland  a          coupled hidden markov models for complex action
recognition  in proceedings of the ieee computer society conference on computer vision
and pattern recognition 
cohen  p          fluent learning  elucidating the structure of episodes  in proceedings of the
fourth symposium on intelligent data analysis 
cohen  w          grammatically biased learning  learning logic programs using an explicit antecedent description lanugage  artificial intelligence             
cohen  w     hirsh  h          learning the classic description logic  theoretical and experimental results  in proceedings of the fourth international conference on principles of knowledge
representation and reasoning  pp         
de raedt  l     dehaspe  l          clausal discovery  machine learning            
dehaspe  l     de raedt  l          dlab  a declarative language bias formalism  in proceedings
of the ninth international syposium on methodologies for intelligent systems  pp         
fikes  r     nilsson  n          strips  a new approach to the application of theorem proving to
problem solving  artificial intelligence         
hoppner  f          discovery of temporal patternslearning rules about the qualitative behaviour
of time series  in proceedings of the fifth european conference on principles and practice
of knowledge discovery in databases 
   

fif ern   g ivan     s iskind

kam  p     fu  a          discovering temporal patterns for interval based events  in proceedings
of the second international conference on data warehousing and knowledge discovery 
klingspor  v   morik  k     rieger  a  d          learning concepts from sensor data of a mobile
robot  artificial intelligence                  
lang  k   pearlmutter  b     price  r          results of the abbadingo one dfa learning competition and a new evidence driven state merging algorithm  in proceedings of the fourth
international colloquium on grammatical inference 
lavrac  n   dzeroski  s     grobelnik  m          learning nonrecursive definitions of relations
with linus  in proceedings of the fifth european working session on learning  pp     
    
mann  r     jepson  a  d          toward the computational perception of action  in proceedings
of the ieee computer society conference on computer vision and pattern recognition  pp 
        santa barbara  ca 
mannila  h   toivonen  h     verkamo  a  i          discovery of frequent episodes in sequences 
in proceedings of the first international conference on knowledge discovery and data mining 
mitchell  t          generalization as search  artificial intelligence               
morales  e          pal  a pattern based first order inductive system  machine learning         
    
muggleton  s          inverting entailment and progol  machine intelligence             
muggleton  s     feng  c          efficient induction of logic programs  in muggleton  s   ed   
inductive logic programming  pp          academic press 
muggleton  s     de raedt  l          inductive logic programming  theory and methods  journal
of logic programming                
pinhanez  c     bobick  a          scripts in machine understanding of image sequences  in
proceedings of the aaai fall symposium series on computational models for integrating
language and vision 
plotkin  g  d          automatic methods of inductive inference  ph d  thesis  edinburgh university 
regier  t  p          the acquisition of lexical semantics for spatial terms  a connectionist model
of perceptual categorization  ph d  thesis  university of california at berkeley 
roth  d     yih  w          relational learning via propositional algorithms  an information extraction case study  in proeedings of the seventeenth international joint conference on artificial
intelligence 
shoham  y          temporal logics in ai  semantical and ontological considerations  artificial
intelligence               
siskind  j  m          visual event classification via force dynamics  in proceedings of the seventeenth national conference on artificial intelligence  pp          austin  tx 
siskind  j  m          grounding the lexical semantics of verbs in visual perception using force
dynamics and event logic  journal of artificial intelligence research           
   

fil earning t emporal e vents

siskind  j  m     morris  q          a maximum likelihood approach to visual event classification  in proceedings of the fourth european conference on computer vision  pp         
cambridge  uk  springer verlag 
talmy  l          force dynamics in language and cognition  cognitive science            
yamoto  j   ohya  j     ishii  k          recognizing human action in time sequential images using
hidden markov model  in proceedings of the ieee conference on computer vision and
pattern recognition  pp         

   

fi