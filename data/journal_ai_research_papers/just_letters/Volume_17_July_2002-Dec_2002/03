journal artificial intelligence research                  

submitted       published      

specific to general learning temporal events
application learning event definitions video
alan fern
robert givan
jeffrey mark siskind

afern   purdue   edu
givan   purdue   edu
qobi   purdue   edu

school electrical computer engineering
purdue university  west lafayette        usa

abstract
develop  analyze  evaluate novel  supervised  specific to general learner simple temporal logic use resulting algorithm learn visual event definitions video
sequences  first  introduce simple  propositional  temporal  event description language called
ama sufficiently expressive represent many events yet sufficiently restrictive support
learning  give algorithms  along lower upper complexity bounds  subsumption generalization problems ama formulas  present positive examplesonly
specific to general learning method based algorithms  present polynomialtimecomputable syntactic subsumption test implies semantic subsumption without
equivalent it  generalization algorithm based syntactic subsumption used place
semantic generalization improve asymptotic complexity resulting learning algorithm 
finally  apply algorithm task learning relational event definitions video
show yields definitions competitive hand coded ones 

   introduction
humans conceptualize world terms objects events  reflected fact
talk world using nouns verbs  perceive events taking place objects 
interact world performing events objects  reason effects
actual hypothetical events performed us others objects  learn new
object event types novel experience  paper  present evaluate novel implemented techniques allow computer learn new event types examples  show results
application techniques learning new event types automatically constructed
relational  force dynamic descriptions video sequences 
wish acquired knowledge event types support multiple modalities  humans
observe someone faxing letter first time quickly able recognize future occurrences
faxing  perform faxing  reason faxing  thus appears likely humans use
learn event representations sufficiently general support fast efficient use multiple
modalities  long term goal research allow similar cross modal learning use
event representations  intend learned representations used vision  as described
paper   planning  something beginning investigate   robotics  something
left future  
crucial requirement event representations capture invariants event
type  humans classify picking cup table picking dumbbell floor
picking up  suggests human event representations relational  abstract

c      ai access foundation morgan kaufmann publishers  rights reserved 

fif ern   g ivan     iskind

relational notion picking parameterized participant objects rather distinct
propositional notions instantiated specific objects  humans classify event picking
matter whether hand moving slowly quickly  horizontally vertically  leftward
rightward  along straight path circuitous one  appears characteristics
participant object motion distinguish picking event types  rather  fact
object picked changes supported resting initial location
supported grasped agent  suggests primitive relations used
build event representations force dynamic  talmy        
another desirable property event representations perspicuous  humans
introspect describe defining characteristics event types  introspection allows us create dictionaries  support introspection  prefer representation language
allows characteristics explicitly manifest event definitions emergent consequences distributed parameters neural networks hidden markov models 
develop supervised learner event representation possessing desired characteristics follows  first  present simple  propositional  temporal logic called ama
sublanguage variety familiar temporal languages  e g  linear temporal logic  ltl bacchus   kabanza        event logic siskind         logic expressive enough describe
variety interesting temporal events  restrictive enough support effective learner 
demonstrate below  proceed develop specific to general learner ama logic giving algorithms complexity bounds subsumption generalization problems involving
ama formulas  show semantic subsumption intractable  provide weaker syntactic notion subsumption implies semantic subsumption checked polynomial
time  implemented learner based upon syntactic subsumption 
next show means adapt  propositional  ama learner learn relational concepts 
evaluate resulting relational learner complete system learning force dynamic event
definitions positive only training examples given real video sequences  first
system perform visual event recognition video  review prior work compare
current work later paper  fact  two prior systems built one
authors  h oward  siskind   morris        learns classify events video using temporal 
relational representations  representations force dynamic  l eonard  siskind 
      classifies events video using temporal  relational  force dynamic representations
learn representations  uses library hand code representations  work adds
learning component l eonard   essentially duplicating performance hand coded
definitions automatically 
demonstrated utility learner visual eventlearning domain 
note many domains interesting concepts take form structured temporal sequences events  machine planning  macro actions represent useful temporal patterns
action  computer security  typical application behavior  represented perhaps temporal patterns system calls  must differentiated compromised application behavior  and likewise
authorized user behavior intrusive behavior  
follows  section   introduces application domain recognizing visual events
provides informal description system learning event definitions video  section  
introduces ama language  syntax semantics  several concepts needed analysis
language  section   develops analyzes algorithms subsumption generalization
problems language  introduces practical notion syntactic subsumption  sec   

fil earning emporal e vents

tion   extends basic propositional learner handle relational data negation  control
exponential run time growth  section   presents results visual event learning  sections  
  compare related work conclude 

   system overview
section provides overview system learning recognize visual events video 
aim provide intuitive picture system providing technical details  formal
presentation event description language  algorithms  theoretical empirical results appears sections     first introduce application domain visual event recognition
l eonard system  event recognizer upon learner built  second  describe
positive only learner fits overall system  third  informally introduce ama
event description language used learner  finally  give informal presentation
learning algorithm 
    recognizing visual events
l eonard  siskind        system recognizing visual events video camera input
example simple visual event hand picking block  research originally
motivated problem adding learning component l eonardallowing l eonard
learn recognize event viewing example events type  below  give high level
description l eonard system 
l eonard three stage pipeline depicted figure    raw input consists video frame
image sequence depicting events  first  segmentation and tracking component transforms
input polygon movie  sequence frames  frame set convex polygons placed
around tracked objects video  figure  a shows partial video sequence pick event
overlaid corresponding polygon movie  next  model reconstruction component
transforms polygon movie force dynamic model  model describes changing
support  contact  attachment relations tracked objects time  constructing
model somewhat involved process described siskind         figure  b shows
visual depiction force dynamic model corresponding pick event  finally  eventrecognition component armed library event definitions determines events occurred
model and  accordingly  video  figure  c shows text output input
event recognizer pick event  first line corresponds output indicates
interval s  pick occurred  remaining lines text encoding
event recognizer input  model reconstruction output   indicating time intervals various
force dynamic relations true video 
event recognition component l eonard represents event types event logic formulas following simplified example  representing x picking z  

 

p ick u p  x  y  z      s upports  z      c ontacts  z       s upports  x      attached  x    

formula asserts event x picking z defined sequence two states
z supports way contact first state x supports way attachment
second state  upports   c ontacts   attached primitive force dynamic relations 
formula specific example general class ama formulas use
learning 
   

fif ern   g ivan     iskind

image
sequence

segmentation
tracking

polygonscene
sequence

model
reconstruction

training
models
event
labels

model
sequence

event
learner

event
classification

event
labels

learned event
definitions

figure    upper boxes represent three primary components l eonards pipeline 
lower box depicts event learning component described paper  input
learning component consists training models target events  e g   movies pick
events  along event labels  e g   p ick u p  hand  red  green   output
event definition  e g   temporal logic formula defining p ick u p  x  y  z    
    adding learning component
prior work reported paper  definitions l eonard event recognition library
hand coded  here  add learning component l eonard learn recognize
events  figure   shows event learner fits overall system  input event
learner consists force dynamic models model reconstruction stage  along event
labels  output consists event definitions used event recognizer  take
supervised learning approach force dynamic model reconstruction process applied
training videos target event type  resulting force dynamic models along labels
indicating target event type given learner induces candidate definition
event type 
example  input learner might consist two models corresponding two videos 
one hand picking red block green block label p ick u p  hand  red  green 
one hand picking green block red block label p ick u p  hand  green  red the
output would candidate definition p ick u p  x  y  z   applicable previously unseen
pick events  note learning component positive only sense learning
target event type uses positive training examples  where target event occurs 
use negative examples  where target event occur   positive only setting
interest appears humans able learn many event definitions given primarily
positive examples  practical standpoint  positive only learner removes often difficult
task collecting negative examples representative event learned
 e g   typical non pickup event   
construction learner involves two primary design choices  first  must choose
event representation language serve learners hypothesis space  i e   space event definitions may output   second  must design algorithm selecting good event definition
hypothesis space given set training examples event type 
    ama hypothesis space
full event logic supported l eonard quite expressive  allowing specification
wide variety temporal patterns  formulas   help support successful learning  use
   

fil earning emporal e vents

 a 

frame  

frame  

frame  

frame   

frame   

frame   

frame  

frame  

frame  

frame   

frame   

frame   

 b 

 pick up hand red green                   

 c 

 supported  red            
 supported  hand                       
 supports  red hand                       
 supports  hand red             
 supports  green red            
 supports  green hand            
 contacts  red green                     
 attached  red hand            
 attached  red green           

figure    l eonard recognizes pick event   a  frames raw video input automatically generated polygon movie overlaid   b  frames visual depiction
automatically generated force dynamic properties   c  text input output
event classifier corresponding depicted movie  top line output
remaining lines make input encodes changing force dynamic properties 
green represents block table red represents block picked up 

   

fif ern   g ivan     iskind

restrictive subset event logic  called ama  learners hypothesis space  subset excludes
many practically useless formulas may confuse learner  still retaining substantial
expressiveness  thus allowing us represent learn many useful event types  restriction
ama formulas form syntactic learning bias 
basic ama formulas called states express constant properties time intervals arbitrary duration  example  upports  z      c ontacts  z    state tells us
z must support contact   general  state conjunction number
primitive propositions  in case force dynamic relations   using ama describe
sequences states  example   s upports  z      c ontacts  z        s upports  x     
attached  x     sequence two states  first state given second
state indicating x must support attached   formula true whenever first
state true time interval  followed immediately second state true
time interval meeting first time interval  sequences called timelines since
meets ands  general  timelines contain number states  finally 
conjoin timelines get ama formulas  ands mas   example  ama formula

  s upports  z  y    c ontacts  z  y      s upports  x  y    attached  x  y     
  s upports  u  v    attached  u  v      s upports  w  v    c ontacts  w  v   
defines event two timelines must true simultaneously time interval 
using ama formulas represent events listing various property sequences  ma timelines  
must occur parallel event unfolds  important note  however 
transitions states different timelines ama formula occur relation one
another  example  ama formula  transition two states first
timeline occur before  after  exactly transition states second timeline 
important assumption leveraged learner primitive propositions used construct states describe liquid properties  shoham         purposes  say property
liquid holds time interval holds subintervals  force dynamic
properties produced l eonard liquide g   hand upports block interval
clearly hand supports block subintervals  primitive propositions
liquid  properties described states  conjunctions primitives  liquid  however  properties described ama formulas not  general  liquid 
    specific to general learning positive data
recall examples wish classify learn force dynamic models 
thought  and derived from  movies depicting temporal events  recall
learner outputs definitions ama hypothesis space  given ama formula  say
covers example model true model  particular target event type  such
p ick u p    ultimate goal learner output ama formula covers example
model model depicts instance target event type  understand
learner  useful define generality relationship ama formulas  say ama
formula   general  less specific  ama formula     covers every
example   covers  and possibly more   
   formal analysis  use two different notions generality  semantic syntactic   section 
ignore distinctions  note  however  algorithm informally describe later section based
syntactic notion generality 

   

fil earning emporal e vents

learning goal find ama formula consistent set positiveonly training data  one result trivial solution returning formula covers
examples  rather fix problem adding negative training examples  which rule
trivial solution   instead change learning goal finding least general
formula covers positive examples   learning approach pursued
variety different languages within machine learning literature  including clausal first order
logic  plotkin         definite clauses  muggleton   feng         description logic  cohen  
hirsh         important choose appropriate hypothesis space bias learning
approach hypothesis returned may simply  or resemble  one two extremes  either
disjunction training examples universal hypothesis covers examples 
experiments  found that  enough training data  least general ama formula often
converges usefully 
take standard specific to general machine learning approach finding least general
ama formula covers set positive examples  approach relies computation two
functions  least general covering formula  lgcf  example model least general
generalization  lgg  set ama formulas  lgcf example model least general
ama formula covers example  intuitively  lgcf ama formula captures
information model  lgg set ama formulas least general ama
formula general formula set  intuitively  lgg formula set
ama formula captures largest amount common information among formulas 
viewed differently  lgg formula set covers examples covered formulas 
covers examples possible  while remaining ama   
resulting specific to general learning approach proceeds follows  first  use lgcf
function transform positive training model ama formula  second  return lgg
resulting formulas  result represents least general ama formula covers
positive training examples  thus  specify learner  remains provide algorithms computing lgcf lgg ama language  informally describe
algorithms computing functions  formally derived analyzed sections       
    computing ama lgcf
increase readability presentation  follows  dispense presenting examples primitive properties meaningfully named force dynamic relations  rather 
examples utilize abstract propositions b  current application  propositions correspond exclusively force dynamic properties  may applications 
demonstrate system computes lgcf example model 
consider following example model  fa         b         c         d         d       g   here 
take number                represent time interval arbitrary  possibly varying
number  duration nothing changes  fact p  i  j   indicates proposition p continuously true throughout time intervals numbered j   model
depicted graphically  shown figure    top four lines figure indicate time
   avoids need negative examples corresponds finding specific boundary version space
 mitchell        
   existence uniqueness lgcf lgg defined formal property hypothesis space
proven ama sections        respectively 

   

fif ern   g ivan     iskind

 

 



 

 





b

b

 

 

b

b
c


a d







  a b d   a b   b d   b c d

figure    lgcf computation  top four horizontal lines figure indicate intervals propositions a  b  c true model given
fa         b         c         d         d       g   bottom line shows model
divided intervals transitions occur  lgcf timeline 
shown bottom figure  state no transition intervals 
state simply contains true propositions within corresponding interval 
intervals propositions a  b  c  true model  bottom line
figure shows model divided five time intervals propositions
change truth value  division possible assumption propositions
liquid  allows us  example  break time interval true three consecutive subintervals true  dividing model intervals transitions 
compute lgcf simply treating intervals state timeline 
states contain propositions true corresponding time interval 
resulting five state timeline shown bottom figure  show later simple
computation returns lgcf model  thus  see lgcf model always
timeline 
    computing ama lgg
describe algorithm computing lgg two ama formulasthe lgg
formulas computed via sequence   pairwise lgg applications  discussed later 
consider two timelines       a   b   c    b   c   d   e      a   b   e   a   e   d  
useful consider various ways timelines true simultaneously along
arbitrary time interval  this  look various ways two timelines
aligned along time interval  figure  a shows one many possible alignments
timelines  call alignments interdigitationsin general  exponentially many
interdigitations  one ordering state transitions differently  note interdigitation
allowed constrain two transitions different timelines occur simultaneously  though
depicted figure   
   thus  interdigitation provides ordering relation transitions need anti symmetric  reflexive 
transitive  total 

   

fil earning emporal e vents

 a 

a b e
 b 

a b c

b c d



e d

e

a b c a b c a b c b c d
a b e
a b  



e d

e d



  true  



e
e d
 

e

figure    generalizing timelines  a   b   c    b   c   d   e  a   b   e   a   e   d    a 
one exponentially many interdigitations two timelines   b  computing
interdigitation generalization corresponding interdigitation part  a   states
formed intersecting aligned states two timelines  state true represents
state propositions 

given interdigitation two timelines  easy construct new timeline must
true whenever either timelines true  i e   construct generalization two timelines  
figure  b  give construction interdigitation given figure  a  top two
horizontal lines figure correspond interdigitation  divided every state
either timeline two identical states  whenever transition occurs state
timeline  resulting pair timelines simultaneous transitions viewed
sequence state pairs  one timeline  bottom horizontal line labeled
timeline one state state pair  state intersection
proposition sets state pair  here  true represents empty set propositions  state
true anywhere 
call resulting timeline interdigitation generalization  ig       
clear ig true whenever either     true  particular    holds along
time interval model  sequence consecutive  meeting  subintervals
sequence states   true  construction  ig aligned relative   along
interval view states sets  states ig subsets corresponding
aligned state s      thus  ig states true model alignment  showing
ig true model 
general  exponentially many igs two input timelines  one possible
interdigitation two  clearly  since ig generalization input timelines 
conjunction igs  conjunction ama formula generalizes
input timelines  fact  show later paper ama formula lgg
two timelines  show conjunction igs     serves
lgg 
   

fif ern   g ivan     iskind

  a   b   b  e  true  e   
  a   b   b  true  e   
  a   b   b  true  true  e   
  a   b   b  true  e   
  a   b   b  true  d  e   
  a   b   true  true  e   
  a   b   true  e   
  a   b   true  d  e   
  a   b   a  true  true  e   
  a   b   a  true  e   
  a   b   a  true  d  e   
  a   b   a  d  e   
  a   b   a  true  d  e 
formula lgg  contains redundant timelines pruned  first 
clear different igs result timelines  remove one copy
timeline lgg  second  note timeline   general timeline
      equivalent thus  prune away timelines generalizations
others  later paper  show efficiently test whether one timeline general
another  performing pruning steps  left first next last
timelines formulathus    a   b   a  d  e      a   b   b  e  true  e  lgg  
   
demonstrated compute lgg pairs timelines  use
procedure compute lgg pairs ama formulas  given two ama formulas compute
lgg simply conjoining lggs pairs timelines  one ama formula 
i e   formula
m 
n
 
lgg i    j  
j

lgg two ama formulas               n    j
timelines 
informally described lgcf lgg operations needed carry
specific to general learning approach described above  follows  formally develop
operations analyze theoretical properties corresponding problems  discuss
needed extensions bring  exponential  propositional  negation free  operations
practice 

   representing events ama
present formal account ama hypothesis space analytical development
algorithms needed specific to general learning ama  readers primarily interested
high level view algorithms empirical evaluation may wish skip sections    
instead proceed directly sections      discuss several practical extensions
basic learner present empirical evaluation 
study subset interval based logic called event logic  siskind        utilized
l eonard event recognition video sequences  logic interval based explicitly rep   

fil earning emporal e vents

resenting possible interval relationships given originally allen        calculus
interval relations  e g   overlaps  meets  during   event logic formulas allow definition
event types specify static properties intervals directly dynamic properties
hierarchically relating sub intervals using allen relations  paper  formal syntax
semantics full event logic needed proposition   given appendix a 
restrict attention much simpler subset event logic call ama  defined
below  believe choice event logic rather first order logic  well restriction
ama fragment event logic  provide useful learning bias ruling large number
practically useless concepts maintaining substantial expressive power  practical utility
bias demonstrated via empirical results visual eventrecognition application 
ama seen restriction ltl  bacchus   kabanza        conjunction
until  similar motivations  present syntax semantics ama along
key technical properties ama used throughout paper 
    ama syntax semantics
natural describe temporal events specifying sequence properties must hold
consecutive time intervals  example  hand picking block might become block
supported hand block supported hand  represent
sequences timelines    sequences conjunctive state restrictions  intuitively 
timeline given sequence propositional conjunctions  separated semicolons 
taken represent set events temporally match sequence consecutive conjunctions 
ama formula conjunction number timelines  representing events
simultaneously viewed satisfying conjoined timelines  formally  syntax
ama formulas given by 
state

ama

    true j prop j prop   state
     state  j  state  
   may omit parens
    j   ama

prop primitive proposition  sometimes called primitive event type   take
grammar formally define terms timeline  formula  ama formula  state  k formula formula k states  k  ama formula ama formula
whose timelines k  ma timelines  often treat states proposition sets
true empty set ama formulas ma timeline sets  may treat formulas
sets statesit important note  however  formulas may contain duplicate states 
duplication significant  reason  treating timelines sets 
formally intend sets state index pairs  where index gives states position formula  
indicate explicitly avoid encumbering notation  implicit index must
remembered whenever handling duplicate states 
semantics ama formulas defined terms temporal models  temporal model
  hm  set prop propositions pair mapping natural numbers
 representing time  truth assignments prop  closed natural number interval  
note siskind        gives continuous time semantics event logic models
   stands meets and  timeline meet sequence conjunctively restricted intervals 

   

fif ern   g ivan     iskind

defined terms real valued time intervals  temporal models defined use discrete
natural number time indices  however  results still apply continuous time semantics   that semantics bounds number state changes continuous timeline countable number   important note natural numbers domain representing
time discretely  prescribed unit continuous time represented natural
number  instead  number represents arbitrarily long period continuous time
nothing changed  similarly  states timelines represent arbitrarily long periods time
conjunctive restriction given state holds  satisfiability relation ama
formulas given follows 




state satisfied model hm  iff  x  assigns p true every x   p



ama formula           n satisfied iff satisfied m 

  s 

timeline s    s            sn satisfied model hm   t  t   i iff exists t  
 t  t    hm   t  t    i satisfies s  either hm   t     t   i hm   t        t   i satisfies
s            sn  

condition defining satisfaction timelines may appear unintuitive first due
fact two ways s            sn satisfied  reason becomes clear recalling using natural numbers represent continuous time intervals  intuitively 
continuous time perspective  timeline satisfied consecutive continuous time
intervals satisfying sequence consecutive states timeline  transition
consecutive states si si   occur either within interval constant truth assignment  that
happens satisfy states  exactly boundary two time intervals constant truth
value  definition  cases correspond s            sn satisfied time
intervals  t     t     t        t    respectively 
satisfies say model covers m  say ama  
subsumes ama   iff every model   model     written       say  
properly subsumes     written                 alternatively  may state
    saying   general  or less specific      covers     siskind
       provides method determine whether given model satisfies given ama formula 
finally  useful associate distinguished timeline model  projection
model   hm   i  j  i written map m  timeline s    s            sj state sk
gives true propositions  i   k     k j i  intuitively  projection gives
sequence propositional truth assignments beginning end model  later
show projection model viewed representing model precise
sense 
following two examples illustrate basic behaviors ama formulas 
example    stretchability   s    s    s    s    s    s            s    s    s    s    s    s    s    s    s 
equivalent timelines  general  timelines property duplicating state
results formula equivalent original formula  recall that  given model hm  i 
view truth assignment  x  representing continuous time interval  interval
conceptually divided arbitrary number subintervals  thus state satisfied
hm   x  x i  state sequence              
   

fil earning emporal e vents

example    infinite descending chains   given propositions b   timeline  
subsumed formulas a  b   a  b   a  b   a  b   a  b   a  b          
intuitively clear semantics viewed continuous time perspective  interval
b true broken arbitrary number subintervals
b hold  example illustrates infinite descending chains ama
formulas entire chain subsumes given formula  but member equivalent given
formula   general  ama formula involving propositions b subsume  

 a   b  

    motivation ama
timelines natural way capture stretchable sequences state constraints 
consider conjunction sequences  i e   ama  several reasons language enrichment  first all  show ama least general generalization  lgg 
uniquethis true ma  second  informally  argue parallel conjunctive constraints important learning efficiency  particular  space formulas
length k grows size exponentially k   making difficult induce long formulas 
however  finding several shorter timelines characterize part long sequence
changes exponentially easier   at least  space search exponentially smaller   ama
conjunction timelines places shorter constraints simultaneously often captures
great deal concept structure  reason  analyze ama well and 
empirical work  consider k  ama 
ama language propositional  intended applications relational  first order 
including visual event recognition  later paper  show propositional ama learning algorithms develop effectively applied relational domains  approach
first order learning distinctive automatically constructing object correspondence across examples  cf  lavrac  dzeroski    grobelnik        roth   yih         similarly  though ama
allow negative state constraints  section     discuss extend results
incorporate negation learning algorithms  crucial visual event recognition 
    conversion first order clauses
note ama formulas translated various ways first order clauses 
straightforward  however  use existing clausal generalization techniques learning 
particular  capture ama semantics clauses  appears necessary define subsumption
generalization relative background theory restricts us continuous time first order
model space 
example  consider ama formulas       b     a  b b
propositionsfrom example   know       now  consider straightforward clausal
translation formulas giving c    a i     b  i   c    a i      b  i      eets  i    i     
  pan  i    i     ij variables represent time intervals  eets indicates
two time intervals meet other  pan function returns time interval equal
union two time interval arguments  meaning intend capture satisfying
assignments c  c  indicate intervals     satisfied  respectively 
clear that  contrary want  c    c   i e    j  c    c     since easy
find unintended first order models satisfy c    c    thus translation 
similar translations  capture continuous time nature ama semantics 
   

fif ern   g ivan     iskind

order capture ama semantics clausal setting  one might define first order theory
restricts us continuous time modelsfor example  allowing derivation property b
holds interval  property holds sub intervals  given theory  
j  c    c    desired  however  well known least general generalizations relative background theories need exist  plotkin         prior work clausal
generalization simply subsume results ama language 
note particular training set  may possible compile continuous time background theory finite adequate set ground facts  relative ground theories 
clausal lggs known always exist thus could used application  however 
compiling approaches look promising us require exploiting analysis similar one given paperi e   understanding ama generalization subsumption
problem separately clausal generalization exploiting understanding compiling
background theory  pursued compilations further 
even given compilation procedure  problems using existing clausal generalization techniques learning ama formulas  clausal translations
ama found  resulting generalizations typically fall outside  clausal translations
formulas the  ama language  language bias ama lost  preliminary empirical work video event recognition domain using clausal inductive logic programming  ilp 
systems  found learner appeared lack necessary language bias find effective
event definitions  believe would possible find ways build language bias
ilp systems  chose instead define learn within desired language bias directly 
defining class ama formulas  studying generalization operation class 
    basic concepts properties ama
use following convention naming results  propositions theorems key
results work  theorems results technical difficulty  lemmas
technical results needed later proofs propositions theorems  number
results one sequence  regardless type  proofs theorems propositions provided
main textomitted proofs lemmas provided appendix 
give pseudo code methods non deterministic style  non deterministic language functions return one value non deterministically  either contain
non deterministic choice points  call non deterministic functions  since nondeterministic function return one possible value  depending choices made
choice points encountered  specifying function natural way specify richly structured set  if function arguments  relation  if function arguments   actually
enumerate values set  or relation  arguments provided  one simply use
standard backtracking search different possible computations corresponding different
choices choice points 
      ubsumption



g eneralization



tates

basic formulas deal states  conjunctions propositions   propositional
setting computing subsumption generalization state level straightforward  state s 
subsumes s   s  s    iff s  subset s    viewing states sets propositions  this 
derive intersection states least general subsumer states union
states likewise general subsumee 
   

fil earning emporal e vents

      nterdigitations
given set timelines  need consider different ways model could simultaneously satisfy timelines set  start model  i e   first time point  
initial state timeline must satisfied  time point model  one
timelines transition second state timelines must satisfied place
initial state  initial state timelines remains satisfied  sequence
transitions subsets timelines  final state timeline holds  way
choosing transition sequence constitutes different interdigitation timelines 
viewed differently  model simultaneously satisfying timelines induces co occurrence
relation tuples timeline states  one timeline  identifying tuples co occur
point model  represent concept formally set tuples co occurring states 
i e   co occurrence relation  sometimes think set tuples ordered sequence
transitions  intuitively  tuples interdigitation represent maximal time intervals
timeline transition  tuples giving co occurring states
time interval 
relation r x  xn simultaneously consistent orderings           n  if 
whenever r x            xn   r x             x n    either xi x i   i  x i xi   i  say
r piecewise total projection r onto component totali e   every state xi
appears r 
definition    interdigitation   interdigitation set f            n g timelines cooccurrence relation   n  viewing timelines sets states    piecewise total
simultaneously consistent state orderings   say two states  
s    j    j co occur iff tuple contains s    sometimes refer
sequence tuples  meaning sequence lexicographically ordered state orderings 
note exponentially many interdigitations even two timelines  relative
total number states timelines   example   page     shows interdigitation two
timelines  pseudo code non deterministically generating arbitrary interdigitation set
timelines found figure    given interdigitation timelines s    s            sm
t    t            tn  and possibly others   following basic properties interdigitations easily
verifiable 
     j   si tk co occur k  
  

  k  sj co occur tk

 

 

 s    t     sm   tn   

first use interdigitations syntactically characterize subsumption timelines 
definition    witnessing interdigitation   interdigitation two timelines    
witness     iff every pair co occurring states s      s       
s  subset s   i e   s  s    
following lemma proposition establish equivalence witnessing interdigitations
subsumption 
   recall  that  formally  timelines viewed sets state index pairs  rather sets states  ignore
distinction notation  readability purposes  treating timelines though state duplicated 

   

fif ern   g ivan     iskind

  

an interdigitation  f                n g 

   input  timelines             n
   output  interdigitation f            n g

  
  

s     hhead              head n  i 
  n  ji j    
return hs  i 
 
   ji j    g 
      a non empty subset of  t     

  
  
  
  
  

     n
    
 i    rest i  
else  i     

  
   
   
   

return extend tuple  s    an interdigitation  f              n g   

   

figure    pseudo code an interdigitation    non deterministically computes interdigitation set f            n g timelines  function head   returns first
state timeline   rest   returns first state removed  extend tuple x i  
extends tuple adding new first element x form longer tuple  a non emptysubset of s   non deterministically returns arbitrary non empty subset  
lemma    timeline model m  satisfies   witnessing
interdigitation map m   
proposition    timelines        

     

  iff interdigitation witnesses

proof  show backward direction induction number states n timeline    
n      existence witnessing interdigitation     implies every state  
subset single state     thus model   model        
now  suppose induction backward direction theorem holds whenever   n
fewer states  given arbitrary model n     state   interdigitation w
witnesses       must show model   conclude     desired 
write   s            sn     t            tm   witnessing interdigitation  w must identify
maximal prefix t            tm   made states co occur s  thus
subsets s    since   hm   t  t   i satisfies     definition must exist t      t  t   
hm   t  t    i satisfies s   and thus t            tm   hm    satisfies s            sn     equal
either  t     t     t        t     either case  straightforward construct  w   witnessing
interdigitation s            sn   tm              tm use induction hypothesis show
hm    must satisfy tm             tm   follows satisfies   desired 
forward direction  assume       let model    
map m   clear exists satisfies     follows satisfies    
lemma   implies witnessing interdigitation map m    thus
       
 

 

 

 

   

fil earning emporal e vents

      l east g eneral c overing f ormula
logic discriminate two models contains formula satisfies one other 
turns ama formulas discriminate two models exactly much richer internal positive event logic  ipel  formulas so  internal formulas define event occurrence
terms properties within defining interval  is  satisfaction hm  depends
proposition truth values given inside interval   positive formulas
contain negation  appendix gives full syntax semantics ipel  which used
state prove lemma      fact ama discriminate models well ipel
indicates restriction ama formulas retains substantial expressive power leads
following result serves least general covering formula  lgcf  component
specific to general learning procedure  formally  lgcf model within formula language
l  e g  ama ipel  formula l covers covering formula
l strictly less general  intuitively  lgcf model  unique  representative
formula model  analysis uses concept model embedding  say model
embeds model m  iff map m  map m    
lemma   

e

  ip el  model embeds model satisfies e   satisfies e  

proposition    projection model lgcf internal positive event logic  and
hence ama   semantic equivalence 
proof  consider model m  know map m  covers m  remains show
map m  least general formula so  semantic equivalence 
let e ipel formula covers m  let m  model covered map m 
want show e covers m    know  lemma    witnessing
interdigitation map m    map m   thus  proposition    map m    map m 
showing m  embeds m  combining facts lemma   follows e covers
m  hence map m  e    
proposition   tells us that  ipel  lgcf model exists  unique 
timeline  given property  ama formula covers timelines covered
another ama formula         thus  remainder paper  considering
subsumption formulas  abstract away temporal models deal instead
timelines  proposition   tells us compute lgcf model constructing
projection model  based definition projection  straightforward
derive lgcf algorithm runs time polynomial size model    note
projection may contain repeated states  practice  remove repeated states  since
change meaning resulting formula  as described example    
      c ombining nterdigitation



g eneralization



pecialization

interdigitations useful analyzing conjunctions disjunctions timelines 
conjoining set timelines  model conjunction induces interdigitation timelines
co occurring states simultaneously hold model point  viewing states
sets  states resulting unioning co occurring states must hold   constructing
   take size model   hm  sum x   number true propositions  x  

   

fif ern   g ivan     iskind

interdigitation taking union tuple co occurring states get sequence states 
get timeline forces conjunction timelines hold  call sequence
interdigitation specialization timelines  dually  interdigitation generalization involving
intersections states gives timeline holds whenever disjunction set timelines
holds 
definition    interdigitation generalization  specialization  set timelines
timeline s            sm   that  interdigitation tuples  sj intersection
 respectively  union  components jth tuple sequence   set interdigitation
generalizations  respectively  specializations  called ig    respectively  is    
example    suppose s    s    s    t    t    t  sets propositions  i e   states   consider timelines   s    s    s    t    t    t    relation

f hs   t    hs   t    hs   t    hs   t  g
interdigitation states s  s  co occur t    s  co occurs
t  t    corresponding ig members

s    t    s    t    s    t    s    t 
s    t    s    t    s    t    s    t 

  ig fs  g 
  is fs  g  

t  s    t  s    t  s    t  s    interdigitation witnesses

t 

timeline ig    dually  is    subsumes  is subsumed by  timeline
easily verified using proposition    complexity analyses  note number states
member ig   is   bounded number states
timelines bounded total number states timelines
  number interdigitations   thus members ig   is    exponential total number states  algorithms present later computing lggs
require computation ig    is    give pseudo code compute
quantities  figure   gives pseudo code function an ig member non deterministically
computes arbitrary member ig    an is member same  except replace intersection union   given set timelines compute ig   executing possible
deterministic computation paths function call an ig member    i e   computing set
results obtainable non deterministic function possible decisions non deterministic
choice points 
give useful lemma proposition concerning relationships conjunctions disjunctions concepts  the former ama concepts   convenience here 
use disjunction concepts  producing formulas outside ama obvious interpretation 
lemma    given formula subsumes member set formulas 
subsumes member   ig    dually  subsumed member  
subsumed member   is    case  length   bounded
size  

   

fil earning emporal e vents

an ig member  f                n g 

   input  timelines             n
   output  member ig f                n g 

return map  intersect tuple   an interdigitation  f            n g   
figure    pseudo code an ig member  non deterministically computes member
ig t   set timelines  function intersect tuple i   takes tuple
sets argument returns intersection  higher order function map f   
takes function f tuple arguments returns tuple length
obtained applying f element making tuple results 
proposition   

following hold 

    and to or  conjunction set timelines equals disjunction timelines
is   
    or to and  disjunction set timelines subsumed conjunction
timelines ig   
proof  prove or to and  recall that        ig       
w
v
immediate       ig     using dual argument  show
w
v
v
w
  is         remains vto show       isw     equivalent showing
timeline subsumed     subsumed   is     by proposition     consider
v
timeline    this implies member subsumes   lemma
w
  implies     is       get   is   
desired   
using and to or  reduce ama subsumption subsumption  exponential increase problem size 
proposition   
         

ama

 



     

      is        

proof  forward direction show contrapositive  assume     is     
      w        thus  timeline
       
w
tells us   is             thus   is           and to or get
       
backward direction assume     is                 
w
tells us     is           thus        is            

   subsumption generalization
section study subsumption generalization ama formulas  first  give
polynomial time algorithm deciding subsumption formulas show
deciding subsumption ama formulas conp complete  second give algorithms complexity bounds construction least general generalization  lgg  formulas based
   

fif ern   g ivan     iskind

ma subsumes         
   input      s            sm  
   output     

  t            tn

   path v    vm n sg         return true  example 
 a 
 b 

 c 

create array reachable i j   boolean values  false   
  j n 
     m  reachable i        true 
j      n  reachable    j      true 
    
j      n
reachable i  j       ti sj     reachable i
reachable i  j
reachable i

reachable m  n  return true 



   j    
    
   j     

   otherwise  return false 
figure    pseudo code subsumption algorithm 
defined main text 

sg         subsumption graph

analysis subsumption  including existence  uniqueness  lower upper bounds  algorithm
lgg ama formulas  third  introduce polynomial timecomputable syntactic notion
subsumption algorithm computes corresponding syntactic lgg exponentially faster semantic lgg algorithm  fourth  section      give detailed example
showing steps performed lgg algorithms compute semantic syntactic lggs
two ama formulas 
    subsumption
methods rely critically novel algorithm deciding subsumption question    
formulas     polynomial time  note merely searching possible
interdigitations     witnessing interdigitation provides obvious decision procedure
subsumption questionhowever  are  general  exponentially many interdigitations  reduce subsumption problem finding path graph pairs states
      polynomial time operation  pseudo code resulting subsumption algorithm shown figure    main data structure used subsumption algorithm
subsumption graph 
definition    subsumption graph two timelines     s      sm     t      tn
 written sg          directed
graph g   hv  e v   fvi j j   m    j ng  

 directed  edge set e equals hvi j   vi  j j si tj   si tj   i       j j   j      
 

 

 

 

achieve polynomial time bound one simply use polynomial time pathfinding algorithm  case special structure subsumption graph exploited determine
   

fil earning emporal e vents

desired path exists  mn  time  example method shown pseudo code illustrates 
following theorem asserts correctness algorithm assuming correct polynomial time
path finding method used 
lemma    given timelines     s            sm     t            tn   witnessing
interdigitation     iff path subsumption graph sg         v   
vm n  
theorem   
mial time 

given timelines       ma subsumes         decides  

  polyno 

proof  algorithm clearly runs polynomial time  lemma   tells us line   algorithm
return true iff witnessing interdigitation  combining proposition   shows
algorithm returns true iff        
given polynomial time algorithm subsumption  proposition   immediately suggests
exponential time algorithm deciding ama subsumptionby computing subsumption
exponentially many timelines one formula timelines formula 
next theorem suggests cannot better worst casewe argue
ama subsumption conp complete reduction boolean satisfiability  readers uninterested
technical details argument may skip directly section     
develop correspondence boolean satisfiability problems  include negation 
ama formulas  lack negation  imagine boolean variable two ama
propositions  one true one false  particular  given boolean satisfiability problem
n variables p            pn   take set propn set containing  n ama propositions
truek falsek k   n  represent truth assignment pi
variables ama state sa given follows 

sa   ftruei j   n  a pi     trueg   ffalsei j   n  a pi     falseg
proposition   suggests  checking ama subsumption critically involves exponentially
many interdigitation specializations timelines one ama formulas  proof 
design ama formula whose interdigitation specializations seen correspond truth
assignments  boolean variables  shown following lemma 
lemma    

given n  let conjunction timelines
n
 
i  

f propn  truei  falsei  propn    propn  falsei  truei  propn g 

following facts truth assignments boolean variables p            pn  
   truth assignment a  propn   sa   propn semantically equivalent member
is    
     is    truth assignment propn   sa   propn  
   truth assignment function mapping boolean variables true false 

   

fif ern   g ivan     iskind

lemma hand  tackle complexity ama subsumption 
theorem    

deciding ama subsumption conp complete 

proof  first show deciding ama subsumption     conp providing
polynomial length certificate answer  certificate non subsumption
interdigitation timelines   yields member is      subsumed    
certificate checked polynomial time  given interdigitation  corresponding
member is      computed time polynomial size     test
whether resulting timeline subsumed timeline   using polynomial time masubsumption algorithm  proposition   guarantees       iff timeline is     
subsumed every timeline     certificate exist exactly
answer subsumption query no 
show conp hardness reduce problem deciding satisfiability   sat formula
  c      cm problem recognizing non subsumption ama formulas  here 
ci  li     li     li     li j either proposition p chosen p   fp            pn g
negation  p  idea reduction construct ama formula view
exponentially many members is    representing truth assignments  construct
timeline view representing  s show satisfiable iff    
let defined lemma     let formula s            sm  

si  

ffalsej j li k   pj kg  
ftruej j li k    pj kg 

si thought asserting ci   start showing satisfiable
    assume satisfied via truth assignment awe know lemma   
    is    semantically equivalent propn   sa   propn   show
propn   sa   propn subsumed   conclude   using proposition    desired 
suppose contradiction propn   sa   propn subsumed state sa must
subsumed state si   consider corresponding clause ci   since satisfies
ci satisfied least one literals li k must true  assume li k   pj  a
dual argument holds li k    pj    si contains falsej sa contains truej
falsej thus  sa   si  since si   sa    contradicting choice i 
complete proof  assume unsatisfiable show   using
proposition    consider arbitrary   is   we show     lemma   
know truth assignment   propn   sa   propn   since unsatisfiable
know ci satisfied hence  ci satisfied a  implies
primitive proposition si sa   let w following interdigitation  
propn   sa   propn   s            sm  

fhpropn  s  hpropn  s  hpropn  sii hsa  sii hpropn  sii hpropn  si  i hpropn  smig

see tuple co occurring states given state subsumed
state   thus w witnessing interdigitation propn   sa   propn  
holds proposition  combining   propn   sa   propn get      
given hardness result later define weaker polynomial timecomputable subsumption
notion use learning algorithms 
   

fil earning emporal e vents

    least general generalization 
ama lgg set ama formulas ama formula general
formula set strictly general formula  existence
ama lgg nontrivial infinite chains increasingly specific formulas
generalize given formulas  example   demonstrated chains subsumee
extended ama subsumees  example  member chain p   q  p   q  p   q 
p   q  p   q  p   q        covers      p   q   q     p    p   q   despite complications 
ama lgg exist 
theorem     lgg finite set ama formulas subsumed
generalizations  
proof  let set   is       let conjunction timelines
generalize size larger   since finite number primitive
propositions  finite number timelines  well defined    show
least general generalization   first  note timeline generalizes thus
 by proposition     must generalize   now  consider arbitrary generalization    
proposition   implies   must generalize formula   lemma   implies
timeline   must subsume timeline longer size subsumes
timelines   must timeline   choice   every timeline
  subsumes timeline   follows   subsumes   lgg subsumed
lggs   desired   


 

given ama lgg exists unique show compute it  first step
strengthen or to and proposition   get lgg sublanguage 
theorem     set formulas  conjunction timelines ig   ama
lgg  
proof  let specified conjunction  since timeline ig   subsumes timelines
  subsumes member   show least general formula  consider
ama formula   subsumes members   since timeline   must subsume
members   lemma   implies timeline   subsumes member ig   thus
timeline   subsumes   implies      
characterize ama lgg using ig 
theorem    



ig    is     ama lgg set ama formulas 

proof  let   f             n g e         n   know ama lgg
must subsume e   would fail subsume one   using and to or represent
w
w
e disjunction timelines given e     is             is  n     ama
lgg must least general formula subsumes e i e   ama lgg set

timelines fis   j   g  theorem    tells us lgg timelines given

ig  fis   j   g    
   must least one timeline  timeline state true

   

fif ern   g ivan     iskind

  
  
  
  
  
  
  
  
  

   
   
   
   
   

   

semantic lgg f                 g 

   input  ama formulas            
   output  lgg f             g

   fg 
    
all values an is member     
             
     f     j    g 
    s       fg 
g    fg 
all values an ig member s   
      g        
g     f     g j    g 
g     g g      fg 
v

return  

g 

figure    pseudo code computing semantic ama lgg set ama formulas 
theorem    leads directly algorithm computing ama lggfigure   gives
pseudo code computation  lines     pseudo code correspond computation

fis   j   g  timelines included set subsumed timelines
already set  which checked polynomial time subsumption algorithm  
pruning  accomplished test line    often drastically reduces size timeline set perform subsequent ig computationthe final result affected
pruning since subsequent ig computation generalization step  remainder

pseudo code corresponds computation ig  fis   j   g  include
timelines final result subsume timeline set  pruning step  the test
line     sound since one timeline subsumes another  conjunction timelines
equivalent specific one  section       traces computations algorithm
example lgg calculation 
since sizes is   ig   exponential sizes inputs  code
figure   doubly exponential input size  conjecture cannot better this 
yet proven doubly exponential lower bound ama case  input
formulas timelines algorithm takes singly exponential time  since is fg   
ma  prove exponential lower bound input formulas ma  again 
readers uninterested technical details proof safely skip forward section     
argument  take available primitive propositions set fpi j j  
n    j ng  consider timelines


    s     s             sn 
    s     s             s n  
   



fil earning emporal e vents



si    pi       pi n
s j   p  j     pn j  

show ama lgg     must contain exponential number timelines 
particular  show ama lgg equivalent conjunction subset
ig f      g   certain timelines may omitted subset 
lemma     ama lgg set
timelines ig   j   j j j

timelines equivalent conjunction  

proof  lemma   implies timeline must subsume timeline     ig   
conjunction     must equivalent   since clearly covers covered
lgg   since   formed taking one timeline ig   timeline  
j   j j j    complete argument showing exponentially many
timelines ig f      g  cannot omitted conjunction remains lgg 
notice i  j si   s j   pi j   implies state ig f      g 
contains exactly one proposition  since state formed intersecting state  
    furthermore  definition interdigitation  applied here  implies following two facts
timeline q    q            qm ig f      g  
   q 

  p    qm   pn n 

   consecutive states qk
  i  j

  pi j qk     pi  j   i  either      j   either j j     
  j  
 

 

together facts imply timeline ig f      g  sequence propositions starting
p    ending pn n consecutive propositions pi j   pi  j different
i  equal     j   equal j j      call timeline ig f      g  square
pair consecutive propositions pi j pi  j either i    j     j  
following lemma implies square timeline omitted conjunction timelines
ig         remain lgg      
 

 

 

 

lemma     let     given let   ig f      g  
timelines subset omits square timeline       
v

  whose

n     hence exponenthe number square timelines ig f      g  equal  n       
n    
tial size       completed proof following result 

theorem    

smallest lgg two formulas exponentially large 

proof  lemma     ama lgg       equivalent conjunction
number timelines chosen ig f      g   however  lemma     conjunction
n     timelines  must     must exponentially
must least  n       
n    
large   
conjecture    

smallest lgg two ama formulas doubly exponentially large 
   

fif ern   g ivan     iskind

show lower bound ama lgg complexity merely consequence
existence large ama lggs  even small lgg  expensive compute
due difficulty testing ama subsumption 
theorem     determining whether formula ama lgg two given ama formulas  
  co np hard  co nexp  size three formulas together 
proof  show co np hardness use straightforward reduction ama subsumption  given
two ama formulas     decide     asking whether   ama lgg  
    clearly     iff   lgg two formulas 
show co nexp upper bound  note check exponential time whether  
  using proposition   polynomial time subsumption algorithm  remains
show check whether least subsumer  since theorem    shows
lgg     ig is        is        lgg   ig is        is       
thus  proposition    least subsumer  must timelines     is   
    ig is        is               use exponentially long certificates
answers  certificate pair interdigitation i  interdigitation i 
is        is       corresponding members     is        ig is        is      
        given pair certificates i  i      computed polynomial time 
  computed exponential time  subsumption checked
polynomial time  relative size  exponential   lgg
ig is        is        certificates exist   
    syntactic subsumption syntactic least general generalization 
given intractability results semantic ama subsumption  introduce tractable generality notion  syntactic subsumption  discuss corresponding lgg problem  use
syntactic forms generality efficiency familiar ilp  muggleton   de raedt       
where  example   subsumption often used place entailment generality relation 
unlike ama semantic subsumption  syntactic subsumption requires checking polynomially
many subsumptions  polynomial time  via theorem    
definition    ama   syntactically subsumed ama    written  
timeline         timeline            

syn    iff

proposition     ama syntactic subsumption decided polynomial time 
syntactic subsumption trivially implies semantic subsumptionhowever  converse
hold general  consider ama formulas  a  b      b   a   a  b   b
primitive propositions   a  b      b   a  a  b   a  however  neither a  b
a  b   b   a  b   a  a  b   syntactically subsume  a  b      b   a  
syntactic subsumption fails recognize constraints derived interaction
timelines within formula 
syntactic least general generalization  syntactic ama lgg syntactically least general
ama formula syntactically subsumes input ama formulas  here  least means
   

fil earning emporal e vents

formula properly syntactically subsumed syntactic lgg syntactically subsume input
formulas  based hardness gap syntactic semantic ama subsumption  one might
conjecture similar gap exists syntactic semantic lgg problems  proving
gap exists requires closing gap lower upper bounds ama lgg shown
theorem    favor upper bound  suggested conjecture     cannot yet
show hardness gap semantic syntactic lgg  give syntactic lgg algorithm
exponentially efficient best semantic lgg algorithm found  that
theorem      first  show syntactic lggs exist unique mutual syntactic
subsumption  and hence semantic equivalence  
theorem     exists syntactic lgg ama formula set syntactically subsumed syntactic generalizations  
proof  let conjunction timelines syntactically generalize
size larger   proof theorem     well defined  show
syntactic lgg   first  note syntactically generalizes timeline
generalizes timeline every member   choice   consider arbitrary
syntactic generalization     definition syntactic subsumption  timeline
  must subsume timeline member   lemma   implies
timeline   size larger subsumes subsumed  
choice   timeline   must timeline   follows   syntactically subsumes
  syntactic lgg subsumed syntactic generalizations    
general  know semantic syntactic lggs different  though clearly syntactic
lgg semantic generalization must subsume semantic lgg  example   a  b    
 b   a   a  b   semantic lgg a  b   a  discussed above  syntactic lgg
 a  b   true     true  b   a   subsumes a  b   subsumed a  b   a  even
so  formulas 
proposition    

ama   syn

equivalent  

proof  forward direction immediate since already know syntactic subsumption implies
semantic subsumption  reverse direction  note implies timeline
subsumes thus since single timeline timeline subsumes timeline
definition syntactic subsumption   
proposition    

syntactic ama lgg formula set semantic lgg  

proof  now  consider syntactic lgg   proposition    implies semantic
generalization   consider semantic lgg     show   conclude
semantic lgg   proposition    implies   syntactically subsumes   follows
    syntactically subsumes   but      syntactically subsumed   syntactic
lgg follows     syntactically subsumes   would least syntactic
generalization            implies     desired   
note stronger result stating formula syntactic lgg set formulas semantic lgg immediate consequence results above 
   

fif ern   g ivan     iskind

first examination  strengthening appears trivial  given equivalence syn
  however  semantically least necessarily stronger condition syntactically leastwe ruled possibility semantically least generalization may
syntactically subsume another generalization semantically  but syntactically  equivalent 
 this question open  found example phenomenon either  
proposition    together theorem    nice consequence learning approach
syntactic lgg two ama formulas semantic lgg formulas  long
original formulas syntactic lggs sets timelines  learning approach starts training examples converted timelines using lgcf operation 
syntactic lggs computed  whether combining training examples once  incrementally computing syntactic lggs parts training data  always syntactic lggs sets
timelines hence semantic lggs  spite fact syntactic subsumption
weaker semantic subsumption  note  however  resulting semantic lggs may
considerably larger smallest semantic lgg  which may syntactic lgg all  
using proposition     show cannot hope polynomial time syntactic lgg
algorithm 
theorem    

smallest syntactic lgg two formulas exponentially large 

proof  suppose always syntactic lgg two formulas exponentially large 
since proposition    formula semantic lgg  always semantic lgg
two formulas exponentially large  contradicts theorem      
discouraging  algorithm syntactic lgg whose time complexity
matches lower bound  unlike semantic lgg case  best algorithm
doubly exponential worst case  theorem    yields exponential time method computing
semantic lgg set timelines since timeline   is       simply
conjoin timelines ig    given set ama formulas  syntactic lgg algorithm uses
method compute polynomially many semantic lggs sets timelines  one chosen
input formula  conjoins results 
theorem    
            n  

formula

  ig f            n g  syntactic lgg ama formulas

v





proof  let   ig f            n g   timeline must subsume
output ig set containing timeline thus syntactically subsumes  
show syntactically least formula  consider   syntactically subsumes every
  show syn   conclude  timeline     subsumes timeline ti    
i  assumption syn     lemma      must subsume member
ig ft            tn g and member timeline timeline     subsumes
timeline   conclude syn     desired   
v

theorem yields algorithm computes syntactic ama lgg exponential time
pseudo code method given figure    exponential time bound follows fact
exponentially many ways choose             line   
exponentially many semantic lgg members line    since timelines the
product two exponentials still exponential 
   

fil earning emporal e vents

  
  
  
  
  
  

syntactic lgg f                 g 

   input  ama formulas f             g
   output  syntactic lgg f             g

g    fg 

h               

semantic lgg f            g 

  
  
  
   

v

return  

      g        
g     f     g j    g 
g     g g      fg 

g 

figure    pseudo code computes syntactic ama lgg set ama formulas 
formula returned algorithm shown actually subset syntactic lgg given
theorem     subset syntactically  and hence semantically  equivalent formula
specified theorem  possibly smaller due pruning achieved statement
lines     timeline pruned set  semantically  subsumed timeline
set  one timeline kept semantically equivalent group timelines  random  
pruning timelines sound  since timeline pruned output subsumes
formula outputthis fact allows easy argument pruned formula syntactically equivalent  i e  mutually syntactically subsumed by  unpruned formula  section      
traces computations algorithm example lgg calculation  note empirical evaluation discussed section    cost terms accuracy using
efficient syntactic vs  semantic lgg  know learned definitions made errors
direction overly specificthus  since semantic lgg least specific
syntactic lgg would advantage using semantic algorithm 
method exponential amount work even result small  typically
many timelines pruned output subsume remains   still
open question whether output efficient algorithm computing syntactic ama
lggthis problem conp conjecture conp complete  one route settling
question determine output complexity semantic lgg input formulas 
believe problem conp complete  proven this  problem p 
output efficient method computing syntactic ama lgg based theorem    
summary algorithmic complexity results section found table  
conclusions section paper 
    examples  least general generalization calculations
work details semantic syntactic lgg calculation  consider
ama formulas    a  b      b   a    a  b   a  semantic lgg a  b  
syntactic lgg  a  b   true     true  b   a  

   

fif ern   g ivan     iskind

      emantic lgg e xample
first step calculating semantic lgg  according algorithm given figure   
compute interdigitation specializations input formulas  i e   is   is      trivially 
is       a  b   a  calculate is     must consider possible interdigitations   three 

f ha  b   hb  b   hb  ai g
f ha  b   hb  ai g
f ha  b   ha  ai   hb  ai g
interdigitation leads corresponding member is    unioning  conjoining  states
tuple  is   

f  a   b    b    a   b   
 a   b   
 a   b    a   a   b   g 
lines    semantic lgg algorithm compute set   equal union
timelines is    is    subsumed timelines removed  formulas  see
timeline is    subsumed thus      a  b   a 
computing   algorithm returns conjunction timelines ig s    redundant
timelines removed  i e   subsuming timelines removed   case  ig s     a  b   a 
trivially  one timeline   thus algorithm correctly computes semantic lgg
a  b   a 
      yntactic lgg e xample
syntactic lgg algorithm  shown figure    computes series semantic lggs
timeline sets  returning conjunction results  after pruning   line   algorithm  cycles
timeline tuples cross product input ama formulas  case tuples
t    ha  b   a  a  b t    ha  b   a  b   aifor tuple  algorithm
computes semantic lgg tuples timelines 
semantic lgg computation tuple uses algorithm given figure   
argument always set timelines rather ama formulas  reason  lines  
  superfluous  timeline     is           case tuple t    lines   
algorithm compute   fa  b   a  a  b g  remains compute interdigitationgeneralizations  i e   ig s     returning conjunction timelines pruning  lines
     figure     set interdigitations are 

f ha  ai   hb  ai   hb  b   hb  ai g
f ha  ai   hb  b   hb  ai g
f ha  ai   ha  b   hb  b   hb  ai g
f ha  ai   ha  b   hb  ai g
f ha  ai   ha  b   ha  ai   hb  ai g
intersecting states interdigitation tuples get ig s   

f a  true  b   true  a  b   true  a  true  b   true  a  true  true  a  true  a  true g
   

fil earning emporal e vents

since timeline a  b   true subsumed timelines ig s    timelines
pruned  thus semantic lgg algorithm returns a  b   true semantic lgg timelines
t   
next syntactic lgg algorithm computes semantic lgg timelines t    following
steps t    find semantic lgg timelines t  true  b   a  since
a  b   true true  b   subsume one another  set g computed lines   
syntactic lgg algorithm equal f a  b   true  true  b   g  thus  algorithm computes
syntactic lgg  a  b   true     true  b   a   note that  case  syntactic
lgg general semantic lgg 

   practical extensions
implemented specific to general ama learning algorithm based lgcf syntactic lgg algorithms presented earlier  implementation includes four practical extensions 
first extension aims controlling exponential complexity limiting length
timelines consider  second describe often efficient lgg algorithm based
modified algorithm computing pairwise lggs  third extension deals applying
propositional algorithm relational data  necessary application domain visual event
recognition  fourth  add negation ama language show compute corresponding lgcfs lggs using algorithms ama  without negation   adding negation
ama turns crucial achieving good performance experiments  end
section review overall complexity implemented system 
    k ama least general generalization
already indicated syntactic ama lgg algorithm takes exponential time relative
lengths timelines ama input formulas  motivates restricting ama
language k  ama practice  formulas contain timelines k states 
k increased algorithm able output increasingly specific formulas cost
exponential increase computational time  visual eventrecognition experiments shown
later  increased k   resulting formulas became overly specific computational bottleneck reachedi e   application best values k practically computable
ability limit k provided useful language bias 
use k  cover operator order limit syntactic lgg algorithm k  ama  k  cover
ama formula syntactically least general k  ama formula syntactically subsumes
inputit easy show k  cover formula formed conjoining k  ma
timelines syntactically subsume formula  i e   subsume timeline formula   
figure    gives pseudo code computing k  cover ama formula  shown
algorithm correctly computes k  cover input ama formula  algorithm calculates
set least general k  ma timelines subsume timeline inputthe resulting k  ma
formulas conjoined redundant timelines pruned using subsumption test  note
k  cover ama formula may exponentially larger formula  however 
practice  found k  covers exhibit undue size growth 
given k  cover algorithm restrict learner k  ama follows     compute
k cover ama input formula     compute syntactic ama lgg resulting kama formulas     return k  cover resulting ama formula  primary bottleneck
   

fif ern   g ivan     iskind

  
  
  
  
  
  
  
  
  
   
   

   
   
   
   
   
   
   
   

v

k cover k   im  
v
   input  positive natural number k   ama formula  im
v
   output  k  cover  im
g    fg 
    

   hp            pn all values a k partition  k    


     p               pn   
      g        
g     f     g j    g 
g     g g      fg 
v
return   g 
p

a k partition  k  s            sj  

   input  positive natural number k   timeline s            sj
   output  tuple k sets consecutive states partitions s            sj

k return hfs g          fsj gi 
k     return hfs            sj gi 
l    a member of f              j k    g  
p    fs            sl g 
j

return extend tuple  p    a k partition  k

   pick next block size
   construct next block

   sl             sj    

figure     pseudo code non deterministically computing k cover ama formula  along
non deterministic helper function selecting k block partition states
timeline 

original syntactic lgg algorithm computing exponentially large set interdigitationgeneralizationsthe k  limited algorithm limits complexity computes interdigitationgeneralizations involving k  ma timelines 
    incremental pairwise lgg computation
implemented learner computes syntactic k ama lgg ama formula setshowever 
directly use algorithm describe above  rather compute lgg formula
sets via single call algorithm  typically efficient break computation
sequence pairwise lgg calculations  describe approach potential
efficiency gains 
straightforward show syntactic semantic subsumption
lgg                  lgg      lgg                 ama formulas  thus 
recursively applying transformation incrementally compute lgg ama formulas via sequence   pairwise lgg calculations  note since lgg operator
   

fil earning emporal e vents

commutative associative final result depend order process
formulas  refer incremental pairwise lgg strategy incremental approach
strategy makes single call k ama lgg algorithm  passing entire formula
set  direct approach 
simplify discussion consider computing lgg formula set
argument extended easily ama formulas  and hence k ama   recall syntactic
lgg algorithm figure   computes lgg   conjoining timelines ig   subsume others  eliminating subsuming timelines form pruning  incremental
approach applies pruning step pair input formulas processedin contrast 
direct approach must compute interdigitation generalization input formulas
pruning happen  resulting savings substantial  typically compensates
extra effort spent checking pruning  i e  testing subsumption timelines
incremental lgg computed   formal approach describing savings constructed


based observation  ig f     g  ig fg     lgg        ig fg   
seen compute lgg   f      g  latter possibly much cheaper
compute due pruning  is  lgg         typically contains much smaller number
timelines ig f      g  
based observations implemented system uses incremental approach
compute lgg formula set  describe optimization used system speedup
computation pairwise lggs  compared directly running algorithm figure    given
pair ama formulas               m               n   let syntactic
lgg obtained running algorithm figure    algorithm constructs computing
lggs timeline pairs  i e   lgg   i     j   j   conjoining results
removing subsuming timelines  turns often avoid computing many
lggs  see consider case exists j   i   j   know
lgg   i     j       j tells us   j considered inclusion  it may
pruned   furthermore know lgg involving   j subsume   j thus
pruned   shows need compute lggs involving   j   rather
need consider adding   j constructing  
observation leads modified algorithm  used system  computing
syntactic lgg pair ama formulas  new algorithm computes lggs
non subsuming timelines  given ama formulas       modified algorithm proceeds
follows     compute subsumer set   f     j        s t    g   f     j     
  s t    g     let ama           result removing timelines        
     let   syntactic lgg       computed running algorithm figure  
 if either  i empty   empty      let   conjunction timelines
subsume timeline        return           method avoids computing lggs
involving subsuming timelines  an exponential operation  cost performing polynomially
many subsumption tests  a polynomial operation   noticed significant advantage
using procedure experiments  particular  advantage tends grow process
training examples  due fact incrementally process training examples
resulting formulas become generalthus  general formulas likely
subsuming timelines  best case   syn    i e   timelines   subsuming   see step   produces empty formula thus step    the expensive step  performs
workin case return set     desired 
   

fif ern   g ivan     iskind

    relational data
l eonard produces relational models involve objects  force dynamic  relations
objects  thus event definitions include variables allow generalization objects 
example  definition p ick u p  x  y  z   recognizes p ick u p  hand  block  table  well
p ick u p  man  box  floor   despite fact k  ama learning algorithm propositional 
still able use learn relational definitions 
take straightforward object correspondence approach relational learning  view
models output l eonard containing relations applied constants  since  currently 
support supervised learning  set distinct training examples event type 
implicit correspondence objects filling role across different training models given type  example  models showing p ick u p  hand  block  table 
p ick u p  man  box  floor  implicit correspondences given hhand  mani  hblock  boxi 
htable  floori  outline two relational learning methods differ much objectcorrespondence information require part training data 
      c omplete bject c orrespondence
first approach assumes complete object correspondence given  input  along
training examples  given information  propositionalize training models
replacing corresponding objects unique constants  propositionalized models given
propositional k  ama learning algorithm returns propositional k  ama formula 
lift propositional formula replacing constant distinct variable  lavrac et al 
       taken similar approach 
      partial bject c orrespondence
approach assumes complete object correspondence information  sometimes
possible provide correspondences  for example  color coding objects fill identical
roles recording training movies   information always available 
partial object correspondence  or even none all  available  automatically complete
correspondence apply technique 
moment  assume evaluation function takes two relational models
candidate object correspondence  input  yields evaluation correspondence quality  given set training examples missing object correspondences  perform greedy
search best set object correspondence completions models  method works
storing set p propositionalized training examples  initially empty  set u unpropositionalized training examples  initially entire training set   first step  p empty 
evaluate pairs examples u   possible correspondences  select pair yields
highest score  remove examples involved pair u   propositionalize according best correspondence  add p   subsequent step  use previously
computed values pairs examples  one u one p   possible correspondences  select example u correspondence yields highest average
score relative models p example removed u   propositionalized according
winning correspondence  added p   fixed number objects  effort expended
polynomial size training set  however  number objects b appear
training example allowed grow  number correspondences must considered grows
   

fil earning emporal e vents

bb   reason  important events involved manipulate modest number
objects 
evaluation function based intuition object roles visual events  as well
events domains  often inferred considering changes initial
final moments event  specifically  given two models object correspondence 
first propositionalize models according correspondence  next  compute add
delete lists model  add list set propositions true final
moment initial moment  delete list set propositions true
initial moment final moment  add delete lists motivated strips action
representations  fikes   nilsson         given addi deletei lists models     
evaluation function returns sum cardinalities add    add  delete   
delete    heuristic measures similarity add delete lists two
models  intuition behind heuristic similar intuition behind strips actiondescription languagei e   differences initial final moments
event occurrence related target event  event effects described add
delete lists  found evaluation function works well visual event domain 
note  full object correspondences given learner  rather automatically
extracted learner   training examples interpreted specifying target event
took place well objects filled various event roles  e g   p ick u p  a b c    rather 
object correspondences provided training examples interpreted specifying
existence target event occurrence specify objects fill roles  i e   training
example labeled p ick u p rather p ick u p  a b c    accordingly  rules learned
correspondences provided allow us infer target event occurred
objects filled event roles  example object correspondences manually provided
learner might produce rule 
 

   s upports  z  y    c ontacts  z  y   
p ick u p  x  y  z    
 s upports  x  y    attached  x  y  

 

whereas learner automatically extracts correspondences would instead produce rule 
 

   s upports  z  y    c ontacts  z  y   
p ick u p  
 s upports  x  y    attached  x  y  

 

worth noting  however  upon producing second rule availability single training
example correspondence information allows learner determine roles variables 
upon output first rule  thus  assumption learner reliably
extract object correspondences  need label training examples correspondence information order obtain definitions explicitly recognize object roles 
    negative information
ama language allow negated propositions  negation  however  sometimes necessary adequately define event type  section  consider language ama  
superset ama  addition negated propositions  first give syntax semantics
ama   extend ama syntactic subsumption ama   next  describe approach
   

fif ern   g ivan     iskind

learning ama formulas using above presented algorithms ama  show approach correctly computes ama lgcf syntactic ama lgg  finally  discuss
alternative  related approach adding negation designed reduce overfitting appears
result full consideration negated propositions 
ama syntax ama  new grammar building states negated
propositions 
literal
state

    true j prop j   prop
    literal j literal   state

prop primitive proposition  semantics ama
state satisfaction 

ama except



positive literal p  negative literal
true  false   every x      

  p   satisfied model hm  iff  x  assigns p



state l      lm satisfied model hm  iff literal li satisfied hm  i 

subsumption  important difference ama ama proposition    establishing existence witnessing interdigitations subsumption  longer true  
words  two timelines         ama         need
interdigitation witnesses       see this  consider ama timelines 

     a   b   c   b  a  b   a   b     c 
    b  a  c  a  b  a    c  a  b
argue 
   interdigitation witnesses       see this  first show that 
witness  second fourth states    each b  must interdigitate align
either first fifth  fifth ninth states    also  b   either
cases  third state   interdigitate states   subsume it 
   even so  still       see this  consider model hm  satisfies    
must interval  i    i    within hm   i    i   i satisfies third state    
state a  two cases 
 a  proposition c true point hm   i    i   i  then  one verify hm 
satisfies     following alignment 

 
 

 
 

 a   b   c   b 
b 

a 
a  c  a 

b 
b 

 a   b     c 
a    c  a  b

    note important use notation   p rather  p   event logic  formula  p
satisfied model whenever p false instant model  rather  event logic interprets   p
indicating p never true model  as defined above   notice first form negation yield
liquid propertyi e    p true along interval necessarily subintervals  second form
negation  however  yield liquid property provided p liquid  important learning algorithms 
since assume states built liquid properties 

   

fil earning emporal e vents

 b  proposition c false everywhere hm   i    i   i  then  one verify hm 
satisfies     following alignment 

   
 a   b   c  
   
b  a  c  a 
follows      

b 
b 

a 
a    c  a 

b   a   b     c 
b

light examples  conjecture computationally hard compute ama
subsumption even timelines  reason  extend definition syntactic subsumption ama way provides clearly tractable subsumption test analogous
discussed ama 
definition    ama   syntactically subsumed ama    written   syn     iff
timeline         timeline       witnessing interdigitation
     
difference definition previous one ama need
test witnessing interdigitations timelines rather subsumption timelines 
ama formulas  note new old definition equivalent  due proposition    
however  ama new definition weaker  result general lgg formulas 
one might expect  ama syntactic subsumption implies semantic subsumption tested
polynomial time using subsumption graph described lemma   test witnesses 
learning  rather design new lgcf lgg algorithms directly handle ama  
instead compute functions indirectly applying algorithms ama transformed
problem  intuitively  adding new propositions models  i e   training examples  represent proposition negations  assume training example models
set propositions p   fp            pn g  introduce new set p   fp            pn g propositions
use construct new training models p   p assigning true pi time
model iff pi false model time  forming new set training models  each
twice many propositions original models  compute least general ama formula
covers new models  by computing ama lgcfs applying syntactic ama lgg
algorithm   resulting ama formula propositions p   p   finally replace pi
  pi resulting ama formula   propositions p turns
syntactic subsumption   least general ama formula covers original training
models 
show correctness transformational approach computing ama
lgcf syntactic lgg  first  introduce notation  let set models
p   let set models p   p   time  i  exactly one pi
pi true  let following mapping m  hm    m   hm  i 
unique hm       j   i     j   assigns pi true iff  j   assigns pi
true  notice inverse functional mapping m  approach handling
negation using purely ama algorithms begins applying original training models 
follows  consider ama formulas propositions p   ama formulas
propositions p   p  
let f mapping ama ama   ama   f     ama formula
identical except   pi replaced pi   notice inverse f func   

fif ern   g ivan     iskind

tion ama ama corresponds final step approach described above 
following lemma shows one to one correspondence satisfaction ama
formulas models satisfaction ama formulas models m 
lemma     model hm      ama  
 hm  i  

covers hm 

iff

f     covers

using lemma  straightforward show transformational approach computes
ama lgcf semantic subsumption  and hence syntactic subsumption  
proposition    



hm    m  let ama lgcf model  hm  i  
lgcf hm  i  equivalence 

f      unique ama

then 

proof  know covers  hm  i   therefore lemma    know f      covers
hm  i  show f     least general formula ama covers hm  i 
sake contradiction assume     ama covers hm      f      
follows model hm       covered f          lemma   
f      covers  hm  i  since unique ama lgcf  hm  i  
equivalence  f       however   hm       i  covered
f      gives contradiction  thus    exist  follows
ama lgcf  uniqueness ama lgcf equivalence follows ama
closed conjunction  two non equivalent lgcf formulas  could
conjoined get lgcf formula strictly less one them   
use fact f operator preserves syntactic subsumption  particular  given
two timelines         clear witnessing interdigitation     trivially
converted witness f      f       and vice versa   since syntactic subsumption defined
terms witnessing interdigitations  follows         ama       syn     iff
 f       syn f         using property  straightforward show compute syntactic
ama lgg using syntactic ama lgg algorithm 
proposition    

ama

formulas

             

let



               f    g  then  f      unique syntactic ama

syntactic ama lgg
lgg f             g 

proof  know i  f     syn thus  since f   preserves syntactic subsumption 
i  syn f        shows f       generalization inputs 
show f       least formula  sake contradiction assume
f       least  follows must     ama    syn f      
i  syn     combining fact f preserves syntactic subsumption  get
f        syn i  f     f        contradicts fact lgg 
must f       syntactic ama lgg  argued elsewhere  uniqueness
lgg follows fact ama closed conjunction   
propositions ensure correctness transformational approach computing
syntactic lgg within ama   case semantic subsumption  transformational approach
correctly compute ama lgg  see this  recall given two timelines         ama         witnessing interdigitation  clearly
   

fil earning emporal e vents

semantic subsumption  ama lgg         however  semantic ama lgg
f      f      f       reason since witness f      f     
 and f  i   timelines   know proposition   f        f       thus  f     
cannot returned ama lgg  since subsume input formulasthis shows
transformational approach return     f    f        here  transformational
approach produce ama formula general    
computational side  note that  since transformational approach doubles number propositions training data  algorithms specifically designed ama may
efficient  algorithms might leverage special structure transformed examples
ama algorithms ignorein particular  exactly one pi pi true time 
boundary negation  experiments  actually compare two methods assigning truth
values pi propositions training data models  first method  called full negation 
assigns truth values described above  yielding syntactically least general ama formula
covers examples  found  however  using full negation often results learning overly
specific formulas  help alleviate problem  second method places bias use
negation  choice bias inspired idea that  often  much useful information
characterizing event type pre  post conditions  second method  called boundary
negation  differs full negation allows pi true initial final moments
model  and pi false   pi must false times  is  allow
informative negative information beginnings ends training examples 
found boundary negation provides good trade off negation  i e   ama  
often produces overly general results  full negation  i e   ama    often produces overly
specific much complicated results 
    overall complexity scalability
review overall complexity visual event learning component discuss
scalability issues  given training set temporal models  i e   set movies   system
following     propositionalize training models  translating negation descried section     
   compute lgcf propositional model     compute k  ama lgg lgcfs 
   return lifted  variablized  version lgg  steps two four require little computational
overhead  linear sizes input output respectively  steps one three
computational bottlenecks systemthey encompass inherent exponential complexity
arising relational temporal problem structure 
step one  recall section       system allows user annotate training examples object correspondence information  technique propositionalizing models
shown exponential number unannotated objects training example  thus 
system requires number objects relatively small correspondence information
given small number objects  often event class definitions interested
involve large number objects  true  controlled learning setting
manage relational complexity generating training examples small number  or
zero  irrelevant objects  case domains studied empirically paper 
less controlled setting  number unannotated objects may prohibit use
correspondence techniquethere least three ways one might proceed  first  try
   

fif ern   g ivan     iskind

develop efficient domain specific techniques filtering objects finding correspondences 
is  particular problem may possible construct simple filter removes irrelevant
objects consideration find correspondences remaining objects  second 
provide learning algorithm set hand coded first order formulas  defining set
domain specific features  e g   spirit roth   yih         features used
propositionalize training instances  third  draw upon ideas relational learning
design truly first order version k  ama learning algorithm  example  one could use
existing first order generalization algorithms generalize relational state descriptions  effectively
approach pushes object correspondence problem k  ama learning algorithm rather
treating preprocessing step  since well known computing first order lggs
intractable  plotkin         practical generalization algorithms retain tractability constraining
lggs various ways  e g   muggleton   feng        morales        
step three  system uses ideas section     speedup k  ama lgg computation
set training data  nevertheless  computational complexity still exponential k thus 
practice restricted using relatively small values k   restriction limit
performance visual event experiments  expect limit direct applicability
system complex problems  particular  many event types interest may
adequately represented via k  ama k small  event types  however  often contain
significant hierarchical structurei e   decomposed set short sub event types 
interesting research direction consider using k  ama learner component
hierarchical learning systemthere could used learn k  ama sub event types  note
learner alone cannot applied hierarchically requires liquid primitive events 
learns non liquid composite event types  work required  and intended  construct
hierarchical learner based perhaps non liquid ama learning 
finally  recall compute lgg examples  system uses sequence  
pairwise lgg calculations  fixed k   pairwise calculation takes polynomial time  however  since size pairwise lgg grow least constant factor respect
inputs  worst case time complexity computing sequence   pairwise lggs exponential m  expect worst case primarily occur target event type
compact k  ama representationin case hierarchical approach described
appropriate  compact representation  empirical experience indicates
growth occurin particular  pairwise lgg tends yield significant pruning  problems  reasonable assumptions amount pruning   imply time
complexity computing sequence   pairwise lggs polynomial m 

   experiments
    data set
data set contains examples   different event types  pick up  put down  stack  unstack  move 
assemble  disassemble  involve hand two three blocks  detailed
description sample video sequences event types  see siskind         key frames
sample video sequences event types shown figure     results segmentation 
    particular  assume size pairwise k ama lgg usually bounded sizes k covers
inputs 

   

fil earning emporal e vents

tracking  model reconstruction overlaid video frames  recorded    movies
  event classes resulting total     movies comprising       frames   
replaced one assemble movie  assemble left qobi      duplicate copy another  assembleleft qobi     segmentation tracking errors 
event classes hierarchical occurrences events one class contain occurrences events one simpler classes  example  movie depicting
ove  a  b  c  d  event  i e  moves b c d  contains subintervals p ick u p  a  b  c 
p ut  a  b  d  events occur  experiments  learning definition event
class movies event class used training  train movies
event classes may depict occurrence event class learned subevent 
however  evaluating learned definitions  wish detect events correspond
entire movie well subevents correspond portions movie  example  given
movie depicting ove  a  b  c  d  event  wish detect ove a  b  c  d  event
p ick u p  a  b  c  p ut  a  b  d  subevents well  movie type data
set  set intended events subevents detected  definition
detect intended event  deem error false negative  definition detects unintended
event  deem error false positive  example  movie depicts ove a  b  c  d  event 
intended events ove a  b  c  d   p ick u p  a  b  c   p ut  a  b  c   definition
pick detects occurrence p ick u p  c  b  a  p ick u p  b  a  c   p ick u p  a  b  c  
charged two false positives well one false negative  evaluate definitions
terms false positive negative rates describe below 
    experimental procedure
event type  evaluate k  ama learning algorithm using leave one movie out crossvalidation technique training set sampling  parameters learning algorithm k
degree negative information used  value either p  positive propositions
only  bn  boundary negation  n  full negation  parameters evaluation procedure
include target event type e training set size n   given information  evaluation
proceeds follows  movie  the held out movie      movies  apply k ama learning algorithm randomly drawn training sample n movies    movies
event type e  or    movies one      use l eonard detect occurrences
learned event definition   based e event type   record number false
positives false negatives   detected l eonard   let fp fn total number
false positives false negatives observed     held out movies respectively  repeat
entire process calculating fp fn    times record averages fp fn   
since event types occur frequently data others simpler events
occur subevents complex events vice versa  report fp fn directly 
instead  normalize fp dividing total number times l eonard detected target
event correctly incorrectly within     movies normalize fn dividing total
    source code data used experiments available online appendix   
ftp   ftp ecn purdue edu qobi ama tar z 
    record times experiments  system fast enough give live demos n     
k     boundary negation  giving best results show  though dont typically record    training
videos live demo reasons   less favorable parameter settings  particularly k     full
negation  take  real time  hour so 

   

fif ern   g ivan     iskind

pick

put

stack

unstack

move

assemble

disassemble

figure     key frames sample videos   event types 

   

fil earning emporal e vents

number correct occurrences target event within     movies  i e   human assessment
number occurrences target event   normalized value fp estimates probability target event occur given predicted occur  normalized
value fn estimates probability event predicted occur given
occur 
    results
evaluate k  ama learning approach  ran leave one movie out experiments  described
above  varying k     n       example movies recorded color coded objects
provide complete object correspondence information  compared learned event definitions
performance two sets hand coded definitions  first set hd  hand coded definitions
appeared siskind         response subsequent deeper understanding behavior
l eonard model reconstruction methods  manually revised definitions yield another
set hd  hand coded definitions gives significantly better fn performance cost
fp performance  appendix c gives event definitions hd  hd  along set
machine generated definitions  produced k  ama learning algorithm  given training data
k        bn 
      bject c orrespondence
evaluate algorithm finding object correspondences  ignored correspondence information provided color coding applied algorithm training models event
type  algorithm selected correct correspondence     training models  thus 
data set  learning results correspondence information given identical
correspondences manually provided  except that  first case  rules
specify particular object roles  discussed section        since evaluation procedure uses
role information  rest experiments use manual correspondence information  provided
color coding  rather computing it 
correspondence technique perfect experiments  may suited
event types  furthermore  likely produce errors noise levels increase  since
correspondence errors represent form noise learner makes special provisions
handling noise  results likely poor errors common  example 
worst case  possible single extremely noisy example cause lgg trivial  i e  
formula true   cases  forced improve noise tolerance learner 
      varying k

first three rows table   show fp fn values   event types k   f       g  
n       the maximum     bn  similar trends found   p   n 
general trend that  k increases  fp decreases remains fn increases remains
same  trend consequence k  cover approach  because  k increases 
k  ama language contains strictly formulas  thus k    k    k   cover formula
never general k   cover  strongly suggests  prove  fp
non increasing k fn non decreasing k  
results show   ama overly general put assemble  i e  gives high
fp  contrast    ama achieves fp     event type  pays penalty fn compared
   

fif ern   g ivan     iskind

k
  bn

pick

put

stack

unstack

move

assemble

disassemble

fp
fn

 
 

    
    

 
    

 
    

 
 

    
 

 
 

 

bn

fp
fn

 
 

 
   

 
    

 
    

 
    

 
    

 
    

 

bn

fp
fn

 
 

 
   

 
    

 
    

 
    

 
    

 
    

 

p

fp
fn

    
 

   
    

 
    

    
    

 
    

 
    

 
    

 

bn

fp
fn

 
 

 
   

 
    

 
    

 
    

 
    

 
    

 

n

fp
fn

 
    

 
    

 
    

 
    

 
    

 
   

 
   

hd 

fp
fn

    
    

    
    

 
    

 
    

 
    

 
   

 
   

hd 

fp
fn

    
   

    
    

 
    

 
    

 
   

 
    

 
   

table    fp fn learned definitions  varying k   hand coded definitions 
  ama  since   ama achieves fp      likely advantage moving k  ama
k      is  expected result fn become larger  effect demonstrated
  ama table 
      varying

rows four six table   show fp fn   event types   fp  bn  ng  n      
k      similar trends observed values k   general trend that 
degree negative information increases  learned event definitions become specific 
words  fp decreases fn increases  makes sense since  negative information
added training models  specific structure found data exploited
k  ama formulas  see that    p  definitions pick put
overly general  produce high fp  alternatively    n  learned definitions
overly specific  giving fp      cost high fn  experiments  well others 
found   bn yields best worlds  fp     event types lower fn
achieved   n 
experiments shown demonstrated that  without negation pick put down 
increase k arbitrarily  attempt specialize learned definitions  never significantly reduce fp  indicates negative information plays particularly important role
constructing definitions event types 

   

fil earning emporal e vents

      c omparison



h  c oded efinitions

bottom two rows table   show results hd  hd    yet attempted
automatically select parameters learning  i e  k    rather  focus comparing
hand coded definitions parameter set judged best performing across event
types  believe  however  parameters could selected reliably using cross validation
techniques applied larger data set  case  parameters would selected perevent type basis would likely result even favorable comparison hand coded
definitions 
results show learned definitions significantly outperform hd  current data
set  hd  definitions found produce large number false negatives current
data set  notice that  although hd  produces significantly fewer false negatives event types 
produces false positives pick put down  hand definitions
utilize pick put macros defining events 
performance learned definitions competitive performance hd   
main differences performance are   a  pick put down  learned hd  definitions
achieve nearly fn learned definitions achieve fp     whereas hd  significant
fp   b  unstack disassemble  learned definitions perform moderately worse hd 
respect fn   c  learned definitions perform significantly better hd  assemble
events 
conjecture manual revision could improve hd  perform well  and perhaps better than  learned definitions every event class  nonetheless  view experiment
promising  demonstrates learning technique able compete with  sometimes
outperform  significant hand coding efforts one authors 
      varying n
practical interest know training set size affects algorithms performance 
application  important method work well fairly small data sets  tedious
collect event data  table   shows fn learning algorithm event type  n
reduced       experiments  used k       bn  note fp    
event types n hence shown  expect fn increase n decreased 
since  specific to general learning  data yields more general definitions  generally  fn
flat n       increases slowly      n       increases abruptly     n      
see that  several event types  fn decreases slowly  n increased       
indicates larger data set might yield improved results event types 
      p erspicuity



l earned efinitions

one motivation using logic based event representation support perspicuityin respect
results mixed  note perspicuity fuzzy subjective concept  realizing this 
say event definition perspicuous humans knowledge language
would find definition natural  here  assume human detailed knowledge model reconstruction process learner trying fit  adding assumption
would presumably make definitions qualify perspicuous  many complex features learned definitions appear fact due idiosyncrasies model reconstruction
process  sense  evaluating perspicuity output entire system 
   

fif ern   g ivan     iskind

learner itself  key route improving perspicuity sense would improve
intuitive properties model reconstruction output without change learner 
learned hand coded definitions similar respect accuracy  typically
learned definitions much less perspicuous  simplest event types  however  learned
definitions arguably perspicuous  look issue detail  appendix c gives
hand coded definitions hd  hd  along set machine generated definitions 
learned definitions correspond output k  ama learner run    training
movies event type k       bn  i e   best performing configuration
respect accuracy  
perspicuous definitions  p ick u p  x  y  z   p ut  x  y  z   definitions particular interest since short state sequences appear adequate representing event types
thus  hope perspicuous   ama definitions  fact  hand coded definitions involve short sequences  consider hand coded definitions p ick u p x  y  z  the definitions
roughly viewed   ma timelines form begin trans end    state begin asserts facts
indicate z held x end asserts facts indicate held
x z   state trans intended model fact l eonards model reconstruction
process always handle transition begin end smoothly  so definition
begin end work well   make similar observations p ut own x  y  z   
figure    gives learned   ama definitions p ick u p  x  y  z   p ut  x  y  z  
definitions contain six two   ma timelines respectively  since definitions consists
multiple parallel timelines  may first seem perspicuous  however  closer examination
reveals that  definition  single timeline arguably perspicuouswe
placed perspicuous timelines beginning definition  perspicuous timelines
natural begin trans end interpretation  fact  practically equivalent definitions
p ick u p  x  y  z   p ut  x  y  z   hd     
mind  notice hd  definitions overly general indicated significant
false positive rates  learned definitions  however  yield false positives without significant
increase false negatives  learned definitions improve upon hd  essentially specializing
hd  definitions  i e   perspicuous timelines  conjoining non perspicuous
timelines  non perspicuous timelines often intuitive  capture patterns
events help rule non events  example  learned definition p ick u p  x  y  z  
non perspicuous timelines indicate attached  y  z   true transition period
event  attachment relationship make intuitive sense  rather  represents
systematic error made model reconstruction process pick events 
summary  see learned definitions p ick u p  x  y  z   p ut  x  y  z  
contain perspicuous timeline one non perspicuous timelines  perspicuous timelines give intuitive definition events  whereas non perspicuous timelines capture nonintuitive aspects events model reconstruction process important practice 
note that  experienced users  primary difficulty hand coding definitions l eonard
    note event logic definition p ick u p x  y  z   hd  written compact form   ma 
definition converted   ma  and hence   ama   rather  hd  cannot translated exactly   ma
since uses disjunctionit disjunction two   ma timelines 
    primary difference hd  definitions contain negated propositions  learner considers
proposition negation proposition true point training movies  many negated
propositions hd  never appear positively  thus included learned definitions 

   

fil earning emporal e vents

determining non perspicuous properties must included  typically requires many
iterations trial error  automated technique relieve user task  alternatively 
could view system providing guidance task 
large definitions  tack  w  x  y  z   u nstack  w  x  y  z   events nearly identical
put pick respectively  difference picking
putting onto two block  rather single block  tower  i e   composed blocks z   
thus  might expect perspicuous   ama definitions  however  see
learned definitions tack  w  x  y  z   u nstack  w  x  y  z   figures       involve
many timelines p ick u p  w  x    p ut  w  x     accordingly 
definitions quite overwhelming much less perspicuous 
despite large number timelines  definitions general structure
pick put down  particular  contain distinguished perspicuous timeline 
placed beginning definition  conjoined many non perspicuous timelines 
clear that  above  perspicuous timelines natural begin trans end interpretation
and  again  similar definitions hd     case  however  definitions
hd  overly general  committing false positives   thus  inclusion
non perspicuous timelines detrimental effect since unnecessarily specialize definition
resulting false negatives 
suspect primary reason large number non perspicuous timelines relative
definitions pick put stems increased difficulty constructing
force dynamic models  inclusion two block tower examples causes modelreconstruction process produce unintended results  particularly transition periods
tack u nstack   result often many unintuitive physically incorrect patterns
involving three blocks hand produced transition period  learner
captures patterns roughly via non perspicuous timelines  likely generalizing
definitions including training examples would filter timelines  making
overall definition perspicuous  alternatively  interest consider pruning learned
definitions  straightforward way generate negative examples  these 
could remove timelines  generalizing definition  contribute toward rejecting
negative examples  unclear prune definitions without negative examples 
hierarchical events  ove w  x  y  z    ssemble  w  x  y  z    isassemble  w  x  y  z  
inherently hierarchical  composed four simpler event types  hand coded definitions leverage structure utilizing simpler definitions macros  light 
clear that  viewed non hierarchically   as learner does  events involve relatively
long state sequences  thus    ama adequate writing perspicuous definitions 
spite representational shortcoming  learned   ama definitions perform quite well 
performance supports one arguments using ama section      namely  given
easier find short rather long sequences  practical approach finding definitions long
events conjoin short sequences within events  examining timelines learned
  ama definitions reveals might expect  timeline captures often understandable
property long event sequence  conjunction timelines cannot considered
perspicuous definition  future direction utilize hierarchical learning techniques
improve perspicuity definitions maintaining accuracy 
   

fif ern   g ivan     iskind

n

pick

put

stack

unstack

move

assemble

disassemble

  
  
  
  
  
 

   
   
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

    
    
    
    
    
    

table    fn k

       bn  various values n  

note  however  that  level  learned definition ove  w  x  y  z   given figure    perspicuous  particular  first   ma timeline naturally interpreted giving
pre  post conditions move action  is  initially x supported hand w
empty finally x supported z hand w empty  thus  care preand post conditions  might consider timeline perspicuous  remaining timelines
definition capture pieces internal event structure facts indicating x moved
hand  weaker case made assemble disassemble  first timeline
learned definitions figures       interpreted giving pre  post conditions 
however  cases  pre post  conditions assemble disassemble  quite incomplete 
incompleteness due inclusion examples model reconstruction process
properly handle initial final  moments 

   related work
discuss two bodies related work  first  present previous work visual event recognition relates experiments here  second  discuss previous approaches learning
temporal patterns positive data 
    visual event recognition
system unique combines positive only learning temporal  relational 
force dynamic representation recognize events real video  prior work investigated various subsets features systembut  date  system combined pieces
together  incorporating one pieces system significant endeavor  respect  competing approaches directly compare system against  given this 
following representative list systems common features ours  meant
comprehensive focuses pointing primary differences systems ours  primary differences actually render systems loosely related
ours 
borchardt        presents representation temporal  relational  force dynamic event definitions definitions neither learned applied video  regier        presents techniques learning temporal event definitions learned definitions neither relational  force
dynamic  applied video  addition learning technique truly positive onlyrather 
extracts implicit negative examples event type positive examples event types 
   

fil earning emporal e vents

yamoto  ohya  ishii         brand essa         siskind morris         brand  oliver 
pentland         bobick ivanov        present techniques learning temporal event
definitions video learned definitions neither relational force dynamic  pinhanez
bobick        brand      a  present temporal  relational event definitions recognize
events video definitions neither learned force dynamic  brand      b  mann
jepson        present techniques analyzing force dynamics video neither formulate
event definitions apply techniques recognizing events learning event definitions 
    learning temporal patterns
divide body work three main categories  temporal data mining  inductive logic
programming  finite statemachine induction 
temporal data mining  sequence mining literature contains many general to specific  levelwise  algorithms finding frequent sequences  agrawal   srikant        mannila  toivonen 
  verkamo        kam   fu        cohen        hoppner         explore specific togeneral approach  previous work  researchers studied problem mining temporal
patterns using languages interpreted placing constraints partially totally ordered
sets time points  e g   sequential patterns  agrawal   srikant        episodes  mannila et al  
       languages place constraints time points rather time intervals work
here  recently work mining temporal patterns using interval based pattern
languages  kam   fu        cohen        hoppner        
though languages learning frameworks vary among approaches  share two
central features distinguish approach  first  typically goal
finding frequent patterns  formulas  within temporal data setour approach focused
finding patterns frequency one  covering positive examples   first learning
application visual event recognition yet required us find patterns frequency less
one  however  number ways extend method direction
becomes necessary  e g   deal noisy training data   second  approaches
use standard general to specific level wise search techniques  whereas chose take specificto general approach  one direction future work develop general to specific level wise
algorithm finding frequent formulas compare specific to general approach 
another direction design level wise version specific to general algorithmwhere
example  results obtained k  ama lgg used efficiently calculate
 k      ama lgg  whereas level wise approach conceptually straightforward general tospecific framework clear specific to general case  familiar
temporal data mining systems take specific to general approach 
first order learning section      pointed difficulties using existing first order
clausal generalization techniques learning ama formulas  spite difficulties  still
possible represent temporal events first order logic  either without capturing ama
semantics precisely  apply general purpose relational learning techniques  e g   inductive
logic programming  ilp   muggleton   de raedt         ilp systems require positive
negative training examples hence suitable current positive only framework 
exceptions include g olem  muggleton   feng         p rogol  muggleton         c lau dien  de raedt   dehaspe         among others  performed full evaluation
   

fif ern   g ivan     iskind

inputs

ama

subsumption
semantic
syntactic
p
p
conp complete p

semantic ama lgg
lower upper size
p
conp exp
conp nexp   exp 

syntactic ama lgg
lower upper size
p
conp exp
p
conp exp

table    complexity results summary  lgg complexities relative input plus output size 
size column reports worst case smallest correct output size    indicates
conjecture 
systems  early experiments visual event recognition domain confirmed belief
horn clauses  lacking special handling time  give poor inductive bias  particular  many
learned clauses find patterns simply make sense temporal perspective and 
turn  generalize poorly  believe reasonable alternative approach may incorporate
syntactic biases ilp systems done  example  cohen         dehaspe de raedt
        klingspor  morik  rieger         work  however  chose work directly
temporal logic representation 
finite state machines finally  note much theoretical empirical research
learning finite state machines  fsms   angluin        lang  pearlmutter    price        
view fsms describing properties strings  symbol sequences   case  however 
interested describing sequences propositional models rather sequences symbols 
suggests learning type factored fsm arcs labeled sets propositions
rather single symbols  factored fsms may natural direction extend
expressiveness current language  example allowing repetition  aware
work concerned learning factored fsms  however  likely inspiration drawn
symbol based fsm learning algorithms 

   conclusion
presented simple logic representing temporal events called ama shown
theoretical empirical results learning ama formulas  empirically  weve given first
system learning temporal  relational  force dynamic event definitions positive only input
applied system learn definitions real video input  resulting
performance matches event definitions hand coded substantial effort human
domain experts  theoretical side  table   summarizes upper lower bounds
shown subsumption generalization problems associated logic 
case  provided provably correct algorithm matching upper bound shown 
table shows worst case size smallest lgg could possibly take relative input
size  ama inputs  key results table polynomial time
subsumption ama syntactic subsumption  conp lower bound ama subsumption 
exponential size lggs worst case  apparently lower complexity syntactic ama
lgg versus semantic lgg  described build learner based results applied
visual event learning domain  date  however  definitions learn neither crossmodal perspicuous  performance learned definitions matches hand   

fil earning emporal e vents

coded ones  wish surpass hand coding  future  intend address cross modality
applying learning technique planning domain  believe addressing perspicuity
lead improved performance 

acknowledgments
authors wish thank anonymous reviewers helping improve paper  work
supported part nsf grants         iis         iis  nsf graduate fellowship
fern  center education research information assurance security
purdue university  part work performed siskind nec research institute 
inc 

appendix a  internal positive event logic
give syntax semantics event logic called internal positive event logic
 ipel   logic used main text motivate choice small subset
logic  ama  showing  proposition    ama define set models ipel
define 
event type  i e   set models  said internal whenever contains model
  hm  i  contains model agrees truth assignments  i     
full event logic allows definition non internal events  example  formula      p
satisfied hm  interval   entirely preceding p satisfied
hm    i  thus internal  applications considering appear require
non internal events  thus currently consider events internal 
call event type positive contains model   hm        i     truth
assignment assigning propositions value true  positive event type cannot require proposition false point time 
ipel fragment full propositional event logic describe positive internal
events  conjecture  yet proven  positive internal events representable
full event logic siskind        represented ipel formula  formally  syntax
ipel formulas given

e     true j prop j e    e  j  r e  j e   r e   
 

ei ipel formulas  prop primitive proposition  sometimes called primitive event
type   r subset thirteen allen interval relations fs f d b m o   si fi di bi ai oi g  allen 
       r  subset restricted set allen relations fs f d  g  semantics
allen relation given table    difference ipel syntax full propositional
event logic event logic allows negation operator  that  full event logic  r 
subset thirteen allen relations  operators     used define ama formulas
merely abbreviations ipel operators  f g  fmg respectively  ama subset
ipel  though distinguished subset indicated proposition    
thirteen allen interval relations binary relations set closed naturalnumber intervals  table   gives definitions relations  defining  m    m    r  n    n   
allen relation r   satisfiability ipel formulas defined follows 
   

fif ern   g ivan     iskind

i 
 m    m   
 m    m   
 m    m   
 m    m   
 m    m   
 m    m   
 m    m   

relation

f

b


 

i 
 n    n   
 n    n   
 n    n   
 n    n   
 n    n   
 n    n   
 n    n   

english
starts
finishes


meets
overlaps
equals

definition
m    n  m 
m  n  m 
m  n  m 

n 
  n 
n 

m  n 
m    n  m        n 
m  n  m  n 
m    n  m    n 

inverse
si

di
bi
mi
oi
 

table    thirteen allen relations  adapted semantics  

true satisfied every model 
prop satisfied model hm  iff  x  assigns prop true every x    
e    e  satisfied model iff satisfies e  satisfies e  
 re satisfied model hm  iff r   r interval     r
hm    satisfies e  
e   r e  satisfied model hm  iff r   r exist intervals i  i 
i  r i    pan  i    i      hm  i  satisfies e  hm  i  satisfies e   
prop primitive proposition  e ei ipel formulas  r set allen relations 
pan  i    i    minimal interval contains i  i    definition  easy
show  induction number operators connectives formula  ipel formulas
define internal events  one verify definition satisfiability given earlier ama
formulas corresponds one give here 

appendix b  omitted proofs
lemma    timeline model m  satisfies witnessing
interdigitation map m   
proof  assume   hm  satisfies timeline   s            sn   let    
map m   straightforward argue  induction length   exists mapping
v   states sub intervals  

  v    s    i  satisfies s 
v   s   includes initial time point  
v   sn  includes final time point  
      n     v   si   meets v   si     see table    
   

fil earning emporal e vents

let v relation states   members   true   v    s   note
conditions v   ensure every   every   appear tuple v  not
necessarily together   use v construct witnessing interdigitation w  
let r total  one to one  onto function time points corresponding states    
noting   one state time point       map hm  i   note r preserves
ordering that  j   r i  later r j       let w composition v r
relations v r 
show w interdigitation  first show state   appears
tuple w   w piecewise total  states must appear  trivially  appears
tuple v   r total  states   appear   appears tuple v   r
onto states    
suffices show states   w  s  s    w  t  t    implies
s  later t      w simultaneously consistent  conditions defining v  
imply every number   v  s  less equal every j   v  t   order preservation
property r  noted above  implies every state s    v r s  later state
t    v r t      desired  w interdigitation 
argue w witnesses     consider       w  s  t  
construction w   must   v    s  ith state     since     map m  
follows set true propositions  i   since   v    s   know  i  satisfies
s  follows t  s   

  ipel  model embeds model satisfies e satisfies e  
proof  consider models   hm  m    hm       embeds m    let
  map m      map m     assume e   ipel satisfied m    show
e satisfied m 
know definition embedding   thus witnessing interdigitation w   proposition    know one to one correspondence
numbers  i     states      denote state      corresponding    i       
lemma    e

si  ti    correspondence allows us naturally interpret w mapping v subsets
  subsets follows  i       v  i     equals set   i    i    
si co occurs ti w   use following properties v  
 

 

   i   sub interval     v  i     sub interval  

   i   sub interval     hm  v  i    i embeds hm     i   i 

   i   i   sub intervals     r allen relation  i   ri   iff v  i    rv  i     
   i   i   sub intervals     v  s pan  i     i        pan  v  i      v  i      

  

v  i        

sketch proofs properties     use induction length i    
definition interdigitation     since v  i     interval  map hm  v  i    i  well defined 
map hm  v  i    i  map hm     i   i  follows assumption embeds m      
appendix a  see allen relations defined terms relation natural
   

fif ern   g ivan     iskind

number endpoints intervals  show v preserves  but    singleton sets
 i e   every member v  fi  g  every member v  fj   g  i  j     v commutes set union  follows v preserves allen interval relations     use fact
v preserves sense argued  along fact pan  i     i     depends
minimum maximum numbers i   i        follows definition interdigitation
construction v  
use induction number operators connectives e prove that  m 
satisfies e   must m  base case e   prop  prop primitive proposition 
true  since m  satisfies e   know prop true    x    x        since w witnesses
    know that  prop true    x   prop true  x   x   v  x    
therefore  since v  i         prop true    x   x     hence m  satisfies e  
inductive case  assume claim holds ipel formulas fewer n operators connectiveslet e    e  two formulas  e   e    e    claim trivially
holds  e    r e    r must subset set relations fs f d  g  notice e
written disjunction  r e  formulas  r single allen relation r  thus 
suffices handle case r single allen relation  suppose e    fsg e    since m 
satisfies e   must sub interval i     i     hm     i   satisfies e    let
i    v  i      know properties v v  i         and  hence  i    furthermore  know hm  i  embeds hm     i   i  and  thus  inductive hypothesis  hm  i 
satisfies e    combining facts  get e satisfied m  similar arguments hold
remaining three allen relations  finally  consider case e   e   r e    r
set allen relations  again  suffices handle case r single allen relation
r  since m  satisfies e   e   r e    know sub intervals i   i    
pan  i     i           i   r i     hm     i   satisfies e    hm     i   satisfies e    facts 
properties v   easy verify satisfies e    
lemma    given formula subsumes member set formulas 
subsumes member   ig    dually  subsumed member  
subsumed member   is    case  length  
bounded size  
proof  prove result ig    proof is   follows similar lines  let

 

f            ng    s           sm  assume   n    proposition    i  witnessing interdigitation wi   combine wi

interdigitation   show corresponding member ig   subsumed
  construct interdigitation   first notice that  sj   wi specifies set
states  possibly single state least one  co occur sj   furthermore  since
wi interdigitation  easy show set states corresponds consecutive subsequence states let j i timeline corresponding subsequence 
let j   fj i j   ng  ffj interdigitation j   take union
ffj     j m  show interdigitation   since state appearing
must co occur least one state sj least one wi   least one tuple ffj  
and  hence  tuple piecewise total 
now  define restriction i j components j     j   relation given
taking set pairs formed shortening tuples omitting components except
   

fil earning emporal e vents

ith j th  likewise define ffi j
k k   show interdigitation  suffices
show i j simultaneously consistent  consider states si sj timelines
j   respectively  i j  si   sj    suppose ti occurs si i  tj   j  
i j  ti   tj   holds  suffices show sj later tj j   since i j  si   sj   i j  ti   tj   
i j
 
 
must ffi j
k  si   sj   ffk  ti   tj    respectively  k k   know k k
 
si ti wi simultaneously consistent  k   k   sj later tj j  
ffk must simultaneously consistent  interdigitation  otherwise  k   k     sj
later tj j   desired  wj simultaneously consistent  simultaneously
consistent  interdigitation  
let   member ig   corresponding   show     know
state s      intersection states tuple ffj say s  derives
ffj   consider interdigitation          sj   s     sj   s       
s  derives ffj     piecewise total  every tuple   derives ffj   ffj
empty    simultaneously consistent tuples   deriving later ffk must later
lexicographic ordering   given simultaneous consistency wk interdigitations used
construct ffj   finally  know sj subsumes  i e   subset of  state tuple
ffj   wk witnessing interdigitation k   and  hence  subsumes  is subset
of  intersection states  therefore  sj   co occurs s       
s  sj   thus    witnessing interdigitation     proposition      
size bound   follows  since  pointed main text  size member
ig   upper bounded number states    
 

lemma    given timelines     s            sm     t            tn   witnessing
interdigitation     iff path subsumption graph sg         v   
vm n  
proof  subsumption
graph sg         equal hv  e v   fvi j j   m    j ng

e   hvi j   vi  j j si tj   si tj   i       j j   j       note
correspondence vertices state tupleswith vertex vi j corresponding hsi   tj i 
forward direction  assume w witnessing interdigitation      
know that  states si tj co occur w   si tj since w witnesses      
vertices corresponding tuples w called co occurrence vertices  satisfy
first condition belonging edge e  that si tj    follows definition
interdigitation v    vm n co occurrence vertices  consider co occurrence
vertex vi j equal vm n   lexicographically least co occurrence vertex vi  j vi j
 ordering vertices
ordering
pair subscripts   show i  j   i    j   satisfy

requirements vi j   vi  j   e   not  either i        j     j      i        
co occurrence vertex vi   j   contradicting w piecewise total  j     j     
since w piecewise total  must co occurrence vertex vi  j      i    
i     i    contradicts simultaneous consistency w   i     i  contradicts
lexicographically least choice vi  j   follows every co occurrence vertex vm n
edge another co occurrence vertex closer manhattan distance vm n   thus
path v    vm n  
reverse direction assume path vertices sg         v    vm n
given by  vi   j    vi   j            vir  js i    j       ir   m  js   n  let w set state
 

 

 

 

 

 

 

  

  

 

 

   

 

fif ern   g ivan     iskind

tuples corresponding vertices along path  w must simultaneously consistent
orderings directed edges non decreasing orderings  w must
piecewise total edge cross one state transition either      
edge set definition  w interdigitation  finally  definition edge set e ensures
tuple hsi   tj w property si tj   w witnessing interdigitation
      showing       desired   
lemma     given n  let conjunction timelines
n
 
i  

f propn  truei  falsei  propn    propn  falsei  truei  propn g 

following facts truth assignments boolean variables p            pn  
   truth assignment a  propn   sa   propn semantically equivalent member
is    
     is    truth assignment propn   sa   propn  
proof  prove first part lemma  construct interdigitation
corresponding member is    equivalent propn   sa   propn   intuitively  construct
ensuring tuple consists states form truek falsek agree
truth assignmentthe union states tuple  taken is    equal sa   let
  ft    t    t    t    t  g interdigitation exactly five state tuples ti   assign
states timeline tuples follows 
   k     k




n a pk   true 

timeline s    s    s    s    q  ruek   f alsek   q  assign state si tuple ti  
assign state s  t  well 
timeline s     s     s     s     q  f alsek   ruek   q  assign state s i tuple ti    
state s   tuple t  well 

   k     k n a pk   false  assign states tuples item  
interchanging roles ruek f alsek  

clear piecewise total simultaneously consistent state orderings
  interdigitation  union states t    t    t    t  equal
propn   since propn included state tuples  furthermore  see
union states t  equal sa   thus  member is    corresponding equal
propn   propn   sa   propn   propn   semantically equivalent propn   sa   propn  
desired 
prove second part lemma  let member is     first argue
every state must contain either truek falsek   k n  k   since contains propn   truek   falsek   propn   every member is    must subsumed propn   truek  
falsek   propn   so  subsumed propn   truek   falsek   propn   every state propn  
truek   falsek   propn contains either truek falsek   implying   desired 
   

fil earning emporal e vents

next  claim   k n  either truek falsek i e   either states
include truek   states include falsek  and possibly both   prove claim  assume 
sake contradiction  that  k     truek   falsek   combining assumption first claim  see must states s  contains ruek
f alsek   s  contains f alsek ruek   respectively  consider interdigitation
corresponds member is     know s  equal union
states tuples     respectively      must include one state timeline
s    s    s    s    propn   truek   falsek   propn s     s     s     s     propn   falsek   truek   propn  
clearly  since include falsek   includes states s  s     likewise   includes
states s  s     follows simultaneously consistent state orderings
s    s    s    s  s     s     s     s     contradicting choice interdigitation  shows
either truek falsek  
define truth assignment   k n  a pk   truek  
since for k   truek falsek   follows state subsumed
sa   furthermore  since begins ends propn   easy give interdigitation
propn   sa   propn witnesses propn   sa   propn   thus 
propn   sa   propn    
lemma     let     given page      proof theorem     let  
v
ig f      g     whose timelines subset omits square
timeline       
proof  since timelines   subset timelines   know     remains
show       show constructing timeline covered      
let   s    s            s n   square timeline included     recall
si single proposition proposition set p   fpi j j   n    j ng  that 
consecutive states si si     si   pi j   si   either pi   j pi j      define new
timeline   s    s            s n   si    p si    show    so     
that    fg     so     
sake contradiction  assume must interdigitation w
witnessing   show induction that     w  si   sj   implies j   i 
base case       know s    s    since s    s    w  s    s    false  since
w witnesses subsumption  inductive case  assume claim holds i    i 
w  si   sj    know si   si   thus    j   w piecewise total  must
w  si     sj   j     and  induction hypothesis  must j        since w
simultaneously consistent sk sk state orderings      i  j   j  
follows j   desired  given claim  see s n   cannot co occur w
state   contradicting fact w piecewise total  thus    
let     s             s m timeline fg  construct interdigitation
witnesses     note assumed square    need be  let j smallest
index sj    s j since s    s     p             know j must exist 
range   j m  use index j guide construction interdigitation  let w
interdigitation     exactly following co occurring states  i e   state tuples  
 

 

     j

   si   co occurs s i  
   

fif ern   g ivan     iskind

m  sj co occurs s i 
j      n    si co occurs s m  

   j
  

easy check w piecewise total simultaneously consistent state
orderings   interdigitation  show w witnesses  
showing states subsumed states co occur w   co occurring
states si   s i corresponding first item s i   si implies s i
contained si     giving si   s i   consider co occurring states sj s i
second item above  since square  choose k l sj     pk l   sj either
pk   l pk l    addition  since sj     s j   s j either pk   l   pk l   pk   l  
sj    s j   cases  find state   s j equal sj follows
noting proposition indices never decrease across timeline        therefore
that  j   sj s i   finally  co occurring states si s m item three above 
si s m   since s m   pn n  states   thus  shown co occurring
states w   state subsumed co occurring state     therefore  w witnesses
    implies      
lemma     model hm      ama  
 hm  i  

covers hm 

iff

f     covers

proof  recall set models propositions set p   fp            pn g
assume ama uses primitive propositions p  possibly negated  
set propositions p   fp            pn g  assume formulas ama use propositions
p   p set models p   p   i  exactly one pi pi
true time  note f     ama  hm  i  m  prove lemma via
straightforward induction structure proving result literals  states 
timelines  finally ama formulas 
prove result literals  consider two cases  the third case true trivial   first 
single proposition pi       f  pi     pi   consider model hm    let
hm        hm  i   following relationships yield desired result 

covers hm 

iff
iff
iff

     i  assigns pi true
       i  assigns pi true
    pi covers  hm  i 

 by definition satisfiability 
 by definition  
 by definition satisfiability 

second case negated proposition   pi here  get     pi   let
hm    hm        hm  i   following relationships yield desired result 

covers hm 

iff
iff
iff

     i  assigns pi false
       i  assigns pi true
    pi covers  hm  i 

 by definition satisfiability 
 by definition  
 by definition satisfiability 

proves lemma literals 
    note
pk   l    

required square possible    equal
 

sj

   

sj

i e   could equal

fil earning emporal e vents

prove result states  use induction number k literals state  base
case k      the state single literal  proven above  assume lemma
holds states k fewer literals let   l      lk   hm    m 
inductive assumption know   l      lk covers hm  iff f    covers  hm  i  
base case know lk   covers hm  iff f  lk     covers  hm  i   facts
definition satisfiability states  get covers hm  iff f      f  lk     covers
 hm  i   clearly f property f      f  lk       f      showing lemma holds
states 
prove result timelines  use induction number k states timeline 
base case k      the timeline single state  proven above  assume
lemma holds timelines k fewer states  let   s            sk   hm   t  t   i  
hm      t  t   i    hm   t  t   i   following relationships 

covers hm   t  t   i

iff
iff
iff
iff

exists t      t  t     s  covers hm   t  t    i
  s            sk   covers either hm   t     t   i hm   t        t   i
exists t      t  t     f  s    covers hm      t  t    i
f    covers either hm      t     t   i hm      t        t   i
f  s     f    covers hm      t  t   i
f     covers hm      t  t   i

first iff follows definition satisfiability  second follows inductive
hypothesis  base case  fact  t  t     hm  i    hm     i  third
follows definition satisfiability  fourth follows fact f  s     f     
f     
finally  prove result ama formulas  induction number k timelines
formula  base case k      the formula single timeline  proven
above  assume lemma holds ama formulas k fewer timelines
let         k   hm    m  inductive assumption  know
          k covers hm  iff f       covers  hm  i   base case 
know k   covers hm  iff f  k     covers  hm  i   facts definition
satisfiability  get covers hm  iff f         f  k     covers  hm  i   clearly f
property f         f  k       f      showing lemma holds ama formulas 
completes proof   

appendix c  hand coded learned definitions used experiments
give two sets hand coded definitions  hd  hd    used experimental
evaluation  give set learned ama event definitions seven event types 
learned definitions correspond output k  ama learning algorithm  given available
training examples     examples per event type   k       bn  event definitions
written event logic    p denotes negation proposition p 

   

fif ern   g ivan     iskind

 

 

 

p ick u p  x  y  z  

 

p ut own x  y  z  

 

tack  w  x  y  z  

 

u nstack  w  x  y  z  

 

ove w  x  y  z  
ssemble w  x  y  z  
isassemble w  x  y  z  

 

 

 

 
 
 
 
 
 

  x       z   x     z   y 
c
b upported y       attached x  z   
b    
    c
c
b  
  attached x  y      s upports x  y  
 
 
c
b  
 
 
 
c
b  
 
 
upports  z    
 
 
 
c
b  
 
 
 
 
 
c
b  
 
 
 
 
upported x      attached y  z         
 
c
b  
 
 
 
 
c
b  
 
 
 
  s upports y  x      s upports y  z   
 
 
c
b  
 
 
 
b  
 
  s upports x  z       s upports z  x 
  c
c
b  
c
b
b     attached x      attached y  z     
    c
 
c
b  
attached x      upports x    
 
 
c
b  
 
 
 
 
 
c
b  
 
  s upports z  y  
 
 
 
 
c
b  
 
 
 
 
 
c
b  
 
 
 

upported
 
x
 
 
 
 

ttached
 
y 
z
 
 
 
 
 
 
c
b  
 
 
 
 
 
 

   
 
 

upports
 
y 
x
 
 
 
 

upports
 
y 
z
 
 
 
 
 
 
 
 
  s upports x  z       s upports z  x 
 
 
  x       z   x     z   y 
c
b upported y       attached x  z   
b    
    c
c
b  

ttached
 
x      upports  x    
 
 
c
b  
 
 
c
 
b  
 
 
 
 

upports
 
z 

 
 
 
 
c
 
b  
 
 
 
 
c
b  
 
 
 
 
 

upported
 
x
 
 
 
 

ttached
 
y 
z
 
 
 
 
 
c
b  
 
 
 
 
 
c
b  
 
 
 
 
 

upports
 
y 
x
 
 
 
 

upports
 
y 
z
 
 
 
 
c
b  
 
 
 
b  
 
 
 

upports
 
x 
z
 
 
 
 

upports
 
z 
x
 
  c
c
b  
c
b
b     attached x      attached y  z     
    c
 
c
b  

ttached
 
x         upports x    
 
 
 
 
  c
b  
   
 
c
b  
 
  upports  z    
 
 
    c
b  
 
 
c
b  
 
    s upported x      attached y  z       
 
 
    c
b  
 
 
 

   
    s upports y  x      s upports y  z       
 
 
 
 
 
 
  s upports x  z       s upports z  x 
 
 
  z   w     z   x     z   y 
  p ut own w  x      upports z      
 attached z  y 


  z   w     z   x     z   y 
p ick u p  w  x      upports z       attached z   
  y   z    p ick u p w  x  y   p ut own w  x  z   
p ut own w  y  z    f g tack  w  x  y  z  
u nstack w  x  y  z    f g p ick u p  x  y  z  
 

 

figure     hd  event logic definitions seven event types 

   

fil earning emporal e vents

 

 
  x       z   x     z   y 
b
c
 y      attached  x  z   
b upported
c
 
  c
b    
b  
c

ttached  x        s upports  x    
 
 
 
 
b  
c
   
 
 
 
b  
c
    upports  z      c ontacts  z    
 
 
 
b  
c
 
 
 
 
 
b  
c
      s upported  x      attached  y  z       f  mg  
 
b  
c
 
 
 
 
 
b  
c
 
      s upports  y  x      s upports  y  z     
 
 
b  
c
 
 
 
p ick u p  x  y  z     b  
  c
 
b
c

upports  x  z       s upports  z  x 
 
 
 
b
 
c
b  
c
 

ttached
 
x 

 
 

upports
 
x 

 
 
 
b  
c
 
   
 
 
b  
c
 
 
 
 

upports
 
z 

 
 
 
 
 
b  
c
 
   
 
 
b  
c
 
 
 
 
 

upported
 
x
 
 
 
 

ttached
 
y 
z
 
 
 
 
b  
c
   
 
 
 
b  
c
 
 
 
 
 
   

 
 

upports
 
y 
x
 
 
 
 

upports
 
y 
z
 
 
 
 
 
 
 
 
  s upports x  z      s upports z  x 
 
 
  x       z   x     z   y 
c
b
 y      attached  x  z   
c
b upported
 
  c
b    
c
b  

ttached  x      upports  x    
 
 
c
b  
 
   
 
 
c
b  
 
 

upports
 
z 

 
 
 
 
 
 
 
c
b  
 
   
 
 
c
b  
 
 
 
 

upported
 
x
 
 
 
 

ttached
 
y 
z
 
 
 
 
 
 
f
  

g
c
b  
 
 
 
  c
   
b  
 
 
 
 
 
 

upports
 
y 
x
 
 
 
 

upports
 
y 
z
 
 
  b 
c
 
 
 
p ut  x  y  z     b  
  c
 
c
b
 
 

upports  x  z       s upports  z  x 
 
c
b
 
c
b  
 
 
 attached  x  y      s upports x  y  
 
c
b  
 
 
 
 
 
c
b  
 
    upports  z      c ontacts  z    
 
 
c
b  
 
 
 
 
 
b  
c
 
      s upported  x      attached  y  z     
 
b  
c
 
 
 
 
 
b  
  c
 
 
    s upports  y  x      s upports  y  z     

   
 
 
 
 
 
 
  s upports x  z      s upports z  x 

figure     part hd  event logic definitions 

   

fif ern   g ivan     iskind

 

 

  w   x     y   w     y   x 
b   z   w     z   x     z    
c
b
c
b upported  x      attached w    
c
b    
  c
 
b  
c
attached w  x    upports  w  x  
 
b  
 
c
   
 
b  
 
 
c
 
 
upports y  x  
 
b  
 
 
 
 
c
   
 
b  
 
 
c
upports z      c ontacts z    
 
b  
 
 
 
 
c
   
 
b  
 
c
 
 
 
attached z    
 
f
mg  
b  
 
 
c
 
 
   
 
 
b  
c
 
 
 
upported w      attached x      
 
b  
 
c
 
 
   
 
b  
c
 
 
  s upports x  w      s upports x  y  
 
b  
 
 
  c
b  
c
b
c
    s upports w        s upports y  w 
 
b  
c
  attached w  x      s upports w  x  
 
b  
c
 
 
b  
c
 
 
 
 
upports y  x    c ontacts  y  x  
 
b  
c
 
 
 
 
 
b  
c
 
 
 
 
 
b  
c
  upports z      c ontacts z    
 
 
 
 
 
b  
c
 
 
 
 
b  
c
 
      attached z    
 
 
 
b  
c
 
 
 
 
b  
c
      s upported w      attached x      
 
 
 
 
   

 
 
  s upports x  w      s upports x  y  
 
 
 
 
 
 
  s upports w  y      s upports y  w 
 
 
  w   x     y   w     y   x 
c
b   z   w     z   x     z    
c
b
c
b upported x      attached w    
  c
b    
 
c
b  
  attached w  x      s upports w  x  
 
c
 
b  
 
c
 
 
b  
 
 
 
c
 
  upports  y  x    c ontacts  y  x  
b  
 
 
 
 
c
 
 
b  
 
 
c
 
b  
 
    upports  z      c ontacts  z    
 
 
c
 
 
b  
 
c
 
    attached z    
b  
   f mg  
 
 
 
c
 
 
b  
 
 
c
 
b  
      s upported w      attached x      
 
 
c
 
b  
 
 
  s upports x  w      s upports x  y  
 
c
 
b  
 
 
 
c
b
c
b
    s upports w        s upports y  w   
c
b  
attached w  x    upports w  x  
 
c
 
b  
 
 
 
c
 
 
b  
 
c
 
 
b  
      s upports y  x  
 
 
 
c
 
 
b  
 
  upports  z      c ontacts  z    
c
 
 
b  
 
 
 
 
c
 
 
b  
 
c
 
 
b  
      attached z    
 
 
 
c
 
 
b  
 
c
 
b  
      s upported w      attached x      
 
 
 
 

 
   
  s upports x  w      s upports x  y  
 
 
 
 
 
 
  s upports w  y      s upports y  w 
  y   z    p ick u p w  x  y   p ut own w  x  z   
p ut own w  y  z    f g tack  w  x  y  z  
u nstack  w  x  y  z    f g p ick u p  x  y  z  
  

tack w  x  y  z  

 

 

  

u nstack w  x  y  z  

ove w  x  y  z  
ssemble w  x  y  z  
isassemble w  x  y  z  

 

 

 
 
 
 
 
 

 

 

figure     part ii hd  event logic definitions 

   

fil earning emporal e vents

   
     
upported  y     upports  z    
 
 
 
 
 
b  
    c ontacts  y  z       upports x    
    
 
 
b  
 
 
 
b  
 
  attached x  y      attached y  z  
 
b  
b
upported y   
b    
    
 
b  
upported  y     upports  x    
 
 
 
b  
 
 
b  
 
 
 
attached x        upports z    
 
 
 
b  
 
 
b
  c ontacts y  z       attached y  z  
b  
b   upported y   
 
b  

 
 
b  
upported y     attached x    
b
 
 
b  
attached y  z  
 
b  
 
 
 
b
b    s upported y     attached x      
b    s upported y     c ontacts  y  z       
b
b
b    s upported y     attached y  z         
b
b     s upported y     attached x    
   
b  
upported  y     upports  z    
 
 
b  
 
    c ontacts  y  z       upports x    
b  
    
 
b  
b
 

ttached
 
x        attached y  z  
 
b  
 
 
b  
 

upported
 

 
 

upports
 
z 

  
 
 
 
 
b  
 
b    s upported y     attached x    
 
b  
b    s upported y     upports  z      
 
 
b  
 
 
   s upported y     attached x      
b  
b    
   
b

upported
 
    upports  x    
b  
 
   
   
 
    attached x        upports z    
 
 
 
 

 

 

 

 

p ick u p  x  y  z  

 

 

 

 

 

 

 

 

  c ontacts y  z       attached y  z  

p ut own x  y  z  

 

 

 
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c


   
     
upported y     upports x      attached x    
 
 
 
 
      upports z        c ontacts y  z   
 
b  
    
 
b  
 
 
 
b  
  attached y  z  
b
 
b   upported  y   
 
 
b  


 
 
 
b  
upported y     upports  z      c ontacts z    
 
b  
 
 
 
b  
 

upports
 
x       

ttached
 
x   
b  
 

b  
b
upported  y     attached x     
 
 
upported  y     attached x      attached y  z    
 
 

 
 

 

 

 

upported  y  

figure     learned   ama definitions p ick u p  x  y  z   p ut  x  y  z   

   

 
c
c
c
c
c
c
c
c
c
c
c
c


fif ern   g ivan     iskind

   
 
 
b  
b
b  
b  
b  
b  
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b  
b
b
b  
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b  
b
b
b  
b  
b
b  
b
b
b  
b
b  
b
b
b
b
b  
b  
b
b
b  
b
b  
b  
b  
b  
b
   
 
 

h

 

 

 

 

upported y   attached w  x  upports z    c ontacts y  z  
upports x   
upports y  x 
c ontacts x   
attached x   

  

    

    

    



 

 
 
 
 

 

c
c
   c
upported y     upported x    upports y  x    c ontacts x      c ontacts y  z   
 
  c
c
  s upports x  y      attached w  x      attached x  y       attached y  z 
c
c
 s upported y    attached w  x    
c
 s upported y    attached x  y    
 
c
c
 s upported y    upported x    upports y  x    c ontacts x  y  
 
c
c
 s upported y    attached w  x    
 s upported y    upports x  y    attached w  x    attached x  y    attached y  z       c
c
c
 s upported y    upported x s upports y  x  
 
c
c
 s upported y    attached w  x    
c
 s upported y    upported x    upports x  y    upports y  x    attached w  x      
c
c
 s upported y    upported x    upports y  x  
 
c
c
 s upported y    attached w  x    upports z  y    c ontacts y  z    
c
 s upported y    attached y  z    
 
c
c
 s upported y    upported x    upports y  x    c ontacts y  z    
c
c
 s upported y    attached w  x    upports z  y    c ontacts y  z    
c
 s upported y    attached w  x    attached y  z    
 
c
c
 hs upported y    upported x    upports y  x  
 
c
upported y     attached w  x    upports z      c ontacts y  z   
c
 
 
c
  s upports x  y      s upports y  x      c ontacts x  y      attached x  y 
c
 
c
 s upported y    attached w  x    
 
c
 s upported y    upported x    upports y  x  
c
 
c
 s upported y    attached w  x    
c
 s upported y    attached w  x    upports z  y    c ontacts y  z      
c
c
 s upported y    upported x  
c
 
c
 s upported y    attached w  x    
c
 s upported y    attached w  x    upports z  y    upported x      
c
c
 s upported y    upported x  
c
 
c
 hs upported y    attached w  x    
 
c
upported y     c ontacts y  z     upports z      upported x  
c
 
 
c
  s upports x  y      attached x  y 
 
c
c
 s upported y    upported x  
 
c
upported y   
c
h
 
c
upported y     c ontacts y  z     upports z      upported x  
 
 
c
  s upports x  y      attached x  y      attached y  z 
c
 
c
 s upported y    upported x    upports y  x    
c
c
 s upported y    attached w  x    
c
 s upported y    c ontacts y  z    upported x      
c
c
 s upported y    upported x    upported y x 
  c
 s upported y    attached w  x    
  c
c
 hs upported y    upported x    upports y  x    

 c
upported y     upported x    upports y  x    c ontacts x      c ontacts y  z   
  c
c
  s upports x  y      attached w  x      attached x  y      attached y  z 
  c

upported y   
  c
h

 
  c
upported y     upported x    upports y  x    upports z    
c
 
c
c
ontacts x      c ontacts y  z  
h
 
upported y     upported x    upports y  x    c ontacts x      c ontacts y  z   
 
 

 s upported y    
h

  s upports x  y      attached w  x      attached x  y      attached y  z 

figure     learned   ama definition tack  w  x  y  z   

   

fil earning emporal e vents

   
 
 
b  
 
b  
 
b  
 
b  
b
b  
b  
 
b  
b  
 
 
b  
b  
b
b  
b
b
b
b
b  
b
b
b
b
b
b  
b
b
b
b
b  
b
b
b
b
b  
b
b  
 
b  
 
b  
b  
b
b  
b  
 
b  
 
b  
b  
b
b  
 
b  
 
b  
b  
b
b  
b  
 
b  
 
b  
b
b  
b
b
b
b
b  
b
b  
 
b
b
b  
b  
b
b  
b
b
b
b
b  
b
b
b
b
b
b  
b  
b  
b
 
 
 

 

 

 

upported x    upported y     upports y  x  
 
 
 
 
c ontacts x      c ontacts y  z       s upports w  x  
 
 
 
 
  s upports x  y      attached w  x      attached x  y 
 
 
  s upported x    upported y    
 
 
upported x    upported y     attached w  x    upports z    
 
 
 
  c ontacts y  z     attached w  x      s upports x    
   
 
 
   
 
  s upports y  x      c ontacts x  y  
 
 
  attached x  y      attached y  z 
 
 s upported x    upported y    upports y  x    
 s upported x    upported y    attached w  x    attached y  z      
 s upported x    upported y    attached w  x    c ontacts y  z    
 s upported x    upported y    upports y  x    c ontacts y  z    
 s upported x    upported y    attached y  z    
 
 s upported x    upported y    attached w  x    c ontacts y  z    
 s upported x    upported y    upports y  x    c ontacts x  y    
 s upported x    upported y    upports y  x    attached x  y      
 s upported x    upported y    attached w  x    
 s upported x    upported y    upports y  x    
 s upported x    upported y    c ontacts y  z      
 s upported x    upported y    attached w  x  
 
 s upported x    upported y    upports y  x    
 
 
 
 
  s upported x    upported y    attached w  x    
   
 
upported x    upported y     attached w  x    upports z    
 
   
  c ontacts y  z     attached w  x      s upports x    
   
 
 
 
  s upports y  x      c ontacts x  y  
 
 
attached x        attached y  z  
 
 
 
   
upported x    upported y     upports y  x  
 
 
 
  c ontacts x      c ontacts y  z   
   
 
 
    
  s upports w  x      s upports x  y  
 
  attached w  x      attached x  y 
 
 
 
 
 s upported x    upported y    upports y  x    
 
 
 s upported x    upported y    attached w  x  
 s upported x    upported y    upports y  x    c ontacts y  z      
 s upported x    upported y    upports y  x    attached y  z      
 s upported x    upported y    attached w  x  
 
 s upported x    upported y    upports y  x    

 
 
upported x    upported y     upports y  x    attached y  z   
 
 
upports x      attached w  x    attached x   
 
 
 s upported x    upported y    attached w  x  
 
 s upported x    upported y    
 s upported x    upported y    upports y  x    attached w  x      
 s upported x    upported y    upports w  x    attached w  x    
 s upported x    upported y    upports y  x    
 s upported x    upported y    upports w  x    attached w  x      
 s upported x    upported y    attached w  x  
 
 s upported x    upported y    upports y  x    

 
 
upported x    upported y     c ontacts y  z   
 
  s upports x  y      attached x  y      attached y  z 
 
 
 s upported x    upported y  

figure     learned   ama definition u nstack  w  x  y  z   

   

 
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c


fif ern   g ivan     iskind

   
 
 
 
b  
 
b  
 
b  
 
b  
b  
b
b  
b  
 
b  
 
b  
 
b  
 
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
b
b  
b  
b  
b  
b  
b
 
 
 

 
 
 

upported  x    upports  y  x    c ontacts  y  x  
  s upports w  x      s upports z  x      c ontacts x  z  
  attached w  x      attached  y  x      attached  x  z 

 
 
 

upported  x  
upported  x    upports  z  x    c ontacts  x  z   
 
 
   s upports  w  x      s upports  y  x      c ontacts  y  x  
  attached w  x      attached  y  x      attached  x  z 
 s upported  x    upports  y  x      
 
 s upported  x    attached  w  x        
 
upported  x 
 
 
upported  x  
 
 s upported  x    attached  w  x    attached  x  z         
 
upported  x 
 
 
 s upported  x    
 
 s upported  x    attached  x  z         
 s upported  x    c ontacts  x  z     
 
 
upported  x  
 
 s upported  x    attached  w  x    upports  w  x        
 
upported  x 
 
 
upported  x  
 
 s upported  x    attached  w  x    attached  y  x        
 
upported  x 
 
 s upported  x    c ontacts  y  x      
 
 s upported  x    attached  y  x      
 
upported  x 
 

figure     learned   ama definition ove  w  x  y  z   

   

 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

 

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c


fil earning emporal e vents

   
 
 
 
b  
 
 
b  
b  
 
 
b  
b
b
b  
b  
 
b  
 
b  
 
b  
 
b  
 
b
b  
b  
b  
b  
 
b  
 
b
b
b  
b  
 
b  
 
b  
b  
b
b  
b  
b
b  
b  
b  
b
b  
b  
b
b  
b  
b  
b
b  
b  
b
b  
b  
b  
b
b  
b  
 
 
 

 

 

 

  s upported  x      s upports z  y      s upports y  x      
 
 
 
 
 
   c ontacts  x        c ontacts  z    
    
 
 
 
 
 
  attached w  x      attached  z  y 
 

true
 
 
 
 
 
 
 

 

upported  x    upported  y     upports  z    
 
upports  y  x    c ontacts  x    
 
c ontacts  z        attached  w   
  s upported  x      s upports z  y      s upports y  x  
  c ontacts x  y      c ontacts z  y  
  attached w  x      attached  z  y 

attached  w    
upported  y  

 
 
 

true 

 
 
 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 s upported  y      attached  w  x      attached  z  y        
 
upported  y  
 
 
true 
 
 s upported  y    attached  z  y        
 s upported  y    c ontacts  z  y    
true 
 s upported  y    upports  z  y c ontacts  z  y    attached  w  x    
upported  y  
 
 
true 
 
 s upported  y    attached  w  y attached  z  y      
 
upported  y  
figure     learned   ama definition ssemble  w  x  y  z   

   

 

 

 

 
 
 
 
 

 

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c


fif ern   g ivan     iskind

   
 
 
b  
 
b  
 
b  
 
b  
 
b  
b
b  
b  
b  
 
b  
 
b  
 
b  
 
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b  
b  
b
b  
b  
b  
b  
b  
b
b
b  
b
b  
b  
b
 
 

 

 

 

upported  x    upported y     upports y  x    upports  z    
 
 
   
 
  c ontacts  x      c ontacts z        upports w  x  
    
 
 
 
   
    upports w        upports x        attached x  w  
 
 
 
  attached w  y      attached x  y      attached z  y 
 
upported y   
 
 
 
 
 
 
upported  y       upported x      upports w  x  
 
 
 
 
    upports z        upports y  x      c ontacts x        
 
 
 
  c ontacts z  y      attached x  w      attached  z  y 
  upported  x    upported  y     
 

 
 
upported x    upported  y     upports  w  x  
 
 
upports z      c ontacts  z      attached x  w 
 
 
 
upported y  
 


upported x    upported  y     upports  z    
 
 
 
 
upports y  x    c ontacts  x      c ontacts z   
 
  upported  x    upported  y     upports  y  x    attached  x        
 
 
upported y  
 
  upported  x    upported  y     upports  y  x    c ontacts  z        
 


 
upported x    upported  y     upports  x    
 
 
upports y  z     attached x      attached z   
 
 
 
upported y  
 
  upported  x    upported  y     upports  y  x    
 
 

 
upported x    upported  y     upports  x    
 
 
upports y  z     attached x      attached z      attached x  w 
 
 
 
upported y  
 
upported y   
 
  upported  y     attached  w      attached  z      
 
 
upported y  
 
upported y   
 
  upported  y     upports  w      attached  w      
 
upported y  

 
 

 
 

 
 

 

 
 

 

 

 
 

 
 

figure     learned   ama definition isassemble  w  x  y  z   

   

 
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c


fil earning emporal e vents

references
agrawal  r     srikant  r          mining sequential patterns  proceedings eleventh
international conference data engineering  pp      
allen  j  f          maintaining knowledge temporal intervals  communications acm 
               
angluin  d          learning regular sets queries counterexamples  information
computation            
bacchus  f     kabanza  f          using temporal logics express search control knowledge
planning  artificial intelligence             
bobick  a  f     ivanov  y  a          action recognition using probabilistic parsing  proceedings ieee computer society conference computer vision pattern recognition 
pp          santa barbara  ca 
borchardt  g  c          event calculus  proceedings ninth international joint conference
artificial intelligence  pp          los angeles  ca 
brand  m       a   inverse hollywood problem  video scripts storyboards via
causal analysis  proceedings fourteenth national conference artificial intelligence  pp          providence  ri 
brand  m       b   physics based visual understanding  computer vision image understanding                
brand  m     essa  i          causal analysis visual gesture understanding  proceedings
aaai fall symposium computational models integrating language vision 
brand  m   oliver  n     pentland  a          coupled hidden markov models complex action
recognition  proceedings ieee computer society conference computer vision
pattern recognition 
cohen  p          fluent learning  elucidating structure episodes  proceedings
fourth symposium intelligent data analysis 
cohen  w          grammatically biased learning  learning logic programs using explicit antecedent description lanugage  artificial intelligence             
cohen  w     hirsh  h          learning classic description logic  theoretical experimental results  proceedings fourth international conference principles knowledge
representation reasoning  pp         
de raedt  l     dehaspe  l          clausal discovery  machine learning            
dehaspe  l     de raedt  l          dlab  declarative language bias formalism  proceedings
ninth international syposium methodologies intelligent systems  pp         
fikes  r     nilsson  n          strips  new approach application theorem proving
problem solving  artificial intelligence         
hoppner  f          discovery temporal patternslearning rules qualitative behaviour
time series  proceedings fifth european conference principles practice
knowledge discovery databases 
   

fif ern   g ivan     iskind

kam  p     fu  a          discovering temporal patterns interval based events  proceedings
second international conference data warehousing knowledge discovery 
klingspor  v   morik  k     rieger  a  d          learning concepts sensor data mobile
robot  artificial intelligence                  
lang  k   pearlmutter  b     price  r          results abbadingo one dfa learning competition new evidence driven state merging algorithm  proceedings fourth
international colloquium grammatical inference 
lavrac  n   dzeroski  s     grobelnik  m          learning nonrecursive definitions relations
linus  proceedings fifth european working session learning  pp     
    
mann  r     jepson  a  d          toward computational perception action  proceedings
ieee computer society conference computer vision pattern recognition  pp 
        santa barbara  ca 
mannila  h   toivonen  h     verkamo  a  i          discovery frequent episodes sequences 
proceedings first international conference knowledge discovery data mining 
mitchell  t          generalization search  artificial intelligence               
morales  e          pal  pattern based first order inductive system  machine learning         
    
muggleton  s          inverting entailment progol  machine intelligence             
muggleton  s     feng  c          efficient induction logic programs  muggleton  s   ed   
inductive logic programming  pp          academic press 
muggleton  s     de raedt  l          inductive logic programming  theory methods  journal
logic programming                
pinhanez  c     bobick  a          scripts machine understanding image sequences 
proceedings aaai fall symposium series computational models integrating
language vision 
plotkin  g  d          automatic methods inductive inference  ph d  thesis  edinburgh university 
regier  t  p          acquisition lexical semantics spatial terms  connectionist model
perceptual categorization  ph d  thesis  university california berkeley 
roth  d     yih  w          relational learning via propositional algorithms  information extraction case study  proeedings seventeenth international joint conference artificial
intelligence 
shoham  y          temporal logics ai  semantical ontological considerations  artificial
intelligence               
siskind  j  m          visual event classification via force dynamics  proceedings seventeenth national conference artificial intelligence  pp          austin  tx 
siskind  j  m          grounding lexical semantics verbs visual perception using force
dynamics event logic  journal artificial intelligence research           
   

fil earning emporal e vents

siskind  j  m     morris  q          maximum likelihood approach visual event classification  proceedings fourth european conference computer vision  pp         
cambridge  uk  springer verlag 
talmy  l          force dynamics language cognition  cognitive science            
yamoto  j   ohya  j     ishii  k          recognizing human action time sequential images using
hidden markov model  proceedings ieee conference computer vision
pattern recognition  pp         

   


