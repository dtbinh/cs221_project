journal artificial intelligence research                  

submitted       published      

unified model structural organization
language music
rens bod

rens illc uva  nl

institute logic  language computation
university amsterdam  nieuwe achtergracht    
     wv amsterdam  netherlands 
school computing  university leeds
ls   jt leeds  uk

abstract
general model predict perceived phrase structure language
music  usually assumed humans separate faculties language
music  work focuses commonalities rather differences
modalities  aiming finding deeper  faculty   key idea perceptual system
strives simplest structure  the  simplicity principle    biased
likelihood previous structures  the  likelihood principle    present series dataoriented parsing  dop  models combine two principles tested
penn treebank essen folksong collection  experiments show    
combination two principles outperforms use either them      exactly
model parameter setting achieves maximum accuracy language
music  argue results suggest interesting parallel linguistic
musical structuring 

   introduction  problem structural organization
widely accepted human cognitive system tends organize perceptual information
hierarchical descriptions conveniently represented tree structures  tree
structures used describe linguistic perception  e g  wundt        chomsky 
       musical perception  e g  longuet higgins        lerdahl   jackendoff       
visual perception  e g  palmer        marr         yet  little attention paid
commonalities different forms perception question whether
exists general  underlying mechanism governs perceptual organization  paper
studies exactly question  acknowledging differences perceptual
modalities  general model predict perceived tree structure sensory
input  studying question  use empirical methodology  model
might hypothesize tested manually analyzed benchmarks
linguistically annotated penn treebank  marcus et al        musically annotated
essen folksong collection  schaffrath         argue general model
structural organization language  music vision  carry experiments
linguistic musical benchmarks  since benchmark visual tree structures currently
available  best knowledge 
figure   gives three simple examples linguistic  musical visual information
corresponding tree structures printed  these examples resp  taken martin et al 
      lerdahl   jackendoff        dastani        

     ai access foundation morgan kaufmann publishers  rights reserved 

fibod

list sales products     

np
np
np
v

dt

pp
n

p

pp
n

p

n

list sales products     

figure    examples linguistic  musical visual input tree structures
thus  tree structure describes parts input combine constituents
constituents combine representation whole input  note linguistic
tree structure labeled syntactic categories  whereas musical visual tree
structures unlabeled  language syntactic constraints
words combined larger constituents  e g  english determiner combined
noun precedes noun  expressed rule np dt n  
music  and lesser extent vision  restrictions  principle note
may combined note 
apart differences  fundamental commonality  perceptual input
undergoes process hierarchical structuring found input itself  main
problem thus  derive perceived tree structure given input 
problem trivial may illustrated fact inputs assigned
following  alternative tree structures 


np
pp

np
v

dt

n

p

pp
n

p

n

list sales products     

figure    alternative tree structures inputs figure  
alternative structures possible perceived  linguistic tree
structure figure   corresponds meaning different tree figure   
two musical tree structures correspond different groupings motifs  two
visual structures correspond different visual gestalts  alternative tree
structures possible  plausible  correspond structures
actually perceived human cognitive system 
phenomenon input may assigned different structural organizations
known ambiguity problem  problem one hardest problems modeling
human perception  even language  phrase structure grammar may specify
words combined constituents  ambiguity problem notoriously hard  cf 
manning   schtze         charniak            argues many sentences wall
street journal one million different parse trees  ambiguity problem
   

fia unified model structural organization language music

musical input even harder  since virtually constraints notes may
combined constituents  talking rhythm perception music  longuet higgins
lee        note  any given sequence note values principle infinitely ambiguous 
ambiguity seldom apparent listener   
following section  discuss two principles traditionally proposed
solve ambiguity  likelihood principle simplicity principle  section   
argue new integration two principles within data oriented parsing framework 
hypothesis human cognitive system strives simplest structure generated
shortest derivation  biased frequency previously
perceived structures  section    go computational aspects model 
section    discuss linguistic musical test domains  section   presents empirical
investigation comparison model  finally  section    give discussion
approach go combinations simplicity likelihood proposed
literature 

   two principles  likelihood simplicity
predict set possible tree structures tree actually
perceived human cognitive system  field visual perception  two competing
principles traditionally proposed govern structural organization  first  initiated
helmholtz         advocates likelihood principle  perceptual input organized
probable structure  second  initiated wertheimer        developed
gestalt psychologists  advocates simplicity principle  perceptual system
viewed finding simplest rather probable structure  see chater       
overview   two principles used linguistic musical structuring 
following  briefly review principles modality 
    likelihood
likelihood principle particularly influential field natural language
processing  see manning schtze        review   field  appropriate
tree structure sentence assumed likely structure  likelihood tree
computed probabilities parts  e g  phrase structure rules  turn
estimated large manually analyzed language corpus  i e  treebank  state of the art
probabilistic parsers collins         charniak        bod      a  obtain around
    precision recall penn wall street journal treebank  marcus et al        
likelihood principle applied musical perception  e g  raphael       
bod      b c   probabilistic natural language processing  probable musical
tree structure computed probabilities rules fragments taken large
annotated musical corpus  musical benchmark used models
essen folksong collection  schaffrath        
vision science  huge interest probabilistic models  e g  hoffman       
kersten         mumford        even seen fit declare dawning stochasticity 
unfortunately  visual treebanks currently available 
    simplicity
simplicity principle long tradition field visual perception psychology  e g 
restle        leeuwenberg        simon        buffart et al        van der helm        
   

fibod

field  visual pattern formalized constituent structure means visual coding
language based primitive elements line segments angles  perception
described process selecting simplest structure corresponding  shortest
encoding  visual pattern 
notion simplicity applied music perception  collard et al         use
coding language leeuwenberg        predict metrical structure four preludes
bach s well tempered clavier  well known music perception theory
proposed lerdahl jackendoff         theory contains two kinds rules   wellformedness rules   preference rules   role well formedness rules define
kinds formal objects  grouping structures  theory employs  grouping structures
listener actually hears  described preference rules describe gestaltpreferences kind identified wertheimer         therefore seen
embodiment simplicity principle 
notions simplicity exist language processing  example  frazier       
viewed arguing parser prefers simplest structure containing minimal
attachments  bod      a  defines simplest tree structure sentence structure
generated smallest number subtrees given treebank 

   combining likelihood simplicity
key idea current paper principles play role perceptual organization 
albeit rather different ones  simplicity principle general cognitive preference
economy  likelihood principle probabilistic bias due previous perceptual
experiences  informally stated  working hypothesis human cognitive system
strives simplest structure generated shortest derivation 
biased frequency previously perceived structures  some combinations
simplicity likelihood discussed section     formally instantiate working
hypothesis  first need model defines set possible structures input 
paper  chosen model defines set phrase structures input
basis treebank previously analyzed input  known data oriented
parsing dop model  see bod        collins   duffy         dop learns grammar
extracting subtrees given treebank combines subtrees analyze fresh input 
chosen dop     uses subtrees arbitrary size  thereby capturing nonlocal dependencies      obtained competitive results various benchmarks
 bod      a b  collins   duffy         following  first review dop model
discuss use likelihood simplicity principles approach  next  show
two principles combined instantiate working hypothesis 
    data oriented parsing
section  illustrate dop model linguistic example  for rigorous definition
dop  reader referred bod         come back musical examples
section    suppose given following extremely small linguistic treebank two
trees resp  wanted dress rack saw dog telescope
 actual treebanks contain tens thousands trees  cf  marcus et al        

   

fia unified model structural organization language music





np

vp

v

np



wanted



pp
np

saw

np

dress p

vp
v

pp

np


vp

np

np

p

dog telescope

rack

figure    example treebank
dop model parse new sentence  e g  saw dress telescope 
combining subtrees treebank means substitution operation  indicated   







vp

np



np

pp
np

p

dress



 

telescope



vp



pp
np

v

vp

np

vp

pp
np

v
saw

saw

p

np

dress telescope

figure    parsing sentence combining subtrees figure  
thus substitution operation combines two subtrees substituting second subtree
leftmost nonlexical leaf node first subtree  the result may combined
third subtree  etc    combination subtrees results tree structure
whole sentence called derivation  since many different subtrees  various
sizes  typically many different derivations produce  however  tree 
instance 





np


vp

np


vp
v
saw

dress

p

vp

np


pp
np



 

np

vp
v

telescope

pp
np

p

np

saw dress telescope

figure    different derivation produces parse tree
   

fibod

interesting case occurs different derivations produce different
parse trees  happens sentence ambiguous  example  dop produces
following alternative parse tree saw dress telescope 




np

vp

v

v
saw


p



 

pp

np

np

telescope

np

vp

v

np

saw
np


pp

pp

np


dress

dress

p

np

telescope

figure    different derivation produces different parse tree
    likelihood dop
bod         dop enriched likelihood principle predict perceived tree
structure set possible structures  model  call likelihood dop 
computes probable tree input occurrence frequencies subtrees 
probability subtree t  p t   computed number occurrences t       divided
total number occurrences treebank subtrees root label t 
let r t  return root label t  may write 
p t   

 t 

t   r t    r  t 

  t   

probability derivation t    tn computed product probabilities
subtrees ti 
p t    tn    

p ti 

seen  may different derivations generate parse tree 
probability parse tree thus sum probabilities distinct derivations  let
tid i th subtree derivation produces tree t  probability given

p t   

p tid 

   

fia unified model structural organization language music

parsing sentence s  interested trees assigned s 
denote ts  best parse tree  tbest  according likelihood dop tree
maximizes probability ts 
tbest   arg max p ts 
ts

thus likelihood dop computes probability tree sum products 
product corresponds probability certain derivation generating tree 
distinguishes likelihood dop statistical parsing models identify exactly
one derivation parse tree thus compute probability tree one
product probabilities  e g  charniak        collins        eisner         likelihood dop s
probability model allows including counts subtrees wide range sizes  everything
counts single level rules counts entire trees 
note subtree probabilities likelihood dop directly estimated
relative frequencies treebank trees  relative frequency estimator obtains
competitive results several domains  bonnema et al        bod      a  de pauw        
maximize likelihood training data  johnson        
may hidden derivations relative frequency estimator cannot deal with   
estimation procedures take account hidden derivations maximize
likelihood training data  example  bod      b  presents likelihood dop model
estimates subtree probabilities maximum likelihood re estimation procedure
based expectation maximization algorithm  dempster et al         however  since
relative frequency estimator far outperformed estimator  see
bod et al      b   stick relative frequency estimator current paper 
    simplicity dop
likelihood dop justice preference humans display simplest
structure generated shortest derivation input  bod      a   simplest tree
structure input defined tree constructed smallest number
subtrees treebank  refer model simplicity dop  instead
producing probable parse tree input  simplicity dop thus produces parse
tree generated shortest derivation consisting fewest treebank subtrees 
independent probabilities subtrees  define length derivation d 
l d   number subtrees d  thus   t    tn l d    n  let derivation
results parse tree t  best parse tree  tbest  according simplicity dop
tree produced derivation minimal length 
tbest   arg min l d ts  
ts

section      ts parse tree sentence s  example  given treebank figure
   simplest parse tree saw dress telescope given figure    since
  subtrees restricted depth   relative frequency estimator coincide

maximum likelihood estimator  depth   dop model corresponds stochastic context free
grammar  well known dop models allow subtrees greater depth outperform depth  
dop models  bod        collins   duffy        

   

fibod

parse tree generated derivation two treebank subtrees 
parse tree figure    and parse tree  needs least three treebank subtrees
generated   
shortest derivation may unique  happen different parse trees
sentence generated minimal number treebank subtrees  also
probable parse tree may unique  never happens practice   case
back frequency ordering subtrees  is  subtrees root label
assigned rank according frequency treebank  frequent subtree  or
subtrees  root label gets rank    second frequent subtree gets rank    etc 
next  rank  shortest  derivation computed sum ranks
subtrees involved  derivation smallest sum  highest rank  taken final
best derivation producing final best parse tree simplicity dop  see bod      a  
performed one little adjustment rank subtree  adjustment averages
rank subtree ranks sub subtrees  is  instead simply taking
rank subtree  compute rank subtree  arithmetic  mean ranks
sub subtrees  including subtree itself   effect technique
redresses low ranked subtree contains high ranked sub subtrees 
simplicity dop likelihood dop obtain rather similar parse accuracy
wall street journal essen folksong collection  in terms precision recall    see
section     best trees predicted two models quite match  suggests
combined model  justice simplicity likelihood  may boost accuracy 
    combining likelihood dop simplicity dop  sl dop ls dop
underlying idea combining likelihood simplicity human perceptual system
searches simplest tree structure  generated shortest derivation 
biased likelihood tree structure  is  instead selecting simplest tree
per se  combined model selects simplest tree among n likeliest trees  n
free parameter  course ways combine simplicity likelihood
within dop framework  straightforward alternative would select
probable tree among n simplest trees  suggesting perceptual system
searching probable structure among simplest ones  refer
first combination simplicity likelihood  which selects simplest among n
likeliest trees  simplicity likelihood dop sl dop  second combination
 which selects likeliest among n simplest trees  likelihood simplicity dop lsdop  note n    simplicity likelihood dop equal likelihood dop  since
one probable tree select from  likelihood simplicity dop equal
simplicity dop  since one simplest tree select from  moreover  n gets large 
sl dop converges simplicity dop ls dop converges likelihood dop 
varying parameter n  able compare likelihood dop  simplicity dop
several instantiations sl dop ls dop 

  one might argue straightforward metric simplicity would return parse tree

smallest number nodes  rather smallest number treebank subtrees   metric
known perform quite badly  see manning   schtze        bod      a  

   

fia unified model structural organization language music

   computational issues
bod        showed standard chart parsing techniques applied likelihood dop 
treebank subtree converted context free rule r lefthand side r
corresponds root label righthand side r corresponds frontier labels
t  indices link rules original subtrees maintain subtree s internal
structure probability  rules used create derivation forest sentence
 using chart parser    see charniak         probable parse computed
sampling sufficiently large number random derivations forest   monte carlo
disambiguation   see bod         technique successfully applied parsing
atis portion penn treebank  marcus et al         extremely time consuming 
mainly number random derivations sampled reliably
estimate probable parse increases exponentially sentence length  see
goodman         therefore questionable whether bod s sampling technique scaled
larger domains wall street journal  wsj  portion penn treebank 
goodman        showed likelihood dop reduced compact stochastic
context free grammar  scfg  contains exactly eight scfg rules node
training set trees  although goodman s method still allow efficient computation
probable parse  in fact  problem computing probable parse
likelihood dop np hard    see sima an         method allow efficient
computation  maximum constituents parse   i e  parse tree likely
largest number correct constituents  unfortunately  goodman s scfg reduction method
beneficial indeed subtrees used  maximum parse accuracy usually
obtained restricting subtrees  example  bod      a  shows  optimal 
subtree set achieving highest parse accuracy wsj obtained restricting
maximum number words subtree    restricting maximum depth
unlexicalized subtrees    goodman        shows subtree restrictions 
subtree depth  may incorporated reduction method  found reduction
method optimal subtree set 
paper therefore use bod s subtree to rule conversion method likelihooddop  use bod s monte carlo sampling technique derivation forests 
turned computationally prohibitive  instead  use well known viterbi
optimization algorithm chart parsing  cf  charniak        manning   schtze       
allows computing k probable derivations input cubic time  using
algorithm  estimate probable parse tree input       
probable derivations  summing probabilities derivations generate tree 
although approach guarantee probable parse tree actually found 
shown bod      a  perform least well estimation probable
parse monte carlo techniques atis corpus  moreover  approach known
obtain significantly higher accuracy selecting parse tree generated single
probable derivation  bod        goodman         therefore consider
paper 
simplicity dop  first convert treebank subtrees rewrite rules
likelihood dop  next  simplest tree  i e  shortest derivation  efficiently
computed viterbi optimization way probable derivation  provided
assign rules equal probabilities  case shortest derivation equal
probable derivation  seen follows  rule probability p
probability derivation involving n rules equal p n  since   p   derivation
   

fibod

fewest rules greatest probability  experiments section    give
rule probability mass equal   r  r number distinct rules derived bod s
method  mentioned      shortest derivation may unique  case
compute shortest derivations input apply ranking scheme
derivations  ranks shortest derivations computed summing ranks
subtrees involve  shortest derivation smallest sum subtree ranks
taken produce best parse tree 
sl dop ls dop  compute either n likeliest n simplest trees means
viterbi optimization  next  either select simplest tree among n likeliest ones  for
sl dop  likeliest tree among n simplest ones  for ls dop   experiments  n
never larger       

   test domains
linguistic test domain used wall street journal  wsj  portion penn
treebank  marcus et al         portion contains approx         sentences
manually annotated perceived linguistic tree structures using predefined set
lexico syntactic labels  since wsj extensively used described
literature  cf  manning   schtze        charniak        collins        bod      a  
go here 
musical test domain used european folksongs essen folksong
collection  schaffrath        huron         correspond approx        folksongs
manually enriched perceived musical grouping structures  essen
folksong collection previously used bod      b  temperley        test
musical parsers  current paper presents first experiments likelihood dop 
simplicity dop  sl dop ls dop collection  essen folksongs
represented staff notation encoded essen associative code  esac  
pitch encodings esac resemble  solfege   scale degree numbers used replace
movable syllables  do    re    mi   etc  thus   corresponds  do     corresponds  re   etc 
chromatic alterations represented adding either      b  number 
plus       minus       signs added number note falls resp 
principle octave  thus          refer al  do   different octaves  
duration represented adding period underscore number  period      
increases duration     underscore       increases duration      
one underscore may added number  number duration indicator 
duration corresponds smallest value  thus pitches esac encoded integers
    possibly preceded followed symbols octave  chromatic alteration
duration  pitch encoding treated atomic symbol  may simple    
complex          pause represented    possibly followed duration
indicators  treated atomic symbol  loudness timbre indicators used
esac 
phrase boundaries indicated hard returns esac  phrases unlabeled  cf 
section   paper   yet make esac annotations readable dop models 
added three basic labels phrase structures  label  s  whole song 
label  p  phrase  label  n  atomic symbol  way  obtained
conventional tree structures could directly employed dop models parse new
input  use label  n  distinguishes annotations previous work  bod 
   

fia unified model structural organization language music

    b c  used labels song phrase   s   p    addition  n 
enhances productivity robustness musical parsing model  although leads
much larger number subtrees 
example  assume simple melody consisting two phrases              
tree structure given figure   

p

p

n

n

n

n

 

 

 

 

figure    example musical tree structure consisting two phrases
subtrees extracted tree structure include following 

p
n

p
p

n

n

n

n

 

 

 

 

figure    subtrees extracted tree figure  
thus first subtree indicates phrase starting note    followed exactly one
 unspecified  note  phrase followed exactly one  unspecified  phrase 
subtrees used parse new musical input way explained
linguistic parsing section   

   experimental evaluation comparison
evaluate dop models  used blind testing method randomly divides
treebank training set test set  strings test set parsed
means subtrees training set  applied standard parseval metrics
precision recall compare proposed parse tree p corresponding correct test
set parse tree follows  cf  black et al        
  correct constituents p

  correct constituents p

precision  

recall  

  constituents p

  constituents

constituent p  correct  exists constituent label spans
atomic symbols  i e  words notes    since precision recall obtain rather
  precision recall scores computed using  evalb  program  available via

http   www cs nyu edu cs projects proteus evalb  

   

fibod

different results  see bod      b   often balanced single measure
performance  known f score  see manning   schtze        
f score  

  precision recall
precision   recall

experiments  divided treebanks  i e  wsj essen folksong
collection     training test set splits      wsj used test material time
 sentences    words   essen folksong collection test sets       folksongs
used time  words test set unknown training set 
guessed categories using statistics word endings  hyphenation capitalization
 cf  bod      a   unknown notes  previous work  bod      a   limited
maximum size subtrees depth     used random samples         subtrees
depth          next  restricted maximum number atomic symbols
subtree    maximum depth unlexicalized subtrees    subtrees
smoothed technique described bod               based simple good turing
estimation  good        
table   shows mean f scores obtained sl dop ls dop language
music various values n  recall n    sl dop equal likelihood dop
ls dop equal simplicity dop 

n
 
 
  
  
  
  
  
  
  
  
   
     

sl dop

ls dop

 simplest among n likeliest 

 likeliest among n simplest 

language

music

language

music

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

table    f scores obtained sl dop ls dop language music

  random subtree samples selected first exhaustively computing complete set

subtrees  this computationally prohibitive   instead  particular depth     sampled
subtrees randomly selecting node random tree training set  selected
random expansions node subtree particular depth obtained  repeated
procedure         times depth        

   

fia unified model structural organization language music

table shows increase accuracy sl dop ls dop
value n increases       accuracy sl dop decreases n   
converges simplicity dop  i e  ls dop n     accuracy ls dop continues
increase converges likelihood dop  i e  sl dop n     highest accuracy
obtained sl dop    n     language music  thus sl dop outperforms
likelihood dop simplicity dop  selection simplest structure
top likeliest ones turns promising model selection likeliest
structure top simplest ones  according paired t testing  accuracy
improvement sl dop n    sl dop n    when equal likelihood dop 
statistically significant language  p        music  p       
surprising sl dop reaches highest accuracy small value n 
even surprising exactly model  with parameter setting  obtains
maximum accuracy language music  model embodies idea
perceptual system strives simplest structure searches among
probable structures 
compare results language others  tested sl dop n   
standard division wsj  uses sections      training  approx        
sentences  section    testing       sentences     words   see e g  manning  
schtze        charniak        collins         division  sl dop achieved f score
      best previous models obtained f score        collins        bod 
    a   terms error reduction  sl dop improves      models 
common report accuracy sentences    words wsj  sldop obtained f score       
musical results compared bod      b c   tested three probabilistic
parsing models increasing complexity training test set splits essen
folksong collection  best results obtained hybrid dop markov parser 
      f score  significantly worse best result       obtained sl dop
splits essen folksongs  difference may explained fact
hybrid dop markov parser bod      b c  takes account context higher
nodes tree sister nodes  dop models presented
current paper take subtree account  almost  arbitrary width depth  thereby
covering larger amount musical context  moreover  mentioned section    models
bod      b c  use label  n  notes  instead  markov approach used
parse new sequences notes 
would interesting compare musical results melodic parser
temperley         uses system preference rules similar lerdahl jackendoff
        evaluated essen folksong collection 
tested several test sets       randomly selected folksongs  temperley used one test
set    folksongs moreover cleaned eliminating folksongs irregular
meter  temperley             therefore difficult compare results temperley s 
yet  noteworthy temperley s parser correctly identified       phrase
boundaries  although lower       obtained sl dop  temperley s parser
 trained  previously analyzed examples model  though note
temperley s results obtained tuning optimal phrase length parser
average phrase length essen folksong collection  
perhaps mentioned parsing models trained treebanks widely
used natural language processing  still rather uncommon musical processing 
   

fibod

musical parsing models  including temperley s  employ rule based approach
parsing based combination low level rules     prefer phrase boundaries
large intervals     higher level rules     prefer phrase boundaries changes
harmony   low level rules usually based well known gestalt principles
proximity similarity  wertheimer         prefer phrase boundaries larger
intervallic distances  however  bod      c  shown gestalt principles
predict incorrect phrase boundaries number folksongs  higher level
phenomena cannot alleviate incorrect predictions  folksongs contain phrase
boundary falls large pitch time interval  which called
jump phrases  rather intervals    would predicted gestalt principles 
moreover  musical factors  melodic parallelism  meter harmony  predict
exactly incorrect phrase boundaries cases  see bod      b c details  
conjectured jump phrases inherently memory based  reflecting idiomdependent pitch contours  cf  huron        snyder         best captured
memory based model tries mimic musical experience listener
certain culture  bod      c  

   discussion conclusion
seen combination simplicity likelihood quite rewarding linguistic
musical structuring  suggesting interesting parallel two modalities  yet  one
may question whether model massively memorizes re uses previously perceived
structures cognitive plausibility  although question important want
claim cognitive relevance model  appears evidence people store
various kinds previously heard fragments  language  jurafsky        music
 saffran et al         people store fragments arbitrary size  proposed dop 
overview article  jurafsky        reports large body psycholinguistic evidence
showing people store lexical items bigrams  frequent phrases
even whole sentences  case sentences  people store idiomatic sentences 
 regular  high frequency sentences   thus  least language
evidence humans store fragments arbitrary size provided fragments
certain minimal frequency  suggests humans need always parse new input
rules grammar  productively re use previously analyzed fragments 
yet  evidence people store fragments hear  suggested dop 
high frequency fragments seem memorized  however  human perceptual
faculty needs learn fragments stored  initially need keep track
fragments  with possibility forgetting them  otherwise frequencies never
accumulate  results model continuously incrementally updates fragment
memory given new input    correspondence dop approach 
approaches  cf  daelemans        scha et al        spiro        
acknowledge importance rule based system acquiring fragment memory 
substantial memory available may efficient construct tree means
already parsed fragments constructing entirely means rules  many cognitive
  results derived differences reaction times sentence recognition

frequency  whole  test sentences varied  variables  lexical frequency 
bigram frequency  plausibility  syntactic semantic complexity  etc   kept constant 

   

fia unified model structural organization language music

activities advantageous store results  immediately retrieved
memory  rather computing time scratch  shown 
example  manual reaches  rosenbaum et al         arithmetic operations  rickard et al 
       word formation  baayen et al         mention few  linguistic musical
parsing may exception this 
stressed experiments reported paper limited least two
respects  first  musical test domain rather restricted  wide variety linguistic
treebanks currently available  see manning   schtze         number musical
treebanks extremely limited  thus need larger richer annotated musical
corpora covering broader domains  development annotated corpora may timeconsuming  experience natural language processing shown worth
effort  since corpus based parsing systems dramatically outperform grammar based parsing
systems  second limitation experiments evaluated parse
results rather parse process  is  assessed accurately
models mimic input output behavior human annotator  without investigating
process annotator arrived perceived structures  unlikely humans
process perceptual input computing        likely derivations using random samples
        subtrees current paper  yet  many applications suffices
know perceived structure rather process led structure 
seen combination simplicity likelihood predicts perceived structure
high degree accuracy 
proposals integrating principles simplicity likelihood
human perception  see chater       review   chater notes context
information theory  shannon         principles simplicity likelihood identical 
context  simplicity principle interpreted minimizing expected length encode
message i  log  p bits  leads result maximizing
probability i  used information theoretical definition simplest structure
simplicity dop  would return structure likelihood dop  improved
results would obtained combination two  hand  defining
simplest structure one generated smallest number subtrees  independent
probabilities  created notion simplicity provably different notion
likely structure  which  combined likelihood dop  obtained improved results 
another integration two principles may provided notion minimum
description length mdl  cf  rissanen         mdl principle viewed
preferring statistical model allows shortest encoding training data 
relevant encoding consists two parts  first part encodes model data 
second part encodes data terms model  in bit length   mdl closely related
stochastic complexity  rissanen        kolmogorov complexity  li vitanyi        
used natural language processing estimating parameters stochastic
grammar  e g  osborne         leave open research question whether
mdl successfully used estimating parameters dop s subtrees  however 
since mdl known give asymptotically results maximum likelihood estimation
 mle   rissanen         application dop may lead unproductive model 
maximum likelihood estimator assign training set trees empirical
frequencies  assign   weight trees  see bonnema       proof  
would result model generate training data strings 
johnson        argues may overlearning problem rather problem
   

fibod

mle per se  standard methods  cross validation regularization  would seem
principle ways avoid overlearning  leave issue future
investigation 
idea general underlying model language music uncontroversial 
linguistics usually assumed humans separate language faculty  lerdahl
jackendoff        argued separate music faculty  work propose
separate faculties exist  wants focus commonalities rather
differences faculties  aiming finding deeper  faculty  may hold
perception general  hypothesis perceptual system strives simplest
structure searches among likeliest structures 

acknowledgements
thanks aline honingh  remko scha  neta spiro  menno van zaanen three anonymous
reviewers excellent comments  preliminary version paper presented
keynote talk lcg workshop   learning computational grammars   tbingen        

references
baayen  r  h   dijkstra  t    schreuder  r          singular plurals dutch  evidence
parallel dual route model  journal memory language             
black  e   abney  s   flickinger  d   gnadiec  c   grishman  r   harrison  p   hindle  d  
ingria  r   jelinek  f   klavans  j   liberman  m   marcus  m   roukos  s   santorini  b 
  strzalkowski  t          procedure quantitatively comparing syntactic
coverage english  proceedings darpa speech natural language
workshop  pacific grove  morgan kaufmann 
bod  r          using annotated language corpus virtual stochastic grammar 
proceedings aaai     menlo park  ca 
bod  r          beyond grammar  experience based theory language  stanford 
csli publications  lecture notes number     
bod  r       a   parsing shortest derivation  proceedings coling      
saarbrcken  germany 
bod  r       b   combining semantic syntactic structure language modeling 
proceedings icslp       beijing  china 
bod  r       a   minimal set fragments achieves maximal parse
accuracy  proceedings acl       toulouse  france 
bod  r       b   memory based model music analysis  proceedings international
computer music conference  icmc        havana  cuba 
bod  r       c   memory based models melodic analysis  challenging gestalt
principles  journal new music research                 available
http   staff science uva nl  rens jnmr   pdf 
   

fia unified model structural organization language music

bod  r   hay  j    jannedy  s   eds        a   probabilistic linguistics  cambridge 
mit press   in press 
bod  r   scha  r    sima an  k   eds        b   data oriented parsing  stanford  csli
publications   in press 
bonnema  r          probability models dop  bod et al       b  
bonnema  r   bod  r    scha  r          dop model semantic interpretation 
proceedings acl eacl     madrid  spain 
buffart  h   leeuwenberg  e    restle   f          analysis ambiguity visual pattern
completion  journal experimental psychology  human perception
performance              
charniak  e          statistical language learning  cambridge  mit press 
charniak  e          statistical techniques natural language parsing  ai magazine 
winter             
charniak  e          maximum entropy inspired parser  proceedings anlpnaacl       seattle  washington 
chater  n          search simplicity  fundamental cognitive principle 
quarterly journal experimental psychology    a             
chomsky  n          aspects theory syntax  cambridge  mit press 
collard  r   vos  p    leeuwenberg  e          melody tells metre music  
zeitschrift fr psychologie             
collins  m          head driven statistical models natural language parsing  phdthesis  university pennsylvania  pa 
collins  m          discriminative reranking natural language parsing  proceedings
icml       stanford  ca 
collins  m    duffy  n          new ranking algorithms parsing tagging  kernels
discrete structures  voted perceptron  proceedings acl      
philadelphia  pa 
daelemans  w          introduction special issue memory based language
processing  journal experimental theoretical artificial intelligence       
        
dastani  m          languages perception  illc dissertation series          university
amsterdam 
dempster  a   laird  n    rubin  d          maximum likelihood incomplete data via
em algorithm  journal royal statistical society           

   

fibod

de pauw  g          aspects pattern matching data oriented parsing  proceedings
coling       saarbrcken  germany 
eisner  j          bilexical grammars cubic time probabilistic parser  proceedings
fifth international workshop parsing technologies  boston  mass 
frazier  l          comprehending sentences  syntactic parsing strategies  phd 
thesis  university connecticut 
good  i          population frequencies species estimation population
parameters  biometrika             
goodman  j          efficient algorithms parsing dop model  proceedings
empirical methods natural language processing  philadelphia  pa 
goodman  j          efficient parsing dop pcfg reductions  bod et al      b 
von helmholtz  h          treatise physiological optics  vol      dover  new york 
hoffman  d          visual intelligence  new york  norton   company  inc 
huron  d          melodic arch western folksongs  computing musicology         
johnson  m          dop estimation method biased inconsistent  computational
linguistics            
jurafsky  d          probabilistic modeling psycholinguistics  comprehension
production  bod et al      a   available http   www colorado edu ling jurafsky 
prob ps 
kersten  d          high level vision statistical inference  gazzaniga   s   ed    new
cognitive neurosciences  cambridge  mit press 
leeuwenberg  e          perceptual coding language perceptual auditory
patterns  american journal psychology              
lerdahl  f    jackendoff  r          generative theory tonal music  cambridge 
mit press 
li  m    vitanyi  p          introduction kolmogorov complexity
applications   nd ed    new york  springer 
longuet higgins  h          perception melodies  nature              
longuet higgins  h  lee  c          rhythmic interpretation monophonic music 
mental processes  studies cognitive science  cambridge  mit press 
manning  c    schtze  h          foundations statistical natural language
processing  cambridge  mit press 
marcus  m   santorini  b     marcinkiewicz  m          building large annotated corpus
english  penn treebank  computational linguistics       
   

fia unified model structural organization language music

marr  d          vision  san francisco  freeman 
martin  w   church  k    patil  r          preliminary analysis breadth first parsing
algorithm  theoretical experimental results  bolc  l   ed    natural language
parsing systems  springer verlag  berlin 
mumford  d          dawning age stochasticity  based lecture
accademia nazionale dei lincei   available http   www dam brown edu people 
mumford papers dawning ps 
osborne  m          minimal description length based induction definite clause grammars
noun phrase identification  proceedings eacl workshop computational
natural language learning  bergen  norway 
palmer  s          hierarchical structure perceptual representation  cognitive
psychology             
raphael  c          automatic segmentation acoustic musical signals using hidden
markov models  ieee transactions pattern analysis machine intelligence 
               
restle   f          theory serial pattern learning  structural trees  psychological
review           
rickard  t   healy  a    bourne jr   e          cognitive structure basic arithmetic
skills  operation  order symbol transfer effects  journal experimental
psychology  learning  memory cognition                
rissanen  j          modeling shortest data description  automatica              
rissanen  j          stochastic complexity statistical inquiry  series computer
science   volume     world scientific       
rosenbaum  d   vaughan  j   barnes  h    jorgensen  m          time course movement
planning  selection handgrips object manipulation  journal experimental
psychology  learning  memory cognition                
saffran  j   loman  m    robertson  r          infant memory musical experiences 
cognition      b      
scha  r   bod  r    sima an  k          memory based syntactic analysis  journal
experimental theoretical artificial intelligence                 
schaffrath  h          essen folksong collection humdrum kern format  d 
huron  ed    menlo park  ca  center computer assisted research
humanities 
shannon  c          mathematical theory communication  bell system technical
journal                       
sima an  k          computational complexity probabilistic disambiguation means
tree grammars  proceedings coling     copenhagen  denmark 
   

fibod

simon  h          complexity representation patterned sequences symbols 
psychological review              
snyder  b          music memory  cambridge  mit press 
spiro  n          combining grammar based memory based models perception
time signature phase  anagnostopoulou  c   ferrand  m    smaill  a   eds   
music artificial intelligence  lecture notes artificial intelligence  vol       
springer verlag          
temperley  d          cognition basic musical structures  cambridge  mit
press 
wertheimer  m          untersuchungen zur lehre von der gestalt  psychologische
forschung            
wundt  w          sprachgeschichte und sprachpsychologie  engelmann  leipzig 

   


