journal of artificial intelligence research               

submitted       published     

a critical assessment of
benchmark comparison in planning
adele e  howe
eric dahlman

computer science department
colorado state university  fort collins  co      

howe cs colostate edu
dahlman cs colostate edu

abstract
recent trends in planning research have led to empirical comparison becoming commonplace  the field has started to settle into a methodology for such comparisons  which
for obvious practical reasons requires running a subset of planners on a subset of problems 
in this paper  we characterize the methodology and examine eight implicit assumptions
about the problems  planners and metrics used in many of these comparisons  the problem assumptions are  pr   the performance of a general purpose planner should not be
penalized biased if executed on a sampling of problems and domains  pr   minor syntactic
differences in representation do not affect performance  and pr   problems should be solvable by strips capable planners unless they require adl  the planner assumptions are 
pl   the latest version of a planner is the best one to use  pl   default parameter settings
approximate good performance  and pl   time cut offs do not unduly bias outcome  the
metrics assumptions are  m   performance degrades similarly for each planner when run
on degraded runtime environments  e g   machine platform  and m   the number of plan
steps distinguishes performance  we find that most of these assumptions are not supported
empirically  in particular  that planners are affected differently by these assumptions  we
conclude with a call to the community to devote research resources to improving the state
of the practice and especially to enhancing the available benchmark problems 

   introduction
in recent years  comparative evaluation has become increasingly common for demonstrating
the capabilities of new planners  planners are now being directly compared on the same
problems taken from a set of domains  as a result  recent advances in planning have
translated to dramatic increases in the size of the problems that can be solved  weld 
       and empirical comparison has highlighted those improvements 
comparative evaluation in planning has been significantly inuenced and expedited by
the artificial intelligence planning and scheduling  aips  conference competitions  these
competitions have had the dual effect of highlighting progress in the field and providing
a relatively unbiased comparison of state of the art planners  when individual researchers
compare their planners to others  they include fewer other planners and fewer test problems
because of time constraints 
to support the first competition in       mcdermott         drew mcdermott defined 
with contributions from the organizing committee  a shared problem domain definition
language  pddl  mcdermott et al          planning domain definition language   using

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fihowe   dahlman

a common language means that planners  performance can be directly compared  without
entailing hand translation or factoring in different representational capabilities 
as a second benefit  the lack of translation  or at least human accomplished translation  meant that performance could be compared on a large number of problems and
domains   in fact  the five competition planners were given a large number of problems
     problems for the adl track and     for the strips track  within seven domains 
including one domain that the planner developers had never seen prior to the competition 
so the first competition generated a large collection of benchmarks  seven domains used in
the competition plus    more that were considered for use  all    domains are available
at ftp   ftp cs yale edu pub mcdermott domains   the second competition added three
novel domains to that set 
a third major benefit of the competitions is that they appear to have motivated researchers to develop systems that others can use  the number of entrants went from five in
the first competition to    in the second  additionally  all of the      competitors and six
out of sixteen of the      competitors made their code available on web sites  thus  others
can perform their own comparisons 
in this paper  we describe the current practice of comparative evaluation as it has evolved
since the aips competitions and critically examine some of the underlying assumptions
of that practice  we summarize existing evidence about the assumptions and describe
experimental tests of others that had not previously been considered  the assumptions
are organized into three groups concerning critical decisions in the experiment design  the
problems tested  the planners included and the performance metrics collected 
comparisons  as part of competitions or by specific researchers  have proven to be enormously useful to motivating progress in the field  our goal is to understand the assumptions
so that readers know how far the comparative results can be generalized  in contrast to the
competitions  the community cannot legislate fairness in individual researcher s comparative evaluations  but readers may be able to identify cases in which results should be viewed
either skeptically or with confidence  thus  we conclude the paper with some observations
and a call for considerably more research into new problems  metrics and methodologies to
support planner evaluation 
also in contrast to the competitions  our goal is not to declare a winner  our goal is
also not to critique individual studies  consequently  to draw attention away from such a
possible interpretation  whenever possible  we report all results using letter designators that
were assigned randomly to the planners 

   planning competitions and other direct comparisons

recently  the aips competitions have spurred considerable interest in comparative evaluation  the roots of comparative planner evaluation go back considerably further  however 
although few researchers were able to run side by side comparisons of their planners with
   to solve a particular planning problem  i e   construct a sequence of actions to transform an initial state to
a goal state   planners require a domain theory and a problem description  the domain theory represents
the abstract actions that can be executed in the environment  typically  the domain descriptions include
variables that can be instantiated to specific objects or values  multiple problems can be defined for
each domain  problem descriptions require an initial state description  a goal state and an association
with some domain 

 

fia critical assessment of benchmark comparison in planning

others  they were able to demonstrate performance of their planner on well known problems  which could be viewed as de facto benchmarks  sussman s anomaly  sussman       
in blocksworld was the premier planning benchmark problem and domain for many years 
every planner needed to  cut its teeth  on it 
as researchers tired of blocksworld  many called for additional benchmark problems
and environments  mark drummond  leslie kaelbling and stanley rosenschein organized
a workshop on benchmarks and metrics  drummond  kaelbling    rosenschein        
testbed environments  such as martha pollack s tileworld  pollack   ringuette        or
steve hanks s truckworld  hanks  nguyen    thomas         were used for comparing
algorithms within planners  by       ucpop  penberthy   weld        was distributed
with a large set of problems      problems in    domains  for demonstration purposes  in
      barry fox and mark ringer set up a planning and scheduling benchmarks web page
 http   www newosoft com  benchmrx   to collect problem definitions  with an emphasis
on manufacturing applications  recently  planet  a coordinating organization for european planning and scheduling researchers  has proposed a planning benchmark collection
initiative  http   planet dfki de  
clearly  benchmark problems have become well established means for demonstrating
planner performance  however  the practice has known benefits and pitfalls  hanks  pollack
and cohen        discuss them in some detail in the context of agent architecture design 
the benefits include providing metrics for comparison and supporting experimental control 
the pitfalls include a lack of generality in the results and a potential for the benchmarks to
unduly inuence the next generation of solutions  in other words  researchers will construct
solutions to excel on the benchmarks  regardless of whether the benchmarks accurately
represent desired real applications 
to obtain the benefits just listed for benchmarks  the problems often are idealized or
simplified versions of real problems  as cohen        points out   most research papers in
ai  or at least at an aaai conference  exploit benchmark problems  yet few of them relate
the benchmarks to target tasks  this may be a significant problem  for example  in a study
of owshop scheduling  benchmarks  we found that performance on the standard benchmark set did not generalize to performance on problems with realistic structure  watson 
barbulescu  howe    whitley         a study of just blocksworld problems found that the
best known blocksworld benchmark problems are atypical in that they require only short
plans for solution and optimal solutions are easy to find  slaney   thiebaux        
in spite of these diculties  benchmark problems and the aips competitions have considerably inuenced comparative planner evaluations  for example  in the aips      conference proceedings  chien  kambhampati    knoblock         all of the papers on improvements to classical planning     out of    papers at the conference  relied heavily on
comparative evaluation using benchmark problems  the other papers concerned scheduling 
specific applications  theoretical analyses or special extensions to the standard paradigm
 e g   pomdp  sensing   of the    classical papers  six used problems from the aips  
competition benchmark set  six used problems from kautz and selman s distribution of
problems with blackbox  kautz        and three added some of their own problems as
well  each paper showed results on a subset of problems from the benchmark distributions
   scheduling is an area related to planning in which the actions are already known  but their sequence still
needs to be determined  flowshop scheduling is a type of manufacturing scheduling problem 

 

fihowe   dahlman

 e g   drew mcdermott s from the first competition  with logistics  blocksworld  rocket and
gripper domains being most popular  used in          and   papers  respectively   the availability of planners from the competition was also exploited  eight of the papers compared
their systems to other aips   planners  blackbox  stan  ipp and hsp  in         and  
papers  respectively  

   assumptions of direct comparison
a canonical planner evaluation experiment follows the procedure in table    the procedure
is designed to compare performance of a new planner to the previous state of the art and
highlight superior performance in some set of cases for the new planner  the exact form
of an experiment depends on its purpose  e g   showing superiority on a class of problem or
highlighting the effect of some design decision 
   select and or construct a subset of planner domains
   construct problem set by 
 running large set of benchmark problems
 selecting problems with desirable features
 varying some facet of the problem to increase diculty  e g   number of blocks 
   select other planners that are 
 representative of the state of the art on the problems or
 similar to or distinct from the new planner  depending on the point of the comparison or advance of the new planner or
 available and able to parse the problems
   run all problems on all planners using default parameters and setting an upper limit
on time allowed
   record which problems were solved  how many plan steps actions were in the solution
and how much cpu time was required to either solve the problem  fail or time out
table    canonical comparative planner evaluation experiment 
the protocol depends on three selections  problems  planners and evaluation metrics 
it is simply not practical or even desirable to run all available planners on all available
problems  thus  one needs to make informed decisions about which to select  a purpose
of this paper is to examine the assumptions underlying these decisions to help make them
more informed  every planner comparison does not adopt every one of these assumptions 
but the assumptions are ones commonly found in planner comparisons  for example  those
comparisons designed for a specific purpose  e g   to show scale up on certain problems
or suitability of the planner for logistics problems  will carefully select particular types of
problems from the benchmark sets 
 

fia critical assessment of benchmark comparison in planning

problems many planning systems were developed to solve a particular type of planning
problem or explore a specific type of algorithmic variation  consequently  one would expect
them to perform better on the problems on which and for which they were developed  even
were they not designed for a specific purpose  the test set used during development may have
subtly biased the development  the community knows that planner performance depends
on problem features  but not in general  how  when and why  researchers tend to design
planners to be general purpose  consequently  comparisons assume that
the performance of a general purpose planner should not be penalized biased if
executed on a sampling of problems and domains  problem assumption    

the community also knows that problem representation inuences planner performance 
for example  benchmark problem sets include many versions of blocksworld problems  designed by different planner developers  these versions vary in their problem representation 
both minor apparently syntactic changes  e g   how clauses are ordered within operators 
initial conditions and goals  and whether any information is extraneous  and changes reecting addition of domain knowledge  e g   what constraints are included and whether
variables are typed   consequently  comparisons assume that
syntactic representational modifications either do not matter or affect each planner equally  problem assumption    

pddl includes a field   requirements  for the capabilities required of a planner to solve
the problem  pddl    defined    values for the  requirements field  the base default requirement is  strips  meaning strips derived add and delete sets for action effects   adl
 from pednault s action description language  requires variable typing  disjunctive preconditions  equality as a built in predicate  quantified preconditions and conditional effects
in addition the  strips capability  yet  many planners either ignore the  requirements
field or reject the problem only if it specifies  adl  ignoring many of the other requirements
that could also cause trouble   thus  comparisons assume that
problems in the benchmark set should be solvable by a strips planner unless
they require  adl  problem assumption    

planners the wonderful trend of making planners publicly available has led to a dilemma
in determining which to use and how to configure them  the problem is compounded by the
longevity of some of these planner projects  some projects have produced multiple versions 
consequently  comparisons tend to assume that
the latest version of the planner is the best  planner assumption    

these planners may also include parameters  for example  the blackbox planner allows the
user to define a strategy for applying different solution methods  researchers expect that
parameters affect performance  consequently  comparisons assume that
default parameter settings approximate good performance  planner assumption
   
 

fihowe   dahlman

experiments invariably use time cut offs for concluding planning that has not yet found
a solution or declared failure  many planners would need to exhaustively search a large space
to declare failure  for practical reasons  a time out threshold is set to determine when to
halt a planner  with a failure declared when the time out is reached  thus  comparisons
assume that
if one picks a suciently high time out threshold  then it is highly unlikely that
a solution would have been found had slightly more time been granted  planner
assumption    

metrics ideally  performance would be measured based on how well the planner does

its job  i e   constructing the  best  possible plan to solve the problem  and how eciently
it does so  because no planner has been shown to solve all possible problems  the basic
metric for performance is the number or percentage of problems actually solved within the
allowed time  this metric is commonly reported in the competitions  however  research
papers tend not to report it directly because they typically test a relatively small number
of problems 
eciency is clearly a function of memory and effort  memory size is limited by the
hardware  effort is measured as cpu time  preferably but not always on the same platform
in the same language  the problems with cpu time are well known  programmer skill
varies  research code is designed more for fast prototyping than fast execution  numbers in
the literature cannot be compared to newer numbers due to processor speed improvements 
however  if cpu times are regenerated in the experimenter s environment then one assumes
that
performance degrades similarly with reductions in capabilities of the runtime
environment  e g   cpu speed  memory size   metric assumption    

in other words  an experimenter or user of the system does not expect that code has been
optimized for a particular compiler operating system hardware configuration  but it should
perform similarly when moved to another compatible environment 
the most commonly reported comparison metric is computation time  the second most
is number of steps or actions  for planners that allow parallel execution  in a plan  although
planning seeks solutions to achieving goals  the goals are defined in terms of states of the
world  which does not lend itself well to general measures of quality  in fact  quality is likely
to be problem dependent  e g   resource cost  amount of time to execute  robustness   which
is why number of plan steps has been favored  comparisons assume that
number of steps in a resulting plan varies between planner solutions and approximates quality  metric assumption    

any comparison  competitions especially  has the unenviable task of determining how to
trade off or combine the three metrics  number solved  time  and number of steps   thus 
if number of steps does not matter  then the comparison could be simplified 
we converted each assumption into a testable question  we then either summarized the
literature on the question or ran an experiment to test it 
 

fia critical assessment of benchmark comparison in planning

    our experimental setup
some of the key issues have been examined previously  directly or indirectly  for those 
we simply summarize the results in the subsections that follow  however  some are open
questions  for those  we ran seven well known planners on a large set of      benchmark
problems  the planners all accept the pddl representation  although some have built in
translators for pddl to their internal representation and others rely on translators that we
added  when several versions of a planner were available  we included them all  for a total
of    planners   the basic problem set comprises the ucpop benchmarks  the aips   and
     competition test sets and an additional problem set developed for a specific application 
with the exception of the permuted problems  see the section on problem assumption
  for specifics   the problems were run on     mhz ultrasparc   s with     megabytes
of memory running sunos      whenever possible  versions compiled by the developers
were used  when only source code was available  we compiled the systems according to the
developers  instructions  the planners written in common lisp were run under allegro
common lisp version        the other planners were compiled with gcc  egcs version
          each planner was given a    minute limit of wall clock time  to find a solution 
however  all times reported are run times returned by the operating system 
      planners

the planners are all what have been called primitive action planners  wilkins   desjardins 
       planners that require relatively limited domain knowledge and construct plans from
simple action descriptions  because the aips   competition required planners to accept
pddl  the majority of planners used in this study were competition entrants or are later
versions thereof     the common language facilitated comparison between the planners without having to address the effects of a translation step  the two exceptions were ucpop and
prodigy  however  their representations are similar to pddl and were translated automatically  the planners represent five different approaches to planning  plan graph analysis 
planning as satisfiability  planning as heuristic search  state space planning with learning
and partial order planning  when possible  we used multiple versions of a planner  and not
necessarily the most recent  because we conducted this study over some period of time  almost     years   we froze the set early on  we are not comparing the performance to declare
a winner and so did not think that the lack of recent versions undermined the results of
testing our assumptions 

ipp  koehler  nebel  hoffmann    dimopoulos        extends the graphplan  blum  

furst        algorithm to accept a richer plan description language  in its early versions 
this language was a subset of adl that extends the strips formalism of graphplan
to allow for conditional and universally quantified effects in operators  until version     
negation was handled via the introduction of new predicates for the negated preconditions
   we used actual time on lightly loaded machines because occasionally a system would thrash due to
inadequate memory resulting in little progress over considerable time 
   we used the bus system as the manager for running the planners  howe  dahlman  hansen  scheetz   
von mayrhauser         which was implemented with the aips   competition planners  this facilitated
the running of so many different planners  but did somewhat bias what was included 

 

fihowe   dahlman

and corresponding mutual exclusion rules  subsequent versions handle it directly  koehler 
       we used the aips   version of ipp as well as the later     version 
sgp  sensory graph plan   weld  anderson    smith        also extends graphplan to
a richer domain description language  primarily focusing on uncertainty and sensing  as
with ipp  some of this transformation is performed using expansion techniques to remove
quantification  sgp also directly supports negated preconditions and conditional effects 
sgp tends to be slower  it is implemented in common lisp instead of c  than some of the
other graphplan based planners  we used sgp version    b 
stan  state analysis   fox   long        extends the graphplan algorithm in part by
adding a preprocessor  called tim  to infer type information about the problem and domain 
this information is then used within the planning algorithm to reduce the size of the search
space that the graphplan algorithm would search  stan also incorporated optimized data
structures  bit vectors of the planning graph  that help avoid many of the redundant calculations performed by graphplan  additionally  stan maintains a wave front during graph
construction to track remaining goals and so limit graph construction  subsequent versions
incorporated further analyses  e g   symmetry exploitation  and an additional simpler planning engine  four versions of stan were tested  the aips   competition version  version
     version    s and a development snapshot of version     
blackbox  kautz   selman        converts planning problems into boolean satisfiability
problems  which are then solved using a variety of different techniques  the user indicates
which techniques should be tried in what order  in constructing the satisfiability problem 
blackbox uses the planning graph constructed as in graphplan  for blackbox  we used
version     and version    b 
hsp  heuristic search planner   bonet   geffner        is based on heuristic search  the
planner uses a variation of hill climbing with random restarts to solve planning problems 
the heuristic is based on using the graphplan algorithm to solve a relaxed form of the
planning problem  in this study  we used version      which is an algorithmic refinement of
the version entered into the aips   competition  and version     
prodigy    the prodigy research group        combines state space planning with backward chaining from the goal state  a plan under construction consists of a head plan of
totally ordered actions starting from the initial state and a tail plan of partially ordered
actions related to the goal state  although not ocially entered into the competition  informal results presented at the aips   competition suggested that prodigy performed well
in comparison to the entrants  we used prodigy version     
ucpop  barrett  golden  penberthy    weld        is a partial order causal link
planner  the decision to include ucpop was based on several factors  first  it does
not expand quantifiers and negated preconditions  for some domains  the expansion from
grounding operators can be so great as to make the problem insolvable  second  ucpop
is based on a significantly different algorithm in which interest has recently resurfaced  we
used ucpop version     
   we thank eugene fink for code that translates pddl to prodigy 

 

fia critical assessment of benchmark comparison in planning

source
  of domains   of problems
benchmarks
  
   
aips     
 
   
aips     
 
   
developers
 
  
application
 
  
table    summary of problems in our testing set  source of the problems  the number of
domains and problems within those domains 
      test problems

following standard practice  our experiments require planners to solve commonly available
benchmark problems and the aips competition problems  in addition  to test our assumptions about the inuence of domains  assumption pr   and representations of problems
 assumption pr    we will also include permuted benchmark problems and some other application problems  this section describes the set of problems and domains in our study 
focusing on their source and composition 
the problems require only strips capabilities  i e   add and delete lists   we chose this
least common denominator for several reasons  first  more capable planners can still handle
strips requirements  thus  this maximized the number of planners that could be included
in our experiment  also  not surprisingly  more problems of this type are available  second 
we are examining assumptions of evaluation  including the effect of required capabilities on
performance  we do not propose to duplicate the effort of the competitions in singling out
planners for distinction  but rather  our purpose is to determine what factors differentially
affect planners 
the bulk of the problems came from the aips   and aips      problem sets and
the set of problems distributed with the pddl specification  the remaining problems
were solicited from several sources  the source and counts of problems and domains are
summarized in table   
benchmark problems the preponderance of problems in planning test sets are  toy
problems   well known synthetic problems designed to test some attribute of planners 
the blocksworld domain has long been included in any evaluation because it is well known 
can have subgoal interactions and supports constructing increasingly complex problems
 e g   towers of more blocks   a few benchmark problems are simplified versions of realistic
planning problems  e g   the at tire  refrigerator repair or logistics domains  we used the
set included with the ucpop planner  these problems were contributed by a large number
of people and include multiple encodings of some problems domains  especially blocksworld 
aips competitions       and      for the first aips competition  drew mcdermott solicited problems from the competitors as well as constructing some of his own  such
as the mystery domain  which had semantically useless names for objects and operators 
problems were generated for each domain automatically  the competition included    
problems from six domains  robot movement in a grid  gripper in which balls had to be
 

fihowe   dahlman

moved between rooms by a robot with two grippers  logistics of transporting packages  organizing snacks for movie watching  and two mystery domains  which were disguised logistics
problems 
the format of the      competition required entrants to execute     problems in the
first round  of these problems     could not be solved by any planner  for round two  the
planners executed    new problems in three domains  one of which had not been included
in the first round 
the      competition attracted    competitors in three tracks  strips  adl and
a hand tailored track  it required performance on problems in five domains  logistics 
blocksworld  parts machining  freecell  a card game   and miconic    elevator control 
these domains were determined by the organizing committee  with fahiem bacchus as the
chair  and represented a somewhat broader range  we chose problems from the untyped
strips track for our set 
from a scientific standpoint  one of the most interesting conclusions of both competitions was the observed trade offs in performance  planners appeared to excel on different
problems  either solving more from a set or finding a solution faster  in       ipp solved
more problems and found shorter plans in round two  stan solved its problems the fastest 
hsp solved the most problems in round one  and blackbox solved its problems the fastest
in round one  in       awards were given to two groups of distinguished planners across
the different categories of planners  strips  adl and hand tailored   because according
to the judges   it was impossible to say that any one planner was the best  bacchus        
talplanner and ff were in the highest distinguished planner group  the graphs of performance do show differences in computation time relative to other planners and to problem
scale up  however  each planner failed to solve some problems  which makes these trends
harder to interpret  the computation time graphs have gaps  
the purpose of these competitions was to showcase planner technology at which they
succeeded admirably  the planners solved much harder problems than could have been
accomplished in years past  because of this trend in planners handling increasingly dicult
problems  the competition test sets may become of historical interest for tracking the field s
progress 
problems solicited from planner developers we also asked planner developers what
problems she had used during development  one developer  maria fox  sent us a domain
 sodor  which is a logistics application  and set of problems that they had used  we would
have included other domains and problems had we received any others 
other applications the miconic elevator domain from the aips     competition was
derived from an actual planning application  the domain and problems were extremely
simplified  e g   removing the arithmetic  
to add another realistic problem to the comparison  we included one other planning application to the set of test domains  generating cases to test a software interface  because
of the similarities between software interface test cases and plans  we developed a system 
several years ago  for automatically generating interface test cases using an ai planner 
the system was designed to generate test cases for the user interface to storage technology s robot tape library  howe  von mayrhauser    mraz         the interface  i e   the
commands in the interface  was coded as the domain theory  for example  the mount com  

fia critical assessment of benchmark comparison in planning

mand action s description required that a drive be empty and had the effect of changing
the position of the tape being mounted and changing the status of the tape drive  problems
described initial states of the tape library  e g   where tapes were resident  what was the
status of the devices and software controller  and goal states that a human operator might
wish to achieve 
at the time  we found that only the simplest problems could be generated using the
planners available  we included this application in part because we knew it would be a
challenge  as part of the test set  we include three domain theories  different ways of
coding the application involving      operators  and twenty four problems for each domain 
we included only    because we wanted to include enough problems to see some effect  but
not too many to overly bias the results  these problems were relatively simple  requiring
the movement of no more than one tape coupled with some status changes  but they were
still more dicult than could be solved in our original system 

    problem assumptions
general purpose planners exhibit differential capabilities on domains and sometimes even
problems within a domain  thus  the selection of problem set would seem to be critical
to evaluation  for example  many problems in benchmark sets are variants of logistics
problems  thus  a general purpose planner that was actually tailored for logistics may appear
to be better overall on current benchmarks  in this section  we will empirically examine
some possible problem set factors that may inuence performance results 

problem assumption    to what extent is performance of general purpose
planners biased toward particular problems domains  although most planners
are developed as general purpose  the competitions and previous studies have shown that
planners excel on different domains problems  unfortunately  the community does not yet
have a good understanding of why a planner does well on a particular domain  we studied
the impact of problem selection on performance in two ways 
first  we assessed whether performance might be positively biased toward problems
tested during development  each developer  was asked to indicate which domains they used
during development  we then compared each planner s performance on their development
problems  i e   the development set  to the problems remaining in the complete test set
 rest   we ran  x    tests comparing number of problems solved versus failed in the
development and test sets  we included only the number solved and failed in the analysis
as timed out problems made no difference to the results  
the results of this analysis are summarized in table    figure   graphically displays the
ratio of successes to failures for the development and other problems  all of the planners
except c performed significantly better on their development problems  this suggests that
these planners have been tailored  intentionally or not  for particular types of problems and
that they will tend to do better on test sets biased accordingly  for example  one of the
   we decided against studying some of the planners in this way because the representations for their
development problems were not pddl 
   one planner was the exception to this rule  in one case  the planner timed out far more frequently on
non development problems 

  

fihowe   dahlman

development
planner sol  fail
a
  
  
b
  
  
c
  
 
g
  
  
h
  
 
i
   
  
j
   
  
k
  
  
l
  
  

rest
sol  fail
 
p
                    
                   
      
          
                   
                   
                    
                    
                   
                   

table      results comparing outcome on development versus other problems 
planners in our set  stan  was designed with an emphasis on logistics problems  fox  
long        

figure    histogram of ratios of success failures for development and other problems for
each of the planners 
the above analysis introduces a variety of biases  the developers tended to give us short
lists that probably were not really representative of what they actually used  the set used
is a moving target  rather than stationary as this suggests  the set of problems included
in experimentation for publication may be different still  consequently  for the second
part  we broadened the question to determine the effect of different subsets of problems on
  

fia critical assessment of benchmark comparison in planning

n
 
  
  
  

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

 
 
 
 
 

rank dominance
       
        
        
       
       

       total pairs
        
  
       
  
       
  
      
  

table    rank dominance counts for    samples of domains with domain sizes  n  of five
through    
performance  for each of    trials  we randomly selected n domains  and their companion
problems  to form the problem set  we counted how many of these problems could be
solved by each planner and then ranked the relative performance of each planner  thus 
for each value of n  we obtained    planner rankings  we focused on rankings of problems
solved for two reasons  first  each domain includes a different number of problems  making
the count of problems variable across each of the trials  second  relative ranking gets to the
heart of whether one planner might be considered to be an improvement over another 
we tested values of           and    for n     is half of the domains at our disposal  
to give a sense of the variability in size  at n      the most problems solved in a trial
varied from    to     to assess the changes in rankings across the trials  we computed rank
dominance for all pairs of planners  rank dominance is defined as the number of trials in
which planner x s rank was lower than planner y s  note  ties would count toward neither
planner   the    planners in our study resulted in    dominance pairings  if the relative
ranking between two planners is stable  then one would expect one to always dominate the
other  i e   have rank dominance of    
table   shows the number of pairs having each value        of rank dominance for the
four values for n  for a given pair  we used the highest number as the rank dominance for
the pair  e g   if one always has a lower rank  then the pair s rank dominance is    or if
both have five  then it is five  because of ties  the maximum can be less than five  the
data suggest that even when picking half of the domains  the rankings are not completely
stable  in     of the pairings  one always dominates  but     have a     or greater chance
of switching relative ranking  the values degrade as n decreases with only     always
dominating for n     

problem assumption    how do syntactic representation differences affect
performance  although it is well known that some planners  performance depends on

representation  joslin   pollack        srinivasan   howe         two recent developments
in planner research suggest that the effect needs to be better understood  first  a common
representation  i e   pddl  may bias performance  some planners rely on a pre processing
step to convert pddl to their native representation  a step that usually requires making
arbitrary choices about ordering and coding  second  an advantage of planners based on
graphplan is that they are supposed to be less vulnerable to minor changes in representa  

fihowe   dahlman

planner
a
b
c
d
e
f
g
h
i
j
k
l
m

all none subset
      
  
      
  
      
  
       
  
       
   
       
   
      
  
      
  
       
   
       
   
      
  
       
  
       
  

table    the number of problems for which the planners were able to solve all  none or
only a subset of the permutations 
tion  although the reasoning for the claim is sound  the exigencies of implementation may
require re introduction of representation sensitivity 
to evaluate the sensitivity to representation  ten permutations of each problem in the
aips     set were generated  resulting in      permuted problems  the permutations were
constructed by randomly reordering the preconditions in the operator definitions and the
order of the definitions of the operators within the domain definition 
we limited the number of problems in this study because ten permutations of all problems would be prohibitive  we selected the aips     problems for attention because this
was the most recently developed benchmark set  even within that set  not all of the domains
were permuted because some would not result in different domains under the transformation we used  for the purposes of this investigation  we limited the set of modifications to
permutations of preconditions and operators because these were known to affect some planners and because practical considerations limited the number of permutations that could be
executed  finally  for expediency  we ran the permutations on a smaller number of faster
platforms because it expedited throughput and computation time was not a factor in this
study 
to analyze the data  we divided the performance on the permutations of the problems
into three groups based on whether the planner was able to solve all of the permutations 
none of the permutations or only a subset of the permutations  if a planner is insensitive to
the minor representational changes  then the subset count should be zero  from the results
in table    we can see that all of the planners were affected by the permutation operation 
the susceptibility to permuting the problem was strongly planner dependent              
p            demonstrating that some planners are more vulnerable than others 
by examining the number in the subset column  one can assess the degree of susceptibility  all of the planners were sensitive to reorderings  even those that relied on graphplan
  

fia critical assessment of benchmark comparison in planning

 
 
   
   
   
   
 
 
   
   
 
  
   

  
 
   
   
   
   
 
  
   
   
 
  
   

 
 
   
   
   
   
 
 
   
   
 
  
   

 
 
 
 
 
 
 
 
 
 
 
 
 

   
   
   
   
   
   
   
   
   
   
   
   
   

 
 
   
   
   
   
 
  
   
   
 
  
   

pre 
 

 
 
   
   
   
   
 
 
   
   
 
  
   

pre 
safety
strips
typing

 
 
 
 
 
 
 
 
 
 
 
 
 

 

a
b
c
d
e
f
g
h
i
j
k
l
m

feature

axioms
cond  eff 
dis  pre 
equality

planner

 
 
   
   
   
   
 
 
   
   
 
  
   

table    the number of problems claiming to require each pddl feature solved by each
planner 

methodology  the most sensitive were e  f  i and j  which included some graphplan based
planners and in which     of the problems had mixed results on the permutations  with c
and l being least sensitive       were affected  

problem assumption    does performance depend on pddl requirements
features  the planners were all intended to handle strips problems  some of the

problems in the test set claim to require features other than strips  one would expect
that some of the planners would not be able to handle those problems  in addition  those
planners that claim to be able to handle a given feature may not do as well as other planners 
table   shows the effects of feature requirements on the ability to solve problems  the data
in this table are based on the features specified with the  requirements list in the pddl
definition of the domain 
we did not verify that the requirements were accurate or necessary  thus  the problem
may be solvable by ignoring a part of the pddl syntax that is not understood  or the
problem may have been mislabeled by its designer  this is evident in cases where a planner
that does not support a given feature still appears to be able to solve the corresponding
problem  some planners  e g   older versions of stan  will reject any problem that requires
more than strips without trying to solve it  an adl problem that only makes use of
strips features would not be attempted 
as guidance on which planner to use when  these results must be viewed with some
skepticism  for example  it would appear based on these results that planner i might be
  

fihowe   dahlman

a good choice for problems with conditional effects as it was able to solve many of these
problems  this would be a mistake  since that planner cannot actually handle these types
of problems  in these cases  the problems claim to require adl  but in fact  they only make
use of the strips subset 
clearly  certain problems can only be solved by specific planners  for instance  c and
m are the only planners that are able to handle safety constraints  while based on the data 
only c  d and e appear to handle domain axioms  about half the planners had trouble
with the typed problems  some of the gaps appear to be due to problems in the translation
to native representation 

    planners

publicly available  general purpose planners tend to be large programs developed over a
period of years and enhanced to include additional features over time  thus  several versions
are likely to be available  and those versions are likely to have features that can be turned
on off via parameter settings 
when authors release later versions of their planning systems  the general assumption is
that these newer versions will outperform their predecessors  however  this may not be the
case in practice  for instance  a planner could be better optimized toward a specific class
of problem which then in turn hurts its performance on other problems  also  advanced
capabilities  even when unused  may incur overhead in the solution of all problems 
so for comparison purposes  should one use the latest version  first  we tested this
question in a study comparing multiple versions of four of the planners  second  each
planner relies on parameter settings to tune its performance  some  such as blackbox  have
many parameters  others have none  comparisons tend to use the default or published
parameter settings because few people usually understand the effects of the parameters
and tuning can be extremely time consuming  so does this practice undermine a fair
comparison 
planner assumption    is the latest version the best  in this study  we compared
performance of multiple versions of four planners  labeled for this section with w  x  y and
z  with larger version numbers indicating subsequent versions   we considered two criteria
for improvement  outcome of planning and computation time for solved problems  the
outcome of planning is one of  solved  failed or timed out  on each criterion  we statistically
analyzed the data for superior performance of one of the versions  the outcome results for
all the planners are summarized in table    as the table shows  rarely does a new version
result in more problems being solved  only z improved the number of our test problems
solved in subsequent versions 
to check for whether the differences in outcome are significant  we ran  x    tests with
planner version as independent variable and outcome as dependent  table   summarizes
the results of the   analysis  for z  we compared each version to its successor only  the
differences are significant except for y and the transition from z   to    this was expected
because these two versions were extremely similar  
another planner performance metric  which we evaluated  was the speed of solution  for
this analysis  we limited the comparison to just those problems that were solved by both
versions of the planner  we then classified each problem by whether the later version solved
  

fia critical assessment of benchmark comparison in planning

planner version solved failed timeout  solved 
w
 
   
   
   
w
 
   
    
   
 
x
 
   
   
 
x
 
   
   
   
 
y
 
   
   
   
y
 
   
   
   
 
z
 
   
    
   
z
 
   
   
   
 
z
 
   
   
   
 
z
 
   
   
   
 
table    version performance  counts of outcome and change in number solved 
old
new
planner version version  
p
w
 
 
            
x
 
 
           
y
 
 
   
   
z
 
 
          
z
 
 
         
z
 
 
           
table      results comparing versions of the same planner 
the problem faster  slower  or in the same time as the preceding version  from the results
in table    we see that all of the planners improved in the average speed of solution for
subsequent versions  with the exception of z  transition from the   to   versions   however 
z did increase the number of problems solved between those versions 
planner old new faster slower same total
w
 
 
   
  
  
   
x
 
 
   
   
 
   
y
 
 
   
  
  
   
z
 
 
  
   
  
   
z
 
 
   
  
  
   
z
 
 
   
  
  
   
table    improvements in execution speed across versions  the faster column counts the
number of cases in which the new version solved the problem faster  slower specifies
those cases in which the new version took longer to solve a given problem 
  

fihowe   dahlman

planner assumption    do parameter settings matter to a fair comparison 

in this planner set  only three have obvious  easily manipulable parameters  blackbox  hsp
and ucpop  blackbox has an extensive set of parameters that control everything from
how much trace information to print to the sequence of solver applications  hsp s function
can be varied to include  or not  loop detection  change the search heuristic and vary the
number paths to expand  for ucpop  the user can change the strategies governing node
orderings and aw selection 
we did not run any experiments for this assumption because not all of the planners
have parameters and because it is clear from the literature that the parameters do matter 
blackbox relies heavily on random restarts and trying alternative sat solvers  in kautz
and selman         the authors of blackbox carefully study aspects of blackbox s design and
demonstrate differential performance using different sat solvers  they propose hypotheses
for the performance differences and are working on better models of performance variation 
at the heart of hsp is heuristic search  thus  its performance varies depending on
the heuristics  experiments with both hsp and ff  a planner that builds on some ideas
from hsp  have shown the importance of heuristic selection in search space expansion 
computation time and problem scale up  haslum   geffner        hoffmann   nebel 
      
as with hsp  heuristic search is critical to ucpop s performance  a set of studies have
explored alternative settings to the aw selection heuristics employed by ucpop  joslin  
pollack        srinivasan   howe        genevini   schubert         producing dramatic
improvements on some domains with some heuristics  as pollack et al         confirmed 
a good default strategy could be derived  but its performance was not the best under some
circumstances 
thus  because parameters can control fundamental aspects of algorithms  such as their
search strategies  the role of parameters in comparisons cannot be easily dismissed 

planner assumption    are time cut offs unfair  planners often do not admit

to failure  instead  the planner stops when it has used the allotted time and not found
a solution  so setting a time threshold is a requirement of any planner execution  in
a comparison  one might always wonder whether enough time was allotted to be fair  
perhaps the solution was almost found when execution was terminated 
to determine whether our cut off of    minutes was fair  we examined the distribution
of times for declared successes and failures   across the planners and the problem set  we
found that the distributions were skewed  approximately log normal with long right tails 
and that the planners were quick to declare success or failure  if they were going to do so 
table    shows the max  mean  median and standard deviation for success and failure times
for each of the planners  the differences between mean and median indicate the distribution
skew  as do the low standard deviations relative to the observed max times  the max time
shows that on rare occasions the planners might make a decision within   minutes of our
cut off 
   we separated the two because we usually observed a significant difference in the distributions of time to
succeed and time to fail   about half the planners were quick to succeed and slow to fail  the other half
reversed the relationship 

  

fia critical assessment of benchmark comparison in planning

planner
a
b
c
d
e
f
g
h
i
j
k
l
m

successes
max mean median
          
   
           
   
           
   
          
   
         
   
           
   
           
   
           
   
            
   
           
   
           
    
           
   
           
    

sd
    
     
     
    
     
     
     
    
     
     
     
     
     

failures
max mean median
           
   
           
    
   
   
    
          
   
            
     
           
   
           
    
           
   
    
   
   
         
    
           
    
           
    
   
   
   

sd
     
    
   
     
     
     
    
     
   
    
    
     
   

table     max  mean  median and standard deviations  sd  for the computation times to
success and failure for each planner 

what this table does not show  but the observed distributions do show  is that very
few values are greater than half of the time until the cut off  figures   and   display
the distributions for planner f  which had means in the middle of the set of planners and
quite typical distributions  consequently  at least for these problems  any cut off above   
minutes      seconds  would not significantly change the results 

   

   

   

 
 

                                                  
success time

figure    histogram of times  in seconds  for planner f to succeed 
  

fihowe   dahlman

   

   

   

 
 

  

                                             
fail time

figure    histogram of times  in seconds  for planner f to fail 

    performance metrics

most comparisons emphasize the number of problems solved and the cpu time to completion as metrics  often  the problems are organized in increasing diculty to show scale up 
comparing based on these metrics leaves a lot open to interpretation  for example  some
planners are designed to find the optimal plan  as measured by number of steps in either
a parallel or sequential plan  consequently  these planners may require more computation 
thus  by ignoring plan quality  these planners may be unfairly judged  we also hypothesize
that the hardware and software platform for the tests can vary the results  if a planner is
developed for a machine with  gb of memory  then likely its performance will degrade with
less  a key issue is whether the effect is more or less uniform across the set of planners 
in this section  we examine these two issues  execution platform and effect of plan
quality 

metric assumption    does performance vary between planners when run
on different hardware platforms  often when a planner is run at a competition or

in someone else s lab  the hardware and software platforms differ from the platform used
during development  clearly  slowing down the processor speed should slow down planning 
requiring higher cut offs  reduction in memory may well change the set of problems that
can be solved or increase the processing time due to increased swapping  changing the
hardware configuration may change the way memory is cached and organized  favoring
some planners  internal representations over others  changing compilers could also affect
the amount and type of optimizations in the code  the exact effects are probably unknown 
the assumption is that such changes affect all planners more or less equally 
to test this  we ran the planners on a less powerful  lower memory machine and compared
the results on the two platforms  the base sun ultrasparc        with    mb of memory
and ultrasparc       with    mb of memory  the operating system and compilers were
the same versions for both machines  the same problems were run on both platforms  we
followed much the same methodology as in the comparison of planner versions  comparing
on both number of problems solved and time to solution  table    shows the results as
measured by problems solved  failed or timed out for each planner on the two platforms 
  

fia critical assessment of benchmark comparison in planning

planner platform solved failed timed out  
p   reduction
a
ultra  
  
   
  
ultra   
  
   
           
 
b
ultra  
   
   
  
ultra   
   
   
           
 
c
ultra  
   
 
   
ultra   
   
 
            
 
d
ultra  
   
  
   
ultra   
   
  
             
   
e
ultra  
   
   
  
ultra   
   
   
           
 
f
ultra  
   
   
  
ultra   
   
   
           
 
g
ultra  
   
   
  
ultra   
   
   
           
 
h
ultra  
   
   
  
ultra   
   
   
           
 
i
ultra  
   
   
  
ultra   
   
   
           
 
j
ultra  
   
   
 
ultra   
   
   
          
 
k
ultra  
   
   
  
ultra   
   
   
           
 
l
ultra  
   
   
  
ultra   
   
   
           
 
m
ultra  
   
  
   
ultra   
   
  
            
 
table     number of problems solved  failed and timed out for each planner on the two
hardware platforms  last column is the percentage reduction in the number
solved from the faster to slower platforms 

  

fihowe   dahlman

planner
a
b
c
d
e
f
g
h
i
j
k
l
m

faster
  mean 
  
    
   
    
   
     
   
     
   
    
   
     
   
    
   
    
   
     
   
     
   
     
   
     
   
     

slower
sd    mean 
       
       
         
    
        
    
       
       
       
       
        
        
       
       
         
    

sd 
    
    

    

same total
 
 
 
 
 
 
 
 
 
 
 
 
 

  
   
   
   
   
   
   
   
   
   
   
   
   

table     improvements in execution speed moving from slower to faster platform  counts
only problems that were solved on both platforms  for faster and slower  the
mean and standard deviation  sd  of difference is also provided 
as before  we also looked at change in time to solution  table    shows how the time
to solution changes for each planner  not surprisingly  faster processor and more memory
nearly always lead to better performance  somewhat surprisingly  the difference is far less
than the doubling that might be expected  the mean differences are much less than the
mean times on the faster processor  see table    for the mean solution times  
also  the effect seems to vary between the planners  based on the counts  the lisp based
planners appear to be less susceptible to this trend  the only ones that sometimes were faster
on the slower platform   however  the advantages are very small  affecting primarily the
smaller problems  we think that this effect is due to the need to load in a lisp image
at startup from a centralized server  thus  computation time for small problems will be
dominated by any network delay  older versions of planners appear to be less sensitive to
the switch in platform 
in this study  the platforms make little difference to the results  despite a more than
doubling of processor speed and doubling of memory  however  the two platforms are
underpowered when compared to the development platforms for some of the planners  we
chose these platforms because they differed in only a few characteristics  processor speed
and memory amount  and because we had access to    identically configured machines  to
really observe a difference   gb  of memory or more may be needed 
recent trends in planning technology have exploited cheap memory  translations to
propositional representations  compilation of the problems and built in caching and memory
management techniques  thus  some planners are designed to trade off memory for time 
   we propose this figure because it is the amount requested by some of the participants in the aips     
planning competition 

  

fia critical assessment of benchmark comparison in planning

these planners will understandably be affected by memory limitations for some problems 
given the results of this study  we considered performing a more careful study of memory
by artificially limiting memory for the planners but did not do so because we did not have
access to enough suciently large machines to likely make a difference and because we could
not devise a scheme for fairly doing so across all the planners  which are implemented in
different languages and require different software run time environments  
another important factor may be memory architecture management  some planners
include their own memory managers  which map better to some hardware platforms than
to others  e g   hsp uses a linear organization that appears to fit well with intel s memory
architecture  

metric assumption    do the number of plan steps vary  several researchers

have examined the issue of measuring plan quality and directing planning based on it  e g  
 perez        estlin   mooney        rabideau  englehardt    chien         the number
of steps in a plan is a rather weak measure of plan quality  but so far  it is the only one
that has been widely used for primitive action planning 
we expect that some planners sacrifice quality  as measured by plan length  for speed 
thus  ignoring even this measure of plan quality may be unfair to some planners  to
check whether this appears to be a factor in our problem set  we counted the plan length
in the plans returned in output and compared the lengths across the planners  because
not all of the planners construct parallel plans  we adopted the most general definition 
sequential plan length  we then compared the plan lengths returned by each planner on
every successfully solved problem 
we found that     of the problems were solved by only one planner  not necessarily the
same one   the planners found equal length solutions for     of those that remained     
problems   we calculated the standard deviation  sd  of plan length for solutions to each
problem and then analyzed the sds  we found that the minimum observed sd was      
the maximum was        the mean was      and the standard deviation was       thirteen
cases showed sds higher than     obviously  these cases involved fairly long plans  up to
    steps   the cases were for problems from the logistics and gripper domains 
to check whether some planners favored minimal lengths  we counted the number of
cases in which each planner found the shortest length plan  ties were attributed to all
planners  when there was some variance in plan length  table    lists the results  most
planners find the shortest length plans on about one third of these problems  planner f
was designed to optimize plan length  which shows in the results  with one exception  the
older planners rarely find the shortest plans 

   interpretation of results and recommendations
the previous section presented our summarization and analysis of the planner runs  in
this section  we reect on what those results mean for empirical comparison of planners  we
summarize the results and recommend some partial solutions  it is not possible to guarantee
fairness and we propose no magic formula for performing evaluations  but the state of the
practice in general can certainly be improved  we propose three general recommendations
and    recommendations targeted to specific assumptions 
  

fihowe   dahlman

planner count
a
   
b
   
c
 
d
   
e
 
f
   
g
   
h
   
i
   
j
 
k
   
l
   
m
   
table     number of plans on which each planner found the shortest plan  the data only
include problems for which different length plans were found 
many of the targeted recommendations amount to requesting problem and planner developers to be more precise about the requirements for and expectations of their contributions 
because the planners are extremely complex and time consuming to build  the documentation may be inadequate to determine how a subsequent version differs from the previous or
under what conditions  e g   parameter settings  problem types  the planner can be fairly
compared  with the current positive trend in making planners available  it behooves the
developer to include such information in the distribution of the system 
the most sweeping recommendation is to shift the research focus away from developing
the best general purpose planner  even in the competitions  some of the planners identified
as superior have been ones designed for specific classes of problems  e g   ff and ipp  the
competitions have done a great job of exciting interest and encouraging the development
and public availability of planners that incorporate the same representation 
however  to advance the research  the most informative comparative evaluations are
those designed for a specific purpose   to test some hypothesis or prediction about the
performance of a planner    an experimental hypothesis focuses the analysis and often
leads naturally to justified design decisions about the experiment itself  for example  hoffmann and nebel  the authors of the fast forward  ff  system  state in the introduction to
their jair paper that ff s development was motivated by a specific set of the benchmark
domains  because the system is heuristic  they designed the heuristics to fit the expectations needs of those domains  hoffmann   nebel         additionally  in part of their
evaluation  they compare to a specific system on which their own system had commonalities
and point out the various advantages or disadvantages of their design decisions on specific
    paul cohen has advocated such an experimental methodology for all of artificial intelligence based on
hypotheses  predictions and models in considerable detail  see cohen              

  

fia critical assessment of benchmark comparison in planning

problems  follow up work or researchers comparing their own systems to ff now have a
well defined starting point for any comparison 

recommendation    experiments should be driven by hypotheses  re 

searchers should precisely articulate in advance of the experiments their expectations about how their new planner or augmentations to an existing planner add
to the state of the art  these expectations should in turn justify the selection
of problems  other planners and metrics that form the core of the comparative
evaluation 
a general issue is whether the results are accurate  we reported the results as they are
output by the planners  if a planner stated in its output that it had been successful  we
took it at face value  however  by examining some of the output  we determined that some
claims of successful solution were erroneous   the proposed solution would not work  the
only way to ensure that the output is correct is with a solution checker  drew mcdermott
used a solution checker in the aips   competition  however  the planners do not all
provide output in a compatible format with his checker  thus  another concern with any
comparative evaluation is that the output needs to be cross checked  because we are not
declaring a winner  i e   that some planner exhibited superior performance   we do not think
that the lack of a solution checker casts serious doubt on our results  for the most part  we
have only been concerned with factors that cause the observed success rates to change 

recommendation    just as input has been standardized with pddl  output
should be standardized  at least in the format of returned plans 

another general issue is whether the benchmark sets are representative of the space of
interesting planning problems  we did not test this directly  in fact  we are not sure how
one could do so   but the clustering of results and observations by others in the planning
community suggest that the set is biased toward logistics problems  additionally  many of
the problems are getting dated and no longer distinguish performance  some researchers
have begun to more formally analyze the problem set  either in service of building improved
planners  e g   hoffmann   nebel        or to better understand planning problems  for
example  in the related area of scheduling  our group has identified distinctive patterns in
the topology of search spaces for different types of classical scheduling problems and has
related the topology to performance of algorithms  watson  beck  barbulescu  whitley   
howe         within planning  hoffmann has examined the topology of local search spaces
in some of the small problems in the benchmark collection and found a simple structure
with respect to some well known relaxations  hoffmann         additionally  he has worked
out a partial taxonomy  based on three characteristics  for the analyzed domains  helmert
has analyzed the computational complexity of a subclass of the benchmarks  transportation
problems  and has identified key features that affect the diculty of such problems  helmert 
      

recommendation    the benchmark problem sets should themselves be eval 

uated and over hauled  problems that can be easily solved should be removed 
researchers should study the benchmark problems domains to classify them
  

fihowe   dahlman

into problem types and key characteristics  developers should contribute application problems and realistic versions of them to the evolving set 
the remainder of this section describes other recommendations for improving the state
of the art in planner comparisons 

problem assumption    are general purpose planners biased toward particular problems domains  the set of problems on which a planner was developed
can have a strong effect on the performance of the planner  this can be either the effect
of unintentional over specialization or the result of a concerted effort on the part of the
developers to optimize their system to solve a specific problem  with one exception  every
planner fared better on the tailored subset of problems  training set   consequently  we
must conclude that the choice of a subset of problems may well affect the outcome of any
comparison 
a fair planner comparison must account for likely biases in the problem set  good
performance on a certain class of problems does not imply good performance in general 
a large performance differential for planners with a targeted problem domain  i e   do well
on their focus problems and poorly on others  may well indicate that the developers have
succeeded in optimizing the performance of their planner 
recommendation    problem sets should be constructed to highlight the
designers  expectations about superior performance for their planner  and they
should be specific about this selection criteria 
on the other hand  if the goal is to demonstrate across the board performance  then
our results at randomly selecting domains suggests that biases can be mitigated 
recommendation    if highlighting performance on  general  problems is
the goal  then the problem set should be selected randomly from the benchmark
domains 

problem assumption    how do syntactic representation differences affect
performance  many studies  including this  have shown that planners may be sensitive

to representational features  just because representations can be translated automatically
does not mean that performance will be unaffected  just because an algorithm should
theoretically be insensitive to a factor does not mean that in practice it is  all of the
planners showed some sensitivity to permuted problems  and the degree of sensitivity varied 
this outcome suggests that translators and even minor variations on problem descriptions
impact outcome and should be used with care  especially when the sensitivity is not the
focus of the study and some other planner is more vulnerable to the effect 
recommendation    representation translators should be avoided by using
native versions of problems and testing multiple versions of problems if necessary 
with many planner developers participating in the aips competitions  this should become
less of an issue 
more importantly  researchers should be explicitly testing the effect of alternative phrasings of planning problems to determine the sensitivity of performance and to separate the
effects of advice tuning from the essence of the problem 
  

fia critical assessment of benchmark comparison in planning

recommendation    studies should consider the role of minor syntactic vari 

ations in performance and include permuted problems  i e   initial conditions 
goals  preconditions and actions  in their problem sets because they can demonstrate robustness  provide an opportunity for learning and protect developers
from accidentally over fitting their algorithm to the set of test problems 

problem assumption    does performance depend on pddl requirements
features  the planners did not perform quite as advertised or expected given some

problem features  this discrepancy could have many possible causes  problems incorrectly
specified  planners with less sensitivity than thought  solutions not being correct  etc  for
example  many of the problems in the benchmark set were not designed for the competitions
or even intended to be widely used and so may not have been specified carefully enough 

recommendation    when problems are contributed to the benchmark set 
developers should verify that the requirements stated in the description of each
problem correctly reect the subset of features needed  planner evaluators
should then use only those problems that match a planner s capabilities 

depending on the cause  the results can be skewed  e g   a planner may be unfairly
maligned for being unable to solve a problem that it was specifically designed not to solve 
the above recommendation addresses gaps in the specification of the problem set  but some
mismatches between the capabilities specifiable in pddl and those that planners possess
remain 

recommendation    planner developers should develop a vocabulary for
their planner s capabilities  as in the pddl ags  and specify the expected
capabilities in the planner s distribution 

planner assumption    is the latest version the best  our results suggest that

new versions run faster  but often do not solve more problems  thus  the newest version may
not represent the  best   depending on your definition  performance for the class of planner 
some competitions in other fields  e g   the automatic theorem proving community  require
the previous year s best performer to compete as well  this has the advantage of establishing
a baseline of performance as well as allowing a comparison to how the focus may shift over
time 

recommendation     if the primary evaluation metric is speed  then a newer

version may be the best competition  if it is number of problems solved or if one
wishes to establish what progress has been made  then it may be worth running
against an older version as well  if recommendation   has been followed  then
evaluators should select a version based on this guidance 

planner assumption    what are the effect of parameter settings  perfor 

mance of some planners does vary with the parameter settings  unfortunately  it often is
dicult to figure out how to set the parameters properly  and changing settings makes it
dicult to compare results across experiments  generally  this is not an issue because the
  

fihowe   dahlman

developers and other users tend to rely on the default parameter settings  unfortunately 
sometimes the developers exploit alternative settings in their own experiments  complicating
later comparison 

recommendation     if a planner includes parameters  the developer should
guide users in their settings  if they do not  then the default settings should be
used by both the developers and others in experiments to facilitate comparison 

planner assumption    are time cut offs unfair  we found little benefit from
increasing time cut offs beyond    minutes for our problems 

recommendation     if total computation time is a bottleneck  then run the

problems in separate batches  incrementally increasing the time cut off between
runs and including only unresolved problems in subsequent runs  when no
additional problems are solved in a run  stop 

metric assumption    do alternative platforms lead to different performance  in our experiments  performance did not vary as much as we expected  this
result suggests that researchers in general are not developing for specific hardware software
configurations  but recent trends suggest otherwise  at least with regards to memory  again 
because these systems are research prototypes  it behooves the developer to be clear about
his her expectations and anyone subsequently using the system to accommodate those requests in their studies 

recommendation     as with other factors in planner design  researchers

must clearly state the hardware software requirements for their planners  if the
design is based on platform assumptions  additionally  a careful study of memory versus time trade offs should be undertaken  given the recent trends in memory exploitation 

metric assumption    do the number of plan steps vary  they certainly can 

if one neglects quality measures  then some planners are being penalized in efforts to declare
a best planner 

recommendation     to expedite generalizing across studies  reports should
describe performance in terms of what was solved  how many of what types  
how much time was required and what were the quality of the solutions  tradeoffs should be reported  when possible  e g       increase in computation time
for     decrease in plan length  additionally  if the design goal was to find an
optimal solution  compare to other planners with that as their design goal 

good metrics of plan quality are sorely needed  the latest specification of the pddl
specification supports the definition of problem specific metrics  fox   long         these
metrics indicate whether total time  a new concept supported by specification of action
durations  or specified functions should be minimized or maximized  this addition is an
excellent start  but general metrics other than just plan length and total time are also
needed to expedite comparisons across problems 
  

fia critical assessment of benchmark comparison in planning

recommendation     developing good metrics is a valuable research contri 

bution  researchers should consider it a worthwhile project  conference organizers and reviewers should encourage papers on the topic  and planner developers
should implement their planners to be responsive to new quality metrics  i e  
support tunable heuristics or evaluation criteria  

   conclusions

fair evaluation and comparison of planners is hard  many apparently benign factors exert
significant effects on performance  superior performance of one planner over another on
a problem that neither was intentionally designed to solve may be explained by minor
representational features  however  comparative analysis on general problems is of practical
importance as it is not practical to create a specialized solution to every problem 
we have analyzed the effects of experiment design decisions in empirical comparison of
planners and made some recommendations for ameliorating the effects of these decisions 
most of the recommendations are common sense suggestions for improving the current
methodology 
to expand beyond the current methodology will require at least two substantive changes 
first  the field needs to question whether we should be trying to show performance on
planning problems in general  a shift from general comparisons to focused comparisons  on
problem class or mechanism or on hypothesis testing  could produce significant advances in
our understanding of planning 
second  the benchmark problem sets require attention  many of the problems should be
discarded because they are too simple to show much  the domains are far removed from
real applications  it may be time to revisit testbeds  for example  several researchers in
robotics have constructed an interactive testbed for comparing motion planning algorithms
 piccinocchi  ceccarelli  piloni    bicchi         the testbed consists of a user interface for
defining new problems  a collection of well known algorithms and a simulator for testing
algorithms on specific problems  thus  the user can design his her own problems and compare performance of various algorithms  including their own  on them via a web site  such a
testbed affords several advantages over the current paradigm of static benchmark problems
and developer conducted comparisons  in particular  replicability and extendability of the
test set  alternatively  challenging problem sets can be developed by modifying deployed
applications  wilkins   desjardins        engelhardt  chien  barrett  willis    wilklow 
      
in recent years  the planning community has significantly improved the size of planning
problems that can be solved in reasonable time and has advanced the state of the art in
empirical comparison of our systems  to interpret the results of empirical comparisons
and understand how they should motivate further development in planning  the community
needs to understand the effects of the empirical methodology itself  the purpose of this
paper is to further that understanding and initiate a dialogue about the methodology that
should be used 

  

fihowe   dahlman

acknowledgments
this research was partially supported by a career award from the national science
foundation iri         and by a grant from air force oce of scientific research f                the u s  government is authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copyright notation thereon  we are most
grateful to the reviewers for the careful reading of and well considered comments on the
submitted version  we hope we have done justice to your suggestions 

references

bacchus 
f 
       
aips     
planning
competition 
http   www cs toronto edu aips     selfcontainedaips      ppt 
barrett  a   golden  k   penberthy  s     weld  d          ucpop user s manual  dept 
of computer science and engineering  university of washington  seattle  wa  tr
         
blum  a  l     furst  m  l          fast planning through planning graph analysis  artificial
intelligence journal                    
bonet  b     geffner  h          planning as heuristic search  new results  in proceedings
of the fifth european conference on planning  ecp     durham  uk 
chien  s   kambhampati  s     knoblock  c  a   eds          proceedings of the fifth
international conference on artificial intelligence planning and scheduling  aips
       aaai press  breckenridge  co 
cohen  p  r          a survey of the eighth national conference on artificial intelligence 
pulling together or pulling apart  ai magazine                
cohen  p  r          empirical methods for artificial intelligence  mit press 
drummond  m  e   kaelbling  l  p     rosenschein  s  j          collected notes from the
benchmarks and metrics workshop  artificial intelligence branch fia        nasa
ames research center 
engelhardt  b   chien  s   barrett  t   willis  j     wilklow  c          the data chaser
and citizen explorer benchmark problem sets  in proceedings of the sixth european
conference on planning  ecp     toledo  spain 
estlin  t  a     mooney  r  j          learning to improve both ecicency and quality of
planning  in proceedings of the fifteenth international joint conference on artificial
intelligence  pp             nagoya  japan 
fox  m     long  d          the ecient implementation of the plan graph in stan 
journal of artificial intelligence research             
fox  m     long  d          pddl     an extension to pddl for expressing temporal
planning domains  available at http   www dur ac uk d p long pddl  ps gz 
  

fia critical assessment of benchmark comparison in planning

genevini  a     schubert  l          accelerating partial order planners  some techniques
for effective search control and pruning  journal of artificial intelligence research    
       
hanks  s   nguyen  d     thomas  c          a beginner s guide to the truckworld simulator  dept  of computer science and engineering uw cse tr           university
of washington 
hanks  s   pollack  m  e     cohen  p  r          benchmarks  test beds  controlled
experimentation and the design of agent architectures  ai magazine        
haslum  p     geffner  h          admissible heuristics for optimal planning  in proceedings of the fifth international conference on artificial intelligence planning and
scheduling  aips        pp           breckenridge  co  aaai press 
helmert  m          on the complexity of planning in transportation domains  in  th
european conference on planning  ecp      lecture notes in artificial intelligence 
new york  springer verlag 
hoffmann  j          local search topology in planning benchmarks  an empirical analysis 
in proceedings of the   th international joint conference on artificial intelligence
seattle  wa  usa 
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research              
howe  a  e   dahlman  e   hansen  c   scheetz  m     von mayrhauser  a          exploiting competitive planner performance  in proceedings of the fifth european conference
on planning  durham  uk 
howe  a  e   von mayrhauser  a     mraz  r  t          test case generation as an ai
planning problem  automated software engineering                
joslin  d     pollack  m          least cost aw repair  a plan refinement strategy for
partial order planning  in proceedings of the twelfth national conference on artificial
intelligence  pp             seattle  wa 
kautz  h     selman  b          blackbox  a new approach to the application of
theorem proving to problem solving  in working notes of the aips   workshop on
planning as combinatorial search  pittsburgh  pa 
kautz 
h 
blackbox 
a sat technology planning system 
http   www cs washington edu homes kautz blackbox index html 
kautz  h     selman  b          unifying sat based and graph based planning  in proceedings of the sixteenth international joint conference on artificial intelligence  stockholm  sweden 
koehler  j          handling of conditional effects and negative goals in ipp  tech  rep 
     institute for computer science  albert ludwigs university  freiburg  germany 
  

fihowe   dahlman

koehler  j   nebel  b   hoffmann  j     dimopoulos  y          extending planning graphs
to an adl subset  in proceedings of the fourth european conference in planning 
mcdermott  d   ghallab  m   howe  a   knoblock  c   ram  a   veloso  m   weld  d    
wilkins  d          the planning domain definition language 
mcdermott  d          the      ai planning systems competition  ai magazine         
      
penberthy  j  s     weld  d  s          ucpop  a sound  complete  partial order planner
for adl  in proceedings of the third international conference on knowledge representation and reasoning  pp          
perez  m  a          learning search control knowledge to improve plan quality  ph d 
thesis  carnegie mellon university 
piccinocchi  s   ceccarelli  m   piloni  f     bicchi  a          interactive benchmark for
planning algorithms on the web  in proceedings of ieee international conference on
robotics and automation 
pollack  m  e     ringuette  m          introducing the tileworld  experimentally evaluating agent architectures  in proceedings of the eight national conference on artificial
intelligence  pp           boston  ma 
pollack  m   joslin  d     paolucci  m          flaw selection strategies for partial order
planning  journal of artificial intelligence research             
rabideau  g   englehardt  b     chien  s          using generic prferences to incrementally
improve plan quality  in proceedings of the fifth international conference on artificial
intelligence planning and scheduling  aips        breckenridge  co 
slaney  j     thiebaux  s          blocks world revisited  artificial intelligence journal 
                   
srinivasan  r     howe  a  e          comparison of methods for improving search eciency
in a partial order planner  in proceedings of the   th international joint conference
on artificial intelligence  pp             montreal  canada 
sussman  g  a          a computational model of skill acquisition  tech  rep  memo no 
ai tr      mit ai lab 
the prodigy research group         prodigy      the manual and tutorial  school of
computer science         carnegie mellon university 
watson  j   barbulescu  l   howe  a     whitley  l  d          algorithm performance and
problem structure for ow shop scheduling  in proceedings of the sixteenth national
conference on artificial intelligence  aaai      orlando  fl 
watson  j   beck  j   barbulescu  l   whitley  l  d     howe  a          toward a descriptive model of local search cost in job shop scheduling  in proceedings of sixth
european conference on planning  ecp      toledo  spain 
  

fia critical assessment of benchmark comparison in planning

weld  d   anderson  c     smith  d          extending graphplan to handle uncertainty
and sensing actions  in proceedings of the fifteenth national conference on artificial
intelligence madison  wi 
weld  d  s          recent advances in ai planning  ai magazine                 
wilkins  d  e     desjardins  m          a call for knowledge based planning  ai magazine 
               

  

fi