journal of artificial intelligence research                  

submitted       published      

a unified model of structural organization in
language and music
rens bod

rens illc uva  nl

institute for logic  language and computation
university of amsterdam  nieuwe achtergracht    
     wv amsterdam  the netherlands  and
school of computing  university of leeds
ls   jt leeds  uk

abstract
is there a general model that can predict the perceived phrase structure in language and
music  while it is usually assumed that humans have separate faculties for language and
music  this work focuses on the commonalities rather than on the differences between these
modalities  aiming at finding a deeper  faculty   our key idea is that the perceptual system
strives for the simplest structure  the  simplicity principle    but in doing so it is biased by the
likelihood of previous structures  the  likelihood principle    we present a series of dataoriented parsing  dop  models that combine these two principles and that are tested on the
penn treebank and the essen folksong collection  our experiments show that     a
combination of the two principles outperforms the use of either of them  and     exactly the
same model with the same parameter setting achieves maximum accuracy for both language
and music  we argue that our results suggest an interesting parallel between linguistic and
musical structuring 

   introduction  the problem of structural organization
it is widely accepted that the human cognitive system tends to organize perceptual information
into hierarchical descriptions that can be conveniently represented by tree structures  tree
structures have been used to describe linguistic perception  e g  wundt        chomsky 
       musical perception  e g  longuet higgins        lerdahl   jackendoff        and
visual perception  e g  palmer        marr         yet  little attention has been paid to the
commonalities between these different forms of perception and to the question whether there
exists a general  underlying mechanism that governs all perceptual organization  this paper
studies exactly that question  acknowledging the differences between the perceptual
modalities  is there a general model that can predict the perceived tree structure for sensory
input  in studying this question  we will use an empirical methodology  any model that we
might hypothesize will be tested against manually analyzed benchmarks such as the
linguistically annotated penn treebank  marcus et al        and the musically annotated
essen folksong collection  schaffrath         while we will argue for a general model of
structural organization in language  music and vision  we will carry out experiments only with
linguistic and musical benchmarks  since no benchmark of visual tree structures is currently
available  to the best of our knowledge 
figure   gives three simple examples of linguistic  musical and visual information with their
corresponding tree structures printed below  these examples are resp  taken from martin et al 
      lerdahl   jackendoff        and dastani        

     ai access foundation and morgan kaufmann publishers  all rights reserved 

fibod

list the sales of products in     
s
np
np
np
v

dt

pp
n

p

pp
n

p

n

list the sales of products in     

figure    examples of linguistic  musical and visual input with their tree structures
thus  a tree structure describes how parts of the input combine into constituents and how
these constituents combine into a representation for the whole input  note that the linguistic
tree structure is labeled with syntactic categories  whereas the musical and visual tree
structures are unlabeled  this is because in language there are syntactic constraints on how
words can be combined into larger constituents  e g  in english a determiner can be combined
with a noun only if it precedes that noun  which is expressed by the rule np  dt n   while
in music  and to a lesser extent in vision  there are no such restrictions  in principle any note
may be combined with any other note 
apart from these differences  there is also a fundamental commonality  the perceptual input
undergoes a process of hierarchical structuring which is not found in the input itself  the main
problem is thus  how can we derive the perceived tree structure for a given input  that this
problem is not trivial may be illustrated by the fact that the inputs above can also be assigned
the following  alternative tree structures 
s

np
pp

np
v

dt

n

p

pp
n

p

n

list the sales of products in     

figure    alternative tree structures for the inputs in figure  
these alternative structures are possible in that they can be perceived  the linguistic tree
structure in figure   corresponds to a meaning which is different from the tree in figure   
the two musical tree structures correspond to different groupings into motifs  and the two
visual structures correspond to different visual gestalts  but while the alternative tree
structures are all possible  they are not plausible  they do not correspond to the structures that
are actually perceived by the human cognitive system 
the phenomenon that the same input may be assigned different structural organizations is
known as the ambiguity problem  this problem is one of the hardest problems in modeling
human perception  even in language  where a phrase structure grammar may specify which
words can be combined into constituents  the ambiguity problem is notoriously hard  cf 
manning   schtze         charniak            argues that many sentences from the wall
street journal have more than one million different parse trees  the ambiguity problem for
   

fia unified model of structural organization in language and music

musical input is even harder  since there are virtually no constraints on how notes may be
combined into constituents  talking about rhythm perception in music  longuet higgins and
lee        note that  any given sequence of note values is in principle infinitely ambiguous 
but this ambiguity is seldom apparent to the listener   
in the following section  we will discuss two principles that have traditionally been proposed
to solve ambiguity  the likelihood principle and the simplicity principle  in section    we will
argue for a new integration of the two principles within the data oriented parsing framework 
our hypothesis is that the human cognitive system strives for the simplest structure generated
by the shortest derivation  but that in doing so it is biased by the frequency of previously
perceived structures  in section    we go into the computational aspects of our model  in
section    we discuss the linguistic and musical test domains  section   presents an empirical
investigation and comparison of our model  finally  in section    we give a discussion of our
approach and go into other combinations of simplicity and likelihood that have been proposed
in the literature 

   two principles  likelihood and simplicity
how can we predict from the set of all possible tree structures the tree that is actually
perceived by the human cognitive system  in the field of visual perception  two competing
principles have traditionally been proposed to govern structural organization  the first  initiated
by helmholtz         advocates the likelihood principle  perceptual input will be organized
into the most probable structure  the second  initiated by wertheimer        and developed by
other gestalt psychologists  advocates the simplicity principle  the perceptual system is
viewed as finding the simplest rather than the most probable structure  see chater        for
an overview   these two principles have also been used in linguistic and musical structuring 
in the following  we briefly review these principles for each modality 
    likelihood
the likelihood principle has been particularly influential in the field of natural language
processing  see manning and schtze        for a review   in this field  the most appropriate
tree structure of a sentence is assumed to be its most likely structure  the likelihood of a tree
is computed from the probabilities of its parts  e g  phrase structure rules  which are in turn
estimated from a large manually analyzed language corpus  i e  a treebank  state of the art
probabilistic parsers such as collins         charniak        and bod      a  obtain around
    precision and recall on the penn wall street journal treebank  marcus et al        
the likelihood principle has also been applied to musical perception  e g  by raphael       
and bod      b c   as in probabilistic natural language processing  the most probable musical
tree structure can be computed from the probabilities of rules or fragments taken from a large
annotated musical corpus  a musical benchmark which has been used by some models is the
essen folksong collection  schaffrath        
also in vision science  there is a huge interest in probabilistic models  e g  hoffman       
kersten         mumford        has even seen fit to declare the dawning of stochasticity 
unfortunately  no visual treebanks are currently available 
    simplicity
the simplicity principle has a long tradition in the field of visual perception psychology  e g 
restle        leeuwenberg        simon        buffart et al        van der helm         in
   

fibod

this field  a visual pattern is formalized as a constituent structure by means of a visual coding
language based on primitive elements such as line segments and angles  perception is
described as the process of selecting the simplest structure corresponding to the  shortest
encoding  of a visual pattern 
the notion of simplicity has also been applied to music perception  collard et al         use
the coding language of leeuwenberg        to predict the metrical structure for four preludes
from bach s well tempered clavier  more well known in music perception is the theory
proposed by lerdahl and jackendoff         their theory contains two kinds of rules   wellformedness rules  and  preference rules   the role of well formedness rules is to define the
kinds of formal objects  grouping structures  the theory employs  what grouping structures a
listener actually hears  is then described by the preference rules which describe gestaltpreferences of the kind identified by wertheimer         and which can therefore also be seen
as an embodiment of the simplicity principle 
notions of simplicity also exist in language processing  for example  frazier        can be
viewed as arguing that the parser prefers the simplest structure containing minimal
attachments  bod      a  defines the simplest tree structure of a sentence as the structure
generated by the smallest number of subtrees from a given treebank 

   combining likelihood and simplicity
the key idea of the current paper is that both principles play a role in perceptual organization 
albeit rather different ones  the simplicity principle as a general cognitive preference for
economy  and the likelihood principle as a probabilistic bias due to previous perceptual
experiences  informally stated  our working hypothesis is that the human cognitive system
strives for the simplest structure generated by the shortest derivation  but that in doing so it is
biased by the frequency of previously perceived structures  some other combinations of
simplicity and likelihood will be discussed in section     to formally instantiate our working
hypothesis  we first need a model that defines the set of possible structures of an input  in this
paper  we have chosen for a model that defines the set of phrase structures for an input on
the basis of a treebank of previously analyzed input  and which is known as the data oriented
parsing or dop model  see bod        collins   duffy         dop learns a grammar by
extracting subtrees from a given treebank and combines these subtrees to analyze fresh input 
we have chosen dop because     it uses subtrees of arbitrary size  thereby capturing nonlocal dependencies  and     it has obtained very competitive results on various benchmarks
 bod      a b  collins   duffy         in the following  we first review the dop model and
discuss the use of the likelihood and simplicity principles by this approach  next  we show how
these two principles can be combined to instantiate our working hypothesis 
    data oriented parsing
in this section  we illustrate the dop model with a linguistic example  for a rigorous definition
of dop  the reader is referred to bod         we will come back to some musical examples
in section    suppose we are given the following extremely small linguistic treebank of two
trees for resp  she wanted the dress on the rack and she saw the dog with the telescope
 actual treebanks contain tens of thousands of trees  cf  marcus et al        

   

fia unified model of structural organization in language and music

s

s

np

vp

she v

np

she

wanted

on the

pp
np

saw the

np

dress p

vp
v

pp

np
the

vp

np

np

p

dog with the telescope

rack

figure    an example treebank
the dop model can parse a new sentence  e g  she saw the dress with the telescope  by
combining subtrees from this treebank by means of a substitution operation  indicated as   



s

the

vp

np



np

pp
np

p

dress

s

 

with the telescope

she

vp

she

pp
np

v

vp

np

vp

pp
np

v
saw the

saw

p

np

dress with the telescope

figure    parsing a sentence by combining subtrees from figure  
thus the substitution operation combines two subtrees by substituting the second subtree on
the leftmost nonlexical leaf node of the first subtree  the result of which may be combined
with a third subtree  etc    a combination of subtrees that results in a tree structure for the
whole sentence is called a derivation  since there are many different subtrees  of various
sizes  there are typically also many different derivations that produce  however  the same tree 
for instance 



s

np
the

vp

np
she

vp
v
saw

dress

p

vp

np
she

pp
np

s

 

np

vp
v

with the telescope

pp
np

p

np

saw the dress with the telescope

figure    a different derivation which produces the same parse tree
   

fibod

the more interesting case occurs when there are different derivations that produce different
parse trees  this happens when a sentence is ambiguous  for example  dop also produces
the following alternative parse tree for she saw the dress with the telescope 



s
np

vp

she v

v
saw


p

s

 

pp

np

np

with the telescope

np

vp

she v

np

saw
np
the

pp

pp

np
the

dress

dress

p

np

with the telescope

figure    a different derivation which produces a different parse tree
    likelihood dop
in bod         dop is enriched with the likelihood principle to predict the perceived tree
structure from the set of possible structures  this model  which we will call likelihood dop 
computes the most probable tree of an input from the occurrence frequencies of the subtrees 
the probability of a subtree t  p t   is computed as the number of occurrences of t    t    divided
by the total number of occurrences of treebank subtrees that have the same root label as t 
let r t  return the root label of t  then we may write 
p t   

 t 

 t   r t    r  t 

  t   

the probability of a derivation t    tn is computed by the product of the probabilities of its
subtrees ti 
p t    tn    

 i p ti 

as we have seen  there may be different derivations that generate the same parse tree  the
probability of a parse tree t is thus the sum of the probabilities of its distinct derivations  let
tid be the i th subtree in the derivation d that produces tree t  then the probability of t is given
by
p t   

d i p tid 

   

fia unified model of structural organization in language and music

in parsing a sentence s  we are only interested in the trees that can be assigned to s  which we
denote by ts  the best parse tree  tbest  according to likelihood dop is then the tree which
maximizes the probability of ts 
tbest   arg max p ts 
ts

thus likelihood dop computes the probability of a tree as a sum of products  where each
product corresponds to the probability of a certain derivation generating the tree  this
distinguishes likelihood dop from most other statistical parsing models that identify exactly
one derivation for each parse tree and thus compute the probability of a tree by only one
product of probabilities  e g  charniak        collins        eisner         likelihood dop s
probability model allows for including counts of subtrees of a wide range of sizes  everything
from counts of single level rules to counts of entire trees 
note that the subtree probabilities in likelihood dop are directly estimated from their
relative frequencies in the treebank trees  while the relative frequency estimator obtains
competitive results on several domains  bonnema et al        bod      a  de pauw         it
does not maximize the likelihood of the training data  johnson         this is because there
may be hidden derivations which the relative frequency estimator cannot deal with    there
are estimation procedures that do take into account hidden derivations and that maximize the
likelihood of the training data  for example  bod      b  presents a likelihood dop model
which estimates the subtree probabilities by a maximum likelihood re estimation procedure
based on the expectation maximization algorithm  dempster et al         however  since the
relative frequency estimator has so far not been outperformed by any other estimator  see
bod et al      b   we will stick to the relative frequency estimator for the current paper 
    simplicity dop
likelihood dop does not do justice to the preference humans display for the simplest
structure generated by the shortest derivation of an input  in bod      a   the simplest tree
structure of an input is defined as the tree that can be constructed by the smallest number of
subtrees from a treebank  we will refer to this model as simplicity dop  instead of
producing the most probable parse tree for an input  simplicity dop thus produces the parse
tree generated by the shortest derivation consisting of the fewest treebank subtrees 
independent of the probabilities of these subtrees  we define the length of a derivation d 
l d   as the number of subtrees in d  thus if d   t    tn then l d    n  let d t be a derivation
which results in parse tree t  then the best parse tree  tbest  according to simplicity dop is
the tree which is produced by a derivation of minimal length 
tbest   arg min l d ts  
ts

as in section      ts is a parse tree of a sentence s  for example  given the treebank in figure
   the simplest parse tree for she saw the dress with the telescope is given in figure    since
  only if the subtrees are restricted to depth   does the relative frequency estimator coincide with the

maximum likelihood estimator  such a depth   dop model corresponds to a stochastic context free
grammar  it is well known that dop models which allow subtrees of greater depth outperform depth  
dop models  bod        collins   duffy        

   

fibod

that parse tree can be generated by a derivation of only two treebank subtrees  while the
parse tree in figure    and any other parse tree  needs at least three treebank subtrees to be
generated   
the shortest derivation may not be unique  it can happen that different parse trees of a
sentence are generated by the same minimal number of treebank subtrees  also the most
probable parse tree may not be unique  but this never happens in practice   in that case we
will back off to a frequency ordering of the subtrees  that is  all subtrees of each root label
are assigned a rank according to their frequency in the treebank  the most frequent subtree  or
subtrees  of each root label gets rank    the second most frequent subtree gets rank    etc 
next  the rank of each  shortest  derivation is computed as the sum of the ranks of the
subtrees involved  the derivation with the smallest sum  or highest rank  is taken as the final
best derivation producing the final best parse tree in simplicity dop  see bod      a  
we performed one little adjustment to the rank of a subtree  this adjustment averages the
rank of a subtree by the ranks of its own sub subtrees  that is  instead of simply taking the
rank of a subtree  we compute the rank of a subtree as the  arithmetic  mean of the ranks of
all its sub subtrees  including the subtree itself   the effect of this technique is that it
redresses a very low ranked subtree if it contains high ranked sub subtrees 
while simplicity dop and likelihood dop obtain rather similar parse accuracy on the
wall street journal and the essen folksong collection  in terms of precision recall    see
section     the best trees predicted by the two models do not quite match  this suggests that a
combined model  which does justice to both simplicity and likelihood  may boost the accuracy 
    combining likelihood dop and simplicity dop  sl dop and ls dop
the underlying idea of combining likelihood and simplicity is that the human perceptual system
searches for the simplest tree structure  generated by the shortest derivation  but in doing so it
is biased by the likelihood of the tree structure  that is  instead of selecting the simplest tree
per se  our combined model selects the simplest tree from among the n likeliest trees  where n
is our free parameter  there are of course other ways to combine simplicity and likelihood
within the dop framework  a straightforward alternative would be to select the most
probable tree from among the n simplest trees  suggesting that the perceptual system is
searching for the most probable structure only from among the simplest ones  we will refer to
the first combination of simplicity and likelihood  which selects the simplest among the n
likeliest trees  as simplicity likelihood dop or sl dop  and to the second combination
 which selects the likeliest among the n simplest trees  as likelihood simplicity dop or lsdop  note that for n    simplicity likelihood dop is equal to likelihood dop  since there is
only one most probable tree to select from  and likelihood simplicity dop is equal to
simplicity dop  since there is only one simplest tree to select from  moreover  if n gets large 
sl dop converges to simplicity dop while ls dop converges to likelihood dop  by
varying the parameter n  we will be able to compare likelihood dop  simplicity dop and
several instantiations of sl dop and ls dop 

  one might argue that a more straightforward metric of simplicity would return the parse tree with the

smallest number of nodes  rather than the smallest number of treebank subtrees   but such a metric is
known to perform quite badly  see manning   schtze        bod      a  

   

fia unified model of structural organization in language and music

   computational issues
bod        showed how standard chart parsing techniques can be applied to likelihood dop 
each treebank subtree t is converted into a context free rule r where the lefthand side of r
corresponds to the root label of t and the righthand side of r corresponds to the frontier labels
of t  indices link the rules to the original subtrees so as to maintain the subtree s internal
structure and probability  these rules are used to create a derivation forest for a sentence
 using a chart parser    see charniak         and the most probable parse is computed by
sampling a sufficiently large number of random derivations from the forest   monte carlo
disambiguation   see bod         while this technique has been successfully applied to parsing
the atis portion in the penn treebank  marcus et al         it is extremely time consuming 
this is mainly because the number of random derivations that should be sampled to reliably
estimate the most probable parse increases exponentially with the sentence length  see
goodman         it is therefore questionable whether bod s sampling technique can be scaled
to larger domains such as the wall street journal  wsj  portion in the penn treebank 
goodman        showed how likelihood dop can be reduced to a compact stochastic
context free grammar  scfg  which contains exactly eight scfg rules for each node in the
training set trees  although goodman s method does still not allow for an efficient computation
of the most probable parse  in fact  the problem of computing the most probable parse in
likelihood dop is np hard    see sima an         his method does allow for an efficient
computation of the  maximum constituents parse   i e  the parse tree that is most likely to have
the largest number of correct constituents  unfortunately  goodman s scfg reduction method
is only beneficial if indeed all subtrees are used  while maximum parse accuracy is usually
obtained by restricting the subtrees  for example  bod      a  shows that the  optimal 
subtree set achieving highest parse accuracy on the wsj is obtained by restricting the
maximum number of words in each subtree to    and by restricting the maximum depth of
unlexicalized subtrees to    goodman        shows that some subtree restrictions  such as
subtree depth  may be incorporated by his reduction method  but we have found no reduction
method for our optimal subtree set 
in this paper we will therefore use bod s subtree to rule conversion method for likelihooddop  but we will not use bod s monte carlo sampling technique from derivation forests  as
this turned out to be computationally prohibitive  instead  we will use the well known viterbi
optimization algorithm for chart parsing  cf  charniak        manning   schtze        which
allows for computing the k most probable derivations of an input in cubic time  using this
algorithm  we will estimate the most probable parse tree of an input from the        most
probable derivations  summing up the probabilities of derivations that generate the same tree 
although this approach does not guarantee that the most probable parse tree is actually found 
it is shown in bod      a  to perform at least as well as the estimation of the most probable
parse by monte carlo techniques on the atis corpus  moreover  this approach is known to
obtain significantly higher accuracy than selecting the parse tree generated by the single most
probable derivation  bod        goodman         which we will therefore not consider in this
paper 
for simplicity dop  we also first convert the treebank subtrees into rewrite rules just as
with likelihood dop  next  the simplest tree  i e  the shortest derivation  can be efficiently
computed by viterbi optimization in the same way as the most probable derivation  provided
that we assign all rules equal probabilities  in which case the shortest derivation is equal to the
most probable derivation  this can be seen as follows  if each rule has a probability p then the
probability of a derivation involving n rules is equal to p n  and since   p   the derivation with
   

fibod

the fewest rules has the greatest probability  in our experiments in section    we give each
rule a probability mass equal to   r  where r is the number of distinct rules derived by bod s
method  as mentioned in      the shortest derivation may not be unique  in that case we
compute all shortest derivations of an input and then apply our ranking scheme to these
derivations  the ranks of the shortest derivations are computed by summing up the ranks of
the subtrees they involve  the shortest derivation with the smallest sum of subtree ranks is
taken to produce the best parse tree 
for sl dop and ls dop  we compute either n likeliest or n simplest trees by means of
viterbi optimization  next  we either select the simplest tree among the n likeliest ones  for
sl dop  or the likeliest tree among the n simplest ones  for ls dop   in our experiments  n
will never be larger than       

   the test domains
as our linguistic test domain we used the wall street journal  wsj  portion in the penn
treebank  marcus et al         this portion contains approx         sentences that have been
manually annotated with the perceived linguistic tree structures using a predefined set of
lexico syntactic labels  since the wsj has been extensively used and described in the
literature  cf  manning   schtze        charniak        collins        bod      a   we will
not go into it any further here 
as our musical test domain we used the european folksongs in the essen folksong
collection  schaffrath        huron         which correspond to approx        folksongs that
have been manually enriched with their perceived musical grouping structures  the essen
folksong collection has been previously used by bod      b  and temperley        to test
their musical parsers  the current paper presents the first experiments with likelihood dop 
simplicity dop  sl dop and ls dop on this collection  the essen folksongs are not
represented by staff notation but are encoded by the essen associative code  esac   the
pitch encodings in esac resemble  solfege   scale degree numbers are used to replace the
movable syllables  do    re    mi   etc  thus   corresponds to  do     corresponds to  re   etc 
chromatic alterations are represented by adding either a     or a  b  after the number  the
plus       and minus       signs are added before the number if a note falls resp  above or
below the principle octave  thus       and    refer al to  do   but on different octaves  
duration is represented by adding a period or an underscore after the number  a period      
increases duration by     and an underscore       increases duration by       more than
one underscore may be added after each number  if a number has no duration indicator  its
duration corresponds to the smallest value  thus pitches in esac are encoded by integers
from   to   possibly preceded or followed by symbols for octave  chromatic alteration and
duration  each pitch encoding is treated as an atomic symbol  which may be as simple as    
or as complex as          a pause is represented by    possibly followed by duration
indicators  and is also treated as an atomic symbol  no loudness or timbre indicators are used
in esac 
phrase boundaries are indicated by hard returns in esac  the phrases are unlabeled  cf 
section   of this paper   yet to make the esac annotations readable for our dop models 
we added three basic labels to the phrase structures  the label  s  to each whole song  the
label  p  to each phrase  and the label  n  to each atomic symbol  in this way  we obtained
conventional tree structures that could directly be employed by our dop models to parse new
input  the use of the label  n  distinguishes our annotations from those in previous work  bod 
   

fia unified model of structural organization in language and music

    b c  where we only used labels for song and phrase   s  and  p    the addition of  n 
enhances the productivity and robustness of the musical parsing model  although it also leads
to a much larger number of subtrees 
as an example  assume a very simple melody consisting of two phrases               then its
tree structure is given in figure   
s
p

p

n

n

n

n

 

 

 

 

figure    example of a musical tree structure consisting of two phrases
subtrees that can be extracted from this tree structure include the following 
s
p
n

p
p

n

n

n

n

 

 

 

 

figure    some subtrees that can be extracted from the tree in figure  
thus the first subtree indicates a phrase starting with a note    followed by exactly one other
 unspecified  note  with the phrase itself followed by exactly one other  unspecified  phrase 
such subtrees can be used to parse new musical input in the same way as has been explained
for linguistic parsing in section   

   experimental evaluation and comparison
to evaluate our dop models  we used the blind testing method which randomly divides a
treebank into a training set and a test set  where the strings from the test set are parsed by
means of the subtrees from the training set  we applied the standard parseval metrics of
precision and recall to compare a proposed parse tree p with the corresponding correct test
set parse tree t as follows  cf  black et al        
  correct constituents in p

  correct constituents in p

precision  

recall  

  constituents in p

  constituents in t

a constituent in p is  correct  if there exists a constituent in t of the same label that spans the
same atomic symbols  i e  words or notes    since precision and recall can obtain rather
  the precision and recall scores were computed by using the  evalb  program  available via

http   www cs nyu edu cs projects proteus evalb  

   

fibod

different results  see bod      b   they are often balanced by a single measure of
performance  known as the f score  see manning   schtze        
f score  

   precision  recall
precision   recall

for our experiments  we divided both treebanks  i e  the wsj and the essen folksong
collection  into    training test set splits      of the wsj was used as test material each time
 sentences     words   while for the essen folksong collection test sets of       folksongs
were used each time  for words in the test set that were unknown in the training set  we
guessed their categories by using statistics on word endings  hyphenation and capitalization
 cf  bod      a   there were no unknown notes  as in previous work  bod      a   we limited
the maximum size of the subtrees to depth     and used random samples of         subtrees
for each depth     and       next  we restricted the maximum number of atomic symbols in
each subtree to    and the maximum depth of unlexicalized subtrees to    all subtrees were
smoothed by the technique described in bod               based on simple good turing
estimation  good        
table   shows the mean f scores obtained by sl dop and ls dop for language and
music and for various values of n  recall that for n    sl dop is equal to likelihood dop
while ls dop is equal to simplicity dop 

n
 
 
  
  
  
  
  
  
  
  
   
     

sl dop

ls dop

 simplest among n likeliest 

 likeliest among n simplest 

language

music

language

music

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

     
     
     
     
     
     
     
     
     
     
     
     

table    f scores obtained by sl dop and ls dop for language and music

  these random subtree samples were not selected by first exhaustively computing the complete set of

subtrees  this was computationally prohibitive   instead  for each particular depth     we sampled
subtrees by randomly selecting a node in a random tree from the training set  after which we selected
random expansions from that node until a subtree of the particular depth was obtained  we repeated
this procedure         times for each depth     and     

   

fia unified model of structural organization in language and music

the table shows that there is an increase in accuracy for both sl dop and ls dop if the
value of n increases from   to     but while the accuracy of sl dop decreases after n   
and converges to simplicity dop  i e  ls dop at n     the accuracy of ls dop continues
to increase and converges to likelihood dop  i e  sl dop at n     the highest accuracy is
obtained by sl dop at     n      for both language and music  thus sl dop outperforms
both likelihood dop and simplicity dop  and the selection of the simplest structure out of
the top likeliest ones turns out to be a more promising model than the selection of the likeliest
structure out of the top simplest ones  according to paired t testing  the accuracy
improvement of sl dop at n    over sl dop at n    when it is equal likelihood dop  is
statistically significant for both language  p        and music  p       
it is surprising that sl dop reaches highest accuracy at such a small value for n  but it is
even more surprising that exactly the same model  with the same parameter setting  obtains
maximum accuracy for both language and music  this model embodies the idea that the
perceptual system strives for the simplest structure but in doing so it only searches among a
few most probable structures 
to compare our results for language with others  we also tested sl dop at n    on the
now standard division of the wsj  which uses sections   to    for training  approx        
sentences  and section    for testing       sentences      words   see e g  manning  
schtze        charniak        collins         on this division  sl dop achieved an f score
of       while the best previous models obtained an f score of        collins        bod 
    a   in terms of error reduction  sl dop improves with      over these other models  it
is common to also report the accuracy for sentences     words on the wsj  for which sldop obtained an f score of       
our musical results can be compared to bod      b c   who tested three probabilistic
parsing models of increasing complexity on the same training test set splits from the essen
folksong collection  the best results were obtained with a hybrid dop markov parser 
      f score  this is significantly worse than our best result of       obtained by sl dop
on the same splits from the essen folksongs  this difference may be explained by the fact that
the hybrid dop markov parser in bod      b c  only takes into account context from higher
nodes in the tree and not from any sister nodes  while the dop models presented in the
current paper take any subtree into account of  almost  arbitrary width and depth  thereby
covering a larger amount of musical context  moreover  as mentioned in section    the models
in bod      b c  did not use the label  n  for notes  instead  a markov approach was used to
parse new sequences of notes 
it would also be interesting to compare our musical results to the melodic parser of
temperley         who uses a system of preference rules similar to lerdahl and jackendoff
        and which is also evaluated on the essen folksong collection  but while we have
tested on several test sets of       randomly selected folksongs  temperley used only one test
set of    folksongs that was moreover cleaned up by eliminating folksongs with irregular
meter  temperley             it is therefore difficult to compare our results with temperley s 
yet  it is noteworthy that temperley s parser correctly identified       of the phrase
boundaries  although this is lower than the       obtained by sl dop  temperley s parser is
not  trained  on previously analyzed examples like our model  though we note that
temperley s results were obtained by tuning the optimal phrase length of his parser on the
average phrase length of the essen folksong collection  
it should perhaps be mentioned that while parsing models trained on treebanks are widely
used in natural language processing  they are still rather uncommon in musical processing 
   

fibod

most musical parsing models  including temperley s  employ a rule based approach where the
parsing is based on a combination of low level rules    such as  prefer phrase boundaries at
large intervals     and higher level rules    such as  prefer phrase boundaries at changes of
harmony   the low level rules are usually based on the well known gestalt principles of
proximity and similarity  wertheimer         which prefer phrase boundaries at larger
intervallic distances  however  in bod      c  we have shown that the gestalt principles
predict incorrect phrase boundaries for a number of folksongs  and that higher level
phenomena cannot alleviate these incorrect predictions  these folksongs contain a phrase
boundary which falls just before or after a large pitch or time interval  which we have called
jump phrases  rather than at such intervals    as would be predicted by the gestalt principles 
moreover  other musical factors  such as melodic parallelism  meter and harmony  predict
exactly the same incorrect phrase boundaries for these cases  see bod      b c for details  
we have conjectured that such jump phrases are inherently memory based  reflecting idiomdependent pitch contours  cf  huron        snyder         and that they can be best captured
by a memory based model that tries to mimic the musical experience of a listener from a
certain culture  bod      c  

   discussion and conclusion
we have seen that our combination of simplicity and likelihood is quite rewarding for linguistic
and musical structuring  suggesting an interesting parallel between the two modalities  yet  one
may question whether a model which massively memorizes and re uses previously perceived
structures has any cognitive plausibility  although this question is only important if we want to
claim cognitive relevance for our model  there appears to be some evidence that people store
various kinds of previously heard fragments  both in language  jurafsky        and music
 saffran et al         but do people store fragments of arbitrary size  as proposed by dop 
in his overview article  jurafsky        reports on a large body of psycholinguistic evidence
showing that people not only store lexical items and bigrams  but also frequent phrases and
even whole sentences  for the case of sentences  people not only store idiomatic sentences 
but also  regular  high frequency sentences   thus  at least for language there is some
evidence that humans store fragments of arbitrary size provided that these fragments have a
certain minimal frequency  and this suggests that humans need not always parse new input by
the rules of a grammar  but that they can productively re use previously analyzed fragments 
yet  there is no evidence that people store all fragments they hear  as suggested by dop 
only high frequency fragments seem to be memorized  however  if the human perceptual
faculty needs to learn which fragments will be stored  it will initially need to keep track of all
fragments  with the possibility of forgetting them  otherwise frequencies can never
accumulate  this results in a model which continuously and incrementally updates its fragment
memory given new input    which is in correspondence with the dop approach  and also with
some other approaches  cf  daelemans        scha et al        spiro         while we
acknowledge the importance of a rule based system in acquiring a fragment memory  once a
substantial memory is available it may be more efficient to construct a tree by means of
already parsed fragments than constructing it entirely by means of rules  for many cognitive
  these results are derived from differences in reaction times in sentence recognition where only the

frequency of the  whole  test sentences is varied  while all other variables  such as lexical frequency 
bigram frequency  plausibility  syntactic semantic complexity  etc   are kept constant 

   

fia unified model of structural organization in language and music

activities it is advantageous to store results  so that they can immediately be retrieved from
memory  rather than computing them each time from scratch  this has been shown  for
example  for manual reaches  rosenbaum et al         arithmetic operations  rickard et al 
       word formation  baayen et al         to mention a few  and linguistic and musical
parsing may be no exception to this 
it should be stressed that the experiments reported in this paper are limited in at least two
respects  first  our musical test domain is rather restricted  while a wide variety of linguistic
treebanks is currently available  see manning   schtze         the number of musical
treebanks is extremely limited  there is thus a need for larger and richer annotated musical
corpora covering broader domains  the development of such annotated corpora may be timeconsuming  but experience from natural language processing has shown that it is worth the
effort  since corpus based parsing systems dramatically outperform grammar based parsing
systems  a second limitation of our experiments is that we have only evaluated the parse
results rather than the parse process  that is  we have only assessed how accurately our
models can mimic the input output behavior of a human annotator  without investigating the
process by which an annotator arrived at the perceived structures  it is unlikely that humans
process perceptual input by computing        most likely derivations using random samples of
        subtrees  as we did in the current paper  yet  for many applications it suffices to
know the perceived structure rather than the process that led to that structure  and we have
seen that our combination of simplicity and likelihood predicts the perceived structure with a
high degree of accuracy 
there have been other proposals for integrating the principles of simplicity and likelihood in
human perception  see chater       for a review   chater notes that in the context of
information theory  shannon         the principles of simplicity and likelihood are identical  in
this context  the simplicity principle is interpreted as minimizing the expected length to encode
a message i  which is log  p i bits  and which leads to the same result as maximizing the
probability of i  if we used this information theoretical definition of simplest structure in
simplicity dop  it would return the same structure as likelihood dop  and no improved
results would be obtained by a combination of the two  on the other hand  by defining the
simplest structure as the one generated by the smallest number of subtrees  independent of
their probabilities  we created a notion of simplicity which is provably different from the notion
of most likely structure  and which  combined with likelihood dop  obtained improved results 
another integration of the two principles may be provided by the notion of minimum
description length or mdl  cf  rissanen         the mdl principle can be viewed as
preferring the statistical model that allows for the shortest encoding of the training data  the
relevant encoding consists of two parts  the first part encodes the model of the data  and the
second part encodes the data in terms of the model  in bit length   mdl is closely related to
stochastic complexity  rissanen        and kolmogorov complexity  li and vitanyi        
and has been used in natural language processing for estimating the parameters of a stochastic
grammar  e g  osborne         we will leave it as an open research question as to whether
mdl can be successfully used for estimating the parameters of dop s subtrees  however 
since mdl is known to give asymptotically the same results as maximum likelihood estimation
 mle   rissanen         its application to dop may lead to an unproductive model  this is
because the maximum likelihood estimator will assign the training set trees their empirical
frequencies  and assign   weight to all other trees  see bonnema       for a proof   this
would result in a model which can only generate the training data and no other strings 
johnson        argues that this may be an overlearning problem rather than a problem with
   

fibod

mle per se  and that standard methods  such as cross validation or regularization  would seem
in principle to be ways to avoid such overlearning  we will leave this issue to future
investigation 
the idea of a general underlying model for language and music is not uncontroversial  in
linguistics it is usually assumed that humans have a separate language faculty  and lerdahl and
jackendoff        have argued for a separate music faculty  this work does not propose that
these separate faculties do not exist  but wants to focus on the commonalities rather than on
the differences between these faculties  aiming at finding a deeper  faculty  which may hold
for perception in general  our hypothesis is that the perceptual system strives for the simplest
structure but in doing so it only searches among the likeliest structures 

acknowledgements
thanks to aline honingh  remko scha  neta spiro  menno van zaanen and three anonymous
reviewers for their excellent comments  a preliminary version of this paper was presented as
a keynote talk at the lcg workshop   learning computational grammars   tbingen        

references
baayen  r  h   dijkstra  t    schreuder  r          singular and plurals in dutch  evidence
for a parallel dual route model  journal of memory and language             
black  e   abney  s   flickinger  d   gnadiec  c   grishman  r   harrison  p   hindle  d  
ingria  r   jelinek  f   klavans  j   liberman  m   marcus  m   roukos  s   santorini  b 
  strzalkowski  t          a procedure for quantitatively comparing the syntactic
coverage of english  in proceedings darpa speech and natural language
workshop  pacific grove  morgan kaufmann 
bod  r          using an annotated language corpus as a virtual stochastic grammar  in
proceedings aaai     menlo park  ca 
bod  r          beyond grammar  an experience based theory of language  stanford 
csli publications  lecture notes number     
bod  r       a   parsing with the shortest derivation  in proceedings coling      
saarbrcken  germany 
bod  r       b   combining semantic and syntactic structure for language modeling  in
proceedings icslp       beijing  china 
bod  r       a   what is the minimal set of fragments that achieves maximal parse
accuracy  in proceedings acl       toulouse  france 
bod  r       b   a memory based model for music analysis  in proceedings international
computer music conference  icmc        havana  cuba 
bod  r       c   memory based models of melodic analysis  challenging the gestalt
principles  journal of new music research                 available at
http   staff science uva nl  rens jnmr   pdf 
   

fia unified model of structural organization in language and music

bod  r   hay  j    jannedy  s   eds        a   probabilistic linguistics  cambridge  the
mit press   in press 
bod  r   scha  r    sima an  k   eds        b   data oriented parsing  stanford  csli
publications   in press 
bonnema  r          probability models for dop  in bod et al       b  
bonnema  r   bod  r    scha  r          a dop model for semantic interpretation  in
proceedings acl eacl     madrid  spain 
buffart  h   leeuwenberg  e    restle   f          analysis of ambiguity in visual pattern
completion  journal of experimental psychology  human perception and
performance              
charniak  e          statistical language learning  cambridge  the mit press 
charniak  e          statistical techniques for natural language parsing  ai magazine 
winter             
charniak  e          a maximum entropy inspired parser  in proceedings anlpnaacl       seattle  washington 
chater  n          the search for simplicity  a fundamental cognitive principle  the
quarterly journal of experimental psychology    a             
chomsky  n          aspects of the theory of syntax  cambridge  the mit press 
collard  r   vos  p    leeuwenberg  e          what melody tells about metre in music  
zeitschrift fr psychologie             
collins  m          head driven statistical models for natural language parsing  phdthesis  university of pennsylvania  pa 
collins  m          discriminative reranking for natural language parsing  in proceedings
icml       stanford  ca 
collins  m    duffy  n          new ranking algorithms for parsing and tagging  kernels
over discrete structures  and the voted perceptron  in proceedings acl      
philadelphia  pa 
daelemans  w          introduction to special issue on memory based language
processing  journal of experimental and theoretical artificial intelligence       
        
dastani  m          languages of perception  illc dissertation series          university
of amsterdam 
dempster  a   laird  n    rubin  d          maximum likelihood from incomplete data via
the em algorithm  journal of the royal statistical society           

   

fibod

de pauw  g          aspects of pattern matching in data oriented parsing  in proceedings
coling       saarbrcken  germany 
eisner  j          bilexical grammars and a cubic time probabilistic parser  in proceedings
fifth international workshop on parsing technologies  boston  mass 
frazier  l          on comprehending sentences  syntactic parsing strategies  phd 
thesis  university of connecticut 
good  i          the population frequencies of species and the estimation of population
parameters  biometrika             
goodman  j          efficient algorithms for parsing the dop model  in proceedings
empirical methods in natural language processing  philadelphia  pa 
goodman  j          efficient parsing of dop with pcfg reductions  in bod et al      b 
von helmholtz  h          treatise on physiological optics  vol      dover  new york 
hoffman  d          visual intelligence  new york  norton   company  inc 
huron  d          the melodic arch in western folksongs  computing in musicology         
johnson  m          the dop estimation method is biased and inconsistent  computational
linguistics            
jurafsky  d          probabilistic modeling in psycholinguistics  comprehension and
production  in bod et al      a   available at http   www colorado edu ling jurafsky 
prob ps 
kersten  d          high level vision as statistical inference  in gazzaniga   s   ed    the new
cognitive neurosciences  cambridge  the mit press 
leeuwenberg  e          a perceptual coding language for perceptual and auditory
patterns  american journal of psychology              
lerdahl  f    jackendoff  r          a generative theory of tonal music  cambridge  the
mit press 
li  m    vitanyi  p          an introduction to kolmogorov complexity and its
applications   nd ed    new york  springer 
longuet higgins  h          perception of melodies  nature              
longuet higgins  h  and lee  c          the rhythmic interpretation of monophonic music 
mental processes  studies in cognitive science  cambridge  the mit press 
manning  c    schtze  h          foundations of statistical natural language
processing  cambridge  the mit press 
marcus  m   santorini  b     marcinkiewicz  m          building a large annotated corpus
of english  the penn treebank  computational linguistics       
   

fia unified model of structural organization in language and music

marr  d          vision  san francisco  freeman 
martin  w   church  k    patil  r          preliminary analysis of a breadth first parsing
algorithm  theoretical and experimental results  in bolc  l   ed    natural language
parsing systems  springer verlag  berlin 
mumford  d          the dawning of the age of stochasticity  based on a lecture at the
accademia nazionale dei lincei   available at http   www dam brown edu people 
mumford papers dawning ps 
osborne  m          minimal description length based induction of definite clause grammars
for noun phrase identification  in proceedings eacl workshop on computational
natural language learning  bergen  norway 
palmer  s          hierarchical structure in perceptual representation  cognitive
psychology             
raphael  c          automatic segmentation of acoustic musical signals using hidden
markov models  ieee transactions on pattern analysis and machine intelligence 
               
restle   f          theory of serial pattern learning  structural trees  psychological
review           
rickard  t   healy  a    bourne jr   e          on the cognitive structure of basic arithmetic
skills  operation  order and symbol transfer effects  journal of experimental
psychology  learning  memory and cognition                
rissanen  j          modeling by the shortest data description  automatica              
rissanen  j          stochastic complexity in statistical inquiry  series in computer
science   volume     world scientific       
rosenbaum  d   vaughan  j   barnes  h    jorgensen  m          time course of movement
planning  selection of handgrips for object manipulation  journal of experimental
psychology  learning  memory and cognition                
saffran  j   loman  m    robertson  r          infant memory for musical experiences 
cognition      b      
scha  r   bod  r    sima an  k          memory based syntactic analysis  journal of
experimental and theoretical artificial intelligence                 
schaffrath  h          the essen folksong collection in the humdrum kern format  d 
huron  ed    menlo park  ca  center for computer assisted research in the
humanities 
shannon  c          a mathematical theory of communication  bell system technical
journal                       
sima an  k          computational complexity of probabilistic disambiguation by means of
tree grammars  in proceedings coling     copenhagen  denmark 
   

fibod

simon  h          complexity and the representation of patterned sequences as symbols 
psychological review              
snyder  b          music and memory  cambridge  the mit press 
spiro  n          combining grammar based and memory based models of perception of
time signature and phase  in anagnostopoulou  c   ferrand  m    smaill  a   eds   
music and artificial intelligence  lecture notes in artificial intelligence  vol       
springer verlag          
temperley  d          the cognition of basic musical structures  cambridge  the mit
press 
wertheimer  m          untersuchungen zur lehre von der gestalt  psychologische
forschung            
wundt  w          sprachgeschichte und sprachpsychologie  engelmann  leipzig 

   

fi