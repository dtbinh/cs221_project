journal artificial intelligence research                  

submitted       published     

domain adaptation statistical classifiers
hal daume iii
daniel marcu

hdaume isi edu
marcu isi edu

information sciences institute
university southern california
     admiralty way  suite     
marina del rey  ca       usa

abstract
basic assumption used statistical learning theory training data
test data drawn underlying distribution  unfortunately  many
applications  in domain test data drawn distribution related 
identical  out of domain distribution training data  consider
common case labeled out of domain data plentiful  labeled in domain data
scarce  introduce statistical formulation problem terms simple mixture
model present instantiation framework maximum entropy classifiers
linear chain counterparts  present efficient inference algorithms special
case based technique conditional expectation maximization  experimental
results show approach leads improved performance three real world tasks
four different data sets natural language processing domain 

   introduction
generalization properties current statistical learning techniques predicated
assumption training data test data come underlying
probability distribution  unfortunately  many applications  assumption inaccurate 
often case plentiful labeled data exists one domain  or coming one
distribution   one desires statistical model performs well another related 
identical domain  hand labeling data new domain costly enterprise  one
often wishes able leverage original  out of domain data building model
new  in domain data  seek eliminate annotation in domain
data  instead seek minimize amount new annotation effort required achieve
good performance  problem known domain adaptation transfer 
paper  present novel framework understanding domain adaptation
problem  key idea framework treat in domain data drawn
mixture two distributions  truly in domain distribution general domain
distribution  similarly  out of domain data treated drawn mixture
truly out of domain distribution general domain distribution  apply
framework context conditional classification models conditional linear chain
sequence labeling models  inference may efficiently solved using technique
conditional expectation maximization  apply model four data sets varying degrees divergence in domain out of domain data obtain
c
    
ai access foundation  rights reserved 

fidaume iii   marcu

predictive accuracies higher large number baseline systems second
model proposed literature problem 
domain adaptation problem arises frequently natural language processing domain  millions dollars spent annotating text resources
morphological  syntactic semantic information  however  resources
based text news domain  in cases  wall street journal   sort
language appears text wall street journal highly specialized is 
circumstances  poor match domains  instance 
recent surge interest performing summarization  elhadad  kan  klavans    mckeown        information extraction  hobbs        biomedical texts  summarization
electronic mail  rambow  shrestha  chen    lauridsen         information extraction
transcriptions meetings  conversations voice mail  huang  zweig    padmanabhan 
       among others  conversely  machine translation domain  parallel
resources machine translation system depend parameter estimation drawn
transcripts political meetings  yet translation systems often targeted news
data  munteanu   marcu        

   statistical domain adaptation
multiclass classification problem  one typically assumes existence training set
    xn   yn   x     n n    x input space finite set 
assumed  xn   yn   drawn fixed  unknown base distribution p
training set independent identically distributed  given p  learning problem
find function f   x obtains high predictive accuracy  this typically
done either explicitly minimizing regularized empirical error  maximizing
probabilities model parameters  
    domain adaptation
context domain adaptation  situation becomes complicated  assume
given two sets training data   o  d i    out of domain indomain data sets  respectively  longer assume single fixed 
known distribution drawn  rather assume  o  drawn
distribution p o  d i  drawn distribution p i    learning problem
find function f obtains high predictive accuracy data drawn p  i     indeed 
model turn symmetric respect  i  d o    contexts
consider obtaining good predictive model  i  makes intuitive sense  
assume  d  o      n  o   d i      n  i    typically n  i  n  o   
before  assume n  o  out of domain data points drawn iid p o 
n  i  in domain data points drawn iid p i   
obtaining good adaptation model requires careful modeling relationship
p i  p o    two distributions independent  in obvious intuitive
sense   out of domain data  o  useless building model p i  may
well ignore it  hand  p i  p o  identical  adaptation
necessary simply use standard learning algorithm  practical problems 
though  p i  p o  neither identical independent 
   

fidomain adaptation statistical classifiers

    prior work
relatively little prior work problem  nearly focused
specific problem domains  n gram language models generative syntactic parsing
models  standard approach used treat out of domain data prior knowledge
estimate maximum posterior values model parameters prior
distribution  approach applied successfully language modeling  bacchiani
  roark        parsing  roark   bacchiani         parsing domain  hwa
       gildea        shown simple techniques based using carefully chosen
subsets data parameter pruning improve performance adapted
parser  models assume data distribution p  d     parameters prior
distribution parameters p       hyper parameters   estimate
hyperparameters out of domain data find maximum posteriori
parameters in domain data  prior fixed 
context conditional discriminative models  domain adaptation
work aware model chelba acero         model
uses out of domain data estimate prior distribution  context
maximum entropy model  specifically  maximum entropy model trained
out of domain data  yielding optimal weights problem  weights used
mean weights gaussian prior learned weights in domain data 
though effective experimentally  practice estimating prior distribution
out of domain data fixing estimation in domain data leaves much
desired  theoretically  strange estimate fix prior distribution data 
made apparent considering form models  denoting in domain data
parameters  i    respectively  out of domain data parameters
d o    obtain following form prior estimation models 




  arg max p   arg max p    p




 o 



 i 
 
p  

   

one would difficult time rationalizing optimization problem anything
experimental performance  moreover  models unusual
treat in domain data out of domain data identically  intuitively 
difference two sets data  simply come different  related distributions 
yet  prior based models highly asymmetric respect two data sets 
makes generalization one domain data set difficult  finally 
see  model propose paper  alleviates problems 
outperforms experimentally 
second generic approach domain adaptation problem build
domain model use predictions features domain data 
successfully used context named entity tagging      approach attractive
makes assumptions underlying classifier  fact  multiple classifiers
used 
   

fidaume iii   marcu

    framework
paper  propose following relationship in domain out ofdomain distributions  assume instead two underlying distributions 
actually three underlying distributions  denote q  o    q  g  q  i   
consider p o  mixture q  o  q  g    consider p i  mixture q  i 
q  g    one intuitively view q  o  distribution distribution data truly
out of domain  q  i  distribution data truly in domain q  g  distribution
data general domains  thus  knowing q  g  q  i  sufficient build
model in domain data  out of domain data help us providing
information q  g  available considering in domain data 
example  part of speech tagging  assignment tag determiner  dt 
word likely general decision  independent domain  however 
wall street journal  monitor almost always verb  vb   technical documentation
likely noun  q  g  distribution account case the dt 
q  o  account monitor vb q  i  account monitor nn 

   domain adaptation maximum entropy models
domain adaptation framework outlined section     completely general
applied statistical learning model  section apply loglinear conditional maximum entropy models linear chain counterparts  since
models proved quite effective many learning tasks  first review maximum
entropy framework  extend domain adaptation problem  finally
discuss domain adaptation linear chain maximum entropy models 
    maximum entropy models
maximum entropy framework seeks conditional distribution p  y   x  closest
 in sense kl divergence  uniform distribution matches set training data respect feature function expectations  della pietra  della pietra   
lafferty         introducing one lagrange multiplier feature function  
optimization problem results probability distribution form 
p  y   x      

 
z x

h

exp   f  x  y 

   

p
here  u  v denotes scalar product two vectors u v  given by  u  v   ui vi  
normalization constant eq      z x   obtained summing exponential
possible classes   y  probability distribution known exponential
distribution gibbs distribution  learning  or optimization  problem find
vector maximizes likelihood eq      practice  prevent over fitting  one
typically optimizes penalized  log  likelihood  isotropic gaussian prior mean
  covariance matrix   placed parameters  chen   rosenfeld        
graphical model standard maximum entropy model depicted left
figure    figure  circular nodes correspond random variables square nodes
   

fidomain adaptation statistical classifiers

correspond fixed variables  shaded nodes observed training data empty
nodes hidden unobserved  arrows denote conditional dependencies 
general  feature functions f  x  y  may arbitrary real valued functions  however 
paper restrict attention binary features  practice  harsh
restriction  many problems natural language domain naturally employ binary
features  for real valued features  binning techniques applied   additionally 
notational convenience  assume features  x  y  written product
form gi  y hi  x  arbitrary binary functions g outputs binary features h
inputs  latter assumption means consider x binary vector
xi   hi  x   following simplify notation significantly  the extension full
case straightforward  messy  therefore considered remainder
paper   considering x vector  may move class dependence parameters
consider matrix y i weight hi class y  write
refer column vector corresponding class y  x considered
column vector  write   x shorthand dot product x weights
class y  modified notation  may rewrite eq     as 
p  y   x      

 
z x

h

exp   x

   

combining gaussian prior weights  obtain following form
log posterior data set 


n

h
x
x
 
yn   xn log
l   log p     d           
exp y    xn   const
 
 
n  

   



parameters estimated using convex optimization technique  practice 
limited memory bfgs  nash   nocedal        averick   more        seems good
choice  malouf        minka        use algorithm experiments
described paper  order perform calculations  one must able compute
gradient eq     respect   available closed form 
    maximum entropy genre adaptation model
extending maximum entropy model account in domain out of domain
data framework described earlier requires addition several extra model param i   i 
eters  particular  in domain data point  xn   yn    assume existence
 i 
 i 
 i   i 
binary indicator variable zn   value zn     indicates  xn   yn   drawn q  i 
 i 
 the truly in domain distribution   value zn     indicates drawn q  g 
 o   o 
 the general domain distribution   similarly  out of domain data point  x n   yn   
 o 
 o 
assume binary indicator variable zn   zn     means data point drawn
 o 
q  the truly out of domain distribution  value   means drawn
q  g   the general domain distribution   course  indicator variables
observed data  must infer values automatically 
   

fidaume iii   marcu

 

 





g

yni

yn
xn

xni

zni

n





yno

n

xno









zno



g





n





figure     left  standard logistic regression model   right  mega model 
according model  zn binary random variables assume
drawn bernoulli distribution parameter  i   for in domain   o   for outof domain   furthermore  assume three vectors   i     o   g 
corresponding q  i    q  o  q  g    respectively  instance  zn      assume
 i 
xn classified using  i    finally  model binary vectors xn  respec o 
tively xn s  drawn independently bernoulli distributions parameterized
 i 
 g   respectively   o   g     again  zn      assume xn
drawn according  i    corresponds nave bayes assumption generative
probabilities xn vectors  finally  place common beta prior nave bayes
parameters    allowing range  i  o  g   full hierarchical model is 
  

f   a  b
 i 
zn    i 
 i 

 i 

 i 

 i 

 i 

 g 

xnf   zn   f   f
 i 

      
 o 
zn    o 

bet a  b 
ber   i   
z  i 

ber  fn  

 i 

 i 

yn   xn   zn    i     g  gibbs xn   zn  

 o 

 o 

 o 

 o 

 o 

 g 

xnf   zn   f   f
 o 

nor      i 
ber   o   

   

z  o 

ber  fn  

 o 

 o 

yn   xn   zn    o     g  gibbs xn   zn  

term model maximum entropy genre adaptation model  the mega
model   corresponding graphical model shown right figure    generative story in domain data point x i  follows 
   select whether x i  truly in domain general domain indicate
z  i   i  g   choose z  i    probability  i  z  i    g probability
   i   
 i 

   component f x i    choose xf   probability zf
z  i 

probability   f  
 i 

   choose class according eq     using parameter vector z  
   

 i 

 

fidomain adaptation statistical classifiers

story out of domain data points identical  uses truly out of domain
general domain parameters  rather truly in domain parameters generaldomain parameters 
    linear chain models
straightforward extension maximum entropy classification model maximum
entropy markov model  memm   mccallum  freitag    pereira        obtained
assuming targets yn sequences labels  canonical example model
part speech tagging  word sequence assigned part speech tag 
introducing first order markov assumption tag sequence  one obtains linear chain
model viewed discriminative counterpart standard  generative 
hidden markov model  parameters models estimated using
limited memory bfgs  extension mega model linear chain framework
similarly straightforward  assumption label  part speech tag 
indicator variable z  versus global indicator variable z entire tag sequence  
techniques described herein may applied conditional random field
framework lafferty  mccallum  pereira         fixes bias problem
memm performing global normalization rather per state normalization  is 
however  subtle difficulty direct application crfs  specifically  one would need
decide single z variable would assigned entire sentence  word
individually  memm case  natural one z per word  however 
crf would computationally expensive  remainder  continue
use memm model efficiency purposes 

   conditional expectation maximization
inference mega model slightly complex standard maximum entropy models  however  inference solved efficiently using conditional expectation
maximization  cem   variant standard expectation maximization  em  algorithm
 dempster  laird    rubin         due jebara pentland         high level  em
useful computing generative models hidden variables  cem useful
computing discriminative models hidden variables  mega model belongs
latter family  cem appropriate choice 
standard em family algorithms maximizes joint likelihood data 
particular   xn   yn  n
n   data z  discrete  hidden variable  m step em
proceeds maximizing bound given eq    
log p  x        log

x
z

p  z  x        log ezp    x   p  x    z   

   

eq      ez denotes expectation  one may apply jensens inequality
equation  states f  e x   e f  x   whenever f convex  taking f   log 
able decompose log expectation expectation log  typically
separates terms makes taking derivatives solving resolution optimization problem tractable  unfortunately  em cannot directly applied conditional models  such
   

fidaume iii   marcu

mega model  form eq     models result m step
requires maximization equation form given eq     
log p  y   x      log
l   log

x
z

x
z

p  z    x      log ezp    x   p  y   x  z   
p  z  x      log

x
z

p  z  x    

   
   

jensens inequality applied first term eq      maximized
readily standard em  however  applying jensens inequality second term would
lead upper bound likelihood  since term appears negated 
conditional em solution  jebara   pentland        bound change
log likelihood iterations  rather log likelihood itself  change loglikelihood written eq      denotes parameters iteration t 


lc   log p   x  log p   x  t 

   

rewriting conditional distribution p  y   x  p  x  y  divided p  x  
express lc log joint distribution difference minus log marginal
distribution  here  apply jensens inequality first term  the joint difference  
second  because appears negated   fortunately  jensens
bound employ  standard variational upper bound logarithm function is 
log x x    leads lower bound negation  exactly desired 
bound attractive reasons      tangent logarithm      tight 
    makes contact current operating point  according maximization
previous time step       simply linear function      terminology
calculus variations  variational dual logarithm  see  smith        
applying jensens inequality first term eq     variational dual
second term  obtain change log likelihood moving model parameters
t  time   time  which shall denote qt   bounded l qt  
qt defined eq       h   e z   x    z       e z   x   
z      expectations taken respect parameters previous iteration 


p

x
p z  x   
z p z  x  

  
    
q  
hz log
p
t   
p  z  x    t   
z p  z  x  
zz

applying two bounds  jensens inequality variational bound  
removed sums logs  hard deal analytically  full derivation
given appendix a  remaining expression lower bound change likelihood 
maximization result maximization likelihood 
map variant standard em  change e step priors
placed parameters  assumption standard em wish maximize
p     x  y  p    p  y     x  prior probability ignored  leaving
likelihood term parameters given data  map estimation  make
assumption instead use true prior p     so  need add factor
log p    definition qt eq      
   

fidomain adaptation statistical classifiers

t 
jn z
n

mt 
n


  log p xn   yn   zn   t 
n zn

p
t   
 
n zn  f  
z n p x n   zn  



xnf
 xnf
zn
zn

 


f   
f
f

xnf
 xnf
q
zn
zn

 
 


 
f   f
f
f

 

qf

table    notation used mega model equations 
important note although make use full joint distribution p  x  y  z  
objective function model conditional  joint distribution used
process creating bound  overall optimization maximize conditional likelihood labels given input  particular  bound using full joint likelihood
holds parameters marginal 

   parameter estimation mega model
made explicit eq       relevant distributions performing cem full joint
distributions input variables x  output variables y  hidden variables z 
additionally  require marginal distribution x variables z variables 
finally  need compute expectations z variables  derive expectation
step section present final solution maximization step class
variables  derivation equations maximization given appendix b 
q bound complete conditional likelihood mega modelis given below 





p
 i 
 i 
 i 
 i 
 i 
z
 
x
p
p
z
 
x
 

 i 
x
n
n
n
n
n
z


   
p n
h i 
qt  
n log
 i 
 i 
 i 
  z  i    x i 
 
p z n   x n   yn
 i  p
n
n
n   z  i 
zn
n






p
 o 
 o   o 
 o 
 o 
 o 
n
p zn   xn   yn
 o  p zn   xn
x x
z

   

p n
h o 
 
n log
 o 
 o 
 o 
 o 
 o 
 
 
p z n   x n   yn
z n   xn
 o  p
n   z  o 
z
n  i 
x



    

n

n

equation  p     probability distribution previous iteration  first
term eq      bound in domain data  second term bound
out of domain data  optimizations described section  nearly
identical terms in domain parameters out of domain parameters  brevity 
explicitly write equations in domain parameters  corresponding
out of domain equations easily derived these  moreover  reduce notational
overload  elide superscripts denoting in domain out of domain obvious
context  notational brevity  use notation depicted table   
    expectation step
e step concerned calculating hn given current model parameters  since zn
        easily find hn   p  zn        calculated follows 
   

fidaume iii   marcu

p  zn   z   xn   yn        
p  zn   z     p  xn     zn   z  p  yn     zn   z 
  p
z p  zn   z     p  xn     zn   z  p  yn     zn   z 

h
 
z      z n z
exp zyn   xn
zxn  z

    

here  z partition function before  easily calculated z       
expectation found dividing value z     sum both 
    m step
shown appendix b    directly compute
value solving simple

quadratic equation  compute   a  b  where 
 
b  
    m step

pn

t 
n    hn mn  n   n    
pn
  n   mt 
n  n   n    
pn
n   hn
pn
t 
n   mn  n   n    

 



viewing qt function   easy see optimization variable convex 
analytical solution available  gradient qt respect  i 
seen identical gradient standard maximum entropy posterior  eq     
data point weighted according posterior probability     h n    may
thus use identical optimization techniques computing optimal variables standard
maximum entropy models  difference data points weighted 
similar story holds  o    case  g    obtain standard maximum entropy
 i 
gradient  computed n  i    n  o  data points  xn weighted hn
 o 
 o 
xn weighted hn   shown appendix b   
    m step
case   cannot obtain analytical solution finding maximizes
qt   however  compute simple derivatives qt respect single component
 i 
f maximized analytically  shown appendix b    compute f

  a  b  where 
pn



n     hn   jn       n   f
 
p
  n
n   jn       n   f
pn
   hn   xnf
  
b   pn n  
n   jn       n   f

   



fidomain adaptation statistical classifiers

algorithm megacem
  
  
initialize f        f                g  i  o  f  
parameters havent converged iterations remain
   expectation step   
n      n  i 
 i 
compute in domain marginal probabilities  mn
 i 
compute in domain expectations  hn   eq     
end
n      n  o 
 o 
compute out of domain marginal probabilities  mn
 o 
compute out of domain expectations  hn eq     
end
   maximization step   
analytically update  i   o  according equations shown section    
optimize  i     o   g  using bfgs
iterations remain and or havent converged
update according derivation section    
end
end
return    
figure    full training algorithm mega model 
case  o  identical   g    difference must replace
sum data points two sums  one in domain out of domain
points  and  before    hn must replaced hn   made explicit
appendix  thus  optimize variables  simply iterate optimize
component analytically  given above  convergence 
    training algorithm
full training algorithm depicted figure    convergence properties cem
algorithm ensure converge  local  maximum posterior space  local
optima become problem practice  one alternatively use stochastic optimization
algorithm  temperature applied enabling optimization jump local
optima early on  however  explore idea work  context
application  extension required 
    cem convergence
one immediate question conditional em model described many
em iterations required model converge  experiments    iterations
   

fidaume iii   marcu

convergence cem optimization
  
  
  

negative log likelihood    e  

  
  
  
  
 
 
 
 
 

 

 

 
 
number iterations

 

 

figure    convergence training algorithm 

cem sufficient  often     necessary  make clear 
figure    plotted negative complete log likelihood model first
data set  described section      three separate maximizations full
training algorithm  see figure     first involves updating variables  second
involves optimizing variables third involves optimizing variables 
compute likelihood steps 
running total   cem iterations still relatively efficient model  dominating expense weighted maximum entropy optimization  which    cem iterations 
must computed    times  each iteration requires optimization three
sets variables   worst take    times amount time train model
complete data set  the union in domain out of domain data   practice
resume optimization ending point previous iteration  causes
subsequent optimizations take much less time 
    prediction
training supplied us model parameters  subsequent task apply
parameters unseen data obtain class predictions  assume test data indomain  i e   drawn either q i  q g  notation introduction  
obtain decision rule form given eq      new test point x 

  arg max p  y   x   
yy
x
  arg max
p  z   x    p  y   x  z   
yy

  arg max
yy

z

x
z

p  z     p  x   z    p  y   x  z   
   

fidomain adaptation statistical classifiers



  arg max

f




 g 

f

xf

 g 

  f

 xf





h
 g 
exp   x

zx  g 
h



 x
f
xf
 xf exp  i 


 i 
 i 

      
f
  f
zx  i 
yy

f   

    

f   

thus  decision rule simply select class highest probability according maximum entropy classifiers  weighted linearly marginal probabilities
new data point drawn q i  versus q g    sense  model
seen linearly interpolating in domain model general domain model 
interpolation parameter input specific 

   experimental results
section  describe result applying mega model several datasets
varying degrees divergence in domain out of domain data  however 
describing data results  discuss systems compare 
    baseline systems
though little literature problem thus real systems
compare  several obvious baselines  describe section 
onlyi  model obtained simply training standard maximum entropy model
in domain data  completely ignores out of domain data serves
baseline case data unavailable 
onlyo  model obtained training standard maximum entropy model
out of domain data  completely ignoring in domain data  serves baseline
expected performance without annotating new data  gives sense close
out of domain distribution in domain distribution 
lini  model obtained linearly interpolating onlyi onlyo systems 
interpolation parameter estimated held out  development  in domain data 
means that  practice  extra in domain data would need annotated order create
development set  alternatively  cross validation could used 
mix  model obtained training maximum entropy model union
out of domain in domain data sets 
mixw  model obtained training maximum entropy model union
out of domain in domain data sets  out of domain data downweighted effectively equinumerous in domain data 
feats  model uses out of domain data build one classifier uses
classifiers predictions features in domain data  described       
   

fidaume iii   marcu

prior  adaptation model described section      out of domain
data used estimate prior in domain classifier  case maximum
entropy models consider here  weights learned out of domain data used
mean gaussian prior distribution placed weights training
in domain data  described chelba acero        
cases  tune model hyperparameters using performance development data 
development data taken random     training data cases 
appropriate hyperparameters found      folded back training set 
    data sets
evaluate models three different problems  first two problems come
automatic content extraction  ace  data task  data selected ace
program specifically looks data different domains  third problem
tackled chelba acero         required annotate data themselves 
      mention type classification
first problem  mention type  subcomponent entity mention detection
task  an extension named entity tagging task  wherein pronouns nominals
marked  addition simple names   assume extents mentions
marked simply need identify type  one of  person  geo political entity 
organization  location  weapon vehicle  out of domain data  use newswire
broadcast news portions ace      training data  in domain data  use
fisher conversations data  example out of domain sentence is 
again  prime battleground constitutional allocation power
nom
nam
federal governmentnom
gpe statesgpe   congressorg
bar
federal regulatory agenciesorg  
example in domain sentence is 
nom
pro
nom
mypro
per wifeper iper transported across continent gpe
whq pro
whereloc iper born

use   k out of domain examples  each mention corresponds one example    k
in domain examples     test examples  accuracy computed     loss  use
standard feature functions employed named entity models  include lexical items 
stems  prefixes suffixes  capitalization patterns  part of speech tags  membership
information gazetteers locations  businesses people  accuracies reported
result running ten fold cross validation 
      mention tagging
second problem  mention tagging precursor mention type task 
attempt tag entity mentions raw text  use standard begin in out
encoding use maximum entropy markov model perform tagging  mccallum
et al          out of domain data  use newswire broadcast news
   

fidomain adaptation statistical classifiers

data  in domain data  use broadcast news data transcribed
automatic speech recognition  in domain data lacks capitalization  punctuation  etc  
contains transcription errors  speech recognition word error rate approximately
      tagging task     k out of domain examples  in context tagging 
example single word    k in domain examples   k test examples 
accuracy f measure across segmentation  use features mention
type identification task  scores reported ten fold cross validation 
      recapitalization
final problem  recap  task recapitalizing text  following chelba acero
        use maximum entropy markov model  possible tags are 
lowercase  capitalized  upper case  punctuation mixed case  out of domain
data task comes wall street journal  two separate in domain data sets
come broadcast news text cnn npr abc primetime  respectively  use
   m out of domain examples  one example one word   cnn npr data  use
   k in domain training examples   k test examples  abc primetime data 
use   k in domain training examples  k test examples  use identical features
chelba acero         order maintain comparability results described
chelba acero         perform cross validation experiments  use
train test split described paper 
    feature selection
maximum entropy models used classification adept dealing
many irrelevant and or redundant features  nave bayes generative model  use
model distribution input variables  overfit features  turned
problem mention type mention tagging problems 
recap problems  caused errors  alleviate problem  recap
problem only  applied feature selection algorithm features used nave
bayes model  the entire feature set used maximum entropy model   specifically 
took   k top features according information gain criteria predict indomain versus out of domain  as opposed feature selection class label   forman
       provides overview different selection techniques  
    results
results shown table    see training in domain data
always outperforms training out of domain data  linearly interpolated model
improve base models significantly  placing data one bag helps 
clear advantage re weighting domain data  prior model
feats model perform roughly comparably  prior model edging
small margin   model outperforms prior model feats model 
   value   k selected arbitrarily initial run model development data 
tuned optimize either development test performance 
   numbers result prior model data chelba acero        differ slightly
reported paper  two potential reasons this  first  numbers

   

fidaume iii   marcu

 d o   

 d i   
accuracy
onlyo
onlyi
lini
mix
mixw
feats
prior
megam
  reduction
mix
prior

mention
type
  k
 k

mention
tagging
   k
 k

recap
abc
   m
 k

recap
cnn
   m
  k

average
 

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    

    
    

    
    

    
    

    
    

table    experimental results  first set rows show sizes in domain
out of domain training data sets  second set rows  accuracy  show
performance various models four tasks  last two rows   
reduction  show percentage reduction error rate using mega model
baseline model  mix  best alternative method  prior  

applied mcnemars test  gibbons   chakraborti        section       gage statistical significance results  comparing results prior model
mega model  for mention tagging experiment  compute mcnemars test simple
hamming accuracy rather f score  suboptimal  know
compute statistical significance f score   mention type task  difference
statistical significant p      level  mention tagging task  p       
recapitalization tasks  difference abc data significant p     
level  cnn npr data significant p       level 
mention type task  improved baseline model trained in domain
data accuracy              relative improvement        mention
tagging  improve       f measure        relative improvement      
abc recapitalization task  for much in domain data available   increase
performance              relative improvement       cnn npr
recapitalization task  with little in domain data   increase performance      
       relative improvement      

reported based using   m examples  consider    m example case  second 
likely subtle differences training algorithms used  nevertheless  whole  relative
improvements agree paper 

   

fidomain adaptation statistical classifiers

mention type identification task

mention tagging task

  

  

onlyout
chelba
megam

  

onlyout
chelba
megam

  
  

  

accuracy

fmeasure

  

  

  
  

  

  
  

  
 
  

 

  

 

 

  
  
amount domain data used  log scale 

  
 
  

 

  

 

  
amount domain data used  log scale 

figure    learning curves prior megam models 
    learning curves
particular interest amount annotated in domain data needed see marked
improvement onlyo baseline well adapted system  show figure  
learning curves mention type mention tagging problems  along x axis 
plot amount in domain data used  along y axis  plot accuracy  plot
three lines  flat line onlyo model use in domain data 
curves prior megam models  see  model maintains accuracy
models  prior curve actually falls baseline
type identification task  

   model introspection
seen previous sections mega model routinely outperforms competing models  despite clear performance improvement  question remains open regarding
internal workings models   i  variable captures degree indomain data set truly in domain  z variables model aim capture 
test data point  whether general domain in domain  section  discuss
particular values parameters model learns variables 
present two analyses  first  section       inspect models inner workings
mention type task section        analysis  look specifically
expected values hidden variables found model  second analysis
 section       look ability model judge degree relatedness  defined
variables 
   fisher data personal conversations  hence much higher degree first
second person pronouns news   the baseline always guesses person achieves      
accuracy   able intelligently use out of domain data in domain model
unsure  performance drops  observed prior model 

   

 

  

fidaume iii   marcu

pre context
home trenton
veterans administration
know american
gives
capable getting
fisher thing calling


      entity      
      new jersey      
      hospital      
      government     
   

   
      anything      
   

   
   
kid
   

post context
thats

chills

ha ha screwed


true
gpe
org
org
per
wea
per
per

hyp
gpe
loc
org
per
per
per
per

p  z   i 
    
    
    
    
    
    
    

table    examples test data mention type task  true column
correct entity type hyp column models prediction  final
column probability example truly in domain model 

    model expectations
focus discussion  consider mention type task  section       
table    shown seven test data examples mention type task  precontext text appears entity post context text
appears after  report true class class model hypothesizes  finally 
report probability example truly in domain  according model 
see  three examples model thinks general domain new
jersey  hospital government  believes me  anything kid
in domain  general  probabilities tend skewed toward     
uncommon nave bayes models  shown two errors data  first 
model thinks hospital location truly organization 
difficult distinction make  training data  hospitals often used locations 
second example error anything capable getting anything
here  long distance context example discussion biological warfare
saddam hussein  anything supposed refer type biological warhead 
model mistakingly thinks person  error likely due fact
model identifies word anything likely truly in domain  the word
common newswire   learned truly in domain entities people 
thus  lacking evidence otherwise  model incorrectly guesses anything person 
interesting observe model believes entity gives
chills closer general domain fisher thing calling ha ha
screwed up  likely occurs context ha ha occurred anywhere
out of domain training data  twice in domain training data  unlikely
example would misclassified otherwise  me fairly clearly person  
example shows model able take context account deciding domain 
decisions made model  shown table   seem qualitatively reasonable 
numbers perhaps excessively skewed  ranking believable  in domain
data primarily conversations random  not necessarily news worthy  topics 
hence highly colloquial  contrastively  out of domain data formal news 
model able learn entities new jersey government
news words kid 
   

fidomain adaptation statistical classifiers

 i 
 o 

mention
type
    
    

mention
tagging
    
    

recap
cnn
    
    

recap
abc
    
    

table    values variables discovered mega model algorithm 
    degree relatedness
section  analyze values found model  low values  i 
 o  mean in domain data significantly different out of domain data 
high values mean similar  high value means
general domain model used cases  tasks mention type 
values middling around      mention type   i        o       
indicating significant difference in domain out of domain
data  exact values tasks shown table   
values make intuitive sense  distinction conversation data
news data  for mention type task  significantly stronger difference
manually automatically transcribed newswire  for mention tagging task  
values reflect qualitative distinction  rather strong difference
values recapitalization tasks expected priori  however  post hoc
analysis shows result reasonable  compute kl divergence unigram
language model out of domain data set in domain data sets 
kl divergence cnn data       divergence abc data      
confirms abc data perhaps different baseline out of domain
cnn data  reflected values 
interested cases little difference in domain
out of domain data  simulate case  performed following experiment 
consider mention type task  use training portion out ofdomain data  randomly split data half  assigning half in domain
out of domain  theory  model learn may rely general
domain model  performed experiment ten fold cross validation found
average value selected model       strictly less
one  show model able identify similar domains 

   conclusion discussion
paper  presented mega model domain adaptation discriminative  conditional  learning framework  described efficient optimization algorithms
based conditional em technique  experimentally shown  four data sets 
model outperforms large number baseline systems  including current state
art model  requiring significantly less in domain data 
although focused specifically discriminative modeling maximum entropy
framework  believe novel  basic idea work foundedto break
in domain distribution p i  out of domain distribution p o  three distributions  q  i   
   

fidaume iii   marcu

q  o  q  g  general  particular  one could perform similar analysis case
generative models obtain similar algorithms  though case generative model 
standard em could used   model could applied domain adaptation
language modeling machine translation 
exception work described section      previous work in domain adaptation quite rare  especially discriminative learning framework  substantial literature language modeling speech community  adaptation
concerned based adapting new speakers  iyer  ostendorf    gish 
      kalai  chen  blum    rosenfeld         learning perspective  mega
model similar mixture experts model  model seen constrained experts model  three experts  constraints specify in domain
data come one two experts  out of domain data come
one two experts  with single expert overlapping two   attempts
build discriminative mixture experts models make heuristic approximations order
perform necessary optimization  jordan   jacobs         rather apply conditional
em  gives us strict guarantees monotonically increase data  incomplete 
log likelihood iteration training 
domain adaptation problem closely related multitask learning  also known
learning learn inductive transfer   multitask learning  one attempts learn
function solves many machine learning problems simultaneously  related problem
discussed thrun         caruana        baxter         among others 
similarity multitask learning domain adaptation deal
data drawn related  distinct distributions  primary difference domain
adaptation cares predicting one label type  multitask learning cares
predicting many 
various sub communities natural language processing family begin continue branch domains newswire  importance developing models
new domains without annotating much new data become important 
mega model first step toward able migrate simple classification style models  classifiers maximum entropy markov models  across domains  continued research
area adaptation likely benefit work done active learning
learning large amounts unannotated data 

acknowledgments
thank ciprian chelba alex acero making data available  thank ryan
mcdonald pointing feats baseline  previously considered 
thank kevin knight dragos munteanu discussions related project 
paper greatly improved suggestions reviewers  including reviewers
previous  shorter version  work partially supported darpa ito grant n                nsf grant iis          nsf grant iis          usc dean fellowship
hal daume iii 
   

fidomain adaptation statistical classifiers

appendix a  conditional expectation maximization
appendix  derive eq      eq     making use jensens inequality
variational bound  interested reader referred work jebara pentland
       details  discussion consider bound change log
likelihood iteration   iteration t  l c   given eq      



p   x 
p x     p  
l   log
  log
p  y   x  t   
p  x    t     p  y   t   



p x  y 
p x 
  log
log
p  x  y  t   
p  x  t   
c

    
    

here  effectively rewritten log change ratio conditionals
difference log change ratio joints log change ratio
marginals  may rewrite eq      introducing hidden variables z as 


p
p


z p x  z 
z p x  y  z 
log p
l   log p
t   
t   
z p  x  y  z 
z p  x  z 
c

    

apply jensens inequality first term eq      obtain 

c

l

x
z

 

 


p

p x  y  z   t 
p x  y  z 
z p x  z 
p
log
log p
t   
t   
p  x  y  z  t   
z   p  x  y  z  
z p  x  z 
 
 z
 

    

hx y z t 

eq       expression denoted hx y z t  joint expectation z
previous iterations parameter settings  unfortunately  cannot apply jensens inequality remaining term eq      appears negated  applying
variational dual  log x x    term  obtain following  final bound 
c



l q  

x
z



p

p x  y  z 
z p x  z 
  
hx y z t  log
p
t   
p  x  y  z  t   
z p  x  z 

    

applying bound eq      distributions chosen model yields eq      

appendix b  derivation estimation equations
given model structure parameterization mega modelgiven section     
eq      obtain following expression joint probability data 
   

fidaume iii   marcu



p x  y  z            


f
n



zn
zn
ber xnf   f  gibbs yn   xn    
ber zn    
 


n  
f   



f
n

 xnf
xnf


 
  zfn
zfn
zn      zn

n  
f   
 
h
x
 
h
exp zynn   xn
exp zcn   xn

c

    

marginal distribution obtained removing last two terms  the exp
sum exps  final equation  plugging eq      eq      using notation
eq       obtain following expression qt  
qt  

x


 

n
x

n  



log nor           i   
 

x
zn

 

f
x

f   



  

log bet  f   a  b 

hn zn log      zn   log       log n zn
 

f
x

xnf log zynn

f   

zn
mt 
n   

 

 zn

log

x
c

n zn    

 

exp

h

zcn   xn






jn z
n

 
    

well analogous term out of domain data  j defined table   
b   m step
computing   simply differentiate qt  see eq       respect   obtaining 
n

qt x hn   hn
 
 
  mt 
n  n   n    


 

    

n  

solving   leads directly quadratic expression form 

   

 

 
 

n
x

mt 
n  n  

n  

       

n
x

n  

n    

 

 hn mt 
n  n   n    
   



 

fidomain adaptation statistical classifiers

 

 

 



n
x

n  

hn

 

    

solving directly gives desired update equation 
b   m step
optimizing  i    rewrite qt   eq       neglecting irrelevant terms  as 

qt     

n
x

n  

   hn  


f
x


f   

xnf yn  f log

x
c

h

exp c   xn





  log nor        i 

    

eq       bracketed expression exactly log likelihood term obtained
standard logistic regression models  thus  optimization q respect  i 
 o  performed using weighted version standard logistic regression optimization 
weights defined   hn    case  g    obtain weighted logistic regression
model  n  i    n  o  data points  weights defined hn  
b   m step
case  i   o    rewrite eq      remove irrelevant terms  as 
qt    i     

f
x

f   

log bet f   a  b   

n
x


n  

   hn   log n   mt 
n     n  



    

due presence product term   cannot compute analytical solution
maximization problem  however  take derivatives component wise  in f  
obtain analytical solutions  when combined prior   admits iterative
solution maximizing qt maximizing component separately convergence 
computing derivatives qt respect f requires differentiating n   respect
f   convenient form  recalling notation table   


n      n   f  
 xnf f      xnf     f      n   f
f
f

    

using result  maximize qt respect f solving 
 
n
x
xnf    f      xnf  f
h
   hn  
qf
 
f
f    f  
n  
 

    

 
f    f  
 
n
x
jn       n   f
f  

jn       n   f  

 

 
n
x
 
   hn    xnf
  
f    f  
n  

   

n  

fidaume iii   marcu

equating zero yields quadratic expression form 

     f   
   f  

 

 

n
x

jn     
n  
  n
x


 

 n   f

 

  hn   jn       n   f

n  
n
x

   f       

n  

   hn   xnf

 



 
    
 o 

final equation solved analytically  similar expression arises f  
 g 

case f   obtain quadratic form sums entire data set
hn replacing occurrences    hn   

   

 

 






n  i 
  x
 i 
 i 
 g 

jn    i 


n   f

f

n  

n  

 g 

f

 

 




 g   

f

 

 o 
n
x

n  i 



x

n  

x

n  

jn    o  n   f


 i 
h i 
n   jn   n   f

n  i 

  

 i 

 i 

 o 

 o 

n  o 
 i 

h i 
n xnf  

x

n  



 o 




h o 
n xnf

 o 
n
x

n  

 o 

 o 

 o 
h o 
n   jn   n   f



 
    

again  solved analytically  values j  m       defined table   

references
averick  b  m     more  j  j          evaluation large scale optimization problems
vector parallel architectures  siam journal optimization    
baxter  j          model inductive bias learning  journal artificial intelligence
research              
bacchiani  m     roark  b          unsupervised langauge model adaptation  proceedings
international conference acoustics  speech signal processing  icassp  
caruana  r          multitask learning  knowledge based source inductive bias  machine learning            
chelba  c     acero  a          adaptation maximum entropy classifier  little data
help lot  proceedings conference empirical methods natural
language processing  emnlp   barcelona  spain 
chen  s     rosenfeld  r          gaussian prior smoothing maximum entropy
models  tech  rep  cmucs         carnegie mellon university  computer science
department 
   

fidomain adaptation statistical classifiers

della pietra  s   della pietra  v  j     lafferty  j  d          inducing features random
fields  ieee transactions pattern analysis machine intelligence             
    
dempster  a   laird  n     rubin  d          maximum likelihood incomplete data
via em algorithm  journal royal statistical society  b   
elhadad  n   kan  m  y   klavans  j     mckeown  k          customization unified
framework summarizing medical literature  journal artificial intelligence
medicine                 
forman  g          extensive empirical study feature selection metrics text classification  journal machine learning research              
gibbons  j  d     chakraborti  s          nonparametric statistical inference  marcel
dekker  inc 
gildea  d          corpus variation parser performance  proceedings conference empirical methods natural language processing  emnlp  
hobbs  j  r          information extraction biomedical text  journal biomedical
informatics                 
huang  j   zweig  g     padmanabhan  m          information extraction voicemail 
proceedings conference association computational linguistics
 acl  
hwa  r          supervised grammar induction using training data limited constituent
information  proceedings conference association computational
linguistics  acl   pp       
iyer  r   ostendorf  m     gish  h          using out of domain data improve in domain
language models  ieee signal processing        
jebara  t     pentland  a          maximum conditional likelihood via bound maximization
cem algorithm  advances neural information processing systems
 nips  
jordan  m     jacobs  r          hierarchical mixtures experts em algorithm 
neural computation            
kalai  a   chen  s   blum  a     rosenfeld  r          on line algorithms combining
language models  icassp 
lafferty  j   mccallum  a     pereira  f          conditional random fields  probabilistic
models segmenting labeling sequence data  proceedings international
conference machine learning  icml  
malouf  r          comparison algorithms maximum entropy parameter estimation 
proceedings conll 
mccallum  a   freitag  d     pereira  f          maximum entropy markov models
information extraction segmentation  proceedings international conference machine learning  icml  
   

fidaume iii   marcu

minka  t  p          comparison numerical optimizers logistic regression  http 
  www stat cmu edu  minka papers logreg  
munteanu  d     marcu  d          improving machine translation performance exploiting non parallel corpora  computational linguistics  appear 
nash  s     nocedal  j          numerical study limited memory bfgs method
truncated newton method large scale optimization  siam journal
optimization            
rambow  o   shrestha  l   chen  j     lauridsen  c          summarizing email threads 
proceedings conference north american chapter association
computational linguistics  naacl  short paper section 
roark  b     bacchiani  m          supervised unsupervised pcfg adaptation
novel domains  proceedings conference north american chapter
association computational linguistics human language technology
 naacl hlt  
smith  d  r          variational methods optimization  dover publications  inc   mineola  new york 
thrun  s          learning n th thing easier learning first  advances
neural information processing systems  nips  

   


