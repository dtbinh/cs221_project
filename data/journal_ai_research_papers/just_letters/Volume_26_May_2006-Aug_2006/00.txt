journal of artificial intelligence research              

submitted        published      

a logic for reasoning about evidence
joseph y  halpern

halpern cs cornell edu

cornell university  ithaca  ny       usa

riccardo pucella

riccardo ccs neu edu

northeastern university  boston  ma       usa

abstract
we introduce a logic for reasoning about evidence that essentially views evidence as
a function from prior beliefs  before making an observation  to posterior beliefs  after
making the observation   we provide a sound and complete axiomatization for the logic 
and consider the complexity of the decision problem  although the reasoning in the logic
is mainly propositional  we allow variables representing numbers and quantification over
them  this expressive power seems necessary to capture important properties of evidence 

   introduction
consider the following situation  essentially taken from halpern and tuttle        and
fagin and halpern         a coin is tossed  which is either fair or double headed  the coin
lands heads  how likely is it that the coin is double headed  what if the coin is tossed
   times and it lands heads each time  intuitively  it is much more likely that the coin
is double headed in the latter case than in the former  but how should the likelihood be
measured  we cannot simply compute the probability of the coin being double headed 
assigning a probability to that event requires that we have a prior probability on the coin
being double headed  for example  if the coin was chosen at random from a barrel with
one billion fair coins and one double headed coin  it is still overwhelmingly likely that the
coin is fair  and that the sequence of    heads is just unlucky  however  in the problem
statement  the prior probability is not given  we can show than any given prior probability
on the coin being double headed increases significantly as a result of seeing    heads  but 
intuitively  it seems that we should be able to say that seeing    heads in a row provides
a great deal of evidence in favor of the coin being double headed without invoking a prior 
there has been a great deal of work in trying to make this intuition precise  which we now
review 
the main feature of the coin example is that it involves a combination of probabilistic outcomes  e g   the coin tosses  and nonprobabilistic outcomes  e g   the choice of the
coin   there has been a great deal of work on reasoning about systems that combine both
probabilistic and nondeterministic choices  see  for example  vardi         fischer and zuck
        halpern  moses  and tuttle         halpern and tuttle         de alfaro        
he  seidel  and mciver         however  the observations above suggest that if we attempt
to formally analyze this situation in one of those frameworks  which essentially permit only
the modeling of probabilities  we will not be able to directly capture this intuition about
increasing likelihood  to see how this plays out  consider a formal analysis of the situation
in the halpern tuttle        framework  suppose that alice nonprobabilistically chooses

c
    
ai access foundation  all rights reserved 

fihalpern   pucella

one of two coins  a fair coin with probability     of landing heads  or a double headed coin
with probability   of landing heads  alice tosses this coin repeatedly  let k be a formula
stating  the kth coin toss lands heads  what is the probability of k according to bob 
who does not know which coin alice chose  or even the probability of alices choice 
according to the halpern tuttle framework  this can be modeled by considering the
set of runs describing the states of the system at each point in time  and partitioning
this set into two subsets  one for each coin used  in the set of runs where the fair coin
is used  the probability of k is      in the set of runs where the double headed coin is
used  the probability of k is    in this setting  the only conclusion that can be drawn is
 prb  k            prb  k          this is of course the probability from bobs point of
view  alice presumably knows which coin she is using   intuitively  this seems reasonable 
if the fair coin is chosen  the probability that the kth coin toss lands heads  according to
bob  is      if the double headed coin is chosen  the probability is    since bob does not
know which of the coins is being used  that is all that can be said 
but now suppose that  before the    st coin toss  bob learns the result of the first    
tosses  suppose  moreover  that all of these landed heads  what is the probability that the
   st coin toss lands heads  by the same analysis  it is still either     or    depending on
which coin is used 
this is hardly useful  to make matters worse  no matter how many coin tosses bob
witnesses  the probability that the next toss lands heads remains unchanged  but this
answer misses out on some important information  the fact that all of the first     coin
tosses are heads is very strong evidence that the coin is in fact double headed  indeed  a
straightforward computation using bayes rule shows that if the prior probability of the
coin being double headed is   then after observing that all of the     tosses land heads 
the probability of the coin being double headed becomes

             

 

     
 
             

however  note that it is not possible to determine the posterior probability that the coin is
double headed  or that the    st coin toss is heads  without the prior probability   after
all  if alice chooses the double headed coin with probability only         then it is still
overwhelmingly likely that the coin used is in fact fair  and that bob was just very unlucky
to see such an unrepresentative sequence of coin tosses 
none of the frameworks described above for reasoning about nondeterminism and probability takes the issue of evidence into account  on the other hand  evidence has been
discussed extensively in the philosophical literature  much of this discussion occurs in the
philosophy of science  specifically confirmation theory  where the concern has been historically to assess the support that evidence obtained through experimentation lends to various
scientific theories  carnap        popper        good        milne          kyburg       
provides a good overview of the literature  
in this paper  we introduce a logic for reasoning about evidence  our logic extends a
logic defined by fagin  halpern and megiddo         fhm from now on  for reasoning about
likelihood expressed as either probability or belief  the logic has first order quantification
over the reals  so includes the theory of real closed fields   as does the fhm logic  for
reasons that will shortly become clear  we add observations to the states  and provide an
 

fia logic for reasoning about evidence

additional operator to talk about the evidence provided by particular observations  we also
refine the language to talk about both the prior probability of hypotheses and the posterior
probability of hypotheses  taking into account the observation at the states  this lets us
write formulas that talk about the relationship between the prior probabilities  the posterior
probabilities  and the evidence provided by the observations 
we then provide a sound and complete axiomatization for the logic  to obtain such an
axiomatization  we seem to need first order quantification in a fundamental way  roughly
speaking  this is because ensuring that the evidence operator has the appropriate properties
requires us to assert the existence of suitable probability measures  it does not seem possible to do this without existential quantification  finally  we consider the complexity of the
satisfiability problem  the complexity problem for the full language requires exponential
space  since it incorporates the theory of real closed fields  for which an exponential space
lower bound is known  ben or  kozen    reif         however  we show that the satisfiability problem for a propositional fragment of the language  which is still strong enough to
allow us to express many properties of interest  is decidable in polynomial space 
it is reasonable to ask at this point why we should bother with a logic of evidence  our
claim is that many decisions in practical applications are made on the basis of evidence 
to take an example from security  consider an enforcement mechanism used to detect and
react to intrusions in a computer system  such an enforcement mechanism analyzes the
behavior of users and attempts to recognize intruders  clearly the mechanism wants to
make sensible decisions based on observations of user behaviors  how should it do this 
one way is to think of an enforcement mechanism as accumulating evidence for or against
the hypothesis that the user is an intruder  the accumulated evidence can then be used as
the basis for a decision to quarantine a user  in this context  it is not clear that there is a
reasonable way to assign a prior probability on whether a user is an intruder  if we want
to specify the behavior of such systems and prove that they meet their specifications  it is
helpful to have a logic that allows us to do this  we believe that the logic we propose here
is the first to do so 
the rest of the paper is organized as follows  in the next section  we formalize a notion
of evidence that captures the intuitions outlined above  in section    we introduce our logic
for reasoning about evidence  in section    we present an axiomatization for the logic and
show that it is sound and complete with respect to the intended models  in section    we
discuss the complexity of the decision problem of our logic  in section    we examine some
alternatives to the definition of weight of evidence we use  for ease of exposition  in most
of the paper  we consider a system where there are only two time points  before and after
the observation  in section    we extend our work to dynamic systems  where there can be
multiple pieces of evidence  obtained at different points in time  the proofs of our technical
results can be found in the appendix 

   measures of confirmation and evidence
in order to develop a logic for reasoning about evidence  we need to first formalize an
appropriate notion of evidence  in this section  we review various formalizations from the
literature  and discuss the formalization we use  evidence has been studied in depth in the
philosophical literature  under the name of confirmation theory  confirmation theory aims

 

fihalpern   pucella

at determining and measuring the support a piece of evidence provides an hypothesis  as we
mentioned in the introduction  many different measures of confirmation have been proposed
in the literature  typically  a proposal has been judged on the degree to which it satisfies
various properties that are considered appropriate for confirmation  for example  it may be
required that a piece of evidence e confirms an hypothesis h if and only if e makes h more
probable  we have no desire to enter the debate as to which class of measures of confirmation
is more appropriate  for our purposes  most confirmation functions are inappropriate 
they assume that we are given a prior on the set of hypotheses and observations  by
marginalization  we also have a prior on hypotheses  which is exactly the information we do
not have and do not want to assume  one exception is measures of evidence that use the
log likelihood ratio  in this case  rather than having a prior on hypotheses and observations 
it suffices that there be a probability h on observations for each hypothesis h  intuitively 
h  ob  is the probability of observing ob when h holds  given an observation ob  the degree
of confirmation that it provides for an hypothesis h is


h  ob 
 
l ob  h    log
h  ob 
where h represents the hypothesis other than h  recall that this approach applies only if
there are two hypotheses   thus  the degree of confirmation is the ratio between these
two probabilities  the use of the logarithm is not critical here  using it ensures that the
likelihood is positive if and only if the observation confirms the hypothesis  this approach
has been advocated by good               among others  
one problem with the log likelihood ratio measure l as we have defined it is that it can
be used only to reason about evidence discriminating between two competing hypotheses 
namely between an hypothesis h holding and the hypothesis h not holding  we would like
a measure of confirmation along the lines of the log likelihood ratio measure  but that can
handle multiple competing hypotheses  there have been a number of such generalizations 
for example  by pearl        and chan and darwiche         we focus here on the generalization given by shafer        in the context of the dempster shafer theory of evidence
based on belief functions  shafer         it was further studied by walley         the
description here is taken mostly from halpern and fagin         while this measure of
confirmation has a number of nice properties of which we take advantage  much of the work
presented in this paper can be adapted to different measures of confirmation 
we start with a finite set h of mutually exclusive and exhaustive hypotheses  thus 
exactly one hypothesis holds at any given time  let o be the set of possible observations
 or pieces of evidence   for simplicity  we assume that o is finite  just as in the case of loglikelihood  we also assume that  for each hypotheses h  h  there is a probability measure
h on o such that h  ob  is the probability of ob if hypothesis h holds  furthermore  we
assume that the observations in o are relevant to the hypotheses  for every observation
ob  o  there must be an hypothesis h such that h  ob        the measures h are often
called likelihood functions in the literature   we define an evidence space  over h and o 
   another related approach  the bayes factor approach  is based on taking the ratio of odds rather than
likelihoods  good        jeffrey         we remark that in the literature  confirmation is usually taken
with respect to some background knowledge  for ease of exposition  we ignore background knowledge
here  although it can easily be incorporated into the framework we present 

 

fia logic for reasoning about evidence

to be a tuple e    h  o     where  is a function that assigns to every hypothesis h  h
the likelihood function  h    h    for simplicity  we usually write h for  h   when the
the function  is clear from context  
given an evidence space e  we define the weight that the observation ob lends to hypothesis h  written we  ob  h   as
h  ob 
 
h  h h   ob 

we  ob  h    p

   

the measure we always lies between   and    intuitively  if we  ob  h       then ob fully
confirms h  i e   h is certainly true if ob is observed   while if we  ob  h       then ob
disconfirms h  i e  
p h is certainly false
p if ob is observed   moreover  for each fixed observation
ob for which

 ob 
 
  
h
hh
hh we  ob  h       and thus the weight of evidence
we looks like a probability measure for each ob  while this has some useful technical
consequences  one should not interpret we as a probability measure  roughly speaking  the
weight we  ob  h  is the likelihood that h is the right hypothesis in the light of observation
ob   the advantages of we over other known measures of confirmation are that  a  it is
applicable when we are not given a prior probability distribution on the hypotheses   b  it
is applicable when there are more than two competing hypotheses  and  c  it has a fairly
intuitive probabilistic interpretation 
an important problem in statistical inference  casella   berger        is that of choosing
the best parameter  i e   hypothesis  that explains observed data  when there is no prior
on the parameters  the best parameter is typically taken to be the one that maximizes
the likelihood of the data given that parameter  since we is just a normalized likelihood
function  the parameter that maximizes the likelihood will also maximize we   thus  if all
we are interested in is maximizing likelihood  there is no need to normalize the evidence as
we do  we return to the issue of normalization in section    
note that if h    h    h     then we in some sense generalizes the log likelihood ratio
measure  more precisely  for a fixed observation ob  we  ob    induces the same relative order
on hypotheses as l ob     and for a fixed hypothesis h  we    h  induces the same relative
order on observations as l   h  
proposition      for all ob  we have we  ob  hi    we  ob  h i   if and only if l ob  hi   
l ob  h i    for i         and for all h  ob  and ob     we have we  ob  h   we  ob     h  if and
only if l ob  h   l ob     h  
   we could have taken the log of the ratio to make we parallel the log likelihood ratio l defined earlier 
but there are technical advantages in having the weight of evidence be a number between   and   
   another representation of evidence that has similar characteristics to we is shafers original representation of evidence via belief functions  shafer         defined as
wes  ob  h   

h  ob 
 
maxhh h  ob 

this measure is known in statistical hypothesis testing as the generalized likelihood ratio statistic  it is
another generalization of the log likelihood ratio measure l  the main difference between we and wes is
how they behave when one considers the combination of evidence  which we discuss later in this section 
as walley        and halpern and fagin        point out  we gives more intuitive results in this case 
we remark that the parameter  hypothesis  that maximized likelihood also maximizes wes   so wes can
also be used in statistical inference 

 

fihalpern   pucella

although we  ob    behaves like a probability measure on hypotheses for every observation ob  one should not think of it as a probability  the weight of evidence of a combined
hypothesis  for instance  is not generally the sum of the weights of the individual hypotheses  halpern   pucella      a   rather  we  ob    is an encoding of evidence  but what
is evidence  halpern and fagin        have suggested that evidence can be thought of as
a function mapping a prior probability on the hypotheses to a posterior probability  based
on the observation made  there is a precise sense in which we can be viewed as a function
that maps a prior probability   on the hypotheses h to a posterior probability ob based
on observing ob  by applying dempsters rule of combination  shafer         that is 
ob      we  ob    

   

where  combines two probability distributions on h to get a new probability distribution
on h defined as follows 
p
   h    h 
        h    phh
 
hh    h    h 
 dempsters rule of combination is used to combine belief functions  the definition of  is
more complicated when considering arbitrary belief functions  but in the special case where
the belief functions are in fact probability measures  it takes the form we give here  
bayes rule is the standard way of updating a prior probability based on an observation 
but it is only applicable when we have a joint probability distribution on both the hypotheses
and the observations  or  equivalently  a prior on hypotheses together with the likelihood
functions h for h  h   something which we do not want to assume we are given  in
particular  while we are willing to assume that we are given the likelihood functions  we
are not willing to assume that we are given a prior on hypotheses  dempsters rule of
combination essentially simulates the effects of bayes rule  the relationship between
dempsters rule and bayes rule is made precise by the following well known theorem 
proposition       halpern   fagin        let e    h  o    be an evidence space  suppose
that p is a probability on h  o such that p  h   ob     h   o    h  ob  for all h  h
and all ob  o  let   be the probability on h induced by marginalizing p   that is     h   
p   h   o   for ob  o  let ob      we  ob     then ob  h    p   h   o   h   ob   
in other words  when we do have a joint probability on the hypotheses and observations  then dempsters rule of combination gives us the same result as a straightforward
application of bayes rule 
example      to get a feel for how this measure of evidence can be used  consider a
variation of the two coins example in the introduction  assume that the coin chosen by
alice is either double headed or fair  and consider sequences of a hundred tosses of that coin 
let o    m      m        the number of heads observed   and let h    f  d   where f
is the coin is fair  and d is the coin is double headed  the probability spaces associated
with the hypotheses are generated by the following probabilities for simple observations m 



     
  if m      
f  m       
d  m   
  otherwise 
 
m
 

fia logic for reasoning about evidence

 we extend by additivity to the whole set o   take e    h  o     where  f     f and
 d    d   for any observation m         the weight in favor of f is given by

     
we  m  f    

 

     m

     
      
m

    

which means that the support of m is unconditionally provided to f   indeed  any such
sequence of tosses cannot appear with the double headed coin  thus  if m         we get
that
 
     
we  m  d   
     
         m
what happens when the hundred coin tosses are all heads  it is straightforward to check
that
 
 
 
    
   
w
     
d 
 
 
we       f          
 
e
 
        
        
        
        

this time there is overwhelmingly more evidence in favor of d than f  
note that we have not assumed any prior probability  thus  we cannot talk about the
probability that the coin is fair or double headed  what we have is a quantitative assessment
of the evidence in favor of one of the hypotheses  however  if we assume a prior probability
 on the coin being fair and m heads are observed after     tosses  then the probability
that the coin is fair is   if m         if m       then  applying the rule of combination  the
posterior probability of the coin being fair is                  
t
u
can we characterize weight functions using a small number of properties  more precisely 
given sets h and o  and a function f from o  h to         are there properties of f that
ensure that there are likelihood functions  such that f   we for e    h  o     as
we saw earlier  for a fixed observation ob  f essentially acts like a probability measure on
h  however  this is not sufficient to guarantee that f is a weight function  consider the
following example  with o    ob     ob     and h    h    h    h    
f  ob     h         
f  ob     h         
f  ob     h         

f  ob     h         
f  ob     h         
f  ob     h          

it is straightforward to check that f  ob       and f  ob       are probability measures on h 
but that there is no evidence space e    h  o    such that f   we   indeed  assume that
we do have such h    h    h    by the definition of weight of evidence  and the fact that f
is that weight of evidence  we get the following system of equations 
h   ob    
h   ob     h   ob     h   ob    
h   ob    
h   ob     h   ob     h   ob    
h   ob    
h   ob     h   ob     h   ob    

h   ob    
h   ob     h   ob     h   ob    
h   ob    
h   ob     h   ob     h   ob    
h   ob    
h   ob     h   ob     h   ob    

     
     
     

     
     
      

it is now immediate that there exist   and   such that hi  ob j     j f  ob j   hi    for
i            indeed  j   h   ob j     h   ob j     h   ob j    for j         moreover  since hi is
a probability measure  we must have that
hi  ob       hi  ob         f  ob     hi       f  ob     hi       
 

fihalpern   pucella

for i            thus 
                                            
these constraints are easily seen to be unsatisfiable 
this argument generalizes to arbitrary functions f   thus  a necessary condition for f to
be a weight function is that there exists i for each observation ob i such that h  ob i    
i f  ob i   h  for each hypothesis h is a probability measure  that is    f  ob     h        
k f  ob k   h       in fact  when combined with the constraint that f  ob    is a probability
measure for a fixed ob  this condition turns out to be sufficient  as the following theorem
establishes 
theorem      let h    h            hm   and o    ob             ob n    and let f be a real valued
function with domain o  h such that f  ob  h           then there exists an evidence space
e    h  o    such that f   we if and only if f satisfies the following properties 
wf   for every ob  o  f  ob    is a probability measure on h 
p
wf   there exists x            xn     such that  for all h  h  ni   f  ob i   h xi     
this characterization is fundamental to the completeness of the axiomatization of the
logic we introduce in the next section  the characterization is complicated by the fact
that the weight of evidence is essentially a normalized likelihood  the likelihood of an
observation given a particular hypothesis is normalized using the sum of all the likelihoods
of that observation  for all possible hypotheses  one consequence of this  as we already
mentioned above  is that the weight of evidence is always between   and    and superficially
behaves like a probability measure  in section    we examine the issue of normalization
more carefully  and describe the changes to our framework that would occur were we to
take unnormalized likelihoods as weight of evidence 
let e    h  o    be an evidence space  let o be the set of sequences of observations
hob             ob k i over o   assume that the observations are independent  that is  for each basic
hypothesis h  take h  hob             ob k i   the probability of observing a particular sequence of
observations given h  to be h  ob        h  ob k    the product of the probability of making
each observation in the sequence  let e     h  o       with this assumption  it is well
known that dempsters rule of combination can be used to combine evidence in this setting 
that is 
we   hob             ob k i      we  ob            we  ob k    
 halpern   fagin        theorem       it is an easy exercise to check that the weight
provided by the sequence of observations hob             ob k i can be expressed in terms of the
weight of the individual observations 
we   ob     h     we   ob k   h 
 
   
k  
h  h we   ob   h      we   ob   h  

we   hob             ob k i  h    p

   

   we use superscript rather than subscripts to index observations in a sequence so that these observations
will not be confused with the basic observations ob             ob n in o 

 

fia logic for reasoning about evidence

if we let   be a prior probability on the hypotheses  and hob        ob k i be the probability on
the hypotheses after observing ob             ob k   we can verify that
hob       ob k i      we   hob             ob k i    
example      consider a variant of example      where we take the coin tosses as individual observations  rather than the number of heads that turn up in one hundred coin
tosses  as before  assume that the coin chosen by alice is either double headed or fair  let
o    h  t    the result of an individual coin toss  where h is the coin landed heads and
t is the coin landed tails  let h    f  d   where f is the coin is fair  and d is the
coin is double headed  let e     h  o       the probability measure h associated with
the hypothesis h are generated by the following probabilities for simple observations 
f  h   

 
 

d  h      

thus  for example  f  hh  h  t  hi          d  hh  h  hi       and h  hh  h  t  hi   
  
we can now easily verify results similar to those that were obtained in example     
for instance  the weight of observing t in favor of f is given by
we   t  f    

 
 

  

 
 

    

which again indicates that observing t provides unconditional support to f   a doubleheaded coin cannot land tails 
how about sequences of observations  the weight provided by the sequence hob             ob k i
for hypothesis h is given by equation      thus  if h   hh          hi  a sequence of a hundred
coin tosses  we can check that
we   h  f    

 

 
    
 
      

 

 
        

we   h  d   

unsurprisingly  this is the same result as in example     

    
 
 
 
 
        
        
t
u

   reasoning about evidence
we introduce a logic lfo  ev for reasoning about evidence  inspired by a logic introduced in
fhm for reasoning about probability  the logic lets us reason about the weight of evidence
of observations for hypotheses  moreover  to be able to talk about the relationship between
prior probabilities  evidence  and posterior probabilities  we provide operators to reason
about the prior and posterior probabilities of hypotheses  we remark that up to now we
have been somewhat agnostic about whether the priors exist but are not given  or not
known  or whether the prior does not exist at all  it is beyond the scope of this paper to
enter the debate about whether it always appropriate to assume the existence of a prior 
although the definition of evidence makes sense even if the priors does not exist  our logic
implicitly assumes that there are priors  although they may not be known   since we provide
 

fihalpern   pucella

operators for reasoning about the prior  we make use of these operators in some of the
examples below  however  the fragment of the logic that does not use these operators is
appropriate for prior free reasoning 
the logic has both propositional features and first order features  we take the probability of propositions and the weight of evidence of observations for hypotheses  and view
probability and evidence as propositions  but we allow first order quantification over numerical quantities  such as probabilities and evidence  the logic essentially considers two
time periods  which can be thought of as the time before an observation is made and the
time after an observation is made  in this section  we assume that exactly one observation
is made   we consider sequences of observations in section     thus  we can talk of the
probability of a formula  before an observation is made  denoted pr      the probability
of  after the observation  denoted pr    and the evidence provided by the observation ob
for an hypothesis h  denoted w ob  h   of course  we want to be able to use the logic to
relate all these quantities 
formally  we start with two finite sets of primitive propositions  h    h            hnh  
representing the hypotheses  and o    ob             ob no   representing the observations  let
lh  h   be the propositional sublanguage of hypothesis formulas obtained by taking primitive propositions in h and closing off under negation and conjunction  we use  to range
over formulas of that sublanguage 
a basic term has the form pr      pr    or w ob  h   where  is an hypothesis formula 
ob is an observation  and h is an hypothesis  as we said  we interpret pr     as the
prior probability of   pr   as the posterior probability of   and w ob  h  as the weight of
evidence of observation ob for hypothesis h  it may seem strange that we allow the language
to talk about the prior probability of hypotheses  although we have said that we do not
want to assume that the prior is known  we could  of course  simplify the syntax so that
it did not include formulas of the form pr     or pr    the advantage of having them is
that  even if the prior is not known  given our view of evidence as a function from priors
to posteriors  we can make statements such as if the prior probability of h is      ob is
observed  and the weight of evidence of ob for h is      then the posterior probability of h
is      this is just
pr   h         ob  w ob  h         pr h        
a polynomial term has the form t         tn   where each term ti is a product of integers 
basic terms  and variables  which range over the reals   a polynomial inequality formula
has the form p  c  where p is a polynomial term and c is an integer  let lfo  ev  h   o  
be the language obtained by starting out with the primitive propositions in h and o
and polynomial inequality formulas  and closing off under conjunction  negation  and firstorder quantification  let true be an abbreviation for an arbitrary propositional tautology
involving only hypotheses  such as h   h    let false be an abbreviation for true  with
this definition  true and false can be considered as part of the sublanguage lh  h   
it should be clear that while we allow only integer coefficients to appear in polynomial
terms  we can in fact express polynomial terms with rational coefficients by crossmultiplying 
for instance     pr        pr        can be represented by the polynomial inequality formula
 pr      pr         while there is no difficulty in giving a semantics to polynomial terms
that use arbitrary real coefficients  we need the restriction to integers in order to make use
  

fia logic for reasoning about evidence

of results from the theory of real closed fields in both the axiomatization of section   and
the complexity results of section   
we use obvious abbreviations where needed  such as    for          for
    x for x    pr    pr    c for pr        pr    c  pr    pr   for
pr    pr       pr    c for pr    c  pr     c for  pr    c   and pr     c
for  pr    c    pr    c   and analogous abbreviations for inequalities involving pr 
and w  
example      consider again the situation given in example      let o   the observations 
consist of primitive propositions of the form heads m   where m is an integer with    m 
     indicating that m heads out of     tosses have appeared  let h consist of the two
primitive propositions fair and doubleheaded  the computations in example     can be
written as follows 
w heads       fair                   w heads       doubleheaded                      
we can also capture the fact that the weight of evidence of an observation maps a prior
probability into a posterior probability by dempsters rule of combination  for example 
the following formula captures the update of the prior probability  of the hypothesis fair
upon observation of a hundred coin tosses landing heads 
pr   fair      w heads       fair                   pr fair                     
we develop a deductive system to derive such conclusions in the next section 

t
u

now we consider the semantics  a formula is interpreted in a world that specifies which
hypothesis is true and which observation was made  as well as an evidence space to interpret
the weight of evidence of observations and a probability distribution on the hypotheses to
interpret prior probabilities and talk about updating based on evidence   we do not need
to include a posterior probability distribution  since it can be computed from the prior and
the weights of evidence using equation       an evidential world is a tuple w    h  ob    e  
where h is a hypothesis  ob is an observation   is a probability distribution on h   and e
is an evidence space over h and o  
to interpret propositional formulas in lh  h    we associate with each hypothesis formula
 a set      of hypotheses  by induction on the structure of  
  h      h 
       h      
                           
to interpret first order formulas that may contain variables  we need a valuation v that
assigns a real number to every variable  given an evidential world w    h  ob    e  and a
valuation v  we assign to a polynomial term p a real number  p w v in a straightforward way 
 x w v   v x 
 a w v   a
 pr     w v         
  

fihalpern   pucella

 pr   w v      we  ob          
 w ob     h    w v   we  ob     h   
 t  t   w v    t   w v   t   w v
 p    p   w v    p   w v    p   w v  
note that  to interpret pr    the posterior probability of  after having observed ob  the
observation at world w   we use equation      which says that the posterior is obtained by
combining the prior probability  with we  ob    
we define what it means for a formula  to be true  or satisfied  at an evidential world
w under valuation v  written  w  v       as follows 
 w  v     h if w    h  ob    e  for some ob    e
 w  v     ob if w    h  ob    e  for some h    e
 w  v      if  w  v      
 w  v        if  w  v      and  w  v     
 w  v     p  c if  p w v  c
 w  v     x if  w  v         for all v   that agree with v on all variables but x 
if  w  v      is true for all v  we write simply w      it is easy to check that if 
is a closed formula  that is  one with no free variables   then  w  v      if and only if
 w  v          for all v  v     therefore  given a closed formula   if  m  w  v       then in fact
w      we will typically be concerned only with closed formulas  finally  if w     for
all evidential worlds w  we write     and say that  is valid  in the next section  we will
characterize axiomatically all the valid formulas of the logic 
example      the following formula is valid  that is  true in all evidential worlds 
    w ob  h           w ob  h             pr   h            ob   pr h           
in other words  at all evidential worlds where the weight of evidence of observation ob for
hypothesis h  is     and the weight of evidence of observation ob for hypothesis h  is     
it must be the case that if the prior probability of h  is at least       and ob is actually
observed  then the posterior probability of h  is at least        this shows the extent to
which we can reason about the evidence independently of the prior probabilities 
t
u
the logic imposes no restriction on the prior probabilities to be used in the models 
this implies  for instance  that the formula
fair  pr   fair     
is satisfiable  there exists an evidential world w such that the formula is true at w  in other
words  it is consistent for an hypothesis to be true  despite the prior probability of it being
true being    it is a simple matter to impose a restriction on the models that they be such
that if h is true at a world  then  h      for the prior  at that world 
  

fia logic for reasoning about evidence

we conclude this section with some remarks concerning the semantic model  our semantic model implicitly assumes that the prior probability is known and that the likelihood
functions  i e   the measures h   are known  of course  in many situations there will be
uncertainty about both  indeed  our motivation for focusing on evidence is precisely to deal
with situations where the prior is not known  handling uncertainty about the prior is easy
in our framework  since our notion of evidence is independent of the prior on hypotheses  it
is straightforward to extend our model by allowing a set of possible worlds  with a different
prior in each  but using the same evidence space for all of them  we can then extend the
logic with a knowledge operator  where a statement is known to be true if it is true in all
the worlds  this allows us to make statements like i know that the prior on hypothesis
h is between  and   since observation ob provides evidence     for h  i know that the
posterior on h given ob is between             and             
dealing with uncertainty about the likelihood functions is somewhat more subtle  to
understand the issue  suppose that one of two coins will be chosen and tossed  the bias of
coin    i e   the probability that coin   lands heads  is between     and      the bias of coin
  is between     and      here there is uncertainty about the probability that coin   will
be picked  this is uncertainty about the prior  and there is uncertainty about the bias of
each coin  this is uncertainty about the likelihood functions   the problem here is that  to
deal with this  we must consider possible worlds where there is a possibly different evidence
space in each world  it is then not obvious how to define weight of evidence  we explore
this issue in more detail in a companion paper  halpern   pucella      a  

   axiomatizing evidence
in this section we present a sound and complete axiomatization ax h   o   for our logic 
the axiomatization can be divided into four parts  the first part  consisting of the
following axiom and inference rule  accounts for first order reasoning 
taut  all substitution instances of valid formulas of first order logic with equality 
mp  from  and    infer  
instances of taut include  for example  all formulas of the form     where  is an
arbitrary formula of the logic  it also includes formulas such as  x    if x is not free
in   in particular   x h    h for hypotheses in h   and similarly for observations in
o   note that taut includes all substitution instances of valid formulas of first order logic
with equality  in other words  any valid formula of first order logic with equality where
free variables are replaced with arbitrary terms of our language  including pr      pr   
w ob  h   is an instance of taut  axiom taut can be replaced by a sound and complete
axiomatization for first order logic with equality  as given  for instance  in shoenfield       
or enderton        
the second set of axioms accounts for reasoning about polynomial inequalities  by relying
on the theory of real closed fields 
rcf  all instances of formulas valid in real closed fields  and  thus  true about the reals  
with nonlogical symbols                                     

  

fihalpern   pucella

formulas that are valid in real closed fields include  for example  the fact that addition on the
reals is associative  xyz  x y  z   x  y  z    that   is the identity for multiplication 
x x    x   and formulas relating the constant symbols  such as k           k times  and
           as for taut  we could replace rcf by a sound and complete axiomatization
for real closed fields  cf  fagin et al         shoenfield        tarski        
the third set of axioms essentially captures the fact that there is a single hypothesis
and a single observation that holds per state 
h   h       hnh  
h   hi  hj if i    j 
o   ob        ob no  
o   ob i  ob j if i    j 
these axioms illustrate a subtlety of our logic  like most propositional logics  ours is
parameterized by primitive propositions  in our case  h and o   however  while axiomatizations for propositional logics typically do not depend on the exact set of primitive
propositions  ours does  clearly  axiom h  is sound only if the hypothesis primitives are
exactly h            hnh   similarly  axiom o  is sound only if the observation primitives are
exactly ob             ob no   it is therefore important for us to identify the primitive propositions
when talking about the axiomatization ax h   o   
the last set of axioms concerns reasoning about probabilities and evidence proper  the
axioms for probability are taken from fhm 
pr   pr   true      
pr   pr        
pr   pr            pr            pr       
pr   pr         pr       if      is a propositional tautology 
axiom pr  simply says that the event true has probability    axiom pr  says that probability is nonnegative  axiom pr  captures finite additivity  it is not possible to express
countable additivity in our logic  on the other hand  just as in fhm  we do not need an
axiom for countable additivity  roughly speaking  as we establish in the next section  if
a formula is satisfiable at all  it is satisfiable in a finite structure  similar axioms capture
posterior probability formulas 
po   pr true      
po   pr      
po   pr          pr          pr     
po   pr       pr     if      is a propositional tautology 

  

fia logic for reasoning about evidence

finally  we need axioms to account for the behavior of the evidence operator w  what
are these properties  for one thing  the weight function acts essentially like a probability
on hypotheses  for each fixed observation  except that we are restricted to taking the weight
of evidence of basic hypotheses only  this gives the following axioms 
e   w ob  h     
e   w ob  h           w ob  hnh       
second  evidence connects the prior and posterior beliefs via dempsters rule of combination  as in      this is captured by the following axiom   note that  since we do not
have division in the language  we crossmultiply to clear the denominator  
e   ob   pr   h w ob  h    pr h pr   h   w ob  h           pr h pr   hnh  w ob  hnh    
this is not quite enough  as we saw in section    property wf  in theorem     is
required for a function to be an evidence function  the following axiom captures wf  in
our logic 
e   x        xno  x           xno      w ob     h   x         w ob no   h   xno    
    w ob     hnh  x         w ob no   hnh  xno      
note that axiom e  is the only axiom that requires quantification  moreover  axioms e 
and e  both depend on h and o  
as an example  we show that if h and h  are distinct hypotheses in h   then the formula
 w ob  h         w ob  h          
is provable  first  by rcf  the following valid formula of the theory of real closed fields is
provable 
xy x        y        x   y      
moreover  if  x  y  is any first order logic formula with two free variables x and y  then
 xy  x  y      w ob  h   w ob  h    
is a substitution instance of a valid formula of first order logic with equality  and hence is
an instance of taut  thus  by mp  we can prove that
w ob  h         w ob  h           w ob  h    w ob  h        
which is provably equivalent  by taut and mp  to its contrapositive
w ob  h    w ob  h         w ob  h         w ob  h           
by an argument similar to that above  using rcf  taut  mp  e   and e   we can derive
w ob  h    w ob  h       
and by mp  we obtain the desired conclusion   w ob  h         w ob  h           
  

fihalpern   pucella

theorem      ax h   o   is a sound and complete axiomatization for lfo  ev  h   o   with
respect to evidential worlds 
as usual  soundness is straightforward  and to prove completeness  it suffices to show
that if a formula  is consistent with ax h   o    it is satisfiable in an evidential structure  however  the usual approach for proving completeness in modal logic  which involves
considering maximal consistent sets and canonical structures does not work  the problem
is that there are maximal consistent sets of formulas that are not satisfiable  for example 
there is a maximal consistent set of formulas that includes pr       and pr      n for
n                 this is clearly unsatisfiable  our proof follows the techniques developed in
fhm 
to express axiom e   we needed to have quantification in the logic  this is where the
fact that our representation of evidence is normalized has a nontrivial effect on the logic  e 
corresponds to property wf   which essentially says that a function is a weight of evidence
function if one can find such a normalization factor  an interesting question is whether it
is possible to find a sound and complete axiomatization for the propositional fragment of
our logic  without quantification or variables   to do this  we need to give quantifier free
axioms to replace axiom e   this amounts to asking whether there is a simpler property
than wf  in theorem     that characterizes weight of evidence functions  this remains
an open question 

   decision procedures
in this section  we consider the decision problem for our logic  that is  the problem of
deciding whether a given formula  is satisfiable  in order to state the problem precisely 
however  we need to deal carefully with the fact that the logic is parameterized by the sets
h and o of primitive propositions representing hypotheses and observations  in most
logics  the choice of underlying primitive propositions is essentially irrelevant  for example 
if a propositional formula  that contains only primitive propositions in some set  is
true with respect to all truth assignments to   then it remains true with respect to all
truth assignments to any set      this monotonicity property does not hold here  for
example  as we have already observed  axiom h  clearly depends on the set of hypotheses
and observations  it is no longer valid if the set is changed  the same is true for o   e  
and e  
this means that we have to be careful  when stating decision problems  about the role
of h and o in the algorithm  a straightforward way to deal with this is to assume that
the satisfiability algorithm gets as input h   o   and a formula   lfo  ev  h   o    because
lfo  ev  h   o   contains the full theory of real closed fields  it is unsurprisingly difficult to
decide  for our decision procedure  we can use the exponential space algorithm of ben or 
kozen  and reif        to decide the satisfiability of real closed field formulas  we define
the length    of  to be the number of symbols required to write   where we count the
length of each coefficient as    similarly  we define kk to be the length of the longest
coefficient appearing in f   when written in binary 
theorem      there is a procedure that runs in space exponential in    kk for deciding 
given h and o   whether a formula  of lfo  ev  h   o   is satisfiable in an evidential world 
  

fia logic for reasoning about evidence

this is essentially the best we can do  since ben or  kozen  and reif        prove that
the decision problem for real closed fields is complete for exponential space  and our logic
contains the full language of real closed fields 
while we assumed that the algorithm takes as input the set of primitive propositions
h and o   this does not really affect the complexity of the algorithm  more precisely  if
we are given a formula  in lfo  ev over some set of hypotheses and observations  we can
still decide whether  is satisfiable  that is  whether there are sets h and o of primitive
propositions containing all the primitive propositions in  and an evidential world w that
satisfies  
theorem      there is a procedure that runs in space exponential in    kk for deciding
whether there exists sets of primitive propositions h and o such that   lfo  ev  h   o  
and  is satisfiable in an evidential world 
the main culprit for the exponential space complexity is the theory of real closed fields 
which we had to add to the logic to be able to even write down axiom e  of the axiomatization ax h   o     however  if we are not interested in axiomatizations  but simply in
verifying properties of probabilities and weights of evidence  we can consider the following
propositional  quantifier free  fragment of our logic  as before  we start with sets h and
o of hypothesis and observation primitives  and form the sublanguage lh of hypothesis
formulas  basic terms have the form pr      pr    and w ob  h   where  is an hypothesis
formula  ob is an observation  and h is an hypothesis  a quantifier free polynomial term
has the form a  t         an tn   where each ai is an integer and each ti is a product of
basic terms  a quantifier free polynomial inequality formula has the form p  c  where
p is a quantifier free polynomial term  and c is an integer  for instance  a quantifier free
polynomial inequality formula takes the form pr        w ob  h     pr    pr        
let lev  h   o   be the language obtained by starting out with the primitive propositions
in h and o and quantifier free polynomial inequality formulas  and closing off under conjunction and negation  since quantifier free polynomial inequality formulas are polynomial
inequality formulas  lev  h   o   is a sublanguage of lfo  ev  h   o    the logic lev  h   o  
is sufficiently expressive to express many properties of interest  for instance  it can certainly
express the general connection between priors  posteriors  and evidence captured by axiom
e   as well as specific relationships between prior probability and posterior probability
through the weight of evidence of a particular observation  as in example      reasoning
about the propositional fragment of our logic lev  h   o   is easier than the full language  
   recall that axiom e  requires existential quantification  thus  we can restrict to the sublanguage
consisting of formulas with a single block of existential quantifiers in prefix position  the satisfiability
problem for this sublanguage can be shown to be decidable in time exponential in the size of the formula
 renegar        
   in a preliminary version of this paper  halpern   pucella         we examined the quantifier free fragment
of lfo  ev  h   o   that uses only linear inequality formulas  of the form a  t         an tn  c  where each
ti is a basic term  we claimed that the problem of deciding  given h and o   whether a formula  of
this fragment is satisfiable in an evidential world is np complete  we further claimed that this result
followed from a small model theorem  if  is satisfiable  then it is satisfiable in an evidential world over
a small number of hypotheses and observations  while this small model theorem is true  our argument
that the satisfiability problem is in np also implicitly assumed that the numbers associated with the
probability measure and the evidence space in the evidential world were small  but this is not true

  

fihalpern   pucella

theorem      there is a procedure that runs in space polynomial in    kk for deciding 
given h and o   whether a formula  of lev  h   o   is satisfiable in an evidential world 
theorem     relies on cannys        procedure for deciding the validity of quantifierfree formulas in the theory of real closed fields  as in the general case  the complexity is
unaffected by whether or not the decision problem takes as input the sets h and o of
primitive propositions 
theorem      there is a procedure that runs in space polynomial in    kk for deciding
whether there exists sets of primitive propositions h and o such that   lev  h   o   and
 is satisfiable in an evidential world 

   normalized versus unnormalized likelihoods
the weight of evidence we used throughout this paper is a generalization of the log likelihood
ratio advocated by good               as we pointed out earlier  this measure of confirmation is essentially a normalized likelihood  the likelihood of an observation given a particular
hypothesis is normalized by the sum of all the likelihoods of that observation  for all possible hypotheses  what would change if we were to take the  unnormalized  likelihoods h
themselves as weight of evidence  some things would simplify  for example  wf  is a
consequence of normalization  as is the corresponding axiom e   which is the only axiom
that requires quantification 
the main argument for normalizing likelihood is the same as that for normalizing probability measures  just like probability  when using normalized likelihood  the weight of
evidence is always between   and    and provides an absolute scale against which to judge
all reports of evidence  the impact here is psychologicalit permits one to use the same
rules of thumb in all situations  since the numbers obtained are independent from the context of their use  thus  for instance  a weight of evidence of      in one situation corresponds
to the same amount of evidence as a weight of evidence of      in a different situation 
any acceptable decision based on this weight of evidence in the first situation ought to be
acceptable in the other situation as well  the importance of having such a uniform scale
depends  of course  on the intended applications 
for the sake of completeness  we now describe the changes to our framework required
to use unnormalized likelihoods as a weight of evidence  define weu  ob  h    h  ob  
in general  even though the formula  involves only linear inequality formulas  every evidential world
satisfies axiom e   this constraint enables us to write formulas for which there exist no models where
the probabilities and weights of evidence are rational  for example  consider the formula
pr   h      w ob     h     pr   h         pr   h     pr h           w ob     h         
any evidential world satisfying the formula must satisfy
pr   h      w ob     h            


   

which is irrational  the exact complexity of this fragment remains open  we can use our techniques to
show that it is in pspace  but we have no matching lower bound   in particular  it may indeed be in
np   we re examine this fragment of the logic in section    under a different interpretation of weights
of evidence 

  

fia logic for reasoning about evidence

first  note that we can update a prior probability   via a set of likelihood functions h
using a form of dempsters rule of combination  more precisely  we can define   weu  ob   
to be the probability measure defined by
   h h  ob 
 
 
h  h    h  h   ob 

    weu  ob     h    p

the logic we introduced in section   applies just as well to this new interpretation of
weights of evidence  the syntax remains unchanged  the models remain evidential worlds 
and the semantics of formulas simply take the new interpretation of weight of evidence
into account  in particular  the assignment  p w v now uses the above definition of weu   and
becomes
 pr   w v      weu  ob          
 w ob     h    w v   weu  ob     h    
the axiomatization of this new logic is slightly different and somewhat simpler than the
one in section    in particular  e  and e   which say that w ob  h  acts as a probability
measure for each fixed ob  are replaced by axioms that say that w ob  h  acts as a probability
measure for each fixed h 
e     w ob  h     
e     w ob     h         w ob no   h      
axiom e  is unchanged  since weu is updated in essentially the same way as we   axiom e 
becomes unnecessary 
what about the complexity of the decision procedure  as in section    the complexity
of the decision problem for the full logic lfo  ev  h   o   remains dominated by the complexity of reasoning in real closed fields  of course  now  we can express the full axiomatization
for the unnormalized likelihood interpretation of weight of evidence in the lev  h   o   fragment  which can be decided in polynomial space  a further advantage of the unnormalized
likelihood interpretation of weight of evidence  however  is that it leads to a useful fragment
of lev  h   o   that is perhaps easier to decide 
suppose that we are interested in reasoning exclusively about weights of evidence  with
no prior or posterior probability  this is the kind of reasoning that actually underlies
many computer science applications involving randomized algorithms  halpern   pucella 
    b   as before  we start with sets h and o of hypothesis and observation primitives 
and form the sublanguage lh of hypothesis formulas  a quantifier free linear term has
the form a  w ob     h           an w ob n   hn    where each ai is an integer  each ob i is an
observation  and each hi is an hypothesis  a quantifier free linear inequality formula has
the form p  c  where p is a quantifier free linear term and c is an integer  for example 
w ob     h     w ob  h     is a quantifier free linear inequality formula 
let lw  h   o   be the language obtained by starting out with the primitive propositions
in h and o and quantifier free linear inequality formulas  and closing off under conjunction
and negation  since quantifier free linear inequality formulas are polynomial inequality
formulas  lw  h   o   is a sublanguage of lfo  ev  h   o    reasoning about lw  h   o   is
easier than the full language  and possibly easier than the lev  h   o   fragment 
  

fihalpern   pucella

theorem      the problem of deciding  given h and o   whether a formula  of lw  h   o  
is satisfiable in an evidential world is np complete 
as in the general case  the complexity is unaffected by whether or not the decision
problem takes as input the sets h and o of primitive propositions 
theorem      the problem of deciding  for a formula   whether there exists sets of
primitive propositions h and o such that   lw  h   o   and  is satisfiable in an
evidential world is np complete 

   evidence in dynamic systems
the evidential worlds we have considered until now are essentially static  in that they model
only the situation where a single observation is made  considering such static worlds lets
us focus on the relationship between the prior and posterior probabilities on hypotheses
and the weight of evidence of a single observation  in a related paper  halpern   pucella 
    b   we consider evidence in the context of randomized algorithms  we use evidence to
characterize the information provided by  for example  a randomized algorithm for primality
when it says that a number is prime  the framework in that work is dynamic  sequences of
observations are made over time  in this section  we extend our logic to reason about the
evidence of sequences of observations  using the approach to combining evidence described
in section   
there are subtleties involved in trying to find an appropriate logic for reasoning about
situations like that in example      the most important one is the relationship between
observations and time  by way of illustration  consider the following example  bob is
expecting an email from alice stating where a rendezvous is to take place  calm under
pressure  bob is reading while he waits  we assume that bob is not concerned with the
time  for the purposes of this example  one of three things can occur at any given point in
time 
    bob does not check if he has received email 
    bob checks if he has received email  and notices he has not received an email from
alice 
    bob checks if he has received email  and notices he has received an email from alice 
how is his view of the world affected by these events  in      it should be clear that 
all things being equal  bobs view of the world does not change  no observation is made 
contrast this with     and      in      bob does make an observation  namely that he has
not yet received alices email  the fact that he checks indicates that he wants to observe a
result  in      he also makes an observation  namely that he received an email from alice 
in both of these cases  the check yields an observation  that he can use to update his view
of the world  in case      he essentially observed that nothing happened  but we emphasize
again that this is an observation  to be distinguished from the case where bob does not
even check whether email has arrived  and should be explicit in the set o in the evidence
space 

  

fia logic for reasoning about evidence

this discussion motivates the models that we use in this section  we characterize
an agents state by the observations that she has made  including possibly the nothing
happened observation  although we do not explicitly model time  it is easy to incorporate
time in our framework  since the agent can observe times or clock ticks  the models in this
section are admittedly simple  but they already highlight the issues involved in reasoning
about evidence in dynamic systems  as long as agents do not forget observations  there is
no loss of generality in associating an agents state with a sequence of observations  we do 
however  make the simplifying assumption that the same evidence space is used for all the
observations in a sequence  in other words  we assume that the evidence space is fixed for
the evolution of the system  in many situations of interest  the external world changes  the
possible observations may depend on the state of the world  as may the likelihood functions 
there are no intrinsic difficulties in extending the model to handle state changes  but the
additional details would only obscure the presentation 
in some ways  considering a dynamic setting simplifies things  rather than talking
about the prior and posterior probability using different operators  we need only a single
probability operator that represents the probability of an hypothesis at the current time 
to express the analogue of axiom e  in this logic  we need to be able to talk about the
probability at the next time step  this can be done by adding the next time operator
 to the logic  where  holds at the current time if  holds at the next time step   we
further extend the logic to talk about the weight of evidence of a sequence of observations 
 ev
we define the logic lfo
dyn as follows  as in section    we start with a set of primitive
propositions h and o   respectively representing the hypotheses and the observations 
again  let lh  h   be the propositional sublanguage of hypotheses formulas obtained by
taking primitive propositions in h and closing off under negation and conjunction  we use
 to range over formulas of that sublanguage 
a basic term now has the form pr   or w ob  h   where  is an hypothesis formula 
ob   hob             ob k i is a nonempty sequence of observations  and h is an hypothesis  if
ob   hob   i  we write w ob     h  rather than w hob   i  h   as before  a polynomial term has
the form t         tn   where each term ti is a product of integers  basic terms  and variables
 which intuitively range over the reals   a polynomial inequality formula has the form
 ev
p  c  where p is a polynomial term and c is an integer  let lfo
dyn  h   o   be the language
obtained by starting out with the primitive propositions in h and o and polynomial
inequality formulas  and closing off under conjunction  negation  first order quantification 
and application of the  operator  we use the same abbreviations as in section   
the semantics of this logic now involves models that have dynamic behavior  rather
than just considering individual worlds  we now consider sequences of worlds  which we
call runs  representing the evolution of the system over time  a model is now an infinite
run  where a run describes a possible dynamic evolution of the system  as before  a run
records the observations being made and the hypothesis that is true for the run  as well as
a probability distribution describing the prior probability of the hypothesis at the initial
state of the run  and an evidence space e  over h and o to interpret w  we define an
evidential run r to be a map from the natural numbers  representing time  to histories of
   following the discussion above  time steps are associated with new observations  thus   means that
 is true at the next time step  that is  after the next observation  this simplifies the presentation of
the logic 

  

fihalpern   pucella

the system up to that time  a history at time m records the relevant information about the
runthe hypothesis that is true  the prior probability on the hypotheses  and the evidence
space e  and the observations that have been made up to time m  hence  a history has
the form h h    e     ob             ob k i  we assume that r      h h    e   i for some h    and
e    while r m    h h    e     ob             ob m i for m      we define a point of the run to be a
pair  r  m  consisting of a run r and time m 
we associate with each propositional formula  in lh  h   a set      of hypotheses  just
as we did in section   
in order to ascribe a semantics to first order formulas that may contain variables  we
need a valuation v that assigns a real number to every variable  given a valuation v  an
evidential run r  and a point  r  m   where r m    h h    e     ob             ob m i  we can assign
to a polynomial term p a real number  p r m v using essentially the same approach as in
section   
 x r m v   v x 
 a r m v   a
 pr   r m v      we   hob             ob m i           
where r m    h h    e     ob             ob m i
 w ob  h    r m v   we   ob  h   
where r m    h h    e     ob             ob m i
 t  t   r m v    t   r m v   t   r m v
 p    p   r m  v    p   r m v    p   r m v  
we define what it means for a formula  to be true  or satisfied  at a point  r  m  of
an evidential run r under valuation v  written  r  m  v       using essentially the same
approach as in section   
 r  m  v     h if r m    h h    e          i
 r  m  v     ob if r m    h h    e             obi
 r  m  v      if  r  m  v      
 r  m  v        if  r  m  v      and  r  m  v     
 r  m  v     p  c if  p r m v  c
 r  m  v      if  r  m      v     
 r  m  v     x if  r  m  v         for all valuations v   that agree with v on all variables
but x 
if  r  m  v      is true for all v  we simply write  r  m       if  r  m      for all points
 r  m  of r  then we write r     and say that  is valid in r  finally  if r     for all
evidential runs r  we write     and say that  is valid 
it is straightforward to axiomatize this new logic  the axiomatization shows that we
can capture the combination of evidence directly in the logic  a pleasant property  most of
  

fia logic for reasoning about evidence

the axioms from section   carry over immediately  let the axiomatization axdyn  h   o  
consists of the following axioms and inference rules  first order reasoning  taut  mp   reasoning about polynomial inequalities  rcf   reasoning about hypotheses and observations
 h  h  o  o    reasoning about probabilities  po   only  since we do not have pr  in
the language   and reasoning about weights of evidence  e   e   e    as well as new axioms
we now present 
basically  the only axiom that needs replacing is e   which links prior and posterior
probabilities  since this now needs to be expressed using the  operator  moreover  we
need an axiom to relate the weight of evidence of a sequence of observation to the weight
of evidence of the individual observations  as given by equation     
e   ob  x  pr h    x  
pr h w ob  h    xpr h   w ob  h           xpr hnh  w ob  hnh    
e   w ob     h     w ob k   h    w hob             ob k i  h w ob     h       w ob k   h          
w hob             ob k i  h w ob     hnh      w ob k   hnh   
to get a complete axiomatization  we also need axioms and inference rules that capture
the properties of the temporal operator  
t            
t      
t   from  infer  
finally  we need axioms to say that the truth of hypotheses as well as the value of polynomial
terms not containing occurrences of pr is time independent 
t      
t    p  c   p  c if p does not contain an occurrence of pr 
t    x   x   
 ev
theorem      axdyn  h   o   is a sound and complete axiomatization for lfo
dyn  h   o  
with respect to evidential runs 

   conclusion
in the literature  reasoning about the effect of observations is typically done in a context
where we have a prior probability on a set of hypotheses which we can condition on the
observations made to obtain a new probability on the hypotheses that reflects the effect of
the observations  in this paper  we have presented a logic of evidence that lets us reason
about the weight of evidence of observations  independently of any prior probability on the
hypotheses  the logic is expressive enough to capture in a logical form the relationship
between a prior probability on hypotheses  the weight of evidence of observations  and the
result posterior probability on hypotheses  but we can also capture reasoning that does not
involve prior probabilities 
  

fihalpern   pucella

while the logic is essentially propositional  obtaining a sound and complete axiomatization seems to require quantification over the reals  this adds to the complexity of the
logicthe decision problem for the full logic is in exponential space  however  an interesting and potentially useful fragment  the propositional fragment  is decidable in polynomial
space 
acknowledgments  a preliminary version of this paper appeared in the proceedings of
the nineteenth conference on uncertainty in artificial intelligence  pp                this
work was mainly done while the second author was at cornell university  we thank dexter
kozen and nimrod megiddo for useful discussions  special thanks to manfred jaeger for
his careful reading of the paper and subsequent comments  manfred found the bug in our
proof that the satisfiability problem for the quantifier free fragment of lfo  ev  h   o   that
uses only linear inequality formulas is np complete  his comments also led us to discuss the
issue of normalization  we also thank the reviewers  whose comments greatly improved the
paper  this work was supported in part by nsf under grants ctc          itr         
and iis          by onr under grant n                 by the dod multidisciplinary
university research initiative  muri  program administered by the onr under grants
n                and n                 and by afosr under grant f                

appendix a  proofs
proposition      for all ob  we have we  ob  hi    we  ob  h i   if and only if l ob  hi   
l ob  h i    for i         and for all h  ob  and ob     we have we  ob  h   we  ob     h  if and
only if l ob  h   l ob     h  
proof  let ob be an arbitrary observation  the result follows from the following argument 
we  ob  hi    we  ob  h i  
iff hi  ob   hi  ob    h i  ob    h i  ob   hi  ob    h i  ob  
iff hi  ob hi  ob   h i  ob h i  ob 
iff hi  ob  h i  ob   h i  ob  hi  ob 
iff l ob  hi    l ob  h i   
a similar argument establishes the result for hypotheses 

t
u

theorem      let h    h            hm   and o    ob             ob n    and let f be a real valued
function with domain o  h such that f  ob  h           then there exists an evidence space
e    h  o  h            hm   such that f   we if and only if f satisfies the following properties 
wf   for every ob  o  f  ob    is a probability measure on h 
p
wf   there exists x            xn     such that  for all h  h  ni   f  ob i   h xi     
proof     assume that f   we for some evidence space e    h  o  h            hm    it is
routine to verify wf   that for a fixed ob  o  wep
 ob    is a probability measure on h 
to verify wf   note that we can simply take xi   h  h h   ob i   
   let f be a function from o  h to        that satisfies wf  and wf   let
x            xnh be the positive reals guaranteed by wf   it is straightforward to verify that
  

fia logic for reasoning about evidence

taking h  ob i     f  ob i   h  xi for each h  h yields an evidence space e such that f  
we  
u
t
the following lemmas are useful to prove the completeness of the axiomatizations in
this paper  these results depend on the soundness of the axiomatization ax h   o   
lemma a    ax h   o   is a sound axiomatization for the logic lfo  ev  h   o   with respect to evidential worlds 
proof  it is easy to see that each axiom is valid in evidential worlds 

t
u

lemma a    for all hypothesis formulas     h       hk is provable in ax h   o   
when         h            hk   
proof  using taut  we can show that  is provably equivalent to a formula   in disjunctive
normal form  moreover  by axiom h   we can assume without loss of generality that each
of the disjuncts in   consists of a single hypothesis  thus   is h       hk   an easy
induction on structure shows that for an hypothesis formula  and evidential world w  we
have that w     iff w    h for some h        moreover  it follows immediately from the
soundness of the axiomatization  lemma a    that   h          hk is provable iff for all
evidential worlds w  w     iff w    hi for some i              k   thus    h          hk is
provable iff         h            hk   
t
u
an easy consequence of lemma a   is that   is provably equivalent to   if and only if
                
lemma a    let  be an hypothesis formula  the formulas
p
pr h  and
pr    
h    

pr      

p

pr   h 

h    

are provable in ax h   o   
proof  let h    h            hnh   and o    ob             ob no    we prove the result for pr 
we proceed by induction on the size of       for the base case  assume that            
by lemma a    this implies that  is provably equivalent to false  by po   pr    
pr false   and it is easy to check that pr false      is provable using po   po   and po  
thus pr        as required  if          n          then         hi            hin      and by
lemma a     is provably equivalent to hi       hin     by po   pr     pr   hin      
pr   hin      it is easy to check that   hin   is provably equivalent to hin    using
h    and similarly   hin   is provably equivalent to hi       hin   thus  pr    
     hin       n  by the induction
is provable  since    hi p
pr hin       pr hi       hin   p
hypothesis  pr hi    hin     h hi      hin   pr h    h     hi   pr h   thus  pr    
 
n  
p
p
pr hin       h     hi   pr h   that is  pr     h     pr h   as required 
n  

the same argument applies mutatis mutandis for pr    using axioms pr   instead of
po   
u
t
  

fihalpern   pucella

theorem      ax h   o   is a sound and complete axiomatization for the logic with respect to evidential worlds 
proof  soundness was established in lemma a    to prove completeness  recall the following definitions  a formula  is consistent with the axiom system ax h   o   if  is
not provable from ax h   o    to prove completeness  it is sufficient to show that if  is
consistent  then it is satisfiable  that is  there exists an evidential world w and valuation v
such that  w  v      
as in the body of the paper  let h    h            hnh   and o    ob             ob no    let  be
a consistent formula  by way of contradiction  assume that  is unsatisfiable  we reduce
the formula  to an equivalent formula in the language of real closed fields  let u            unh  
v            vno   x            xnh   y            yno   and z             zn  h           z no           znnho be new variables  where 
intuitively 
 ui gets value   if hypothesis hi holds    otherwise 
 vi gets value   if observation ob i holds    otherwise 
 xi represents pr   hi   
 yi represents pr hi   
 zi j represents w ob i   hj   
let v represent that list of new variables  consider the following formulas  let h be the
formula saying that exactly one hypothesis holds 
 u       u             unh      unh       u         unh     
similarly  let o be the formula saying that exactly one observation holds 
 v       v             vno      vnh       v         vnh     
let pr be the formula that expresses that pr  is a probability measure 
pr   x          xnh     x         xnh     
similarly  let po be the formula that expresses that pr is a probability measure 
po   y          ynh     y         ynh     
finally  we need formulas saying that w is a weight of evidence function  the formula
w  p simply says that w satisfies wf   that is  it acts as a probability measure for a fixed
observation 
z            z  nh     zno            zno  nh   
z           z  nh          zno           zno  nh     
the formula w  f says that w satisfies wf  
w            wno  w           wno      z    w         zno    wno    
    z  nh w         zno  nh wno     
  

fia logic for reasoning about evidence

where w            wno are new variables 
finally  the formula w  up captures the fact that weights of evidence can be viewed as updating a prior probability into a posterior probability  via dempsters rule of combination 
 v        x  z      y  x  z           y  xnh z  nh 
    xnh z  nh   ynh x  z           ynh xnh z  nh   

 vno       x  zno      y  x  zno           y  xnh zno  nh 
    xnh zno  nh   ynh x  zno            ynh xnh zno  nh    
let  be the formula in the language of real closed fields obtained from  by replacing
each occurrence of the primitive proposition
hi by ui      each occurrencepof ob i by vi  
p
   each occurrence of pr     by hi      xi   each occurrence of pr   by hi      yi   each
occurrence of w ob i   hj   by zi j   and each occurrence of an integer coefficient k by           
 k times   finally  let   be the formula v h  o  pr  po  w  p  w  f  w  up    
it is easy to see that if  is unsatisfiable over evidential worlds  then   is false when
interpreted over the real numbers  therefore    must be a formula valid in real closed
fields  and hence an instance of rcf  thus    is provable  it is straightforward to show 
using lemma a    that  itself is provable  contradicting the fact that  is consistent 
thus   must be satisfiable  establishing completeness 
t
u
as we mentioned at the beginning of section    lfo  ev is not monotone with respect to
validity  axiom h  depends on the set of hypotheses and observations  and will in general
no longer be valid if the set is changed  the same is true for o   e   and e   we do 
however  have a form of monotonicity with respect to satisfiability  as the following lemma
shows 
lemma a    given h and o   let  be a formula of lfo  ev  h   o    and let h  h
and o  o be the hypotheses and observations that occur in   if  is satisfiable in an
evidential world over h and o   then  is satisfiable in an evidential world over  h and
 o   where   h      h      and   o      o      
proof  we do this in two steps  to clarify the presentation  first  we show that we can
add a single hypothesis and observation to h and o and preserve satisfiability of   this
means that the second step below can assume that h    h and o    o  assume that
 is satisfied in an evidential world w    h  ob    e  over h and o   so that there exists
v such that  w  v       let  h   h   h    where h is a new hypothesis not in h  
and let  o   o   ob     where ob  is a new observation not in o   define the evidential
world w     h  ob      e     over  h and  o   where e   and   are defined as follows  define the
probability measure   by taking 
 
 h  if h  h
 
  h   
 
if h   h  

  

fihalpern   pucella

similarly  define the evidence space e       h    o       derived from e    h   o     by taking 


h  ob  if h  h and ob  o



 
if h  h and ob   ob 
 h  ob   

 
if h   h and ob  o



 
if h   h and ob  ob   
thus   h extends the existing h by assigning a probability of   to the new observation ob   
in contrast  the new probability  h assigns probability   to the new observation ob    we
can check that  w    v      
the second step is to collapse all the hypotheses and observations that do not appear
in  into one of the hypotheses that do not appear in h and o  which by the previous step
are guaranteed to exist  by the previous step  we can assume that h    h and o    o 
assume  is satisfiable in an evidential world w    h  ob    e  over h and o   that is 
there exists v such that  w  v       pick an hypothesis and an observation from h and o
as follows  depending on the hypothesis h and observation ob in w  let h be h if h   h 
otherwise  let h be an arbitrary element of h  h  let  h   h   h    similarly  let ob 
be ob if ob   o  otherwise  let ob  be an arbitrary element of o  o  let  o   o   ob    
let w     h  ob      e     be an evidential world over  h and  o obtained from w as follows 
define the probability measure   by taking 
 
 h 
if h  h
   h    p
 

h  h h  h   if h   h  
define e       h    o       derived from e    h   o     by taking 


if h  h and ob  o

h  ob 


p  
 
h  ob  
if h  h and ob   ob 
 h  ob    pob o o

if h   h and ob  o

h  h h h   ob 

p

p
 


h  h h
ob   o o h   ob   if h   h and ob   ob  
we can check by induction that  w    v      

t
u

theorem      there is a procedure that runs in space exponential in    kk for deciding 
given h and o   whether a formula  of lfo  ev  h   o   is satisfiable in an evidential world 
proof  let  be a formula of lfo  ev  h   o    by lemma a     is satisfiable if we can
construct a probability measure  on  h   h   h    where h is the set of hypotheses
appearing in   and h   h  and probability measures h            hm on  o   o   ob   
 where o is the set of observations appearing in  and ob    o  such that e     h    o     
w    h  ob    e  with  w  v      for some h  ob  and v 
the aim now is to derive a formula   in the language of real closed fields that asserts
the existence of these probability measures  more precisely  we can adapt the construction
of the formula   from  in the proof of theorem      the one change we need to make
is ensure that   is polynomial in the size of   which the construction in the proof of
  

fia logic for reasoning about evidence

theorem     does not guarantee  the culprit is the fact that we encode integer constants k
as         it is straightforward to modify the construction so that we use a more efficient
representation of integer constants  namely  a binary representation  for example  we can
write    as                      which can be expressed in the language of real closed fields as
                                                 we can check that if k is a coefficient of length
k  when written in binary   it can be written as a term of length o k  in the language of
real closed fields  thus  we modify the construction of   in the proof of theorem     so
that integer constants k are represented using the above binary encoding  it is easy to see
that      is polynomial in    kk  since   h   and   o   are both polynomial in      we can
now use the exponential space algorithm of ben or  kozen  and reif        on     if   is
satisfiable  then we can construct the required probability measures  and  is satisfiable 
otherwise  no such probability measures exist  and  is unsatisfiable 
t
u
theorem      there is a procedure that runs in space exponential in    kk for deciding
whether there exist sets of primitive propositions h and o such that   lfo  ev  h   o  
and  is satisfiable in an evidential world 
proof  let h            hm be the hypotheses appearing in   and ob             ob n be the hypotheses
appearing in   let h    h            hm   h   and o    ob             ob n   ob     where h and ob 
are an hypothesis and observation not appearing in   clearly   h   and  o   are polynomial
in     by lemma a    if  is satisfiable in an evidential world  it is satisfiable in an evidential
world over h and o   by theorem      we have an algorithm to determine if  is satisfied
in an evidential world over h and o that runs in space exponential in    kk 
t
u
theorem      there is a procedure that runs in space polynomial in    kk for deciding 
given h and o   whether a formula  of lev  h   o   is satisfiable in an evidential world 
proof  the proof of this result is very similar to that of theorem      let  be a formula
of lev  h   o    by lemma a     is satisfiable if there exists a probability measure  on
 h   h   h    where h is the set of hypotheses appearing in   and h   h   probability
measures h            hm on  o   o   ob     where o is the set of observations appearing in
 and ob    o   a hypothesis h  observation o  and valuation v such that  w  v       where
w    h  ob    e  and e     h    o     
we derive a formula   in the language of real closed fields that asserts the existence
of these probability measures by adapting the construction of the formula   from  in
the proof of theorem      as in the proof of theorem      we need to make sure that  
is polynomial in the size of   which the construction in the proof of theorem     does
not guarantee  we modify the construction so that we use a more efficient representation
of integer constants  namely  a binary representation  for example  we can write    as
                     which can be expressed in the language of real closed fields as            
                                     we can check that if k is a coefficient of length k
 when written in binary   it can be written as a term of length o k  in the language of
real closed fields  we modify the construction of   in the proof of theorem     so that
integer constants k are represented using this binary encoding  it is easy to see that      is
polynomial in    kk  since   h   and   o   are both polynomial in      the key now is to
notice that the resulting formula   can be written as x        xn       for some quantifierfree formula      in this form  we can apply the polynomial space algorithm of canny       
  

fihalpern   pucella

to      if    is satisfiable  then we can construct the required probability measures  and 
is satisfiable  otherwise  no such probability measures exist  and  is unsatisfiable 
u
t
theorem      there is a procedure that runs in space polynomial in    kk for deciding
whether there exists sets of primitive propositions h and o such that   lev  h   o   and
 is satisfiable in an evidential world 
proof  let h            hm be the hypotheses appearing in   and ob             ob n be the hypotheses
appearing in   let h    h            hm   h   and o    ob             ob n   ob     where h and ob 
are an hypothesis and observation not appearing in   clearly   h   and  o   are polynomial
in     by lemma a    if  is satisfiable in an evidential world  it is satisfiable in an evidential
world over h and o   by theorem      we have an algorithm to determine if  is satisfied
in an evidential world over h and o that runs in space polynomial in    kk 
t
u
the proofs of theorem     and     rely on the following small model result  a variation
on lemma a   
lemma a    given h and o   let  be a formula of lfo  ev  h   o    and let h  h
and o  o be the hypotheses and observations that occur in   if  is satisfiable in an
evidential world over h and o   then  is satisfiable in an evidential world over  h and
 o where   h      h      and   o      o       and where  for each h   h and ob   o   the
likelihood h  ob  is a rational number with size o    kk      log      
proof  let  be a formula satisfiable in an evidential world over h and o   by lemma a   
 is satisfiable in an evidential world over  h and  o   where   h      h    and   o      o    
to force the likelihoods to be small  we adapt theorem     in fhm  which says that
if a formula f in the fhm logic is satisfiable  it is satisfiable in a structure where the
probability assigned to each state of the structure is a rational number with size o  f   kf k 
 f   log  f      the formulas in lw   h    o   are just formulas in the fhm logic  the result
adapts immediately  and yields the required bounds for the size of the likelihoods 
t
u
theorem      the problem of deciding  given h and o   whether a formula  of lw  h   o  
is satisfiable in an evidential world is np complete 
proof  to establish the lower bound  observe that we can reduce propositional satisfiability
to satisfiability in lw  h   o    more precisely  let f be a propositional formula  where
p            pn are the primitive propositions appearing in f   let o    ob             ob n   ob    be
a set of observations  where observation ob i corresponds to the primitive proposition pi  
and ob  is another  distinct  observation  let h be an arbitrary set of hypotheses  and let
h be an arbitrary hypothesis in h   consider the formula f obtained by replacing every
occurrence of pi in f by w ob i   h       it is straightforward to verify that f is satisfiable
if and only if f is satisfiable in lw  h   o     we need the extra observation ob  to take
care of the case f is satisfiable in a a model where each of p            pn is false  in that case 
w ob     h       w ob n   h       but we can take w ob    h        this establishes the lower
bound 
the upper bound is straightforward  by lemma a    an evidential world over h and
o can be guessed in time polynomial in  h      o        kk  since the prior probability
in the world requires assigning a value to  h   hypotheses  and the evidence space requires
  

fia logic for reasoning about evidence

 h   likelihood functions  each assigning a value to  o   observations  of size polynomial in
   kk  we can verify that a world satisfies  in time polynomial in    kk    h      h   
this establishes that the problem is in np 
t
u
theorem      the problem of deciding  for a formula   whether there exists sets of
primitive propositions h and o such that   lw  h   o   and  is satisfiable in an
evidential world is np complete 
proof  for the lower bound  we reduce from the decision problem of lw  h   o   over fixed
h and o   let h    h            hm   and o    ob             ob n    and let  be a formula in
lw  h   o    we can check that  is satisfiable in evidential world over h and o if and
only if    h       hm     ob        ob n   is satisfiable in an evidential world over arbitrary
 h and  o   thus  by theorem      we get our lower bound 
for the upper bound  by lemma a    if  is satisfiable  it is satisfiable in an evidential
world over h and o   where h   h   h    h consists of the hypotheses appearing in  
o   o   ob     o consists of the observations appearing in   and h and ob  are new
hypotheses and observations  thus   h            and  o            as in the proof of
theorem      such a world can be guessed in time polynomial in    kk    h      o    and
therefore in time polynomial in    kk  we can verify that this world satisfies  in time
polynomial in    kk  establishing that the problem is in np 
t
u
 ev
theorem      axdyn  h   o   is a sound and complete axiomatization for lfo
dyn  h   o  
with respect to evidential runs 
proof  it is easy to see that each axiom is valid in evidential runs  to prove completeness  we
follow the same procedure as in the proof of theorem      showing that if  is consistent 
then it is satisfiable  that is  there exists an evidential run r and valuation v such that
 r  m  v      for some point  r  m  of r 
as in the body of the paper  let h    h            hnh   and o    ob             ob no    let  be
a consistent formula  the first step of the process is to reduce the formula  to a canonical
form with respect to the  operator  intuitively  we push down every occurrence of a  to
the polynomial inequality formulas present in the formula  it is easy to see that axioms and
inference rules t t  can be used to establish that  is provably equivalent to a formula
  where every occurrence of  is in the form of subformulas n  ob  and n  p  c   where
p is a polynomial term that contains at least one occurrence of the pr operator  we use the
notation n  for          the n fold application of  to   we write    for   let n
be the maximum coefficient of  in    
by way of contradiction  assume that    and hence   is unsatisfiable  as in the proof
of theorem      we reduce the formula   to an equivalent formula in the language of real
closed fields  let u            unh   v             vn  o           v n           vnno   y             yn  o           y n           ynno   and
zhi       ik i             zhi       ik i nh  for every sequence hi            ik i  be new variables  where  intuitively 
 ui gets value   if hypothesis hi holds    otherwise 
 vin gets value   if observation ob i holds at time n    otherwise 
 yin represents pr hi   at time n 
  

fihalpern   pucella

 zhi       ik i j represents w hob i            ob ik i  hj   
the main difference with the construction in the proof of theorem     is that we have variables vin representing the observations at every time step n  rather than variables representing observations at the only time step  variables yin representing each hypothesis probability
at every time step  rather than variables representing prior and posterior probabilities  and
variables zhi       ik i j representing the weight of evidence of sequences of observations  rather
than variables representing the weight of evidence of single observations  let v represent
that list of new variables  we consider the same formulas as in the proof of theorem     
modified to account for the new variables  and the fact that we are reasoning over multiple
time steps  more specifically  the formula h is unchanged  instead of o   we consider formulas  o           n
o saying that exactly one observation holds at each time time step  where
no is given by 
 v n      v n            vnno      vnnh       v n        vnnh     
let  o    o      n
o  
similarly  instead of pr and po   we consider formulas  p           n
p expressing that pr is
a probability measure at each time step  where np is given by 
y n         ynnh     y n        ynnh     
let p    p      n
p  
similarly  we consider w  p and w  f   except where we replace variables zi j by zhii j   to
reflect the fact that we now consider sequences of observations  the formula w  up   capturing
the update of a prior probability into a posterior probability given by e   is replaced by
the formulas  w  up           n
w  up representing the update of the probability at each time step 
n
where w  up is given by the obvious generalization of w  up  
z  nh 
 v n       y n  z      y n y n  z           y n ynn 
h
n 
n 
n
z  nh   
    ynh z  nh   ynh y  z           ynnh ynn 
h

zno  nh 
 vnno       y n  zno      y n y n  zno           y n ynn 
h
n y n  z
n y n  z
    ynn 
z
 
 
 
 
y
 
y
no  nh
no   
no  nh    
nh  
nh nh
h
let  w  up    w  up      n
w  up  
finally  we need a new formula w  c capturing the relationship between the weight
of evidence of a sequence of observations  and the weight of evidence of the individual
observations  to capture axiom e  
 
zhi  i h     zhik i h    zhi       ik i h  zhi  i h     zhik i h 
 kn
       zhi       ik i h  zhi  i hnh    zhik i hnh 
 i       ik no
 
 
zhi  i hnh    zhik i hnh   zhi       ik i hnh zhi  i h     zhik i h 
 kn
       zhi       ik i hnh zhi  i hnh    zhik i hnh  
 i       ik no

  

fia logic for reasoning about evidence

let  be the formula in the language of real closed fields obtained from  by replacing
each occurrence of the primitive proposition hi by ui      each occurrence of n ob i by
vin      and within
each polynomial inequality formula n  p  c   replacing each occurrence
p
of pr   by hi      yin   each occurrence of w hob i            ob ik i  hj   by zhi       ik i j   and each
occurrence of an integer coefficient k by             k times   finally  let   be the formula
v h   o  p  w  p  w  f   w  up  w  c    
it is easy to see that if  is unsatisfiable over evidential systems  then   is false about
the real numbers  therefore    must be a formula valid in real closed fields  and hence an
instance of rcf  thus    is provable  it is straightforward to show  using the obvious
variant of lemma a   that  itself is provable  contradicting the fact that  is consistent 
thus   must be satisfiable  establishing completeness 
t
u

references
ben or  m   kozen  d     reif  j  h          the complexity of elementary algebra and
geometry  journal of computer and system sciences                 
canny  j  f          some algebraic and geometric computations in pspace  in proc    th
annual acm symposium on the theory of computing  stoc     pp         
carnap  r          logical foundations of probability  second edition   university of
chicago press 
casella  g     berger  r  l          statistical inference  second edition   duxbury 
chan  h     darwiche  a          on the revision of probabilistic beliefs using uncertain
evidence  artificial intelligence            
de alfaro  l          formal verification of probabilistic systems  ph d  thesis  stanford
university  available as technical report stan cs tr         
enderton  h  b          a mathematical introduction to logic  academic press 
fagin  r     halpern  j  y          reasoning about knowledge and probability  journal
of the acm                 
fagin  r   halpern  j  y     megiddo  n          a logic for reasoning about probabilities 
information and computation                  
fischer  m  j     zuck  l  d          reasoning about uncertainty in fault tolerant distributed systems  technical report yaleu dcs tr     yale university 
good  i  j          probability and the weighing of evidence  charles griffin   co  ltd 
good  i  j          weights of evidence  corroboration  explanatory power  information
and the utility of experiments  journal of the royal statistical society  series b     
       
halpern  j  y     fagin  r          two views of belief  belief as generalized probability
and belief as evidence  artificial intelligence             
halpern  j  y   moses  y     tuttle  m  r          a knowledge based analysis of zero
knowledge  in proc    th annual acm symposium on the theory of computing
 stoc     pp         
  

fihalpern   pucella

halpern  j  y     pucella  r          a logic for reasoning about evidence  in proc    th
conference on uncertainty in artificial intelligence  uai     pp         
halpern  j  y     pucella  r       a   evidence with uncertain likelihoods  in proc    th
conference on uncertainty in artificial intelligence  uai     pp         
halpern  j  y     pucella  r       b   probabilistic algorithmic knowledge  logical methods
in computer science          
halpern  j  y     tuttle  m  r          knowledge  probability  and adversaries  journal
of the acm                 
he  j   seidel  k     mciver  a          probabilistic models for the guarded command
language  science of computer programming                  
jeffrey  r  c          probability and the art of judgement  cambridge university press 
kyburg  jr   h  e          recent work in inductive logic  in machan  t     lucey  k 
 eds    recent work in philosophy  pp         rowman   allanheld 
milne  p          log p h eb  p h b   is the one true measure of confirmation  philosophy of
science           
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann 
popper  k  r          the logic of scientific discovery  hutchinson 
renegar  j          on the computational complexity and geometry of the first order theory
of the reals  journal of symbolic computation                 
shafer  g          a mathematical theory of evidence  princeton university press 
shafer  g          belief functions and parametric models  with commentary   journal of
the royal statistical society  series b             
shoenfield  j  r          mathematical logic  addison wesley  reading  mass 
tarski  a          a decision method for elementary algebra and geometry   nd edition  
univ  of california press 
vardi  m  y          automatic verification of probabilistic concurrent finite state programs 
in proc    th ieee symposium on the foundations of computer science  focs    
pp         
walley  p          belief function representations of statistical evidence  annals of statistics 
                 

  

fi