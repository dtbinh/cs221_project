journal of artificial intelligence research                  

submitted       published     

domain adaptation for statistical classifiers
hal daume iii
daniel marcu

hdaume isi edu
marcu isi edu

information sciences institute
university of southern california
     admiralty way  suite     
marina del rey  ca       usa

abstract
the most basic assumption used in statistical learning theory is that training data
and test data are drawn from the same underlying distribution  unfortunately  in many
applications  the in domain test data is drawn from a distribution that is related  but
not identical  to the out of domain distribution of the training data  we consider the
common case in which labeled out of domain data is plentiful  but labeled in domain data is
scarce  we introduce a statistical formulation of this problem in terms of a simple mixture
model and present an instantiation of this framework to maximum entropy classifiers and
their linear chain counterparts  we present efficient inference algorithms for this special
case based on the technique of conditional expectation maximization  our experimental
results show that our approach leads to improved performance on three real world tasks
on four different data sets from the natural language processing domain 

   introduction
the generalization properties of most current statistical learning techniques are predicated
on the assumption that the training data and test data come from the same underlying
probability distribution  unfortunately  in many applications  this assumption is inaccurate 
it is often the case that plentiful labeled data exists in one domain  or coming from one
distribution   but one desires a statistical model that performs well on another related  but
not identical domain  hand labeling data in the new domain is a costly enterprise  and one
often wishes to be able to leverage the original  out of domain data when building a model
for the new  in domain data  we do not seek to eliminate the annotation of in domain
data  but instead seek to minimize the amount of new annotation effort required to achieve
good performance  this problem is known both as domain adaptation and transfer 
in this paper  we present a novel framework for understanding the domain adaptation
problem  the key idea in our framework is to treat the in domain data as drawn from
a mixture of two distributions  a truly in domain distribution and a general domain
distribution  similarly  the out of domain data is treated as if drawn from a mixture of
a truly out of domain distribution and a general domain distribution  we apply this
framework in the context of conditional classification models and conditional linear chain
sequence labeling models  for which inference may be efficiently solved using the technique
of conditional expectation maximization  we apply our model to four data sets with varying degrees of divergence between the in domain and out of domain data and obtain
c
    
ai access foundation  all rights reserved 

fidaume iii   marcu

predictive accuracies higher than any of a large number of baseline systems and a second
model proposed in the literature for this problem 
the domain adaptation problem arises very frequently in the natural language processing domain  in which millions of dollars have been spent annotating text resources for
morphological  syntactic and semantic information  however  most of these resources are
based on text from the news domain  in most cases  the wall street journal   the sort
of language that appears in text from the wall street journal is highly specialized and is 
in most circumstances  a poor match to other domains  for instance  there has been a
recent surge of interest in performing summarization  elhadad  kan  klavans    mckeown        or information extraction  hobbs        of biomedical texts  summarization of
electronic mail  rambow  shrestha  chen    lauridsen         information extraction from
transcriptions of meetings  conversations or voice mail  huang  zweig    padmanabhan 
       among others  conversely  in the machine translation domain  most of the parallel
resources that machine translation system depend on for parameter estimation are drawn
from transcripts of political meetings  yet the translation systems are often targeted at news
data  munteanu   marcu        

   statistical domain adaptation
in the multiclass classification problem  one typically assumes the existence of a training set
d     xn   yn    x  y      n  n    where x is the input space and y is a finite set  it is
assumed that each  xn   yn   is drawn from a fixed  but unknown base distribution p and that
the training set is independent and identically distributed  given p  the learning problem
is to find a function f   x  y that obtains high predictive accuracy  this is typically
done either by explicitly minimizing the regularized empirical error  or by maximizing the
probabilities of the model parameters  
    domain adaptation
in the context of domain adaptation  the situation becomes more complicated  we assume
that we are given two sets of training data  d  o  and d i    the out of domain and indomain data sets  respectively  we no longer assume that there is a single fixed  but
known distribution from which these are drawn  but rather assume that d  o  is drawn from
a distribution p o  and d i  is drawn from a distribution p i    the learning problem is to
find a function f that obtains high predictive accuracy on data drawn from p  i     indeed 
our model will turn out to be symmetric with respect to d  i  and d o    but in the contexts
we consider obtaining a good predictive model of d  i  makes more intuitive sense   we will
assume that  d  o      n  o  and  d i      n  i    where typically we have n  i   n  o    as
before  we will assume that the n  o  out of domain data points are drawn iid from p o  and
that the n  i  in domain data points are drawn iid from p i   
obtaining a good adaptation model requires the careful modeling of the relationship
between p i  and p o    if these two distributions are independent  in the obvious intuitive
sense   then the out of domain data d  o  is useless for building a model of p i  and we may as
well ignore it  on the other hand  if p i  and p o  are identical  then there is no adaptation
necessary and we can simply use a standard learning algorithm  in practical problems 
though  p i  and p o  are neither identical nor independent 
   

fidomain adaptation for statistical classifiers

    prior work
there has been relatively little prior work on this problem  and nearly all of it has focused on
specific problem domains  such as n gram language models or generative syntactic parsing
models  the standard approach used is to treat the out of domain data as prior knowledge
and then to estimate maximum a posterior values for the model parameters under this prior
distribution  this approach has been applied successfully to language modeling  bacchiani
  roark        and parsing  roark   bacchiani         also in the parsing domain  hwa
       and gildea        have shown that simple techniques based on using carefully chosen
subsets of the data and parameter pruning can improve the performance of an adapted
parser  these models assume a data distribution p  d     with parameters  and a prior
distribution over these parameters p       with hyper parameters   they estimate the
 hyperparameters from the out of domain data and then find the maximum a posteriori
parameters for the in domain data  with the prior fixed 
in the context of conditional and discriminative models  the only domain adaptation
work of which we are aware is the model of chelba and acero         this model again
uses the out of domain data to estimate a prior distribution  but does so in the context
of a maximum entropy model  specifically  a maximum entropy model is trained on the
out of domain data  yielding optimal weights for that problem  these weights are then used
as the mean weights for the gaussian prior on the learned weights for the in domain data 
though effective experimentally  the practice of estimating a prior distribution from
out of domain data and fixing it for the estimation of in domain data leaves much to be
desired  theoretically  it is strange to estimate and fix a prior distribution from data  this is
made more apparent by considering the form of these models  denoting the in domain data
and parameters by d  i  and   respectively  and the out of domain data and parameters by
d o  and   we obtain the following form for these prior estimation models 




   arg max p    arg max p    p d




 o 

 

 i 
 
p d  

   

one would have a very difficult time rationalizing this optimization problem by anything
other than experimental performance  moreover  these models are unusual in that they do
not treat the in domain data and the out of domain data identically  intuitively  there is
no difference in the two sets of data  they simply come from different  related distributions 
yet  the prior based models are highly asymmetric with respect to the two data sets  this
also makes generalization to more than one out of domain data set difficult  finally  as
we will see  the model we propose in this paper  which alleviates all of these problems 
outperforms them experimentally 
a second generic approach to the domain adaptation problem is to build an out of
domain model and use its predictions as features for the in domain data  this has been
successfully used in the context of named entity tagging      this approach is attractive
because it makes no assumptions about the underlying classifier  in fact  multiple classifiers
can be used 
   

fidaume iii   marcu

    our framework
in this paper  we propose the following relationship between the in domain and the out ofdomain distributions  we assume that instead of two underlying distributions  there are
actually three underlying distributions  which we will denote q  o    q  g  and q  i    we then
consider p o  to be a mixture of q  o  and q  g    and consider p i  to be a mixture of q  i  and
q  g    one can intuitively view the q  o  distribution as a distribution of data that is truly
out of domain  q  i  as a distribution of data that is truly in domain and q  g  as a distribution
of data that is general to both domains  thus  knowing q  g  and q  i  is sufficient to build
a model of the in domain data  the out of domain data can help us by providing more
information about q  g  than is available by just considering the in domain data 
for example  in part of speech tagging  the assignment of the tag determiner  dt  to
the word the is likely to be a general decision  independent of domain  however  in the
wall street journal  monitor is almost always a verb  vb   but in technical documentation
it will most likely be a noun  the q  g  distribution should account for the case of the dt 
the q  o  should account for monitor vb and q  i  should account for monitor nn 

   domain adaptation in maximum entropy models
the domain adaptation framework outlined in section     is completely general in that
it can be applied to any statistical learning model  in this section we apply it to loglinear conditional maximum entropy models and their linear chain counterparts  since these
models have proved quite effective in many learning tasks  we will first review the maximum
entropy framework  then will extend it to the domain adaptation problem  finally we will
discuss domain adaptation in linear chain maximum entropy models 
    maximum entropy models
the maximum entropy framework seeks a conditional distribution p  y   x  that is closest
 in the sense of kl divergence  to the uniform distribution but also matches a set of training data d with respect to feature function expectations  della pietra  della pietra   
lafferty         by introducing one lagrange multiplier i for each feature function fi   this
optimization problem results in a probability distribution of the form 
p  y   x      

 
z x

h
i
exp   f  x  y 

   

p
here  u  v denotes the scalar product of two vectors u and v  given by  u  v   i ui vi  
the normalization constant in eq      z x   is obtained by summing the exponential over
all possible classes y    y  this probability distribution is also known as an exponential
distribution or a gibbs distribution  the learning  or optimization  problem is to find the
vector  that maximizes the likelihood in eq      in practice  to prevent over fitting  one
typically optimizes a penalized  log  likelihood  where an isotropic gaussian prior with mean
  and covariance matrix    i is placed over the parameters   chen   rosenfeld        
the graphical model for the standard maximum entropy model is depicted on the left of
figure    in this figure  circular nodes correspond to random variables and square nodes
   

fidomain adaptation for statistical classifiers

correspond to fixed variables  shaded nodes are observed in the training data and empty
nodes are hidden or unobserved  arrows denote conditional dependencies 
in general  the feature functions f  x  y  may be arbitrary real valued functions  however 
in this paper we will restrict our attention to binary features  in practice  this is not a harsh
restriction  many problems in the natural language domain naturally employ only binary
features  for real valued features  binning techniques can be applied   additionally  for
notational convenience  we will assume that the features fi  x  y  can be written in product
form as gi  y hi  x  for arbitrary binary functions g over outputs and binary features h over
inputs  the latter assumption means that we can consider x to be a binary vector where
xi   hi  x   in the following this will simplify notation significantly  the extension to the full
case is straightforward  but messy  and is therefore not considered in the remainder of this
paper   by considering x as a vector  we may move the class dependence to the parameters
and consider  to be a matrix where y i is the weight for hi for class y  we will write
y to refer to the column vector of  corresponding to class y  as x is also considered a
column vector  we write y   x as shorthand for the dot product between x and the weights
for class y  under this modified notation  we may rewrite eq     as 
p  y   x      

 
z x

h
i
exp y   x

   

combining this with a gaussian prior on the weights  we obtain the following form for
the log posterior of a data set 


n
i
h
x
x
 
yn   xn  log
l   log p     d   s            
exp y    xn    const
 
 
n  

   

y y

the parameters  can be estimated using any convex optimization technique  in practice 
limited memory bfgs  nash   nocedal        averick   more        seems to be a good
choice  malouf        minka        and we will use this algorithm for the experiments
described in this paper  in order to perform these calculations  one must be able to compute
the gradient of eq     with respect to   which is available in closed form 
    the maximum entropy genre adaptation model
extending the maximum entropy model to account for both in domain and out of domain
data in the framework described earlier requires the addition of several extra model param i   i 
eters  in particular  for each in domain data point  xn   yn    we assume the existence of a
 i 
 i 
 i   i 
binary indicator variable zn   a value zn     indicates that  xn   yn   is drawn from q  i 
 i 
 the truly in domain distribution   while a value zn     indicates that it is drawn from q  g 
 o   o 
 the general domain distribution   similarly  for each out of domain data point  x n   yn   
 o 
 o 
we assume a binary indicator variable zn   where zn     means this data point is drawn
 o 
from q  the truly out of domain distribution  and a value of   means that it is drawn
from q  g   the general domain distribution   of course  these indicator variables are not
observed in the data  so we must infer their values automatically 
   

fidaume iii   marcu

 

 



i

g

yni

yn
xn

xni

zni

n

i

o

yno

n

xno

i





i

zno



g

o



n

o

o

figure     left  the standard logistic regression model   right  the mega model 
according to this model  the zn s are binary random variables that we assume are
drawn from a bernoulli distribution with parameter   i   for in domain  and   o   for outof domain   furthermore  we assume that there are three  vectors    i     o  and  g 
corresponding to q  i    q  o  and q  g    respectively  for instance  if zn      then we assume
 i 
that xn should be classified using  i    finally  we model the binary vectors xn s  respec o 
tively xn s  as being drawn independently from bernoulli distributions parameterized by
 i 
 and   g   respectively    o  and   g     again  when zn      we assume that xn is
drawn according to   i    this corresponds to a nave bayes assumption over the generative
probabilities of the xn vectors  finally  we place a common beta prior over the nave bayes
parameters    allowing  to range over  i  o  g   the full hierarchical model is 
  

 f   a  b
 i 
zn     i 
 i 

 i 

 i 

 i 

 i 

 g 

xnf   zn    f    f
 i 

       
 o 
zn     o 

 bet a  b 
 ber   i   
z  i 

 ber  fn  

 i 

 i 

yn   xn   zn    i     g   gibbs xn   zn  

 o 

 o 

 o 

 o 

 o 

 g 

xnf   zn    f    f
 o 

 nor       i 
 ber   o   

   

z  o 

 ber  fn  

 o 

 o 

yn   xn   zn    o     g   gibbs xn   zn  

we term this model the maximum entropy genre adaptation model  the mega
model   the corresponding graphical model is shown on the right in figure    the generative story for an in domain data point x i  is as follows 
   select whether x i  will be truly in domain or general domain and indicate this by
z  i    i  g   choose z  i    i with probability   i  and z  i    g with probability
     i   
 i 

   for each component f of x i    choose xf to be   with probability  zf
z  i 

probability     f  
 i 

   choose a class y according to eq     using the parameter vector  z  
   

 i 

and   with

fidomain adaptation for statistical classifiers

the story for out of domain data points is identical  but uses the truly out of domain
and general domain parameters  rather than the truly in domain parameters and generaldomain parameters 
    linear chain models
the straightforward extension of the maximum entropy classification model to the maximum
entropy markov model  memm   mccallum  freitag    pereira        is obtained by
assuming that the targets yn are sequences of labels  the canonical example for this model
is part of speech tagging  each word in a sequence is assigned a part of speech tag  by
introducing a first order markov assumption on the tag sequence  one obtains a linear chain
model that can be viewed as the discriminative counterpart to the standard  generative 
hidden markov model  the parameters of these models can be estimated again using
limited memory bfgs  the extension of the mega model to the linear chain framework is
similarly straightforward  under the assumption that each label  part of speech tag  has its
own indicator variable z  versus a global indicator variable z for the entire tag sequence  
the techniques described herein may also be applied to the conditional random field
framework of lafferty  mccallum  and pereira         which fixes a bias problem of the
memm by performing global normalization rather than per state normalization  there is 
however  a subtle difficulty in a direct application to crfs  specifically  one would need
to decide if a single z variable would be assigned to an entire sentence  or to each word
individually  in the memm case  it is most natural to have one z per word  however  to
do so in a crf would be computationally more expensive  in the remainder  we continue
to use the memm model for efficiency purposes 

   conditional expectation maximization
inference in the mega model is slightly more complex than in standard maximum entropy models  however  inference can be solved efficiently using conditional expectation
maximization  cem   a variant of the standard expectation maximization  em  algorithm
 dempster  laird    rubin         due to jebara and pentland         at a high level  em
is useful for computing in generative models with hidden variables  while cem is useful for
computing in discriminative models with hidden variables  the mega model belongs to the
latter family  so cem is the appropriate choice 
the standard em family of algorithms maximizes a joint likelihood over data  in
particular  if  xn   yn  n
n   are data and z is a  discrete  hidden variable  the m step of em
proceeds by maximizing the bound given in eq    
log p  x  y       log

x
z

p  z  x  y       log ezp    x   p  x  y   z   

   

in eq      ez denotes an expectation  one may now apply jensens inequality to this
equation  which states that f  e x    e f  x   whenever f is convex  taking f   log  we
are able to decompose the log of an expectation into the expectation of a log  this typically
separates terms and makes taking derivatives and solving the resolution optimization problem tractable  unfortunately  em cannot be directly applied to conditional models  such
   

fidaume iii   marcu

as the mega model  of the form in eq     because such models result in an m step that
requires the maximization of an equation of the form given in eq     
log p  y   x      log
l   log

x
z

x
z

p  z  y   x      log ezp    x   p  y   x  z   
p  z  x  y      log

x
z

p  z  x    

   
   

jensens inequality can be applied to the first term in eq      which can be maximized
readily as in standard em  however  applying jensens inequality to the second term would
lead to an upper bound on the likelihood  since that term appears negated 
the conditional em solution  jebara   pentland        is to bound the change in
log likelihood between iterations  rather than the log likelihood itself  the change in loglikelihood can be written as in eq      where t denotes the parameters at iteration t 


lc   log p y   x  t  log p y   x  t 

   

by rewriting the conditional distribution p  y   x  as p  x  y  divided by p  x   we can
express lc as the log of the joint distribution difference minus the log of the marginal
distribution  here  we can apply jensens inequality to the first term  the joint difference  
but not to the second  because it appears negated   fortunately  jensens is not the only
bound we can employ  the standard variational upper bound of the logarithm function is 
log x  x     this leads to a lower bound of the negation  which is exactly what is desired 
this bound is attractive for other reasons      it is tangent to the logarithm      it is tight 
    it makes contact at the current operating point  according to the maximization at the
previous time step       it is a simply linear function  and     in the terminology of the
calculus of variations  it is the variational dual to the logarithm  see  smith        
applying jensens inequality to the first term in eq     and the variational dual to the
second term  we obtain that the change of log likelihood in moving from model parameters
t  at time t    to t at time t  which we shall denote qt   is bounded by l  qt   where
qt is defined by eq       where h   e z   x    when z     and    e z   x    when
z      with expectations taken with respect to the parameters from the previous iteration 


p
t
x
p z  x  y   t
z p z  x   
t
  
    
q  
hz log
p
t   
p  z  x  y   t   
z p  z  x   
zz

by applying the two bounds  jensens inequality and the variational bound   we have
removed all sums of logs  which are hard to deal with analytically  the full derivation is
given in appendix a  the remaining expression is a lower bound on the change in likelihood 
and maximization of it will result in maximization of the likelihood 
as in the map variant of standard em  there is no change to the e step when priors
are placed on the parameters  the assumption in standard em is that we wish to maximize
p     x  y   p    p  y     x  where the prior probability of  is ignored  leaving just the
likelihood term of the parameters given the data  in map estimation  we do not make this
assumption and instead use a true prior p     in doing so  we need only to add a factor of
log p    to the definition of qt in eq      
   

fidomain adaptation for statistical classifiers

t 
jn z
n

mt 
n


  log p xn   yn   zn   t 
n zn

p
t    
 
n zn  f  
z n p x n   zn   



xnf 
 xnf
zn
zn

 


f   
f
f

xnf 
 xnf
q
zn
zn

 
 


 
f   f
f
f

 

qf

table    notation used for mega model equations 
it is important to note that although we do make use of a full joint distribution p  x  y  z  
the objective function of our model is conditional  the joint distribution is only used in the
process of creating the bound  the overall optimization is to maximize the conditional likelihood of the labels given the input  in particular  the bound using the full joint likelihood
holds for any parameters of the marginal 

   parameter estimation for the mega model
as made explicit in eq       the relevant distributions for performing cem are the full joint
distributions over the input variables x  the output variables y  and the hidden variables z 
additionally  we require the marginal distribution over the x variables and the z variables 
finally  we need to compute expectations over the z variables  we will derive the expectation
step in this section and present the final solution for the maximization step for each class
of variables  the derivation of the equations for the maximization is given in appendix b 
the q bound on complete conditional likelihood for the mega modelis given below 





p
 i 
 i 
 i 
 i 
 i 
z
 
x
p
p
z
 
x
 
y
 i 
x
n
n
n
n
n
z


    
p n 
h i 
qt  
n log
 i 
 i 
 i 
  z  i    x i 
 
p z n   x n   yn
 i  p
n
n
n   z  i 
zn
n






p
 o 
 o   o 
 o 
 o 
 o 
n
p zn   xn   yn
 o  p zn   xn
x x
z

    

p n 
h o 
 
n log
 o 
 o 
 o 
 o 
 o 
 
 
p z n   x n   yn
z n   xn
 o  p
n   z  o 
z
n  i 
x



    

n

n

in this equation  p     is the probability distribution at the previous iteration  the first
term in eq      is the bound for the in domain data  while the second term is the bound for
the out of domain data  in all the optimizations described in this section  there are nearly
identical terms for the in domain parameters and the out of domain parameters  for brevity 
we will only explicitly write the equations for the in domain parameters  the corresponding
out of domain equations can be easily derived from these  moreover  to reduce notational
overload  we will elide the superscripts denoting in domain and out of domain when obvious
from context  for notational brevity  we will use the notation depicted in table   
    expectation step
the e step is concerned with calculating hn given current model parameters  since zn 
        we easily find hn   p  zn        which can be calculated as follows 
   

fidaume iii   marcu

p  zn   z   xn   yn        
p  zn   z     p  xn     zn   z  p  yn     zn   z 
  p
z p  zn   z     p  xn     zn   z  p  yn     zn   z 
i
h
 
  z       z n z
exp zyn   xn
zxn  z

    

here  z is the partition function from before  this can be easily calculated for z        
and the expectation can be found by dividing the value for z     by the sum over both 
    m step for 
as shown in appendix b    we can directly compute
the value of  by solving a simple

quadratic equation  we can compute  as a   a   b  where 
a  
b  
    m step for 

pn

t 
n    hn  mn  n    n    
pn
  n   mt 
n  n    n    
pn
n   hn
 pn
t 
n   mn  n    n    

 



viewing qt as a function of   it is easy to see that optimization for this variable is convex 
an analytical solution is not available  but the gradient of qt with respect to  i  can be
seen to be identical to the gradient of the standard maximum entropy posterior  eq      but
where each data point is weighted according to its posterior probability      h n    we may
thus use identical optimization techniques for computing optimal  variables as for standard
maximum entropy models  the only difference is that the data points are now weighted  a
similar story holds for  o    in the case of  g    we obtain the standard maximum entropy
 i 
gradient  computed over all n  i    n  o  data points  where each xn is weighted by hn and
 o 
 o 
each xn is weighted by hn   this is shown in appendix b   
    m step for 
like the case for   we cannot obtain an analytical solution for finding the  that maximizes
qt   however  we can compute simple derivatives for qt with respect to a single component
 i 
 f which can be maximized analytically  as shown in appendix b    we can compute f

as a   a   b  where 
pn



n      hn   jn        n   f
a   
p
  n
n   jn        n   f
pn
    hn   xnf
  
b   pn n  
n   jn        n   f

   



fidomain adaptation for statistical classifiers

algorithm megacem
  
  
initialize f        f                for all    g  i  o  and all f  
while parameters havent converged or iterations remain do
   expectation step   
for n      n  i  do
 i 
compute the in domain marginal probabilities  mn
 i 
compute the in domain expectations  hn   by eq     
end for
for n      n  o  do
 o 
compute the out of domain marginal probabilities  mn
 o 
compute the out of domain expectations  hn by eq     
end for
   maximization step   
analytically update   i  and   o  according to the equations shown in section    
optimize  i     o  and  g  using bfgs
while iterations remain and or  havent converged do
update s according to derivation in section    
end while
end while
return     
figure    the full training algorithm for the mega model 
the case for   o  is identical  for   g    the only difference is that we must replace each
sum to over the data points with two sums  one for each of the in domain and out of domain
points  and  as before  the    hn s must be replaced with hn   this is made explicit in the
appendix  thus  to optimize the  variables  we simply iterate through and optimize each
component analytically  as given above  until convergence 
    training algorithm
the full training algorithm is depicted in figure    convergence properties of the cem
algorithm ensure that this will converge to a  local  maximum in the posterior space  if local
optima become a problem in practice  one can alternatively use a stochastic optimization
algorithm  in which a temperature is applied enabling the optimization to jump out of local
optima early on  however  we do not explore this idea further in this work  in the context
of our application  this extension was not required 
    cem convergence
one immediate question about the conditional em model we have described is how many
em iterations are required for the model to converge  in our experiments    iterations of
   

fidaume iii   marcu

convergence of cem optimization
  
  
  

negative log likelihood    e  

  
  
  
  
 
 
 
 
 

 

 

 
 
number of iterations

 

 

figure    convergence of training algorithm 

cem is more than sufficient  and often only   or   are necessary  to make this more clear 
in figure    we have plotted the negative complete log likelihood of the model on the first
data set  described below in section      there are three separate maximizations in the full
training algorithm  see figure     the first involves updating the  variables  the second
involves optimizing the  variables and the third involves optimizing the  variables  we
compute the likelihood after each of these steps 
running a total   cem iterations is still relatively efficient in our model  the dominating expense is in the weighted maximum entropy optimization  which  at   cem iterations 
must be computed    times  each iteration requires the optimization of each of the three
sets of  variables   at worst this will take    times the amount of time to train a model on
the complete data set  the union of the in domain and out of domain data   but in practice
we can resume each optimization at the ending point of the previous iteration  which causes
the subsequent optimizations to take much less time 
    prediction
once training has supplied us with model parameters  the subsequent task is to apply these
parameters to unseen data to obtain class predictions  we assume all this test data is indomain  i e   is drawn either from q i  or q g  in the notation of the introduction   and
obtain a decision rule of the form given in eq      for a new test point x 

y   arg max p  y   x   
yy
x
  arg max
p  z   x    p  y   x  z   
yy

  arg max
yy

z

x
z

p  z     p  x   z    p  y   x  z   
   

fidomain adaptation for statistical classifiers



  arg max  

f
y



 g 

f

xf 

 g 

   f

 xf




i
h
 g 
exp y   x

zx  g 
h
i


 x
f 
xf 
 xf exp  i 
y
y
 i 
 i 

        
f
   f
zx  i 
yy

f   

    

f   

thus  the decision rule is to simply select the class which has highest probability according to the maximum entropy classifiers  weighted linearly by the marginal probabilities
of the new data point being drawn from q i  versus q g    in this sense  our model can be
seen as linearly interpolating an in domain model and a general domain model  but where
the interpolation parameter is input specific 

   experimental results
in this section  we describe the result of applying the mega model to several datasets with
varying degrees of divergence between the in domain and out of domain data  however 
before describing the data and results  we will discuss the systems against which we compare 
    baseline systems
though there has been little literature on this problem and thus few real systems against
which to compare  there are several obvious baselines  which we describe in this section 
onlyi  this model is obtained simply by training a standard maximum entropy model
on the in domain data  this completely ignores the out of domain data and serves as a
baseline case for when such data is unavailable 
onlyo  this model is obtained by training a standard maximum entropy model on the
out of domain data  completely ignoring the in domain data  this serves as a baseline for
expected performance without annotating any new data  it also gives a sense of how close
the out of domain distribution is to the in domain distribution 
lini  this model is obtained by linearly interpolating the onlyi and onlyo systems 
the interpolation parameter is estimated on held out  development  in domain data  this
means that  in practice  extra in domain data would need to be annotated in order to create
a development set  alternatively  cross validation could be used 
mix  this model is obtained by training a maximum entropy model on the union of the
out of domain and in domain data sets 
mixw  this model is also obtained by training a maximum entropy model on the union
of the out of domain and in domain data sets  but where the out of domain data is downweighted so that is effectively equinumerous with the in domain data 
feats  this model uses the out of domain data to build one classifier and then uses this
classifiers predictions as features for the in domain data  as described by       
   

fidaume iii   marcu

prior  this is the adaptation model described in section      where the out of domain
data is used to estimate a prior for the in domain classifier  in the case of the maximum
entropy models we consider here  the weights learned from the out of domain data are used
as the mean of the gaussian prior distribution placed over the weights in the training of
the in domain data  as is described by chelba and acero        
in all cases  we tune model hyperparameters using performance on development data 
this development data is taken to be a random     of the training data in all cases  once
appropriate hyperparameters are found  the     is folded back in to the training set 
    data sets
we evaluate our models on three different problems  the first two problems come from the
automatic content extraction  ace  data task  this data was selected because the ace
program specifically looks at data in different domains  the third problem is the same as
that tackled by chelba and acero         which required them to annotate data themselves 
      mention type classification
the first problem  mention type  is a subcomponent of the entity mention detection
task  an extension of the named entity tagging task  wherein pronouns and nominals are
marked  in addition to simple names   we assume that the extents of the mentions are
marked and we simply need to identify their type  one of  person  geo political entity 
organization  location  weapon or vehicle  as the out of domain data  we use the newswire
and broadcast news portions of the ace      training data  as the in domain data  we use
the fisher conversations data  an example out of domain sentence is 
once again  a prime battleground will be the constitutional allocation of power 
nom
nam
between the federal governmentnom
gpe and the statesgpe   and between congressorg
bar
and federal regulatory agenciesorg  
an example in domain sentence is 
nom
pro
nom
mypro
per wifeper if iper had not been transported across the continent gpe from
whq pro
whereloc iper was born and and

we use   k out of domain examples  each mention corresponds to one example    k
in domain examples and     test examples  accuracy is computed as     loss  we use
the standard feature functions employed in named entity models  include lexical items 
stems  prefixes and suffixes  capitalization patterns  part of speech tags  and membership
information on gazetteers of locations  businesses and people  the accuracies reported are
the result of running ten fold cross validation 
      mention tagging
the second problem  mention tagging is the precursor to the mention type task  in
which we attempt to tag entity mentions in raw text  we use the standard begin in out
encoding and use a maximum entropy markov model to perform the tagging  mccallum
et al          as the out of domain data  we use again the newswire and broadcast news
   

fidomain adaptation for statistical classifiers

data  as the in domain data  we use broadcast news data that has been transcribed by
automatic speech recognition  the in domain data lacks capitalization  punctuation  etc  
and also contains transcription errors  speech recognition word error rate is approximately
      for the tagging task  we have    k out of domain examples  in the context of tagging 
an example is a single word   but now  k in domain examples and   k test examples 
accuracy is f measure across the segmentation  we use the same features as in the mention
type identification task  the scores reported are after ten fold cross validation 
      recapitalization
the final problem  recap  is the task of recapitalizing text  following chelba and acero
        we again use a maximum entropy markov model  where the possible tags are 
lowercase  capitalized  all upper case  punctuation or mixed case  the out of domain
data in this task comes from the wall street journal  and two separate in domain data sets
come from broadcast news text from cnn npr and abc primetime  respectively  we use
   m out of domain examples  one example is one word   for the cnn npr data  we use
   k in domain training examples and   k test examples  for the abc primetime data  we
use   k in domain training examples and  k test examples  we use identical features to
chelba and acero         in order to maintain comparability to the results described by
chelba and acero         we do not perform cross validation for these experiments  we use
the same train test split as described in their paper 
    feature selection
while the maximum entropy models used for the classification are adept at dealing with
many irrelevant and or redundant features  the nave bayes generative model  which we use
to model the distribution of the input variables  can overfit on such features  this turned
out not to be a problem for the mention type and mention tagging problems  but
for the recap problems  it caused some errors  to alleviate this problem  for the recap
problem only  we applied a feature selection algorithm just to the features used for the nave
bayes model  the entire feature set was used for the maximum entropy model   specifically 
we took the   k top features according to the information gain criteria to predict indomain versus out of domain  as opposed to feature selection for class label   forman
       provides an overview of different selection techniques  
    results
our results are shown in table    where we can see that training only on in domain data
always outperforms training only on out of domain data  the linearly interpolated model
does not improve on the base models significantly  placing all the data in one bag helps 
and there is no clear advantage to re weighting the out domain data  the prior model
and the feats model perform roughly comparably  with the prior model edging out by a
small margin   our model outperforms both the prior model and the feats model 
   the value of   k was selected arbitrarily after an initial run of the model on development data  it was
not tuned to optimize either development or test performance 
   our numbers for the result of the prior model on the data from chelba and acero        differ slightly
from those reported in their paper  there are two potential reasons for this  first  most of their numbers

   

fidaume iii   marcu

 d o   

 d i   
accuracy
onlyo
onlyi
lini
mix
mixw
feats
prior
megam
  reduction
mix
prior

mention
type
  k
 k

mention
tagging
   k
 k

recap
abc
   m
 k

recap
cnn
   m
  k

average
 

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    

    
    

    
    

    
    

    
    

    
    

table    experimental results  the first set of rows show the sizes of the in domain and
out of domain training data sets  the second set of rows  accuracy  show the
performance of the various models on each of the four tasks  the last two rows   
reduction  show the percentage reduction in error rate by using the mega model
over the baseline model  mix  and the best alternative method  prior  

we applied mcnemars test  gibbons   chakraborti        section       to gage statistical significance of these results  comparing the results of the prior model with our own
mega model  for the mention tagging experiment  we compute mcnemars test on simple
hamming accuracy rather than f score  this is suboptimal  but we do not know how to
compute statistical significance for the f score   for the mention type task  the difference
is statistical significant at the p       level  for the mention tagging task  p         for
the recapitalization tasks  the difference on the abc data is significant only at the p      
level  while for the cnn npr data it is significant at the p        level 
in the mention type task  we have improved a baseline model trained only on in domain
data from an accuracy of       up to        a relative improvement of        for mention
tagging  we improve from       f measure up to        a relative improvement of      
in the abc recapitalization task  for which much in domain data is available   we increase
performance from       to        a relative improvement of       in the cnn npr
recapitalization task  with very little in domain data   we increase performance from      
to        a relative improvement of      

are reported based on using all   m examples  we consider only the    m example case  second  there
are likely subtle differences in the training algorithms used  nevertheless  on the whole  our relative
improvements agree with those in their paper 

   

fidomain adaptation for statistical classifiers

mention type identification task

mention tagging task

  

  

onlyout
chelba
megam

  

onlyout
chelba
megam

  
  

  

accuracy

fmeasure

  

  

  
  

  

  
  

  
 
  

 

  

 

 

  
  
amount of in domain data used  log scale 

  
 
  

 

  

 

  
amount of in domain data used  log scale 

figure    learning curves for prior and megam models 
    learning curves
of particular interest is the amount of annotated in domain data needed to see a marked
improvement from the onlyo baseline to a well adapted system  we show in figure   the
learning curves on the mention type and mention tagging problems  along the x axis 
we plot the amount of in domain data used  along the y axis  we plot the accuracy  we plot
three lines  a flat line for the onlyo model that does not use any in domain data  and
curves for the prior and megam models  as we can see  our model maintains an accuracy
above both the other models  while the prior curve actually falls below the baseline in the
type identification task  

   model introspection
we have seen in the previous sections that the mega model routinely outperforms competing models  despite this clear performance improvement  a question remains open regarding
the internal workings of the models  the   i  variable captures the degree to which the indomain data set is truly in domain  the z variables in the model aim to capture  for each
test data point  whether it is general domain or in domain  in this section  we discuss
the particular values of the parameters the model learns for these variables 
we present two analyses  in the first  section       we inspect the models inner workings
on the mention type task from section        in this analysis  we look specifically at
the expected values of the hidden variables found by the model  in the second analysis
 section       we look at the ability of the model to judge degree of relatedness  as defined
by the  variables 
   this is because the fisher data is personal conversations  it hence has a much higher degree of first
and second person pronouns than news   the baseline that always guesses person achieves a      
accuracy   by not being able to intelligently use the out of domain data only when the in domain model
is unsure  performance drops  as observed in the prior model 

   

 

  

fidaume iii   marcu

pre context
my home is in trenton
veterans administration
you know by the american
gives
is he capable of getting
the fisher thing calling
when i was a

      entity      
      new jersey      
      hospital      
      government     
   
me
   
      anything      
   
me
   
   
kid
   

post context
and thats where
because what is
chills because if
over here
ha ha they screwed up
that that was a

true
gpe
org
org
per
wea
per
per

hyp
gpe
loc
org
per
per
per
per

p  z   i 
    
    
    
    
    
    
    

table    examples from the test data for the mention type task  the true column is
the correct entity type and the hyp column is our models prediction  the final
column is the probability this example is truly in domain under our model 

    model expectations
to focus our discussion  we will consider only the mention type task  section        in
table    we have shown seven test data examples from the mention type task  the precontext is the text that appears before the entity and the post context is the text that
appears after  we report the true class and the class our model hypothesizes  finally  we
report the probability of this example being truly in domain  according to our model 
as we can see  the three examples that the model thinks are general domain are new
jersey  hospital and government  it believes that me  anything and kid are all
in domain  in general  the probabilities tend to be skewed toward   and    which is not
uncommon for nave bayes models  we have shown two errors in this data  in the first 
our model thinks that hospital is a location when truly it is an organization  this is a
difficult distinction to make  in the training data  hospitals were often used as locations 
the second example error is anything in is he capable of getting anything over
here  the long distance context of this example is a discussion about biological warfare
and saddam hussein  and anything is supposed to refer to a type of biological warhead 
our model mistakingly thinks this is a person  this error is likely due to the fact that our
model identifies that the word anything is likely to be truly in domain  the word is not
so common in newswire   it has also learned that most truly in domain entities are people 
thus  lacking evidence otherwise  the model incorrectly guesses that anything is a person 
it is interesting to observe that the model believes that the entity me in gives me
chills is closer to general domain than the me in the fisher thing calling me ha ha they
screwed up  this likely occurs because the context ha ha has not occurred anywhere in
the out of domain training data  and twice in the in domain training data  it is unlikely this
example would have been misclassified otherwise  me is fairly clearly a person   but this
example shows that our model is able to take context into account in deciding the domain 
all of the decisions made by the model  shown in table   seem qualitatively reasonable 
the numbers are perhaps excessively skewed  but the ranking is believable  the in domain
data is primarily from conversations about random  not necessarily news worthy  topics 
and is hence highly colloquial  contrastively  the out of domain data is from formal news 
the model is able to learn that entities like new jersey and government have more to
do with news that words like me and kid 
   

fidomain adaptation for statistical classifiers

  i 
  o 

mention
type
    
    

mention
tagging
    
    

recap
cnn
    
    

recap
abc
    
    

table    values for the  variables discovered by the mega model algorithm 
    degree of relatedness
in this section  we analyze the values of  found by the model  low values of   i  and
  o  mean that the in domain data was significantly different than the out of domain data 
high values mean that they were similar  this is because a high value for  means that
the general domain model will be used in most cases  for all tasks but mention type  the
values of  were middling around      for mention type    i  was      and   o  was      
indicating that there was a significant difference between the in domain and out of domain
data  the exact values for all tasks are shown in table   
these values for  make intuitive sense  the distinction between conversation data
and news data  for the mention type task  is significantly stronger than the difference
between manually and automatically transcribed newswire  for the mention tagging task  
the values for  reflect this qualitative distinction  the rather strong difference between
the  values for the recapitalization tasks was not expected a priori  however  a post hoc
analysis shows this result is reasonable  we compute the kl divergence between a unigram
language model for the out of domain data set and each of the in domain data sets  the
kl divergence for the cnn data was       while the divergence for the abc data      
this confirms that the abc data is perhaps more different from the baseline out of domain
than the cnn data  as reflected by the  values 
we are also interested in cases where there is little difference between in domain and
out of domain data  to simulate this case  we have performed the following experiment 
we consider again the mention type task  but use only the training portion of the out ofdomain data  we randomly split the data in half  assigning each half to in domain and
out of domain  in theory  the model should learn that it may rely only on the general
domain model  we performed this experiment under ten fold cross validation and found
that the average value of  selected by the model was       while this is strictly less than
one  it does show that the model is able to identify that these are very similar domains 

   conclusion and discussion
in this paper  we have presented the mega model for domain adaptation in the discriminative  conditional  learning framework  we have described efficient optimization algorithms
based on the conditional em technique  we have experimentally shown  in four data sets 
that our model outperforms a large number of baseline systems  including the current state
of the art model  and does so requiring significantly less in domain data 
although we focused specifically on discriminative modeling in a maximum entropy
framework  we believe the novel  basic idea on which this work is foundedto break the
in domain distribution p i  and out of domain distribution p o  into three distributions  q  i   
   

fidaume iii   marcu

q  o  and q  g  is general  in particular  one could perform a similar analysis in the case of
generative models and obtain similar algorithms  though in the case of a generative model 
standard em could be used   such a model could be applied to domain adaptation in
language modeling or machine translation 
with the exception of the work described in section      previous work in domain adaptation is quite rare  especially in the discriminative learning framework  there is a substantial literature in the language modeling speech community  but most of the adaptation with
which they are concerned is based on adapting to new speakers  iyer  ostendorf    gish 
      kalai  chen  blum    rosenfeld         from a learning perspective  the mega
model is most similar to a mixture of experts model  our model can be seen as a constrained experts model  with three experts  where the constraints specify that in domain
data can only come from one of two experts  and out of domain data can only come from
one of two experts  with a single expert overlapping between the two   most attempts to
build discriminative mixture of experts models make heuristic approximations in order to
perform the necessary optimization  jordan   jacobs         rather than apply conditional
em  which gives us strict guarantees that we monotonically increase the data  incomplete 
log likelihood of each iteration in training 
the domain adaptation problem is also closely related to multitask learning  also known
as learning to learn and inductive transfer   in multitask learning  one attempts to learn a
function that solves many machine learning problems simultaneously  this related problem
is discussed by thrun         caruana        and baxter         among others  the
similarity between multitask learning and domain adaptation is that they both deal with
data drawn from related  but distinct distributions  the primary difference is that domain
adaptation cares only about predicting one label type  while multitask learning cares about
predicting many 
as the various sub communities of the natural language processing family begin and continue to branch out into domains other than newswire  the importance of developing models
for new domains without annotating much new data will become more and more important 
the mega model is a first step toward being able to migrate simple classification style models  classifiers and maximum entropy markov models  across domains  continued research
in the area of adaptation is likely to benefit from other work done in active learning and in
learning with large amounts unannotated data 

acknowledgments
we thank ciprian chelba and alex acero for making their data available  we thank ryan
mcdonald for pointing out the feats baseline  which we had not previously considered 
we also thank kevin knight and dragos munteanu for discussions related to this project 
this paper was greatly improved by suggestions from reviewers  including reviewers of a
previous  shorter version  this work was partially supported by darpa ito grant n                nsf grant iis          nsf grant iis          and a usc dean fellowship to
hal daume iii 
   

fidomain adaptation for statistical classifiers

appendix a  conditional expectation maximization
in this appendix  we derive eq      from eq     by making use of jensens inequality and
the variational bound  the interested reader is referred to the work of jebara and pentland
       for further details  our discussion will consider a bound in the change of the log
likelihood between iteration t    and iteration t  l c   as given in eq      



p y   x  t
p x  y   t  p y   t
l   log
  log
p  y   x  t   
p  x  y   t     p  y   t   


t
p x  y  
p x  t
  log
 log
p  x  y  t   
p  x  t   
c

    
    

here  we have effectively rewritten the log change in the ratio of the conditionals as the
difference between the log change in the ratio of the joints and the log change in the ratio
of the marginals  we may rewrite eq      by introducing the hidden variables z as 


p
p
t
t
z p x  z  
z p x  y  z  
 log p
l   log p
t   
t   
z p  x  y  z  
z p  x  z  
c

    

we can now apply jensens inequality to the first term in eq      to obtain 

c

l 

x
z

 

  


p
t
p x  y  z   t 
p x  y  z  t
z p x  z  
p
log
 log p
t   
t   
p  x  y  z  t   
z   p  x  y  z   
z p  x  z  
 
 z
 

    

hx y z t 

in eq       the expression denoted hx y z t  is the joint expectation of z under the
previous iterations parameter settings  unfortunately  we cannot also apply jensens inequality to the remaining term in eq      because it appears negated  by applying the
variational dual  log x  x     to this term  we obtain the following  final bound 
c

t

l  q  

x
z



p
t
p x  y  z  t
z p x  z  
  
hx y z t  log
p
t   
p  x  y  z  t   
z p  x  z  

    

applying the bound from eq      to the distributions chosen in our model yields eq      

appendix b  derivation of estimation equations
given the model structure and parameterization of the mega modelgiven in section     
eq      we obtain the following expression for the joint probability of the data 
   

fidaume iii   marcu



p x  y  z              


f
n

y
y
zn
zn
ber xnf    f  gibbs yn   xn     
ber zn    
 


n  
f   



f 
n
y
 xnf
xnf 
y

 
    zfn
 zfn
 zn       zn 

n  
f   
  
h
i x
i   
h
exp zynn   xn
exp zcn   xn

c

    

the marginal distribution is obtained by removing the last two terms  the exp and the
sum of exps  from the final equation  plugging eq      into eq      and using the notation
from eq       we obtain the following expression for qt  
qt  

x


 

n
x

n  



log nor            i   
 

x
zn

 

f
x

f   



  

log bet  f   a  b 

hn zn log        zn   log        log n zn
 

f
x

xnf log zynn

f   

zn
mt 
n    

  

 zn

 log

x
c

n zn    

 

exp

h

zcn   xn

i



t
jn z
n

 
    

as well as an analogous term for the out of domain data  j and m are defined in table   
b   m step for 
for computing   we simply differentiate qt  see eq       with respect to   obtaining 
n

qt x hn    hn
 
 
  mt 
n  n    n    


 

    

n  

solving this for   leads directly to a quadratic expression of the form 

    

 

 
 

n
x

mt 
n  n  

n  

        

n
x

n  

 n    

 

 hn  mt 
n  n    n    
   



 

fidomain adaptation for statistical classifiers

  

 

 



n
x

n  

hn

 

    

solving this directly for  gives the desired update equation 
b   m step for 
for optimizing  i    we rewrite qt   eq       neglecting all irrelevant terms  as 

qt     

n
x

n  

    hn  


f
x


f   

xnf yn  f  log

x
c

h

exp c   xn


i


  log nor         i 

    

in eq       the bracketed expression is exactly the log likelihood term obtained for
standard logistic regression models  thus  the optimization of q with respect to   i  and
 o  can be performed using a weighted version of standard logistic regression optimization 
with weights defined by   hn    in the case of  g    we obtain a weighted logistic regression
model  but over all n  i    n  o  data points  and with weights defined by hn  
b   m step for 
in the case of   i  and   o    we rewrite eq      and remove all irrelevant terms  as 
qt    i     

f
x

f   

log bet f   a  b   

n
x


n  

    hn   log n    mt 
n      n  



    

due to the presence of the product term in   we cannot compute an analytical solution
to this maximization problem  however  we can take derivatives component wise  in f  
and obtain analytical solutions  when combined with the prior   this admits an iterative
solution for maximizing qt by maximizing each component separately until convergence 
computing derivatives of qt with respect to f requires differentiating n   with respect to
f   this has a convenient form  recalling the notation from table   


n      n   f  
 xnf f       xnf      f      n   f
f
f

    

using this result  we can maximize qt with respect to f by solving 
 
n
x
xnf     f        xnf  f
 h t i
    hn  
qf
 
f
f     f  
n  
 

    

 
f     f  
 
n
x
jn        n   f
 f   

jn        n   f  

 

 
n
x
 
    hn    xnf
  
f     f  
n  

   

n  

fidaume iii   marcu

equating this to zero yields a quadratic expression of the form 

     f   
   f  

 

 

n
x

jn     
n  
  n
x


 

  n   f

 

   hn   jn        n   f

n  
n
x

   f       

n  

    hn   xnf

 



 
    
 o 

this final equation can be solved analytically  a similar expression arises for f   in
 g 

the case of f   we obtain a quadratic form with sums over the entire data set and with
hn replacing the occurrences of     hn   

   

 

 






n  i 
  x
 i 
 i 
 g 

jn     i  


n   f

f

n  

n  

 g 

f

 

 




 g    

f

 

 o 
n
x

n  i 



x

n  

x

n  

jn     o  n   f 


 i 
h i 
n   jn    n   f 

n  i 

  

 i 

 i 

 o 

 o 

n  o 
 i 

h i 
n xnf  

x

n  



 o 




h o 
n xnf

 o 
n
x

n  

 o 

 o 

 o 
h o 
n   jn    n   f



 
    

again  this can be solved analytically  the values j  m    and    are defined in table   

references
averick  b  m     more  j  j          evaluation of large scale optimization problems on
vector and parallel architectures  siam journal of optimization    
baxter  j          a model of inductive bias learning  journal of artificial intelligence
research              
bacchiani  m     roark  b          unsupervised langauge model adaptation  in proceedings
of the international conference on acoustics  speech and signal processing  icassp  
caruana  r          multitask learning  a knowledge based source of inductive bias  machine learning            
chelba  c     acero  a          adaptation of maximum entropy classifier  little data
can help a lot  in proceedings of the conference on empirical methods in natural
language processing  emnlp   barcelona  spain 
chen  s     rosenfeld  r          a gaussian prior for smoothing maximum entropy
models  tech  rep  cmucs         carnegie mellon university  computer science
department 
   

fidomain adaptation for statistical classifiers

della pietra  s   della pietra  v  j     lafferty  j  d          inducing features of random
fields  ieee transactions on pattern analysis and machine intelligence             
    
dempster  a   laird  n     rubin  d          maximum likelihood from incomplete data
via the em algorithm  journal of the royal statistical society  b   
elhadad  n   kan  m  y   klavans  j     mckeown  k          customization in a unified
framework for summarizing medical literature  journal of artificial intelligence in
medicine                 
forman  g          an extensive empirical study of feature selection metrics for text classification  journal of machine learning research              
gibbons  j  d     chakraborti  s          nonparametric statistical inference  marcel
dekker  inc 
gildea  d          corpus variation and parser performance  in proceedings of the conference on empirical methods in natural language processing  emnlp  
hobbs  j  r          information extraction from biomedical text  journal of biomedical
informatics                 
huang  j   zweig  g     padmanabhan  m          information extraction from voicemail 
in proceedings of the conference of the association for computational linguistics
 acl  
hwa  r          supervised grammar induction using training data with limited constituent
information  in proceedings of the conference of the association for computational
linguistics  acl   pp       
iyer  r   ostendorf  m     gish  h          using out of domain data to improve in domain
language models  ieee signal processing        
jebara  t     pentland  a          maximum conditional likelihood via bound maximization
and the cem algorithm  in advances in neural information processing systems
 nips  
jordan  m     jacobs  r          hierarchical mixtures of experts and the em algorithm 
neural computation            
kalai  a   chen  s   blum  a     rosenfeld  r          on line algorithms for combining
language models  in icassp 
lafferty  j   mccallum  a     pereira  f          conditional random fields  probabilistic
models for segmenting and labeling sequence data  in proceedings of the international
conference on machine learning  icml  
malouf  r          a comparison of algorithms for maximum entropy parameter estimation 
in proceedings of conll 
mccallum  a   freitag  d     pereira  f          maximum entropy markov models for
information extraction and segmentation  in proceedings of the international conference on machine learning  icml  
   

fidaume iii   marcu

minka  t  p          a comparison of numerical optimizers for logistic regression  http 
  www stat cmu edu  minka papers logreg  
munteanu  d     marcu  d          improving machine translation performance by exploiting non parallel corpora  computational linguistics  to appear 
nash  s     nocedal  j          a numerical study of the limited memory bfgs method
and the truncated newton method for large scale optimization  siam journal of
optimization            
rambow  o   shrestha  l   chen  j     lauridsen  c          summarizing email threads 
in proceedings of the conference of the north american chapter of the association
for computational linguistics  naacl  short paper section 
roark  b     bacchiani  m          supervised and unsupervised pcfg adaptation to
novel domains  in proceedings of the conference of the north american chapter
of the association for computational linguistics and human language technology
 naacl hlt  
smith  d  r          variational methods in optimization  dover publications  inc   mineola  new york 
thrun  s          is learning the n th thing any easier than learning the first  in advances
in neural information processing systems  nips  

   

fi