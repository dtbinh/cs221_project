journal artificial intelligence research                  

submitted       published      

multi objective reinforcement learning
continuous pareto manifold approximation
simone parisi

parisi ias tu darmstadt de

technische universitat darmstadt
hochschulstr            darmstadt  germany

matteo pirotta
marcello restelli

matteo pirotta polimi it
marcello restelli polimi it

politecnico di milano
piazza leonardo da vinci           milano  italy

abstract
many real world control applications  economics robotics  characterized
presence multiple conflicting objectives  problems  standard concept
optimality replaced paretooptimality goal find pareto frontier 
set solutions representing different compromises among objectives  despite recent advances multiobjective optimization  achieving accurate representation
pareto frontier still important challenge  paper  propose reinforcement
learning policy gradient approach learn continuous approximation pareto frontier multiobjective markov decision problems  momdps   differently previous
policy gradient algorithms  n optimization routines executed n solutions 
approach performs single gradient ascent run  generating step improved
continuous approximation pareto frontier  idea optimize parameters
function defining manifold policy parameters space  corresponding
image objectives space gets close possible true pareto frontier  besides
deriving compute estimate gradient  discuss nontrivial
issue defining metric assess quality candidate pareto frontiers  finally 
properties proposed approach empirically evaluated two problems 
linear quadratic gaussian regulator water reservoir control task 

   introduction
multiobjective sequential decision problems characterized presence multiple
conflicting objectives found many real world scenarios  economic
systems  shelton         medical treatment  lizotte  bowling    murphy         control
water reservoirs  castelletti  pianosi    restelli         elevators  crites   barto       
robots  nojima  kojima    kubota        ahmadzadeh  kormushev    caldwell        
mention few  problems often modeled multiobjective markov decision
processes  momdps   concept optimality typical mdps replaced
one pareto optimality  defines compromise among different objectives 
last decades  reinforcement learning  rl   sutton   barto        established
effective theoretically grounded framework allows solve singleobjective
mdps whenever either  or little  prior knowledge available system dynamics
dimensionality system controlled high classical optimal control
     ai access foundation  rights reserved 

fiparisi  pirotta    restelli

methods  multiobjective reinforcement learning  morl   instead  concerns momdps
tries solve sequential decision problems two conflicting objectives 
despite successful development rl theory high demand multiobjective
control applications  morl still relatively young unexplored research topic 
morl approaches divided two categories  based number policies
learn  vamplew  dazeley  berry  issabekov    dekker         single multiple
policy  although morl approaches belong former category  present
multiplepolicy approach  able learn set policies approximating pareto frontier 
representation complete pareto frontier  fact  allows posteriori selection
solution encapsulates trade offs among objectives  giving better insights
relationships among objectives  among multiplepolicy algorithms possible
identify two classes  valuebased  lizotte et al         castelletti et al         van moffaert
  nowe         search optimal solutions value functions space  policy gradient approaches  shelton        parisi  pirotta  smacchia  bascetta    restelli        
search policy space  practice  approach different advantages  value
based methods usually stronger guarantees convergence  preferred domains lowdimensional state action spaces prone suffer curse
dimensionality  sutton   barto         hand  policy gradient methods
favorable many domains robotics allow taskappropriate
prestructured policies integrated straightforwardly  deisenroth  neumann    peters 
      experts knowledge incorporated ease  selecting suitable policy
parametrization  learning problem simplified stability well robustness
frequently ensured  bertsekas         nonetheless  approaches lack guarantees uniform covering true pareto frontier quality approximate
frontier  terms accuracy  distance true frontier  covering  its extent  
related metric used measure discrepancy true pareto frontier 
however  nowadays definition metric open problem moo literature 
paper  overcome limitations proposing novel gradientbased morl
approach alternative quality measures approximate frontiers  algorithm  namely
paretomanifold gradient algorithm  pmga   exploiting continuous approximation
locally paretooptimal manifold policy space  able generate arbitrarily
dense approximate frontier  article extension preliminary work presented
pirotta  parisi  restelli        main contributions are  derivation
gradient approach general case  i e   independent metric used measure
quality current solution  section     estimate gradient samples
 section     discussion frontier quality measures effectively integrated
proposed approach  section     thorough empirical evaluation proposed
algorithm metrics performance multiobjective discrete time linear quadratic
gaussian regulator water reservoir management domain  sections      

   preliminaries
section  first briefly summarize terminology used paper discuss
state of the art approaches morl  subsequently  focus describing policy
gradient techniques introduce notation used remainder paper 
   

fimorl continuous pareto manifold approximation

    problem formulation
discretetime continuous markov decision process  mdp  mathematical framework
modeling decision making  described tuple hs  a  p  r    di  rn
continuous state space  rm continuous action space  p markovian
transition model p s   s  a  defines transition density state s 
action a  r   r reward function         discount factor 
distribution initial state drawn  context  behavior
agent defined policy  i e   density distribution  a s  specifies probability
taking action state s  given initial state distribution d  possible define
expected return j associated policy
 t  
 
x


j  
e
r st     st     s   
st p at

t  

r st     st     immediate reward obtained state st   reached executing
action state st   finite infinite time horizon  goal agent
maximize return 
multiobjective markov decision processes  momdps  extension mdps
several pairs reward functions discount factors defined  one
objective  formally  momdp described tuple hs  a  p  r    di  r  
 r            rq  t                q  t qdimensional column vectors reward functions
ri   r discount factors         respectively 
momdps  policy




associated q expected returns j   j            jq  
 t  
 
x
ji  
e
ri  st     st     s   
st p at

t  

unlike happens mdps  momdps single policy dominating others
usually exist  conflicting objectives considered  policy simultaneously maximize them  reason  multiobjective optimization  moo 
concept pareto dominance used  policy strongly dominates policy     denoted
    superior objectives  i e  
 

              q    ji   ji  
similarly  policy weakly dominates policy     denoted     worse
objectives  i e  
 

 

              q    ji ji             q    ji   ji  
policy       policy paretooptimal  speak
locally paretooptimal policies  definition above  except
restrict dominance neighborhood   general  multiple
 locally  paretooptimal policies  solving
momdp

equivalent determine set
              maps socalled pareto
paretooptimal
policies



frontier f   j     
   done harada  sakuma  kobayashi         assume locally paretooptimal solutions
paretooptimal exist 

   

fiparisi  pirotta    restelli

    related work
multiobjective optimization  moo  field  two common solution concepts 
multiobjective singleobjective strategy pareto strategy  former approach
derives scalar objective multiple objectives and  then  uses standard single
objective optimization  soo  techniques  weighted sum  athan   papalambros        
normbased  yu   leitmann        koski   silvennoinen         sequential  romero 
       constrained  waltz         physical programming  messac   ismail yahaya       
min max methods  steuer   choo         latter strategy based concept
pareto dominance considers paretooptimal solutions non inferior solutions among
candidate solutions  main exponent class convex hull method  das  
dennis        messac  ismail yahaya    mattson        
similar moo  current morl approaches divided two categories based
number policies learn  vamplew et al          singlepolicy methods aim
finding best policy satisfies preference among objectives  majority
morl approaches belong category differ way preferences
expressed  easy implement  require priori decision type
solution suffer instability  small changes preferences may result
significant variations solution  vamplew et al          straightforward
common singlepolicy approach scalarization function applied
reward vector order produce scalar signal  usually  linear combination weighted
sum rewards performed weights used express preferences
multiple objective  castelletti  corani  rizzolli  soncinie sessa    weber        natarajan
  tadepalli        van moffaert  drugan    nowe         less common use non
linear mappings  tesauro  das  chan  kephart  levine  rawson    lefurgy        
main advantage scalarization simplicity  however  linear scalarization presents
limitations  able find solutions lie concave linear region
pareto frontier  athan   papalambros        uniform distribution weights may
produce accurate evenly distributed points pareto frontier  das   dennis 
       addition  even frontier convex  solutions cannot achieved
scalarization loss one objective may compensated increment
another one  perny   weng         different singlepolicy approaches based
thresholds lexicographic ordering  gabor  kalmar    szepesvari        different
kinds preferences objective space  mannor   shimkin              
multiplepolicy approaches  contrary  aim learning multiple policies order
approximate pareto frontier  building exact frontier generally impractical
real world problems  thus  goal build approximation frontier contains
solutions accurate  evenly distributed along frontier range similar
pareto one  zitzler  thiele  laumanns  fonseca    da fonseca         many
reasons behind superiority multiplepolicy methods  permit posteriori
selection solution encapsulate trade offs among multiple objectives 
addition  graphical representation frontier give better insights relationships among objectives useful understanding problem
choice solution  however  benefits come higher computational cost 
prevent learning online scenarios  common approach approximate
   

fimorl continuous pareto manifold approximation

pareto frontier perform multiple runs singlepolicy algorithm varying
preferences among objectives  castelletti et al         van moffaert et al         
simple approach suffers disadvantages singlepolicy method used 
besides this  examples multiplepolicy algorithms found literature 
barrett narayanan        proposed algorithm learns deterministic policies defining convex hull pareto frontier single learning process  recent
works focused extension fitted q iteration multiobjective scenario 
lizotte  bowling  murphy         lizotte et al         focused
linear approximation value function  castelletti  pianosi  restelli        able
learn control policy linear combinations preferences among objectives single learning process  finally  wang sebag        proposed montecarlo
tree search algorithm able learn solutions lying concave region frontier 
nevertheless  classic approaches exploit deterministic policies result
scattered pareto frontiers  stochastic policies give continuous range compromises
among objectives  roijers  vamplew  whiteson    dazeley        parisi et al          shelton        section        pioneer use stochastic mixture policies
gradient ascent morl  achieved two well known goals morl  simultaneous
conditional objectives maximization  former  agent must maintain goals
time  algorithm starts mixture policies obtained applying standard
rl techniques independent objective  policy subsequently improved following
convex combination gradients policy space nonnegative w r t 
objectives  objective i  gradient gi expected return w r t  policy
computed vector vi highest dot product gi simultaneously
satisfying nonnegativity condition returns used improving direction
i th reward  vectors vi combined convex form obtain direction
parameter improvement  result policy belongs pareto frontier 
approximation pareto frontier obtained performing repeated searches
different weights reward gradients vi   hand  conditional optimization
consists maximizing objective maintaining certain level performance
others  resulting algorithm gradient search reduced policy space
value constrained objectives greater desired performance 
studies followed work shelton        regard policy gradient
algorithms applied momdps  recently parisi et al         proposed two policy gradient
based morl approaches that  starting initial policies  perform gradient ascent
policy parameters space order determine set nondominated policies 
first approach  called radial    given number p pareto solutions required
approximating pareto frontier  p gradient ascent searches performed  one
following different  uniformly spaced  direction within ascent simplex defined
convex combination singleobjective gradients  second approach  called pareto
following  starts performing singleobjective optimization moves along
pareto frontier using two step iterative process  updating policy parameters following
gradient ascent direction  applying correction procedure move
new solution onto pareto frontier  although methods exploit stochastic policies
proved effective several scenarios  still return scattered solutions
guaranteed uniformly cover pareto frontier  best knowledge  nowadays
   

fiparisi  pirotta    restelli

morl algorithm returning continuous approximation pareto frontier   
following sections present first approach able that  paretomanifold
gradient algorithm  pmga  
    policy parametrization policygradient approaches
singleobjective
mdps 

policygradient approaches consider parameterized policies
    rd   compact notation  a s    policy
parameters space  given policy parametrization   assume policy performance
j   f rq least class c      f called objectives space j defined
expected reward space possible trajectories
z
p      r   d 
j     


trajectory drawn density distribution p     reward vector
r   
accumulated expected discounted reward trajectory   i e  
prepresents
 
ri  st     st      examples parametrized policies used context
ri       tt  
guassian policies gibbs policies  momdps  q gradient directions defined
policy parameter  peters   schaal      b   i e  
z


ji     
p      ri    d   e ln p      ri    

 
 

 
x
b ji    
e ri    
ln  at  st      
   
t  

direction ji associated particular discount factorreward function
b ji    sample based estimate  shown equation     
pair     ri  
differentiability expected return connected differentiability policy
ln p       


 
x

ln  at  st     

t  

remark notation  following use symbol dx f denote
derivative generic function f   rmn rpq w r t  matrix x   notice
following relationship holds scalar functions vector variable  x f    dx f  t   finally 
symbol ix used denote x x identity matrix 

   gradient ascent policy manifold continuous pareto frontier
approximation
section first provide general definition optimization problem want
solve explain solve momdp scenario using gradient
based approach  novel contributes section summarized lemma    
   notable exception moo approach calandra  peters  deisenrothy        gaussian
processes used obtain continuous approximation pareto frontier 
   function class c   continuous  twice differentiable derivatives continuous 
   derivative operator well defined matrices  vectors scalar functions  refer work
magnus neudecker        details 

   

fimorl continuous pareto manifold approximation

objective function gradient described  particular  provide solution
problem evaluating performance continuous approximation pareto
frontier w r t  indicator function  problem non trivial morl
direct access pareto frontier manipulate policy
parameters  provide step by step derivation results leveraging manifold
theory matrix calculus 
    continuous pareto frontier approximation multiobjective
optimization
shown locally paretooptimal solutions locally forms  q   dimensional
manifold  assuming   q  harada  sakuma  kobayashi    ono         follows
 objective problems  paretooptimal solutions described curves policy
parameters objective spaces  idea behind work parametrize locally
paretooptimal solution curve objectives space  order produce continuous
representation pareto frontier 
let generative space open set rb b q  analogous high
dimensional function parameterized curve smooth map   rq class
c l  l     p rk free variables parameters 
respectively  set f    t   together map constitute parametrized
manifold dimension b  denoted f  t    munkres         manifold represents
approximation pareto frontier  goal find best approximation  i e  
parameters minimize distance real frontier
  arg max  f  t     

   

p

  rq r indicator function measuring quality f  t   w r t 
true pareto frontier  notice equation     interpreted special projection
operator  refer figure  a graphical representation   however  since requires
knowledge true pareto frontier  different indicator function needed 
definition metric open problem literature  recently  several metrics
defined  candidate presents intrinsic flaws prevent definition
unique superior metric  vamplew et al          furthermore  see
remainder section  proposed approach needs metric differentiable w r t 
policy parameters  investigate topic section   
general  moo algorithms compute value frontier sum value
points composing discrete approximation  scenario  continuous
approximate frontier available  maps integration pareto manifold
z
idv 
   
l     
f  t  

l    manifold value  dv denotes integral w r t  volume manifold
  f  t   r indicator function measuring pareto optimality point
f  t    assuming continuous  integral given  munkres       
z
z
l     
idv
 i   v ol  dt  t   dt 
f  t  



   

fiparisi  pirotta    restelli

 a 

 b 

figure    transformation maps generic moo setting  figure  a   morl  figure  b    moo possible consider parametrized solutions figure  b   morl necessary  mapping known
closed form determined  discounted  sum rewards 
 
provided integral exists v ol  x    det x x     standard way maximize
previous equation performing gradient ascent  updating parameters according
gradient manifold value w r t  parameters   i e     l     
    continuous pareto frontier approximation multiobjective
reinforcement learning
standard multiobjective optimization function free designed 
morl must satisfy conditions  first thing notice direct map
parameters space objective space unknown  easily
defined reparameterization involving policy space   shown figure  b 
previous section mentioned tight relationship  local 
manifold objective space  local  manifold policy parameters space 
mapping well known defined performance function j   defining
utility policy   means that  given set policy parameterizations  define
associated points objective space  consequence  optimization problem
reformulated search best approximation pareto manifold
policy parameter space  i e   search manifold policy parameter space
best describes optimal pareto frontier 
formally  let   smooth map class c l  l    defined
domain   think map parameterization subset  t    
choice point gives rise point  t   t     means
subset  t   space spanned map   i e    t   bdimensional
parametrized manifold policy parameters space  i e  
 t            t      
and  consequence  associated parameterized pareto frontier bdimensional
open set defined
f  t      j       t     
   

fimorl continuous pareto manifold approximation

    gradient ascent manifold space
point introduced notation needed derive gradient l    
lemma       pirotta et al         let open set rb   let f  t   manifold
parametrized smooth map expressed composition maps j    i e    
j   rq    given continuous function defined point f  t   
integral w r t  volume given
z
z
l     
idv  
 i  j    v ol  d j  dt  t   dt 
f  t  



provided integral exists  associated gradient w r t  parameters given
z

l   
 
 i  j    v ol  t  dt





z





 i  j    v ol  t  vec
 
nb ib di tdt     


  j  dt  t   kronecker product  nb       ib    kbb   symmetric
 b  b    idempotent matrix rank    b b      kbb permutation matrix  magnus
  neudecker         finally 


di   dt  t t iq  d j    di  t     ib j    di  dt  t    
proof  equation manifold value l    follows directly definition
volume integral manifold  munkres        definition function composition 
following  provide detailed derivation i th component gradient 
let   j   dt  t  
z
l   

 
 i  j    v ol  t  dt



z
det tt
 
 
 i  j   
dt 
 v ol  t 


indicator derivative determinant derivative respectively expanded

 i  j      dj i jt   j    di  t  



det tt
det tt vec tt
 
 



 vec
t 
 vec
t 

 
 z
   
 z
     z     z 
  

 b 

b  qb

qb 



det tt
 vec t t
tt
 vec t t





 
  det
vec






   nb ib tt  

   

fiparisi  pirotta    restelli

kronecker product  nb       ib    kbb   symmetric  b  b    idempotent
matrix rank    b b      kbb permutation matrix  magnus   neudecker        
 t 
last term expanded di vec
  start basic property
differential  i e  
 d j  dt  t     d d j   dt  t    j   d dt  t  
then  applying vector operator 
dvec  d j  dt  t     vec  d d j   dt  t     vec  d j   d dt  t   


  dt  t t iq dvec  d j       ib j    dvec  dt  t    
 z
   
 z
  
 z
 
 z
  
 
dq 

bqdq

bqbd

bd 

finally  derivative given

vec j    t 
vec dt  t 


di   dt  t t iq
   ib j   




 
 z
     zi  
 
 z
 
dqd



d 

bd 





  dt  t  iq  d j    di  t     ib j    di  dt  t    

interesting notice gradient manifold value l    requires
compute second derivatives policy performance j    however   d j     
vec j  
denote hessian matrix transformation

 m n 
h
ji

 

 
dn m
ji   


 
n



ji




  dp n  d j     

p     q m    q  number objectives  number rows jacobian
matrix  recall hessian
matrixis defined derivative transpose
jacobian  i e   h j     j  t  
now  little research done second order methods  particular
hessian formulations  first analysis performed kakade         provided
formulation based policy gradient theorem  sutton  mcallester  singh    mansour 
       recently  extended comparison newton method  em algorithm
natural gradient presented furmston barber         sake clarity 
report hessian formulation provided furmston barber        using notation
introduce optimal baseline  in terms variance reduction  formulation 
lemma      momdp  hessian h j   expected discounted reward j
w r t  policy parameters qd matrix obtained stacking hessian
   notable exceptions natural gradient approaches that  although explicitly require
compute second order derivatives  usually considered second order methods 

   

fimorl continuous pareto manifold approximation

component
h j    


vec




ji   






h j    


  
 
 
 
h jq   


z
h ji     

 
p       ri     bi   ln p      ln p     t   h ln p      d 

   




ln p       


 
x

ln  at  st     

h ln p       

t  


 
x

h ln  at  st     

t  
 m n 

optimal baseline hessian estimate h
ji provided equation    
computed done greensmith  bartlett  baxter        order reduce
variance gradient estimate  given component wise


 
 m n 
e p    ri     g
   
 m n 

bi
 
   
 m n 
e p    g
   
 m n 

 m n 

n
g
     
ln p      ln p      h
appendix a 

ln p       derivation  refer

   manifold gradient estimation sample trajectories
morl  prior knowledge reward function state transition
model  need estimate gradient l    trajectory samples  section
aims provide guide estimation manifold gradient  particular  review
results related estimation standard rl components  expected discounted return
gradient  provide finite sample analysis hessian estimate 
formulation gradient l    provided lemma     composed terms
related parameterization manifold policy space terms related
mdp  since map free designed  associated terms  e g   dt  t  
computed exactly  hand  terms related mdp  j     j  
h j    need estimated  estimate expected discounted reward
associated gradient old topic rl literature several results
proposed  kakade        pirotta  restelli    bascetta         literature lacks explicit
analysis hessian estimate  recently  simultaneous perturbation stochastic approximation technique exploited estimate hessian  fonteneau   prashanth        
however  rely formulation provided furmston barber       
hessian estimated trajectory samples obtained current policy  removing
necessity generating policy perturbations 
   

fiparisi  pirotta    restelli

algorithm   paretomanifold gradient algorithm
define policy   parametric function   indicator learning rate
initialize parameters
repeat terminal condition reached
collect n           n trajectories
sample free variable t n  generative space

sample policy parameters  n    t n 
n

 n   n   n 
execute trajectory collect data st     rt 
t  

b ji    according equation    
compute gradients
b ji    according equation    
compute hessians h
compute manifold value derivative l    according equation    
update parameters   l   
since p      unknown  expectation approximated empirical average 
assuming access n trajectories  hessian estimate
 
n

 
x
x
 
b ji     
h
rnt i b
n
n  
t  
 t  
 

 

 
x
x
x

ln ant  snt
ln ant  snt
 
h ln ant  snt  
   
t  




n
 n   n   n 
st     rt 

t  

t  

t  

denotes n th trajectory  formulation resembles def 

inition reinforce estimate given williams        gradient j   
estimates  known likelihood ratio methods  overcome problem determining perturbation parameters occurring finite difference methods  algorithm   describes
complete pmga procedure 
order simplify theoretical analysis hessian estimate  make following assumptions 
assumption      uniform boundedness   reward function  log jacobian
log hessian policy uniformly bounded               q               d  n  
           d   s  a  s     







 m 

 m n 

 
ln  a s   fi g 
firi  s  a   fi ri  
fid ln  a s   fi d 
fih
lemma      given parametrized policy  a s     assumption      i th component log hessian expected return bounded
kh ji   kmax


ri
 
td   g  
 

max norm matrix defined kakmax   maxi j  aij   
   

fimorl continuous pareto manifold approximation

proof  consider definition hessian equation      assumption     
hessian components bounded  m  n 

 

 

 

fiz
x
x


 m n 

ln  at  st    
ln  aj  sj    
ji   fi   p      ri    
fih


n
t  
j  
 fi

 

 
ln  at  st    

n



 

 

 

x
x
x
ri
 

ri
l 
  g  
td   g  
 
l  

t  

j  

previous result used derive bound sample complexity
hessian estimate 
theorem      given parametrized policy  a s     assumption      using
following number  step trajectories

   
 
ri
 
n  
td   g
ln

 i     
b ji    generated equation     probability  
gradient estimate h


b

 
h ji    h ji   
max

proof  hoeffdings inequality implies m  n
n    



pn
b  m n 

 m n 
 bi ai   
i  
  
p fih
ji    h
ji     e

solving equation n noticing lemma     provides bound sample 
obtain

   
 
ri
 
n   
td   g
ln  

 i     

integral estimate computed using standard montecarlo techniques  several
statistical bounds proposed literature  refer robert casella       
survey montecarlo methods 
point paper  reader may expect analysis convergence  or
convergence rate  optimal parametrization  although consider analysis theoretically challenging interesting  provide result related topic 
analysis hard  or even impossible  provide general settings since objective
function nonlinear nonconcave  moreover  analysis simplified scenario  if
possible  almost useless real applications 
   

fiparisi  pirotta    restelli

   metrics multiobjective optimization
section  review indicator functions proposed literature  underlining advantages drawbacks  propose alternatives  recently  moo focused
use indicators turn multiobjective optimization problem singleobjective
one optimizing indicator itself  indicator function used assign every
point given frontier scalar measure gives rough idea discrepancy candidate frontier pareto one  since instead optimizing objective
functions directly indicatorbased algorithms aim finding solution set maximizes
indicator metric  natural question arises correctness change
optimization procedure properties indicator functions enjoy  instance 
hypervolume indicator weighted version among widespread metrics
literature  metrics gained popularity refinements
pareto dominance relation  zitzler  thiele    bader         recently  several works
proposed order theoretically investigate properties hypervolume indicator  e g   friedrich  horoba    neumann         nevertheless  argued
hypervolume indicator may introduce bias search  furthermore another important
issue dealing hypervolume indicator choice reference point 
perspective  main issues metric high computational complexity  the
computation hypervolume indicator  phard problem  see friedrich et al        
and  all  non differentiability  several metrics defined field
moo  refer work okabe  jin  sendhoff        survey  however 
moo literature able provide superior metric among candidates
one suited scenario  again  main issues non differentiability 
capability evaluating discrete representations pareto frontier intrinsic
nature metrics  example  generational distance  another widespread measure
based minimum distance reference frontier  available settings 
overcome issues  mixed different indicator concepts novel differentiable
metrics  insights guided metrics definition related moo
desiderata  recall goal moo compute approximation frontier
including solutions accurate  evenly distributed covering range similar
actual one  zitzler et al          note uniformity frontier intrinsically guaranteed continuity approximation introduced  concepts
mind  need induce accuracy extension indicator function 
stressed clear definition want indicator
maximized real pareto frontier  must ensure indicator function
induces partial ordering frontiers  manifold f  solutions  weakly  dominated
manifold f  ones  f  manifold value must better f  one 
definition      consistent indicator function   let f set  q   dimensional
manifolds associated momdp q objectives  let k manifold
policy parameters
space mapping fk f f true pareto frontier  let
r
li  f    f idv manifold value  indicator function consistent
fk    fh   li  fh     li  fk   fh f  



h   k   k   j h   j   li  fh     li  fk   
   

fimorl continuous pareto manifold approximation

    accuracy metrics
given reference point p  simple indicator obtained computing distance
every point frontier f reference point  i e  
  kj pk    
mentioned hypervolume indicator  choice reference point may
critical  however  natural choice utopia  ideal  point  pu    i e   point
optimizes objectives  case goal minimization indicator
function  denoted iu  utopia indicator    since dominated policy farther
utopia least one paretooptimal solution  accuracy easily guaranteed 
hand  since minimized  measure forces solution collapse
single point  thus consistent  note problem mitigated
 but solved  forcing transformation pass singleobjective
optima  although trick helpful  discuss section    requires
find singleobjective optimal policies order constrain parameters  however 
information required properly set utopia 
concerning accuracy frontier  theoretical perspective  possible
define another metric using definition pareto optimality  point paretooptimal
 brown   smith       
l      

q
x

ji        

q
x

i  

    

  

i  

is  possible identify ascent direction simultaneously improves
objectives  consequence  paretoascent direction l point pareto
frontier null  formally  metric respects paretooptimality defined
follows 
q
x
  minq kl    k    
       
r

i  

denote indicator ipn  pareto norm indicator    utopiabased metric 
extent frontier taken account without constraint optimal
solution collapses single point frontier 
    covering metrics
extension frontier primary concern  maximizing distance
antiutopia  pau   results metric grows frontier dimension  however 
contrary utopia point  antiutopia located half space
reached solutions moo problems  means considering
antiutopiabased metric maximization problem could become unbounded moving
solutions arbitrary far pareto frontier antiutopia point  therefore
measure  denoted iau  antiutopia indicator    provide guarantee
accuracy 
   

fiparisi  pirotta    restelli

    mixed metrics
mentioned indicators provide one desiderata  consequence 
resulting approximate frontier might arbitrary far actual one  order
consider desiderata mix previous concepts following indicator 
  iau w
w penalization function  i e   monotonic function decreases
accuracy input increases  e g   w     ipn w     iu   metrics  denoted
respectively i pn i u   take advantage expansive behavior antiutopia
based indicator accuracy optimalitybased indicator  way
desiderata met single scalar measure  c l  l    differentiable 
another solution mix utopia antiutopiabased indicators different way 
want solutions simultaneously far antiutopia close utopia 
consider following metric  to maximized  
   

iau
   
iu

    free parameters 
next section  show proposed mixed metrics effective driving
pmga close pareto frontier exact approximate scenarios  however 
want make clear consistency guaranteed strongly depends
free parameters         insights discussed section   

   experiments
section  evaluate algorithm two problems  linear quadratic gaussian
regulator water reservoir control task  pmga compared state of the art methods
 peters  mulling    altun        castelletti et al         parisi et al         beume  naujoks 
  emmerich        using hypervolume  vamplew et al         extension
previously defined performance index  pianosi  castelletti    restelli         named loss 
measuring distance approximate pareto front reference one   objective
problems  hypervolume exactly computed   objective problems  given high
computational complexity  hypervolume approximated montecarlo estimate
percentage points dominated frontier cube defined utopia
antiutopia points  estimate one million points used 
 
idea loss index compare true pareto frontier fw    jw
ww

space weights w frontier jw    jbw  ww returned algorithm
weights  jw denotes discounted return new singleobjective mdp defined
linear combination objectives w   formally loss function l defined
l j



jw maxm jbw

z

j

  f  w  p   
ww

jw

p dw  

   

p   probability density simplex w jw   w j normalization factor  i th component j difference best
   

fimorl continuous pareto manifold approximation

worst value i th objective pareto frontier  i e   ji   max ji   min ji   
m 
means that  weight  policy minimizes loss function chosen jw
true pareto frontier f known  reference one used 
since pmga returns continuous frontiers two scores designed discrete
ones  evaluation frontiers discretized  also  figures presented
section show discretized frontiers order allow better representation  besides
hypervolume loss function  report number solutions returned
algorithm number rollouts  i e   total number episodes simulated
learning process   data collected simulation results averaged
ten trials    experiments  pmga learning rate


 
   
 

l      l   
positive definite  symmetric matrix userdefined parameter 
stepsize rule comes formulation gradient ascent constrained problem
predefined distance metric  peters        underlies derivation natural
gradient approaches  however  since algorithm exploits vanilla gradient  i e  
consider euclidean space  metric identity matrix i 
remainder section organized follows  start studying behavior
metrics proposed section   effects parametrization  t  lqg 
subsequently  focus attention sample complexity  meant number rollouts
needed approximate pareto front  finally  analyze quality algorithm
water reservoir control task  complex real world scenario  compare
state of the art multiobjective techniques  case study  domains first
presented results reported discussed 
    linear quadratic gaussian regulator  lqg 
first case study discrete time linear quadratic gaussian regulator  lqg 
multi dimensional continuous state action spaces  peters   schaal      b  
lqg problem defined following dynamics
st     ast   bat  

n  k st    

r st       st qst rat
st n dimensional column vectors  a  b  q  r rnn   q symmetric
semidefinite matrix  r symmetric positive definite matrix  dynamics
coupled  i e   b identity matrices  policy gaussian parameters
  vec k   k rnn   finally  constant covariance matrix   used 
lqg easily extended account multiple conflicting objectives 
particular  problem minimizing distance origin w r t  i th axis
taken account  considering cost action axes
x
ri  st       s t i
a t j  
i  j

   source code available https   github com sparisi mips 

   

fiparisi  pirotta    restelli

since maximization i th objective requires null action axes 
objectives conflicting  reward formulation violates positiveness matrix
ri   change adding sufficiently small  perturbation




x
x
ri  st            s t i  
a t j
s t j   a t i  
i  j

j  i

parameters used experiments following               initial
state s            t s                t    objective case  respectively 
following sections compare performance proposed metrics several settings 
made use tables summarize results end set experiments 
       objective case results
lqg scenario particular instructive since terms involved definition returns  gradients hessians computed exactly  therefore focus studying
different policy manifold parametrizations  t  metrics i 
unconstrained parametrization  domain problematic since defined
control actions range        controls outside range lead divergence
system  primary concern therefore related boundedness control
actions  leading following parametrization manifold policy space 


     exp       t   
 
       
   t   
     exp       t   
utopia antiutopia points                        respectively  metrics iau
iu normalized order   reference point   learning step parameter
equation         
case  exploiting nonmixed metrics  pmga able learn good approximation pareto frontier terms accuracy covering  using utopiabased
indicator  learned frontier collapses one point knee front 
behavior occurs using ipn   using antiutopia point reference point solutions
dominated approximate frontier gets wider  diverging true frontier
expanding opposite half space  behaviors surprising  considering
definition indicator functions  explained section   
contrary  shown figure    mixed metrics able achieve
accuracy covering  starting   set             t   algorithm
able learn even starting different random parameters  free metric parameters
set       i pn       i u                 although
shown figure  i u behaved similarly i pn   notice cases
first accuracy obtained pushing parametrization onto pareto frontier 
frontier expanded toward extrema order attain covering 
   recall initially defined   kj pk     slightly modify normalizing policy
performance w r t  reference point    kj p  k       component wise operator 
   section   study sensitivity proposed metrics parameters  

   

fimorl continuous pareto manifold approximation

table    summary  dimensional lqg  unconstrained 
metrics
nonmixed
issues 

accuracy
covering
 
 
iu   ipn   frontier collapses one point
iau   diverging behavior dominated solutions found
 
 

mixed

partial solution
final approximation
true pareto frontier

   

   

  
l   

  

j 

   

  

   

  

 

  

end

 

   
   

   

   

   

 

  

j 

   

iterations

 a  learning process mixed metric i pn  

   

  

   
  

l   

j 

     

 

   

   

 

 
end
   
   

   

   

   

   

 

j 

  

   

iterations

 b  learning process mixed metric  

figure    learning processes  objective lqg without constraint
parametrization  numbers denote iteration  end denotes frontier obtained
terminal condition reached  left  approximated pareto frontiers 
right corresponding l     using i pn  figure  a    figure  b   approximated frontier overlaps true one  however  using   pmga converges faster 

   

fiparisi  pirotta    restelli

constrained parametrization  alternative approach consists forcing policy
manifold pass extreme points true front knowing parameterizations singleobjective optimal policies  general  requires additional
optimizations collection additional trajectories must accounted
results  however  extreme points required set utopia antiutopia  moreover  case optimal singleobjective policies available literature 
reasons  count additional samples report total number rollouts 
using constrained parameterization  two improvements easily obtained  first 
number free parameters decreases and  consequence  learning process
simplified  second  approximate frontier forced sufficiently large area
cover extrema  thus  problem covering shown nonmixed indicators
alleviated or  cases  completely eliminated   dimensional lqg 
parametrization forced pass extrema frontier following 


     exp           t                  t   
   t   
 
       
     exp           t                  t   
initial parameter vector           t   constraint able correct diverging
behavior iu ipn   returned accurate wide approximation pareto
frontier  shown figure  a  notice much faster convergence  since algorithm required learn fewer parameters  two instead four   however  iau still shows
diverging behavior initial parameters    in figure  b            t   
contrary  solutions obtained metrics independent initial    
algorithm converges close true frontier even starting parametrization
generating initial frontier far away true one 
       objective case results
unconstrained parametrization 


     exp       t      t     
   t         exp       t      t       
     exp       t      t     

simplex           

utopia antiutopia points                                  respectively  metrics
iau   iu normalized  initial parameters drawn uniform distribution  
u nif                     causes numerical issues  learning rate parameter     
 objective scenario  frontiers learned iu ipn collapse single
point  iau divergent trend  figure  a   however  unlike  objective lqr 
i pn failed correctly approximate pareto frontier  reason tuning
difficult  given difference magnitude ipn iau contrary 
i u                    returned high quality approximate frontier 
latter shown figure  b  although small areas true pareto frontier
covered approximate one  stress fact policies found
paretooptimal  strength metrics found normalization
utopia antiutopiabased indicators  expedient  indeed  allows easier tuning
free metric parameters  magnitude single components similar 
insights tuning mixed metrics parameters discussed section   
   

fimorl continuous pareto manifold approximation

table    summary  dimensional lqg  constrained 
metrics
nonmixed  iu   ipn
nonmixed  iau
issues 
mixed

accuracy
covering
 
 
 
 
iau   diverging behavior dominated solutions found
 
 

partial solution
final approximation
true pareto frontier

   

 

   
l   

j 

   

 

   

 

   

end
   
   

   

   

   

   

 

 

j 

  

  

  

  

   

   

iterations

 a  learning process utopiabased metric iu  

   

  

   

   

j 

 

   

l   

 
 

   
   

   

 

   

   

   

   

 

j 

  

  

  

  

iterations

 b  learning process antiutopiabased metric iau  

figure    learning process  objective lqg parametrization forced pass
extreme points frontier  constraints able correct behavior
iu  figure  a   convergence faster previous parametrization  however 
iau still diverges  figure  b   returned frontier includes dominated solutions  since
metric considers covering frontier accuracy 

   

fiparisi  pirotta    restelli

table    summary  dimensional lqg  unconstrained 
metrics
nonmixed
issues 

accuracy
covering
 
 
iu   ipn   frontier collapses one point
iau   diverging behavior dominated solutions found
 
 
i pn   difficult tuning
 
 

mixed  i pn
issues 
mixed  i u  

true pareto frontier
approximate frontier

j 

     

   

   

   

     

j 

     

   

     

   

j 

j 

     

j 

 a  frontier approximated antiutopiabased metric iau  

j 

true pareto frontier
approximate frontier

   
   

   
   

   
   

   
j 

   

   

j 

   
   

   
   

j 

j 

   

 b  frontier approximated mixed metric  

figure    resulting frontiers  objective lqg using unconstrained parametrization  frontiers discretized better representation  iau learning
diverges  figure  a   correctly approximates pareto frontier  figure  b   

   

fimorl continuous pareto manifold approximation

constrained parametrization 


     exp a     t   b    t    t     t     t  t     
 
     exp a  b    t      t    t     t     t  t     
   t   
 
 
 
     exp c        b t         b t    t    t    t  t    
              

b               

simplex           

c               

initial parameters        numerical results reported table   
hypervolume computed normalizing objective w r t  antiutopia  figure  
shows frontiers obtained using utopia antiutopiabased indicators  clearly
see that  unlike  objective case  even constrained parametrization metrics
lead poor solutions  failing providing mo desiderata  figure  a  using iu
frontier still tends collapse towards center true one  order minimize
distance utopia point  only constraint prevents that   although shown
figures  similar slightly broader frontier returned using ipn   however 
stress solutions belong pareto frontier  i e   nondominated solutions
found  figure  b shows frontier obtained iau   expected  algorithm tries
produce frontier wide possible  order increase distance antiutopia
point  behavior leads dominated solutions learning process diverges 
contrary  using mixed metrics i pn          i u              
            pmga able completely accurately cover pareto frontier  shown
figures  a  b  worth notice different magnitude free parameter
i pn compared  objective case       already discussed  due
substantial difference magnitude iau ipn   contrary  tuning
mixed metrics easier  similar parameters used unconstrained
parametrization proved effective  come back topic section   
finally  shown table    i u achieve best numerical results  first
attains highest hypervolume lowest loss  latter attains fastest
convergence  superiority resides easy differentiability tuning  especially compared i pn   reasons  chosen empirical analysis
sample complexity comparison state of the art algorithms
real world mo problem  discussed next sections 
table    performance comparison different metrics  objective lqg
constrained parametrization  reference frontier hypervolume        
metric

hypervolume

loss

 iterations

iu

      

      e   

  

iau

 





ipn

      

      e   

   

i pn

      

      e   

  

i u

      

      e   

  



      

      e   

  

   

fiparisi  pirotta    restelli

table    summary  dimensional lqg  constrained 
metrics
nonmixed
issues 
mixed

accuracy
covering
 
 
iu   ipn   frontier collapses one point
iau   diverging behavior dominated solutions found
 
 

j 

true pareto frontier
approximate frontier

   
   

   
   

   
   

   
j 

   

   

j 

   
   

   
   

j 

j 

   

 a  frontier approximated utopiabased metric iu  

j 

true pareto frontier
approximate frontier

   
   

   
   

   
   

   
   
j 

j 

   
   

   

j 

   
j 

   

 b  frontier approximated antiutopiabased metric iau  

figure    results parametrization forced pass extreme points
frontier  using iu  figure  a   frontier shrinks much allowed parametrization  constraint therefore able solve issues metric  
objective scenario  contrary  using iau frontier gets wider diverges
true one  in figure  b  intermediate frontier shown  

   

fimorl continuous pareto manifold approximation

j 

true pareto frontier
approximate frontier

   

 

   
   

   
j 

   

   

   
   
   

j 

   

   

   
 

 a  frontier objectives space 

   

   
 
   

 b  frontier policy parameters space 

figure    results using constrained parametrization  shown figure  a  
approximate frontier perfectly overlaps true one  despite small discrepancies
policy parameters space learned parameters optimal ones  figure  b   
similar frontiers obtainable i pn i u  

      empirical sample complexity analysis
section  provide empirical analysis sample complexity pmga  meant
number rollouts needed approximate pareto frontier  goal identify
relevant parameter estimate mdp terms j    j   hj   
analysis performed  dimensional lqg domain varying number
policies used estimate integral per iteration pmga number episodes
policy evaluation  steps episode fixed     first used
parametrization forced pass extreme points frontier           t  
produces initial approximate frontier far true one  parameter
learning rate equation     set       parameter i u set
     performance criterion  choose total number rollouts required reach
loss smaller       hypervolume larger       reference one 
criteria used conditions convergence  both satisfied  
evaluation  mdp terms computed closed form  terminal condition must
reached          episodes otherwise algorithm forced end  symbol used
represent latter case 
table  a results relevant parameter number episodes
used estimate mdp terms  parameter controls variance estimate 
i e   accuracy estimate l     increasing number episodes 
estimation process less prone generate misleading directions  happens  instance 
oneepisode case parameters move towards wrong direction  contrary 
number points used estimate integral  denoted table  t  seems
significant impact final performance algorithm  influences
number model evaluations needed reach prescribed accuracy  best behavior 
   

fiparisi  pirotta    restelli

table    total number episodes needed converge varying number points  t
approximate integral number episodes  ep per point  symbol
used terminal condition reached 
 a  parametrization constrained pass extreme points frontier  one
point sufficient move whole frontier towards right direction 

 ep

 

 

  

  

  

 



       

       

          

          

 



             

             

             

             

  



             

             

              

              

  



             

              

              

              

  



             

              

              

              

 t

 b  contrary  using unconstrained parametrization  pmga needs sufficient number
episodes enough points correct update step 

 ep

 

 

  

  

  

 











 









              

  







              

              

  







              

              

  





               

              



 t

samplebased perspective  obtained exploiting one point
integral estimate  although surprising  simple explanation exists  forcing
parameterization pass singleobjective optima  correct estimation
gradient direction single point enough move entire frontier toward
true one  i e   move parameters towards optimal ones 
contrary  unconstrained parametrization used  one point sufficient
anymore  shown table  b  case  initial parameter vector set    
            t   learning rate parameter       terminal condition requires
frontier loss smaller     hypervolume larger     reference
frontier  without constraint  algorithm needs accuracy evaluation
single points i e   sufficient number episodes enough points move whole
frontier towards right direction  accuracy gradient estimate l    therefore
depends number points number episodes  pmga requires
much rollouts converge  best behavior  samplebased perspective 
obtained exploiting five points integral estimate    episodes
policy evaluation 
   

fimorl continuous pareto manifold approximation

    water reservoir
water reservoir modeled momdp continuous state variable representing water volume stored reservoir  continuous action controlling
water release  state transition model depending stochastic reservoir inflow  
set conflicting objectives  domain proposed pianosi et al         
formally  state transition function described mass balance equation
st     st   t   max at   min at      st reservoir storage time t  t  
reservoir inflow time      generated white noise normal distribution
t   n            release decision  minimum maximum
releases associated storage st according relations   st   max st         
work consider three objectives  flooding along lake shores  irrigation
supply hydro power supply  immediate rewards defined
r   st     st       max ht   h     
r   st     st       max       
r   st     st       max e et        
ht     st    s reservoir level  in following experiments       h
flooding threshold  h          max at   min at      release reservoir 
water demand          e electricity demand  e         et   electricity
production
et     g h    ht    
           dimensional conversion coefficient  g        gravitational
acceleration      turbine efficiency h             water density  r  denotes
negative cost due flooding excess level  r  negative deficit
water supply r  negative deficit hydro power production 
original work  discount factor set   objectives
initial state drawn finite set  however  different settings used learning
evaluation phases  given intrinsic stochasticity problem  policies
evaluated       episodes     steps  learning phase requires different
number episodes    steps  depending algorithm  discuss details
results section 
since problem continuous exploit gaussian policy model


 a s      n    s t      
  rd basis functions                 optimal policies
objectives linear state variable  use radial basis approximation


 s    e

ksci k 
wi

 

used four centers ci uniformly placed interval           widths wi    
total six policy parameters 
   

fiparisi  pirotta    restelli

      results
evaluate effectiveness algorithm analyzed performance
frontiers found weighted sum stochastic dynamic programming  pianosi et al         
multi objective fqi  pianosi et al          episodic version relative entropy policy
search  peters et al         deisenroth et al          sms emoa  beume et al         
two recent policy gradient approaches  i e   radial algorithm paretofollowing
algorithm  parisi et al          since optimal pareto front available  one
found sdp chosen reference one loss computation  mofqi learns
deterministic policies  i e   standard deviation gaussian set zero 
trained using         samples dataset         tuples  objective
problem         samples dataset          tuples  objective problem 
remaining competing algorithms learn stochastic policies  number episodes
required policy update step    reps      pfa ra     sms emoa 
given episodic formulation  reps draws parameters upper distribution
      n       
diagonal covariance matrix  set zero  however  since algorithm
learns parameters         overall learned policy still stochastic  sms emoa
maximum population size            objective case  respectively 
crossover uniform mutation  chance     occur  adds white
noise random chromosomes  iteration  top     individuals kept
next generation guarantee solution quality decrease  finally  mofqi
scalarizes objectives using weights sdp  i e         weights  
 objective case  respectively  reps uses instead        linearly spaced weights  ra
follows        linearly spaced directions and  along pfa  exploits natural
gradient  peters   schaal      a  adaptive learning step equation         
  f   f fisher information matrix  concerning parametrization
pmga  used complete first degree polynomial  objective case


     t          t
      t            t


     t          t

 
       
   t   
 

               t
     t             t
       t             t
similarly   objective case complete second degree polynomial used


            t           t  t      t          t  
            t           t  t    t          t  


            t           t  t          t     t  
 

   t   
            t           t  t          t      t    simplex          
 
 

             t           t  t    t         t 
 

             t           t  t         t  

 

parameterizations forced pass near extreme points pareto frontier 
computed singleobjective policy search  cases starting parameter
   

fimorl continuous pareto manifold approximation

   
   

l   

j   water demand 

 

 

 

  

sdp
pmga    
pmga end  

    
  
    

  

           
iterations

   

 

 a 

   

 

         
j   flooding 

 

 b 

figure    results  objective water reservoir  even starting arbitrary poor
initial parametrization  pmga able approach true pareto frontier  figure  b   
figure  a   trend manifold metric l    averaged ten trials 

vector                        t   last parameter set    order guarantee
generation sufficiently explorative policies    responsible variance
gaussian distribution  however  fair comparison  competing algorithms
take advantage information  mean initial policies calculated accordingly behavior optimal ones described castelletti et al          i e  
                    t   initial standard deviation set      guarantee sufficient exploration  parametrization avoids completely random poor quality initial
policies  utopia antiutopia points set                     
objective case                                 objective one 
according results presented section        integral estimate pmga
performed using montecarlo algorithm fed one random point  instance variable t     trajectories    steps used estimate gradient
hessian policy  regarding learning rate  adaptive one described equation     used      evaluation              points used
integral estimate    objective case  respectively  already discussed  given
results obtained lqg problem order show capability approximate algorithm  decided consider indicator               
main reasons efficiency  in table   attained fastest convergence 
easy differentiability  finally  recall results averaged ten trials 
figure  b reports initial final frontiers first two objectives
considered  even starting far true pareto frontier  pmga able approach
it  increasing covering accuracy approximate frontier  also  shown figure  a  despite low number exploited samples  algorithm presents almost
monotonic trend learning process  converges iterations 
   

fiparisi  pirotta    restelli

j   water demand 

   

  

    

sdp
pfa
ra
mofqi
reps
sms emoa
pmga
   

   

   

 

   

   

j   flooding 

   

   

 

   

figure    visual comparison  objective water reservoir  pmga frontier comparable ones obtained state of the art algorithms terms accuracy covering 
however  continuous one  others scattered 
table    numerical algorithm comparison  objective water reservoir  sdp
reference frontier hypervolume        nine solutions 
algorithm

hypervolume

loss

 rollouts

 solutions

             

             

              



pfa

             

             

              

         

ra

             

             

              

        

 

             

       

 

reps

             

             

              

        

sms emoa

             

             

                

        

pmga

mofqi

figure   offers visual comparison pareto points tables     report
numerical evaluation  including hypervolume loss achieved algorithms
w r t  sdp approximation    pmga attains best performance    
objective cases  followed pfa  sms emoa returns good approximation 
slowest  requiring ten times amount samples used pmga  mofqi
outperforms pmga sample complexity  loss highest  finally  figure  
shows hypervolume trend pmga comparison sample complexity
 objective case  pmga substantially sample efficient algorithms 
attaining larger hypervolume much fewer rollouts  example  capable
generating frontier hypervolume ra one tenth rollouts 
outperforms pfa half samples needed latter 
   results regarding mofqi include loss number rollouts hypervolume
number solutions available original paper 

   

fimorl continuous pareto manifold approximation

     

pmga

pfa         
sms emoa          

hypervolume

    

reps         

     

ra         

    
     
    
     

     

     

     

      

      

      

      

 rollouts
figure    comparison sample complexity  objective case using hypervolume
evaluation score  brackets number rollouts needed algorithm produce
best frontier  pmga clearly outperforms competing algorithms  requires
much fewer samples generate frontiers better hypervolume 
table    numerical algorithm comparison  objective water reservoir  sdp
reference frontier hypervolume           solutions 
algorithm

hypervolume

loss

 rollouts

 solutions

             

             

              



pfa

             

             

                

        

ra

             

             

                

          

 

             

       

 

reps

             

             

               

      

sms emoa

             

             

                

          

pmga

mofqi

   metrics tuning
section want examine deeply tuning mixed metric parameters 
order provide reader better insights correct use metrics  performance pmga strongly depends indicator used and  thereby  configuration
critical  precise  mixed metrics  obtained best approximate pareto
frontiers experiments conducted section    include trade off accuracy
covering  expressed parameters  following  analyze fundamental
concepts behind metrics study performance influenced changes
parameters 
   

fiparisi  pirotta    restelli

approximate frontier
     

true pareto frontier

   

   

   

   

   

   

   

   
   

 a     

     

   
   

   

   

   

 b       

   

   

   

   

 c     

figure     approximate frontiers  objective lqg learned pmga using i pn
varying   figure  a  indicator penalize enough dominated solutions 
figure  c  frontier wide enough  contrary  figure  b 
algorithm achieves accuracy covering 

    tuning
first indicator  to maximized  analyze
  iau w 
w penalization term  previous sections proposed w     ipn
w     iu   order take advantage expansive behavior antiutopiabased
indicator accuracy optimalitybased indicator  section study
performance mixed metric changing   proposing simple tuning process 
idea set initial value increase  or decrease  approximate
frontier contains dominated solutions  or wide enough   figure    shows different
approximate frontiers obtained different values exact  objective lqg
   iterations using w     ipn   starting     indicator behaves mostly
iau   meaning small  figure   a   increasing    figure   c 
algorithm converges  approximate frontier completely cover true one 
i e   ipn mostly condition behavior metric  finally         figure   b 
approximate frontier perfectly matches true one metric correctly mixes two
single indicators 
however  already discussed section    use w     ipn problematic
difference magnitude iau ipn make tuning hard
point metric becomes ineffective  drawback solved using w     iu
normalizing reference point indicators  i e   iu iau   i j  p    kj p  k    
normalization bounds utopia antiutopiabased metrics similar intervals 
i e                respectively   
    ratio two vectors a b component wise operation 

   

fimorl continuous pareto manifold approximation

j 

j 

u

 

  

j 

au

 
 a 

j 

u

j 

au

 
 b 

u

 

j 

au

 
 c 

figure     examples pareto frontiers  figures  a   b  frontiers convex 
latter objectives normalized  figure  c  frontier concave 

    tuning
second mixed indicator  to maximized  takes advantage expansive behavior antiutopiabased indicator accuracy utopiabased one 
defined
iau
   
   
iu
    free parameters 
better understand insights guided metric definition  consider
different scenarios according shape pareto frontier  figure   a frontier
convex normalized objectives  case point closer
antiutopia utopia is  sure  dominated solution  ratio iau  iu
point frontier always greater   hence reasonable set  
     therefore  need know exactly antiutopia point
drawback antiutopiabased metric iau disappears  since take account
distance utopia point  nevertheless  setting points critical 
magnitude strongly affect pmga performance  example shown figure   b 
frontier normalized objectives different magnitude 
case  setting        indicator evaluated extrema frontier
 j          t j           t   equal          respectively  first value
negative  approximate frontier includes points true pareto frontier 
j  would perform better true pareto frontier 
contrary  frontier concave  figure   c  true point
closer antiutopia utopia dominated solution  ratio iau  iu
point frontier  with exception  eventually  ends  always
smaller one  keeping              pmga would try collapse frontier
single point  order maximize indicator  therefore  parameters need
changed accordingly trial and error  instance  returned frontier
achieve accuracy  possible solution decrease   increase    
   

fiparisi  pirotta    restelli

   conclusion
paper proposed novel gradientbased approach  namely paretomanifold
gradient algorithm  pmga   learn continuous approximation pareto frontier
momdps  idea define parametric function describes manifold
policy parameters space  maps manifold objectives space  given metric
measuring quality manifold objectives space  i e   candidate frontier  
shown compute  and estimate trajectory samples  gradient w r t 
parameters   updating parameters along gradient direction generates new
policy manifold associated improved  w r t  chosen metric  continuous frontier
objectives space  although provided derivation independent
parametric function metric used measure quality candidate solutions 
terms strongly influence final result  regarding former  achieved
high quality results forcing parameterization pass singleobjective
optima  however  trick might require domain expertise additional samples
therefore could always applicable  regarding latter  presented different
alternative metrics  examined pros cons one  shown properties
empirical analysis discussed general tuning process promising ones 
evaluation included sample complexity analysis investigate performance
pmga  comparison state of the art algorithms morl  results 
approach outperforms competing algorithms quality frontier sample
complexity  would interesting study properties theoretical perspective
order provide support empirical evidence  leave open problems
investigation convergence rate approximation error true pareto
frontier  however  think hard provide analysis general setting 
future research address study metrics parametric functions
produce good results general case  particular  investigate problems
many objectives  i e   three  highdimensional policies  since complexity manifold parameterization grows number objectives policy
parameters  polynomial parameterization could effective complex problems alternative parameterizations found  another interesting direction
research concerns importance sampling techniques reducing sample complexity
gradient estimate  since frontier composed continuum policies  likely
trajectory generated specific policy partially used estimation
quantities related similar policies  thus decreasing number samples needed
montecarlo estimate integral  moreover  would interesting investigate automatic techniques tuning metric parameters applicability
pmga multi agent scenario  e g   roijers  whiteson    oliehoek        

   

fimorl continuous pareto manifold approximation

appendix a  optimal baseline
theorem a    componentdependent baseline   optimal baseline  i  j  component
 i j 
hessian estimate hrf  jd    given equation    

 i j 
bh 



 
 i j 
g    



e r   

 
 
 i j 
e g    

 


 i j 

g

 i j 

      ln p      j ln p        h

ln p       

given baseline b  variance reduction obtained optimal baseline bh 
var  hrf  jd    b   var  hrf  j    bh      


 i j   

 i j 
 
b
bh 
 i j 
e
g    
 

n
 i j 

proof  let g

     i  j  th component g    
 i j 

g

 i j 

      ln p      j ln p        h

ln p       

 i j 

variance hrf  jd    given by  
var



 i j 
hrf  jd



i 
 
  h

 i j 
 i j 
 i j 
g    
e r    b i j  g    
     e r    b





 

 
 i j 
 i j 
 
 i j   
 e b
g    
  e r    g    





 
h
i 
 i j 
 i j 
 b i j  e r    g    
e r   g    
 




minimizing previous equation w r t  b i j  get

 i j 

bh 



 
 i j 
e r    g    

 
   
 i j 
e g    

    use compact notation e    denote e    

   

fiparisi  pirotta    restelli

excess variance given




 i j 
 i j 
 i j 
var g     r    b i j    var g     r    bh   




 
 
 

 
 i j 
 i j 
 i j 
 i j 
 i j 
 
 b
e r    g    
 e b
g    
  e r    g    





h

 

 
i 
 i j 
 i j   
 i j 
 i j 
 
e bh 
e r   g    
e r    g    
g    





  h
i 
 i j 
 i j 
 i j 
  e r   g    
   bh  e r    g    




 
 

 
 i j 
 i j 
 i j 
 i j 
  b
e g    
 b
e r    g    




 

 
 
 i j 
 i j 
 i j 
 i j 
bh  e g    
   bh  e r    g    






 
 

 

 i j 
 i j 
 i j 
 
 i j 
e g    
 b
e r    g    
  b








   



 i j 

 
e r    g    

 i j 




g    
  e

 i j 
e g    


 
 i j 


 

e r    g    
 i j 
 




r    g    
   

 
e

 i j 
e g    

 

 
 

 i j 
 i j 
 i j 
 i j 
 b
e r    g    
e g    
  b






   
 i j 
e r    g    

 
 
 i j 
e g    




 
 i j 
g    



e r   


 i j   
 i j 

  b
 b
 

 i j 
e g    
e







 i j 

  b

 i j 

g


   

 


 i j   
bh  e




 
 i j 
g    


 

   



   
 i j 

e r    g    



 




 

 i j 
e g    


fimorl continuous pareto manifold approximation

references
ahmadzadeh  s   kormushev  p     caldwell  d          multi objective reinforcement
learning auv thruster failure recovery  adaptive dynamic programming
reinforcement learning  adprl        ieee symposium on  pp     
athan  t  w     papalambros  p  y          note weighted criteria methods compromise solutions multi objective optimization  engineering optimization         
       
barrett  l     narayanan  s          learning optimal policies multiple criteria 
proceedings   th international conference machine learning  icml    
pp        new york  ny  usa  acm 
bertsekas  d  p          dynamic programming suboptimal control  survey
adp mpc   european journal control                    
beume  n   naujoks  b     emmerich  m          sms emoa  multiobjective selection based
dominated hypervolume  european journal operational research               
     
brown  m     smith  r  e          directed multi objective optimization  international
journal computers  systems  signals             
calandra  r   peters  j     deisenrothy  m          pareto front modeling sensitivity
analysis multi objective bayesian optimization  nips workshop bayesian
optimization  vol    
castelletti  a   corani  g   rizzolli  a   soncinie sessa  r     weber  e          reinforcement learning operational management water system  ifac workshop
modeling control environmental issues  keio university  yokohama  japan 
pp         
castelletti  a   pianosi  f     restelli  m          tree based fitted q iteration multiobjective markov decision problems  neural networks  ijcnn        international joint conference on  pp     
castelletti  a   pianosi  f     restelli  m          multiobjective reinforcement learning
approach water resources systems operation  pareto frontier approximation
single run  water resources research                   
crites  r  h     barto  a  g          elevator group control using multiple reinforcement
learning agents  machine learning                   
das  i     dennis  j          closer look drawbacks minimizing weighted sums
objectives pareto set generation multicriteria optimization problems  structural
optimization               
das  i     dennis  j  e          normal boundary intersection  new method generating
pareto surface nonlinear multicriteria optimization problems  siam journal
optimization                
deisenroth  m  p   neumann  g     peters  j          survey policy search robotics 
foundations trends robotics                
   

fiparisi  pirotta    restelli

fonteneau  r     prashanth  l  a          simultaneous perturbation algorithms batch
off policy search    rd ieee conference decision control  cdc       los
angeles  ca  usa  december              pp            ieee 
friedrich  t   horoba  c     neumann  f          multiplicative approximations
hypervolume indicator  proceedings   th annual conference genetic
evolutionary computation  gecco     pp          new york  ny  usa  acm 
furmston  t     barber  d          unifying perspective parametric policy search
methods markov decision processes  pereira  f   burges  c   bottou  l    
weinberger  k   eds    advances neural information processing systems     pp 
          curran associates  inc 
gabor  z   kalmar  z     szepesvari  c          multi criteria reinforcement learning 
shavlik  j  w   ed    proceedings fifteenth international conference
machine learning  icml        madison  wisconsin  usa  july              pp 
        morgan kaufmann 
greensmith  e   bartlett  p  l     baxter  j          variance reduction techniques
gradient estimates reinforcement learning  journal machine learning research 
            
harada  k   sakuma  j     kobayashi  s          local search multiobjective function
optimization  pareto descent method  proceedings  th annual conference
genetic evolutionary computation  gecco     pp          new york  ny 
usa  acm 
harada  k   sakuma  j   kobayashi  s     ono  i          uniform sampling local paretooptimal solution curves pareto path following applications multi objective
ga  lipson  h   ed    genetic evolutionary computation conference  gecco
      proceedings  london  england  uk  july             pp          acm 
kakade  s          optimizing average reward using discounted rewards  helmbold  d  p  
  williamson  r  c   eds    computational learning theory    th annual conference
computational learning theory  colt       th european conference
computational learning theory  eurocolt       amsterdam  netherlands  july
             proceedings  vol       lecture notes computer science  pp     
     springer 
koski  j     silvennoinen  r          norm methods partial weighting multicriterion optimization structures  international journal numerical methods
engineering                   
lizotte  d  j   bowling  m     murphy  s  a          linear fitted q iteration multiple
reward functions  journal machine learning research               
lizotte  d  j   bowling  m  h     murphy  s  a          efficient reinforcement learning
multiple reward functions randomized controlled trial analysis  furnkranz  j  
  joachims  t   eds    proceedings   th international conference machine
learning  icml      june              haifa  israel  pp          omnipress 
   

fimorl continuous pareto manifold approximation

magnus  j  r     neudecker  h          matrix differential calculus applications
statistics econometrics  wiley ser  probab  statist   texts references
section  wiley 
mannor  s     shimkin  n          steering approach multi criteria reinforcement
learning  dietterich  t   becker  s     ghahramani  z   eds    advances neural
information processing systems     pp            mit press 
mannor  s     shimkin  n          geometric approach multi criterion reinforcement
learning  j  mach  learn  res             
messac  a     ismail yahaya  a          multiobjective robust design using physical programming  structural multidisciplinary optimization                 
messac  a   ismail yahaya  a     mattson  c  a          normalized normal constraint method generating pareto frontier  structural multidisciplinary
optimization               
munkres  j  r          analysis manifolds  adv  books classics series  westview press 
natarajan  s     tadepalli  p          dynamic preferences multi criteria reinforcement
learning  raedt  l  d     wrobel  s   eds    machine learning  proceedings
twenty second international conference  icml        bonn  germany  august
            vol      acm international conference proceeding series  pp         
acm 
nojima  y   kojima  f     kubota  n          local episode based learning multiobjective behavior coordination mobile robot dynamic environments  fuzzy
systems        fuzz       th ieee international conference on  vol     pp 
       vol   
okabe  t   jin  y     sendhoff  b          critical survey performance indices
multi objective optimisation  evolutionary computation        cec         
congress on  vol     pp         vol   
parisi  s   pirotta  m   smacchia  n   bascetta  l     restelli  m          policy gradient
approaches multi objective sequential decision making       international joint
conference neural networks  ijcnn       beijing  china  july             pp 
          ieee 
perny  p     weng  p          finding compromise solutions multiobjective markov
decision processes  coelho  h   studer  r     wooldridge  m   eds    ecai        th european conference artificial intelligence  lisbon  portugal  august       
      proceedings  vol      frontiers artificial intelligence applications  pp 
        ios press 
peters  j          machine learning motor skills robotics  ph d  thesis  university
southern california 
peters  j   mulling  k     altun  y          relative entropy policy search  fox  m  
  poole  d   eds    proceedings twenty fourth aaai conference artificial
intelligence  aaai        pp            aaai press 
   

fiparisi  pirotta    restelli

peters  j     schaal  s       a   natural actor critic  neurocomputing                      
progress modeling  theory  application computational intelligenc   th
european symposium artificial neural networks        th european symposium
artificial neural networks      
peters  j     schaal  s       b   reinforcement learning motor skills policy gradients 
neural networks                   robotics neuroscience 
pianosi  f   castelletti  a     restelli  m          tree based fitted q iteration multiobjective markov decision processes water resource management  journal hydroinformatics                 
pirotta  m   parisi  s     restelli  m          multi objective reinforcement learning
continuous pareto frontier approximation  bonet  b     koenig  s   eds    proceedings twenty ninth aaai conference artificial intelligence  january       
      austin  texas  usa   pp            aaai press 
pirotta  m   restelli  m     bascetta  l          adaptive step size policy gradient
methods  burges  c  j  c   bottou  l   ghahramani  z     weinberger  k  q   eds   
advances neural information processing systems       th annual conference
neural information processing systems       proceedings meeting held december
           lake tahoe  nevada  united states   pp           
robert  c     casella  g          monte carlo statistical methods  springer texts
statistics  springer verlag new york 
roijers  d  m   vamplew  p   whiteson  s     dazeley  r          survey multi objective
sequential decision making  journal artificial intelligence research            
roijers  d  m   whiteson  s     oliehoek  f  a          computing convex coverage sets
faster multi objective coordination  journal artificial intelligence research     
       
romero  c          extended lexicographic goal programming  unifying approach  omega 
             
shelton  c  r          importance sampling reinforcement learning multiple
objectives  ph d  thesis  massachusetts institute technology 
steuer  r  e     choo  e  u          interactive weighted tchebycheff procedure
multiple objective programming  mathematical programming                 
sutton  r  s     barto  a  g          reinforcement learning  introduction  bradford
book  bradford book 
sutton  r  s   mcallester  d  a   singh  s  p     mansour  y          policy gradient
methods reinforcement learning function approximation  solla  s   leen 
t     muller  k   eds    advances neural information processing systems     pp 
          mit press 
tesauro  g   das  r   chan  h   kephart  j   levine  d   rawson  f     lefurgy  c         
managing power consumption performance computing systems using reinforcement learning  platt  j   koller  d   singer  y     roweis  s   eds    advances
neural information processing systems     pp            curran associates  inc 
   

fimorl continuous pareto manifold approximation

vamplew  p   dazeley  r   berry  a   issabekov  r     dekker  e          empirical evaluation methods multiobjective reinforcement learning algorithms  machine learning 
               
van moffaert  k   drugan  m  m     nowe  a          scalarized multi objective reinforcement learning  novel design techniques  adaptive dynamic programming
reinforcement learning  adprl        ieee symposium on  pp         
van moffaert  k     nowe  a          multi objective reinforcement learning using sets
pareto dominating policies  journal machine learning research               
waltz  f  m          engineering approach  hierarchical optimization criteria  automatic
control  ieee transactions on                 
wang  w     sebag  m          hypervolume indicator dominance reward based multiobjective monte carlo tree search  machine learning                   
williams  r          simple statistical gradient following algorithms connectionist reinforcement learning  machine learning                  
yu  p     leitmann  g          compromise solutions  domination structures  salukvadzes solution  journal optimization theory applications                 
zitzler  e   thiele  l     bader  j          set based multiobjective optimization  evolutionary computation  ieee transactions on               
zitzler  e   thiele  l   laumanns  m   fonseca  c  m     da fonseca  v  g          performance assessment multiobjective optimizers  analysis review  evolutionary
computation  ieee transactions on                

   


