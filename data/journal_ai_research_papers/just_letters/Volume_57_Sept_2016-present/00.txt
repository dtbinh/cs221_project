journal of artificial intelligence research               

submitted        published      

learning continuous time bayesian networks in
non stationary domains
simone villa
fabio stella

villa disco unimib it
stella disco unimib it

department of informatics  systems and communication
university of milano bicocca
viale sarca            milan  italy

abstract
non stationary continuous time bayesian networks are introduced  they allow the
parents set of each node to change over continuous time  three settings are developed for
learning non stationary continuous time bayesian networks from data  known transition
times  known number of epochs and unknown number of epochs  a score function for each
setting is derived and the corresponding learning algorithm is developed  a set of numerical
experiments on synthetic data is used to compare the effectiveness of non stationary continuous time bayesian networks to that of non stationary dynamic bayesian networks  furthermore  the performance achieved by non stationary continuous time bayesian networks
is compared to that achieved by state of the art algorithms on four real world datasets 
namely drosophila  saccharomyces cerevisiae  songbird and macroeconomics 

   introduction
the identification of relationships and statistical dependencies between components in multivariate time series  and the ability of reasoning about whether and how these dependencies
change over time is crucial in many research domains such as biology  economics  finance 
traffic engineering and neurology  to mention just a few  in biology  for example  knowing
the gene regulatory network allows to understand complex biological mechanisms ruling the
cell  in such a context  bayesian networks  bns   pearl        segal  peer  regev  koller 
  friedman        scutari   denis         dynamic bayesian networks  dbns   dean
  kanazawa        zou   conzen        vinh  chetty  coppel    wangikar        and
continuous time bayesian networks  ctbns   nodelman  shelton    koller        acerbi 
zelante  narang    stella        have been used to reconstruct transcriptional regulatory
networks from gene expression data  the effectiveness of discrete dbns has been investigated to identify functional correlations among neuroanatomical regions of interest  burge 
lane  link  qiu    clark         while a useful primer on bns for functional magnetic resonance imaging data analysis has been made available  mumford   ramsey         however 
the mentioned applications require the time series to be generated from a stationary distribution  i e  one which does not change over time  while stationarity is a reasonable
assumption in many situations  there are cases where the data generating process is clearly
non stationary  indeed  in the last years  researchers from different disciplines  ranging from
economics to computational biology  to sociology and to medicine have become interested
in representing relationships and dependencies which change over time 
c
    
ai access foundation  all rights reserved 

fivilla   stella

specifically  researchers have been interested in analyzing the temporal evolution of
genetic networks  lebre  becq  devaux  stumpf    lelandais         the flow over neural
information networks  smith  yu  smulders  hartemink    jarvis         heart failure  liu 
hommersom  van der heijden    lucas         complications in type   diabetes  marini 
trifoglio  barbarini  sambo  camillo  malovini  manfrini  cobelli    bellazzi        and
the dependence structure among financial markets during crisis  durante   dunson        
according to the specialized literature on evolution models  robinson   hartemink        
they can be divided into two main categories  structurally non stationary  i e  those models
which are allowed to change their structure over time  and parametrically non stationary 
i e  those models which only allow the parameters values to change over time 
in this paper  the structurally non stationary continuous time bayesian network model
 nsctbn  is introduced  a nsctbn consists of a sequence of ctbns which improves expressiveness over a single ctbn  indeed  a nsctbn allows the parents set of each node to
change over time at specific transition times and thus it allows to model non stationary systems  to learn a nsctbn  the bayesian score for learning ctbns is extended  nodelman 
shelton    koller         the nsctbn version of the bayesian score is still decomposable
by variable and it depends on the knowledge setting which can be  known transition times 
where transition times are known  known number of epochs  where only the number of transition times is known  and unknown number of epochs  where the number of transition times
is unknown  a learning algorithm for each knowledge setting is designed and developed 
experiments against non stationary dynamic bayesian networks  nsdbns   robinson  
hartemink         i e  the discrete time counterparts of nsctbns  have been performed 
the main contributions of this paper are the following 
 definition of the structurally non stationary continuous time bayesian network model 
 derivation of the bayesian score decomposition under each knowledge setting 
 the design of algorithms for learning nsctbns under different knowledge settings 
a novel dynamic programming algorithm for learning nsctbns under the known
transition times setting is described  while learning nsctbns under the others settings
is performed by simulated annealing  exploiting the dynamic programming algorithm 
 performance comparison between nsctbns and nsdbns under all knowledge settings
for a rich set of synthetic data generated by nsctbns and nsdbns 
 performance comparison between nsctbns and state of the art algorithms on realworld datasets  namely drosophila  saccharomyces cerevisiae and songbird 
 a nsctbn learned on a macroeconomics dataset consisting of variables evolving at
different time granularities spanning from  st january      to   st march      
the rest of the paper is organized as follows  in section   continuous time bayesian networks are introduced together with their learning problem from complete data  section  
introduces non stationary continuous time bayesian networks  presents three learning settings and derives their corresponding bayesian score functions  algorithms for learning
nsctbns under different learning settings are described in section    numerical experiments on synthetic and real world datasets are presented in section    section   closes the
paper by making conclusions and indicating directions for further research activities 
 

filearning continuous time bayesian networks in non stationary domains

   continuous time bayesian networks
continuous time bayesian networks combine bayesian networks and homogeneous markov
processes together to efficiently model discrete state continuous time dynamical systems
 nodelman et al          they are particularly useful for modeling domains in which variables evolve at different time granularities  such as to model the presence of people at their
computers  nodelman   horvitz         to study reliability of dynamical systems  boudali
  dugan         to model failures in server farms  herbrich  graepel    murphy         to
detect network intrusion  xu   shelton         to analyze social networks  fan   shelton 
       to model cardiogenic heart failure  gatti  luciani    stella        and to reconstruct
gene regulatory networks  acerbi   stella        acerbi  vigano  poidinger  mortellaro 
zelante    stella         recently  the complexity of inference in continuous time bayesian
networks has been studied  sturlaugson   sheppard        
    basics
the representation ability of continuous time bayesian networks is inherent to the factorization of the system dynamics into local continuous time markov processes that depend on
a limited set of states  the continuous time bayesian network model is defined as follows 
definition    continuous time bayesian network  nodelman et al          let x be a
set of random variables x    x    x            xn    each x has a finite domain of values
v al x     x    x            xi    a continuous time bayesian network over x consists of two
    specified as a bayesian network over x 
components  the first is an initial distribution px
the second is a continuous time transition model specified as  a directed  possibly cyclic 
p a x 
graph g whose nodes are x    x            xn   a conditional intensity matrix  cim   qx
 
for each variable x  x  where p a x  denotes the set of parents of x in the graph g 
p a x 

the conditional intensity matrix qx

consists of the set of intensity matrices

qxpa  u
 qxpa  xu 
 

 
qxpai xu 


u
qpa
x

 
 
 
 


qxpa  xui
qxpa  xui 
 

 
pau
qxi

where
pau ranges over all possible configurations of the parents set p a x   while qxpai u  
p
pau
pau
pau
xj   xi qxi xj   off diagonal elements of qx   i e  qxi xj   are proportional to the probability
that the variable x transitions from state xi to state xj given the parents state pau   the
pau
u
intensity matrix qpa
x can be equivalently summarized with two independent sets  q x  
pau
 qxi      i  i   i e  the set of intensities parameterizing the exponential distributions
pau
pau
pau
u
over when the next transition occurs  and  pa
x    xi xj   qxi xj  qxi      i  j  i  j    i  
i e  the set of probabilities parameterizing the multinomial distributions over where the
state transitions  note that the ctbn model assumes that only one single variable can
change state at any specific instant  while its transition dynamics are specified by its parents
via the cim  and they are independent of all other variables given its markov blanket 
 

fivilla   stella

    structural learning
given a fully observed dataset d  i e  a dataset consisting of multiple trajectories  whose
states and transition times are fully known  the problem of learning the structure of a ctbn
has been addressed as the problem of selecting the graph g  which maximizes the bayesian
score computed on the dataset d  nodelman et al         
bs  g   d    ln p  g    ln p  d g  

   

where p  g  is the prior over the graph g and p  d g  is the marginal likelihood 
the prior p  g  over the graph g  which allows us to prefer some ctbns structures
over others  is usually assumed to satisfy the structure modularity property  friedman  
koller         i e  to decompose into the following product of terms 
y
p  g   
p  p a x    p ag  x   
   
xx

a term for each parents set p ag  x  in the graph g  a uniform prior over g is often used 
the marginal likelihood p  d g  depends on the prior over parameters p  q g    g  g  which
is usually assumed to satisfy the global parameter independence  the local parameter independence and the parameter modularity properties  which are outlined below 
global parameter independence  spiegelhalter   lauritzen        states that the paramp a  x 
p a  x 
eters q x g
and  x g
associated with each variable x in a graph g are independent 
thus the prior over parameters decomposes by variable as follows 
y
p a  x  p a  x 
p  q g    g  g   
p  q x g    x g  g  
   
xx

local parameter independence  spiegelhalter   lauritzen        asserts that the parameters associated with each configuration pau of the parents p ag  x  of a variable x are
independent  therefore  the parameters associated with each variable x are decomposable
by parent configuration pau as follows 
yy
p a  x  p a  x 
u
   
p  q x g    x g  g   
p  qxpai u    pa
xi  g  
pau xi

parameter modularity  geiger   heckerman        asserts that if a variable x has the same
parents p ag  x    p ag    x  in two distinct graphs g and g     then the probability density
functions of the parameters associated with x must be identical 
p ag  x 

p  q x

p ag  x 

  x

p ag    x 

 g    p  q x

p ag    x 

  x

 g     

   

furthermore  we also assume that the sets of parameters characterizing the exponential distributions are independent of the sets of parameters characterizing the multinomial
distributions 
p  q g    g  g    p  q g  g p   g  g  
   
   a trajectory is defined to be a sequence of pairs  t  x t    where each transition time t      t   is
associated with the state x t  of all the random variables corresponding to the nodes of the ctbn 

 

filearning continuous time bayesian networks in non stationary domains

a dirichlet distribution is selected as the prior for the parameters associated with the multinomial distribution  while a gamma distribution is selected as the prior for the parameters
associated with the exponential distribution  i e 

p  qxpai u   gamma xpai u   xpai u  
   

pau
pau
pau
p   xi   dir xi x            xi xi  
   
where xpai u   xpai u   xpai xu            xpai xui are the priors hyperparameters  in particular  the  hyperparameters represent the pseudocounts for the number of transitions from state to state 
while the  parameter represents the imaginary amount of time spent in each state before
any data is observed  note that the hyperparameter xpai u is inversely proportional to the
number of joint states of the parents of x  conditioning on the dataset d  we obtain the
following posteriors over parameters 

u
 
   
p  qxpai u  d  gamma xpai u   mxpai u   xpai u   txpa
i

pau
pau
pau
pau
pau
    
p   xi  d  dir xi x    mxi x            xi xi   mxi xi  
u
and mxpai xuj are the sufficient statistics of the ctbn  nodelman et al         
where txpa
i
u
in particular  txpa
is the amount of time spent by the variable x in the state xi while its
i
parents p a x  are in state pau   while mxpai xuj is the number of times that the variable x
transitions from the state xi to the state xj while its parents p a x  are in state pau    
in the bayesian score     the term p  g  does not grow with the size of dataset d 
thus  the significant term is the marginal likelihood p  d g   in the case of complete data 
while exploiting the parameters independence     and the global parameter independence
property      the marginal likelihood can be written as follows 
y
p a  x 
p a  x 
p  d g   
m l q x g  d  m l  x g  d  
    

xx
p ag  x 

where m l q x

 d  is the marginal likelihood of q derived as follows 
yy
pau xi

p ag  x 

and m l  x

  xpai u   mxpai u       xpai u  
u
  xpai u       xpai u   txpa
i  

u
 pa
xi    

pau
u
 pa
xi  mxi    

 

 d  is the marginal likelihood of  derived as follows 

y y
y  xpai xuj   mxpai xuj
  xpai u  
 
pau
pau 
pau 

 
 
m
 


x
x
x
i
i
i xj
pa x  x
x   x
u

i

j

i

    

    

j

under the bayesian dirichlet equivalent  bde  metric version for ctbns  nodelman        
in this case  the bde metric uses the priors     and      while the parameter modularity     
as well as the global     and the local     parameter independence properties are assumed
to be satisfied 
   please note that the number of times the p
variable x leaves the state xi while its parents p a x  are in
pau
u
state pau is computed as follows mxpa
 
xj   xi mxi xj  
i

 

fivilla   stella

in conclusion  the bayesian score     can be computed in closed form by assuming the
structure modularity property     is satisfied  and using the bde metric as follows 
x
p a  x 
p a  x 
ln p  p a x    p ag  x     ln m l q x g  d    ln m l  x g  d       
bs g   d   
xx

since the graph g of a ctbn does not have acyclicity constraints  it is possible to maximize
the bayesian score      by separately optimizing the parents set p a x  for each variable
x  it is worthwhile to mention that if the maximum number of parents is set  then the
search of the optimal value of the bayesian score      can be performed in polynomial time 
the search can be performed by enumerating each possible parents set or by using a greedy
hill climbing procedure with operators to add  delete or reverse edges of the graph g 

   non stationary continuous time bayesian networks
continuous time bayesian networks are both structurally stationary  as the graph does not
change over time  and parametrically stationary  as the conditional intensity matrices do
not change over time  these stationarity assumptions are reasonable in many situations 
but there are cases where the data generating process is intrinsically non stationary and
thus ctbns can no longer be used  therefore  in this section  we extend ctbns to become
structurally non stationary  i e  we allow the ctbns structure to change over continuous
time 
    definition
in the non stationary continuous time bayesian network model  the graph of the ctbn
is replaced by a graphs sequence g    g    g            ge    where a graph ge represents the
causal dependency structure of the model for the epoch e                 e     this model is
structurally non stationary because of the introduction of the graphs sequence and it can
handle transition times that are common to the whole network and or node specific 
following the notations and definitions used for non stationary dynamic bayesian networks  we let t    t            te    be the transition times sequence  i e  the times at which
the causal dependency structure ge   active at epoch e  is replaced by the causal dependency
structure ge     which becomes active at epoch e      an epoch is defined to be the period
of time between two consecutive transitions  i e  the epoch e is active during the period of
time starting at te  and ending at te   the graph ge     which is active during the epoch
e      differs from the graph ge   which is active during the epoch e  in a set of edges that
we call the set of edge changes ge  
figure   shows a graphs sequence g    g    g    g    g    consisting of four epochs  e     
with transition times t    t    t    t     each epoch is associated with a set of edge changes 
specifically  the graph g  differs from the graph g  by the following set of edge changes
g     x   x    x    x    x    x     the graph g  differs from the graph g  by the
following set of edge changes g     x   x    and the graph g  differs from the graph
g  by the following set of edge changes g     x   x    x   x    x    x    x    x    
   it is worthwhile to mention that the first epoch  i e  the epoch starting at time   and ending at time t 
is associated with the graph g    while the last epoch  i e  the epoch starting at time te  and ending at
time t  the supremum of the considered time interval  i e     t   is associated with the graph ge  

 

filearning continuous time bayesian networks in non stationary domains

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

 

 

 

 

t 

t 

 

t 

t

figure    graphs sequence g    g    g    g    g    of a nsctbn with four epochs  e      and
three transition times  t    t    t    t     where the edges are gained and lost over time 
non stationary continuous time bayesian networks allow each node to have its own
sequence of parents sets  each parents set being active at a given epoch  therefore  we
introduce the concept of homogeneous interval h x     h            hm   associated with node
x  which is defined as the union of consecutive epochs during which the same parents set
p a x  is active for the node x  note that if each epoch is associated with a different
parents set  then m is equal to e 
a non stationary continuous time bayesian network is defined as follows 
definition     structurally  non stationary continuous time bayesian network  let x be
a set of random variables x            xn   each x has a finite domain of values v al x   
 x            xi    a  structurally  non stationary continuous time bayesian network nns  
 b  mns   over x consists of two components 
    specified as a bayesian network b over x 
 an initial distribution px

 a non stationary continuous time transition model mns specified as 
 a sequence of directed  possibly cyclic  graphs g    ge  e
e   whose nodes are
x            xn   where e represents the number of epochs 
p a  x 

g
 a conditional intensity matrix  qx h x 
  x  x  where p ag  x  denotes the
parents sets of x in g  and h x  denotes the intervals associated with x 

p a  x 

g
the conditional intensity matrix qx h x 
consists of a set of intensity matrices
u
qxpa   h
m
pa
 q u
x  x   hm
 

 
pau
qxi x   hm



u
qpa
x hm

 
 
 
 


qxpa  xui  hm
qxpa  xui  hm 
 

 
pau
qxi  hm

one for each configuration pau of each parents set p a x   p ag  x  which is active during
the interval hm  h x   
u
   note that the following equation qxpai  h
 
m

p

xj   xi

 

qxpai xuj  hm still holds 

fivilla   stella

    learning framework
learning a nsctbn from a fully observed dataset d can be done using the bayesian learning
framework taking into account the entire graphs sequence g  in the nsctbns case  we must
specify the prior probability over the graphs sequence g and  for each possible sequence  the
density measure over possible values of the parameters q g and  g   once they prior p  g 
and the likelihood p  q g    g  g  are given  the marginal likelihood p  d g  can be computed
and the bayesian score can be evaluated  it is important to note that we are focused on
recovering the graphs sequence g and not on detecting possible changes of the parameters 
in fact  we identify non stationarity in the parameters of the model  i e  the entries of the
conditional intensity matrices  that are significant enough to result in structural changes of
the graph  others changes are assumed to be small enough not to alter the graph structure 
      prior probability over graphs
given the transition times t   and thus the number of epochs e  we assume that the prior
over the nsctbns structure g can be written as follows 
p  g t     p  g         ge  t     p  g    g         ge   t     p  g   p  g         ge   t   
    
equation      is justified because we assume that the probability distribution over edge
changes only is a function of the number of changes performed  which can also be defined
independently of the initial graph g    if some knowledge about particular edges or the
overall topology is available for the initial network  then we can use an informative prior
p  g    otherwise we can resort to a uniform distribution  as in ctbns  p  g    must satisfy the structure modularity assumption      while the prior over the set of edge changes
p  g            ge   t   defines the way in which edges change through adjacent epochs 
      prior probability over parameters
the prior over parameters p  q g    g  g  t   is selected to satisfy the following assumptions 
independence between the sets of parameters characterizing the exponential and the multinomial distributions      parameter modularity     and parameter independence  the latter
assumption is divided into three components for nsctbns  global parameter independence 
interval parameter independence and local parameter independence 
global parameter independence asserts that the parameters associated with each node
in a nsctbns graphs sequence are independent  so the prior over parameters decomposes
by variable x as follows 
y
p ag  x 
p ag  x 
p  q g    g  g  t    
p  q x h x 
   x h x 
 g  t   
    
xx

interval parameter independence states that the parameters associated with each interval
of the active parents for each node are independent  so the parameters associated with each
x and its parents sets p ag  x  are decomposable by interval hm  h x  as follows 
p a  x 

p a  x 

g
g
p  q x h x 
   x h x 
 g  t    

y
hm

 

p a  x 

p a  x 

p  q x hgm    x hgm  g  t   

    

filearning continuous time bayesian networks in non stationary domains

local parameter independence states that the parameters associated with each state of
a variable in a given interval are independent  thus the parameters associated with each x
in the interval hm  h x  are decomposable by parent configuration pau as follows 
yy
p a  x  p a  x 
u
u
    
   pa
p  qxpai  h
p  q x hgm    x hgm  g  t    
xi  hm  g  t   
m
pau xi

as in the ctbns case  a dirichlet distribution is used as prior for the parameters of the
multinomial distribution and a gamma distribution is used as prior for the parameters of
u
is the
the exponential distribution  the sufficient statistics are modified as follows  txpa
i  hm
amount of time spent in state x   xi while p a x    pau in the interval h x    hm   while
mxpai xuj  hm is the number of transitions from state x   xi to state x   xj while p a x    pau
p
in the interval h x    hm   we let mxpai  hu m   xj   xi mxpai xuj  hm to be the number of times
x leaves state xi while its parents p a x  are in state pau during the interval h x    hm  
      marginal likelihood
given the graphs sequence g  and the transition times t   the marginal likelihood p  d g  t  
of the dataset d can be computed in closed form using the priors and the sufficient statistics
previously defined  to derive the bayesian dirichlet equivalent metric for nsctbns  we
make the same assumptions as those for ctbns  in this case  the parameter independence
assumption is divided into global       interval      and local      parameter independence 
therefore  the marginal likelihood becomes 
y
p ag  x 
p ag  x 
p  d g  t    
m l q x h x 
 d  m l  x h x 
 d  
    
xx

the marginal likelihood of q in equation      can be calculated as follows 
 pau    


xi  hm
pau
pau
u
 
 

 
m
 xpai  h
y
y
y
xi  hm
xi  hm
m
p ag  x 
m l q x h x   d   
 pau  m pau      


xi  hm
xi  hm
pau
pau
pa
hm pau xi   u    
xi  hm   txi  hm
xi  hm

    

while the marginal likelihood of  in equation      can be calculated as follows 




pau
pau
u
 xpai  h


 
m
y
y
y
y
xi xj  hm
xi xj  hm
m
p ag  x 




m l  x h x 
 d   
 
pau
pau
pau
 xi xj  hm
hm pau xi  xj  xi  hm   mxi  hm
xi   xj
    
it is important to note that for nsctbns  the pseudocounts  as well as the imaginary
amount of time  are associated with each interval  this aspect requires a careful choice in
order not to be too biased towards these values when small intervals are analyzed 
a possible correction is to weight the ctbns hyperparameters by a quantity proportional to the time interval width  hm  hm     where hm denotes the total time  thus  the
nsctbns hyperparameters could be defined as follows 
xpai xuj  hm
xpai  hu m

 hm  hm   
 
hm
 hm  hm   
  xpai u
 
hm
  xpai xuj

 

    
    

fivilla   stella

if you want to control the parameter priors using only two hyperparameters  and   
then you can use the uniform bde for nsctbns  bdeu   in this case  the hyperparameters
defined in      and      are divided by the number u of possible configurations of the
parents p a x  of node x times the cardinality i of the domain of x  as follows 
xpai xuj  hm

 

xpai  hu m

 

  hm  hm   
 
ui
hm
  hm  hm   
 
ui
hm

    
    

equations      and      rescale the hyperparameters in such a way not to be biased with
respect to the epochs length  while equations      and      are based on the uniform
distribution and they have been used for performing all numerical experiments 
    bayesian score decomposition
the bayesian score can be decomposed by variable based on the information available
about the transition times  in this regard  three knowledge settings are used to derive the
bayesian score  namely  known transition times  ktt   known number of epochs  kne 
and unknown number of epochs  une  
      known transition times
in this setting  the transition times t are known  thus  the prior probability over the
graphs sequence p  g t   decomposes as in equation       while the marginal likelihood
decomposes by variable x according to equation      
therefore  the bayesian score bs g   d  t   can be written as follows 
bs g   d  t     ln p  g      ln p  g            ge   t  
p a  x 

p a  x 

g
g
  ln m l q x h x 
 d    ln m l  x h x 
 d  

    

in such a setting the structural learning problem of a non stationary continuous time
bayesian network consists of finding the graph g  active during the first epoch  e      and
the e    sets of edge changes g            ge  together with the corresponding parameters
values  which maximize the bayesian score defined in equation      
the graphs g            ge are selected by making assumptions on the ways by which the
edges change over continuous time  a common approach  robinson   hartemink       
consists of assuming that the graphs sequence g    g            ge   depends on a parameter
which controls the number of edge changes over continuous time  this approach uses a
truncated geometric distribution  with parameter p      exp c    to model the number
of parents changes occurring at transition time te    
x
ce  
 ge  x   
    
xx

the variable ce counts the number of edge changes between two consecutive graphs ge and
ge     while the parameter c controls the impact of the number of edge changes ce on the
score function      
  

filearning continuous time bayesian networks in non stationary domains

if the edge changes ge are assumed to be mutually independent  then the probability
for the edge changes through subsequent epochs can be written as follows 
p  g            ge   t    

e 
y
e  

e 
y
    exp c    exp c   ce

 exp c   ce  
    exp c   cmax   

    

e  

where cmax is the truncation term  therefore  if we assume a truncated geometric distribution on the number of parents changes occurring at each transition times and equation
     holds  then the bayesian score      decomposes by variable x as follows 
bs g   d  t    

x

ln p  p a x    p ag   x    c

xx
p a  x 

g
  ln m l q x h x 
 d   

e 
x

ce
e  
p ag  x 
ln m l  x h x 
 d  

    

it is worthwhile to notice that the number of parents changes ce for each epoch e
penalizes the bayesian score  and thus it discourages sudden variations in the parents set
between consecutive epochs  while the parameter c controls the impact of such changes on
the score function      
      known number of epochs
if the transition times t are unknown  then the bayesian score can be written as follows 
bs g  t   d    ln p  g  t     ln p  d g  t   

    

assuming that p  g  t     p  g p  t   the bayesian score      becomes 
bs g  t   d    ln p  g    ln p  t     ln p  d g  t   

    

if the number of epochs e is known  then the prior probability p  g  over the graphs
sequence g decomposes as in equation       while a truncated geometric distribution can
be used on the number of parents changes occurring at each transition time  as in the
known transition times setting 
any choice for p  t   can be made to include prior knowledge about the set of transition
times  however  if no information is available  a uniform prior on p  t   is used  implying
that all possible values of transition times are equally likely for a given number of epochs
e  thus  the bayesian score      can be decomposed by variable x as follows 
bs g  t   d    ln p  t    
 

x

ln p  p a x    p ag   x    c

xx
p ag  x 
ln m l q x h x   d 

e 
x

ce

e  
p a  x 

g
  ln m l  x h x 
 d  

    

where ce counts the number of edge changes between two consecutive parents sets  while c
controls the impacts on bs g  t   d  of such edge changes  as it happens under the ktt
setting 
  

fivilla   stella

      unknown number of epochs
if the number of epochs e is unknown  then transition times t are unknown as well 
under this setting  we learn a nsctbn by exploiting what introduced under the ktt
and kne settings  we assume that the structure of the non stationary continuous time
bayesian network can evolve at different speeds over continuous time  such an assumption
is incorporated by using a truncated geometric distribution with parameter p    exp e  
on the number of epochs  in general  large values of e encode the strong prior belief that
the structure of the nsctbn changes slowly  i e  few epochs exist  
following what we presented under the ktt setting  the bayesian score can be obtained
by subtracting the parameter e times the number of epochs e  therefore  the bayesian
score bs g  t   d  decomposes by variable x as follows 
bs g  t   d    ln p  t    e e  

x

ln p  p a x    p ag   x    c

ce

e  

xx
p a  x 

e 
x

p a  x 

g
g
  ln m l q x h x 
 d    ln m l  x h x 
 d  

    

note that the bayesian score      contains two parameters  namely c and e   which
encode our prior belief about the structure of the nsctbn  specifically  the parameter c
regulates our prior belief about the smoothness of the edge changes  e g  encouraging or
discouraging the edge changes per epoch   while the parameter e regulates our prior belief
about the number of epochs  e g  encouraging or discouraging the creation of epochs  

   structural learning
the optimal structure of nsctbns can be found by separately maximizing the components
of the bayesian score associated with each node  this can be achieved by using an exact optimization algorithm based on dynamic programming when the transition times are
given  by contrast  when only the number of epochs is known or no information about the
transition times is available  we have to resort to approximate techniques based on monte
carlo or on simulated annealing  we present the exact algorithm for solving the structural
learning problem under the ktt setting  then  we briefly outline the stochastic algorithms
to solve the structural learning problem under the kne setting and under the une setting 
    known transition times
under this setting the bayesian score decomposes according to equation       thus  the
optimal graphs sequence g  can be found by separately searching the optimal parents sequence g x for each node x  to solve the problem of finding the optimal parents sequence
g x for node x we consider a sequence consisting of m intervals h x     h            hm   and
s possible parents  so to have z    s possible parents sets  to find the optimal parents
sequence g x we must compute m  z marginal likelihood terms associated with q and  
one marginal likelihood term for each possible parents set p az  x  and each interval hm  
then  an optimization algorithm can be used to find the maximum of the component of the
bayesian score associated with the node x 
  

filearning continuous time bayesian networks in non stationary domains

an exhaustive search would be prohibitive  as it would require evaluating z m scores 
one for each possible parents sequence g x   unfortunately  also a greedy search strategy
that computes the parents set which maximizes the bayesian score for each interval is not
viable  in fact  the function that counts the parents changes ce in      binds the choice of
the subsequent parents set  i e  it binds ge to ge    
however  the relation between the score of the variable x associated with the parents set
p a x 
p a x  in the interval hm   denoted as bsx hm   and the score associated with the parents set
p a x 

p a x  in the interval hm    denoted as bsx hm    can be defined by recursion as follows 
n
o
p a x 
p a  x 
p a x  p a x 
bsx hm   max bsx hzm   c cx e   ln m l q x hm    x hm  d   
    
p az

where cx e    ge  x    while the marginal likelihoods of q and  are grouped together 
p a x 
the score bsx hm   associated with the parents set p a x  for node x in the interval hm   is
introduced to clarify the recursion used in the algorithm    note that this score depends on
all the components of the score up to hm   in particular  not only the marginal likelihoods
component is involved  but also the term cx e   which counts the parents changes  is included
as it binds the choice of subsequent parents sets  equation      is exploited by dynamic
programming to select the optimal parents sequence g x for each node x 
algorithm   takes as input the marginal likelihoods of q and  for each interval and
parents set  the prior probability about the initial parents set  the number of parents
changes  and the parameter c   algorithm   ensures the optimal parents sequence g x for
the node x and its corresponding optimal bayesian score  its core is the computation of
the m  z score matrix  denoted by sc  through the dynamic programming recursion  the
dynamic programming recursion for the interval h   m      is defined as follows 
p a  x 

sc z   ln m l q x hz 

p a  x 

   x hz 

 d    ln p  p az  x    p agh   x   

    

for    z  z  while  for the intervals hm  m              m    the recursion is 
n
o
p a  x  p a  x 
z
u
scm
  max scm 
  ln m l q x hzm    x hzm  d   c cx e  
 uz

after filling the score matrix sc  the value maxz  sc m  z   is the optimal bayesian score 
while the optimal parents sequence is reconstructed backwards from m to   by using the
index matrix in   the cost of computing the dynamic programming recursion is o m z     
which is polynomial for a fixed maximum number of parents s 
the problem of selecting the optimal parents sequence has an interesting graph representation  indeed  it is possible to create a graph whose nodes are associated with marginal
likelihoods of q and  for the interval hm and for the parents set p az  x   while each node associated with the interval hm is linked with all the nodes associated with the interval hm    
each arc is associated with a weight computed as the difference between the marginal likelihoods in the interval hm for the parents set p az  x  and the cost of switching from the
parents set of the interval hm  to the parents set of the interval hm   two special nodes
are added to represent the start and the end of the optimal parents sequence  such a graph
does not have cycles  thus the selection of the optimal parents sequence for each node can
be reduced to the longest path problem from the start node to the end node of a directed
acyclic graph  and thus it can be solved using either dynamic or linear programming 
  

fivilla   stella

algorithm   learnkttx
require  matrix containing the marginal likelihoods of q and  m lx m  z   vector containing the prior probability about the initial parents set p r z   matrix containing the
number of parents changes c z  z  and the parameter for the parents changes c  
ensure  score matrix sc m  z  and index matrix in  m  z  
   initialize sc m  z     in  m  z     
   for m             m do
  
for z             z do
  
if  m      then
  
sc m  z   ln m lx m  z    ln p r z 
  
else
  
for w             z do
  
score  sc m     w    ln m lx m  z   c c w  z 
  
if  score   sc m  z   then
   
sc m  z   score
   
in  m  z   w
   
end if
   
end for
   
end if
   
end for
    end for
learning a nsctbn can be done following the following four steps procedure  i  use
u
the dataset d to compute for each variable x the sufficient statistics txpa
and mxpai xuj  hm
i  hm
according to the given transition times t   ii  compute the marginal likelihoods      and      
and then fill the m lx matrix  iii  run algorithm   for each node x to get the corresponding
optimal parents sequence  iv  collect the optimal parents sequence for each node x and
compute the corresponding cims using the sufficient statistics already computed in step i  
if we allow the intervals to differ from the transition times  i e  they can be obtained
as one of all the possible unions of transition times  then we have to repeat the learning
procedure for all the e   e       cases  it is possible to speed up the computation
because the sufficient statistics can be aggregated through intervals  in such a way  we read
the dataset once  while the precomputed marginal likelihoods can be stored and reused for
the same intervals  moreover  the computations can be performed in parallel for each node 
    known number of epochs
in this setting  we know the number of epochs  but the transition times are not given  so we
cannot directly apply algorithm    however  once a tentative allocation t of the transition
times is given  we can apply algorithm   to obtain the optimal nsctbns structure  under
the assumption that t is not too different from the true transition times t   to find an
optimal tentative allocation t    i e  an allocation that is as close as possible to t   we apply
the simulated annealing  sa  algorithm  kirkpatrick  gelatt    vecchi        
  

filearning continuous time bayesian networks in non stationary domains

simulated annealing is an iterative algorithm that attempts to find the global optimum
x of a given function f  x  through a stochastic search over the feasible region  at iteration
k  when the sa algorithm is assumed to be in state xk   it samples a proposal state x 
according to some proposal distribution x   p     xk    then  the sa algorithm computes the
quantity    exp   f  x   f  x     ct    where ct is the computational temperature  the
sa algorithm accepts the proposal state x  with probability equal to min       concisely 
sa always accepts any proposal state x  where f  x      f  x  by setting xk     x    while it
accepts the proposal state x  when f  x      f  x  with probability  by setting xk     x 
with probability  and xk     xk with probability        i e  in this case the state of
the sa algorithm does not change  the computational temperature reduces over iterations
according to a cooling schedule  it has been shown that if one cools sufficiently slowly  then
the algorithm will probably find the global optimum  kirkpatrick et al          the design
of the cooling schedule is an important part of the sa algorithm  bertsimas   tsitsiklis 
       a possible approach is to use an exponential cooling schedule defined as follows 
ctk   ct    k   where ct  represents the initial temperature  typically set to       is the
cooling rate  usually set to be close to      while k is the current iteration  murphy        
in the nsctbns case  the state of the sa algorithm x is associated with the tentative
allocation t   while the function f  x  is the bayesian score       algorithm   takes as input
the sufficient statistics  the parameters used to run algorithm   and the parameters of the
sa algorithm  it solves the structural learning problem under the kne setting for a given
variable x by ensuring the optimal tentative allocation t  and its corresponding score 
algorithm   learnknex
require  sufficient statistics suffstatsx  prior probability p r    number of parents
changes c      parameter c   tentative allocation t   initial temperature ct    cooling
rate   number of iterations iters  truncation parameter z and standard deviation  
ensure  optimal tentative allocation t  and best bayesian score bestsc 
   initialize k     t   t  
   m lx  getmlx suffstatsx   t  
   bestsc  learnkttx m lx  p r    c      c  
   while  k   iters  do
  
t  tentativeallocation t    z   
  
m lx  getmlx suffstatsx   t  
  
tentsc  learnkttx m lx  p r    c      c  
  
ct  ct    kn

o
  
accp rob  min    exp   bestsctentsc 
ct
   
ur  unirand  
   
if  ur  accp rob  then
   
t   t
   
currsc  tentsc
   
end if
   
k k  
    end while
    bestsc  currsc
  

fivilla   stella

the simulated annealing parameters we used include the tentative allocation t   the
initial temperature ct    the cooling rate  and the number of iterations iters for the
exponential cooling schedule  moreover  the truncation parameter z and standard deviation
 are used for the selection of the new tentative allocation t   according to the random
procedure shown in algorithm    this procedure selects a transition time through a discrete
uniform distribution  uniranddiscr t    and perturbs it according to a truncated normal
distribution  stdnormrand    having a standard deviation equal to   with the addition of
point masses at z and z  where z represents the truncation parameter 
algorithm   tentativeallocation
require  tentative allocation t   truncation parameter z and standard deviation  
ensure  new tentative allocation t    
   t  uniranddiscr t  
   t    t   t
   nr  stdnormrand  
   if  nr   z  then
  
nr  z
   end if
   if  nr   z  then
  
nr  z
   end if
    t  t   nr  
    t    t  t

    unknown number of epochs
in this setting the number of epochs is unknown  thus the structural learning algorithm
must be able to move across a different number of epochs  as well as the corresponding
transition times  also in this case  we used a simulated annealing algorithm where the state
x is the tentative allocation t and the function to be optimized f  x  is the bayesian score
shown in equation       the cooling schedule has been set the same as the one used under
the kne setting  the proposal distribution differs from the one used under the kne setting
as it uses two additional operators  namely the split and the merge operators  the split
operator allows to split a given interval  tm   tm     into two subintervals  tm   t  and  t  tm    
where tm   tm    t   the merge operator allows to merge contiguous intervals  tm    tm  
and  tm   tm     to form the wider interval  tm    tm     where tm    tm   tm    t  
the new state is obtained by sampling the number of epochs changes ec from a multinoulli distribution with parameters  p    p    p     where p  represents the probability that the
number of epochs of the next iteration  t   is decreased by one  p  represents the probability
that the number of epochs of the next iteration  t   is increased by one  and p  represents
the probability that number of epochs of the next iteration  t   does not change with respect
to the current one  if ec is equal to    then algorithm   is invoked  if ec is equal to    then
the merge operator is applied before invoking algorithm    while if ec is equal to    then
the split operator is applied before invoking algorithm   
  

filearning continuous time bayesian networks in non stationary domains

algorithm   solves the structural learning problem of nsctbn under the une setting
for a given node x by ensuring the optimal tentative allocation t  and its corresponding
bayesian score  this algorithm is similar to the one used under the kne settings  but it
uses algorithm   to apply the split and merge operators  the left t  function in algorithm
  returns the transition time in t which comes immediately before transition time t 
algorithm   learnunex
require  sufficient statistics suffstatsx  prior probability p r    number of parents
changes c      parameter c   parameter e   tentative allocation t   initial temperature
ct    cooling rate   number of iterations iters  truncation parameter z  standard deviation   split probability sp and merge probability mp 
ensure  optimal tentative allocation t  and best bayesian score bestsc 
   initialize k     t   t  
   bestsc learnkttx getmlx suffstatsx  t    p r    c      c   e  t  
   while  k   iters  do
  
t  splitmerge t    sp  mp 
  
t  tentativeallocation t   z   
  
tentsc  learnkttx getmlx suffstatsx  t    p r    c      c   e  t  
  
ct  ct    kn

o
  
accp rob  min    exp   bestsctentsc 
ct
  
ur  unirand  
   
if  ur  accp rob  then
   
t   t
   
currsc  tentsc
   
end if
   
k k  
    end while
    bestsc  currsc
algorithm   splitmerge
require  tentative allocation t   split probability sp and merge probability mp 
ensure  new tentative allocation t    
   t    t
   p  unirand  
   if  p   mp  then
  
t  uniranddiscr t  
  
t    t   t
   else
  
if  p    mp   sp   then
  
t  uniranddiscr t  t  
  
nt  left t    tleft t 
 
   
t    t  nt
   
end if
    end if
  

fivilla   stella

   numerical experiments
numerical experiments are performed on both synthetic and real world datasets  synthetic
datasets are used to compare nsctbns to nsdbns under the ktt  kne and une knowledge settings in terms of accuracy  precision  recall and f  measure  the following real world
datasets  drosophila  saccharomyces cerevisiae and songbird  are used to compare nsctbns
to state of the art algorithms  i e  tsni  a method based on ordinary differential equations  
nsdbn  robinson   hartemink        and non homogeneous dynamic bayesian networks
with bayesian regularization  tvdbn   dondelinger  lebre    husmeier         under the
une knowledge setting  drosophila  saccharomyces cerevisiae and songbird datasets are
collected at fixed time intervals  thus we analyzed an additional real world dataset  consisting of financial economic variables evolving at different time granularities  to exploit the
expressiveness of nsctbns when events occur asynchronously  note that while the performance comparison using synthetic datasets benefits from the knowledge of the ground
truth  the same does not apply to the performance comparison using real world datasets
because the ground truth is not available  in such cases  the comparison exploits partial
and meta knowledge available in the specialized literature 
    synthetic datasets
artificially generated datasets include data sampled from a rich set of nsdbn models 
i e  nsdbn generated datasets  and a rich set of nsctbn models  i e  nsctbn generated
datasets  such nsdbn and nsctbn models consist of five nodes associated with binary
and ternary variables  numerical experiments concern learning the parents sets  transition
times and the number of epochs for a single node  this choice is motivated by the fact that
structural learning for nsctbn can be performed for each single node independently from
the remaining ones  however  when transition times are unknown  having multiple parents
sets changes could make it easier to correctly identify the times of change 
      nsdbn generated datasets
nsdbn generated datasets were sampled from nsdbn models  associated with the following
number of epochs e                in particular  for each number of epochs e     different
nsdbn instances were sampled to obtain a number of datasets equal to     each one consisting of a single trajectory  thus     synthetic datasets were used to learn the structure
of nsdbn and nsctbn  number of models     under the ktt  kne and une settings 
structural learning experiments were performed with c             and e              
for nsctbn and s             and with m                 for nsdbn    an overall number
of       experiments have been performed  in particular  we performed number of epochs 
number of datasets  number of c or s  number of models               experiments
under the ktt setting      under the kne setting  while number of epochs  number of
datasets  number of c or s  number of e or m  number of models               
experiments have been performed under the une setting 
   inter slice arcs are allowed  while intra slice arcs are not allowed  this holds true for all nsdbn models
sampled to obtain the nsdbn generated datasets 
   it is worthwhile to mention that the s and m parameters are the nsdbn counterparts of the c and
e parameters for the nsctbn 

  

filearning continuous time bayesian networks in non stationary domains

the nsdbn jar executable   robinson   hartemink        was used for structural learning of nsdbn  where we set the maximum number of proposed networks to         and
the burn in period to        for nsdbn  nsctbn were learned by using the following parameters setting  iters          ct                   z            sp        mp       
     and        using the bdeu metric  furthermore  for nsdbn and nsctbn we set
the maximum number of parents to    only arcs that occurred in more than    percent
of the samples  belong to the inferred nsdbn and nsctbn models  accuracy  acc   precision  p rc   recall  rec  and f  measure  f    achieved by nsdbn and nsctbn learned
under the ktt  kne and une settings are reported in table      and   respectively  it
is worthwhile to mention that under the kne and une settings  nsdbns and nsctbns
almost always identified the correct number of epochs and the location of their associated
transition times  accuracy  precision  recall and f  measure have been computed in two
different ways  firstly  we included all arcs of the true network for each epoch  secondly  we
excluded the self reference arcs  i e  those arcs connecting the same node in two consecutive
time slices of the true network for each epoch  in fact  while each node of a nsctbn has the
self reference arc by default  the same does not happen for nsdbns  this means that in the
first case a nsdbn is required to learn arcs that a nsctbn is not required to do  therefore 
to ensure a fair comparison of nsctbn to nsdbn we adopted the second case  tables   
  and   report the performance measure values computed by excluding self reference arcs
from the set of arcs of the true networks for each epoch 
table    nsctbn compared to nsdbn under the ktt setting for nsdbn generated data 
average  min  subscript  and max  superscript  performance values over    networks and
c for nsctbn and s for nsdbn 
number of epochs e
 
 

 
acc
p rec
rec
f 

 

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

according to tables      and    nsdbns consistently achieve greater accuracy values
than those achieved by nsctbns under the three settings  furthermore  for nsdbns the
accuracy is stable with respect to the number of epochs e while the same does not happen
for nsctbns  indeed  when the number of epochs e is greater than    nsctbns achieve
accuracy values which are significantly smaller than those achieved when the number of
epochs e is equal to   or    the same does not happen to nsdbns where the accuracy is
robust with respect to the number of epochs e 
   we acknowledge the precious help of alex hartemink who let us use the nsdbn jar executable program
for learning nsdbn models  furthermore  he also provided the drosophila and songbird datasets 
   samples are obtained under the same parameters values 

  

fivilla   stella

table    nsctbn compared to nsdbn under the kne setting for nsdbn generated data 
average  min  subscript  and max  superscript  performance values over    networks and
c for nsctbn and s for nsdbn 
number of epochs e
 
 

 
acc
p rec
rec
f 

 

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

table    nsctbn compared to nsdbn under the une setting for nsdbn generated data 
average  min  subscript  and max  superscript  performance values over    networks and
c   e for nsctbn and s   m for nsdbn 
number of epochs e
 
 

 
acc
p rec
rec
f 

 

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

a different picture emerges when focusing on the task to discover positive arcs  indeed 
in such a case nsctbns achieve values of precision  recall and f  measure  which are always
greater than those achieved by nsdbns  nsctbns achieve precision values which are robust
with respect to the knowledge settings and the number of epochs e  the same does not
hold true for the recall performance measure  indeed  nsctbns achieve a robust recall
with respect to the knowledge settings  ktt  kne and une   while the recall achieved
by nsctbns significantly degrades when moving from   to   epochs under all knowledge
settings  the same happens to the f  measure achieved by nsctbns  the results of
numerical experiments suggest that nsctbns are more effective than nsdbns to discover
positive arcs  even if the datasets have been generated using nsdbns  a possible explanation
for this behavior is that learning nsdbns is more difficult than learning nsctbns  in
particular  nsdbns must learn self reference arcs while nsctbns do not  furthermore  for
each node  nsctbns learn locally the sequence of parents sets while the same does not
happen for nsdbns  in fact  nsdbns learn globally the sequence of parents sets for all
nodes  i e  they globally learn the sequence of networks  and thus they solve a learning
problem which is more difficult than the one solved by nsctbns 
  

filearning continuous time bayesian networks in non stationary domains

      nsctbn generated datasets
we generated    synthetic datasets with e                these datasets are then used to
learn the structure of nsctbn under the three knowledge settings  the same parameters
setting is used as the one used for nsctbn learning from nsdbn generated datasets  for
nsctbn  we used              and the bdeu metric   while  in this case  we did not
perform structural learning experiments for nsdbn models    the graphical structures of
the nsctbn models sampled to obtain the datasets are the same as those sampled to
obtain the nsdbn datasets  the goal of these experiments is to analyze the performance
of nsctbn structural learning algorithms under the three knowledge settings 
the analysis of data reported in tables      and   brings us to conclude that the
nsctbn structural learning algorithms work very well under the three settings according
to the considered performance measures  accuracy  recall and f  measure decrease slightly
when the number of epochs increases from   to    in particular  the recall measure suffers
the greatest decrease from   to      when the number of epochs increases from   to   
accuracy and f  measure are very robust with respect to the number of epochs  while
precision is the most robust performance measure with respect to different datasets and
different values of the number of epochs under all knowledge settings 
table    nsctbn under the ktt setting for nsctbn generated data  average  min
 subscript  and max  superscript  performance values over    networks and c  

acc
p rec
rec
f 

 
    
        
    
        
    
        
    
        

number of
 
        
    
        
    
        
    
        
    

epochs e
 
        
    
        
    
        
    
        
    

 
        
    
        
    
        
    
        
    

table    nsctbn under the kne setting for nsctbn generated data  average  min
 subscript  and max  superscript  performance values over    networks and c  

acc
p rec
rec
f 

 
    
        
    
        
    
        
    
        

number of
 
        
    
        
    
        
    
        
    

epochs e
 
        
    
        
    
        
    
        
    

 
        
    
        
    
        
    
        
    

   nsctbn generated data are asynchronous involving different time granularities  thus nsdbn cannot be
directly applied  an option is to preprocess these datasets to adapt them to nsdbns  given that this
would be strongly arbitrary and be penalizing for nsdbns  we decided to learn only the nsctbn models 

  

fivilla   stella

table    nsctbn under the une setting for nsctbn generated data  average  min
 subscript  and max  superscript  performance values over    networks and c and e  

 
    
        
    
        
    
        
    
        

acc
p rec
rec
f 

number of
 
        
    
        
    
        
    
        
    

epochs e
 
        
    
        
    
        
    
        
    

 
        
    
        
    
        
    
        
    

the best and worst values of accuracy for e     reported in table   belong to the
experiments performed on synthetic dataset number   and number   respectively  their
results are illustrated hereafter  figure   a  shows the graphs sequence of the true nsctbn
for the synthetic datasets number    while figure   b  displays the posterior distribution
over epochs  right   together with the distribution of the corresponding transition times
 left    of the learned nsctbn in the une case  figure   shows the same information as
those depicted in figure    but for the synthetic dataset number    in the latter case  the
distribution over epochs is slightly in favor of the correct number of epochs 

 a  true nsctbn model 
distribution of the transition times

distribution of the number of epochs

 

 
true
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

 

  

  

  

  

  

     
time

  

  

  

  

  

  

 

 
number of epochs

 b  learned nsctbn model results 

figure    nsctbn generated dataset number     a  true graphs sequence over e   epochs
and  b  distribution of the transition times  left  and posterior over epochs  right  associated
with the nsctbn inferred under the une setting 

    transition times whose distance is less than     have been aggregated 

  

filearning continuous time bayesian networks in non stationary domains

 a  true nsctbn model 
distribution of the transition times

distribution of the number of epochs

 

 
true
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

 

  

  

  

  

  

  

     
time

  

  

  

  

  

  

  

 

 
 
number of epochs

 b  learned nsctbn model results 

figure    nsctbn generated dataset number     a  true graphs sequence over e   epochs
and  b  distribution of the transition times  left  and posterior over epochs  right  associated
with the nsctbn inferred under the une setting 

    real world datasets
it is very difficult to find real world datasets where the corresponding ground truth model
is completely known and or a uniform consensus from domain experts has been reached 
therefore  we decided to use the following three well known datasets  drosophila  saccharomyces cerevisiae and songbird to compare the performance of nsctbns to that of nsdbns
and other state of the art algorithms  i e  tsni and tvdbn  such datasets are publicly
available  clearly described and a rich and detailed discussion about their likely ground truth
models is given in the specialized literature  furthermore  a macroeconomics dataset is introduced and analyzed  this dataset consists of    financial economic variables collected
at different time granularity spanning from  st january      to   st march      
      drosophila
the drosophila dataset includes the mrna expression levels of       genes at    successive time points spanning the four stages of the drosophila melanogaster life cycle  lebre
et al          the embryonic     time points   larval     time points  and pupal stage    
time points  and the first    days of adulthood    time points   for comparative purposes
 dondelinger et al          we have analyzed the reduced drosophila dataset consisting of
gene expression time series of    genes involved in wing muscle development  given that
nsctbns are based on discrete variables  we binarized the expression level of the    genes
for the reduced drosophila dataset as done in the literature  zhao  serpedin    dougherty 
      guo  hanneke  fu    xing        robinson   hartemink        
  

fivilla   stella

firstly  the network inference task of the embryonic  larval  pupal and adulthood morphogenic stages was performed under the ktt setting  robinson   hartemink        dondelinger et al          the nsctbn structural learning was performed using the following
parameter values c                                                      and by setting the maximum number of parents to    the nsctbn learned with different c values are combined 
and only arcs that occurred in more than    percent of samples are included into the
inferred non stationary continuous time bayesian network  no other techniques predict
non stationary directed networks  robinson   hartemink         so precision  recall and
f  measure  computed with respect to networks inferred by zhao et al         and guo et
al          are reported in table   for nsdbn  nsctbn and tvdbn  dondelinger et al  
       the networks associated with the four epochs  as inferred with the nsctbn on the
reduced drosophila dataset under the ktt setting  are depicted in figure   
table    precision  prec   recall  rec  and f  measure  f    achieved by nsctbn  nsdbn 
and tvdbn on the drosophila dataset are computed with respect to networks inferred by
zhao et al         and guo et al          average values  average  of precision  recall and
f  measure achieved by zhao et al         and guo et al         are also reported 

nsdbn
nsctbn
tvdbn

zhao
prec
    
    
    

et al        
rec
f 
         
         
         

guo et al        
prec rec
f 
              
              
              

prec
    
    
    

average
rec
f 
         
         
         

according to table    no optimal algorithm exists for the reduced drosophila dataset 
if the network retrieved by zhao et al         is used as ground truth  then nsdbn is the
best model  while if the network retrieved by guo et al         network is used as ground
truth  then tvdbn is the optimal one as far as the f  measure is concerned  if the average
performance is computed  then nsdbn is the best model and tvdbn is the worst  while
nsctbn achieves an f  value that is close to the one achieved by nsdbn 
secondly  we investigated whether the transition times inferred by structural learning of
nsctbn under the une setting correspond to the known transitions between stages  lebre
et al         dondelinger et al          the network inference task was performed by learning
nsctbn under the une setting with the following parameter values c                   
and e                   furthermore  we set the maximum number of parents to    the
number of iterations to       and the number of runs to     
figure   shows the distribution of the transition times    left  and the posterior over
the number of epochs  right   the number of epochs is correctly detected to be   even if a
probability close to     is associated with   epochs  however  the transition times are not
all correctly identified  the embryonic stage is not correctly identified  the larval stage is
correctly discovered to start at time point     while it is inferred to end at time point   
    each stem represents the posterior probability that the corresponding time point starts a new epoch 
therefore  a stem at time point t means that an epoch ends at time point t     while the next epoch
starts at time point t 

  

filearning continuous time bayesian networks in non stationary domains

instead of     nsctbn did not identify the pupal and the adulthood stages  but it identified
two additional transition times     and      the same behavior is observed for nsdbns 
while tvdbns are capable to correctly identify the pupal and the adulthood stages  however  the tvdbn    tvdbn exp and tvdbn bino inferred networks  dondelinger et al  
      consist of a number of epochs ranging from   to   

mhc

mhc
gfl

gfl

mlc 

mlc 

eve

eve

msp   

msp   

actn

actn

myo  f

myo  f

up

up

prm

prm
twi

twi

sls

sls

 a  embryonic  epoch from   to     

 b  larval  epoch from    to     

mhc

mhc
gfl

gfl

mlc 

mlc 

eve

eve

msp   

msp   

actn

actn

myo  f

myo  f

up

up

prm

prm
twi

twi

sls

sls

 c  pupal  epoch from    to     

 d  adulthood  epoch from    to     

figure    networks inferred with nsctbn under the kkt setting on the reduced drosophila
dataset  only arcs that occurred in more than    percent of the networks associated with
different c values are included in the inferred nsctbn model 

  

fivilla   stella

distribution of the transition times

distribution of the number of epochs

 

 
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

                                                                
time

 

 
 
number of epochs

figure    transition time graph  left  and posterior probability histogram over the number
of epochs e  right  associated with the nsctbn model learned from the drosophila reduced
dataset under the une setting when c       and e     

      saccharomyces cerevisiae
the saccharomyces cerevisiae dataset is obtained from a synthetic regulatory network with
  genes in saccharomyces cerevisiae  cantone  marucci  iorio  ricci  belcastro  bansal 
santini  di bernardo  di bernardo    cosma         it is obtained by measuring gene
expression time series with rt pcr  reverse transcription polymerase chain reaction  for
   and    time points under two conditions related to the carbon source  galactose  switch
on experimental condition  and glucose  switch off experimental condition   we merged the
time series from the two experimental conditions under exclusion of the boundary point as
done in the literature  dondelinger et al          the obtained time series was binarized in
such a way that a   indicates that the gene expression level is greater than or equal to its
sample mean  while a   indicates the gene expression level is smaller than its sample mean 
the obtained dataset was used to infer the saccharomyces cerevisiae networks associated
with the switch on and switch off experimental conditions 
the network inference task was performed by learning a nsctbn under the une setting
with the following parameter values c                    and e                     furthermore  we set the maximum number of parents to    the number of iterations to       and
the number of runs to      only arcs that occurred in more than    percent of the runs are
included in the inferred nsctbn model  precision  recall and f  measure values achieved
by nsctbn are compared to those achieved by the state of the art algorithms  i e  tsni 
nsdbn and tvdbn  in table   
the result of the performed numerical experiment shows that nsctbn is competitive with respect to state of the art algorithms  while it achieves non optimal results only
for precision associated with the switch on experimental condition  under this condition 
 
nsctbn achieves a precision equal to         
   while the optimal value achieved by tsni
 
and tvdbn is            on the contrary  nsctbn achieves the best recall value  which is
equal to              under the switch off experimental condition  nsctbn achieves the best
value for both precision  which is equal to              and recall  which is equal to             
  

filearning continuous time bayesian networks in non stationary domains

we also computed the overall performance of the structural learning algorithms  in this
case  focusing the attention on the f  measure  we can conclude that nsctbn        is
comparable to tvdbn         which is considered to be the state of the art algorithm for
the structural learning task applied to the saccharomyces cerevisiae dataset  the networks
inferred by the nsctbn model under the switch on and switch off experimental conditions 
using c       and c      are depicted in figure   
table    nsctbn compared to tsni  nsdbn  and tvdbn when learning from the saccharomyces cerevisiae dataset  nsctbn is learned under the une setting  c        e      
time point    is used as the transition time between the switch on and the switch off experimental conditions  tsni  nsdbn and tvdbn networks are described in the specialized
literature  precision  recall and f  measure are reported for the switch on and switch off
experimental conditions  the number of true positive arcs  superscript  and the sum of
true and false positive arcs  subscript  are reported for precision  while the number of true
positive arcs  superscript  and the sum of true positive and false negative arcs  subscript 
are reported for recall  performance values achieved by aggregating the inferred networks
over the two epochs are also reported 

tsni
nsdbn
tvdbn
nsctbn

switch on
p rec
rec
             
             
             
              

f 
    
    
    
    

switch off
p rec rec
f 
                  
                  
                  
                  

gal 

f 
    
    
    
    

gal 

gal  

cbf 

swi 

aggregated
p rec
rec
               
               
               
      
      
  
  

gal  

ash 

swi 

 a  switch on network 

cbf 

ash 

 b  switch off network 

figure    switch on  a  and switch off  b  networks inferred with nsctbn from the saccharomyces cerevisiae dataset under the une setting when c        e      the two pictures
report the positive arcs  black continuous   the false negative arcs  red dashed  and the
false positive arcs  green dotted  of the inferred networks 
  

fivilla   stella

figure   shows the posterior distribution of the number of epochs  left  together with the
distribution of the transition times  right  for the nsctbn learned with c       and e     
the transition between the switch on and switch off experimental conditions is known to
occur at time point     i e  the switch off epoch starts at time point      it is worthwhile
to notice that the small number of arcs  associated with the synthetic regulatory network
of saccharomyces cerevisiae  suggests that one should be very careful when evaluating the
result of the performed numerical experiment  in particular  we think that overstatements
on the effectiveness and or superiority of different structural learning algorithms for the
learning task on the saccharomyces cerevisiae dataset should be avoided 
distribution of the transition times

distribution of the number of epochs

 

 
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

 

 

 

 

                                         
time

 

 
number of epochs

figure    transition time graph  left  and posterior probability over the number of epochs
 right  associated with the nsctbn inferred from the saccharomyces cerevisiae dataset
under the une setting when c       and e      the maximum aposteriori estimate over
the number of epochs is associated with e     epochs  epoch   starts at time point   and
ends at time point     while epoch   starts at time point    and ends at time point    

      songbird
the songbird dataset was collected with eight electrodes placed into the vocal nuclei of six
female zebra finches  smith et al          voltage changes were recorded from populations
of neurons while the birds were provided with four different two second auditory stimuli 
each presented from    to    times  voltages were post processed with a root mean square
transformation and binned to   ms  robinson   hartemink        
the songbird dataset is used to learn neural information flow networks  i e  the networks
that represent the transmission of information between different regions of the songbird
brain  a neural information flow network represents the dynamic utilization of the potential
pathways along which information can travel  the identification of the neural information
flow networks in songbirds during auditory stimuli allows you to understand how sounds
are stored and processed in the songbirds brain  the songbird dataset consists of data
of   variables recorded from electrodes for two seconds pre stimulus  two seconds during
stimulus and two seconds post stimulus for six birds  the stimuli are hear song  i e  the
bird hears another bird singing  and white noise  i e  the bird hears a white noise stimulus 
  

filearning continuous time bayesian networks in non stationary domains

we show the results of the nsctbn learned on two out of the six birds of the songbird
dataset  namely bird     and bird      the results obtained for the other four birds are
similar  given that nsctbns are based on discrete variables  the values of the   variables
were discretized into three bins using uniform quantiles                  according to the literature
 robinson   hartemink         the inference task of the neural information flow networks
was performed by learning a nsctbn under the une setting with the following parameter
values c                            and e                             we set the maximum
number of parents to    the number of iterations to     and the number of runs to    
figure    a  and  b  show the probability of transition  left  and the posterior probability
over the number of epochs  right  for bird     and bird     under the white noise stimulus 
figure    a  and  b  show the probability of transition  left  and the posterior probability
over the number of epochs  right  for bird     and bird     under the hear song stimulus 
distribution of the transition times

distribution of the number of epochs

   

 
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number of epochs

 a  white noise stimulus for bird      learned model results 
distribution of the transition times

distribution of the number of epochs

   

 
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number of epochs

 b  white noise stimulus for bird      learned model results 

figure    distribution of the transition times and posterior distribution over epochs for
nsctbn under the une setting on the songbird dataset for the white noise stimulus 
  

fivilla   stella

the location of the transition time points under the white noise stimulus and the hearsong stimulus are accurately inferred for bird     and bird      the posterior distribution
over the number of epochs for birds     and     under the white noise stimulus is nearly
equally split between   and   epochs  while under the hear song stimulus it is peaked over  
epochs  therefore  both the number of epochs and the location of the transition time points
are reliably recovered by the nsctbn learned under the une setting  unfortunately  we
were not able to find any additional information to validate the learned nsctbns for this
dataset  moreover  a comparison across different birds to eventually develop a consensus
network is not possible due to the songbird data collection settings  indeed  each of the six
birds is characterized by its own electrodes  which make difficult to obtain a correspondence
map across different birds 

distribution of the transition times

distribution of the number of epochs

   

 
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number of epochs

 a  hear song stimulus for bird      learned model results 
distribution of the transition times

distribution of the number of epochs

   

 
retrieved
   
posterior probability

probability of transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number of epochs

 b  hear song stimulus for bird      learned model results 

figure    distribution of the transition times and posterior distribution over epochs for
nsctbn under the une setting on the songbird dataset for the hear song stimulus 

  

filearning continuous time bayesian networks in non stationary domains

      macroeconomics
the macroeconomics dataset consists of    financial economic time series pertaining to the
economy of the united states  time series have different time granularity and span from
 st january      to   st march       more specifically  five time series have daily granularity  namely crude oil  oil   usd to eur spot exchange rate  usdeur   gold  gold  
s p    equity index  s p     and the    years treasury bond yield rate  us  yrsnote  
eleven time series have monthly granularity  namely production of total industry  pti  
real manufacturing and trade industries sales  rmtis   personal income  pi   unemployment  un   consumer price index  cpi   federal funds rate  rate   producer price index
 ppi   non farm payrolls  nfp   new one family houses sold  nhsold   new houses for sale
 nhsale  and new private house permits  nhpermit   finally  the gross domestic product
 gdp  time series has quarterly granularity 
the goal of this study is to discover how the financial and economic environment evolves
over time  in particular  we focused the attention to detect business cycles   and the
associated change of relationships among financial and economic variables  given that the
duration of a business cycle is highly variable  the ability to identify the turning point of a
cycle  i e  when a recession starts  is of considerable importance to policymakers  financial
companies as well as to individuals  a substantial literature is available about the business
cycle turning points detection generally relying on markov switching models  hamilton  
raj         however  these models are not able to represent some important features such
as the dependence structure among variables in each business cycle 
in order to use the nsctbn model in such a context  we applied a binary discretization
to the variable associated with each time series  discretization was performed using a lookback period of   year  i e  if the current value is greater than the past one  then the binary
variable is set to   otherwise  it is set to    the approach of looking back into the past
is widely used in finance  moskowitz  ooi    pedersen         nsctbns learning was
performed under the une setting using the following parameter values  c               
e                   maximum parents per node      iterations and    runs 
figure    shows the probability of transition  left side  left axis  versus the s p   
equity index used as a reference  left side  right axis  and the posterior probability over the
number of epochs  right side   the nsctbn consists of three epochs with transition times
close to the end of july      and the end of november       if we compare these dates to
the turning points of the us business cycle reported by the national bureau of economic
research     then we see that we are not far from the turning point of march      and very
close to the one of december       while we missed the turning point which occurred in
july       probably because of the limited length of the dataset 
figure    shows the structure of the nsctbn model corresponding to the most probable
number of epochs  i e  e      an arc is included in the nsctbn model when it occurs in
more than     of the performed runs in each epoch  the retrieved networks correspond to
the following time periods  from january      to july       epoch     from august     
to november       epoch    and from december      to march       epoch    
    business cycles are fluctuations in aggregate economic activity  they are recurrent  i e  it is possible to
identify expansion recession cycles   persistent and not periodic  i e  they differ in length and severity  
    the official business cycle turning points and dates are available at http   www nber org cycles html

  

fivilla   stella

distribution of the transition times vs s p   

distribution of the number of epochs
    

 

   

    

   

   

    

   

    

   

   

 

posterior probability

value

probability of transition

retrieved  left 
s p     right 

 
    

    

    

    
    
time

    

   

   

 
    

    

   

 

 

 
 
 
number of epochs

figure     distribution of the transition times and s p    behavior over time  left   posterior probability over epochs  right  for the learned nsctbn under the une setting 
usdeur

oil
gold

usdeur
oil

un

gold

us  yrs

us  yrs

ppi

nhper

cpi

ppi

nhper

pti

sp   

pti

sp   

pi
rmtis

nhsale

rate

pi

cpi

rmtis

rate
nfp

gdp
nhsold
nfp

gdp
un

nhsale

 a  epoch    jan        jul       

nhsold

 b  epoch    aug        nov       
usdeur
oil

ppi

us  yrs

gold
nhsale

sp   
pti

cpi

rate

nhper
pi
rmtis
nfp
un
gdp

nhsold

 c  epoch    dec        mar       

figure     nsctbn learned on the macroeconomics dataset under the une setting 
nsctbn corresponds to the most probable number of epochs  e       an arc is included
in the nsctbn model when it occurs in more than     of the runs in each epoch 

  

filearning continuous time bayesian networks in non stationary domains

the novelty of this approach to the economic analysis opens the door to many considerations and new speculations about the economic variables during business cycles  in
this paper  we highlight two patterns emerging from the learned nsctbn model  the well
known relevant role of the personal income  pi  and its relation to the unemployment  un 
 mankiw        and the less known relation of the non farm payrolls  nfp  to the s p   
equity index  s p      miao  ramchander    zumwalt        

   conclusions
we introduced non stationary continuous time bayesian networks and developed three
structural learning algorithms to be used under different knowledge settings  i e  ktt 
kne and une  for the problem to be analyzed  the structural learning algorithm in
the known transition times case is exact and it exploits graph theory to infer the optimal
nsctbns structure  it has a polynomial time complexity under the assumption that the
maximum number of parents for each node is fixed  all the nsctbns structural learning algorithms are competitive to state of the art algorithms when synthetic and real world
datasets are considered  this statement is proved by a rich set of numerical experiments 
nsctbns can be adapted to use different score metrics  as far as the considered score
metrics integrates over the non structural parameters  nsctbns exploit an interesting
property of ctbns and offer the possibility to learn the optimal nsctbns structure for
each single variable  this could be extremely useful in the case when the non stationary
behavior of the analyzed system is not synchronous  and thus it may be the case that each
node changes its parents independently from how other nodes change their parents set 
however  two main limitations exist with nsctbns  i  the variables are assumed to be
discrete  specifically each variable of the dataset must take value over a countable number of
states and ii  finding the optimal value for the c and e hyperparameters can be extremely
difficult  the same is true for nsdbns   concerning i   the problem of discretizing continuous
variables has been studied for a long time and robust solutions have been described in the
specialized literature  discretizing continuous variables whose value is measured over time
has not been studied intensively and many issues still remain  the problem ii  of selecting
the optimal value of hyperparameters is known in the specialized literature and much can
be done when experts provide their valuable apriori knowledge  however  when such apriori
knowledge is poor or not available at all  selecting optimal hyperparameter values can be
extremely difficult  it is important to note that one of the strong limitations to studying
and comparing non stationary models is the lack of ground truth models 
possible directions for further research include the application of nsctbns structural
learning algorithms to other datasets  such as the arabidopsis thaliana dataset  grzegorczyk 
aderhold    husmeier        as well as other financial datasets supported by in depth
economic analyses  another interesting perspective is the study and development of a
modeling approach  going towards the direction of allowing each node to change its parents
set asynchronously  furthermore  we think that to increase the applicability to real world
time series data of the proposed nsctbns structural learning algorithms the issue of timeseries discretization must be addressed  in particular  we think that this issue must be
addressed in an integrated manner with the nsctbns structural learning algorithm 
  

fivilla   stella

finally  it could be interesting to apply the framework of nsctbns to address the
task of classification of objects in a streaming context when using a probabilistic graphical model based approach  borchani  martinez  masegosa  langseth  nielsen  salmeron 
fernandez  madsen    saez      a  borchani  martnez  masegosa  langseth  nielsen 
salmeron  fernandez  madsen    saez      b  

acknowledgments
the authors wish to thank alexander hartemink for having kindly provided the nsdbn
jar executable and the associated datasets  a special thank goes to marco grzegorczyk for
providing the arabidopsis thaliana dataset together with fundamental information to analyze
it  the authors are greatly indebted to anonymous referees for their constructive comments
and their extremely helpful suggestions  which contributed to significantly improve the
quality of the paper  a special thank goes to the associate editor manfred jaeger 
fabio stella is the corresponding author of this article 

references
acerbi  e     stella  f          continuous time bayesian networks for gene network reconstruction  a comparative study on time course data  in the   th international
symposium on bioinformatics research and applications  zhangjiajie  china       
   
acerbi  e   vigano  e   poidinger  m   mortellaro  a   zelante  t     stella  f         
continuous time bayesian networks identify prdm  as a negative regulator of th   cell
differentiation in humans  scientific reports           
acerbi  e   zelante  t   narang  v     stella  f          gene network inference using
continuous time bayesian networks  a comparative study and application to th   cell
differentiation  bmc bioinformatics         
ahmed  a     xing  e  p          recovering time varying networks of dependencies in social
and biological studies  proceedings of the national academy of sciences           
           
bertsimas  d     tsitsiklis  j          simulated annealing  statistical science              
borchani  h   martinez  a  m   masegosa  a   langseth  h   nielsen  t  d   salmeron  a  
fernandez  a   madsen  a  l     saez  r       a   dynamic bayesian modeling for
risk prediction in credit operations  in the   th scandinavian conference on artificial
intelligence  scai        halmstad  sweden 
borchani  h   martnez  a  m   masegosa  a  r   langseth  h   nielsen  t  d   salmeron 
a   fernandez  a   madsen  a  l     saez  r       b   modeling concept drift  a
probabilistic graphical model based approach  in the   th international symposium
on intelligent data analysis  ida        saint etienne  france 
boudali  h     dugan  j  b          a continuous time bayesian network reliability modeling  and analysis framework  ieee transactions on reliability               
  

filearning continuous time bayesian networks in non stationary domains

burge  j   lane  t   link  h   qiu  s     clark  v  p          discrete dynamic bayesian
network analysis of fmri data  human brain mapping                 
cantone  i   marucci  l   iorio  f   ricci  m  a   belcastro  v   bansal  m   santini  s  
di bernardo  m   di bernardo  d     cosma  m  p          a yeast synthetic network
for in vivo assessment of reverse engineering and modeling approaches  cell          
         
dean  t     kanazawa  k          a model for reasoning about persistence and causation 
comput  intell                 
dondelinger  f   lebre  s     husmeier  d          non homogeneous dynamic bayesian
networks with bayesian regularization for inferring gene regulatory networks with
gradually time varying structure  machine learning                 
durante  d     dunson  d  b          bayesian dynamic financial networks with timevarying predictors  statistics   probability letters           
fan  y     shelton  c  r          learning continuous time social network dynamics  in
the   th conference on uncertainty in artificial intelligence  uai        montreal 
canada 
friedman  n     koller  d          being bayesian about bayesian network structure  a
bayesian approach to structure discovery in bayesian networks  machine learning 
          
gatti  e   luciani  d     stella  f          a continuous time bayesian network model
for cardiogenic heart failure  flexible services and manufacturing journal         
       
geiger  d     heckerman  d          a characterization of dirchlet distributions through
local and global independence  annals of statistics               
grzegorczyk  m   aderhold  a     husmeier  d          inferring bi directional interactions between circadian clock genes and metabolism with model ensembles  statistical
applications in genetics and molecular biology                 
guo  f   hanneke  s   fu  w     xing  e  p          recovering temporally rewiring networks  a model based approach  in machine learning  proceedings of the   th international conference  icml        corvallis  usa  june              pp         
hamilton  j  d     raj  b   eds            advances in markov switching models  applications in business cycle research and finance  studies in empirical economics 
springer verlag 
herbrich  r   graepel  t     murphy  b          structure from failure  in the  nd usenix
workshop on tackling computer systems problems with machine learning techniques
 sysml      cambridge  usa  pp     
kirkpatrick  s   gelatt  c  d     vecchi  m  p          optimization by simulated annealing 
science                     
lebre  s   becq  j   devaux  f   stumpf  m     lelandais  g          statistical inference of
the time varying structure of gene regulation networks  bmc systems biology        
     
  

fivilla   stella

liu  m   hommersom  a   van der heijden  m     lucas  p  j          hybrid time bayesian
networks  international journal of approximate reasoning   
mankiw  n  g          principles of macroeconomics   th edition   south western college
pub 
marini  s   trifoglio  e   barbarini  n   sambo  f   camillo  b  d   malovini  a   manfrini 
m   cobelli  c     bellazzi  r          a dynamic bayesian network model for longterm simulation of clinical complications in type   diabetes  journal of biomedical
informatics               
miao  h   ramchander  s     zumwalt  j  k          s p     index futures price jumps
and macroeconomic news  journal of futures markets                   
moskowitz  t  j   ooi  y  h     pedersen  l  h          time series momentum  journal
of financial economics                  
mumford  j  a     ramsey  j  d          bayesian networks for fmri  a primer  neuroimage 
           
murphy  k  p          machine learning  a probabilistic perspective  the mit press 
nodelman  u          continuous time bayesian networks  ph d  thesis  stanford university 
nodelman  u     horvitz  e          continuous time bayesian networks for inferring users
presence and activities with extensions for modeling and evaluation  tech  rep  msrtr          microsoft research 
nodelman  u   shelton  c  r     koller  d          continuous time bayesian networks  in
the   th conference on uncertainty in artificial intelligence  uai        edmonton 
canada  pp         
nodelman  u   shelton  c     koller  d          learning continuous time bayesian networks  in the   th conference on uncertainty in artificial intelligence  uai       
acapulco  mexico  pp         
pearl  j          probabilistic reasoning in intelligent systems   networks of plausible inference  morgan kaufmann series in representation and reasoning  morgan kaufmann 
robinson  j  w     hartemink  a  j          learning non stationary dynamic bayesian
networks  journal of machine learning research               
scutari  m     denis  j  b          bayesian networks with examples in r  chapman and
hall  boca raton  isbn                
segal  e   peer  d   regev  a   koller  d     friedman  n          learning module networks  journal of machine learning research            
smith  a  v   yu  j   smulders  t  v   hartemink  a  j     jarvis  e  d          computational inference of neural information flow networks  plos computational biology 
        e     
spiegelhalter  d  j     lauritzen  s  l          sequential updating of conditional probabilities on directed graphical structures  networks                 
  

filearning continuous time bayesian networks in non stationary domains

sturlaugson  l     sheppard  j  w          inference complexity in continuous time bayesian
networks  in the   th conference on uncertainty in artificial intelligence  uai
       quebec city  canada  pp         
vinh  n  x   chetty  m   coppel  r     wangikar  p  p          gene regulatory network
modeling via global optimization of high order dynamic bayesian network  bmc
bioinformatics          
xu  j     shelton  c  r          continuous time bayesian networks for host level network
intrusion detection  in the european conference on machine learning and principles
and practice of knowledge discovery in databases  ecml pkdd        antwerp 
belgium  pp         
zhao  w   serpedin  e     dougherty  e  r          inferring gene regulatory networks
from time series data using the minimum description length principle  bioinformatics 
                  
zou  m     conzen  s  d          a new dynamic bayesian network  dbn  approach for identifying gene regulatory networks from time course microarray data  bioinformatics 
             

  

fi