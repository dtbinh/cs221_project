journal of artificial intelligence research                  

submitted        published      

effective heuristics for suboptimal best first search
christopher wilt
wheeler ruml

wilt at cs unh edu
ruml at cs unh edu

department of computer science
university of new hampshire
durham  nh       usa

abstract
suboptimal heuristic search algorithms such as weighted a  and greedy best first search
are widely used to solve problems for which guaranteed optimal solutions are too expensive
to obtain  these algorithms crucially rely on a heuristic function to guide their search 
however  most research on building heuristics addresses optimal solving  in this paper 
we illustrate how established wisdom for constructing heuristics for optimal search can fail
when considering suboptimal search  we consider the behavior of greedy best first search
in detail and we test several hypotheses for predicting when a heuristic will be effective for
it  our results suggest that a predictive characteristic is a heuristics goal distance rank
correlation  gdrc   a robust measure of whether it orders nodes according to distance to
a goal  we demonstrate that gdrc can be used to automatically construct abstractionbased heuristics for greedy best first search that are more effective than those built by
methods oriented toward optimal search  these results reinforce the point that suboptimal
search deserves sustained attention and specialized methods of its own 

   introduction
a  is a best first search that expands nodes in order of f  n  where f  n    g n    h n  
while the optimal solutions provided by a   hart  nilsson    raphael        are the most
desirable  time and memory often prevent the application of this algorithm  when a  fails
because of either insufficient time or memory  practitioners sometimes turn to bounded
suboptimal algorithms that may not return the optimal solution  but that return a solution
that is guaranteed to be no more than a certain factor more expensive than the optimal
solution 
the most well known of these is likely weighted a   pohl         which is a best first
search that expands nodes in f  order  where f   n    g n    w  h n    w         variants
of weighted a  are used in a wide variety of applications  including domain independent
planning  helmert        richter   westphal        and robotics  likhachev  gordon   
thrun        likhachev   ferguson         weighted a  is also a component of a number
of anytime algorithms  for example  anytime restarting weighted a   richter  thayer 
  ruml        and anytime repairing a   likhachev et al         both use weighted a  
anytime nonparametric a   van den berg  shah  huang    goldberg        doesnt use
weighted a  per se  but rather its limiting case  greedy best first search  doran   michie 
       best first search on h n   all of these anytime algorithms have  built in  the implicit
assumption that weighted a  with a high weight or greedy best first search will find a
solution faster than a  or weighted a  with a small weight 
c
    
ai access foundation  all rights reserved 

fiwilt   ruml

in many popular heuristic search benchmark domains  e g   sliding tile puzzles  grid path
planning  towers of hanoi  topspin  robot motion planning and the traveling salesman
problem  increasing the weight does lead to a faster search  until the weight becomes so
large that weighted a  has the same expansion order as greedy best first search  which
results in the fastest search  the first contribution of this paper is to provide illustrations
of how  in some domains  greedy best first search performs worse than weighted a   and is
sometimes even worse than a  
we show that the failure of greedy best first search is not merely a mathematical curiosity  only occurring in hand crafted counterexamples  but rather a phenomenon that can
occur in real domains  including variants of popular single agent heuristic benchmarks  our
second contribution is to empirically characterize conditions when this occurs  knowledge
that is important for anyone using a suboptimal search  this is also an important first step
in a predictive theoretical understanding of the behavior of suboptimal heuristic search 
the root cause of the failure of greedy best first search can be ultimately traced back
to the heuristic  which is used to guide a greedy best first search to a goal  for a   there
are a number of well documented techniques for constructing an effective heuristic  we
revisit these guidelines in the context of greedy best first search  our third contribution is
to show that  if one follows the well established guidelines for creating a quality heuristic for
a   the results can be poor  we present several examples where following the a  wisdom
for constructing a heuristic leads to slower results for greedy best first search  we use
these examples to understand the requirements that greedy best first search places on its
heuristic 
our fourth contribution is a quantitative metric for assessing a greedy heuristic  goal
distance rank correlation  gdrc   gdrc can be used to predict whether or not greedy
best first search is likely to perform well  gdrc can also be used to compare different
heuristics for the same domain  allowing us to make more informed decisions about which
heuristic to select if there are a variety of choices  as is the case for abstraction based
heuristics like pattern databases  this quantitative metric can be used to automatically
construct a heuristic for greedy best first search by iteratively refining an abstraction and
measuring how good each candidate heuristic is  we show that iteratively refining an
abstraction using a simple hill climbing search guided by gdrc can yield heuristics that
are more powerful than those built by traditional methods oriented toward optimal search 
this work increases our understanding of greedy best first search  one of the most popular and scaleable heuristic search techniques  more generally  it suggests that techniques
developed for optimal search are not necessarily appropriate for suboptimal search  suboptimal search is markedly different from optimal search  and deserves its own theory and
methods 

   a conundrum  ineffective weighted a 
the starting point for our investigation of heuristics for suboptional search begins with a
curious empirical observation  although weighted a  is one of the most popular way of
speeding up heuristic search  increasing the weight of weighted a  does not always work 
in order to get a better grasp on the question of when increasing the weight is ineffective 
we first need some empirical data 
   

fieffective heuristics for suboptimal best first search

domain
dynamic robot
hanoi     
pancake     
   tiles  unit 
grid
topspin    
topspin    
   tiles  inverse 
city navigation    
city navigation    
city navigation    

average solution
length
      
     
     
     
       
    
     
     
     
     
     

total
states
          
           
       
           
         
           
           
           
      
      
      

branching
factor
     
 
  
   
   
  
  
   
   
    
    

unit cost
no
yes
yes
yes
yes
yes
yes
no
no
no
no

table    domain attributes for benchmark domains considered
    benchmark domains
we consider six standard benchmark domains  the sliding tile puzzle  the towers of hanoi
puzzle  grid path planning  the pancake problem  topspin  and dynamic robot navigation 
we selected these domains because they represent a wide variety of interesting heuristic
search features  such as branching factor  state space size  and solution length  since we
would like to compare against a   we are forced to use somewhat smaller puzzles than it is
possible to solve using state of the art suboptimal searches  our requirement for problem
size was that the problem be solvable by a   weighted a   and greedy best first search in
main memory  eight gigabytes   basic statistics about each of these domain variants are
summarized in table   
for the sliding tile    puzzle         we used random instances and the manhattan
distance heuristic  we used the    puzzle  rather than the    puzzle for two reasons  first 
optimally solving    puzzles using a  without running out of memory requires significant
resources  at least    gigabytes  significantly more than our eight gigabyte limit  according
to burns et al          in addition to that  we consider the sliding tile puzzle with non unit
cost functions  these non unit problems are significantly more difficult to solve than the
unit cost variants  the non unit version of sliding tile puzzle we consider uses the inverse
cost function  where the cost of moving a tile n is   n  the manhattan distance heuristic 
when weighted appropriately  is both admissible and consistent for this cost function  for
the towers of hanoi  we considered the    disk   peg problem  and used two disjoint pattern
databases  one for the bottom    disks  and one for the top two disks  korf   felner 
       for the pancake problem  we used the gap heuristic  helmert         for grid path
planning  we used maps that were     x     cells  with     of the cells blocked  using the
manhattan distance heuristic with four way movement  in the topspin puzzle  the objective
is to sort a circular permutation by iteratively reversing a continuous subsequence of fixed
size  an example of a topspin puzzle is in figure    we considered a problem with   
disks with a turnstile that would turn either three or four disks  denoted by topspin    and
topspin     for a heuristic  we used a pattern database with   contiguous disks present 
   

fiwilt   ruml

figure    a    disk topspin puzzle 
and the remaining   disks abstracted  for the dynamic robot navigation problem  we used a
   x    world  with    headings and    speeds  in dynamic robot navigation  the objective
is to navigate a robot from one location and heading to another location and heading  while
respecting the dynamics of the robot  the robot is not able to change direction and speed
instantaneously  so not all combinations of heading speed can be reached from a given
state  in addition to that  some states in this domain represent dead ends  for example 
a state where the robot is moving at full speed directly towards an obstacle will produce
no children  because the robot will crash no matter what control action is applied  the
objective is to minimize the total travel time  the actions do not all have the same cost 
we also introduce a new domain we call city navigation  designed to simulate navigation
using a system similar to american interstate highways or air transportation networks  in
this domain  there are cities scattered randomly on a    x    square  connected by a random
tour which guarantees it is possible to get from any city to any other city  each city is also
connected to its nc nearest neighbors  all links between cities cost the euclidean distance
     each city contains a collection of locations  randomly scattered throughout the city
 which is a  x  square   locations in a city are connected in a random tour  with each
place also connected to the nearest np places  links between places cost the true distance
multiplied by a random number between   and      within each city there is a special
nexus node that contains all connections in and out of this city  the goal is to navigate
from a randomly selected start location to a randomly selected end location  for example 
we might want to go from location   in city   to location    in city    each citys nexus
node is location    so to reach the goal in the example problem we must navigate from
location   to location   in city    then find a path from city   to city    then a path from
location   in city   to location    in city    an example instance of this type can be
seen in figure    the circles in the left part of the figure are locations  connected to other
locations  the nexus node  location    is also connected to the nexus nodes of neighboring
   

fieffective heuristics for suboptimal best first search

figure    a city navigation problem with np   nc      with    cities and    locations in
each city 

cities  the right part of the figure shows the entire world  with cities shrunk down to a
circle 
city navigation instances are classified by np and nc   we consider problems with varying
numbers of connections  but always having     cities and     places in each city  since each
location within a city has a global position  the heuristic is direct euclidean distance  in
this domain  solutions vary in length  and it is straightforward to manipulate the accuracy
of the heuristic  this domain bears some similarity to the ipc logistics domain in which
locations within cities are connected by roads  but special airport locations are used to
travel between cities 
    results
figures   and   show the number of expansions required by a   greedy best first search  and
weighted a  with weights of                       and     these plots allow us to compare
greedy best first search with weighted a  and a   and to determine whether increasing the
weight speeds up the search  or slows down the search 
looking at the plots in figure    it is easy to see that as we increase the weight the
number of expansions goes down  but in figure    the opposite is true  in each of these
domains  increasing the weight initially speeds up the search  as a  is relaxed into weighted
a   but as weighted a  transforms into greedy best first search  the number of nodes
required to solve the problem increases  in two of the domains  topspin with a turnstile of
size   and city navigation      the number of nodes expanded by greedy best first search is
higher than the number of nodes expanded by a   explaining this phenomenon is a central
goal of this paper 
   

fiwilt   ruml

      

      

      

      

     

    

 

 

 

a                      g

a                      g

dynamic robot

a                      g

topspin   

unit tile

 e   

      

total nodes expanded

   

total nodes expanded

total nodes expanded

total nodes expanded

total nodes expanded

total nodes expanded

   e   

   pancake problem

grid navigation

   disk hanoi

     

     

 

 

a                      g

   

 

a                      g

a               

      g

figure    domains where increasing the weight speeds up search  numbers denote weighted
a  run with a specific weight  and g denotes greedy best first search 

   characteristics of effective heuristics
we have established that increasing the weight in weighted a  does not always speed up
the search  and in some situations can actually slow down search  the fact that a  is
sometimes faster than greedy best first search and sometimes slower than greedy best first
search suggests that some heuristics work well for a  and poorly for greedy best first search 
and that some heuristics work well for greedy best first search but not for a   thus  the
question is precisely what is driving this difference  and what each algorithm  a  and greedy
best first search  needs out of the heuristic 
we first review the literature for suggestions about how to make a good heuristic for
a   with this in mind  we then apply the a  rules for constructing an effective heuristic
to greedy best first search  this leads us to observations on effective heuristics for greedy
best first search that are distinct from the common recommendations for building a good
heuristic for a  
    effective heuristics for a 
much of the literature about what constitutes a good heuristic centers on how well the
heuristic works for a   for finding optimal solutions using a   the first and most important
   

fieffective heuristics for suboptimal best first search

inverse tile

topspin   

total nodes expanded

total nodes expanded

      

     

 

     

    

 

a                      g

a                      g
city navigation    

city navigation    

    

total nodes expanded

total nodes expanded

    

    

 

    

    

 

a               

      g

a               

      g

figure    domains where increasing the weight slows down search  numbers denote
weighted a  with the specified weight  and g denotes greedy best first search 

requirement is that the heuristic be admissible  meaning for all nodes n  h  n   the true
cheapest path from n to a goal  is greater than or equal to h n   if the heuristic is not
admissible  a  degenerates into a  no star  which is not guaranteed to find the shortest
path 
it is generally believed that consistency is also important  due to the fact that inadmissible heuristics can lead to an exponential number of re expansions  martelli        
this situation  however  rarely arises in practice and felner et  al         argue that
inconsistency is generally not as much of a problem as is generally believed 
the most widespread rule for making a good heuristic for a  is  dominance is good  nilsson        pearl         a heuristic h  is said to dominate h  if n  g   h   n   h   n  
this makes sense  because due to admissibility  larger values are closer to h   furthermore
a  must expand every node n it encounters where f  n  is less than the cost of an optimal
solution  so large h often reduces expansions  dominance represents the current gold standard for comparing two heuristics  in practice  heuristics are often informally evaluated by
   

fiwilt   ruml

their average value or by their value at the initial state over a benchmark set  in either
case  the general idea remains the same  bigger heuristics are better 
if we ignore the effects of tie breaking as well as the effects of duplicate states  a  and
the last iteration of ida  expand the same number of nodes  this allows us to apply the
formula from korf  reid  and edelkamp         they predict that the number of nodes
ida  will expand at cost bound c is 
e n  c  p    

c
x

ni p  c  i 

i  

the function p  h  in the kre equation represents the equilibrium heuristic distribution 
which is the probability that a node chosen randomly and uniformly among all nodes at a
given depth of the brute force search tree has heuristic value less than or equal to h  korf
et al          this quantity tends to decrease as h gets larger  depending on how the nodes
in the space are distributed  the dominance relation also transfers to the kre equation 
meaning that if a heuristic h  dominates a different heuristic h    the kre equations predicts
that the expected expansions using h  will be less than or equal to the expected expansions
using h   
when considering pattern database  pdb  heuristics  korfs conjecture        can lend
m
insight into the performance of ida   which tells us that we can expect   log m 
t   n
with m being the amount of memory the pdb in question takes up  t is the amount of time
we expect an ida  search to consume  and n is a constant  korf         if we are willing
to apply results regarding ida  to a  this equation tells us that we should expect larger
pattern databases to provide faster search for a   to summarize  the prevailing wisdom
regarding heuristics is that bigger is better  both in terms of average heuristic value and
pattern database size 
    the behavior of greedy best first search
as we shall see  this advice regarding heuristics is all very helpful when considering only
a   what happens if we apply this same wisdom to greedy best first search  we answer
this question by taking a detailed look at the behavior of greedy best first search on three
of our benchmark problems  the towers of hanoi  the topspin puzzle  and the sliding tile
puzzle 
      towers of hanoi
the first domain we consider is the towers of hanoi  the most successful heuristic for
optimally solving   peg towers of hanoi problems is disjoint pattern databases  korf  
felner         disjoint pattern databases boost the heuristic value by providing information
about the disks on the top of the puzzle  for example  consider a    disk puzzle  split into
two disjoint pattern databases  eight disks in the bottom pattern database  and four disks
in the top pattern database  with a   the best results are achieved when using the full
disjoint pattern database  with greedy best first search  however  faster search results when
we do not use a disjoint pattern database  and instead only use the   disk pattern database 
the exact numbers are presented in the unit rows of table    all problems are randomly
generated towers of hanoi states  with the goal being to get all disks onto the first peg 
   

fieffective heuristics for suboptimal best first search

cost
unit
square
rev square

heuristic
    pdb
    pdb
    pdb
    pdb
    pdb
    pdb

a  exp
         
         
       
       
         
         

greedy exp
      
   
     
   
       
   

table    average number of nodes expanded to solve       disk towers of hanoi problems 
  

  

  

  

minimum h

minimum h

  

  

  

  

  

 
  

 
   

   
   
expansions

   

    

  

                                        
expansions

figure    the minimum h value on open as the search progresses  using different pattern
databases  single on left  two disjoint additive ones on the right  

the theory for a  corroborates the empirical evidence observed here  the disjoint pattern database dominates the single pattern database  so absent unusual effects from tiebreaking  it is no surprise that the disjoint pattern database results in faster a  search 
the reason for the different behaviour of a  and greedy best first search is simple  with
greedy best first search using a single pattern database  it is possible to follow the heuristic
directly to a goal  having the h value of the head of the open list monotonically decrease 
to see this  note that every combination of the bottom disks has an h value  and all possible
arrangements of the disks on top will also share that same h value  the disks on top can
always be moved around independently of where the bottom disks are  consequently  it is
always possible to arrange the top disks such that the next move of the bottom disks can
be done  while not disturbing any of the bottom disks  thus leaving h constant  eventually 
h decreases because more progress has been made putting the bottom disks of the problem
in order  this process repeats until h      at which point greedy best first search simply
considers possible configurations of the top disks until the goal has been found 
this phenomenon can be seen in the left pane of figure    where the minimum h value
of the open list monotonically decreases as the number of expansions the search has done
   

fiwilt   ruml

h     

h  
figure    two towers of hanoi states  one near a goal  top  and one far from a goal
 bottom  

increases  the heuristic created by the single pattern database creates an extremely effective
gradient for the greedy best first search algorithm to follow for two reasons  first  there are
no local minima at all  only the global minimum where the goal is  in this context  we define
a minimum as a region of the space m where n  m   every path from n to a goal node
has at least one node n with h n     h n   second  there are exactly     states associated
with each configuration of the bottom   disks  this means that every     expansions  h is
guaranteed to decrease  in practice  a state with a lower h tends to be found much faster 
in the right pane of figure    the heuristic is a disjoint pattern database  we can see
that the h value of the head of the open list fluctuates substantially when using a disjoint
pattern database  indicating that greedy best first searchs policy of follow small h is much
less successful  this is because those states with the bottom disks very near their goal that
are paired with a very poor arrangement of the disks on top are assigned large heuristic
values  which delays the expansion of these nodes  this is illustrated in figure    the top
state is significantly closer to a goal  despite having a higher h value than the bottom state 
if we ignore the top disks completely  the top state has h     compared to the bottom
states h      which correctly conveys the fact that the top state is significantly closer to
a goal  the disjoint pdb causes substantial confusion for greedy best first search  because
prior to making any progress with any of the   bottom disks  the greedy best first search
considers states where the top   disks are closer to their destination  if the bottom state is
expanded  it will produce children with lower heuristic values which will be explored before
ever considering the top state  which is the state that should be explored first  eventually 
all descendants of the bottom state with h    are explored  at which point the top state
is expanded  but this causes the h value of the head of the open list to go up and down 
to summarize  the disjoint pattern database makes a gradient that is more difficult
for greedy best first search to follow because nodes can have a small h for more than one
reason  being near the goal because the bottom pattern database is returning a small value 
or being not particularly near the goal  but having the top disks arranged on the target peg 
this suggests the following observation regarding heuristics for greedy best first search 
   

fieffective heuristics for suboptimal best first search

observation    all else being equal  greedy best first search tends to work well when it is
possible to reach the goal from every node via a path where h monotonically decreases along
the path 
while this may seem self evident  our example has illustrated how it conflicts with the
common wisdom in heuristic construction  it is also important to note that this observation
makes no comment about the relative magnitude of the heuristic  which for greedy best first
is completely irrelevant  all that matters is the relative ordering of the nodes when ordered
using the heuristic 
another way to view this phenomenon is in analogy to the sussman anomaly  sussman 
       the sussman anomaly occurs when one must undo a subgoal prior to being able
to reach the global goal  in the context of towers of hanoi problems  the goal is to get
all of the disks on the target peg  but solving the problem may involve doing and then
undoing some subgoals of putting the top disks on the target peg  the presence of the top
pattern database encourages greedy best first searches to privilege states where subgoals
which eventually have to be undone have been accomplished 
korf        discusses different kinds of subgoals  and how different kinds of heuristic
searches are able to leverage subgoals  greedy best first search uses the heuristic to create
subgoals  attempting to follow the h to a goal  for example  in a unit cost domain  the first
subgoal is to find a node with h   h root      if the heuristic follows observation    these
subgoals form a perfect serialization  and the subgoals can be achieved one after another 
as the heuristic deviates from observation    the subgoals induced by the heuristic cannot
be serialized 
another important factor is  of course  the number of distinct nodes at each heuristic
level one encounters prior to finding a better node  consider  for example  one of the worst
heuristics  h      technically  this heuristic follows observation   because all paths only
contain nodes with h      but the one plateau contains all nodes in the entire space  which is
obviously undesirable  hoffmann        discusses this general idea using the term maximal
bench exit distance  and once again  the idea is that in domains in which this quantity is
small  both greedy best first search and his enforced hill climbing method perform well 
because finding nodes with lower h is straightforward 
these effects can be exacerbated if the cost of the disks on the top is increased relative
to the cost of the disks on the bottom  if we define the cost of moving a disk as being
proportional to the disks size  we get the square cost metric  where the cost of moving disk
n is n    we could also imagine the tower being stacked in reverse  requiring that the larger
disks always be on top of the smaller disks  in which case we get the reverse square cost
function  in either case  we expect that the number of expansions that greedy best first
search will require will be lower when using only the bottom pattern database  and this is
indeed the effect we observe in table    however  if the top disks are heavier than the disks
on the bottom  greedy best first search suffers even more than when we considered the unit
cost problem  expanding an order of magnitude more nodes  this is because the pattern
database with information about the top disks is returning values that are substantially
larger than the bottom pattern database  due to the fact that the top pattern database
considers the most expensive operators  if the situation is reversed  however  and the top
pattern database uses only the lowest cost operators  the top pattern databases contribution
to h is a much smaller proportion of the total expansions  since greedy best first search
   

fiwilt   ruml

    

    

greedy search with disjoint pdb

greedy search with disjoint pdb

    

    

    
    

   

minimum h

minimum h

    

    

   

   
   

   

   
   
 
 

   
    

    
    
expansions

    

 
 

    

      

      

             
expansions

      

      

      

figure    the minimum h value on open during searches using disjoint pattern databases
with different cost functions  square on left  reverse square on right  

performs best when the top pattern database isnt even present  it naturally performs better
when the contribution of the top pattern database is smaller 
this phenomenon is vividly illustrated in the execution times in figure    in the left of
the figure  the disks in the top pattern database are much cheaper to move than the disks
in the bottom pattern database  and are therefore contributing a much smaller proportion
of the total value of h  in the right part of the figure  the disks in the top pattern database
are much more expensive to move than the disks in the bottom pattern database  so the
top pattern database makes a much larger contribution to h  causing substantially more
confusion 
hoffmann        notes that the success of the ff heuristic in many domains is attributable to the fact that the h  heuristic produces a heuristic with no local minima  a
heuristic with no local minima precisely matches our observation    because it will always
be possible to reach the goal via a path where h monotonically decreases 
      topspin
we considered topspin with    disks and a turnstile that flipped   disks using pattern
databases that contained          and   of the    total disks 
korfs conjecture predicts that the larger pattern databases will be more useful for
a   and should therefore be considered to be stronger heuristics  and indeed  as the pdb
becomes larger  the number of expansions done by a  dramatically decreases  this can be
seen in figure    each box plot  tukey        is labeled with either a  or g  for greedy
best first search   and a number  denoting the number of disks that the pdb tracks  each
box denotes the middle     of the data  so the top of the box is the upper quartile  the
bottom of the box is the bottom quartile  and the height of the box is the interquartile
range  the horizontal line in the middle of the box represents the median  the grey stripe
indicates the     confidence interval about the mean  the circles denote points that are
more than     times the interquartile range away from either the first quartile or the third
   

fieffective heuristics for suboptimal best first search

     topspin with different pdb s

expansions

     

     

a   g  a   g   a   g  a   g  

figure    topspin puzzle with different heuristics  a followed by a number denotes a
with that number of disks in the pdb heuristic  g followed by a number denotes
greedy best first search with that number of disks in the pdb heuristic 

quartile  and the whiskers represent the range of the non outlier data  as we move from left
to right  as the pdb heuristic tracks more disks  it gets substantially better for a   while
there are also reductions for greedy best first search in terms of expansions  the gains are
nowhere near as impressive as compared to a  
the reason that greedy best first search does not perform better when given a larger
heuristic is that  with the larger heuristic  states with h     may still be quite far from a
goal  for example  consider the topspin state represented as follows  where a denotes an
abstracted disk 
state                a a a a a a
the turnstile swaps the orientation of   disks  but there are configurations such that
putting the abstracted disks in order requires moving a disk that is not abstracted  such as 
state                             
for a topspin state  the abstraction process takes the largest n disks and converts
them to abstracted disks  and abstracted disks are all treated the same  so state   would be
abstracted into state    which means that it abstracts to the same state as the goal  making
its heuristic    if we wanted to expand state    we could do so and one of the children is
state    whose heuristic is still   
state                             
consider a different child  for example the child obtained by rotating the middle   disks 
   

fiwilt   ruml

state                             
which abstracts into 
state            a a     a a a a
the heuristic for state   is not    because state   abstracts into state    state   is an
abstract state that is different from state    the abstracted goal  so the heuristic of state
  is not   
if we abstract disks       we still have the same abstract state as before  so the heuristic
is still    moving a disk that is not abstracted will increase the heuristic  but moving only
abstracted disks will leave the heuristic at    unfortunately  transforming state   into a
goal cannot be done without moving at least one of the disks whose index is between   and
   because the turnstile is of size   
this means that the subgraph consisting of only nodes with h     in the topspin
problem is disconnected  thus  when greedy best first search encounters a state with h     
the state could be a h     state that is connected to the goal via only h     states  which
would be desirable  or the state could be a h     state that is connected to the goal via only
paths that contain at least one h      nodes  which would be undesirable  if this is the case 
greedy best first search will first expand all the h     nodes connected to the first h    
node  which by hypothesis is not connected to a goal node via paths only containing h    
nodes   and will then return to expanding nodes with h      looking to find a different
h     node 
the abstraction controls the number and size of h     regions  for example  if we
abstract   disks  there are two strongly connected regions of h     nodes  each containing
    nodes  if we instead abstract   disks  there are    strongly connected h     regions 
each with    nodes  for the heuristic that abstracts   disks  there is a     chance that
any given h     node is connected to the goal via only h     nodes  but once greedy
best first search has entered the correct h     region  finding the goal node is largely up
to chance  for the heuristic that abstracts   disks  the probability that any given h    
node is connected to the goal via only h     nodes is lower  once the correct h     region
is found  however  it is much easier to find the goal  because the region contains only   
nodes  as compared to     nodes  empirically  we can see that these two effects roughly
cancel one another out  because the total number of expansions done by greedy best first
search remains roughly constant no matter which heuristic is used  this brings us to our
next observation 
observation    all else being equal  nodes with h     should be connected to goal nodes
via paths that only contain h     nodes 
one can view this as an important specific case of observation    interestingly  some types
of heuristics  such as the delete relaxation heuristics used in domain independent planning 
obey this observation implicitly by never allowing non goal states to have h values of   
one obvious way to make a heuristic satisfy this recommendation is to change the
heuristic for all non goal states to be the same as the minimum cost operator from the
domain with cost of   if we do this  we can simply restate the recommendation substituting
 for    and we arrive at a similar result 
   

fieffective heuristics for suboptimal best first search

a
 

a
a
 

a
a
  

 
 
  

 
a

 
a
 

a
 
a

 
a
  

figure    different tile abstractions  a denotes a tile that is abstracted  
abstraction
outer l  figure   left 
checker  figure   right 
outer l missing  
outer l missing   and  
instance specific
gdrc generated
average   tile pdb
worst   tile pdb

greedy exp
   
      
     
      
     
   
      
       

a  exp
         
         
dnf
dnf
       
         
         
         

table    average number of expansions required by greedy best first search and a  to solve
     tile instances with different pattern databases  dnf denotes at least one
instance would require more than  gb to solve 

      sliding tiles
the sliding tile puzzle is one of the most commonly used benchmark domains in heuristic
search  as such  this domain is one of the best understood  pattern database heuristics
have been shown to be the strongest heuristics for this domain  and have been the strongest
heuristics for quite some time  korf   taylor        felner  korf  meshulam    holte 
       we use the    puzzle        as a case study because the smaller size of this puzzle
allows creating and testing hundreds of different pattern databases  the central problem
when constructing a pattern database for a sliding tile puzzle is selecting a good abstraction 
the abstraction that keeps only the outer l  shown in the left part of figure    is
extremely effective for greedy best first search  because once greedy best first search has
put all abstracted tiles in their proper places  all that remains is to find the goal  which is
easy to do using even a completely uninformed search on the remaining puzzle  as there are
only           states with h     and the h     states form a connected subgraph  this is
analogous to the heuristic directing the search algorithm to follow the process outlined by
parberry         in which large sliding tile puzzles are solved by first solving the outer l 
and then treating the remaining problem as a smaller sliding tile puzzle 
compare this to what happens when greedy best first search is run on a checkerboard
abstraction  as shown in the right part of figure    once greedy best first search has identified a node with h      there is a very high chance that the remaining abstracted tiles
are not configured properly  and that at least one of the non abstracted tiles will have to
be moved  this effect can be seen in table    where the average number of expansions required by a  is comparable with either abstraction  while the average number of expansions
required by greedy best first search is larger by two orders of magnitude 
   

fiwilt   ruml

the sheer size of the pdb is not as important for greedy best first search as it is for
a   in table    we can see that as we weaken the pattern database by removing the   and
  tiles  the number of expansions required increases by a factor of    for greedy best first
search  for a  using the pdb with the   tile missing    instances are unsolvable within  
gb of memory  approximately    million nodes in our java implementation   with both
the   and the   tile missing  a  is unable to solve    instances within the same limit  it is
worth noting that even without the   tile  the outer l abstraction is still more effective for
greedy best first search as compared to the checkerboard abstraction 
the underlying reason behind the inefficiency of greedy best first search using certain
pattern databases is the fact that the less useful pattern databases have nodes with h    
that are nowhere near the goal  this provides further evidence in favor of observation   
greedy best first search concentrates its efforts on finding and expanding nodes with a low h
value  and if some of those nodes are  in reality  not near a goal  this clearly causes problems
for the algorithm  because a  uses f   and g contributes to f   a  is able to eliminate some
of these states from consideration  not expand them  because the high g value helps to give
the node a high f value  which causes a  to relegate the node to the back of the expansion
queue 
the checkerboard pattern database also helps to make clear another problem facing
greedy best first search heuristics  once the algorithm discovers a node with h      if that
node is not connected to any goal via only h     nodes  the algorithm will eventually run
out of h     nodes to expand  and will begin expanding nodes with h      when expanding
h     nodes  greedy best first search will either find more h     nodes to examine for goals 
or it will eventually exhaust all of the h     nodes as well  and be forced to consider h    
nodes  a natural question to ask is how far the algorithm has to back off before it will be
able to find a goal  this leads us to our next observation 

observation    all else being equal  greedy best first search tends to work well when the
difference between the minimum h value of the nodes in a local minimum and the minimum
h that will allow the search to escape from the local minimum and reach a goal is low 

this phenomenon is clearly illustrated when considering instance specific pattern databases
 holte  grajkowskic    tanner         in an instance specific pattern database  the tiles
that start out closest to their goals are abstracted first  leaving the tiles that are furthest
away from their goals to be represented in the pattern database  this helps to maximize
the heuristic values of the states near the root  but due to consistency this can also have the
undesirable side effect of making states that are required to be included in a path to the goal
have high heuristic values as well  raising the heuristic value of the initial state is helpful for
a  search  as evidenced by the reduction in the number of expansions for a  using instancespecific abstractions of the same size  shown in table    unfortunately  this approach is
still not as powerful for greedy best first search as the simpler outer l abstraction  or even
the smaller variant missing the    this is because some instance specific pattern databases
use patterns that are difficult for greedy best first search to use effectively  similar to the
problems encountered when using the checkerboard abstraction 
   

fieffective heuristics for suboptimal best first search

domain

greedy
works

greedy
fails

towers of hanoi
grid
pancake
dynamic robot
unit tiles
topspin   
topspin   
inverse tiles
city nav    
city nav    

heuristic
  error
     
     
    
     
     
     
     
     
     
     

h n  h  n 
correlation
 pearson 
      
      
      
      
      
      
      
      
      
      

h n  h  n 
correlation
 spearman 
      
      
      
      
      
      
      
      
      
      

h n  h  n 
correlation
 kendall 
      
      
      
      
      
      
      
      
      
      

table    average   error and correlation between h n  and h  n 

   predicting effectiveness of greedy heuristics
in the previous section  we saw that common wisdom regarding effective heuristics for
optimal search did not carry over to suboptimal search  instead  our examples motivated
three general observations regarding what greedy best first search looks for in a heuristic 
while these qualitative observations are perhaps helpful heuristics for heuristic design  it is
also useful to have a simple  quantitative metric for evaluating and comparing heuristics 
we begin by considering two intuitively reasonable quantitative metrics  the percent
error in h  and the correlation between h and h   for each of these metrics  we show that
the metric cannot be used to predict whether or not greedy best first search will perform
worse than weighted a   then we consider a measure of search distance to go called d  
d  n  is the same as h if we change the graph by making all edges cost    we find that
the correlation between h and d can be used to predict when greedy best first search will
perform poorly 
    percent error in h n 
the first metric we consider is perhaps the most intuitive measure of heuristic performance 
the percent error in h  we define the percent error in the heuristic as h  n h n 
  since greedy
h  n 
best first search increases the importance of the heuristic  it is reasonable to conclude that
if the heuristic has a large amount of error  relying upon it heavily  as greedy best first
search does  is not going to lead to a fast search 
in table    we have the average percent error in the heuristic for each of the domains
considered  surprisingly  the average percentage error bears little relation to whether or
not greedy best search will be a poor choice  towers of hanoi  unit tiles  and topspin    
three domains where greedy best first search is effective  have as much or more heuristic
error than domains where greedy best first search works poorly  this leads us to conclude
that you cannot measure the average heuristic percent error and use this to predict whether
or not increasing the weight will speed up or slow down search 
   

fiwilt   ruml

to see intuitively why this makes sense  note that greedy best first search only really
requires that the nodes get put in h  n  order by the heuristic  the exact magnitude  and
therefore error  of the heuristic is unimportant  but magnitude has a huge effect on the
average percent error  this can be seen if we consider the heuristic h n    h r n    r  r 
for a very large or very tiny r  which will always guide greedy best first search directly to
an optimal goal  while exhibiting arbitrarily high average percent error in the heuristic as
r increases or decreases away from   
    h  h correlation
the next metric we consider is the correlation between h and h   while considering the
percent error in h as a metric  we noted that greedy best first search has run time linear
in the solution length of the optimal solution if the nodes are in h  n  order  one way to
quantify this observation is to measure the correlation between the two values  we will do
this in three different ways 
the most well known correlation coefficient is pearsons correlation coefficient r  which
measures how well the relationship between h n  and h  n  can be modeled using a linear
function  such a relationship would mean that weighting the heuristic appropriately can
reduce the error in the heuristic  which could reasonably be expected to lead to a faster
search  in addition  if the relationship between h n  and h  n  is a linear function  then
order will be preserved  putting nodes in order of h n  will also put the nodes in order of
h  n   which leads to an effective greedy best first search  for each domain  we calculated
pearsons correlation coefficient between h  n  and h n   and the results are in the second
column of table   
another reasonable way to measure the heuristic correlation is to use rank correlation 
rank correlation measures how well one permutation  or order  respects another permutation  or order   in the context of search  we can use this to ask how similar the order one
gets by putting nodes in h order is to the order one gets by putting nodes in h order  rank
correlation coefficients are useful because they are less sensitive to outliers  and are able to
detect relationships that are not linear 
spearmans rank correlation coefficient    is the best known rank correlation coefficient 
 is pearsons r between the ranked variables  this means that the smallest of n heuristic
values is mapped to    the largest of the n heuristic values is mapped to n   this is done
for both h and h   at which point we simply calculate persons r using the rankings  in
the context of greedy best first search  if spearmans rank correlation coefficient is high 
this means that the h n  and h  n  put nodes in very close to the same order  expanding
nodes in h  n  order leads to greedy best first search running in time linear in the solution
length  so it is reasonable to conclude that a strong spearmans rank correlation coefficient
between h  n  and h n  would lead to an effective greedy best first search  for each domain 
we calculate the spearmans rank correlation coefficient between h  n  and h n   and the
results are in the third column of table   
a more natural metric for measuring this relationship can be achieved by using kendalls
         kendalls  is another rank correlation coefficient  but it measures the amount
of concordance between the two rankings  concordance is having the rankings for two
elements agree  in the context of greedy best first search  a concordant pair is a pair of
   

fieffective heuristics for suboptimal best first search

nodes such that h n      h n    and h  n      h  n    or h n      h n    and h  n      h  n    
kendalls  is the proportion of pairwise comparisons that are concordant  if h puts nodes
in h order  all pairwise comparisons will be concordant  and kendalls  will be    if h puts
nodes in reverse h order  all comparisons will be discordant  and kendalls  will be     if
sorting nodes on h puts nodes in a random order  we expect that half of the comparisons
will be concordant and half of the comparisons will be discordant 
kendalls  can also be understood in the context of bubble sort  the kendall  distance
is the number of swaps that a bubble sort would do in order to change one list into the
other  in this case  it is the number of swaps that a bubble sort would do rearranging a list
of nodes sorted on h so the list is sorted on h   kendalls  is calculated by normalizing the
kendall  distance  which is done by dividing by n  n         
since  and  are both rank correlation coefficients  they are related  but we argue that
 is the more natural statistic  consider this question  given an open list containing n
nodes  how likely is it that the node with the smallest h will be at the front of the open
list  given that the nodes are ordered on h  we can use  to predict that the node will 
on average  be in the middle of the list if h and h are completely unrelated  and closer to
the front of the open list the stronger the   h  h   correlation is  the reason is that if we
assume that the nodes on the open list are a random selection of nodes   tells us how often
a random comparison is correct  we can use therefore  to predict how far back the node
with the minimum h is   has no such natural interpretation  making  the more natural
statistic  it is worth nothing that  and  are generally related to one another  in that one
can be used to predict the other  gibbons         this relationship means that in practice 
it is generally possible to use either metric 
returning to table    the results lead us to reject the correlation between h and h as
a metric for predicting how well greedy best first search will work  for all three correlation
coefficients  there are examples of domains where greedy best first search fails with high
h n  h  n  correlations  and examples of domains where greedy best first search works well
with poor h n  h  n  correlations  for example  in topspin     we have a kendalls  of
     but this is lower than the  for inverse tiles and both city navigation problems we
consider 
    h  d correlation
the strategy of greedy best first search is to discover a goal quickly by expanding nodes
with small h n  values  if nodes with small h n  are far away from a goal it is reasonable to
believe greedy best first search would perform poorly  we will denote by d  n  the count of
edges between a node n and the nearest goal  where distance is not measured by summing
the cost of the edges in the path  but rather by counting the edges in the path 
d  n  is equivalent to h  n  if we modify the graph so that all edges cost    looking at
the plot of h n  vs h  n  in the left half of figure     we can see that for city navigation
    there is a reasonable relationship between h n  and h  n   in that the nodes with low
h n  tend to have small h  n  values  we denote the distance to the nearest goal in terms
   malte helmert has noted  personal communication  that kendalls    as described  is not an ideal metric
to use for sequences that contain ties  for integer valued heuristics  especially  ties may be very common 
one way to account for ties in the rankings to use kendalls   b statistic  kendall   gibbons       
instead of   also known as   a   kendalls   b accounts for ties in the rankings 

   

fiwilt   ruml

h vs h  in city navigation    

h vs d  in city navigation    
  

   

d 

h 

  

  

 
 
 

  

   

h

  

  

h

figure     plot of h n  vs h  n   and h n  vs d  n  for city navigation    
domain

greedy
works

greedy
fails

towers of hanoi
grid
pancake
dynamic robot
unit tiles
topspin   
topspin   
inverse tiles
city nav    
city nav    

h n  d  n 
correlation
 pearson 
      
      
      
      
      
      
      
      
      
      

h n  d  n 
correlation
 spearman 
      
      
      
      
      
      
      
      
       
      

h n  d  n 
correlation
 kendall 
      
      
      
      
      
      
      
      
       
      

table    correlation between h n  and d  n 

of the number of edges in the state space graph as d  n   the right half of figure    shows
a plot of h n  vs d  n   we can clearly see that in the city navigation     domain  there
is almost no relationship between h n  and d  n   meaning that nodes that receive a small
h n  value can be found any distance away from a goal  which could explain why greedy
best first search works so poorly for this domain  despite the fact that h n  and h  n  are
so closely related 
if nodes with small h n  values are also likely to have small d  n  values  and these
nodes are therefore close to a goal  in terms of expansions away  expanding nodes with
small h n  values will quickly lead to a goal  the converse is also reasonable  if the nodes
with small h n  value have a uniform distribution of d  n  values  and thus many of these
nodes are far away from a goal in terms of expansions away   expanding these nodes will
not quickly lead to a goal 
   

fieffective heuristics for suboptimal best first search

   
   

log expansions 

   
   
   
   
   
   
   
   
   

   

   

   

   
gdrc

   

   

   

   

figure     average log of expansions done by greedy best first search with different heuristics  plotted according to their gdrc 

for each domain  we quantify this concept by calculating pearsons correlation coefficient  spearmans rank correlation coefficient  and kendalls  between d  n  and h n  
looking at table    we can see that  using both kendalls  and pearsons r  we are finally
able to separate the domains on which greedy best first search performs well from the domains on which greedy best first search performs poorly  for kendalls    we can draw a
line at approximately     that can be used to separate the domains where greedy best first
search works well and the domains where greedy best first search works poorly  likewise 
for pearsons r  we can draw a line at approximately      we will call this type of metric
the goal distance rank correlation  gdrc  and  unless otherwise noted  compute it using
kendalls   
the correlation between d  n  and h n  connects to our three observations  although
the connection is not a mathematical necessity  as counterexamples can be constructed  
note that a heuristic that obeys observation   will produce paths where h monotonically
decreases to a goal  consider the nodes along a path to a goal  by hypothesis  h will
monotonically decrease along this path  now  consider one of the nodes at the goal end of
the path  since h monotonically decreases along the path  the nodes at the goal end of the
path have a low h  and because they are near the end of the path  they also have a low d
value  while little else can be said about the nodes in general  this restriction improves the
heuristics gdrc compared to a situation in which the nodes with low d are allowed to
have high h values  a similar argument can be used to show how following observation  
helps to produce a heuristic with a high gdrc 
observation   discusses nodes in a local minimum  and the difference in h between nodes
in the local minimum  and nodes that are on the edge of the local minimum  if we assume
that in order to escape a local minimum one must go through one of the nodes on the edge of
   

fiwilt   ruml

the local minimum  then we know that the nodes in the local minimum must have a higher
d than the nodes on the edge of the local minimum  but we also know that their h is lower
 because the node is in the local minimum   this means that the h ranking incorrectly
orders all nodes in the local minimum as compared to the nodes on the edge of the local
minimum  a clear problem for producing a high gdrc  if the nodes in the local minimum
have a low d because they can get to the goal through a very high h node  the relationship
between this observation and gdrc is weaker  consider personal transportation as an
example  an action such as  call a taxi might result in reaching states very near the goal
in one step  but at very high cost  if the heuristic recognizes the cost of this action  the node
will correctly have a high h  but be very close to the goal as measured with d because of the
call a taxi path  while such a situation clearly causes problems for domains attempting to
follow observation    we believe that domains with this kind of attribute are  in practice 
quite uncommon  for example  none of our example domains exhibit this trait 
    comparing heuristics
because it is a quantitative metric  gdrc can be used to compare different heuristics for
the same domain  to test its effectiveness  we ran experiments on the towers of hanoi
problem using    different disjoint and non disjoint pattern databases  we considered
pattern databases with between   and   disks  as well as a selection of pairings of the pdbs
where the total number of disks is less than or equal to     for each pattern database  we
calculated the gdrc for the heuristic produced by the pdb  in figure    we plot  for each
pdb  the gdrc of the pdb on the x axis and the average of the log of the number of
expansions required by greedy best first search to solve    random    disk towers of hanoi
problems on the y axis  as we can see from the figure  when the gdrc is below roughly
     greedy best first search performs very poorly  but as the gdrc increases  the average
number of expansions done by greedy best first search decreases  this suggests that it is
possible to use gdrc to directly compare heuristics against one another 
we can see similar behavior in a different domain in the left part of figure     each
dot represents one of the     possible disjoint     pattern databases  one   tile pdb and
one   tile pdb that are disjoint  for the      sliding tile puzzle with inverse costs  on the
y axis is the log of the average expansions required to solve     random instances  on the
x axis is the gdrc  since we are using a non unit problem  h and d are not the same 
so we can also calculate the correlation between h and h   in the right part of figure    
this correlation is on the x axis 
as we can see  gdrc and the rank correlation between h and h can both yield useful
information about how well greedy best first search is likely to work 
for the domains we have tested  the correlation between h and d neatly predicts when
greedy best first search performs worse than weighted a   or a    it is not perfect  however 
if we consider the heuristic h n    h  n   any measure of the correlation between h n  and
h  n  will be perfect  but the relationship between h n  and d  n  for such a heuristic could
be arbitrarily poor  as the heuristic approaches truth  the h n  h  n  correlations will
approach    which allows weighted a  to scale gracefully  as greedy best first search will
have linear run time  no matter what the correlation between h n  and d  n  is  in this
situation  looking solely to the correlation between h n  and d  n  to determine whether
   

fi   

   

   

   

   

   
log   expansions 

log   expansions 

effective heuristics for suboptimal best first search

   
   

   
   

   

   

   

   

   
    

    

    

gdrc

    

    

   
    

    

    

    

    
    
    
    
gdrc  with h  instead of d  

    

    

figure     average log of expansions done by greedy best first search with each of the
possible         disjoint pdb heuristics  plotted against gdrc  left  and the
correlation between h n  and h  n 
domain

city nav    

heuristic h n  h  n 
  error correlation
 pearson 
     
      

h n  h  n 
correlation
 spearman 
      

h n  d  n 
correlation
 pearson 
      

h n  d  n 
correlation
 spearman 
      

table    average   error  correlation between h n  and h  n   and correlation between
h n  and d  n  in city nav    

or not greedy best first search will be faster than weighted a  may produce an incorrect
answer 
this can be seen in the city navigation     domain  city navigation     is similar
to the other city navigation problems we consider  except that the cities and places are
better connected  allowing more direct routes to be taken  since the routes are more direct 
and thus shorter  the heuristic is more accurate  table   shows the various correlations
and percent error in h n  for city navigation      figure    shows that as we increase the
weight  despite the very weak correlation between h n  and d  n   there is no catastrophe 
greedy best first search expands roughly the same number of nodes as weighted a  with
the best weight for speed  this occurs because of the extreme strength of the heuristic 
which correlates to h  n  at      an extremely strong correlation 
the next question is which correlation matters more  h  n  or d  n   clearly  a perfect
correlation between h  n  and h n  or d  n  and h n  will lead to a fast greedy best first
search  which leads us to the conclusion that in order for greedy best first search to be
effective  nodes with small h n  that get expanded are required to have at least one virtue 
they should either be close to the goal measured in terms of remaining search distance
 small d  n   or close to the goal measured in terms of remaining cost  small h  n    we
   

fiwilt   ruml

city navigation    

total nodes expanded

    

    

    

 

a               

      g

figure     expansions done by a   weighted a   and greedy best first search on city
navigation    

have seen empirically that as the two correlations break down  the d  n  correlation allows
greedy best first search to survive longer  in tested domains where the d  n  h n  is above
     greedy best first search does well  whereas we have seen domains where the h  n h n  correlation is as high as      or      depending on which correlation metric is being
used  where greedy best first search performs poorly 
the importance of the correlation between h n  and d  n  reflects the importance of
node ordering for greedy best first search  in optimal search  the search cannot terminate
when a solution is found  but rather when the solution is known to be optimal because
all other paths have been pruned  the larger the heuristic values  the sooner nodes can
be pruned  this means that in optimal search  heuristic size is of paramount importance 
bigger is better  with greedy best first search  the heuristic is used to guide the search to a
solution  so relative magnitude of the heuristic  or the error in the heuristic  has no bearing
on the performance of the search  as we saw when we considered the percent error in h  it
is common for researchers to say that a s heuristic guides the search  but our discussion
reveals why this language should be reserved for suboptimal search 
some heuristics are able to satisfy both the needs of a  and greedy best first search
simultaneously  for example  the dynamic robot navigation heuristic works extremely well
for both a  and greedy best first search  because it is both big  and therefore good for a  
and good at differentiating nodes that are near the goal from those far away from the goal 
helping greedy best first search 

   building a heuristic by searching on gdrc
as shown by haslum  botea  helmert  bonet  and koenig         given a metric for assessing the quality of a heuristic  we can use that metric to automatically construct effective
abstraction based heuristics simply by searching the space of abstractions  in many domains  a heuristic can be constructed by initially abstracting everything  and slowly refining
   

fieffective heuristics for suboptimal best first search

the abstraction to construct a heuristic  while haslum et al         were concerned with
optimal search and hence use pruning power to evaluate heuristics  our focus on greedy bestfirst search suggests that gdrc might serve as a useful metric  for example  in the topspin
problem  we begin with a heuristic that abstracts all disks  we then consider all pdbs that
can be devised by abstracting everything except one disk  and measure the gdrc of each
pattern database  the gdrc can be effectively estimated by doing a breadth first search
backwards from the goal  we used        nodes for a    disk problem  to establish d values
for nodes  and the h value can be looked up in the pattern database  we then sample     of
the nodes generated in this way  and used the sample to calculate an estimate of kendalls
   while we elected to sample      of the nodes  a sample of any size can be taken provided
the confidence interval is sufficiently small to tell which  is better  last  we take the pdb
with the highest value as the incumbent pdb  this process repeats until either all pdbs
have a worse gdrc than the previous best pdb  or until the pdb has reached the desired
size  the reason we allow the algorithm to possibly terminate early is to cover the case of
gdrc decreasing with larger pdbs  if increasing the size of the pdb decreases gdrc 
it is likely that further increasing the size of the pdb will degrade the gdrc even more 
so we elect to terminate  the full algorithm is detailed in algorithm    while this simple
hill climbing search appears effective  a more sophisticated search strategy could certainly
be employed instead 
    topspin
algorithm   hill climbing pdb builder
   alltokens    tokens in problem that can be abstracted 
   remainingtokens   allt okens
   bestpdb   build pdb by abstracting allt okens
   besttau    
   function trypdb tokens 
  
pdb   build pdb by abstracting allt okens   tokens
  
allnodes   nodes discovered by breadth first search backwards from the goal state s 
  
sample   randomly sample     of the nodes from allnodes
  
return calctau sample  pdb 
    while bestp db size   max allowed size do
   
localbestpdb  localbesttau  localbesttoken    n one  bestt au  n one 
   
for all currenttoken  remainingt okens do
   
currenttau  currentpdb   trypdb ref inedt okens   token  
   
if currentt au   localbestt au then
   
set local best variables to current
   
if localbestp db    none then
   
set best variables to local best variables
   
remainingtokens   remainingt okens   localbestt okens
   
else
   
break
    return bestpdb
   

fiwilt   ruml

pdb
contiguous
big operators
random

greedy exp
      
      
        

a  exp
         
      
         

avg  value
     
     
     

table    expansions to solve topspin problem with the stripe cost function using different
pdbs

when used to generate unit cost topspin pattern databases  hill climbing on gdrc
always produced pdbs where the abstracted disks were all connected to one another  and
the refined disks were also all connected to one another  this prevents the abstraction from
creating regions where h      but where the goal is nowhere near the h     nodes  per
observation   
with unit cost topspin problems  abstractions where all of the disks are connected to
one another work well for both greedy best first search and a   if we change the cost
function such that moving an even disk costs   and moving an odd disk costs     we get the
stripe cost function  so called because the costs are striped across the problem  the most
effective pdbs for a  are those that keep as many odd disks as possible  because moving
the odd disks is much more expensive than moving an even disk  if we use such a big
operator pattern database for greedy best first search  the algorithm will align the high
cost odd disks  but will have great difficulty escaping from the resulting local minimum  if
we use hill climbing on gdrc to build the heuristic  we end up with a contiguous heuristic
that keeps the abstracted and the refined disks connected to one another  table   provides
results of how the various pattern databases did solving a suite of instances  we can see the
importance of creating a good pattern database when we consider the random row in the
table  which contains the average number of expansions from    different randomly selected
  disk pattern databases 
    towers of hanoi
we can already infer from figure    that  if we greedily select the pdb with the best 
from a collection of pdbs  we would select the best one  but it is certainly also possible
to use a hill climbing search to incrementally construct a pdb  when creating a pdb
heuristic for the towers of hanoi  one maps the full size problem onto an abstracted version
of the problem by removing some of the disks in the larger problem  and re indexing the
remaining disks so they map to the disks in the smaller problem  with this technique  a
critical component in terms of performance is which disks are abstracted 
we define a mapping as a selection of disks to abstract  in our example  we once again
consider a    disk problem using an   disk pdb  so we must select   of the    total disks
to abstract  in figure    the   glyphs each represent a randomly selected abstraction 
and the heuristic it produced  as we can see  some abstractions produced extremely poor
quality heuristics as measured by gdrc and by the average number of expansions done
by greedy best first search solving problems using that heuristic  other heuristics fared
significantly better both in terms of gdrc and average expansions by greedy best first
   

fieffective heuristics for suboptimal best first search

analysis
of heuristics generated by different hanoi pdb mappings
 
 

log expansions

 
 
 
 
 
   

randomly selected mappings
best mapping
hill climbing mappings
   

   

   
gdrc

   

   

   

figure     expansions using different towers of hanoi pdb abstractions 
search  if we examine the plot in figure    we can see that there are several clusters of
heuristics  the heuristics with a gdrc of about   are clustered together  all requiring
greedy best first search to expand between     and     nodes  these heuristics are the
worst heuristics  because they largest disk is abstracted  the next cluster of heuristics have
a gdrc of about     to     and require greedy best first search to expand between       and
    nodes  these are the heuristics where the largest disk is not abstracted  but the second
largest disk is abstracted  these abstractions also produce very poor quality heuristics 
but the heuristics represent a significant improvement over the heuristics where the largest
disk is abstracted  each color in figure    represents a mapping with a different largest
abstracted disk  with the blue x glyph representing the best pattern database where only
the smallest disks are abstracted  as we can see in this plot  there is a definite overall trend
where mappings that product heuristics with a higher gdrc tend to fare better overall in
terms of average total expansions used by greedy best first search 
the hill climbing algorithm selected the heuristic that contained disks                  
   and    skipping the   disk   an example of how the hill climbing algorithm selected
this heuristic can be seen in the green circles and line in figure    when starting from
an abstraction that abstracted the largest disk  the hill climbing algorithm climbed a hill
leading to a reasonable  albeit not the most effective  heuristic  despite failing to find the
optimal heuristic  the selected heuristic is quite reasonable nonetheless  falling between the
  th and the   th percentile overall  a significant improvement for an automated approach 
    city navigation
in addition to building pattern database heuristics using hill climbing on gdrc  it is also
possible to build a portal style heuristic by hill climbing on gdrc  using the city navigation
domain  we defined a portal heuristic  goldenberg  felner  sturtevant    schaeffer       
by selecting a number of nodes to be portal nodes  we used the same number of nodes as
   

fiwilt   ruml

heuristic
random portals
nexus portals
hill climbed portals

average gdrc
    
    
    

average greedy best first search expansions
    
   
    

table    expansions and gdrc using different ways to select portal nodes

cities        and calculating the true distance from every node to the closest portal node 
the heuristic for two nodes is the true distance between the portals associated with each
node minus the distance of each node to its own portal  in the event that this quantity is
negative    is used  this heuristic is highly accurate across long distances because it uses
the true distance between the portals  but it is obviously less accurate when comparing
two nodes that share the same portal  when constructing portal heuristics  the critical
difference between an effective portal heuristic and a poor quality portal heuristic is the
selection of which nodes are portals  we allowed our algorithm to automate this process
by hill climbing on gdrc  the algorithm is initialized with a random array of nodes as
the portals  at each step  the algorithm iterates through the indexes in its array of portals 
considering moving the location that is currently serving as a portal to a different location 
in our implementation  we considered two times the number of cities  or     different
random places  we then assessed the grdc of the new heuristic using a sample of        
randomly selected pairs of places  if moving the city to a new location improved gdrc  we
kept the portal array with the new place  otherwise  we discarded the change as its gdrc
is inferior to that of the incumbent  when we reach the end of the array  we restart at
the beginning  if we reach a point where we are at the same position in the array  and all
other aspects of the array remain unchanged since the last time we modified that index  the
algorithm terminates  returning the array of portals for use in a heuristic 
results from this experiment are shown in table    the average gdrc is the gdrc
that one obtains by selecting         random pairs of start and end nodes and calculating
gdrc using those nodes  the average greedy best first search expansions is the average
number of expansions needed to solve a city navigation problem with a random start and
goal 
we considered three different methods for selecting portal nodes  the first was to
completely randomize the selection of portal nodes  which unsurprisingly resulted in the
lowest grdc and the highest number of expansions  the most successful method for
selecting portal nodes was to identify the nexus nodes  and use those nodes as the portals 
unsurprisingly  this method led to the highest gdrc  and the fewest number of expansions 
this result further demonstrates the usefulness of gdrc in identifying a quality heuristic for
greedy best first search  last  our automatic algorithm for finding portal nodes performed
significantly better than random  while still trailing the hand selected portals  we believe
that a better search strategy may be able to better capture the potential performance gain
offered by high gdrc heuristics 
   

fieffective heuristics for suboptimal best first search

    sliding tile puzzle
we can also compare the gdrc generated pdbs to instance specific pdbs for the sliding
tile puzzle  holte et al          on this domain  in order to get an accurate estimate
of    we had to increase the number of nodes expanded going backwards from        to
           following the hill climbing procedure  the algorithm selected a pattern database
that tracked the                and    tiles  the results of using this pdb are shown in table
   while this abstraction is not as strong as the outer l abstraction  it is the fourth best
pdb for minimizing the average number of expansions done by greedy best first search
out of the     possible   tile pattern databases  the automatically constructed pdb is
two orders of magnitude faster than the number of expansions one would expect to do
using an average   tile pdb  and three orders of magnitude faster than the worst   tile
pdb for greedy best first search  the gdrc generated pdb works substantially better for
greedy best first search then the state of the art instance specific pdbs  requiring about one
twentieth of the expansions  one additional advantage that the gdrc generated pdb has
over instance specific pdbs is the fact that gdrc produces a single pdb  unlike instance
specific pdbs  which produce a new pdb for every problem 
in summary  these results show that gdrc is useful for predicting the relative quality
of heuristics for greedy best first search  they also showed that it is possible to leverage this
quantitative metric to automatically construct a heuristic for greedy best first search  and
that the automatically created heuristics are extraordinarily effective for greedy best first
search 

   related work
as a metric  gdrc predicts that heuristics that have a high rank correlation with d will
work well  in general  the objective of h is to approximate h   not d   so one alternative way
to find a quality heuristic is to leverage this fact and try to construct a heuristic directly
that mimics d   generally referred to as d  indeed  this approach is generally quite successful
 as opposed to relying exclusively on h   handily outperforming h in many situations  wilt
  ruml        
gaschnig        describes how to predict the worst case number of nodes expanded
by a   and also discusses how weighting the heuristic can affect the worst case final node
expansion count  his predictions  however  have two limitations  first  the predictions
assume the search space is a tree  and not a graph  as is the case for many applications of
heuristic search  in addition to that  the worst case predictions only depend on the amount
of error present in the heuristic  where error is measured as relative deviation from h  n  
for a   this criterion makes a certain amount of sense  but for greedy best first search 
we have seen that relative deviation from h  n  cannot be used to predict when greedy
best first search will perform poorly  gaschnig points out that increasing the weight ad
infinitium may decrease performance  which is precisely the phenomenon we documented
in section   
chenoweth and davis        show that if the heuristic is rapidly growing with logarithmic cluster  a greedy best first search can be done in polynomial time  a heuristic is
rapidly growing with logarithmic cluster if  for every node n  h n  is within a logarithmic
factor of a monotonic function f of h  n   and f grows at least as fast as the function
   

fiwilt   ruml

g x    x  we are not aware of any heuristics that have been proven to be rapidly growing
with logarithmic cluster 
a number of works consider the question of predicting search algorithm performance
 korf et al         pearl        helmert   roger         although the subject attracting
by far the most attention is determining how many nodes will be expanded by an optimal
search algorithm  as we saw in section    the behavior of optional search does not in general
predict the behavior of gbrs  lelis  zilles  and holte        did an empirical analysis of
suboptimal search algorithms  predicting the number of nodes that would be expanded by
weighted ida   but it is not clear if those methods can predict greedy best first search
behavior  and thus tell us if increasing the weight too far can be detrimental 
korf        provides an early discussion of how increasing the weight may actually be
bad  showing that when recursive best first search or iterative deepening a  is used with a
weight that is too large  expansions actually increase  this paper is also an early example
of exploring how the weight interacts with the expansion count  something central to our
work 
hoffmann        discusses why the ff heuristic  hoffmann   nebel        is an effective way to solve many planning benchmarks when used in conjunction with enforced
hill climbing  the paper shows that in many benchmark problems  the heuristic has small
bounded size plateaus  implying that the breadth first search part of the enforced hill climbing algorithm is bounded  which means that those problems can be solved quickly  sometimes in linear time  although enforced hill climbing is a kind of greedy best first search 
its behaviour is very different from greedy best first search when a promising path turns
into a local minimum  greedy best first search considers nodes from all over the search
space  possibly allowing very disparate nodes to compete with one another for expansion 
enforced hill climbing limits consideration to nodes that are near the local minimum  with
nearness measured in edge count   which means that the algorithm only cares about how
the heuristic performs in a small local region of the space  hoffmann        extends this
concept  describing a process for automatically proving that a domain will have small local
minima 
xu  fern  and yoon        discuss constructing heuristics for a suboptimal heuristic
search  but the algorithm they consider is a beam search  beam searches inadmissibly
prune nodes to save space and time  so their function is ultimately being used not to rank
nodes  but to make a decision as to whether or not to keep any one node  the function
that xu et al  create can be used to rank nodes  but the input function requires a variety of
features of the state to function  and is created by using training data from trial search runs 
our approach of creating a heuristic by hill climbing on gdrc does not require training
instances  nor does it require any information about the states themselves  hill climbing
on gdrc does  however  have the limitation that the automatic generation of heuristics
only works when an appropriate search space can be defined  as with abstraction based
heuristics 

   conclusion
suboptimal heuristic searches rely heavily on the heuristic node evaluation function  we
first showed that greedy best first search can sometimes perform worse than a   and that
   

fieffective heuristics for suboptimal best first search

although in many domains there is a general trend where a larger weight on the heuristics
in weighted a  leads to a faster search  there are also domains where a larger weight leads
to a slower search  it has long been understood that greedy best first search has no bounds
on performance  and given a poor heuristic  greedy best first search could very well expand
the entire state space  or never terminate if the state space is infinite  our work shows
that poor performance is not just a theoretical curiosity  but that this behavior can occur
in practice 
we then considered characteristics of effective heuristics for greedy best first search  we
showed several examples in which the conventional guidelines for building heuristics for a 
can actually harm the performance of greedy best first search  we used this experience
to develop alternative observations and desiderata for heuristics for use with greedy bestfirst search  the first is that from every node  there should be a path to a goal that only
decreases in h  the second  an important special case of the first  is that nodes with h    
should be connected to a goal via nodes with h      the third observation is that nodes
that require including high h nodes in the solution should themselves have as high an h
value as possible 
we then showed that the domains where greedy best first search is effective share a
common trait of the heuristic function  the true distance from a node to a goal  defined
as d  n   correlates well with h n   this information is important for anyone running
suboptimal search in the interest of speed  because it allows them to identify whether or
not the assumption that weighting speeds up search is true or not  critical knowledge for
deciding which algorithm to use 
finally  we showed that goal distance rank correlation  gdrc  can be used to compare
different heuristics for greedy best first search  and demonstrated how it can be used to
automatically construct effective abstraction heuristics for greedy best first search 
recent work has shown that search algorithms explicitly designed for the suboptimal
setting can outperform methods like weighted a   which is a simple unprincipled derivative
of an optimal search  thayer   ruml        thayer  benton    helmert        stern 
puzis    felner         our results indicate that the same holds true for heuristic functions
as well  suboptimal search deserves its own specialized methods  given the importance of
suboptimal methods in solving large problems quickly  we hope that this investigation spurs
further analysis of suboptimal search algorithms and the heuristic functions they rely on 

   acknowledgments
we gratefully acknowledge support from nsf  award           preliminary expositions of
these results were published by wilt and ruml              

references
burns  e  a   hatem  m   leighton  m  j     ruml  w          implementing fast heuristic
search code  in proceedings of the fifth symposium on combinatorial search 
chenoweth  s  v     davis  h  w          high performance a  search using rapidly
growing heuristics  in proceedings of the twelfth international joint conference on
articial intelligence  pp         
   

fiwilt   ruml

doran  j  e     michie  d          experiments with the graph traverser program  in
proceedings of the royal society of london  series a  mathematical and physical
sciences  pp         
felner  a   korf  r  e   meshulam  r     holte  r  c          compressed pattern databases 
journal of artificial intelligence research  jair              
felner  a   zahavi  u   holte  r   schaeffer  j   sturtevant  n  r     zhang  z         
inconsistent heuristics in theory and practice  artificial intelligence                  
     
gaschnig  j          exactly how good are heuristics   toward a realistic predictive theory
of best first search  in proceedings of the fifth international joint conference on
articial intelligence  pp         
gibbons  j  d          nonparametric statistical inference  marcel decker  inc 
goldenberg  m   felner  a   sturtevant  n     schaeffer  j          portal based truedistance heuristics for path finding  in proceedings of the third symposium on combinatorial search 
hart  p  e   nilsson  n  j     raphael  b          a formal basis for the heuristic determination of minimum cost paths  ieee transactions on systems science and cybernetics 
ssc               
haslum  p   botea  a   helmert  m   bonet  b     koenig  s          domain independent
construction of pattern database heuristics for cost optimal planning  in proceedings
of aaai     pp           
helmert  m          the fast downward planning system  journal of artificial intelligence
research             
helmert  m          landmark heuristics for the pancake problem  in proceedings of the
third symposium on combinatorial search 
helmert  m     roger  g          how good is almost perfect   in proceedings of the
twenty third aaai conference on artificial intelligence  aaai        pp         
hoffmann  j          where ignoring delete lists works  local search topology in planning
benchmarks  journal of artifial intelligence research             
hoffmann  j          analyzing search topology without running any search  on the connection between causal graphs and h    journal of artificial intelligence research     
       
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research             
holte  r   grajkowskic  j     tanner  b          hierachical heuristic search revisitied  in
symposium on abstracton reformulation and approximation  pp         
kendall  m  g          a new measure of rank correlation  biometrika                 
kendall  m     gibbons  j  d          rank correlation methods  fifth edition   edward
arnold 
   

fieffective heuristics for suboptimal best first search

korf  r     felner  a          disjoint pattern database heuristics  artificial intelligence 
         
korf  r  e          planning as search  a quantitative approach  artificial intelligence 
             
korf  r  e          linear space best first search  artificial intelligence           
korf  r  e          finding optimal solutions to rubiks cube using pattern databases  in
proceedings of the fourteenth national conference on artificial intelligence  aaai   
pp          aaai press 
korf  r  e          analyzing the performance of pattern database heuristics  in proceedings
of the   nd national conference on artificial intelligence  aaai    pp           
aaai press 
korf  r  e   reid  m     edelkamp  s          time complexity of iterative deepening a  
artificial intelligence              
korf  r  e     taylor  l  a          finding optimal solutions to the twenty four puzzle 
in aaai  vol     pp           
lelis  l   zilles  s     holte  r  c          improved prediction of ida s performance via
epsilon truncation  in proceedings of the fourth symposium on combinatorial search 
likhachev  m   gordon  g     thrun  s          ara   anytime a  with provable bounds
on sub optimality  in proceedings of the seventeenth annual conference on neural
information processing systems 
likhachev  m     ferguson  d          planning long dynamically feasible maneuvers for
autonomous vehicles  international journal robotic research                 
martelli  a          on the complexity of admissible search algorithms  artificial intelligence             
nilsson  n  j          principles of artificial intelligence  tioga publishing co 
parberry  i          a real time algorithm for the  n      puzzle  information processing
letters               
pearl  j          heuristics  intelligent search strategies for computer problem solving 
addison wesley 
pohl  i          heuristic search viewed as path finding in a graph  artificial intelligence 
          
richter  s   thayer  j  t     ruml  w          the joy of forgetting  faster anytime search
via restarting  in proceedings of the twentieth international conference on automated
planning and scheduling 
richter  s     westphal  m          the lama planner  guiding cost based anytime
planning with landmarks  journal of artifial intelligence research             
stern  r  t   puzis  r     felner  a          potential search  a bounded cost search
algorithm  in proceedings of the   st international conference on automated planning
and scheduling  icaps 
   

fiwilt   ruml

sussman  g  j          a computer model of skill acquisition  new york  new american
elsevier 
thayer  j  t   benton  j     helmert  m          better parameter free anytime search by
minimizing time between solutions  in proceedings of the fifth annual symposium on
combinatorial search  socs      
thayer  j  t     ruml  w          bounded suboptimal search  a direct approach using inadmissible estimates  in proceedings of the twenty sixth international joint
conference on articial intelligence  ijcai      pp         
tukey  j  w          exploratory data analysis  addison wesley  reading  ma 
van den berg  j   shah  r   huang  a     goldberg  k  y          anytime nonparametric
a   in proceedings of the twenty fifth national conference on articial intelligence 
wilt  c     ruml  w          when does weighted a  fail   in proceedings of the fifth
symposium on combinatorial search 
wilt  c     ruml  w          speedy versus greedy search  in proceedings of the seventh
symposium on combinatorial search 
wilt  c     ruml  w          building a heuristic for greedy search  in proceedings of the
eighth symposium on combinatorial search 
xu  y   fern  a     yoon  s          learning linear ranking functions for beam search with
application to planning  the journal of machine learning research               

   

fi