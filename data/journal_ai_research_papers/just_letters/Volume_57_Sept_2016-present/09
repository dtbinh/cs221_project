journal artificial intelligence research                  

submitted        published      

embarrassingly parallel search constraint programming
arnaud malapert
jean charles regin
mohamed rezgui

arnaud malapert unice fr
jean charles regin unice fr
rezgui i s unice fr

universite cote dazur  cnrs  i s  france

abstract
introduce embarrassingly parallel search  eps  method solving constraint
problems parallel  show method matches even outperforms state ofthe art algorithms number problems using various computing infrastructures  eps
simple method master decomposes problem many disjoint subproblems solved independently workers  approach three advantages 
efficient method  involves almost communication synchronization
workers  implementation made easy master workers rely
underlying constraint solver  require modify it  paper describes
method  applications various constraint problems  satisfaction  enumeration 
optimization   show method adapted different underlying solvers
 gecode  choco   or tools  different computing infrastructures  multi core  data centers  cloud computing   experiments cover unsatisfiable  enumeration optimization
problems  cover first solution search makes results hard analyze  variability observed optimization problems  lesser
extent optimality proof required  eps offers good average performance 
matches outperforms available parallel implementations gecode well
solvers portfolios  moreover  perform in depth analysis various factors
make approach efficient well anomalies occur  last  show
decomposition key component efficiency load balancing 

   introduction
second half   th century  frequency processors doubled every    months
so  clear years period free lunch  put sutter
larus         behind us  outlined bordeaux  hamadi  samulowitz        
available computational power keep increasing exponentially  increase
terms number available processors  terms frequency per unit  multi core
processors norm raises significant challenges software development 
data centers high performance computing readily accessible many academia
industry  cloud computing  amazon  microsoft azure  google          offers massive
infrastructures rent computing storage used demand 
facilities anyone gain access super computing facilities moderate cost 
distributed computing offers possibilities put computational resources common
effectively obtains massive capabilities  examples include seti home  anderson  cobb 
korpela  lebofsky    werthimer         distributed net  distributed computing technologies inc      sharcnet  bauer         main challenge therefore scale  i e  
cope growth 
c
    
ai access foundation  rights reserved 

fimalapert  regin    rezgui

constraint programming  cp  appealing technology variety combinatorial
problems grown steadily last three decades  strengths cp
use constraint propagation combined efficient search algorithms  constraint
propagation aims removing combinations values variable domains cannot
appear solution  number years  possible gains offered parallel
computing attracted attention 
parallel computing form computation many calculations carried
simultaneously  almasi   gottlieb        operating principle large problems
often divided smaller ones  solved parallel  different forms
parallel computing exist  bit level  instruction level  data task parallelism  task
parallelism common approach parallel branch and bound  b b  algorithms  mattson  sanders    massingill        achieved processor executes different
thread  or process  different data  parallel computer programs difficult write sequential ones  concurrency introduces several new classes
potential software bugs  race conditions common  example 
memory shared  several tasks algorithm modify data
time  could render program incorrect  mutual exclusion allows worker lock
certain resources obtain exclusive access  create starvation
workers must wait worker frees resources  moreover  indeterminism
parallel programs makes behaviour execution unpredictable  i e  results
different program runs may differ  so  communication synchronization among different
sub tasks address issue  typically greatest obstacles good
performance  another central bottleneck load balancing  i e  keeping processors busy
much possible 
wilkinson allen        introduced embarrassingly parallel paradigm assumes computation divided number completely independent parts
part executed separate processor  paper  introduce
embarrassingly parallel search  eps  method constraint problems show
method often outperforms state of the art parallel b b algorithms number
problems various computing infrastructures  master decomposes problem
many disjoint subproblems solved independently workers  since constraint program trivially embarrassingly parallel  decomposition procedure must
carefully designed  approach three advantages  efficient method 
involves almost communication  synchronization  mutual exclusion workers 
implementation simple master workers rely underlying
constraint solver require modify it  additionally  deterministic
certain restrictions 
paper integrates results series publications  regin  rezgui    malapert 
            rezgui  regin    malapert         however  paper includes novel contributions  implementations  results  new implementation eps top
java library choco   choco        uses new decomposition procedure  new results
given implementations top c   library gecode  schulte       
or tools  perron  nikolaj    vincent         problems types instances
tested  eps compared parallelizations gecode several static
   

fiembarrassingly parallel search cp

solvers portfolios  perform in depth analysis various components  especially decomposition procedures  well anomalies occur 
paper organized follows  section   presents constraint programming background  amdahls law  related work parallel constraint solving  section   gives
detailed description embarrassingly parallel search method  section   gives extensive experimental results various implementations  gecode  choco   or tools 
different computing infrastructures  multi core  data center  cloud computing  well
comparisons state of the art parallel implementations static solver portfolios 

   related work
here  present constraint programming background  two important parallelization
measures related amdahls law  related work parallel constraint solving 
    constraint programming background
constraint programming  cp  attracted high attention among experts many areas
potential solving hard real life problems  extensive review
constraint programming  refer reader handbook rossi  van beek 
walsh         constraint satisfaction problem  csp  consists set x variables
defined corresponding set possible values  the domains d  set c constraints 
constraint relation subset variables restricts possible values
variables take simultaneously  important feature constraints declarative
manner  i e  specify relationship must hold  current domain d x 
variable x x always  non strict  subset initial domain  partial assignment
represents case domains variables reduced singleton
 namely variable assigned value   solution csp assignment
value variable constraints simultaneously satisfied 
solutions found searching systematically possible assignments
values variables  backtracking scheme incrementally extends partial assignment
specifies consistent values variables  toward complete solution 
repeatedly choosing value another variable  variables labeled  given value 
sequentially  node search tree  uninstantiated variable selected
node extended resulting new branches node represent alternative
choices may examined order find solution  branching strategy
determines next variable instantiated  order values
domain selected  partial assignment violates constraints  backtracking
performed recently assigned variable still alternative values available
domain  clearly  whenever partial assignment violates constraint  backtracking
able eliminate subspace cartesian product variable domains 
filtering algorithm associated constraint removes inconsistent values
domains variables  i e  assignments cannot belong solution
constraint  constraints handled constraint propagation mechanism
allows reduction domains variables global fixpoint reached  no
domain reductions possible   fact  constraint specifies relationship must hold
filtering algorithm computational procedure enforces relationship 
   

fimalapert  regin    rezgui

generally  consistency techniques complete  i e  remove inconsistent
values domains variables 
backtracking scheme consistency techniques used alone completely
solve csp  combination allows search space explored complete
efficient way  propagation mechanism allows reduction variable
domains pruning search tree whereas branching strategy improve
detection solutions  or failures unsatisfiable problems  
here  consider complete standard backtracking scheme depth first traversal
search tree combined following variable selection strategies  note different
variable selection strategies used although one time  lex selects variable
according lexicographic ordering  dom selects variable smallest remaining domain  haralick   elliott         ddeg selects variable largest dynamic degree  beck 
prosser    wallace         is  variable constrained largest number
unassigned variables  boussemart  hemery  lecoutre  sais        proposed conflictdirected variable ordering heuristics every time constraint causes failure
search  weight incremented one  variable weighted degree 
sum weights constraints variable occurs  wdeg selects
variable largest weighted degree  current domain variable incorporated give dom ddeg dom wdeg selects variable minimum ratio
current domain size dynamic weighted degree  boussemart et al        
beck et al          dom bwdeg variant follows binary labeling scheme  impact
selects variable value pair strongest impact  i e  leads strongest
search space reduction  refalo        
optimization problems  consider standard top down algorithm maintains
lower bound  lb  upper bound  ub  objective value  ub lb  subtree
pruned cannot contain better solution 
    parallelization measures amdahls law
two important parallelization measures speedup efficiency  let t c  wallclock time parallel algorithm c number cores let t   
wall clock time sequential algorithm  speedup su c    t      t c  measure
indicating many times parallel algorithm performs faster due parallelization 
efficiency eff  c    su c    c normalized version speedup  speedup
value divided number cores  maximum possible speedup single program
result parallelization known amdahls law  amdahl         states
small portion program cannot parallelized limit overall speedup
available parallelization  let b        fraction algorithm strictly
sequential  time t c 
algorithm takes finish executed c cores
corresponds to  t c    t    b    c    b    therefore  theoretical speedup su c  is 

su c   

 
b      b 
 
c

   

fiembarrassingly parallel search cp

according amdahls law  speedup never exceed number cores  i e  linear
speedup  this  terms efficiency measure  means efficiency always less
  
note sequential parallel b b algorithms always explore
search space  therefore  super linear speedups parallel b b algorithms contradiction amdahls law processors access high quality solutions early
iterations  turn brought reduction search tree problem size 
    parallel constraint solving
designing developing parallel programs manual process programmer responsible identifying implementing parallelism  barney   livermore         section  discuss parallel constraint solving  parallel
logic programming  refer reader surveys de kergommeaux codognet
        gupta  pontelli  ali  carlsson  hermenegildo         parallel integer
programming  refer reader surveys crainic  le cun  roucairol        
bader  hart  phillips         gendron crainic        
main approaches parallel constraint solving roughly divided following main categories  search space shared memory  search space splitting  portfolio algorithms  problem splitting  approaches require communication synchronization 
important issue load balancing refers practice distributing
approximately equal amounts work among tasks processors kept busy
time 
      search space shared memory
methods implemented many cores sharing list open nodes
search tree  nodes least one children still unvisited  
starved processors pick promising node list expand it 
defining different node evaluation functions  one implement different strategies  dfs 
bfs others   perron        proposed comprehensive framework tested
  processors  vidal  bordeaux  hamadi        reported good performance parallel
best first search    processors  although kind mechanism intrinsically provides
excellent load balancing  known scale beyond certain number processors 
beyond point  performance starts decrease  indeed  shared memory system 
threads must contend communicating memory problem
exacerbated cache consistency transactions 
      search space splitting
search space splitting strategies exploring parallelism provided search space
common approaches  branching done  different branches explored
parallel  pruul  nemhauser    rushmeier         one challenge load balancing 
branches search tree typically extremely imbalanced require non negligible
overhead communication work stealing  lai   sahni        
work stealing method originally proposed burton sleep        first
implemented lisp parallel machines  halstead         search space dynamically
   

fimalapert  regin    rezgui

split resolution  worker finished explore subproblem  asks
workers another subproblem  another worker agrees demand  splits
dynamically current subproblem two disjoint subproblems sends one subproblem
starving worker  starving worker steals work busy one  note
form locking necessary avoid several starving workers steal
subproblems  starving worker asks workers turn receives new
subproblem  termination work stealing method must carefully designed reduce
overhead almost workers starving  almost work remains  recent works
based approach zoeteweij arbab         jaffar  santosa  yap 
zhu         michel  see  hentenryck         chu  schulte  stuckey        
work stealing uses communication  synchronization computation time 
cannot easily scaled thousands processors  address issues  xie
davenport        allocated specific processors coordination tasks  allowing increase
number processors  linear scaling     processors  used
parallel supercomputer performance starts decline 
machado  pedro  abreu        proposed hierarchical work stealing scheme correlated cluster physical infrastructure  order reduce communication overhead 
worker first tries steal local node  considering remote nodes  starting
closest remote node   approach achieved good scalability     cores
n queens quadratic assignment problems  constraint optimization problems 
maintaining best solution worker would require large communication
synchronization overhead  but  machado et al  observed scalability lowered
lazy dissemination so far best solution  i e  workers use
obsolete best solution 
general purpose programming languages designed multi threaded parallel computing
charm    kale   krishnan        cilk    leiserson        budiu  delling   
werneck        ease implementation work stealing approaches  otherwise 
work stealing framework bobpp  galea   le cun        le cun  menouer    vanderswalmen        provides interface solvers parallel computers  bobpp 
work shared via global priority queue search tree decomposed allocated
different cores demand search algorithm execution  periodically  worker
tests starving workers exist  case  worker stops search path
root node highest right open node saved inserted global priority
queue  then  worker continues search left open node  otherwise 
starving worker exists  worker continues search locally using solver  starving
workers notified insertions global priority queue  one picks
node starts search  using or tools underlying solver  menouer le cun
        menouer le cun        observed good speedups golomb ruler
problem    marks          workers     queens problem         
workers   experiments investigate exploration overhead caused approach 
bordeaux et al         proposed another promising approach based search space
splitting mechanism based work stealing approach  use hashing function
allocating implicitly leaves processors  processor applies search
strategy allocated search space  well designed hashing constraints address
load balancing issue  approach gives linear speedup    processors
   

fiembarrassingly parallel search cp

n queens problem  speedups stagnate       processors  however 
got moderate results     industrial sat instances 
presented earlier works embarrassingly parallel search method based
search space splitting loose communications  regin et al               rezgui et al  
      
fischetti  monaci  salvagnin        proposed another paradigm called selfsplit
worker able autonomously determine  without communication
workers  job parts process  selfsplit decomposed three phases 
enumeration tree initially built workers  sampling   enough open nodes
generated  sampling phase ends worker applies deterministic rule
identify solve nodes belong  solving   single worker gathers results
others  merging   selfsplit exhibited linear speedups    processors good
speedups    processors five benchmark instances  selfsplit assumes sampling
bottleneck overall computation whereas happen practice  regin
et al         
sometimes  complex applications good domain specific strategies
known  parallel algorithm exploit domain specific strategy  moisan  gaudreault  quimper         moisan  quimper  gaudreault        proposed
parallel implementation classic backtracking algorithm  limited discrepancy search
 lds   known efficient centralized context good variable value
selection heuristic provided  harvey   ginsberg         xie davenport        proposed processor locally uses lds search trees allocated  by
tree splitting work stealing algorithm  global system replicate lds
strategy 
cube and conquer  heule  kullmann  wieringa    biere        approach parallelizing sat solvers  cube conjunction literals dnf formula disjunction
cubes  sat problem split several disjoint subproblems dnf formulas
solved independently workers  cube and conquer using conflictdriven clause learning  cdcl  solver lingeling outperforms parallel sat solvers
instances sat      benchmarks  outperformed many
instances  thus  concurrent cube and conquer  van der tak  heule    biere        tries
predict instances works well abort parallel search seconds
favor sequential cdcl solver not 
      las vegas algorithms   portfolios
explore parallelism provided different viewpoints problem 
instance using different algorithms parameter tuning  idea exploited
non parallel context  gomes   selman         communication required
excellent level load balancing achieved  all workers visit search space   even
approach causes high level redundancy processors  shows really good
performance  greatly improved using randomized restarts  luby  sinclair   
zuckerman        worker executes restart strategy  recently  cire 
kadioglu  sellmann        executed luby restart strategy  whole  parallel 
proved achieves asymptotic linear speedups and  practice  often obtained
   

fimalapert  regin    rezgui

linear speedups  besides  authors proposed allow processors share information
learned search  hamadi  jabbour    sais        
one challenge find scalable source diverse viewpoints provide orthogonal
performance therefore complementary interest  distinguish
two aspects parallel portfolios  assumptions made number available
processors possible handpick set solvers settings complement
optimally  want face arbitrarily high number processors 
need automated methods generate portfolio size demand  bordeaux et al  
       so  portfolio designers became interested feature selection  gomes   selman       
            kautz  horvitz  ruan  gomes    selman         features characterize problem
instances number variables  domain sizes  number constraints  constraints arities 
many portfolios select best candidate solvers pool based static features
learning dynamic behaviour solvers  sat portfolio isac  amadini  gabbrielli 
  mauro        cp portfolio cphydra  omahony  hebrard  holland  nugent   
osullivan        use feature selection choose solvers yield best performance 
additionally  cphydra exploits knowledge coming resolution training set
instances candidate solver  then  given instance  cphydra determines k
similar instances training set determines time limit candidate
solver based constraint program maximizing number solved instances within
global time limit    minutes  briefly  cphydra determines switching policy
solvers  choco   abscon  mistral  
many recent sat solvers based portfolio manysat  hamadi et al  
       satzilla  xu  hutter  hoos    leyton brown         sartagnan  stephan   michael 
       hydra  xu  hoos    leyton brown         pminisat  chu  stuckey    harwood 
      based minisat  een   sorensson         combine portfolio based
algorithm selection automatic algorithm configuration using different underlying solvers 
example  satzilla  xu et al         exploits per instance variation among solvers
using learned runtime models 
general  main advantage algorithms portfolio approach many strategies automatically tried time  useful defining good
search strategies difficult task 
      problem splitting
problem splitting another idea relates parallelism  problem split
pieces solved processor  problem typically becomes difficult
solve centralized case processor complete view problem 
so  reconciling partial solutions subproblem becomes challenging  problem
splitting typically relates distributed csps  framework introduced yokoo  ishida 
kuwabara        problem naturally split among agents  privacy
reasons  distributed csp frameworks proposed hirayama
yokoo         chong hamadi         ezzahir  bessiere  belaissaoui  bouyakhf
        leaute  ottens  szymanek         wahbi  ezzahir  bessiere  bouyakhf
       
   

fiembarrassingly parallel search cp

      parallel constraint propagation
approaches thought of  typically based parallelization one key algorithm solver  instance constraint propagation  nguyen   deville        hamadi 
      rolf   kuchcinski         however  parallelizing propagation challenging  kasif 
      scalability limited amdahls law  approaches focus
particular topologies make assumptions problem 
      concluding remarks
note oldest approaches  scalability issues still investigated
small number processors  typically around       processors  one major
issue approaches may  and must  resort communication  communication
parallel agents costly general  shared memory models multi core 
typically means access shared data structure one cannot avoid
form locking  cost message passing cross cpu even significantly higher  communication additionally makes difficult get insights solving process since
executions highly inter dependent understanding parallel executions notoriously
complex 
parallel b b algorithms explore leaves search tree different order
would single processor system  could pity situations
know really good search strategy  entirely exploited parallel algorithm 
many approaches  experiments parallel programming involve great deal nondeterminism  running algorithm twice instance  identical number
threads parameters  may result different solutions  sometimes different
runtimes 

   embarrassingly parallel search
section  present details embarrassingly parallel search  first  section    
introduces key concepts guided design choices  then  section     introduces
several search space splitting strategies implemented via top down bottom up decomposition procedures presented section      section     gives details architecture
communication  section     explains manage queue subproblems
order obtain deterministic parallel algorithm  section     gives details
implementation 
    key concepts
introduce key concepts guided design choices  massive static decomposition 
loose communication  non intrusive implementation  toward deterministic algorithm 
      massive static decomposition
master decomposes problem p subproblems
solved parallel independently workers  so  solving process equivalent
real time scheduling p jobs w parallel identical machines known p   cmax  korf  
   

fimalapert  regin    rezgui

schreiber         efficient algorithms exists p   cmax even simple list scheduling
algorithms  based priority rules     w    approximation  desirable properties
defined section     ensure low precision processing times makes problems
easier  hold precision number workers fixed  increase number
subproblems  problems get harder perfect schedules appear  get
easier  case  number p subproblems range one three
orders magnitude larger number workers w  low  chance
finding perfect schedules  therefore obtain good speedups  low  large 
decomposition takes longer becomes difficult  conditions met 
unlikely worker assigned work other  therefore 
decomposition statistically balanced  beside  reach good speedups practice 
total solving time subproblems must close sequential solving time
problem 
advantage master workers independent  use different
filtering algorithms  branching strategies  even underlying solvers  decomposition
crucial step  bottleneck computation quality greatly
impacts parallelization efficiency 
      loose communication
p subproblems solved parallel independently w workers  load balancing
must statistically obtained decomposition  allow work stealing
order drastically reduce communication  course  communication still needed
dispatch subproblems  gather results possibly exchange useful additional
information  objective bound values  loose communication allows use star network
without risk congestion  central node  foreman  connected nodes  master
workers  
      non intrusive implementation
sake laziness efficiency  rely much possible underlying solver s 
computing infrastructure  consequently  modify little possible underlying solver  consider nogoods clauses exchanges techniques
intrusive increase communication overhead  additionally  logging fault
tolerance respectively delegated underlying solver infrastructure 
      toward determinism
deterministic algorithm algorithm which  given particular input  always produce output  underlying machine always passing sequence states  determinism already challenging sequential b b algorithms
due complexity  randomization  restarts  learning  optimization   still
difficult parallel b b algorithms 
here  always guarantee reproducibility real time assignment subproblems workers stored  reproducibility means always possible replay
solving process  restrictions detailed later  parallel algorithm made
deterministic additional cost  moreover  parallel algorithm able
   

fiembarrassingly parallel search cp

mimic sequential algorithm  i e  produce identical solutions  requires
parallel algorithm visits tree leaves order sequential algorithm 
generally  would useful debugging  performance evaluation  incremental problem
solving parallel algorithm may produce identical solutions matter many
workers present computing infrastructure used 
conversely  real time scheduling algorithm applied subproblems  would
allow improve diversification using randomization  exploit past information
provided solving process  experiments  use fifo scheduling
subproblems  scheduling policy would change shape size
search tree and  therefore  reduces relevance speedups  unlike eps  work stealing
approaches deterministic offer control subproblem scheduling 
    search space splitting strategies
here  extend approach search space splitting proposed bordeaux et al         
called splitting hashing  let us recall c set constraints problem 
split search space problem p parts  one approach assign subproblem
   p  extended set constraints c hi hi hashing constraint 
constrains subproblem particular subset search space  hashing constraints
must necessarily sound effective  nontrivial  statistically balanced 
sound hashing constraints must partition search space  pi   hi must cover entire
initial search space  completeness   mutual intersections hi hj      j p 
preferably empty  non overlapping  
effective addition hashing constraints effectively allow worker
efficiently skip portions search space assigned current subproblem 
subproblem must significantly easier original problem  causes overhead 
refer recomputation overhead 
nontrivial addition hashing constraints lead immediate
failure underlying solver  thus  generating trivial subproblems might paid
exploration overhead  many would discarded propagation
mechanism sequential algorithm 
statistically balanced workers given amount work 
decomposition appropriate  number p subproblems significantly larger
number w workers  thus unlikely given worker would assigned
significantly work worker real time scheduling algorithm  however  possible solving one subproblem requires significantly work another
subproblem 
bordeaux et al         defined hashing constraints selecting subset x variables
p
problem stating hi    p  follows  xx x mod p  effectively
decomposes problem p problems p within reasonable limits  p      imposes
parity constraints sum variables  splitting repeated scale up
arbitrary number processors  splitting obviously sound  less effective
   

fimalapert  regin    rezgui

cp solvers sat solvers  here  study assignment splitting node splitting
generate given number p  subproblems 
      assignment splitting
let us consider non empty subset x x ordered variables  x    x            xd    vector    v            vd   tuple x vj d xj    j              d   let h      dj    xj   vj  
hashing constraints restrict search space solutions extending tuple  
q
total decomposition x splits initial problem di   d xi   subproblems  i e  one
subproblem per tuple  total decomposition clearly sound effective  efficient
practice  indeed  regin et al         showed number trivial subproblems
grow exponentially 
table decomposition  subproblem defined set tuples allows reach
exactly number p  subproblems  let ordered list ofj tuples
x
k
 t  
 
 t     p   then  first subproblem defined first k   p  tuples  second
subproblem defined following k tuples  on  so  subproblems defined
number tuples possibly exception last 
tuple solver consistent propagation extended set constraints c
h    underlying solver detect unsatisfiability  order obtain nontrivial
decompositions  total table decompositions restricted solver consistent tuples 
      node splitting
node splitting allows parallel algorithm exploit domain specific strategies
decomposition good strategy known  let us recall concepts search
trees  perron        basis decomposition procedures introduced later 
decompose problems  one needs able map individual parts search tree
hashing constraints  parts called open nodes  open nodes defined 
present search tree decomposed set open nodes 
open nodes node expansion search tree partitioned three sets  open
nodes  closed nodes  unexplored nodes  here  make assumption
arity search tree  i e  maximal number children nodes  subsets
following properties 
ancestors open node closed nodes 
unexplored node exactly one open node ancestor 
closed node open node ancestor 
set open nodes called search frontier illustrated figure    search
active
path
closed

open

frontier

unexplored

figure    node status search frontier search tree 
   

fiembarrassingly parallel search cp

frontier evolves simply process known node expansion  removes open node
frontier  transforms removed node closed node  adds unexplored
children frontier  node expansion operation happens
search  corresponds branch operation b b algorithm 
point search  search frontier sound nontrivial decomposition
original problem open node associated subproblem  decomposition effective branching strategy effective  let us remark
assignment splitting seen special case node splitting static ordering
used variables values 
active path jumps search tree expanding one node another may
require changing state  at least variables domains  search process
first node second  so  worker charge exploring open node must reconstruct
state visits  done using active path jumping operation 
going search tree  search process builds active path 
list ancestors current open node  illustrated figure    worker
moves one node another  jump search tree  make jump 
simply recomputes every move root gets target node  causes
overhead  refer recomputation overhead  recomputation change
search frontier expand node 
    decomposition procedures
decomposition challenge find depth search frontier contains
approximately p  nodes  assignment splitting strategy implemented top down
procedure starts root node incrementally visits next levels  whereas
node splitting strategy implemented bottom up procedure starts form
level deep enough climbs back previous levels 
      top down decomposition
challenge top down decomposition find ordered variables produce
approximately p  solver consistent tuples  algorithm   realizes solver consistent table
decomposition iterated depth bounded depth first searches early removals inconsistent assignments  regin et al         
algorithm starts root node
empty list tuples  line     computes list p  tuples solver consistent
table decomposition iterativly increasing decomposition depth  let us assume
exists static order variables  iteration  determines new lower
bound  line    decomposition depth d  i e  number variables involved
decomposition  lower bound uses cartesian product current domains
next variables xd     xd           then  depth bounded depth first search extends
decomposition new depth updates list tuples  line     current tuples
added constraints model search  line    order reduce
redundant work  search  extended tuples propagated  line    reduce
domains  improve next lower bound decomposition depth  tuple
solver consistent  not proven infeasible  last search  process repeated
number  t   tuples greater equal p    end  tuples aggregated
   

fimalapert  regin    rezgui

algorithm    top down decomposition 

 
 

 

 

 
 

 
 

 

  

data  csp  x   d  c  number subproblems p 
result  list tuples
  
 
   simulate breadth first search  iterated depth bounded dfss 
repeat
   determine
decomposition
depth    

n lower bound
ql

 
min l max     t    i d    d xi    p  

  

   extend current decomposition new variables    
depthboundeddfs  x   c   h      d   x            xd    
   break 
   propagate tuples  without failure     
propagate  x   c   h      d  
 t     p   
   aggregate tuples generate exactly p  subproblems   
aggregatetuples t      subproblems become simultaneously available 
  
foreach sendsubproblem  x   c h     d  

generate exactly p  subproblems  practice  consecutive tuples aggregated 
subproblems become simultaneously available aggregation 
sometimes  sequential decomposition bottleneck amdahls law  so 
parallel decomposition procedure increases scalability  regin et al         
two steps differ algorithm    first  instead starting depth   empty list
tuples  line   algorithm     first list quickly generated least five tuples per
worker 
n

 
 

ql
min l i    d xi      w  
qd
i   d xi   

second  iteration  tuple extended parallel instead extending sequentially tuples  line   algorithm     parallel decomposition change ordering
compared sequential one  again  subproblems become available
end decomposition 

 

   
run parallel
foreach
   extend tuple parallel   
    depthboundeddfs  x   c h     d   x            xd    

 

  

 
 
 

top down procedures assume variable ordering used decomposition static  next decomposition procedure bypasses limitation handles
branching strategy 
   

fiembarrassingly parallel search cp

algorithm    bottom up decomposition 
 

 
 
 
 
 

 
 
 
  

data  csp  x   d  c   decomposition depth d    subproblem limit p  
p   
   generate subproblems visiting top real tree    
node callback decomposition node 
depth node  d 
sendsubproblem  node  
p p     
p p
   decrease dynamically depth    
d  max    d     
p   p 
backtrack 
dfs  x  c d  

      bottom up decomposition
bottom up decomposition explores search frontier depth d  approximately p  nodes  simplest form  decomposition depth d  provided
user good knowledge problem  algorithm   explores search frontier depth
d  using depth first search illustrated figure   a   search callback identifies
node level d   line     sends immediately active path  defines subproblem 
subproblem solved worker  decomposition depth dynamic 
reduced number subproblems becomes large  line     aims
compensate poor choice decomposition depth d    practice  depth reduced
one unit current number subproblems exceeds given limit p   limit
initially set p     p  doubled time reached  contrary 
depth static  p      never changes whatever number subproblems 
practice  common user provides decomposition depth 
automated procedure without users intervention needed  algorithm   aims
identifying topmost search frontier approximately p  open nodes sampling
estimation  procedure divided three phases  build partial tree sampling

final depth

search frontier

dynamic



p nodes

static
p nodes

initial depth

 p nodes

 a  decomposition 

 b  estimation 

figure    bottom up decomposition estimation 
   

fimalapert  regin    rezgui

algorithm    bottom up estimation 

 

 
 
 
 
 
 
 
 

  
  

data  csp  x   d  c  number subproblems p   
data  time limit t  node limit n  maximum depth large enough
result  decomposition depth d 
   set counters width levels   
foreach     d  width d    
   build partial tree sampling    
node callback estimation node 
depth node  

width d  width d      
width d  p     
else backtrack 
hasfinished  t n  break 
dfs  x  c d  
   estimate level widths tree decomposition depth 
width estimatewidths width  
d  estimatedepth width  p    

  

top real search tree  estimate level widths real tree  determine
decomposition depth d  greedy heuristic 
since need explore top search tree  upper bound decomposition depth fixed  maximum decomposition depth must chosen according
number workers expected number subproblems per worker 
small  decomposition could generate subproblems  large 
sampling time increases decomposition quality could decrease 
sampling phase builds partial tree p  open nodes level using
callback depth first search  number open nodes level partial
tree counted callback  maximum depth reduced time p  nodes
opened given level  line     sampling ends within limits  top
tree entirely visited estimation needed  otherwise  line     one needs
estimate widths topmost levels tree depending partial tree 
estimation straightforward adaptation one proposed cornuejols  karamanov 
li        deal n ary search tree  line      practice  main issue
higher arity is  lower precision estimation  therefore  greedy heuristic
determines decomposition depth based estimated number nodes per level 
number nodes partial tree  line      heuristics minimizes
absolute deviation estimated number nodes expected number p   
several levels identical absolute deviation  lowest level estimated
number subproblems greater equal p  selected 
    architecture communication
describe messages exchanged actors depending problems type  then 
typical use case illustrates solving process optimization problem  briefly 
   

fiembarrassingly parallel search cp

communication network star network foreman acts pipe transmit
messages master workers 
      actors messages
total number messages depends linearly number workers  w 
number subproblems  p   messages synchronous sake simplicity
means work must wait communications completed  barney   livermore 
       interleaving computation communication single greatest benefit using
asynchronous communications since work done communications taking
place  however  asynchronous communications complicate architecture  instance
message requests answer 
master control unit decomposes problem collects final results 
sends following messages  create foreman  give subproblem foreman 
wait foreman gather results  destroy foreman  master deals
foreman  decomposition time elapsed time create
wait messages  workers time elapsed time first give destroy
messages  wall clock time elapsed time creation destruction
master 
foreman central node star network  queuing system stores subproblems received master dispatches workers  gathers results
collected workers  foreman allows master concentrate problems
decomposition  performance bottleneck  handling communications
workers  sends following messages  create worker  give subproblem worker 
collect  send  final results master  destroy worker  foreman
detects search ended  sends collect message containing final results
master 
workers search engines  send following messages  find subproblem  the
foreman must answer give message   collect  send  results foreman 
results contain essential information solution s  solving process  workers
know foreman  worker acquires new work  receives give message
foreman   acquired subproblem recomputed causes recomputation overhead 
work stealing context  schulte        noticed higher node search
tree  smaller recomputation overhead  construction  topmost nodes
used here 
      problems types
discuss specificities first solution  solution  best solution searches 
first solution search search complete soon solution found 
workers must immediately terminated well decomposition procedure 
solution search search complete subproblems solved 
   

fimalapert  regin    rezgui

best solution search main design issue best solution search maintain
so far best solution  sequential b b algorithm always knows so far best solution 
difficult achieve concurrent setting several workers  maintaining best
solution worker could lead large communication synchronization overheads 
instead prefer solution foreman workers maintain so far best
solution follows  default  give collect messages foreman
workers carry objective information  additionally  worker send better messages
foreman intermediate solution  foreman send best solution
workers  instance  worker finds new solution  informs foreman
sending better message solution accepted threshold function  similarly 
foreman receives new solution collect better message  checks
whether solution really better  solution accepted threshold function 
foreman sends another better message workers  architecture sketched
entails worker might always know so far best solution  consequence 
parts search tree explored  pruned away worker
exact knowledge  thus  loose coupling might paid exploration
overhead 

master

foreman

worker  

worker  

opt

 allocate resources 
   create   
   create   
   create   
give

find
give
opt

better

 best solution search 

give

give

give
collect
find
wait

give
collect
better

opt

better

 best solution search 

collect

collect
opt
 release resources 
   destroy   
   destroy   

   destroy   

master

foreman
master

worker
master 

worker
master 

figure    sequence diagram solving process two workers 
   

fiembarrassingly parallel search cp

      use case
figure   sequence diagram illustrating solving process optimization problem
two workers  shows actors operate chronological order 
first horizontal frame resource allocation  master creates foreman 
foreman creates workers  immediately creation  master worker
load original problem  foreman transparently manages concurrent queue subproblems produced master consumed workers  that  workers
jumps search tree 
foreman creation  master starts decomposition original problem
p     subproblems  soon subproblem generated  master gives
foreman  here  give find messages interleaved node splitting
decomposition proposed section        assignment splitting decomposition proposed
section       would produce unique give message subproblems 
decomposition finished  master sends wait message foreman waits
collect response containing final result  last collect message triggers
resource deallocation 
time worker starving  asks foreman subproblem waits it 
here  first subproblem assigned first worker second worker waits
second subproblem  best solution search frames correspond specific messages
optimization problems  first worker quickly finds good solution sends
foreman via better message  second subproblem generated master
given foreman  turn  foreman gives second subproblem updated
objective information second worker  second problem quickly solved
second worker sends collect message foreman  collect message
stands find message  then  third  last  subproblem assigned second
worker 
foreman broadcasts better message good quality solution
received first worker  note message useless first worker 
foreman detects termination solving process sends collect message
master three following conditions met  master waiting  subproblems
queue empty  workers starving  last horizontal frame resource
deallocation 
    queuing determinism
foreman plays role queuing system receives subproblems master
dispatches workers  section  show eps modified
return solution sequential algorithm useful several
scenarios debugging performance evaluation  generally  queuing policy
applied select next subproblem solve 
let us assume subproblems p    p            pp sent foreman fixed
order case sequential top down procedure bottom up procedure 
otherwise  fixed order subproblems obtained sorting subproblems 
first solution found sequential algorithm belongs satisfiable subproblem
pi smallest index  i e  leftmost solution  let us assume parallel
   

fimalapert  regin    rezgui

algorithm finds first solution subproblem pj j   i  then 
necessary solve problems pk k   j one must wait problem
pk k   j determine leftmost solution  satisfiable subproblem
smallest index 
easily extended optimization problems slightly modifying cutting
constraints  usually  cutting constraint stated new solution found
allows strictly improving solution  contrary constraints  cutting
constraint always propagated backtracking  here  solution found solving
subproblem pj   cutting constraint allows strictly improving solution
subproblems k j  allows equivalent solution subproblems k   j 
so  parallel algorithm returns solution sequential one
subproblem visited order  moreover  solution returned parallel
algorithm depend number workers  decomposition 
experiments  queuing policy fifo policy ensures subproblems
solved order speedups relevant  however  guaranty
sequential parallel algorithms return solution 

   experimental results
here  describe experiments eps carry detailed data analysis  aim
answer following questions  eps efficient  different number workers 
different solvers  different computing platforms  compared parallel
approaches  influence different components  decomposition procedures 
search strategies  constraint models   eps robust flexible  anomalies
occur 
section     presents benchmark instances  execution environments  parameters settings  different implementations  first  section      analyze evaluate
top down bottom up decomposition procedures well importance search
strategy  especially decomposition  then  evaluate efficiency scalability
parallel solvers multi core machine  section       data center  section      
cloud platform  section       sections  compare implementations
eps work stealing approaches whenever possible  section      analyze efficiency parallel solver depending search strategy  section     
transform reasonable effort parallel solver distributed parallel solver
using batch scheduler provided data center  anomalies parallel solver
explained resolved distributed equivalent  last  section     discusses
performance parallel solvers compared static portfolios built underlying
sequential solvers data center 
    experimental protocol
section  introduce benchmark instances  execution environments  metrics
notations  give details implementations 
   

fiembarrassingly parallel search cp

      benchmark instances
lot benchmark instances available literature  aim select difficult
instances various models represent problems tackled cp  ideally  instance
difficult none solvers solve quickly  indeed  parallel solving relevant
shortens long wall clock time  here  consider unsatisfiable  enumeration
optimization problems instances  ignore problem finding first feasible
solution parallel speedup completely uncorrelated number
workers  making results hard analyze  consider optimization problems
variability observed  lesser extent optimality
proof required  variability unsatisfiable enumeration instances lowered 
therefore  often used test bed parallel computing  besides  unsatisfiable
instances practical importance  instance software testing  enumeration
important users compare various solutions 
first set called fzn selection    instances selected     
instances either repository maintained kjellerstrand        directly
minizinc     distribution written flatzinc language  nicta optimisation research
group         instance solved     seconds less   hour
gecode  selection composed   unsatisfiable    enumeration     optimization
instances 
set xcsp composed instances categories acad real xcsp
     roussel   lecoutre         consists difficult instances solved within
   hours choco   malapert   lecoutre         first subset called xcsp  composed
  unsatisfiable   enumeration instances whereas second subset called xcsp 
composed    unsatisfiable   enumeration instances  set xcsp  composed
instances easier solve xcsp  
besides  consider two classical problems  n queens golomb ruler
problems widely used literature  gent   walsh        
      implementation details
implemented eps method top three solvers  choco        written java  gecode
      or tools rev       written c    use two parallelism implementation
technologies  threads  mueller et al         kleiman  shah    smaalders       
mpi  lester        gropp   lusk         typical difference
threads  of process  run shared memory space  mpi standardized
portable message passing system exchange information processes running
separate memory spaces  therefore  thread technology handle multiple nodes
cluster whereas mpi does 
c    use threads implemented pthreads  posix library  mueller et al  
      kleiman et al         used unix systems  java  use standard java thread
technology  hyde        
many implementations mpi openmpi  gabriel  fagg  bosilca  angskun 
dongarra  squyres  sahay  kambadur  barrett  lumsdaine  et al          intel mpi  intel
corporation         mpi ch  mpi ch team        ms mpi  krishna  balaji  lusk 
thakur    tiller        lantz         mpi standard api  characteristics
   

fimalapert  regin    rezgui

machine never taken account  so  machine providers bull  ibm intel
provide mpi implementation according specifications delivered machine  thus  cluster provided bull custom intel mpi     library  openmpi
      installed  microsoft azure supports ms mpi   library 
or tools uses sequential top down decomposition c   threads  gecode uses
parallel top down decomposition c   threads mpi technologies  fact  gecode
use c   pthread multi core computer  openmpi data center 
ms mpi cloud platform  gecode or tools use lex variable selection
heuristic top down decomposition requires fixed variable ordering  choco 
uses bottom up decomposition java threads  every case  foreman schedules
jobs fifo mimic much possible sequential algorithm speedups
relevant  needed  master workers read model file 
always take value selection heuristic selects smallest value whatever
variable selection heuristic 
      execution environments
use three execution environments representative computing platforms available nowadays 
multi core dell computer     gb ram   intel e            ghz processors running scientific linux      each processor    cores  
data center centre de calcul interactif hosted universite nice sophia
antipolis provides cluster composed    nodes       cores  running centos
     node    gb ram   intel e            ghz processors    cores  
cluster managed oar  capit  da costa  georgiou  huard  martin  mounie  neyron 
  richard         i e   versatile resource task manager  thread technology
limited single node cluster  choco  use    physical cores whereas gecode
use number nodes thanks mpi 
cloud computing cloud platform managed microsoft company  microsoft
azure  enables deploy applications windows server technology  li        
node    gb ram intel xeon e      e     ghz processors    physical cores 
allowed simultaneously use   nodes     cores  managed microsoft hpc
cluster       microsoft corporation        
computing infrastructures provide hyper threading technologies  hyper threading
improves parallelization computations  doing multiple tasks once   core
physically present  operating system addresses two logical cores  shares
workload among possible  multi core computer provides hyper threading 
whereas deactivated cluster  available cloud 
      setting parameters
time limit solving instance set    hours whatever solver 
number workers strictly less number cores  w   c   always
unused cores  usually  one chooses w   c  workers work simultaneously 
multi core computer  use two workers per physical core  w    c  hyperthreading efficient experimentally demonstrated appendix a  target number
   

fiembarrassingly parallel search cp

p  subproblems depends linearly number w workers  p       w  allows
statistical balance workload without increasing much total overhead  regin
et al         
experiments  network ram memory loads low regards
capacities computing infrastructures  indeed  total number messages depends
linearly number workers number subproblems  ram pre allocated
computing infrastructure allows it  last  workers almost produce input output
disk access 
      metrics notations
let solving time  in seconds  algorithm let su speedup parallel
algorithm  tables  row gives results obtained different algorithms given
instance  row  best solving times speedups indicated bold  dashes
indicate instance solved algorithm  question marks indicate
speedup cannot computed sequential solver solve instance
within time limit  arithmetic means  abbreviated am  computed solving times 
whereas geometrical means  abbreviated gm  computed speedups efficiency 
missing values  i e  dashes question marks  ignored computing statistics 
use scoring procedure based borda count voting system  brams  
fishburn         benchmark instance treated voter ranks solvers 
solver scores points related number solvers beats  precisely 
solver scores points problem p comparing performance solver s 
follows 
gives better answer s    scores   point 
else answer gives worse answer s    scores   point 
else scoring based execution time comparison  s s  give indistinguishable
answers  
let t  respectively denote wall clock times solvers s  given problems
instance  case indistinguishable answers  scores f  t  t    according borda system
used minizinc challenge  but  function f capture users preferences
well  indeed  solver solves n problems     seconds n others      seconds
whereas solver s  solves first n problems     seconds n others    
seconds  solvers obtain score n whereas users would certainly
prefer s    so  use another scoring function g t  t    g t  interpreted
utility function solving problems instance within seconds  function g t 
strictly decreasing     toward    remaining points shared using function f  

f  t  t     

t 
  t 

g t  t      g t    g t g t    f  t  t   

g t   

 
   loga  t          

using function g  a       previous example  solvers s  respectively
scored      n      n points 
   

fimalapert  regin    rezgui

    analysis decomposition
section  compare quality performance top down bottom up
decomposition procedures introduced section     
      decomposition quality
top down decomposition always returns target number p       w subproblems
whereas guaranteed bottom up decomposition  figure   a  boxplot
number subproblems per worker  p   w  bottom up decomposition
choco  depending number workers  boxplots display differences among populations without making assumptions underlying statistical distribution 
non parametric  box boxplot spans range values first quartile
third quartile  whiskers extend end box range equal
    times interquartile range  points lie outside range whiskers
considered outliers  drawn individual circles 
number workers w                decompositions xcsp instances
using one variable selection heuristic among lex  dom  dom ddeg dom wdeg  dom bwdeg 
impact  combined minval  considered  bottom up decomposition obtains
satisfying average performance  mostly        subproblems per worker 
respecting much possible branching strategy  however  anomalies occur 
first  decomposition sensitive shape search tree  sometimes  model
contains variables large domains forbid accurate decomposition 
instance  first second levels knights      search tree respectively contain
           nodes  significant underestimation
tree size  especially branching high arity  instance  width second
level fapp          estimated around     nodes contains     
nodes  contrary  underestimation occur top nodes eliminated
search tree low arity  apart underestimation  decomposition accurate
search trees low arity 
top down decomposition accurate  requires fixed variable ordering  whereas
bottom up decomposition less accurate  handles branching strategy 
 

   

   
instances    

subproblems per worker

    

  

   

   

 
   

   
  

  

 

   

workers

choco  w   
choco  w    
gecode w   
gecode w    
   

 

  

   

time  s 

 a  number subproblems per worker 

 b  decomposition time 

figure    analysis decomposition procedures  w                
   

    

fiembarrassingly parallel search cp

      decomposition time
figure   b  gives percentage decompositions done within given time  choco 
times reported variable selection heuristics xcsp instances  gecode times
reported lex xcsp fzn instances 
implementation differences  times reported choco  gecode
slightly different  indeed  decomposition time alone given gecode  choco 
times take account estimation time  time taken foreman fill
queue subproblems  time taken workers empty queue  let us
remind subproblems become available top down decomposition
complete whereas become available fly bottom up decomposition 
cases  reported time lower bound solving time 
top down decomposition faster bottom up decomposition
parallelism  fact  gecode decomposition often faster estimation time alone 
one compelling example instance knights      highest time  around
    seconds  well poor quality structure problem unsuited
bottom up decomposition  variables large domains  more
     values   almost domain reduction top tree 
propagation long 
conclude  parallel top down decomposition gecode fast accurate
bottom up decomposition offers greater flexibility  less robustness 
      influence search strategy
analyze influence search strategies decomposition resolution 
apply variable selection heuristic decomposition  master  another one
resolution  workers   table   gives solving times combinations lex
dom solving instances xcsp   results reported significant
differences among solving times  choice variable selection heuristic critical
decomposition resolution  indeed  initial choices made branching
least informed important  lead largest subtrees
search hardly recover early mistakes  on  master workers
use variable selection heuristic 

instances

worker
master

costasarray   
latinsquare dg  
lemma       mod
pigeons   
quasigroup    
queenattacking  
squares    

lex

dom

lex

dom

lex

dom

     
     
     
      
     
     
     

     
     
     
     
     
     
      

     
     
     
     
     
     
     

     
     
     
     
     
     
      

table    solving times different search strategies  choco   multi core  w    c       
   

fimalapert  regin    rezgui

    multi core
section  use parallel solvers based thread technologies solve instances
xcsp  n queens problem using multi core computer  let us recall
two worker per physical core hyper threading activated  w    c        show
eps frequently gives linear speedups  outperforms work stealing approach
proposed schulte         nielsen        
      performance analysis
table   gives solving times speedups parallel solvers using    workers
xcsp  instances  choco  tested lex dom whereas gecode or tools
use lex  compared work stealing approach denoted gecode ws  schulte 
      nielsen         first  implementations eps faster efficient
work stealing  eps often reaches linear speedups number cores whereas never
happens work stealing  even worse  three instances solved within   
hours time limit using work stealing whereas using sequential solver 
choco   dom efficient parallel lex remains slightly slower
average  decomposition key bad performance instances knights     
lemma       mod  outlined before  decomposition knights      takes
     seconds generates much subproblems  forbids speedup  issue
lessened using sequential decomposition or tools resolved parallel
top down decomposition gecode  note sequential solving times or tools
gecode respectively       times higher  similarly  long decomposition time
choco  lemma       mod leads low speedup  however  moderate efficiency
choco  gecode squares     caused decomposition 
gecode or tools often efficient faster choco   solvers show
different behaviors even using variable selection heuristic
instances

costasarray   
knights     
latinsquare dg  
lemma       mod
ortholatin  
pigeons   
quasigroup    
queenattacking  
series   
squares    
 t  gm  su 
borda score  rank 

choco  lex

choco  dom

gecode

or tools

gecode ws



su



su



su



su



su

     
      
     
     
     
      
     
     
    
     

    
   
    
   
    
    
    
    
    
    

     
      
     
     
     
     
     
     
    
      

    
   
    
   
    
    
    
    
    
    

    
     
     
   
     
     
    
       
    
    

    
    
    
    
    
    
    
 
    
    

    
      
     
   
     
     
    

    
    

    
    
    
    
    
    
    

    
    

     

      
   
      

    

     
     

   

   
    
   

    

   
   

     

    

     

    

      

    

     

    

      

   

        

        

        

        

       

table    solving times speedups  multi core  w    c        gecode or tools use
lex heuristic 

   

fiembarrassingly parallel search cp

propagation mechanisms decompositions differ  furthermore  parallel top down
decomposition gecode preserve ordering subproblems regard
sequential algorithm 
      variations n queens problem
here  verify effectiveness eps classic csp settings  consider four models
well known n queens problem  n        n queens puzzle problem placing
n chess queens n n chessboard two queens threaten other  here 
enumerate solutions heuristics lex dom reasonable choices  models are 
alldifferent global constraints enforce arc consistency  ac   alldifferent constraints enforce bound consistency  bc   arithmetic inequalities constraints  neq  
dedicated global constraint  jc   milano   trick        ch     
table   gives solving times speedups choco     workers
decomposition depth either      striking result splitting
technique gives excellent results  linear speedup    processors
exception jc model  unfortunate since jc model clearly best model
sequential solver  here  dom always better choice lex  number
subproblems dom whatever model whereas total number nodes
changes  indicates filtering weak top search tree 
works report good results  often linear speedups n queens problem  bordeaux et al         reported linear speedups    cores    queens 
improvement    cores  whereas machado et al         scales     workers using hierarchical work stealing approach  menouer le cun        reported
speedups around   using    cores    queens  pedro  abreu  pedro  abreu
       reported speedups around    using    cores  zoeteweij arbab        reported
linear speedups    cores    queens  pedro et al         reported speedup
   using    cores  so  eps efficiency slightly average  similar
results observed       queens 
previous experimental setting favor eps exploring search
space exhaustively  problem highly symmetric  indeed  variance subproblems solving time low  especially higher levels consistency  note
lower speedups jc model probably caused load balancing issues
subproblems neq model greater mean variance 

model

lex

dom

d  

bc
ac
neq
jc

d  

d  

d  



su



su



su



su

     
      
     
     

    
    
    
    

     
      
     
     

    
    
    
    

     
      
     
     

    
    
    
    

     
      
     
     

    
    
    
    

table    variations    queens problem  choco   multi core  w    c       
   

fimalapert  regin    rezgui

instances

lex


dom
su



dom ddeg
su



dom bwdeg

su



su

dom wdeg


cc        
                  
 

                 
costasarray   
          
                               
     
crossword m  
         




         
     
crossword m c 
                                                  
fapp         





                 
knights     
          
                                
     
knights     
          
                                        
knights     
         
                                     
langford     
       
                                         
       
                                  
     
langford     
langford     

          
                     
latinsquare dg 
           
                     
         
     
lemma       mod
         
                   
         
     
          
                     
                
ortholatin  
pigeons   
                                                   
                                
         
     
quasigroup    
queenattacking  
                                            
     
queensknights 





                 
ruler       a 
                      


        
    
                                                  
ruler       a 
scen   f 





           

series   
         
                               
     
                               
               
squares    
squaresunsat 





                 
 t  gm  su 
borda score  rank 
 
 

      

   

        

      

          

        

   

      

        

          

        

 

crossword m  words      
crossword m c words vg    ext
queensknights      mul   squaresunsat      

 

su

impact


su

           
 
    
          
             
              
   


               
               
   
         
          
 
               
          
 
    
        
   
         
    
          
              
   
          
             
   


   
         
              
   


   
         
    
          
   


   

         

      

   

        

latinsquare dg  

table    detailed speedups solving times depending variable selection heuristics
 choco   data center  w       

  

  

speedup

  

  

  

 

 

lex

dom

ddeg

bwdeg

wdeg

impact

variable selection

figure    speedups variable selection heuristics  choco   data center  w       

   

fiembarrassingly parallel search cp

    data center
section  study influence search strategy solving times
speedups  scalability     workers  compare eps work stealing approach 
      influence search strategy
study performance choco  using    workers solving xcsp instances using
variable selection heuristics presented section      figure   boxplot
speedups variable selection heuristic  first  speedups lower dom bwdeg
decomposition effective  binary branching states constraint x  
left branch x    right branch  so  workload left right
branches imbalanced  case  positive decisions left branches
taken account  second  without learning  lex dom   parallel algorithm
efficient robust terms speedup  learning  dom bwdeg  dom wdeg 
impact   parallel algorithm may explore different search tree sequential
one  indeed  master explores top tree changes learning 
possibly branching decisions  worker learns subproblems 
whole search tree  frequently causes exploration overhead solving
queensknights      mul  twelve times nodes using dom wdeg  or  sometimes gives
super linear speedup solving quasigroup      three times less nodes using impact  
last  low speedups occur variable selection heuristics 
table   gives solving times speedups obtained different variable selection
heuristics  borda scores computed choco   table    gecode  table     first 
variable selection heuristics strictly dominates others either sequential parallel 
however  dom wdeg robust outlined borda scores  fact  variability solving times different heuristics reduced parallelization 
remains important  second  spite low speedups  dom bwdeg remains second
best variable selection heuristic parallel solving best one sequential 
average  using advanced variable selection heuristics dom bwdeg  dom wdeg 
impact gives lower solving times lex dom spite lower speedups  highlights
fact decomposition procedures handle branching strategy  section       
investigate low speedups instance crossword m c words vg   
caused variable selection heuristics 
      scalability     workers
table   compares gecode implementations eps work stealing  ws  solving
xcsp instances using        workers  eps faster efficient work
stealing     workers  work stealing ranked last using borda score 
    workers  eps average almost    times faster work stealing 
efficient parallelize sequential solver  multi core
machine  gecode faster choco  instances xcsp   here  performance
gecode mitigated outlined borda scores  five instances
solved within time limit gecode reported table    six instances
solved    workers whereas twelve instances solved sequential
solver  way comparison  five instances solved choco  using lex
   

fimalapert  regin    rezgui

w     

instances

w      

eps

cc        
costasarray   
crossword m c 
crossword m  
knights     
knights     
knights     
langford     
langford     
langford     
latinsquare dg  
lemma       mod
ortholatin  
pigeons   
quasigroup    
queenattacking  
ruler       a 
ruler       a 
series   
squares    

eps

ws



su



su



su



su


    
     
     
      
      
      
       
      
       
     
   
     
     
    
       
    
     
    
    


    
    
    
 
 
    
 
 
 
    
    
    
    
    
 
    
    
    
    


    
     
     
       

      
       
       

     
   
     
      
    
       
     
     
    
    


    
   
    
 

   
 
 

    
   
    
   
    
 
    
    
   
   


   
    
    
     
     
    
     
    
     
    
   
    
    
   
      
   
   
   
   


     
     
     
 
 
     
 
 
 
     
    
     
     
     
 
     
     
     
     


    
    
    
      

     
      
      

     
   
    
      
   
      
    
    
   
   


    
    
    
 

    
 
 

    
    
    
   
    
 
    
    
    
    

      

    

      

   

      

     

      

    

 t  gm  su 
borda score  rank 
 

ws

        

crossword m  words      

 

        

crossword m c words vg    ext

table    speedups solving times xcsp  gecode  lex  data center  w           
heuristics whereas instances solved sequential parallel using dom wdeg
dom bwdeg  again  highlights importance search strategy 
figure   boxplot speedups different numbers workers solving fzn
instances  median speedups around w  average dispersion remains
low 
   
   

speedup  su 

   
  
  
  
 
 
 

  

  

  

   

   

   

workers  w 

figure    scalability     workers  gecode  lex  data center  
   

fiembarrassingly parallel search cp

instance

eps

market split s    
market split s    
market split u    
pop stress     
nmseq    
pop stress     
fillomino   
steiner triples   
nmseq    
golombruler   
cc base mzn rnd test   
ghoulomb       
still life free  x 
bacp  
depot placement st    
open stacks    wbp        
bacp   
still life still life  
talent scheduling alt film   
 t  gm  su 

ws



su



su

     
     
     
     
     
     
     
     
     

    
    
    
    
   
    
    
    
   

     
     
     
      
     
     
     
     
     

    
    
    
   
   
   
    
   
   

     
      
     
     
     
     
     
     
     
    

    
   
   
   
    
    
    
    
    
    

     
      
      
      
     
      
     
     
     
     

    
   
   
   
   
   
    
   
    
    

     

    

     

   

table    solving times speedups fzn  gecode  lex  cloud  w       

    cloud computing
eps deployed microsoft azure cloud platform  available computing
infrastructure organized follows  cluster nodes computes application  one head
node manages cluster nodes  proxy nodes load balances communication
cluster nodes  contrary data center  cluster nodes may far
communication time may take longer  proxy nodes requires   cores managed
service provider  here    nodes   cores    gb ram memory provide   
workers  cluster nodes  managed mpi 
table   compares gecode implementations eps work stealing solving
fzn instances    workers  briefly  eps always faster work stealing 
therefore  efficient parallelize sequential solver  work
stealing suffers higher communication overhead cloud data center 
furthermore  architecture computing infrastructure location cluster
nodes mostly unknown forbid improvements work stealing
proposed machado et al          xie davenport        
    embarrassingly distributed search
section  transform reasonable effort parallel solver  eps  distributed
parallel solver  edps  using batch scheduler oar  capit et al         provided
   

fimalapert  regin    rezgui

data center  fact  batch scheduler oar plays foreman  parallel choco 
solver modified workers write subproblems files instead solving
them  then  script submits jobs subproblems oar batch scheduler  waits
termination  gathers results  oar schedules jobs cluster using
priority fifo backfilling fair share based priorities  backfilling allows start
lower priority jobs without delaying highest priority jobs whereas fair share means
user application preferred way  main drawback new worker must
created subproblem  worker process allocated oar predefined
resources  worker either sequential  eds  parallel solver  edps  
approach offers practical advantage resource reservation data center 
indeed  asking mpi process  one wait enough resources available
process starts  here  resources  cores nodes  nibbled soon
become available drastically reduce waiting time  furthermore  bypasses
limitations threads technology allowing use multiple nodes data center 
however  clearly increases recomputation overhead because  worker solves single
subproblem instead multiple subproblems  so  model creation initial propagation
realized often  introduces non negligible submission overhead
time taken create submit jobs oar batch scheduler 
      anomaly crossword m c words vg    ext
investigate low speedups solving instance crossword m c words vg   
variable selection heuristic  see table     compare results parallel
 eps  w       distributed  eds sequential worker  algorithms different decomposition depths  d             table   gives solving times  speedups  efficiencies 
number distinct cores used distributed algorithm bad estimator computing efficiencies  used short period time  therefore 
number c cores used compute efficiency eds edps estimated
ratio total runtime wall clock time 
first  parallel algorithm always slower sequential one  however 
speedups distributed algorithms significant even decrease quickly
decomposition depth increases  fall efficiency shows eds scalable
sequential workers  indeed  recomputation  especially submission overhead
become important number subproblems increases 
second  bad performance parallel algorithms caused statistically
imbalanced decomposition would observe similar performance distributed
algorithm  profiling parallel algorithm particular instances suggests bad
eds


p

 
 
 

   
   
    

eps  w      



su

eff



su

eff

    
     
     

    
   
   

     
     
     

      
      
      

   
   
   

     
     
     

table    eds eps crossword instance  choco   dom  data center  
   

fiembarrassingly parallel search cp

performance comes underlying solver itself  indeed  number instructions
similar sequential parallel algorithms whereas numbers context switches 
cache references cache misses increase considerably  fact  parallel algorithms
spent half time internal methods extensional constraints  i e 
relation constraint specified listing satisfying tuples  issue occurred
computing infrastructure different java virtual machines  note instances
use extensional constraints  impose fewer consequences  issue would
happen mpi implementation shared memory  so  advocates
implementations eps based mpi rather thread technology 
      variations golomb ruler problem
golomb ruler set marks integer positions along imaginary ruler
two pairs marks distance apart  number marks ruler
order  largest distance two marks length  here  enumerate
optimal rulers  minimal length specific number marks  simple constraint
model inspired one galinier  jaumard  morales  pesant       
heuristics lex dom reasonable choice  table   gives solving times  speedups 
efficiencies parallel algorithm  w        distributed algorithm sequential
workers  w       distributed algorithms parallel workers  w     
worker decomposition depth dw      different master decomposition depths d 
first  eps obtains almost linear speedup decomposition depth large enough 
without surprise  speedups lower enough subproblems  second 
distributed algorithm eds sequential workers efficient number subproblems remains low  otherwise  still give speedups  dom   wastes
resources since efficiency low  fact  submitting many jobs batch
scheduler  lex  lead high submission overhead  around    minutes  globally degrades performance  finally  distributed algorithms parallel workers offer
good trade off speedups efficiencies allows use many resources
submitting jobs thus reducing submission recomputation overheads  note eds     tested roughly equivalent eps
   workers  edps     tested submission overhead becomes
important 

edps  w       dw     

eds


p



su

eff



su

lex

 
 
 

  
   
     


     
       


    
   


     
     

     
     


    
     


dom

 
 
 

  
   
    


      
      


    
    


     
     

      
     


    
     


eps  w      


su

eff

     
     


       
      
      

   
    
    

     
     
     

     
     


       
      
      

   
    
    

     
     
     

eff

table    eds eps golomb ruler    marks  choco   data center  
   

fimalapert  regin    rezgui

parallel approaches reported good performance golomb ruler problem  instance  michel et al          chu et al         respectively reported linear
speedups     workers  eds efficient work stealing proposed
menouer le cun        using    workers ruler    marks efficient
selfsplit fischetti et al         using    workers ruler    marks 
last  enumerated optimal golomb rulers       marks using edps 
master workers use lex heuristic  master decomposition depth equal
  generates around     hundreds subproblems     parallel workers
decomposition depth dw equal    settings  used     cores
data center solving process  so  bypasses limitations number
cores used mpi imposed administrator  furthermore  solving process starts
immediately cores grabbed soon become available whereas mpi
process waits enough cores becomes simultaneously available  enumerating optimal
rulers       marks respectively took           seconds  knowledge 
first time constraint solver finds rulers  furthermore reasonable amount time  however  optimal rulers discovered via exhaustive
computer search  shearer         recently  distributed computing technologies inc
     found optimum rulers    marks  beside  plane construction  atkinson   hassenklover        allows find larger optimal rulers 
    comparison portfolios
portfolio approaches exploit variability performance observed several
solvers  several parameter settings solver  use   portfolios  portfolio
cphydra  omahony et al         uses features selection top solvers mistral 
gecode  choco   cphydra uses case based reasoning determine solve
unseen problem instance exploiting case base problem solving experience  aims
find feasible solution within    minutes  handle optimization solution problems time limit hard coded  static fixed size portfolios
 choco   cag  or tools  use different variable selection heuristics  see section      well
randomization restarts  details choco  cag found  malapert  
lecoutre         cag portfolio extends choco  portfolio using solvers
abscon gecode  so  cag always produces better results choco   or tools
portfolio gold medal minizinc challenge            seem unfair
compare parallel solvers portfolios using different numbers workers  designing
scalable portfolio  up     workers  difficult task almost implementation
publicly available 
table   gives solving times eps portfolios solving xcsp instances
data center  first  cphydra    workers solves   among    unsatisfiable instances
 cc         pigeons      less   seconds whereas difficult
approaches  or tools second less efficient approach solves fewer
problems often takes longer confirmed low borda score  parallel choco 
using dom wdeg better average choco  portfolio even portfolio solves
instances much faster scen   f  queensknights      mul  case 
diversification provided portfolio outperforms speedups offered parallel
   

fiembarrassingly parallel search cp

instances

eps
choco 

cc        
costasarray   
crossword m  words      
crossword m c words vg    ext
fapp         
knights     
knights     
knights     
langford     
langford     
langford     
latinsquare dg  
lemma       mod
ortholatin  
pigeons   
quasigroup    
queenattacking  
queensknights      mul
ruler       a 
ruler       a 
scen   f 
series   
squares    
squaresunsat      
arithmetic mean
borda score  rank 

portfolio

gecode

choco 

cag

or tools

w     

w     

w      

w     

w     

w     

      
     
     
      
      
     
      
      
      
     
      
     
     
     
      
     
     
      
    
      

     
     
      


    
     
     

      
      
      
       
      
       
     
   
     
     
    
       

    
     

    
    



   
    
    

     
     
    
     
    
     
    
   
    
    
   
      

   
   

   
   


      
      
     
     
    
      
      
      
      
      
       
    
     
      
       
      
      
     
     
      
    
      
      
      

   
     
     
     
   
   
   
     
      
      
       
    
    
      
      
     
      
   
     
      
   
     
     
      

      
      
       
       



       



      
    
      
       
     


      


     
     


      

      

     

      

      

      

        

        

        

        

        

        

table    solving times eps portfolio  data center  
b b algorithm  emphasized cag portfolio solves instances
obtains several best solving times  parallel gecode    workers often slower
less robust portfolios choco  cag  however  increasing number
workers     clearly makes fastest solver  still less robust five instances
solved within time limit 
conclude  choco  cag portfolios robust thanks inherent diversification  solving times vary one instance another     workers 
implementations eps outperform cphydra or tools portfolio  competitive
choco  portfolio  slightly dominated cag portfolio  fact 
good scaling eps key beat portfolios 

   conclusion
introduced embarrassingly parallel search  eps  method solving constraint
satisfaction problems constraint optimization problems  approach several
advantages  first  efficient method matches even outperforms state of the   

fimalapert  regin    rezgui

art algorithms number problems using various computing infrastructures  second 
involves almost communication synchronization mostly relies underlying
sequential solver implementation debugging made easier  last 
simplicity method allows propose many variants adapted specific applications
computing infrastructures  moreover  certain restrictions  parallel algorithm
deterministic  even mimic sequential algorithm important
practice either production debugging 
several interesting perspectives around eps  first  modified order
provide diversification learn useful information solving subproblems 
instance  easily combined portfolio approach subproblems
solved several search strategies  second  thanks simplicity  simplest variants
eps could implemented meta searches  rendl  guns  stuckey    tack        
would offer convenient way parallelize applications satisfactory efficiency  last 
another perspective predict solution time large combinatorial problem  based
known solution times small set subproblems based statistical machine
learning approaches 

acknowledgments
would thank much christophe lecoutre  laurent perron  youssef hamadi 
carine fedele  bertrand lecun tarek menouer comments advices
helped improve paper  work supported cnrs oseo
 bpi france  within isi project pajero  work granted access hpc
visualization resources centre de calcul interactif hosted universite nice sophia
antipolis  microsoft azure cloud  wish thank anonymous
referees comments 

appendix a  efficiency hyper threading
section  show hyper threading technology improves efficiency eps
solving instances xcsp  multi core computer  figure   boxplot
speedups provided hyper threading parallel solver among choco   gecode 
or tools  here  speedups indicate many times parallel solver using    workers
 w    c  faster one using    workers  w   c   maximum speedup according
amdahls law   
choco  tested lex dom whereas gecode or tools use lex 
compared work stealing approach proposed schulte        denoted
gecode ws  hyper threading clearly improves parallel efficiency eps whereas
performance work stealing roughly remains unchanged  interesting
eps high cpu demand resources physical core shared
two logical cores  indeed  performance hyper threading known
application dependent  exception lemma       mod squares      choco 
or tools faster    workers  lemma       mod  choco  decomposition
   workers takes longer generates many subproblems  instance solved
   

fiembarrassingly parallel search cp

hyperthreading speedup

 

 

   

choco  lex

choco  dom

gecode

or tools

gecode ws

figure    speedups provided hyper threading  multi core  w           
easily or tools  less two seconds  becomes difficult improve efficiency  squares      decomposition changes according number workers 
cannot explain hyper threading improve eps  parallel efficiency
gecode reduced multiple instances interest hyper threading less obvious
choco  or tools  conclude  hyper threading globally improves efficiency
eps limited interest work stealing 

references
almasi  g  s     gottlieb  a          highly parallel computing  benjamin cummings
publishing co   inc   redwood city  ca  usa 
amadini  r   gabbrielli  m     mauro  j          empirical evaluation portfolios
approaches solving csps gomes  c     sellmann  m eds   integration
ai techniques constraint programming combinatorial optimization
problems  vol       lecture notes computer science  pp          springer
berlin heidelberg 
amdahl  g          validity single processor approach achieving large scale
computing capabilities proceedings april              spring joint computer conference  afips     pp          new york  ny  usa  acm 
anderson  d  p   cobb  j   korpela  e   lebofsky  m     werthimer  d          seti home 
experiment public resource computing commun  acm                
atkinson  m  d     hassenklover  a          sets integers distinct differences tech 
rep  scs tr     school computer science  carlton university  ottawa ontario 
canada 
bader  d   hart  w     phillips  c          parallel algorithm design branch
bound g  h ed   tutorials emerging methodologies applications operations research  vol     international series operations research   management
science  pp         springer new york 
   

fimalapert  regin    rezgui

barney  b     livermore  l          introduction parallel computing
computing llnl gov tutorials parallel comp  

https   

bauer  m  a          high performance computing  software challenges proceedings
     international workshop parallel symbolic computation  pasco    
pp        new york  ny  usa  acm 
beck  c   prosser  p     wallace  r          trying fail first recent advances
constraints  pp        springer berlin heidelberg 
bordeaux  l   hamadi  y     samulowitz  h          experiments massively parallel
constraint solving  boutilier  boutilier         pp         
boussemart  f   hemery  f   lecoutre  c     sais  l          boosting systematic search
weighting constraints proceedings   th eureopean conference artificial intelligence  ecai      including prestigious applicants intelligent systems 
pais  pp         
boutilier  c ed           ijcai       proceedings   st international joint conference
artificial intelligence  pasadena  california  usa  july       
brams  s  j     fishburn  p  c          voting procedures arrow  k  j   sen  a  k  
  suzumura  k eds   handbook social choice welfare  vol    handbook
social choice welfare  chap     pp          elsevier 
budiu  m   delling  d     werneck  r          dryadopt  branch and bound distributed
data parallel execution engines parallel distributed processing symposium
 ipdps        ieee international  pp            ieee 
burton  f  w     sleep  m  r          executing functional programs virtual tree
processors proceedings      conference functional programming
languages computer architecture  fpca     pp          new york  ny  usa 
acm 
capit  n   da costa  g   georgiou  y   huard  g   martin  c   mounie  g   neyron  p    
richard  o          batch scheduler high level components proceedings
fifth ieee international symposium cluster computing grid  ccgrid      volume     volume     ccgrid     pp          washington  dc  usa 
ieee computer society 
choco  t          choco  open source java constraint programming library ecole des
mines de nantes  research report          
chong  y  l     hamadi  y          distributed log based reconciliation proceedings
     conference ecai         th european conference artificial intelligence august    september          riva del garda  italy  pp          amsterdam 
netherlands  netherlands  ios press 
chu  g   schulte  c     stuckey  p  j          confidence based work stealing parallel constraint programming gent  i  p ed   cp  vol       lecture notes
computer science  pp          springer 
chu  g   stuckey  p  j     harwood  a          pminisat  parallelization minisat
    tech  rep   nicta   national ict australia 
   

fiembarrassingly parallel search cp

cire  a  a   kadioglu  s     sellmann  m          parallel restarted search proceedings
twenty eighth aaai conference artificial intelligence  aaai    pp     
     aaai press 
cornuejols  g   karamanov  m     li  y          early estimates size branchand bound trees informs journal computing           
crainic  t  g   le cun  b     roucairol  c          parallel branch and bound algorithms
parallel combinatorial optimization         
de kergommeaux  j  c     codognet  p          parallel logic programming systems acm
computing surveys  csur                  
distributed computing technologies inc       distributed net home page http   
www distributed net  
een  n     sorensson  n          minisat  sat solver conflict clause minimization
sat       
ezzahir  r   bessiere  c   belaissaoui  m     bouyakhf  e  h          dischoco  platform
distributed constraint programming dcr    eighth international workshop
distributed constraint reasoning   conjunction ijcai    pp        hyderabad  india 
fischetti  m   monaci  m     salvagnin  d          self splitting workload parallel
computation simonis  h ed   integration ai techniques constraint
programming    th international conference  cpaior       cork  ireland  may             proceedings  pp          cham  springer international publishing 
gabriel  e   fagg  g   bosilca  g   angskun  t   dongarra  j   squyres  j   sahay  v   kambadur  p   barrett  b   lumsdaine  a   et al          open mpi  goals  concept 
design next generation mpi implementation recent advances parallel
virtual machine message passing interface  pp         springer 
galea  fran c     le cun  b          bob     framework exact combinatorial
optimization methods parallel machines international conference high performance computing   simulation       hpcs    conjunction   st
european conference modeling simulation  ecms        pp         
galinier  p   jaumard  b   morales  r     pesant  g          constraint based approach
golomb ruler problem  rd international workshop integration ai
techniques 
gendron  b     crainic  t  g          parallel branch and bound algorithms  survey
synthesis operations research                   
gent  i     walsh  t          csplib  benchmark library constraints proceedings  th international conference principles practice constraint
programming  cp     pp         
gomes  c     selman  b          algorithm portfolio design  theory vs  practice
proceedings thirteenth conference uncertainty artificial intelligence  pp 
       
   

fimalapert  regin    rezgui

gomes  c     selman  b          search strategies hybrid search spaces tools
artificial intelligence        proceedings    th ieee international conference  pp 
        ieee 
gomes  c     selman  b          hybrid search strategies heterogeneous search spaces
international journal artificial intelligence tools           
gomes  c     selman  b          algorithm portfolios artificial intelligence            
gropp  w     lusk  e          mpi communication library  design portable
implementation scalable parallel libraries conference         proceedings the 
pp          ieee 
gupta  g   pontelli  e   ali  k  a   carlsson  m     hermenegildo  m  v          parallel
execution prolog programs  survey acm transactions programming languages
systems  toplas                  
halstead  r          implementation multilisp  lisp multiprocessor proceedings
     acm symposium lisp functional programming  lfp     pp 
     new york  ny  usa  acm 
hamadi  y          optimal distributed arc consistency constraints            
hamadi  y   jabbour  s     sais  l          manysat  parallel sat solver  journal
satisfiability  boolean modeling computation                
haralick  r     elliott  g          increasing tree search efficiency constraint satisfaction problems artificial intelligence                 
harvey  w  d     ginsberg  m  l          limited discrepancy search proceedings
fourteenth international joint conference artificial intelligence  ijcai    
montreal quebec  canada  august               volumes  pp         
heule  m  j   kullmann  o   wieringa  s     biere  a          cube conquer  guiding
cdcl sat solvers lookaheads hardware software  verification testing 
pp        springer 
hirayama  k     yokoo  m          distributed partial constraint satisfaction problem
principles practice constraint programming cp    pp          springer 
hyde  p          java thread programming  vol     sams 
intel corporation         intel mpi library https   software intel com en us intel
 mpi library 
jaffar  j   santosa  a  e   yap  r  h  c     zhu  k  q          scalable distributed depthfirst search greedy work stealing   th ieee international conference
tools artificial intelligence  pp         ieee computer society 
kale  l     krishnan  s          charm    portable concurrent object oriented system
based c    vol      acm 
kasif  s          parallel complexity discrete relaxation constraint satisfaction networks artificial intelligence             
kautz  h   horvitz  e   ruan  y   gomes  c     selman  b          dynamic restart policies
  th national conference artificial intelligence aaai iaai             
   

fiembarrassingly parallel search cp

kjellerstrand  h          hakan kjellerstrands blog http   www hakank org  
kleiman  s   shah  d     smaalders  b          programming threads  sun soft press 
korf  r  e     schreiber  e  l          optimally scheduling small numbers identical
parallel machines borrajo  d   kambhampati  s   oddi  a     fratini  s eds  
icaps  aaai 
krishna  j   balaji  p   lusk  e   thakur  r     tiller  f          implementing mpi
windows  comparison common approaches unix recent advances
message passing interface  vol       lecture notes computer science  pp 
        springer berlin heidelberg 
lai  t  h     sahni  s          anomalies parallel branch and bound algorithms commun  acm                 
lantz  e          windows hpc server   using microsoft message passing interface  msmpi  
le cun  b   menouer  t     vander swalmen  p          bobpp http   forge prism
 uvsq fr projects bobpp 
leaute  t   ottens  b     szymanek  r          frodo      open source framework
distributed constraint optimization  boutilier  boutilier         pp         
leiserson  c  e          cilk   concurrency platform journal supercomputing 
               
lester  b          art parallel programming  prentice hall englewood cliffs  nj 
li  h          introducing windows azure  apress  berkely  ca  usa 
luby  m   sinclair  a     zuckerman  d          optimal speedup las vegas algorithms
inf  process  lett              
machado  r   pedro  v     abreu  s          scalability constraint programming
hierarchical multiprocessor systems icpp  pp          ieee 
malapert  a     lecoutre  c          propos de la bibliotheque de modeles xcsp
  emes journees francophones de programmation par contraintes jfpc    
angers  france 
mattson  t   sanders  b     massingill  b          patterns parallel programming  first
ed    addison wesley professional 
menouer  t     le cun  b          anticipated dynamic load balancing strategy parallelize constraint programming search      ieee   th international symposium
parallel distributed processing workshops phd forum  pp           
menouer  t     le cun  b          adaptive n p portfolio solving constraint
programming problems top parallel bobpp framework      ieee   th
international symposium parallel distributed processing workshops phd
forum 
michel  l   see  a     hentenryck  p  v          transparent parallelization constraint
programming informs journal computing             
   

fimalapert  regin    rezgui

microsoft corporation         microsoft hpc pack      r  hpc pack      http   
technet microsoft com en us library jj       aspx 
milano  m     trick  m          constraint integer programming  toward unified
methodology  springer us  boston  ma 
moisan  t   gaudreault  j     quimper  c  g          parallel discrepancy based search
principles practice constraint programming  vol       lecture notes
computer science  pp        springer berlin heidelberg 
moisan  t   quimper  c  g     gaudreault  j          parallel depth bounded discrepancy
search simonis  h ed   integration ai techniques constraint programming    th international conference  cpaior       cork  ireland  may       
      proceedings  pp          cham  springer international publishing 
mpi ch team         high performance portable mpi http   www mpich org  
mueller  f   et al          library implementation posix threads unix 
usenix winter  pp       
nguyen  t     deville  y          distributed arc consistency algorithm science
computer programming                    concurrent constraint programming 
nicta optimisation research group         minizinc flatzinc http   www g  
 csse unimelb edu au minizinc  
nielsen  m          parallel search gecode masters thesis  kth royal institute
technology 
omahony  e   hebrard  e   holland  a   nugent  c     osullivan  b          using casebased reasoning algorithm portfolio constraint solving irish conference
artificial intelligence cognitive science  pp         
pedro  v   abreu  s   pedro  v     abreu  s          distributed work stealing constraint solving corr  abs                
perron  l          search procedures parallelism constraint programming principles practice constraint programming cp     th international conference 
cp    alexandria  va  usa  october              proceedings  pp          berlin 
heidelberg  springer berlin heidelberg 
perron  l   nikolaj  v  o     vincent  f          or tools tech  rep   google 
pruul  e   nemhauser  g     rushmeier  r          branch and bound parallel computation  historical note operations research letters          
refalo  p          impact based search strategies constraint programming wallace 
m ed   principles practice constraint programming    th international conference  cp       toronto  canada  vol       lecture notes computer science 
pp          springer 
regin  j  c   rezgui  m     malapert  a          embarrassingly parallel search principles practice constraint programming    th international conference  cp
      uppsala  sweden  september              proceedings  pp          springer
berlin heidelberg  berlin  heidelberg 
   

fiembarrassingly parallel search cp

regin  j  c   rezgui  m     malapert  a          improvement embarrassingly parallel search data centers osullivan  b ed   principles practice constraint
programming    th international conference  cp       lyon  france  september            proceedings  vol       lecture notes computer science  pp         
springer international publishing  cham 
rendl  a   guns  t   stuckey  p     tack  g          minisearch  solver independent
meta search language minizinc pesant  g   pesant  g     pesant  g eds  
principles practice constraint programming    st international conference 
cp       cork  ireland  august    september          proceedings  vol      
lecture notes computer science  pp          springer international publishing 
cham 
rezgui  m   regin  j  c     malapert  a          using cloud computing solving
constraint programming problems first workshop cloud computing optimization  conference workshop cp       lyon  france 
rolf  c  c     kuchcinski  k          parallel consistency constraint programming
pdpta          international conference parallel distributed processing techniques applications            
rossi  f   van beek  p     walsh  t eds           handbook constraint programming 
elsevier 
roussel  o     lecoutre  c          xml representation constraint networks format
http   www cril univ artois fr cpai   xcsp   competition pdf 
schulte  c          parallel search made simple proceedings trics  techniques
implementing constraint programming systems  post conference workshop
cp       pp        singapore 
schulte  c          gecode  generic constraint development environment http   www
 gecode org  
shearer  j  b          new optimum golomb rulers ieee trans  inf  theor          
       
stephan  k     michael  k          sartagnan   parallel portfolio sat solver
lockless physical clause sharing pragmatics sat 
sutter  h     larus  j          free lunch over  fundamental turn toward toward
concurrency dr  dobbs journal             
van der tak  p   heule  m  j     biere  a          concurrent cube and conquer theory
applications satisfiability testingsat       pp          springer 
vidal  v   bordeaux  l     hamadi  y          adaptive k parallel best first search 
simple efficient algorithm multi core domain independent planning
proceedings third international symposium combinatorial search  aaai
press 
wahbi  m   ezzahir  r   bessiere  c     bouyakhf  e  h          dischoco    platform
distributed constraint reasoning proceedings ijcai   workshop
distributed constraint reasoning  dcr    pp          barcelona  catalonia  spain 
   

fimalapert  regin    rezgui

wilkinson  b     allen  m          parallel programming  techniques application
using networked workstations parallel computers   nd ed    prentice hall inc 
xie  f     davenport  a          massively parallel constraint programming supercomputers  challenges initial results integration ai techniques
constraint programming combinatorial optimization problems   th international conference  cpaior       bologna  italy  june              proceedings  vol 
     lecture notes computer science  pp          berlin  heidelberg  springer
berlin heidelberg 
xu  l   hoos  h     leyton brown  k          hydra  automatically configuring algorithms portfolio based selection aaai conference artificial intelligence 
vol      pp         
xu  l   hutter  f   hoos  h     leyton brown  k          satzilla  portfolio based algorithm selection sat journal artificial intelligence research             
yokoo  m   ishida  t     kuwabara  k          distributed constraint satisfaction dai
problems proceedings      distributed ai workshop  bandara  tx 
zoeteweij  p     arbab  f          component based parallel constraint solver de
nicola  r   ferrari  g  l     meredith  g eds   coordination  vol       lecture
notes computer science  pp          springer 

   


