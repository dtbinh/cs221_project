journal artificial intelligence research                 

submitted       published      

primer neural network models
natural language processing
yoav goldberg

yoav goldberg gmail com

computer science department
bar ilan university  israel

abstract
past years  neural networks re emerged powerful machine learning
models  yielding state of the art results fields image recognition speech
processing  recently  neural network models started applied textual
natural language signals  promising results  tutorial surveys neural
network models perspective natural language processing research  attempt
bring natural language researchers speed neural techniques  tutorial
covers input encoding natural language tasks  feed forward networks  convolutional
networks  recurrent networks recursive networks  well computation graph
abstraction automatic gradient computation 

   introduction
decade  core nlp techniques dominated machine learning approaches
used linear models support vector machines logistic regression  trained
high dimensional yet sparse feature vectors 
recently  field seen success switching linear models
sparse inputs non linear neural network models dense inputs 
neural network techniques easy apply  sometimes almost drop in replacements
old linear classifiers  many cases strong barrier entry  tutorial
attempt provide nlp practitioners  as well newcomers  basic background 
jargon  tools methodology allow understand principles behind
neural network models apply work  tutorial expected
self contained  presenting different approaches unified notation
framework  repeats lot material available elsewhere  points
external sources advanced topics appropriate 
primer intended comprehensive resource go
develop next advances neural network machinery  though may serve good entry
point   rather  aimed readers interested taking existing  useful
technology applying useful creative ways favourite nlp problems 
in depth  general discussion neural networks  theory behind them  advanced
optimization methods advanced topics  reader referred existing
resources  particular  book bengio  goodfellow  courville        highly
recommended 
c
    
ai access foundation  rights reserved 

figoldberg

    scope
focus applications neural networks language processing tasks  however 
subareas language processing neural networks deliberately left
scope tutorial  include vast literature language modeling acoustic
modeling  use neural networks machine translation  multi modal applications
combining language signals images videos  e g  caption generation  
caching methods efficient runtime performance  methods efficient training large
output vocabularies attention models discussed  word embeddings
discussed extent needed understand order use inputs
models  unsupervised approaches  including autoencoders recursive
autoencoders  fall scope  applications neural networks language
modeling machine translation mentioned text  treatment means
comprehensive 
    note terminology
word feature used refer concrete  linguistic input word  suffix 
part of speech tag  example  first order part of speech tagger  features might
current word  previous word  next word  previous part speech  term input
vector used refer actual input fed neural network classifier 
similarly  input vector entry refers specific value input  contrast
lot neural networks literature word feature overloaded
two uses  used primarily refer input vector entry 
    mathematical notation
use bold upper case letters represent matrices  x  y  z   bold lower case letters
represent vectors  b   series related matrices vectors  for example 
matrix corresponds different layer network   superscript indices
used  w    w     rare cases want indicate power matrix
vector  pair brackets added around item exponentiated   w      w      
unless otherwise stated  vectors assumed row vectors  use  v    v    denote
vector concatenation 
choice use row vectors  right multiplied matrices  xw   b 
somewhat non standard lot neural networks literature use column vectors
left multiplied matrices  wx   b   trust reader able adapt
column vectors notation reading literature  

   choice use row vectors notation inspired following benefits  matches way
input vectors network diagrams often drawn literature  makes hierarchical layered
structure network transparent puts input left most variable rather
nested  results fully connected layer dimensions din dout rather dout din   maps
better way networks implemented code using matrix libraries numpy 

   

fia primer neural networks nlp

   neural network architectures
neural networks powerful learning models  discuss two kinds neural network
architectures  mixed matched feed forward networks recurrent  
recursive networks  feed forward networks include networks fully connected layers 
multi layer perceptron  well networks convolutional pooling
layers  networks act classifiers  different strengths 
fully connected feed forward neural networks  section    non linear learners
can  part  used drop in replacement wherever linear learner used 
includes binary multiclass classification problems  well complex structured prediction problems  section     non linearity network  well
ability easily integrate pre trained word embeddings  often lead superior classification accuracy  series works  managed obtain improved syntactic parsing results
simply replacing linear model parser fully connected feed forward network  straight forward applications feed forward network classifier replacement
 usually coupled use pre trained word vectors  provide benefits ccg
supertagging   dialog state tracking   pre ordering statistical machine translation 
language modeling   iyyer  manjunatha  boyd graber  daume iii        demonstrate
multi layer feed forward networks provide competitive results sentiment classification factoid question answering 
networks convolutional pooling layers  section    useful classification
tasks expect find strong local clues regarding class membership 
clues appear different places input  example  document classification
task  single key phrase  or ngram  help determining topic document
 johnson   zhang         would learn certain sequences words good
indicators topic  necessarily care appear document 
convolutional pooling layers allow model learn find local indicators 
regardless position  convolutional pooling architecture show promising results
many tasks  including document classification   short text categorization   sentiment
classification   relation type classification entities    event detection    paraphrase
identification    semantic role labeling    question answering    predicting box office rev 

   chen manning         weiss  alberti  collins  petrov        pei  ge  chang       
durrett klein       
   lewis steedman       
   henderson  thomson  young       
   de gispert  iglesias  byrne       
   bengio  ducharme  vincent  janvin        vaswani  zhao  fossum  chiang       
   johnson zhang       
   wang  xu  xu  liu  zhang  wang  hao      a 
   kalchbrenner  grefenstette  blunsom        kim       
    zeng  liu  lai  zhou  zhao         dos santos  xiang  zhou       
    chen  xu  liu  zeng  zhao         nguyen grishman       
    yin schutze       
    collobert  weston  bottou  karlen  kavukcuoglu  kuksa       
    dong  wei  zhou  xu       

   

figoldberg

enues movies based critic reviews    modeling text interestingness    modeling
relation character sequences part of speech tags   
natural language often work structured data arbitrary sizes 
sequences trees  would able capture regularities structures 
model similarities structures  many cases  means encoding
structure fixed width vector  pass another statistical
learner processing  convolutional pooling architectures allow us
encode arbitrary large items fixed size vectors capturing salient features 
sacrificing structural information  recurrent  section    
recursive  section     architectures  hand  allow us work sequences
trees preserving lot structural information  recurrent networks  elman 
      designed model sequences  recursive networks  goller   kuchler       
generalizations recurrent networks handle trees  discuss
extension recurrent networks allow model stacks  dyer  ballesteros  ling 
matthews    smith        watanabe   sumita        
recurrent models shown produce strong results language modeling      well sequence tagging    machine translation    dependency parsing   
sentiment analysis    noisy text normalization    dialog state tracking    response generation    modeling relation character sequences part of speech tags   
recursive models shown produce state of the art near state of the art results
constituency   dependency   parse re ranking  discourse parsing    semantic relation
classification    political ideology detection based parse trees    sentiment classification   
target dependent sentiment classification   question answering   
   
   
   
   

   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   

bitvai cohn       
gao  pantel  gamon  he  deng       
dos santos zadrozny       
notable works mikolov  karafiat  burget  cernocky  khudanpur         mikolov 
kombrink  lukas burget  cernocky  khudanpur         mikolov         duh  neubig  sudoh 
tsukada         adel  vu  schultz         auli  galley  quirk  zweig        auli gao
      
irsoy cardie         xu  auli  clark         ling  dyer  black  trancoso  fermandez  amir 
marujo  luis      b 
sundermeyer  alkhouli  wuebker  ney         tamura  watanabe  sumita         sutskever 
vinyals  le        cho  van merrienboer  gulcehre  bahdanau  bougares  schwenk  bengio
     b 
dyer et al          watanabe sumita       
wang  liu  sun  wang  wang      b 
chrupala       
mrksic  seaghdha  thomson  gasic  su  vandyke  wen  young       
sordoni  galley  auli  brockett  ji  mitchell  nie  gao  dolan       
ling et al       b 
socher  bauer  manning  ng       
le zuidema         zhu  qiu  chen  huang      a 
li  li  hovy       
hashimoto  miwa  tsuruoka  chikayama         liu  wei  li  ji  zhou  wang       
iyyer  enns  boyd graber  resnik      b 
socher  perelygin  wu  chuang  manning  ng  potts         hermann blunsom       
dong  wei  tan  tang  zhou  xu       
iyyer  boyd graber  claudino  socher  daume iii      a 

   

fia primer neural networks nlp

   feature representation
discussing network structure depth  important pay attention
features represented  now  think feed forward neural network
function nn x  takes input din dimensional vector x produces dout
dimensional output vector  function often used classifier  assigning input x
degree membership one dout classes  function complex 
almost always non linear  common structures function discussed section   
here  focus input  x  dealing natural language  input x encodes
features words  part of speech tags linguistic information  perhaps
biggest conceptual jump moving sparse input linear models neural network
based models stop representing feature unique dimension  the called
one hot representation  representing instead dense vectors  is  core
feature embedded dimensional space  represented vector space   
embeddings  the vector representation core feature  trained
parameter function nn  figure   shows two approaches feature
representation 
feature embeddings  the values vector entries feature  treated
model parameters need trained together components
network  methods training  or obtaining  feature embeddings discussed later 
now  consider feature embeddings given 
general structure nlp classification system based feed forward neural
network thus 
   extract set core linguistic features f            fk relevant predicting
output class 
   feature interest  retrieve corresponding vector v fi   
   combine vectors  either concatenation  summation combination both 
input vector x 
   feed x non linear classifier  feed forward neural network  
biggest change input  then  move sparse representations
feature dimension  dense representation feature mapped
vector  another difference extract core features feature combinations  elaborate changes briefly 
    dense vectors vs  one hot representations
benefits representing features vectors instead unique ids 
always represent features dense vectors  lets consider two kinds
representations 

    different feature types may embedded different spaces  example  one may represent word
features using     dimensions  part of speech features using    dimensions 

   

figoldberg

figure    sparse vs  dense feature representations  two encodings information  current word dog  previous word the  previous pos tag det 
 a  sparse feature vector  dimension represents feature  feature combinations receive dimensions  feature values binary  dimensionality
high   b  dense  embeddings based feature vector  core feature
represented vector  feature corresponds several input vector entries  explicit encoding feature combinations  dimensionality low 
feature to vector mappings come embedding table 

   

fia primer neural networks nlp

one hot feature dimension 
dimensionality one hot vector number distinct features 

features completely independent one another  feature word
dog dis similar word thinking word cat  
dense feature d dimensional vector 
dimensionality vector d 

model training cause similar features similar vectors information
shared similar features 
one benefit using dense low dimensional vectors computational  majority
neural network toolkits play well high dimensional  sparse vectors 
however  technical obstacle  resolved engineering
effort 
main benefit dense representations generalization power  believe
features may provide similar clues  worthwhile provide representation
able capture similarities  example  assume observed word dog
many times training  observed word cat handful times 
all  words associated dimension  occurrences dog
tell us anything occurrences cat  however  dense vectors representation
learned vector dog may similar learned vector cat  allowing
model share statistical strength two events  argument assumes
good vectors somehow given us  section   describes ways obtaining vector
representations 
cases relatively distinct features category  believe
correlations different features  may use one hot representation  however  believe going correlations different features
group  for example  part of speech tags  may believe different verb
inflections vb vbz may behave similarly far task concerned  may
worthwhile let network figure correlations gain statistical strength
sharing parameters  may case circumstances 
feature space relatively small training data plentiful  wish
share statistical information distinct words  gains made using
one hot representations  however  still open research question 
strong evidence either side  majority work  pioneered collobert   weston 
      collobert et al        chen   manning        advocate use dense  trainable
embedding vectors features  work using neural network architecture sparse
vector encodings see work johnson zhang        
finally  important note representing features dense vectors integral
part neural network framework  consequentially differences
using sparse dense feature representations subtler may appear first 
fact  using sparse  one hot vectors input training neural network amounts
dedicating first layer network learning dense embedding vector
feature based training data  touch section     
   

figoldberg

    variable number features  continuous bag words
feed forward networks assume fixed dimensional input  easily accommodate
case feature extraction function extracts fixed number features  feature
represented vector  vectors concatenated  way  region
resulting input vector corresponds different feature  however  cases number
features known advance  for example  document classification common
word sentence feature   thus need represent unbounded
number features using fixed size vector  one way achieving socalled continuous bag words  cbow  representation  mikolov  chen  corrado    dean 
       cbow similar traditional bag of words representation
discard order information  works either summing averaging embedding
vectors corresponding features   
cbow f         fk    

k
 x
v fi  
k

   

i  

simple variation cbow representation weighted cbow  different
vectors receive different weights 
 
wcbow f         fk     pk

i   ai

k
x

ai v fi  

   

i  

here  feature associated weight ai   indicating relative importance
feature  example  document classification task  feature may correspond
word document  associated weight ai could words tf idf score 
    distance position features
linear distance two words sentence may serve informative feature 
example  event extraction task   may given trigger word candidate
argument word  asked predict argument word indeed argument
trigger  distance  or relative position  trigger argument strong
signal prediction task  traditional nlp setup  distances usually encoded
binning distances several groups  i e                        associating
bin one hot vector  neural architecture  input vector composed
binary indicator features  may seem natural allocate single input entry
distance feature  numeric value entry distance  however 
approach taken practice  instead  distance features encoded similarly
    note v fi  s one hot vectors rather dense feature representations  cbow  eq
   wcbow  eq    would reduce traditional  weighted  bag of words representations 
turn equivalent sparse feature vector representation binary indicator feature
corresponds unique word 
    event extraction task involves identification events predefined set event types 
example identification purchase events terror attack events  event type triggered
various triggering words  commonly verbs   several slots  arguments  needs filled
 i e  purchased  purchased  amount   

   

fia primer neural networks nlp

feature types  bin associated d dimensional vector  distanceembedding vectors trained regular parameters network  zeng et al        
dos santos et al         zhu et al       a  nguyen   grishman        
    feature combinations
note feature extraction stage neural network settings deals extraction core features  contrast traditional linear model based nlp systems
feature designer manually specify core features interest
interactions  e g   introducing feature stating word
x feature stating tag combined feature stating word x tag
sometimes even word x  tag previous word z   combination
features crucial linear models introduce dimensions input 
transforming space data points closer linearly separable 
hand  space possible combinations large  feature designer
spend lot time coming effective set feature combinations  one
promises non linear neural network models one needs define
core features  non linearity classifier  defined network structure 
expected take care finding indicative feature combinations  alleviating need
feature combination engineering 
kernel methods  shawe taylor   cristianini         particular polynomial kernels
 kudo   matsumoto         allow feature designer specify core features 
leaving feature combination aspect learning algorithm  contrast neuralnetwork models  kernels methods convex  admitting exact solutions optimization
problem  however  computational complexity classification kernel methods scales
linearly size training data  making slow practical purposes 
suitable training large datasets  hand  computational
complexity classification using neural networks scales linearly size network 
regardless training data size 
    dimensionality
many dimensions allocate feature  unfortunately  theoretical bounds even established best practices space  clearly  dimensionality
grow number members class  you probably want assign
dimensions word embeddings part of speech embeddings  much
enough  current research  dimensionality word embedding vectors range
   hundreds  and  extreme cases  thousands  since dimensionality vectors direct effect memory requirements processing time  good
rule thumb would experiment different sizes  choose good trade off
speed task accuracy 
    vector sharing
consider case features share vocabulary  example 
assigning part of speech given word  may set features considering
   

figoldberg

previous word  set features considering next word  building input
classifier  concatenate vector representation previous word
vector representation next word  classifier able distinguish two
different indicators  treat differently  two features share
vectors  vector dog previous word vector dog nextword  assign two distinct vectors  this  again  mostly empirical
question  believe words behave differently appear different positions
 e g   word x behaves word previous position  x behaves z
next position  may good idea use two different vocabularies assign
different set vectors feature type  however  believe words behave
similarly locations  something may gained using shared vocabulary
feature types 
    networks output
multi class classification problems k classes  networks output k dimensional
vector every dimension represents strength particular output class 
is  output remains traditional linear models scalar scores items discrete
set  however  see section    k matrix associated output
layer  columns matrix thought dimensional embeddings
output classes  vector similarities vector representations k classes
indicate models learned similarities output classes 
    historical note
representing words dense vectors input neural network popularized bengio
et al         context neural language modeling  introduced nlp tasks
pioneering work collobert  weston colleagues                 using embeddings
representing words arbitrary features popularized following chen
manning        

   feed forward neural networks
section introduces feed forward neural networks  starts popular brain
inspired metaphor triggered them  quickly switches back using mathematical
notation  discuss structure feed forward neural networks  representation
power  common non linearities loss functions 
    brain inspired metaphor
name suggests  neural networks inspired brains computation mechanism 
consists computation units called neurons  metaphor  neuron computational unit scalar inputs outputs  input associated weight 
    work bengio  collobert  weston colleagues popularized approaches 
first use them  earlier authors use dense continuous space vectors representing word inputs
neural networks include lee et al         forcada neco         similarly  continuous space
language models used machine translation already schwenk et al         

   

fia primer neural networks nlp

neuron multiplies input weight  sums   them  applies non linear
function result  passes output  neurons connected other 
forming network  output neuron may feed inputs one neurons 
networks shown capable computational devices  weights set
correctly  neural network enough neurons non linear activation function
approximate wide range mathematical functions  we precise
later  
output
layer

hidden
layer

hidden
layer

input layer

r

r

y 

y 

y 

r

r

r

r

r

r

r

r

x 

x 

x 

x 

r

figure    feed forward neural network two hidden layers 
typical feed forward neural network may drawn figure    circle
neuron  incoming arrows neurons inputs outgoing arrows neurons outputs  arrow carries weight  reflecting importance  not shown   neurons
arranged layers  reflecting flow information  bottom layer incoming
arrows  input network  top most layer outgoing arrows 
output network  layers considered hidden  sigmoid shape
inside neurons middle layers represent non linear function  i e   logistic
function        exa    applied neurons value passing output 
figure  neuron connected neurons next layer called
fully connected layer affine layer 
brain metaphor sexy intriguing  distracting cumbersome
manipulate mathematically  therefore switch using concise mathematical
notation  values row neurons network thought vector 
figure   input layer   dimensional vector  x   layer   dimensional vector  h     fully connected layer thought linear transformation
    summing common operation  functions  max  possible

   

figoldberg

  dimensions   dimensions  fully connected layer implements vector matrix
multiplication  h   xw weight connection ith neuron
input row jth neuron output row wij     values h transformed non linear function g applied value passed
next input  whole computation input output written as   g xw    w 
w  weights first layer w  weights second one 
    mathematical notation
point on  abandon brain metaphor describe networks exclusively
terms vector matrix operations 
simplest neural network perceptron  linear function inputs 
nnperceptron  x    xw   b

   

x rdin   w rdin dout   b rdout
w weight matrix  b bias term    order go beyond linear functions 
introduce non linear hidden layer  the network figure   two layers   resulting
multi layer perceptron one hidden layer  mlp    feed forward neural network
one hidden layer form 
nnmlp   x    g xw    b   w    b 

   

x rdin   w  rdin d    b  rd    w  rd  d    b  rd 
w  b  matrix bias term first linear transformation
input  g non linear function applied element wise  also called non linearity
activation function   w  b  matrix bias term second linear
transform 
breaking down  xw   b  linear transformation input x din dimensions
d  dimensions  g applied d  dimensions  matrix w  together
bias vector b  used transform result d  dimensional output
vector  non linear activation function g crucial role networks ability
represent complex functions  without non linearity g  neural network
represent linear transformations input   
add additional linear transformations non linearities  resulting mlp
two hidden layers  the network figure   form  
nnmlp   x     g    g    xw    b   w    b    w 

   

perhaps clearer write deeper networks using intermediary variables 
    see p
case  denote weight ith input jth neuron h wij   value
hj hj    i   xi wij  
    network figure   include bias terms  bias term added layer adding
additional neuron incoming connections  whose value always   
    see why  consider sequence linear transformations still linear transformation 

   

fia primer neural networks nlp

nnmlp   x   y
h   g    xw    b   
h   g    h  w    b   

   

 h  w 
vector resulting linear transform referred layer  outer most
linear transform results output layer linear transforms result hidden
layers  hidden layer followed non linear activation  cases 
last layer example  bias vectors forced    dropped  
layers resulting linear transformations often referred fully connected 
affine  types architectures exist  particular  image recognition problems benefit
convolutional pooling layers  layers uses language processing 
discussed section    networks several hidden layers said deep
networks  hence name deep learning 
describing neural network  one specify dimensions layers
input  layer expect din dimensional vector input  transform
dout dimensional vector  dimensionality layer taken dimensionality
output  fully connected layer l x    xw   b input dimensionality din
output dimensionality dout   dimensions x   din   w din dout b
  dout  
output network dout dimensional vector  case dout      networks
output scalar  networks used regression  or scoring  considering
value output  binary classification consulting sign output 
networks dout   k     used k class classification  associating
dimension class  looking dimension maximal value  similarly 
output vector entries positive sum one  output interpreted
distribution class assignments  such output normalization typically achieved
applying softmax transformation output layer  see section      
matrices bias terms define linear transformations parameters network  common refer collection parameters   together
input  parameters determine networks output  training algorithm
responsible setting values networks predictions correct  training
discussed section   
    representation power
terms representation power  shown hornik  stinchcombe  white       
cybenko        mlp  universal approximator approximate
desired non zero amount error family functions   include continuous
functions closed bounded subset rn   function mapping finite
    specifically  feed forward network linear output layer least one hidden layer squashing activation function approximate borel measurable function one finite dimensional space
another 

   

figoldberg

dimensional discrete space another  may suggest reason go beyond
mlp  complex architectures  however  theoretical result discuss
learnability neural network  it states representation exists  say
easy hard set parameters based training data specific learning
algorithm   guarantee training algorithm find correct function
generating training data  finally  state large hidden layer
be  indeed  telgarsky        show exist neural networks many layers
bounded size cannot approximated networks fewer layers unless layers
exponentially large 
practice  train neural networks relatively small amounts data using local
search methods variants stochastic gradient descent  use hidden layers
relatively modest sizes  up several thousands   universal approximation theorem
give guarantees non ideal  real world conditions  definitely
benefit trying complex architectures mlp   many cases 
however  mlp  indeed provide strong results  discussion representation power feed forward neural networks  see book bengio et al         section
     

    common non linearities
non linearity g take many forms  currently good theory
non linearity apply conditions  choosing correct non linearity
given task part empirical question  go common nonlinearities literature  sigmoid  tanh  hard tanh rectified linear unit
 relu   nlp researchers experimented forms non linearities
cube tanh cube 

      sigmoid
sigmoid activation function  x           ex    called logistic function 
s shaped function  transforming value x range         sigmoid
canonical non linearity neural networks since inception  currently considered
deprecated use internal layers neural networks  choices listed
prove work much better empirically 

      hyperbolic tangent  tanh 
 x

hyperbolic tangent tanh x    ee x  
activation function s shaped function  trans  
forming values x range        
   

fia primer neural networks nlp

      hard tanh
hard tanh activation function approximation tanh function faster
compute take derivatives of 


  x    
hardtanh x     
   
x  


x
otherwise
      rectifier  relu 
rectifier activation function  glorot  bordes    bengio         known
rectified linear unit simple activation function easy work
shown many times produce excellent results    relu unit clips value x    
   despite simplicity  performs well many tasks  especially combined
dropout regularization technique  see section      
 
 
relu x    max    x   
x

x  
otherwise

   

rule thumb  relu units work better tanh  tanh works better
sigmoid   
    output transformations
many cases  output layer vector transformed  common transformation
softmax  
x  x            xk
e xi
softmax xi     pk
xj
j   e

   

    technical advantages relu sigmoid tanh activation functions
involve expensive to compute functions  importantly saturate  sigmoid
tanh activation capped    gradients region functions near zero 
driving entire gradient near zero  relu activation problem  making
especially suitable networks multiple layers  susceptible vanishing gradients
problem trained saturating units 
    addition activation functions  recent works nlp community experiment
reported success forms non linearities  cube activation function  g x     x    
suggested chen manning         found effective non linearities
feed forward network used predict actions greedy transition based dependency
parser  tanh cube activation function g x    tanh  x     x  proposed pei et al         
found effective non linearities feed forward network used
component structured prediction graph based dependency parser 
cube tanh cube activation functions motivated desire better capture interactions different features  activation functions reported improve performance
certain situations  general applicability still determined 

   

figoldberg

result vector non negative real numbers sum one  making discrete
probability distribution k possible outcomes 
softmax output transformation used interested modeling probability distribution possible output classes  effective  used
conjunction probabilistic training objective cross entropy  see section      
below  
softmax transformation applied output network without hidden
layer  result well known multinomial logistic regression model  known
maximum entropy classifier 
    embedding layers
now  discussion ignored source x  treating arbitrary vector 
nlp application  x usually composed various embeddings vectors 
explicit source x  include networks definition  introduce c   
function core features input vector 
common c extract embedding vector associated feature 
concatenate them 
x   c f    f    f      v f     v f     v f    
nnmlp   x   nnmlp   c f    f    f    
 nnmlp    v f     v f     v f     

    

  g  v f     v f     v f    w    b    w    b 
another common choice c sum embedding vectors  this assumes embedding vectors share dimensionality  
x   c f    f    f     v f      v f      v f   
nnmlp   x   nnmlp   c f    f    f    
 nnmlp   v f      v f      v f    

    

  g  v f      v f      v f    w    b    w    b 
form c essential part networks design  many papers  common
refer c part network  likewise treat word embeddings v fi   resulting
embedding layer lookup layer  consider vocabulary  v   words 
embedded dimensional vector  collection vectors thought
 v   embedding matrix e row corresponds embedded feature  let
 v   dimensional vector  zeros except one index  corresponding
value ith feature  value    this called one hot vector  
multiplication e select corresponding row e  thus  v fi   defined
terms e  
v fi     e
   

    

fia primer neural networks nlp

similarly 
cbow f         fk    

k
x

 fi e     

i  

k
x

 e

    

i  

input network considered collection one hot vectors 
elegant well defined mathematically  efficient implementation typically involves
hash based data structure mapping features corresponding embedding vectors 
without going one hot representation 
tutorial  take c separate network architecture  networks
inputs always dense real valued input vectors  c applied input passed
network  similar feature function familiar linear models terminology  however  training network  input vector x remember constructed 
propagate error gradients back component embedding vectors  appropriate
 error propagation discussed section    
      note notation
describing network layers get concatenated vectors x  z input 
authors use explicit concatenation   x  y  z w  b  others use affine transformation
 xu   yv   zw   b   weight matrices u  v  w affine transformation
different one another  two notations equivalent 
      note sparse vs  dense features
consider network uses traditional sparse representation input vectors 
embedding layer  assuming set available features v k
features f            fk   v   networks input is 
x 

k
x

 v  

x n 



i  

    

first layer  ignoring non linear activation  is 
k
x
xw   b    
 w

    

i  

w r v  d   b rd
layer selects rows w corresponding input features x sums them 
adding bias term  similar embedding layer produces cbow
representation features  matrix w acts embedding matrix 
main difference introduction bias vector b  fact embedding
layer typically undergo non linear activation rather passed directly
first layer  another difference scenario forces feature receive separate
vector  row w  embedding layer provides flexibility  allowing example
features next word dog previous word dog share vector 
   

figoldberg

however  differences small subtle  comes multi layer feed forward
networks  difference dense sparse inputs smaller may seem
first sight 
    loss functions
training neural network  more training section   below   much
training linear classifier  one defines loss function l y  y   stating loss predicting
true output y  training objective minimize loss across
different training examples  loss l y  y  assigns numerical score  a scalar 
networks output given true expected output y    loss function
bounded below  minimum attained cases networks output
correct 
parameters network  the matrices wi   biases bi commonly embeddings e  set order minimize loss l training examples  usually 
sum losses different training examples minimized  
loss arbitrary function mapping two vectors scalar  practical
purposes optimization  restrict functions easily compute
gradients  or sub gradients   cases  sufficient advisable rely common
loss function rather defining own  detailed discussion loss functions
neural networks see work lecun  chopra  hadsell  ranzato  huang         lecun
huang        bengio et al          discuss loss functions
commonly used neural networks nlp 
      hinge  binary 
binary classification problems  networks output single scalar intended
output          classification rule sign y   classification considered
correct      meaning share sign  hinge loss  known
margin loss svm loss  defined as 
lhinge binary   y  y    max      y 

    

loss   share sign  y     otherwise  loss linear 
words  binary hinge loss attempts achieve correct classification 
margin least   
      hinge  multiclass 
hinge loss extended multiclass setting crammer singer         let
  y            yn networks output vector  one hot vector correct
output class 
classification rule defined selecting class highest score 
prediction   arg max yi


    

    notation  models output expected output vectors  many cases
natural think expected output scalar  class assignment   cases  simply
corresponding one hot vector 

   

fia primer neural networks nlp

denote   arg maxi yi correct class  k   arg maxi  t yi highest scoring
class k    t  multiclass hinge loss defined as 
lhinge multiclass   y  y    max       yt yk   

    

multiclass hinge loss attempts score correct class classes
margin least   
binary multiclass hinge losses intended used linear output
layer  hinge losses useful whenever require hard decision rule 
attempt model class membership probability 
      log loss
log loss common variation hinge loss  seen soft version
hinge loss infinite margin  lecun et al         
llog  y  y    log     exp  yt yk   

    

      categorical cross entropy loss
categorical cross entropy loss  also referred negative log likelihood   used
probabilistic interpretation scores desired 
let   y            yn vector representing true multinomial distribution
labels            n  let   y            yn networks output  transformed
softmax activation function  represent class membership conditional distribution
yi   p  y   i x   categorical cross entropy loss measures dissimilarity
true label distribution predicted label distribution y  defined cross
entropy 
lcross entropy  y  y   

x

yi log yi  

    



hard classification problems training example single correct
class assignment  one hot vector representing true class  cases  cross
entropy simplified to 
lcross entropy hard classification   y  y    log yt  

    

correct class assignment  attempts set probability mass assigned
correct class    scores transformed using softmax
function represent conditional distribution  increasing mass assigned correct
class means decreasing mass assigned classes 
cross entropy loss common neural networks literature  produces
multi class classifier predict one best class label predicts
distribution possible labels  using cross entropy loss  assumed
networks output transformed using softmax transformation 
   

figoldberg

      ranking losses
settings  given supervision term labels  rather pairs
correct incorrect items x x    goal score correct items incorrect
ones  training situations arise positive examples  generate
negative examples corrupting positive example  useful loss scenarios
margin based ranking loss  defined pair correct incorrect examples 
lranking margin   x  x      max       nn x  nn x     

    

nn x  score assigned network input vector x  objective
score  rank  correct inputs incorrect ones margin least   
common variation use log version ranking loss 
lranking log   x  x      log     exp  nn x  nn x      

    

examples using ranking hinge loss language tasks include training auxiliary tasks used deriving pre trained word embeddings  see section    
given correct word sequence corrupted word sequence  goal score
correct sequence corrupt one  collobert   weston         similarly  van
de cruys        used ranking loss selectional preferences task  network trained rank correct verb object pairs incorrect  automatically derived
ones  weston  bordes  yakhnenko  usunier        trained model score correct
 head relation trail  triplets corrupted ones information extraction setting 
example using ranking log loss found work gao et al         
variation ranking log loss allowing different margin negative positive
class given work dos santos et al         

   word embeddings
main component neural network approach use embeddings representing
feature vector low dimensional space  vectors come from 
section survey common approaches 
    random initialization
enough supervised training data available  one treat feature embeddings
model parameters  initialize embedding vectors random values 
let network training procedure tune good vectors 
care taken way random initialization performed  method
used effective word vec implementation  mikolov et al         mikolov  sutskever 
chen  corrado    dean        initialize word vectors uniformly sampled random
   
numbers range    d
   d   number dimensions  another option
use xavier
 see section        initialize uniformly sampled values
h initialization

   

   
   

fia primer neural networks nlp

practice  one often use random initialization approach initialize embedding vectors commonly occurring features  part of speech tags individual
letters  using form supervised unsupervised pre training initialize
potentially rare features  features individual words  pre trained vectors
either treated fixed network training process  or  commonly 
treated randomly initialized vectors tuned task hand 
    supervised task specific pre training
interested task a  limited amount labeled data  for
example  syntactic parsing   auxiliary task b  say  part of speech tagging 
much labeled data  may want pre train word vectors
perform well predictors task b  use trained vectors training
task a  way  utilize larger amounts labeled data task b 
training task either treat pre trained vectors fixed  tune
task a  another option train jointly objectives  see section  
details 
    unsupervised pre training
common case auxiliary task large enough amounts
annotated data  or maybe want help bootstrap auxiliary task training better
vectors   cases  resort unsupervised methods  trained huge
amounts unannotated text 
techniques training word vectors essentially supervised learning 
instead supervision task care about  instead create practically
unlimited number supervised training instances raw text  hoping tasks
created match  or close enough to  final task care about   
key idea behind unsupervised approaches one would embedding
vectors similar words similar vectors  word similarity hard define
usually task dependent  current approaches derive distributional
hypothesis  harris         stating words similar appear similar contexts 
different methods create supervised training instances goal either
predict word context  predict context word 
important benefit training word embeddings large amounts unannotated
data provides vector representations words appear supervised training set  ideally  representations words similar
related words appear training set  allowing model generalize better
unseen events  thus desired similarity word vectors learned unsupervised algorithm captures aspects similarity useful performing
intended task network 
    interpretation creating auxiliary problems raw text inspired ando zhang      a 
ando zhang      b  

   

figoldberg

common unsupervised word embedding algorithms include word vec     mikolov et al  
             glove  pennington  socher    manning        collobert weston
             embeddings algorithm  models inspired neural networks
based stochastic gradient training  however  deeply connected another
family algorithms evolved nlp ir communities  based
matrix factorization  for discussion see levy   goldberg      b  levy et al         
arguably  choice auxiliary problem  what predicted  based kind
context  affects resulting vectors much learning method
used train them  thus focus different choices auxiliary problems
available  skim details training methods  several software packages
deriving word vectors available  including word vec   gensim   implementing
word vec models word windows based contexts  word vecf   modified
version word vec allowing use arbitrary contexts  glove   implementing
glove model  many pre trained word vectors available download web 
beyond scope tutorial  worth noting word embeddings
derived unsupervised training algorithms wide range applications nlp
beyond using initializing word embeddings layer neural network model 
    training objectives
given word w context c  different algorithms formulate different auxiliary tasks 
cases  word represented d dimensional vector initialized
random value  training model perform auxiliary tasks well result good
word embeddings relating words contexts  turn result
embedding vectors similar words similar other 
language modeling inspired approaches taken mikolov et al         
mnih kavukcuoglu        well glove  pennington et al         use auxiliary tasks
goal predict word given context  posed probabilistic
setup  trying model conditional probability p  w c  
approaches reduce problem binary classification  addition
set observed word context pairs  set created random words
context pairings  binary classification problem then  given  w  c  pair
come not  approaches differ set constructed 
structure classifier  objective optimized  collobert
weston              take margin based binary ranking approach  training feed forward
neural network score correct  w  c  pairs incorrect ones  mikolov et al              
take instead probabilistic version  training log bilinear model predict probability
p   w  c  d w  c  pair come corpus rather random sample 
    often treated single algorithm  word vec actually software package including various
training objectives  optimization methods hyperparameters  see work rong       
levy  goldberg  dagan        discussion 
    https   code google com p word vec 
    https   radimrehurek com gensim 
    https   bitbucket org yoavgo word vecf
    http   nlp stanford edu projects glove 

   

fia primer neural networks nlp

    choice contexts
cases  contexts word taken words appear
surrounding  either short window around it  within sentence  paragraph
document  cases text automatically parsed syntactic parser 
contexts derived syntactic neighbourhood induced automatic parse
trees  sometimes  definitions words context change include parts words 
prefixes suffixes 
neural word embeddings originated world language modeling 
network trained predict next word based sequence preceding words  bengio
et al          there  text used create auxiliary tasks aim predict
word based context k previous words  training language modeling
auxiliary prediction problems indeed produce useful embeddings  approach needlessly
restricted constraints language modeling task  one allowed look
previous words  care language modeling
resulting embeddings  may better ignoring constraint taking context
symmetric window around focus word 
      window approach
common approach sliding window approach  auxiliary tasks
created looking sequence  k     words  middle word callled focus word
k words side contexts  then  either single task created
goal predict focus word based context words  represented either
using cbow  see mikolov et al        vector concatenation  see collobert   weston 
        k distinct tasks created  pairing focus word different context
word   k tasks approach  popularized mikolov et al         referred
skip gram model  skip gram based approaches shown robust efficient train
 mikolov et al         pennington et al          often produce state art results 
effect window size size sliding window strong effect resulting vector similarities  larger windows tend produce topical similarities  i e 
dog  bark leash grouped together  well walked  run walking   smaller windows tend produce functional syntactic similarities  i e 
poodle  pitbull  rottweiler  walking running approaching  
positional windows using cbow skip gram context representations 
different context words within window treated equally  distinction
context words close focus words farther
it  likewise distinction context words appear focus
words context words appear it  information easily factored
using positional contexts  indicating context word relative position
focus words  i e  instead context word becomes the     indicating
word appears two positions right focus word   use positional context
together smaller windows tend produce similarities syntactic 
strong tendency grouping together words share part speech  well
functionally similar terms semantics  positional vectors shown ling 
   

figoldberg

dyer  black  trancoso      a  effective window based vectors
used initialize networks part of speech tagging syntactic dependency parsing 
variants many variants window approach possible  one may lemmatize words
learning  apply text normalization  filter short long sentences  remove
capitalization  see  e g   pre processing steps described dos santos   gatti        
one may sub sample part corpus  skipping probability creation tasks
windows common rare focus words  window size may
dynamic  using different window size turn  one may weigh different positions
window differently  focusing trying predict correctly close word context
pairs away ones  choices effect resulting vectors 
hyperparameters  and others  discussed levy et al         
      sentences  paragraphs documents
using skip grams  or cbow  approach  one consider contexts word
words appear sentence  paragraph document 
equivalent using large window sizes  expected result word vectors
capture topical similarity  words topic  i e  words one would expect
appear document  likely receive similar vectors  
      syntactic window
work replace linear context within sentence syntactic one  levy  
goldberg      a  bansal  gimpel    livescu         text automatically parsed
using dependency parser  context word taken words
proximity parse tree  together syntactic relation
connected  approaches produce highly functional similarities  grouping together words
fill role sentence  e g  colors  names schools  verbs movement  
grouping syntactic  grouping together words share inflection  levy  
goldberg      a  
      multilingual
another option using multilingual  translation based contexts  hermann   blunsom 
      faruqui   dyer         example  given large amount sentence aligned parallel
text  one run bilingual alignment model ibm model   model    i e 
using giza   software   use produced alignments derive word contexts 
here  context word instance foreign language words aligned it 
alignments tend result synonym words receiving similar vectors  authors
work instead sentence alignment level  without relying word alignments  gouws 
bengio    corrado        train end to end machine translation neural network
use resulting word embeddings  hill  cho  jean  devin    bengio         appealing
method mix monolingual window based approach multilingual approach 
creating kinds auxiliary tasks  likely produce vectors similar
window based approach  reducing somewhat undesired effect window   

fia primer neural networks nlp

based approach antonyms  e g  hot cold  high low  tend receive similar
vectors  faruqui   dyer        
      character based sub word representations
interesting line work attempts derive vector representation word
characters compose it  approaches likely particularly useful tasks
syntactic nature  character patterns within words strongly related
syntactic function  approaches benefit producing small
model sizes  only one vector character alphabet together handful
small matrices needs stored   able provide embedding vector every
word may encountered  dos santos gatti         dos santos zadrozny
       kim et al         model embedding word using convolutional network
 see section    characters  ling et al       b  model embedding word
using concatenation final states two rnn  lstm  encoders  section      one
reading characters left right  right left  produce
strong results part of speech tagging  work ballesteros et al         show
two lstms encoding ling et al       b  beneficial representing words
dependency parsing morphologically rich languages 
deriving representations words representations characters motivated unknown words problem encounter word
embedding vector  working level characters alleviates
problem large extent  vocabulary possible characters much smaller
vocabulary possible words  however  working character level
challenging  relationship form  characters  function  syntax  semantics 
language quite loose  restricting oneself stay character level may
unnecessarily hard constraint  researchers propose middle ground  word
represented combination vector word vectors sub word
units comprise it  sub word embeddings help sharing information
different words similar forms  well allowing back off subword level
word observed  time  models forced rely solely
form enough observations word available  botha blunsom        suggest model embedding vector word sum word specific vector
vector available  vectors different morphological components comprise
 the components derived using morfessor  creutz   lagus         unsupervised
morphological segmentation method   gao et al         suggest using core features
word form unique feature  hence unique embedding vector 
letter trigrams word 

   neural network training
neural network training done trying minimize loss function training set 
using gradient based method  roughly speaking  training methods work repeatedly
computing estimate error dataset  computing gradient respect
error  moving parameters opposite direction gradient 
models differ error estimate computed  moving opposite
   

figoldberg

direction gradient defined  describe basic algorithm  stochastic gradient
descent  sgd   briefly mention approaches pointers
reading  gradient calculation central approach  gradients efficiently
automatically computed using reverse mode differentiation computation graph
general algorithmic framework automatically computing gradient network
loss function  discussed section     
    stochastic gradient training
common approach training neural networks using stochastic gradient descent
 sgd  algorithm  bottou        lecun  bottou  orr    muller      a  variant it 
sgd general optimization algorithm  receives function f parameterized  
loss function  desired input output pairs  attempts set parameters
loss f respect training examples small  algorithm works
follows 
algorithm   online stochastic gradient descent training
   input  function f  x    parameterized parameters  
   input  training set inputs x            xn desired outputs y            yn  
   input  loss function l 
   stopping criteria met
  
sample training example xi   yi
  
compute loss l f  xi      yi  
  
g gradients l f  xi      yi   w r t
  
g
   return
pnthe goal algorithm set parameters minimize total loss
i   l f  xi      yi   training set  works repeatedly sampling training example computing gradient error example respect parameters
 line    input expected output assumed fixed  loss treated
function parameters   parameters updated opposite
direction gradient  scaled learning rate  line     learning rate either
fixed throughout training process  decay function time step t   
discussion setting learning rate  see section     
note error calculated line   based single training example  thus
rough estimate corpus wide loss aiming minimize  noise
loss computation may result inaccurate gradients  common way reducing
noise estimate error gradients based sample examples 
gives rise minibatch sgd algorithm 
lines     algorithm estimates gradient corpus loss based
minibatch  loop  g contains gradient estimate  parameters
updated toward g  minibatch size vary size       n  higher
values provide better estimates corpus wide gradients  smaller values allow
    learning rate decay required order prove convergence sgd 

   

fia primer neural networks nlp

algorithm   minibatch stochastic gradient descent training
   input  function f  x    parameterized parameters  
   input  training set inputs x            xn desired outputs y            yn  
   input  loss function l 
   stopping criteria met
  
sample minibatch examples   x    y              xm   ym   
  
g  
  
   
  
compute loss l f  xi      yi  
 
  
g g   gradients
l f  xi      yi   w r t
g
    return
   

updates turn faster convergence  besides improved accuracy gradients
estimation  minibatch algorithm provides opportunities improved training efficiency 
modest sizes m  computing architectures  i e  gpus  allow efficient parallel
implementation computation lines     properly decreasing learning rate 
sgd guaranteed converge global optimum function convex  however 
used optimize non convex functions neural network 
longer guarantees finding global optimum  algorithm proved robust
performs well practice   
training neural network  parameterized function f neural network 
parameters linear transformation matrices  bias terms  embedding matrices
on  gradient computation key step sgd algorithm  well
neural network training algorithms  question is  then  compute
gradients networks error respect parameters  fortunately 
easy solution form backpropagation algorithm  rumelhart  hinton    williams 
      lecun  bottou  bengio    haffner      b   backpropagation algorithm fancy
name methodically computing derivatives complex expression using chainrule  caching intermediary results  generally  backpropagation algorithm
special case reverse mode automatic differentiation algorithm  neidinger       
section    baydin  pearlmutter  radul    siskind        bengio         following
section describes reverse mode automatic differentiation context computation
graph abstraction 
      beyond sgd
sgd algorithm often produce good results  advanced algorithms available  sgd momentum  polyak        nesterov momentum
 sutskever  martens  dahl    hinton        nesterov              algorithms variants
sgd previous gradients accumulated affect current update  adap    recent work neural networks literature argue non convexity networks manifested proliferation saddle points rather local minima  dauphin  pascanu  gulcehre  cho 
ganguli    bengio         may explain success training neural networks despite
using local search techniques 

   

figoldberg

tive learning rate algorithms including adagrad  duchi  hazan    singer         adadelta
 zeiler         rmsprop  tieleman   hinton        adam  kingma   ba       
designed select learning rate minibatch  sometimes per coordinate basis 
potentially alleviating need fiddling learning rate scheduling  details
algorithms  see original papers book bengio et al         sections           
many neural network software frameworks provide implementations algorithms 
easy sometimes worthwhile try different variants 
    computation graph abstraction
one compute gradients various parameters network hand
implement code  procedure cumbersome error prone  purposes  preferable use automatic tools gradient computation  bengio        
computation graph abstraction allows us easily construct arbitrary networks  evaluate
predictions given inputs  forward pass   compute gradients parameters
respect arbitrary scalar losses  backward pass  
computation graph representation arbitrary mathematical computation
graph  directed acyclic graph  dag  nodes correspond mathematical
operations  bound  variables edges correspond flow intermediary values
nodes  graph structure defines order computation terms
dependencies different components  graph dag tree 
result one operation input several continuations  consider example
graph computation  a b       a b      
 
 

 
 

 



b

 

computation b shared  restrict case computation
graph connected 
since neural network essentially mathematical expression  represented
computation graph 
example  figure  a presents computation graph mlp one hiddenlayer softmax output transformation  notation  oval nodes represent mathematical operations functions  shaded rectangle nodes represent parameters  bound
variables   network inputs treated constants  drawn without surrounding node 
input parameter nodes incoming arcs  output nodes outgoing arcs 
output node matrix  dimensionality indicated
node 
graph incomplete  without specifying inputs  cannot compute output 
figure  b shows complete graph mlp takes three words inputs  predicts
distribution part of speech tags third word  graph used
prediction  training  output vector  not scalar  graph
take account correct answer loss term  finally  graph  c shows
   

fia primer neural networks nlp

  
neg
  
log
  
 a 

 b 

 c 

pick

    

    

    

softmax

softmax

softmax

    

    

    

add

add

add

    

    

    

mul

mul

mul

    
tanh

     
w 

    

    
b 

     
w 

tanh

    

    
b 

    

    

add

add

add

    

    

    

mul

mul

mul

      
w 

     

    
b 

concat

      
w 

     

    

      

    

w 

tanh

    

     
x

 

     

    
b 

concat

w 

    

    

    

    

    

    

lookup

lookup

lookup

lookup

lookup

lookup



black

dog



black

dog

 v     
e

b 

b 

 v     
e

figure    computation graph mlp    a  graph unbound input   b  graph
concrete input   c  graph concrete input  expected output  loss
node 

computation graph specific training example  inputs  embeddings
of  words the  black  dog  expected output noun  whose index
    pick node implements indexing operation  receiving vector index  in
case     returning corresponding entry vector 
graph built  straightforward run either forward computation  compute result computation  backward computation  computing gradients  
show below  constructing graphs may look daunting  actually easy
using dedicated software libraries apis 
      forward computation
forward pass computes outputs nodes graph  since nodes output
depends incoming edges  trivial compute outputs
nodes traversing nodes topological order computing output
node given already computed outputs predecessors 
   

figoldberg

formally  graph n nodes  associate node index according
topological ordering  let function computed node  e g  multiplication 
addition           let  i  parent nodes node i     i     j    j  
children nodes node  these arguments    denote v i  output node
i  is  application output values arguments    i   variable
input nodes  constant function    i  empty  forward algorithm
computes values v i      n   
algorithm   computation graph forward pass
       n
  
let a                 i 
  
v i   v a             v am   

      backward computation  derivatives  backprop 
backward pass begins designating node n scalar      output loss node 
running forward computation node  backward computation computes
n
gradients respect nodes value  denote d i  quantity
 

backpropagation algorithm used compute values d i  nodes i 
backward pass fills table d i  follows 
algorithm   computation graph backward pass  backpropagation 
   d n    
     n    
p
fj
  
d i  j i  d j 


fj
partial derivative fj      j   w r t argument    j  

value depends function fj values v a             v am    where a             
   j   arguments  computed forward pass 

quantity

thus  order define new kind node  one need define two methods  one
calculating forward value v i  based nodes inputs  another calculating

x    i  
x
information automatic differentiation see work neidinger       
section    baydin et al          depth discussion backpropagation
algorithm computation graphs  also called flow graphs  see work bengio et al 
       section       lecun et al       b  bengio         popular yet technical
presentation  see online post olah      a  
   

fia primer neural networks nlp

      software
several software packages implement computation graph model  including theano    
chainer     penne   cnn pycnn     packages support essential components  node types  defining wide range neural network architectures  covering
structures described tutorial more  graph creation made almost transparent
use operator overloading  framework defines type representing graph nodes
 commonly called expressions   methods constructing nodes inputs parameters 
set functions mathematical operations take expressions input result
complex expressions  example  python code creating computation
graph figure   c  using pycnn framework is 
import pycnn pc
  model initialization 
model   pc model  
pw    model add parameters          
pb    model add parameters    
pw    model add parameters         
pb    model add parameters    
words   model add lookup parameters           
  building computation graph 
pc renew cg     create new graph 
  wrap model parameters graph nodes 
w    pc parameter pw  
b    pc parameter pb  
w    pc parameter pw  
b    pc parameter pb  
def get index x   return     place holder
  generate embeddings layer 
vthe
  pc lookup words  get index  the   
vblack   pc lookup words  get index  black   
vdog
  pc lookup words  get index  dog   
  connect leaf nodes complete graph 
x   pc concatenate  vthe  vblack  vdog  
output   pc softmax w   pc tanh w  x  b   b  
loss    pc log pc pick output     
loss value   loss forward  
loss backward     gradient computed
  stored corresponding
  parameters 

code involves various initializations  first block defines model parameters
shared different computation graphs  recall graph corresponds
specific training example   second block turns model parameters graphnode  expression  types  third block retrieves expressions embeddings
   
   
   
   

http   deeplearning net software theano 
http   chainer org
https   bitbucket org ndnlp penne
https   github com clab cnn

   

figoldberg

input words  finally  fourth block graph created  note transparent
graph creation almost one to one correspondence creating
graph describing mathematically  last block shows forward backward
pass  software frameworks follow similar patterns 
theano involves optimizing compiler computation graphs  blessing
curse  one hand  compiled  large graphs run efficiently either
cpu gpu  making ideal large graphs fixed structure 
inputs change instances  however  compilation step costly 
makes interface bit cumbersome work with  contrast  packages focus
building large dynamic computation graphs executing fly without
compilation step  execution speed may suffer respect theanos optimized
version  packages especially convenient working recurrent
recursive networks described sections        well structured prediction settings
described section   
      implementation recipe
using computation graph abstraction  pseudo code network training algorithm
given algorithm   
algorithm   neural network training computation graph abstraction  using minibatches size   
   define network parameters 
   iteration     n
  
training example xi   yi dataset
  
loss node build computation graph xi   yi   parameters 
  
loss node forward  
  
gradients loss node   backward  
  
parameters update parameters parameters  gradients 
   return parameters 
here  build computation graph user defined function builds computation
graph given input  output network structure  returning single loss node 
update parameters optimizer specific update rule  recipe specifies new
graph created training example  accommodates cases network
structure varies training example  recurrent recursive neural networks 
discussed sections        networks fixed structures  mlps 
may efficient create one base computation graph vary inputs
expected outputs examples 
      network composition
long networks output vector    k matrix   trivial compose networks
making output one network input another  creating arbitrary networks 
computation graph abstractions makes ability explicit  node computation
graph computation graph designated output node  one
   

fia primer neural networks nlp

design arbitrarily deep complex networks  able easily evaluate train
thanks automatic forward gradient computation  makes easy define
train networks structured outputs multi objective training  discuss
section    well complex recurrent recursive networks  discussed sections
     
    optimization issues
gradient computation taken care of  network trained using sgd another
gradient based optimization algorithm  function optimized convex 
long time training neural networks considered black art done
selected few  indeed  many parameters affect optimization process  care
taken tune parameters  tutorial intended comprehensive
guide successfully training neural networks  list prominent issues 
discussion optimization techniques algorithms neural networks  refer
book bengio et al         ch      theoretical discussion analysis  refer
work glorot bengio         various practical tips recommendations 
see work lecun et al       a  bottou        
      initialization
non convexity loss function means optimization procedure may get stuck
local minimum saddle point  starting different initial points  e g 
different random values parameters  may result different results  thus 
advised run several restarts training starting different random initializations 
choosing best one based development set    amount variance
results different different network formulations datasets  cannot predicted
advance 
magnitude random values important effect success training 
effective scheme due glorot bengio         called xavier initialization
glorots first name  suggests initializing weight matrix w rdin dout as 


 

 
 
w u
   
din   dout
din   dout
 

    

u  a  b  uniformly sampled random value range  a  b   suggestion
based properties tanh activation function  works well many occasions 
preferred default initialization method many 
analysis et al         suggests using relu non linearities  weights
initialized
sampling zero mean gaussian distribution whose standard
q
 
deviation
din   initialization found et al work better xavier
initialization image classification task  especially deep networks involved 
    debugging  reproducibility results  advised used fixed random seed 

   

figoldberg

      vanishing exploding gradients
deep networks  common error gradients either vanish  become exceedingly
close    explode  become exceedingly high  propagate back computation graph  problem becomes severe deeper networks  especially
recursive recurrent networks  pascanu  mikolov    bengio         dealing
vanishing gradients problem still open research question  solutions include making
networks shallower  step wise training  first train first layers based auxiliary
output signal  fix train upper layers complete network based
real task signal   performing batch normalization  ioffe   szegedy         for every
minibatch  normalizing inputs network layers zero mean unit
variance  using specialized architectures designed assist gradient flow  e g  
lstm gru architectures recurrent networks  discussed section      dealing
exploding gradients simple effective solution  clipping gradients
norm exceeds given threshold  let g gradients parameters
network  kgk l  norm  pascanu et al         suggest set  g threshold
kgk g
kgk   threshold 
      saturation dead neurons
layers tanh sigmoid activations become saturated resulting output values
layer close one  upper limit activation function  saturated
neurons small gradients  avoided  layers relu activation
cannot saturated  die values negative thus clipped zero
inputs  resulting gradient zero layer  network train
well  advisable monitor network layers many saturated dead neurons 
saturated neurons caused large values entering layer  may controlled
changing initialization  scaling range input values  changing
learning rate  dead neurons caused signals entering layer negative  for
example happen large gradient update   reducing learning rate
help situation  saturated layers  another option normalize values
saturated layer activation  i e  instead g h    tanh h  using g h    k tanh h 
tanh h k  
layer normalization effective measure countering saturation  expensive
terms gradient computation  related technique batch normalization  due ioffe
szegedy         activations layer normalized
mean   variance   across mini batch  batch normalization techniques
became key component effective training deep networks computer vision 
writing  less popular natural language applications 
      shuffling
order training examples presented network important 
sgd formulation specifies selecting random example turn  practice 
implementations go training example order  advised shuffle training
examples pass data 
   

fia primer neural networks nlp

      learning rate
selection learning rate important  large learning rates prevent network
converging effective solution  small learning rates take long time
converge  rule thumb  one experiment range initial learning rates
range         e g                       monitor networks loss time  decrease
learning rate loss stops improving  learning rate scheduling decreases rate
function number observed minibatches  common schedule dividing initial
learning rate iteration number  leon bottou        recommends using learning
rate form            t     initial learning rate  learning
rate use tth training example  additional hyperparameter 
recommends determining good value   based small sample data prior
running entire dataset 
      minibatches
parameter updates occur either every training example  minibatches size    every k
training examples  problems benefit training larger minibatch sizes 
terms computation graph abstraction  one create computation graph
k training examples  connecting k loss nodes averaging node 
whose output loss minibatch  large minibatched training
beneficial terms computation efficiency specialized computing architectures
gpus  replacing vector matrix operations matrix matrix operations  beyond
scope tutorial 
    regularization
neural network models many parameters  overfitting easily occur  overfitting
alleviated extent regularization  common regularization method
l  regularization  placing squared penalty parameters large values adding
additive   kk  term objective function minimized  set
model parameters  k k  squared l  norm  sum squares values  
hyperparameter controlling amount regularization 
recently proposed alternative regularization method dropout  hinton  srivastava 
krizhevsky  sutskever    salakhutdinov         dropout method designed prevent
network learning rely specific weights  works randomly dropping
 setting    half neurons network  or specific layer  training
example  work wager et al         establishes strong connection dropout
method l  regularization 
dropout technique one key factors contributing strong results
neural network methods image classification tasks  krizhevsky  sutskever    hinton 
       especially combined relu activation units  dahl  sainath    hinton 
       dropout technique effective nlp applications neural networks 
   

figoldberg

   cascading multi task learning
combination online training methods automatic gradient computations using
computation graph abstraction allows easy implementation model cascading 
parameter sharing multi task learning 
    model cascading
powerful technique large networks built composing smaller
component networks  example  may feed forward network predicting
part speech word based neighbouring words and or characters compose
it  pipeline approach  would use network predicting parts speech 
feed predictions input features neural network syntactic chunking
parsing  instead  could think hidden layers network encoding
captures relevant information predicting part speech  cascading
approach  take hidden layers network connect  and part
speech prediction themselves  inputs syntactic network 
larger network takes input sequences words characters  outputs
syntactic structure  computation graph abstraction allows us easily propagate
error gradients syntactic task loss way back characters 
combat vanishing gradient problem deep networks  well make better
use available training material  individual component networks parameters
bootstrapped training separately relevant task  plugging
larger network tuning  example  part of speech predicting network
trained accurately predict parts of speech relatively large annotated corpus 
plugging hidden layer syntactic parsing network less training
data available  case training data provide direct supervision tasks 
make use training creating network two outputs  one task 
computing separate loss output  summing losses single node
backpropagate error gradients 
model cascading common using convolutional  recursive recurrent
neural networks  where  example  recurrent network used encode sentence
fixed sized vector  used input another network  supervision
signal recurrent network comes primarily upper network consumes
recurrent networks output inputs 
    multi task learning
used related prediction tasks necessarily feed one another 
believe information useful one type prediction useful
tasks  example  chunking  named entity recognition  ner 
language modeling examples synergistic tasks  information predicting chunk
boundaries  named entity boundaries next word sentence rely
shared underlying syntactic semantic representation  instead training separate network
task  create single network several outputs  common approach
multi layer feed forward network  whose final hidden layer  or concatenation
   

fia primer neural networks nlp

hidden layers  passed different output layers  way  parameters
network shared different tasks  useful information learned one
task help disambiguate tasks  again  computation graph abstraction
makes easy construct networks compute gradients them 
computing separate loss available supervision signal  summing
losses single loss used computing gradients  case several
corpora  different kind supervision signal  e g  one corpus ner
another chunking   training procedure shuffle available training
example  performing gradient computation updates respect different loss
every turn  multi task learning context language processing introduced
discussed work collobert et al          examples cascaded multi task
learning feed forward network  see work zhang weiss         context
recurrent neural networks  see work luong  le  sutskever  vinyals  kaiser
       sgaard goldberg        

   structured output prediction
many problems nlp involve structured outputs  cases desired output
class label distribution class labels  structured object sequence 
tree graph  canonical examples sequence tagging  e g  part of speech tagging 
sequence segmentation  chunking  ner   syntactic parsing  section  discuss
feed forward neural network models used structured tasks  later sections
discuss specialized neural network models dealing sequences  section    
trees  section     
    greedy structured prediction
greedy approach structured prediction decompose structure prediction
problem sequence local prediction problems training classifier perform
local decision  test time  trained classifier used greedy manner  examples
approach left to right tagging models  gimenez   marquez        greedy
transition based parsing  nivre         approaches easily adapted use neural
networks simply replacing local classifier linear classifier svm
logistic regression model neural network  demonstrated chen manning
       lewis steedman        
greedy approaches suffer error propagation  mistakes early decisions
carry influence later decisions  overall higher accuracy achievable nonlinear neural network classifiers helps offsetting problem extent  addition 
training techniques proposed mitigating error propagation problem either
attempting take easier predictions harder ones  the easy first approach goldberg   elhadad        making training conditions similar testing conditions
exposing training procedure inputs result likely mistakes  hal daume iii 
langford    marcu        goldberg   nivre         effective training
greedy neural network models  demonstrated ma  zhang  zhu         easy first
tagger  ballesteros  goldberg  dyer  smith         dynamic oracle training
greedy dependency parsing  
   

figoldberg

    search based structured prediction
common approach predicting natural language structures search based  indepth discussion search based structure prediction nlp  see book smith        
techniques easily adapted use neural network  neural networks
literature  models discussed framework energy based learning  lecun
et al         section     presented using setup terminology familiar
nlp community 
search based structured prediction formulated search problem possible structures 
predict x    arg max score x  y 

    

yy x 

x input structure  output x  in typical example x sentence
tag assignment parse tree sentence   y x  set valid
structures x  looking output maximize score
x  pair 
scoring function defined linear model 
score x  y    w  x  y 

    

feature extraction function w weight vector 
order make search optimal tractable  structure decomposed
parts  feature function defined terms parts   p  part local
feature extraction function 
x
 x  y   
 p 
    
pparts x y 

part scored separately  structure score sum component
parts scores 

score x  y   w  x  y    w

x

 p   

py

x
py

w  p   

x

score p 

    

py

p shorthand p parts x  y   decomposition parts
exists inference algorithm allows efficient search best scoring
structure given scores individual parts 
one trivially replace linear scoring function parts neuralnetwork 

score x  y   

x

score p   

py

x

nn c p  

py

c p  maps part p din dimensional vector 
case one hidden layer feed forward network 
   

    

fia primer neural networks nlp

score x  y   

x

nnmlp   c p    

x
 g c p w    b    w

    

py

py

c p  rdin   w  rdin d    b  rd    w rd    common objective structured
prediction making gold structure score higher structure     leading
following  generalized perceptron  loss 

max
score x      score x  y 
 


    

terms implementation  means  create computation graph cgp
possible parts  calculate score  then  run inference scored parts
find best scoring structure     connect output nodes computation graphs
corresponding parts gold  predicted  structure  y     summing node cgy
 cg y    connect cgy cg y using minus node  cgl   compute gradients 
argued lecun et al         section     generalized perceptron loss may
good loss function training structured prediction neural networks
margin  margin based hinge loss preferred 

max      max
score x      score x  y  
 
  y

    

trivial modify implementation work hinge loss 
note cases lose nice properties linear model  particular 
model longer convex  expected  even simplest non linear neural
network already non convex  nonetheless  could still use standard neural network
optimization techniques train structured model 
training inference slower  evaluate neural network  and take
gradients   parts x  y   times 
structured prediction vast field beyond scope tutorial  loss
functions  regularizers methods described by  e g   smith         cost augmented
decoding  easily applied adapted neural network framework   
      probabilistic objective  crf 
probabilistic framework  conditional random fields  crf   treat parts
scores clique potential  see discussions smith       lafferty  mccallum   
pereira        define score structure be 
    one keep mind resulting objectives longer convex  lack formal guarantees bounds associated convex optimization problems  similarly  theory  learning bounds
guarantees associated algorithms automatically transfer neural versions 

   

figoldberg

p
exp  py score p  
p
scorecrf  x  y    p  y x    p
  y x  exp  py   score p  
p
exp  py nn  p   
p
 p
  y x  exp  py   nn  p   

    

scoring function defines conditional distribution p  y x  
p wish set parameters network corpus conditional log likelihood  xi  yi  training log p  yi  xi  
maximized 
loss given training example  x  y  then  log scorecrf  x  y   taking
gradient respect loss involved building associated computation
graph  tricky part denominator  the partition function  requires summing
potentially exponentially many structures y  however  problems 
dynamic programming algorithm exists efficiently solving summation polynomial
time  i e  forward backward viterbi recurrences sequences cky insideoutside recurrences tree structures   algorithm exists  adapted
create polynomial size computation graph 
efficient enough algorithm computing partition function available 
approximate methods used  example  one may use beam search inference 
partition function sum structures remaining beam instead
exponentially large y x  
sequence level crfs neural network clique potentials discussed peng  bo 
xu        do  arti  others         applied sequence labeling
biological data  ocr data speech signals  wang manning       
apply traditional natural language tagging tasks  chunking ner   hinge
based approach used pei et al         arc factored dependency parsing 
probabilistic approach durrett klein        crf constituency parser 
approximate beam based partition function effectively used zhou et al        
transition based parser 
      reranking
searching possible structures intractable  inefficient hard integrate
model  reranking methods often used  reranking framework  charniak
  johnson        collins   koo        base model used produce list kbest scoring structures  complex model trained score candidates
k best list best structure respect gold one scored highest 
search performed k items rather exponential space 
complex model condition  extract features from  arbitrary aspects scored
structure  reranking methods natural candidates structured prediction using neuralnetwork models  allow modeler focus feature extraction network
structure  removing need integrate neural network scoring decoder 
indeed  reranking methods often used experimenting neural models
straightforward integrate decoder  convolutional  recurrent recursive
networks  discussed later sections  works using reranking approach
   

fia primer neural networks nlp

include schwenk et al          socher et al          auli et al          le
zuidema        zhu et al       a  
      memm hybrid approaches
formulations are  course  possible  example  memm  mccallum 
freitag    pereira        trivially adapted neural network world replacing
logistic regression  maximum entropy  component mlp 
hybrid approaches neural networks linear models explored 
particular  weiss et al         report strong results transition based dependency parsing
two stage model  first stage  static feed forward neural network  mlp  
trained perform well individual decisions structured problem
isolation  second stage  neural network model held fixed  different layers
 output well hidden layer vectors  input concatenated used
input features linear structured perceptron model  collins        trained
perform beam search best resulting structure  clear training
regime effective training single structured prediction neural network  use
two simpler  isolated models allowed researchers perform much extensive
hyper parameter search  e g  tuning layer sizes  activation functions  learning rates
on  model feasible complicated networks 

   convolutional layers
sometimes interested making predictions based ordered sets items  e g 
sequence words sentence  sequence sentences document on  
consider example predicting sentiment  positive  negative neutral  sentence 
sentence words informative sentiment  words less
informative  good approximation  informative clue informative regardless
position sentence  would feed sentence words
learner  let training process figure important clues  one possible solution
feeding cbow representation fully connected network mlp  however 
downside cbow approach ignores ordering information completely 
assigning sentences good  actually quite bad bad 
actually quite good exact representation  global position
indicators good bad matter classification task 
local ordering words  that word appears right word bad 
important  naive approach would suggest embedding word pairs  bi grams  rather
words  building cbow embedded bigrams  architecture
could effective  result huge embedding matrices  scale longer ngrams  suffer data sparsity problems share statistical strength
different n grams  the embedding quite good good completely
independent one another  learner saw one training 
able deduce anything based component words  
convolution and pooling  also called convolutional neural networks  cnns  architecture
elegant robust solution modeling problem  convolutional neural network
designed identify indicative local predictors large structure  combine
   

figoldberg

produce fixed size vector representation structure  capturing local aspects
informative prediction task hand 
convolution and pooling architectures  lecun   bengio        evolved neural
networks vision community  showed great success object detectors recognizing object predefined category  cat  bicycles  regardless position
image  krizhevsky et al          applied images  architecture using
  dimensional  grid  convolutions  applied text  mainly concerned
  d  sequence  convolutions  convolutional networks introduced nlp community pioneering work collobert  weston colleagues        used
semantic role labeling  later kalchbrenner et al         kim        used
sentiment question type classification 
    basic convolution   pooling
main idea behind convolution pooling architecture language tasks apply
non linear  learned  function instantiation k word sliding window
sentence  function  also called filter  transforms window k words
dimensional vector captures important properties words window  each
dimension sometimes referred literature channel   then  pooling
operation used combine vectors resulting different windows single
d dimensional vector  taking max average value observed
channels different windows  intention focus important
features sentence  regardless location  d dimensional vector
fed network used prediction  gradients propagated
back networks loss training process used tune parameters
filter function highlight aspects data important task
network trained for  intuitively  sliding window run sequence 
filter function learns identify informative k grams 
formally  consider sequence words x   x            xn   corresponding demb dimensional word embedding v xi     d convolution layer   width k works
moving sliding window size k sentence  applying filter
window sequence  v xi    v xi              v xi k      filter function usually
linear transformation followed non linear activation function 
let concatenated vector ith window wi    v xi    v xi              v xi k     
wi rkdemb   depending whether pad sentence k   words side 
may get either   n k      narrow convolution    n   k     windows  wide
convolution   kalchbrenner et al          result convolution layer vectors
p            pm   pi rdconv where 
pi   g wi w   b 

    

g non linear activation function applied element wise  w rkdemb dconv
b rdconv parameters network  pi dconv dimensional vector  encoding
     d refers convolution operating   dimensional inputs sequences  opposed  d
convolutions applied images 

   

fia primer neural networks nlp

  
w

max

quick brown fox jumped lazy dog
quick brown

mul tanh

quick brown fox

mul tanh

brown fox jumped

mul tanh

fox jumped

mul tanh

jumped

mul tanh

lazy

mul tanh

lazy dog

mul tanh

convolution

pooling

figure     d convolution pooling sentence quick brown fox jumped
lazy dog  narrow convolution  no padding added sentence 
window size    word translated   dim embedding vector
 not shown   embedding vectors concatenated  resulting   dim
window representations  seven windows transfered    
filter  linear transformation followed element wise tanh   resulting seven
  dimensional filtered representations  then  max pooling operation applied 
taking max dimension  resulting final   dimensional pooled
vector 

information wi   ideally  dimension captures different kind indicative information  vectors combined using max pooling layer  resulting single
dconv dimensional vector c 
cj   max pi  j 
  im

    

pi  j  denotes jth component pi   effect max pooling operation get
salient information across window positions  ideally  dimension specialize
particular sort predictors  max operation pick important
predictor type 
figure   provides illustration process 
resulting vector c representation sentence dimension
reflects salient information respect prediction task  c fed
downstream network layers  perhaps parallel vectors  culminating
output layer used prediction  training procedure network calculates
loss respect prediction task  error gradients propagated
way back pooling convolution layers  well embedding layers    
    besides useful prediction  by product training procedure set parameters w  b
embeddings v   used convolution pooling architecture encode arbitrary length

   

figoldberg

max pooling common pooling operation text applications 
pooling operations possible  second common operation average
pooling  taking average value index instead max 
    dynamic  hierarchical k max pooling
rather performing single pooling operation entire sequence  may want
retain positional information based domain understanding prediction
problem hand  end  split vectors pi   distinct groups  apply
pooling separately group  concatenate   resulting dconv  dimensional
vectors c            c    division pi groups performed based domain knowledge  example  may conjecture words appearing early sentence
indicative words appearing late  split sequence   equally
sized regions  applying separate max pooling region  example  johnson
zhang        found classifying documents topics  useful   
average pooling regions  clearly separating initial sentences  where topic usually
introduced  later ones  sentiment classification task single max pooling
operation entire sentence optimal  suggesting one two strong
signals enough determine sentiment  regardless position sentence  
similarly  relation extraction kind task may given two words asked
determine relation them  could argue words first word 
words second word  words provide three different kinds
information  chen et al          thus split pi vectors accordingly  pooling
separately windows resulting group 
another variation using hierarchy convolutional layers  succession convolution pooling layers  stage applies convolution sequence 
pools every k neighboring vectors  performs convolution resulting pooled sequence 
applies another convolution on  architecture allows sensitivity increasingly
larger structures 
finally  kalchbrenner et al         introduced k max pooling operation 
top k values dimension retained instead best one  preserving
order appeared text  example a  consider following matrix 

 
 

 

 
 

 
 
 
 
 


 
 

 

 
 



  max pooling column vectors result           max pooling


     
result following matrix 
whose rows concatenated
     


           
sentences fixed size vectors  sentences share kind predictive information
close other 

   

fia primer neural networks nlp

k max pooling operation makes possible pool k active indicators
may number positions apart  preserves order features  insensitive
specific positions  discern finely number times feature
highly activated  kalchbrenner et al         
    variations
rather single convolutional layer  several convolutional layers may applied
parallel  example  may four different convolutional layers  different
window size range     capturing n gram sequences varying lengths  result
convolutional layer pooled  resulting vectors concatenated
fed processing  kim        
convolutional architecture need restricted linear ordering sentence  example  et al         generalize convolution operation work
syntactic dependency trees  there  window around node syntactic tree 
pooling performed different nodes  similarly  liu et al         apply
convolutional architecture top dependency paths extracted dependency trees  le
zuidema        propose perform max pooling vectors representing different
derivations leading chart item chart parser 

    recurrent neural networks modeling sequences stacks
dealing language data  common work sequences  words
 sequences letters   sentences  sequences words  documents  saw feedforward networks accommodate arbitrary feature functions sequences
use vector concatenation vector addition  cbow   particular  cbow representations allows encode arbitrary length sequences fixed sized vectors  however 
cbow representation quite limited  forces one disregard order features  convolutional networks allow encoding sequence fixed size vector 
representations derived convolutional networks improvement
cbow representation offer sensitivity word order  order sensitivity
restricted mostly local patterns  disregards order patterns far apart
sequence 
recurrent neural networks  rnns   elman        allow representing arbitrarily sized
structured inputs fixed size vector  paying attention structured properties
input 
     rnn abstraction
use xi j denote sequence vectors xi           xj   rnn abstraction takes
input ordered list input vectors x         xn together initial state vector s   
returns ordered list state vectors s         sn   well ordered list output
vectors y         yn   output vector yi function corresponding state vector
si   input vectors xi presented rnn sequential fashion  state
vector si output vector yi represent state rnn observing inputs
x  i   output vector yi used prediction  example  model
   

figoldberg

predicting conditional probability event e given sequence m  i defined
p e   j x  i     softmax yi w   b  j   jth element output vector resulting
softmax operation  rnn model provides framework conditioning
entire history x            xi without resorting markov assumption traditionally
used modeling sequences    indeed  rnn based language models result good
perplexity scores compared n gram based models 
mathematically  recursively defined function r takes input state
vector si input vector xi     results new state vector si     additional
function used map state vector si output vector yi     constructing
rnn  much constructing feed forward network  one specify dimension
inputs xi well dimensions outputs yi   dimensions states
si function output dimension   
rnn s    x  n    s  n   y  n
si   r si    xi  

    

yi   o si  
xi rdin   yi rdout   si rf  dout  
functions r across sequence positions  rnn keeps
track states computation state vector kept passed
invocations r 
graphically  rnn traditionally presented figure   
yi

si 

r o



xi

si

figure    graphical representation rnn  recursive  
    kth order markov assumption states observation time independent observations
times  k   j  j     given observations times      k  assumption
basis many sequence modeling technique n gram models hidden markov models 
    using function somewhat non standard  used order unify different rnn models
presented next section  simple rnn  elman rnn  gru architectures 
identity mapping  lstm architecture selects fixed subset state 
    rnn architectures state dimension independent output dimension
possible  current popular architectures  including simple rnn  lstm gru
follow flexibility 

   

fia primer neural networks nlp

presentation follows recursive definition  correct arbitrary long sequences 
however  finite sized input sequence  and input sequences deal finite 
one unroll recursion  resulting structure figure   
y 

s 

r o

x 

y 

y 

s 

r o

s 

r o

x 

y 

s 

x 

r o

x 

y 

s 

r o

s 

x 



figure    graphical representation rnn  unrolled  

usually shown visualization  include parameters order
highlight fact parameters shared across time steps  different
instantiations r result different network structures  exhibit different
properties terms running times ability trained effectively using
gradient based methods  however  adhere abstract interface 
provide details concrete instantiations r simple rnn  lstm
gru section     that  lets consider modeling rnn abstraction 
first  note value si based entire input x         xi   example 
expanding recursion     get 

s   r s    x   


z      
 r r s    x     x   


z      
 r r r s    x     x     x   

    



z      
 r r r r s    x     x     x     x   
thus  sn  as well yn   could thought encoding entire input sequence   
encoding useful  depends definition usefulness  job network
training set parameters r state conveys useful information
task tying solve 
    note that  unless r specifically designed this  likely later elements input
sequence stronger effect sn earlier ones 

   

figoldberg

     rnn training
viewed figure   easy see unrolled rnn deep neural
network  or rather  large computation graph somewhat complex nodes  
parameters shared across many parts computation  train
rnn network  then  need create unrolled computation graph
given input sequence  add loss node unrolled graph  use backward
 backpropagation  algorithm compute gradients respect loss 
procedure referred rnn literature backpropagation time  bptt
 werbos           various ways supervision signal applied 
       acceptor
one option base supervision signal final output vector  yn   viewed
way  rnn acceptor  observe final state  decide outcome   
example  consider training rnn read characters word one one
use final state predict part of speech word  this inspired ling
et al       b   rnn reads sentence and  based final state decides
conveys positive negative sentiment  this inspired wang et al       b  rnn
reads sequence words decides whether valid noun phrase  loss
cases defined terms function yn   o sn    error gradients
backpropagate rest sequence  see figure       loss take
familiar form cross entropy  hinge  margin  etc 
       encoder
similar acceptor case  encoder supervision uses final output vector  yn  
however  unlike acceptor  prediction made solely basis final
vector  final vector treated encoding information sequence 
used additional information together signals  example  extractive
document summarization system may first run document rnn  resulting
    variants bptt algorithm include unrolling rnn fixed number input symbols
time  first unroll rnn inputs x  k   resulting s  k   compute loss  backpropagate
error network  k steps back   then  unroll inputs xk    k   time using sk
initial state  backpropagate error k steps  on  strategy based
observations simple rnn variant  gradients k steps tend vanish  for large enough
k   omitting negligible  procedure allows training arbitrarily long sequences 
rnn variants lstm gru designed specifically mitigate vanishing
gradients problem  fixed size unrolling less motivated  yet still used  example
language modeling book without breaking sentences  similar variant unrolls
network entire sequence forward step  propagates gradients back k steps
position 
    terminology borrowed finite state acceptors  however  rnn potentially infinite
number states  making necessary rely function lookup table mapping states
decisions 
    kind supervision signal may hard train long sequences  especially simplernn  vanishing gradients problem  generally hard learning task 
tell process parts input focus 

   

fia primer neural networks nlp

loss
predict  
calc loss
y 
s 

r o

s 

x 

r o

s 

x 

r o

s 

x 

r o

s 

x 

r o

x 

figure    acceptor rnn training graph 
vector yn summarizing entire document  then  yn used together
features order select sentences included summarization 
       transducer
another option treat rnn transducer  producing output input
reads in  modeled way  compute local loss signal llocal  yi   yi  
outputs yp
based true label yi   loss unrolled sequence be 
l y  n
  y  n     ni   llocal  yi   yi    using another combination rather sum
average weighted average  see figure     one example transducer
sequence tagger  take xi n feature representations n words
sentence  yi input predicting tag assignment word based
words   i  ccg super tagger based architecture provides state of the art
ccg super tagging results  xu et al         
loss

sum

predict  
calc loss

predict  
calc loss

y 
s 

r o

x 

predict  
calc loss

y 
s 

r o

x 

predict  
calc loss

y 
s 

r o

x 

predict  
calc loss

y 
s 

r o

x 

y 
s 

r o

x 

figure    transducer rnn training graph 
natural use case transduction setup language modeling 
sequence words x  i used predict distribution  i     th word  rnn based
language models shown provide better perplexities traditional language models
 mikolov et al         sundermeyer  schluter    ney        mikolov        jozefowicz 
vinyals  schuster  shazeer    wu        
using rnns transducers allows us relax markov assumption traditionally taken language models hmm taggers  condition entire prediction
   

figoldberg

history  power ability condition arbitrarily long histories demonstrated
generative character level rnn models  text generated character character  character conditioning previous ones  sutskever  martens    hinton        
generated texts show sensitivity properties captured n gram language
models  including line lengths nested parenthesis balancing  good demonstration
analysis properties rnn based character level language models  see work
karpathy  johnson  li        
       encoder   decoder
finally  important special case encoder scenario encoder decoder framework
 cho  van merrienboer  bahdanau    bengio      a  sutskever et al          rnn
used encode sequence vector representation yn   vector representation
used auxiliary input another rnn used decoder  example 
machine translation setup first rnn encodes source sentence vector
representation yn   state vector fed separate  decoder  rnn
trained predict  using transducer like language modeling objective  words
target language sentence based previously predicted words well yn  
supervision happens decoder rnn  gradients propagated
way back encoder rnn  see figure    
loss

sum

predict  
calc loss

predict  
calc loss

y 
sd 

rd  od

y 
sd 

 oe

x 

se 

sd 

rd  od

x 

 oe

x 

se 

predict  
calc loss

y 

rd  od

x 

se 

predict  
calc loss

y 
sd 

rd  od

x 

 oe

x 

se 

predict  
calc loss
y 
sd 

rd  od

x 

 oe

x 

se 

x 

 oe

se 

x 

figure    encoder decoder rnn training graph 
approach shown surprisingly effective machine translation  sutskever
et al         using lstm rnns  order technique work  sutskever et al  found
effective input source sentence reverse  xn corresponds first
   

fia primer neural networks nlp

word sentence  way  easier second rnn establish relation
first word source sentence first word target sentence 
another use case encoder decoder framework sequence transduction  here 
order generate tags t            tn   encoder rnn first used encode sentence
x  n fixed sized vector  vector fed initial state vector another
 transducer  rnn  used together x  n predict label ti position
i  approach used filippova  alfonseca  colmenares  kaiser  vinyals       
model sentence compression deletion 
     multi layer  stacked  rnns
rnns stacked layers  forming grid  hihi   bengio         consider k rnns 
j
rnn            rnnk   jth rnn states sj  n outputs y  n
  input
first rnn x  n   input jth rnn  j    outputs rnn
j 
k  
it  y  n
  output entire formation output last rnn  y  n
layered architectures often called deep rnns  visual representation   layer
rnn given figure    
y 

y 

y  
s  

r   o 

y  
s  

y  
s  

r   o 

r   o 

x 

r   o 

r   o 

r   o 

r   o 

r   o 

x 

r   o 

x 

r   o 

y  
s  

y  
s  

y  
s  

y 

y  
s  

y  
s  

y  
s  

y 

y  
s  

y  
s  

y  
s  

y 

r   o 

r   o 

x 

s  

y  
s  

y  
s  

r   o 

r   o 

s  

y  
s  

r   o 

s  

x 

figure       layer  deep  rnn architecture 
theoretically clear additional power gained deeper
architecture  observed empirically deep rnns work better shallower ones
tasks  particular  sutskever et al         report   layers deep architecture crucial achieving good machine translation performance encoder decoder
framework  irsoy cardie        report improved results moving onelayer birnn architecture several layers  many works report result using
layered rnn architectures  explicitly compare   layer rnns 
     bidirectional rnns  birnn 
useful elaboration rnn bidirectional rnn  birnn  commonly referred
birnn   schuster   paliwal        graves           consider task sequence
tagging sentence x            xn   rnn allows us compute function ith word
    used specific rnn architecture lstm  model called bilstm 

   

figoldberg

xi based past words x  i including it  however  following words
xi n may useful prediction  evident common sliding window approach
focus word categorized based window k words surrounding it  much
rnn relaxes markov assumption allows looking arbitrarily back
past  birnn relaxes fixed window size assumption  allowing look arbitrarily far
past future 
consider input sequence x  n   birnn works maintaining two separate states 
f
si sbi input position i  forward state sfi based x    x            xi  
backward state sbi based xn   xn            xi   forward backward states
generated two different rnns  first rnn  rf     fed input sequence x  n
is  second rnn  rb   ob   fed input sequence reverse  state
representation si composed forward backward states 
output position based concatenation two output vectors
yi    yif   yib      of  sfi    ob  sbi     taking account past future 
vector yi used directly prediction  fed part input
complex network  two rnns run independently other  error gradients position flow forward backward two rnns  visual
representation birnn architecture given figure    
ythe

ybrown

concat

concat

sb 

rb  ob
y f

sf 

rf  of

xthe

concat
y b

y b
sb  

rb  ob

sb  

rf  of

rb  ob
y f

sf 

rf  of

xbrown

xfox



concat
y b

y f
sf 

yjumped

yfox

concat
y b

sb  

rb  ob
y f

sf 

rf  of

xjumped

y b
sb  

sb  

rb  ob
y f

sf 

sf 

rf  of

x

figure     birnn sentence brown fox jumped   
use birnns sequence tagging introduced nlp community irsoy
cardie        
     rnns representing stacks
algorithms language processing  including transition based parsing  nivre 
       require performing feature extraction stack  instead confined
looking k top most elements stack  rnn framework used provide
fixed sized vector encoding entire stack 
main intuition stack essentially sequence  stack state
represented taking stack elements feeding order rnn  resulting
final encoding entire stack  order computation efficiently  without
   

fia primer neural networks nlp

performing o n  stack encoding operation time stack changes   rnn state
maintained together stack state  stack push only  would
trivial  whenever new element x pushed stack  corresponding vector x
used together rnn state si order obtain new state si     dealing
pop operation challenging  solved using persistent stack
data structure  okasaki        goldberg  zhao    huang         persistent  immutable 
data structures keep old versions intact modified  persistent stack
construction represents stack pointer head linked list  empty stack
empty list  push operation appends element list  returning new head 
pop operation returns parent head  keeping original list intact 
point view someone held pointer previous head  stack
change  subsequent push operation add new child node  applying
procedure throughout lifetime stack results tree  root
empty stack path node root represents intermediary stack state 
figure    provides example tree  process applied
computation graph construction  creating rnn tree structure instead chain
structure  backpropagating error given node affect elements
participated stack node created  order  figure    shows
computation graph stack rnn corresponding last state figure    
modeling approach proposed independently dyer et al         watanabe
sumita        transition based dependency parsing 
head

head


head







    push

head




b

    push b

b



head

c





    push c

b

c





    pop

    push

head







b
head

    pop

c







b

c





b

c

b

head

e

e





c





b

f

c

head
    pop

    push e

    push f

figure     immutable stack construction sequence operations push a  push b 
push c  pop  push d  pop  pop  push e  push f 

     note reading literature
unfortunately  often case inferring exact model form reading
description research paper quite challenging  many aspects models
   

figoldberg

ya e

r o

ya e f

sa e

ya b d xe

sa

ya



r o

xa

ya b

sa

r o

xb

sa b

ya c

r o

sa e f

xf

sa b d

r o

sa b

r o

xd

sa c

xc

figure     stack rnn corresponding final state figure    

yet standardized  different researchers use terms refer slightly
different things  list examples  inputs rnn either one hot vectors
 in case embedding matrix internal rnn  embedded representations 
input sequence padded start of sequence and or end of sequence symbols 
not  output rnn usually assumed vector expected
fed additional layers followed softmax prediction  as case
presentation tutorial   papers assume softmax part rnn itself 
multi layer rnn  state vector either output top most layer 
concatenation outputs layers  using encoder decoder framework 
conditioning output encoder interpreted various different ways 
on  top that  lstm architecture described next section many small
variants  referred common name lstm  choices
made explicit papers  require careful reading  others still even
mentioned  hidden behind ambiguous figures phrasing 
reader  aware issues reading interpret model descriptions 
writer  aware issues well  either fully specify model mathematical
notation  refer different source model fully specified  source
available  using default implementation software package without knowing
details  explicit fact specify software package use  case 
dont rely solely figures natural language text describing model 
often ambiguous 
   

fia primer neural networks nlp

    concrete rnn architectures
turn present three different instantiations abstract rn n architecture
discussed previous section  providing concrete definitions functions r o 
simple rnn  srnn   long short term memory  lstm  gated
recurrent unit  gru  
     simple rnn
simplest rnn formulation  known elman network simple rnn  s rnn  
proposed elman        explored use language modeling mikolov        
s rnn takes following form 
si  rsrnn  si    xi     g xi wx   si  ws   b 
yi  osrnn  si     si

    

si   yi rds   xi rdx   wx rdx ds   ws rds ds   b rds
is  state position linear combination input position
previous state  passed non linear activation  commonly tanh relu  
output position hidden state position   
spite simplicity  simple rnn provides strong results sequence tagging
 xu et al         well language modeling  comprehensive discussion using
simple rnns language modeling  see phd thesis mikolov        
     lstm
s rnn hard train effectively vanishing gradients problem  pascanu
et al          error signals  gradients  later steps sequence diminish quickly
back propagation process  reach earlier input signals  making hard
s rnn capture long range dependencies  long short term memory  lstm 
architecture  hochreiter   schmidhuber        designed solve vanishing gradients
problem  main idea behind lstm introduce part state representation
memory cells  a vector  preserve gradients across time  access
memory cells controlled gating components smooth mathematical functions
simulate logical gates  input state  gate used decide much new
input written memory cell  much current content
memory cell forgotten  concretely  gate g       n vector values
range        multiplied component wise another vector v rn   result
added another vector  values g designed close either      i e 
using sigmoid function  indices v corresponding near one values g allowed
pass  corresponding near zero values blocked 
    authors treat output position complicated function state  e g  linear
transformation  mlp  presentation  transformation output
considered part rnn  separate computations applied rnns output 

   

figoldberg

mathematically  lstm architecture defined as   

sj   rlstm  sj    xj     cj   hj  
cj  cj  f   g

hj   tanh cj  

  xj wxi   hj  whi  

f   xj wxf   hj  whf  
  xj w

xo

  hj  w

ho

    

 

g   tanh xj wxg   hj  whg  
yj   olstm  sj    hj

sj r dh   xi rdx   cj   hj   i  f   o  g rdh   wx rdx dh   wh rdh dh  
symbol used denote component wise product  state time j composed two vectors  cj hj   cj memory component hj hidden
state component  three gates  i  f o  controlling input  f orget output 
gate values computed based linear combinations current input xj
previous state hj    passed sigmoid activation function  update candidate g
computed linear combination xj hj    passed tanh activation function  memory cj updated  forget gate controls much previous
memory keep  cj  f    input gate controls much proposed update
keep  g i   finally  value hj  which output yj   determined based
content memory cj   passed tanh non linearity controlled
output gate  gating mechanisms allow gradients related memory part cj
stay high across long time ranges 
discussion lstm architecture see phd thesis alex graves
        well online post olah      b   analysis behavior
lstm used character level language model  see work karpathy et al 
       
explanation motivation behind gating mechanism lstm
 and gru  relation solving vanishing gradient problem recurrent neural
networks  see sections         detailed course notes cho        
lstms currently successful type rnn architecture  responsible many state of the art sequence modeling results  main competitor
lstm rnn gru  discussed next 
    many variants lstm architecture presented here  example  forget gates
part original proposal hochreiter schmidhuber         shown important
part architecture  variants include peephole connections gate tying  overview
comprehensive empirical comparison various lstm architectures see work greff  srivastava 
koutnk  steunebrink  schmidhuber        

   

fia primer neural networks nlp

       practical considerations
training lstm networks  jozefowicz et al         strongly recommend always
initialize bias term forget gate close one  applying dropout
rnn lstm  zaremba et al         found crucial apply dropout
non recurrent connection  i e  apply layers
sequence positions 
     gru
lstm architecture effective  quite complicated  complexity
system makes hard analyze  computationally expensive work with 
gated recurrent unit  gru  recently introduced cho et al       b  alternative
lstm  subsequently shown chung et al         perform comparably
lstm several  non textual  datasets 
lstm  gru based gating mechanism  substantially
fewer gates without separate memory component 
sj   rgru  sj    xj       z  sj    z sj
z   xj wxz   sj  wsz  
r   xj wxr   sj  wsr  
sj   tanh xj wxs    sj  r wsg  

    

yj   ogru  sj    sj
sj   sj rds   xi rdx   z  r rds   wx rdx ds   ws rds ds  
one gate  r  used control access previous state sj  compute proposed update sj   updated state sj  which serves output yj   determined based
interpolation previous state sj  proposal sj   proportions
interpolation controlled using gate z   
gru shown effective language modeling machine translation 
however  jury still gru  lstm possible alternative rnn
architectures  subject actively researched  empirical exploration
gru lstm architectures  see work jozefowicz et al         
     variants
gated architectures lstm gru help alleviating vanishing gradients problem simple rnn  allow rnns capture dependencies span
long time ranges  researchers explore simpler architectures lstm
gru achieving similar benefits 
mikolov et al         observed matrix multiplication si  ws coupled
nonlinearity g update rule r simple rnn causes state vector si undergo
    states often called h gru literature 

   

figoldberg

large changes time step  prohibiting remembering information long
time periods  propose split state vector si slow changing component ci
 context units  fast changing component hi     slow changing component ci
updated according linear interpolation input previous component  ci  
    xi wx    ci            update allows ci accumulate previous
inputs  fast changing component hi updated similarly simple rnn update
rule  changed take ci account well    hi    xi wx    hi  wh   ci wc   
finally  output yi concatenation slow fast changing parts
state  yi    ci   hi    mikolov et al  demonstrate architecture provides competitive
perplexities much complex lstm language modeling tasks 
approach mikolov et al  interpreted constraining block
matrix ws s rnn corresponding ci multiply identity matrix  see
mikolov et al         details   le  jaitly  hinton        propose even simpler
approach  set activation function s rnn relu  initialize biases b
zeroes matrix ws identify matrix  causes untrained rnn copy
previous state current state  add effect current input xi set
negative values zero  setting initial bias towards state copying  training
procedure allows ws change freely  le et al  demonstrate simple modification
makes s rnn comparable lstm number parameters several
tasks  including language modeling 

    modeling trees recursive neural networks
rnn useful modeling sequences  language processing  often natural
desirable work tree structures  trees syntactic trees  discourse trees 
even trees representing sentiment expressed various parts sentence  socher
et al          may want predict values based specific tree nodes  predict values
based root nodes  assign quality score complete tree part tree 
cases  may care tree structure directly rather reason spans
sentence  cases  tree merely used backbone structure helps
guide encoding process sequence fixed size vector 
recursive neural network  recnn  abstraction  pollack         popularized nlp
richard socher colleagues  socher  manning    ng        socher  lin  ng    manning        socher et al         socher        generalization rnn sequences
 binary  trees   
much rnn encodes sentence prefix state vector  recnn encodes
tree node state vector rd   use state vectors either predict
values corresponding nodes  assign quality values node  semantic
representation spans rooted nodes 
    depart notation mikolov et al         reuse symbols used lstm description 
    update rule diverges s rnn update rule fixing non linearity sigmoid
function  using bias term  however  changes discussed central
proposal 
    presented terms binary parse trees  concepts easily transfer general recursively defined
data structures  major technical challenge definition effective form r 
combination function 

   

fia primer neural networks nlp

main intuition behind recursive neural networks subtree represented dimensional vector  representation node p children c  c 
function representation nodes  vec p    f  vec c     vec c      f
composition function taking two d dimensional vectors returning single d dimensional
vector  much rnn state si used encode entire sequence x    i  recnn
state associated tree node p encodes entire subtree rooted p  see figure   
illustration 

s 
combine

n p   

vp  

combine

n p   

v  

figure     illustration recursive neural network  representations v np 
combined form representation vp  representations vp
np  combined form representation s 

     formal definition
consider binary parse tree n word sentence  reminder  ordered 
unlabeled tree string x            xn represented unique set triplets  i  k  j  
s t  k j  triplet indicates node spanning words xi j parent
nodes spanning xi k xk   j   triplets form  i  i  i  correspond terminal symbols
tree leaves  the words xi    moving unlabeled case labeled one 
represent tree set   tuples  a b  c  i  k  j   whereas i  k j indicate spans
before  a  b c node labels nodes spanning xi j   xi k xk   j
respectively  here  leaf nodes form  a a  a  i  i  i   pre terminal
symbol  refer tuples production rules  example  consider syntactic
tree sentence boy saw duck 
   

figoldberg


vp

np

np

det noun verb


boy

saw

det noun


duck

corresponding unlabeled labeled representations  
unlabeled
       
       
       
       
       
       
       
       
       

labeled
 det  det  det          
 noun  noun  noun          
 verb  verb  verb          
 det  det  det          
 noun  noun  noun          
 np  det  noun          
 vp  verb  np          
 np  det  noun          
 s  np  vp          

corresponding span
x   
x    boy
saw

duck
duck
saw duck
boy
boy saw duck


set production rules uniquely converted set tree nodes qi j
 indicating node symbol span xi j   simply ignoring elements
 b  c  k  production rule  position define recursive neural
network 
recursive neural network  recnn  function takes input parse tree
n word sentence x            xn   sentences words represented d dimensional
vector xi   tree represented set production rules  a b  c  i  j  k  
  recnn returns output corresponding set
denote nodes qi j


inside state vectors si j   inside state vector sa
i j r represents corresponding
  encodes entire structure rooted node  sequence rnn 
tree node qi j
tree shaped recnn defined recursively using function r  inside vector
given node defined function inside vectors direct children    formally 



recnn x            xn       sa
i j r   qi j  

sa
i i  v xi  

b
c
sa
i j  r a  b  c  si k   sk   j  

    
b
c
  qk   j

qi k

    le zuidema        extend recnn definition node has  addition inside
state vector  outside state vector representing entire structure around subtree rooted
node  formulation based recursive computation classic inside outside
algorithm  thought birnn counterpart tree recnn  details  see work
le zuidema 

   

fia primer neural networks nlp

function r usually takes form simple linear transformation  may
may followed non linear activation function g 
c
b
c
r a  b  c  sb
i k   sk   j     g  si k   sk   j  w 

    

formulation r ignores tree labels  using matrix w r dd
combinations  may useful formulation case node labels exist  e g 
tree represent syntactic structure clearly defined labels 
unreliable  however  labels available  generally useful include
composition function  one approach would introduce label embeddings v a 
mapping non terminal symbol dnt dimensional vector  change r include
embedded symbols combination function 
c
b
c
r a  b  c  sb
i k   sk   j     g  si k   sk   j   v a   v b  w 

    

 here  w r d  dnt    approach taken qian  tian  huang  liu  zhu 
zhu         alternative approach  due socher et al         untie weights
according non terminals  using different composition matrix b  c pair
symbols   
bc
c
b
c
 
r a  b  c  sb
i k   sk   j     g  si k   sk   j  w

    

formulation useful number non terminal symbols  or number
possible symbol combinations  relatively small  usually case phrase structure
parse trees  similar model used hashimoto et al         encode subtrees
semantic relation classification task 
     extensions variations
definitions r suffer vanishing gradients problem
simple rnn  several authors sought replace functions inspired long shortterm memory  lstm  gated architecture  resulting tree shaped lstms  tai  socher   
manning        zhu  sobhani    guo      b   question optimal tree representation
still much open research question  vast space possible combination
functions r yet explored  proposed variants tree structured rnns includes
recursive matrix vector model  socher  huval  manning    ng        recursive neural
tensor network  socher et al          first variant  word represented
combination vector matrix  vector defines words static semantic
content before  matrix acts learned operator word  allowing
subtle semantic compositions addition weighted averaging implied
concatenation followed linear transformation function  second variant  words
associated vectors usual  composition function becomes expressive
basing tensor instead matrix operations 
    explored literature  trivial extension would condition transformation matrix
a 

   

figoldberg

     training recursive neural networks
training procedure recursive neural network follows recipe training
forms networks  define loss  spell computation graph  compute gradients
using backpropagation     train parameters using sgd 
regard loss function  similar sequence rnn one associate loss
either root tree  given node  set nodes  case
individual nodes losses combined  usually summation  loss function based
labeled training data associates label quantity different tree
nodes 
additionally  one treat recnn encoder  whereas inside vector associated node taken encoding tree rooted node  encoding
potentially sensitive arbitrary properties structure  vector
passed input another network 
discussion recursive neural networks use natural language
tasks  refer phd thesis richard socher        

    conclusions
neural networks powerful learners  providing opportunities ranging non linear
classification non markovian modeling sequences trees  hope exposition helps nlp researchers incorporate neural network models work take
advantage power 

references
adel  h   vu  n  t     schultz  t          combination recurrent neural networks
factored language models code switching language modeling  proceedings
  st annual meeting association computational linguistics  volume    short papers   pp          sofia  bulgaria  association computational
linguistics 
ando  r     zhang  t       a   high performance semi supervised learning method
text chunking  proceedings   rd annual meeting association
computational linguistics  acl     pp      ann arbor  michigan  association
computational linguistics 
ando  r  k     zhang  t       b   framework learning predictive structures
multiple tasks unlabeled data  journal machine learning research    
         
auli  m   galley  m   quirk  c     zweig  g          joint language translation modeling recurrent neural networks  proceedings      conference
empirical methods natural language processing  pp            seattle  washington  usa  association computational linguistics 
    introduction computation graph abstraction  specific backpropagation procedure
computing gradients recnn defined referred back propagation
structure  bpts  algorithm  goller   kuchler        

   

fia primer neural networks nlp

auli  m     gao  j          decoder integration expected bleu training recurrent
neural network language models  proceedings   nd annual meeting
association computational linguistics  volume    short papers   pp         
baltimore  maryland  association computational linguistics 
ballesteros  m   dyer  c     smith  n  a          improved transition based parsing
modeling characters instead words lstms  proceedings      conference empirical methods natural language processing  pp          lisbon 
portugal  association computational linguistics 
ballesteros  m   goldberg  y   dyer  c     smith  n  a          training exploration
improves greedy stack lstm parser  arxiv             cs  
bansal  m   gimpel  k     livescu  k          tailoring continuous word representations
dependency parsing  proceedings   nd annual meeting association
computational linguistics  volume    short papers   pp          baltimore 
maryland  association computational linguistics 
baydin  a  g   pearlmutter  b  a   radul  a  a     siskind  j  m          automatic
differentiation machine learning  survey  arxiv             cs  
bengio  y          practical recommendations gradient based training deep architectures  arxiv            cs  
bengio  y   ducharme  r   vincent  p     janvin  c          neural probabilistic language model  j  mach  learn  res               
bengio  y   goodfellow  i  j     courville  a          deep learning  book preparation
mit press 
bitvai  z     cohn  t          non linear text regression deep convolutional
neural network  proceedings   rd annual meeting association
computational linguistics  th international joint conference natural language processing  volume    short papers   pp          beijing  china  association
computational linguistics 
botha  j  a     blunsom  p          compositional morphology word representations
language modelling  proceedings   st international conference
machine learning  icml   beijing  china   award best application paper  
bottou  l          stochastic gradient descent tricks  neural networks  tricks
trade  pp          springer 
charniak  e     johnson  m          coarse to fine n best parsing maxent discriminative reranking  proceedings   rd annual meeting association
computational linguistics  acl     pp          ann arbor  michigan  association
computational linguistics 
chen  d     manning  c          fast accurate dependency parser using neural
networks  proceedings      conference empirical methods natural
language processing  emnlp   pp          doha  qatar  association computational linguistics 
   

figoldberg

chen  y   xu  l   liu  k   zeng  d     zhao  j          event extraction via dynamic
multi pooling convolutional neural networks  proceedings   rd annual
meeting association computational linguistics  th international
joint conference natural language processing  volume    long papers   pp     
     beijing  china  association computational linguistics 
cho  k          natural language understanding distributed representation 
arxiv             cs  stat  
cho  k   van merrienboer  b   bahdanau  d     bengio  y       a   properties
neural machine translation  encoderdecoder approaches  proceedings ssst   eighth workshop syntax  semantics structure statistical translation 
pp          doha  qatar  association computational linguistics 
cho  k   van merrienboer  b   gulcehre  c   bahdanau  d   bougares  f   schwenk  h    
bengio  y       b   learning phrase representations using rnn encoderdecoder
statistical machine translation  proceedings      conference empirical
methods natural language processing  emnlp   pp            doha  qatar 
association computational linguistics 
chrupala  g          normalizing tweets edit scripts recurrent neural embeddings 
proceedings   nd annual meeting association computational linguistics  volume    short papers   pp          baltimore  maryland  association
computational linguistics 
chung  j   gulcehre  c   cho  k     bengio  y          empirical evaluation gated
recurrent neural networks sequence modeling  arxiv            cs  
collins  m          discriminative training methods hidden markov models  theory
experiments perceptron algorithms  proceedings      conference empirical methods natural language processing  pp      association
computational linguistics 
collins  m     koo  t          discriminative reranking natural language parsing 
computational linguistics               
collobert  r     weston  j          unified architecture natural language processing 
deep neural networks multitask learning  proceedings   th international
conference machine learning  pp          acm 
collobert  r   weston  j   bottou  l   karlen  m   kavukcuoglu  k     kuksa  p         
natural language processing  almost  scratch  journal machine learning
research               
crammer  k     singer  y          algorithmic implementation multiclass kernelbased vector machines  journal machine learning research            
creutz  m     lagus  k          unsupervised models morpheme segmentation
morphology learning  acm trans  speech lang  process                  
cybenko  g          approximation superpositions sigmoidal function  mathematics
control  signals systems                
   

fia primer neural networks nlp

dahl  g   sainath  t     hinton  g          improving deep neural networks lvcsr
using rectified linear units dropout       ieee international conference
acoustics  speech signal processing  icassp   pp           
dauphin  y  n   pascanu  r   gulcehre  c   cho  k   ganguli  s     bengio  y         
identifying attacking saddle point problem high dimensional non convex
optimization  ghahramani  z   welling  m   cortes  c   lawrence  n  d     weinberger  k  q   eds    advances neural information processing systems     pp 
          curran associates  inc 
de gispert  a   iglesias  g     byrne  b          fast accurate preordering smt
using neural networks  proceedings      conference north american
chapter association computational linguistics  human language technologies  pp            denver  colorado  association computational linguistics 
do  t   arti  t     others         neural conditional random fields  international
conference artificial intelligence statistics  pp         
dong  l   wei  f   tan  c   tang  d   zhou  m     xu  k          adaptive recursive neural
network target dependent twitter sentiment classification  proceedings
  nd annual meeting association computational linguistics  volume
   short papers   pp        baltimore  maryland  association computational
linguistics 
dong  l   wei  f   zhou  m     xu  k          question answering freebase
multi column convolutional neural networks  proceedings   rd annual
meeting association computational linguistics  th international
joint conference natural language processing  volume    long papers   pp     
     beijing  china  association computational linguistics 
dos santos  c     gatti  m          deep convolutional neural networks sentiment
analysis short texts  proceedings coling         th international conference computational linguistics  technical papers  pp        dublin  ireland 
dublin city university association computational linguistics 
dos santos  c   xiang  b     zhou  b          classifying relations ranking
convolutional neural networks  proceedings   rd annual meeting
association computational linguistics  th international joint conference natural language processing  volume    long papers   pp          beijing 
china  association computational linguistics 
dos santos  c     zadrozny  b          learning character level representations partof speech tagging  proceedings   st international conference machine
learning  icml   pp           
duchi  j   hazan  e     singer  y          adaptive subgradient methods online learning
stochastic optimization  journal machine learning research          
     
duh  k   neubig  g   sudoh  k     tsukada  h          adaptation data selection using neural language models  experiments machine translation  proceedings
   

figoldberg

  st annual meeting association computational linguistics  volume    short papers   pp          sofia  bulgaria  association computational
linguistics 
durrett  g     klein  d          neural crf parsing  proceedings   rd annual
meeting association computational linguistics  th international
joint conference natural language processing  volume    long papers   pp     
     beijing  china  association computational linguistics 
dyer  c   ballesteros  m   ling  w   matthews  a     smith  n  a          transitionbased dependency parsing stack long short term memory  proceedings
  rd annual meeting association computational linguistics
 th international joint conference natural language processing  volume    long
papers   pp          beijing  china  association computational linguistics 
elman  j  l          finding structure time  cognitive science                 
faruqui  m     dyer  c          improving vector space word representations using multilingual correlation  proceedings   th conference european chapter
association computational linguistics  pp          gothenburg  sweden 
association computational linguistics 
filippova  k   alfonseca  e   colmenares  c  a   kaiser  l     vinyals  o          sentence
compression deletion lstms  proceedings      conference
empirical methods natural language processing  pp          lisbon  portugal 
association computational linguistics 
forcada  m  l     neco  r  p          recursive hetero associative memories translation 
biological artificial computation  neuroscience technology  pp     
     springer 
gao  j   pantel  p   gamon  m   he  x     deng  l          modeling interestingness
deep neural networks  proceedings      conference empirical methods
natural language processing  emnlp   pp       doha  qatar  association
computational linguistics 
gimenez  j     marquez  l          svmtool  general pos tagger generator based
support vector machines  proceedings  th lrec  lisbon  portugal 
glorot  x     bengio  y          understanding difficulty training deep feedforward
neural networks  international conference artificial intelligence statistics 
pp         
glorot  x   bordes  a     bengio  y          deep sparse rectifier neural networks 
international conference artificial intelligence statistics  pp         
goldberg  y     elhadad  m          efficient algorithm easy first non directional
dependency parsing  human language technologies       annual conference
north american chapter association computational linguistics  pp 
        los angeles  california  association computational linguistics 
goldberg  y     levy  o          word vec explained  deriving mikolov et al s negativesampling word embedding method  arxiv            cs  stat  
   

fia primer neural networks nlp

goldberg  y     nivre  j          training deterministic parsers non deterministic
oracles  transactions association computational linguistics            
    
goldberg  y   zhao  k     huang  l          efficient implementation beam search
incremental parsers  proceedings   st annual meeting association
computational linguistics  volume    short papers   pp          sofia  bulgaria 
association computational linguistics 
goller  c     kuchler  a          learning task dependent distributed representations
backpropagation structure  proc  icnn     pp         
ieee 
gouws  s   bengio  y     corrado  g          bilbowa  fast bilingual distributed representations without word alignments  proceedings   nd international
conference machine learning  pp         
graves  a          supervised sequence labelling recurrent neural networks  ph d 
thesis  technische universitat munchen 
greff  k   srivastava  r  k   koutnk  j   steunebrink  b  r     schmidhuber  j         
lstm  search space odyssey  arxiv             cs  
hal daume iii  langford  j     marcu  d          search based structured prediction 
machine learning journal  mlj  
harris  z          distributional structure  word                  
hashimoto  k   miwa  m   tsuruoka  y     chikayama  t          simple customization
recursive neural networks semantic relation classification  proceedings
     conference empirical methods natural language processing  pp 
          seattle  washington  usa  association computational linguistics 
he  k   zhang  x   ren  s     sun  j          delving deep rectifiers  surpassing
human level performance imagenet classification  arxiv             cs  
henderson  m   thomson  b     young  s          deep neural network approach
dialog state tracking challenge  proceedings sigdial      conference 
pp          metz  france  association computational linguistics 
hermann  k  m     blunsom  p          role syntax vector space models
compositional semantics  proceedings   st annual meeting association computational linguistics  volume    long papers   pp          sofia 
bulgaria  association computational linguistics 
hermann  k  m     blunsom  p          multilingual models compositional distributed
semantics  proceedings   nd annual meeting association computational linguistics  volume    long papers   pp        baltimore  maryland 
association computational linguistics 
hihi  s  e     bengio  y          hierarchical recurrent neural networks long term
dependencies  touretzky  d  s   mozer  m  c     hasselmo  m  e   eds    advances
neural information processing systems    pp          mit press 
   

figoldberg

hill  f   cho  k   jean  s   devin  c     bengio  y          embedding word similarity
neural machine translation  arxiv            cs  
hinton  g  e   srivastava  n   krizhevsky  a   sutskever  i     salakhutdinov  r  r 
        improving neural networks preventing co adaptation feature detectors 
arxiv            cs  
hochreiter  s     schmidhuber  j          long short term memory  neural computation 
                
hornik  k   stinchcombe  m     white  h          multilayer feedforward networks
universal approximators  neural networks                
ioffe  s     szegedy  c          batch normalization  accelerating deep network training
reducing internal covariate shift  arxiv             cs  
irsoy  o     cardie  c          opinion mining deep recurrent neural networks 
proceedings      conference empirical methods natural language
processing  emnlp   pp          doha  qatar  association computational linguistics 
iyyer  m   boyd graber  j   claudino  l   socher  r     daume iii  h       a   neural
network factoid question answering paragraphs  proceedings     
conference empirical methods natural language processing  emnlp   pp 
        doha  qatar  association computational linguistics 
iyyer  m   enns  p   boyd graber  j     resnik  p       b   political ideology detection
using recursive neural networks  proceedings   nd annual meeting
association computational linguistics  volume    long papers   pp           
baltimore  maryland  association computational linguistics 
iyyer  m   manjunatha  v   boyd graber  j     daume iii  h          deep unordered
composition rivals syntactic methods text classification  proceedings
  rd annual meeting association computational linguistics  th
international joint conference natural language processing  volume    long papers   pp            beijing  china  association computational linguistics 
johnson  r     zhang  t          effective use word order text categorization
convolutional neural networks  proceedings      conference north
american chapter association computational linguistics  human language technologies  pp          denver  colorado  association computational
linguistics 
jozefowicz  r   vinyals  o   schuster  m   shazeer  n     wu  y          exploring
limits language modeling  arxiv             cs  
jozefowicz  r   zaremba  w     sutskever  i          empirical exploration recurrent network architectures  proceedings   nd international conference
machine learning  icml      pp           
kalchbrenner  n   grefenstette  e     blunsom  p          convolutional neural network
modelling sentences  proceedings   nd annual meeting association computational linguistics  volume    long papers   pp          baltimore 
maryland  association computational linguistics 
   

fia primer neural networks nlp

karpathy  a   johnson  j     li  f  f          visualizing understanding recurrent
networks  arxiv             cs  
kim  y          convolutional neural networks sentence classification  proceedings      conference empirical methods natural language processing
 emnlp   pp            doha  qatar  association computational linguistics 
kim  y   jernite  y   sontag  d     rush  a  m          character aware neural language
models  arxiv             cs  stat  
kingma  d     ba  j         
arxiv            cs  

adam  method stochastic optimization 

krizhevsky  a   sutskever  i     hinton  g  e          imagenet classification deep
convolutional neural networks  pereira  f   burges  c  j  c   bottou  l     weinberger  k  q   eds    advances neural information processing systems     pp 
          curran associates  inc 
kudo  t     matsumoto  y          fast methods kernel based text analysis 
proceedings   st annual meeting association computational linguistics volume    acl     pp        stroudsburg  pa  usa  association computational
linguistics 
lafferty  j   mccallum  a     pereira  f  c          conditional random fields  probabilistic
models segmenting labeling sequence data  proceedings icml 
le  p     zuidema  w          inside outside recursive neural network model
dependency parsing  proceedings      conference empirical methods
natural language processing  emnlp   pp          doha  qatar  association
computational linguistics 
le  p     zuidema  w          forest convolutional network  compositional distributional semantics neural chart without binarization  proceedings
     conference empirical methods natural language processing  pp 
          lisbon  portugal  association computational linguistics 
le  q  v   jaitly  n     hinton  g  e          simple way initialize recurrent networks
rectified linear units  arxiv             cs  
lecun  y     bengio  y          convolutional networks images  speech  timeseries  arbib  m  a   ed    handbook brain theory neural networks 
mit press 
lecun  y   bottou  l   orr  g     muller  k       a   efficient backprop  orr  g    
k  m   eds    neural networks  tricks trade  springer 
lecun  y   bottou  l   bengio  y     haffner  p       b   gradient based learning applied
pattern recognition  proceedings ieee                    
lecun  y   chopra  s   hadsell  r   ranzato  m     huang  f          tutorial energybased learning  predicting structured data       
lecun  y     huang  f          loss functions discriminative training energybased
models  proceedings aistats  aistats 
   

figoldberg

lee  g   flowers  m     dyer  m  g          learning distributed representations conceptual knowledge application script based story processing  connectionist
natural language processing  pp          springer 
levy  o     goldberg  y       a   dependency based word embeddings  proceedings
  nd annual meeting association computational linguistics  volume
   short papers   pp          baltimore  maryland  association computational
linguistics 
levy  o     goldberg  y       b   neural word embedding implicit matrix factorization  ghahramani  z   welling  m   cortes  c   lawrence  n  d     weinberger 
k  q   eds    advances neural information processing systems     pp           
curran associates  inc 
levy  o   goldberg  y     dagan  i          improving distributional similarity
lessons learned word embeddings  transactions association computational linguistics                
lewis  m     steedman  m          improved ccg parsing semi supervised supertagging  transactions association computational linguistics                
li  j   li  r     hovy  e          recursive deep models discourse parsing  proceedings      conference empirical methods natural language processing
 emnlp   pp            doha  qatar  association computational linguistics 
ling  w   dyer  c   black  a  w     trancoso  i       a   two too simple adaptations
word vec syntax problems  proceedings      conference north
american chapter association computational linguistics  human language technologies  pp            denver  colorado  association computational
linguistics 
ling  w   dyer  c   black  a  w   trancoso  i   fermandez  r   amir  s   marujo  l    
luis  t       b   finding function form  compositional character models
open vocabulary word representation  proceedings      conference
empirical methods natural language processing  pp            lisbon  portugal 
association computational linguistics 
liu  y   wei  f   li  s   ji  h   zhou  m     wang  h          dependency based neural
network relation classification  proceedings   rd annual meeting
association computational linguistics  th international joint conference natural language processing  volume    short papers   pp          beijing 
china  association computational linguistics 
luong  m  t   le  q  v   sutskever  i   vinyals  o     kaiser  l          multi task sequence
sequence learning  arxiv             cs  stat  
ma  j   zhang  y     zhu  j          tagging web  building robust web tagger
neural network  proceedings   nd annual meeting association computational linguistics  volume    long papers   pp          baltimore 
maryland  association computational linguistics 
ma  m   huang  l   zhou  b     xiang  b          dependency based convolutional neural
networks sentence embedding  proceedings   rd annual meeting
   

fia primer neural networks nlp

association computational linguistics  th international joint conference natural language processing  volume    short papers   pp          beijing 
china  association computational linguistics 
mccallum  a   freitag  d     pereira  f  c          maximum entropy markov models
information extraction segmentation   icml  vol      pp         
mikolov  t   chen  k   corrado  g     dean  j          efficient estimation word
representations vector space  arxiv            cs  
mikolov  t   joulin  a   chopra  s   mathieu  m     ranzato  m          learning longer
memory recurrent neural networks  arxiv            cs  
mikolov  t   karafiat  m   burget  l   cernocky  j     khudanpur  s          recurrent
neural network based language model   interspeech         th annual conference international speech communication association  makuhari  chiba 
japan  september              pp           
mikolov  t   kombrink  s   lukas burget  cernocky  j  h     khudanpur  s          extensions recurrent neural network language model  acoustics  speech signal
processing  icassp        ieee international conference on  pp            ieee 
mikolov  t   sutskever  i   chen  k   corrado  g  s     dean  j          distributed representations words phrases compositionality  burges  c  j  c  
bottou  l   welling  m   ghahramani  z     weinberger  k  q   eds    advances
neural information processing systems     pp            curran associates  inc 
mikolov  t          statistical language models based neural networks  ph d  thesis  ph 
d  thesis  brno university technology 
mnih  a     kavukcuoglu  k          learning word embeddings efficiently noisecontrastive estimation  burges  c  j  c   bottou  l   welling  m   ghahramani  z  
  weinberger  k  q   eds    advances neural information processing systems    
pp            curran associates  inc 
mrksic  n   seaghdha  d   thomson  b   gasic  m   su  p  h   vandyke  d   wen  t  h  
  young  s          multi domain dialog state tracking using recurrent neural
networks  proceedings   rd annual meeting association computational linguistics  th international joint conference natural language
processing  volume    short papers   pp          beijing  china  association
computational linguistics 
neidinger  r          introduction automatic differentiation matlab objectoriented programming  siam review                 
nesterov  y          method solving convex programming problem convergence
rate    k    soviet mathematics doklady  vol      pp         
nesterov  y          introductory lectures convex optimization  kluwer academic publishers 
nguyen  t  h     grishman  r          event detection domain adaptation
convolutional neural networks  proceedings   rd annual meeting
   

figoldberg

association computational linguistics  th international joint conference natural language processing  volume    short papers   pp          beijing 
china  association computational linguistics 
nivre  j          algorithms deterministic incremental dependency parsing  computational linguistics                 
okasaki  c          purely functional data structures  cambridge university press  cambridge  u k   new york 
olah  c       a   calculus computational graphs  backpropagation  retrieved
http   colah github io posts         backprop  
olah  c       b   understanding lstm networks  retrieved http   colah 
github io posts         understanding lstms  
pascanu  r   mikolov  t     bengio  y          difficulty training recurrent
neural networks  arxiv            cs  
pei  w   ge  t     chang  b          effective neural network model graph based
dependency parsing  proceedings   rd annual meeting association
computational linguistics  th international joint conference natural
language processing  volume    long papers   pp          beijing  china  association computational linguistics 
peng  j   bo  l     xu  j          conditional neural fields  bengio  y   schuurmans 
d   lafferty  j  d   williams  c  k  i     culotta  a   eds    advances neural
information processing systems     pp            curran associates  inc 
pennington  j   socher  r     manning  c          glove  global vectors word representation  proceedings      conference empirical methods natural
language processing  emnlp   pp            doha  qatar  association computational linguistics 
pollack  j  b          recursive distributed representations  artificial intelligence     
      
polyak  b  t          methods speeding convergence iteration methods 
ussr computational mathematics mathematical physics              
qian  q   tian  b   huang  m   liu  y   zhu  x     zhu  x          learning tag embeddings
tag specific composition functions recursive neural network  proceedings
  rd annual meeting association computational linguistics
 th international joint conference natural language processing  volume    long
papers   pp            beijing  china  association computational linguistics 
rong  x          word vec parameter learning explained  arxiv            cs  
rumelhart  d  e   hinton  g  e     williams  r  j          learning representations
back propagating errors  nature                     
schuster  m     paliwal  k  k          bidirectional recurrent neural networks  ieee
transactions signal processing                    
   

fia primer neural networks nlp

schwenk  h   dchelotte  d     gauvain  j  l          continuous space language models
statistical machine translation  proceedings coling acl main
conference poster sessions  pp          association computational linguistics 
shawe taylor  j     cristianini  n          kernel methods pattern analysis  cambridge
university press 
smith  n  a          linguistic structure prediction  synthesis lectures human language technologies  morgan claypool 
socher  r          recursive deep learning natural language processing computer
vision  ph d  thesis  stanford university 
socher  r   bauer  j   manning  c  d     ng  a  y          parsing compositional
vector grammars  proceedings   st annual meeting association
computational linguistics  volume    long papers   pp          sofia  bulgaria 
association computational linguistics 
socher  r   huval  b   manning  c  d     ng  a  y          semantic compositionality
recursive matrix vector spaces  proceedings      joint conference
empirical methods natural language processing computational natural
language learning  pp            jeju island  korea  association computational
linguistics 
socher  r   lin  c  c  y   ng  a  y     manning  c  d          parsing natural scenes
natural language recursive neural networks  getoor  l     scheffer  t 
 eds    proceedings   th international conference machine learning  icml
      bellevue  washington  usa  june      july          pp          omnipress 
socher  r   manning  c     ng  a          learning continuous phrase representations
syntactic parsing recursive neural networks  proceedings deep
learning unsupervised feature learning workshop  nips        pp     

socher  r   perelygin  a   wu  j   chuang  j   manning  c  d   ng  a     potts  c         
recursive deep models semantic compositionality sentiment treebank 
proceedings      conference empirical methods natural language
processing  pp            seattle  washington  usa  association computational
linguistics 
sgaard  a     goldberg  y          deep multi task learning low level tasks supervised
lower layers  proceedings   th annual meeting association
computational linguistics  volume    short papers   pp          association
computational linguistics 
sordoni  a   galley  m   auli  m   brockett  c   ji  y   mitchell  m   nie  j  y   gao  j  
  dolan  b          neural network approach context sensitive generation
conversational responses  proceedings      conference north
american chapter association computational linguistics  human language technologies  pp          denver  colorado  association computational
linguistics 
sundermeyer  m   alkhouli  t   wuebker  j     ney  h          translation modeling
bidirectional recurrent neural networks  proceedings      conference
   

figoldberg

empirical methods natural language processing  emnlp   pp        doha 
qatar  association computational linguistics 
sundermeyer  m   schluter  r     ney  h          lstm neural networks language
modeling   interspeech 
sutskever  i   martens  j   dahl  g     hinton  g          importance initialization
momentum deep learning  proceedings   th international conference
machine learning  icml      pp           
sutskever  i   martens  j     hinton  g  e          generating text recurrent neural
networks  proceedings   th international conference machine learning
 icml      pp           
sutskever  i   vinyals  o     le  q  v  v          sequence sequence learning
neural networks  ghahramani  z   welling  m   cortes  c   lawrence  n  d    
weinberger  k  q   eds    advances neural information processing systems     pp 
          curran associates  inc 
tai  k  s   socher  r     manning  c  d          improved semantic representations
tree structured long short term memory networks  proceedings   rd annual meeting association computational linguistics  th international joint conference natural language processing  volume    long papers  
pp            beijing  china  association computational linguistics 
tamura  a   watanabe  t     sumita  e          recurrent neural networks word
alignment model  proceedings   nd annual meeting association
computational linguistics  volume    long papers   pp            baltimore 
maryland  association computational linguistics 
telgarsky  m          benefits depth neural networks  arxiv             cs  stat  
tieleman  t     hinton  g          lecture    rmsprop  divide gradient running
average recent magnitude  coursera  neural networks machine learning 
van de cruys  t          neural network approach selectional preference acquisition  proceedings      conference empirical methods natural language processing  emnlp   pp        doha  qatar  association computational
linguistics 
vaswani  a   zhao  y   fossum  v     chiang  d          decoding large scale neural language models improves translation  proceedings      conference
empirical methods natural language processing  pp            seattle  washington  usa  association computational linguistics 
wager  s   wang  s     liang  p  s          dropout training adaptive regularization 
burges  c  j  c   bottou  l   welling  m   ghahramani  z     weinberger  k  q 
 eds    advances neural information processing systems     pp          curran
associates  inc 
wang  m     manning  c  d          effect non linear deep architecture sequence
labeling   ijcnlp  pp           
   

fia primer neural networks nlp

wang  p   xu  j   xu  b   liu  c   zhang  h   wang  f     hao  h       a   semantic clustering convolutional neural network short text categorization  proceedings
  rd annual meeting association computational linguistics
 th international joint conference natural language processing  volume    short
papers   pp          beijing  china  association computational linguistics 
wang  x   liu  y   sun  c   wang  b     wang  x       b   predicting polarities tweets
composing word embeddings long short term memory  proceedings
  rd annual meeting association computational linguistics
 th international joint conference natural language processing  volume    long
papers   pp            beijing  china  association computational linguistics 
watanabe  t     sumita  e          transition based neural constituent parsing  proceedings   rd annual meeting association computational linguistics
 th international joint conference natural language processing  volume
   long papers   pp            beijing  china  association computational linguistics 
weiss  d   alberti  c   collins  m     petrov  s          structured training neural
network transition based parsing  proceedings   rd annual meeting
association computational linguistics  th international joint conference natural language processing  volume    long papers   pp          beijing 
china  association computational linguistics 
werbos  p  j          backpropagation time  it  
proceedings ieee                     
weston  j   bordes  a   yakhnenko  o     usunier  n          connecting language
knowledge bases embedding models relation extraction  proceedings
     conference empirical methods natural language processing  pp 
          seattle  washington  usa  association computational linguistics 
xu  w   auli  m     clark  s          ccg supertagging recurrent neural network 
proceedings   rd annual meeting association computational linguistics  th international joint conference natural language processing
 volume    short papers   pp          beijing  china  association computational
linguistics 
yin  w     schutze  h          convolutional neural network paraphrase identification 
proceedings      conference north american chapter association computational linguistics  human language technologies  pp         
denver  colorado  association computational linguistics 
zaremba  w   sutskever  i     vinyals  o          recurrent neural network regularization 
arxiv            cs  
zeiler  m  d          adadelta  adaptive learning rate method  arxiv          
 cs  
zeng  d   liu  k   lai  s   zhou  g     zhao  j          relation classification via convolutional deep neural network  proceedings coling         th international
   

figoldberg

conference computational linguistics  technical papers  pp            dublin 
ireland  dublin city university association computational linguistics 
zhang  y     weiss  d          stack propagation  improved representation learning syntax  proceedings   th annual meeting association computational
linguistics  volume    long papers   pp            association computational
linguistics 
zhou  h   zhang  y   huang  s     chen  j          neural probabilistic structuredprediction model transition based dependency parsing  proceedings
  rd annual meeting association computational linguistics  th
international joint conference natural language processing  volume    long papers   pp            beijing  china  association computational linguistics 
zhu  c   qiu  x   chen  x     huang  x       a   re ranking model dependency
parser recursive convolutional neural network  proceedings   rd
annual meeting association computational linguistics  th international joint conference natural language processing  volume    long papers  
pp            beijing  china  association computational linguistics 
zhu  x   sobhani  p     guo  h       b   long short term memory tree structures 
arxiv             cs  

   


