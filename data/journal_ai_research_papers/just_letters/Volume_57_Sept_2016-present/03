journal artificial intelligence research                  

submitted        published      

lightweight random indexing
polylingual text classification
alejandro moreo fernandez
andrea esuli

alejandro moreo isti cnr it
andrea esuli isti cnr it

istituto di scienza e tecnologie dellinformazione
consiglio nazionale delle ricerche
      pisa 

fabrizio sebastiani

fsebastiani qf org qa

qatar computing research institute
hamad bin khalifa university
po box       doha  qa

abstract
multilingual text classification  mltc  text classification task documents
written one among set l natural languages  documents must
classified classification scheme  irrespective language  two main
variants mltc  namely cross lingual text classification  cltc  polylingual text
classification  pltc   pltc  focus paper  assume  differently
cltc  language l representative set training documents 
pltc consists improving accuracy  l  monolingual classifiers
leveraging training documents written   l     languages 
obvious solution  consisting generating single polylingual classifier juxtaposed
monolingual vector spaces  usually infeasible  since dimensionality resulting
vector space roughly  l  times monolingual one  thus often unmanageable 
response  use machine translation tools multilingual dictionaries
proposed  however  resources always available  always free use 
one machine translation free dictionary free method that  best knowledge  never applied pltc before  random indexing  ri   analyse ri
terms space time efficiency  propose particular configuration  that
dub lightweight random indexing lri   running experiments two well known public benchmarks  reuters rcv  rcv   a comparable corpus  jrc acquis  a parallel
one   show lri outperform  both terms effectiveness efficiency  number
previously proposed machine translation free dictionary free pltc methods
use baselines 

   introduction
rapid growth multicultural multilingual information accessible internet  properly classify texts written different languages become problem
relevant practical interest  multilingual text classification  mltc  text classification task documents written one among set l    l            l l   
natural languages  documents must classified classification scheme  irrespective language  two main variants mltc  namely
cross lingual text classification  cltc  polylingual text classification  pltc  
c
    
ai access foundation  rights reserved 

fimoreo  esuli    sebastiani

cltc task characterized fact that  languages subset lt
l  training documents  task thus consists classifying unlabelled
documents written languages lt  i e   target languages  leveraging
training documents expressed languages ls   l lt  i e   source languages  
cltc thus transfer learning problem  pan   yang         one needs transfer
knowledge acquired learning training data ls   task classifying
documents lt   previous work mltc indeed focuses cltc  fewer efforts
devoted pltc  instead focus paper 
pltc  representative set training documents languages l assumed
available  therefore  straightforward solution may consist training  l  independent
monolingual classifiers  one language  however  solution suboptimal 
classifier obtained disregarding additional supervision could obtained
using training documents written   l     languages  pltc thus
consists leveraging training documents written languages l improve
classification accuracy could obtained simply training  l  independent 
monolingual classifiers 
however  pltc entails number obstacles work detriment efficient
representation  see this  assume generate single polylingual vector space  hereafter 
juxtaposed vector space  juxtaposing monolingual vector spaces  vector
space monolingual dataset usually consists tens even hundreds thousands
features  juxtaposed vector space polylingual dataset  dimensionality gets
roughly multiplied number distinct languages consideration  substantial increase feature space would degrade performance many classification
algorithms  so called curse dimensionality  would bring
severe degradation efficiency  additionally  co occurrence based techniques tend lose
power representations polylingual  since terms belonging different languages
rarely co occur   a problem usually referred feature disjointness  
response  authors proposed use machine translation  mt  tools
device simultaneously cope high dimensionality feature disjointness
pltc  idea reduce problem monolingual case  typically english  
is  non english training documents automatically translated english  added
english training set  monolingual  english  classifier trained  classification
time  non english unlabelled documents translated english classified 
 of course  idea used cltc  case  training documents
translate   however  mt based pltc  and cltc  techniques suffer number
drawbacks  wei  yang  lee  shi    yang          i  automatically translated texts usually
present different statistical properties respect human translations   ii  mt systems
always available language pairs   iii  training statistical mt system
free toolkits available requires collecting large corpora parallel text
domain interest  always easy 
thesaurus based dictionary based methods  side  represent lighter
approach mltc  multilingual dictionary thesaurus encompasses different languages available  kind unification vector representation may
attempted  customarily done replacing non english words english equivalents dictionary  replacing terms thesaurus codes invariant
   

filightweight random indexing polylingual text classification

across languages  e g   babelnet synsets ehrmann  cecconi  vannella  mccrae  cimiano 
  navigli         however  bilingual dictionaries thesauri available language pairs  automatically constructing domain dependent bilingual resource requires
suitable parallel corpus sentence level alignment 
    distributional representations
classification purposes  textual document usually represented vector vector
space according bag of words  bow  model  i e   distinct term corresponds
dimension vector space  juxtaposed vector space  columns
document by term matrix thus informative one languages 
since distinct term corresponds dimension vector space  bow model
agnostic respect semantic similarities among terms  is  dimension
term governor orthogonal dimension related term president 
dimension unrelated term transport  semantic relations among terms
uncovered detecting co occurrences  i e   contexts words tend
used together  idea rests distributional hypothesis  according words
similar meanings tend co occur contexts  harris         detecting
co occurrences  possible establish parallelism term meaning geometrical properties vector space  distributed semantic models  dsms sometimes
called word space models sahlgren        aim learning continuous compact distributed term representations  recently called word embeddings  mikolov 
sutskever  chen  corrado    dean      b   dsms gained lot attention
machine learning community  delivering improved results many natural language processing tasks  bengio  schwenk  senecal  morin    gauvain        bullinaria   levy       
collobert  weston  bottou  karlen  kavukcuoglu    kuksa         dsm based methods
categorised  see pennington  socher    manning        baroni  dinu    kruszewski 
      belonging  a  class context counting models  often based
matrix factorization  e g   latent semantic analysis  lsa deerwester  dumais  furnas 
landauer    harshman        osterlund  odling    sahlgren          b  class
context predicting models  e g   methods based deep learning architectures  bengio 
      mikolov et al       b  
however  multilingual contexts huge quantities plain text language
processed order learn meaningful word representations  incurs high computational costs  trying find representations large multilingual vocabulary
thus become computationally prohibitive  attempts recently made
direction  leveraging multilingual external resources wikipedia articles
 al rfou  perozzi    skiena         bilingual dictionaries  gouws   sgaard        
word aligned parallel corpora  klementiev  titov    bhattarai         sentence aligned
parallel corpora  zou  socher  cer    manning        hermann   blunsom        lauly 
boulanger    larochelle        chandar  lauly  larochelle  khapra  ravindran  raykar   
saha         document aligned parallel corpora  vulic   moens         however 
external resources may always available language combinations and 
available  e g   wikipedia articles   may uneven quality quantity
languages english  alternatively  approaches require computationally
   

fimoreo  esuli    sebastiani

expensive post processing step align word representations across languages  mikolov  le 
  sutskever      a  faruqui   dyer        
article discuss efficient representation mechanisms pltc  i 
mt free   ii  require external resources   iii  incur high computational
costs  particular  investigate suitability random indexing  ri kanerva 
kristofersson    holst        sahlgren        effective representation mechanism
original co occurrence matrix pltc  ri context counting model belonging
family random projections methods  kaski        papadimitriou  raghavan  tamaki   
vempala         produces linear projections nearly orthogonal reduced space
original distances vectors approximately preserved  hecht nielsen 
      johnson  lindenstrauss    schechtman         ri expected deliver fast
semantically meaningful representations reduced space  viewed cheaper
approximation lsa  sahlgren         ri column polylingual
matrix produced depend single specific language  as instead
bow representation   hypothesize could advantageous pltc  since
entire new space becomes potentially informative languages once  thus making
problem easily separable enough dimensions considered  ri already
applied bilingual scenarios  gorman   curran        sahlgren   karlgren        
best knowledge tested pltc case far  monolingual
tc  ri found competitive  superior  bow  sahlgren   coster        
article demonstrate ri outperforms bow model pltc 
method present article  dub lightweight random indexing  lri  
inspired works achlioptas        li  hastie  church       
sparse random projections  goes one step pushing sparsity limit  lri
designed orthogonality projection base maximized  causes sparsity
preserved projection  empirically show lri helps support vector
machines  svms  deliver better classification accuracies pltc respect many
popular alternative vector space models  including main random projection variants 
lsa based approaches  polylingual topic models   requiring substantially
less computation effort 
contribution work twofold  first  conduct comparative empirical
study several pltc approaches two representative scenarios  first
training corpus comparable topic level  i e   documents direct translations
other  simply similar topics  exemplified rcv  rcv 
dataset   second training corpus parallel document level  i e  
text available languages thanks intervention human translators 
scenario exemplified jrc acquis dataset   show lri yields best results
settings  terms effectiveness efficiency  second contribution 
present analytical study useful better understand nature random
mapping methods 
rest paper organized follows  section   discuss related work 
section   present problem statement  describe random indexing method
detail  present proposal  section   reports results experiments
conducted  section   presents analytical study computational efficiency  section
  concludes 
   

filightweight random indexing polylingual text classification

   related work
section gives overview main approaches pltc emerged
literature  distinguish three groups methods  according whether problem approached  i  leveraging external resources   ii  combining outcome independent
monolingual classifiers   iii  reducing dimensionality resulting multilingual
feature space  discussion includes references cltc techniques
consider relevant pltc approach 
    exploiting external multilingual resources
multilingual text classification relatively recent area research  previous
efforts within devoted cltc subtask  cltc labelled
information languages  previous approaches typically relied automatic translation
mechanisms means fill gap source target languages 
main difference cltc pltc lies fact pltc exploits labelled
documents belonging different languages learning  despite this  two tasks
close knit relation  since cross lingual adaptation generally carried
means external resources  parallel corpora  bilingual dictionaries 
statistical thesauri 
suitable  unlabelled  multilingual corpus containing short aligned pieces texts
available  correlations among groups words two languages could explored 
cross lingual kernel canonical correlation analysis  cl kcca  proposed vinokourov  shawe taylor  cristianini        means obtain semantic cross lingual
representation  investigating correlations aligned text fragments  cl kcca
takes advantage kernel functions order map aligned texts high dimensional
space manner correlations mapped aligned texts jointly
maximized  cross lingual representation could used classification  retrieval 
clustering tasks  cl kcca investigated combination support vector machines  svms  applied cross lingual patent classification li shawe taylor
        method  called svm  k  learns two svm based classifiers searching two
linear projections original feature space language distance
projections  instead correlation projections  two aligned texts minimized 
similar vein  polylingual topic models  mimno  wallach  naradowsky  smith   
mccallum        proposed extension latent dirichlet allocation  lda
blei  ng    jordan        polylingual case  lda generative model
assigns probability distributions documents latent topics  latent topics
terms  distributions viewed compact representations documents
latent space  since topics discovered polylingual lda  plda  aligned across
languages  documents represented common vector space regardless language
written in  however  plda  which use baseline experimental
section  requires parallel collection documents aligned sentence level 
bilingual dictionaries used straightforward manner carry word byword translation feature space  however  dictionary based translations suffer
several deficiencies  e g   context unaware translations might perform poorly handling
polysemic words  dictionaries might suffer substantial lack coverage novel terms
   

fimoreo  esuli    sebastiani

domain dependent terminology  dictionaries might available language
pairs  free use  response drawbacks  automatic acquisition
statistical bilingual dictionaries proposed  wei et al         explored cooccurrence based method measure polylingual statistical strength correlation
among words parallel corpus  correlations taken account reinforce weight feature order select important  highly weighted  ones 
gliozzo strapparava        experimented bilingual dictionaries and  interestingly  provided means automatically obtain multilingual domain model  mdm  
natural extension domain models multiple languages  additional multilingual resources available  domain model defines soft relations words
domain topics  absence multilingual dictionary  mdm could automatically
obtained comparable corpus performing latent semantic analysis  explained
detail below  
argued words shared across languages play important role
searching semantic latent space  accordingly  steinberger  pouliquen  ignat
       exploit language independent tokens shared across languages 
propose simple method link documents existing external resources thesauri 
nomenclatures  gazetteers  finally  de melo siersdorfer        use ontologies
map original features onto synset like identifiers  documents translated
language independent feature space 
mt tools  side  provide elaborated translations texts  represent promising research field multilingual tasks  unfortunately  above mentioned
problems regarding availability  accessibility  performance still hold case 
effect different translation strategies cltc investigated bel  koster 
villegas         rigutini  maggini  liu         wei  lin  yang        
even available  mt tools may expensive resources  reason 
experiments prettenhofer stein        restrict use mt tool limited budget
calls  structural correspondence learning  scl  method  initially proposed
domain adaptation  indeed applied cltc  key idea method consists
discovering cross lingual correspondences pairs terms  dubbed pivot features 
later used bridge across two languages  pivot features play important
role bilingual tasks  since establish pairs words behave similarly source
target languages  allowing one find cross language structural correspondences  one
special type pivot features obviously words shared across languages 
proper nouns  technical terms  yet lexicalized terms  stemmed forms etymologically
related terms  nastase strapparava        found etymological ancestors words
actually add useful information  allowing transcend cross lingual boundaries 
method however depends availability etymological thesauri  such wikipedias
wiktionary  etymological wordnet   remains restricted historically interrelated
languages 
sum  applicability multilingual methods discussed section usually
constrained availability external resources  aim overcoming limitations  restrict investigations dictionary free  mt free multilingual methods 
   

filightweight random indexing polylingual text classification

    monolingual classifiers multiview learning
given availability representative set labelled documents language 
simple baseline  known nave polylingual classifier  could obtained delegating
classification process individual monolingual classifiers  built upon separate
monolingual data  solution sub optimal  classifier exploit labelled
information languages  type information might provide insights
different perspectives semantics classes 
garca adeva  calvo  lopez de ipina        compared different nave strategies 
considering one single polylingual classifier  i e   classifier works juxtaposed
representation   c   vs  various monolingual ones  nc   one language independent
preprocessor   p  vs  various language specific ones  np   using various learning methods
bilingual spanish basque benchmark  experimentation combinations npnc np  c  consider baselines  yielded best results terms
running time  memory usage  accuracy 
even though training separate language specific classifiers simple way approach
pltc task  strategies could improve final accuracy better
merging outcomes classifier  multiview learning  xu  tao    xu        tc
deals parallel texts  i e   case document available languages 
language considered separate source  shown amini  usunier 
goutte        multiview majority voting algorithm  returns label output
highest number language specific classifiers  outperforms nave polylingual
classifier multiview gibbs classifier  bases predictions mean prediction
language specific classifier  amini goutte        proposed co regularization
approach multiview text classification minimizes joint loss function takes
account language specific classifier loss  however  availability parallel
corpus containing documents views strong restriction  usually
alleviated leveraging machine translation tools automatically generate missing
documents views 
    dimensionality reduction multilingual classification
one main challenges juxtaposed vector space approach pltc concerns
relevant increase number features represent documents  i e   dimensionality vector space  rigutini et al          feature selection methods attempt
select reduced subset informative features original set f size
subset much smaller  f   reduced set yields high classification
effectiveness  tc problem usually tackled via filtering approach  relies
mathematical function meant measure contribution feature classification task  yang pedersen        showed filtering approaches may improve
performance classification  even aggressive reduction ratios  e g   removal    
features  
another important dimensionality reduction technique latent semantic analysis
 lsa aka latent semantic indexing   originated information retrieval
community  deerwester et al          later applied cross lingual classification  gliozzo   strapparava        xiao   guo        cross lingual problems general
   

fimoreo  esuli    sebastiani

 dumais  letsche  littman    landauer         lsa maps original document term matrix lower dimensional latent semantic space attempts capture  linear 
relations among original features documents  mapping carried
means singular value decomposition  svd  original document term matrix  
svd decomposes   v u   diagonal matrix containing eigenvalues   approximation mk   vk k ukt original matrix computed
taking k largest eigenvalues setting remaining ones    mk said
rank k optimal terms frobenius norm  vk uk orthogonal matrices
explain relations among pairs terms pairs documents  respectively 
although lsa successfully used discover hidden relations indirectly
correlated features  case terms belonging different languages  suffers
high computational costs  random mappings arise alternative lsa 
perform comparably different machine learning tasks preserving important
characteristics lsa  bringing about  time  significant savings
terms computational cost  fradkin   madigan         random projections  rps
papadimitriou et al         random mappings  rms kaski        two equivalent
formulations deriving johnson lindenstrauss lemma  johnson et al         
states distances euclidean space approximately preserved projected onto
lower dimensional random space  formulations based fundamental
result hecht nielsen         proved many nearly orthogonal
truly orthogonal directions high dimensional spaces 
rp like methods formalized terms projection original documentterm matrix means random matrix   i e   m d n   m d  f    f  n  
approximates identity matrix   d   f   indicate number documents
terms collection  n stands reduced dimensionality  typically
chosen advance  definition random projection matrix fundamental
aspect method  achlioptas        demonstrated random distribution
zero mean unit variance satisfies johnson lindenstrauss lemma  proposed two
simple distributions definition elements ij    ij   random projection
matrix  setting parameter distribution equation   either         

 
   probability  s

  probability    s
ij  

   

 
  probability  s
achlioptas proved configuration     used speed computation  since case     data non zero  sparse random projection  pand
therefore     computations skipped  similarly  li et al         set    f  
   f    log  f    very sparse random projections  significantly speed computation still preserving inner distances 
random indexing  ri   first proposed kanerva et al          equivalent formulation rps accommodates achlioptas theory  sahlgren        defines ri
approximate alternative lsa semantic representation  ri maintains dictionary
random index vectors feature original space  random index vector consists n dimensional sparse vector k non zero values  randomly distributed across
      the method explained detail section     work gorman
   

filightweight random indexing polylingual text classification

curran        different weighting criteria random index vectors dictionary
proven useful improving matrix representation  ri tested different tasks 
search  rangan         query expansion  sahlgren  karlgren  coster    jarvinen 
       image text compression  bingham   mannila         event detection  jurgens   stevens         fradkin madigan        showed that  since ri distances
approximately preserved  distance based learners k nearest neighbours  k nn 
svms preferable learning randomly indexed instances  accordingly 
sahlgren coster        applied ri  monolingual  text classification using svms 
suggested random indexing representation  there dubbed bag concepts
bocs sahlgren   coster        performed comparably bow representation 
performance ri tested sahlgren karlgren        gorman
curran        realm automatic bilingual lexicon acquisition 
above discussed works indicate ri promising dimensionality reduction technique representing polylingual data  proposal inspired works achlioptas
       li et al         sparse projections taking level sparsity extreme  extends application ri tc  sahlgren   coster        pltc  which 
best knowledge  never done far  following section first
describe method detail  propose particular setting aimed overcoming
certain obstacles could arise polylingual setting 

   lightweight random indexing polylingual text classification
text classification  tc  formalized task approximating unknown target
function   c          indicates documents ought classified 
means function   c          called classifier  coincide
much possible terms given evaluation metric  denotes domain
documents  c    c    c         c c    set predefined classes  values     
indicate membership non membership document class  respectively 
consider multilabel classification  is  setting document
could belong zero  one  several classes time  consider flat
version problem  hierarchical relations among classes exist  adopt
  vs  strategy  according multilabel classification problem solved
 c  independent binary classification problems 
document collection represented via matrix m d  f  
 

d 
w  
w   w  f  
d   w  
w   w  f  



       
  
  
  
     
 
 
 
w d   w d   w d  f  
d  d 

   

 d   f   number documents features collection  real
values wij represent weight feature fj document di   usually determined
function frequency feature document collection 
polylingual text classification adds one fundamental aspect tc  i e   different documents may belong different languages  let   l return language
   

fimoreo  esuli    sebastiani

given document written  l    l    l            l l    pool languages   l      
s l 
let f   i   denote vocabulary collection  expressed
union language specific vocabularies   polylingual setting assumes
distribution p   d    li   across training set approximately uniform  is 
representative quantity labelled documents language 
usually small amount shared features across languages  e g   proper
nouns     implies hd     d       d        d      h  denotes dot
product   incidentally  means direct similarity comparison among documents
expressed different languages  e g   using cosine similarity  would doomed fail  
thus possible  language li   perform reordering rows columns

m  m   
matrix allows polylingual matrix expressed  
 
  m   
 m   
   
  d    d    li     fi   monolingual matrix representation
m 
language li  
 d  matrix containing words shared across two
m 
languages    denotes all zero matrices 
    random indexing
random indexing maps observable problem feature random vector vector
space number dimensions determined number different unique
features want map  instead fixed advance  originally  ri proposed
performing semantic comparisons terms  document thus mapped
random index vector accumulated  via vector addition  terms row
term document matrix time term occurred document  case 
instead interested performing semantic comparisons documents  terms 
thus  term assigned n dimensional random index vector  accumulated
j th row document term matrix every time term found document dj  
random index vectors nearly orthogonal  comply conditions spelled
achlioptas         see section       i e   zero mean distribution unit variance 
satisfy johnson lindenstrauss lemma  random index vector created randomly
setting k n non zero values  equally distributed       n dimensional
vector n typically order thousands  n fixed  recommended
choice k literature k   n      dub configuration ri     use
comparative experiments  vectors ri   sparse  using sparse data structure
representations could bring memory savings  m d n   m d  f    f  n matrix
multiplication  see section      completely skipped  building m d n on the fly
scanning document accumulating corresponding random index vectors
term read  avoids need allocate entire matrix m d  f   memory 
according sahlgren         main advantages ri summarized follows 
method  i  incremental  provides intermediate results data read
   note formulations polylingual problem  e g   ones amini et al        
prettenhofer stein         actually impose    j fj     means
shared words across languages  proper nouns  given multiple representations languagespecific features 

   

filightweight random indexing polylingual text classification

in   ii  avoids so called huge matrix step  i e   allocating entire m d  f   matrix
memory    iii  scalable  since adding new elements data increase
dimensionality space  e g   new features represented via new random index 
via new dimension  
bow matrices typically weighted normalized better represent importance
word document avoid giving long documents priori importance 
respectively  weighting schemes could incorporated ri formalism
simple manner  e g   time random index added document row  first
multiplied weight term document  brings improved
accuracy shown gorman curran         however  work
shown incremental nature algorithm sacrificed non linear weights
taken account  experiments  weighting criterion use well known
tfidf method  expressed
tfidf  di   fj     tf  di   fj   log

 d 
 d   tf  d  fj       

   

tf  di   fj   counts number occurrences feature fj document di   weights
normalized via cosine normalization 
wij   qp

tfidf  di   fj  

fk f

   

tfidf  di   fk   

    lightweight random indexing
preliminary experiments application ri method dimensionality
reduction  observed svms required time train training set
processed ri  original high dimensional vector space  see section
      observed correlation training times choice k 
choice n smaller impact efficiency 
optimizing choice k ri though means achieve two main goals 
 i  able encode large number different features reduced space   ii 
increasing chance two random index vectors orthogonal 
respect  i   easy show that  want assign different n dimensional
index vector
k non zero values original feature  ri could encode maximum
c n  k    nk  k features  representation capacity   c n  k  grows rapidly function
either n k  example  c                       huge capacity clearly
exceeds representation requirements imposed current future dataset  however 
even small values k capacity becomes large enough encode reasonable
dataset  e g   c                    distinct features 
respect  ii   random projection based algorithms rely hecht nielsen
       lemma find nearly orthogonal directions reduced space  two vectors  u
 v inpan inner product space said orthogonal whenever h u   v     
h u   v   ui vi dot product  random indexes chosen sparse order
increase probability dot product equals zero  non zero products evenly
distributed       leaving expected value outcome close zero 
   

fimoreo  esuli    sebastiani

figure    probability orthogonality two random index vectors function k
n 

means monte carlo algorithm  estimated probability orthogonality
two randomly generated vectors grid sample values n k  results 
plotted figure    reveal smaller values k main factor favouring
orthogonality two random index vectors  n smaller impact 
many random index vectors lack orthogonality  information conveyed original distinct features  predominantly pair wise semantically unrelated  gets mixed
up  causing learner difficulty learning meaningful separation patterns
them  orthogonality random index vectors plays even important role
features shared across languages  shown work gliozzo strapparava         shared words play relevant role bringing useful information across
languages  corresponding random index vectors orthogonal respect
vectors  information contribute process maximized  instead
diluted less informative features 
following observations above  propose use random indexing fixed
k      dub configuration lightweight random indexing  lri   hypothesis
setting could advantageous mechanism reduce dimensionality  so
mitigate problem feature disjointness pltc   since sufficient order
represent large feature vocabularies preserving vector orthogonality  note
choosing k      n    f    would equivalent performing random permutation
   

filightweight random indexing polylingual text classification

 

 

 

 

 

 

 
 

output  dictionary 
   generate random index vector feature
      f     
   choose  st dimension sequentially
dim   i mod n       
   choose  nd dimension uniformly random
   dimensions chosen line  
dim  rand          n      dim      
   assign  st non zero value uniformly random
  
val  rand  
         
 
    nd non zero value
  
val  rand  
         
 
   create sparse random index vector
random index vector   dim    val      dim    val      
   build feature vector mapping
dictionary map fi     random index vector   
end
algorithm    feature dictionary lightweight random indexing 

feature indexes bow representation  k     minimum value actual
ri performed 
algorithm   formalizes process creating dictionary  is  creating mapping
consisting one random vector original feature  mapping created training
time used classifying unlabelled documents this means that  line    f
set features present training set   value      used instead   order
obtain vectors length one  note two dimensions selected different
manner  step line   ensuring latent dimensions used approximately
number times  step line   ensuring dimension chosen
previous step chosen twice 
proposal presents following advantages respect standard ri   and 
general  respect ri k     
index vector two non zero values  mapping allocated
memory number original features  projection performed
quickly 
given fixed value n  higher probability instantiation
ri generating truly pairwise orthogonal random vectors 
parameter k becomes constant needs tuning 

   

fimoreo  esuli    sebastiani

   experiments
section experimentally compare lightweight random indexing  lri  method
representation approaches proposed literature 
    baselines implementation details
baselines compare lri chosen following methods 
group three categories according common characteristics 
orthogonal mappings  methods using canonical basis co occurrence matrix 
polybow  classifier operates juxtaposed bow representation  polybow
corresponds np  c setup garca adeva et al         
fs  feature selection polybow using information gain term scoring function round robin  forman        term selection policy 
majority voting  multiview voting algorithm returns label output
highest number language specific classifiers  amini et al         
monobow  lower bound baseline uses set nave monolingual classifiers
 monobow corresponds np nc setup garca adeva et al         
mt  upper bound baseline based statistical machine translation  translates non english training test documents english 
random mappings  dimensionality reduction methods relying random projections 
ri     random indexing k   n      sahlgren   coster        
ach  achlioptas mapping ternary distribution obtained setting    
equation    achlioptas        
non random mappings  dimensionality reduction methods relying mappings
random 
cl lsa  cross lingual latent semantic analysis  dumais et al         
mdm  multilingual domain models  gliozzo   strapparava        
plda  polylingual latent dirichlet allocation  mimno et al         
assume language labels available advance  training testing
documents  note ri methods polybow represent documents
feature space  irrespective language label  conversely  monobow keeps separate
language specific classifier language  class label test document
decided classifier associated documents language label  test plda
majority voting jrc acquis parallel corpus  since documents
require separate view languages available  majority voting maintains
separate classifier distinct language    experiments   test document
thus classified using   classification decisions voting  one language specific
   assumption fair  current language identification models deliver accuracies close     

   

filightweight random indexing polylingual text classification

view  singular value decomposition used rohde        package 
used haddow  hoang  bertoldi  bojar  heafield        implementation generate
set statistical translation systems trained sentence aligned parallel data provided
europarl data release  koehn         note that  since used method described
gliozzo strapparava        automatically obtain bilingual model mdm 
mt method using external knowledge  plda used richardson
       implementation  uses gibbs sampling  adhere common practice
fixing budget iterations        implemented lri method
baseline methods part esuli  fagni  moreo        framework 
used support vector machines  svms  learning device cases  since
consistently delivered state of the art results tc far  used well known
joachims        implementation joachims         default parameters 
    evaluation measures
effectiveness measure use well known f    harmonic mean precision
   recall    defined f                 t p     t p   f p   f n   p  
f p   f n stand numbers true positives  false positives  false negatives 
respectively  take f      p   f p   f n      since classifier correctly
classified examples negative 
compute micro averaged f   denoted f    macro averaged f   denoted
f m    f  obtained  i  computing class specific values pr   f pr   f nr    ii 
obtaining p summation pr  same f p f n    applying
f  formula  f m obtained first computing class specific f  values
averaging across classes  fact f m attributes equal importance
classes means low frequency classes important high frequency ones
determining f m scores  f  instead influenced high frequency classes
low frequency ones  high values f m thus tend indicate classifier performs well
low prevalence classes  high values f  may indicate classifier
performs well high prevalence classes 
    datasets
performed experiments two publicly available corpora  rcv  rcv   a
comparable corpus  jrc acquis  a parallel corpus  
      rcv  rcv 
rcv  publicly available collection consisting         english news stories generated reuters    aug         aug       lewis  yang  rose    li         rcv 
instead polylingual collection  containing         news stories generated
timeframe thirteen languages english  dutch  french  german  chinese 
japanese  russian  portuguese  spanish  latinoamerican spanish  italian  danish  norwegian  swedish   union rcv  rcv   hereafter referred rcv  rcv  
corpus comparable topic level  news stories direct translations
simply refer related events different languages  since cor   

fimoreo  esuli    sebastiani

pus parallel  training document given language general
counterpart languages 
rcv  rcv  randomly selected       news stories   languages  english 
italian  spanish  french  german  pertaining last   months  from           
             performed         train test split  thus obtaining training set
       documents        language  test set        documents       
language     experiments restricted attention    classes
 out      least one positive training example five languages 
average number classes per document       ranging minimum  
maximum     number positive examples per class language combination ranges
minimum   maximum       
preprocessed corpus removing stop words stemming terms using
porter stemmer english  snowball stemmer languages 
resulted total         stemmed terms  distributed across languages shown table
  

english
italian
spanish
french
german

english
      

italian
     
      

spanish
     
     
      

french
     
     
     
      

german
     
     
     
     
      

appearing
  languages
  languages
  languages
  languages
  languages

 
       
      
     
     
   

table    feature distribution across languages rcv  rcv  comparable corpus 
leftmost part table  cell row column j represents
number features shared across i specific j specific sections
dataset   the table symmetric  better clarity entries
diagonal omitted   rightmost part table indicates many
features shared across x language specific sections dataset 

      jrc acquis
jrc acquis corpus  version      version acquis communautaire collection
parallel legislative texts european union law written     s     
 steinberger  pouliquen  widiger  ignat  erjavec  tufis    varga         jrc acquis
publicly available research purposes  covers    official european languages 
corpus parallel sentence level  i e   document exists    languages 
sentence by sentence translation  corpus labelled according ontology based
eurovoc thesaurus  consists       classes  experiments
restricted attention    classes top level eurovoc hierarchy 
   information required replicate experiments  e g   ids selected documents  assigned
labels  etc   publicly available  moreo         source code used experiments accessible
part esuli et al         framework

   

filightweight random indexing polylingual text classification

english
italian
spanish
french
german

english
       

italian
      
       

spanish
      
      
       

french
      
      
      
       

german
      
      
      
      
       

appearing
  languages
  languages
  languages
  languages
  languages

 
       
      
      
      
      

table    feature distribution across languages jrc acquis parallel corpus 
meaning cells table    note high number features          appear five languages  due presence
proper names  languages  note high number
features           unique german language  due
presence word compounds  phenomenon present german language
four languages 

selected       texts        languages  english  italian  spanish 
french  german  removed documents without labels  thus obtaining       documents per language  taken first     documents training          i e  
      language  remaining              i e         language 
testing  average number classes per document      ranging minimum
  maximum     number positive examples per class language combination
ranges minimum    maximum       
preprocessing rcv  rcv  carried dataset  obtaining         distinct features distributed across languages shown table    since
jrc acquis corpus parallel  language specific document guaranteed
counterpart languages  results relatively large number
terms  e g   proper nouns  appearing several languages  note that  despite fact
dataset parallel sentence level  interested indexing entire documents
whole  thus disregard sentence order  thus consider corpus parallel
document level 
use jrc acquis corpus order test performance lri cases
co occurrence matrix compacted  defined work dumais et al 
        precisely  compact representation  l  translation equivalent documents
vector consisting concatenation  l  vectors represent one  monolingual  document  different juxtaposed representation used
previous chapters  vector corresponding one monolingual document
zeros positions corresponding features languages  compact
matrix thus obtained matrix resulting juxtaposed representations
compressing  l  rows single  compact  row storing sum 
   

fimoreo  esuli    sebastiani

    results
section present results experiments  first compare lri set
monolingual classifiers  section         explore dimensionality reduction
aspect polylingual problem  section        
      polylingual information
first case study  investigate much addition polylingual information
affects accuracy monolingual classifier  scenario  compare lri
polybow  train documents languages  lower bound monobow 
trains documents language test documents  upper
bound mt  first translates training test documents english  note
mt baseline tested jrc acquis corpus documents
already available direct translation languages  experiment vector space
reduced  i e   set n    f   lri vector spaces polybow
lri number dimensions  values lri averaged    runs 
results illustrated figure   show simple addition examples different languages  polybow  brings improvement accuracy respect
monolingual solution  monobow   improvement likely achieved thanks words
shared across languages  however  lri clearly outperforms polybow  improvements
polybow monobow range               lri achieves improvements ranging               lri obtains smallest improvement
monobow terms f m  on italian          polybow performs slightly worse
monobow          improvements marked f m f    indicating
improvements especially take place infrequent classes 
substantial impact f m f   
general  training documents coming languages  polybow  lri  mt 
seems preferable training language specific documents  monobow  
particularly mt baseline  obtained best results cases
sole exception english  lri obtained best result  exception might
explained fact automatically translated documents tend exhibit different
statistical properties respect documents written humans  means
english test documents  which translations  might tune training
documents  which mostly result automatic translation  
language specific classification performance much homogeneous jrcacquis rcv  rcv   explained fact jrc acquis parallel
corpus  therefore language benefits information 
significant difference performance among different languages  means
effects due different difficulty various languages minor  instead  differences
rcv  rcv  explained different amount information training
sets carry corresponding test sets  example  spanish classifier worst
performer  one obtains best benefit  with respect monobow
baseline  addition polylingual information  as polybow  lri  mt  
   

filightweight random indexing polylingual text classification

figure    monolingual classification rcv  rcv   top  jrc acquis  bottom   using f m  left  f   right  evaluation measure 

note experiment matrices polybow lri feed learning
algorithm size  difference two methods  likely
cause difference effectiveness  polybow useful dimensions
specific language packed specific portion vector space  lri spreads
across entire vector space  causing dimensions become potentially useful
languages  note substantial increase number useful dimensions
available language allows model create easily separable representations 
discuss aspect section     
      dimensionality reduction
polybow setup dimensionality vector space substantially increased
languages considered training  following experiments explore
dimensionality reduction aspect problem  address realistic polylingual
scenario  training test data contain examples language 
   

fimoreo  esuli    sebastiani

n
monobow
polybow
mt
cl lsa
mdm
ach
ri  
lri

   
     
     
     
     
     
     

     
     
     
     
     
     
     

f m
     
     
     
     
     

      
     
     
     
     

full
     
     
     
     

   
     
     
     
     
     
     

     
     
     
     
     
     
     

f 
     
     
     
     
     

      
     
     
     
     

full
     
     
     
     

figure    effects dimensionality reduction rcv  rcv   english italian   dotted
lines indicate reference values  e g   green red lines represent performance
lri polybow  respectively  dimensionality reduced  values
bold highlights best performing method dimension 

first run sample bilingual experiment rcv  rcv   as language
english picked italian   total amount features dataset
        restricting experiment two languages allows us compare lri  i 
methods proposed bilingual representations  mdm    ii  methods
would computationally expensive considering languages  such ach 
see below   explore effect dimensionality reduction  number selected
features ranging             figure     adhere common practice
establishes number dimensions ranging          lsa mdm  results
random projection methods  ach  ri     lri  averaged    runs 
lri obtains good results macro  micro averaged f    methods exhibit alternating performance two measures  ri   obtains comparable results
terms f m performs poorly f    contrast  polybow performs comparably
terms f  worse terms f m   two tailed t test paired examples reveals
difference terms f m lri ri   statistically significant 
   

filightweight random indexing polylingual text classification

n
monobow
polybow
mt
cl lsa
ri  
lri

   
     
     
     
     

     
     
     
     
     

f m
     
     
     
     

      
     
     
     

full
     
     
     
     

   
     
     
     
     

     
     
     
     
     

f 
     
     
     
     

      
     
     
     

full
     
     
     
     

figure    accuracy different pltc methods rcv  rcv    languages  different
levels dimensionality reduction 

lri significantly outperforms ri   f  rest dimensionality reduction
methods evaluation measures  p          surprisingly  cl lsa mdm
perform worse nave classifier  monobow  features  however 
remarked outperform baselines          dimensions 
seen section    apart drastic dimensionality reduction  methods affected large computational costs negatively impact run times
memory resources needed  consistently previous observations  see figure     lri 
polybow  monobow  mt comparable terms f    lri outperforms
tested algorithms terms f m  
test scalability method several languages involved  extend
experiment five languages  english  italian  spanish  french  german  rcv  rcv 
 figure     note case algorithms able complete execution
due memory constraints  hence incomplete plots table  concretely  ach
last iterations ri   overflowed memory resources trying allocate        
         matrix  insights space time complexity reported section   
results ri   lri average    runs use different random seeds 
results confirm previous observations  ri   behaves similarly lri terms
f m  i e   statistically significant difference  worse terms f   p         
   

fimoreo  esuli    sebastiani

n
polybow
cl lsa
plda
majority vote
ri  
lri

   
     
     
     
     
     

     
     
     
     
     
     

f m
     
     
     
     

      
     
     
     

full
     
     
     

   
     
     
     
     
     

     
     
     
     
     
     

f 
     
     
     
     

      
     
     
     

full
     
     
     

figure    accuracy different pltc methods jrc acquis   languages  different
levels dimensionality reduction 

polybow behaves opposite way  i e   performs worse lri terms
f m  p          comparably terms f    dimensionality reduction method 
lri thus outperforms methods considering f m f   
dimensionality reduction applied  upper bound mt comparable lri
f m f   
finally  used jrc acquis reproduce one last polylingual scenario  namely  one
texts aligned document level  even situation common
practice  exceptions include  say  proceedings official events   scenario interesting
since dataset may serve test bed multiview learning methods  amini et al  
       since documents jrc acquis translated humans  results affected
noise mt tools might introduce  figure   shows results obtained considering
compacted matrix jrc acquis  a                 matrix   tested majority voting  combines classification decisions five independently trained
monobow classifiers parallel versions documents  plda  first defines
generative model based polylingual topics trains tests probability distributions topics assigned document  set number polylingual
latent topics           respectively 
lri clearly superior polybow case  difference performance
lri ri   seems lower case  especially terms f m   t test revealed
   

filightweight random indexing polylingual text classification

however lri superior ri   statistically significant sense  p          however 
considered lri delivers best performance without reducing dimensionality polylingual matrix  ri   able accomplish projection due
memory restrictions  something expand following section  plda 
turn  succeeded discovering polylingual topics aligned across languages 
proved less effective terms classification performance 

   analysis
experiments observed substantial differences terms efficiency among
compared methods  particularly ach  ri     lri  example  ri  
exhausted memory resources n          lri able represent even fullsized  d  f   matrix  see figure     given strong relationship two methods 
would expected delivered similar performance  anomaly prompted us
investigate issue depth  section presents analytical study terms
efficiency methods discussed previous section 
    space efficiency
data samples ml usually represented co occurrence matrix  tc matrix
suffers high dimensionality  luckily enough sparse  sparse  low density
matrix suggests use non exhaustive data structure  zero values
stored explicitly 
random projection direct impact sparsity  feature contained
document  k non zero values placed projected matrix  ach situation
worse  since feature mapped  average  n   non zero values  example 
n          feature mapped          non zero values ri  
ach  respectively 
example  rerun rcv  rcv  experiments english italian
languages  examined matrix density  percentage non zero values
total matrix size  memory footprint  absolute number non zero values  
results displayed figure   
lri requires double space respect standard bow  succeeds preserving
sparsity  ri   drastically increases matrix density produces large memory
footprint  mdm  lsa  ach operate dense matrices  however  since mdm
lsa produce extreme dimensionality reduction  overall memory footprint remains
much lower ri   and  especially  ach  n    f    lri must allocate
          values  this indicated lri  full  figure     ri    n        
must allocate             values  requiring       times space   ach  n        
must allocate             values        times space   note even though mdm
lsa reduce significantly dimensionality  e g                      
need allocate values memory lri  full  
example  let us suppose non zero value represented double
 typically    bytes modern programming languages   means roughly need
   mb ach    mb ri     whereas lri requires   mb  although
difference substantial   even taking account actual memory needed higher
   

fimoreo  esuli    sebastiani

figure    matrix density  left  memory footprint  right  rcv  rcv  englishitalian run                  full training matrix size  

values indexed hash table  still represent real problem
terms space modern computers  however  note matrix
data structure need allocate memory  mapping dictionary  i e   data
structure linking original feature random index vector  allocated
memory  dictionary queried many times terms document
want classify  dictionary small enough  which lri   may able
allocate cache order significantly speed indexing new documents 
assuming sparse representation  random index vector described list k
pairs  di   vi    di indicates latent dimension vi encodes value  example 
k     random vector                          could represented                  
bit set   encodes    bit set   encodes    equation   
space occupation dictionary random indexing method depends  i   f   
number indexes   ii  k  number non zero values index   iii 
number bits needed indicate one latent position encode possible non trivial
values  is 
cost rik     o  f   k  log  n   log     

   

turns that  given expected number non zero values ach n    using
dense representation index cheaper  position thus indicates one
three possible values index  cost terms space ach index dictionary
described
cost ach    o  f   n log    
   

   

filightweight random indexing polylingual text classification

method
lri
ri  
ach

index type
sparse
sparse
dense

index size
 
   
      

index cell
log  n   log    bits encode dimi vali   resp 
log  n   log    bits encode dimi vali   resp 
log    bits encode ij

memory required
    mb
     mb
      mb

table    memory occupation feature dictionary different random mapping methods jrc acquis dataset   f                meanings dimi vali
algorithm    meaning ij equation   

assuming reduced dimensionality set fixed percentage original dimensionality  i e   n    f          following hold 
cost ri    o  f    log   f     
cost ach    o  f       

   

cost lri    o  f   log   f   
however  hidden constants play key role practice  example  computed
total amount memory required method storing index dictionaries
n           jrc acquis   f               resulting values reported table
   observed  index dictionary ach requires    mb  space
required ri based versions one three orders magnitude smaller 
words  index dictionary lri could easily fit current cache memories  ri  
ach need resort higher capacity  thus slower  storage devices 
    time efficiency
usually case sparsity benefits space occupation  execution
time  example  computational cost svd o  f     d   document by term
matrix  however  implementation svdlibc specifically optimized sparse matrices
requires o c f   d   steps  c average number non zero values vector 
figure   plot run times experiments bilingual  english italian 
rcv  rcv  experiment paying attention time required  i  obtaining
transformed index training set   ii  training learning algorithm  svm    iii 
obtaining transformed index test set   iv  classifying test documents 
experiments run intel i    bit processor    cores  running
     mhz    gbs ram memory 
results show takes     minutes generate test classifier
uses bow representation  time slightly reduced   minutes
     features selected  total time lri roughly higher factor   
     full       n         minutes  respectively  notwithstanding this  figures
still low compared methods  training testing times grow
substantially ri ach  regarding latent methods  pointed
time required preparing matrices grow substantially  due large
   

fimoreo  esuli    sebastiani

figure    run times rcv  rcv   english italian setting  

computational cost inherent svd matrix multiplication  case random
indexing methods times negligible 
comparing overall memory footprint  figure    right  execution times  figure    seems clear strong correlation them  investigated
dependency experiments computing pearson correlation them 
pearson correlation quantifies degree linear dependence two variables 
ranges    meaning perfect negative correlation      meaning perfect positive
correlation  whereas   means linear dependency  found linear
pearson correlation               number non zero values
matrix times required training testing  respectively  brings additional
support observation  preserving sparsity projection favours execution
times pltc 
    effect k random indexing
previous work ri  see  e g   sahlgren   karlgren        sahlgren   coster        tend
set k    dimension vector  smaller values k  about k        
explored  karlgren  holst    sahlgren         works related random
projections  see  e g   achlioptas        li et al         noticed sparse projection
matrices help speed computation 
besides run times  sparsity projection matrix affects orthogonality
random projection  turn impact preservation relative
distances  two random vectors ri rj said orthogonal angle
   degrees  although probability two randomly picked vectors
orthogonal increases dimensionality vector space grows  karlgren et al         
random projection approaches choose sparse random vectors  maximize
probability 
   

filightweight random indexing polylingual text classification

figure    probability distribution angle two arbitrary vectors highdimensional space  left   excess kurtosis function non zero values
projection matrix        dimensions  right  

could thus establish parallelism degree orthogonality projection matrix probability distribution angle two random vectors 
probability distribution skewed towards    degrees  closer orthogonal
projection base is  better distances preserved  propose quantify
orthogonality means excess kurtosis distribution angle    aim 
studied kurtosis angle distributions  as estimated via monte carlo
algorithm  varies function matrix sparsity k        dimension projection
matrix  figure    right  
figure   shows orthogonality projection  fixed dimensionality 
rapidly degrades density increases  lri thus expected produce nearly
orthogonal indexing  followed ri ach 
investigated relation orthogonality pltc accuracy 
aim  run series experiments bilingual version rcv  rcv  
varying  from        number k non zero values  from              
reduced dimensionality n  figure   shows contour lines  equally valued points
  dimensional representation  classification performance  here measured terms
f     execution time  probability pairwise orthogonality  i e   probability
hri   rj     two randomly chosen random index vectors  
following trends directly observed results   i  accuracy improves
n increases k decreases   ii  run times tend grow n k increase 
 iii  higher dimensionality n smaller parameter k  higher
probability finding two orthogonal random indexes 
   excess kurtosis random variable x typically defined fourth standardized moment minus
   i e   ekurt x         

   

fimoreo  esuli    sebastiani

figure    impact dimensionality n  on x axis  number k non zero values  on
axis  classification accuracy  left   execution time  center   probability
finding orthogonal pair random indexes  right   darker regions represent
lower values 

figure    behaviour lri method propose described green
horizontal line bottom plot  ris behaviour described blue
diagonal line coordinates  n           k        n            k         performance ri improves cost space time efficiency  gradually disrupting
orthogonality base  contrary  following desirable features lri
evident  dimensionality increases  i  accuracy improves without penalizing execution
times  due preservation sparsity   ii  orthogonality base improved 

   conclusions
compared several techniques polylingual text classification  checking suitability dimensionality reduction techniques techniques generation alternative representations co occurrence matrix  two pltc benchmarks  one parallel
one comparable   investigation indicates reducing dimensionality
data sufficient reasonable efficiency  in terms time space  required 
based observation proposed variant random indexing  method originated within ir community that  best knowledge  never tested
pltc date  proposal  lightweight random indexing  yielded best results
terms  both time space  efficiency  terms classification accuracy 
lightweight random indexing obtained best results terms macroand micro averaged f    lightweight random indexing preserves matrix sparsity 
means memory footprint training time penalized  example 
figures     may see lightweight random indexing  in full configuration
is  random vectors dimensionality original space 
improved latent semantic analysis  in n          configuration is 
   

filightweight random indexing polylingual text classification

dimensionality reduced space        margin        terms f 
       reduction execution time        reduction memory footprint 
even though lightweight random indexing works well dimensionality reduction method  achieves best performance projection reduce
original dimensionality  apparently  bow representation might expected
preferable case  truly orthogonal  however  polylingual bow
representation features informative restricted set data  e g  
german term entire dimension reserved vector space model 
dimension useful documents written german  random projections instead
map feature space space shared among languages once  effect
dimension space becomes informative represent documents regardless
language originally written in  configuration  projection
space larger actual number different features single language  reminiscent kernel trick effect  informative space language enlarged
thus becomes easily separable 
light experiments  lightweight random indexing important advantages
respect previous pltc approaches  first  method machine translationfree  dictionary free  require sort additional resources apart
labelled collection  projected matrix preserves sparsity  direct effect
reducing running time total memory usage  respect original random
indexing technique  lightweight random indexing presents following advantages   i 
probability finding pair truly orthogonal indexes higher   ii  requires less memory
allocate index dictionary   iii  avoids need tuning k parameter 
lri proven effective pltc  conjecture could bring similar
benefits related tasks  cltc  cross lingual information retrieval  well
tackling problems dealing sparse heterogeneous sources data general 
discussed above  one reasons k     safe configuration still preserves
representation capacity  however  might hold circumstances  e g  
processing huge streams dynamic data  e g   streams tweets   certain
point representation capacity might saturate dimensionality space
chosen carefully  cases  opting configurations k     might mitigate
problem 
another fact emerges experiments dimensionality reduction
necessarily synonym computational efficiency  reason modern secondary
storage data structures optimized operate sparse data  dimensionality drastically reduced  matrix density may increase  net effect may decrease
efficiency  true benefit thus achieved extent trade off sparsity
separability preserved  dimension  lri proved extremely effective 
although results encouraging  investigations still needed shed
light foundations random projection methods  first question whether
criterion better choose random index vectors  given current criterion
random  seems might room better motivated strategies  possibly leveraging
class labels taking account document language labels  considering
random indexing originally proposed context ir community  wonder
whether proposed approach could produce similar improvements ir tasks
   

fimoreo  esuli    sebastiani

query expansion bilingual lexicon acquisition  finally  could interesting combine
lightweight random indexing reflexive random indexing  cohen  schvaneveldt   
widdows        rangan         recent formulation model iteratively
alternates row indexing column indexing original co occurrence matrix 

acknowledgements
fabrizio sebastiani leave consiglio nazionale delle ricerche  italy 

references
achlioptas  d          database friendly random projections  proceedings   th
acm symposium principles database systems  pods        pp         
santa barbara  us 
al rfou  r   perozzi  b     skiena  s          polyglot  distributed word representations
multilingual nlp  proceedings   th conference computational natural
language learning  conll        pp          sofia  bl 
amini  m  r     goutte  c          co classification approach learning multilingual corpora  machine learning                   
amini  m  r   usunier  n     goutte  c          learning multiple partially observed
views  application multilingual text categorization  proceedings   rd
annual conference neural information processing systems  nips        pp    
    vancouver  ca 
baroni  m   dinu  g     kruszewski  g          dont count  predict  systematic comparison context counting vs  context predicting semantic vectors  proceedings
  nd annual meeting association computational linguistics  acl
       pp          baltomore  us 
bel  n   koster  c  h     villegas  m          cross lingual text categorization  proceedings  th european conference research advanced technology
digital libraries  ecdl        pp          trondheim  no 
bengio  y          learning deep architectures ai  foundations trends machine
learning              
bengio  y   schwenk  h   senecal  j  s   morin  f     gauvain  j  l          neural probabilistic language models  innovations machine learning  pp          springer 
heidelberg  de 
bingham  e     mannila  h          random projection dimensionality reduction  applications image text data  proceedings  th acm international
conference knowledge discovery data mining  kdd        pp          san
francisco  us 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal
machine learning research             
   

filightweight random indexing polylingual text classification

bullinaria  j  a     levy  j  p          extracting semantic representations word
co occurrence statistics  computational study  behavior research methods         
       
chandar  s   lauly  s   larochelle  h   khapra  m  m   ravindran  b   raykar  v  c     saha 
a          autoencoder approach learning bilingual word representations 
proceedings   th annual conference neural information processing systems
 nips        pp            montreal  ca 
cohen  t   schvaneveldt  r     widdows  d          reflective random indexing indirect inference  scalable method discovery implicit connections  journal
biomedical informatics                 
collobert  r   weston  j   bottou  l   karlen  m   kavukcuoglu  k     kuksa  p         
natural language processing  almost  scratch  journal machine learning
research               
de melo  g     siersdorfer  s          multilingual text classification using ontologies 
proceedings   th european conference information retrieval  ecir       
pp          roma  it 
deerwester  s  c   dumais  s  t   furnas  g  w   landauer  t  k     harshman  r  a 
        indexing latent semantic analysis  journal american society
information science                 
dumais  s  t   letsche  t  a   littman  m  l     landauer  t  k          automatic crosslanguage retrieval using latent semantic indexing  working notes aaai
spring symposium cross language text speech retrieval  pp        stanford 
us 
ehrmann  m   cecconi  f   vannella  d   mccrae  j  p   cimiano  p     navigli  r         
representing multilingual data linked data  case babelnet      proceedings  th conference language resources evaluation  lrec        pp 
        reykjavik  is 
esuli  a   fagni  t     moreo  a          jatecs  java text categorization system  
github  retrieved september           https   github com jatecs jatecs 
faruqui  m     dyer  c          improving vector space word representations using multilingual correlation  proceedings   th conference european chapter
association computational linguistics  eacl        pp          gothenburg 
se 
forman  g          pitfall solution multi class feature selection text classification  proceedings   st international conference machine learning
 icml        pp        banff  ca 
fradkin  d     madigan  d          experiments random projections machine
learning  proceedings  th acm international conference knowledge
discovery data mining  kdd        pp          washington  us 
garca adeva  j  j   calvo  r  a     lopez de ipina  d          multilingual approaches
text categorisation  european journal informatics professional              
   

fimoreo  esuli    sebastiani

gliozzo  a     strapparava  c          cross language text categorization acquiring
multilingual domain models comparable corpora  proceedings acl
workshop building using parallel texts  pp       ann arbor  us 
gliozzo  a     strapparava  c          exploiting comparable corpora bilingual dictionaries cross language text categorization  proceedings   th annual
meeting association computational linguistics  acl        pp         
sydney  au 
gorman  j     curran  j  r          random indexing using statistical weight functions 
proceedings  th conference empirical methods natural language processing  emnlp        pp          sydney  au 
gouws  s     sgaard  a          simple task specific bilingual word embeddings 
proceedings north american chapter association computational
linguistics human language technologies conference  naacl hlt        pp 
         
haddow  b   hoang  h   bertoldi  n   bojar  o     heafield  k          moses statistical
machine translation system  moses website  retrieved september          
http   www statmt org moses  
harris  z  s          mathematical structures language  wiley  new york  us 
hecht nielsen  r          context vectors  general purpose approximate meaning representations self organized raw data  computational intelligence  imitating life 
pp        ieee press 
hermann  k  m     blunsom  p          multilingual models compositional distributed
semantics  proceedings   nd annual meeting association computational linguistics  acl        pp        baltimore  us 
joachims  t          svmperf  support vector machine multivariate performance
measures  cornell university website  retrieved september          
http   www cs cornell edu people tj svm light svm perf html 
joachims  t          support vector method multivariate performance measures 
proceedings   nd international conference machine learning  icml       
pp          bonn  de 
johnson  w  b   lindenstrauss  j     schechtman  g          extensions lipschitz maps
banach spaces  israel journal mathematics                 
jurgens  d     stevens  k          event detection blogs using temporal random indexing 
proceedings workshop events emerging text types  pp       borovets 
bg 
kanerva  p   kristofersson  j     holst  a          random indexing text samples latent semantic analysis  proceedings   nd annual conference cognitive
science society  cogsci        p        philadelphia  us 
karlgren  j   holst  a     sahlgren  m          filaments meaning word space 
proceedings   th european conference information retrieval  ecir       
pp          glasgow  uk 
   

filightweight random indexing polylingual text classification

kaski  s          dimensionality reduction random mapping  fast similarity computation
clustering  proceedings ieee international joint conference neural
networks  ijcnn        pp          anchorage  us 
klementiev  a   titov  i     bhattarai  b          inducing crosslingual distributed representations words  proceedings   th international conference computational linguistics  coling        pp            mumbai  in 
koehn  p          europarl  parallel corpus statistical machine translation  mt
summit  vol     pp        publicly available http   www statmt org europarl  
lauly  s   boulanger  a     larochelle  h          learning multilingual word representations using bag of words autoencoder  arxiv e prints  arxiv            cs cl  
lewis  d  d   yang  y   rose  t  g     li  f          rcv   new benchmark collection
text categorization research  journal machine learning research     apr          
publicly available http   www jmlr org papers volume  lewis  a lyrl     
rcv v  readme htm 
li  p   hastie  t  j     church  k  w          sparse random projections  proceedings
  th acm sigkdd international conference knowledge discovery
data mining  kdd        pp          philadelphia  us 
li  y     shawe taylor  j          advanced learning algorithms cross language patent
retrieval classification  information processing management              
     
mikolov  t   le  q  v     sutskever  i       a   exploiting similarities among languages
machine translation  arxiv e prints  arxiv            cs cl  
mikolov  t   sutskever  i   chen  k   corrado  g  s     dean  j       b   distributed
representations words phrases compositionality  proceedings
  th annual conference neural information processing systems  nips        pp 
          lake tahoe  us 
mimno  d   wallach  h  m   naradowsky  j   smith  d  a     mccallum  a          polylingual topic models  proceedings      conference empirical methods
natural language processing  emnlp        pp          singapore  sn 
moreo  a          data resources reproducing experiments polylingual text classification  human language technologies  hlt  group website  retrieved september
          http   hlt isti cnr it pltc 
nastase  v     strapparava  c          bridging languages etymology  case
cross language text categorization  proceedings   st annual meeting
association computational linguistics  acl        pp          sofia  bl 
osterlund  a   odling  d     sahlgren  m          factorization latent variables distributional semantic models  proceedings conference empirical methods
natural language processing  emnlp        pp          lisbon  pt 
pan  s  j     yang  q          survey transfer learning  ieee transactions
knowledge data engineering                    
   

fimoreo  esuli    sebastiani

papadimitriou  c  h   raghavan  p   tamaki  h     vempala  s          latent semantic
indexing  probabilistic analysis  proceedings   th acm symposium
principles database systems  pods        pp          seattle  us 
pennington  j   socher  r     manning  c  d          glove  global vectors word
representation  proceedings conference empirical methods natural
language processing  emnlp        pp            doha  qa 
prettenhofer  p     stein  b          cross language text classification using structural
correspondence learning  proceedings   th annual meeting association
computational linguistics  acl        pp            uppsala  se 
rangan  v          discovery related terms corpus using reflective random indexing  proceedings icail      workshop setting standards searching
electronically stored information  pittsburgh  us 
richardson  j          polylda    atlassian bitbucket  retrieved september          
https   bitbucket org trickytoforget polylda 
rigutini  l   maggini  m     liu  b          em based training algorithm crosslanguage text categorization  proceedings  rd ieee wic acm international conference web intelligence  wi        pp          compiegne  fr 
rohde  d          c library computing singular value decompositions  svdlibc 
retrieved september           http   tedlab mit edu  dr svdlibc  
sahlgren  m          vector based semantic analysis  representing word meanings based
random labels  proceedings esslli workshop semantic knowledge
acquistion categorization  helsinki  fi 
sahlgren  m          introduction random indexing  proceedings workshop
methods applications semantic indexing  copenhagen  dk 
sahlgren  m          word space model  using distributional analysis represent syntagmatic paradigmatic relations words high dimensional vector spaces 
ph d  thesis  swedish institute computer science  university stockholm  stockholm  se 
sahlgren  m     coster  r          using bag of concepts improve performance
support vector machines text categorization  proceedings   th international conference computational linguistics  coling        geneva  ch 
sahlgren  m     karlgren  j          automatic bilingual lexicon acquisition using random
indexing parallel corpora  natural language engineering                 
sahlgren  m   karlgren  j   coster  r     jarvinen  t          sics clef       automatic query expansion using random indexing  working notes crosslanguage evaluation forum workshop  clef        pp          roma  it 
steinberger  r   pouliquen  b     ignat  c          exploiting multilingual nomenclatures
language independent text features interlingua cross lingual text analysis
applications  proceedings  th slovenian language technology conference 
ljubljana  sl 
   

filightweight random indexing polylingual text classification

steinberger  r   pouliquen  b   widiger  a   ignat  c   erjavec  t   tufis  d     varga 
d          jrc acquis  multilingual aligned parallel corpus     languages  proceedings  th international conference language resources
evaluation  lrec        pp            genova  it  publicly available
https   ec europa eu jrc en language technologies jrc acquis 
vinokourov  a   shawe taylor  j     cristianini  n          inferring semantic representation text via cross language correlation analysis  proceedings   th annual
conference neural information processing systems  nips        pp           
vancouver  ca 
vulic  i     moens  m  f          monolingual cross lingual information retrieval models
based  bilingual  word embeddings  proceedings   th international acm
sigir conference research development information retrieval  sigir
       pp          santiago  cl 
wei  c  p   lin  y  t     yang  c  c          cross lingual text categorization  conquering
language boundaries globalized environments  information processing management                 
wei  c  p   yang  c  s   lee  c  h   shi  h     yang  c  c          exploiting poly lingual
documents improving text categorization effectiveness  decision support systems 
         
xiao  m     guo  y          novel two step method cross language representation
learning  proceedings   th annual conference neural information processing systems  nips        pp            lake tahoe  us 
xu  c   tao  d     xu  c          survey multi view learning  arxiv e prints 
arxiv            cs lg  
yang  y     pedersen  j  o          comparative study feature selection text categorization  proceedings   th international conference machine learning
 icml        pp          nashville  us 
zou  w  y   socher  r   cer  d  m     manning  c  d          bilingual word embeddings
phrase based machine translation  proceedings conference empirical
methods natural language processing  emnlp        pp            melbourne 
au 

   


