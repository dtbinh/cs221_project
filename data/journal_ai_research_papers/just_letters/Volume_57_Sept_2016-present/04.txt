journal of artificial intelligence research                  

submitted       published      

multi objective reinforcement learning through
continuous pareto manifold approximation
simone parisi

parisi ias tu darmstadt de

technische universitat darmstadt
hochschulstr            darmstadt  germany

matteo pirotta
marcello restelli

matteo pirotta polimi it
marcello restelli polimi it

politecnico di milano
piazza leonardo da vinci           milano  italy

abstract
many real world control applications  from economics to robotics  are characterized by
the presence of multiple conflicting objectives  in these problems  the standard concept
of optimality is replaced by paretooptimality and the goal is to find the pareto frontier 
a set of solutions representing different compromises among the objectives  despite recent advances in multiobjective optimization  achieving an accurate representation of the
pareto frontier is still an important challenge  in this paper  we propose a reinforcement
learning policy gradient approach to learn a continuous approximation of the pareto frontier in multiobjective markov decision problems  momdps   differently from previous
policy gradient algorithms  where n optimization routines are executed to have n solutions 
our approach performs a single gradient ascent run  generating at each step an improved
continuous approximation of the pareto frontier  the idea is to optimize the parameters
of a function defining a manifold in the policy parameters space  so that the corresponding
image in the objectives space gets as close as possible to the true pareto frontier  besides
deriving how to compute and estimate such gradient  we will also discuss the nontrivial
issue of defining a metric to assess the quality of the candidate pareto frontiers  finally 
the properties of the proposed approach are empirically evaluated on two problems  a
linear quadratic gaussian regulator and a water reservoir control task 

   introduction
multiobjective sequential decision problems are characterized by the presence of multiple
conflicting objectives and can be found in many real world scenarios  such as economic
systems  shelton         medical treatment  lizotte  bowling    murphy         control of
water reservoirs  castelletti  pianosi    restelli         elevators  crites   barto        and
robots  nojima  kojima    kubota        ahmadzadeh  kormushev    caldwell        
just to mention a few  such problems are often modeled as multiobjective markov decision
processes  momdps   where the concept of optimality typical of mdps is replaced by the
one of pareto optimality  that defines a compromise among the different objectives 
in the last decades  reinforcement learning  rl   sutton   barto        has established
as an effective and theoretically grounded framework that allows to solve singleobjective
mdps whenever either no  or little  prior knowledge is available about system dynamics or
the dimensionality of the system to be controlled is too high for classical optimal control
     ai access foundation  all rights reserved 

fiparisi  pirotta    restelli

methods  multiobjective reinforcement learning  morl   instead  concerns momdps
and tries to solve sequential decision problems with two or more conflicting objectives 
despite the successful development in rl theory and a high demand for multiobjective
control applications  morl is still a relatively young and unexplored research topic 
morl approaches can be divided in two categories  based on the number of policies
they learn  vamplew  dazeley  berry  issabekov    dekker         single and multiple
policy  although most of morl approaches belong to the former category  here we present
a multiplepolicy approach  able to learn a set of policies approximating the pareto frontier 
a representation of the complete pareto frontier  in fact  allows a posteriori selection of a
solution and encapsulates all the trade offs among the objectives  giving better insights into
the relationships among the objectives  among multiplepolicy algorithms it is possible to
identify two classes  valuebased  lizotte et al         castelletti et al         van moffaert
  nowe         that search for optimal solutions in value functions space  and policy gradient approaches  shelton        parisi  pirotta  smacchia  bascetta    restelli         that
search through policy space  in practice  each approach has different advantages  value
based methods usually have stronger guarantees of convergence  but are preferred in domains with lowdimensional state action spaces as they are prone to suffer from the curse
of dimensionality  sutton   barto         on the other hand  policy gradient methods
have been very favorable in many domains such as robotics as they allow taskappropriate
prestructured policies to be integrated straightforwardly  deisenroth  neumann    peters 
      and experts knowledge can be incorporated with ease  by selecting a suitable policy
parametrization  the learning problem can be simplified and stability as well as robustness
can frequently be ensured  bertsekas         nonetheless  both approaches lack of guarantees of uniform covering of the true pareto frontier and the quality of the approximate
frontier  in terms of accuracy  distance from the true frontier  and covering  its extent  
is related to the metric used to measure the discrepancy from the true pareto frontier 
however  nowadays the definition of such metric is an open problem in moo literature 
in this paper  we overcome these limitations proposing a novel gradientbased morl
approach and alternative quality measures for approximate frontiers  the algorithm  namely
paretomanifold gradient algorithm  pmga   exploiting a continuous approximation of
the locally paretooptimal manifold in the policy space  is able to generate an arbitrarily
dense approximate frontier  this article is an extension of a preliminary work presented
by pirotta  parisi  and restelli        and its main contributions are  the derivation of the
gradient approach in the general case  i e   independent from the metric used to measure
the quality of the current solution  section     how to estimate such gradient from samples
 section     a discussion about frontier quality measures that can be effectively integrated
in the proposed approach  section     a thorough empirical evaluation of the proposed
algorithm and metrics performance in a multiobjective discrete time linear quadratic
gaussian regulator and in a water reservoir management domain  sections   and    

   preliminaries
in this section  we first briefly summarize the terminology as used in the paper and discuss
about state of the art approaches in morl  subsequently  we focus on describing policy
gradient techniques and we introduce the notation used in the remainder of the paper 
   

fimorl through continuous pareto manifold approximation

    problem formulation
a discretetime continuous markov decision process  mdp  is a mathematical framework
for modeling decision making  it is described by a tuple hs  a  p  r    di  where s  rn
is the continuous state space  a  rm is the continuous action space  p is a markovian
transition model where p s   s  a  defines the transition density between state s and s  under
action a  r   s  a  s  r is the reward function           is the discount factor  and d
is a distribution from which the initial state is drawn  in this context  the behavior of an
agent is defined by a policy  i e   a density distribution  a s  that specifies the probability
of taking action a in state s  given the initial state distribution d  it is possible to define
the expected return j  associated to a policy  as
 t  
 
x

t
j  
e
 r st   at   st     s   d  
st p at 

t  

being r st   at   st     the immediate reward obtained when state st   is reached executing
action at from state st   and t the finite or infinite time horizon  the goal of the agent is
to maximize such a return 
multiobjective markov decision processes  momdps  are an extension of mdps in
which several pairs of reward functions and discount factors are defined  one for each
objective  formally  a momdp is described by a tuple hs  a  p  r    di  where r  
 r            rq  t and                 q  t are qdimensional column vectors of reward functions
ri   s  a  s  r and discount factors i          respectively 
in momdps  any policy




 is associated to q expected returns j   j            jq   where
 t  
 
x
ji  
e
it ri  st   at   st     s   d  
st p at 

t  

unlike what happens in mdps  in momdps a single policy dominating all the others
usually does not exist  as when conflicting objectives are considered  no policy can simultaneously maximize all of them  for this reason  in multiobjective optimization  moo 
the concept of pareto dominance is used  policy  strongly dominates policy      denoted
by        if it is superior on all objectives  i e  
 

      i              q    ji   ji  
similarly  policy  weakly dominates policy      denoted by        if it is not worse on all
objectives  i e  
 

 

      i              q    ji  ji  i              q    ji   ji  
if there is no policy    such that       the policy  is paretooptimal  we can also speak
of locally paretooptimal policies  for which the definition is the same as above  except
that we restrict the dominance to a neighborhood of   in general  there are multiple
 locally  paretooptimal policies  solving
a momdp

  is equivalent to determine the set
                   which maps to the socalled pareto
of all paretooptimal
policies

 
 
frontier f   j        
   as done by harada  sakuma  and kobayashi         we assume that locally paretooptimal solutions
that are not paretooptimal do not exist 

   

fiparisi  pirotta    restelli

    related work
in multiobjective optimization  moo  field  there are two common solution concepts 
multiobjective to singleobjective strategy and pareto strategy  the former approach
derives a scalar objective from the multiple objectives and  then  uses the standard single
objective optimization  soo  techniques  weighted sum  athan   papalambros        
normbased  yu   leitmann        koski   silvennoinen         sequential  romero 
       constrained  waltz         physical programming  messac   ismail yahaya       
and min max methods  steuer   choo         the latter strategy is based on the concept of
pareto dominance and considers paretooptimal solutions as non inferior solutions among
the candidate solutions  the main exponent of this class is the convex hull method  das  
dennis        messac  ismail yahaya    mattson        
similar to moo  current morl approaches can be divided into two categories based
on the number of policies they learn  vamplew et al          singlepolicy methods aim
at finding the best policy that satisfies a preference among the objectives  the majority
of morl approaches belong to this category and differ for the way in which preferences
are expressed  they are easy to implement  but require a priori decision about the type
of the solution and suffer of instability  as small changes on the preferences may result
in significant variations in the solution  vamplew et al          the most straightforward
and common singlepolicy approach is the scalarization where a function is applied to the
reward vector in order to produce a scalar signal  usually  a linear combination weighted
sum of the rewards is performed and the weights are used to express the preferences over
multiple objective  castelletti  corani  rizzolli  soncinie sessa    weber        natarajan
  tadepalli        van moffaert  drugan    nowe         less common is the use of non
linear mappings  tesauro  das  chan  kephart  levine  rawson    lefurgy         the
main advantage of scalarization is its simplicity  however  linear scalarization presents some
limitations  it is not able to find solutions that lie in the concave or linear region of the
pareto frontier  athan   papalambros        and a uniform distribution of the weights may
not produce accurate and evenly distributed points on the pareto frontier  das   dennis 
       in addition  even if the frontier is convex  some solutions cannot be achieved through
scalarization because a loss in one objective may not be compensated by an increment
in another one  perny   weng         different singlepolicy approaches are based on
thresholds and lexicographic ordering  gabor  kalmar    szepesvari        or different
kinds of preferences over the objective space  mannor   shimkin              
multiplepolicy approaches  on the contrary  aim at learning multiple policies in order
to approximate the pareto frontier  building the exact frontier is generally impractical in
real world problems  thus  the goal is to build an approximation of the frontier that contains
solutions that are accurate  evenly distributed along the frontier and have a range similar
to pareto one  zitzler  thiele  laumanns  fonseca    da fonseca         there are many
reasons behind the superiority of the multiplepolicy methods  they permit a posteriori
selection of the solution and encapsulate all the trade offs among the multiple objectives 
in addition  a graphical representation of the frontier can give better insights into the relationships among the objectives that can be useful for understanding the problem and the
choice of the solution  however  all these benefits come at a higher computational cost 
that can prevent learning in online scenarios  the most common approach to approximate
   

fimorl through continuous pareto manifold approximation

the pareto frontier is to perform multiple runs of a singlepolicy algorithm by varying the
preferences among the objectives  castelletti et al         van moffaert et al          it
is a simple approach but suffers from the disadvantages of the singlepolicy method used 
besides this  few other examples of multiplepolicy algorithms can be found in literature 
barrett and narayanan        proposed an algorithm that learns all the deterministic policies defining the convex hull of the pareto frontier in a single learning process  recent
works have focused on the extension of fitted q iteration to the multiobjective scenario 
while lizotte  bowling  and murphy         and lizotte et al         have focused on a
linear approximation of the value function  castelletti  pianosi  and restelli        are able
to learn the control policy for all the linear combinations of preferences among the objectives in a single learning process  finally  wang and sebag        proposed a montecarlo
tree search algorithm able to learn solutions lying in the concave region of the frontier 
nevertheless  these classic approaches exploit only deterministic policies that result in
scattered pareto frontiers  while stochastic policies give a continuous range of compromises
among objectives  roijers  vamplew  whiteson    dazeley        parisi et al          shelton        section        was the pioneer both for the use of stochastic mixture policies and
gradient ascent in morl  he achieved two well known goals in morl  simultaneous and
conditional objectives maximization  in the former  the agent must maintain all goals at the
same time  the algorithm starts with a mixture of policies obtained by applying standard
rl techniques to each independent objective  the policy is subsequently improved following
a convex combination of the gradients in the policy space that are nonnegative w r t  all
the objectives  for each objective i  the gradient gi of the expected return w r t  the policy
is computed and the vector vi having the highest dot product with gi and simultaneously
satisfying the nonnegativity condition for all the returns is used as improving direction
for the i th reward  the vectors vi are combined in a convex form to obtain the direction
of the parameter improvement  the result is a policy that belongs to the pareto frontier 
an approximation of the pareto frontier is obtained by performing repeated searches with
different weights of the reward gradients vi   on the other hand  conditional optimization
consists in maximizing an objective while maintaining a certain level of performance over
the others  the resulting algorithm is a gradient search in a reduced policy space in which
the value of constrained objectives are greater than the desired performance 
only a few studies followed the work of shelton        in regard to policy gradient
algorithms applied to momdps  recently parisi et al         proposed two policy gradient
based morl approaches that  starting from some initial policies  perform gradient ascent
in the policy parameters space in order to determine a set of nondominated policies  in
the first approach  called radial    given the number p of pareto solutions that are required
for approximating the pareto frontier  p gradient ascent searches are performed  each one
following a different  uniformly spaced  direction within the ascent simplex defined by the
convex combination of singleobjective gradients  the second approach  called pareto
following  starts by performing a singleobjective optimization and then it moves along the
pareto frontier using a two step iterative process  updating the policy parameters following
some other gradient ascent direction  and then applying a correction procedure to move the
new solution onto the pareto frontier  although such methods exploit stochastic policies and
proved to be effective in several scenarios  they still return scattered solutions and are not
guaranteed to uniformly cover the pareto frontier  to the best of our knowledge  nowadays
   

fiparisi  pirotta    restelli

there is no morl algorithm returning a continuous approximation of the pareto frontier   
in the following sections we present the first approach able to do that  the paretomanifold
gradient algorithm  pmga  
    policy parametrization in policygradient approaches
in singleobjective
mdps 

  policygradient approaches consider parameterized policies  
          rd   where  is a compact notation for  a s    and  is the policy
parameters space  given a policy parametrization   we assume the policy performance
j     f  rq to be at least of class c      f is called objectives space and j is defined as
the expected reward over the space of all possible trajectories t
z
p      r   d 
j     
t

where   t is a trajectory drawn from density distribution p     with reward vector
r    that
the accumulated expected discounted reward over trajectory    i e  
prepresents
  t
i ri  st   at   st      examples of parametrized policies used in this context are
ri       tt  
guassian policies and gibbs policies  in momdps  q gradient directions are defined for
each policy parameter   peters   schaal      b   i e  
z


 ji     
 p      ri    d   e t  ln p      ri    
t
 
 
t
 
x
b  ji    
 e t ri    
 ln   at  st       
   
t  

where each direction  ji is associated to a particular discount factorreward function
b  ji    is its sample based estimate  as shown by equation      the
pair   i   ri   and 
differentiability of the expected return is connected to the differentiability of the policy by
 ln p       

t
 
x

 ln  at  st     

t  

a remark on notation  in the following we will use the symbol dx f to denote the
derivative of a generic function f   rmn  rpq w r t  matrix x   notice that the
following relationship holds for scalar functions of vector variable  x f    dx f  t   finally 
the symbol ix will be used to denote an x  x identity matrix 

   gradient ascent on policy manifold for continuous pareto frontier
approximation
in this section we first provide a general definition of the optimization problem that we want
to solve and then we explain how we can solve it in the momdp scenario using a gradient
based approach  the novel contributes of this section are summarized in lemma     where
   a notable exception is the moo approach by calandra  peters  and deisenrothy        where gaussian
processes are used to obtain a continuous approximation of the pareto frontier 
   a function is of class c   when it is continuous  twice differentiable and the derivatives are continuous 
   the derivative operator is well defined for matrices  vectors and scalar functions  refer to the work
of magnus and neudecker        for details 

   

fimorl through continuous pareto manifold approximation

the objective function and its gradient are described  in particular  we provide a solution
to the problem of evaluating the performance of a continuous approximation of the pareto
frontier w r t  to an indicator function  this problem is non trivial in morl because we
do not have direct access to the pareto frontier and we can only manipulate the policy
parameters  we provide a step by step derivation of these results leveraging on manifold
theory and matrix calculus 
    continuous pareto frontier approximation in multiobjective
optimization
it has been shown that locally paretooptimal solutions locally forms a  q    dimensional
manifold  assuming d   q  harada  sakuma  kobayashi    ono         it follows that in
 objective problems  paretooptimal solutions can be described by curves both in policy
parameters and objective spaces  the idea behind this work is to parametrize the locally
paretooptimal solution curve in the objectives space  in order to produce a continuous
representation of the pareto frontier 
let the generative space t be an open set in rb with b  q  the analogous high
dimensional function of a parameterized curve is a smooth map    t  rq of class
c l  l      where t  t and   p  rk are the free variables and the parameters 
respectively  the set f     t   together with the map  constitute a parametrized
manifold of dimension b  denoted by f  t    munkres         this manifold represents our
approximation of the pareto frontier  the goal is to find the best approximation  i e   the
parameters  that minimize the distance from the real frontier
   arg max i   f  t     

   

p

where i    rq  r is some indicator function measuring the quality of f  t   w r t  the
true pareto frontier  notice that equation     can be interpreted as a special projection
operator  refer to figure  a for a graphical representation   however  since i  requires
the knowledge of the true pareto frontier  a different indicator function is needed  the
definition of such metric is an open problem in literature  recently  several metrics have
been defined  but each candidate presents some intrinsic flaws that prevent the definition
of a unique superior metric  vamplew et al          furthermore  as we will see in the
remainder of the section  the proposed approach needs a metric that is differentiable w r t 
policy parameters  we will investigate this topic in section   
in general  moo algorithms compute the value of the frontier as the sum of the value
of the points composing the discrete approximation  in our scenario  where a continuous
approximate frontier is available  it maps to an integration on the pareto manifold
z
idv 
   
l     
f  t  

where l    is the manifold value  dv denotes the integral w r t  the volume of the manifold
and i   f  t    r is an indicator function measuring the pareto optimality of each point
of f  t    assuming i to be continuous  the above integral is given by  munkres       
z
z
l     
idv 
 i     v ol  dt   t   dt 
f  t  

t

   

fiparisi  pirotta    restelli

 a 

 b 

figure    transformation maps in a generic moo setting  figure  a   and in morl  figure  b    while in moo it is also possible to consider parametrized solutions as in figure  b   in morl this is necessary  as the mapping between  i and fi is not known in
closed form but determined by the  discounted  sum of the rewards 
  
provided this integral exists and v ol  x    det x t  x     a standard way to maximize
the previous equation is by performing gradient ascent  updating the parameters according
to the gradient of the manifold value w r t  the parameters   i e          l     
    continuous pareto frontier approximation in multiobjective
reinforcement learning
while in standard multiobjective optimization the function  is free to be designed  in
morl it must satisfy some conditions  the first thing to notice is that the direct map
between the parameters space t and the objective space is unknown  but can be easily
defined through a reparameterization involving the policy space   as shown in figure  b  in
the previous section we have mentioned that there is a tight relationship between the  local 
manifold in the objective space and the  local  manifold in the policy parameters space 
this mapping is well known and it is defined by the performance function j   defining the
utility of a policy     this means that  given a set of policy parameterizations  we can define
the associated points in the objective space  as a consequence  the optimization problem
can be reformulated as the search for the best approximation of the pareto manifold in the
policy parameter space  i e   to the search of the manifold in the policy parameter space
that best describes the optimal pareto frontier 
formally  let    t   be a smooth map of class c l  l     defined on the same
domain of    we think of the map  as a parameterization of the subset   t   of  
each choice of a point t  t gives rise to a point   t  in   t      this means that only
a subset   t   of the space  can be spanned by map    i e     t   is a bdimensional
parametrized manifold in the policy parameters space  i e  
  t              t   t  t    
and  as a consequence  the associated parameterized pareto frontier is the bdimensional
open set defined as
f  t      j          t     
   

fimorl through continuous pareto manifold approximation

    gradient ascent in the manifold space
at this point we have introduced all the notation needed to derive the gradient  l    
lemma       pirotta et al         let t be an open set in rb   let f  t   be a manifold
parametrized by a smooth map  expressed as composition of maps j and     i e     
j     t  rq    given a continuous function i defined at each point of f  t    the
integral w r t  the volume is given by
z
z
l     
idv  
 i   j      v ol  d j  dt   t   dt 
f  t  

t

provided this integral exists  the associated gradient w r t  the parameters i is given by
z

l   
 
 i   j      v ol  t  dt
i

t
i

z

t t 

t
t
 i   j      v ol  t  vec t t
 
nb ib  t di tdt     
t

where t   d j  dt   t    is the kronecker product  nb       ib    kbb   is a symmetric
 b   b    idempotent matrix with rank    b b      and kbb is a permutation matrix  magnus
  neudecker         finally 


di t   dt   t t  iq d  d j    di   t     ib  d j    di  dt   t    
proof  the equation of the manifold value l    follows directly from the definition of
volume integral of a manifold  munkres        and the definition of function composition 
in the following  we provide a detailed derivation of the i th component of the gradient 
let t   d j  t  dt   t   then
z
l   

 
 i   j      v ol  t  dt
i
t i

z
det tt t
 
 
 i   j     
dt 
 v ol  t 
i
t
the indicator derivative and the determinant derivative can be respectively expanded as

 i   j        dj i jt    d j  t    di   t  
i


det tt t
det tt t vec tt t t
 
 
t
t 
i
 vec
t 
 vec
t 
i
 
 z
   
 z
     z     z 
  

 b 

b  qb

qb 

where

det tt t
 vec t t
tt t
 vec t t



t t
t
 
  det t t
vec t t


t



   nb ib  tt  

   

fiparisi  pirotta    restelli

and  is the kronecker product  nb       ib    kbb   is a symmetric  b   b    idempotent
matrix with rank    b b      and kbb is a permutation matrix  magnus   neudecker        
 t 
the last term to be expanded is di t  vec
i   we start from a basic property of the
differential  i e  
d  d j  dt   t     d d j   dt   t    d j   d dt   t  
then  applying the vector operator 
dvec  d j  dt   t     vec  d d j   dt   t     vec  d j   d dt   t   


  dt   t t  iq dvec  d j       ib  d j    dvec  dt   t    
 z
   
 z
  
 z
 
 z
  
 
dq 

bqdq

bqbd

bd 

finally  the derivative is given by

 vec d j     t 
vec dt   t 


di t   dt   t t  iq
   ib  d j   
t

i

 
 z
     zi  
 
 z
 
dqd



d 

bd 



t

  dt   t   iq d  d j    di   t     ib  d j    di  dt   t    

it is interesting to notice that the gradient of the manifold value l    requires to
compute the second derivatives of the policy performance j    however  d  d j     
vec d j  
does not denote the hessian matrix but a transformation of it
 t
 m n 
h
ji

 

 
dn m
ji   


 
 n



ji
 m



  dp n  d j     

where p   i   q m     and q  number of objectives  is the number of rows of the jacobian
matrix  recall that the hessian
 matrixis defined as the derivative of the transpose of the
jacobian  i e   h j     d d j  t  
up to now  little research has been done on second order methods  and in particular
on hessian formulations  a first analysis was performed by kakade         who provided a
formulation based on the policy gradient theorem  sutton  mcallester  singh    mansour 
       recently  an extended comparison between newton method  em algorithm and
natural gradient was presented by furmston and barber         for the sake of clarity  we
report the hessian formulation provided by furmston and barber        using our notation
and we introduce the optimal baseline  in terms of variance reduction  for such formulation 
lemma      for any momdp  the hessian h j   of the expected discounted reward j
w r t  the policy parameters  is a qd  d matrix obtained by stacking the hessian of each
   notable exceptions are the natural gradient approaches that  although they do not explicitly require to
compute second order derivatives  are usually considered second order methods 

   

fimorl through continuous pareto manifold approximation

component
h j    


vec
 t



ji   
 t

t



h j    


  
 
 
 
h jq   

where
z
h ji     

 
p       ri      bi    ln p       ln p     t   h ln p      d 

   

t

and
 ln p       

t
 
x

 ln  at  st     

h ln p       

t  

t
 
x

h ln  at  st     

t  
 m n 

the optimal baseline of the hessian estimate h
ji provided in equation     can
be computed as done by greensmith  bartlett  and baxter        in order to reduce the
variance of the gradient estimate  it is given component wise by


  
 m n 
e p    ri     g
   
 m n 

bi
 
    
 m n 
e p    g
   
 m n 

 m n 

n
where g
      m
 ln p       ln p      h
to appendix a 

ln p       for its derivation  we refer

   manifold gradient estimation from sample trajectories
in morl  having no prior knowledge about the reward function and the state transition
model  we need to estimate the gradient  l    from trajectory samples  this section
aims to provide a guide to the estimation of the manifold gradient  in particular  we review
results related to the estimation of standard rl components  expected discounted return
and its gradient  and we provide a finite sample analysis of the hessian estimate 
the formulation of the gradient  l    provided in lemma     is composed by terms
related to the parameterization of the manifold in the policy space and terms related to
the mdp  since the map  is free to be designed  the associated terms  e g   dt   t   can
be computed exactly  on the other hand  the terms related to the mdp  j     d j  
and h j    need to be estimated  while the estimate of the expected discounted reward
and the associated gradient is an old topic in rl literature and several results have been
proposed  kakade        pirotta  restelli    bascetta         literature lacks of an explicit
analysis of the hessian estimate  recently  the simultaneous perturbation stochastic approximation technique was exploited to estimate the hessian  fonteneau   prashanth        
however  we rely on the formulation provided by furmston and barber        where the
hessian is estimated from trajectory samples obtained through the current policy  removing
the necessity of generating policy perturbations 
   

fiparisi  pirotta    restelli

algorithm   paretomanifold gradient algorithm
define policy   parametric function    indicator i and learning rate 
initialize parameters 
repeat until terminal condition is reached
collect n           n trajectories
sample free variable t n  from the generative space

sample policy parameters   n     t n 
n
o
 n   n   n  t
execute trajectory and collect data st   at   rt 
t  

b  ji    according to equation    
compute gradients 
b  ji    according to equation    
compute hessians h
compute manifold value derivative  l    according to equation    
update parameters        l   
since p      is unknown  the expectation is approximated by the empirical average 
assuming to have access to n trajectories  the hessian estimate is
 
n
t
 
x
x
 
b  ji     
h
it rnt i  b
n
n  
t  
 t t  
 
t
 
t
 
x
x
x

 ln ant  snt
 ln ant  snt
 
h ln ant  snt  
   
t  

where

o
n
 n   n   n  t
st   at   rt 

t  

t  

t  

denotes the n th trajectory  this formulation resembles the def 

inition of reinforce estimate given by williams        for the gradient  j    such
estimates  known as likelihood ratio methods  overcome the problem of determining the perturbation of the parameters occurring in finite difference methods  algorithm   describes
the complete pmga procedure 
in order to simplify the theoretical analysis of the hessian estimate  we make the following assumptions 
assumption      uniform boundedness   the reward function  the log jacobian and the
log hessian of the policy are uniformly bounded  i              q  m              d  n  
           d   s  a  s     s  a  s     
fi
fi
fi
fi
fi
fi
fi
fi  m 
fi
fi  m n 
fi
  fi
ln  a s   fi  g 
firi  s  a  s  fi  ri  
fid ln  a s   fi  d 
fih
lemma      given a parametrized policy  a s     under assumption      the i th component of the log hessian of the expected return can be bounded by
kh ji   kmax 


ri t  t 
 
td   g  
 

where the max norm of a matrix is defined as kakmax   maxi j  aij   
   

fimorl through continuous pareto manifold approximation

proof  consider the definition of the hessian in equation      under assumption      the
hessian components can be bounded by  m  n 
fi
 
t
 
t
 
fi
fi fiz
x
x


fi  m n 
fi fi
ln  at  st    
ln  aj  sj    
ji   fi   fi p      ri    
fih
fi t
 m
 n
t  
j  
 fi
fi
 
fi
 
ln  at  st     fi
fi
 m  n


t
 
t
 
t
 

x
x
x
ri t  t 
 
d
 ri
 l  
d   g  
td   g  
 
l  

t  

j  

the previous result can be used to derive a bound on the sample complexity of the
hessian estimate 
theorem      given a parametrized policy  a s     under assumption      using the
following number of t  step trajectories

    
 
ri t  t 
 
n  
td   g
ln

 i      
b  ji    generated by equation     is such that with probability    
the gradient estimate h


b

 i  
h ji     h ji   
max

proof  hoeffdings inequality implies that m  n
n    
fi
fi
i
 pn
fi b  m n 
fi
 m n 
 bi ai   
i  
  
p fih
ji     h
ji     i fi   e

solving the equation for n and noticing that lemma     provides a bound on each sample 
we obtain

   
 
ri t  t 
 
n   
td   g
ln  

 i      

the integral estimate can be computed using standard montecarlo techniques  several
statistical bounds have been proposed in literature  we refer to robert and casella       
for a survey on montecarlo methods 
at this point of the paper  the reader may expect an analysis of the convergence  or
convergence rate  to the optimal parametrization  although we consider this analysis theoretically challenging and interesting  we will not provide any result related to this topic 
this analysis is hard  or even impossible  to provide in general settings since the objective
function is nonlinear and nonconcave  moreover  an analysis of a simplified scenario  if
possible  will be almost useless in real applications 
   

fiparisi  pirotta    restelli

   metrics for multiobjective optimization
in this section  we review some indicator functions proposed in literature  underlining advantages and drawbacks  and propose some alternatives  recently  moo has focused on
the use of indicators to turn a multiobjective optimization problem into a singleobjective
one by optimizing the indicator itself  the indicator function is used to assign to every
point of a given frontier a scalar measure that gives a rough idea of the discrepancy between the candidate frontier and the pareto one  since instead of optimizing the objective
functions directly indicatorbased algorithms aim at finding a solution set that maximizes
the indicator metric  a natural question arises about the correctness of this change in the
optimization procedure and on the properties the indicator functions enjoy  for instance 
the hypervolume indicator and its weighted version are among the most widespread metrics
in literature  these metrics have gained popularity because they are refinements of the
pareto dominance relation  zitzler  thiele    bader         recently  several works have
been proposed in order to theoretically investigate the properties of the hypervolume indicator  e g   friedrich  horoba    neumann         nevertheless  it has been argued that the
hypervolume indicator may introduce a bias in the search  furthermore another important
issue when dealing with the hypervolume indicator is the choice of the reference point  from
our perspective  the main issues of this metric are the high computational complexity  the
computation of the hypervolume indicator is a  phard problem  see friedrich et al        
and  above all  the non differentiability  several other metrics have been defined in the field
of moo  we refer to the work by okabe  jin  and sendhoff        for a survey  however  the
moo literature has not been able to provide a superior metric and among the candidates
no one is suited for our scenario  again  the main issues are the non differentiability  the
capability of evaluating only discrete representations of the pareto frontier and the intrinsic
nature of the metrics  for example  the generational distance  another widespread measure
based on the minimum distance from a reference frontier  is not available in our settings 
to overcome these issues  we mixed different indicator concepts into novel differentiable
metrics  the insights that have guided our metrics definition are related to the moo
desiderata  recall that the goal of moo is to compute an approximation of the frontier
including solutions that are accurate  evenly distributed and covering a range similar to the
actual one  zitzler et al          note that the uniformity of the frontier is intrinsically guaranteed by the continuity of the approximation we have introduced  having these concepts
in mind  we need to induce accuracy and extension through the indicator function 
we have not stressed but it is clear from the definition that we want the indicator to
be maximized by the real pareto frontier  we also must ensure that the indicator function
induces a partial ordering over frontiers  manifold f  solutions are all  weakly  dominated
by manifold f  ones  then f  manifold value must be better than f  one 
definition      consistent indicator function   let f be the set of all  q   dimensional
manifolds associated to a momdp with q objectives  let k   be the manifold in the
policy parameters
space mapping to fk  f and f  be the true pareto frontier  let
r
li  f    f idv be the manifold value  an indicator function i is consistent if
fk    fh   li  fh     li  fk    fh  f   

and

h   k    i  k    j  h   j  i   li  fh     li  fk   
   

fimorl through continuous pareto manifold approximation

    accuracy metrics
given a reference point p  a simple indicator can be obtained by computing the distance
between every point of a frontier f and the reference point  i e  
i   kj  pk    
as mentioned for the hypervolume indicator  the choice of the reference point may be
critical  however  a natural choice is the utopia  ideal  point  pu    i e   the point that
optimizes all the objectives  in this case the goal is the minimization of such indicator
function  denoted by iu  utopia indicator    since any dominated policy is farther from the
utopia than at least one paretooptimal solution  the accuracy can be easily guaranteed  on
the other hand  since it has to be minimized  this measure forces the solution to collapse
into a single point  thus it is not consistent  note that this problem can be mitigated
 but not solved  by forcing the transformation  to pass through the singleobjective
optima  although this trick can be helpful  as we will discuss in section    it requires to
find the singleobjective optimal policies in order to constrain the parameters  however 
this information is also required to properly set the utopia 
concerning the accuracy of the frontier  from a theoretical perspective  it is possible to
define another metric using the definition of pareto optimality  a point  is paretooptimal
when  brown   smith       
l      

q
x

i  ji        

q
x

i  

i     

i    

i  

that is  it is not possible to identify an ascent direction that simultaneously improves all
the objectives  as a consequence  the paretoascent direction l of any point on the pareto
frontier is null  formally  a metric that respects the paretooptimality can be defined as
follows 
q
x
i   minq kl    k    
i      i    
r

i  

we denote this indicator with ipn  pareto norm indicator    as for the utopiabased metric 
the extent of the frontier is not taken into account and without any constraint the optimal
solution collapses into a single point on the frontier 
    covering metrics
if the extension of the frontier is the primary concern  maximizing the distance from the
antiutopia  pau   results in a metric that grows with the frontier dimension  however 
on the contrary of the utopia point  the antiutopia is located in the half space that can
be reached by the solutions of the moo problems  this means that by considering the
antiutopiabased metric the maximization problem could become unbounded by moving
solutions arbitrary far from both the pareto frontier and the antiutopia point  therefore
this measure  denoted by iau  antiutopia indicator    does not provide any guarantee about
accuracy 
   

fiparisi  pirotta    restelli

    mixed metrics
all the mentioned indicators provide only one of the desiderata  as a consequence  the
resulting approximate frontier might be arbitrary far from the actual one  in order to
consider both the desiderata we can mix the previous concepts into the following indicator 
i   iau  w
where w is a penalization function  i e   it is a monotonic function that decreases as the
accuracy of the input increases  e g   w      ipn or w      iu   these metrics  denoted
respectively by i pn and i u   take advantage of the expansive behavior of the antiutopia
based indicator and the accuracy of some optimalitybased indicator  in this way all the
desiderata can be met by a single scalar measure  that is also c l  l     differentiable 
another solution is to mix utopia and antiutopiabased indicators in a different way 
as we want solutions that are simultaneously far from the antiutopia and close to the utopia 
we consider the following metric i  to be maximized  
i    

iau
    
iu

where   and   are free parameters 
in the next section  we will show that the proposed mixed metrics are effective in driving
pmga close to the pareto frontier both in exact and approximate scenarios  however  we
want to make clear that their consistency is not guaranteed as it strongly depends on the
free parameters     and     more insights are discussed in section   

   experiments
in this section  we evaluate our algorithm on two problems  a linear quadratic gaussian
regulator and a water reservoir control task  pmga is compared to state of the art methods
 peters  mulling    altun        castelletti et al         parisi et al         beume  naujoks 
  emmerich        using the hypervolume  vamplew et al         and an extension of a
previously defined performance index  pianosi  castelletti    restelli         named loss 
measuring the distance of an approximate pareto front from a reference one  for  objective
problems  the hypervolume is exactly computed  for  objective problems  given its high
computational complexity  the hypervolume is approximated with a montecarlo estimate
as the percentage of points dominated by the frontier in the cube defined by the utopia and
antiutopia points  for the estimate one million points were used 
 
the idea of the loss index is to compare the true pareto frontier fw    jw
ww over a
m
space of weights w to the frontier jw    jbw  ww returned by an algorithm m over the
same weights  jw denotes the discounted return of a new singleobjective mdp defined by
the linear combination of the objectives over w   formally the loss function l is defined as
l j

m

jw  maxm jbw

z

j

  f  w  p   
ww

jw

p dw  

   

where p   is a probability density over the simplex w and jw   w  j is the normalization factor  where the i th component of j is the difference between the best and the
   

fimorl through continuous pareto manifold approximation

worst value of the i th objective of the pareto frontier  i e   ji   max ji    min ji    this
m 
means that  for each weight  the policy that minimizes the loss function is chosen in jw
if the true pareto frontier f is not known  a reference one is used 
since pmga returns continuous frontiers and the two scores are designed for discrete
ones  for the evaluation all the frontiers have been discretized  also  figures presented in
this section show discretized frontiers in order to allow a better representation  besides the
hypervolume and the loss function  we report also the number of solutions returned by an
algorithm and the number of rollouts  i e   the total number of episodes simulated during
the learning process   all data have been collected in simulation and results are averaged
over ten trials    in all the experiments  pmga learning rate is
s

 
   
 
t
 l    m    l   
where m is a positive definite  symmetric matrix and  is a userdefined parameter  this
stepsize rule comes from the formulation of the gradient ascent as a constrained problem
with a predefined distance metric m  peters        and underlies the derivation of natural
gradient approaches  however  since our algorithm exploits the vanilla gradient  i e   we
consider the euclidean space  the metric m is the identity matrix i 
the remainder of the section is organized as follows  we start by studying the behavior
of the metrics proposed in section   and the effects of the parametrization   t  on the lqg 
subsequently  we focus our attention on sample complexity  meant as the number of rollouts
needed to approximate the pareto front  finally  we analyze the quality of our algorithm
on the water reservoir control task  a more complex real world scenario  and compare it
to some state of the art multiobjective techniques  for each case study  domains are first
presented and then results are reported and discussed 
    linear quadratic gaussian regulator  lqg 
the first case of study is a discrete time linear quadratic gaussian regulator  lqg  with
multi dimensional and continuous state and action spaces  peters   schaal      b   the
lqg problem is defined by the following dynamics
st     ast   bat  

at  n  k  st    

r st   at     st t qst  at t rat
where st and at are n dimensional column vectors  a  b  q  r  rnn   q is a symmetric
semidefinite matrix  and r is a symmetric positive definite matrix  dynamics are not
coupled  i e   a and b are identity matrices  the policy is gaussian with parameters
   vec k   where k  rnn   finally  a constant covariance matrix    i is used 
the lqg can be easily extended to account for multiple conflicting objectives  in
particular  the problem of minimizing the distance from the origin w r t  the i th axis has
been taken into account  considering the cost of the action over the other axes
x
ri  st   at     s t i 
a t j  
i  j

   source code available at https   github com sparisi mips 

   

fiparisi  pirotta    restelli

since the maximization of the i th objective requires to have null action on the other axes 
objectives are conflicting  as this reward formulation violates the positiveness of matrix
ri   we change it adding a sufficiently small  perturbation




x
x
ri  st   at           s t i  
a t j    
s t j   a t i   
i  j

j  i

the parameters used for all the experiments are the following                 and initial
state s            t and s                t for the   and  objective case  respectively  the
following sections compare the performance of the proposed metrics under several settings 
we will made use of tables to summarize the results at the end of each set of experiments 
       objective case results
the lqg scenario is particular instructive since all terms involved in the definition of returns  gradients and hessians can be computed exactly  we can therefore focus on studying
different policy manifold parametrizations   t  and metrics i 
unconstrained parametrization  the domain is problematic since it is defined only
for control actions in the range        and controls outside this range lead to divergence of
the system  our primary concern was therefore related to the boundedness of the control
actions  leading to the following parametrization of the manifold in the policy space 


     exp       t   
 
t         
     t   
     exp       t   
utopia and antiutopia points are            and             respectively  and metrics iau and
iu are normalized in order to have   as reference point   the learning step parameter  in
equation     is      
in this case  exploiting nonmixed metrics  pmga was not able to learn a good approximation of the pareto frontier in terms of accuracy and covering  using utopiabased
indicator  the learned frontier collapses in one point on the knee of the front  the same
behavior occurs using ipn   using antiutopia point as reference point the solutions are
dominated and the approximate frontier gets wider  diverging from the true frontier and
expanding on the opposite half space  these behaviors are not surprising  considering the
definition of these indicator functions  as explained in section   
on the contrary  as shown in figure    all mixed metrics are able to achieve both
accuracy and covering  the starting   was set to             t   but the algorithm was also
able to learn even starting from different random parameters  the free metric parameters
were set to        for i pn        for i u and to              for i    although not
shown in the figure  i u behaved very similarly to i pn   we can notice that in both cases
first accuracy is obtained by pushing the parametrization onto the pareto frontier  then the
frontier is expanded toward the extrema in order to attain covering 
   recall that we have initially defined i   kj  pk     here we slightly modify it by normalizing the policy
performance w r t  the reference point  i   kj p   k     where   is a component wise operator 
   in section   we will study the sensitivity of the proposed metrics to their parameters  and  

   

fimorl through continuous pareto manifold approximation

table    summary of  dimensional lqg  unconstrained 
metrics
nonmixed
issues 

accuracy
covering
 
 
iu   ipn   frontier collapses in one point
iau   diverging behavior and dominated solutions found
 
 

mixed

partial solution
final approximation
true pareto frontier

   

   

  
l   

  

j 

   

  

   

  

 

  

end

 

   
   

   

   

   

 

  

j 

   

iterations

 a  learning process with mixed metric i pn  

   

  

   
  

l   

j 

     

 

   

   

 

 
end
   
   

   

   

   

   

 

j 

  

   

iterations

 b  learning process with mixed metric i  

figure    learning processes for the  objective lqg without any constraint on the
parametrization  numbers denote the iteration  end denotes the frontier obtained when
the terminal condition is reached  on the left  the approximated pareto frontiers  on the
right the corresponding l     using both i pn  figure  a   and i  figure  b   the approximated frontier overlaps with the true one  however  using i   pmga converges faster 

   

fiparisi  pirotta    restelli

constrained parametrization  an alternative approach consists in forcing the policy
manifold to pass through the extreme points of the true front by knowing the parameterizations of the singleobjective optimal policies  in general  this requires additional
optimizations and the collection of additional trajectories that must be accounted for in the
results  however  the extreme points are required to set the utopia and antiutopia  moreover  in our case the optimal singleobjective policies were available in literature  for these
reasons  we do not count additional samples when we report the total number of rollouts 
using a constrained parameterization  two improvements can be easily obtained  first 
the number of free parameters decreases and  as a consequence  the learning process is
simplified  second  the approximate frontier is forced to have a sufficiently large area to
cover all the extrema  thus  the problem of covering shown by nonmixed indicators can
be alleviated or  in some cases  completely eliminated  for the  dimensional lqg  a
parametrization forced to pass through the extrema of the frontier is the following 


     exp            t                  t   
     t   
 
t         
     exp            t                  t   
the initial parameter vector is           t   the constraint was able to correct the diverging
behavior of iu and ipn   which returned an accurate and wide approximation of the pareto
frontier  as shown in figure  a  we also notice a much faster convergence  since the algorithm is required to learn fewer parameters  two instead of four   however  iau still shows
the same diverging behavior for some initial parameters    in figure  b            t    on
the contrary  solutions obtained with the other metrics are independent from the initial    
as the algorithm converges close to the true frontier even starting from a parametrization
generating an initial frontier far away from the true one 
       objective case results
unconstrained parametrization 


     exp       t      t     
     t         exp       t      t        
     exp       t      t     

t  simplex           

utopia and antiutopia points are                 and                  respectively  and metrics
iau   iu are normalized  the initial parameters are drawn from a uniform distribution   
u nif                     causes numerical issues  and the learning rate parameter is      
as in the  objective scenario  frontiers learned with iu and ipn collapse in a single
point  while iau has a divergent trend  figure  a   however  unlike the  objective lqr 
i pn also failed in correctly approximate the pareto frontier  the reason is that the tuning
of  is difficult  given the difference in magnitude between ipn and iau on the contrary 
i u with        and i with              returned a high quality approximate frontier 
the latter is shown in figure  b  although some small areas of the true pareto frontier
are not covered by the approximate one  we stress the fact that all the policies found were
paretooptimal  the strength of these metrics is to be found in the normalization of both
utopia and antiutopiabased indicators  this expedient  indeed  allows for an easier tuning
of the free metric parameters  as the magnitude of the single components is very similar 
more insights into the tuning of mixed metrics parameters are discussed in section   
   

fimorl through continuous pareto manifold approximation

table    summary of  dimensional lqg  constrained 
metrics
nonmixed  iu   ipn
nonmixed  iau
issues 
mixed

accuracy
covering
 
 
 
 
iau   diverging behavior and dominated solutions found
 
 

partial solution
final approximation
true pareto frontier

   

 

   
l   

j 

   

 

   

 

   

end
   
   

   

   

   

   

 

 

j 

  

  

  

  

   

   

iterations

 a  learning process with utopiabased metric iu  

   

  

   

   

j 

 

   

l   

 
 

   
   

   

 

   

   

   

   

 

j 

  

  

  

  

iterations

 b  learning process with antiutopiabased metric iau  

figure    learning process for the  objective lqg with a parametrization forced to pass
through the extreme points of the frontier  the constraints are able to correct the behavior
of iu  figure  a   and the convergence is faster than the previous parametrization  however 
iau still diverges  figure  b   and the returned frontier includes dominated solutions  since
the metric considers only the covering of the frontier and not the accuracy 

   

fiparisi  pirotta    restelli

table    summary of  dimensional lqg  unconstrained 
metrics
nonmixed
issues 

accuracy
covering
 
 
iu   ipn   frontier collapses in one point
iau   diverging behavior and dominated solutions found
 
 
i pn   difficult tuning of 
 
 

mixed  i pn
issues 
mixed  i u   i

true pareto frontier
approximate frontier

j 

     

   

   

   

     

j 

     

   

     

   

j 

j 

     

j 

 a  frontier approximated with antiutopiabased metric iau  

j 

true pareto frontier
approximate frontier

   
   

   
   

   
   

   
j 

   

   

j 

   
   

   
   

j 

j 

   

 b  frontier approximated with mixed metric i  

figure    resulting frontiers for the  objective lqg using an unconstrained parametrization  frontiers have been discretized for better representation  with iau the learning
diverges  figure  a   while i correctly approximates the pareto frontier  figure  b   

   

fimorl through continuous pareto manifold approximation

constrained parametrization 


     exp a     t    b     t     t      t      t  t     
 
     exp a   b     t      t     t      t      t  t     
     t    
 
 
 
     exp c        b t         b t     t     t     t  t    
a               

b               

t  simplex           

c               

the initial parameters are        numerical results are reported in table    where the
hypervolume has been computed normalizing the objective w r t  the antiutopia  figure  
shows the frontiers obtained using utopia and antiutopiabased indicators  we can clearly
see that  unlike the  objective case  even with a constrained parametrization these metrics
lead to poor solutions  failing in providing all mo desiderata  in figure  a  using iu the
frontier still tends to collapse towards the center of the true one  in order to minimize the
distance from the utopia point  only the constraint on  prevents that   although not shown
in the figures  a similar but slightly broader frontier is returned using ipn   however  we
stress that all solutions belong to the pareto frontier  i e   only nondominated solutions are
found  figure  b shows the frontier obtained with iau   as expected  the algorithm tries to
produce a frontier as wide as possible  in order to increase the distance from the antiutopia
point  this behavior leads to dominated solutions and the learning process diverges 
on the contrary  using mixed metrics i pn          i u          and i     
            pmga is able to completely and accurately cover the pareto frontier  as shown
in figures  a and  b  it is worth to notice the different magnitude of the free parameter  in
i pn compared to the  objective case  for which  was      as already discussed  this is due
to the substantial difference in magnitude between iau and ipn   on the contrary  the tuning
for the other mixed metrics was easier  as similar parameters used for the unconstrained
parametrization proved to be effective  we will come back to this topic in section   
finally  as shown in table    i u and i achieve the best numerical results  as the first
attains the highest hypervolume and the lowest loss  while the latter attains the fastest
convergence  their superiority also resides in their easy differentiability and tuning  especially compared to i pn   for these reasons  we have chosen them for an empirical analysis
on sample complexity and for a comparison against some state of the art algorithms on a
real world mo problem  which will be discussed in the next sections 
table    performance comparison between different metrics on the  objective lqg with
constrained parametrization  the reference frontier has a hypervolume of        
metric

hypervolume

loss

 iterations

iu

      

      e   

  

iau

 





ipn

      

      e   

   

i pn

      

      e   

  

i u

      

      e   

  

i

      

      e   

  

   

fiparisi  pirotta    restelli

table    summary of  dimensional lqg  constrained 
metrics
nonmixed
issues 
mixed

accuracy
covering
 
 
iu   ipn   frontier collapses in one point
iau   diverging behavior and dominated solutions found
 
 

j 

true pareto frontier
approximate frontier

   
   

   
   

   
   

   
j 

   

   

j 

   
   

   
   

j 

j 

   

 a  frontier approximated with utopiabased metric iu  

j 

true pareto frontier
approximate frontier

   
   

   
   

   
   

   
   
j 

j 

   
   

   

j 

   
j 

   

 b  frontier approximated with antiutopiabased metric iau  

figure    results with a parametrization forced to pass through the extreme points of the
frontier  using iu  figure  a   the frontier shrinks as much as allowed by the parametrization  the constraint is therefore not able to solve the issues of the metric as in the  
objective scenario  on the contrary  using iau the frontier gets wider and diverges from the
true one  in figure  b  an intermediate frontier is shown  

   

fimorl through continuous pareto manifold approximation

j 

true pareto frontier
approximate frontier

   

 

   
   

   
j 

   

   

   
   
   

j 

   

   

   
 

 a  frontier in objectives space 

   

   
 
   

 b  frontier in policy parameters space 

figure    results using i and a constrained parametrization  as shown in figure  a  
the approximate frontier perfectly overlaps the true one  despite small discrepancies in the
policy parameters space between the learned parameters and the optimal ones  figure  b   
similar frontiers are obtainable with i pn and i u  

      empirical sample complexity analysis
in this section  we provide an empirical analysis of the sample complexity of pmga  meant
as the number of rollouts needed to approximate the pareto frontier  the goal is to identify
the most relevant parameter in the estimate of mdp terms j    d j   and hj   
the analysis is performed on the  dimensional lqg domain by varying the number of
policies used to estimate the integral per iteration of pmga and the number of episodes
for each policy evaluation  the steps of each episode are fixed to     we first used the
parametrization forced to pass through the extreme points of the frontier with           t  
that produces an initial approximate frontier far from the true one  the parameter of the
learning rate in equation     was set to        and the parameter of i u was set to
      as performance criterion  we choose the total number of rollouts required to reach
a loss smaller than        and a hypervolume larger than       of the reference one 
these criteria are also used as conditions for convergence  both have to be satisfied   for
the evaluation  mdp terms are computed in closed form  the terminal condition must be
reached in          episodes otherwise the algorithm is forced to end  the symbol  is used
to represent the latter case 
from table  a it results that the most relevant parameter is the number of episodes
used to estimate the mdp terms  this parameter controls the variance in the estimate 
i e   the accuracy of the estimate of  l     by increasing the number of episodes  the
estimation process is less prone to generate misleading directions  as happens  for instance 
in the oneepisode case where parameters move towards a wrong direction  on the contrary 
the number of points used to estimate the integral  denoted in the table by  t  seems to
have no significant impact on the final performance of the algorithm  but it influences the
number of model evaluations needed to reach the prescribed accuracy  the best behavior 
   

fiparisi  pirotta    restelli

table    total number of episodes needed to converge on varying the number of points  t
to approximate the integral and the number of episodes  ep per point  the symbol  is
used when the terminal condition is not reached 
 a  if the parametrization is constrained to pass through the extreme points of the frontier  only one
point t is sufficient to move the whole frontier towards the right direction 

 ep

 

 

  

  

  

 



        

        

           

           

 



              

              

              

              

  



              

              

               

               

  



              

               

               

               

  



              

               

               

               

 t

 b  on the contrary  using an unconstrained parametrization  pmga needs both a sufficient number
of episodes and enough points t for a correct update step 

 ep

 

 

  

  

  

 











 









               

  







               

               

  







               

               

  





                

               



 t

from a samplebased perspective  has been obtained by exploiting only one point for the
integral estimate  although it can be surprising  a simple explanation exists  by forcing
the parameterization to pass through the singleobjective optima  a correct estimation of
the gradient direction of a single point t is enough to move the entire frontier toward the
true one  i e   to move the parameters towards the optimal ones 
on the contrary  if the unconstrained parametrization is used  one point is not sufficient
anymore  as shown in table  b  in this case  the initial parameter vector was set to    
            t   the learning rate parameter to        and the terminal condition requires a
frontier with loss smaller than     and hypervolume larger than     of the reference
frontier  without any constraint  the algorithm needs both accuracy in the evaluation of
single points i e   a sufficient number of episodes and enough points t to move the whole
frontier towards the right direction  the accuracy of the gradient estimate  l    therefore
depends on both the number of points t and the number of episodes  and pmga requires
much more rollouts to converge  the best behavior  from a samplebased perspective  has
been obtained by exploiting five points for the integral estimate and    episodes for the
policy evaluation 
   

fimorl through continuous pareto manifold approximation

    water reservoir
a water reservoir can be modeled as a momdp with a continuous state variable s representing the water volume stored in the reservoir  a continuous action a controlling the
water release  a state transition model depending also on the stochastic reservoir inflow  
and a set of conflicting objectives  this domain was proposed by pianosi et al         
formally  the state transition function can be described by the mass balance equation
st     st   t    max at   min at   at    where st is the reservoir storage at time t  t   is the
reservoir inflow from time t to t      generated by a white noise with normal distribution
t    n            at is the release decision  at and at are the minimum and the maximum
releases associated to storage st according to the relations at   st and at   max st         
in this work we consider three objectives  flooding along the lake shores  irrigation
supply and hydro power supply  the immediate rewards are defined by
r   st   at   st        max ht    h     
r   st   at   st        max   t      
r   st   at   st        max e  et        
where ht     st    s is the reservoir level  in the following experiments s       h is the
flooding threshold  h        t   max at   min at   at    is the release from the reservoir   is
the water demand          e is the electricity demand  e         and et   is the electricity
production
et      g  h    t ht    
where             is a dimensional conversion coefficient  g        the gravitational
acceleration       the turbine efficiency and h             the water density  r  denotes
the negative of the cost due to the flooding excess level  r  is the negative of the deficit in
water supply and r  is the negative of the deficit in hydro power production 
like in the original work  the discount factor is set to   for all the objectives and the
initial state is drawn from a finite set  however  different settings are used for the learning
and evaluation phases  given the intrinsic stochasticity of the problem  all policies are
evaluated over       episodes of     steps  while the learning phase requires a different
number of episodes over    steps  depending on the algorithm  we will discuss the details
in the results section 
since the problem is continuous we exploit a gaussian policy model


 a s      n     s t       
where    s  rd are the basis functions  d      and            as the optimal policies
for the objectives are not linear in the state variable  we use a radial basis approximation


i  s    e

ksci k 
wi

 

we used four centers ci uniformly placed in the interval           and widths wi of     for
a total of six policy parameters 
   

fiparisi  pirotta    restelli

      results
to evaluate the effectiveness of our algorithm we have analyzed its performance against the
frontiers found by a weighted sum stochastic dynamic programming  pianosi et al         
multi objective fqi  pianosi et al          the episodic version of relative entropy policy
search  peters et al         deisenroth et al          sms emoa  beume et al         
and two recent policy gradient approaches  i e   radial algorithm and paretofollowing
algorithm  parisi et al          since the optimal pareto front is not available  the one
found by sdp is chosen as reference one for the loss computation  mofqi learns only
deterministic policies  i e   the standard deviation  of the gaussian is set to zero  and
has been trained using         samples with a dataset of         tuples for the  objective
problem and         samples with a dataset of          tuples for the  objective problem 
the remaining competing algorithms all learn stochastic policies  the number of episodes
required for a policy update step is    for reps      for pfa and ra     for sms emoa 
given its episodic formulation  reps draws the parameters  from an upper distribution
      n       
where  is a diagonal covariance matrix  while  is set to zero  however  since the algorithm
learns the parameters          the overall learned policy is still stochastic  sms emoa
has a maximum population size of     and     for the   and  objective case  respectively 
the crossover is uniform and the mutation  which has a chance of     to occur  adds a white
noise to random chromosomes  at each iteration  the top     individuals are kept in the
next generation to guarantee that the solution quality will not decrease  finally  mofqi
scalarizes the objectives using the same weights as sdp  i e      and    weights for the   and
 objective case  respectively  reps uses instead    and     linearly spaced weights  ra
also follows    and     linearly spaced directions and  along with pfa  exploits the natural
gradient  peters   schaal      a  and the adaptive learning step in equation      with     
and m   f   where f is the fisher information matrix  concerning the parametrization of
pmga  we used a complete first degree polynomial for the  objective case


      t           t
       t            t


       t           t 

 
t         
     t    
 

       t           t 
       t             t 
        t             t
similarly  for the  objective case a complete second degree polynomial is used


             t           t  t      t           t  
              t           t  t     t           t   


              t           t  t           t      t   
 

     t    
             t           t  t           t      t     t  simplex          
 
 

               t           t  t     t          t  
 

              t           t  t          t  

 

both parameterizations are forced to pass near the extreme points of the pareto frontier 
computed through singleobjective policy search  in both cases the starting parameter
   

fimorl through continuous pareto manifold approximation

   
   

l   

j   water demand 

 

 

 

  

sdp
pmga    
pmga end  

    
  
    

  

           
iterations

   

 

 a 

   

 

         
j   flooding 

 

 b 

figure    results for the  objective water reservoir  even starting from an arbitrary poor
initial parametrization  pmga is able to approach the true pareto frontier  figure  b    in
figure  a   the trend of the manifold metric l    averaged over ten trials 

vector is                        t   the last parameter is set to    in order to guarantee
the generation of sufficiently explorative policies  as    is responsible for the variance of
the gaussian distribution  however  for a fair comparison  also all competing algorithms
take advantage of such information  as the mean of their initial policies is calculated accordingly to the behavior of the optimal ones described by castelletti et al          i e  
                     t   the initial standard deviation is set to       to guarantee sufficient exploration  this parametrization avoids completely random and poor quality initial
policies  utopia and antiutopia points were set to          and           for the  
objective case                  and               for the  objective one 
according to the results presented in section        the integral estimate in pmga is
performed using a montecarlo algorithm fed with only one random point  for each instance of variable t     trajectories by    steps are used to estimate the gradient and the
hessian of the policy  regarding the learning rate  the adaptive one described in equation     was used with       for the evaluation        and       points are used for the
integral estimate in the   and  objective case  respectively  as already discussed  given
the results obtained for the lqg problem and in order to show the capability of the approximate algorithm  we have decided to consider only the indicator i        and        
the main reasons are its efficiency  in table   it attained the fastest convergence  and its
easy differentiability  finally  we recall that all the results are averaged over ten trials 
figure  b reports the initial and final frontiers when only the first two objectives are
considered  even starting very far from the true pareto frontier  pmga is able to approach
it  increasing covering and accuracy of the approximate frontier  also  as shown in figure  a  despite the very low number of exploited samples  the algorithm presents an almost
monotonic trend during the learning process  which converges in a few iterations 
   

fiparisi  pirotta    restelli

j   water demand 

   

  

    

sdp
pfa
ra
mofqi
reps
sms emoa
pmga
   

   

   

 

   

   

j   flooding 

   

   

 

   

figure    visual comparison for the  objective water reservoir  pmga frontier is comparable to the ones obtained by state of the art algorithms in terms of accuracy and covering 
however  it is the only continuous one  as the others are scattered 
table    numerical algorithm comparison for the  objective water reservoir  the sdp
reference frontier has a hypervolume of        and nine solutions 
algorithm

hypervolume

loss

 rollouts

 solutions

              

              

               



pfa

              

              

               

          

ra

              

              

               

         

 

              

       

 

reps

              

              

               

         

sms emoa

              

              

                 

         

pmga

mofqi

figure   offers a visual comparison of the pareto points and tables   and   report a
numerical evaluation  including the hypervolume and the loss achieved by the algorithms
w r t  the sdp approximation    pmga attains the best performance both in the   and  
objective cases  followed by pfa  sms emoa also returns a good approximation  but is the
slowest  requiring more than ten times the amount of samples used by pmga  only mofqi
outperforms pmga on sample complexity  but its loss is the highest  finally  figure  
shows the hypervolume trend for pmga and a comparison on sample complexity for the
 objective case  pmga is substantially more sample efficient than the other algorithms 
attaining a larger hypervolume with much fewer rollouts  for example  it is capable of
generating a frontier with the same hypervolume of ra with only one tenth of the rollouts 
or it outperforms pfa with only half of the samples needed by the latter 
   results regarding mofqi include only the loss and the number of rollouts as the hypervolume and the
number of solutions are not available from the original paper 

   

fimorl through continuous pareto manifold approximation

     

pmga

pfa         
sms emoa          

hypervolume

    

reps         

     

ra         

    
     
    
     

     

     

     

      

      

      

      

 rollouts
figure    comparison of sample complexity on the  objective case using the hypervolume
as evaluation score  in brackets the number of rollouts needed by an algorithm to produce
its best frontier  pmga clearly outperforms all the competing algorithms  as it requires
much fewer samples to generate frontiers with better hypervolume 
table    numerical algorithm comparison for the  objective water reservoir  the sdp
reference frontier has a hypervolume of        and    solutions 
algorithm

hypervolume

loss

 rollouts

 solutions

              

              

               



pfa

              

              

                 

         

ra

              

              

                 

           

 

              

       

 

reps

              

              

                

       

sms emoa

              

              

                 

           

pmga

mofqi

   metrics tuning
in this section we want to examine more deeply the tuning of mixed metric parameters  in
order to provide the reader with better insights for a correct use of such metrics  the performance of pmga strongly depends on the indicator used and  thereby  their configuration
is critical  to be more precise  mixed metrics  which obtained the best approximate pareto
frontiers in the experiments conducted in section    include a trade off between accuracy
and covering  expressed by some parameters  in the following  we analyze the fundamental
concepts behind these metrics and study how their performance is influenced by changes in
the parameters 
   

fiparisi  pirotta    restelli

approximate frontier
     

true pareto frontier

   

   

   

   

   

   

   

   
   

 a      

     

   
   

   

   

   

 b        

   

   

   

   

 c      

figure     approximate frontiers for the  objective lqg learned by pmga using i pn
on varying   in figure  a  the indicator does not penalize enough for dominated solutions 
while in figure  c  the frontier is not wide enough  on the contrary  in figure  b  the
algorithm achieves both accuracy and covering 

    i tuning
the first indicator  to be maximized  that we analyze is
i   iau  w 
where w is a penalization term  in the previous sections we proposed w      ipn and
w      iu   in order to take advantage of the expansive behavior of the antiutopiabased
indicator and the accuracy of an optimalitybased indicator  in this section we study the
performance of this mixed metric by changing   proposing a simple tuning process  the
idea is to set  to an initial value and then increase  or decrease  it if the approximate
frontier contains dominated solutions  or is not wide enough   figure    shows different
approximate frontiers obtained with different values of  in the exact  objective lqg after
   iterations and using w      ipn   starting with      the indicator behaves mostly
like iau   meaning that  was too small  figure   a   increasing  to    figure   c  the
algorithm converges  but the approximate frontier does not completely cover the true one 
i e   ipn mostly condition the behavior of the metric  finally  with         figure   b  the
approximate frontier perfectly matches the true one and the metric correctly mixes the two
single indicators 
however  as already discussed in section    the use of w      ipn can be problematic
as the difference in magnitude between iau and ipn can make the tuning of  hard up to
the point the metric becomes ineffective  such a drawback can be solved using w      iu
and normalizing the reference point indicators  i e   iu and iau   by i j  p    kj p   k    
as the normalization bounds the utopia and antiutopiabased metrics in similar intervals 
i e         and        respectively   
    the ratio between two vectors a b is a component wise operation 

   

fimorl through continuous pareto manifold approximation

j 

j 

u

 

  

j 

au

 
 a 

j 

u

j 

au

 
 b 

u

 

j 

au

 
 c 

figure     examples of pareto frontiers  in figures  a  and  b  the frontiers are convex 
but in the latter objectives are not normalized  in figure  c  the frontier is concave 

    i tuning
the second mixed indicator  to be maximized  also takes advantage of the expansive behavior of the antiutopiabased indicator and the accuracy of the utopiabased one  it is
defined as
iau
i    
    
iu
where   and   are free parameters 
to better understand the insights that have guided our metric definition  we can consider
different scenarios according to the shape of the pareto frontier  in figure   a the frontier
is convex and we normalized the objectives  in this case any point that is closer to the
antiutopia than the utopia is  for sure  a dominated solution  the ratio iau  iu of any
point on the frontier will always be greater than   and hence it is reasonable to set  
and   both to    therefore  we do not need to know exactly the antiutopia point and the
drawback of the antiutopiabased metric iau disappears  since we also take into account the
distance from the utopia point  nevertheless  the setting of these points is critical  as their
magnitude can strongly affect pmga performance  an example is shown in figure   b 
where the frontier is not normalized and the objectives have different magnitude  in this
case  setting both   and   to    the indicator i evaluated at the extrema of the frontier
 j          t and j           t   is equal to      and     respectively  as the first value
is negative  an approximate frontier that includes all the points of the true pareto frontier 
but j  would perform better than the true pareto frontier 
on the contrary  if the frontier is concave  figure   c  it is not true that any point that
is closer to the antiutopia than the utopia is a dominated solution  and the ratio iau  iu
of any point on the frontier  with the exception  eventually  of its ends  will always be
smaller than one  keeping       and        pmga would try to collapse the frontier
into a single point  in order to maximize the indicator  therefore  the parameters need to
be changed accordingly by trial and error  for instance  if the returned frontier does not
achieve accuracy  a possible solution is to decrease   or to increase    
   

fiparisi  pirotta    restelli

   conclusion
in this paper we have proposed a novel gradientbased approach  namely paretomanifold
gradient algorithm  pmga   to learn a continuous approximation of the pareto frontier in
momdps  the idea is to define a parametric function  that describes a manifold in the
policy parameters space  that maps to a manifold in the objectives space  given a metric
measuring the quality of the manifold in the objectives space  i e   the candidate frontier  
we have shown how to compute  and estimate from trajectory samples  its gradient w r t 
the parameters of    updating the parameters along the gradient direction generates a new
policy manifold associated to an improved  w r t  the chosen metric  continuous frontier
in the objectives space  although we have provided a derivation independent from the
parametric function and the metric used to measure the quality of the candidate solutions 
both these terms strongly influence the final result  regarding the former  we achieved
high quality results by forcing the parameterization to pass through the singleobjective
optima  however  this trick might require domain expertise and additional samples and
therefore could not always be applicable  regarding the latter  we have presented different
alternative metrics  examined pros and cons of each one  shown their properties through
an empirical analysis and discussed a general tuning process for the most promising ones 
the evaluation also included a sample complexity analysis to investigate the performance
of pmga  and a comparison to state of the art algorithms in morl  from the results  our
approach outperforms the competing algorithms both in quality of the frontier and sample
complexity  it would be interesting to study these properties from a theoretical perspective
in order to provide support to the empirical evidence  we leave as open problems the
investigation of the convergence rate and of the approximation error of the true pareto
frontier  however  we think it will be hard to provide this analysis in the general setting 
future research will further address the study of metrics and parametric functions that
can produce good results in the general case  in particular  we will investigate problems
with many objectives  i e   more than three  and highdimensional policies  since the complexity of the manifold parameterization grows with the number of objectives and policy
parameters  a polynomial parameterization could not be effective in more complex problems and alternative parameterizations have to be found  another interesting direction of
research concerns importance sampling techniques for reducing the sample complexity in
the gradient estimate  since the frontier is composed of a continuum of policies  it is likely
that a trajectory generated by a specific policy can be partially used also for the estimation
of quantities related to similar policies  thus decreasing the number of samples needed for
the montecarlo estimate of the integral  moreover  it would be interesting to investigate automatic techniques for the tuning of the metric parameters and the applicability of
pmga to the multi agent scenario  e g   roijers  whiteson    oliehoek        

   

fimorl through continuous pareto manifold approximation

appendix a  optimal baseline
theorem a    componentdependent baseline   the optimal baseline for the  i  j  component
 i j 
of the hessian estimate hrf  jd    given in equation     is

 i j 
bh 



 
 i j 
g    



e t r   

 
  
 i j 
e g    

 

where
 i j 

g

 i j 

      i ln p      j ln p        h

ln p       

given a baseline b  the variance reduction obtained through the optimal baseline bh  is
var  hrf  jd    b    var  hrf  j    bh      


 i j   

 i j 
  
b
 bh 
 i j 
e
g    
 
 t
n
 i j 

proof  let g

    be the  i  j  th component of g    
 i j 

g

 i j 

      i ln p      j ln p        h

ln p       

 i j 

the variance of hrf  jd    is given by  
var



 i j 
hrf  jd



i 
  
    h

 i j 
 i j 
 i j 
g    
 e r     b i j  g    
     e r     b





  

  
 i j 
 i j 
 
 i j   
 e b
g    
  e r    g    





 
 h
i 
 i j 
 i j 
  b i j  e r    g    
 e r   g    
 




minimizing the previous equation w r t  b i j  we get

 i j 

bh 



  
 i j 
e r    g    

 
    
 i j 
e g    

    we use the compact notation e    to denote e t    

   

fiparisi  pirotta    restelli

the excess of variance is given by




 i j 
 i j 
 i j 
var g     r     b i j     var g     r     bh   




  
  
  

  
 i j 
 i j 
 i j 
 i j 
 i j 
 
  b
e r    g    
 e b
g    
  e r    g    





 h

  
 
  
i 
 i j 
 i j   
 i j 
 i j 
 
 e bh 
 e r   g    
 e r    g    
g    





    h
i 
 i j 
 i j 
 i j 
  e r   g    
   bh  e r    g    




   
  

  
 i j 
 i j 
 i j 
 i j 
  b
e g    
  b
e r    g    




  

  
  
 i j 
 i j 
 i j 
 i j 
 bh  e g    
   bh  e r    g    






  
 

 

 i j 
 i j 
 i j 
 
 i j 
e g    
  b
e r    g    
  b








    



 i j 

  
 e r    g    

 i j 




g    
    e

 i j 
e g    
 

   
 i j 


  

 e r    g    
 i j 
 




r    g    
   

 
e

 i j 
e g    

  

  
  

 i j 
 i j 
 i j 
 i j 
  b
e r    g    
e g    
  b




 

   
 i j 
e r    g    

 
  
 i j 
e g    




 
 i j 
g    



e r   


  i j   
 i j 

  b
  b
  

 i j 
e g    
e







 i j 

  b

 i j 

g


   

  


 i j   
bh  e




 
 i j 
g    


 

   



     
 i j 
 
 e r    g    
 


 



 
 

 i j 
e g    


fimorl through continuous pareto manifold approximation

references
ahmadzadeh  s   kormushev  p     caldwell  d          multi objective reinforcement
learning for auv thruster failure recovery  in adaptive dynamic programming and
reinforcement learning  adprl        ieee symposium on  pp     
athan  t  w     papalambros  p  y          a note on weighted criteria methods for compromise solutions in multi objective optimization  engineering optimization         
       
barrett  l     narayanan  s          learning all optimal policies with multiple criteria 
in proceedings of the   th international conference on machine learning  icml    
pp        new york  ny  usa  acm 
bertsekas  d  p          dynamic programming and suboptimal control  a survey from
adp to mpc   european journal of control                     
beume  n   naujoks  b     emmerich  m          sms emoa  multiobjective selection based
on dominated hypervolume  european journal of operational research               
      
brown  m     smith  r  e          directed multi objective optimization  international
journal of computers  systems  and signals             
calandra  r   peters  j     deisenrothy  m          pareto front modeling for sensitivity
analysis in multi objective bayesian optimization  in nips workshop on bayesian
optimization  vol    
castelletti  a   corani  g   rizzolli  a   soncinie sessa  r     weber  e          reinforcement learning in the operational management of a water system  in ifac workshop
on modeling and control in environmental issues  keio university  yokohama  japan 
pp         
castelletti  a   pianosi  f     restelli  m          tree based fitted q iteration for multiobjective markov decision problems  in neural networks  ijcnn   the      international joint conference on  pp     
castelletti  a   pianosi  f     restelli  m          a multiobjective reinforcement learning
approach to water resources systems operation  pareto frontier approximation in a
single run  water resources research                   
crites  r  h     barto  a  g          elevator group control using multiple reinforcement
learning agents  machine learning                   
das  i     dennis  j          a closer look at drawbacks of minimizing weighted sums of
objectives for pareto set generation in multicriteria optimization problems  structural
optimization               
das  i     dennis  j  e          normal boundary intersection  a new method for generating
the pareto surface in nonlinear multicriteria optimization problems  siam journal
on optimization                
deisenroth  m  p   neumann  g     peters  j          a survey on policy search for robotics 
foundations and trends in robotics                
   

fiparisi  pirotta    restelli

fonteneau  r     prashanth  l  a          simultaneous perturbation algorithms for batch
off policy search  in   rd ieee conference on decision and control  cdc       los
angeles  ca  usa  december              pp            ieee 
friedrich  t   horoba  c     neumann  f          multiplicative approximations and the
hypervolume indicator  in proceedings of the   th annual conference on genetic and
evolutionary computation  gecco     pp          new york  ny  usa  acm 
furmston  t     barber  d          a unifying perspective of parametric policy search
methods for markov decision processes  in pereira  f   burges  c   bottou  l    
weinberger  k   eds    advances in neural information processing systems     pp 
          curran associates  inc 
gabor  z   kalmar  z     szepesvari  c          multi criteria reinforcement learning 
in shavlik  j  w   ed    proceedings of the fifteenth international conference on
machine learning  icml        madison  wisconsin  usa  july              pp 
        morgan kaufmann 
greensmith  e   bartlett  p  l     baxter  j          variance reduction techniques for
gradient estimates in reinforcement learning  journal of machine learning research 
            
harada  k   sakuma  j     kobayashi  s          local search for multiobjective function
optimization  pareto descent method  in proceedings of the  th annual conference
on genetic and evolutionary computation  gecco     pp          new york  ny 
usa  acm 
harada  k   sakuma  j   kobayashi  s     ono  i          uniform sampling of local paretooptimal solution curves by pareto path following and its applications in multi objective
ga  in lipson  h   ed    genetic and evolutionary computation conference  gecco
      proceedings  london  england  uk  july             pp          acm 
kakade  s          optimizing average reward using discounted rewards  in helmbold  d  p  
  williamson  r  c   eds    computational learning theory    th annual conference
on computational learning theory  colt      and  th european conference on
computational learning theory  eurocolt       amsterdam  the netherlands  july
             proceedings  vol       of lecture notes in computer science  pp     
     springer 
koski  j     silvennoinen  r          norm methods and partial weighting in multicriterion optimization of structures  international journal for numerical methods in
engineering                   
lizotte  d  j   bowling  m     murphy  s  a          linear fitted q iteration with multiple
reward functions  journal of machine learning research               
lizotte  d  j   bowling  m  h     murphy  s  a          efficient reinforcement learning with
multiple reward functions for randomized controlled trial analysis  in furnkranz  j  
  joachims  t   eds    proceedings of the   th international conference on machine
learning  icml      june              haifa  israel  pp          omnipress 
   

fimorl through continuous pareto manifold approximation

magnus  j  r     neudecker  h          matrix differential calculus with applications
in statistics and econometrics  wiley ser  probab  statist   texts and references
section  wiley 
mannor  s     shimkin  n          the steering approach for multi criteria reinforcement
learning  in dietterich  t   becker  s     ghahramani  z   eds    advances in neural
information processing systems     pp            mit press 
mannor  s     shimkin  n          a geometric approach to multi criterion reinforcement
learning  j  mach  learn  res             
messac  a     ismail yahaya  a          multiobjective robust design using physical programming  structural and multidisciplinary optimization                 
messac  a   ismail yahaya  a     mattson  c  a          the normalized normal constraint method for generating the pareto frontier  structural and multidisciplinary
optimization               
munkres  j  r          analysis on manifolds  adv  books classics series  westview press 
natarajan  s     tadepalli  p          dynamic preferences in multi criteria reinforcement
learning  in raedt  l  d     wrobel  s   eds    machine learning  proceedings of
the twenty second international conference  icml        bonn  germany  august
            vol      of acm international conference proceeding series  pp         
acm 
nojima  y   kojima  f     kubota  n          local episode based learning of multiobjective behavior coordination for a mobile robot in dynamic environments  in fuzzy
systems        fuzz     the   th ieee international conference on  vol     pp 
       vol   
okabe  t   jin  y     sendhoff  b          a critical survey of performance indices for
multi objective optimisation  in evolutionary computation        cec     the     
congress on  vol     pp         vol   
parisi  s   pirotta  m   smacchia  n   bascetta  l     restelli  m          policy gradient
approaches for multi objective sequential decision making  in      international joint
conference on neural networks  ijcnn       beijing  china  july             pp 
          ieee 
perny  p     weng  p          on finding compromise solutions in multiobjective markov
decision processes  in coelho  h   studer  r     wooldridge  m   eds    ecai        th european conference on artificial intelligence  lisbon  portugal  august       
      proceedings  vol      of frontiers in artificial intelligence and applications  pp 
        ios press 
peters  j          machine learning of motor skills for robotics  ph d  thesis  university
of southern california 
peters  j   mulling  k     altun  y          relative entropy policy search  in fox  m  
  poole  d   eds    proceedings of the twenty fourth aaai conference on artificial
intelligence  aaai        pp            aaai press 
   

fiparisi  pirotta    restelli

peters  j     schaal  s       a   natural actor critic  neurocomputing                       
progress in modeling  theory  and application of computational intelligenc   th
european symposium on artificial neural networks        th european symposium
on artificial neural networks      
peters  j     schaal  s       b   reinforcement learning of motor skills with policy gradients 
neural networks                    robotics and neuroscience 
pianosi  f   castelletti  a     restelli  m          tree based fitted q iteration for multiobjective markov decision processes in water resource management  journal of hydroinformatics                 
pirotta  m   parisi  s     restelli  m          multi objective reinforcement learning with
continuous pareto frontier approximation  in bonet  b     koenig  s   eds    proceedings of the twenty ninth aaai conference on artificial intelligence  january       
      austin  texas  usa   pp            aaai press 
pirotta  m   restelli  m     bascetta  l          adaptive step size for policy gradient
methods  in burges  c  j  c   bottou  l   ghahramani  z     weinberger  k  q   eds   
advances in neural information processing systems       th annual conference on
neural information processing systems       proceedings of a meeting held december
           lake tahoe  nevada  united states   pp           
robert  c     casella  g          monte carlo statistical methods  springer texts in
statistics  springer verlag new york 
roijers  d  m   vamplew  p   whiteson  s     dazeley  r          a survey of multi objective
sequential decision making  journal of artificial intelligence research            
roijers  d  m   whiteson  s     oliehoek  f  a          computing convex coverage sets
for faster multi objective coordination  journal of artificial intelligence research     
       
romero  c          extended lexicographic goal programming  a unifying approach  omega 
             
shelton  c  r          importance sampling for reinforcement learning with multiple
objectives  ph d  thesis  massachusetts institute of technology 
steuer  r  e     choo  e  u          an interactive weighted tchebycheff procedure for
multiple objective programming  mathematical programming                 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  a bradford
book  bradford book 
sutton  r  s   mcallester  d  a   singh  s  p     mansour  y          policy gradient
methods for reinforcement learning with function approximation  in solla  s   leen 
t     muller  k   eds    advances in neural information processing systems     pp 
          mit press 
tesauro  g   das  r   chan  h   kephart  j   levine  d   rawson  f     lefurgy  c         
managing power consumption and performance of computing systems using reinforcement learning  in platt  j   koller  d   singer  y     roweis  s   eds    advances in
neural information processing systems     pp            curran associates  inc 
   

fimorl through continuous pareto manifold approximation

vamplew  p   dazeley  r   berry  a   issabekov  r     dekker  e          empirical evaluation methods for multiobjective reinforcement learning algorithms  machine learning 
               
van moffaert  k   drugan  m  m     nowe  a          scalarized multi objective reinforcement learning  novel design techniques  in adaptive dynamic programming and
reinforcement learning  adprl        ieee symposium on  pp         
van moffaert  k     nowe  a          multi objective reinforcement learning using sets of
pareto dominating policies  journal of machine learning research               
waltz  f  m          an engineering approach  hierarchical optimization criteria  automatic
control  ieee transactions on                 
wang  w     sebag  m          hypervolume indicator and dominance reward based multiobjective monte carlo tree search  machine learning                   
williams  r          simple statistical gradient following algorithms for connectionist reinforcement learning  machine learning                  
yu  p     leitmann  g          compromise solutions  domination structures  and salukvadzes solution  journal of optimization theory and applications                 
zitzler  e   thiele  l     bader  j          on set based multiobjective optimization  evolutionary computation  ieee transactions on               
zitzler  e   thiele  l   laumanns  m   fonseca  c  m     da fonseca  v  g          performance assessment of multiobjective optimizers  an analysis and review  evolutionary
computation  ieee transactions on                

   

fi