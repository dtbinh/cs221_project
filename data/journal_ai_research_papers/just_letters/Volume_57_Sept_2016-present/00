journal artificial intelligence research               

submitted        published      

learning continuous time bayesian networks
non stationary domains
simone villa
fabio stella

villa disco unimib it
stella disco unimib it

department informatics  systems communication
university milano bicocca
viale sarca            milan  italy

abstract
non stationary continuous time bayesian networks introduced  allow
parents set node change continuous time  three settings developed
learning non stationary continuous time bayesian networks data  known transition
times  known number epochs unknown number epochs  score function
setting derived corresponding learning algorithm developed  set numerical
experiments synthetic data used compare effectiveness non stationary continuous time bayesian networks non stationary dynamic bayesian networks  furthermore  performance achieved non stationary continuous time bayesian networks
compared achieved state of the art algorithms four real world datasets 
namely drosophila  saccharomyces cerevisiae  songbird macroeconomics 

   introduction
identification relationships statistical dependencies components multivariate time series  ability reasoning whether dependencies
change time crucial many research domains biology  economics  finance 
traffic engineering neurology  mention few  biology  example  knowing
gene regulatory network allows understand complex biological mechanisms ruling
cell  context  bayesian networks  bns   pearl        segal  peer  regev  koller 
  friedman        scutari   denis         dynamic bayesian networks  dbns   dean
  kanazawa        zou   conzen        vinh  chetty  coppel    wangikar       
continuous time bayesian networks  ctbns   nodelman  shelton    koller        acerbi 
zelante  narang    stella        used reconstruct transcriptional regulatory
networks gene expression data  effectiveness discrete dbns investigated identify functional correlations among neuroanatomical regions interest  burge 
lane  link  qiu    clark         useful primer bns functional magnetic resonance imaging data analysis made available  mumford   ramsey         however 
mentioned applications require time series generated stationary distribution  i e  one change time  stationarity reasonable
assumption many situations  cases data generating process clearly
non stationary  indeed  last years  researchers different disciplines  ranging
economics computational biology  sociology medicine become interested
representing relationships dependencies change time 
c
    
ai access foundation  rights reserved 

fivilla   stella

specifically  researchers interested analyzing temporal evolution
genetic networks  lebre  becq  devaux  stumpf    lelandais         flow neural
information networks  smith  yu  smulders  hartemink    jarvis         heart failure  liu 
hommersom  van der heijden    lucas         complications type   diabetes  marini 
trifoglio  barbarini  sambo  camillo  malovini  manfrini  cobelli    bellazzi       
dependence structure among financial markets crisis  durante   dunson        
according specialized literature evolution models  robinson   hartemink        
divided two main categories  structurally non stationary  i e  models
allowed change structure time  parametrically non stationary 
i e  models allow parameters values change time 
paper  structurally non stationary continuous time bayesian network model
 nsctbn  introduced  nsctbn consists sequence ctbns improves expressiveness single ctbn  indeed  nsctbn allows parents set node
change time specific transition times thus allows model non stationary systems  learn nsctbn  bayesian score learning ctbns extended  nodelman 
shelton    koller         nsctbn version bayesian score still decomposable
variable depends knowledge setting be  known transition times 
transition times known  known number epochs  number transition times known  unknown number epochs  number transition times
unknown  learning algorithm knowledge setting designed developed 
experiments non stationary dynamic bayesian networks  nsdbns   robinson  
hartemink         i e  discrete time counterparts nsctbns  performed 
main contributions paper following 
definition structurally non stationary continuous time bayesian network model 
derivation bayesian score decomposition knowledge setting 
design algorithms learning nsctbns different knowledge settings 
novel dynamic programming algorithm learning nsctbns known
transition times setting described  learning nsctbns others settings
performed simulated annealing  exploiting dynamic programming algorithm 
performance comparison nsctbns nsdbns knowledge settings
rich set synthetic data generated nsctbns nsdbns 
performance comparison nsctbns state of the art algorithms realworld datasets  namely drosophila  saccharomyces cerevisiae songbird 
nsctbn learned macroeconomics dataset consisting variables evolving
different time granularities spanning  st january        st march      
rest paper organized follows  section   continuous time bayesian networks introduced together learning problem complete data  section  
introduces non stationary continuous time bayesian networks  presents three learning settings derives corresponding bayesian score functions  algorithms learning
nsctbns different learning settings described section    numerical experiments synthetic real world datasets presented section    section   closes
paper making conclusions indicating directions research activities 
 

filearning continuous time bayesian networks non stationary domains

   continuous time bayesian networks
continuous time bayesian networks combine bayesian networks homogeneous markov
processes together efficiently model discrete state continuous time dynamical systems
 nodelman et al          particularly useful modeling domains variables evolve different time granularities  model presence people
computers  nodelman   horvitz         study reliability dynamical systems  boudali
  dugan         model failures server farms  herbrich  graepel    murphy        
detect network intrusion  xu   shelton         analyze social networks  fan   shelton 
       model cardiogenic heart failure  gatti  luciani    stella        reconstruct
gene regulatory networks  acerbi   stella        acerbi  vigano  poidinger  mortellaro 
zelante    stella         recently  complexity inference continuous time bayesian
networks studied  sturlaugson   sheppard        
    basics
representation ability continuous time bayesian networks inherent factorization system dynamics local continuous time markov processes depend
limited set states  continuous time bayesian network model defined follows 
definition    continuous time bayesian network  nodelman et al          let x
set random variables x    x    x            xn    x finite domain values
v al x     x    x            xi    continuous time bayesian network x consists two
    specified bayesian network x 
components  first initial distribution px
second continuous time transition model specified as  directed  possibly cyclic 
p a x 
graph g whose nodes x    x            xn   conditional intensity matrix  cim   qx
 
variable x x  p a x  denotes set parents x graph g 
p a x 

conditional intensity matrix qx

consists set intensity matrices

qxpa  u
qxpa  xu 
 

 
qxpai xu 


u
qpa
x

 
 
 
 


qxpa  xui
qxpa  xui
 

 
pau
qxi


pau ranges possible configurations parents set p a x   qxpai u  
p
pau
pau
pau
xj   xi qxi xj   off diagonal elements qx   i e  qxi xj   proportional probability
variable x transitions state xi state xj given parents state pau  
pau
u
intensity matrix qpa
x equivalently summarized two independent sets  q x  
pau
 qxi     i   i e  set intensities parameterizing exponential distributions
pau
pau
pau
u
next transition occurs  pa
x    xi xj   qxi xj  qxi     i  j i  j    i  
i e  set probabilities parameterizing multinomial distributions
state transitions  note ctbn model assumes one single variable
change state specific instant  transition dynamics specified parents
via cim  independent variables given markov blanket 
 

fivilla   stella

    structural learning
given fully observed dataset d  i e  dataset consisting multiple trajectories  whose
states transition times fully known  problem learning structure ctbn
addressed problem selecting graph g maximizes bayesian
score computed dataset  nodelman et al         
bs  g   d    ln p  g    ln p  d g  

   

p  g  prior graph g p  d g  marginal likelihood 
prior p  g  graph g  allows us prefer ctbns structures
others  usually assumed satisfy structure modularity property  friedman  
koller         i e  decompose following product terms 

p  g   
p  p a x    p ag  x   
   
xx

term parents set p ag  x  graph g  uniform prior g often used 
marginal likelihood p  d g  depends prior parameters p  q g   g  g 
usually assumed satisfy global parameter independence  local parameter independence parameter modularity properties  outlined below 
global parameter independence  spiegelhalter   lauritzen        states paramp  x 
p  x 
eters q x g
x g
associated variable x graph g independent 
thus prior parameters decomposes variable follows 

p  x  p  x 
p  q g   g  g   
p  q x g   x g  g  
   
xx

local parameter independence  spiegelhalter   lauritzen        asserts parameters associated configuration pau parents p ag  x  variable x
independent  therefore  parameters associated variable x decomposable
parent configuration pau follows 
yy
p  x  p  x 
u
   
p  q x g   x g  g   
p  qxpai u   pa
xi  g  
pau xi

parameter modularity  geiger   heckerman        asserts variable x
parents p ag  x    p ag    x  two distinct graphs g g     probability density
functions parameters associated x must identical 
p ag  x 

p  q x

p ag  x 

  x

p ag    x 

 g    p  q x

p ag    x 

  x

 g     

   

furthermore  assume sets parameters characterizing exponential distributions independent sets parameters characterizing multinomial
distributions 
p  q g   g  g    p  q g  g p   g  g  
   
   trajectory defined sequence pairs  t  x t    transition time      
associated state x t  random variables corresponding nodes ctbn 

 

filearning continuous time bayesian networks non stationary domains

dirichlet distribution selected prior parameters associated multinomial distribution  gamma distribution selected prior parameters
associated exponential distribution  i e 

p  qxpai u   gamma xpai u   xpai u  
   

pau
pau
pau
p   xi   dir xi x            xi xi  
   
xpai u   xpai u   xpai xu            xpai xui priors hyperparameters  particular  hyperparameters represent pseudocounts number transitions state state 
parameter represents imaginary amount time spent state
data observed  note hyperparameter xpai u inversely proportional
number joint states parents x  conditioning dataset d  obtain
following posteriors parameters 

u
 
   
p  qxpai u  d  gamma xpai u   mxpai u   xpai u   txpa


pau
pau
pau
pau
pau
    
p   xi  d  dir xi x    mxi x            xi xi   mxi xi  
u
mxpai xuj sufficient statistics ctbn  nodelman et al         
txpa

u
particular  txpa
amount time spent variable x state xi

parents p a x  state pau   mxpai xuj number times variable x
transitions state xi state xj parents p a x  state pau    
bayesian score     term p  g  grow size dataset d 
thus  significant term marginal likelihood p  d g   case complete data 
exploiting parameters independence     global parameter independence
property      marginal likelihood written follows 

p  x 
p  x 
p  d g   
l q x g  d  l  x g  d  
    

xx
p ag  x 

l q x

 d  marginal likelihood q derived follows 
yy
pau xi

p ag  x 

l  x

 xpai u   mxpai u       xpai u  
u
 xpai u       xpai u   txpa
 

u
 pa
xi    

pau
u
 pa
xi  mxi    

 

 d  marginal likelihood derived follows 


xpai xuj   mxpai xuj
 xpai u  
 
pau
pau
pau

 
 

 


x
x
x


xj
pa x  x
x   x
u



j



    

    

j

bayesian dirichlet equivalent  bde  metric version ctbns  nodelman        
case  bde metric uses priors          parameter modularity     
well global     local     parameter independence properties assumed
satisfied 
   please note number times p
variable x leaves state xi parents p a x 
pau
u
state pau computed follows mxpa
 
xj   xi mxi xj  


 

fivilla   stella

conclusion  bayesian score     computed closed form assuming
structure modularity property     satisfied  using bde metric follows 
x
p  x 
p  x 
ln p  p a x    p ag  x     ln l q x g  d    ln l  x g  d       
bs g   d   
xx

since graph g ctbn acyclicity constraints  possible maximize
bayesian score      separately optimizing parents set p a x  variable
x  worthwhile mention maximum number parents set 
search optimal value bayesian score      performed polynomial time 
search performed enumerating possible parents set using greedy
hill climbing procedure operators add  delete reverse edges graph g 

   non stationary continuous time bayesian networks
continuous time bayesian networks structurally stationary  graph
change time  parametrically stationary  conditional intensity matrices
change time  stationarity assumptions reasonable many situations 
cases data generating process intrinsically non stationary
thus ctbns longer used  therefore  section  extend ctbns become
structurally non stationary  i e  allow ctbns structure change continuous
time 
    definition
non stationary continuous time bayesian network model  graph ctbn
replaced graphs sequence g    g    g            ge    graph ge represents
causal dependency structure model epoch e                e     model
structurally non stationary introduction graphs sequence
handle transition times common whole network and or node specific 
following notations definitions used non stationary dynamic bayesian networks  let    t            te    transition times sequence  i e  times
causal dependency structure ge   active epoch e  replaced causal dependency
structure ge     becomes active epoch e      epoch defined period
time two consecutive transitions  i e  epoch e active period
time starting te  ending te   graph ge     active epoch
e      differs graph ge   active epoch e  set edges
call set edge changes ge  
figure   shows graphs sequence g    g    g    g    g    consisting four epochs  e     
transition times    t    t    t     epoch associated set edge changes 
specifically  graph g  differs graph g  following set edge changes
g     x  x    x    x    x    x     graph g  differs graph g 
following set edge changes g     x  x    graph g  differs graph
g  following set edge changes g     x  x    x  x    x    x    x    x    
   worthwhile mention first epoch  i e  epoch starting time   ending time t 
associated graph g    last epoch  i e  epoch starting time te  ending
time  the supremum considered time interval  i e     t   associated graph ge  

 

filearning continuous time bayesian networks non stationary domains

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

x 

 

 

 

 

t 

t 

 

t 



figure    graphs sequence g    g    g    g    g    nsctbn four epochs  e     
three transition times     t    t    t     edges gained lost time 
non stationary continuous time bayesian networks allow node
sequence parents sets  parents set active given epoch  therefore 
introduce concept homogeneous interval h x     h            hm   associated node
x  defined union consecutive epochs parents set
p a x  active node x  note epoch associated different
parents set  equal e 
non stationary continuous time bayesian network defined follows 
definition     structurally  non stationary continuous time bayesian network  let x
set random variables x            xn   x finite domain values v al x   
 x            xi     structurally  non stationary continuous time bayesian network nns  
 b  mns   x consists two components 
    specified bayesian network b x 
initial distribution px

non stationary continuous time transition model mns specified as 
sequence directed  possibly cyclic  graphs g    ge  e
e   whose nodes
x            xn   e represents number epochs 
p  x 

g
conditional intensity matrix  qx h x 
  x x  p ag  x  denotes
parents sets x g  h x  denotes intervals associated x 

p  x 

g
conditional intensity matrix qx h x 
consists set intensity matrices
u
qxpa   h

pa
q u
x  x   hm
 

 
pau
qxi x   hm



u
qpa
x hm

 
 
 
 


qxpa  xui  hm
qxpa  xui  hm
 

 
pau
qxi  hm

one configuration pau parents set p a x  p ag  x  active
interval hm h x   
u
   note following equation qxpai  h
 


p

xj   xi

 

qxpai xuj  hm still holds 

fivilla   stella

    learning framework
learning nsctbn fully observed dataset done using bayesian learning
framework taking account entire graphs sequence g  nsctbns case  must
specify prior probability graphs sequence g and  possible sequence 
density measure possible values parameters q g g   prior p  g 
likelihood p  q g   g  g  given  marginal likelihood p  d g  computed
bayesian score evaluated  important note focused
recovering graphs sequence g detecting possible changes parameters 
fact  identify non stationarity parameters model  i e  entries
conditional intensity matrices  significant enough result structural changes
graph  others changes assumed small enough alter graph structure 
      prior probability graphs
given transition times   thus number epochs e  assume prior
nsctbns structure g written follows 
p  g t     p  g         ge  t     p  g    g         ge   t     p  g   p  g         ge   t   
    
equation      justified assume probability distribution edge
changes function number changes performed  defined
independently initial graph g    knowledge particular edges
overall topology available initial network  use informative prior
p  g    otherwise resort uniform distribution  ctbns  p  g    must satisfy structure modularity assumption      prior set edge changes
p  g            ge   t   defines way edges change adjacent epochs 
      prior probability parameters
prior parameters p  q g   g  g    selected satisfy following assumptions 
independence sets parameters characterizing exponential multinomial distributions      parameter modularity     parameter independence  latter
assumption divided three components nsctbns  global parameter independence 
interval parameter independence local parameter independence 
global parameter independence asserts parameters associated node
nsctbns graphs sequence independent  prior parameters decomposes
variable x follows 

p ag  x 
p ag  x 
p  q g   g  g     
p  q x h x 
  x h x 
 g    
    
xx

interval parameter independence states parameters associated interval
active parents node independent  parameters associated
x parents sets p ag  x  decomposable interval hm h x  follows 
p  x 

p  x 

g
g
p  q x h x 
  x h x 
 g     


hm

 

p  x 

p  x 

p  q x hgm   x hgm  g    

    

filearning continuous time bayesian networks non stationary domains

local parameter independence states parameters associated state
variable given interval independent  thus parameters associated x
interval hm h x  decomposable parent configuration pau follows 
yy
p  x  p  x 
u
u
    
  pa
p  qxpai  h
p  q x hgm   x hgm  g     
xi  hm  g    

pau xi

ctbns case  dirichlet distribution used prior parameters
multinomial distribution gamma distribution used prior parameters
u

exponential distribution  sufficient statistics modified follows  txpa
 hm
amount time spent state x   xi p a x    pau interval h x    hm  
mxpai xuj  hm number transitions state x   xi state x   xj p a x    pau
p
interval h x    hm   let mxpai  hu   xj   xi mxpai xuj  hm number times
x leaves state xi parents p a x  state pau interval h x    hm  
      marginal likelihood
given graphs sequence g  transition times   marginal likelihood p  d g   
dataset computed closed form using priors sufficient statistics
previously defined  derive bayesian dirichlet equivalent metric nsctbns 
make assumptions ctbns  case  parameter independence
assumption divided global       interval      local      parameter independence 
therefore  marginal likelihood becomes 

p ag  x 
p ag  x 
p  d g     
l q x h x 
 d  l  x h x 
 d  
    
xx

marginal likelihood q equation      calculated follows 
 pau    


xi  hm
pau
pau
u
 
 

 

xpai  h



xi  hm
xi  hm

p ag  x 
l q x h x   d   
 pau  m pau      


xi  hm
xi  hm
pau
pau
pa
hm pau xi u    
xi  hm   txi  hm
xi  hm

    

marginal likelihood equation      calculated follows 




pau
pau
u
xpai  h


 





xi xj  hm
xi xj  hm

p ag  x 




l  x h x 
 d   
 
pau
pau
pau
xi xj  hm
hm pau xi  xj xi  hm   mxi  hm
xi   xj
    
important note nsctbns  pseudocounts well imaginary
amount time associated interval  aspect requires careful choice
order biased towards values small intervals analyzed 
possible correction weight ctbns hyperparameters quantity proportional time interval width  hm hm     hm denotes total time  thus 
nsctbns hyperparameters could defined follows 
xpai xuj  hm
xpai  hu

 hm hm   
 
hm
 hm hm   
  xpai u
 
hm
  xpai xuj

 

    
    

fivilla   stella

want control parameter priors using two hyperparameters  
use uniform bde nsctbns  bdeu   case  hyperparameters
defined           divided number u possible configurations
parents p a x  node x times cardinality domain x  follows 
xpai xuj  hm

 

xpai  hu

 

 hm hm   
 
ui
hm
 hm hm   
 
ui
hm

    
    

equations           rescale hyperparameters way biased
respect epochs length  equations           based uniform
distribution used performing numerical experiments 
    bayesian score decomposition
bayesian score decomposed variable based information available
transition times  regard  three knowledge settings used derive
bayesian score  namely  known transition times  ktt   known number epochs  kne 
unknown number epochs  une  
      known transition times
setting  transition times known  thus  prior probability
graphs sequence p  g t   decomposes equation       marginal likelihood
decomposes variable x according equation      
therefore  bayesian score bs g   d    written follows 
bs g   d      ln p  g      ln p  g            ge   t  
p  x 

p  x 

g
g
  ln l q x h x 
 d    ln l  x h x 
 d  

    

setting structural learning problem non stationary continuous time
bayesian network consists finding graph g  active first epoch  e     
e   sets edge changes g            ge  together corresponding parameters
values  maximize bayesian score defined equation      
graphs g            ge selected making assumptions ways
edges change continuous time  common approach  robinson   hartemink       
consists assuming graphs sequence g    g            ge   depends parameter
controls number edge changes continuous time  approach uses
truncated geometric distribution  parameter p     exp c    model number
parents changes occurring transition time te    
x
ce  
 ge  x   
    
xx

variable ce counts number edge changes two consecutive graphs ge
ge     parameter c controls impact number edge changes ce
score function      
  

filearning continuous time bayesian networks non stationary domains

edge changes ge assumed mutually independent  probability
edge changes subsequent epochs written follows 
p  g            ge   t    

e 

e  

e 

   exp c    exp c   ce

 exp c   ce  
   exp c   cmax   

    

e  

cmax truncation term  therefore  assume truncated geometric distribution number parents changes occurring transition times equation
     holds  bayesian score      decomposes variable x follows 
bs g   d     

x

ln p  p a x    p ag   x   c

xx
p  x 

g
  ln l q x h x 
 d   

e 
x

ce
e  
p ag  x 
ln l  x h x 
 d  

    

worthwhile notice number parents changes ce epoch e
penalizes bayesian score  thus discourages sudden variations parents set
consecutive epochs  parameter c controls impact changes
score function      
      known number epochs
transition times unknown  bayesian score written follows 
bs g    d    ln p  g      ln p  d g    

    

assuming p  g      p  g p  t   bayesian score      becomes 
bs g    d    ln p  g    ln p  t     ln p  d g    

    

number epochs e known  prior probability p  g  graphs
sequence g decomposes equation       truncated geometric distribution
used number parents changes occurring transition time 
known transition times setting 
choice p  t   made include prior knowledge set transition
times  however  information available  uniform prior p  t   used  implying
possible values transition times equally likely given number epochs
e  thus  bayesian score      decomposed variable x follows 
bs g    d    ln p  t    
 

x

ln p  p a x    p ag   x   c

xx
p ag  x 
ln l q x h x   d 

e 
x

ce

e  
p  x 

g
  ln l  x h x 
 d  

    

ce counts number edge changes two consecutive parents sets  c
controls impacts bs g    d  edge changes  happens ktt
setting 
  

fivilla   stella

      unknown number epochs
number epochs e unknown  transition times unknown well 
setting  learn nsctbn exploiting introduced ktt
kne settings  assume structure non stationary continuous time
bayesian network evolve different speeds continuous time  assumption
incorporated using truncated geometric distribution parameter p    exp e  
number epochs  general  large values e encode strong prior belief
structure nsctbn changes slowly  i e  epochs exist  
following presented ktt setting  bayesian score obtained
subtracting parameter e times number epochs e  therefore  bayesian
score bs g    d  decomposes variable x follows 
bs g    d    ln p  t   e e  

x

ln p  p a x    p ag   x   c

ce

e  

xx
p  x 

e 
x

p  x 

g
g
  ln l q x h x 
 d    ln l  x h x 
 d  

    

note bayesian score      contains two parameters  namely c e  
encode prior belief structure nsctbn  specifically  parameter c
regulates prior belief smoothness edge changes  e g  encouraging
discouraging edge changes per epoch   parameter e regulates prior belief
number epochs  e g  encouraging discouraging creation epochs  

   structural learning
optimal structure nsctbns found separately maximizing components
bayesian score associated node  achieved using exact optimization algorithm based dynamic programming transition times
given  contrast  number epochs known information
transition times available  resort approximate techniques based monte
carlo simulated annealing  present exact algorithm solving structural
learning problem ktt setting  then  briefly outline stochastic algorithms
solve structural learning problem kne setting une setting 
    known transition times
setting bayesian score decomposes according equation       thus 
optimal graphs sequence g found separately searching optimal parents sequence g x node x  solve problem finding optimal parents sequence
g x node x consider sequence consisting intervals h x     h            hm  
possible parents  z    s possible parents sets  find optimal parents
sequence g x must compute z marginal likelihood terms associated q  
one marginal likelihood term possible parents set p az  x  interval hm  
then  optimization algorithm used find maximum component
bayesian score associated node x 
  

filearning continuous time bayesian networks non stationary domains

exhaustive search would prohibitive  would require evaluating z scores 
one possible parents sequence g x   unfortunately  greedy search strategy
computes parents set maximizes bayesian score interval
viable  fact  function counts parents changes ce      binds choice
subsequent parents set  i e  binds ge ge    
however  relation score variable x associated parents set
p a x 
p a x  interval hm   denoted bsx hm   score associated parents set
p a x 

p a x  interval hm    denoted bsx hm    defined recursion follows 
n

p a x 
p  x 
p a x  p a x 
bsx hm   max bsx hzm  c cx e   ln l q x hm   x hm  d   
    
p az

cx e    ge  x    marginal likelihoods q grouped together 
p a x 
score bsx hm   associated parents set p a x  node x interval hm  
introduced clarify recursion used algorithm    note score depends
components score hm   particular  marginal likelihoods
component involved  term cx e   counts parents changes  included
binds choice subsequent parents sets  equation      exploited dynamic
programming select optimal parents sequence g x node x 
algorithm   takes input marginal likelihoods q interval
parents set  prior probability initial parents set  number parents
changes  parameter c   algorithm   ensures optimal parents sequence g x
node x corresponding optimal bayesian score  core computation
z score matrix  denoted sc  dynamic programming recursion 
dynamic programming recursion interval h   m      defined follows 
p  x 

sc z   ln l q x hz 

p  x 

  x hz 

 d    ln p  p az  x    p agh   x   

    

  z z  while  intervals hm  m                 recursion is 
n

p  x  p  x 
z
u
scm
  max scm 
  ln l q x hzm   x hzm  d  c cx e  
 uz

filling score matrix sc  value maxz  sc m  z   optimal bayesian score 
optimal parents sequence reconstructed backwards   using
index matrix   cost computing dynamic programming recursion o m z     
polynomial fixed maximum number parents s 
problem selecting optimal parents sequence interesting graph representation  indeed  possible create graph whose nodes associated marginal
likelihoods q interval hm parents set p az  x   node associated interval hm linked nodes associated interval hm    
arc associated weight computed difference marginal likelihoods interval hm parents set p az  x  cost switching
parents set interval hm  parents set interval hm   two special nodes
added represent start end optimal parents sequence  graph
cycles  thus selection optimal parents sequence node
reduced longest path problem start node end node directed
acyclic graph  thus solved using either dynamic linear programming 
  

fivilla   stella

algorithm   learnkttx
require  matrix containing marginal likelihoods q lx m  z   vector containing prior probability initial parents set p r z   matrix containing
number parents changes c z  z  parameter parents changes c  
ensure  score matrix sc m  z  index matrix  m  z  
   initialize sc m  z     m  z    
             
  
z            z
  
 m     
  
sc m  z  ln lx m  z    ln p r z 
  
else
  
w            z
  
score sc m    w    ln lx m  z  c c w  z 
  
 score   sc m  z  
   
sc m  z  score
   
 m  z  w
   
end
   
end
   
end
   
end
    end
learning nsctbn done following following four steps procedure  i  use
u
dataset compute variable x sufficient statistics txpa
mxpai xuj  hm
 hm
according given transition times   ii  compute marginal likelihoods           
fill lx matrix  iii  run algorithm   node x get corresponding
optimal parents sequence  iv  collect optimal parents sequence node x
compute corresponding cims using sufficient statistics already computed step i  
allow intervals differ transition times  i e  obtained
one possible unions transition times  repeat learning
procedure e  e      cases  possible speed computation
sufficient statistics aggregated intervals  way  read
dataset once  precomputed marginal likelihoods stored reused
intervals  moreover  computations performed parallel node 
    known number epochs
setting  know number epochs  transition times given 
cannot directly apply algorithm    however  tentative allocation transition
times given  apply algorithm   obtain optimal nsctbns structure 
assumption different true transition times   find
optimal tentative allocation   i e  allocation close possible   apply
simulated annealing  sa  algorithm  kirkpatrick  gelatt    vecchi        
  

filearning continuous time bayesian networks non stationary domains

simulated annealing iterative algorithm attempts find global optimum
x given function f  x  stochastic search feasible region  iteration
k  sa algorithm assumed state xk   samples proposal state x 
according proposal distribution x  p     xk    then  sa algorithm computes
quantity   exp   f  x  f  x     ct    ct computational temperature 
sa algorithm accepts proposal state x  probability equal min       concisely 
sa always accepts proposal state x  f  x      f  x  setting xk     x   
accepts proposal state x  f  x      f  x  probability setting xk     x 
probability xk     xk probability       i e  case state
sa algorithm change  computational temperature reduces iterations
according cooling schedule  shown one cools sufficiently slowly 
algorithm probably find global optimum  kirkpatrick et al          design
cooling schedule important part sa algorithm  bertsimas   tsitsiklis 
       possible approach use exponential cooling schedule defined follows 
ctk   ct  k   ct  represents initial temperature  typically set     
cooling rate  usually set close      k current iteration  murphy        
nsctbns case  state sa algorithm x associated tentative
allocation   function f  x  bayesian score       algorithm   takes input
sufficient statistics  parameters used run algorithm   parameters
sa algorithm  solves structural learning problem kne setting given
variable x ensuring optimal tentative allocation corresponding score 
algorithm   learnknex
require  sufficient statistics suffstatsx  prior probability p r    number parents
changes c      parameter c   tentative allocation   initial temperature ct    cooling
rate   number iterations iters  truncation parameter z standard deviation  
ensure  optimal tentative allocation best bayesian score bestsc 
   initialize k     
   lx getmlx suffstatsx    
   bestsc learnkttx m lx  p r    c      c  
    k   iters 
  
tentativeallocation t   z   
  
lx getmlx suffstatsx    
  
tentsc learnkttx m lx  p r    c      c  
  
ct ct  kn


  
accp rob min    exp  bestsctentsc 
ct
   
ur unirand  
   
 ur accp rob 
   

   
currsc tentsc
   
end
   
k k  
    end
    bestsc currsc
  

fivilla   stella

simulated annealing parameters used include tentative allocation  
initial temperature ct    cooling rate number iterations iters
exponential cooling schedule  moreover  truncation parameter z standard deviation
used selection new tentative allocation   according random
procedure shown algorithm    procedure selects transition time discrete
uniform distribution  uniranddiscr t    perturbs according truncated normal
distribution  stdnormrand    standard deviation equal   addition
point masses z z  z represents truncation parameter 
algorithm   tentativeallocation
require  tentative allocation   truncation parameter z standard deviation  
ensure  new tentative allocation    
   uniranddiscr t  
      
   nr stdnormrand  
    nr   z 
  
nr z
   end
    nr   z 
  
nr z
   end
      nr
     

    unknown number epochs
setting number epochs unknown  thus structural learning algorithm
must able move across different number epochs  well corresponding
transition times  case  used simulated annealing algorithm state
x tentative allocation function optimized f  x  bayesian score
shown equation       cooling schedule set one used
kne setting  proposal distribution differs one used kne setting
uses two additional operators  namely split merge operators  split
operator allows split given interval  tm   tm     two subintervals  tm   t   t  tm    
tm   tm     merge operator allows merge contiguous intervals  tm    tm  
 tm   tm     form wider interval  tm    tm     tm    tm   tm    
new state obtained sampling number epochs changes ec multinoulli distribution parameters  p    p    p     p  represents probability
number epochs next iteration  t   decreased one  p  represents probability
number epochs next iteration  t   increased one  p  represents
probability number epochs next iteration  t   change respect
current one  ec equal    algorithm   invoked  ec equal   
merge operator applied invoking algorithm    ec equal   
split operator applied invoking algorithm   
  

filearning continuous time bayesian networks non stationary domains

algorithm   solves structural learning problem nsctbn une setting
given node x ensuring optimal tentative allocation corresponding
bayesian score  algorithm similar one used kne settings 
uses algorithm   apply split merge operators  left t  function algorithm
  returns transition time comes immediately transition time t 
algorithm   learnunex
require  sufficient statistics suffstatsx  prior probability p r    number parents
changes c      parameter c   parameter e   tentative allocation   initial temperature
ct    cooling rate   number iterations iters  truncation parameter z  standard deviation   split probability sp merge probability mp 
ensure  optimal tentative allocation best bayesian score bestsc 
   initialize k     
   bestsc learnkttx getmlx suffstatsx     p r    c      c   e  t  
    k   iters 
  
splitmerge t   sp  mp 
  
tentativeallocation t   z   
  
tentsc learnkttx getmlx suffstatsx     p r    c      c   e  t  
  
ct ct  kn


  
accp rob min    exp  bestsctentsc 
ct
  
ur unirand  
   
 ur accp rob 
   

   
currsc tentsc
   
end
   
k k  
    end
    bestsc currsc
algorithm   splitmerge
require  tentative allocation   split probability sp merge probability mp 
ensure  new tentative allocation    
    
   p unirand  
    p   mp 
  
uniranddiscr t  
  
   
   else
  
 p    mp   sp  
  
uniranddiscr t  
  
nt left t    tleft t 
 
   
  nt
   
end
    end
  

fivilla   stella

   numerical experiments
numerical experiments performed synthetic real world datasets  synthetic
datasets used compare nsctbns nsdbns ktt  kne une knowledge settings terms accuracy  precision  recall f  measure  following real world
datasets  drosophila  saccharomyces cerevisiae songbird  used compare nsctbns
state of the art algorithms  i e  tsni  a method based ordinary differential equations  
nsdbn  robinson   hartemink        non homogeneous dynamic bayesian networks
bayesian regularization  tvdbn   dondelinger  lebre    husmeier        
une knowledge setting  drosophila  saccharomyces cerevisiae songbird datasets
collected fixed time intervals  thus analyzed additional real world dataset  consisting financial economic variables evolving different time granularities  exploit
expressiveness nsctbns events occur asynchronously  note performance comparison using synthetic datasets benefits knowledge ground
truth  apply performance comparison using real world datasets
ground truth available  cases  comparison exploits partial
meta knowledge available specialized literature 
    synthetic datasets
artificially generated datasets include data sampled rich set nsdbn models 
i e  nsdbn generated datasets  rich set nsctbn models  i e  nsctbn generated
datasets  nsdbn nsctbn models consist five nodes associated binary
ternary variables  numerical experiments concern learning parents sets  transition
times number epochs single node  choice motivated fact
structural learning nsctbn performed single node independently
remaining ones  however  transition times unknown  multiple parents
sets changes could make easier correctly identify times change 
      nsdbn generated datasets
nsdbn generated datasets sampled nsdbn models  associated following
number epochs e               particular  number epochs e     different
nsdbn instances sampled obtain number datasets equal     one consisting single trajectory  thus     synthetic datasets used learn structure
nsdbn nsctbn  number models     ktt  kne une settings 
structural learning experiments performed c             e              
nsctbn                             nsdbn    overall number
      experiments performed  particular  performed number epochs
number datasets number c number models               experiments
ktt setting      kne setting  number epochs number
datasets number c number e number models               
experiments performed une setting 
   inter slice arcs allowed  intra slice arcs allowed  holds true nsdbn models
sampled obtain nsdbn generated datasets 
   worthwhile mention parameters nsdbn counterparts c
e parameters nsctbn 

  

filearning continuous time bayesian networks non stationary domains

nsdbn jar executable   robinson   hartemink        used structural learning nsdbn  set maximum number proposed networks        
burn in period        nsdbn  nsctbn learned using following parameters setting  iters          ct                  z           sp        mp       
          using bdeu metric  furthermore  nsdbn nsctbn set
maximum number parents    arcs occurred    percent
samples  belong inferred nsdbn nsctbn models  accuracy  acc   precision  p rc   recall  rec  f  measure  f    achieved nsdbn nsctbn learned
ktt  kne une settings reported table        respectively 
worthwhile mention kne une settings  nsdbns nsctbns
almost always identified correct number epochs location associated
transition times  accuracy  precision  recall f  measure computed two
different ways  firstly  included arcs true network epoch  secondly 
excluded self reference arcs  i e  arcs connecting node two consecutive
time slices true network epoch  fact  node nsctbn
self reference arc default  happen nsdbns  means
first case nsdbn required learn arcs nsctbn required do  therefore 
ensure fair comparison nsctbn nsdbn adopted second case  tables   
    report performance measure values computed excluding self reference arcs
set arcs true networks epoch 
table    nsctbn compared nsdbn ktt setting nsdbn generated data 
average  min  subscript  max  superscript  performance values    networks
c nsctbn nsdbn 
number epochs e
 
 

 
acc
p rec
rec
f 

 

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

according tables         nsdbns consistently achieve greater accuracy values
achieved nsctbns three settings  furthermore  nsdbns
accuracy stable respect number epochs e happen
nsctbns  indeed  number epochs e greater    nsctbns achieve
accuracy values significantly smaller achieved number
epochs e equal      happen nsdbns accuracy
robust respect number epochs e 
   acknowledge precious help alex hartemink let us use nsdbn jar executable program
learning nsdbn models  furthermore  provided drosophila songbird datasets 
   samples obtained parameters values 

  

fivilla   stella

table    nsctbn compared nsdbn kne setting nsdbn generated data 
average  min  subscript  max  superscript  performance values    networks
c nsctbn nsdbn 
number epochs e
 
 

 
acc
p rec
rec
f 

 

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

table    nsctbn compared nsdbn une setting nsdbn generated data 
average  min  subscript  max  superscript  performance values    networks
c   e nsctbn   nsdbn 
number epochs e
 
 

 
acc
p rec
rec
f 

 

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

    
        
    
        
    
        
    
        

        
    
        
    
        
    
        
    

        
    
        
    
        
    
        
    

different picture emerges focusing task discover positive arcs  indeed 
case nsctbns achieve values precision  recall f  measure  always
greater achieved nsdbns  nsctbns achieve precision values robust
respect knowledge settings number epochs e 
hold true recall performance measure  indeed  nsctbns achieve robust recall
respect knowledge settings  ktt  kne une   recall achieved
nsctbns significantly degrades moving     epochs knowledge
settings  happens f  measure achieved nsctbns  results
numerical experiments suggest nsctbns effective nsdbns discover
positive arcs  even datasets generated using nsdbns  possible explanation
behavior learning nsdbns difficult learning nsctbns 
particular  nsdbns must learn self reference arcs nsctbns not  furthermore 
node  nsctbns learn locally sequence parents sets
happen nsdbns  fact  nsdbns learn globally sequence parents sets
nodes  i e  globally learn sequence networks  thus solve learning
problem difficult one solved nsctbns 
  

filearning continuous time bayesian networks non stationary domains

      nsctbn generated datasets
generated    synthetic datasets e               datasets used
learn structure nsctbn three knowledge settings  parameters
setting used one used nsctbn learning nsdbn generated datasets  for
nsctbn  used            bdeu metric   while  case 
perform structural learning experiments nsdbn models    graphical structures
nsctbn models sampled obtain datasets sampled
obtain nsdbn datasets  goal experiments analyze performance
nsctbn structural learning algorithms three knowledge settings 
analysis data reported tables        brings us conclude
nsctbn structural learning algorithms work well three settings according
considered performance measures  accuracy  recall f  measure decrease slightly
number epochs increases      particular  recall measure suffers
greatest decrease        number epochs increases     
accuracy f  measure robust respect number epochs 
precision robust performance measure respect different datasets
different values number epochs knowledge settings 
table    nsctbn ktt setting nsctbn generated data  average  min
 subscript  max  superscript  performance values    networks c  

acc
p rec
rec
f 

 
    
        
    
        
    
        
    
        

number
 
        
    
        
    
        
    
        
    

epochs e
 
        
    
        
    
        
    
        
    

 
        
    
        
    
        
    
        
    

table    nsctbn kne setting nsctbn generated data  average  min
 subscript  max  superscript  performance values    networks c  

acc
p rec
rec
f 

 
    
        
    
        
    
        
    
        

number
 
        
    
        
    
        
    
        
    

epochs e
 
        
    
        
    
        
    
        
    

 
        
    
        
    
        
    
        
    

   nsctbn generated data asynchronous involving different time granularities  thus nsdbn cannot
directly applied  option preprocess datasets adapt nsdbns  given
would strongly arbitrary penalizing nsdbns  decided learn nsctbn models 

  

fivilla   stella

table    nsctbn une setting nsctbn generated data  average  min
 subscript  max  superscript  performance values    networks c e  

 
    
        
    
        
    
        
    
        

acc
p rec
rec
f 

number
 
        
    
        
    
        
    
        
    

epochs e
 
        
    
        
    
        
    
        
    

 
        
    
        
    
        
    
        
    

best worst values accuracy e     reported table   belong
experiments performed synthetic dataset number   number   respectively 
results illustrated hereafter  figure   a  shows graphs sequence true nsctbn
synthetic datasets number    figure   b  displays posterior distribution
epochs  right   together distribution corresponding transition times
 left    learned nsctbn une case  figure   shows information
depicted figure    synthetic dataset number    latter case 
distribution epochs slightly favor correct number epochs 

 a  true nsctbn model 
distribution transition times

distribution number epochs

 

 
true
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

 

  

  

  

  

  

     
time

  

  

  

  

  

  

 

 
number epochs

 b  learned nsctbn model results 

figure    nsctbn generated dataset number     a  true graphs sequence e   epochs
 b  distribution transition times  left  posterior epochs  right  associated
nsctbn inferred une setting 

    transition times whose distance less     aggregated 

  

filearning continuous time bayesian networks non stationary domains

 a  true nsctbn model 
distribution transition times

distribution number epochs

 

 
true
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

 

  

  

  

  

  

  

     
time

  

  

  

  

  

  

  

 

 
 
number epochs

 b  learned nsctbn model results 

figure    nsctbn generated dataset number     a  true graphs sequence e   epochs
 b  distribution transition times  left  posterior epochs  right  associated
nsctbn inferred une setting 

    real world datasets
difficult find real world datasets corresponding ground truth model
completely known and or uniform consensus domain experts reached 
therefore  decided use following three well known datasets  drosophila  saccharomyces cerevisiae songbird compare performance nsctbns nsdbns
state of the art algorithms  i e  tsni tvdbn  datasets publicly
available  clearly described rich detailed discussion likely ground truth
models given specialized literature  furthermore  macroeconomics dataset introduced analyzed  dataset consists    financial economic variables collected
different time granularity spanning  st january        st march      
      drosophila
drosophila dataset includes mrna expression levels       genes    successive time points spanning four stages drosophila melanogaster life cycle  lebre
et al          embryonic     time points   larval     time points  pupal stage    
time points  first    days adulthood    time points   comparative purposes
 dondelinger et al          analyzed reduced drosophila dataset consisting
gene expression time series    genes involved wing muscle development  given
nsctbns based discrete variables  binarized expression level    genes
reduced drosophila dataset done literature  zhao  serpedin    dougherty 
      guo  hanneke  fu    xing        robinson   hartemink        
  

fivilla   stella

firstly  network inference task embryonic  larval  pupal adulthood morphogenic stages performed ktt setting  robinson   hartemink        dondelinger et al          nsctbn structural learning performed using following
parameter values c                                                      setting maximum number parents    nsctbn learned different c values combined 
arcs occurred    percent samples included
inferred non stationary continuous time bayesian network  techniques predict
non stationary directed networks  robinson   hartemink         precision  recall
f  measure  computed respect networks inferred zhao et al         guo et
al          reported table   nsdbn  nsctbn tvdbn  dondelinger et al  
       networks associated four epochs  inferred nsctbn
reduced drosophila dataset ktt setting  depicted figure   
table    precision  prec   recall  rec  f  measure  f    achieved nsctbn  nsdbn 
tvdbn drosophila dataset computed respect networks inferred
zhao et al         guo et al          average values  average  precision  recall
f  measure achieved zhao et al         guo et al         reported 

nsdbn
nsctbn
tvdbn

zhao
prec
    
    
    

et al        
rec
f 
         
         
         

guo et al        
prec rec
f 
              
              
              

prec
    
    
    

average
rec
f 
         
         
         

according table    optimal algorithm exists reduced drosophila dataset 
network retrieved zhao et al         used ground truth  nsdbn
best model  network retrieved guo et al         network used ground
truth  tvdbn optimal one far f  measure concerned  average
performance computed  nsdbn best model tvdbn worst 
nsctbn achieves f  value close one achieved nsdbn 
secondly  investigated whether transition times inferred structural learning
nsctbn une setting correspond known transitions stages  lebre
et al         dondelinger et al          network inference task performed learning
nsctbn une setting following parameter values c                   
e                   furthermore  set maximum number parents   
number iterations       number runs     
figure   shows distribution transition times    left  posterior
number epochs  right   number epochs correctly detected   even
probability close     associated   epochs  however  transition times
correctly identified  embryonic stage correctly identified  larval stage
correctly discovered start time point     inferred end time point   
    stem represents posterior probability corresponding time point starts new epoch 
therefore  stem time point means epoch ends time point    next epoch
starts time point t 

  

filearning continuous time bayesian networks non stationary domains

instead     nsctbn identify pupal adulthood stages  identified
two additional transition times          behavior observed nsdbns 
tvdbns capable correctly identify pupal adulthood stages  however  tvdbn    tvdbn exp tvdbn bino inferred networks  dondelinger et al  
      consist number epochs ranging     

mhc

mhc
gfl

gfl

mlc 

mlc 

eve

eve

msp   

msp   

actn

actn

myo  f

myo  f





prm

prm
twi

twi

sls

sls

 a  embryonic  epoch       

 b  larval  epoch        

mhc

mhc
gfl

gfl

mlc 

mlc 

eve

eve

msp   

msp   

actn

actn

myo  f

myo  f





prm

prm
twi

twi

sls

sls

 c  pupal  epoch        

 d  adulthood  epoch        

figure    networks inferred nsctbn kkt setting reduced drosophila
dataset  arcs occurred    percent networks associated
different c values included inferred nsctbn model 

  

fivilla   stella

distribution transition times

distribution number epochs

 

 
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

                                                                
time

 

 
 
number epochs

figure    transition time graph  left  posterior probability histogram number
epochs e  right  associated nsctbn model learned drosophila reduced
dataset une setting c       e     

      saccharomyces cerevisiae
saccharomyces cerevisiae dataset obtained synthetic regulatory network
  genes saccharomyces cerevisiae  cantone  marucci  iorio  ricci  belcastro  bansal 
santini  di bernardo  di bernardo    cosma         obtained measuring gene
expression time series rt pcr  reverse transcription polymerase chain reaction 
      time points two conditions related carbon source  galactose  switch
experimental condition  glucose  switch experimental condition   merged
time series two experimental conditions exclusion boundary point
done literature  dondelinger et al          obtained time series binarized
way   indicates gene expression level greater equal
sample mean    indicates gene expression level smaller sample mean 
obtained dataset used infer saccharomyces cerevisiae networks associated
switch switch experimental conditions 
network inference task performed learning nsctbn une setting
following parameter values c                    e                     furthermore  set maximum number parents    number iterations      
number runs      arcs occurred    percent runs
included inferred nsctbn model  precision  recall f  measure values achieved
nsctbn compared achieved state of the art algorithms  i e  tsni 
nsdbn tvdbn  table   
result performed numerical experiment shows nsctbn competitive respect state of the art algorithms  achieves non optimal results
precision associated switch experimental condition  condition 
 
nsctbn achieves precision equal         
   optimal value achieved tsni
 
tvdbn            contrary  nsctbn achieves best recall value 
equal              switch experimental condition  nsctbn achieves best
value precision  equal              recall  equal             
  

filearning continuous time bayesian networks non stationary domains

computed overall performance structural learning algorithms 
case  focusing attention f  measure  conclude nsctbn       
comparable tvdbn         considered state of the art algorithm
structural learning task applied saccharomyces cerevisiae dataset  networks
inferred nsctbn model switch switch experimental conditions 
using c       c      depicted figure   
table    nsctbn compared tsni  nsdbn  tvdbn learning saccharomyces cerevisiae dataset  nsctbn learned une setting  c        e      
time point    used transition time switch switch experimental conditions  tsni  nsdbn tvdbn networks described specialized
literature  precision  recall f  measure reported switch switch
experimental conditions  number true positive arcs  superscript  sum
true false positive arcs  subscript  reported precision  number true
positive arcs  superscript  sum true positive false negative arcs  subscript 
reported recall  performance values achieved aggregating inferred networks
two epochs reported 

tsni
nsdbn
tvdbn
nsctbn

switch
p rec
rec
             
             
             
              

f 
    
    
    
    

switch
p rec rec
f 
                  
                  
                  
                  

gal 

f 
    
    
    
    

gal 

gal  

cbf 

swi 

aggregated
p rec
rec
               
               
               
      
      
  
  

gal  

ash 

swi 

 a  switch network 

cbf 

ash 

 b  switch network 

figure    switch  a  switch  b  networks inferred nsctbn saccharomyces cerevisiae dataset une setting c        e      two pictures
report positive arcs  black continuous   false negative arcs  red dashed 
false positive arcs  green dotted  inferred networks 
  

fivilla   stella

figure   shows posterior distribution number epochs  left  together
distribution transition times  right  nsctbn learned c       e     
transition switch switch experimental conditions known
occur time point     i e  switch epoch starts time point      worthwhile
notice small number arcs  associated synthetic regulatory network
saccharomyces cerevisiae  suggests one careful evaluating
result performed numerical experiment  particular  think overstatements
effectiveness and or superiority different structural learning algorithms
learning task saccharomyces cerevisiae dataset avoided 
distribution transition times

distribution number epochs

 

 
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

 

 

 

 

                                         
time

 

 
number epochs

figure    transition time graph  left  posterior probability number epochs
 right  associated nsctbn inferred saccharomyces cerevisiae dataset
une setting c       e      maximum aposteriori estimate
number epochs associated e     epochs  epoch   starts time point  
ends time point     epoch   starts time point    ends time point    

      songbird
songbird dataset collected eight electrodes placed vocal nuclei six
female zebra finches  smith et al          voltage changes recorded populations
neurons birds provided four different two second auditory stimuli 
presented       times  voltages post processed root mean square
transformation binned   ms  robinson   hartemink        
songbird dataset used learn neural information flow networks  i e  networks
represent transmission information different regions songbird
brain  neural information flow network represents dynamic utilization potential
pathways along information travel  identification neural information
flow networks songbirds auditory stimuli allows understand sounds
stored processed songbirds brain  songbird dataset consists data
  variables recorded electrodes two seconds pre stimulus  two seconds
stimulus two seconds post stimulus six birds  stimuli hear song  i e 
bird hears another bird singing  white noise  i e  bird hears white noise stimulus 
  

filearning continuous time bayesian networks non stationary domains

show results nsctbn learned two six birds songbird
dataset  namely bird     bird      results obtained four birds
similar  given nsctbns based discrete variables  values   variables
discretized three bins using uniform quantiles                  according literature
 robinson   hartemink         inference task neural information flow networks
performed learning nsctbn une setting following parameter
values c                            e                             set maximum
number parents    number iterations     number runs    
figure    a   b  show probability transition  left  posterior probability
number epochs  right  bird     bird     white noise stimulus 
figure    a   b  show probability transition  left  posterior probability
number epochs  right  bird     bird     hear song stimulus 
distribution transition times

distribution number epochs

   

 
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number epochs

 a  white noise stimulus bird      learned model results 
distribution transition times

distribution number epochs

   

 
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number epochs

 b  white noise stimulus bird      learned model results 

figure    distribution transition times posterior distribution epochs
nsctbn une setting songbird dataset white noise stimulus 
  

fivilla   stella

location transition time points white noise stimulus hearsong stimulus accurately inferred bird     bird      posterior distribution
number epochs birds         white noise stimulus nearly
equally split     epochs  hear song stimulus peaked  
epochs  therefore  number epochs location transition time points
reliably recovered nsctbn learned une setting  unfortunately 
able find additional information validate learned nsctbns
dataset  moreover  comparison across different birds eventually develop consensus
network possible due songbird data collection settings  indeed  six
birds characterized electrodes  make difficult obtain correspondence
map across different birds 

distribution transition times

distribution number epochs

   

 
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number epochs

 a  hear song stimulus bird      learned model results 
distribution transition times

distribution number epochs

   

 
retrieved
   
posterior probability

probability transition

   

   

   

   

 

   

   

   

 

   

 

   

 

   

 
time

   

 

   

 

   

 

 

 
 
number epochs

 b  hear song stimulus bird      learned model results 

figure    distribution transition times posterior distribution epochs
nsctbn une setting songbird dataset hear song stimulus 

  

filearning continuous time bayesian networks non stationary domains

      macroeconomics
macroeconomics dataset consists    financial economic time series pertaining
economy united states  time series different time granularity span
 st january        st march       specifically  five time series daily granularity  namely crude oil  oil   usd eur spot exchange rate  usdeur   gold  gold  
s p    equity index  s p        years treasury bond yield rate  us  yrsnote  
eleven time series monthly granularity  namely production total industry  pti  
real manufacturing trade industries sales  rmtis   personal income  pi   unemployment  un   consumer price index  cpi   federal funds rate  rate   producer price index
 ppi   non farm payrolls  nfp   new one family houses sold  nhsold   new houses sale
 nhsale  new private house permits  nhpermit   finally  gross domestic product
 gdp  time series quarterly granularity 
goal study discover financial economic environment evolves
time  particular  focused attention detect business cycles  
associated change relationships among financial economic variables  given
duration business cycle highly variable  ability identify turning point
cycle  i e  recession starts  considerable importance policymakers  financial
companies well individuals  substantial literature available business
cycle turning points detection generally relying markov switching models  hamilton  
raj         however  models able represent important features
dependence structure among variables business cycle 
order use nsctbn model context  applied binary discretization
variable associated time series  discretization performed using lookback period   year  i e  current value greater past one  binary
variable set   otherwise  set    approach looking back past
widely used finance  moskowitz  ooi    pedersen         nsctbns learning
performed une setting using following parameter values  c               
e                   maximum parents per node      iterations    runs 
figure    shows probability transition  left side  left axis  versus s p   
equity index used reference  left side  right axis  posterior probability
number epochs  right side   nsctbn consists three epochs transition times
close end july      end november       compare dates
turning points us business cycle reported national bureau economic
research     see far turning point march     
close one december       missed turning point occurred
july       probably limited length dataset 
figure    shows structure nsctbn model corresponding probable
number epochs  i e  e      arc included nsctbn model occurs
    performed runs epoch  retrieved networks correspond
following time periods  january      july       epoch     august     
november       epoch    december      march       epoch    
    business cycles fluctuations aggregate economic activity  recurrent  i e  possible
identify expansion recession cycles   persistent periodic  i e  differ length severity  
    official business cycle turning points dates available http   www nber org cycles html

  

fivilla   stella

distribution transition times vs s p   

distribution number epochs
    

 

   

    

   

   

    

   

    

   

   

 

posterior probability

value

probability transition

retrieved  left 
s p     right 

 
    

    

    

    
    
time

    

   

   

 
    

    

   

 

 

 
 
 
number epochs

figure     distribution transition times s p    behavior time  left   posterior probability epochs  right  learned nsctbn une setting 
usdeur

oil
gold

usdeur
oil

un

gold

us  yrs

us  yrs

ppi

nhper

cpi

ppi

nhper

pti

sp   

pti

sp   

pi
rmtis

nhsale

rate

pi

cpi

rmtis

rate
nfp

gdp
nhsold
nfp

gdp
un

nhsale

 a  epoch    jan        jul       

nhsold

 b  epoch    aug        nov       
usdeur
oil

ppi

us  yrs

gold
nhsale

sp   
pti

cpi

rate

nhper
pi
rmtis
nfp
un
gdp

nhsold

 c  epoch    dec        mar       

figure     nsctbn learned macroeconomics dataset une setting 
nsctbn corresponds probable number epochs  e       arc included
nsctbn model occurs     runs epoch 

  

filearning continuous time bayesian networks non stationary domains

novelty approach economic analysis opens door many considerations new speculations economic variables business cycles 
paper  highlight two patterns emerging learned nsctbn model  well
known relevant role personal income  pi  relation unemployment  un 
 mankiw        less known relation non farm payrolls  nfp  s p   
equity index  s p      miao  ramchander    zumwalt        

   conclusions
introduced non stationary continuous time bayesian networks developed three
structural learning algorithms used different knowledge settings  i e  ktt 
kne une  problem analyzed  structural learning algorithm
known transition times case exact exploits graph theory infer optimal
nsctbns structure  polynomial time complexity assumption
maximum number parents node fixed  nsctbns structural learning algorithms competitive state of the art algorithms synthetic real world
datasets considered  statement proved rich set numerical experiments 
nsctbns adapted use different score metrics  far considered score
metrics integrates non structural parameters  nsctbns exploit interesting
property ctbns offer possibility learn optimal nsctbns structure
single variable  could extremely useful case non stationary
behavior analyzed system synchronous  thus may case
node changes parents independently nodes change parents set 
however  two main limitations exist nsctbns  i  variables assumed
discrete  specifically variable dataset must take value countable number
states ii  finding optimal value c e hyperparameters extremely
difficult  the true nsdbns   concerning i   problem discretizing continuous
variables studied long time robust solutions described
specialized literature  discretizing continuous variables whose value measured time
studied intensively many issues still remain  problem ii  selecting
optimal value hyperparameters known specialized literature much
done experts provide valuable apriori knowledge  however  apriori
knowledge poor available all  selecting optimal hyperparameter values
extremely difficult  important note one strong limitations studying
comparing non stationary models lack ground truth models 
possible directions research include application nsctbns structural
learning algorithms datasets  arabidopsis thaliana dataset  grzegorczyk 
aderhold    husmeier        well financial datasets supported in depth
economic analyses  another interesting perspective study development
modeling approach  going towards direction allowing node change parents
set asynchronously  furthermore  think increase applicability real world
time series data proposed nsctbns structural learning algorithms issue timeseries discretization must addressed  particular  think issue must
addressed integrated manner nsctbns structural learning algorithm 
  

fivilla   stella

finally  could interesting apply framework nsctbns address
task classification objects streaming context using probabilistic graphical model based approach  borchani  martinez  masegosa  langseth  nielsen  salmeron 
fernandez  madsen    saez      a  borchani  martnez  masegosa  langseth  nielsen 
salmeron  fernandez  madsen    saez      b  

acknowledgments
authors wish thank alexander hartemink kindly provided nsdbn
jar executable associated datasets  special thank goes marco grzegorczyk
providing arabidopsis thaliana dataset together fundamental information analyze
it  authors greatly indebted anonymous referees constructive comments
extremely helpful suggestions  contributed significantly improve
quality paper  special thank goes associate editor manfred jaeger 
fabio stella corresponding author article 

references
acerbi  e     stella  f          continuous time bayesian networks gene network reconstruction  comparative study time course data    th international
symposium bioinformatics research applications  zhangjiajie  china       
   
acerbi  e   vigano  e   poidinger  m   mortellaro  a   zelante  t     stella  f         
continuous time bayesian networks identify prdm  negative regulator th   cell
differentiation humans  scientific reports           
acerbi  e   zelante  t   narang  v     stella  f          gene network inference using
continuous time bayesian networks  comparative study application th   cell
differentiation  bmc bioinformatics         
ahmed  a     xing  e  p          recovering time varying networks dependencies social
biological studies  proceedings national academy sciences           
           
bertsimas  d     tsitsiklis  j          simulated annealing  statistical science              
borchani  h   martinez  a  m   masegosa  a   langseth  h   nielsen  t  d   salmeron  a  
fernandez  a   madsen  a  l     saez  r       a   dynamic bayesian modeling
risk prediction credit operations    th scandinavian conference artificial
intelligence  scai        halmstad  sweden 
borchani  h   martnez  a  m   masegosa  a  r   langseth  h   nielsen  t  d   salmeron 
a   fernandez  a   madsen  a  l     saez  r       b   modeling concept drift 
probabilistic graphical model based approach    th international symposium
intelligent data analysis  ida        saint etienne  france 
boudali  h     dugan  j  b          continuous time bayesian network reliability modeling  analysis framework  ieee transactions reliability               
  

filearning continuous time bayesian networks non stationary domains

burge  j   lane  t   link  h   qiu  s     clark  v  p          discrete dynamic bayesian
network analysis fmri data  human brain mapping                 
cantone  i   marucci  l   iorio  f   ricci  m  a   belcastro  v   bansal  m   santini  s  
di bernardo  m   di bernardo  d     cosma  m  p          yeast synthetic network
vivo assessment reverse engineering modeling approaches  cell          
        
dean  t     kanazawa  k          model reasoning persistence causation 
comput  intell                 
dondelinger  f   lebre  s     husmeier  d          non homogeneous dynamic bayesian
networks bayesian regularization inferring gene regulatory networks
gradually time varying structure  machine learning                 
durante  d     dunson  d  b          bayesian dynamic financial networks timevarying predictors  statistics   probability letters           
fan  y     shelton  c  r          learning continuous time social network dynamics 
  th conference uncertainty artificial intelligence  uai        montreal 
canada 
friedman  n     koller  d          bayesian bayesian network structure 
bayesian approach structure discovery bayesian networks  machine learning 
          
gatti  e   luciani  d     stella  f          continuous time bayesian network model
cardiogenic heart failure  flexible services manufacturing journal         
       
geiger  d     heckerman  d          characterization dirchlet distributions
local global independence  annals statistics               
grzegorczyk  m   aderhold  a     husmeier  d          inferring bi directional interactions circadian clock genes metabolism model ensembles  statistical
applications genetics molecular biology                 
guo  f   hanneke  s   fu  w     xing  e  p          recovering temporally rewiring networks  model based approach  machine learning  proceedings   th international conference  icml        corvallis  usa  june              pp         
hamilton  j  d     raj  b   eds            advances markov switching models  applications business cycle research finance  studies empirical economics 
springer verlag 
herbrich  r   graepel  t     murphy  b          structure failure   nd usenix
workshop tackling computer systems problems machine learning techniques
 sysml      cambridge  usa  pp     
kirkpatrick  s   gelatt  c  d     vecchi  m  p          optimization simulated annealing 
science                     
lebre  s   becq  j   devaux  f   stumpf  m     lelandais  g          statistical inference
time varying structure gene regulation networks  bmc systems biology        
     
  

fivilla   stella

liu  m   hommersom  a   van der heijden  m     lucas  p  j          hybrid time bayesian
networks  international journal approximate reasoning   
mankiw  n  g          principles macroeconomics   th edition   south western college
pub 
marini  s   trifoglio  e   barbarini  n   sambo  f   camillo  b  d   malovini  a   manfrini 
m   cobelli  c     bellazzi  r          dynamic bayesian network model longterm simulation clinical complications type   diabetes  journal biomedical
informatics              
miao  h   ramchander  s     zumwalt  j  k          s p     index futures price jumps
macroeconomic news  journal futures markets                   
moskowitz  t  j   ooi  y  h     pedersen  l  h          time series momentum  journal
financial economics                  
mumford  j  a     ramsey  j  d          bayesian networks fmri  primer  neuroimage 
           
murphy  k  p          machine learning  probabilistic perspective  mit press 
nodelman  u          continuous time bayesian networks  ph d  thesis  stanford university 
nodelman  u     horvitz  e          continuous time bayesian networks inferring users
presence activities extensions modeling evaluation  tech  rep  msrtr          microsoft research 
nodelman  u   shelton  c  r     koller  d          continuous time bayesian networks 
  th conference uncertainty artificial intelligence  uai        edmonton 
canada  pp         
nodelman  u   shelton  c     koller  d          learning continuous time bayesian networks    th conference uncertainty artificial intelligence  uai       
acapulco  mexico  pp         
pearl  j          probabilistic reasoning intelligent systems   networks plausible inference  morgan kaufmann series representation reasoning  morgan kaufmann 
robinson  j  w     hartemink  a  j          learning non stationary dynamic bayesian
networks  journal machine learning research               
scutari  m     denis  j  b          bayesian networks examples r  chapman
hall  boca raton  isbn                
segal  e   peer  d   regev  a   koller  d     friedman  n          learning module networks  journal machine learning research            
smith  a  v   yu  j   smulders  t  v   hartemink  a  j     jarvis  e  d          computational inference neural information flow networks  plos computational biology 
        e     
spiegelhalter  d  j     lauritzen  s  l          sequential updating conditional probabilities directed graphical structures  networks                 
  

filearning continuous time bayesian networks non stationary domains

sturlaugson  l     sheppard  j  w          inference complexity continuous time bayesian
networks    th conference uncertainty artificial intelligence  uai
       quebec city  canada  pp         
vinh  n  x   chetty  m   coppel  r     wangikar  p  p          gene regulatory network
modeling via global optimization high order dynamic bayesian network  bmc
bioinformatics          
xu  j     shelton  c  r          continuous time bayesian networks host level network
intrusion detection  european conference machine learning principles
practice knowledge discovery databases  ecml pkdd        antwerp 
belgium  pp         
zhao  w   serpedin  e     dougherty  e  r          inferring gene regulatory networks
time series data using minimum description length principle  bioinformatics 
                  
zou  m     conzen  s  d          new dynamic bayesian network  dbn  approach identifying gene regulatory networks time course microarray data  bioinformatics 
             

  


