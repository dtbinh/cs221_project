journal of artificial intelligence research                  

submitted        published      

lightweight random indexing for
polylingual text classification
alejandro moreo fernandez
andrea esuli

alejandro moreo isti cnr it
andrea esuli isti cnr it

istituto di scienza e tecnologie dellinformazione
consiglio nazionale delle ricerche
      pisa  it

fabrizio sebastiani

fsebastiani qf org qa

qatar computing research institute
hamad bin khalifa university
po box       doha  qa

abstract
multilingual text classification  mltc  is a text classification task in which documents are
written each in one among a set l of natural languages  and in which all documents must be
classified under the same classification scheme  irrespective of language  there are two main
variants of mltc  namely cross lingual text classification  cltc  and polylingual text
classification  pltc   in pltc  which is the focus of this paper  we assume  differently
from cltc  that for each language in l there is a representative set of training documents 
pltc consists of improving the accuracy of each of the  l  monolingual classifiers by
also leveraging the training documents written in the other   l      languages  the
obvious solution  consisting of generating a single polylingual classifier from the juxtaposed
monolingual vector spaces  is usually infeasible  since the dimensionality of the resulting
vector space is roughly  l  times that of a monolingual one  and is thus often unmanageable 
as a response  the use of machine translation tools or multilingual dictionaries has been
proposed  however  these resources are not always available  or are not always free to use 
one machine translation free and dictionary free method that  to the best of our knowledge  has never been applied to pltc before  is random indexing  ri   we analyse ri in
terms of space and time efficiency  and propose a particular configuration of it  that we
dub lightweight random indexing  lri   by running experiments on two well known public benchmarks  reuters rcv  rcv   a comparable corpus  and jrc acquis  a parallel
one   we show lri to outperform  both in terms of effectiveness and efficiency  a number
of previously proposed machine translation free and dictionary free pltc methods that
we use as baselines 

   introduction
with the rapid growth of multicultural and multilingual information accessible on the internet  how to properly classify texts written in different languages has become a problem
of relevant practical interest  multilingual text classification  mltc  is a text classification task in which documents are written each in one among a set l    l            l l    of
natural languages  and in which all documents must be classified under the same classification scheme  irrespective of the language  there are two main variants of mltc  namely
cross lingual text classification  cltc  and polylingual text classification  pltc  
c
    
ai access foundation  all rights reserved 

fimoreo  esuli    sebastiani

cltc is a task characterized by the fact that  for all languages in a subset lt 
l  there are no training documents  the task thus consists of classifying the unlabelled
documents written in the languages in lt  i e   the target languages  by leveraging the
training documents expressed in the other languages ls   l lt  i e   the source languages  
cltc is thus a transfer learning problem  pan   yang         where one needs to transfer
the knowledge acquired by learning from the training data in ls   to the task of classifying
documents in lt   most previous work on mltc indeed focuses on cltc  and fewer efforts
have been devoted to pltc  which is instead the focus of this paper 
in pltc  a representative set of training documents for all languages in l is assumed to
be available  therefore  a straightforward solution may consist in training  l  independent
monolingual classifiers  one for each language  however  such solution is suboptimal  as
each classifier is obtained by disregarding the additional supervision that could be obtained
by using the training documents written in the other   l      languages  pltc thus
consists of leveraging the training documents written in all languages in l to improve
the classification accuracy that could be obtained by simply training the  l  independent 
monolingual classifiers 
however  pltc entails a number of obstacles that work to the detriment of efficient
representation  to see this  assume we generate a single polylingual vector space  hereafter 
the juxtaposed vector space  by juxtaposing the monolingual vector spaces  the vector
space for a monolingual dataset usually consists of tens or even hundreds of thousands of
features  for the juxtaposed vector space of a polylingual dataset  this dimensionality gets
roughly multiplied by the number of distinct languages under consideration  such a substantial increase in the feature space would degrade the performance of many classification
algorithms  because of the so called curse of dimensionality  and would also bring about
a severe degradation in efficiency  additionally  co occurrence based techniques tend to lose
power when representations are polylingual  since terms belonging to different languages
rarely co occur  if at all  a problem usually referred to as feature disjointness  
as a response  some authors have proposed the use of machine translation  mt  tools
as a device to simultaneously cope with both high dimensionality and feature disjointness in
pltc  the idea is to reduce the problem to the monolingual case  typically english   that
is  non english training documents are automatically translated into english  are added to
the english training set  and a monolingual  english  classifier is trained  at classification
time  non english unlabelled documents are translated into english and are then classified 
 of course  this idea can also be used in cltc  in this case  there are no training documents
to translate   however  these mt based pltc  and cltc  techniques suffer from a number
of drawbacks  wei  yang  lee  shi    yang          i  automatically translated texts usually
present different statistical properties with respect to human translations   ii  mt systems
are not always available for all language pairs  and  iii  training a statistical mt system
from any of the free toolkits available requires collecting large corpora of parallel text in
the domain of interest  which is not always easy 
thesaurus based and dictionary based methods  on the other side  represent a lighter
approach in mltc  if a multilingual dictionary or thesaurus that encompasses the different languages is available  some kind of unification of the vector representation may
be attempted  this is customarily done by replacing non english words with their english equivalents in the dictionary  or by replacing all terms with thesaurus codes invariant
   

filightweight random indexing for polylingual text classification

across languages  e g   babelnet synsets  ehrmann  cecconi  vannella  mccrae  cimiano 
  navigli         however  bilingual dictionaries or thesauri are not available for all language pairs  and automatically constructing a domain dependent bilingual resource requires
a suitable parallel corpus with sentence level alignment 
    distributional representations
for classification purposes  a textual document is usually represented as a vector in a vector
space according to the bag of words  bow  model  i e   each distinct term corresponds to a
dimension of the vector space  in the juxtaposed vector space  most of the columns in the
document by term matrix are thus informative for only one of the languages 
since each distinct term corresponds to a dimension of the vector space  the bow model
is agnostic with respect to semantic similarities among terms  that is  the dimension for
term governor is orthogonal to the dimension for the related term president  as it is to
the dimension for the unrelated term transport  the semantic relations among terms can
be uncovered by detecting their co occurrences  i e   the contexts in which words tend to be
used together  this idea rests on the distributional hypothesis  according to which words
with similar meanings tend to co occur in the same contexts  harris         by detecting
co occurrences  it is possible to establish a parallelism between term meaning and geometrical properties in the vector space  distributed semantic models  dsms  sometimes also
called word space models in sahlgren        aim at learning continuous and compact distributed term representations  which have recently been called word embeddings  mikolov 
sutskever  chen  corrado    dean      b   dsms have gained a lot of attention from the
machine learning community  delivering improved results in many natural language processing tasks  bengio  schwenk  senecal  morin    gauvain        bullinaria   levy       
collobert  weston  bottou  karlen  kavukcuoglu    kuksa         dsm based methods
can be categorised  see pennington  socher    manning        baroni  dinu    kruszewski 
      as belonging  a  to the class of context counting models  which are often based on
matrix factorization  e g   latent semantic analysis  lsa  deerwester  dumais  furnas 
landauer    harshman        osterlund  odling    sahlgren         or  b  to the class
of context predicting models  e g   methods based on deep learning architectures  bengio 
      mikolov et al       b  
however  in multilingual contexts huge quantities of plain text for each language should
be processed in order to learn meaningful word representations  which incurs high computational costs  trying to find such representations for a large multilingual vocabulary
can thus become computationally prohibitive  some attempts have recently been made
in this direction  by leveraging multilingual external resources such as wikipedia articles
 al rfou  perozzi    skiena         or bilingual dictionaries  gouws   sgaard         or
word aligned parallel corpora  klementiev  titov    bhattarai         or sentence aligned
parallel corpora  zou  socher  cer    manning        hermann   blunsom        lauly 
boulanger    larochelle        chandar  lauly  larochelle  khapra  ravindran  raykar   
saha         or document aligned parallel corpora  vulic   moens         however  such
external resources may not always be available for all language combinations and  when
they are available  e g   wikipedia articles   they may be of uneven quality and quantity for
languages other than english  alternatively  other approaches require a computationally
   

fimoreo  esuli    sebastiani

expensive post processing step to align word representations across languages  mikolov  le 
  sutskever      a  faruqui   dyer        
in this article we discuss efficient representation mechanisms for pltc that  i  are
mt free   ii  do not require external resources  and  iii  do not incur high computational
costs  in particular  we investigate the suitability of random indexing  ri  kanerva 
kristofersson    holst        sahlgren        as an effective representation mechanism of
the original co occurrence matrix in pltc  ri is a context counting model belonging to the
family of random projections methods  kaski        papadimitriou  raghavan  tamaki   
vempala         that produces linear projections into a nearly orthogonal reduced space
where the original distances between vectors are approximately preserved  hecht nielsen 
      johnson  lindenstrauss    schechtman         ri is expected to deliver fast and
semantically meaningful representations in a reduced space  and can be viewed as a cheaper
approximation of lsa  sahlgren         ri is such that each column from the polylingual
matrix produced by it will not depend on any single specific language  as it does instead in
the bow representation   we hypothesize this could be advantageous in pltc  since the
entire new space becomes potentially informative for all languages at once  thus making the
problem more easily separable if enough dimensions are considered  while ri has already
been applied to bilingual scenarios  gorman   curran        sahlgren   karlgren        
to the best of our knowledge it has not been tested on the pltc case so far  in monolingual
tc  ri was found to be competitive  but not superior  to bow  sahlgren   coster        
in this article we demonstrate that ri outperforms the bow model in pltc 
the method we present in this article  that we dub lightweight random indexing  lri  
is inspired by the works of achlioptas        and li  hastie  and church        on very
sparse random projections  and goes one step further by pushing sparsity to the limit  lri is
designed so that the orthogonality of the projection base is maximized  which causes sparsity
to be preserved after the projection  we empirically show that lri helps support vector
machines  svms  to deliver better classification accuracies in pltc with respect to many
popular alternative vector space models  including the main random projection variants 
lsa based approaches  and polylingual topic models   while also requiring substantially
less computation effort 
the contribution of this work is twofold  first  we conduct a comparative empirical
study of several pltc approaches in two representative scenarios  the first is when the
training corpus is comparable at the topic level  i e   documents are not direct translations of
each other  but are simply about similar topics  this is here exemplified by the rcv  rcv 
dataset   and the second is when the training corpus is parallel at the document level  i e  
each text is available in all languages thanks to the intervention of human translators  this
scenario is exemplified by the jrc acquis dataset   we show that lri yields the best results
in both settings  in terms of both effectiveness and efficiency  as a second contribution  we
present an analytical study that can be useful to better understand the nature of random
mapping methods 
the rest of this paper is organized as follows  in section   we discuss related work 
in section   we present the problem statement  describe the random indexing method in
detail  and present our proposal  section   reports the results of the experiments we have
conducted  section   presents an analytical study on computational efficiency  while section
  concludes 
   

filightweight random indexing for polylingual text classification

   related work
this section gives an overview of the main approaches to pltc that have emerged in the
literature  we distinguish three groups of methods  according to whether the problem is approached  i  by leveraging external resources   ii  by combining the outcome of independent
monolingual classifiers  or  iii  by reducing the dimensionality of the resulting multilingual
feature space  this discussion also includes some references to cltc techniques that we
consider relevant to pltc and to our approach 
    exploiting external multilingual resources
multilingual text classification is a relatively recent area of research  and most previous
efforts within it were devoted to the cltc subtask  as in cltc there is no labelled
information for all languages  previous approaches typically relied on automatic translation
mechanisms as a means to fill the gap between the source and the target languages  the
main difference between cltc and pltc lies in the fact that pltc exploits labelled
documents belonging to different languages during learning  despite this  the two tasks
have a close knit relation  since in both of them cross lingual adaptation is generally carried
out by means of external resources  such as parallel corpora  bilingual dictionaries  and
statistical thesauri 
if a suitable  unlabelled  multilingual corpus containing short aligned pieces of texts
is available  correlations among groups of words in the two languages could be explored 
cross lingual kernel canonical correlation analysis  cl kcca  was proposed by vinokourov  shawe taylor  and cristianini        as a means to obtain a semantic cross lingual
representation  by investigating correlations between aligned text fragments  cl kcca
takes advantage of kernel functions in order to map aligned texts into a high dimensional
space in such a manner that the correlations between the mapped aligned texts are jointly
maximized  this cross lingual representation could then be used for classification  retrieval 
or clustering tasks  cl kcca was investigated in combination with support vector machines  svms  and applied to cross lingual patent classification by li and shawe taylor
        their method  called svm  k  learns two svm based classifiers by searching two
linear projections in the original feature space of each language such that the distance of the
projections  instead of the correlation of the projections  of two aligned texts is minimized 
in a similar vein  polylingual topic models  mimno  wallach  naradowsky  smith   
mccallum        have been proposed as an extension of latent dirichlet allocation  lda
 blei  ng    jordan        to the polylingual case  lda is a generative model which
assigns probability distributions to documents over latent topics  and to latent topics over
terms  these distributions can be viewed as compact representations for documents in a
latent space  since topics discovered by polylingual lda  plda  are aligned across all
languages  documents are represented in a common vector space regardless of the language
they are written in  however  plda  which we will use as a baseline in the experimental
section  requires a parallel collection of documents aligned at the sentence level 
bilingual dictionaries can be used in a straightforward manner to carry out a word byword translation of the feature space  however  dictionary based translations suffer from
several deficiencies  e g   context unaware translations might perform poorly when handling
polysemic words  dictionaries might suffer from a substantial lack of coverage of novel terms
   

fimoreo  esuli    sebastiani

and domain dependent terminology  and dictionaries might not be available for all language
pairs  or not be free to use  as a response to these drawbacks  the automatic acquisition
of statistical bilingual dictionaries has been proposed  wei et al         explored a cooccurrence based method to measure the polylingual statistical strength of the correlation
among words in a parallel corpus  these correlations are then taken into account to reinforce the weight of each feature in order to select the most important  highly weighted  ones 
gliozzo and strapparava        experimented with bilingual dictionaries and  more interestingly  provided a means to automatically obtain a multilingual domain model  mdm  
a natural extension of domain models to multiple languages  when no additional multilingual resources are available  a domain model defines soft relations between words and
domain topics  in the absence of a multilingual dictionary  a mdm could be automatically
obtained from a comparable corpus by performing latent semantic analysis  explained in
more detail below  
it has been argued that words that are shared across languages play an important role
when searching the semantic latent space  accordingly  steinberger  pouliquen  and ignat
       exploit language independent tokens which are shared across the languages  and
propose a simple method to link documents with existing external resources such as thesauri 
nomenclatures  and gazetteers  finally  de melo and siersdorfer        use ontologies to
map original features onto synset like identifiers  so that the documents are translated into
a language independent feature space 
mt tools  on the other side  provide more elaborated translations of texts  and represent a promising research field for multilingual tasks  unfortunately  the above mentioned
problems regarding availability  accessibility  and performance still hold in this case  the
effect of different translation strategies on cltc has been investigated by bel  koster  and
villegas         rigutini  maggini  and liu         and wei  lin  and yang        
even when available  mt tools may be expensive resources  for this reason  in their
experiments prettenhofer and stein        restrict the use of an mt tool to a limited budget
of calls  their structural correspondence learning  scl  method  initially proposed for
domain adaptation  was indeed applied to cltc  the key idea of the method consists of
discovering cross lingual correspondences between pairs of terms  dubbed pivot features 
that are later used to bridge across the two languages  pivot features play an important
role in bilingual tasks  since they establish pairs of words that behave similarly in the source
and target languages  allowing one to find cross language structural correspondences  one
such special type of pivot features are obviously the words shared across languages  such as
proper nouns  technical terms  not yet lexicalized terms  or stemmed forms of etymologically
related terms  nastase and strapparava        found that etymological ancestors of words
do actually add useful information  allowing to transcend cross lingual boundaries  this
method however depends on the availability of etymological thesauri  such as wikipedias
wiktionary  or etymological wordnet   and remains restricted to historically interrelated
languages 
in sum  the applicability of the multilingual methods discussed in this section is usually
constrained by the availability of external resources  with the aim of overcoming these limitations  we will restrict our investigations to dictionary free  mt free multilingual methods 
   

filightweight random indexing for polylingual text classification

    monolingual classifiers and multiview learning
given the availability of a representative set of labelled documents for each language  a
simple baseline  known as the nave polylingual classifier  could be obtained by delegating
the classification process to individual monolingual classifiers  each built upon separate
monolingual data  such a solution is sub optimal  as each classifier does not exploit labelled
information from the other languages  a type of information that might provide insights or
different perspectives on the semantics of the classes 
garca adeva  calvo  and lopez de ipina        compared different nave strategies 
considering one single polylingual classifier  i e   a classifier that works on the juxtaposed
representation   c   vs  various monolingual ones  nc   and one language independent
preprocessor   p  vs  various language specific ones  np   using various learning methods
in a bilingual spanish basque benchmark  in their experimentation the combinations npnc and np  c  which we will consider here as baselines  yielded the best results in terms
of running time  memory usage  and accuracy 
even though training separate language specific classifiers is a simple way to approach
the pltc task  there are some strategies that could improve the final accuracy by better
merging the outcomes of each classifier  multiview learning  xu  tao    xu        for tc
deals with parallel texts  i e   with the case when each document is available in all languages 
where each language is considered as a separate source  it was shown by amini  usunier  and
goutte        that a multiview majority voting algorithm  which returns the label output by
the highest number of language specific classifiers  outperforms both the nave polylingual
classifier and a multiview gibbs classifier  which bases its predictions on the mean prediction
of each language specific classifier  amini and goutte        proposed a co regularization
approach for multiview text classification which minimizes a joint loss function that takes
into account each language specific classifier loss  however  the availability of a parallel
corpus containing all the documents views is a very strong restriction  that is usually
alleviated by leveraging machine translation tools that automatically generate the missing
documents views 
    dimensionality reduction for multilingual classification
one of the main challenges in the juxtaposed vector space approach to pltc concerns
the relevant increase in the number of features that represent the documents  i e   the dimensionality of the vector space  rigutini et al          feature selection methods attempt
to select a reduced subset of informative features from the original set f so that the size of
this subset is much smaller than  f   and so that the reduced set yields high classification
effectiveness  in tc the problem is usually tackled via a filtering approach  which relies
on a mathematical function meant to measure the contribution of each feature to the classification task  yang and pedersen        showed that filtering approaches may improve
the performance of classification  even for aggressive reduction ratios  e g   removal of    
of the features  
another important dimensionality reduction technique is latent semantic analysis
 lsa  aka latent semantic indexing   which originated from the information retrieval
community  deerwester et al          and has been later applied to cross lingual classification  gliozzo   strapparava        xiao   guo        and cross lingual problems in general
   

fimoreo  esuli    sebastiani

 dumais  letsche  littman    landauer         lsa maps the original document term matrix into a lower dimensional latent semantic space that attempts to capture the  linear 
relations among the original features and the documents  this mapping is carried out by
means of a singular value decomposition  svd  of the original document term matrix m  
svd decomposes m as m   v u t   where  is a diagonal matrix containing all the eigenvalues of m   the approximation mk   vk k ukt of the original matrix m can be computed
by taking the k largest eigenvalues of  and setting the remaining ones to    mk is then said
to be rank k optimal in terms of the frobenius norm  vk and uk are orthogonal matrices
that explain the relations among pairs of terms and pairs of documents  respectively 
although lsa can successfully be used to discover hidden relations between indirectly
correlated features  as is the case for terms belonging to different languages  it suffers
from high computational costs  random mappings arise as an alternative to lsa  as
they perform comparably in different machine learning tasks by preserving some important
characteristics of lsa  and by bringing about  at the same time  significant savings in
terms of computational cost  fradkin   madigan         random projections  rps 
papadimitriou et al         and random mappings  rms  kaski        are two equivalent
formulations deriving from the johnson lindenstrauss lemma  johnson et al          which
states that distances in a euclidean space are approximately preserved if projected onto
a lower dimensional random space  these formulations are also based on the fundamental
result of hecht nielsen         who proved that there are many more nearly orthogonal
than truly orthogonal directions in high dimensional spaces 
rp like methods can be formalized in terms of the projection of the original documentterm matrix m by means of a random matrix   i e   m d n   m d  f     f  n   where
t approximates the identity matrix   d  and  f   indicate the number of documents and
terms in the collection  and n stands for the reduced dimensionality  which is typically
chosen in advance  the definition of the random projection matrix  is a fundamental
aspect of the method  achlioptas        demonstrated that any random distribution with
zero mean and unit variance satisfies the johnson lindenstrauss lemma  and proposed two
simple distributions for the definition of the elements ij    ij   of the random projection
matrix  by setting the parameter distribution s of equation   to either s     or s     

 
    with probability  s

  with probability     s
ij  
s
   

 
  with probability  s
achlioptas proved that the configuration in which s     can be used to speed up computation  since in this case only     of the data is non zero  sparse random projection  pand
therefore     of the computations can be skipped  similarly  li et al         set s    f  
and s    f    log  f    very sparse random projections  to significantly speed up the computation while still preserving the inner distances 
random indexing  ri   first proposed by kanerva et al          is an equivalent formulation of rps that also accommodates achlioptas theory  sahlgren        defines ri as an
approximate alternative to lsa for semantic representation  ri maintains a dictionary of
random index vectors for each feature in the original space  each random index vector consists of an n dimensional sparse vector with k non zero values  randomly distributed across
   and    the method is explained in detail in section     in the work of gorman and
   

filightweight random indexing for polylingual text classification

curran        different weighting criteria for random index vectors in the dictionary were
proven useful for improving the matrix representation  ri has been tested in different tasks 
such as search  rangan         query expansion  sahlgren  karlgren  coster    jarvinen 
       image and text compression  bingham   mannila         and event detection  jurgens   stevens         fradkin and madigan        showed that  since in ri distances
are approximately preserved  distance based learners such as k nearest neighbours  k nn 
and svms are preferable when learning from randomly indexed instances  accordingly 
sahlgren and coster        applied ri to  monolingual  text classification using svms 
and suggested that the random indexing representation  there dubbed bag of concepts 
bocs in sahlgren   coster        performed comparably to the bow representation  the
performance of ri has also been tested by sahlgren and karlgren        and gorman and
curran        in the realm of automatic bilingual lexicon acquisition 
the above discussed works indicate that ri is a promising dimensionality reduction technique for representing polylingual data  our proposal is inspired by the works of achlioptas
       and li et al         on sparse projections by taking the level of sparsity to the extreme  and extends the application of ri in tc  sahlgren   coster        to pltc  which 
to the best of our knowledge  has never been done so far  in the following section we will first
describe the method in detail  and then propose a particular setting aimed at overcoming
certain obstacles that could arise in the polylingual setting 

   lightweight random indexing for polylingual text classification
text classification  tc  can be formalized as the task of approximating an unknown target
function    d  c           that indicates how documents ought to be classified  by
means of a function    d  c           called the classifier  such that  and  coincide
as much as possible in terms of a given evaluation metric  here d denotes the domain
of documents  c    c    c         c c    is a set of predefined classes  while values    and  
indicate membership and non membership of the document in the class  respectively  we
will here consider multilabel classification  that is  the setting in which each document
could belong to zero  one  or several classes at the same time  we will consider the flat
version of the problem  in which no hierarchical relations among classes exist  we adopt
the   vs  all strategy  according to which the multilabel classification problem is solved as
 c  independent binary classification problems 
a document collection d can be represented via a matrix m d  f  
    

d 
w  
w      w  f  
 d     w  
w      w  f   
 


m        
  
   
  
        
 
 
  
w d   w d      w d  f  
d  d 

   

where  d  and  f   are the number of documents and features in the collection  and real
values wij represent the weight of feature fj in document di   which is usually determined
as a function of the frequency of the feature in the document and in the collection 
polylingual text classification adds one fundamental aspect to tc  i e   different documents may belong to different languages  let    d  l return the language in which
   

fimoreo  esuli    sebastiani

a given document is written  where l    l    l            l l    is the pool of languages   l      
s l 
let f   i   fi denote the vocabulary of the collection  that can be expressed as the
union of the language specific vocabularies fi   the polylingual setting assumes that the
distribution p   d    li   across the training set is approximately uniform  that is  there is
a representative quantity of labelled documents for each language 
there is usually only a small amount of shared features across languages  e g   proper
nouns     and this implies that hd     d    i    if  d        d      where h  i denotes the dot
product   incidentally  this means that a direct similarity comparison among documents
expressed in different languages  e g   using cosine similarity  would be doomed to fail   it
is thus possible  for any language li   to perform a reordering of the rows and columns in

m  m   
the matrix that allows the polylingual matrix m to be expressed as m  
 
  m  m  
where  m    m
     is
 the   d  d    d    li      fi   monolingual matrix representation for
m 
language li  
is a  d    matrix containing all the  words that are shared across two
m 
or more languages  and   denotes all zero matrices 
    random indexing
random indexing maps each observable problem feature into a random vector in a vector
space in which the number of dimensions is not determined by the number of different unique
features we want to map  but is instead fixed in advance  originally  ri was proposed for
performing semantic comparisons between terms  each document was thus mapped into a
random index vector that was then accumulated  via vector addition  into the terms row
of a term document matrix each time the term occurred in that document  in our case  we
are instead interested in performing semantic comparisons between documents  not terms 
thus  each term fi is assigned an n dimensional random index vector  that is accumulated
into the j th row of a document term matrix every time the term is found in document dj  
random index vectors are nearly orthogonal  and comply with the conditions spelled out
by achlioptas         see section       i e   zero mean distribution with unit variance  so as
to satisfy the johnson lindenstrauss lemma  a random index vector is created by randomly
setting k  n non zero values  equally distributed between    and    in an n dimensional
vector where n is typically on the order of the thousands  once n is fixed  a recommended
choice of k in the literature is k   n      we dub this configuration ri     and will use it
in our comparative experiments  as vectors in ri   are sparse  using sparse data structure
representations could bring about memory savings  the m d n   m d  f     f  n matrix
multiplication  see section      can be completely skipped  building m d n on the fly by
scanning each document and accumulating the corresponding random index vectors as each
term is read  this also avoids the need to allocate the entire matrix m d  f   in memory 
according to sahlgren         the main advantages of ri can be summarized as follows 
the method  i  is incremental  and provides intermediate results before all the data are read
   note that other formulations of the polylingual problem  e g   the ones by amini et al         and
prettenhofer and stein         do actually impose that if i    j then fi  fj     this means that
shared words across languages  such as proper nouns  are given multiple representations as languagespecific features 

   

filightweight random indexing for polylingual text classification

in   ii  avoids the so called huge matrix step  i e   allocating the entire m d  f   matrix
in memory   and  iii  is scalable  since adding new elements to the data does not increase
the dimensionality of the space  e g   new features are represented via a new random index 
and not via a new dimension  
bow matrices are typically weighted and normalized to better represent the importance
of the word to each document and to avoid giving long documents more a priori importance 
respectively  weighting schemes could also be incorporated into the ri formalism in a
simple manner  e g   each time a random index is added to a document row  it can first be
multiplied by the weight of that term in that document  that this brings about improved
accuracy was shown by gorman and curran         however  in the same work it was also
shown that the incremental nature of the algorithm is sacrificed if non linear weights are
taken into account  in our experiments  as the weighting criterion we use the well known
tfidf method  expressed as
tfidf  di   fj     tf  di   fj    log

 d 
 d  d   tf  d  fj       

   

where tf  di   fj   counts the number of occurrences of feature fj in document di   weights are
then normalized via cosine normalization  as
wij   qp

tfidf  di   fj  

fk f

   

tfidf  di   fk   

    lightweight random indexing
during preliminary experiments on the application of ri as a method for dimensionality
reduction  we observed that svms required more time to train when the training set had
been processed with ri  than with the original high dimensional vector space  see section
      we also observed a correlation between training times and the choice of k  while the
choice of n had a smaller impact on efficiency 
optimizing the choice of k in ri can be though of as a means to achieve two main goals 
 i  being able to encode a large number of different features in a reduced space  and  ii 
increasing the chance that two random index vectors are orthogonal 
with respect to  i   it is easy to show that  if we want to assign a different n dimensional
index vector with
 k non zero values to each original feature  ri could encode a maximum
of c n  k    nk  k features  representation capacity   c n  k  grows rapidly as a function
of either n or k  just as an example  c                        such a huge capacity clearly
exceeds the representation requirements imposed by any current or future dataset  however 
even with small values of k the capacity becomes large enough to encode any reasonable
dataset  e g   c                    distinct features 
with respect to  ii   random projection based algorithms rely on the hecht nielsen
       lemma to find nearly orthogonal directions in a reduced space  two vectors  u
and  v inpan inner product space are said to be orthogonal whenever h u   v i      where
h u   v i   i ui vi is the dot product  random indexes are chosen so as to be sparse in order
to increase the probability that the dot product equals zero  with non zero products evenly
distributed between    and    leaving the expected value of the outcome close to zero  by
   

fimoreo  esuli    sebastiani

figure    probability of orthogonality of two random index vectors as a function of k and
n 

means of a monte carlo algorithm  we estimated the probability of orthogonality between
any two randomly generated vectors for a grid of sample values for n and k  the results 
plotted in figure    reveal that smaller values of k are the main factor in favouring the
orthogonality of two random index vectors  while n has a smaller impact 
if many random index vectors lack orthogonality  the information conveyed by the original distinct features  which are predominantly pair wise semantically unrelated  gets mixed
up  causing the learner to have more difficulty in learning meaningful separation patterns
from them  the orthogonality of random index vectors plays an even more important role
for features that are shared across languages  as shown in work by gliozzo and strapparava         these shared words play a relevant role in bringing useful information across
languages  if their corresponding random index vectors are orthogonal with respect to all
the other vectors  the information they contribute to the process is maximized  instead of
being diluted by other less informative features 
following the observations above  we propose the use of random indexing with a fixed
k      we dub this configuration lightweight random indexing  lri   our hypothesis is
that this setting could be advantageous as a mechanism to reduce dimensionality  so as
to mitigate the problem of feature disjointness in pltc   since it is sufficient in order to
represent large feature vocabularies while also preserving vector orthogonality  note that
choosing k      when n    f    would be equivalent to performing a random permutation of
   

filightweight random indexing for polylingual text classification

 

 

 

 

 

 

 
 

output  dictionary 
   generate a random index vector for each feature
for i     to   f       do
   we choose the  st dimension sequentially
dim    i mod n       
   we choose the  nd dimension uniformly at random
   from the dimensions not chosen in line  
dim   rand          n      dim      
   we assign the  st non zero value uniformly at random
   
val   rand   
         
 
   same for the  nd non zero value
   
val   rand   
         
 
   we create the sparse random index vector
random index vector    dim    val      dim    val      
   we build the feature vector mapping
dictionary map fi     random index vector   
end
algorithm    feature dictionary for lightweight random indexing 

feature indexes in a bow representation  k     is the minimum value for which an actual
ri is performed 
algorithm   formalizes the process of creating a dictionary  that is  of creating a mapping
consisting of one random vector for each original feature  the mapping is created at training
time and is then used for classifying the unlabelled documents this means that  in line    f
is the set of features present in the training set   the value      is used instead of   in order
to obtain vectors of length one  note that the two dimensions are selected in a different
manner  with the step at line   ensuring that all latent dimensions are used approximately
the same number of times  and the step at line   ensuring that the dimension chosen in
the previous step is not chosen twice 
our proposal presents the following advantages with respect to standard ri   and  in
general  with respect to any ri with k     
 each index vector has only two non zero values  the mapping can be allocated in
memory for any number of original features  and the projection is performed very
quickly 
 given a fixed value of n  it has a higher probability than any other instantiation of
ri of generating truly pairwise orthogonal random vectors 
 parameter k becomes a constant that needs no tuning 

   

fimoreo  esuli    sebastiani

   experiments
in this section we experimentally compare our lightweight random indexing  lri  method
to other representation approaches proposed in the literature 
    baselines and implementation details
as the baselines against which to compare lri we have chosen the following methods  that
we group in three categories according to their common characteristics 
orthogonal mappings  methods using a canonical basis for the co occurrence matrix 
polybow  a classifier that operates on the juxtaposed bow representation  polybow
corresponds to the np  c setup in garca adeva et al         
fs  feature selection on polybow using information gain as the term scoring function and round robin  forman        as the term selection policy 
majority voting  a multiview voting algorithm that returns the label output by
the highest number of language specific classifiers  amini et al         
monobow  a lower bound baseline that uses a set of nave monolingual classifiers
 monobow corresponds to the np nc setup in garca adeva et al         
mt  an upper bound baseline based on statistical machine translation  which translates all non english training and test documents into english 
random mappings  dimensionality reduction methods relying on random projections 
ri     random indexing with k   n      sahlgren   coster        
ach  achlioptas mapping with ternary distribution obtained by setting s     in
equation    achlioptas        
non random mappings  dimensionality reduction methods relying on mappings which
are not random 
cl lsa  cross lingual latent semantic analysis  dumais et al         
mdm  multilingual domain models  gliozzo   strapparava        
plda  polylingual latent dirichlet allocation  mimno et al         
we will here assume language labels are available in advance  for both training and testing
documents  note that ri methods and polybow represent all documents in the same
feature space  irrespective of their language label  conversely  monobow keeps a separate
language specific classifier for each language  the class label for a test document is then
decided by the classifier associated to the documents language label  we test plda and
majority voting only on the jrc acquis parallel corpus  since for all documents they
require a separate view in all languages to be available  majority voting maintains a
separate classifier for each distinct language    in our experiments   each test document is
thus classified after using   classification decisions in voting  one for each language specific
   this assumption is fair  as current language identification models deliver accuracies very close to     

   

filightweight random indexing for polylingual text classification

view  for singular value decomposition we have used the rohde        package  we have
used the haddow  hoang  bertoldi  bojar  and heafield        implementation to generate
a set of statistical translation systems trained on the sentence aligned parallel data provided
by the europarl data release  koehn         note that  since we used the method described
by gliozzo and strapparava        to automatically obtain the bilingual model in mdm 
mt is the only method using external knowledge  for plda we have used the richardson
       implementation  which uses gibbs sampling  we adhere to the common practice
of fixing the budget of iterations to        we have implemented the lri method and
the other baseline methods as part of the esuli  fagni  and moreo        framework  we
have used support vector machines  svms  as the learning device in all cases  since it has
consistently delivered state of the art results in tc so far  for it we used the well known
joachims        implementation of joachims         with default parameters 
    evaluation measures
as the effectiveness measure we use the well known f    the harmonic mean of precision
   and recall    defined as f                 t p     t p   f p   f n   where t p  
f p   and f n stand for the numbers of true positives  false positives  and false negatives 
respectively  we take f      when t p   f p   f n      since the classifier has correctly
classified all examples as negative 
we compute both micro averaged f   denoted by f    and macro averaged f   denoted
by f m    f  is obtained by  i  computing the class specific values t pr   f pr   and f nr    ii 
obtaining t p as the summation of the t pr s  same for f p and f n    and then applying
the f  formula  f m is obtained by first computing the class specific f  values and then
averaging them across all classes  the fact that f m attributes equal importance to all
classes means that low frequency classes will be as important as high frequency ones in
determining f m scores  f  is instead more influenced by high frequency classes than by
low frequency ones  high values of f m thus tend to indicate that the classifier performs well
also on low prevalence classes  while high values of f  may just indicate that the classifier
performs well on high prevalence classes 
    datasets
we have performed our experiments on two publicly available corpora  rcv  rcv   a
comparable corpus  and jrc acquis  a parallel corpus  
      rcv  rcv 
rcv  is a publicly available collection consisting of the         english news stories generated by reuters from    aug      to    aug       lewis  yang  rose    li         rcv 
is instead a polylingual collection  containing over         news stories generated in the
same timeframe in thirteen languages other than english  dutch  french  german  chinese 
japanese  russian  portuguese  spanish  latinoamerican spanish  italian  danish  norwegian  swedish   the union of rcv  and rcv   hereafter referred to as rcv  rcv   is
a corpus comparable at topic level  as news stories are not direct translations of each other
are but simply refer to the same or to related events in different languages  since the cor   

fimoreo  esuli    sebastiani

pus is not parallel  each training document for a given language in general does not have a
counterpart in the other languages 
from rcv  rcv  we randomly selected       news stories for   languages  english 
italian  spanish  french  german  pertaining to the last   months  from            to
             and we performed a         train test split  thus obtaining a training set
of        documents        for each language  and a test set of        documents       
for each language     in our experiments we have restricted our attention to the    classes
 out of      with at least one positive training example for each of the five languages 
the average number of classes per document is       ranging from a minimum of   to a
maximum of     the number of positive examples per class language combination ranges
from a minimum of   to a maximum of       
we preprocessed the corpus by removing stop words and by stemming terms using
the porter stemmer for english  and the snowball stemmer for the other languages  this
resulted in a total of         stemmed terms  distributed across languages as shown in table
  

english
italian
spanish
french
german

english
      

italian
     
      

spanish
     
     
      

french
     
     
     
      

german
     
     
     
     
      

appearing in
  languages
  languages
  languages
  languages
  languages

 
       
      
     
     
   

table    feature distribution across languages for the rcv  rcv  comparable corpus 
in the leftmost part of the table  the cell in row i and column j represents the
number of features that are shared across the i specific and the j specific sections
of the dataset   the table is symmetric  so for better clarity the entries below the
diagonal have been omitted   the rightmost part of the table indicates how many
features are shared across x language specific sections of the dataset 

      jrc acquis
the jrc acquis corpus  version      is a version of the acquis communautaire collection
of parallel legislative texts from european union law written between the     s and     
 steinberger  pouliquen  widiger  ignat  erjavec  tufis    varga         jrc acquis is
publicly available for research purposes  and covers    official european languages  the
corpus is parallel at the sentence level  i e   each document exists in all    languages  as a
sentence by sentence translation  the corpus is labelled according to the ontology based
eurovoc thesaurus  which consists of more than       classes  for our experiments we have
restricted our attention to the    classes in the top level of the eurovoc hierarchy 
   all the information required to replicate the experiments  e g   ids of the selected documents  assigned
labels  etc   is publicly available  moreo         the source code we used in our experiments is accessible
as part of the esuli et al         framework

   

filightweight random indexing for polylingual text classification

english
italian
spanish
french
german

english
       

italian
      
       

spanish
      
      
       

french
      
      
      
       

german
      
      
      
      
       

appearing in
  languages
  languages
  languages
  languages
  languages

 
       
      
      
      
      

table    feature distribution across languages for the jrc acquis parallel corpus  the
meaning of the cells is the same as in table    note the high number of features          which appear in all five languages  this is due to the presence of
proper names  which are the same in all languages  note also the high number of
features           which are unique to the german language  this is due to the
presence of word compounds  a phenomenon present in the german language but
not in the other four languages 

we have selected the       texts from      for   languages  english  italian  spanish 
french  and german  and removed documents without labels  thus obtaining       documents per language  we have taken the first     documents for training          i e  
      for each language  and the remaining              i e         for each language  for
testing  the average number of classes per document is      ranging from a minimum of
  to a maximum of     the number of positive examples per class language combination
ranges from a minimum of    to a maximum of       
the same preprocessing as for rcv  rcv  was carried out on this dataset  obtaining         distinct features distributed across languages as shown in table    since the
jrc acquis corpus is parallel  each language specific document is guaranteed to have a
counterpart in each of the other languages  which results in a relatively large number of
terms  e g   proper nouns  appearing in several languages  note that  despite the fact that
the dataset is parallel at the sentence level  we are interested in indexing entire documents
as a whole  and thus disregard sentence order  we thus consider the corpus as parallel at
the document level 
we use the jrc acquis corpus in order to test the performance of lri in cases in
which the co occurrence matrix has been compacted  as defined in the work of dumais et al 
        more precisely  the compact representation of  l  translation equivalent documents
is a vector consisting of the concatenation of the  l  vectors that each represent one  monolingual  such document  this is different from the juxtaposed representation used in the
previous chapters  where the vector corresponding to one monolingual document has all
zeros in the positions corresponding to the features of the other languages  the compact
matrix can thus be obtained from the matrix resulting from the juxtaposed representations
by compressing  l  rows into a single  compact  row storing their sum 
   

fimoreo  esuli    sebastiani

    results
in this section we present the results of our experiments  we first compare lri to a set
of monolingual classifiers  section         and then we explore the dimensionality reduction
aspect of the polylingual problem  section        
      polylingual information
as a first case of study  we investigate how much the addition of polylingual information
affects the accuracy of a monolingual classifier  in this scenario  we compare lri and
polybow  which train on documents from all languages  with the lower bound monobow 
which trains only on documents of the same language of test documents  and with the upper
bound mt  that first translates all training and test documents into english  note that
the mt baseline is not tested in the jrc acquis corpus because each of the documents is
already available as a direct translation in all languages  in this experiment the vector space
is not being reduced  i e   we set n    f   for lri so that the vector spaces for polybow
and lri have the same number of dimensions  values for lri were averaged after    runs 
the results illustrated in figure   show that the simple addition of examples in different languages  polybow  brings about an improvement in accuracy with respect to the
monolingual solution  monobow   this improvement is likely achieved thanks to the words
shared across languages  however  lri clearly outperforms polybow  the improvements
of polybow over monobow range from       to         while lri achieves improvements ranging from       to         when lri obtains its smallest improvement over
monobow in terms of f m  on italian          polybow performs slightly worse than
monobow          the improvements are more marked for f m than for f    indicating
that the improvements especially take place in the more infrequent classes  which have a
substantial impact on f m but not on f   
in general  training on documents coming from all languages  polybow  lri  and mt 
seems to be preferable to training from language specific documents only  monobow   this
is particularly so for the mt baseline  which obtained the best results in all cases with the
sole exception of english  where lri obtained the best result  this exception might be
explained by the fact that automatically translated documents tend to exhibit different
statistical properties with respect to documents written by humans  which means that the
english test documents  which are not translations  might not be in tune with the training
documents  which are mostly the result of automatic translation  
the language specific classification performance is much more homogeneous in jrcacquis than in rcv  rcv   this can be explained by the fact that jrc acquis is a parallel
corpus  and therefore each language benefits from the very same information  there is no
significant difference in performance among the different languages  which means that the
effects due to the different difficulty of the various languages are minor  instead  differences
in rcv  rcv  can be explained by the different amount of information that the training
sets carry on the corresponding test sets  for example  the spanish classifier is the worst
performer  and is the one that obtains the best benefit  with respect to the monobow
baseline  from the addition of polylingual information  as in polybow  lri  and mt  
   

filightweight random indexing for polylingual text classification

figure    monolingual classification on rcv  rcv   top  and jrc acquis  bottom   using f m  left  and f   right  as the evaluation measure 

note that in this experiment the matrices that polybow and lri feed to the learning
algorithm are of the same size  the difference between the two methods  which is the likely
cause for their difference in effectiveness  is that in polybow the useful dimensions for a
specific language are packed in a specific portion of the vector space  while lri spreads
them across the entire vector space  causing all dimensions to become potentially useful
for all languages  note that this substantial increase in the number of useful dimensions
available for each language allows the model to create more easily separable representations 
we further discuss this aspect in section     
      dimensionality reduction
in the polybow setup the dimensionality of the vector space is substantially increased
when more languages are considered during training  the following experiments explore
the dimensionality reduction aspect of the problem  and address a realistic polylingual
scenario  where both training and test data contain examples for each language 
   

fimoreo  esuli    sebastiani

n
monobow
polybow
mt
cl lsa
mdm
ach
ri  
lri

   
     
     
     
     
     
     

     
     
     
     
     
     
     

f m
     
     
     
     
     

      
     
     
     
     

full
     
     
     
     

   
     
     
     
     
     
     

     
     
     
     
     
     
     

f 
     
     
     
     
     

      
     
     
     
     

full
     
     
     
     

figure    effects of dimensionality reduction on rcv  rcv   english and italian   dotted
lines indicate reference values  e g   green and red lines represent the performance
of lri and polybow  respectively  when dimensionality is not reduced  values
in bold highlights the best performing method for each dimension 

we first run a sample bilingual experiment on rcv  rcv   as the language other
than english we have picked italian   the total amount of features in this dataset is
        restricting the experiment to two languages allows us to compare lri  i  against
methods that were proposed for bilingual representations  mdm   and  ii  against methods
that would be too computationally expensive if considering more languages  such as ach 
see below   we explore the effect of dimensionality reduction  with the number of selected
features ranging from     to         figure     we adhere to the common practice that
establishes a number of dimensions ranging from     to      in lsa and mdm  results
for random projection methods  ach  ri     and lri  are averaged after    runs 
lri obtains good results on both macro  and micro averaged f    while the other methods exhibit alternating performance on the two measures  ri   obtains comparable results
in terms of f m but performs poorly on f    in contrast  polybow performs comparably
in terms of f  but worse in terms of f m   a two tailed t test on paired examples reveals
that the difference in terms of f m between lri and ri   is not statistically significant 
   

filightweight random indexing for polylingual text classification

n
monobow
polybow
mt
cl lsa
ri  
lri

   
     
     
     
     

     
     
     
     
     

f m
     
     
     
     

      
     
     
     

full
     
     
     
     

   
     
     
     
     

     
     
     
     
     

f 
     
     
     
     

      
     
     
     

full
     
     
     
     

figure    accuracy of different pltc methods on rcv  rcv  on   languages  for different
levels of dimensionality reduction 

and that lri significantly outperforms ri   in f  and the rest of dimensionality reduction
methods for both evaluation measures  with p          surprisingly  cl lsa and mdm
perform worse than the nave classifier  monobow  with all features  however  it should
be remarked that they outperform all other baselines with only     and      dimensions 
as will be seen in section    apart from the drastic dimensionality reduction  these methods are affected by large computational costs that negatively impact on the run times and
memory resources needed  consistently with our previous observations  see figure     lri 
polybow  monobow  and mt are comparable in terms of f    but lri outperforms all
tested algorithms in terms of f m  
to test the scalability of our method when several languages are involved  we extend the
experiment to five languages  english  italian  spanish  french  german  in rcv  rcv 
 figure     note that in this case not all algorithms were able to complete their execution
due to memory constraints  hence the incomplete plots and table  concretely  ach and the
last iterations for ri   overflowed memory resources when trying to allocate a         
         matrix  more insights about space and time complexity are reported in section   
results for ri   and lri are the average of    runs that use different random seeds 
these results confirm the previous observations  ri   behaves similarly to lri in terms
of f m  i e   with no statistically significant difference  but worse in terms of f   p         
   

fimoreo  esuli    sebastiani

n
polybow
cl lsa
plda
majority vote
ri  
lri

   
     
     
     
     
     

     
     
     
     
     
     

f m
     
     
     
     

      
     
     
     

full
     
     
     

   
     
     
     
     
     

     
     
     
     
     
     

f 
     
     
     
     

      
     
     
     

full
     
     
     

figure    accuracy of different pltc methods on jrc acquis on   languages  for different
levels of dimensionality reduction 

while polybow behaves in an opposite way  i e   performs worse than lri in terms of
f m  p          and comparably in terms of f    as a dimensionality reduction method 
lri thus outperforms the other methods when considering both f m and f    when no
dimensionality reduction is applied  only the upper bound mt is comparable to lri in
both f m and f   
finally  we used jrc acquis to reproduce one last polylingual scenario  namely  one
in which texts are aligned at the document level  even if this situation is not common in
practice  exceptions include  say  proceedings of official events   this scenario is interesting
since such a dataset may serve as a test bed for multiview learning methods  amini et al  
       since documents in jrc acquis were translated by humans  results are not affected
by any noise mt tools might introduce  figure   shows the results obtained considering the
compacted matrix of jrc acquis  a                  matrix   on which we also tested majority voting  which combines the classification decisions of the five independently trained
monobow classifiers on the parallel versions of the documents  and plda  that first defines
the generative model based on polylingual topics and then trains and tests on the probability distributions over topics assigned to each document  we set the number of polylingual
latent topics to     and       respectively 
lri is clearly superior to polybow in this case  the difference in performance between
lri and ri   seems to be lower in this case  especially in terms of f m   the t test revealed
   

filightweight random indexing for polylingual text classification

however that lri is superior to ri   in a statistically significant sense  p          however 
it should be considered that lri delivers its best performance without reducing the dimensionality of the polylingual matrix  while ri   is not able to accomplish the projection due
to memory restrictions  this is something we will expand on in the following section  plda 
in turn  succeeded in discovering polylingual topics that were aligned across languages  but
proved less effective in terms of classification performance 

   analysis
during our experiments we observed substantial differences in terms of efficiency among
some of the compared methods  particularly ach  ri     and lri  for example  ri  
exhausted memory resources for n           while lri was able to represent even the fullsized  d  f   matrix  see figure     given the strong relationship between the two methods 
we would have expected they delivered similar performance  this anomaly prompted us to
investigate the issue more in depth  this section presents an analytical study in terms of
efficiency of the methods discussed in the previous section 
    space efficiency
data samples in ml are usually represented as a co occurrence matrix  in tc this matrix
suffers from high dimensionality  but luckily enough it is also sparse  a sparse  low density
matrix suggests the use of a non exhaustive data structure  in which zero values are not
stored explicitly 
the random projection has a direct impact on sparsity  for each feature contained in a
document  k non zero values are placed in the projected matrix  for ach the situation is
worse  since each feature is mapped  on average  into n   non zero values  as an example 
for n          each feature will be mapped into    and       non zero values in ri   and
in ach  respectively 
as an example  we have rerun our rcv  rcv  experiments with english and italian
as the only languages  and examined their matrix density  percentage of non zero values
over the total matrix size  and memory footprint  absolute number of non zero values   the
results are displayed in figure   
lri requires double the space with respect to standard bow  but succeeds in preserving
sparsity  while ri   drastically increases the matrix density and produces a large memory
footprint  mdm  lsa  and ach operate on dense matrices  however  since both mdm and
lsa produce an extreme dimensionality reduction  the overall memory footprint remains
much lower than that of ri   and  especially  of ach  when n    f    lri must allocate
about           values  this is indicated as lri  full  in figure     while ri    n        
must allocate about              values  requiring       times more space   ach  n        
must allocate              values        times more space   note that even though mdm
and lsa reduce significantly the dimensionality  e g   from        to      or         they
need to allocate more values in memory than lri  full  
as an example  let us suppose that each non zero value is represented as a double
 typically    bytes in most modern programming languages   this means we roughly need
   mb for ach and    mb for ri     whereas lri requires only   mb  although the
difference is substantial   even taking into account that the actual memory needed is higher
   

fimoreo  esuli    sebastiani

figure    matrix density  left  and memory footprint  right  in the rcv  rcv  englishitalian run                   full training matrix size  

if the values are indexed in a hash table  they still do not represent any real problem in
terms of space for most modern computers  however  note that the matrix is not the only
data structure we need to allocate in memory  also the mapping dictionary  i e   the data
structure linking each original feature to its random index vector  should be allocated in
memory  the dictionary will be queried as many times as there are terms in any document
we want to classify  if the dictionary is small enough  which it is in lri   we may be able
to allocate it in cache in order to significantly speed up the indexing of new documents 
assuming a sparse representation  a random index vector can be described as a list of k
pairs  di   vi    where di indicates a latent dimension and vi encodes its value  for example 
for k     the random vector                          could be represented as                  
where a bit set to   encodes    and a bit set to   encodes    as from equation   
the space occupation for the dictionary of a random indexing method depends on  i   f   
the number of indexes   ii  k  the number of non zero values for each index  and  iii  the
number of bits needed to indicate one latent position and to encode all possible non trivial
values  that is 
cost rik     o  f    k  log  n   log     

   

it turns out that  given that the expected number of non zero values for ach is n    using
a dense representation for each index is cheaper  each position thus indicates one of the
three possible values for the index  the cost in terms of space of the ach index dictionary
is described by
cost ach    o  f    n  log    
   

   

filightweight random indexing for polylingual text classification

method
lri
ri  
ach

index type
sparse
sparse
dense

index size
 
   
      

index cell
log  n   log    bits to encode dimi and vali   resp 
log  n   log    bits to encode dimi and vali   resp 
log    bits to encode ij

memory required
    mb
     mb
      mb

table    memory occupation for the feature dictionary for different random mapping methods on the jrc acquis dataset   f                the meanings of dimi and vali
are as in algorithm    the meaning of ij is as in equation   

assuming the reduced dimensionality is set to a fixed percentage of the original dimensionality  i e   n    f   with          the following hold 
cost ri    o  f    log   f     
cost ach    o  f       

   

cost lri    o  f   log   f   
however  the hidden constants play a key role in practice  as an example  we have computed
the total amount of memory required for each method for storing the index dictionaries for
n           in jrc acquis  where  f               the resulting values are reported in table
   as it can be observed  for the index dictionary ach requires    mb  while the space
required for the ri based versions is one to three orders of magnitude smaller  in other
words  the index dictionary for lri could easily fit in current cache memories  while ri  
and ach need to resort to higher capacity  and thus slower  storage devices 
    time efficiency
it is usually the case that sparsity benefits not only space occupation  but also execution
time  as an example  the computational cost of svd is o  f     d   for a document by term
matrix  however  the implementation svdlibc is specifically optimized for sparse matrices
and requires o c f   d   steps  where c is the average number of non zero values in a vector 
in figure   we plot run times for the experiments on the bilingual  english italian 
rcv  rcv  experiment by paying attention to the time required for  i  obtaining the
transformed index for the training set   ii  training the learning algorithm  svm    iii 
obtaining the transformed index for the test set  and  iv  classifying the test documents 
all the experiments were run on an intel i    bit processor with    cores  running at
     mhz  and   gbs ram memory 
the results show that it takes about     minutes to generate and test the classifier
that uses the bow representation  time is slightly reduced to about   minutes when only
     features are selected  the total time for lri is roughly higher by a factor of    up
to      full  and      n         minutes  respectively  notwithstanding this  these figures
are still low when compared to the other methods  both training and testing times grow
very substantially for ri and ach  regarding latent methods  it should be pointed out
that the time required for preparing the matrices also grow substantially  due to the large
   

fimoreo  esuli    sebastiani

figure    run times on rcv  rcv   english and italian setting  

computational cost inherent in svd and matrix multiplication  while in the case of random
indexing methods these times are negligible 
by comparing the overall memory footprint  figure    right  with execution times  figure    it seems clear that there is a strong correlation between them  we have investigated
this dependency in our experiments by computing the pearson correlation between them 
the pearson correlation quantifies the degree of linear dependence between two variables 
and ranges from    meaning perfect negative correlation  to     meaning perfect positive
correlation  whereas   means that there is not any linear dependency  we found a linear
pearson correlation of        and        between the number of non zero values in the
matrix and times required for training and testing  respectively  which brings additional
support to our observation  preserving sparsity during the projection favours execution
times in pltc 
    the effect of k in random indexing
previous work in ri  see  e g   sahlgren   karlgren        sahlgren   coster        tend to
set k to about    of the dimension of the vector  smaller values of k  about k         have
also been explored  karlgren  holst    sahlgren         other works related to random
projections  see  e g   achlioptas        li et al         have noticed that sparse projection
matrices help to speed up computation 
besides run times  sparsity in the projection matrix also affects the orthogonality of
the random projection  which in turn has an impact on the preservation of the relative
distances  two random vectors ri and rj are said to be orthogonal if the angle between
them is    degrees  although the probability that any two randomly picked vectors are
orthogonal increases as the dimensionality of the vector space grows  karlgren et al         
most random projection approaches choose sparse random vectors  so as to maximize this
probability 
   

filightweight random indexing for polylingual text classification

figure    probability distribution of the angle between any two arbitrary vectors in highdimensional space  left   and excess kurtosis as a function of the non zero values
in a projection matrix of        dimensions  right  

we could thus establish a parallelism between the degree of orthogonality of any projection matrix and the probability distribution of the angle of any two of its random vectors 
the more this probability distribution is skewed towards    degrees  the closer to orthogonal
the projection base is  and the better distances are preserved  we propose to quantify the
orthogonality by means of the excess kurtosis of the distribution of this angle    to this aim 
we have studied how the kurtosis of the angle distributions  as estimated via a monte carlo
algorithm  varies as a function of the matrix sparsity k for any        dimension projection
matrix  figure    right  
figure   shows that the orthogonality of the projection  for a fixed dimensionality 
rapidly degrades as the density increases  lri is thus expected to produce the most nearly
orthogonal indexing  followed by ri and then by ach 
we have further investigated the relation between orthogonality and pltc accuracy 
to this aim  we have run a series of experiments on the bilingual version of rcv  rcv  
varying  from   to      the number k of non zero values and  from       to         the
reduced dimensionality n  figure   shows the contour lines  equally valued points in the
  dimensional representation  for classification performance  here measured in terms of
f     execution time  and probability of pairwise orthogonality  i e   the probability that
hri   rj i     for any two randomly chosen random index vectors  
the following trends can be directly observed from the results   i  accuracy improves
as n increases and k decreases   ii  run times tend to grow when both n and k increase 
and  iii  the higher the dimensionality n and the smaller the parameter k  the higher the
probability of finding two orthogonal random indexes 
   the excess kurtosis of a random variable x is typically defined as its fourth standardized moment minus
   i e   ekurt x          

   

fimoreo  esuli    sebastiani

figure    impact of dimensionality n  on the x axis  and number k of non zero values  on the
y axis  on classification accuracy  left   execution time  center   and probability
of finding an orthogonal pair of random indexes  right   darker regions represent
lower values 

in figure    the behaviour of the lri method we propose is described by the green
horizontal line at the bottom of each plot  while ris behaviour is described by the blue
diagonal line from coordinates  n           k       to  n            k         the performance in ri improves at the cost of space and time efficiency  and by gradually disrupting
the orthogonality of the base  on the contrary  the following desirable features of lri are
evident  when dimensionality increases  i  accuracy improves without penalizing execution
times  due to the preservation of sparsity  and  ii  the orthogonality of the base is improved 

   conclusions
we have compared several techniques for polylingual text classification  checking their suitability as dimensionality reduction techniques and as techniques for the generation of alternative representations for the co occurrence matrix  on two pltc benchmarks  one parallel
and one comparable   our investigation indicates that reducing the dimensionality of the
data is not sufficient if reasonable efficiency  in terms of both time and space  is required 
based on this observation we have proposed a variant of random indexing  a method originated within the ir community that  to the best of our knowledge  was never tested in
pltc up to date  our proposal  lightweight random indexing  yielded the best results not
only in terms of  both time and space  efficiency  but also in terms of classification accuracy 
for which lightweight random indexing obtained the best results both in terms of macroand micro averaged f    lightweight random indexing preserves matrix sparsity  which
means that both memory footprint and training time are not penalized  for example  from
figures   and   we may see that lightweight random indexing  in the full configuration
 that is  where the random vectors have the same dimensionality of the original space 
improved over latent semantic analysis  in the n          configuration  that is  where
   

filightweight random indexing for polylingual text classification

the dimensionality of the reduced space is        by a margin of        in terms of f 
with an        reduction in execution time and an        reduction in memory footprint 
even though lightweight random indexing works very well as a dimensionality reduction method  it achieves its best performance when the projection does not reduce
the original dimensionality  apparently  the bow representation might be expected to be
preferable in such a case  because it is truly orthogonal  however  in the polylingual bow
representation most of the features are only informative for a restricted set of the data  e g  
a german term has an entire dimension reserved for it in the vector space model  and this
dimension is useful only for documents written in german  random projections instead
map the feature space into a space that is shared among all languages at once  the effect
is that any dimension of the space becomes informative to represent documents regardless
of the language they were originally written in  this configuration  in which the projection
space is larger than the actual number of different features for a single language  is reminiscent of the kernel trick effect  because the informative space for each language is enlarged
and thus becomes more easily separable 
in the light of our experiments  lightweight random indexing has important advantages
with respect to previous pltc approaches  first  the method is machine translationfree  dictionary free  and does not require any sort of additional resources apart from the
labelled collection  the projected matrix preserves sparsity  which has a direct effect in
reducing both running time and total memory usage  with respect to the original random
indexing technique  lightweight random indexing presents the following advantages   i  the
probability of finding a pair of truly orthogonal indexes is higher   ii  it requires less memory
to allocate the index dictionary  and  iii  it avoids the need for tuning the k parameter 
lri has proven to be very effective in pltc  and we conjecture it could bring similar
benefits in other related tasks  such as cltc  cross lingual information retrieval  as well as
when tackling problems dealing with sparse and heterogeneous sources of data in general  as
discussed above  one of the reasons why k     is a safe configuration is that it still preserves
the representation capacity  however  this might not hold under all circumstances  e g  
when processing huge streams of very dynamic data  e g   streams of tweets   at a certain
point the representation capacity might saturate if the dimensionality of the space has not
been chosen carefully  in these cases  opting for configurations with k     might mitigate
the problem 
another fact that emerges from our experiments is that dimensionality reduction is not
necessarily a synonym of computational efficiency  the reason is that modern secondary
storage data structures are optimized to operate on sparse data  and when the dimensionality is drastically reduced  matrix density may increase  and the net effect may be a decrease
in efficiency  a true benefit is thus achieved to the extent that the trade off between sparsity
and separability is preserved  on this dimension  lri proved extremely effective 
although results are encouraging  further investigations are still needed to shed some
light on the foundations of random projection methods  a first question is whether there is
any criterion to better choose the random index vectors  given that the current criterion is
random  it seems there might be room for better motivated strategies  possibly by leveraging
class labels or by taking into account the document language labels  considering that
random indexing was originally proposed in the context of the ir community  we wonder
whether the proposed approach could produce similar improvements on ir tasks such as
   

fimoreo  esuli    sebastiani

query expansion or bilingual lexicon acquisition  finally  it could be interesting to combine
lightweight random indexing with reflexive random indexing  cohen  schvaneveldt   
widdows        rangan         a more recent formulation of the model that iteratively
alternates between row indexing and column indexing in the original co occurrence matrix 

acknowledgements
fabrizio sebastiani is on leave from consiglio nazionale delle ricerche  italy 

references
achlioptas  d          database friendly random projections  in proceedings of the   th
acm symposium on principles of database systems  pods        pp         
santa barbara  us 
al rfou  r   perozzi  b     skiena  s          polyglot  distributed word representations for
multilingual nlp  in proceedings of the   th conference on computational natural
language learning  conll        pp          sofia  bl 
amini  m  r     goutte  c          a co classification approach to learning from multilingual corpora  machine learning                   
amini  m  r   usunier  n     goutte  c          learning from multiple partially observed
views  an application to multilingual text categorization  in proceedings of the   rd
annual conference on neural information processing systems  nips        pp    
    vancouver  ca 
baroni  m   dinu  g     kruszewski  g          dont count  predict  a systematic comparison of context counting vs  context predicting semantic vectors  in proceedings
of the   nd annual meeting of the association for computational linguistics  acl
       pp          baltomore  us 
bel  n   koster  c  h     villegas  m          cross lingual text categorization  in proceedings of the  th european conference on research and advanced technology for
digital libraries  ecdl        pp          trondheim  no 
bengio  y          learning deep architectures for ai  foundations and trends in machine
learning              
bengio  y   schwenk  h   senecal  j  s   morin  f     gauvain  j  l          neural probabilistic language models  in innovations in machine learning  pp          springer 
heidelberg  de 
bingham  e     mannila  h          random projection in dimensionality reduction  applications to image and text data  in proceedings of the  th acm international
conference on knowledge discovery and data mining  kdd        pp          san
francisco  us 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal of
machine learning research             
   

filightweight random indexing for polylingual text classification

bullinaria  j  a     levy  j  p          extracting semantic representations from word
co occurrence statistics  a computational study  behavior research methods         
       
chandar  s   lauly  s   larochelle  h   khapra  m  m   ravindran  b   raykar  v  c     saha 
a          an autoencoder approach to learning bilingual word representations  in
proceedings of the   th annual conference on neural information processing systems
 nips        pp            montreal  ca 
cohen  t   schvaneveldt  r     widdows  d          reflective random indexing and indirect inference  a scalable method for discovery of implicit connections  journal of
biomedical informatics                 
collobert  r   weston  j   bottou  l   karlen  m   kavukcuoglu  k     kuksa  p         
natural language processing  almost  from scratch  journal of machine learning
research               
de melo  g     siersdorfer  s          multilingual text classification using ontologies  in
proceedings of the   th european conference on information retrieval  ecir       
pp          roma  it 
deerwester  s  c   dumais  s  t   furnas  g  w   landauer  t  k     harshman  r  a 
        indexing by latent semantic analysis  journal of the american society for
information science                 
dumais  s  t   letsche  t  a   littman  m  l     landauer  t  k          automatic crosslanguage retrieval using latent semantic indexing  in working notes of the aaai
spring symposium on cross language text and speech retrieval  pp        stanford 
us 
ehrmann  m   cecconi  f   vannella  d   mccrae  j  p   cimiano  p     navigli  r         
representing multilingual data as linked data  the case of babelnet      in proceedings of the  th conference on language resources and evaluation  lrec        pp 
        reykjavik  is 
esuli  a   fagni  t     moreo  a          jatecs  java text categorization system   in
github  retrieved september           from https   github com jatecs jatecs 
faruqui  m     dyer  c          improving vector space word representations using multilingual correlation  in proceedings of the   th conference of the european chapter of the
association for computational linguistics  eacl        pp          gothenburg 
se 
forman  g          a pitfall and solution in multi class feature selection for text classification  in proceedings of the   st international conference on machine learning
 icml        pp        banff  ca 
fradkin  d     madigan  d          experiments with random projections for machine
learning  in proceedings of the  th acm international conference on knowledge
discovery and data mining  kdd        pp          washington  us 
garca adeva  j  j   calvo  r  a     lopez de ipina  d          multilingual approaches to
text categorisation  european journal for the informatics professional              
   

fimoreo  esuli    sebastiani

gliozzo  a     strapparava  c          cross language text categorization by acquiring
multilingual domain models from comparable corpora  in proceedings of the acl
workshop on building and using parallel texts  pp       ann arbor  us 
gliozzo  a     strapparava  c          exploiting comparable corpora and bilingual dictionaries for cross language text categorization  in proceedings of the   th annual
meeting of the association for computational linguistics  acl        pp         
sydney  au 
gorman  j     curran  j  r          random indexing using statistical weight functions 
in proceedings of the  th conference on empirical methods in natural language processing  emnlp        pp          sydney  au 
gouws  s     sgaard  a          simple task specific bilingual word embeddings  in
proceedings of the north american chapter of the association for computational
linguistics and human language technologies conference  naacl hlt        pp 
         
haddow  b   hoang  h   bertoldi  n   bojar  o     heafield  k          moses statistical
machine translation system  in moses website  retrieved september           from
http   www statmt org moses  
harris  z  s          mathematical structures of language  wiley  new york  us 
hecht nielsen  r          context vectors  general purpose approximate meaning representations self organized from raw data  in computational intelligence  imitating life 
pp        ieee press 
hermann  k  m     blunsom  p          multilingual models for compositional distributed
semantics  in proceedings of the   nd annual meeting of the association for computational linguistics  acl        pp        baltimore  us 
joachims  t          svmperf  support vector machine for multivariate performance
measures  in cornell university website  retrieved september           from
http   www cs cornell edu people tj svm light svm perf html 
joachims  t          a support vector method for multivariate performance measures  in
proceedings of the   nd international conference on machine learning  icml       
pp          bonn  de 
johnson  w  b   lindenstrauss  j     schechtman  g          extensions of lipschitz maps
into banach spaces  israel journal of mathematics                 
jurgens  d     stevens  k          event detection in blogs using temporal random indexing 
in proceedings of the workshop on events in emerging text types  pp       borovets 
bg 
kanerva  p   kristofersson  j     holst  a          random indexing of text samples for latent semantic analysis  in proceedings of the   nd annual conference of the cognitive
science society  cogsci        p        philadelphia  us 
karlgren  j   holst  a     sahlgren  m          filaments of meaning in word space  in
proceedings of the   th european conference on information retrieval  ecir       
pp          glasgow  uk 
   

filightweight random indexing for polylingual text classification

kaski  s          dimensionality reduction by random mapping  fast similarity computation
for clustering  in proceedings of the ieee international joint conference on neural
networks  ijcnn        pp          anchorage  us 
klementiev  a   titov  i     bhattarai  b          inducing crosslingual distributed representations of words  in proceedings of the   th international conference on computational linguistics  coling        pp            mumbai  in 
koehn  p          europarl  a parallel corpus for statistical machine translation  in mt
summit  vol     pp        publicly available in http   www statmt org europarl  
lauly  s   boulanger  a     larochelle  h          learning multilingual word representations using a bag of words autoencoder  arxiv e prints  arxiv            cs cl  
lewis  d  d   yang  y   rose  t  g     li  f          rcv   a new benchmark collection for
text categorization research  journal of machine learning research     apr          
publicly available in http   www jmlr org papers volume  lewis  a lyrl     
rcv v  readme htm 
li  p   hastie  t  j     church  k  w          very sparse random projections  in proceedings
of the   th acm sigkdd international conference on knowledge discovery and
data mining  kdd        pp          philadelphia  us 
li  y     shawe taylor  j          advanced learning algorithms for cross language patent
retrieval and classification  information processing and management              
     
mikolov  t   le  q  v     sutskever  i       a   exploiting similarities among languages
for machine translation  arxiv e prints  arxiv            cs cl  
mikolov  t   sutskever  i   chen  k   corrado  g  s     dean  j       b   distributed
representations of words and phrases and their compositionality  in proceedings of the
  th annual conference on neural information processing systems  nips        pp 
          lake tahoe  us 
mimno  d   wallach  h  m   naradowsky  j   smith  d  a     mccallum  a          polylingual topic models  in proceedings of the      conference on empirical methods in
natural language processing  emnlp        pp          singapore  sn 
moreo  a          data resources for reproducing experiments in polylingual text classification  in human language technologies  hlt  group website  retrieved september
          from http   hlt isti cnr it pltc 
nastase  v     strapparava  c          bridging languages through etymology  the case of
cross language text categorization  in proceedings of the   st annual meeting of the
association for computational linguistics  acl        pp          sofia  bl 
osterlund  a   odling  d     sahlgren  m          factorization of latent variables in distributional semantic models  in proceedings of the conference on empirical methods
in natural language processing  emnlp        pp          lisbon  pt 
pan  s  j     yang  q          a survey on transfer learning  ieee transactions on
knowledge and data engineering                    
   

fimoreo  esuli    sebastiani

papadimitriou  c  h   raghavan  p   tamaki  h     vempala  s          latent semantic
indexing  a probabilistic analysis  in proceedings of the   th acm symposium on
principles of database systems  pods        pp          seattle  us 
pennington  j   socher  r     manning  c  d          glove  global vectors for word
representation  in proceedings of the conference on empirical methods on natural
language processing  emnlp        pp            doha  qa 
prettenhofer  p     stein  b          cross language text classification using structural
correspondence learning  in proceedings of the   th annual meeting of the association
for computational linguistics  acl        pp            uppsala  se 
rangan  v          discovery of related terms in a corpus using reflective random indexing  in proceedings of the icail      workshop on setting standards for searching
electronically stored information  pittsburgh  us 
richardson  j          polylda    in atlassian bitbucket  retrieved september          
from https   bitbucket org trickytoforget polylda 
rigutini  l   maggini  m     liu  b          an em based training algorithm for crosslanguage text categorization  in proceedings of the  rd ieee wic acm international conference on web intelligence  wi        pp          compiegne  fr 
rohde  d          a c library for computing singular value decompositions  in svdlibc 
retrieved september           from http   tedlab mit edu  dr svdlibc  
sahlgren  m          vector based semantic analysis  representing word meanings based
on random labels  in proceedings of the esslli workshop on semantic knowledge
acquistion and categorization  helsinki  fi 
sahlgren  m          an introduction to random indexing  in proceedings of the workshop
on methods and applications of semantic indexing  copenhagen  dk 
sahlgren  m          the word space model  using distributional analysis to represent syntagmatic and paradigmatic relations between words in high dimensional vector spaces 
ph d  thesis  swedish institute for computer science  university of stockholm  stockholm  se 
sahlgren  m     coster  r          using bag of concepts to improve the performance of
support vector machines in text categorization  in proceedings of the   th international conference on computational linguistics  coling        geneva  ch 
sahlgren  m     karlgren  j          automatic bilingual lexicon acquisition using random
indexing of parallel corpora  natural language engineering                 
sahlgren  m   karlgren  j   coster  r     jarvinen  t          sics at clef       automatic query expansion using random indexing  in working notes of the crosslanguage evaluation forum workshop  clef        pp          roma  it 
steinberger  r   pouliquen  b     ignat  c          exploiting multilingual nomenclatures
and language independent text features as an interlingua for cross lingual text analysis
applications  in proceedings of the  th slovenian language technology conference 
ljubljana  sl 
   

filightweight random indexing for polylingual text classification

steinberger  r   pouliquen  b   widiger  a   ignat  c   erjavec  t   tufis  d     varga 
d          the jrc acquis  a multilingual aligned parallel corpus with     languages  in proceedings of the  th international conference on language resources
and evaluation  lrec        pp            genova  it  publicly available in
https   ec europa eu jrc en language technologies jrc acquis 
vinokourov  a   shawe taylor  j     cristianini  n          inferring a semantic representation of text via cross language correlation analysis  in proceedings of the   th annual
conference on neural information processing systems  nips        pp           
vancouver  ca 
vulic  i     moens  m  f          monolingual and cross lingual information retrieval models
based on  bilingual  word embeddings  in proceedings of the   th international acm
sigir conference on research and development in information retrieval  sigir
       pp          santiago  cl 
wei  c  p   lin  y  t     yang  c  c          cross lingual text categorization  conquering
language boundaries in globalized environments  information processing and management                 
wei  c  p   yang  c  s   lee  c  h   shi  h     yang  c  c          exploiting poly lingual
documents for improving text categorization effectiveness  decision support systems 
         
xiao  m     guo  y          a novel two step method for cross language representation
learning  in proceedings of the   th annual conference on neural information processing systems  nips        pp            lake tahoe  us 
xu  c   tao  d     xu  c          a survey on multi view learning  arxiv e prints 
arxiv            cs lg  
yang  y     pedersen  j  o          a comparative study on feature selection in text categorization  in proceedings of the   th international conference on machine learning
 icml        pp          nashville  us 
zou  w  y   socher  r   cer  d  m     manning  c  d          bilingual word embeddings
for phrase based machine translation  in proceedings of the conference on empirical
methods on natural language processing  emnlp        pp            melbourne 
au 

   

fi