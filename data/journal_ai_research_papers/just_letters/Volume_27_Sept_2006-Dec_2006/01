journal artificial intelligence research                

submitted        published     

generative prior knowledge discriminative classification
arkady epshteyn
gerald dejong

aepshtey uiuc edu
dejong uiuc edu

department computer science
university illinois urbana champaign
    n  goodwin
urbana  il        usa

abstract
present novel framework integrating prior knowledge discriminative classifiers  framework allows discriminative classifiers support vector machines
 svms  utilize prior knowledge specified generative setting  dual objective
fitting data respecting prior knowledge formulated bilevel program 
solved  approximately  via iterative application second order cone programming 
test approach  consider problem using wordnet  a semantic database
english language  improve low sample classification accuracy newsgroup categorization  wordnet viewed approximate  readily available source background
knowledge  framework capable utilizing flexible way 

   introduction
svm  vapnik        classification accuracy many classification tasks often
competitive human subjects  number training examples required
achieve accuracy prohibitively large domains  intelligent user interfaces 
example  must adopt behavior individual user limited amount
interaction order useful  medical systems diagnosing rare diseases generalize
well seeing examples  natural language processing task performs
processing level n grams phrases  which frequent translation systems 
cannot expect see sequence words sufficient number times even large
training corpora  moreover  supervised classification methods rely manually labeled
data  expensive obtain  thus  important improve classification
performance small datasets  classifiers competitive humans
ability generalize seeing examples  various techniques
proposed address problem  active learning  tong   koller      b  campbell 
cristianini    smola         hybrid generative discriminative classification  raina  shen 
ng    mccallum         learning to learn extracting common information related
learning tasks  thrun        baxter        fink         using prior knowledge 
work  concentrate improving small sample classification accuracy
prior knowledge  prior knowledge proven useful classification  scholkopf 
simard  vapnik    smola        wu   srihari        fung  mangasarian    shavlik       
epshteyn   dejong        sun   dejong         notoriously hard apply practice
mismatch form prior knowledge employed
classification algorithms  either prior probabilities explicit constraints hypothesis
c
    
ai access foundation  rights reserved 

fiepshteyn   dejong

space classifier  domain theories articulated human experts 
unfortunate various ontologies domain theories available abundance 
considerable amount manual effort required incorporate existing prior knowledge
native learning bias chosen algorithm  would take apply
existing domain theory automatically classification task specifically
designed  work  take first steps towards answering question 
experiments  domain theory exemplified wordnet  linguistic
database semantic connections among english words  miller         apply wordnet standard benchmark task newsgroup categorization  conceptually  generative
model describes world works  discriminative model inextricably linked
specific classification task  thus  reason believe generative interpretation
domain theory would seem natural generalize better across different
classification tasks  section   present empirical evidence is  indeed 
case wordnet context newsgroup classification  reason  interpret
domain theory generative setting  however  many successful learning algorithms
 such support vector machines  discriminative  present framework allows
use generative prior discriminative classification setting 
algorithm assumes generative distribution data given
bayesian framework  p rob data model  prior p rob   model  known  however 
instead performing bayesian model averaging  assume single model
selected a priori  observed data manifestation model  i e  
drawn according p rob data m     goal learning algorithm estimate
  estimation performed two player sequential game full information 
bottom  generative  player chooses bayes optimal discriminator function f  m  
probability distribution p rob data model      without taking training data
account  given model   model chosen top  discriminative  player
way prior probability occurring  given p rob   m    high  forces
bottom player minimize training set error bayes optimal discriminator
f  m    estimation procedure gives rise bilevel program  show that 
problem known np hard  approximation solved efficiently iterative
application second order cone programming 
remaining issue construct generative prior p rob   model  automatically domain theory  describe solve problem section   
argue generative setting appropriate capturing expert knowledge  employing wordnet illustrative example  section    give necessary
preliminary information important known facts definitions  framework incorporating generative prior discriminative classification described detail section
   demonstrate efficacy approach experimentally presenting results
using wordnet newsgroup classification section    theoretical explanation
improved generalization ability discriminative classifier constrained generative
prior knowledge appears section    section   describes related work  section   concludes
paper outlines directions future research 
  

figenerative prior knowledge discriminative classification

   generative vs  discriminative interpretation domain knowledge
wordnet viewed network  nodes representing words links representing
relationships two words  such synonyms  hypernyms  is a   meronyms  partof   etc    important property wordnet semantic distance   length
 in links  shortest path two words  semantic distance approximately
captures degree semantic relatedness two words  set experiment
evaluate usefulness wordnet task newsgroup categorization  posting
represented bag of words  binary feature representing presence
corresponding word  evaluation done pairwise classification tasks
following two settings 
   generative framework assumes posting x    x        xn   generated
distinct probability distribution newsgroup  simplest version
linear discriminan analysis  lda  classifier posits x  y      n       i 
x  y      n      i  posting x given label         r nn 
identity matrix  classification done assigning probable label
x  y x      p rob x      p rob x      well known  e g  see duda  hart   
stork        decision rule equivalent one given hyperplane
c       
cn   estimated via
      t x     t    t           means bi    


 
maximum likelihood training data  x    y          xm   ym    

   discriminative svm classifier sets separating hyperplane directly minimize
number errors training data 
c        w
cn    bb    arg minw b kwk s t  yi  wt xi   b              m 
 w
b    w

experiment conducted learning to learn framework  thrun        baxter 
      fink         first stage  classifier trained using training data
training task  e g   classifying postings newsgroups atheism guns  
second stage  classifier generalized using wordnets semantic information 
third stage  generalized classifier applied different  test task  e g   classifying
postings newsgroups atheism vs  mideast  without seeing data new
classification task  way classifier generalize setting use
original sample acquire information wordnet  exploit information
help label examples test sample  learning perform task 
system learns utilize classification knowledge implicit wordnet 
describe second third stages two classifiers detail 

   intuitive interpret information embedded wordnet follows  title
newsgroup guns  words semantic distance
gun  e g   artillery  shooter  ordnance distance two  provide
similar degree classification information  quantify intuition  let li train  
j
 
n
 li train
      li train
      li train
  vector semantic distances wordnet
feature word j label training task newsgroup         define
   standard lda classifier assumes x  y      n        x  y      n       
estimates covariance matrix well means       training data  experiments 
take   i 

  

fiepshteyn   dejong

  train  atheism vs  guns
  train  atheism vs  guns
  train  guns vs  mideast
test  atheism vs  mideast
test  guns vs  mideast
test  atheism vs  mideast
 

 

   

 

   

   

   

   

   

   

   

   

   

   

   

   

   

   
 

                                        

 

                                        

 

                                        

legend 

generative
discriminative

figure      test set accuracy percentage versus number training points  
different classification experiments  classification task  random test
set chosen full set articles    different ways  error bars
based     confidence intervals 

p

 v   

c
j
j
 v
i train
j
 v 
 j li train
j l

              denotes cardinality set  compresses

information bi based assumption words equidistant newsgroup
label equally likely appear posting newsgroup  test
performance compressed classifier new task semantic distances given
j
   notice
li test   generative distributions reconstructed via ji     li test
classifier trained tested task  applying function
equivalent averaging components means generative distribution
corresponding equivalence classes words equidistant label 
classifier tested different classification task  reconstruction process reassigns
averages based semantic distances new labels 
   less intuitive interpret wordnet discriminative setting  one possible
interpretation coefficients w j separating hyperplane governed
semantic distances labels  captured compression function    v  u   
p

cj

w
j
j
 u
 v l
  train
  train
j
j
 j l  train
 v l  train
 u 
j l

j
j
reconstructed via w j       l  test
  l  test
  

note lda generative classifier svm discriminative classifier
hypothesis space separating hyperplanes  resulting test set classification
accuracy classifier classification tasks    newsgroup dataset
  

figenerative prior knowledge discriminative classification

 blake   merz        presented figure      x axis graph represents
size training task sample  y axis   classifiers performance test
classification task  generative classifier consistently outperforms discriminative
classifier  converges much faster  two three tasks discriminative classifier
able use prior knowledge nearly effectively generative classifier even
seeing     available training data  generative classifier
consistent performance   note error bars much smaller
discriminative classifier  results clearly show potential using background
knowledge vehicle sharing information tasks  effective sharing
contingent appropriate task decomposition  supplied tuned generative
model 
evidence figure     seemingly contradicts conventional wisdom discriminative training outperforms generative sufficiently large training samples  however 
experiment evaluates two frameworks context using ontology transfer
information learning tasks  never done before  experiment demonstrates interpretation semantic distance wordnet intuitive
generative classification setting  probably better reflects human intuitions
behind wordnet 
however  goal construct classifier performs well without seeing
examples test classification task  want classifier improves
behavior sees new labeled data test classification task  presents us
problem  one best performing classifiers  and certainly best text
classification task according study joachims        svm  discriminative
classifier  therefore  rest work  focus incorporating generative prior
knowledge discriminative classification framework support vector machines 

   preliminaries
observed constraints probability measure half space
captured second order cone constraints gaussian distributions  see  e g   tutorial
lobo  vandenberghe  boyd    lebret         allows efficient processing
constraints within framework second order cone programming  socp   intend
model prior knowledge elliptical distributions  family probability distributions
generalizes gaussians  follows  give brief overview second order
cone programming relationship constraints imposed gaussian probability
distribution  note possible extend argument presented lobo et
al         elliptical distributions 
second order cone program mathematical program form 
min v x

     

x

s t  kai x   bi k cti x   di             n

     

x rn optimization variable v rn   ai r ki xn    bi rki   ci rn  
di r problem parameters  kk represents usual l   norm paper   socps
solved efficiently interior point methods  described lobo et al        
tutorial contains excellent overview theory applications socp 
  

fiepshteyn   dejong

use elliptical distribution model distribution data a priori  elliptical
distributions distributions ellipsoidally shaped equiprobable contours  density
function n variate elliptical distribution form f  g  x    c det    g  x
 t    x     x rn random variable  rn location parameter 
r nxn  positive definite  n n  matrix representing scale parameter  function
g   density generator  c normalizing constant  use notation x e     g  denote random variable x elliptical distribution
parameters     g  choosing appropriate density generator functions g  gaussian
distribution  student t distribution  cauchy distribution  laplace distribution 
logistic distribution seen special cases elliptical distribution  using elliptical distribution relaxes restrictive assumptions user make
imposing gaussian prior  keeping many desirable properties gaussians  as 
   x e     g   r kn    b rk   ax   b e a   b  aat   g 
   x e     g   e x     
   x e     g   v ar x    g   g constant depends
density generator g 
following proposition shows elliptical distributions  constraint p  w x b
    i e   probability x takes values half space  w x   b    less
  equivalent second order cone constraint     
proposition
     x e     g   p rob w x   b       equivalent  w  
   
b  g  w   g  constant depends g  
proof  proof identical one given lobo        lanckriet et al        
gaussian distributions provided completeness 
assume p rob w x   b     

     

let u   w x b  let u denote mean u  denote variance  constraint
    written
uu
u
     
p rob     





properties elliptical distributions  u   w   b    g     w   uu




 
e       g   thus  statement     expressed p robxe     g   x w  b
    w
k
gk

  equivalent w  b
       z    p robxe     g   x z  
    w
k
gk

proposition follows g    g      

proposition      monotonically decreasing g  p robxe   g   x  equivalent


     x   g c    g c     g       
c   constant depends
g  c     
proof  follows directly definition p robxe   g   x  
  

figenerative prior knowledge discriminative classification

   generative prior via bilevel programming
deal binary classification task  classifier function f  x  maps
instances x rn labels         generative setting  probability densities
p rob x y          p rob x y          parameterized            provided  or
estimated data   along prior probabilities class labels  y     
 y       bayes optimal decision rule given classifier
f  x     sign p rob x y          y      p rob x y          y       
sign x       x     otherwise  lda  instance  parameters  
  means two gaussian distributions generating data given label 
informally  approach incorporating prior knowledge straightforward  assume
two level hierarchical generative probability distribution model  low level probability
distribution data given label p rob x y    parameterized   which  turn 
known probability distribution p rob      goal classifier estimate
values parameter vector training set labeled points  x     y       xm   ym   
estimation performed two player sequential game full information 
bottom  generative  player  given   selects bayes optimal decision rule f  x   
top  discriminative  player selects value high probability occurring
 according p rob      force bottom player select decision rule
minimizes discriminative error training set  give formal
specification training problem formulate bilevel program 
assumptions subsequently relaxed enforce tractability flexibility 
use elliptical distribution e         g  model x y      another elliptical
distribution e         g  model x y      parameters            known 
bayes optimal decision rule restricted class linear classifiers   form
fw b  x    sign w x   b  given f  x  minimizes probability error among
linear discriminants  p rob error    p rob w x   b   y      y        p rob w x   b
  y      y            p robxe       g   wt x   b      p robxe       g   wt x   b     
assuming equal prior probabilities classes  model uncertainty
means elliptical distributions          imposing elliptical prior distributions
locations means  e ti     g           addition  ensure optimization
problem well defined  maximize margin hyperplane subject imposed
generative probability constraints 
min kwk

     

    

s t yi  wt xi   b             

p robi e ti  i  g   i           

     
     




 w  b  solves min p robxe       g   w x   b      p robxe       g   w x   b    
w b

     
bilevel mathematical program  i e   optimization problem
constraint region implicitly defined another optimization problem   strongly
   decision rule restricted class classifiers h optimal probability error larger
classifier h  tong   koller      a  

  

fiepshteyn   dejong

np hard even constraints objectives linear  hansen  jaumard 
  savard         however  show possible solve reasonable approximation problem efficiently several iterations second order cone programming 
first  relax second level minimization       breaking two constraints 
p robxe       g   wt x   b    p robxe       g   wt x   b      thus  instead looking bayes optimal decision boundary  algorithm looks decision
boundary low probability error  low error quantified choice  
propositions         enable us rewrite optimization problem resulting
relaxation follows  
min kwk

     

      w b

s t yi  wt xi   b             




   
 i ti           
p robi e ti  i  g   i           

w     b

p robxe       g   wt x   b   
   
  w

w     b

p robxe       g   wt x   b   
   
  w

     
     
     

     

notice form program depend generator function g
elliptical distribution   constants depend it  defines far system
willing deviate prior choice generative model  bounds
tail probabilities error  type type ii  system tolerate assuming
chosen generative model correct  constants depend specific generator
g amount error user willing tolerate  experiments  select
values constants optimize performance  unless user wants control
probability bounds constants  sufficient assume a priori
probability distributions  both prior hyper prior  elliptical  without making
commitments 
algorithm solves problem repeating following two steps 
   fix top level optimization parameters       step combines objectives maximizing margin classifier training data ensuring
decision boundary  approximately  bayes optimal respect given
generative probability densities specified        
   fix bottom level optimization parameters w  b  expand feasible region
program step   function         step fixes decision boundary
pushes means generative distribution far away boundary
constraint       allow 
steps repeated convergence  in practice  convergence detected
optimization parameters change appreciably one iteration next  
step algorithm formulated second order cone program 
  

figenerative prior knowledge discriminative classification

step    fix       removing unnecessary constraints mathematical
program pushing objective constraints  get following socp 
min

      

w b

s t  kwk

      

yi  wt xi   b             

      

wt

  b


   
  w

      

w     b


   
  w

      

step    fix w  b expand span feasible region  measured

w    b  
   
  w


w    b
   
  w



removing unnecessary constraints  get 
w     b w     b


max
   
        
  w
  w



   
 i ti           
s t 

      
      

behavior algorithm illustrated figure     
following theorems state algorithm converges 
theorem
     suppose
n
algorithm produces sequence iterates


 t   t 
 t 
 t 
  quality iterate evaluated margin w t   
        w   b
t  
evaluation function converges 
 t 

 t 

 t 

 t 

proof  let       values prior location parameters  w    b 
minimum error hyperplane algorithm finds end t th step  end
 t     t   
 t      st step  w 
  b 
still feasible region t th step socp 
 t   t

 t 
   b
     t 
w

 

true function f    w

 t   t

 t 
   b
     t 
w

 

   w

  

 w t   t    b t 
     t 
  w



 w t   t    b t 
     t 
  w

monotonically increasing one arguments argument fixed 
 t     t   
fixing    or     fixes exactly one argument  solution  
   
end
 t   

 t      st step
 t   

fixing  

 t 


 t 
 w t 
 b

   
     t 
  w

    f could increased

 t 

using value   beginning step ensures

 t   t
 t 
 w
   b

     t 
  w

  contradicts observation f maximized end
 t   

second step  contradiction reached


 t 
 w t 
 b

   
     t 
  w

    since

minimum error hyperplane previous
iteration feasible region start

 t 


must decrease monotonically one iteration
next iteration  objective w
next  since bounded zero  algorithm converges 
  

fiepshteyn   dejong

  

  

 

 

 

 

 

 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

 
 

 

 

 

 

  
 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 
 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

figure      steps iterative  hard margin  socp procedure 
 the region hyperprior probability larger shaded prior
distribution  covariance matrices represented equiprobable elliptical contours 
example  covariance matrices hyperprior prior distributions
multiples other  data points two different classes represented diamonds
squares  
   data  prior  hyperprior algorithm executed 
   hyperplane discriminator end step    iteration  
   priors end step    iteration  
   hyperplane discriminator end step    iteration  
algorithm converges end step   problem  step   move
hyperplane  
addition convergence objective function  accumulation points
sequence iterates characterized following theorem 
n

 t   t 
theorem      accumulation points sequence         w t    b t   i e   limiting
points convergent subsequences  feasible descent directions original
optimization problem given             
proof  see appendix a 
  

figenerative prior knowledge discriminative classification

point feasible descent directions  sufficiently small step along
directional vector either increase objective function  leave unchanged  take
algorithm outside feasible region  set points feasible descent directions
subset set local minima  hence  convergence point somewhat
weaker result convergence local minimum 
practice  observed rapid convergence usually within     iterations 
finally  may want relax strict assumptions correctness prior linear
separability data introducing slack variables optimization problem above 
results following program 
min

      w b i            

kwk   c 


x

  c             c          

      

i  

s t yi  wt xi   b              


   

 i ti             


w     b
   
  w  




w     b
   
  w  

           
         

         

      
      
      
      
      
      
      

before  problem solved two step iterative socp procedure 
imposing generative prior soft constraints ensures that  amount training
data increases  data overwhelms prior algorithm converges maximummargin separating hyperplane 

   experiments
experiments designed demonstrate usefulness proposed approach
incorporation generative prior discriminative classification  address
broader question showing possible use existing domain theory aid
classification task specifically designed  order construct
generative prior  generative lda classifier trained data training
classification task estimate gaussian location parameters bi           described
section    compression function  v  subsequently computed  also described
j
section     used set hyperprior parameters via ji     li test
          
order apply domain theory effectively task specifically
designed  algorithm must able estimate confidence decomposition
domain theory respect new learning task  order model uncertainty
applicability wordnet newsgroup categorization  system estimated confidence
homogeneity equivalence classes semantic distances computing variance
  

fiepshteyn   dejong

    
bilevel gen discr

   
    
   
    
   
    
   
   

    

   

        
svm

    

   

    

figure      performance bilevel discriminative classifier constrained generative
prior knowledge versus performance svm  point represents unique
pair training test tasks       test task data used training 
results averaged     experiments 

p

random variable  v  follows   v   

j l

c
 ji  v   
j
i train v
j
 j li tran
 v 

  hyperprior confidence

matrices          reconstructed
respect test task semantic distances

j
 li test
   k   j
  identity matrices used
li test          follows   i  j k   
   k    j
covariance matrices lower level prior           i  rest parameters
set follows                   c    c        c       constants
chosen manually optimize performance experiment    for training task  atheism
vs  guns  test task  guns vs  mideast  see figure      without observing data
classification tasks 
resulting classifier evaluated different experimental setups  with different
pairs newsgroups chosen training test tasks  justify following
claims 
   bilevel generative discriminative classifier wordnet derived prior knowledge good low sample performance  showing feasibility automatically
interpreting knowledge embedded wordnet efficacy proposed
algorithm 
   bilevel classifiers performance improves increasing training sample size 
   integrating generative prior discriminative classification framework results
better performance integrating prior directly generative
framework via bayes rule 
  

figenerative prior knowledge discriminative classification

   bilevel classifier outperforms state of the art discriminative multitask classifier
proposed evgeniou pontil        taking advantage wordnet domain
theory 
order evaluate low sample performance proposed classifier  four newsgroups
   newsgroup dataset selected experiments  atheism  guns  middle east 
auto  using categories  thirty experimental setups created possible
ways assigning newsgroups training test tasks  with pair newsgroups assigned
task  constraint training test pairs cannot identical     
experiment  compared following two classifiers 
   bilevel generative discriminative classifier knowledge transfer functions
 v    v          learned labeled training data provided training task  using     available data task   resulting prior
subsequently introduced discriminative classification framework via approximate bilevel programming approach
   vanilla svm classifier minimizes regularized empirical risk 
min

w b i


x
i  

  c  kwk 

s t yi  wt xi   b              

     
     

classifiers trained      available data test classification
task    evaluated remaining       test task data  results  averaged
one hundred randomly selected datasets  presented figure      shows
plot accuracy bilevel generative discriminative classifier versus accuracy
svm classifier  evaluated thirty experimental setups  points
lie   o line  indicating improvement performance due incorporation prior
knowledge via bilevel programming framework  amount improvement ranges
         improvements statistically significant   
level 
next experiment conducted evaluate effect increasing training data
 from test task  performance system  experiment  selected
three newsgroups  atheism  guns  middle east  generated six experimental setups
based possible ways splitting newsgroups unique training test pairs 
addition classifiers     above  following classifiers evaluated 
   state of the art multi task classifier designed evgeniou pontil        
classifier learns set related classification functions ft  x    wtt x   bt classification tasks  training task  test task  given m t  data points  x t   y t         xm t t   ym t t  
   newsgroup articles preprocessed removing words could interpreted nouns
wordnet  preprocessing ensured one part wordnet domain theory exercised
resulted virtually reduction classification accuracy 
   sedumi software  sturm        used solve iterative socp programs 

  

fiepshteyn   dejong

task minimizing regularized empirical risk 
min

w   wt  bt  it

x
x m t 


i  

 

c  x
kwt w  k    c  kw  k 
c 

s t  yit  wtt xit   bt                m t  
            m t  

     
     
     

regularization constraint captures tradeoff final models w close
average model w  large margin training data     
training task data made available classifier  constant c       chosen 
c          selected set                                       optimize
classifiers performance experiment    for training task  atheism vs  guns 
test task  guns vs  mideast  see figure      observing      test task data
 in addition training task data  
   lda classifier described section   trained     test task data  since
classifier bottom level generative classifier used bilevel
algorithm  performance gives upper bound performance bottomlevel classifier trained generative fashion 
figure     shows performance classifiers     function size training
data test task  evaluation done remaining test task data   results
averaged one hundred randomly selected datasets  performance bilevel
classifier improves increasing training data discriminative portion
classifier aims minimize training error generative prior imposed
soft constraints  expected  performance curves classifiers converge
amount available training data increases  even though constants used mathematical program selected single experimental setup  classifiers performance
reasonable wide range data sets across different experimental setups 
possible exception experiment    training task  guns vs  mideast  testing task  atheism
vs  mideast   means constructed elliptical priors much closer
experiments  thus  prior imposed greater confidence
warranted  adversely affecting classifiers performance 
multi task classifier   outperforms vanilla svm generalizing data points
across classification tasks  however  take advantage prior knowledge 
classifier does  gain performance bilevel generative discriminative classifier
due fact relationship classification tasks captured much
better wordnet simple linear averaging weight vectors 
constants involved bilevel classifier generative classifiers bayesian priors  hard fair comparison classifiers constrained
generative priors two frameworks  instead  generatively trained classifier  
gives empirical upper bound performance achievable bottom level classifier
trained generatively test task data  accuracy classifier shown
horizontal plots figure      since discriminative classification known
superior generative classification problem  svm classifier outperforms
  

figenerative prior knowledge discriminative classification

   train atheism vs  guns

   train atheism vs  guns

   train guns vs  mideast

test atheism vs  mideast

test guns vs  mideast

test atheism vs  guns

 

 

 

    

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   

   

    

    
 

 

                              

   train  guns vs  mideast

  

  

  

  

  

  

  

  

  

   train  atheism vs  mideast

    
 

 

 

 

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   
 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

test atheism vs  guns

    

    

  

   train  atheism vs  mideast

test guns vs  mideast

test atheism vs  mideast

  

   

    

    
 

                              

 

  

  

  

  

  

  

  

  

  

legend 

lda max performance
bilevel gen discr
svm
multitask svm

figure      test set accuracy percentage versus number test task training points
two classifiers  svm bilevel gen discr  tested six different classification
tasks  classification experiment  data set split randomly
training test sets     different ways  error bars based    
confidence intervals 

generative classifier given enough data four six experimental setups 
interesting  that  range training sample sizes  bilevel classifier constrained
generative prior outperforms svm trained sample
generative classifier trained much larger sample four setups  means that 
unless prior knowledge outweighs effect learning  cannot enable lda classifier
compete bilevel classifier problems 
finally  set experiments performed determine effect varying mathematical program parameters generalization error  parameter
varied set values  rest parameters held fixed   increased
maximum feasible value   evaluation done setup experiment    for
  

fiepshteyn   dejong

   accuracy function

   accuracy function

    

    

    

    

    

    

    

    

   

   

    

    

    

    

    

    

    

    

 

 
 

    

   

    

   

 

                                   

 

figure      plots test set accuracy percentage versus mathematical program parameter
values  classification task  random training set size   chosen
full set test task articles     different ways  error bars based
    confidence intervals  experiments performed training
task  atheism vs  guns  test task  guns vs  mideast 

training task atheism vs  guns  test task  guns vs  mideast   training set size
  points  results presented figure      increasing value equivalent
requiring hyperplane separator smaller error given prior  decreasing
value equivalent increasing confidence hyperprior  actions
tighten constraints  i e   decrease feasible region   good prior knowledge 
effect improving generalization performance small training samples
since prior imposed higher confidence  precisely observe
plots figure     

   generalization performance
algorithm generalize well low sample sizes  section  derive
theorem demonstrates convergence rate generalization error
constrained generative discriminative classifier depends parameters mathematical program margin  would expected case large margin
classification without prior  particular  show certainty generative prior knowledge increases  upper bound generalization error classifier
constrained prior decreases  increasing certainty prior  mean
either hyper prior becomes peaked  i e   confidence locations
prior means increases  desired upper bounds type type ii probabilities
error classifier decrease  i e   requirement lower level discriminative
player choose restricted bayes optimal hyperplane strictly enforced  
argument proceeds bounding fat shattering dimension classifier constrained prior knowledge  fat shattering dimension large margin classifier
given following definition  taylor   bartlett        
definition      set points    x     xm    shattered set functions f
mapping domain x r real numbers r          rm that 
b       m   function fb f b fb  xi   ri          m  say
  

figenerative prior knowledge discriminative classification

r          rm witness shattering  fat shattering dimension f function
fatf    maps cardinality largest  shattered set s 
specifically  consider class functions
f    x w x   kxk r  kwk     

     





wt     
w  
   


   
 




 
 


 
 


 



 



 
 
 
    
 
 
   
   
  w
  w

following theorem bounds fat shattering dimension classifier 

theorem      let f class a priori constrained functions defined       
let min  p   max  p   denote minimum maximum eigenvalues matrix p  
 
 
    
 
respectively  set points  shattered f    s   r     
 
 

 

    
    
  k  max      
  kt ktk 
 
  max             min  min
      min  min
 
k  k
k  k
max        kt  k 
kt  k   max       
  
kt  k  max         kt  k 

assuming    kti k kti k 

   
 

       

proof  see appendix b 
following corollary follows directly taylor bartletts       
theorem     bounds classifiers generalization error based fat shattering
dimension 
corollary      let g class real valued functions  then  probability least
  independently generated examples z  classifier h   sgn g  sgn g 
 
margin least examples z  error h
 d

 m
 log   m 
 
log 
  


 
f

 
  

g
 
f


class

functions
log   em
g
g


  
 

 

 

defined        df    r                g   f   usual class large margin
classifiers  without prior   result  taylor   bartlett        shows f  
   r 
 
 
 

notice bounds depend r
  however  bound classifier constrained
 
generative prior depends term               particular  increases  tightening constraints  bound decreases  ensuring  expected 
quicker convergence generalization error  similarly  decreasing tightens
constraints decreases upper bound generalization error        
factor             less   upper bound fat shattering dimension df
tighter usual bound no prior case df    
since controls amount deviation decision boundary bayesoptimal hyperplane depends variance hyper prior distribution  tightening
constraints corresponds increasing confidence prior  note high
value represents high level user confidence generative elliptical model 
note two ways increasing tightness hyperprior constraint      
  one user defined parameter   automatically
estimated covariance matrices           matrices estimate extent
  

fiepshteyn   dejong

equivalence classes defined wordnet create appropriate decomposition domain
theory newsgroup categorization task  thus  tight constraint       represents
high level user confidence means generative classification model  estimated
wordnet  good correspondence partition words imposed
semantic distance wordnet elliptical generative model data 
approaches zero approaches highest feasible value  solution bilevel
mathematical program reduces restricted bayes optimal decision boundary computed
solely generative prior distributions  without using data 
hence  shown that  prior imposed increasing level confidence
 which means elliptical generative model deemed good  estimates
means good  turn implies domain theory well suited
classification task hand   convergence rate generalization error classifier
increases  intuitively  precisely desired effect increased confidence prior
since benefit derived training data outweighed benefit derived
prior knowledge  low data samples  result improved accuracy assuming
domain theory good  plots figure     show 

   related work
number approaches combining generative discriminative models  several focus deriving discriminative classifiers generative distributions  tong
  koller      a  tipping        learning parameters generative classifiers via
discriminative training methods  greiner   zhou        roos  wettig  grunwald  myllymaki    tirri         closest spirit approach maximum entropy
discrimination framework  jebara        jaakkola  meila    jebara         performs
discriminative estimation parameters generative model  taking account constraints fitting data respecting prior  one important difference
framework that  estimating parameters  maximum entropy discrimination minimizes distance generative model prior  subject satisfying
discriminative constraint training data classified correctly given margin 
framework  hand  maximizes margin training data subject
constraint generative model far prior  emphasis
maximizing margin allows us derive a priori bounds generalization error
classifier based confidence prior  yet  available maximum entropy framework  another difference approach performs classification
via single generative model  maximum entropy discrimination averages set
generative models weighted probabilities  similar distinction
maximum a posteriori bayesian estimation repercussions tractability  maximum entropy discrimination  however  general framework sense
allowing richer set behaviors based different priors 
ng et al               explore relative advantages discriminative generative
classification propose hybrid approach improves classification accuracy
low sample high sample scenarios  collins        proposes use viterbi
algorithm hmms inferencing  which based generative assumptions   combined
discriminative learning algorithm hmm parameter estimation  research
  

figenerative prior knowledge discriminative classification

directions orthogonal work since explicitly consider question
integration prior knowledge learning problem 
context support vector classification  various forms prior knowledge
explored  scholkopf et al         demonstrate integrate prior knowledge
invariance transformations importance local structure kernel function 
fung et al         use domain knowledge form labeled polyhedral sets augment
training data  wu srihari        allow domain experts specify confidence
examples label  varying effect example separating hyperplane
proportionately confidence  epshteyn dejong        explore effects rotational constraints normal separating hyperplane  sun dejong       
propose algorithm uses domain knowledge  such wordnet  identify relevant
features examples incorporate resulting information form soft constraints
hypothesis space svm classifier  mangasarian et al         suggest use prior
knowledge support vector regression  approaches  prior knowledge takes
form explicit constraints hypothesis space large margin classifier 
work  emphasis generating constraints automatically domain knowledge
interpreted generative setting  demonstrate wordnet application 
generative interpretation background knowledge intuitive natural language
processing problems 
second order cone constraints applied extensively model probability constraints robust convex optimization  lobo et al         bhattacharyya  pannagadatta   
smola        constraints distribution data minimax machines  lanckriet
et al         huang  king  lyu    chan         work  far know  first one
models prior knowledge constraints  resulting optimization problem
connection bayes optimal classification different approaches
mentioned above 
work related empirical bayes estimation  carlin   louis         empirical bayes estimation  hyper prior parameters generative model estimated
using statistical estimation methods  usually maximum likelihood method moments 
marginal distribution data  approach learns parameters
discriminatively using training data 

   conclusions future work 
since many sources domain knowledge  such wordnet  readily available  believe
significant benefit achieved developing algorithms automatically applying
information new classification problems  paper  argued generative paradigm interpreting background knowledge preferable discriminative
interpretation  presented novel algorithm enables discriminative classifiers
utilize generative prior knowledge  algorithm evaluated context complete system which  faced newsgroup classification task  able estimate
parameters needed construct generative prior domain theory  use
construction achieve improved performance new newsgroup classification tasks 
work  restricted hypothesis class linear classifiers  extending
form prior distribution distributions elliptical and or looking
  

fiepshteyn   dejong

bayes optimal classifiers restricted expressive class linear separators
may result improvement classification accuracy non linearly separable domains 
however  obvious approximate expressive form prior knowledge
convex constraints  kernel trick may helpful handling nonlinear problems 
assuming possible represent optimization problem exclusively terms
dot products data points constraints  important issue requires
study 
demonstrated interpreting domain theory generative setting
intuitive produces good empirical results  however  usually multiple ways
interpreting domain theory  wordnet  instance  semantic distance
words one measure information contained domain theory  other 
complicated  interpretations might  example  take account types links
path words  hypernyms  synonyms  meronyms  etc   exploit commonsense observations wordnet words closer category label
likely informative words farther away  comparing multiple ways
constructing generative prior domain theory and  ultimately  selecting one
interpretations automatically fruitful direction research 

acknowledgments
authors thank anonymous reviewers valuable suggestions improving paper  material based upon work supported part national science foundation
award nsf iis          part information processing technology office defense advanced research projects agency award hr               
opinions  findings  conclusions recommendations expressed publication
authors necessarily reflect views national science
foundation defense advanced research projects agency 

appendix a  convergence generative discriminative algorithm
let map h   z z determine algorithm that  given point       generates se
quence  t  t   iterates iteration  t      h  t     iterative algorithm
 t 

 t 

section   generates sequence iterates  t             z applying following
map h 
h   h   h   
 a   
step    h               arg

min

 w b u          

kwk  

set u            defined constraints 


 a   
 a   

yi  w xi   b               

 a   

c   w  b           


wt  b
 
conic constraints cs  w  b       
k    wk

 a   

c   w  b           

  

 a   

figenerative prior knowledge discriminative classification

step    h   w  b    arg

min

       v

 c   w  b            c   w  b          

 a   

set v given constraints
o         t     

 a   

o         t     

 a   



o     t          t   
notice h  h  functions minima optimization problems
                            unique  case step   optimizes
strictly convex function convex set  step   optimizes linear non constant function
strictly convex set 
convergence objective function   t      min w b u    t    t     kwk algorithm
 
 
shown theorem      let denote set points map h
change value objective function  i e   h           
show every accumulation point   t    lies   show every point
         augmented  w   b     h             point feasible descent
directions optimization problem              equivalently expressed as 
min kwk s t          v    w  b  u           

      w b

 a    

order formally state result  need concepts duality theory 
let constrained optimization problem given
min f  x  s t  ci  x              k
x

 a    

following conditions  known karush kuhn tucker kkt  conditions necessary
x local minimum 
proposition a    x local minimum  a              k
p
   f  x     ki   ci  x  
             k 

   ci  x             k 
   ci  x               k 
        k known lagrange multipliers constraints c        ck  
following well known result states kkt conditions sufficient x
point feasible descent directions 
proposition a            k following conditions satisfied x  
p
   f  x     ki   ci  x  
  

fiepshteyn   dejong

             k 
x feasible descent directions problem  a    
proof   sketch  reproduce proof given textbook fletcher         proposition true p
feasible direction vector s  st ci  x    x


        k   hence  f  x     ki   st ci  x      descent direction 
following lemma characterizes points set  

lemma a    let   let  w   b     h      optimizer      let
    a             a    m    a       a      set lagrange multipliers corresponding
constraints solution  w   b    define     h     let  w     b    optimizer
                 a                     a         
                   a       a         
proof  consider case
       

 a    

      

 a    


since   kw  k   kw k  let   set lagrange multipliers corresponding
constraints solution  w     b     since w still feasible optimization problem
given       by argument theorem      minimum problem
unique  happen
 w    b       w   b   
 a    
 w   b     must satisfy kkt conditions        a     implies
c   w              c   w           argument theorem      means
that  kkt condition          
  a        

 a    

therefore  kkt condition           a       w  b             w   w    b  
b                
 
 
 
 
 
 



c   w b        
c   w b        
kwk
x

x


w
    a    c   ww
 
  a    i
    a    c   ww
 
 b     
kwk
 b       
 
 
yi
b

b

i  

b

means kkt conditions         optimization problem     satisfied
 
point  w   b       kkt condition     satisfied feasibility  w   b  
kkt condition     satisfied condition      observations  a     
 a       a     
proofs two cases                                      
analogous 
following theorem states points kkt points  i e   points
kkt conditions satisfied  optimization problem given  a     
  

figenerative prior knowledge discriminative classification

theorem a    let  w   b     h        w   b           kkt point
optimization problem given  a     
proof  let     h     lemma a    consider case
         

 a    

        a         by lemma a    

 a    

 the proofs two cases similar  
kkt conditions h   w   b          


 o         t  
c   w   b          
   a  
 a     
 
 

 a    

kkt conditions h       a       w  b     w   b  
 

kwk
w
kwk
b

 

 


x
i  

 a    i



yi x
yi



 

 a   

 

c   w b        
w
c   w  b       
b

 








 a     
  
 a    m
 a   





  


 a    

 a      a      a       a       w  b             w   b               


c   w b       
kwk


kwk
 

x
w


w
kwk
w
 b     


c
 w
 
 
x
 
yi
b  



 

 
kwk  
b

 


 a   
 a    i  
 

c   w  b        


 
i  
 
kwk
 
 
 
 








c   w b        
 
 
w

c   w  b       




 
 


 
 
b
 a    a   
   a   
 o 
 
 t  
   
 a   




 


 
 
 o       t  
c   w  b        
 
 

 

means kkt conditions         optimization problem  a     satisfied
  
point  w   b               a             a    m    a       a       a    a       a      
  

satisfies kkt conditions         assumption  a     kkt conditions
h  h   
order prove convergence properties iterates  t    use following theorem
due zangwill        
theorem a    let map h   z z determine iterative algorithm via  t     
h  t     let    denote objective function  let set points
map h change value objective function  i e   h         
suppose
  

fiepshteyn   dejong

   h uniformly compact z  i e  compact subset z  z
h   z  z 
   h strictly monotonic z   i e   h         
   h closed z   i e  wi w h wi       h w  
accumulation points sequence  t  lie  
following proposition shows minimization continuous function feasible
set continuous map functions argument forms closed function 
proposition a    given
   real valued continuous function f b 
   point to set map u    b continuous respect hausdorff metric  
dist x      max d x     d y  x    d x      maxxx minyy kx yk 
define function f   b
f  a    arg min f  a  b       b   f  a  b    f  a  b    b  u  a   
b  u  a 

assuming minimum exists unique  then  function f closed a 
proof  proof minor modification one given gunawardana byrne
        let  a t    sequence
a t  a  f  a t    b

 a    

function f closed f  a    b  suppose case  i e  b    f  a   
arg minb  u  a  f  a  b     therefore 
b   arg min f  b    f  a  b    f  a  b 
b  u  a 

 a    

continuity f       a     
f  a t    f  a t     f  a  b 

 a    

continuity u     a     
dist u  a t     u  a     b t  b b t  u  at    t 

 a    

 a       a       a     imply
k f  a t    f  a t       f  a t    b t       k

 a    

contradiction since assumption  f  a t      arg minb  u  at   f  b     a     
b t  u  a t    
   point to set map u  a  maps point set points  u  a  continuous respect distance
metric dist iff a t  implies dist u  a t     u  a     

  

figenerative prior knowledge discriminative classification

proposition a    function h defined  a     a    closed 
proof  let   t    sequence  t    since iterates  t  lie
closed feasible region bounded constraints             boundary u   
piecewise linear   boundary u   t    converges uniformly boundary u    
 t    implies hausdorff distance boundaries converges
zero  since hausdorff distance convex sets equal hausdorff distance
boundaries  dist u   t     u      converges zero  hence  proposition
a   implies h  closed  proposition implies h  closed  composition
closed functions closed  hence h closed 
prove main result section 
theorem      let h function defined  a     a    determines generative discriminative algorithm via  t      h  t     accumulation points
sequence  t  augmented  w   b     h      feasible descent directions
original optimization problem given             
proof  proof verifying h satisfies properties theorem a    closedness
h shown proposition a    strict monotonicity   t    shown theorem
     since iterates  t  closed feasible region bounded constraints             h uniformly compact z  since accumulation points lie  
kkt points original optimization problem theorem a    and  therefore 
feasible descent directions proposition a   

appendix b  generalization generative discriminative classifier
need auxiliary results proving theorem      first proposition bounds
angle rotation two vectors w    w  distance angle
rotation vectors reference vector v sufficiently small 
proposition b    let kw  k   kw  k   kvk      w t v   w t v   
   w t w      
p
   kw  w  k         

proof 

   triangle inequality  arccos w t w    arccos w t v    arccos w t v    arccos  
 since angle two vectors distance measure   taking cosines
sides using trigonometric equalities yields w t w       
   expand kw  w  k    kw  k    kw  k   w t w        w t w     since w t w      
part    kw  w  k          

next proposition bounds angle rotation two vectors
far away measured l   norm distance 
  

fiepshteyn   dejong

proposition b    let ktk     k tk  

tt
ktkkk



   
      

proof  expanding k tk    ktk    kk   tt using k tk      get
  ktk
    kk

kk
ktk

 

tt
ktkkk



 
ktkkk    use triangle inequality ktk k tk kk
ktk   k tk   simplify 

following proposition used bound angle rotation normal
w separating hyperplane mean vector hyper prior distribution 
wt
kwkkk
ktk   
ktk  ktk    

proposition b    let
  min  

  k tk ktk 

wt
kwkktk

       

proof  follows directly propositions b    part    b   
prove theorem      relies parts well known proof fatshattering dimension bound large margin classifiers derived taylor bartlett
       
theorem      let f class a priori constrained functions defined     
let min  p   max  p   denote minimum maximum eigenvalues matrix p  
 
 
    
respectively  set points  shattered f    s   r     
 
 
 

 

    
    
  k  max      
  kt ktk 
 
      min  min
  max             min  min
 
k  k
k  k
max        kt  k 
kt  k   max       
  
kt  k  max         kt  k 

assuming    kti k kti k 

   
 

       



proof  first  use inequality min  p   kwk p     w max  p   kwk relax
constraints
w  
w  


min     
 b   
   
kwk
  w



   

  max      
 b   
     t    k  t  k
min   
   



   
wt  

constraints imposed second prior            t    relaxed

similar fashion produce 

  w

wt     
min     
kwk

 b   

k  t  k max     

 b   

now  show assumptions made statement theorem hold 
 
 
 
p
p
every subset satisfies k  s s   k  r          
assume  shattered f   argument used taylor bartlett       
lemma     shows that  definition fat shattering  exists vector w  

x
x
w   

 s s      s   
 b   
  

figenerative prior knowledge discriminative classification

similarly  reversing labeling s  s  s     exists vector w 
x
x
w     s s   
   s   
 b   
hence   w  w    
implies

p



p

 s s        s    which  cauchy schwartz inequality 

   s 
p
kw  w  k p
k  s s   k

 b   

constraints classifier represented b   b   imply proposition b  
w t t 
w t t 
 
 
kw  kkt  k        kw  kkt  k          now  applying proposition b    part   
simplifying  get
q
kw  w  k  

           

applying analysis constraints b   b    get
q
kw  w  k              
combining b    b    b    get
x

x
 s 



 s s    p

          

 b   

 b   

 b    

defined statement theorem 
taylor bartletts        lemma     proves  using probabilistic method 
satisfies
x
p
x


 b    

 s s     s r 

combining b    b    yields  s 

 r           
 
 

references
baxter  j          model inductive bias learning  journal artificial intelligence
research             
bhattacharyya  c   pannagadatta  k  s     smola  a          second order cone programming formulation classifying missing data  nips 
blake 
c  
 
merz 
c 
       
  
newsgroups
http   people csail mit edu people jrennie   newsgroups   

database 

campbell  c   cristianini  n     smola  a          query learning large margin classifiers  proceedings seventeenth international conference machine learning 
carlin  b     louis  t          bayes empirical bayes methods data analysis 
chapman hall 
collins  m          discriminative training methods hidden markov models  theory
experiments perceptron algorithms  proceedings      conference
empirical methods natural language processing 
  

fiepshteyn   dejong

duda  r   hart  p     stork  d          pattern classification  john wiley   nd edition 
epshteyn  a     dejong  g          rotational prior knowledge svms  proceedings
sixteenth european conference machine learning 
evgeniou  t     pontil  m          regularized multi task learning  proceedings
tenth acm sigkdd international conference knowledge discovery data
mining 
fink  m          object classification single example utilizing class relevance metrics 
advances neural information processing systems 
fletcher  r          practical methods optimization  john wiley sons  west sussex 
england 
fung  g   mangasarian  o     shavlik  j          knowledge based support vector machine
classifiers  advances neural information processing systems 
greiner  r     zhou  w          structural extension logistic regression  discriminative
parameter learning belief net classifiers  proceedings eighteenth national
conference artificial intelligence 
gunawardana  a     byrne  w          convergence theorems generalized alternating
minimization procedures  journal machine learning research              
hansen  p   jaumard  b     savard  g          new branch and bound rules linear bilevel
programming  siam journal scientific statistical computing               
huang  k   king  i   lyu  m  r     chan  l          minimum error minimax probability
machine  journal machine learning research              
jaakkola  t   meila  m     jebara  t          maximum entropy discrimination  advances
neural information processing systems 
jebara  t          machine learning  discriminative generative  kluwer academic
publishers 
joachims  t          text categorization support vector machines  learning many
relevant features  proceedings tenth european conference machine learning 
lanckriet  g  r  g   ghaoui  l  e   bhattacharyya  c     jordan  m  i          minimax
probability machine  advances neural information processing systems 
lobo  m  s   vandenberghe  l   boyd  s     lebret  h          applications second order
cone programming  linear algebra applications                   
mangasarian  o   shavlik  j     wild  e          knowledge based kernel approximation 
journal machine learning research 
miller  g          wordnet  online lexical database  international journal lexicography        
ng  a  y     jordan  m  i          discriminative vs  generative classifiers  comparison
logistic regression naive bayes  advances neural information processing
systems 
  

figenerative prior knowledge discriminative classification

raina  r   shen  y   ng  a  y     mccallum  a          classification hybrid generative discriminative models  advances neural information processing systems 
roos  t   wettig  h   grunwald  p   myllymaki  p     tirri  h          discriminative
bayesian network classifiers logistic regression  machine learning             
scholkopf  b   simard  p   vapnik  v     smola  a          prior knowledge support
vector kernels  advances kernel methods   support vector learning 
sturm  j  f          using sedumi       matlab toolbox optimization symmetric cones  optimization methods software             
sun  q     dejong  g          explanation augmented svm  approach incorporating
domain knowledge svm learning  proceedings twenty second international conference machine learning 
taylor  j  s     bartlett  p          generalization performance support vector machines
pattern classifiers  advances kernel methods  support vector learning 
thrun  s          learning n th thing easier learning first   advances
neural information processing systems 
tipping  m  e          sparse bayesian learning relevance vector machine  journal
machine learning research            
tong  s     koller  d       a   restricted bayes optimal classifiers  proceedings
seventeenth national conference artificial intelligence 
tong  s     koller  d       b   support vector machine active learning applications
text classification  proceedings seventeenth international conference
machine learning 
vapnik  v          nature statistical learning theory  springer verlag 
wu  x     srihari  r          incorporating prior knowledge weighted margin support
vector machines  proceedings tenth acm sigkdd international conference
knowledge discovery data mining 
zangwill  w          convergence conditions nonlinear programming algorithms  management science          

  


