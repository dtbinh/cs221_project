journal of artificial intelligence research                

submitted        published      

cognitive principles in robust multimodal interpretation
joyce y  chai
zahar prasov
shaolin qu

jchai cse msu edu
prasovza cse msu edu
qushaoli cse msu edu

department of computer science and engineering
michigan state university
east lansing  mi       usa

abstract
multimodal conversational interfaces provide a natural means for users to communicate with computer systems through multiple modalities such as speech and gesture  to
build eective multimodal interfaces  automated interpretation of user multimodal inputs
is important  inspired by the previous investigation on cognitive status in multimodal
human machine interaction  we have developed a greedy algorithm for interpreting user
referring expressions  i e   multimodal reference resolution   this algorithm incorporates
the cognitive principles of conversational implicature and givenness hierarchy and applies constraints from various sources  e g   temporal  semantic  and contextual  to resolve
references  our empirical results have shown the advantage of this algorithm in eciently
resolving a variety of user references  because of its simplicity and generality  this approach
has the potential to improve the robustness of multimodal input interpretation 

   introduction
multimodal systems provide a natural and eective way for users to interact with computers
through multiple modalities such as speech  gesture  and gaze  since the rst appearance
of the put that there system  bolt         a number of multimodal systems have been
built  among which there are systems that combine speech  pointing  neal   shapiro       
stock         and gaze  koons  sparrell    thorisson         systems that integrate speech
with pen inputs  e g   drawn graphics   cohen  johnston  mcgee  oviatt  pittman  smith 
chen    clow        wahlster         systems that combine multimodal inputs and outputs
 cassell  bickmore  billinghurst  campbell  chang  vilhjalmsson    yan         systems
in mobile environments  oviatt      a   and systems that engage users in an intelligent
conversation  gustafson  bell  beskow  boye  carlson  edlund  granstrom  house    wiren 
      stent  dowding  gawron  bratt    moore         earlier studies have shown that
multimodal interfaces enable users to interact with computers naturally and eectively
 oviatt            b  
one important aspect of building multimodal systems is multimodal interpretation 
which is a process that identies the meanings of user inputs  in particular  a key element
in multimodal interpretation is known as reference resolution  which is a process that nds
the most proper referents to referring expressions  here a referring expression is a phrase
that is given by a user in her inputs  most likely in speech inputs  to refer to a specic
entity or entities  a referent is an entity  e g   a specic object  to which the user refers 
suppose that a user points to house   on the screen and says how much is this one  in this
c
    
ai access foundation  all rights reserved 

fichai  prasov    qu

case  reference resolution must infer that the referent house   should be assigned to the
referring expression this one  this paper particularly addresses this problem of reference
resolution in multimodal interpretation 
in a multimodal conversation  the way users communicate with a system depends on
the available interaction channels and the situated context  e g   conversation focus  visual
feedback   these dependencies form a rich set of constraints from various aspects  e g  
semantic  temporal  and contextual   a correct interpretation can only be attained by
simultaneously considering these constraints 
previous studies have shown that user referring behavior during multimodal conversation
does not occur randomly  but rather follows certain linguistic and cognitive principles 
in human machine interaction  earlier work has shown strong correlations between the
cognitive status in givenness hierarchy and the form of referring expressions  kehler        
inspired by this early work  we have developed a greedy algorithm for multimodal reference
resolution  this algorithm incorporates the principles of conversational implicature and
givenness hierarchy and applies constraints from various sources  e g   gesture  conversation
context  and visual display   our empirical results have shown the promise of this algorithm
in eciently resolving a variety of user references  one major advantage of this greedy
algorithm is that the prior linguistic and cognitive knowledge can be used to guide the
search and prune the search space during constraint satisfaction  because of its simplicity
and generality  this approach has the potential to improve the robustness of interpretation
and provide a practical solution to multimodal reference resolution  chai  prasov  blaim 
  jin        
in the following sections  we will rst demonstrate dierent types of referring behavior
observed in our studies  we then briey introduce the underlying cognitive principles for
human human communication and describe how these principles can be used in a computational model to eciently resolve multimodal references  finally  we will present the
experimental results 

   multimodal reference resolution
in our previous work  chai  hong    zhou      b  chai  hong  zhou    prasov         a
multimodal conversational system was developed for users to acquire real estate information   
figure   is the snapshot of a graphical user interface  users can interact with this interface
through both speech and gesture  table   shows a fragment of the conversation 
in this fragment  the user exhibits dierent types of referring behavior  for example 
the input from u  is considered as a simple input  this type of simple input only has one
referring expression in the spoken utterance and one accompanying gesture  multimodal
fusion that combines information from speech and gesture will likely resolve what this
refers to  in the second user input  u     there is no accompanying gesture and no referring
expression is explicitly used in the speech utterance  at this time  the system needs to
use the conversation context to infer that the object of interest is the house mentioned in
the previous turn of the conversation  in the third user input  there are multiple referring
expressions and multiple gestures  these types of inputs are considered complex inputs 
   the first prototype of this system was developed at ibm t  j  watson research center with p  hong 
m  zhou  and colleagues at the intelligent multimedia interaction group 

  

fiminimizing conflicts  a heuristic repair method

figure    a snapshot of the multimodal conversational system 

u 
s 
u 
s 
u 
s 

speech  how much does this cost 
gesture  point to a position on the screen
speech  the price is    k
graphics  highlight the house in discussion
speech  how large 
speech       square feet
speech  compare it with this house and this one
gesture      circle    cirle  put two consecutive circles on the screen 
speech  here are your comparison results
graphics  show a table of comparison

table    a fragment demonstrating interaction with dierent types of referring behavior
complex inputs are more dicult to resolve  we need to consider the temporal relations
between the referring expressions and the gestures  the semantic constraints specied by
the referring expressions  and the contextual constraints from the prior conversation  for
example  in the case of u    the system needs to understand that it refers to the house that
was the focus of the previous turn  and this house and this one should be aligned with the
two consecutive gestures  any subtle variations in any of the constraints  including the
temporal ordering  the semantic compatibility  and the gesture recognition results will lead
to dierent interpretations 
from this example  we can see that in a multimodal conversation  the way a user interacts with a system is dependent not only on the available input channels  e g   speech and
gesture   but also upon his her conversation goals  the state of the conversation  and the
multimedia feedback from the system  in other words  there is a rich context that involves
  

fichai  prasov    qu

dependencies from many dierent aspects established during the interaction  interpreting
user inputs can only be situated in this rich context  for example  the temporal relations
between speech and gesture are important criteria that determine how the information from
these two modalities can be combined  the focus of attention from the prior conversation
shapes how users refer to those objects  and thus  inuences the interpretation of referring
expressions  therefore  we need to simultaneously consider the temporal relations between
the referring expressions and the gestures  the semantic constraints specied by the referring expressions  and the contextual constraints from the prior conversation  in this paper 
we present an ecient approach that is driven by cognitive principles to combine temporal 
semantic  and contextual constraints for multimodal reference resolution 

   related work
considerable eort has been devoted to studying user multimodal behavior  cohen       
oviatt      a  and mechanisms to interpret user multimodal inputs  chai et al       b 
gustafson et al         huls  bos    classen        johnston  cohen  mcgee  oviatt 
pittman    smith        johnston        johnston   bangalore        kehler        koons
et al         neal   shapiro        oviatt  deangeli    kuhn        stent et al         stock 
      wahlster        wu   oviatt        zancanaro  stock    strapparava        
for multimodal reference resolution  some early work keeps track of a focus space from
the dialog  grosz   sidner        and a display model to capture all objects visible on the
graphical display  neal  thielman  dobes  m     shapiro         it then checks semantic
constraints such as the type of the candidate objects being referenced and their properties
for reference resolution  a modied centering model for multimodal reference resolution
is also introduced in previous work  zancanaro et al          the idea is that based on
the centering movement between turns  segments of discourse can be constructed  the
discourse entities appearing in the segment that is accessible to the current turn can be
used to constrain the referents to referring expressions  another approach is introduced
to use contextual factors for multimodal reference resolution  huls et al          in this
approach  a salience value is assigned to each instance based on the contextual factors 
to determine the referents of multimodal referring expressions  this approach retrieves the
most salient referent that satises the semantic restrictions of the referring expressions  all
these earlier approaches have some greedy nature  which is largely dependent on semantic
constraints and or constraints from conversation context 
to resolve multimodal references  there are two important issues  first it is the mechanism to combine information from various sources and modalities  the second is the capability to obtain the best interpretation  among all the possible alternatives  given a set of
temporal  semantic  and contextual constraints  in this section  we give a brief introduction
to three recent approaches that address these issues 
    multimodal fusion
approaches to multimodal fusion  johnston        johnston   bangalore         although
they focus on a dierent problem of overall input interpretation  provide eective solutions
to reference resolution  there are two major approaches to multimodal fusion  unication  

fiminimizing conflicts  a heuristic repair method

based approaches  johnston        and nite state approaches  johnston   bangalore 
      
the unication based approach identies referents to referring expressions by unifying
feature structures generated from speech utterances and gestures using a multimodal grammar  johnston et al         johnston         the multimodal grammar combines both
temporal and spatial constraints  temporal constraints encode the absolute temporal relations between speech and gesture  johnston          the grammar rules are predened
based on empirical studies of multimodal interaction  oviatt et al          for example  one
rule indicates that speech and gesture can be combined only when the speech either overlaps
with gesture or follows the gesture within a certain time frame  the unication approach
can also process certain complex cases  as long as they satisfy the predened multimodal
grammar  in which a speech utterance is accompanied by more than one gesture of dierent
types  johnston         using this approach to accommodate various situations such as
those described in figure   will require adding dierent rules to cope with each situation 
if a specic user referring behavior did not exactly match any existing integration rules
 e g   temporal relations   the unication would fail and therefore references would not be
resolved 
the nite state approach applies nite state transducers for multimodal parsing and
understanding  johnston   bangalore         unlike the unication based approach with
chart parsing that is subject to signicant computational complexity concerns  johnston
  bangalore         the nite state approach provides more ecient  tight coupling of
multimodal understanding with speech recognition  in this approach  a multimodal contextfree grammar is dened to transform the syntax of multimodal inputs to the semantic
meanings  the domain specic semantics are directly encoded in the grammar  based
on these grammars  multi tape nite state automata can be constructed  these automata
are used for identifying semantics of combined inputs  rather than absolute temporal
constraints as in the unication based approach  this approach relies on temporal order
between dierent modalities  during the parsing stage  the gesture input from the gesture
tape  e g   pointing to a particular person  that can be combined with the speech expression
in the speech tape  e g   this person  is considered as the referent to the expression  a
problem with this approach is that the multi tape structure only takes input from speech
and gesture and does not incorporate the conversation history into consideration 
    decision list
to identify potential referents  previous work has investigated givenness hierarchy  to
be introduced later  in multimodal interaction  kehler         based on data collected
from wizard of oz experiments  this investigation suggests that users tend to tailor their
expressions to what they perceive to be the systems beliefs concerning the cognitive status
of referents from their prominence  e g   highlight  on the display  the tailored referring
expressions can then be resolved with a high accuracy based on the following decision list 
   if an object is gestured to  choose that object 
   otherwise  if the currently selected object meets all semantic type constraints imposed
by the referring expression  choose that object 
  

fichai  prasov    qu

   otherwise  if there is a visible object that is semantically compatible  then choose
that object 
   otherwise  a full np  such as a proper name  is used to uniquely identify the referent 
from our studies  chai  prasov    hong      a   we found this decision list has the
following limitations 
 depending on the interface design  ambiguities  from a systems perspective  could
occur  for example  given an interface where one object  e g   house  can sometimes
be created on top of another object  e g   town   a pointing gesture could result in
multiple potential objects  furthermore  given an interface with crowded objects  a
nger point could also result in multiple objects with dierent probabilities  the
decision list is not able to handle these ambiguous cases 
 user inputs are not always simple  consisting of no more than one referring expression
and one gesture as indicated in the decision list   in fact  in our study  chai et al  
    a   we found that user inputs can also be complex  consisting of multiple referring
expressions and or multiple gestures  the referents to these referring expressions
could come from dierent sources  such as gesture inputs and conversation context 
the temporal alignment between speech and gesture is also important in determining
the correct referent for a given expression  the decision list is not able to handle these
types of complex inputs 
nevertheless  the previous ndings  kehler        have inspired this work and provided a
basis for the algorithm described in this paper 
    optimization
recently  a probabilistic approach was developed for optimizing reference resolution based
on graph matching  chai et al       b   in the graph matching approach  information
gathered from multiple input modalities and the conversation context is represented as
attributed relational graphs  args   tsai   fu         specically  two graphs are used 
one graph represents referring expressions from speech utterances  i e   called referring
graph   a referring graph contains referring expressions used in a speech utterance and
the relations between these expressions  each node corresponds to one referring expression
and consists of the semantic and temporal information extracted from that expression 
each edge represents the semantic and temporal relation between two referring expressions 
the resulting graph is a fully connected  undirected  graph  for example  as shown in
figure   a   from the speech input compare this house  the green house  and the brown one 
three nodes are generated in the referring graph representing three referring expressions 
each node contains semantic and temporal features related to its corresponding referring
expression  these include the expressions semantic type  house  town  etc    number of
potential referents  type dependent features  size  price  etc    syntactic category of the
expression  and the timestamp of when the expression was produced  each edge contains
features describing semantic and temporal relations between a pair of nodes  the semantic
features simply indicate whether or not two nodes share the same semantic type if this
  

fiminimizing conflicts  a heuristic repair method

figure    reference resolution through probabilistic graph matching

can be inferred from the utterance  otherwise  the semantic type relation is deemed to be
unknown  the temporal features indicate which of the two expressions was uttered rst 
similarly  another graph represents all potential referents gathered from gestures  history  and the visual display  i e   called referent graph   each node in a referent graph
captures the semantic and temporal information about a potential referent  together with
its selection probability  the selection probability is particularly applied to objects indicated by a gesture  because a gesture such as a pointing or a circle can potentially introduce
ambiguity in terms of the intended referents  a selection probability is used to indicate how
likely it is that an object is selected by a particular gesture  this selection probability is
derived by a function of the distance between the location of the entity and the focus point
of the recognized gesture on the display  as in a referring graph  each edge in a referent
graph captures the semantic and temporal relations between two potential referents such
as whether the two referents share the same semantic type and the temporal order between
two referents as they are introduced into the discourse  for example  since the gesture input
consists of two pointings  the referent graph  figure  b  consists of all potential referents
from these two pointings  the objects in the rst dashed rectangle are potential referents
selected by the rst pointing  and those in the second dashed rectangle correspond to the
second pointing  furthermore  the salient objects from the prior conversation are also included in the referent graph since they could be the potential referents as well  e g   the
rightmost dashed rectangle in figure  b  
given these graph representations  the reference resolution problem becomes a probabilistic graph matching problem  gold   rangarajan         the goal is to nd a match
between the referring graph gs and the referent graph gc   that achieves the maximum
compatibility  i e   maximizes q gc   gs    as described in the following equation 
   the subscription s in gs refers to speech referring expressions and c in gc refers to candidate referents 

  

fichai  prasov    qu

q gc   gs    

 
 x   m  n odesim x   m  
x
m p




 

x

y

m

n p  x   m  p  y   n  edgesim xy   mn  

   

p  x   m   is the matching probability between a referent node x and a referring node
m   the overall compatibility q gc   gs   depends on the node compatibility n odesim
and the edge compatibility edgesim  which were further dened by temporal and semantic
constraints  chai et al          when the algorithm converges  p  x   m   gives the matching
probabilities between a referent node x and a referring node m that maximizes the overall
compatibility function  using these matching probabilities  the system is able to identify the
most probable referent x to each referring node m   specically  the referring expression
that matches a potential referent is assigned to the referent if the probability of this match
exceeds an empirically computed threshold  if this threshold is not met  the referring
expression remains unresolved 
theoretically  this approach provides a solution that maximizes the overall satisfaction
of semantic  temporal  and contextual constraints  however  like many other optimization
approaches  this algorithm is non polynomial  it relies on an expensive matching process 
which attempts every possible assignment  in order to converge on an optimal interpretation
based on those constraints  however  previous linguistic and cognitive studies indicate that
user language behavior does not occur randomly  but rather follows certain cognitive principles  therefore  a question arises whether any knowledge from these cognitive principles
can be used to guide this matching process and reduce the complexity 

   cognitive principles
motivated by previous work  kehler         we specically focus on two principles  conversational implicature and givenness hierarchy 
    conversational implicature
grices conversational implicature theory indicates that the interpretation and inference of
an utterance during communication is guided by a set of four maxims  grice         among
these four maxims  the maxim of quantity and the maxim of manner are particularly useful
for our purpose 
the maxim of quantity has two components      make your contribution as informative as is required  for the current purposes of the exchange   and     do not make your
contribution more informative than is required  in the context of multimodal conversation 
this maxim indicates that users generally will not make any unnecessary gestures or speech
utterances  this is especially true for pen based gestures since they usually require a special
eort from a user  therefore  when a pen based gesture is intentionally delivered by a user 
the information conveyed is often a crucial component used in interpretation 
grices maxim of manner has four components      avoid obscurity of expression     
avoid ambiguity      be brief  and     be orderly  this maxim indicates that users will
not intentionally make ambiguous references  they will use expressions  either speech or
gesture  they believe can uniquely describe the object of interest so that listeners  in this
case a computer system  can understand  the expressions they choose depend on the
  

fiminimizing conflicts  a heuristic repair method

status
expression form
in f ocus
it

activated
that  this  this n

f amiliar
that n

u nique identif iable
the n

ref erential
indef inite this n

identif iable
an
figure    givenness hierarchy

information in their mental models about the current state of the conversation  however 
the information in a users mental model might be dierent from the information the system
possesses  when such an information gap happens  dierent ambiguities could occur from
the system point of view  in fact  most ambiguities are not intentionally caused by the
human speakers  but rather by the systems incapability of choosing among alternatives
given incomplete knowledge representation  limited capability of contextual inference  and
other factors  e g   interface design issues   therefore  the system should not anticipate
deliberate ambiguities from users  e g   a user only utters a house to refer to a particular
house on the screen   but rather should focus on dealing with the types of ambiguities
caused by the systems limitations  e g   gesture ambiguity due to the interface design or
speech ambiguity due to incorrect recognition  
these two maxims help positioning the role of gestures in reference resolution  in
particular  these maxims have put the potential referents indicated by a gesture at a very
important position  which is described in section   
    givenness hierarchy
the givenness hierarchy proposed by gundel et al  explains how dierent determiners
and pronominal forms signal dierent information about memory and attention state  i e  
cognitive status   gundel  hedberg    zacharski         as in figure    there are six
cognitive statuses in the hierarchy  for example  in focus indicates the highest attentional
state that is likely to continue to be the topic  activated indicates entities in short term
memory  each of these statuses is associated with some forms of referring expressions  in
this hierarchy  each cognitive status implies the statuses down the list  for example  in focus
implies activated  familiar  etc  the use of a particular expression form not only signals
that the associated cognitive status is met  but also signals that all lower statuses have been
met  in other words  a given form that is used to describe a lower status can also be used to
refer to a higher status  but not vice versa  cognitive statuses are necessary conditions for
  

fichai  prasov    qu

appropriate use of dierent forms of referring expressions  gundel et al  found that dierent
referring expressions almost exclusively correlate with the six statuses in this hierarchy 
the givenness hierarchy has been investigated earlier in algorithms for resolving pronouns and demonstratives in spoken dialog systems  eckert   strube        byron       
and in multimodal interaction  kehler         in particular  we would like to extend the previous work  kehler        and investigate whether conversational implicature and givenness hierarchy can be used to resolve a variety of references from simple to complex  and
from precise to ambiguous  furthermore  the decision list used in kehler        is proposed based on data analysis and has not been implemented or evaluated in a real time
system  therefore  our second goal is to design and implement an ecient algorithm by
incorporating these cognitive principles and empirically compare its performance with the
optimization approach  chai et al          the nite state approach  johnston   bangalore 
       and the decision list approach  kehler        

   a greedy algorithm
a greedy algorithm always makes the choice that looks best at the moment of processing 
that is  it makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution  simple and ecient greedy algorithms can be used to approximate
many optimization problems  here we explore the use of conversational implicature and
givenness hierarchy in designing an ecient greedy algorithm  in particular  we extend the
decision list from kehler        and utilize the concepts from the two cognitive principles
in the following way 
 corresponding to the givenness hierarchy  the following hierarchy holds for potential
referents  f ocus   v isible  this hierarchy indicates that objects in focus have higher
status in terms of attention states than objects in the visual display  here focus
corresponds to the cognitive statuses in focus and activated in the givenness hierarchy 
and visible corresponds to the statuses familiar and uniquely identifiable  note that
givenness hierarchy is ne grained in terms of dierent statuses  our application
may not be able to distinguish the dierence between these statuses  e g   in focus
and activated  and eectively use them  therefore  focus and visible are introduced
here to group some similar statuses  with respect to our application  together  since
there is a need to dierentiate the objects that have been mentioned recently  e g  
in focus and activated  and objects that are accessible either on the graph display
or from the domain model  e g   familiar and unique identiable   we assign them to
dierent modied statuses  e g   focus and visible  
 based on the conversational implicature  since a pen based gesture takes a special effort to deliver  it must convey certain useful information  in fact  objects indicated by
a gesture should have the highest attentional state since they are deliberately singled
out by a user  therefore  by combining     and      we derive a modied hierarchy
gesture   f ocus   v isible   others  here others corresponds to indenite cases
in givenness hierarchy  this modied hierarchy coincides with the processing order
of the kehlers decision list         this modied hierarchy will guide the greedy
  

fiminimizing conflicts  a heuristic repair method

algorithm in its search for solutions  next  we describe in detail the algorithm and
related representations and functions 
    representation
at each turn   i e   after receiving a user input  of the conversation  we use three vectors to
represent the rst three statuses in our modied hierarchy  objects selected by a gesture 
objects in the focus  and objects visible on the display as follows 
 gesture vector  g   captures objects selected by a series of gestures  each element gi
is an object potentially selected by a gesture  for elements gi and gj where i   j  the
gesture that selects objects gi should     temporally precede the gesture that selects
gj or    be the same as the gesture that selects gj since one gesture could result in
multiple objects 
 focus vector  f  captures objects that are in the focus but are not selected by any
gesture  each element represents an object considered to be the focus of attention
from the previous turn of the conversation  there is no temporal precedence relation
between these elements  we consider all the corresponding objects are simultaneously
accessible to the current turn of the conversation 
 captures objects that are visible on the display but are neither
 display vector  d 
selected by any gesture  i e   g  nor in the focus  f   there is also no temporal precedence relation between these elements  all elements are simultaneously accessible 
based on these representations  each object in the domain of interest belongs to either
one of these above vectors or others  each object in the above vectors consists of the
following attributes 
 semantic type of the object  for example  the semantic type could be a house or a
town 
 the attributes of the object  this is a domain dependent feature  a set of attributes
is associated with each semantic type  for example  a house object has price  size 
year built  etc  as its attributes  furthermore  each object has visual properties that
reect the appearance of the object on the display such as color of an object icon 
 the identier of the object  each object has a unique name 
 the selection probability  it refers to the probability that a given object is selected 
depending on the interface design  a gesture could result in a list of potential referents 
we use this selection probability to indicate the likelihood of an object selected by
a gesture  the calculation of the selection probability is described later  for objects
from the focus vector and the display vector  the selection probabilities are set to   n
where n is the total number of objects in the respective vector 
   currently  user inactivity  i e     seconds with no input from either speech or gesture  is used as the
boundary to decide an interaction turn 

  

fichai  prasov    qu

 temporal information  the relative temporal ordering information for the corresponding gesture  instead of applying time stamps as in our previous work  chai et al  
    b   here we only use the index of gestures according to the order of their occurrences  if an object is selected by the rst gesture  then its temporal information
would be   
in addition to vectors that capture potential referents  for each user input  a vector
that represents referring expressions from a speech utterance  r  is also maintained  each
element  i e   a referring expression  has the following information 
 the identier of the potential referent indicated by the referring expression  for
example  the identier of the potential referent to the expression house number eight
is a house object with an identier eight 
 the semantic type of the potential referents indicated by the expression  for example 
the semantic type of the referring expression this house is house 
 the number of potential referents as indicated by the referring expression or the
utterance context  for example  a singular noun phrase refers to one object  a
phrase like three houses provides the exact number of referents  i e      
 type dependent features  any features associated with potential referents  such as
color and price  are extracted from the referring expression 
 the temporal ordering information indicating the order of referring expressions as
they are uttered  again  instead of the specic time stamp  here we only use the
temporal ordering information  if an utterance consists of n consecutive referring
expressions  then the temporal ordering information for each of them would be      
and up to n  
 the syntactic categories of the referring expressions  currently  for each referring
expression  we assign it to one of six syntactic categories  e g   demonstrative and
pronoun   details are explained later 
these four vectors are updated after each user turn in the conversation based on the current
user input and the system state  e g   what is shown on the screen and what was identied
as focus from the previous turn of the conversation  
    algorithm
the ow chart with the pseudo code of the algorithm is shown in figure    for each
multimodal input at a particular turn in the conversation  this algorithm takes the inputs
of a vector  r  of referring expressions with size k  a gesture vector  g   of size m  a focus
 of size l  it rst creates three matrices
vector of  f   of size n  and a display vector  d 
g i  j   f  i  j   and d i  j  to capture the scores of matching each referring expression from
r to each object in the three vectors  calculation of the matching score is described later 
note that  if any of the g  f  and d is empty  then the corresponding matrix  i e   g  f   or
d  is empty 
  

fiminimizing conflicts  a heuristic repair method

initializematchmatrix       
for  i      m  j      k  g i  j    match gi  rj 
for  i      n  j      k  f i  j    match fi  rj 
for  i      l  j      k  d i  j    match di  rj 
 

yes

is g empty

no
greedysortinggesture  
index max        index to the column
for  i      m   
find j t index max  where g i  j  is the largest among the elements in row i 
add a mark   to the g i  j  
index max   j      complete finding the best match from a view of each object
assignreferentsfrommatrix  g  
 

all references resolved 

no

yes

yes
is f empty

no

return results

greedysortingfocus 
for  j      k 
if  rj is resolved 
then cross out column j in f   only keep ones not resolved
for   i      n  
find j where f i  j  is the largest among the elements in row i 
mark   to the f i  j    
assignreferentsfrommatrix  f  
 

all references resolved 

no

greedysortingdisplay 
for  j      k 
if  rj is resolved 
then cross out column j in d 
for   i      l  
find j where d i  j  is the largest among the elements in row i 
mark   to d i  j    
assignreferentsfrommatrix  d  
 

return results

assignreferentsfrommatrix  matrix x  
for  i      k     i e   for each expression ri in column i
if  ri indicates a specific number n and more than n elements
in ith column of x with   
then assign n largest elements with   to ri as referents 
else assign all elements with   to ri as referents 
 

figure    a greedy algorithm for multimodal reference resolution

  

yes

return results

fichai  prasov    qu

based on the matching scores in the three matrices  the algorithm applies a greedy
search that is guided by our modied hierarchy as described earlier  since gesture has
the highest status  the algorithm rst searches the gesture matrix  g  that keeps track of
matching scores between all referring expressions and all objects from gestures  it identies
the highest  or multiple highest  matching scores and assigns all possible objects from
gestures to the expressions  greedysortinggesture  
if more referring expressions are left to be resolved after gestures are processed  the
algorithm looks at objects from the focus matrix  f   since focus is the next highest cognitive status  greedysortingfocus   if there are still more expressions to be resolved  then the
algorithm looks at objects from the display matrix  d   greedysortingdisplay   currently 
our algorithm focuses on these three statuses  certainly  if there are still more expressions
to be resolved after all these steps  the algorithm can consult with proper name resolution 
once all the referring expressions are resolved  the system will output the results  for the
next multimodal input  the system will generate four new vectors and then apply the greedy
algorithm again 
note that in greedysortinggesture  we use index max to keep track of the column index
that corresponds to the largest matching value  as the algorithm incrementally processes
each row in the matrix  this index max should incrementally increase  this is because the referring expressions and the gesture should be aligned according to their order of occurrences 
since objects in the focus matrix and the display matrix do not have temporal precedence
relations  greedysortingfocus and greedysortingdisplay do not use this constraint 
the reason we call this algorithm greedy is that it always nds the best assignment for a
referring expression given a cognitive status in the hierarchy  in other words  this algorithm
always makes the best choice for each referring expression one at a time according to the
order of their occurrence in the utterance  one can imagine that a mistaken assignment
made to an expression can aect the assignment of the following expressions  therefore 
the greedy algorithm may not lead to a globally optimal solution  nevertheless  the general
user behavior following the guiding principles makes this greedy algorithm useful 
one major advantage of this greedy algorithm is that the use of the modied hierarchy can signicantly prune the search space compared to the graph matching approach 
given m referring expressions and n potential referents from various sources  e g   gesture 
conversation context  and visual display   this algorithm can nd a solution in o mn  
furthermore  this algorithm goes beyond simple and precise inputs as illustrated by the
decision list in kehler         the scoring mechanism  described later  and the greedy
sorting process accommodate both complex and ambiguous user inputs 
    matching functions
an important component of the algorithm is the matching score between an object  o  and
a referring expression  e   we use the following equation to calculate the matching score 
m atch o  e     



p  o s   p  s e    compatibility o  e 

   

s g f d 

in this formula  s represents the possible associated status of an object o  it could
have three potential values  g  representing gesture   f  focus   and d  display   this
function is determined by three components 
  

fiminimizing conflicts  a heuristic repair method

 the rst  p  o s   is the object selectivity component that measures the probability
of an object to be the referent given a status  s  of that object  i e   gesture  focus 
or visual display  
 the second  p  s e   is the likelihood of status component that measures the likelihood
of the status of the potential referent given a particular type of referring expression 
 the third  compatibility o  e   is the compatibility component that measures the
semantic and temporal compatibility between the object o and the referring expression
e 
next we explain these three components in detail 
      object selectivity
to calculate p  o s   gesture   we use a function that takes into consideration of the
distance between an object and the focus point of a gesture on the display  chai et al  
    b  
given an object from focus  i e   not selected by any gesture   p  o s   f ocus      n  
where n is the total number of objects that are in the focus vector  if an object is neither
selected by a gesture  nor in the focus  but visible on the screen  then p  o s   display   
  m   where m is the total number of objects that are in the display vector  currently 
we only applied the simplest uniform distribution for objects in focus and on the graphical
display  in the future  we intend to incorporate the recency in conversation discourse to
model p  o s   f ocus  and use visual prominence  e g   based on visual characteristics 
to model p  o s   display   note that  as discussed earlier in section      each object is
associated with only one of the three statuses  in other words  for a given object o  only
one of p  o s   gesture   p  o s   f ocus   and p  o s   display  is non zero 
      likelihood of status
motivated by the givenness hierarchy and earlier work  kehler        that the form of
referring expressions can reect the cognitive status of referred entities in a users mental
model  we use the likelihood of status to measure the probability of a reected status given
a particular type of referring expression  in particular  we use the data reported in kehler
       to derive the likelihood of the status of potential referents given a particular type
of referring expression p  s e   we categorize referring expressions into the following six
categories 
 empty  no referring expression is used in the utterance 
 pronouns  such as it  they  and them
 locative adverbs  such as here and there
 demonstratives  such as this  that  these  and those
 denite noun phrases  noun phrases with the denite article the
 full noun phrases  other types such as proper nouns 
  

fichai  prasov    qu

p  s e 
visible
focus
gesture
sum

empty
 
    
    
 

pronoun
 
    
    
 

locative
 
    
    
 

demonstratives
 
    
    
 

definite
 
    
    
 

full
 
    
    
 

table    likelihood of status of referents given a particular type of expression
table   shows the estimated p  s e   note that  in the original data provided by kehler
        there is zero count for a certain combination of a referring type and a referent status 
these zero counts result in zero probability in the table  we did not use any smoothing
techniques to re distribute the probability mass  furthermore  there is no probability mass
assigned to the status others 
      compatibility measurement
the term compatibility o  e  measures the compatibility between an object o and a referring
expression e  similar to the compatibility measurement in our earlier work  chai et al  
       it is dened by a multiplication of many factors in the following equation 
compatibility o  e    id o  e   sem o  e  



attrk  o  e   t emp o  e 

   

k

in this equation 
id o  e  it captures the compatibility between the identier  or name  for o and the identier
 or name  specied in e  it indicates that the identier of the potential referent  as
expressed in a referring expression  should match the identier of the true referent 
this is particularly useful for resolving proper nouns  for example  if the referring
expression is house number eight  then the correct referent should have the identier
number eight  id o  e      if the identities of o and e are dierent  id o  e      if the
identities of o and e are either the same or one both of them unknown 
sem o  e  it captures the semantic type compatibility between o and e  it indicates that the
semantic type of a potential referent as expressed in the referring expression should
match the semantic type of the correct referent  sem o  e      if the semantic types
of o and e are dierent  sem o  e      if they are the same or unknown 
attrk  o  e  it captures the type specic constraint concerning a particular semantic feature
 indicated by the subscript k   this constraint indicates that the expected features of
a potential referent as expressed in a referring expression should be compatible with
features associated with the true referent  for example  in the referring expression
the victorian house  the style feature is victorian  therefore  an object can only be a
possible referent if the style of that object is victorian  thus  we dene the following 
attrk  o  e      if both o and e have the feature k and the values of the feature k are
not equal  otherwise  attrk  o  e      
  

fiminimizing conflicts  a heuristic repair method

 house  
 house    house  
town   
town    town   
gesture input        i  i   i
speech input  compare it with these houses 
time

figure    an example of a complex input

t emp o  e  it captures the temporal compatibility between o and e  here we only consider the temporal ordering between speech and gesture  specically  the temporal
compatibility is dened as the following 
t emp o  e    exp  orderindex o   orderindex e   

   

the order when the speech and the accompanying gestures occur is important in
deciding which gestures should be aligned with which referring expressions  the
order in which the accompanying gestures are introduced into the discourse should
be consistent with the order in which the corresponding referring expressions are
uttered  for example  suppose a user input consists of three gestures g    g    g  and
two referring expressions  s    s    it will not be possible for g  to align with s  and
g  to align with s    note that  if the status of an object is either focus or visible 
then t emp o  e       this denition of temporal compatibility is dierent from the
function used in our previous work  chai et al         that takes real time stamps
into consideration  section     shows dierent performance results based on dierent
temporal compatibility functions 
    an example
figure   shows an example of a complex input that involves multiple referring expressions
and multiple gestures  because the interface displays house icons on top of town icons  a
point  or circle  could result in both a house and a town object  in this example  the rst
gesture results in both house   and town    the second gesture results in house   and
town    and the third results in house   and town    suppose before this input takes
place  house   is highlighted on the screen from the previous turn of conversation  i e  
house   is in the focus   furthermore  there are eight other objects visible on the screen 
to resolve referents to the expressions it and these houses  the greedy algorithm takes the
following steps 
 and r are created with lengths             respectively to
   the four input vectors  g  f  d 
represent six objects in the gesture vector  one object in the focus  eight more objects
on the graphical display  and two referring expressions used in the utterance 
   gesture matrix g     focus matrix f     and display matrix d   are created 
   these three matrixes are then initialized by equation    figure   shows the resulting
gesture matrix  the probability values of p  s e  come from table    the dierence
  

fichai  prasov    qu

status
 g 

referring expression match

potential
referent

j     

it

j      these houses

                 

i      house  

                     

gesture  
i      town  

              

              

i      house  

                     

                  

i      town  

              

              

gesture  
i      house  

                    

                     

i      town  

              

              

gesture  

 a  gesture matrix
status
 f 

potential
referent

referring expression match

focus

i      house  

j     

it

j      these houses

                 

 b  focus matrix

figure    the gesture matrix  a  and focus matrix  b  for processing the example in figure   
each cell in the referring expression match columns corresponds to an instantiation of
the matching function 

in the compatibility values for the house objects in the gesture matrix is mainly due
to the temporal ordering compatibilities 
   next the greedysortinggesture procedure is executed  for each row in gesture matrix  the algorithm nds the largest legitimate value and marks the corresponding cell
with    the legitimate means that the corresponding cell for the row i     has to
be either on the same column or the column to the right of the corresponding cell
in row i  these values are shown in bold in figure   a   next  starting from each
column  the algorithm checks for each referring expression whether any  exists in
its corresponding column  if so  those objects with  are assigned to the referring
expressions based on the number constraints  in this case  since no specic number is
given in the referring expression these houses  all three marked objects are assigned
to these houses 
   after these houses  there is still it left to be resolved  now the algorithm continues to
execute greedysortingfocus  the focus matrix prior to executing greedysortingfocus
is shown in figure   b   note that since these houses is no longer considered  its
corresponding column is deleted from the focus matrix  similar to the previous step 
the largest non zero match value is marked  shown in bold in figure   b   and assigned
to the remaining referring expression it 
   the resulting display matrix is not shown because at this point  all referring expressions are resolved 
  

fiminimizing conflicts  a heuristic repair method

s    the adj   n  n s 
s     this that  adj   n
s     these those  num    adj   n s
s    it this that  this that the adj  one
s     these those num  adj  ones them
s    here there
s    empty expression
s    proper nouns
s    multiple expressions
total num 

g 
no
gest 
 
 
 
 
 
 
 
 
 
  

g 
one
pt
 
  
 
 
 
 
 
 
 
  

g 
mult 
pts
 
 
 
 
 
 
 
 
 
  

g 
one
cir
 
  
  
  
 
 
 
 
  
  

g 
mult 
cirs
 
 
 
 
 
 
 
 
  
  

g 
pts  
cirs
 
 
 
 
 
 
 
 
 
  

total
num
  
  
  
  
 
 
 
  
  
   

table    detailed description of user referring behavior

   evaluation
we use the data collected from our previous work  chai et al         to evaluate this greedy
algorithm  the questions addressed in our evaluation are the following 
 what is the impact of temporal alignment between speech and gesture on the performance of the greedy algorithm 
 what is the role of modeling the cognitive status in the greedy algorithm 
 how eective is the greedy algorithm compared to the graph matching algorithm
 section      
 what error sources contribute to the failure in real time reference resolution 
 how is the greedy algorithm compared to the nite state approach  section      and
the decision list approach  section      
    experiment setup
the evaluation data were collected from eleven subjects who participated in our study 
each of the subjects was asked to interact with the system using both speech and gestures
 e g   pointing and circle  to accomplish ve tasks related to real estate information seeking 
the rst task was to nd the least expensive house in the most populated town  in order
to accomplish this task  the user would have to rst nd the town that has the highest
population and then nd the least expensive house in this town  the next task involved
obtaining a description of the house located in the previous task  the next task was to
compare the house that was located in the rst task with all of the houses in a particular
town in terms of price  additionally  the least expensive house in this second town should
be determined  another task was to nd the most expensive house in a particular town 
  

fichai  prasov    qu

s    no referring expression
s    one referring expression
s    multiple referring expressions
total num 

g    no
gesture
   a 
    a 
   c 
  

g    one
gesture
   a 
     b 
    c 
   

g    multigesture
   c 
    c 
    c 
  

total
num
 
   
  
   

table    summary of user referring behavior
the last task involved comparing the resulting houses of the previous four tasks  for this
last task  the previous four tasks may have to be completely or partially repeated  these
tasks were designed so that users were required to explore the interface to acquire various
types of information 
the acoustic model for each subject was trained individually to minimize speech recognition errors  the study session was videotaped to capture both audio and video on the
screen movement  including gestures and system responses   the ibm viavoice speech
recognizer was used to process each speech input 
table   provides a detailed description of the referring behavior observed in the study 
the columns indicate whether no gesture  one gesture  pointing or circle   or multiple gestures are involved in a multimodal input  the rows indicate the type of referring expressions
in a speech utterance  each table entry shows the number of a particular combination of
speech and gesture inputs 
table   summarizes table   in terms of whether no gesture  one gesture  or multiple
gestures  shown as columns  and whether no referring expression  one referring expression 
or multiple referring expressions  shown as rows  are involved in the input  note that in
this table an intended input is counted as one input even if this input may be split into a
few turns by our system during the run time 
based on table    we further categorize user inputs into the following three categories 
 simple inputs with one zero alignment  inputs that contain no speech referring
expression with no gesture  i e    s    g      one referring expression with zero gesture
 i e    s    g      and no referring expression with one gesture  i e     s    g     
these types of inputs require the conversation context or visual context to resolve
references  one example of this type is the u  in table    from our data  a total of
   inputs belong to this category  marked  a  in table    
 simple inputs with one one alignment  inputs that contain exactly one referring
expression and one gesture  i e     s    g      these types of inputs can be resolved
mostly by combining gesture and speech using multimodal fusion  a total of    
inputs belong to this category  marked  b  in table    
 complex inputs  inputs that contain more than one referring expression and or gesture  this corresponds to the entry   s    g       s    g      s    g     and
  s    g    in table    one example of this type is u  in table    a total of   
  

fiminimizing conflicts  a heuristic repair method

no  correctly resolved
simple one zero alignment
simple one one alignment
complex
total
accuracy

ordering
 
   
  
   
     

absolute
 
   
  
   
     

combined
 
   
  
   
     

table    performance comparison based on dierent temporal compatibility functions
inputs belong to this category  marked  c  in table     these types of inputs are
particularly challenging to resolve 
in this section  we will focus on dierent performance evaluations based on these three
types of referring behaviors 
    temporal alignment between speech and gesture
in multimodal interpretation  how to align speech and gesture based on their temporal
information is an important question  this is especially the case for complex inputs where
a multimodal input consists of multiple referring expressions and multiple gestures  we
evaluated dierent temporal compatibility functions for the greedy approach  in particular 
we compared the following three functions 
 the ordering temporal constraint as in equation   
 the absolute temporal constraint as dened by the following formula 
t emp o  e    exp  begint ime o   begint ime e   

   

here  the absolute timestamps of the potential referents  e g   indicated by a gesture 
and the referring expressions are used instead of the relative orders of relevant entities
in a user input 
 the combined temporal constraint that combines the two aforementioned constraints 
giving each equal weight in determining the compatibility score between an object
and a referring expression 
the results are shown in table    dierent temporal constraints only aect the processing of complex inputs  the ordering temporal constraint worked slightly better than the
absolute temporal constraint  in fact  temporal alignment between speech and gesture is often one of the problems that may aect interpretation results  previous studies have found
the gestures tend to occur before the corresponding speech unit takes place  oviatt et al  
       the ndings suggest that users tend to tap on the screen rst and then start the
speech utterance  this behavior was observed in a simple command based system  oviatt
et al         where each speech unit corresponds with a single gesture  i e   the simple inputs
in our work  
  

fichai  prasov    qu

non overlap
overlap
total  

speech first
  
  
   

gesture first
   
   
   

total
   
   
    

table    overall temporal relations between speech and gesture

from our study  we found that temporal alignment between gesture and corresponding
speech units is still an issue that needs to be further investigated in order to improve
the robustness in multimodal interpretation  table   shows the percentage of dierent
temporal relations observed in our study  the rows indicate whether there is an overlap
between speech referring expressions and their accompanied gestures  the columns indicate
whether the speech  more precisely  the referring expressions  or the gesture occurred rst 
consistent with the previous ndings  oviatt et al          in most cases      of time  
gestures occurred before the referring expressions were uttered  however  in     of the cases
the speech referring expressions were uttered before the corresponding gesture occurred 
among those cases     had an overlap between the referring expressions and the gesture
and    had no overlap 
furthermore  although multimodal behaviors such as sequential  i e   non overlap  or
simultaneous  e g   overlap  integration are quite consistent during the course of interaction  oviatt  coulston  tomko  xiao  bunsford  wesson    carmichael         there are
some exceptions  figure   shows the temporal alignments from individual users in our study 
user     user    and user   maintained a consistent behavior in that user  s gesture always
happened before and overlapped with the corresponding speech referring expressions  user
 s gesture always occurred ahead of the speech expressions without overlapping  and user
 s speech referring expressions always occurred before the corresponding gestures  without
any overlap   the other users exhibited varied temporal alignment between speech and
gesture during the interaction  it will be dicult for a system using pre dened temporal
constraints to anticipate and accommodate all these dierent behaviors  therefore  it is
desirable to have a mechanism that can automatically learn the user behavior of alignment
and automatically adjust to that behavior 
one potential approach is to introduce a calibration process before real human computer
interaction  in this calibration process  two tasks will be performed by a user  in the rst
task  the user will be asked to describe objects on the graph display with both speech
and deictic gestures  in the second task  the user will be asked to respond to the system
questions by using both speech and deictic gestures  the reason to have users perform
these two tasks is to identify whether there is any dierence between user initiated inputs
and system initiated user responses  based on these tasks  the temporal relations between
the speech units and corresponding gestures can be captured and used in the real time
interaction 
  

fiminimizing conflicts  a heuristic repair method

percentage of occurance

non overlap speech first
overlap speech first

non overlap gesture first
overlap gesture first

 
   
   
   
   
   
   
   
   
   
 
 

 

 

 

 

 

 

 

 

  

  

user index

figure    temporal alignment behavior from our user study

no  correctly resolved
simple one zero alignment
simple one one alignment
complex
total

with cognitive principles
 
   
  
   

without cognitive principles
 
  
  
   

table    the role of cognitive principles in the greedy algorithm

    the role of cognitive principles
to further examine the role of modeling cognitive status in multimodal reference  we compared the two congurations of the greedy algorithm  the rst conguration is based on the
matching score dened in equation    which incorporates the cognitive principles described
earlier  the second conguration only uses the matching score that is completely dependent on the compatibility between a referring expression and a gesture  i e   section       
without using the cognitive principles  i e   p  o s  and p  s e  are not included in equation
   
table   shows the comparison results in terms of these two congurations  the algorithm
using the cognitive principles outperforms the algorithm that does not use the cognitive
principles by more than      the performance dierence applies to both simple inputs
with one one alignment and complex inputs  the results indicate that modeling cognitive
status can potentially improve reference resolution performance 
  

fichai  prasov    qu

total num
total
simple one zero alignment
simple one one alignment
complex

   
  
   
  

graph matching
num  
   
     
 
     
   
     
  
     

greedy
num  
   
     
 
     
   
     
  
     

table    performance comparison between the graph matching algorithm and the greedy
algorithm

    greedy algorithm versus graph matching algorithm
we further compared the greedy algorithm and the graph matching algorithm in terms of
performance and runtime  table   shows the performance comparison  overall  the greedy
algorithm performs comparably with the graph matching algorithm 
to compare the runtime  we ran each algorithm on each user    times where each input
was run     times  in other words  each user input was run      times by each algorithm
to get the average runtime measurement  this experiment was done on a ultrasparc iii
server with    mhz and   bit 
both the greedy algorithm and the graph matching algorithm have the same function
calls to process speech inputs  e g   parsing  and gesture inputs  e g   identify potentially
intended objects   the dierence between these algorithms are the specic implementations
regarding graph creation and matching as in the graph matching algorithm and the greedy
search as in the greedy algorithm  as a result  the average time for the greedy algorithm
to process simple inputs and complex inputs are      milliseconds and      milliseconds
respectively  the average time for the graph matching algorithm to process simple and
complex inputs are      milliseconds and      milliseconds respectively  these results show
that on average the greedy algorithm runs slightly faster than the graph matching algorithm
given our current implementation  although in the worst case  the graph matching algorithm
is asymptotically more complex 
    real time error analysis
to understand the bottleneck in real time multimodal reference resolution  we examined
the error cases where the algorithm failed to provide correct referents 
like in most spoken dialog systems  speech recognition is a major bottleneck  although
we have trained each users acoustic model individually  the speech recognition rate is still
very low  only     of inputs had correctly recognized referring expressions  among these
inputs      of them were resolved with correct referents  fusing inputs from multiple
modalities together can sometimes compensate for the recognition errors  oviatt        
among    inputs in which referring expressions were incorrectly recognized     of them
were correctly assigned referents due to the mutual disambiguation  a mechanism to reduce
  

fiminimizing conflicts  a heuristic repair method

the recognition errors  especially by utilizing information from other modalities  will be
important to provide a robust solution for real time multimodal reference resolution 
the second source of errors comes from another common problem in most spoken dialog
systems  namely out of vocabulary words  for example  area was not in our vocabulary 
so the additional semantic constraint expressed by area was not captured  therefore  the
system could not identify whether a house or a town was referred to when the user uttered
this area  it is important for the system to have a capability to acquire knowledge  e g  
vocabulary  dynamically by utilizing information from other modalities and the interaction
context  furthermore  the errors also came from a lack of understanding of spatial relations
 as in the house just close to the red one  and superlatives  as in the most expensive house  
algorithms for aligning visual features to resolve spatial references are desirable  gorniak
  roy        
in addition to these two main sources  some errors are caused by unsynchronized inputs 
currently  we use an idle status  i e     seconds with no input from either speech or gesture 
as the boundary to delimit an interaction turn  two types of out of synchronization were
observed  the rst type is unsynchronized inputs from the user  such as a big pause between
speech and gesture  and the other comes from the underlying system implementation  the
system captures speech inputs and gesture inputs from two dierent servers through a
tcp ip protocol  a communication delay sometimes split one synchronized input into
two separate turns of inputs  e g   one turn was speech input alone and the other turn was
gesture input alone   a better engineering mechanism for synchronizing inputs is desired 
the disuencies from the users also accounted for a small number of errors  the current algorithm is incapable of distinguishing disuent cases from normal cases  fortunately 
the disuent situations did not occur frequently in our study  only   inputs with disuency   this is consistent with the previous ndings that speech disuency rate is lower in
human machine conversation than in spontaneous speech  brennan         during humancomputer conversation  users tend to speak carefully and utterances tend to be short  recent
ndings indicated that gesture patterns could be used as an additional source to identify
dierent types of speech disuencies during human human conversation  chen  harper   
quek         based on our limited cases  we found that gesture patterns could be indicators
of speech disuencies when they did occur  for example  if a user says show me the red
house  point to house a   the green house  still point to the house a   then the behavior of
pointing to the same house with dierent speech description usually indicates a repair  furthermore  gestures also involve disuencies  for example  repeatedly pointing to an object is
a gesture repetition  failure in identifying these disuencies caused problems with reference
resolution  it will be ideal to have a mechanism that can identify these disuencies using
multimodal information 
    comparative evaluation with two other approaches
to further examine how the greedy algorithm is compared to the nite state approach
 section      and the decision list approach  section       we conducted a comparative evaluation  in the original nite state approach  the n best speech hypotheses are maintained
in the speech tape  in our data here  we only had the best speech hypothesis for each speech
input  therefore  we manually updated some incorrectly recognized words so that the nite
  

fichai  prasov    qu

no  correctly resolved
simple inputs with one one alighment
simple inputs with zero one alighment
complex inputs
total

greedy
   
 
  
   

finite state
   
 
  
   

decision list
  
  
 
   

table    performance comparison with two other approaches
state approach would not be penalized because of the lack of n best speech hypotheses    
the modied data were used in all three approaches  table   shows the comparison results 
as shown in this table  the greedy algorithm correctly resolved more inputs than the
nite state approach and the decision list approach  the major problem with the nite state
approach is that it does not incorporate conversation context in the nite state transducer 
this problem contributes to the failure in resolving simple inputs with zero one alignment
and some of the complex inputs  the major problem with the decision list approach  as
described earlier  is the lack of capabilities to process ambiguous gestures and complex
inputs 
note that the greedy algorithm is not an algorithm to obtain the full semantic interpretation of a multimodal input  but rather it is an algorithm specically for reference
resolution  which uses information from context and gesture to resolve speech referring expressions  in this regard  the greedy algorithm is dierent from the nite state approach
whose goal is to get a full interpretation of user inputs and reference resolution is only a
part of this process 

   conclusion
motivated by earlier investigation on the cognitive status in human machine interaction  this
paper describes a greedy algorithm that incorporates the cognitive principles underlying human referring behavior to resolve a variety of references during human machine multimodal
interaction  in particular  this algorithm relies on the theories of conversation implicature
and givenness hierarchy to eectively guide the system in searching for potential referents  our empirical studies have shown that modeling the form of referring experssions and
its implication on the cognitive status can achieve better results than the algorithm that
only considers the compatibility between referring expressions and potential referents  this
greedy algorithm can eciently achieve comparable performance as a previous optimization
approach based on graph matching  furthermore  because this greedy algorithm handles
a variety of user inputs ranging from precise to ambiguous and from simple to complex 
it outperforms the nite state approach and the decision list approach in our experiments 
because of its simplicity and generality  this approach has a potential to improve the robustness of multimodal interpretation  we have learned from this investigation that prior
   note that we only corrected those inputs where there was a direct correspondence between the recognized
words and transcribed words to maintain the consistency of timestamps 

  

fiminimizing conflicts  a heuristic repair method

knowledge from linguistic and cognitive studies can be very benecial in designing ecient
and practical algorithms for enabling multimodal human machine communication 

acknowledgments
this work was supported by a nsf career award iis          the authors would like
to thank anonymous reviewers for their valuable comments and suggestions 

references
bolt  r          put that there  voice and gesture at the graphics interface  computer
graphics                 
brennan  s          processes that shape conversation and their implications for computational linguistics  in proceedings of   th annual meeting of acl  pp     
byron  d          resolving pronominal reference to abstract entities  in proceedings of
  th annual meeting of acl  pp       
cassell  j   bickmore  t   billinghurst  m   campbell  l   chang  k   vilhjalmsson  h    
yan  h          embodiment in conversational interfaces  rea  in proceedings of the
chi    pp         
chai  j   hong  p   zhou  m     prasov  z          optimization in multimodal interpretation  in proceedings of   nd annual meeting of association for computational
linguistics  acl   pp     
chai  j   prasov  z   blaim  j     jin  r          linguistic theories in ecient multimodal
reference resolution  an empirical study  in proceedings of the   th international
conference on intelligent user interfaces iui   pp       
chai  j   prasov  z     hong  p       a   performance evaluation and error analysis for
multimodal reference resolution in a conversational system  in proceedings of hltnaacl       companion volumn   pp       
chai  j  y   hong  p     zhou  m  x       b   a probabilistic approach to reference resolution in multimodal user interfaces  in proceedings of  th international conference
on intelligent user interfaces  iui   pp       
chen  l   harper  m     quek  f          gesture patterns during speech repairs  in
proceedings of international conference on multimodal interfaces  icmi   pp     
    
cohen  p          the pragmatics of referring and modality of communication  computational linguistics            
cohen  p   johnston  m   mcgee  d   oviatt  s   pittman  j   smith  i   chen  l     clow  j 
        quickset  multimodal interaction for distributed applications  in proceedings
of acm multimedia  pp       
eckert  m     strube  m          dialogue acts  synchronising units and anaphora resolution  in journal of semantics  vol         pp       
  

fichai  prasov    qu

gold  s     rangarajan  a          a graduated assignment algorithm for graph matching 
ieee trans  pattern analysis and machine intelligence                 
gorniak  p     roy  d          grounded semantic composition for visual scenes  journal
of artificial intelligence research             
grice  h  p          logic and conversation  in cole  p     morgan  j   eds    speech acts 
pp        new york  academic press 
grosz  b  j     sidner  c          attention  intention  and the structure of discourse 
computational linguistics                 
gundel  j  k   hedberg  n     zacharski  r          cognitive status and the form of
referring expressions in discourse  language                 
gustafson  j   bell  l   beskow  j   boye  j   carlson  r   edlund  j   granstrom  b   house 
d     wiren  m          adapt   a multimodal conversational dialogue system in
an apartment domain  in proceedings of  th international conference on spoken
language processing  icslp   vol     pp         
huls  c   bos  e     classen  w          automatic referent resolution of deictic and
anaphoric expressions  computational linguistics               
johnston  m          unication based multimodal parsing  in proceedings of colingacl    pp         
johnston  m     bangalore  s          finite state multimodal parsing and understanding 
in proceedings of coling    pp         
johnston  m   cohen  p   mcgee  d   oviatt  s   pittman  j     smith  i          unicationbased multimodal integration  in proceedings of acl    pp         
kehler  a          cognitive status and form of reference in multimodal human computer
interaction  in proceedings of aaai    pp         
koons  d  b   sparrell  c  j     thorisson  k  r          integrating simultaneous input
from speech  gaze  and hand gestures  in maybury  m   ed    intelligent multimedia
interfaces  pp          mit press 
neal  j  g     shapiro  s  c          intelligent multimedia interface technology  in sullivan 
j     tyler  s   eds    intelligent user interfaces  pp        acm  new york 
neal  j  g   thielman  c  y   dobes  z  h   m   s     shapiro  s  c          natural language
with integrated deictic and graphic gestures  in maybury  m     wahlster  w   eds   
intelligent user interfaces  pp        ca  morgan kaufmann press 
oviatt  s   coulston  r   tomko  s   xiao  b   bunsford  r   wesson  m     carmichael  l 
        toward a theory of organized multimodal integration patterns during humancomputer interaction  in proceedings of fifth international conference on multimodal
interfaces  pp       
oviatt  s   deangeli  a     kuhn  k          integration and synchronization of input
modes during multimodal human computer interaction  in proceedings of conference
on human factors in computing systems  chi    pp         
  

fiminimizing conflicts  a heuristic repair method

oviatt  s  l          multimodal interfaces for dynamic interactive maps  in proceedings
of conference on human factors in computing systems  chi    pp        
oviatt  s  l       a   multimodal system processing in mobile environments  in proceedings
of the thirteenth annual acm symposium on user interface software technology
 uist       pp       
oviatt  s  l       b   mutual disambiguation of recognition errors in a multimodal architecture  in proceedings of conference on human factors in computing systems 
chi    pp         
stent  a   dowding  j   gawron  j  m   bratt  e  o     moore  r          the commandtalk
spoken dialog system  in proceedings of acl    pp         
stock  o          alfresco  enjoying the combination of natural language processing and
hypermedia for information exploration  in maybury  m   ed    intelligent multimedia
interfaces  pp          mit press 
tsai  w  h     fu  k  s          error correcting isomorphism of attributed relational
graphs for pattern analysis  ieee trans  sys   man and cyb             
wahlster  w          user and discourse models for multimodal communication  in maybury  m     wahlster  w   eds    intelligent user interfaces  pp          acm press 
wu  l     oviatt  s          multimodal integration   a statistical view  ieee transactions
on multimedia                
zancanaro  m   stock  o     strapparava  c          multimodal interaction for information
access  exploiting cohesion  computational intelligence                 

  

fi