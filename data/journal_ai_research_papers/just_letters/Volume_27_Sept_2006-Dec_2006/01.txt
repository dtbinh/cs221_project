journal of artificial intelligence research                

submitted        published     

generative prior knowledge for discriminative classification
arkady epshteyn
gerald dejong

aepshtey uiuc edu
dejong uiuc edu

department of computer science
university of illinois at urbana champaign
    n  goodwin
urbana  il        usa

abstract
we present a novel framework for integrating prior knowledge into discriminative classifiers  our framework allows discriminative classifiers such as support vector machines
 svms  to utilize prior knowledge specified in the generative setting  the dual objective of
fitting the data and respecting prior knowledge is formulated as a bilevel program  which
is solved  approximately  via iterative application of second order cone programming  to
test our approach  we consider the problem of using wordnet  a semantic database of
english language  to improve low sample classification accuracy of newsgroup categorization  wordnet is viewed as an approximate  but readily available source of background
knowledge  and our framework is capable of utilizing it in a flexible way 

   introduction
while svm  vapnik        classification accuracy on many classification tasks is often
competitive with that of human subjects  the number of training examples required to
achieve this accuracy is prohibitively large for some domains  intelligent user interfaces 
for example  must adopt to the behavior of an individual user after a limited amount of
interaction in order to be useful  medical systems diagnosing rare diseases have to generalize
well after seeing very few examples  any natural language processing task that performs
processing at the level of n grams or phrases  which is frequent in translation systems 
cannot expect to see the same sequence of words a sufficient number of times even in large
training corpora  moreover  supervised classification methods rely on manually labeled
data  which can be expensive to obtain  thus  it is important to improve classification
performance on very small datasets  most classifiers are not competitive with humans
in their ability to generalize after seeing very few examples  various techniques have been
proposed to address this problem  such as active learning  tong   koller      b  campbell 
cristianini    smola         hybrid generative discriminative classification  raina  shen 
ng    mccallum         learning to learn by extracting common information from related
learning tasks  thrun        baxter        fink         and using prior knowledge 
in this work  we concentrate on improving small sample classification accuracy with
prior knowledge  while prior knowledge has proven useful for classification  scholkopf 
simard  vapnik    smola        wu   srihari        fung  mangasarian    shavlik       
epshteyn   dejong        sun   dejong         it is notoriously hard to apply in practice
because there is a mismatch between the form of prior knowledge that can be employed by
classification algorithms  either prior probabilities or explicit constraints on the hypothesis
c
    
ai access foundation  all rights reserved 

fiepshteyn   dejong

space of the classifier  and the domain theories articulated by human experts  this is
unfortunate because various ontologies and domain theories are available in abundance  but
considerable amount of manual effort is required to incorporate existing prior knowledge
into the native learning bias of the chosen algorithm  what would it take to apply an
existing domain theory automatically to a classification task for which it was not specifically
designed  in this work  we take the first steps towards answering this question 
in our experiments  such a domain theory is exemplified by wordnet  a linguistic
database of semantic connections among english words  miller         we apply wordnet to a standard benchmark task of newsgroup categorization  conceptually  a generative
model describes how the world works  while a discriminative model is inextricably linked to
a specific classification task  thus  there is reason to believe that a generative interpretation
of a domain theory would seem to be more natural and generalize better across different
classification tasks  in section   we present empirical evidence that this is  indeed  the
case with wordnet in the context of newsgroup classification  for this reason  we interpret
the domain theory in the generative setting  however  many successful learning algorithms
 such as support vector machines  are discriminative  we present a framework which allows
the use of generative prior in the discriminative classification setting 
our algorithm assumes that the generative distribution of the data is given in the
bayesian framework  p rob data model  and the prior p rob   model  are known  however 
instead of performing bayesian model averaging  we assume that a single model m  has
been selected a priori  and the observed data is a manifestation of that model  i e   it is
drawn according to p rob data m      the goal of the learning algorithm is to estimate
m    this estimation is performed as a two player sequential game of full information 
the bottom  generative  player chooses the bayes optimal discriminator function f  m   for
the probability distribution p rob data model   m    without taking the training data into
account  given the model m   the model m is chosen by the top  discriminative  player in
such a way that its prior probability of occurring  given by p rob   m    is high  and it forces
the bottom player to minimize the training set error of its bayes optimal discriminator
f  m    this estimation procedure gives rise to a bilevel program  we show that  while the
problem is known to be np hard  its approximation can be solved efficiently by iterative
application of second order cone programming 
the only remaining issue is how to construct the generative prior p rob   model  automatically from the domain theory  we describe how to solve this problem in section   
where we also argue that the generative setting is appropriate for capturing expert knowledge  employing wordnet as an illustrative example  in section    we give the necessary
preliminary information and important known facts and definitions  our framework for incorporating generative prior into discriminative classification is described in detail in section
   we demonstrate the efficacy of our approach experimentally by presenting the results
of using wordnet for newsgroup classification in section    a theoretical explanation of
the improved generalization ability of our discriminative classifier constrained by generative
prior knowledge appears in section    section   describes related work  section   concludes
the paper and outlines directions for future research 
  

figenerative prior knowledge for discriminative classification

   generative vs  discriminative interpretation of domain knowledge
wordnet can be viewed as a network  with nodes representing words and links representing
relationships between two words  such as synonyms  hypernyms  is a   meronyms  partof   etc    an important property of wordnet is that of semantic distance   the length
 in links  of the shortest path between any two words  semantic distance approximately
captures the degree of semantic relatedness of two words  we set up an experiment to
evaluate the usefulness of wordnet for the task of newsgroup categorization  each posting
was represented by a bag of words  with each binary feature representing the presence of
the corresponding word  the evaluation was done on pairwise classification tasks in the
following two settings 
   the generative framework assumes that each posting x    x        xn   is generated by
a distinct probability distribution for each newsgroup  the simplest version of a
linear discriminan analysis  lda  classifier posits that x  y       n       i  and
x  y       n      i  for posting x given label y          where i  r nn  is
the identity matrix  classification is done by assigning the most probable label to
x  y x       p rob x      p rob x       it is well known  e g  see duda  hart   
stork        that this decision rule is equivalent to the one given by the hyperplane
c        
cn   are estimated via
       t x      t     t           the means bi    
i
i
 
maximum likelihood from the training data  x    y          xm   ym    

   the discriminative svm classifier sets the separating hyperplane to directly minimize
the number of errors on the training data 
c        w
cn    bb    arg minw b kwk s t  yi  wt xi   b      i          m 
 w
b    w

our experiment was conducted in the learning to learn framework  thrun        baxter 
      fink         in the first stage  each classifier was trained using training data from the
training task  e g   for classifying postings into the newsgroups atheism and guns   in the
second stage  the classifier was generalized using wordnets semantic information  in the
third stage  the generalized classifier was applied to a different  test task  e g   for classifying
postings for the newsgroups atheism vs  mideast  without seeing any data from this new
classification task  the only way for a classifier to generalize in this setting is to use the
original sample to acquire information about wordnet  and then exploit this information
to help it label examples from the test sample  in learning how to perform this task  the
system also learns how to utilize the classification knowledge implicit in wordnet 
we now describe the second and third stages for the two classifiers in more detail 

   it is intuitive to interpret information embedded in wordnet as follows  if the title
of the newsgroup is guns  then all the words with the same semantic distance to
gun  e g   artillery  shooter  and ordnance with the distance of two  provide a
similar degree of classification information  to quantify this intuition  let li train  
j
 
n
 li train
      li train
      li train
  be the vector of semantic distances in wordnet between
each feature word j and the label of each training task newsgroup i          define
   the standard lda classifier assumes that x  y       n        and x  y       n        and
estimates the covariance matrix  as well as the means       from the training data  in our experiments 
we take    i 

  

fiepshteyn   dejong

  train  atheism vs  guns
  train  atheism vs  guns
  train  guns vs  mideast
test  atheism vs  mideast
test  guns vs  mideast
test  atheism vs  mideast
 

 

   

 

   

   

   

   

   

   

   

   

   

   

   

   

   

   
 

                                        

 

                                        

 

                                        

legend 

generative
discriminative

figure      test set accuracy as a percentage versus the number of training points for  
different classification experiments  for each classification task  a random test
set is chosen from the full set of articles in    different ways  error bars are
based on     confidence intervals 

p

i  v   

c
j
j
 v i
i train
j
 v 
 j li train
j l

  i         where      denotes cardinality of a set  i compresses

information in bi based on the assumption that words equidistant from the newsgroup
label are equally likely to appear in a posting from that newsgroup  to test the
performance of this compressed classifier on a new task with semantic distances given
j
   notice
by li test   the generative distributions are reconstructed via ji    i  li test
that if the classifier is trained and tested on the same task  applying the function  i
is equivalent to averaging the components of the means of the generative distribution
corresponding to the equivalence classes of words equidistant from the label  if the
classifier is tested on a different classification task  the reconstruction process reassigns
the averages based on the semantic distances to the new labels 
   it is less intuitive to interpret wordnet in a discriminative setting  one possible
interpretation is that coefficients w j of the separating hyperplane are governed by
semantic distances to labels  as captured by the compression function    v  u   
p

cj

w
j
j
 u
 v l
  train
  train
j
j
 j l  train
 v l  train
 u 
j l

j
j
and reconstructed via w j       l  test
  l  test
  

note that both the lda generative classifier and the svm discriminative classifier have
the same hypothesis space of separating hyperplanes  the resulting test set classification
accuracy for each classifier for a few classification tasks from the    newsgroup dataset
  

figenerative prior knowledge for discriminative classification

 blake   merz        is presented in figure      the x axis of each graph represents the
size of the training task sample  and the y axis   the classifiers performance on the test
classification task  the generative classifier consistently outperforms the discriminative
classifier  it converges much faster  and on two out of three tasks the discriminative classifier
is not able to use prior knowledge nearly as effectively as the generative classifier even after
seeing     of all of the available training data  the generative classifier is also more
consistent in its performance   note that its error bars are much smaller than those of
the discriminative classifier  the results clearly show the potential of using background
knowledge as a vehicle for sharing information between tasks  but the effective sharing is
contingent on an appropriate task decomposition  here supplied by the tuned generative
model 
the evidence in figure     seemingly contradicts the conventional wisdom that discriminative training outperforms generative for sufficiently large training samples  however  our
experiment evaluates the two frameworks in the context of using an ontology to transfer
information between learning tasks  this was never done before  the experiment demonstrates that the interpretation of semantic distance in wordnet is more intuitive in the
generative classification setting  probably because it better reflects the human intuitions
behind wordnet 
however  our goal is not just to construct a classifier that performs well without seeing
any examples of the test classification task  we also want a classifier that improves its
behavior as it sees new labeled data from the test classification task  this presents us
with a problem  one of the best performing classifiers  and certainly the best on the text
classification task according to the study by joachims        is svm  a discriminative
classifier  therefore  in the rest of this work  we focus on incorporating generative prior
knowledge into the discriminative classification framework of support vector machines 

   preliminaries
it has been observed that constraints on the probability measure of a half space can be
captured by second order cone constraints for gaussian distributions  see  e g   the tutorial
by lobo  vandenberghe  boyd    lebret         this allows for efficient processing of such
constraints within the framework of second order cone programming  socp   we intend
to model prior knowledge with elliptical distributions  a family of probability distributions
which generalizes gaussians  in what follows  we give a brief overview of second order
cone programming and its relationship to constraints imposed on the gaussian probability
distribution  we also note that it is possible to extend the argument presented by lobo et
al         to elliptical distributions 
second order cone program is a mathematical program of the form 
min v t x

     

x

s t  kai x   bi k  cti x   di   i           n

     

where x  rn is the optimization variable and v  rn   ai  r ki xn    bi  rki   ci  rn  
di  r are problem parameters  kk represents the usual l   norm in this paper   socps
can be solved efficiently with interior point methods  as described by lobo et al         in
a tutorial which contains an excellent overview of the theory and applications of socp 
  

fiepshteyn   dejong

we use the elliptical distribution to model distribution of the data a priori  elliptical
distributions are distributions with ellipsoidally shaped equiprobable contours  the density
function of the n variate elliptical distribution has the form f  g  x    c det    g  x 
 t    x      where x  rn is the random variable    rn is the location parameter 
  r nxn  is a positive definite  n  n  matrix representing the scale parameter  function
g   is the density generator  and c is the normalizing constant  we will use the notation x  e     g  to denote that the random variable x has an elliptical distribution
with parameters     g  choosing appropriate density generator functions g  the gaussian
distribution  the student t distribution  the cauchy distribution  the laplace distribution 
and the logistic distribution can be seen as special cases of the elliptical distribution  using an elliptical distribution relaxes the restrictive assumptions the user has to make when
imposing a gaussian prior  while keeping many desirable properties of gaussians  such as 
   if x  e     g   a  r kn    and b  rk   then ax   b  e a   b  aat   g 
   if x  e     g   then e x     
   if x  e     g   then v ar x    g   where g is a constant that depends on the
density generator g 
the following proposition shows that for elliptical distributions  the constraint p  w t x b 
      i e   the probability that x takes values in the half space  w t x   b     is less than
  is equivalent to a second order cone constraint for       
proposition
     if x  e     g   p rob w t x   b           is equivalent to  w t   
    
b  g    w  where g  is a constant which only depends on g and  
proof  the proof is identical to the one given by lobo        and lanckriet et al        
for gaussian distributions and is provided here for completeness 
assume p rob w t x   b       

     

let u   w t x b  let u denote the mean of u  and  denote its variance  then the constraint
    can be written as
uu
u
     
p rob          



 

by the properties of elliptical distributions  u   w t    b     g     w  and uu


t

 
e       g   thus  statement     above can be expressed as p robxe     g   x   w  b
    w
k
gk
t
  which is equivalent to  w  b
       where  z    p robxe     g   x  z   the
    w
k
gk

proposition follows with g    g      

proposition      for any monotonically decreasing g  p robxe   g   x    is equivalent


to      x     g c    where g c     g       
c   is a constant which only depends on
g  c     
proof  follows directly from the definition of p robxe   g   x  
  

figenerative prior knowledge for discriminative classification

   generative prior via bilevel programming
we deal with the binary classification task  the classifier is a function f  x  which maps
instances x  rn to labels y          in the generative setting  the probability densities
p rob x y          and p rob x y          parameterized by             are provided  or
estimated from the data   along with the prior probabilities on class labels  y      and
 y       and the bayes optimal decision rule is given by the classifier
f  x     sign p rob x y          y       p rob x y          y       
where sign x       if x    and   otherwise  in lda  for instance  the parameters    and
  are the means of the two gaussian distributions generating the data given each label 
informally  our approach to incorporating prior knowledge is straightforward  we assume
a two level hierarchical generative probability distribution model  the low level probability
distribution of the data given the label p rob x y    is parameterized by   which  in turn 
has a known probability distribution p rob      the goal of the classifier is to estimate the
values of the parameter vector  from the training set of labeled points  x     y       xm   ym   
this estimation is performed as a two player sequential game of full information  the
bottom  generative  player  given   selects the bayes optimal decision rule f  x    the
top  discriminative  player selects the value of  which has a high probability of occurring
 according to p rob      and which will force the bottom player to select the decision rule
which minimizes the discriminative error on the training set  we now give a more formal
specification of this training problem and formulate it as a bilevel program  some of the
assumptions are subsequently relaxed to enforce both tractability and flexibility 
we use an elliptical distribution e         g  to model x y      and another elliptical
distribution e         g  to model x y      if the parameters i   i   i        are known 
the bayes optimal decision rule restricted to the class of linear classifiers   of the form
fw b  x    sign w t x   b  is given by f  x  which minimizes the probability of error among all
linear discriminants  p rob error    p rob w t x   b    y      y        p rob w t x   b 
  y      y            p robxe       g   wt x   b       p robxe       g   wt x   b      
assuming equal prior probabilities for both classes  we now model the uncertainty in the
means of the elliptical distributions i   i        by imposing elliptical prior distributions on
the locations of the means  i  e ti   i   g   i         in addition  to ensure the optimization
problem is well defined  we maximize the margin of the hyperplane subject to the imposed
generative probability constraints 
min kwk

     

    

s t yi  wt xi   b      i          m

p robi e ti  i  g   i      i       

     
     
t

t

 w  b  solves min p robxe       g   w x   b       p robxe       g   w x   b     
w b

     
this is a bilevel mathematical program  i e   an optimization problem in which the
constraint region is implicitly defined by another optimization problem   which is strongly
   a decision rule restricted to some class of classifiers h is optimal if its probability of error is no larger
than that of any other classifier in h  tong   koller      a  

  

fiepshteyn   dejong

np hard even when all the constraints and both objectives are linear  hansen  jaumard 
  savard         however  we show that it is possible to solve a reasonable approximation of this problem efficiently with several iterations of second order cone programming 
first  we relax the second level minimization       by breaking it up into two constraints 
p robxe       g   wt x   b       and p robxe       g   wt x   b        thus  instead of looking for the bayes optimal decision boundary  the algorithm looks for a decision
boundary with low probability of error  where low error is quantified by the choice of  
propositions     and     enable us to rewrite the optimization problem resulting from
this relaxation as follows  
min kwk

     

      w b

s t yi  wt xi   b      i          m




    
 i  ti      i       
p robi e ti  i  g   i      i         i

w t     b

p robxe       g   wt x   b         
       
  w 

w t     b

p robxe       g   wt x   b        
       
  w 

     
     
     

     

notice that the form of this program does not depend on the generator function g of the
elliptical distribution   only constants  and  depend on it   defines how far the system
is willing to deviate from the prior in its choice of a generative model  and  bounds the
tail probabilities of error  type i and type ii  which the system will tolerate assuming its
chosen generative model is correct  these constants depend both on the specific generator
g and the amount of error the user is willing to tolerate  in our experiments  we select
the values of these constants to optimize performance  unless the user wants to control
the probability bounds through these constants  it is sufficient to assume a priori only that
probability distributions  both prior and hyper prior  are elliptical  without making any
further commitments 
our algorithm solves the above problem by repeating the following two steps 
   fix the top level optimization parameters   and     this step combines the objectives of maximizing the margin of the classifier on the training data and ensuring that
the decision boundary is  approximately  bayes optimal with respect to the given
generative probability densities specified by the        
   fix the bottom level optimization parameters w  b  expand the feasible region of the
program in step   as a function of         this step fixes the decision boundary and
pushes the means of the generative distribution as far away from the boundary as the
constraint       will allow 
the steps are repeated until convergence  in practice  convergence is detected when the
optimization parameters do not change appreciably from one iteration to the next   each
step of the algorithm can be formulated as a second order cone program 
  

figenerative prior knowledge for discriminative classification

step    fix   and     removing unnecessary constraints from the mathematical
program above and pushing the objective into constraints  we get the following socp 
min

      

w b

s t   kwk

      

yi  wt xi   b      i          m

      

wt 

  b

 
       
  w 

      

w t     b


       
  w 

      

step    fix w  b and expand the span of the feasible region  as measured by
t
w    b  
     
  w

t
w    b
     
  w



removing unnecessary constraints  we get 
w t     b w t     b
  

max 
     
          
  w 
   w 



    
 i  ti      i       
s t  i

      
      

the behavior of the algorithm is illustrated in figure     
the following theorems state that the algorithm converges 
theorem
     suppose
n
o that the algorithm produces a sequence of iterates


 t   t 
 t 
 t 
  and the quality of each iterate is evaluated by its margin w t   
        w   b
t  
this evaluation function converges 
 t 

 t 

 t 

 t 

proof  let       be the values of the prior location parameters  and w    b  be the
minimum error hyperplane the algorithm finds at the end of the t th step  at the end of
 t     t   
the  t      st step  w 
  b 
is still in the feasible region of the t th step socp  this
 t   t 

 t 
   b
     t  
w

 

is true because the function f    w

 t   t 

 t 
   b
     t  
w

 

    w

  

 w t   t    b t 
      t  
  w 



 w t   t    b t 
      t  
  w 

is monotonically increasing in each one of its arguments when the other argument is fixed 
 t     t   
and fixing    or     fixes exactly one argument  if the solution  
   
at the end
 t   

of the  t      st step were such that
 t   

fixing  

 t 

t
 t 
 w t 
 b

   
      t  
  w 

    then f could be increased by

 t 

and using the value of   from the beginning of the step which ensures that

 t   t 
 t 
 w
   b

      t  
  w 

   which contradicts the observation that f is maximized at the end of
 t   

the second step  the same contradiction is reached if 

t
 t 
 w t 
 b

   
      t  
  w 

    since the

minimum error hyperplane from the previous
iteration is in the feasible region at the start

 t 


must decrease monotonically from one iteration
of the next iteration  the objective w
to the next  since it is bounded below by zero  the algorithm converges 
  

fiepshteyn   dejong

  

  

 

 

 

 

 

 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

 
 

 

 

 

 

  
 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 
 

 

 

 

 

 

 

 
 

 

 

 

 

 

 

figure      steps of the iterative  hard margin  socp procedure 
 the region where the hyperprior probability is larger than  is shaded for each prior
distribution  the covariance matrices are represented by equiprobable elliptical contours 
in this example  the covariance matrices of the hyperprior and the prior distributions are
multiples of each other  data points from two different classes are represented by diamonds
and squares  
   data  prior  and hyperprior before the algorithm is executed 
   hyperplane discriminator at the end of step    iteration  
   priors at the end of step    iteration  
   hyperplane discriminator at the end of step    iteration  
the algorithm converges at the end of step   for this problem  step   does not move the
hyperplane  
in addition to the convergence of the objective function  the accumulation points of the
sequence of iterates can be characterized by the following theorem 
n
o
 t   t 
theorem      the accumulation points of the sequence         w t    b t   i e   limiting
points of its convergent subsequences  have no feasible descent directions for the original
optimization problem given by             
proof  see appendix a 
  

figenerative prior knowledge for discriminative classification

if a point has no feasible descent directions  then any sufficiently small step along any
directional vector will either increase the objective function  leave it unchanged  or take the
algorithm outside of the feasible region  the set of points with no feasible descent directions
is a subset of the set of local minima  hence  convergence to such a point is a somewhat
weaker result than convergence to a local minimum 
in practice  we observed rapid convergence usually within     iterations 
finally  we may want to relax the strict assumptions of the correctness of the prior linear
separability of the data by introducing slack variables into the optimization problem above 
this results in the following program 
min

      w b i            

kwk   c 

m
x

i   c             c          

      

i  

s t yi  wt xi   b      i   i          m


    

 i  ti       i   i       
i

w t     b 
     
    w     



t
w     b 
     
   w     

i     i          m
i     i       

i     i       

      
      
      
      
      
      
      

as before  this problem can be solved with the two step iterative socp procedure 
imposing the generative prior with soft constraints ensures that  as the amount of training
data increases  the data overwhelms the prior and the algorithm converges to the maximummargin separating hyperplane 

   experiments
the experiments were designed both to demonstrate the usefulness of the proposed approach
for incorporation of generative prior into discriminative classification  and to address a
broader question by showing that it is possible to use an existing domain theory to aid in
a classification task for which it was not specifically designed  in order to construct the
generative prior  the generative lda classifier was trained on the data from the training
classification task to estimate the gaussian location parameters bi   i         as described
in section    the compression function i  v  is subsequently computed  also as described
j
in section     and is used to set the hyperprior parameters via ji    i  li test
   i        
in order to apply a domain theory effectively to the task for which it was not specifically
designed  the algorithm must be able to estimate its confidence in the decomposition of the
domain theory with respect to this new learning task  in order to model the uncertainty in
applicability of wordnet to newsgroup categorization  our system estimated its confidence in
homogeneity of equivalence classes of semantic distances by computing the variance of each
  

fiepshteyn   dejong

    
bilevel gen discr

   
    
   
    
   
    
   
   

    

   

        
svm

    

   

    

figure      performance of the bilevel discriminative classifier constrained by generative
prior knowledge versus performance of svm  each point represents a unique
pair of training test tasks  with      of the test task data used for training 
the results are averaged over     experiments 

p

random variable i  v  as follows  i  v   

j l

c
 ji i  v   
j
i train v
j
 j li tran
 v 

  the hyperprior confidence

matrices i   i        were then reconstructed
with respect to the test task semantic distances

j
i  li test
   k   j
  identity matrices were used as
li test   i        as follows   i  j k   
   k    j
covariance matrices of the lower level prior           i  the rest of the parameters
were set as follows                     c    c        c       these constants were
chosen manually to optimize performance on experiment    for the training task  atheism
vs  guns  test task  guns vs  mideast  see figure      without observing any data from any
other classification tasks 
the resulting classifier was evaluated in different experimental setups  with different
pairs of newsgroups chosen for the training and the test tasks  to justify the following
claims 
   the bilevel generative discriminative classifier with wordnet derived prior knowledge has good low sample performance  showing both the feasibility of automatically
interpreting the knowledge embedded in wordnet and the efficacy of the proposed
algorithm 
   the bilevel classifiers performance improves with increasing training sample size 
   integrating generative prior into the discriminative classification framework results
in better performance than integrating the same prior directly into the generative
framework via bayes rule 
  

figenerative prior knowledge for discriminative classification

   the bilevel classifier outperforms a state of the art discriminative multitask classifier
proposed by evgeniou and pontil        by taking advantage of the wordnet domain
theory 
in order to evaluate the low sample performance of the proposed classifier  four newsgroups
from the    newsgroup dataset were selected for experiments  atheism  guns  middle east 
and auto  using these categories  thirty experimental setups were created for all the possible
ways of assigning newsgroups to training and test tasks  with a pair of newsgroups assigned
to each task  under the constraint that the training and test pairs cannot be identical      in
each experiment  we compared the following two classifiers 
   our bilevel generative discriminative classifier with the knowledge transfer functions
i  v   i  v   i        learned from the labeled training data provided for the training task  using     of all the available data for that task   the resulting prior was
subsequently introduced into the discriminative classification framework via our approximate bilevel programming approach
   a vanilla svm classifier which minimizes the regularized empirical risk 
min

w b i

m
x
i  

i   c  kwk 

s t yi  wt xi   b      i   i          m

     
     

both classifiers were trained on      of all the available data from the test classification
task    and evaluated on the remaining       of the test task data  the results  averaged
over one hundred randomly selected datasets  are presented in figure      which shows the
plot of the accuracy of the bilevel generative discriminative classifier versus the accuracy
of the svm classifier  evaluated in each of the thirty experimental setups  all the points
lie above the   o line  indicating improvement in performance due to incorporation of prior
knowledge via the bilevel programming framework  the amount of improvement ranges
from     to      with all of the improvements being statistically significant at the   
level 
the next experiment was conducted to evaluate the effect of increasing training data
 from the test task  on the performance of the system  for this experiment  we selected
three newsgroups  atheism  guns  and middle east  and generated six experimental setups
based on all the possible ways of splitting these newsgroups into unique training test pairs 
in addition to the classifiers   and   above  the following classifiers were evaluated 
   a state of the art multi task classifier designed by evgeniou and pontil         the
classifier learns a set of related classification functions ft  x    wtt x   bt for classification tasks t   training task  test task  given m t  data points  x t   y t         xm t t   ym t t  
   newsgroup articles were preprocessed by removing words which could not be interpreted as nouns by
wordnet  this preprocessing ensured that only one part of wordnet domain theory was exercised and
resulted in virtually no reduction in classification accuracy 
   sedumi software  sturm        was used to solve the iterative socp programs 

  

fiepshteyn   dejong

for each task t by minimizing the regularized empirical risk 
min

w   wt  bt  it

x
x m t 
t

i  

it  

c  x
kwt  w  k    c  kw  k 
c  t

s t  yit  wtt xit   bt       it   i          m t   t
it     i          m t   t

     
     
     

the regularization constraint captures a tradeoff between final models w t being close
to the average model w  and having a large margin on the training data      of the
training task data was made available to the classifier  constant c       was chosen 
and c          was selected from the set                                       to optimize
the classifiers performance on experiment    for the training task  atheism vs  guns 
test task  guns vs  mideast  see figure      after observing      of the test task data
 in addition to the training task data  
   the lda classifier described in section   trained on     of the test task data  since
this classifier is the same as the bottom level generative classifier used in the bilevel
algorithm  its performance gives an upper bound on the performance of the bottomlevel classifier trained in a generative fashion 
figure     shows performance of classifiers     as a function of the size of the training
data from the test task  evaluation was done on the remaining test task data   the results
are averaged over one hundred randomly selected datasets  the performance of the bilevel
classifier improves with increasing training data both because the discriminative portion of
the classifier aims to minimize the training error and because the generative prior is imposed
with soft constraints  as expected  the performance curves of the classifiers converge as the
amount of available training data increases  even though the constants used in the mathematical program were selected in a single experimental setup  the classifiers performance
is reasonable for a wide range of data sets across different experimental setups  with the
possible exception of experiment    training task  guns vs  mideast  testing task  atheism
vs  mideast   where the means of the constructed elliptical priors are much closer to each
other than in the other experiments  thus  the prior is imposed with greater confidence
than is warranted  adversely affecting the classifiers performance 
the multi task classifier   outperforms the vanilla svm by generalizing from data points
across classification tasks  however  it does not take advantage of prior knowledge  while our
classifier does  the gain in performance of the bilevel generative discriminative classifier
is due to the fact that the relationship between the classification tasks is captured much
better by wordnet than by simple linear averaging of weight vectors 
because of the constants involved in both the bilevel classifier and the generative classifiers with bayesian priors  it is hard to do a fair comparison between classifiers constrained
by generative priors in these two frameworks  instead  the generatively trained classifier  
gives an empirical upper bound on the performance achievable by the bottom level classifier
trained generatively on the test task data  the accuracy of this classifier is shown as as
a horizontal in the plots in figure      since discriminative classification is known to be
superior to generative classification for this problem  the svm classifier outperforms the
  

figenerative prior knowledge for discriminative classification

   train atheism vs  guns

   train atheism vs  guns

   train guns vs  mideast

test atheism vs  mideast

test guns vs  mideast

test atheism vs  guns

 

 

 

    

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   

   

    

    
 

 

                              

   train  guns vs  mideast

  

  

  

  

  

  

  

  

  

   train  atheism vs  mideast

    
 

 

 

 

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   

   

    

    

    

   

   
 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

test atheism vs  guns

    

    

  

   train  atheism vs  mideast

test guns vs  mideast

test atheism vs  mideast

  

   

    

    
 

                              

 

  

  

  

  

  

  

  

  

  

legend 

lda max performance
bilevel gen discr
svm
multitask svm

figure      test set accuracy as a percentage versus number of test task training points for
two classifiers  svm and bilevel gen discr  tested on six different classification
tasks  for each classification experiment  the data set was split randomly into
training and test sets in     different ways  the error bars based on    
confidence intervals 

generative classifier given enough data in four out of six experimental setups  what is more
interesting  is that  for a range of training sample sizes  the bilevel classifier constrained
by the generative prior outperforms both the svm trained on the same sample and the
generative classifier trained on a much larger sample in these four setups  this means that 
unless prior knowledge outweighs the effect of learning  it cannot enable the lda classifier
to compete with our bilevel classifier on those problems 
finally  a set of experiments was performed to determine the effect of varying mathematical program parameters  and  on the generalization error  each parameter was
varied over a set of values  with the rest of the parameters held fixed   was increased up
to its maximum feasible value   the evaluation was done in the setup of experiment    for
  

fiepshteyn   dejong

   accuracy as a function of 

   accuracy as a function of 

    

    

    

    

    

    

    

    

   

   

    

    

    

    

    

    

    

    

 

 
 

    

   

    

   

 

                                   

 

figure      plots of test set accuracy as percentage versus mathematical program parameter
values  for each classification task  a random training set of size   was chosen
from the full set of test task articles in     different ways  error bars are based
on     confidence intervals  all the experiments were performed on the training
task  atheism vs  guns  test task  guns vs  mideast 

the training task atheism vs  guns  test task  guns vs  mideast   with the training set size
of   points  the results are presented in figure      increasing the value of  is equivalent
to requiring a hyperplane separator to have smaller error given the prior  decreasing the
value of  is equivalent to increasing the confidence in the hyperprior  both of these actions
tighten the constraints  i e   decrease the feasible region   with good prior knowledge  this
should have the effect of improving generalization performance for small training samples
since the prior is imposed with higher confidence  this is precisely what we observe in the
plots of figure     

   generalization performance
why does the algorithm generalize well for low sample sizes  in this section  we derive a
theorem which demonstrates that the convergence rate of the generalization error of the
constrained generative discriminative classifier depends on the parameters of the mathematical program and not just the margin  as would be expected in the case of large margin
classification without the prior  in particular  we show that as the certainty of the generative prior knowledge increases  the upper bound on the generalization error of the classifier
constrained by the prior decreases  by increasing certainty of the prior  we mean that
either the hyper prior becomes more peaked  i e   the confidence in the locations of the
prior means increases  or the desired upper bounds on the type i and type ii probabilities
of error of the classifier decrease  i e   the requirement that the lower level discriminative
player choose the restricted bayes optimal hyperplane is more strictly enforced  
the argument proceeds by bounding the fat shattering dimension of the classifier constrained by prior knowledge  the fat shattering dimension of a large margin classifier is
given by the following definition  taylor   bartlett        
definition      a set of points s    x     xm   is  shattered by a set of functions f
mapping from a domain x to r if there are real numbers r          rm such that  for each
b        m   there is a function fb in f with b fb  xi    ri      i      m  we say
  

figenerative prior knowledge for discriminative classification

that r          rm witness the shattering  then the fat shattering dimension of f is a function
fatf    that maps  to the cardinality of the largest  shattered set s 
specifically  we consider the class of functions
f    x  w t x   kxk  r  kwk     

     





wt     
w t  
    


   
    




 
 

t
 
 

t
 



 



 
 
 
      
 
 
     
     
   w 
   w 

the following theorem bounds the fat shattering dimension of our classifier 

theorem      let f be the class of a priori constrained functions defined by        and
let min  p   and max  p   denote the minimum and maximum eigenvalues of matrix p  
 
 
    
  where
respectively  if a set of points s is  shattered by f   then  s    r     
 
 

 

    
    
  k  max      
  kt ktk 
 
   max         with     min  min
  and     min  min
 
k  k
k  k
max        kt  k 
kt  k   max       
  
kt  k  max         kt  k 

assuming that      kti k  kti  i k  and i 

    i
 

       

proof  see appendix b 
we have the following corollary which follows directly from taylor and bartletts       
theorem     and bounds the classifiers generalization error based on its fat shattering
dimension 
corollary      let g be a class of real valued functions  then  with probability at least
    over m independently generated examples z  if a classifier h   sgn g   sgn g  has
 
margin at least  on all the examples in z  then the error of h is no more than m
 d 

 m
 log   m 
 
log 
  
where
d
 
f
at
 
  
if
g
 
f
is
the
class
of
functions
log   em
g
g
d

  
 

 

 

defined by        then df     r                if g   f   is the usual class of large margin
classifiers  without the prior   then the result in  taylor   bartlett        shows that d f   
   r 
 
 
 

notice that both bounds depend on r
  however  the bound of the classifier constrained
 
by the generative prior also depends on  and  through the term                in particular  as  increases  tightening the constraints  the bound decreases  ensuring  as expected 
quicker convergence of the generalization error  similarly  decreasing  also tightens the
constraints and decreases the upper bound on the generalization error  for         the
factor              is less than   and the upper bound on the fat shattering dimension df
is tighter than the usual bound in the no prior case on df    
since  controls the amount of deviation of the decision boundary from the bayesoptimal hyperplane and  depends on the variance of the hyper prior distribution  tightening
of these constraints corresponds to increasing our confidence in the prior  note that a high
value  represents high level of user confidence in the generative elliptical model  also
note that there are two ways of increasing the tightness of the hyperprior constraint      
  one is through the user defined parameter   the other is through the automatically
estimated covariance matrices i   i         these matrices estimate the extent to which the
  

fiepshteyn   dejong

equivalence classes defined by wordnet create an appropriate decomposition of the domain
theory for the newsgroup categorization task  thus  tight constraint       represents both
high level of user confidence in the means of the generative classification model  estimated
from wordnet  and a good correspondence between the partition of the words imposed
by the semantic distance of wordnet and the elliptical generative model of the data  as
 approaches zero and  approaches its highest feasible value  the solution of the bilevel
mathematical program reduces to the restricted bayes optimal decision boundary computed
solely from the generative prior distributions  without using the data 
hence  we have shown that  as the prior is imposed with increasing level of confidence
 which means that the elliptical generative model is deemed good  or the estimates of
its means are good  which in turn implies that the domain theory is well suited for the
classification task at hand   the convergence rate of the generalization error of the classifier
increases  intuitively  this is precisely the desired effect of increased confidence in the prior
since the benefit derived from the training data is outweighed by the benefit derived from
prior knowledge  for low data samples  this should result in improved accuracy assuming
the domain theory is good  which is what the plots in figure     show 

   related work
there are a number of approaches to combining generative and discriminative models  several of these focus on deriving discriminative classifiers from generative distributions  tong
  koller      a  tipping        or on learning the parameters of generative classifiers via
discriminative training methods  greiner   zhou        roos  wettig  grunwald  myllymaki    tirri         the closest in spirit to our approach is the maximum entropy
discrimination framework  jebara        jaakkola  meila    jebara         which performs
discriminative estimation of parameters of a generative model  taking into account the constraints of fitting the data and respecting the prior  one important difference with our
framework is that  in estimating these parameters  maximum entropy discrimination minimizes the distance between the generative model and the prior  subject to satisfying the
discriminative constraint that the training data be classified correctly with a given margin 
our framework  on the other hand  maximizes the margin on the training data subject to
the constraint that the generative model is not too far from the prior  this emphasis on
maximizing the margin allows us to derive a priori bounds on the generalization error of
our classifier based on the confidence in the prior which are not  yet  available for the maximum entropy framework  another difference is that our approach performs classification
via a single generative model  while maximum entropy discrimination averages over a set of
generative models weighted by their probabilities  this is similar to the distinction between
maximum a posteriori and bayesian estimation and has repercussions for tractability  maximum entropy discrimination  however  is more general than our framework in a sense of
allowing a richer set of behaviors based on different priors 
ng et al               explore the relative advantages of discriminative and generative
classification and propose a hybrid approach which improves classification accuracy for
both low sample and high sample scenarios  collins        proposes to use the viterbi
algorithm for hmms for inferencing  which is based on generative assumptions   combined
with a discriminative learning algorithm for hmm parameter estimation  these research
  

figenerative prior knowledge for discriminative classification

directions are orthogonal to our work since they do not explicitly consider the question of
integration of prior knowledge into the learning problem 
in the context of support vector classification  various forms of prior knowledge have
been explored  scholkopf et al         demonstrate how to integrate prior knowledge about
invariance under transformations and importance of local structure into the kernel function 
fung et al         use domain knowledge in form of labeled polyhedral sets to augment
the training data  wu and srihari        allow domain experts to specify their confidence
in the examples label  varying the effect of each example on the separating hyperplane
proportionately to its confidence  epshteyn and dejong        explore the effects of rotational constraints on the normal of the separating hyperplane  sun and dejong       
propose an algorithm which uses domain knowledge  such as wordnet  to identify relevant
features of examples and incorporate resulting information in form of soft constraints on
the hypothesis space of svm classifier  mangasarian et al         suggest the use of prior
knowledge for support vector regression  in all of these approaches  prior knowledge takes
the form of explicit constraints on the hypothesis space of the large margin classifier  in this
work  the emphasis is on generating such constraints automatically from domain knowledge
interpreted in the generative setting  as we demonstrate with our wordnet application 
generative interpretation of background knowledge is very intuitive for natural language
processing problems 
second order cone constraints have been applied extensively to model probability constraints in robust convex optimization  lobo et al         bhattacharyya  pannagadatta   
smola        and constraints on the distribution of the data in minimax machines  lanckriet
et al         huang  king  lyu    chan         our work  as far as we know  is the first one
which models prior knowledge with such constraints  the resulting optimization problem
and its connection with bayes optimal classification is very different from the approaches
mentioned above 
our work is also related to empirical bayes estimation  carlin   louis         in empirical bayes estimation  the hyper prior parameters of the generative model are estimated
using statistical estimation methods  usually maximum likelihood or method of moments 
through the marginal distribution of the data  while our approach learns those parameters
discriminatively using the training data 

   conclusions and future work 
since many sources of domain knowledge  such as wordnet  are readily available  we believe
that significant benefit can be achieved by developing algorithms for automatically applying
their information to new classification problems  in this paper  we argued that the generative paradigm for interpreting background knowledge is preferable to the discriminative
interpretation  and presented a novel algorithm which enables discriminative classifiers to
utilize generative prior knowledge  our algorithm was evaluated in the context of a complete system which  faced with the newsgroup classification task  was able to estimate the
parameters needed to construct the generative prior from the domain theory  and use this
construction to achieve improved performance on new newsgroup classification tasks 
in this work  we restricted our hypothesis class to that of linear classifiers  extending
the form of the prior distribution to distributions other than elliptical and or looking for
  

fiepshteyn   dejong

bayes optimal classifiers restricted to a more expressive class than that of linear separators
may result in improvement in classification accuracy for non linearly separable domains 
however  it is not obvious how to approximate this more expressive form of prior knowledge
with convex constraints  the kernel trick may be helpful in handling nonlinear problems 
assuming that it is possible to represent the optimization problem exclusively in terms of
dot products of the data points and constraints  this is an important issue which requires
further study 
we have demonstrated that interpreting domain theory in the generative setting is
intuitive and produces good empirical results  however  there are usually multiple ways
of interpreting a domain theory  in wordnet  for instance  semantic distance between
words is only one measure of information contained in the domain theory  other  more
complicated  interpretations might  for example  take into account types of links on the
path between the words  hypernyms  synonyms  meronyms  etc   and exploit commonsense observations about wordnet such as words that are closer to the category label
are more likely to be informative than words farther away  comparing multiple ways of
constructing the generative prior from the domain theory and  ultimately  selecting one of
these interpretations automatically is a fruitful direction for further research 

acknowledgments
the authors thank the anonymous reviewers for valuable suggestions on improving the paper  this material is based upon work supported in part by the national science foundation
under award nsf iis          and in part by the information processing technology office of the defense advanced research projects agency under award hr               
any opinions  findings  and conclusions or recommendations expressed in this publication
are those of the authors and do not necessarily reflect the views of the national science
foundation or the defense advanced research projects agency 

appendix a  convergence of the generative discriminative algorithm
let the map h   z  z determine an algorithm that  given a point       generates a se
quence  t  t   of iterates through the iteration  t      h  t     the iterative algorithm
 t 

 t 

in section   generates a sequence of iterates  t              z by applying the following
map h 
h   h    h   
 a   
in step    h               arg

min

 w b u          

kwk  

with the set u            defined by constraints 
t

 a   
 a   

yi  w xi   b         i          m

 a   

c   w  b              


wt  b
 
with the conic constraints cs  w  b        s
k    wk

 a   

c   w  b              

  

 a   

figenerative prior knowledge for discriminative classification

in step    h   w  b    arg

min

       v

 c   w  b            c   w  b          

 a   

with the set v given by the constraints
  o         t      

 a   

  o         t      

 a   



with o     t           t  
notice that h  and h  are functions because the minima for optimization problems
              and               are unique  this is the case because step   optimizes a
strictly convex function on a convex set  and step   optimizes a linear non constant function
on a strictly convex set 
convergence of the objective function   t      min w b u    t    t     kwk of the algorithm
 
 
was shown in theorem      let  denote the set of points on which the map h does not
change the value of the objective function  i e       h            we will
show that every accumulation point of   t    lies in   we will also show that every point
           augmented with  w    b     h             is a point with no feasible descent
directions for the optimization problem              which can be equivalently expressed as 
min kwk s t           v    w  b   u           

      w b

 a    

in order to formally state our result  we need a few concepts from the duality theory 
let a constrained optimization problem be given by
min f  x  s t  ci  x      i          k
x

 a    

the following conditions  known as karush kuhn tucker kkt  conditions are necessary
for x to be a local minimum 
proposition a    if x is a local minimum of  a      then         k such that
p
   f  x     ki   i ci  x  
   i    for i          k 

   ci  x      for i          k 
   i ci  x       for i          k 
        k are known as lagrange multipliers of constraints c        ck  
the following well known result states that kkt conditions are sufficient for x  to be
a point with no feasible descent directions 
proposition a    if         k such that the following conditions are satisfied at x  
p
   f  x     ki   i ci  x  
  

fiepshteyn   dejong

   i    for i          k 
then x has no feasible descent directions in the problem  a    
proof   sketch  we reproduce the proof given in a textbook by fletcher         the proposition is true because for any p
feasible direction vector s  st ci  x     for x and for i 
t

        k   hence  s f  x     ki   i st ci  x       so s is not a descent direction 
the following lemma characterizes the points in the set  

lemma a    let     and let  w    b     h      be the optimizer of      and let
     a             a    m    a       a      be a set of lagrange multipliers corresponding to the
constraints for the solution  w    b    define     h     and let  w     b    be the optimizer of
      if           then  a        for some    if           then  a        for some   
if both         and           then  a       a        for some   
proof  consider the case when
       

 a    

      

 a    

and
since     kw  k   kw  k  let   be a set of lagrange multipliers corresponding to the
constraints for the solution  w     b     since w is still feasible for the optimization problem
given by       by the argument in theorem      and the minimum of this problem is
unique  this can only happen if
 w    b       w   b   
 a    
then  w   b   and   must satisfy kkt conditions for        a     implies that
c   w              c   w             by the same argument as in theorem      which means
that  by kkt condition     for      
  a        

 a    

therefore  by kkt condition     for      and  a      at  w  b             w   w    b  
b                
 
 
 
 
 
 


m
c   w b        
c   w b        
kwk
x
y
x
i
i
w
    a    c   ww
 
  a    i
    a    c   ww
 
  b     
kwk
 b       
 
 
yi
b

b

i  

b

which means that kkt conditions         for the optimization problem      are satisfied
 
at the point  w    b   with       kkt condition     is satisfied by feasibility of  w    b  
and kkt condition     is satisfied by the same condition for      and observations  a     
 a      and  a     
the proofs for the other two cases                   and                     are
analogous 
the following theorem states that the points in  are kkt points  i e   points at which
kkt conditions are satisfied  for the optimization problem given by  a     
  

figenerative prior knowledge for discriminative classification

theorem a    if    and let  w    b     h       then  w    b           is a kkt point for
the optimization problem given by  a     
proof  let     h     just like in lemma a    we only consider the case
         

 a    

         a         by lemma a    

 a    

 the proofs for the other two cases are similar  
by kkt conditions for h   w   b    at       


 o         t  
c   w   b          
   a  
for some  a      
 
 

 a    

by kkt conditions for h      and  a      at  w  b     w    b  
 

kwk
w
kwk
b

 

 

m
x
i  

 a    i



yi x i
yi



 

 a   

 

c   w b        
w
c   w  b       
b

 





for some 


 a     
  
 a    m
 a   





    


 a    

by  a      a      a      and  a      at  w  b             w   b               


 c   w b        
kwk


 kwk 
 
y
x
w
i
i
w
 kwk 
w
  b      
m

c
 w
 
 
x
 
 yi 
 b     



   

 
 kwk    
b

 


 a   
 a    i    
    

c   w  b         


   
i  
 
kwk
 
 
 
 








c   w b        
 
 
w 

 c   w  b        




 
 


   
 
b
 a    a    
    a    
 o 
 
 t  
   
 a    




 


 
 
 o       t  
c   w  b        
 
 

 

which means that kkt conditions         for the optimization problem  a     are satisfied
  
at the point  w    b           with      a             a    m    a       a       a    a       a      
  

 also satisfies kkt conditions         by assumption  a     and the kkt conditions for
h  and h   
in order to prove convergence properties of the iterates  t    we use the following theorem
due to zangwill        
theorem a    let the map h   z  z determine an iterative algorithm via  t     
h  t     let    denote the objective function  and let  be the set of points on which the
map h does not change the value of the objective function  i e       h         
suppose
  

fiepshteyn   dejong

   h is uniformly compact on z  i e  there is a compact subset z   z such that
h    z  for   z 
   h is strictly monotonic on z    i e   h         
   h is closed on z    i e  if wi  w and h wi      then    h w  
then the accumulation points of the sequence of  t  lie in  
the following proposition shows that minimization of a continuous function on a feasible
set which is a continuous map of the functions argument forms a closed function 
proposition a    given
   a real valued continuous function f on a  b 
   a point to set map u   a   b continuous with respect to the hausdorff metric  
dist x  y     max d x  y    d y  x    where d x  y     maxxx minyy kx  yk 
define the function f   a  b by
f  a    arg min f  a  b       b   f  a  b    f  a  b    for b   u  a   
b  u  a 

assuming the minimum exists and is unique  then  the function f is closed at a 
proof  this proof is a minor modification of the one given by gunawardana and byrne
        let  a t    be a sequence in a such that
a t   a  f  a t     b

 a    

the function f is closed at a if f  a    b  suppose this is not the case  i e  b    f  a   
arg minb  u  a  f  a  b     therefore 
b   arg min f  b    such that f  a  b    f  a  b 
b  u  a 

 a    

by continuity of f      and  a     
f  a t    f  a t      f  a  b 

 a    

by continuity of u    and  a     
dist u  a t     u  a       b t   b and b t   u  at    for t 

 a    

 a       a      and  a     imply that
k such that f  a t    f  a t       f  a t    b t     for t   k

 a    

which is a contradiction since by assumption  f  a t      arg minb  u  at   f  b    and by  a     
b t   u  a t    
   a point to set map u  a  maps a point a to a set of points  u  a  is continuous with respect to a distance
metric dist iff a t   a implies dist u  a t     u  a      

  

figenerative prior knowledge for discriminative classification

proposition a    the function h defined by  a     a    is closed 
proof  let   t    be a sequence such that  t      since all the iterates  t  lie in
the closed feasible region bounded by constraints             and the boundary of u    is
piecewise linear in   the boundary of u   t    converges uniformly to the boundary of u    
as  t      which implies that the hausdorff distance between the boundaries converges
to zero  since the hausdorff distance between convex sets is equal to the hausdorff distance
between their boundaries  dist u   t     u      also converges to zero  hence  proposition
a   implies that h  is closed  the same proposition implies that h  is closed  a composition
of closed functions is closed  hence h is closed 
we now prove the main result of this section 
theorem      let h be the function defined by  a     a    which determines the generative discriminative algorithm via  t      h  t     then accumulation points  of the
sequence  t  augmented with  w    b     h      have no feasible descent directions for the
original optimization problem given by             
proof  the proof is by verifying that h satisfies the properties of theorem a    closedness
of h was shown in proposition a    strict monotonicity of   t    was shown in theorem
     since all the iterates  t  are in the closed feasible region bounded by constraints             h is uniformly compact on z  since all the accumulation points  lie in   they are
kkt points of the original optimization problem by theorem a    and  therefore  have no
feasible descent directions by proposition a   

appendix b  generalization of the generative discriminative classifier
we need a few auxiliary results before proving theorem      the first proposition bounds
the angle of rotation between two vectors w    w  and the distance between them if the angle
of rotation between each of these vectors and some reference vector v is sufficiently small 
proposition b    let kw  k   kw  k   kvk      if w t v      and w t v       then
   w t w        
p
   kw   w  k           

proof 

   by the triangle inequality  arccos w t w     arccos w t v    arccos w t v     arccos  
 since the angle between two vectors is a distance measure   taking cosines of both
sides and using trigonometric equalities yields w t w         
   expand kw   w  k    kw  k    kw  k    w t w         w t w     since w t w        
from part    kw   w  k            

the next proposition bounds the angle of rotation between two vectors t and  if they
are not too far away from each other as measured by the l   norm distance 
  

fiepshteyn   dejong

proposition b    let ktk     k  tk     then

tt 
ktkkk



     
      

proof  expanding k  tk    ktk    kk    tt  and using k  tk        we get
  ktk
    kk

kk
ktk

 

tt 
ktkkk



 
 ktkkk    we now use the triangle inequality     ktk  k  tk  kk 
ktk   k  tk      and simplify 

the following proposition will be used to bound the angle of rotation between the normal
w of the separating hyperplane and the mean vector t of the hyper prior distribution 
wt 
kwkkk
ktk   
ktk  ktk    

proposition b    let
where    min  

     and k  tk    ktk  then

wt t
kwkktk

         

proof  follows directly from propositions b    part    and b   
we now prove theorem      which relies on parts of the well known proof of the fatshattering dimension bound for large margin classifiers derived by taylor and bartlett
       
theorem      let f be the class of a priori constrained functions defined by      and
let min  p   and max  p   denote the minimum and maximum eigenvalues of matrix p  
 
 
    
respectively  if a set of points s is  shattered by f   then  s    r     
  where
 
 

 

    
    
  k  max      
  kt ktk 
 
  and     min  min
   max         with     min  min
 
k  k
k  k
max        kt  k 
kt  k   max       
  
kt  k  max         kt  k 

assuming that      kti k  kti  i k  and i 

    i
 

       



proof  first  we use the inequality min  p   kwk  p     w  max  p   kwk to relax the
constraints
w t  
w t  
 

 min     
 b   
     
kwk
   w 



    

  max      
 b   
      t       k   t  k 
min   
   



    
wt  

the constraints imposed by the second prior                t      are relaxed

in a similar fashion to produce 

  w

wt     
 min     
kwk

 b   

k   t  k  max     

 b   

now  we show that if the assumptions made in the statement of the theorem hold  then
 
 
 
p
p
every subset so  s satisfies k so   s  s   k   r          
assume that s is  shattered by f   the argument used by taylor and bartlett       
in lemma     shows that  by the definition of fat shattering  there exists a vector w   such
that
x
x
w   
so 
 s  s       s   
 b   
  

figenerative prior knowledge for discriminative classification

similarly  reversing the labeling of s  and s   s     there exists a vector w  such that
x
x
w     s  s    
so     s   
 b   
hence   w   w    
implies that

p

so 

p

 s  s         s    which  by cauchy schwartz inequality 

   s  
p
kw   w  k  p
k so   s  s   k

 b   

the constraints on the classifier represented in b   and b   imply by proposition b   that
w t t 
w t t 
 
 
kw  kkt  k          and kw  kkt  k            now  applying proposition b    part    and
simplifying  we get
q
kw   w  k   

            

applying the same analysis to the constraints b   and b    we get
q
kw   w  k                
combining b    b    and b    we get
x

x
 s  


so 
 s  s     p

           

 b   

 b   

 b    

with  as defined in the statement of the theorem 
taylor and bartletts        lemma     proves  using the probabilistic method  that
some so  s satisfies
x
 p
x


 b    
so 
 s  s      s r 

combining b    and b    yields  s  

 r           
 
 

references
baxter  j          a model of inductive bias learning  journal of artificial intelligence
research             
bhattacharyya  c   pannagadatta  k  s     smola  a          a second order cone programming formulation for classifying missing data  in nips 
blake 
c  
 
merz 
c 
       
  
newsgroups
http   people csail mit edu people jrennie   newsgroups   

database 

campbell  c   cristianini  n     smola  a          query learning with large margin classifiers  in proceedings of the seventeenth international conference on machine learning 
carlin  b     louis  t          bayes and empirical bayes methods for data analysis 
chapman and hall 
collins  m          discriminative training methods for hidden markov models  theory
and experiments with perceptron algorithms  in proceedings of      conference on
empirical methods in natural language processing 
  

fiepshteyn   dejong

duda  r   hart  p     stork  d          pattern classification  john wiley   nd edition 
epshteyn  a     dejong  g          rotational prior knowledge for svms  in proceedings
of the sixteenth european conference on machine learning 
evgeniou  t     pontil  m          regularized multi task learning  in proceedings of the
tenth acm sigkdd international conference on knowledge discovery and data
mining 
fink  m          object classification from a single example utilizing class relevance metrics 
in advances in neural information processing systems 
fletcher  r          practical methods of optimization  john wiley and sons  west sussex 
england 
fung  g   mangasarian  o     shavlik  j          knowledge based support vector machine
classifiers  in advances in neural information processing systems 
greiner  r     zhou  w          structural extension to logistic regression  discriminative
parameter learning of belief net classifiers  in proceedings of the eighteenth national
conference on artificial intelligence 
gunawardana  a     byrne  w          convergence theorems for generalized alternating
minimization procedures  journal of machine learning research              
hansen  p   jaumard  b     savard  g          new branch and bound rules for linear bilevel
programming  siam journal on scientific and statistical computing               
huang  k   king  i   lyu  m  r     chan  l          the minimum error minimax probability
machine  journal of machine learning research              
jaakkola  t   meila  m     jebara  t          maximum entropy discrimination  in advances
in neural information processing systems 
jebara  t          machine learning  discriminative and generative  kluwer academic
publishers 
joachims  t          text categorization with support vector machines  learning with many
relevant features  in proceedings of the tenth european conference on machine learning 
lanckriet  g  r  g   ghaoui  l  e   bhattacharyya  c     jordan  m  i          minimax
probability machine  in advances in neural information processing systems 
lobo  m  s   vandenberghe  l   boyd  s     lebret  h          applications of second order
cone programming  linear algebra and its applications                   
mangasarian  o   shavlik  j     wild  e          knowledge based kernel approximation 
journal of machine learning research 
miller  g          wordnet  an online lexical database  international journal of lexicography        
ng  a  y     jordan  m  i          on discriminative vs  generative classifiers  a comparison
of logistic regression and naive bayes  in advances in neural information processing
systems 
  

figenerative prior knowledge for discriminative classification

raina  r   shen  y   ng  a  y     mccallum  a          classification with hybrid generative discriminative models  in advances in neural information processing systems 
roos  t   wettig  h   grunwald  p   myllymaki  p     tirri  h          on discriminative
bayesian network classifiers and logistic regression  machine learning             
scholkopf  b   simard  p   vapnik  v     smola  a          prior knowledge in support
vector kernels  advances in kernel methods   support vector learning 
sturm  j  f          using sedumi       a matlab toolbox for optimization over symmetric cones  optimization methods and software             
sun  q     dejong  g          explanation augmented svm  an approach to incorporating
domain knowledge into svm learning  in proceedings of the twenty second international conference on machine learning 
taylor  j  s     bartlett  p          generalization performance of support vector machines
and other pattern classifiers  in advances in kernel methods  support vector learning 
thrun  s          is learning the n th thing any easier than learning the first   in advances
in neural information processing systems 
tipping  m  e          sparse bayesian learning and the relevance vector machine  journal
of machine learning research            
tong  s     koller  d       a   restricted bayes optimal classifiers  in proceedings of the
seventeenth national conference on artificial intelligence 
tong  s     koller  d       b   support vector machine active learning with applications
to text classification  in proceedings of the seventeenth international conference on
machine learning 
vapnik  v          the nature of statistical learning theory  springer verlag 
wu  x     srihari  r          incorporating prior knowledge with weighted margin support
vector machines  in proceedings of the tenth acm sigkdd international conference
on knowledge discovery and data mining 
zangwill  w          convergence conditions for nonlinear programming algorithms  management science          

  

fi