journal of artificial intelligence research                  

submitted        published      

anytime point based approximations for large pomdps
joelle pineau

jpineau   cs   mcgill   ca

school of computer science
mcgill university
montreal qc  h a  a  canada

geoffrey gordon

ggordon   cs   cmu   edu

machine learning department
carnegie mellon university
pittsburgh pa        usa

sebastian thrun

thrun   stanford   edu

computer science department
stanford university
stanford ca        usa

abstract
the partially observable markov decision process has long been recognized as a rich framework for real world planning and control problems  especially in robotics  however exact solutions in this framework are typically computationally intractable for all but the smallest problems 
a well known technique for speeding up pomdp solving involves performing value backups at
specific belief points  rather than over the entire belief simplex  the efficiency of this approach 
however  depends greatly on the selection of points  this paper presents a set of novel techniques
for selecting informative belief points which work well in practice  the point selection procedure
is combined with point based value backups to form an effective anytime pomdp algorithm called
point based value iteration  pbvi   the first aim of this paper is to introduce this algorithm and
present a theoretical analysis justifying the choice of belief selection technique  the second aim of
this paper is to provide a thorough empirical comparison between pbvi and other state of the art
pomdp methods  in particular the perseus algorithm  in an effort to highlight their similarities and
differences  evaluation is performed using both standard pomdp domains and realistic robotic
tasks 

   introduction
the concept of planning has a long tradition in the ai literature  fikes   nilsson        chapman 
      mcallester   roseblitt        penberthy   weld        blum   furst         classical
planning is generally concerned with agents which operate in environments that are fully observable 
deterministic  finite  static  and discrete  while these techniques are able to solve increasingly
large state space problems  the basic assumptions of classical planningfull observability  static
environment  deterministic actionsmake these unsuitable for most robotic applications 
planning under uncertainty aims to improve robustness by explicitly reasoning about the type of
uncertainty that can arise  the partially observable markov decision process  pomdp   astrom 
      sondik        monahan        white        lovejoy      b  kaelbling  littman    cassandra        boutilier  dean    hanks        has emerged as possibly the most general representation
for  single agent  planning under uncertainty  the pomdp supersedes other frameworks in terms
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fip ineau   g ordon   t hrun

of representational power simply because it combines the most essential features for planning under
uncertainty 
first  pomdps handle uncertainty in both action effects and state observability  whereas many
other frameworks handle neither of these  and some handle only stochastic action effects  to handle partial state observability  plans are expressed over information states  instead of world states 
since the latter ones are not directly observable  the space of information states is the space of all
beliefs a system might have regarding the world state  information states are easily calculated from
the measurements of noisy and imperfect sensors  in pomdps  information states are typically
represented by probability distributions over world states 
second  many pomdp algorithms form plans by optimizing a value function  this is a powerful approach to plan optimization  since it allows one to numerically trade off between alternative
ways to satisfy a goal  compare actions with different costs rewards  as well as plan for multiple
interacting goals  while value function optimization is used in other planning approachesfor example markov decision processes  mdps   bellman       pomdps are unique in expressing
the value function over information states  rather than world states 
finally  whereas classical and conditional planners produce a sequence of actions  pomdps
produce a full policy for action selection  which prescribes the choice of action for any possible
information state  by producing a universal plan  pomdps alleviate the need for re planning  and
allow fast execution  naturally  the main drawback of optimizing a universal plan is the computational complexity of doing so  this is precisely what we seek to alleviate with the work described
in this paper
most known algorithms for exact planning in pomdps operate by optimizing the value function
over all possible information states  also known as beliefs   these algorithms can run into the wellknown curse of dimensionality  where the dimensionality of planning problem is directly related to
the number of states  kaelbling et al          but they can also suffer from the lesser known curse
of history  where the number of belief contingent plans increases exponentially with the planning
horizon  in fact  exact pomdp planning is known to be pspace complete  whereas propositional
planning is only np complete  littman         as a result  many pomdp domains with only a few
states  actions and sensor observations are computationally intractable 
a commonly used technique for speeding up pomdp solving involves selecting a finite set
of belief points and performing value backups on this set  sondik        cheng        lovejoy 
    a  hauskrecht        zhang   zhang         while the usefulness of belief point updates
is well acknowledged  how and when these backups should be applied has not been thoroughly
explored 
this paper describes a class of point based value iteration  pbvi  pomdp approximations
where the value function is estimated based strictly on point based updates  in this context  the
choice of points is an integral part of the algorithm  and our approach interleaves value backups
with steps of belief point selection  one of the key contributions of this paper is the presentation
and analysis of a set of heuristics for selecting informative belief points  these range from a naive
version that combines point based value updates with random belief point selection  to a sophisticated algorithm that combines the standard point based value update with an estimate of the error
bound between the approximate and exact solutions to select belief points  empirical and theoretical evaluation of these techniques reveals the importance of taking distance between points into
consideration when selecting belief points  the result is an approach which exhibits good perfor   

fia nytime p oint based a pproximations for l arge pomdp s

mance with very few belief points  sometimes less than the number of states   thereby overcoming
the curse of history 
the pbvi class of algorithms has a number of important properties  which are discussed at
greater length in the paper 
 theoretical guarantees  we present a bound on the error of the value function obtained by
point based approximation  with respect to the exact solution  this bound applies to a number
of point based approaches  including our own pbvi  perseus  spaan   vlassis         and
others 
 scalability  we are able to handle problems on the order of     states  which is an order of magnitude larger than problems solved by more traditional pomdp techniques  the
empirical performance is evaluated extensively in realistic robot tasks  including a search formissing person scenario 
 wide applicability  the approach makes few assumptions about the nature or structure of the
domain  the pbvi framework does assume known discrete state  action observation spaces
and a known model  i e   state to state transitions  observation probabilities  costs rewards  
but no additional specific structure  e g   constrained policy class  factored model  
 anytime performance  an anytime solution can be achieved by gradually alternating phases
of belief point selection and phases of point based value updates  this allows for an effective
trade off between planning time and solution quality 
while pbvi has many important properties  there are a number of other recent pomdp approaches which exhibit competitive performance  braziunas   boutilier        poupart   boutilier 
      smith   simmons        spaan   vlassis         we provide an overview of these techniques in the later part of the paper  we also provide a comparative evaluation of these algorithms
and pbvi using standard pomdp domains  in an effort to guide practitioners in their choice of
algorithm  one of the algorithms  perseus  spaan   vlassis         is most closely related to pbvi
both in design and in performance  we therefore provide a direct comparison of the two approaches
using a realistic robot task  in an effort to shed further light on the comparative strengths and weaknesses of these two approaches 
the paper is organized as follows  section   begins by exploring the basic concepts in pomdp
solving  including representation  inference  and exact planning  section   presents the general
anytime pbvi algorithm and its theoretical properties  section   discusses novel strategies to select good belief points  section   presents an empirical comparison of pomdp algorithms using
standard simulation problems  section   pursues the empirical evaluation by tackling complex robot
domains and directly comparing pbvi with perseus  finally  section   surveys a number of existing
pomdp approaches that are closely related to pbvi 

   review of pomdps
partially observable markov decision processes provide a general planning and decision making
framework for acting optimally in partially observable domains  they are well suited to a great
number of real world problems where decision making is required despite prevalent uncertainty 
they generally assume a complete and correct world model  with stochastic state transitions  imperfect state tracking  and a reward structure  given this information  the goal is to find an action
   

fip ineau   g ordon   t hrun

strategy which maximizes expected reward gains  this section first establishes the basic terminology and essential concepts pertaining to pomdps  and then reviews optimal techniques for pomdp
planning 
    basic pomdp terminology
formally  a pomdp is defined by six distinct quantities  denoted  s  a  z  t  o  r   the first three
of these are 
 states  the state of the world is denoted s  with the finite set of all states denoted by s  
 s    s            the state at time t is denoted st   where t is a discrete time index  the state is
not directly observable in pomdps  where an agent can only compute a belief over the state
space s 
 observations  to infer a belief regarding the worlds state s  the agent can take sensor measurements  the set of all measurements  or observations  is denoted z    z    z            the
observation at time t is denoted zt   observation zt is usually an incomplete projection of the
world state st   contaminated by sensor noise 
 actions  to act in the world  the agent is given a finite set of actions  denoted a  
 a    a            actions stochastically affect the state of the world  choosing the right action as
a function of history is the core problem in pomdps 
throughout this paper  we assume that states  actions and observations are discrete and finite 
for mathematical convenience  we also assume that actions and observations are alternated over
time 
to fully define a pomdp  we have to specify the probabilistic laws that describe state transitions
and observations  these laws are given by the following distributions 
 the state transition probability distribution 
t  s  a  s       p r st   s    st    s  at    a  t 

   

is the probability of transitioning to state s    given that the agent is in state s and selects action a  for any  s  a  s     since t is a conditional probability distribution  we have
p
 
s  s t  s  a  s         s  a   as our notation suggests  t is time invariant 
 the observation probability distribution 
o s  a  z     p r zt   z   st    s  at    a  t 

   

is the probability that the agent will perceive observation z upon executing action a in state s 
p
this conditional probability is defined for all  s  a  z  triplets  for which zz o s  a  z   
    s  a   the probability function o is also time invariant 
finally  the objective of pomdp planning is to optimize action selection  so the agent is given
a reward function describing its performance 
   

fia nytime p oint based a pproximations for l arge pomdp s

 the reward function  r s  a    s  a     assigns a numerical value quantifying the
utility of performing action a when in state s  we assume the reward is bounded  rmin  
r   rmax   the goal of the agent is to collect as much reward as possible over time  more
precisely  it wants to maximize the sum 
e 

t
x

 tt  rt   

   

t t 

where rt is the reward at time t  e    is the mathematical expectation  and  where        
is a discount factor  which ensures that the sum in equation   is finite 
these items together  the states s  actions a  observations z  reward r  and the probability
distributions  t and o  define the probabilistic world model that underlies each pomdp 
    belief computation
pomdps are instances of markov processes  which implies that the current world state  st   is sufficient to predict the future  independent of the past  s    s         st     the key characteristic that
sets pomdps apart from many other probabilistic models  such as mdps  is the fact that the state
st is not directly observable  instead  the agent can only perceive observations  z            zt    which
convey incomplete information about the worlds state 
given that the state is not directly observable  the agent can instead maintain a complete trace
of all observations and all actions it ever executed  and use this to select its actions  the action observation trace is known as a history  we formally define
ht     a    z            zt    at    zt  

   

to be the history at time t 
this history trace can get very long as time goes on  a well known fact is that this history
does not need to be represented explicitly  but can instead be summarized via a belief distribution  astrom         which is the following posterior probability distribution 
bt  s     p r st   s   zt   at    zt            a    b    

   

this of course requires knowing the initial state probability distribution 
b   s     p r s    s  

   

which defines the probability that the domain is in state s at time t      it is common either to
specify this initial belief as part of the model  or to give it only to the runtime system which tracks
beliefs and selects actions  for our work  we will assume that this initial belief  or a set of possible
initial beliefs  are available to the planner 
because the belief distribution bt is a sufficient statistic for the history  it suffices to condition
the selection of actions on bt   instead of on the ever growing sequence of past observations and
actions  furthermore  the belief bt at time t is calculated recursively  using only the belief one time
step earlier  bt    along with the most recent action at  and observation zt  
   

fip ineau   g ordon   t hrun

we define the belief update equation       as 
  bt    at    zt     bt  s   
x

 

o s    at    zt   t  s  at    s    bt   s 

s 

p r zt  bt    at   

   

where the denominator is a normalizing constant 
this equation is equivalent to the decades old bayes filter  jazwinski         and is commonly
applied in the context of hidden markov models  rabiner         where it is known as the forward
algorithm  its continuous generalization forms the basis of kalman filters  kalman        
it is interesting to consider the nature of belief distributions  even for finite state spaces  the
belief is a continuous quantity  it is defined over a simplex describing the space of all distributions
over the state space s  for very large state spaces  calculating the belief update  eqn    can be computationally challenging  recent research has led to efficient techniques for belief state computation
that exploit structure of the domain  dean   kanazawa        boyen   koller        poupart  
boutilier        thrun  fox  burgard    dellaert         however  by far the most complex aspect of pomdp planning is the generation of a policy for action selection  which is described next 
for example in robotics  calculating beliefs over state spaces with     states is easily done in realtime  burgard et al          in contrast  calculating optimal action selection policies exactly appears
to be infeasible for environments with more than a few dozen states  kaelbling et al          not
directly because of the size of the state space  but because of the complexity of the optimal policies 
hence we assume throughout this paper that the belief can be computed accurately  and instead
focus on the problem of finding good approximations to the optimal policy 
    optimal policy computation
the central objective of the pomdp perspective is to compute a policy for selecting actions  a
policy is of the form 
 b   a 

   

where b is a belief distribution and a is the action chosen by the policy  
of particular interest is the notion of optimal policy  which is a policy that maximizes the expected future discounted cumulative reward 


   bt      argmax e 


t
x

t t 

fi 
fi
fi
 tt  rt fibt    
fi

   

there are two distinct but interdependent reasons why computing an optimal policy is challenging  the more widely known reason is the so called curse of dimensionality  in a problem with
n physical states   is defined over all belief states in an  n     dimensional continuous space 
the less well known reason is the curse of history  pomdp solving is in many ways like a search
through the space of possible pomdp histories  it starts by searching over short histories  through
which it can select the best short policies   and gradually considers increasingly long histories  unfortunately the number of distinct possible action observation histories grows exponentially with
the planning horizon 
   

fia nytime p oint based a pproximations for l arge pomdp s

the two cursesdimensionality and historyoften act independently  planning complexity
can grow exponentially with horizon even in problems with only a few states  and problems with a
large number of physical states may still only have a small number of relevant histories  which curse
is predominant depends both on the problem at hand  and the solution technique  for example  the
belief point methods that are the focus of this paper specifically target the curse of history  leaving
themselves vulnerable to the curse of dimensionality  exact algorithms on the other hand typically
suffer far more from the curse of history  the goal is therefore to find techniques that offer the best
balance between both 
we now describe a straightforward approach to finding optimal policies by sondik         the
overall idea is to apply multiple iterations of dynamic programming  to compute increasingly more
accurate values for each belief state b  let v be a value function that maps belief states to values in
   beginning with the initial value function 
v   b    max
a

x

r s  a b s  

    

ss

then the t th value function is constructed from the  t     th by the following recursive equation 
 

vt  b    max
a

 
x

x

r s  a b s    

ss

p r z   a  b vt     b  a  z    

    

zz

where   b  a  z  is the belief updating function defined in equation    this value function update
maximizes the expected sum of all  possibly discounted  future pay offs the agent receives in the
next t time steps  for any belief state b  thus  it produces a policy that is optimal under the planning
horizon t  the optimal policy can also be directly extracted from the previous step value function 
 

 

t  b 

  argmax
a

x

r s  a b s    

x

p r z   a  b vt     b  a  z    

    

zz

ss

sondik        showed that the value function at any finite horizon t can be expressed by a set
of vectors  t                    m    each  vector represents an  s  dimensional hyper plane  and
defines the value function over a bounded region of the belief 
x

vt  b    max
t

 s b s  

    

ss

in addition  each  vector is associated with an action  defining the best immediate policy
assuming optimal behavior for the following  t     steps  as defined respectively by the sets
 vt         v     
the t horizon solution set  t   can be computed as follows  first  we rewrite equation    as 


vt  b    max 
aa


x

ss

r s  a b s    

x
zz

max

t 

xx
ss

t  s  a  s   o s    a  z  s   b s        

s  s

notice that in this representation of vt  b   the nonlinearity in the term p  z a  b  from equation   
cancels out the nonlinearity in the term   b  a  z   leaving a linear function of b s  inside the max
operator 
   

fip ineau   g ordon   t hrun

the value vt  b  cannot be computed directly for each belief b  b  since there are infinitely
many beliefs   but the corresponding set t can be generated through a sequence of operations on
the set t   
a z
the first operation is to generate intermediate sets a 
t and t   a  a  z  z  step    
 a   s    r s  a 
a 
t
a z
t



ia z  s 

 

x

    
 

 

 

t  s  a  s  o s   a  z i  s    i  t 

s  s

where each a  and ia z is once again an  s  dimensional hyper plane 
next we create at  a  a   the cross sum over observations    which includes one a z from
each a z
t  step    
a z 
 
    
 a z
at   a 
t
t   t

    

finally we take the union of at sets  step    
t   aa at  

    

this forms the pieces of the backup solution at horizon t  the actual value function vt is
extracted from the set t as described in equation    
using this approach  bounded time pomdp problems with finite state  action  and observation
spaces can be solved exactly given a choice of the horizon t   if the environment is such that the
agent might not be able to bound the planning horizon in advance  the policy t  b  is an approximation to the optimal one whose quality improves in expectation with the planning horizon t  assuming
         
as mentioned above  the value function vt can be extracted directly from the set t   an important aspect of this algorithm  and of all optimal finite horizon pomdp solutions  is that the
value function is guaranteed to be a piecewise linear  convex  and continuous function of the belief  sondik         the piecewise linearity and continuous properties are a direct result of the fact
that vt is composed of finitely many linear  vectors  the convexity property is a result of the
a 
a
maximization operator  eqn      it is worth pointing out that the intermediate sets a z
t   t and t
also represent functions of the belief which are composed entirely of linear segments  this property
holds for the intermediate representations because they incorporate the expectation over observation
probabilities  eqn     
in the worst case  the exact value update procedure described could require time doubly exponential in the planning horizon t  kaelbling et al          to better understand the complexity
of the exact update  let  s  be the number of states   a  the number of actions   z  the number of
observations  and  t    the number of  vectors in the previous solution set  then step   creates
 a   z   t    projections and step   generates  a   t    z  cross sums  so  in the worst case  the
new solution requires 
 t     o  a  t    z   

    

   the symbol  denotes the cross sum operator  a cross sum operation is defined over two sets  a  
 a    a            am   and b    b    b            bn    and produces a third set  c    a    b    a    b            a    bn   a   
b    a    b                    am   bn   

   

fia nytime p oint based a pproximations for l arge pomdp s

 vectors to represent the value function at horizon t  these can be computed in time
o  s    a   t    z    
it is often the case that a vector in t will be completely dominated by another vector over the
entire belief simplex 
i  b   j  b  b 
    
similarly  a vector may be fully dominated by a set of other vectors  e g     in fig    is dominated by the combination of   and      this vector can then be pruned away without affecting
the solution  finding dominated vectors can be expensive  checking whether a single vector is
dominated requires solving a linear program with  s  variables and  t   constraints  nonetheless it
can be time effective to apply pruning after each iteration to prevent an explosion of the solution
size  in practice   t   often appears to grow singly exponentially in t  given clever mechanisms for
pruning unnecessary linear functions  this enormous computational complexity has long been a
key impediment toward applying pomdps to practical problems 

v                   

figure    pomdp value function representation

    point based value backup
exact pomdp solving  as outlined above  optimizes the value function over all beliefs  many
approximate pomdp solutions  including the pbvi approach proposed in this paper  gain computational advantage by applying value updates at specific  and few  belief points  rather than over all
beliefs  cheng        zhang   zhang        poon         these approaches differ significantly
 and to great consequence  in how they select the belief points  but once a set of points is selected 
the procedure for updating their value is standard  we now describe the procedure for updating the
value function at a set of known belief points 
as in section      the value function update is implemented as a sequence of operations on a
set of  vectors  if we assume that we are only interested in updating the value function at a fixed
set of belief points  b    b    b         bq    then it follows that the value function will contain at most
one  vector for each belief point  the point based value function is therefore represented by the
corresponding set                  q   
given a solution set t    we simply modify the exact backup operator  eqn     such that only
one  vector per belief point is maintained  the point based backup now gives an  vector which
is valid over a region around b  it assumes that the other belief points in that region have the same
action choice and lead to the same facets of vt  as the point b  this is the key idea behind all
algorithms presented in this paper  and the reason for the large computational savings associated
with this class of algorithms 
   

fip ineau   g ordon   t hrun

to obtain solution set t from the previous set t    we begin once again by generating intera z
mediate sets a 
t and t   a  a  z  z  exactly as in eqn      step    
a 
 a   s    r s  a 
t
a z
t



ia z  s 

 

    
 

x

 

 

t  s  a  s  o s   a  z i  s    i  t   

s  s

next  whereas performing an exact value update requires a cross sum operation  eqn      by
operating over a finite set of points  we can instead use a simple summation  we construct at   a 
a  step    
at  ba   a 
t  

x

argmax 

a z
zz t

x

 s b s    b  b 

    

ss

finally  we find the best action for each belief point  step    
b   argmax 

x

a
t  aa ss

at  s b s    b  b 

t   bb b

    
    

while these operations preserve only the best  vector at each belief point b  b  an estimate
of the value function at any belief in the simplex  including b 
  b  can be extracted from the set t
just as before 
x

vt  b    max
t

 s b s  

    

ss

to better understand the complexity of updating the value of a set of points b  let  s  be the
number of states   a  the number of actions   z  the number of observations  and  t    the number
of  vectors in the previous solution set  as with an exact update  step   creates  a   z   t   
projections  in time  s    a   z   t      steps   and   then reduce this set to at most  b  components
 in time  s   a   t     z   b    thus  a full point based value update takes only polynomial time 
and even more crucially  the size of the solution set t remains constant at every iteration  the
point based value backup algorithm is summarized in table   
note that the algorithm as outlined in table   includes a trivial pruning step  lines        
whereby we refrain from adding to t any vector already included in it  as a result  it is often the
case that  t     b   this situation arises whenever multiple nearby belief points support the same
vector  this pruning step can be computed rapidly  without solving linear programs  and is clearly
advantageous in terms of reducing the set t  
the point based value backup is found in many pomdp solvers  and in general serves to improve estimates of the value function  it is also an integral part of the pbvi framework 

   anytime point based value iteration
we now describe the algorithmic framework for our new class of fast approximate pomdp algorithms called point based value iteration  pbvi   pbvi class algorithms offer an anytime solution
to large scale discrete pomdp domains  the key to achieving an anytime solution is to interleave
   

fia nytime p oint based a pproximations for l arge pomdp s

t  backup b  t   
for each action a  a
for each observation z  z
for each solution vector i  t 
p
ia z  s     s  s t  s  a  s   o s    a  z i  s     s  s
end
a z
a z
t   i i
end
end
t   
for each belief point hb  b
i
p
p
p
b   argmaxaa
 
 s b s  
ss r s  a b s   
zz maxa z
ss
t
if b 
  t  
t   t  b
end
return t

 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  

table    point based value backup

two main components  the point based update described in table   and steps of belief set selection  the approximate value function we find is guaranteed to have bounded error  compared to the
optimal  for any discrete pomdp domain 
the current section focuses on the overall anytime algorithm and its theoretical properties  independent of the belief point selection process  section   then discusses in detail various novel
techniques for belief point selection 
the overall pbvi framework is simple  we start with a  small  initial set of belief points to
which are applied a first series of backup operations  the set of belief points is then grown  a
new series of backup operations are applied to all belief points  old and new   and so on  until a
satisfactory solution is obtained  by interleaving value backup iterations with expansions of the
belief set  pbvi offers a range of solutions  gradually trading off computation time and solution
quality 
the full algorithm is presented in table    the algorithm accepts as input an initial belief point
set  binit    an initial value       the number of desired expansions  n    and the planning horizon
 t    a common choice for binit is the initial belief b    alternately  a larger set could be used 
especially in cases where sample trajectories are available  the initial value      is typically set to
min
be purposefully low  e g      s    r 
  s  s   when we do this  we can show that the pointbased solution is always be a lower bound on the exact solution  lovejoy      a   this follows
from the simple observation that failing to compute an  vector can only lower the value function 
for problems with a finite horizon  we run t value backups between each expansion of the
belief set  in infinite horizon problems  we select the horizon t so that
 t  rmax  rmin      
where rmax   maxs a r s  a  and rmin   mins a r s  a  
   

    

fip ineau   g ordon   t hrun

the complete algorithm terminates once a fixed number of expansions  n   have been completed  alternately  the algorithm could terminate once the value function approximation reaches a
given performance criterion  this is discussed further below 
the algorithm uses the backup routine described in table    we can assume for the moment
that the expand subroutine  line    selects belief points at random  this performs reasonably
well for small problems where it is easy to achieve good coverage of the entire belief simplex 
however it scales poorly to larger domains where exponentially many points are needed to guarantee
good coverage of the belief simplex  more sophisticated approaches to selecting belief points are
presented in section    overall  the pbvi framework described here offers a simple yet flexible
approach to solving large scale pomdps 
 pbvi main binit       n   t  
b binit
    
for n expansions
for t iterations
  backup b  
end
bnew  expand b  
b   b  bnew
end
return 

 
 
 
 
 
 
 
 
 
  
  

table    algorithm for point based value iteration  pbvi 
for any belief set b and horizon t  the algorithm in table   will produce an estimate of the value
function  denoted vtb   we now show that the error between vtb and the optimal value function v 
is bounded  the bound depends on how densely b samples the belief simplex   with denser
sampling  vtb converges to vt   the t horizon optimal solution  which in turn has bounded error
with respect to v    the optimal solution  so cutting off the pbvi iterations at any sufficiently large
horizon  we can show that the difference between vtb and the optimal infinite horizon v  is not too
large  the overall error in pbvi is bounded  according to the triangle inequality  by 
kvtb  v  k  kvtb  vt k   kvt  v  k  

    

the second term is bounded by  t kv   v  k  bertsekas   tsitsiklis         the remainder of this
section states and proves a bound on the first term  which we denote t  
begin by assuming that h denotes an exact value backup  and h denotes the pbvi backup 
now define  b  to be the error introduced at a specific belief b   by performing one iteration of
point based backup 
 b     hv b  b   hv b  b    
next define  to be the maximum total error introduced by doing one iteration of point based backup 
    hv b  hv b  
  max  b  
b

   

fia nytime p oint based a pproximations for l arge pomdp s

finally define the density b of a set of belief points b to be the maximum distance from any belief
in the simplex  to a belief in set b  more precisely 
b   max
min kb  b  k   
 
b  bb

    

now we can prove the following lemma 
lemma    the error introduced in pbvi when performing one iteration of value backup over b 
instead of over   is bounded by
 rmax  rmin  b

 
proof  let b    be the point where pbvi makes its worst error in value update  and b  b
be the closest    norm  sampled belief to b    let  be the vector that is maximal at b  and   be the
vector that would be maximal at b    by failing to include   in its solution set  pbvi makes an error
of at most    b     b    on the other hand  since  is maximal at b  then    b    b  so 
      b      b 
 

 



   b     b        b     b 
    b     b       b      b
        b   b 
k   k kb   bk 
k   k b



 rmax rmin  b
 

add zero
assume  is optimal at b
re arrange the terms
by holder inequality
by definition of b

the last inequality holds because each  vector represents the reward achievable starting from
some state and following some sequence of actions and observations  therefore the sum of rewards
min
max
must fall between r 
and r 
 
lemma   states a bound on the approximation error introduced by one iteration of point based
value updates within the pbvi framework  we now look at the bound over multiple value updates 
theorem      for any belief set b and any horizon t  the error of the pbvi algorithm t   kvtb 
vt k is bounded by
 rmax  rmin  b
t 
      
proof 
t     vtb  vt   
b  hv    
    hvt 
t  




 


by definition of h

b  hv b        hv b  hv    
  hvt 
t  
t 
t  
 rmax rmin  b
b

    hvt   hvt    
 
 rmax rmin  b
b  v    
    vt 
t  
 

by triangle inequality

 rmax rmin  b
 
 rmax rmin  b
    

by definition of t 

  t 

by lemma  
by contraction of exact value backup

by sum of a geometric series
   

fip ineau   g ordon   t hrun

the bound described in this section depends on how densely b samples the belief simplex  
in the case where not all beliefs are reachable  pbvi does not need to sample all of  densely  but
  fig      the error bounds and convergence results
can replace  by the set of reachable beliefs 
 


hold on   we simply need to re define b   in lemma   
as a side note  it is worth pointing out that because pbvi makes no assumption regarding
the initial value function v b   the point based solution v b is not guaranteed to improve with the
addition of belief points  nonetheless  the theorem presented in this section shows that the bound
on the error between vtb  the point based solution  and v   the optimal solution  is guaranteed
to decrease  or stay the same  with the addition of belief points  in cases where vtb is initialized
min
pessimistically  e g   v b  s    r 
  s  s  as suggested above   then vtb will improve  or stay
the same  with each value backup and addition of belief points 
this section has thus far skirted the issue of belief point selection  however the bound presented
in this section clearly argues in favor of dense sampling over the belief simplex  while randomly
selecting points according to a uniform distribution may eventually accomplish this  it is generally
inefficient  in particular for high dimensional cases  furthermore  it does not take advantage of
the fact that the error bound holds for dense sampling over reachable beliefs  thus we seek more
efficient ways to generate belief points than at random over the entire simplex  this is the issue
explored in the next section 

   belief point selection
in section    we outlined the prototypical pbvi algorithm  while conveniently avoiding the question
of how and when belief points should be selected  there is a clear trade off between including fewer
beliefs  which would favor fast planning over good performance   versus including many beliefs
 which would slow down planning  but ensure a better bound on performance   this brings up the
question of how many belief points should be included  however the number of points is not the only
consideration  it is likely that some collections of belief points  e g   those frequently encountered 
are more likely to produce a good value function than others  this brings up the question of which
beliefs should be included 
a number of approaches have been proposed in the literature  for example  some exact value
function approaches use linear programs to identify points where the value function needs to be
further improved  cheng        littman        zhang   zhang         however this is typically
very expensive  the value function can also be approximated by learning the value at regular points 
using a fixed resolution  lovejoy      a   or variable resolution  zhou   hansen        grid  this
is less expensive than solving lps  but can scales poorly as the number of states increases  alternately  one can use heuristics to generate grid points  hauskrecht        poon         this tends
to be more scalable  though significant experimentation is required to establish which heuristics are
most useful 
this section presents five heuristic strategies for selecting belief points  from fast and naive
random sampling  to increasingly more sophisticated stochastic simulation techniques  the most
effective strategy we propose is one that carefully selects points that are likely to have the largest
impact in reducing the error bound  theorem      
most of the strategies we consider focus on selecting reachable beliefs  rather than getting
uniform coverage over the entire belief simplex  therefore it is useful to begin this discussion by
looking at how reachability is assessed 
   

fia nytime p oint based a pproximations for l arge pomdp s

while some exact pomdp value iteration solutions are optimal for any initial belief  pbvi  and
other related techniques  assume a known initial belief b    as shown in figure    we can use the
initial belief to build a tree of reachable beliefs  in this representation  each path through the tree
corresponds to a sequence in belief space  and increasing depth corresponds to an increasing plan
horizon  when selecting a set of belief points for pbvi  including all reachable beliefs would guarantee optimal performance  conditioned on the initial belief   but at the expense of computational
 can grow exponentially with the planning horitractability  since the set of reachable beliefs   
 which is sufficiently small for computational
zon  therefore  it is best to select a subset b  
tractability  but sufficiently large for good value function approximation  

b 

   
ba z ba z

ba z

  q

    a  z 

   

ba z

  q

   

ba z

   

   
   

   

   

ba z ba z

   
ba z ba z
p  

p  

ba z

p q

   

   

   

   
   

   

   

   
ba z

    ap zq

ba z

    a  z 

ba z

    ap zq

   

   

   

   

figure    the set of reachable beliefs
in domains where the initial belief is not known  or not unique   it is still possible to use reachability analysis by sampling a few initial beliefs  or using a set of known initial beliefs  to seed
multiple reachability trees 
we now discuss five strategies for selecting belief points  each of which can be used within the
pbvi framework to perform expansion of the belief set 
    random belief selection  ra 
the first strategy is also the simplest  it consists of sampling belief points from a uniform distribution over the entire belief simplex  to sample over the simplex  we cannot simply sample each
p
b s  independently over         this would violate the constraint that s b s        instead  we use
the algorithm described in table    see devroye        for more details including proof of uniform
coverage  
this random point selection strategy  unlike the other strategies presented below  does not focus
on reachable beliefs  for this reason  we do not necessarily advocate this approach  however we
include it because it is an obvious choice  it is by far the simplest to implement  and it has been used
in related work by hauskrecht        and poon         in smaller domains  e g       states   it
   all strategies discussed below assume that the belief point set  b  approximately doubles in size on each belief
expansion  this ensures that the number of rounds of value iteration is logarithmic  in the final number of belief
points needed   alternately  each strategy could be used  with very little modification  to add a fixed number of new
belief points  but this may require many more rounds of value iteration  since value iteration is much more expensive
than belief computation  it seems appropriate to double the size of b at each expansion 

   

fip ineau   g ordon   t hrun

bnew  expandra  b   
bnew   b
foreach b  b
s    number of states
for i       s
btmp  i  randuniform      
end
sort btmp in ascending order
for i       s   
bnew  i  btmp  i       btmp  i 
end
bnew   bnew  bnew
end
return bnew

 
 
 
 
 
 
 
 
 
  
  
  
  
  

table    algorithm for belief expansion with random action selection

performs reasonably well  since the belief simplex is relatively low dimensional  in large domains
 e g        states   it cannot provide good coverage of the belief simplex with a reasonable number
of points  and therefore exhibits poor performance  this is demonstrated in the experimental results
presented in section   
all of the remaining belief selection strategies make use of the belief tree  figure    to focus on
reachable beliefs  rather than trying to cover the entire belief simplex 
    stochastic simulation with random action  ssra 
to generate points along the belief tree  we use a technique called stochastic simulation  it involves
running single step forward trajectories from belief points already in b  simulating a single step
forward trajectory for a given b  b requires selecting an action and observation pair  a  z   and
then computing the new belief   b  a  z  using the bayesian update rule  eqn     in the case of
stochastic simulation with random action  ssra   the action selected for forward simulation is
picked  uniformly  at random from the full action set  table   summarizes the belief expansion
procedure for ssra  first  a state s is drawn from the belief distribution b  second  an action a
is drawn at random from the full action set  next  a posterior state s  is drawn from the transition
model t  s  a  s     finally  an observation z is drawn from the observation model o s    a  z   using
the triple  b  a  z   we can calculate the new belief bnew     b  a  z   according to equation     and
add to the set of belief points bnew  
this strategy is better than picking points at random  as described above   because it restricts
bnew to the belief tree  fig      however this belief tree is still very large  especially when the
branching factor is high  due to large numbers of actions observations  by being more selective
about which paths in the belief tree are explored  one can hope to effectively restrict the belief set
further 
   

fia nytime p oint based a pproximations for l arge pomdp s

bnew  expandssra  b   
bnew   b
foreach b  b
s randmultinomial  b 
a randuniform  a 
s   randmultinomial  t  s  a    
z randmultinomial  o s    a    
bnew     b  a  z   see eqn   
bnew   bnew  bnew
end
return bnew

 
 
 
 
 
 
 
 
 
  
  

table    algorithm for belief expansion with random action selection

a similar technique for stochastic simulation was discussed by poon         however the belief set was initialized differently  not using b     and therefore the stochastic simulations were not
restricted to the set of reachable beliefs 
    stochastic simulation with greedy action  ssga 
the procedure for generating points using stochastic simulation with greedy action  ssga  is
based on the well known  greedy exploration strategy used in reinforcement learning  sutton  
barto         this strategy is similar to the ssra procedure  except that rather than choosing an
action randomly  ssea will choose the greedy action  i e   the current best action at the given belief
b  with probability      and will chose a random action with probability   we use          once
the action is selected  we perform a single step forward simulation as in ssra to yield a new belief
point  table   summarizes the belief expansion procedure for ssga 
a similar technique  featuring stochastic simulation using greedy actions  was outlined
by hauskrecht         however in that case  the belief set included all extreme points of the belief
simplex  and stochastic simulation was done from those extreme points  rather than from the initial
belief 
    stochastic simulation with exploratory action  ssea 
the error bound in section   suggests that pbvi performs best when its belief set is uniformly dense
in the set of reachable beliefs  the belief point strategies proposed thus far ignore this information 
the next approach we propose gradually expands b by greedily choosing new reachable beliefs
that improve the worst case density 
unlike ssra and ssga which select a single action to simulate the forward trajectory for
a given b  b  stochastic sampling with exploratory action  ssea  does a one step forward
simulation with each action  thus producing new beliefs  ba    ba          however it does not accept
all new beliefs  ba    ba          but rather calculates the l  distance between each ba and its closest
neighbor in b  we then keep only that point ba that is farthest away from any point already in b 
   

fip ineau   g ordon   t hrun

bnew  expandssga  b   
bnew   b
foreach b  b
s randmultinomial  b 
if randuniform          
a randuniform  a 
else
p
a argmax ss  s b s 
end
s   randmultinomial  t  s  a    
z randmultinomial  o s    a    
bnew     b  a  z   see eqn   
bnew   bnew  bnew
end
return bnew

 
 
 
 
 
 
 
 
 
  
  
  
  
  
  

table    algorithm for belief expansion with greedy action selection

we use the l  norm to calculate distance between belief points to be consistent with the error bound
in theorem      table   summarizes the ssea expansion procedure 
bnew  expandssea  b   
bnew   b
foreach b  b
foreach a  a
s randmultinomial  b 
s   randmultinomial  t  s  a    
z randmultinomial  o s    a    
ba    b  a  z   see eqn   
end
p
bnew   maxaa minb  bnew ss  ba  s   b   s  
bnew   bnew  bnew  see eqn   
end
return bnew

 
 
 
 
 
 
 
 
 
  
  
  
  

table    algorithm for belief expansion with exploratory action selection

    greedy error reduction  ger 
while the ssea strategy above is able to improve the worst case density of reachable beliefs  it
does not directly minimize the expected error  and while we would like to directly minimize the
   

fia nytime p oint based a pproximations for l arge pomdp s

error  all we can measure is a bound on the error  lemma     we therefore propose a final strategy
which greedily adds the candidate beliefs that will most effectively reduce this error bound  our
empirical results  as presented below  show that this strategy is the most successful one discovered
thus far 
to understand how we expand the belief set in the ger strategy  it is useful to re consider the
belief tree  which we reproduce in figure    each node in the tree corresponds to a specific belief 
we can divide these nodes into three sets  set   includes those belief points already in b  in this
case b  and ba  z    set   contains those belief points that are immediate descendants of the points
in b  i e   the nodes in the grey zone   these are the candidates from which we will select the new
points to be added to b  we call this set the envelope  denoted b   set   contains all other reachable
beliefs 

b 

   
ba z ba z

  q

  q

   

   

   

   
   

 

  a  z 

   

   

ba z

   

   
ba z

   

ba z ba z
p  

p  

ba z

p q

   

   

ba z ba z

   
   

   

   

ba z

ba z

    ap zq

   

   

figure    the set of reachable beliefs
we need to decide which belief b should be removed from the envelope b and added to the set
of active belief points b  every point that is added to b will improve our estimate of the value
function  the new point will reduce the error bounds  as defined in section   for points that were
already in b  however  the error bound for the new point itself might be quite large  that means that
the largest error bound for points in b will not monotonically decrease  however  for a particular
point in b  such as the initial belief b    the error bound will be decreasing 
to find the point which will most reduce our error bound  we can look at the analysis of
lemma    lemma   bounds the amount of additional error that a single point based backup introduces  write b  for the new belief which we are considering adding  and write b for some belief
which is already in b  write  for the value hyper plane at b  and write   for b    as the lemma
points out  we have
 b             b   b 
when evaluating this error  we need to minimize over all b  b  also  since we do not know what
  will be until we have done some backups at b    we make a conservative assumption and choose
the worst case value of     rmin         rmax         s    thus  we can evaluate 
 

 b     min
bb

x
ss

max
  s   b   s   b s   b   s   b s 
  r 
rmin
      s   b   s   b s   b   s    b s 

   

    

fip ineau   g ordon   t hrun

while one could simply pick the candidate b   b which currently has the largest error bound  
this would ignore reachability considerations  rather  we evaluate the error at each b  b  by
weighing the error of the fringe nodes by their reachability probability 
 b    

x

 b    max
aa

o b  a  z     b  a  z  


x

  max
aa

    

zz


xx


zz

t  s  a  s   o s    a  z b s     b  a  z   

ss s  s

noting that   b  a  z   b  and    b  a  z   can be evaluated according to equation    
using equation     we find the existing point b  b with the largest error bound  we can now
directly reduce its error by adding to our set one of its descendants  we select the next step belief
  b  a  z  which maximizes error bound reduction 
b

 

b    b  a  z  

where b  a    argmax

    
x

o b  a  z     b  a  z  

    

bb aa zz

z    argmax o b  a  z     b  a  z  

    

zz

table   summarizes the ger approach to belief point selection 
bnew  expandger  b   
bnew   b
n   b 
for i       n
p
b  a    argmaxbb aa zz o b  a  z     b  a  z  
z    argmaxzz o b  a  z     b  a  z  
bnew     b  a  z 
bnew   bnew  bnew
end
return bnew

 
 
 
 
 
 
 
 
 
  

table    algorithm for belief expansion

the complexity of adding one new points with ger is o sazb   where s  states 
a  actions  z  observations  b  beliefs already selected   in comparison  a value backup  for
one point  is o s   azb   and each point typically needs to be updated several times  as we point
out in empirical results below  belief selection  even with ger  takes minimal time compared to
value backup 
this concludes our presentation of belief selection techniques for the pbvi framework  in
summary  there are three factors to consider when picking a belief point      how likely is it to
   we tried this  however it did not perform as well empirically as what we suggest in equation     because it did not
consider the probability of reaching that belief 

   

fia nytime p oint based a pproximations for l arge pomdp s

occur      how far is it from other belief points already selected      what is the current approximate
value for that point  the simplest heuristic  ra  accounts for none of these  whereas some of the
others  ssra  ssga  ssea  account for one  and ger incorporates all three factors 
    belief expansion example
we consider a simple example  shown in figure    to illustrate the difference between the various
belief expansion techniques outlined above  this  d pomdp  littman        has four states  one
of which is the goal  indicated by the star   the two actions  left and right  have the expected
 deterministic  effect  the goal state is fully observable  observation goal   while the other three
states are aliased  observation none   a reward of    is received for being in the goal state 
otherwise the reward is zero  we assume a discount factor of          the initial distribution is
uniform over non goal states  and the system resets to that distribution whenever the goal is reached 

figure     d pomdp
the belief set b is always initialized to contain the initial belief b    figure   shows part of
the belief tree  including the original belief set  top node   and its envelope  leaf nodes   we now
consider what each belief expansion method might do 
b                      
a left

a right

                 

pr z none     

b             

                    

pr z goal       

pr z none     

b             

b                 

pr z goal       

b             

figure     d pomdp belief tree
the random heuristic can pick any belief point  with equal probability  from the entire belief
simplex  it does not directly expand any branches of the belief tree  but it will eventually put samples
nearby 
the stochastic simulation with random action has a     chance of picking each action 
then  regardless of which action was picked  theres a     chance of seeing observation none  and a
    chance of seeing observation goal  as a result  the ssra will select  p r bnew   b              
p r bnew   b               p r bnew   b               p r bnew   b              
   

fip ineau   g ordon   t hrun

the stochastic simulation with greedy action first needs to know the policy at b    a few
iterations of point based updates  section      applied to this initial  single point  belief set reveal
that  b      lef t   as a result  expansion of the belief will greedily select action lef t with proba
bility        a 
        assuming        and  a        action right will be selected for belief

expansion with probability  a 
        combining this along with the observation probabilities  we
can tell that ssga will expand as follows  p r bnew   b                p r bnew   b               
p r bnew   b                p r bnew   b               
predicting the choice of stochastic simulation with exploratory action is slightly more complicated  four cases can occur  depending on the outcomes of random forward simulation from b   
   if action left goes to b   p r        and action right goes to b   p r         then b  will be
selected because   b   b            whereas   b   b             this case will occur with
p r       
   if action left goes to b   p r        and action right goes to b   p r         then b  will be
selected because   b   b           this case will occur with p r       
   if action left goes to b   p r        and action right goes to b   p r         then b  will be
selected because   b   b           this case will occur with p r       
   if action left goes to b   p r        and action right goes to b   p r         then either can
be selected  since they are equidistant to b     in this case each b  and b  has p r        of
being selected 
all told  p r bnew   b          p r bnew   b           p r bnew   b        p r bnew   b    
     
now looking at belief expansion using greedy error reduction  we need to compute the
error    b    a  z    a  z  we consider equation     since b has only one point  b    then necessarily b   b    to estimate   we apply multiple steps of value backup at b  and obtain
                          using b and  as such  we can now estimate the error at each candidate belief   b             b             b             b            note that because b has
only one point  the dominating factor is their distance to b    next  we factor in the observation
probabilities  as in eqns        which allows us to determine that a   lef t and z   none  and
therefore we should select bnew   b   
in summary  we note that ssga  ssea and ger all favor selecting b    whereas ssra picks
each option with equal probability  considering that b  and b  are actually the same   in general 
for a problem of this size  it is reasonable to expand the entire belief tree  any of the techniques
discussed here will be do this quickly  except ra which will not pick the exact nodes in the belief
tree  but will select equally good nearby beliefs  this example is provided simply to illustrate the
different choices made by each strategy 

   a review of point based approaches for pomdp solving
the previous section describes a new class of point based algorithms for pomdp solving  the idea
of using point based updates in pomdps has been explored previously in the literature  and in this
   this may not be obvious to the reader  but it follows directly from the repeated application of equations      

   

fia nytime p oint based a pproximations for l arge pomdp s

section we summarize the main results  for most of the approaches discussed below  the procedure
for updating the value function at a given point remains unchanged  as outlined in section      
rather  the approaches are mainly differentiated by how the belief points are selected  and by how
the updates are ordered 
    exact point based algorithms
some of the earlier exact pomdp techniques use point based backups to optimize the value function over limited regions of the belief simplex  sondik        cheng         these techniques
typically require solving multiple linear programs to find candidate belief points where the value
function is sub optimal  which can be an expensive operation  furthermore  to guarantee that an exact solution is found  relevant beliefs must be generated systematically  meaning that all reachable
beliefs must be considered  as a result  these methods typically cannot scale beyond a handful of
states actions observations 
in work by zhang and zhang         point based updates are interleaved with standard dynamic programming updates to further accelerate planning  in this case the points are not generated
systematically  but rather backups are applied to both a set of witness points and lp points  the
witness points are identified as a result of the standard dynamic programming updates  whereas
the lp points are identified by solving linear programs to identify beliefs where the value has not
yet been improved  both of these procedures are significantly more expensive than the belief selection heuristics presented in this paper and results are limited to domains with at most a dozen
states actions observations  nonetheless this approach is guaranteed to converge to the optimal solution 
    grid based approximations
there exists many approaches that approximate the value function using a finite set of belief points
along with their values  these points are often distributed according to a grid pattern over the belief
space  thus the name grid based approximation  an interpolation extrapolation rule specifies the
value at non grid points as a function of the value of neighboring grid points  these approaches
ignore the convexity of the pomdp value function 
performing value backups over grid points is relatively straightforward  dynamic programming
updates as specified in equation    can be adapted to grid points for a simple polynomial time
algorithm  given a set of grid points g  the value at each bg  g is defined by 
 
g

v  b     max
a

 
x

x

g

b  s r s  a    

ss

p r z   a  b v    b  a  z    

    

zz

if   b  a  z  is part of the grid  then v    b  a  z   is defined by the value backups  otherwise 
v    b  a  z   is approximated using an interpolation rule such as 
v    b  a  z   

 g 
x

 i v  bg
i   

    

i  

p g 

where  i     and i    i       this produces a convex combination over grid points  the
two more interesting questions with respect to grid based approximations are     how to calculate
the interpolation function  and     how to select grid points 
   

fip ineau   g ordon   t hrun

in general  to find the interpolation that leads to the best value function approximation at a point
b requires solving the following linear program 
minimize

 g 
x

 i v  bg
i  

    

i  

subject to

b 

 g 
x

 i bg
i

    

i  
 g 
x

 i     

    

i  

    i         i   g  

    

different approaches have been proposed to select grid points  lovejoy      a  constructs a
fixed resolution regular grid over the entire belief space  a benefit is that value interpolations can be
calculated quickly by considering only neighboring grid points  the disadvantage is that the number
of grid points grows exponentially with the dimensionality of the belief  i e   with the number of
states   a simpler approach would be to select random points over the belief space  hauskrecht 
       but this requires slower interpolation for estimating the value of the new points  both
of these methods are less than ideal when the beliefs encountered are not uniformly distributed 
in particular  many problems are characterized by dense beliefs at the edges of the simplex  i e  
probability mass focused on a few states  and most other states have zero probability   and low
belief density in the middle of the simplex  a distribution of grid points that better reflects the
actual distribution over belief points is therefore preferable 
alternately  hauskrecht        also proposes using the corner points of the belief simplex  e g  
                                                          and generating additional successor belief points through
one step stochastic simulations  eqn    from the corner points  he also proposes an approximate
interpolation algorithm that uses the values at  s   critical points plus one non critical point in the
grid  an alternative approach is that by brafman         which builds a grid by also starting with the
critical points of the belief simplex  but then uses a heuristic to estimate the usefulness of gradually
adding intermediate points  e g   bk      bi      bj   for any pair of points   both hauskrechts
and brafmans methodsgenerally referred to as non regular grid approximationsrequire fewer
points than lovejoys regular grid approach  however the interpolation rule used to calculate the
value at non grid points is typically more expensive to compute  since it involves searching over all
grid points  rather than just the neighboring sub simplex 
zhou and hansen        propose a grid based approximation that combines advantages from
both regular and non regular grids  the idea is to sub sample the regular fixed resolution grid
proposed by lovejoy  this gives a variable resolution grid since some parts of the beliefs can be
more densely sampled than others and by restricting grid points to lie on the fixed resolution grid
the approach can guarantee fast value interpolation for non grid points  nonetheless  the algorithm
often requires a large number of grid points to achieve good performance 
finally  bonet        proposes the first grid based algorithm for pomdps with  optimality
 for any        this approach requires thorough coverage of the belief space such that every point
is within  of a grid point  the value update for each grid point is fast to implement  since the
interpolation rule depends only on the nearest neighbor of the one step successor belief for each
grid point  which can be pre computed   the main limitation is the fact that  coverage of the belief
   

fia nytime p oint based a pproximations for l arge pomdp s

space can only be attained by using exponentially many grid points  furthermore  this method
requires good coverage of the entire belief space  as opposed to the algorithms of section    which
focus on coverage of the reachable beliefs 
    approximate point based algorithms
more similar to the pbvi class of algorithms are those approaches that update both the value and
gradient at each grid point  lovejoy      a  hauskrecht        poon         these methods are able
to preserve the piecewise linearity and convexity of the value function  and define a value function
over the entire belief simplex  most of these methods use random beliefs  and or require the inclusion of a large number of fixed beliefs such as the corners of the probability simplex  in contrast  the
pbvi class algorithms we propose  with the exception of pbvi ra  select only reachable beliefs 
and in particular those belief points that improve the error bounds as quickly as possible  the idea
of using reachability analysis  also known as stochastic simulation  to generate new points was explored by some of the earlier approaches  hauskrecht        poon         however their analysis
indicated that stochastic simulation was not superior to random point placements  we re visit this
question  and conclude otherwise  in the empirical evaluation presented below 
more recently  a technique closely related to pbvi called perseus has been proposed  vlassis
  spaan        spaan   vlassis         perseus uses point based backups similar to the ones
used in pbvi  but the two approaches differ in two ways  first  perseus uses randomly generated
trajectories through the belief space to select a set of belief points  this is in contrast to the beliefpoint selection heuristics outlined above for pbvi  second  whereas pbvi systematically updates
the value at all belief points at every epoch of value iteration  perseus selects a subset of points
to update at every epoch  the method used to select points is the following  points are randomly
sampled one at a time and their value is updated  this continues until the value of all points has
been improved  the insight resides in observing that updating the  vector at one point often also
improves the value estimate of other nearby points  which are then removed from the sampling set  
this approach is conceptually simple and empirically effective 
the hsvi algorithm  smith   simmons        is another point based algorithm  which differs
from pbvi both in how it picks belief points  and in how it orders value updates  it maintains a lower
and an upper bound on the value function approximation  and uses it to select belief points  the
updating of the upper bound requires solving linear programs and is generally the most expensive
step  the ordering of value update is as follows  whenever a belief point is expanded from the
belief tree  hsvi updates only the value of its direct ancestors  parents  grand parents  etc   all the
way back to the initial belief in the head node   this is in contrast to pbvi which performs a batch
of belief point expansions  followed by a batch of value updates over all points  in other respects 
hsvi and pbvi share many similarities  both offer anytime performance  theoretical guarantees 
and scalability  finally the hsvi also takes reachability into account  we will evaluate empirical
differences between hsvi and pbvi in the next section 
finally  the rtbss algorithm  paquet        offers an online version of point based algorithms 
the idea is to construct a belief reachability tree similar to figure    but using the current belief
as the top node  and terminating the tree at some fixed depth d  the value at each node can be
computed recursively over the finite planning horizon d  the algorithm can eliminate some subtrees
by calculating a bound on their value  and comparing it to the value of other computed subtrees 
rtbss can in fact be combined with an offline algorithms such as pbvi  where the offline algorithm
   

fip ineau   g ordon   t hrun

is used to pre compute a lower bound on the exact value function  this can be used to increase subtree
pruning  thereby increasing the depth of the online tree construction and thus also the quality of the
solution  this online algorithm can yield fast results in very large pomdp domains  however the
overall solution quality does not achieve the same error guarantees as the offline approaches 

   experimental evaluation
this section looks at a variety of simulated pomdp domains to evaluate the empirical performance
of pbvi  the first three domainstiger grid  hallway  hallway are extracted from the established pomdp literature  cassandra         the fourthtagwas introduced in some of our
earlier work as a new challenge for pomdp algorithms 
the first goal of these experiments is to establish the scalability of the pbvi framework  this is
accomplished by showing that pbvi type algorithms can successfully solve problems in excess of
    states  we also demonstrate that pbvi algorithms compare favorably to alternative approximate
value iteration methods  finally  following on the example of section      we study at a larger scale
the impact of the belief selection strategy  which confirms the superior performance of the ger
strategy 
    maze problems
there exists a set of benchmark problems commonly used to evaluate pomdp planning algorithms  cassandra         this section presents results demonstrating the performance of pbviclass algorithms on some of those problems  while these benchmark problems are relatively small
 at most    states    actions  and    observations  compared to most robotics planning domains 
they are useful from an analysis point of view and for comparison to previous work 
the initial performance analysis focuses on three well known problems from the pomdp literature  tiger grid  also known as maze     hallway  and hallway   all three are maze navigation
problems of various sizes  the problems are fully described by littman  cassandra  and kaelbling
     a   parameterization is available from cassandra        
figure  a presents results for the tiger grid domain  replicating earlier experiments by brafman         test runs terminate after     steps  theres an automatic reset every time the goal is
reached  and results are averaged over     runs 
figures  b and  c present results for the hallway and hallway  domains  respectively  in this
case  test runs are terminated when the goal is reached or after     steps  whichever occurs first  
and the results are averaged over     runs  this is consistent with earlier experiments by littman 
cassandra  and kaelbling      b  
all three figures compare the performance of three different algorithms 
   pbvi with greedy error reduction  ger  belief point selection  section      
   qmdp  littman et al       b  
   incremental pruning  cassandra  littman    zhang        
the qmdp heuristic  littman et al       b  takes into account partial observability at the current step  but assumes full observability on subsequent steps 
qm dp  b    argmax
aa

   

x
ss

b s qm dp  s  a  

    

fia nytime p oint based a pproximations for l arge pomdp s

the resulting policy has some ability to resolve uncertainty  but cannot benefit from long term
information gathering  or compare actions with different information potential  qmdp can be seen
as providing a good performance baseline  for the three problems considered  it finds a policy
extremely quickly  but the policy is clearly sub optimal 
at the other end of the spectrum  the incremental pruning algorithm  zhang   liu        cassandra et al         is a direct extension of the enumeration algorithm described above  the principal insight is that the pruning of dominated  vectors  eqn     can be interleaved directly with the
cross sum operator  eqn      the resulting value function is the same  but the algorithm is more
efficient because it discards unnecessary vectors earlier on  while incremental pruning algorithm
can theoretically find an optimal policy  for the three problems considered here it would take far too
long  in fact  only a few iterations of exact backups were completed in reasonable time  in all three
problems  the resulting short horizon policy was worse than the corresponding pbvi policy 
as shown in figure    pbvi ger provides a much better time performance trade off  it finds
policies that are better than those obtained with qmdp  and does so in a matter of seconds  thereby
demonstrating that it does not suffer from the same paralyzing complexity as incremental pruning 
while those who take a closer look at these results may be surprised to see that the performance
of pbvi actually decreases at some points  e g   the dip in fig   c   this is not unexpected  it
is important to remember that the theoretical properties of pbvi only guarantee a bound on the
estimate of the value function  but as shown here  this does not necessarily imply that the policy
needs to improve monotonically  nonetheless  as the value function converges  so will the policy
 albeit at a slower rate  
    tag problem
while the previous section establishes the good performance of pbvi on some well known simulation problems  these are quite small and do not fully demonstrate the scalability of the algorithm 
to provide a better understanding of pbvis effectiveness for large problems  this section presents
results obtained when applying pbvi to the tag problem  a robot version of the popular game of
lasertag  in this problem  the agent must navigate its environment with the goal of searching for 
and tagging  a moving target  rosencrantz  gordon    thrun         real world versions of this
problem can take many forms  and in section   we present a similar problem domain where an
interactive service robot must find an elderly patient roaming the corridors of a nursing home 
the synthetic scenario considered here is an order of magnitude larger      states  than most
other pomdp benchmarks in the literature  cassandra         when formulated as a pomdp problem  the goal is for the robot to optimize a policy allowing it to quickly find the person  assuming
that the person moves  stochastically  according to a fixed policy  the spatial configuration of the
environment used throughout this experiment is illustrated in figure   
the state space is described by the cross product of two position features  robot  
 s            s     and person    s            s     sf ound    both start in independently selected random
positions  and the scenario finishes when person   sf ound   the robot can select from five actions 
 north  south  east  west  tag   a reward of   is imposed for each motion action  the tag action
results in a     reward if the robot and person are in the same cell  or    otherwise  throughout the scenario  the robots position is fully observable  and a move action has the predictable
deterministic effect  e  g  
p r robot   s     robot   s    n orth      
   

fip ineau   g ordon   t hrun

   

   
pbvi ger
qmdp
incprune

pbvi ger
qmdp
incprune

   

 

   

reward

reward

   

 

   
   
   

   
   
   
  

 

 

  

 

 

  
  
time  secs 

   
  

 

  

  

 

 

  

 

  
  
time  secs 

 a  tiger grid

 

  

 

  

 b  hallway

    
   

pbvi ger
qmdp
incprune

    

reward

   
    
   
    
   
    
   
  

 

 

  

 

  
time  secs 

 

  

  

 c  hallway 

figure    pbvi performance on well known pomdp problems  each figure shows the sum of
discounted reward as a function of the computation time for a different problem domain 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

 

 

 

 

 

 

 

 

 

 

figure    spatial configuration of the domain

   

fia nytime p oint based a pproximations for l arge pomdp s

and so on for each adjacent cell and direction  the position of the person  on the other hand 
is completely unobservable unless both agents are in the same cell  meanwhile at each step  the
person  with omniscient knowledge  moves away from the robot with p r       and stays in place
with p r        e  g  
p r p erson   s     p erson   s    robot   s         
p r p erson   s     p erson   s    robot   s         
p r p erson   s     p erson   s    robot   s          
figure   shows the performance of pbvi with greedy error reduction on the tag domain  results are averaged over      runs  using different  randomly chosen  start positions for each run 
the qmdp approximation is also tested to provide a baseline comparison  the results show a gradual improvement in pbvis performance as samples are added  each shown data point represents a
new expansion of the belief set with value backups   it also confirms that computation time is directly related to the number of belief points  pbvi requires fewer than     belief points to overcome
qmdp  and the performance keeps on improving as more points are added  performance appears to
be converging with approximately     belief points  these results show that a pbvi class algorithm
can effectively tackle a problem with     states 
 
pbvi ger
qmdp
 

reward

  
  
  
  
  
    
  

 

  

 

 

  
  
time  secs 

 

  

 

  

figure    pbvi performance on tag problem  we show the sum of discounted reward as a function
of the computation time 

this problem is far beyond the reach of the incremental pruning algorithm  a single iteration of
optimal value iteration on a problem of this size could produce over       vectors before pruning 
therefore  it was not applied 
this section describes one version of the tag problem  which was used for simulation purposes
in our work and that of others  braziunas   boutilier        poupart   boutilier        smith  
simmons        vlassis   spaan         in fact  the problem can be re formulated in a variety
of ways to accommodate different environments  person motion models  and observation models 
section   discusses variations on this problem using more realistic robot and person models  and
presents results validated onboard an independently developed robot simulator 
   

fip ineau   g ordon   t hrun

    empirical comparison of pbvi class algorithms
having establish the good performance of pbvi ger on a number of problems  we now consider
empirical results for the different pbvi class algorithms  this allows us to compare the effects
of the various belief expansion heuristics  we repeat the experiments on the tiger grid  hallway 
hallway  and tag domains  as outlined above  but in this case we compare the performance of five
different pbvi class algorithms 
   pbvi ra  pbvi with belief points selected randomly from belief simplex  section      
   pbvi ssra  pbvi with belief points selected using stochastic simulation with random action  section      
   pbvi ssga  pbvi with belief points selected using stochastic simulation with greedy action
 section      
   pbvi ssea  pbvi with belief points selected using stochastic simulation with exploratory
action  section      
   pbvi ger  pbvi with belief points selected using greedy error reduction  section      
all pbvi class algorithms can converge to the optimal value function given a sufficiently large
set of belief points  but the rate at which they converge depends on their ability to generally pick
useful points  and leave out the points containing less information  since the computation time
is directly proportional to the number of belief points  the algorithm with the best performance is
generally the one which can find a good solution with the fewest belief points 
figure   shows a comparison between the performance of each of the five pbvi class algorithms
enumerated above on each of the four problem domains  in these pictures  we present performance
results as a function of computation time  
as seen from these results  in the smallest domaintiger gridpbvi ger is similar in performance to the random approach pbvi ra  in the hallway domain  pbvi ger reaches nearoptimal performance earlier than the other algorithms  in hallway   it is unclear which of the five
algorithms is best  though ger seems to converge earlier 
in the larger tag domain  the situation is more interesting  the pbvi ger combination is
clearly superior to the others  there is reason to believe that pbvi ssea could match its performance  but would require on the order of twice as many points to do so  nonetheless  pbvi ssea
performs better than either pbvi ssra or pbvi ssga  with the random heuristic  pbvi ra  
the reward did not improve regardless of how many belief points were added          and therefore we do not include it in the results  the results presented in figure   suggest that the choice
of belief points is crucial when dealing with large problems  in general  we believe that ger  and
ssea to a lesser degree  is superior to the other heuristics for solving domains with large numbers
of action observation pairs  because it has the ability to selectively chooses which branches of the
reachability tree to explore 
as a side note  we were surprised by ssgas poor performance  in comparison with ssra  on
the tiger grid and tag domains  this could be due to a poorly tuned greedy bias   which we did
   nearly identical graphs can be produced showing performance results as a function of the number of belief points 
this confirms complexity analysis showing that the computation time is directly related to the number of belief
points 

   

fia nytime p oint based a pproximations for l arge pomdp s

   

   
ra
ssra
ssga
ssea
ger

   

   

reward

reward

 

   

 

ra
ssra
ssga
ssea
ger

   
   
   

   
   
   
  

 

 

  

 

  
  
time  secs 

 

  

   
  

 

  

 

  

 a  tiger grid

 

 

  
time  secs 

 

  

  

 b  hallway
 

    

reward

   

 

ssra
ssga
ssea
ger

  
reward

   
    

ra
ssra
ssga
ssea
ger

    
   
    

  
  
  

   
  

    
   
  

 

  

 

  
time  secs 

 

  

    
  

 

  

 c  hallway 

 

  

 

 

  
  
time  secs 

 

  

 

  

 d  tag

figure    belief expansion results showing execution performance as a function of the computation
time 

not investigate at length  future investigations using problems with a larger number of actions may
shed better light on this issue 
in terms of computational requirement  ger is the most expensive to compute  followed by
ssea  however in all cases  the time to perform the belief expansion step is generally negligible
       compared to the cost of the value update steps  therefore it seems best to use the more
effective  though more expensive  heuristic 
the pbvi framework can accommodate a wide variety of strategies  past what is described in
this paper  for example  one could extract belief points directly from sampled experimental traces 
this will be the subject of future investigations 
    comparative analysis
while the results outlined above show that pbvi type algorithms are able to handle a wide spectrum
of large scale pomdp domains  it is not sufficient to compare the performance of pbvi only to
   

fip ineau   g ordon   t hrun

qmdp and incremental pruningthe two ends of the spectrumas done in section      in fact
there has been significant activity in recent years in the development of fast approximate pomdp
algorithms  and so it is worthwhile to spend some time comparing the pbvi framework to these
alternative approaches  this is made easy by the fact that many of these have been validated using
the same set of problems as described above 
table   summarizes the performance of a large number of recent pomdp approximation algorithms  including pbvi  on the four target domains  tiger grid  hallway  hallway   and tag  the
algorithms listed were selected based on the availability of comparable published results or available
code  or in some cases because the algorithm could be re implemented easily 
we compare their empirical performance  in terms of execution performance versus planning 
on a set of simulation domains  however as is often the case  these results show that there is not a
single algorithm that is best for solving all problems  we therefore also compile a summary of the
attributes and characteristics of each algorithm  in an attempt to tell which algorithm may be best
for what types of problems  table   includes  whenever possible  the goal completion rates  sum
of rewards  policy computation time  number of required belief points  and policy size  number of
 vectors  or number of nodes in finite state controllers   the number of belief points and policy
size are often identical  however the latter can be smaller if a single  vector is best for multiple
belief points 
the results marked     were computed by us on a  ghz pentium    other results were likely
computed on different platforms  and therefore time comparisons may be approximate at best 
nonetheless the number of samples and the size of the final policy are both useful indicators of
computation time  the results reported for pbvi correspond to the earliest data point from figures   and   where pbvi ger achieves top performance 
algorithms are listed in order of performance  starting with the algorithm s  achieving the highest reward  all results assume a standard  not lookahead  controller  see hauskrecht        for
definition  
overall  the results indicate that some of the algorithms achieve sub par performance in terms of
expected reward  in the case of qmdp  this is because of fundamental limitations in the algorithm 
while incremental pruning and the exact value directed compression can theoretically reach optimal
performance  they would require longer computation time to do so  the grid method  see tiger grid
results   bpi  see tiger grid  hallway and tag results  and pbua  see tag results  suffer from a
similar problem  but offer much more graceful performance degradation  it is worth noting that none
of these approaches assumes a known initial belief  so in effect they are solving harder problems 
the results for bbsls are not sufficiently extensive to comment at length  but it appears to be able
to find reasonable policies with small controllers  see tag results  
the remaining algorithmshsvi  perseus  and our own pbvi gerall offer comparable
performance on these relatively large pomdp domains  hsvi seems to offer good control performance on the full range of tasks  but requires bigger controllers  and is therefore probably slower 
especially on domains with high stochasticity  e g   tiger grid  hallway  hallway    the trade offs
between perseus and pbvi ger are less clear  the planning time  controller size and performance
quality are quite comparable  and in fact the two approaches are very similar  similarities and
differences between the two approaches are explored further in section   

   

fia nytime p oint based a pproximations for l arge pomdp s

method
tiger grid  maze   
hsvi  smith   simmons       
perseus  vlassis   spaan       
pbua  poon       
pbvi ger   
bpi  poupart   boutilier       
grid  brafman       
qmdp  littman et al       b    
incprune  cassandra et al           
exact vdc  poupart   boutilier          
hallway
pbua  poon       
hsvi  smith   simmons       
pbvi ger   
perseus  vlassis   spaan       
bpi  poupart   boutilier       
qmdp  littman et al       b    
exact vdc  poupart   boutilier          
incprune  cassandra et al           
hallway 
pbvi ger   
perseus  vlassis   spaan       
hsvi  smith   simmons       
pbua  poon       
bpi  poupart   boutilier       
grid  brafman       
qmdp  littman et al       b    
exact vdc  poupart   boutilier          
incprune  cassandra et al           
tag
hsvi  smith   simmons       
pbvi ger   
perseus  vlassis   spaan       
bbsls  braziunas   boutilier       
bpi  poupart   boutilier       
qmdp  littman et al       b    
pbua  poon          
incprune  cassandra et al           

goal 

reward  conf int 

time s 

 b 

  

n a 
n a 
n a 
n a 
n a 
n a 
n a 
n a 
n a 

    
    
    
          
    
    
     
   
   

     
   
     
   
      
n v 
    
  hrs 
  hrs 

n v 
     
   
   
n a 
   
n a 
n a 
n a 

    
   
n v 
   
    
n a 
 
n v 
n v 

   
   
   
n v 
n v 
  
  
  

    
    
          
    
    
     
     
     

   
     
  
  
      
    
  hrs 
  hrs 

   
n v 
  
     
n a 
n a 
n a 
n a 

n v 
    
  
  
    
 
n v 
n v 

   
n v 
   
   
n v 
  
  
  
  

          
    
    
    
    
n v 
     
     
     

 
  
     
     
      
n v 
    
  hrs 
  hrs 

  
     
n v 
    
n a 
   
n a 
n a 
n a 

  
  
    
n v 
    
n a 
 
n v 
n v 

   
   
n v 
n v 
n v 
  
 
 

     
           
     
     
     
      
     
     

     
    
    
      
     
    
  hrs 
  hrs 

n v 
   
     
n a 
n a 
n a 
    
n a 

    
   
   
  
   
 
n v 
n v 

n a  not applicable

n v  not available

    results computed by us

table    results of pbvi for standard pomdp domains

   

fip ineau   g ordon   t hrun

    error estimates
the results presented thus far suggest that the pbvi framework performs best when using the
greedy error reduction  ger  technique for selecting belief points  under this scheme  to decide which belief points will be included  we estimate an error bound at a set of candidate points
and then pick the one with the largest error estimate  the error bound is estimated as described in
equation     we now consider the question of how this estimate evolves as more and more points
are added  the natural intuition is that with the first few points  error estimates will be very large 
but as the density of the belief set increases  the error estimates will become much smaller 
figure    reconsiders the four target domains  tiger grid  hallway  hallway  and tag  in each
case  we present both the reward performance as a function of the number of belief points  top
row graphs   and the error estimate of each point selected according the order in which points were
picked  bottom row graphs   in addition  the bottom graphs also show  in dashed line  a trivial
rmin
bound on the error   vt  vt     rmax 
  valid for any t step value function of an arbitrary
policy  as expected  our bound is typically tighter than the trivial bound  in tag  this only occurs
once the number of belief points exceeds the number of states  which is not surprising  given that our
bound depends on distance between reachable beliefs  and that all states are reachable beliefs in this
domain  overall  it seems that there is reasonably good correspondence between an improvement
in performance and a decrease in our error estimates  we can conclude from this figure that even
though the pbvi error is quite loose  it can in fact be informative in guiding exploration of the belief
simplex 
we note that there is significant variance in our error estimates from one belief point to the next 
as illustrated by the non monotonic behavior of the curves in the bottom graphs of figure     this
behavior can be attributed to a few possibilities  first  there is the fact that the error estimate at
a given belief is only approximate  and the value function used to calculate the error estimate is
itself approximate  in addition  there is the fact that new belief points are always selected from the
envelope of reachable beliefs  not from the set of all reachable beliefs  this suggests that ger could
be improved by maintaining a deeper envelope of candidate belief points  currently the envelope
contains those points that are   step forward simulations from the points already selected  it may
be useful to consider points    steps ahead  we predict this would reduce the jaggedness seen in
figure     and more importantly  also reduce the number of points necessary for good performance 
of course  the tradeoff between the time spent selecting points and the time spent planning would
have to be re evaluated under this light 

   robotic applications
the overall motivation behind the work described in this paper is the desire to provide high quality
robust planning for real world autonomous systems  and in particular for robots  on the practical side  our search for a robust robot controller has been in large part guided by the nursebot
project  pineau  montermerlo  pollack  roy    thrun         the overall goal of the project is to
develop personalized robotic technology that can play an active role in providing improved care
and services to non institutionalized elderly people  pearl  shown in figure     is the main robotic
platform used for this project 
from the many services a nursing assistant robot could provide  engelberger        lacey
  dawson howe         much of the work to date has focused on providing timely cognitive reminders  e g   medications to take  appointments to attend  etc   to elderly subjects  pollack        
   

fia nytime p oint based a pproximations for l arge pomdp s

tigergrid

hallway

   

reward

 

hallway 

   

   

   

   

   

   

   

   

tag
 

  

   
 

  

   
 
 
  

 

  

 

  

 

  

 
 
  

  belief points
  

 

  

 

  

 
 
  

 

  

  belief points

 

  

 

  

  

  

error

 

  

 

  

  belief points

   

  
  

  

   

  

  

   

  
 

   

  
 
 
  

 

  

   

  

  

  
 
 
     

  belief points

 

  

 

  

  belief points

 
 
 
     

 

  

 

  

 

  

  belief points

 
 
  

 

  

 

  

  belief points

 

  

 
 
  

 

  

 

  

 

  

  belief points

figure     sum of discounted reward  top graphs  and estimate of the bound on the error  bottom
graphs  as a function of the number of selected belief points 

figure     pearl the nursebot  interacting with elderly people at a nursing facility
an important component of this task is finding the patient whenever it is time to issue a reminder 
this task shares many similarities with the tag problem presented in section      in this case 
however  a robot generated map of a real physical environment is used as the basis for the spatial
configuration of the domain  this map is shown in figure     the white areas correspond to free
space  the black lines indicate walls  or other obstacles  and the dark gray areas are not visible or
accessible to the robot  one can easily imagine the patients room and physiotherapy unit lying at
either end of the corridor  with a common area shown in the upper middle section 
the overall goal is for the robot to traverse the domain in order to find the missing patient and
then deliver a message  the robot must systematically explore the environment  reasoning about
both spatial coverage and human motion patterns  in order to find the person 
   

fip ineau   g ordon   t hrun

figure     map of the environment
    pomdp modeling
the problem domain is represented jointly by two state features  robotposition  personposition 
each feature is expressed through a discretization of the environment  most of the experiments
below assume a discretization of   meters  which means    discrete cells for each feature  for a total
of     states 
it is assumed that the person and robot can move freely throughout this space  the robots
motion is deterministically controlled by the choice of action  north  south  east  west   the robot
has a fifth action  delivermessage   which concludes the scenario when used appropriately  i e  
when the robot and person are in the same location  
the persons motion is stochastic and falls in one of two modes  part of the time  the person
moves according to brownian motion  e g   moves in each cardinal direction with p r        otherwise stays put   at other times  the person moves directly away from the robot  the tag domain of
section     assumes that the person always moves always moves away the robot  this is not realistic when the person cannot see the robot  the current experiment instead assumes that the person
moves according to brownian motion when the robot is far away  and moves away from the robot
when it is closer  e g      m   the person policy was designed this way to encourage the robot to
find a robust policy 
in terms of state observability  there are two components  what the robot can sense about its
own position  and what it can sense about the persons position  in the first case  the assumption
is that the robot knows its own position at all times  while this may seem like a generous  or
optimistic  assumption  substantial experience with domains of this size and maps of this quality
have demonstrated robust localization abilities  thrun et al          this is especially true when
planning operates at relatively coarse resolution    meters  compared to the localization precision
    cm   while exact position information is assumed for planning in this domain  the execution
phase  during which we actually measure performance  does update the belief using full localization
information  which includes positional uncertainty whenever appropriate 
regarding the detection of the person  the assumption is that the robot has no knowledge of
the persons position unless s he is within a range of   meters  this is plausible given the robots
sensors  however  even in short range  there is a small probability  p r         that the robot will
miss the person and therefore return a false negative 
in general  one could make sensible assumptions about the persons likely position  e g   based
on a knowledge of their daily activities   however we currently have no such information and therefore assume a uniform distribution over all initial positions  the persons subsequent movements
   

fia nytime p oint based a pproximations for l arge pomdp s

are expressed through the motion model described above  i e   a mix of brownian motion and purposeful avoidance  
the reward function is straightforward  r     for any motion action  r      when the
robot decides to delivermessage and it is in the same cell as the person  and r       when
the robot decides to delivermessage in the persons absence  the task terminates when the robot
successfully delivers the message  i e   a   deliverm essage and srobot   sperson    we assume a
discount factor of      
we assume a known initial belief  b    consisting of a uniform distribution over all states  this
is used both for selecting belief points during planning  and subsequently for executing and testing
the final policy 
the initial map  fig      of the domain was collected by a mobile robot  and slightly cleaned
up by hand to remove artifacts  e g   people walking by   we then assumed the model parameters
described here  and applied pbvi planning to the problem as such  value updates and belief point
expansions were applied in alternation until  in simulation  the policy was able to find the person
on     of trials  trials were terminated when the person is found or after     execution steps   the
final policy was implemented and tested onboard the publicly available carmen robot simulator  montemerlo  roy    thrun        
    comparative evaluation of pbvi and perseus
the subtask described here  with its     states  is beyond the capabilities of exact pomdp solvers 
furthermore  as will be demonstrated below  mdp type approximations are not equipped to handle
uncertainty of the type exhibited in this task  the main purpose of our analysis is to evaluate the
effectiveness of the point based approach described in this paper to address this problem  while the
results on the tag domain  section      hint at the fact that pbvi and other algorithms may be able
to handle this task  the more realistic map and modified motion model provide new challenges 
we begin our investigation by directly comparing the performance of pbvi  with ger belief points selection  with that of the perseus algorithm on this complex robot domain  perseus
was described in section    results presented below were produced using code provided by its authors  perseus         results for both algorithms assume that a fixed pomdp model was generated
by the robot simulator  this model is then stored and solved offline by each algorithm 
pbvi and perseus each have a few parameters to set  pbvi requires  number of new belief
points to add at each expansion  badd   and planning horizon for each expansion  h   perseus
requires  number of belief points to generate during random walk  b  and the maximum planning
time  t    results presented below assume the following parameter settings  badd       h      
b         t        both algorithms were fairly robust to changes in these parameters  
figure    summarizes the results of this experiment  these suggest a number of observations 
 as shown in figure    a   both algorithms find the best solution in a similar time  but
pbvi ger has better anytime performance than perseus  e g   a much better policy is found
given only     sec  
 as shown in figure    b   both algorithms require a similar number of  vectors 
 as shown in figure    c   pbvi ger requires many fewer beliefs 
   a     change in parameter value yielded sensibly similar results in terms of reward and number of  vectors 
though of course the time  memory  and number of beliefs varied 

   

fip ineau   g ordon   t hrun

 
  

   
pbvi ger
ger
perseus
qmdp

pbvi ger
perseus
   

  
  alpha vectors

reward

  
  
  
  

  
  
  

  
  
  
    
  

 

 

  

 

 

  
  
time  secs 

  

   
  

 

  

 

  

 

  

pbvi ger
perseus
 

  

  beliefs

 

  

 

  

 

  

 

    
  

 

  

 

  
time  secs 

 

  

 

  

 b 
memory requirement     alphas    beliefs   states

 a 

 

  
time  secs 

 

  

 

  

 c 

 

  

pbvi ger
perseus
 

  

 

  

 

  

 

  

 

  

 

  

 

  
time  secs 

 

  

 

  

 d 

figure     comparison of pbvi and perseus on robot simulation domain
 because it requires fewer beliefs  pbvi ger has much lower memory requirements  this is
quantified in figure    d  
these new results suggest that pbvi and perseus have similar performance if the objective is to
find a near optimal solution  and time and memory are not constrained  in cases where one is willing
to trade off accuracy for time  then pbvi may provide superior anytime performance  and in cases
where memory is limited  pbvis conservative approach with respect to belief point selection is
advantageous  both these properties suggest that pbvi may scale better to very large domains 
    experimental results with robot simulator
the results presented in above assume that the same pomdp model is used for planning and testing  i e   to compute the reward in figure    a    this is useful to carry out a large number of
experiments  the model however cannot entirely capture the dynamics of a realistic robot system 
therefore there is some concern that the policy learned by point based methods will not perform as
well on a realistic robot  to verify the robustness of our approach  the final pbvi control policy
was implemented and tested onboard the publicly available carmen robot simulator  montemerlo
et al         
   

fia nytime p oint based a pproximations for l arge pomdp s

the resulting policy is illustrated in figure     this figure shows five snapshots obtained from
a single run  in this particular scenario  the person starts at the far end of the left corridor  the
persons location is not shown in any of the figures since it is not observable by the robot  the
figure instead shows the belief over person positions  represented by a distribution of point samples
 grey dots in fig       each point represents a plausible hypothesis about the persons position  the
figure shows the robot starting at the far right end of the corridor  fig    a   the robot moves toward
the left until the rooms entrance  fig    b   it then proceeds to check the entire room  fig    c  
once relatively certain that the person is nowhere to be found  it exits the room  fig    d   and
moves down the left branch of the corridor  where it finally finds the person at the very end of the
corridor  fig    e  
this policy is optimized for any start position  for both the person and the robot   the scenario
shown in figure    is one of the longer execution traces since the robot ends up searching the entire
environment before finding the person  it is interesting to compare the choice of action between
snapshots  b  and  d   the robot position in both is practically identical  yet in  b  the robot chooses
to go up into the room  whereas in  d  the robot chooses to move toward the left  this is a direct
result of planning over beliefs  rather than over states  the belief distribution over person positions
is clearly different between those two cases  and therefore the policy specifies a very different course
of action 
figure    looks at the policy obtained when solving this same problem using the qmdp heuristic  four snapshots are offered from different stages of a specific scenario  assuming the person
started on the far left side and the robot on the far right side  fig    a   after proceeding to the
room entrance  fig    b   the robot continues down the corridor until it almost reaches the end
 fig    c   it then turns around and comes back toward the room entrance  where it stations itself
 fig    d  until the scenario is forcibly terminated  as a result  the robot cannot find the person
when s he is at the left edge of the corridor or in the room  whats more  because of the runningaway behavior adopted by the subject  even when the person starts elsewhere in the corridor  as the
robot approaches the person will gradually retreat to the left and similarly escape from the robot 
even though qmdp does not explicitly plan over beliefs  it can generate different policy actions
for cases where the state is identical but the belief is different  this is seen when comparing figure     b  and  d   in both of these  the robot is identically located  however the belief over person
positions is different  in  b   most of the probability mass is to the left of the robot  therefore it travels in that direction  in  d   the probability mass is distributed evenly between the three branches
 left corridor  room  right corridor   the robot is equally pulled in all directions and therefore stops
there  this scenario illustrates some of the strength of qmdp  namely  there are many cases where
it is not necessary to explicitly reduce uncertainty  however  it also shows that more sophisticated
approaches are needed to handle some cases 
these results show that pbvi can perform outside the bounds of simple maze domains  and
is able to handle realistic problem domains  in particular  throughout this evaluation  the robot
simulator was in no way constrained to behave as described in our pomdp model  sec        this
means that the robots actions often had stochastic effects  the robots position was not always
fully observable  and that belief tracking had to be performed asynchronously  i e   not always a
straightforward ordering of actions and observations   despite this misalignment between the model
assumed for planning  and the execution environment  the control policy optimized by pbvi could
successfully be used to complete the task 

   

fip ineau   g ordon   t hrun

 a  t  

 b  t  

 c  t   

 d  t   

 e  t   
figure     example of a pbvi policy successfully finding the person

   

fia nytime p oint based a pproximations for l arge pomdp s

 a  t  

 b  t  

 c  t   

 d  t   
figure     example of a qmdp policy failing to find the person

   

fip ineau   g ordon   t hrun

   discussion
this paper describes a class of anytime point based pomdp algorithms called pbvi  which combines point based value updates with strategic selection of belief points  to solve large pomdps 
further extensions to the pbvi framework  whereby value updates are applied to groups of belief
points according to their spatial distribution  are described by  pineau  gordon    thrun        
the main contributions pertaining to the pbvi framework are now summarized 
scalability  the pbvi framework is an important step towards truly scalable pomdp solutions 
this is achieved by bounding the policy size through the selection of a small set of belief points 
anytime planning  pbvi class algorithms alternates between steps of value updating and steps
of belief point selection  as new points are added  the solution improves  at the expense of increased
computational time  the trade off can be controlled by adjusting the number of points  the algorithm can be terminated either when a satisfactory solution is found  or when planning time is
elapsed 
bounded error  we provide a theoretical bound on the error of the approximation introduced
in the pbvi framework  this result holds for a range of belief point selection methods  and lead
directly to the development of a new pbvi type algorithm  pbvi ger  where estimates of the
error bound are used directly to select belief points  furthermore we find that the bounds can be
useful in assessing when to stop adding belief points 
exploration  we proposed a set of new point selection heuristics  which explore the tree of
reachable beliefs to select useful belief points  the most successful technique described  greedy
error reduction  ger   uses an estimate of the error bound on candidate belief points to select the
most useful points 
improved empirical performance  pbvi has demonstrated the ability to reduce planning time
for a number of well known pomdp problems  including tiger grid  hallway  and hallway   by
operating on a set of discrete points  pbvi algorithms can perform polynomial time value updates 
thereby overcoming the curse of history that paralyzes exact algorithms  the ger technique used to
select points allows us to solve large problems with fewer belief points than alternative approaches 
new problem domain  pbvi was applied to a new pomdp planning domain  tag   for which
it generated an approximate solution that outperformed baseline algorithms qmdp and incremental
pruning  this new domain has since been adopted as a test case for other algorithms  vlassis  
spaan        smith   simmons        braziunas   boutilier        poupart   boutilier        
this fosters an increased ease of comparison between new techniques  further comparative analysis
was provided in section     highlighting similarities and differences between pbvi and perseus 
demonstrated performance  pbvi was applied in the context of a robotic search and rescue
type scenario  where a mobile robot is required to search its environment and find a non stationary
individual  pbvis performance was evaluated using a realistic  independently developed  robot
simulator 
a significant portion of this paper is dedicated to a thorough comparative analysis of point based
methods  this includes evaluating a range of point based selection methods  as well as evaluating
mechanisms for ordering value updates  the comparison of point based selection techniques suggest that the ger method presented in section     is superior to more naive techniques  in terms of
ordering of value updates  the randomized strategy which is used in the perseus algorithm appears
effective to accelerate planning  a natural next step would be to combine the ger belief selection
heuristic with perseuss random value updates  we performed experiments along these lines  but
   

fia nytime p oint based a pproximations for l arge pomdp s

did not achieve any significant speed up over the current performance of pbvi or perseus  e g   as
reported in figure    a    it is likely that when belief points are chosen carefully  as in ger   each
of these points needs to be updated systematically and therefore there is no additional benefit to
using randomized value updates 
looking towards the future  it is important to remember that while we have demonstrated the
ability to solve problems which are large by pomdp standards  many real world domains far exceed
the complex of domains considered in this paper  in particular  it is not unusual for a problem to be
expressed through a number of multi valued state features  in which case the number of states grows
exponentially with the number of features  this is of concern because each belief point and each
 vector has dimensionality  s   where  s  is the number of states  and all dimensions are updated
simultaneously  this is an important issue to address to improve the scalability of point based value
approaches in general 
there are various existing attempts at overcoming the curse of dimensionality in pomdps 
some of thesee  g  the belief compression techniques by roy and gordon       cannot be
incorporated within the pbvi framework without compromising its theoretical properties  as discussed in section     others  in particular the exact compression algorithm by poupart and boutilier
        can be combined with pbvi  however  preliminary experiments in this direction have
yielded little performance improvement  there is reason to believe that approximate value compression would yield better results  but again at the expense of forgoing pbvis theoretical properties  the challenge therefore is to devise function approximation techniques that both reduce the
dimensionality effectively  while maintaining the convexity properties of the solution 
a secondary  but no less important  issue concerning the scalability of pbvi pertains to the
number of belief points necessary to obtain a good solution  while problems addressed thus far can
usually be solved with o  s   belief points  this need not be true  in the worse case  the number
of belief points necessary may be exponential in the plan length  the pbvi framework can accommodate a wide variety of strategies for generating belief points  and the greedy error reduction
technique seems particularly effective  however this is unlikely to be the definitive answer to belief
point selection  in more general terms  this relates closely to the well known issue of exploration
versus exploitation  which arises across a wide array of problem solving techniques 
these promising opportunities for future research aside  the pbvi framework has already
pushed the envelope of pomdp problems that can be solved with existing computational resources 
as the field of pomdps matures  finding ways of computing policies efficiently will likely continue
to be a major bottleneck  we hope that point based algorithms such as the pbvi will play a leading
role in the search for more efficient algorithms 

acknowledgments
the authors wish to thank craig boutilier  michael littman  andrew moore and matthew mason for
many thoughtful comments and discussions regarding this work  we also thank darius braziunas 
pascal poupart  trey smith and nikos vlassis  for conversations regarding their algorithms and
results  the contributions of michael montemerlo and nicholas roy in conducting the empirical
robot evaluations are gratefully acknowledged  finally  we thank three anonymous reviewers and
one dedicated editor  sridhar mahadevan  whose feedback significantly improved the paper  this
work was funded through the darpa mars program and nsfs itr program  project  robotic
assistants for the elderly  pi  j  dunbar jacob  
   

fip ineau   g ordon   t hrun

references
astrom  k  j          optimal control of markov decision processes with incomplete state estimation  journal of mathematical analysis and applications             
bellman  r          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j          neuro dynamic programming  athena scientific 
blum  a  l     furst  m  l          fast planning through planning graph analysis  artificial
intelligence                  
bonet  b          an epsilon optimal grid based algorithm for partially obserable markov decision
processes  in machine learning  proceedings of the      international conference  icml  
pp       
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
and computational leverage  journal of artificial intelligence research          
boyen  x     koller  d          tractable inference for complex stochastic processes  in proceedings of the fourteenth conference on uncertainty in artificial intelligence  uai   pp       
brafman  r  i          a heuristic variable grid solution method for pomdps  in proceedings of
the fourteenth national conference on artificial intelligence  aaai   pp         
braziunas  d     boutilier  c          stochastic local search for pomdp controllers  in proceedings of the nineteenth national conference on artificial intelligence  aaai   pp         
burgard  w   cremers  a  b   fox  d   hahnel  d   lakemeyer  g   schulz  d   steiner  w     thrun 
s          experiences with an interactive museum tour guide robot  artificial intelligence 
         
cassandra  a         
tonys
research ai pomdp code index html 

pomdp

page 

http   www cs brown edu 

cassandra  a   littman  m  l     zhang  n  l          incremental pruning  a simple  fast  exact
method for partially observable markov decision processes  in proceedings of the thirteenth
conference on uncertainty in artificial intelligence  uai   pp       
chapman  d          planning for conjunctive goals  artificial intelligence                
cheng  h  t          algorithms for partially observable markov decision processes  ph d  thesis 
university of british columbia 
dean  t     kanazawa  k          probabilistic temporal reasoning  in proceedings of the seventh
national conference on artificial intelligence  aaai   pp         
devroye  l          non uniform random variate generation  springer verlag  new york 
engelberger  g          handbook of industrial robotics  john wiley and sons 
fikes  r  e     nilsson  n  j          strips  a new approach to the application of theorem
proving to problem solving  artificial intelligence            
hauskrecht  m          incremental methods for computing bounds in partially observable markov
decision processes  in proceedings of the fourteenth national conference on artificial intelligence  aaai   pp         
   

fia nytime p oint based a pproximations for l arge pomdp s

hauskrecht  m          value function approximations for partially observable markov decision
processes  journal of artificial intelligence research           
jazwinski  a  m          stochastic processes and filtering theory  academic  new york 
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in partially
observable stochastic domains  artificial intelligence             
kalman  r  e          a new approach to linear filtering and prediction problems  transactions of
the asme  journal of basic engineering           
lacey  g     dawson howe  k  m          the application of robotics to a mobility aid for the
elderly blind  robotics and autonomous systems             
littman  m  l          algorithms for sequential decision making  ph d  thesis  brown university 
littman  m  l   cassandra  a  r     kaelbling  l  p       a   learning policies for partially obsevable environments  scaling up  tech  rep  cs        brown university  department of
computer science 
littman  m  l   cassandra  a  r     kaelbling  l  p       b   learning policies for partially obsevable environments  scaling up  in proceedings of twelfth international conference on
machine learning  pp         
lovejoy  w  s       a   computationally feasible bounds for partially observed markov decision
processes  operations research                
lovejoy  w  s       b   a survey of algorithmic methods for partially observable markov decision
processes  annals of operations research           
mcallester  d     roseblitt  d          systematic nonlinear planning  in proceedings of the ninth
national conference on artificial intelligence  aaai   pp         
monahan  g  e          a survey of partially observable markov decision processes  theory  models  and algorithms  management science             
montemerlo  m   roy  n     thrun  s          perspectives on standardization in mobile robot
programming  the carnegie mellon navigation  carmen  toolkit  in proceedings of the
ieee rsj international conference on intelligent robots and systems  iros   vol     pp  pp
         
paquet  s          distributed decision making and task coordination in dynamic  uncertain and
real time multiagent environments  ph d  thesis  universite laval 
penberthy  j  s     weld  d          ucpop  a sound  complete  partial order planning for adl 
in proceedings of the third international conference on knowledge representation and reasoning  pp         
perseus        http   staff science uva nl mtjspaan software approx 
pineau  j   gordon  g     thrun  s          applying metric trees to belief point pomdps  in
neural information processing systems  nips   vol     
pineau  j   montermerlo  m   pollack  m   roy  n     thrun  s          towards robotic assistants in
nursing homes  challenges and results  robotics and autonomous systems                  
pollack  m          planning technology for intelligent cognifitve orthotics  in proceedings of the
 th international conference on ai planning   scheduling  aips  
   

fip ineau   g ordon   t hrun

poon  k  m          a fast heuristic algorithm for decision theoretic planning  masters thesis  the
hong kong university of science and technology 
poupart  p     boutilier  c          value directed belief state approximation for pomdps  in
proceedings of the sixteenth conference on uncertainty in artificial intelligence  uai   pp 
       
poupart  p     boutilier  c          value directed compression of pomdps  in advances in neural
information processing systems  nips   vol     
poupart  p     boutilier  c          bounded finite state controllers  in advances in neural information processing systems  nips   vol     
rabiner  l  r          a tutorial on hidden markov models and selected applications in speech
recognition  proceedings of the ieee                
rosencrantz  m   gordon  g     thrun  s          locating moving entities in dynamic indoor
environments with teams of mobile robots  in second international joint conference on
autonomous agents and multiagent systems  aamas   pp         
roy  n     gordon  g          exponential family pca for belief compression in pomdps  in
advances in neural information processing systems  nips   vol      pp           
smith  t     simmons  r          heuristic search value iteration for pomdps  in proceedings of
the twentieth conference on uncertainty in artificial intelligence  uai  
sondik  e  j          the optimal control of partially observable markov processes  ph d  thesis 
stanford university 
spaan  m     vlassis  n          perseus  randomized point based value iteration for pomdps 
journal of artificial intelligence research  jair          
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
thrun  s   fox  d   burgard  w     dellaert  f          robust monte carlo localization for mobile
robots  artificial intelligence                  
vlassis  n     spaan  m  t  j          a fast point based algorithm for pomdps  in proceedings
of the belgian dutch conference on machine learning 
white  c  c          a survey of solution techniques for the partially observed markov decision
process  annals of operations research             
zhang  n  l     liu  w          planning in stochastic domains  problem characteristics and approximation  tech  rep  hkust cs       dept  of computer science  hong kong university of science and technology 
zhang  n  l     zhang  w          speeding up the convergence of value iteration in partially
observable markov decision processes  journal of artificial intelligence research        
   
zhou  r     hansen  e  a          an improved grid based approximation algorithm for pomdps 
in proceedings of the   th international joint conference on artificial intelligence  ijcai  
pp         

   

fi