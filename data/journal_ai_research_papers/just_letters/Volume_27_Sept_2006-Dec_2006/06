journal artificial intelligence research                 

submitted        published      

resource allocation among agents mdp induced
preferences
dmitri a  dolgov

ddolgov ai stanford edu

technical research department  ai   robotics group 
toyota technical center
     green road
ann arbor  mi        usa

edmund h  durfee

durfee umich edu

electrical engineering computer science
university michigan
     hayward st 
ann arbor  mi        usa

abstract
allocating scarce resources among agents maximize global utility is  general  computationally challenging  focus problems resources enable agents execute
actions stochastic environments  modeled markov decision processes  mdps  
value resource bundle defined expected value optimal mdp
policy realizable given resources  present algorithm simultaneously solves
resource allocation policy optimization problems  allows us avoid explicitly representing utilities exponentially many resource bundles  leading drastic
 often exponential  reductions computational complexity  use algorithm
context self interested agents design combinatorial auction allocating resources  empirically demonstrate effectiveness approach showing
can  minutes  optimally solve problems straightforward combinatorial
resource allocation technique would require agents enumerate      resource
bundles auctioneer solve np complete problem input size 

   introduction
problem resource allocation ubiquitous many diverse research fields
economics  operations research  computer science  applications ranging decentralized scheduling  e g   wellman  walsh  wurman    mackie mason        network routing  e g   feldmann  gairing  lucking  monien    rode        transportation
logistics  e g   sheffi        song   regan        bandwidth allocation  e g   mcmillan 
      mcafee   mcmillan         name few  core question resource allocation distribute set scarce resources among set agents  either cooperative
self interested  way maximizes measure global utility  social welfare
 sum agents utilities  one popular criteria 
many domains  agents utility obtaining set resources defined
agent accomplish using resources  example  value vehicle
delivery agent defined additional revenue agent obtain using
vehicle  however  figure best utilize resource  or set resources   agent
c
    
ai access foundation  rights reserved 

fidolgov   durfee

available
resources

available
actions

resource allocation
problem

planning problem
 mdp 

utility function
resources

best plan  
payoff

figure    dependency cycle  formulate planning problems  agents need
know resources get  utility functions  define
input resource allocation problem  depend solutions planning
problems 

often must solve non trivial planning problem actions might long term  nondeterministic effects  therefore  agents value set resources defined solution
planning problem  formulate planning problem agent needs know
resources obtain  leads cyclic dependencies  depicted figure     wherein
input resource allocation problem depends solution planning problem 
vice versa  unfortunately  anything simplest domains  neither resourceallocation planning problem solved closed form  making impossible
obtain parameterized solutions 
focus paper solving interdependent problems resource allocation
stochastic planning  main question consider allocate resources
way maximizes social welfare agents utility function agent
defined markov decision process  puterman        whose action set parameterized
resources  paper  specifically focus non consumable resources  such
vehicles  enable actions  consumed action execution 
briefly mention case consumable resources section    refer work
dolgov        detailed treatment 
assume agents mdps weakly coupled  meaning agents interact
resources  resources allocated  transition reward
functions mdps independent  model weakly coupled mdps connected
via shared resources similar meuleau  hauskrecht  kim  peshkin  kaelbling 
dean  boutilier        benazera  brafman  meuleau  hansen        
differs assume resources allocated once  prior actions
taken  one shot allocation assumption limits approach somewhat 
allows approach apply broadly non cooperative settings  without
assumption  game theoretic analysis agents interactions significantly
complex   importantly  allows us avoid state space explosion  due including
resource information mdp states   limits work finding
approximately optimal solutions non trivial problems 
main result presented paper thus new algorithm that 
conditions  optimally solves resource allocation policy optimization problems
simultaneously  considering two problems together  sidesteps dependency cycle
   

firesource allocation among agents mdp induced preferences

mentioned above  allows us avoid explicit representation utility functions
resource bundles  leading exponential reduction complexity combinatorial
resource allocation flat utility functions  empirically demonstrate resulting
algorithm scales well finding optimal solutions problems involving numerous agents
resources 
algorithm viewed contributing new approach dealing computational complexity resource allocation domains complex utility functions
linearly decomposable resources  due effects substitutability
complementarity   combinatorial allocation problems  finding optimal allocation np complete  often exponentially large  space resource bundles  rothkopf 
pekec    harstad         previous approaches addressing complexity included
determining classes utility functions lead tractable problems  as surveyed
de vries   vohra         iterative algorithms resource allocation preference elicitation  as surveyed sandholm   boutilier         concise languages expressing
agents preferences  sandholm        nisan        boutilier   hoos        boutilier        
novelty approach respect explicitly embraces underlying processes define agents utility functions  cases processes
modeled resource parameterized mdps  so  approach use
mdp based models concise language agents utility functions  importantly  directly exploits structure models drastically reduce computational
complexity simultaneously solving planning resource allocation problems 
context cooperative agents  approach viewed way solving
weakly coupled multiagent mdps  agents transition reward functions independent  space joint actions constrained  as  example  models used
singh cohn        meuleau et al          perspective  concept
resources viewed compact way representing interactions agents 
similarly model used bererton  gordon  thrun         however  work
differs number assumptions  moreover  algorithms easily modified work
models constraints joint actions modeled directly  for example 
via sat formulas  
non cooperative agents  apply resource allocation algorithm mechanismdesign problem  e g   mas colell  whinston    green         goal allocate
resources among agents way maximizes social welfare  given
participating agent selfishly maximizing utility  domains self interested
agents complex preferences exhibit combinatorial effects resources 
combinatorial auctions  e g   de vries   vohra        often used resource allocation 
generalized vickrey auction  gva   mackie mason   varian         extension vickrey clarke groves  vcg  mechanisms  vickrey        clarke        groves 
      combinatorial auctions  particularly attractive nice analytical
properties  as described section       develop variant vcg auction 
agents submit resource parameterized mdps bids  auctioneer simultaneously solves resource allocation policy optimization problems  thus retaining
compact representation agents preferences throughout process  describe extensions mechanism distributing computation encoding mdp information
reduce revelation private information 
   

fidolgov   durfee

remainder paper proceeds follows  brief review mdps
section    present  in section    model decision making agent  resourceparameterized mdp capacity constraints  analyze problem optimal policy
formulation resource parameterized capacity constrained mdp  study properties  present solution algorithm  based formulation  np complete 
problem mixed integer program 
building blocks  move multiagent setting present main
result  algorithm simultaneously allocating resources planning across agents
 section     based algorithm  design combinatorial auction allocating
resources among self interested agents  describe distributed implementation
mechanism  discuss techniques preserving information privacy  section   
analyze computational efficiency approach  empirically demonstrating exponential reductions computational complexity  compared straightforward combinatorial
resource allocation algorithm flat utility functions  finally  section    conclude
discussion possible generalizations extensions approach  conciseness
better readability  proofs generalizations deferred appendices 

   markov decision processes
base model agents decision problems infinite horizon fully observable mdps
total expected discounted reward optimization criterion  although results
applicable classes mdps  mdps average per step rewards  
section introduces notation assumptions  serves brief overview
basic mdp results  see  example  text puterman        detailed discussion
material section  
classical single agent  unconstrained  stationary  fully observable mdp defined
  tuple hs  a  p  ri  where 
finite set states agent in 
finite set actions agent execute 
p            defines transition function  probability agent
goes state upon execution action state p  s  a  

p assume that  action  corresponding transition matrix stochastic 
p  s  a      s  a 
r     r defines reward function  agent obtains reward r s  a 
executes action state s  assume rewards bounded 
discrete time fully observable mdp  time step  agent observes current
state system chooses action according policy  policy said
markovian  or history independent  choice action depend history
states actions encountered past  rather current state
time  if  addition that  policy depend time  called stationary 
definition  stationary policy always markovian  deterministic policy always prescribes
execution action state  randomized policy chooses actions
according probability distribution 
   

firesource allocation among agents mdp induced preferences

following standard notation  puterman         refer different classes policies
xy   x    h  m  s  specifies whether policy history dependent  markovian 
stationary     r  d  specifies whether policy randomized deterministic
 e g   class stationary deterministic policies labeled sd    obviously  hy
sy xr xd   history dependent randomized policies hr stationary
deterministic policies sd least general  respectively 
stationary randomized policy thus mapping states probability distributions
actions               s  a  defines probability action
executed state s  stationary deterministic policy viewed degenerate case
randomized policy one action state nonzero
probability executed 
unconstrained discounted mdp  goal find policy maximizes
total expected discounted reward infinite time horizon  

hx

u        e
  t rt       
   
t  

       discount factor  a unit reward time     worth
agent reward time t   rt  random  reward agent receives time t 
whose distribution depends policy initial distribution state space
           
one important results theory mdps states that  unconstrained discounted mdp total expected reward optimization criterion  always exists optimal policy stationary  deterministic  uniformly optimal 
latter term means policy optimal distributions starting state  
several commonly used ways finding optimal policy  central
concept value function policy  v     r  v  s  expected
cumulative value reward agent would receive started state behaved
according policy   given policy   value every state unique solution
following system  s  linear equations 
x
x
v  s   
r s  a  s  a   
p  s  a v    
s 
   




find optimal policy  handy consider optimal value function v     r 
v  s  represents value state s  given agent behaves optimally 
optimal value function satisfies following system  s  nonlinear equations 
h

x
v  s    max r s  a   
p  s  a v     
s 
   




v 

given optimal value function
optimal policy simply act greedily respect

v  with method tie breaking case multiple optimal actions  
h

 
p
  arg maxa r s  a    p  s  a v     
 s  a   
   
  otherwise 
   notation   x y exponent  xy superscript 
   uniform optimality policies reason included component textbook mdp 

   

fidolgov   durfee

one possible ways solving optimal value function formulate
nonlinear system     linear program  lp   s  optimization variables v s 
 s  a  constraints 
x
min
 s v s 


subject to 

   

v s  r s  a   

x

p  s  a v   

s  a 



arbitrary constant vector  s  positive components   s      s   
many problems  including ones focus paper   useful consider equivalent dual lp  s  a  optimization variables x s  a   s 
constraints  
xx
r s  a x s  a 
max
x





subject to 
x
xx
x   a 
x s  a p  s  a       




   
s 



x s  a   

s  a 

optimization variables x s  a  often called visitation frequencies occupation measure policy  think initial probability distribution  x s  a 
interpreted
p total expected number times action executed state s 
then  x s    x s  a  gives total expected flow state s  constraints
lp interpreted conservation flow states 
optimal policy computed solution dual lp as 
x s  a 
 
   
 s  a    p
x s  a 
p
non negativity guarantees x s  a      s  general  appears
lead randomized policies  however  bounded lp n constraints always basic
feasible solution  e g   bertsimas   tsitsiklis         definition
n non zero components  strictly positive  basic feasible solution lp    
precisely  s  nonzero components  one state   guarantees existence
optimal deterministic policy  policy easily obtained lp solvers
 e g   simplex always produce solutions map deterministic policies  
furthermore  mentioned above  unconstrained discounted mdps  always
exist policies uniformly optimal  optimal initial distributions  
dual lp     yields uniformly optimal policies strictly positive used  however 
   overloading objective function coefficients initial probability distribution
mdp earlier intentional explained shortly 
   note authors  e g   altman        prefer opposite convention      called dual 
     primal 

   

firesource allocation among agents mdp induced preferences

solution  x  dual lp retains interpretation expected number times state
visited action executed initial probability distribution
used lp 
main benefit dual lp     manifested constrained mdps  altman       
kallenberg        heyman   sobel         action  addition producing
reward r s  a   incurs vector costs k  s  a      r k     k   problem maximize expected reward  subject constraints expected costs 
constrained models type arise many domains  telecommunication applications  e g   ross   chen        ross   varadarajan         often desirable
maximize expected throughput  subject conditions average delay  problems 
constraints imposed expected costs  solved polynomial time
using linear programming simply augmenting dual lp     following linear
constraints 
xx
k  s  a x s  a  bk  
k     k  
   




bk upper bound expected cost type k  resulting constrained mdp
differs standard unconstrained mdp  particular  deterministic policies
longer optimal  uniformly optimal policies not  general  exist problems
 kallenberg        
reason easily augmentable constraints  dual lp    
forms basis approach  however  constraints arise resource allocation
problems focus paper different linear constraints     
leading different optimization problems different properties requiring different
solution techniques  as described detail section    
conclude background section introducing simple unconstrained mdp
serve basis running example  refer throughout rest
paper 
example   consider simple delivery domain  depicted figure    agent
obtain rewards delivering furniture  action a    delivering appliances  action a    
delivering appliances produces higher rewards  as shown diagram  
damage delivery vehicle  agent delivers furniture  damage vehicle
negligible  whereas agent delivers appliances  vehicle guaranteed function
reliably first year  state s      state s        probability failure 
per year  vehicle serviced  action a     resetting condition  expense
lowering profits  truck break  state s     repaired  action a    
significant negative impact profits  assume discount factor       
optimal value function v  s          v  s          v  s         
corresponding optimal occupation measure  assuming uniform   following  listing
non zero elements   x s    a         x s    a         x s    a         maps
optimal policy dictates agent start delivering appliances  action a 
state s     service vehicle first year  action a  state s     fix
vehicle ever gets broken  a  s     the latter zero probability happening
policy agent starts state s  s    

   

fidolgov   durfee

a   deliver furniture
p  
r  

s 
 initial 

a   fix truck
p  
r  

a   deliver furniture
p  
r  

a   deliver appliances
p  
r   
a   service truck
p  
r  

s 
  nd year 
a   deliver appliances
p    
r   
p    
r   

s 
 truck broken 

figure    unconstrained mdp example delivery domain  transition probabilities  p 
rewards  r  shown diagram  actions shown result transition
state reward  noop action a  corresponds
nothing  change state produces zero reward 

   agent model  resource parameterized mdp
section  introduce model decision making agent  describe
single agent stochastic policy optimization problem defines agents preferences
resources  show that  single agent problem  formulated mdp whose
action set parameterized resources available agent  stationary deterministic
policies optimal  uniformly optimal policies not  general  exist  show
problem finding optimal policies np complete  finally  present policyoptimization algorithm  based formulation problem mixed integer linear
program  milp  
model agents resource parameterized mdp follows  agent set
actions potentially executable  action requires certain combination
resources  capture local constraints sets resources agent use  use
concept capacities  resource capacity costs associated it 
agent capacity constraints  example  delivery company needs vehicles loading
equipment  resources allocated  make deliveries  execute actions   however 
equipment costs money requires manpower operate  the agents local capacity
costs   therefore  amount equipment agent acquire successfully utilize
constrained factors budget limited manpower  agents local capacity
bounds   two layer model capacities resources represented separately might
seem unnecessarily complex  why fold together impose constraints directly
resources    separation becomes evident useful multiagent model
discussed section    emphasize difference here  resources items
allocated among agents  capacities define inherent limitations individual
agent combinations resources usefully possess 
agents optimization problem choose subset available resources
violate capacity constraints  best policy feasible bundle
resources yields highest utility  words  single agent problem analyzed
   

firesource allocation among agents mdp induced preferences

section constraints total resource amounts  they introduced
multiagent problem next section   constraints due agents
capacity limits  adding limited resource amounts single agent model would
simple matter  since constraints handled simple pruning agents
action space  further  note without capacity constraints  single agent problem
would trivial  would always optimal agent simply take resources
potential use  however  presence capacity constraints  face problem
similar cyclic dependency figure    resource selection problem requires
knowing values resource bundles  defined planning problem 
planning problem ill defined resource bundle chosen  section 
focus single agent problem selecting optimal subset resources satisfies
agents capacity constraints assume agent value acquiring additional
resources exceed capacity bounds 
resources model outlined non consumable  i e   actions require
resources consume execution  mentioned introduction 
work focus non consumable resources  briefly outline case
consumable resources section   
model agents optimization problem n tuple hs  a  p  r  o    c   
b  i 
hs  a  p  ri standard components mdp  defined earlier section   
set resources  e g      production equipment  vehicle           use
refer resource type 
    r function specifies resource requirements actions 
 a  o  defines much resource action needs executable  e g  
 a  vehicle      means action requires one vehicle  
c set capacities agent  e g   c    space  money  manpower          
use c c refer capacity type 
  c   r function specifies capacity costs resources   o  c  defines
much capacity c c unit resource requires  e g    vehicle  money   
       defines monetary cost vehicle   vehicle  manpower      means
two people required operate vehicle  

b   c   r specifies upper bound capacities 
b c  gives upper bound
capacity c c  e g  
b money               defines budget constraint 

b manpower      specifies size workforce  
    r initial probability distribution   s  probability agent
starts state s 
goal find policy yields highest expected reward  conditions resource requirements policy exceed capacity bounds
   

fidolgov   durfee

agent  words  solve following mathematical program  
max u     


subject to 
n
x
x

 o  c  max  a  o h
 s  a 
b c  




   
c c 



h heaviside step function nonnegative argument  defined as 
 
  z     
h z   
  z     
constraint     interpreted follows  argument h nonzero
thep
policy assigns nonzero probability using action least one state  thus 
function
h   s  a   serves indicator

us whether agent plans use
p tells
action policy  max  a  o h   s  a   tells us much resource
agent needs policy  take max respect a  resource
used different actions  therefore  summed resources o  left hand
side gives us total requirements policy terms capacity c 
greater bound
b c  
following example illustrates single agent model 
example   let us augment example   follows  suppose agents needs obtain
truck perform delivery actions  a  a     truck required service
repair actions  a  a     further  deliver appliances  agent needs acquire
forklift  needs hire mechanic able repair vehicle  a     noop
action a  requires resources  maps model three resources  truck  forklift 
mechanic      ot     om    following action resource costs  listing
non zero ones  
 a    ot         a    ot         a            a    ot         a    ot         a    om       
moreover  suppose resources  truck ot   forklift   mechanic om  
following capacity costs  there one capacity type  money  c    c    
 ot   c          of   c          om   c        
agent limited budget
b      can  therefore acquire two
three resources  means optimal solution unconstrained problem
example   longer feasible 

let us observe mdp based model agents preferences presented
fully general discrete indivisible resources  i e   non decreasing utility function
resource bundles represented via resource constrained mdp model described
above 
   formulation assumes stationary policy  supported argument section     

   

firesource allocation among agents mdp induced preferences

theorem   consider finite set n indivisible resources    oi    i     n   
n available units resource  then  non decreasing utility function
defined resource bundles f       m n   r  exists resource constrained mdp
hs  a  p  r  o    c   
b   with resource set o  whose induced utility function
resource bundles f   words  every resource bundle z     m n  
value optimal policy among whose resource requirements exceed z
 call set  z   f  z  
f       m n   r  hs  a  p  r  o    c   
b   
n
h

x


z     m n    z    max  a  oi  h
 s  a  zi   max u        f  z  




 z 

proof  see appendix a   

let us comment theorem   establishes generality mdp based preference model introduced section  construction used proof little practical interest  requires mdp exponentially large state action space  indeed 
advocate mapping arbitrary unstructured utility functions exponentially large
mdps general solution technique  rather  contention techniques apply domains utility functions induced stochastic decision making process
 modeled mdp   thus resulting well structured preferences resources
exploited drastically lower computational complexity resource allocation
algorithms 
    properties single agent constrained mdp
section  analyze constrained policy optimization problem      namely 
show stationary deterministic policies optimal problem  meaning
necessary consider randomized  history dependent policies  however  solutions
problem     not  general  uniformly optimal  optimal initial distribution  
furthermore  show     np hard  unlike unconstrained mdps 
solved polynomial time  littman  dean    kaelbling        
begin showing optimality stationary deterministic policies     
following  use hr refer class history dependent randomized policies  the
general policies   sd hr refer class stationary deterministic
policies 
theorem   given mdp   hs  a  p  r  o    c   
b  resource capacity
hr
constraints  exists policy
feasible solution   exists
stationary deterministic policy sd sd feasible  expected total reward
sd less  
hr   sd sd   u   sd     u     
proof  see appendix a   

result theorem   surprising  intuitively  stationary deterministic
policies optimal  history dependence increase utility policy 
   

fidolgov   durfee

using randomization increase resource costs  latter true including action policy incurs costs terms resources regardless
probability executing action  or expected number times action
executed   true dealing non consumable resources  property hold mdps consumable resources  as discuss detail
section    
show uniformly optimal policies always exist constrained
problem  result well known another class constrained mdps  constraints
imposed total expected costs proportional expected number
times corresponding actions executed  discussed earlier section     mdps
constraints arise  example  bounds imposed expected usage
consumable resources  mentioned section    problems solved using
linear programming augmenting dual lp     linear constraints expected
costs      below  establish result problems non consumable resources
capacity constraints 
observation   always exist uniformly optimal solutions     
words  exist two constrained mdps differ initial conditions   
hs  a  p  r  o    c   
b      hs  a  p  r  o    c   
b    i  policy
optimal problems simultaneously  i e   two policies  
optimal solutions     respectively  following holds 
u        u         

u          u          

    

demonstrate observation example 
example   consider resource constrained problem example    easy see
initial conditions              the agent starts state s  certainty  
optimal policy states s  s  example    s  a  s  a    
which  given initial conditions  results zero probability reaching state s   to
noop a  assigned   policy requires truck forklift  however 
agent starts state s                  optimal policy fix truck  execute a 
s     resort furniture delivery  do a  s  assign noop ao s   
never visited   policy requires mechanic truck  two policies
uniquely optimal corresponding initial conditions  suboptimal initial
conditions  demonstrates uniformly optimal policy exists example 
intuition behind fact uniformly optimal policies not  general  exist
constrained mdps since resource information part mdp state
space  constraints imposed resource usage  principle bellman optimality hold  optimal actions different states cannot chosen independently  
given constrained mdp  possible construct equivalent unconstrained mdp
standard properties optimal solutions  by folding resource information
state space  modeling resource constraints via transition function   resulting
state space exponential number resources 
analyze computational complexity optimization problem     
   

firesource allocation among agents mdp induced preferences

theorem   following decision problem np complete  given instance mdp
hs  a  p  r  o    c   
b  resources capacity constraints  rational number  
exist feasible policy   whose expected total reward  given   less  
proof  see appendix a   

note complexity result stems limited capacities agents
fact define resource requirements policy set resources
needed carry actions nonzero probability executed  if 
however  defined constraints expected resource requirements  actions
low probability executed would lower resource requirements  optimal policies
would randomized  problem would equivalent knapsack continuously
divisible items  solvable polynomial time via lp formulation mdps
linear constraints       
    milp solution
analyzed properties optimization problem      present
formulation     mixed integer linear program  milp   given established
np completeness     previous section  milp  also np complete  reasonable
formulation allows us reap benefits vast selection efficient algorithms
tools  see  example  text wolsey       references therein  
section rest paper assume resource requirements
actions binary  i e    a  o            make assumption simplify
discussion  limit generality results  briefly describe case
non binary resource costs appendix b completeness  refer work
dolgov        detailed discussion examples 
let us rewrite     occupation measure coordinates x adding constraints
    standard lp occupancy coordinates      noticing  for states
nonzero probability visited   s  a  x s  a  either zero nonzero simultaneously 
x
x


x s  a   
a 
 s  a    h
h




that   a  o            total resource requirements policy simplified
follows 
x

n
x
x

max  a  o h
x s  a    h
 a  o 
x s  a   
o 
    








get following program x 
xx
max
x s  a r s  a 
x





subject to 
x
xx
x   a 
x s  a p  s  a       


x



 o  c h



x


s 



 a  o 

x


x s  a 
b c  

c c 



x s  a    

s  a 
   

    

fidolgov   durfee

challenge solving mathematical program constraints nonlinear
due heaviside function h 
linearize heaviside function  augment original
variables
poptimization
x
p
set  o  binary variables  o           o    h
 a  o 
x s  a   
words   o  indicator variable shows whether policy requires resource o 
using  o   rewrite resource constraints      as 
x
 o  c  o 
b c  
c c 
    


linear   synchronize x via following linear inequalities 
x
x
  x
 a  o 
x s  a   o  
o 
    




p
p
x maxo  a  o  x s  a  normalization constant  upper bound argument h   used  bound x guaranteed
exist
p
  max
 a 
o  
since

discounted
problems 

example 


use
x
 
  

 


p
 
x valid occupation measure mdp
s a x s  a        
 
discount factor  
putting together  problem     finding optimal policies resource constraints formulated following milp 
xx
max
x s  a r s  a 
x 





subject to 
x
xx
x   a 
x s  a p  s  a       


x



s 



 o  c  o 
b c  

c c 

x

o 

    



  x

 a  o 



x

x s  a   o  



x s  a    

s  a 

 o         

o 

illustrate milp construction example 
example   let us formulate milp constrained problem example    recall problem three resources    ot     om    truck  forklift 
mechanic   one capacity type c    c     money   actions following resource
requirements  again  listing nonzero ones  
 a    ot         a    ot         a            a    ot         a    ot         a    om      
p
p
   instead using single x resources  different x o   a  o  x s  a  used
every resource  leading uniform normalization potentially better numerical stability
milp solver 

   

firesource allocation among agents mdp induced preferences

resources following capacity costs 
 ot   c        

 of   c        

 om   c        

agent limited budget  i e   capacity bound 
b c        
compute optimal policy arbitrary   formulate problem
milp described above  using binary variables  o      ot     of     om    
express constraint capacity cost following inequality 
  ot       of       om     
constraints synchronizep
occupation measure x binary indicators  o  
set x         maxo  a  o             combining
constraints       get milp    continuous   binary variables 
 s     c     o                  constraints  not counting last two sets range constraints  

mentioned earlier  even though solving programs is  general  np complete
task  wide variety efficient algorithms tools so  therefore 
one benefits formulating optimization problem     milp allows
us make use highly efficient existing tools 

   multiagent resource allocation
consider multiagent problem resource allocation several agents 
resource preferences agents defined constrained mdp model
described previous section  reiterate main assumptions problem 
   weak coupling  assume agents weakly coupled  meuleau et al         
i e   interact shared resources  resources allocated  agents transitions rewards independent  assumption critical
results  
   one shot resource allocation  resources distributed agents
start executing mdps  reallocation resources mdp
phase  assumption critical results  allowing reallocation resources
would violate weak coupling assumption 
   initial central control resources  assume beginning
resource allocation phase  resources controlled single authority 
standard sell auction setting  problems resources distributed
   agents cooperative  assumption weak coupling relaxed  at expense
increase complexity   milp based algorithm simultaneously performing policy optimization resource allocation applied consider joint state spaces interacting
agents  self interested agents  violation weakly coupled assumption would mean
agents would playing markov game  shapley        resources allocated  would
significantly complicate strategic analysis agents bidding strategies initial resource
allocation 

   

fidolgov   durfee

among agents begin with  face problem designing computationallyefficient combinatorial exchange  parkes  kalagnanam    eso        
complicated problem outside scope work  however  many
ideas presented paper could potentially applicable domain well 
   binary resource costs  before  assume agents resource costs binary 
assumption limiting  case non binary resources discussed
appendix b 
formally  input resource allocation problem consists following 
set agents  use refer agent 
 hs  a  pm   rm      
bm i  collection weakly coupled single agent mdps 
defined single agent model section    simplicity  without loss
generality  assume agents state action spaces a 
transition reward functions pm rm   initial conditions  
well resource requirements            capacity bounds

bm   c   r  assume agents discount factor  
assumption trivially relaxed 
c sets resources capacities  defined exactly single agent
model section   
  c   r specifies capacity costs resources  defined exactly
single agent model section   
b     r specifies upper bound amounts shared resources  this
defines additional bound multiagent problem  
given above  goal design mechanism allocating resources
agents economically efficient way  i e   way maximizes social welfare
agents  one often used criteria mechanism design   would
mechanism efficient computational standpoint 
example   suppose two delivery agents  mdp capacity constraints
first agent exactly defined previously examples      mdp
second agent almost first agent  difference
gets slightly higher reward delivering appliances  r   s    a          whereas
r   s    a         first agent   suppose two trucks  one forklift  one
mechanic shared two agents  bounds specified follows 
b ot       

b of       

b om       

problem decide agent get forklift  get
mechanic  trucks plentiful example  

   

firesource allocation among agents mdp induced preferences

    combinatorial auctions
previously mentioned  problem finding optimal resource allocation among
self interested agents complex valuations combinations resources arises
many different domains  e g   ferguson  nikolaou  sairamesh    yemini        wellman
et al         often called combinatorial allocation problem  natural widely
used mechanism solving problems combinatorial auction  ca   e g   de vries
  vohra         ca  agent submits set bids resource bundles
auctioneer  decides resources agent get price 
consider problem allocating among set agents set indivisible resources o  total quantity resource bounded b o   earlier
simplifying assumption actions resource requirements binary implies agents
interested bundles contain one unit particular resource 
combinatorial auction  agent submits bid bm
w  specifying much
agent willing pay  every bundle w w value um
w     
cases  possible express bids without enumerating bundles  for example  using
xor bidding language  sandholm        necessary consider bundles
strictly positive value  subset bundle value  
techniques often reduce complexity resource allocation problem  not 
general  avoid exponential blow number bids  therefore  describe
simplest combinatorial auction flat bids  noted many concise
bidding languages exist special cases reduce number explicit bids 
collecting bids  auctioneer solves winner determination problem
 wdp   solution prescribes resources distributed among
  utility um q
agents prices  agent wins bundle w price qw
w
w
 we assuming risk neutral agents quasi linear utility functions   thus  optimal
bidding strategy agent depends auctioneer allocates resources sets
prices 
vickrey clarke groves  vcg  mechanisms  vickrey        clarke        groves       
widely used family mechanisms certain attractive properties  discussed detail below   instantiation vcg mechanism context
combinatorial auctions generalized vickrey auction  gva   mackie mason   varian         allocates resources sets prices follows  given bids bm
w
agents  auctioneer chooses allocation maximizes sum agents bids 
problem np complete  rothkopf et al         expressed following inm          indicator variables
teger program  optimization variables zw
show whether bundle w assigned agent m  nwo          specifies whether bundle w
contains o  

   related algorithms solving wdp  e g   sandholm         use
integer program      representative formulation class algorithms perform search
space binary decisions resource bundles 

   

fidolgov   durfee

x

max
z

x


zw
bw

mm ww

subject to 
x

zw
  

m 

    

ww

x

x

mm

ww


zw
nwo b o  

o 

first constraint      says agent receive one bundle 
second constraint ensures total amount resource assigned agents
exceed total amount available  notice milp      performs summation
exponentially large sets bundles w w   outlined above  auction xor
bidding  sets would typically smaller  but  general  still exponentially large 
gva assigns resources according optimal solution ze      sets payment
agent to 
x


m  m 
qw
  vm

zew
bw  
    
m    m

vm
value      participate auction  the optimal
value submit bids   second term sum agents bids
solution ze wdp participating 
gva number nice properties  strategy proof  meaning dominant

strategy every agent bid true value every bundle  bm
w   uw   auction
economically efficient  meaning allocates resources maximize social
welfare agents  because  agents bid true values  objective function
     becomes social welfare   finally  gva satisfies participation constraint 
meaning agent decreases utility participating auction 
straightforward way implement gva mdp based problem following 
let agent enumerate resource bundles w satisfy local capacity
constraints defined
bm  c   this sufficient mdp model implies free disposal
resources agents  make assumption auctioneer  
bundle w w   agent would determine feasible action set a w  formulate
mdp  w    hs  a w   pm  w   rm  w   i  pm  w  rm  w  transition
reward functions defined pruned action space a w   every agent would
solve  w  corresponding feasible bundle find optimal policy
em  w   whose



expected discounted reward would define value bundle w  uw   u  e
 w     
mechanism suffers two major complexity problems  first  agents
enumerate exponential number resource bundles compute value
solving corresponding  possibly large  mdp  second  auctioneer solve
np complete winner determination problem exponentially large input  following
sections devoted tackling complexity problems 

example   consider two agent problem described example    two trucks  one
forklift  services one mechanic auctioned off  using straightforward
version combinatorial auction outlined above  agent would consider
   

firesource allocation among agents mdp induced preferences

  o           possible resource bundles  since resource requirements agents
binary  neither agent going bid bundle contains two trucks   every
resource bundle  agent formulate solve corresponding mdp
compute utility bundle 
example  assume agents start state s   different initial conditions
would result different expected rewards  thus different utility functions   value
null resource bundle agents would    since action would able
execute noop a     hand  value bundle  ot     om              
contains resources would      first agent       second one 
value bundle           agent would value            since
optimal policies initial conditions put s  require mechanic  
agents submit bids auctioneer  solve wdp via
integer program       m   o               binary variables  given above 
optimal way allocate resources would assign truck agents 
forklift second agent  mechanic either  or neither  two  thus 
agents would receive bundles                      respectively  resulting social welfare
                    however  least one agents non zero probability
starting state s    value resource bundles involving mechanic would change
drastically  would optimal resource allocation social value 

    avoiding bundle enumeration
avoid enumerating resource bundles non zero value agent  two things
required  i  mechanism support concise bidding language allows
agent express preferences auctioneer compact manner  ii 
agents able find good representation preferences language 
simple way achieve model create auction agents submit
specifications resource parameterized mdps auctioneer bids  language
compact and  given assumption agent formulate planning problem
mdp  require additional computation agents  however 
changes communication protocol agents auctioneer  similarly
concise bidding languages  sandholm        nisan        boutilier   hoos       
boutilier         such  simply moves burden solving valuation problem
agents auctioneer  lead gains computational
efficiency  mechanism implications information privacy issues 
agents reveal local mdps auctioneer  which might want
do   nevertheless  build idea increase efficiency solving
valuation winner determination problems keeping agents
mdp information private  address ways maintaining information privacy next
section  moment focus improving computational complexity agents
valuation auctioneers winner determination problems 
question pose section follows  given bid agent consists mdp  resource information capacity bounds hs  a  pm   rm      
bm i 
auctioneer formulate solve winner determination problem efficiently
   

fidolgov   durfee

simply enumerating agents resource bundles solving standard integer
program      exponential number binary variables 
therefore  goal auctioneer find joint policy  a collection single agent
policies weak coupling assumption  maximizes sum expected total
discounted rewards agents  conditions that  i  agent assigned set
resources violates capacity bound
bm  i e   agent assigned resources
carry   ii  total amounts resources assigned agents exceed
global resource bounds b o   i e   cannot allocate agents resources
available   problem expressed following mathematical program 
x
max
um      




subject to 
x
x

 o  c h  a  o 
 s  a 
bm  c  


x

c c  m 

    




 a  o h

x




 s  a  b o  

o 





obviously  decision version problem np complete  subsumes singleagent mdp capacity constraints  np completeness shown section     
moreover  problem remains np complete even absence single agent capacity
constraints  indeed  global constraint amounts shared resources sufficient
make problem np complete  shown straightforward reduction
knapsack  similar one used single agent case section     
linearize      similarly single agent problem section      yielding
following milp  simply multiagent version       recall assumption
section resource requirements binary  
xxx
max
xm  s  a rm  s  a 
x 







subject to 
x
xx
xm    a 
xm  s  a pm   s  a       


x



s  m 



 o  c   o 
bm  c  

c c  m 
    



x



 o  b o  

o 



  x

x

 a  o 



x

xm  s  a   o  

o  m 



xm  s  a    

s  a  m 



 o         

o  m 

x maxo m  a  o  xm  s  a  upper bound argument h   
used normalization  single agent case  bound guaranteed exist
discounted mdps easy obtain 
p

p

   

firesource allocation among agents mdp induced preferences

milp      allows auctioneer solve wdp without enumerate
possible resource bundles  compared standard wdp formulation      
order  m   o  binary variables        m  o  binary variables 
exponential reduction attained exploiting knowledge agents mdp based
valuations simultaneously solving policy optimization resource allocation problems  given worst case solution time milps exponential number
integer variables  reduction significant impact worst case performance
mechanism  average case running time reduced drastically  demonstrated
experiments  presented section   
example   apply mechanism discussed running example alternative straightforward combinatorial auction presented example    winnerdetermination milp      look follows   m  s  a                   continuous occupation measure variables xm    m  o               binary variables  o  
 m  s               conservation of flow constraints involve continuous
variables only  well  m  c     o     m  o                            constraints
involve binary variables 
capacity constraints agents exactly single agent case described
example    global resource constraints be 
   ot        ot     

   of        of     

   om        om     

notice example one binary decision variable per resource per agent
 yielding   variables simple problem   exponentially fewer
number binary variables straightforward ca formulation example   
requires one binary variable per resource bundle per agent  yielding    variables
problem   given milps np complete number integer variables 
reduction      variables noticeable even small problem one
lead drastic speedup larger domains 

mechanism described instantiation gva  well known
properties vcg mechanisms  auction strategy proof  the agents incentive
lie auctioneer mdps   attains socially optimal resource allocation 
agent decreases utility participating auction 
sum results section  agents submit mdp information auctioneer instead valuations resource bundles  essentially
removed computational burden agents time significantly simplified auctioneers winner determination problem  the number integer variables
wdp reduced exponentially  
    distributing winner determination problem
unlike straightforward combinatorial auction implementation discussed earlier section      agents shared computational burden auctioneer 
mechanism section      agents submit information auctioneer
idle waiting solution  suggests potential improvements
computational efficiency  indeed  given complexity milps  would beneficial
   

fidolgov   durfee

exploit computational power agents offload computation
auctioneer back agents  we assume agents cost helping
would prefer outcome computed faster    thus  would distribute
computation winner determination problem       common objective distributed algorithmic mechanism design  feigenbaum   shenker        parkes   shneidman 
        
concreteness  base algorithm section branch bound
method solving milps  wolsey         exactly techniques work
milp algorithms  e g   cutting planes  perform search space lp
relaxations milp  branch bound milps binary variables  lp relaxations created choosing binary variable setting either      relaxing
integrality constraints binary variables  solution lp relaxation happens integer valued  provides lower bound value global solution 
non integer solution provides upper bound current subproblem   combined
lower bounds  used prune search space 
thus  simple way auctioneer distribute branch bound algorithm
simply farm lp relaxations agents ask solve lps  however 
easy see mechanism strategy proof  indeed  agent tasked
performing computation determining optimal resource allocation
associated payments could benefit lying outcome computation
auctioneer  common phenomenon distributed mechanism implementations 
whenever wdp calculations offloaded agent participating auction 
agent might able benefit sabotaging computation  several methods
ensuring strategy proofness distributed implementation  approach best
suited problem based idea redundant computation  parkes   shneidman 
         multiple agents asked task disagreement
carefully punished discourage lying  rest section  demonstrate
easy implement case 
basic idea simple  let auctioneer distribute lp relaxations agents 
check solutions re solve problems agents return incorrect solutions
 this would make truthful computation weakly dominant strategy agents 
nonzero punishment used achieve strong dominance   strategy
auctioneer removes incentive agents lie yields exactly solution
centralized algorithm  however  order beneficial  complexity
checking solution must significantly lower complexity solving problem 
fortunately  true lps 
suppose auctioneer solve following lp  written two
equivalent ways  let us refer one left primal  one right
   observed parkes shneidman         assumption bit controversial  since desire
efficient computation implies nonzero cost computation  agents cost helping
modeled  is  nonetheless  common assumption distributed mechanism implementations 
    describe one simple way distributing mechanism  others possible 
    redundant computation discussed parkes shneidman        context ex post nash
equilibria  whereas interested dominant strategies  high level idea similar 

   

firesource allocation among agents mdp induced preferences

dual  
min v

max rt x

subject to 

    

subject to 



ax    

v r 

x   

strong duality property  primal lp solution v   dual
solution x   v   rt x   furthermore  given solution primal lp  easy
compute solution dual  complementary slackness  vt   rt b   x   b    
b square invertible matrix composed columns correspond basic
variables solution 
well known properties used auctioneer quickly check optimality
solutions returned agents  suppose agent returns v solution
primal lp  auctioneer calculate dual solution vt   rt b   check whether
rt x   v  thus  expensive operation auctioneer perform
inversion b  done sub cubic time  matter fact 
implementation perspective  would efficient ask agents return
primal dual solutions  since many popular algorithms compute process
solving lps 
thus  provided simple method allows us effectively distribute
winner determination problem  maintaining strategy proofness mechanism
negligible computation overhead auctioneer 
    preserving information privacy
mechanism discussed far drawback requires agents
reveal complete information mdps auctioneer  problem
exacerbated distributed wdp algorithm previous section  since
agent reveal mdp information auctioneer  information
spread agents via lp relaxations global milp  show
alleviate problem 
let us note that  saying agents prefer reveal local information 
implicitly assuming external factor affects agents utilities
captured agents mdps  sensible way measure value information
changes ones decision making process outcomes  since effect part
model  in fact  contradicts weak coupling assumption   cannot domainindependent manner define constitutes useful information  bad
agent reveal much mdp  modeling effects carefully analyzing
interesting research task  outside scope paper  thus 
purposes section  content mechanism hides enough information
make impossible auctioneer agent uniquely determine transition
reward function agent  in fact  information revealed agent map
infinitely many mdps agents     many transformations possible 
present one illustrate concept 
    stringent condition would require agents preferences resource bundles revealed
 parkes         set lower bar here 

   

fidolgov   durfee

main idea approach modify previous mechanism agents
submit private information auctioneer encrypted form allows
auctioneer solve winner determination problem  allow infer
agents original mdps 
first  note that  instead passing mdp auctioneer  agent submit
equivalent lp      so  question becomes  agent transform lp
way auctioneer able solve it  able infer transition
reward functions originating mdp  words  problem reduces
following  given lp l   created mdp   hs  a  p  r  via       need
find transformation l  l  solution transformed lp l  uniquely
map solution original lp l    l  reveal transition reward
functions original mdp  p r   show simple change variables suffices 
suppose agent m  mdp originated lp going ask agent m  solve it 
order maintain linearity problem  to keep simple m  solve   m 
limit linear transformations  consider linear  invertible transformation
primal coordinates u   f v  linear  invertible transformation dual coordinates
  dx  then  lp      transformed  by applying f   switching
dual  applying d  equivalent lp new coordinates y 
max rt d 
subject to 
 f    t ad     f    t  

    

d    
value optimal solution      value optimal solution
      given optimal solution       easy compute solution
original  x   d    indeed  perspective dual  primal transformation f
equivalent linear transformation dual equality constraints ax      given
f non singular  effect solution objective function  furthermore 
dual transformation equivalent change variables modifies solution
value objective function 
however  problem transformations gives away d    indeed 
agent m  able simply read  up set multiplicative constants  transformation constraints d     therefore  diagonal matrices positive
coefficients  which equivalent stretching coordinate system  trivially deduced m    since map    choosing negative multiplier xi
 inverting axis  pointless  flips non negativity constraints yi   
immediately revealing sign m   
let us demonstrate that  given mdp corresponding lp l   
choose f impossible m  determine coefficients l 
 or equivalently original transition reward functions p r   agent m 
receives l   as        knows l  created mdp  columns
constraint matrix original lp l  must sum constant 
x
x
aji    
p  s  a       
    
j



   

firesource allocation among agents mdp induced preferences

a  
p  
r  

a  
p  
r  

s 

s 

a  
p  
r  

a  
p     
r    

a  
p    
r     

a  
p    
r     
a  
p     
r     

a  
p    
r     

a  
p  
r  

s 

s 

a  
p  
r  

a  
p    
r     

 a 

a  
p  
r  

a  
p  
r  

s 

s 

a  
p  
r  

a  
p     
r    

 b 

figure    preserving privacy example  two different mdps lead lp
constraint matrix 

gives m  system  s  nonlinear equations diagonal arbitrary f  
total  s  a     s   free parameters  everything degenerate
cases  which easily handled appropriate choice f    equations
hugely under constrained infinitely many solutions  matter fact 
sacrificing  s  free parameters  m  choose f way
columns constraints l  sum constant           would
effect transforming l  l  corresponds another valid mdp     therefore 
given l    infinitely many original mdps transformations f
map lp l   
consider connection resource capacity costs agents occupation measures global wdp       two things auctioneer
able do  i  determine value agents policy  to able maximize
social welfare   ii  determine resource requirements policies  to check
resource constraints   so  question is  transformation affect these 
noted earlier  transformation change objective function  first
requirement holds  hand  change occupation measure xm  s  a 
arbitrary multipliers  however  multiplicative factor xm  s  a  effect usage
non consumable resources  matters whether corresponding xm  s  a  zero
 step function h nullifies scaling effect   thus  second condition holds 
example   consider two state mdp depicted figure  a represents decisionmaking problem sales company  two states corresponding possible market
conditions  two actions two possible sales strategies  market conditions state
s  much favorable state s   the rewards actions higher  
transitions two states correspond probabilities market conditions changing
rewards reflect expected profitability two states  obtaining numbers
realistic scenario would require performing costly time consuming research 
company might want make information public 
therefore  company participate resource allocation mechanism described above  would want encrypt mdp submitting auctioneer 
   

a  
p    
r     

fidolgov   durfee

mdp following reward function
r                            t  
following transition function 


 
 
p a     
 
       


p a     


           
 
   
   

    

    

using        corresponds following conservation flow constraint matrix 


                 
a 
 
    
           
   
submitting lp auctioneer  agent applies following transformations 


 
 
 
    
  diag                            f  
           
yielding following new constraint matrix 


   
 
 
 
 
 
 
   f   ad  
 
             

    

however  constraint matrix a  corresponds non transformed conservation
flow constraint different mdp  shown figure  b         following
reward function 
r               t  
    
following transition function 


   
p a     
 
   


p a     


   
 
   

    

therefore  auctioneer receives constraint matrix a    way knowing whether agent mdp transition function      transformed
using      mdp transition function      transformed  notice
dynamics two mdps vary significantly  transition probabilities state
connectivity  second mdp reveal information originating mdp
corresponding market dynamics 

sum up  can  large extent  maintain information privacy mechanism
allowing agents apply linear transformations original lps  information
revealed mechanism consists agents resource costs  a  o   capacity bounds

bm  c   sizes state action spaces  the latter hidden adding
dummy states actions mdp  
revealed information used infer agents preferences resource requirements  further  numeric policies revealed  lack information transition
reward functions renders information worthless  as illustrated example   
could multiple originating mdps different properties  
   

firesource allocation among agents mdp induced preferences

   experimental results
section present empirical analysis computational complexity
resource allocation mechanism described section    report results computational complexity mechanism section      agents submit
mdps auctioneer  simultaneously solves resource allocation policyoptimization problems  far additional speedup achieved distributing wdp 
described section      report empirical results  since well established
parallel programming literature parallel versions branch and bound milp
solvers consistently achieve linear speedup  eckstein  phillips    hart         due
fact branch and bound algorithms require little inter process communication 
experiments  implemented multiagent delivery problem  based
multiagent rover domain  dolgov   durfee         problem  agents operate
stochastic grid world delivery locations randomly placed throughout grid 
delivery task requires set resources  limited quantities resources 
random delivery locations grid  location set deliveries
accepts  resource size requirements  capacity cost   delivery agent
bounded space hold resources  limited capacity   agents participate
auction bid delivery resources  setting  value resource depends
resources agent acquires deliveries make  given
bundle resources  agents policy optimization problem find optimal delivery
plan  exact parameters used experiments critical trends seen
results presented below  sake reproducibility domain described
detail appendix c    resource costs experiments presented
binary 
computational complexity constrained optimization problems vary greatly
constraints tightened relaxed  therefore  first step analysis empirical
computational complexity mechanism  investigate running time depends
capacity constraints agent bounds total amounts
resources shared agents  common types constrained optimization
constraint satisfaction problems  natural expect wdp milp
easy solve problem over  under constrained either capacity
resource bounds  empirically verify this  varied local capacity constraint levels  
 meaning agents cannot use resources     meaning agent capacity
use enough resources execute optimal unconstrained policy   well global
constraint levels   meant resources available agents   
meant enough resources assign agent desired resource
bundle  experiments  part milp solver played cplex    
pentium   machine  gb ram  ram bottleneck due use
sparse matrix representations   typical running time profile shown figure   
problem easy over constrained  becomes difficult constraints
relaxed abruptly becomes easy capacity resource levels start
approach utopia 
    investigated other  randomly generated domains  results qualitatively same 

   

fidolgov   durfee

 
   
   
local constraint level

 

t  sec

 

 

 
 

 

   
local constraints

   
 

 

   
   
   
   
   
   
   
 
 

global constraints

   

   
   
global constraint level

   

 

figure    running time mdp based winner determination milp      different levels global  b
  local  b
  constraints  constraint levels fractions
utopian levels needed implement optimal unconstrained policies 
problems involved    agents  operating   by   grid     shared
resource types  data point shown average ten runs randomlygenerated problems 

following experiments aim avoid easy regions constraint levels 
therefore  given complexity profiles  set constraint levels     local
capacity global resource bounds  set discount factor        
value chosen arbitrarily  investigations effect value
running time milp revealed significant trends 
begin comparing performance mdp based auction  section     
performance straightforward ca flat preferences  as described section      
results summarized figure    compares time takes solve
standard winner determination problem space resource bundles     
time needed solve combined mdp wdp problem      used mechanism 
number resources increased  with   agents    by   grid   despite fact
algorithms exponential worst case running time  number integer
variables      exponentially larger milp       effect clearly
demonstrated figure    furthermore  comparison gives extremely optimistic
view performance standard ca  take account additional
complexity valuation problem  requires formulating solving large
number mdps  one per resource bundle   hand  latter embedded
wdp mechanism       thus including time solving valuation problem
comparison would magnify effect  fact  experiments  time
required solve mdps valuation problem significantly greater
time solving resulting wdp milp  however  present quantitative results
effect here  difference implementation  iterating resource
bundles solving mdps done via straightforward implementation matlab 
   

firesource allocation among agents mdp induced preferences

figure    gains computation efficiency  mdp based wdp versus wdp straightforward ca implementation  latter include time solving
mdps compute resource bundle values  error bars show standard
deviation ten runs 
n     o      
 

  

 

  

 

t  sec

  

 

  

 

  

 

  

 

  
  
number agents  m 

  

  

figure    scaling mdp based winner determination milp      agents  agents
operated   by   grids shared    types resources 

milps solved using highly optimized cplex code   parallelization wdp
performed experiments either algorithm 
analyze performance algorithm larger problems infeasible
straightforward ca  figure   illustrates scaling effect number agents
participating auction increased  below  point plot corresponds
single run experiment  with less ten runs performed every value
parameters   solid line mean  recall size wdp scales linearly
   

fidolgov   durfee

n        m     

n       m      

 

  

 

  

 

 

  
t  sec

t  sec

  

 

  

 

  

 

  

 

  

 

  

  
  
  
number resource types  o 

 

  

   

 a 

 

  

  
  
  
number resource types  o 

   

 b 

n       m      
 

  

 

t  sec

  

 

  

 

  

 

  

 

  

 

  

  
  
  
number resource types  o 

   

 c 

 d 

figure     a  c   scaling mdp based winner determination milp     
number resources three sets problems different grid sizes  n 
different numbers agents   m     d   linear scale plot tail data
 c  

number agents  graph therefore reflects rather standard scaling effect
np complete problem  seen plot  problems    agents
   resource types well within reach method  average taking around
   minutes 
next  analyze method scales number resource types  figure  
shows solution time function number resource types three different
sets problems  problems  number actions scaled linearly number
resource types  action required constant number resources  i e   number
   

firesource allocation among agents mdp induced preferences

n       m     

  
  
  

t  sec

  
  
  
  
  
 
 

 

  
  
resources per action

  

figure    complexity mdp based winner determination milp      function
number actions resource requirements 

nonzero  a  o  per action constant  two  regardless total number resource
types  problems exhibit interesting trait wherein running time peaks
relatively low numbers resource types  falls quickly  increases much
slowly number resource types increases  as illustrated figure  d  uses
linear scale   due fact total number resource types
much higher number resources required action  less contention
particular resource among agents one agents actions  therefore 
problems become relatively under constrained solution time increases slowly 
better illustrate effect  ran set experiments inverse ones shown
figure    kept total number resource types constant increased number
resource types required action  results shown figure    running time
profile similar observed earlier varied local global constraints 
total number resources per action low high  problem under  overconstrained relatively easy solve  complexity increases significantly
number resources required resource range        total
number resource types 
based above  would expect actions resource requirements increased
total number resource types  problem would scale gracefully
figure    example  figure   illustrates running time problems number
resources required action scales linearly total number resources  there 
complexity increase significantly faster  however  unreasonable assume
many domains number actions not  fact  increase total
number resource types involved  indeed  natural assume total number
resource types increases problem becomes complicated number
tasks agent perform increases  however  resource requirements
action increase well  delivery agent running example acquires ability
   

fidolgov   durfee

n      m     
 

  

 

  

 

t  sec

  

 

  

 

  

 

  

 

  

 

  

  
  
  
number resource types  o 

  

figure    complexity actions resource requirements grow proportionally total
number resource types  number resource types needed action
    total number resource types  o  

deliver pizza  might need new resources perform actions related new activity 
one would expect resource requirements delivering furniture appliances
change  therefore  believe many real applications  method scale
gracefully total number resource types 
experiments illustrate point domains agents preferences defined underlying markov decision processes  resource allocation
mechanism developed paper lead significant computational advantages 
shown figure    method successfully applied large problems that 
argue  well beyond reach combinatorial resource allocation mechanisms flat
preferences  experiments show  figure     even small problems  combinatorial
resource allocation mechanisms flat preferences time consuming  attempts empirically evaluate simpler mechanisms larger problems proved futile 
instance  method takes one minute solve problem that  standard
ca  requires agents enumerate      bundles auctioneer solve
np complete problem input size 

   generalizations  extensions  conclusions
many possible extensions generalizations work presented here 
briefly outline several below 
treatment paper focused problem resource allocation among
self interested agents  algorithms apply cooperative mdps weaklyinteracting agents  cooperative setting  concept resources viewed
compact way model inter agent constraints inability include combinations joint actions policies  weakly coupled mdps  agents
   

firesource allocation among agents mdp induced preferences

independent transition reward functions  certain combinations joint actions
feasible widely used model agents interactions  e g   singh   cohn        
model resource centric  direct models certainly possible  example  agents use sat formulas describe valid combinations joint actions  case
easily handled via simple modifications single multiagent milps     
      indeed  sat formula expressed set linear inequalities
binary variables  a   or  a  multiagent case   directly added
corresponding milp  see case non binary resources appendix b milp
defined indicators  a   instead  o  used binary case  
mentioned previously  work extended handle consumable resources
used whenever agents execute actions  fact  conditions  problem
considerably simplified domains kinds resources 
important change redefine value particular resource bundle
agent  difficulty that  given policy  total use consumable resources
uncertain  definition value resource bundle becomes ambiguous  one
possibility define value bundle payoff best policy whose expected
resource usage exceed amounts resources bundle  interpretation
 a  o  would change mean amount resource consumed action
every time executed  would make constraints      linear occupation
measure  would tremendously simplify wdp  making polynomial  
analogous models used constrained mdps  altman   shwartz         briefly
described earlier section    information privacy handled similarly case
non consumable resources  however  given transformation   dx  resource cost
function scaled d   since total consumption consumable
resources proportional occupation measure   additional benefit
hiding resource cost functions  unlike case non consumable resources
revealed   detailed treatment model consumable resources
presented work dolgov         including discussion risk sensitive cases 
value resource bundle defined payoff best policy whose probability
exceeding resource amounts bounded 
work exploited structure agents preferences stems underlying
policy optimization problems  however  latter modeled using flat mdps
enumerate possible states actions  flat mdps scale well due
curse dimensionality  bellman         address this  wdp milp modified
work factored mdps  boutilier  dearden    goldszmidt        using factored
resource allocation algorithm  dolgov   durfee         based dual alp
method solving factored mdps developed guestrin         method allows us
exploit types structure resource allocation algorithms  structure agents
preferences induced underlying mdps  well structure mdps themselves 
resource allocation mechanism discussed paper assumed one shot allocation
resources static population agents  interesting extension work would
consider system agents resources arrive depart dynamically 
online mechanism design work  parkes   singh        parkes  singh    yanovsky        
combining mdp based model utility functions dynamics online problems
could valuable result thus appears worthwhile direction future work 
   

fidolgov   durfee

agent population static  periodic re allocation resources allowed  techniques
phasing used solve resulting problem  wu   durfee        
summarize results paper  presented variant combinatorial auction resource allocation among self interested agents whose valuations resource bundles
defined weakly coupled constrained mdps  problems  mechanism 
exploits knowledge structure agents mdp based preferences  achieves
exponential reduction number integer decision variables  turn leads
tremendous speedup straightforward implementation  confirmed experimental results  mechanism implemented achieve reduction computational
complexity without sacrificing nice properties vcg mechanism  optimal outcomes  strategy proofness  voluntary participation   discussed distributed
implementation mechanism retains strategy proofness  using fact
lp solution easily verified   reveal agents private mdp information
 using transformation agents mdps  
believe models solution algorithms described paper significantly
applicability combinatorial resource allocation mechanisms practical problems  utility functions resource bundles defined sequential stochastic
decision making problems 

   acknowledgments
thank anonymous reviewers helpful comments  well colleagues
satinder singh  kang shin  michael wellman  demothenis teneketsis  jianhui wu 
jeffrey cox valuable discussions related work 
material based part upon work supported honeywell international 
darpa ipto coordinators program air force research laboratory
contract no  fa      c      views conclusions contained document authors  interpreted representing official
policies  either expressed implied  defense advanced research projects agency
u s  government 

appendix a  proofs
a   proof theorem  
theorem   consider finite set n indivisible resources    oi    i     n   
n available units resource  then  non decreasing utility function
defined resource bundles f       m n   r  exists resource constrained mdp
hs  a  p  r  o    c   
b   with resource set o  whose induced utility function
resource bundles f   words  every resource bundle z     m n  
value optimal policy among whose resource requirements exceed z
 call set  z   f  z  
f q       m n   r  hs  a  p  r  o    c   
b   
h


n
x


z     m n    z    max  a  oi  h
 s  a  zi   max u        f  z  




   

 z 

firesource allocation among agents mdp induced preferences

s   
a  
s   
 
a  

a  

a  

a 
r f       

a  

s   
 
a  

s    a 
a  

a  

s   
a  

s    a 
a  

a  

a 
r f       
a 
r f       

s 

a 
r  

s   

a  

a 
r f       

s   

figure     creating mdp resources arbitrary non decreasing utility function 
case shown three binary resources  transitions deterministic 

proof  statement shown via straightforward construction mdp
exponential number  one per resource bundle  states actions  present
reduction linear number actions exponential number states  choice
due fact that  although reverse mapping requiring two states exponentially
many actions even straightforward  mdp feels somewhat unnatural 
given arbitrary non decreasing utility function f   corresponding mdp
constructed follows  illustrated figure    n           state space
mdp consists  m   n    states one state  sz   every resource bundle z     m n  
plus sink state  s    

action space mdp   a   aij        n   j     m  consists mn    
actions  actions per resource oi       n   plus additional action a   
transition function p deterministic defined follows  action a  applicable every state leads sink state s    every action aij applicable
states sz   zi    j    leads certainty states zi   j 

 

    aij     sz     sz    zi    j     zi   j 
p  s  a        a      s   


  otherwise 
words  aij applies states j   units resource leads
state amount ith resource increases j 
reward function r defined follows  rewards state s   
action a  action produces rewards states 
 
f    z    ao     sz   z     m n
r s  a   
 
otherwise 
f   simple transformation f compensates effects discounting 
f    z    f  z   
   

p

zi

 

fidolgov   durfee

p
words  takes zi transitions get state sz   contribution
total discounted reward exactly f  z  
resource requirements actions follows  action a  require
resources  every action aij requires j units resource oi  
finally  initial conditions  sz          meaning agent always starts
state corresponds empty resource bundle  state s    figure     
capacity costs limits
b used  set c    
easy see mdp constructed above  given resource bundle z 
policy feasible set  z  zero probability reaching state sz 
z    z  for component i   furthermore  optimal policy set  z 
transition state sz  since f  z  non decreasing  use action a    thus obtaining
total discounted reward f  z  

a   proof theorem  
theorem   given mdp   hs  a  p  r  o    c   
b  resource capacity
constraints  exists policy hr feasible solution   exists
stationary deterministic policy sd sd feasible  expected total reward
sd less  
hr   sd sd   u   sd     u     
proof  let us label a  set actions non zero probability
executed according   i e  
a     a s    s  a      
let us construct unconstrained mdp      hs  a    p    r  i  p  r 
restricted versions p r action domain limited a   
p    a          
r    a    r
p    s  a    p  s  a   r   s  a    r s  a  s  s  a 
due well known property unconstrained infinite horizon mdps total
expected discounted reward optimization criterion    guaranteed optimal
stationary deterministic solution  e g   theorem         puterman         label
sd  
consider sd potential solution   clearly  sd feasible solution 
actions come set a  includes actions uses non zero probability 
means resource requirements  as      sd greater
  indeed 
n
n
x
x


max   a  o h
 s  a   
    
sd  s  a  max   a  o    max  a  o h
aa



aa

aa



first inequality due fact h z    z  second equality
follows definition a   
   

firesource allocation among agents mdp induced preferences

s 

a  
r v u   
a  o    
o   c z  

a  
v u   

 o
   
   
o   c z  

 

r 

s 

a  
r   
     


r 


   

s 

a  
r   
     


sm


  mv u   


am om   
om  c zm 
a  
r   
     


sm  

a  
r   
  


figure     reduction knapsack m oper cmdp  transitions deterministic 

furthermore  observe sd yields total reward     additionally  since sd uniformly optimal solution     is  particular  optimal
initial conditions constrained mdp   therefore  sd constitutes feasible
solution whose expected reward greater equal expected reward
feasible policy  

a   proof theorem  
theorem   following decision problem np complete  given instance mdp
hs  a  p  r  o    c   
b  resources capacity constraints  rational number  
exist feasible policy   whose expected total reward  given   less  
proof  shown theorem    always exists optimal policy    
stationary deterministic  therefore  presence np obvious  since can 
polynomial time  guess stationary deterministic policy  verify satisfies resource
constraints  calculate expected total reward  the latter done solving
standard system linear markov equations     values states  
show np completeness problem  use reduction knapsack  garey
  johnson         recall knapsack np complete problem  asks
whether  given set items z z  cost c z  value v z  
exists subset z   z total value items z   less

c  i e  
p constant vb 
pthe total cost items greater another constant b
c z 

b
c

v z 

v
b
 

reduction

illustrated

figure
  

proceeds
 
 
zz
zz
follows 
given instance knapsack  z    m  let us number items zi  
    m  notational convenience  instance knapsack  create
mdp     states  s    s          sm          actions  a             types resources
   o          om    single capacity c    c    
transition function states defined follows  every state si       m 
two transitions it  corresponding actions ai a    actions lead state
si   probability    state sm   absorbing transitions lead back
itself 
reward cost functions defined follows  want action ai       m 
 which corresponds item zi knapsack  contribute v zi   total discounted
   

fidolgov   durfee

reward  hence  set immediate reward every action ai v zi     i   which  given
transition function implies state si reached exactly step    ensures
action ai ever executed  contribution total discounted reward
v zi     i   i    v zi    action a  produces reward zero states 
resource requirements actions defined follows  action ai       m 
needs resource oi   i e    ai   oj         j  set cost resource oi
cost c zi   item knapsack problem  null action a  requires resources 
order complete construction  set initial distribution                
agent starts state s  probability    define decision parameter   vb
upper bound single capacity
b b
c 
essentially  construction allows agent choose action ai a  every state si  
choosing action ai equivalent putting item zi knapsack  action a 
corresponds choice including zi knapsack  therefore  exists
policy expected payoff less   vb uses
b   b
c
shared resource exists solution original instance
knapsack 


appendix b  non binary resource requirements
describe milp formulation capacity constrained single agent optimization problem     arbitrary resource costs     r  opposed binary costs
assumed main parts paper  corresponding multiagent winnerdetermination problem  the non binary equivalent       follows immediately
single agent milp 
arbitrary resource costs  obtain following non binary equivalent optimization problem      occupation measure coordinates 
max
x

xx


x s  a r s  a 



subject to 
x
xx
x   a 
x s  a p  s  a       


x



s 



n
x

 o  c  max  a  o h
x s  a 
b c  




c c 



x s  a    

s  a 

linearize sum max operators       let us observe inequality
n
x


g ui   max f  z  ui     g u    max f  z  u              g un   max f  z  un  
zz

zz

zz

equivalent following system  z n linear inequalities 
g u   f  z    u      g u   f  z    u              g un  f  zn   un   a 
   

z    z          zn z 

    

firesource allocation among agents mdp induced preferences

applying constraints       express original system  c  nonlinear
constraints  each max  
x

n
x

 o  c  max  a  o h
x s  a 
b c  




c c



following system  c  a  o  constraints max removed 
x

 o  c  ao   o h

x




x s  a 
b c  

c c  ao    ao          a 

    



notice way eliminating maximization exponentially increases number
constraints  expansion enumerates possible actions resource
 i e   enumerates policies resource used action a    used
action a    action a    etc   however  many problems resources used
actions  cases  constraints
q become redundant  number
constraints reduced  c  a  o   c   ao    ao number actions
use resource o 
linearize heaviside function analogously case binary resource costs
section      create binary indicator variable corresponds argument
h   tie occupation measure x via linear inequalities  difference
non binary resource costs  instead using
p indicators resources  use indicators
actions   a           a    h  x s  a   indicator shows whether
action used policy  using expanding max above  represent
optimization problem     following milp 
max
x 

xx


x s  a r s  a 



subject to 
x
xx
x   a 
x s  a p  s  a       


x



s 



 o  c  ao   o  ao  
b c  

c c  ao    ao          a 

    



x

x s  a  x  a  

a 



x s  a    

s  a 

 a         

a 

p
x max x s  a  constant finite upper bound expected number
times action used 
p exists discounted mdp  can  example  let
x           since s a x s  a          x valid occupation measure
mdp discount factor  
example   let us formulate milp constrained problem example    recall
three resources    ot     om    truck  forklift  mechanic   one capacity
   

fidolgov   durfee

type c    c     money   actions following resource requirements  listing
nonzero ones  
 a    ot         a    ot         a            a    ot         a    ot         a    om      
resources following capacity costs 
 ot   c          of   c          om   c        
agent limited budget  i e   capacity bound
b c        
compute optimal policy arbitrary   formulate problem
milp using techniques described above  using binary variables   ai       i    
                    express constraint capacity cost following system
 c  a  o               linear constraints 
                              
                              
                              
                              
                              
   
                              
easy see constraints redundant  fact action
requires small subset resources allows us prune many constraints 
fact  resource used byqmultiple actions ot   therefore  accordance
earlier discussion  need  ao               constraints 
                              
                              
                              
                              
four constraints corresponds case first resource  ot   used
different action 
mentioned earlier  set x         constraints synchronize
occupation measure x binary indicators   combining constraints
q
      get milp    continuous   binary variables   s   c   ao   
 a                   constraints  not counting last two sets range constraints  

finally  let us observe expanding resource action sets  problem
represented using binary resources only  domain contains mostly binary
requirements  may effective expand non binary resource requirements
augmenting resource set o  use binary formulation section     rather
directly applying more general formulation described above 
    create   noop action a    resource costs zero  drops
expressions 

   

firesource allocation among agents mdp induced preferences

appendix c  experimental setup
appendix details experimental domains constructed  delivery
domain  m  agents operating n by n grid sharing  o  resource types 
used following parameters 
resources enable agents carry delivery tasks  problem  o  resource
types   o  delivery actions  performing action      o   requires random
subset resources  where number resources required action
important parameter  whose effect complexity discussed section     probability
task      o   carried location          o i    o     i e   uniformly
distributed          function action id  actions lower ids
rewarding  per definition reward function below  executed
fewer locations  
n     possible delivery locations randomly placed grid  delivery
location assigned set delivery tasks executed  a single location
used multiple delivery tasks  single task carried several
locations   assignment tasks locations done randomly 
agent      o  actions  drive four perpendicular directions
execute one delivery tasks  drive actions result movement intended
direction probability     probability     produce change location 
movement actions incur negative reward  amount depends size
agent  problem  m  agents  movement penalty incurred agent
     m       m      m      i e   distributed uniformly         function
agents id 
execution action corresponding delivery task      o   location
task assigned produces reward    i  o  moves agent new random
location grid  new location chosen randomly problem generation  thus
known agent   transition deterministic  induces topology nearby
remote locations  attempting execution delivery task incorrect location
change state produces zero reward 
agents bid delivery resources  o  types  cglob  m  units
resource  cglob global constraint level  set     experiments 
described detail section     one capacity type  size  size
requirements making deliveries type      o   i  capacity limit agent
cloc    o   o        cloc local constraint level  set    
experiments  described detail section    
initial location agent randomly selected uniform distribution 
discount factor        

references
altman  e          constrained markov decision processes total cost criteria  occupation measures primal lp  methods models operations research         
     
   

fidolgov   durfee

altman  e     shwartz  a          adaptive control constrained markov chains  criteria policies  annals operations research  special issue markov decision
processes             
altman  e          constrained markov decision processes  chapman hall crc 
bellman  r          adaptive control processes  guided tour  princeton university
press 
benazera  m  e   brafman  r  i   meuleau  n     hansen  e          planning continuous resources stochastic domains  proceedings nineteenth international
joint conference artificial intelligence  ijcai      pp           
bererton  c   gordon  g     thrun  s          auction mechanism design multi robot
coordination  thrun  s   saul  l     scholkopf  b   eds    proceedings conference
neural information processing systems  nips   mit press 
bertsimas  d     tsitsiklis  j  n          introduction linear optimization  athena
scientific 
boutilier  c          solving concisely expressed combinatorial auction problems  proceedings eighteenth national conference artificial intelligence  aaai     
pp         
boutilier  c   dearden  r     goldszmidt  m          exploiting structure policy construction  proceedings fourteenth international joint conference artificial
intelligence  ijcai      pp           
boutilier  c     hoos  h  h          bidding languages combinatorial auctions  proceedings seventeenth international joint conference artificial intelligence
 ijcai      pp           
clarke  e  h          multipart pricing public goods  public choice           
de vries  s     vohra  r  v          combinatorial auctions  survey  informs journal
computing                 
dolgov  d          integrated resource allocation planning stochastic multiagent
environments  ph d  thesis  computer science department  university michigan 
dolgov  d  a     durfee  e  h          optimal resource allocation policy formulation loosely coupled markov decision processes  proceedings fourteenth
international conference automated planning scheduling  icaps      pp 
       
dolgov  d  a     durfee  e  h          resource allocation among agents preferences
induced factored mdps  proceedings fifth international joint conference
autonomous agents multiagent systems  aamas      hakodate  japan 
eckstein  j   phillips  c     hart  w          pico  object oriented framework parallel
branch bound  proceedings workshop inherently parallel algorithms
optimization feasibility applications 
feigenbaum  j     shenker  s          distributed algorithmic mechanism design  recent
results future directions  proceedings sixths international workshop
   

firesource allocation among agents mdp induced preferences

discrete algorithms methods mobile computing communications  pp 
     acm press  new york 
feldmann  r   gairing  m   lucking  t   monien  b     rode  m          selfish routing
non cooperative networks  survey  proceedings twenty eights international
symposium mathematical foundations computer science  mfcs      pp    
    springer verlag 
ferguson  d   nikolaou  c   sairamesh  j     yemini  y          economic models allocating resources computer systems  clearwater  s   ed    market based control 
paradigm distributed resource allocation  pp          hong kong  world
scientific 
garey  m  r     johnson  d  s          computers intractability  guide theory
np completeness  w  h  freeman   co 
groves  t          incentives teams  econometrica                 
guestrin  c          planning uncertainty complex structured environments 
ph d  thesis  computer science department  stanford university 
heyman  d  p     sobel  m  j          volume ii  stochastic models operations research 
mcgraw hill  new york 
kallenberg  l          linear programming finite markovian control problems  math 
centrum  amsterdam 
littman  m  l   dean  t  l     kaelbling  l  p          complexity solving markov
decision problems  proceedings eleventh annual conference uncertainty
artificial intelligence  uai     pp          montreal 
mackie mason  j  k     varian  h          generalized vickrey auctions  tech  rep  
university michigan 
mas colell  a   whinston  m  d     green  j  r          microeconomic theory  oxford
university press  new york 
mcafee  r  p     mcmillan  j          analyzing airwaves auction  journal economic
perspectives                
mcmillan  j          selling spectrum rights  journal economic perspectives        
      
meuleau  n   hauskrecht  m   kim  k  e   peshkin  l   kaelbling  l   dean  t     boutilier 
c          solving large weakly coupled markov decision processes  proceedings
fifteenth national conference artificial intelligence  aaai      pp     
    
nisan  n          bidding allocation combinatorial auctions  electronic commerce 
parkes  d          iterative combinatorial auctions  achieving economic computational efficiency  ph d  thesis  department computer information science 
university pennsylvania 
parkes  d  c   kalagnanam  j  r     eso  m          achieving budget balance
vickrey based payment schemes exchanges  proc    th international joint conference artificial intelligence  ijcai      pp           
   

fidolgov   durfee

parkes  d  c     shneidman  j          distributed implementations vickrey clarkegroves mechanisms  proceedings third international joint conference
autonomous agents multi agent systems  aamas      pp         
parkes  d  c     singh  s          mdp based approach online mechanism design 
proceedings seventeenths annual conference neural information processing
systems  nips     
parkes  d  c   singh  s     yanovsky  d          approximately efficient online mechanism
design  proceedings eighteenths annual conference neural information
processing systems  nips     
puterman  m  l          markov decision processes  john wiley   sons  new york 
ross  k     chen  b          optimal scheduling interactive non interactive traffic
telecommunication systems  ieee transactions automatic control             
ross  k     varadarajan  r          markov decision processes sample path constraints  communicating case  operations research             
rothkopf  m  h   pekec  a     harstad  r  m          computationally manageable combinational auctions  management science                   
sandholm  t     boutilier  c          preference elicitation combinatorial auctions 
cramton  shoham    steinberg  eds    combinatorial auctions  chap      mit press 
sandholm  t          algorithm optimal winner determination combinatorial
auctions  proceedings sixteenth international joint conference artificial
intelligence  ijcai      pp          san francisco  ca  usa  morgan kaufmann
publishers inc 
sandholm  t          algorithm optimal winner determination combinatorial auctions  artificial intelligence                 
shapley  l  s          stochastic games  proceedings national academy science  usa 
             
sheffi  y          combinatorial auctions procurement transportation services 
interfaces                 
singh  s     cohn  d          dynamically merge markov decision processes 
jordan  m  i   kearns  m  j     solla  s  a   eds    advances neural information
processing systems  vol      pp            mit press 
song  j     regan  a          combinatorial auctions transportation service procurement  carrier perspective  transportation research record             
vickrey  w          counterspeculation  auctions competitive sealed tenders  journal
finance          
wellman  m  p   walsh  w  e   wurman  p  r     mackie mason  j  k          auction
protocols decentralized scheduling  games economic behavior             
wolsey  l          integer programming  john wiley   sons 
   

firesource allocation among agents mdp induced preferences

wu  j     durfee  e  h          automated resource driven mission phasing techniques
constrained agents  proceedings fourth international joint conference
autonomous agents multiagent systems  aamas      pp         

   


