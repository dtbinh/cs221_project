journal of artificial intelligence research                 

submitted        published      

resource allocation among agents with mdp induced
preferences
dmitri a  dolgov

ddolgov ai stanford edu

technical research department  ai   robotics group 
toyota technical center
     green road
ann arbor  mi        usa

edmund h  durfee

durfee umich edu

electrical engineering and computer science
university of michigan
     hayward st 
ann arbor  mi        usa

abstract
allocating scarce resources among agents to maximize global utility is  in general  computationally challenging  we focus on problems where resources enable agents to execute
actions in stochastic environments  modeled as markov decision processes  mdps   such
that the value of a resource bundle is defined as the expected value of the optimal mdp
policy realizable given these resources  we present an algorithm that simultaneously solves
the resource allocation and the policy optimization problems  this allows us to avoid explicitly representing utilities over exponentially many resource bundles  leading to drastic
 often exponential  reductions in computational complexity  we then use this algorithm
in the context of self interested agents to design a combinatorial auction for allocating resources  we empirically demonstrate the effectiveness of our approach by showing that
it can  in minutes  optimally solve problems for which a straightforward combinatorial
resource allocation technique would require the agents to enumerate up to      resource
bundles and the auctioneer to solve an np complete problem with an input of that size 

   introduction
the problem of resource allocation is ubiquitous in many diverse research fields such as
economics  operations research  and computer science  with applications ranging from decentralized scheduling  e g   wellman  walsh  wurman    mackie mason        and network routing  e g   feldmann  gairing  lucking  monien    rode        to transportation
logistics  e g   sheffi        song   regan        and bandwidth allocation  e g   mcmillan 
      mcafee   mcmillan         just to name a few  the core question in resource allocation is how to distribute a set of scarce resources among a set of agents  either cooperative or
self interested  in a way that maximizes some measure of global utility  with social welfare
 sum of agents utilities  being one of the most popular criteria 
in many domains  an agents utility for obtaining a set of resources is defined by what
the agent can accomplish using these resources  for example  the value of a vehicle to a
delivery agent is defined by the additional revenue that the agent can obtain by using the
vehicle  however  to figure out how to best utilize a resource  or set of resources   an agent
c
    
ai access foundation  all rights reserved 

fidolgov   durfee

available
resources

available
actions

resource allocation
problem

planning problem
 mdp 

utility function on
resources

best plan  
its payoff

figure    dependency cycle  to formulate their planning problems  the agents need to
know what resources they will get  but their utility functions  which define the
input to the resource allocation problem  depend on the solutions to the planning
problems 

often must solve a non trivial planning problem because actions might have long term  nondeterministic effects  therefore  an agents value for a set of resources is defined by a solution
to a planning problem  but to formulate its planning problem the agent needs to know what
resources it will obtain  this leads to cyclic dependencies  depicted in figure     wherein the
input to the resource allocation problem depends on the solution to the planning problem 
and vice versa  unfortunately  for anything but the simplest domains  neither the resourceallocation nor the planning problem can be solved in closed form  making it impossible to
obtain parameterized solutions 
our focus in this paper is on solving the interdependent problems of resource allocation
and stochastic planning  the main question we consider is how to allocate the resources in
a way that maximizes the social welfare of the agents when the utility function of each agent
is defined by a markov decision process  puterman        whose action set is parameterized
by the resources  in this paper  we specifically focus on non consumable resources  such
as vehicles  that enable actions  but are not themselves consumed during action execution 
we briefly mention the case of consumable resources in section    and refer to the work by
dolgov        for a more detailed treatment 
we assume that the agents mdps are weakly coupled  meaning the agents only interact
through the resources  and once the resources are allocated  the transition and reward
functions of their mdps are independent  our model of weakly coupled mdps connected
via shared resources is similar to that of meuleau  hauskrecht  kim  peshkin  kaelbling 
dean  and boutilier        and benazera  brafman  meuleau  and hansen         but
differs in that we further assume that resources are only allocated once  prior to any actions
being taken  while this one shot allocation assumption limits our approach somewhat 
it also allows our approach to apply more broadly to non cooperative settings  without
this assumption  the game theoretic analysis of agents interactions is significantly more
complex   more importantly  it allows us to avoid the state space explosion  due to including
resource information in the mdp states   which limits that other work to finding only
approximately optimal solutions for non trivial problems 
the main result presented in this paper is thus a new algorithm that  under the above
conditions  optimally solves the resource allocation and the policy optimization problems
simultaneously  by considering the two problems together  it sidesteps the dependency cycle
   

firesource allocation among agents with mdp induced preferences

mentioned above  which allows us to avoid an explicit representation of utility functions
on resource bundles  leading to an exponential reduction in complexity over combinatorial
resource allocation with flat utility functions  we empirically demonstrate that the resulting
algorithm scales well to finding optimal solutions for problems involving numerous agents
and resources 
our algorithm can be viewed as contributing a new approach for dealing with the computational complexity of resource allocation in domains with complex utility functions that
are not linearly decomposable in the resources  due to the effects of substitutability and
complementarity   in such combinatorial allocation problems  finding an optimal allocation is np complete in the  often exponentially large  space of resource bundles  rothkopf 
pekec    harstad         previous approaches for addressing the complexity have included
determining classes of utility functions that lead to tractable problems  as surveyed by
de vries   vohra         iterative algorithms for resource allocation and preference elicitation  as surveyed by sandholm   boutilier         and concise languages for expressing
agents preferences  sandholm        nisan        boutilier   hoos        boutilier        
the novelty of our approach with respect to these is that it explicitly embraces the underlying processes that define the agents utility functions  for cases where these processes can
be modeled as resource parameterized mdps  by doing so  not only does our approach use
such mdp based models as a concise language for agents utility functions  but more importantly  it directly exploits the structure in these models to drastically reduce computational
complexity by simultaneously solving the planning and resource allocation problems 
in the context of cooperative agents  our approach can be viewed as a way of solving
weakly coupled multiagent mdps  where agents transition and reward functions are independent  but the space of joint actions is constrained  as  for example  in the models used
by singh and cohn        or meuleau et al          from that perspective  the concept of
resources can be viewed as a compact way of representing the interactions between agents 
similarly to the model used by bererton  gordon  and thrun         however  our work
differs in a number of assumptions  moreover  our algorithms can be easily modified to work
with models where the constraints on the joint actions are modeled directly  for example 
via sat formulas  
for non cooperative agents  we apply our resource allocation algorithm to the mechanismdesign problem  e g   mas colell  whinston    green         where the goal is to allocate
resources among the agents in a way that maximizes the social welfare  given that each
participating agent is selfishly maximizing its own utility  for domains with self interested
agents with complex preferences that exhibit combinatorial effects between the resources 
combinatorial auctions  e g   de vries   vohra        are often used for resource allocation 
the generalized vickrey auction  gva   mackie mason   varian         which is an extension of vickrey clarke groves  vcg  mechanisms  vickrey        clarke        groves 
      to combinatorial auctions  is particularly attractive because of its nice analytical
properties  as described in section       we develop a variant of a vcg auction  where
agents submit their resource parameterized mdps as bids  and the auctioneer simultaneously solves the resource allocation and the policy optimization problems  thus retaining the
compact representation of agents preferences throughout the process  we describe extensions to the mechanism for distributing the computation and for encoding mdp information
to reduce the revelation of private information 
   

fidolgov   durfee

the remainder of this paper proceeds as follows  after a brief review of mdps in
section    we present  in section    our model of a decision making agent  the resourceparameterized mdp with capacity constraints  we analyze the problem of optimal policy
formulation in such a resource parameterized capacity constrained mdp  study its properties  and present a solution algorithm  based on the formulation of this  np complete 
problem as a mixed integer program 
with these building blocks  we move to the multiagent setting and present our main
result  the algorithm for simultaneously allocating resources and planning across agents
 section     based on that algorithm  we then design a combinatorial auction for allocating
resources among self interested agents  we describe a distributed implementation of the
mechanism  and discuss techniques for preserving information privacy  in section    we
analyze the computational efficiency of our approach  empirically demonstrating exponential reductions in computational complexity  compared to a straightforward combinatorial
resource allocation algorithm with flat utility functions  finally  in section    we conclude
with a discussion of possible generalizations and extensions to our approach  for conciseness
and better readability  all proofs and some generalizations are deferred to appendices 

   markov decision processes
we base our model of agents decision problems on infinite horizon fully observable mdps
with the total expected discounted reward optimization criterion  although our results are
also applicable to other classes of mdps  such as mdps with the average per step rewards  
this section introduces our notation and assumptions  and serves as a brief overview of the
basic mdp results  see  for example  the text by puterman        for a detailed discussion
of the material in this section  
a classical single agent  unconstrained  stationary  fully observable mdp can be defined
as a   tuple hs  a  p  ri  where 
 s is a finite set of states the agent can be in 
 a is a finite set of actions the agent can execute 
 p   s  a  s          defines the transition function  the probability that the agent
goes to state   s upon execution of action a  a in state s  s is p  s  a  
we
p assume that  for any action  the corresponding transition matrix is stochastic 
 p  s  a      s  s  a  a 
 r   s  a   r defines the reward function  the agent obtains a reward of r s  a  if it
executes action a  a in state s  s  we assume the rewards are bounded 
in a discrete time fully observable mdp  at each time step  the agent observes the current
state of the system and chooses an action according to its policy  a policy is said to be
markovian  or history independent  if the choice of action does not depend on the history
of states and actions encountered in the past  but rather only on the current state and
time  if  in addition to that  the policy does not depend on time  it is called stationary  by
definition  a stationary policy is always markovian  a deterministic policy always prescribes
the execution of the same action in a state  while a randomized policy chooses actions
according to a probability distribution 
   

firesource allocation among agents with mdp induced preferences

following the standard notation  puterman         we refer to different classes of policies
as xy   where x    h  m  s  specifies whether a policy is history dependent  markovian 
or stationary  and y    r  d  specifies whether the policy is randomized or deterministic
 e g   the class of stationary deterministic policies is labeled sd    obviously  hy  my 
sy and xr  xd   with history dependent randomized policies hr and stationary
deterministic policies sd being the most and the least general  respectively 
a stationary randomized policy is thus a mapping of states to probability distributions
over actions     s  a           where  s  a  defines the probability that action a is
executed in state s  a stationary deterministic policy can be viewed as a degenerate case
of a randomized policy for which there is only one action for each state that has a nonzero
probability of being executed 
in an unconstrained discounted mdp  the goal is to find a policy that maximizes the
total expected discounted reward over an infinite time horizon  

hx
i
u        e
  t rt       
   
t  

where          is the discount factor  a unit reward at time t     is worth the same to the
agent as a reward of  at time t   rt is the  random  reward the agent receives at time t 
whose distribution depends on the policy  and the initial distribution over the state space
   s          
one of the most important results in the theory of mdps states that  for an unconstrained discounted mdp with the total expected reward optimization criterion  there always exists an optimal policy that is stationary  deterministic  and uniformly optimal  where
the latter term means that the policy is optimal for all distributions over the starting state  
there are several commonly used ways of finding the optimal policy  and central to all of
them is the concept of a value function of a policy  v   s   r  where v  s  is the expected
cumulative value of the reward the agent would receive if it started in state s and behaved
according to policy   for a given policy   the value of every state is the unique solution
to the following system of  s  linear equations 
x
x
v  s   
r s  a  s  a    
p  s  a v    
s  s 
   
a



to find the optimal policy  it is handy to consider the optimal value function v    s   r 
where v   s  represents the value of state s  given that the agent behaves optimally  the
optimal value function satisfies the following system of  s  nonlinear equations 
h
i
x
v   s    max r s  a    
p  s  a v      
s  s 
   
a



v 

given the optimal value function
an optimal policy is to simply act greedily with respect

to v  with any method of tie breaking in case of multiple optimal actions  
h
i
 
p
  if a  arg maxa r s  a      p  s  a v      
 s  a   
   
  otherwise 
   notation  here and below  x y is an exponent  while xy is a superscript 
   uniform optimality of policies is the reason why  is not included as a component of a textbook mdp 

   

fidolgov   durfee

one of the possible ways of solving for the optimal value function is to formulate the
nonlinear system     as a linear program  lp  with  s  optimization variables v s  and
 s  a  constraints 
x
min
 s v s 
s

subject to 

   

v s   r s  a    

x

p  s  a v   

s  s  a  a 



where  is an arbitrary constant vector with  s  positive components   s      s  s   
in many problems  including the ones that are the focus of this paper   it is very useful to consider the equivalent dual lp with  s  a  optimization variables x s  a  and  s 
constraints  
xx
r s  a x s  a 
max
x

s

a

subject to 
x
xx
x   a   
x s  a p  s  a       
a

s

   
  s 

a

x s  a    

s  s  a  a 

the optimization variables x s  a  are often called the visitation frequencies or the occupation measure of a policy  if we think of  as the initial probability distribution  then x s  a 
can be interpreted
p as the total expected number of times action a is executed in state s 
then  x s    a x s  a  gives the total expected flow through state s  and the constraints
in the above lp can be interpreted as the conservation of flow through each of the states 
an optimal policy can be computed from a solution to the dual lp as 
x s  a 
 
   
 s  a    p
a x s  a 
p
where non negativity of  guarantees that a x s  a      s  s  in general  this appears to
lead to randomized policies  however  a bounded lp with n constraints always has a basic
feasible solution  e g   bertsimas   tsitsiklis         which by definition has no more than
n non zero components  if  is strictly positive  a basic feasible solution to the lp     will
have precisely  s  nonzero components  one for each state   which guarantees the existence
of an optimal deterministic policy  such a policy can be easily obtained by most lp solvers
 e g   simplex will always produce solutions that map to deterministic policies  
furthermore  as mentioned above  for unconstrained discounted mdps  there always
exist policies that are uniformly optimal  optimal for all initial distributions   the above
dual lp     yields uniformly optimal policies if a strictly positive  is used  however  the
   the overloading of  as the objective function coefficients here and the initial probability distribution
of an mdp earlier is intentional and is explained shortly 
   note that some authors  e g   altman        prefer the opposite convention  where     is called the dual 
and      the primal 

   

firesource allocation among agents with mdp induced preferences

solution  x  to the dual lp retains its interpretation as the expected number of times state s
is visited and action a is executed only for the initial probability distribution  that was
used in the lp 
the main benefit of the dual lp     is manifested in constrained mdps  altman       
kallenberg        heyman   sobel         where each action  in addition to producing a
reward r s  a   also incurs a vector of costs k  s  a    s  a   r k      k   the problem then is to maximize the expected reward  subject to constraints on the expected costs 
constrained models of this type arise in many domains  such as telecommunication applications  e g   ross   chen        ross   varadarajan         where it is often desirable to
maximize expected throughput  subject to conditions on the average delay  such problems 
where constraints are imposed on the expected costs  can be solved in polynomial time
using linear programming by simply augmenting the dual lp     with the following linear
constraints 
xx
k  s  a x s  a   bk  
k      k  
   
s

a

where bk is the upper bound on the expected cost of type k  the resulting constrained mdp
differs from the standard unconstrained mdp  in particular  deterministic policies are no
longer optimal  and uniformly optimal policies do not  in general  exist for such problems
 kallenberg        
for the same reason of being easily augmentable with constraints  the dual lp     also
forms the basis for our approach  however  the constraints that arise in resource allocation
problems that are our focus in this paper are very different from the linear constraints in     
leading to different optimization problems with different properties and requiring different
solution techniques  as described in more detail in section    
we conclude the background section by introducing a simple unconstrained mdp that
will serve as the basis for a running example  to which we will refer throughout the rest of
this paper 
example   consider a simple delivery domain  depicted in figure    where the agent can
obtain rewards for delivering furniture  action a    or delivering appliances  action a    
delivering appliances produces higher rewards  as shown on the diagram   but it does more
damage to the delivery vehicle  if the agent only delivers furniture  the damage to the vehicle
is negligible  whereas if the agent delivers appliances  the vehicle is guaranteed to function
reliably for the first year  state s     but after that  state s    has a     probability of failure 
per year  the vehicle can be serviced  action a     resetting its condition  but at the expense
of lowering profits  if the truck does break  state s     it can be repaired  action a     but with
a more significant negative impact on profits  we will assume a discount factor        
the optimal value function is v   s           v   s           v   s           and the
corresponding optimal occupation measure  assuming a uniform   is the following  listing
only the non zero elements   x s    a          x s    a          x s    a          this maps to
the optimal policy that dictates that the agent is to start by delivering appliances  action a 
in state s     then service the vehicle after the first year  action a  in state s     and fix the
vehicle if it ever gets broken  a  in s     the latter has zero probability of happening under
this policy if the agent starts in state s  or s    

   

fidolgov   durfee

a   deliver furniture
p  
r  

s 
 initial 

a   fix truck
p  
r  

a   deliver furniture
p  
r  

a   deliver appliances
p  
r   
a   service truck
p  
r  

s 
  nd year 
a   deliver appliances
p    
r   
p    
r   

s 
 truck broken 

figure    unconstrained mdp example for a delivery domain  transition probabilities  p 
and rewards  r  are shown on the diagram  actions not shown result in transition
to same state with no reward  there is also a noop action a  that corresponds to
doing nothing  it does not change state and produces zero reward 

   agent model  resource parameterized mdp
in this section  we introduce our model of the decision making agent  and describe the
single agent stochastic policy optimization problem that defines the agents preferences over
resources  we show that  for this single agent problem  formulated as an mdp whose
action set is parameterized on the resources available to the agent  stationary deterministic
policies are optimal  but uniformly optimal policies do not  in general  exist  we also show
that the problem of finding optimal policies is np complete  finally  we present a policyoptimization algorithm  based on a formulation of the problem as a mixed integer linear
program  milp  
we model the agents resource parameterized mdp as follows  the agent has a set of
actions that are potentially executable  and each action requires a certain combination of
resources  to capture local constraints on the sets of resources an agent can use  we use
the concept of capacities  each resource has capacity costs associated with it  and each
agent has capacity constraints  for example  a delivery company needs vehicles and loading
equipment  resources to be allocated  to make its deliveries  execute actions   however 
all equipment costs money and requires manpower to operate it  the agents local capacity
costs   therefore  the amount of equipment the agent can acquire and successfully utilize
is constrained by factors such as its budget and limited manpower  agents local capacity
bounds   this two layer model with capacities and resources represented separately might
seem unnecessarily complex  why not fold them together or impose constraints directly
on resources    but the separation becomes evident and useful in the multiagent model
discussed in section    we emphasize the difference here  resources are the items being
allocated among the agents  while capacities define the inherent limitations of an individual
agent on what combinations of resources it can usefully possess 
the agents optimization problem is to choose a subset of the available resources that
does not violate its capacity constraints  such that the best policy feasible under that bundle
of resources yields the highest utility  in other words  the single agent problem analyzed in
   

firesource allocation among agents with mdp induced preferences

this section has no constraints on the total resource amounts  they are introduced in the
multiagent problem in the next section   and the constraints are only due to the agents
capacity limits  adding limited resource amounts to the single agent model would be a very
simple matter  since such constraints can be handled with a simple pruning of the agents
action space  further  note that without capacity constraints  the single agent problem
would be trivial  as it would always be optimal for the agent to simply take all resources
that are of potential use  however  in the presence of capacity constraints  we face a problem
that is similar to the cyclic dependency in figure    the resource selection problem requires
knowing the values of all resource bundles  which are defined by the planning problem  and
the planning problem is ill defined until a resource bundle is chosen  in this section  we
focus on the single agent problem of selecting an optimal subset of resources that satisfies
the agents capacity constraints and assume the agent has no value for acquiring additional
resources that exceed its capacity bounds 
the resources in the model outlined above are non consumable  i e   actions require
resources but do not consume them during execution  as mentioned in the introduction 
in this work we focus only on non consumable resources  and only briefly outline the case
of consumable resources in section   
we can model the agents optimization problem as an n tuple hs  a  p  r  o    c    
b  i 
 hs  a  p  ri are the standard components of an mdp  as defined earlier in section   
 o is the set of resources  e g   o    production equipment  vehicle           we will use
o  o to refer to a resource type 
    a  o   r is a function that specifies the resource requirements of all actions 
 a  o  defines how much of resource o  o action a  a needs to be executable  e g  
 a  vehicle      means that action a requires one vehicle  
 c is the set of capacities of our agent  e g   c    space  money  manpower           we
will use c  c to refer to a capacity type 
    o c   r is a function that specifies the capacity costs of resources   o  c  defines
how much of capacity c  c a unit of resource o  o requires  e g    vehicle  money   
       defines the monetary cost of a vehicle  while  vehicle  manpower      means
that two people are required to operate the vehicle  
 
b   c   r specifies the upper bound on the capacities  
b c  gives the upper bound
on capacity c  c  e g   
b money               defines the budget constraint  and

b manpower      specifies the size of the workforce  
    s   r is the initial probability distribution   s  is the probability that the agent
starts in state s 
our goal is to find a policy  that yields the highest expected reward  under the conditions that the resource requirements of that policy do not exceed the capacity bounds of
   

fidolgov   durfee

the agent  in other words  we have to solve the following mathematical program  
max u     


subject to 
n
x
x
o
 o  c  max  a  o h
 s  a   
b c  
o

a

   
c  c 

s

where h is the heaviside step function of a nonnegative argument  defined as 
 
  z     
h z   
  z     
the constraint in     can be interpreted as follows  the argument of h is nonzero if
thep
policy  assigns a nonzero probability to using action a in at least one state  thus 
function
h  s  s  a   serves as an indicator

  us whether the agent plans to use
p that tells
action a in its policy  and max  a  o h  s  s  a   tells us how much of resource o the
agent needs for its policy  we take a max with respect to a  because the same resource o can
be used by different actions  therefore  when summed over all resources o  the left hand
side gives us the total requirements of policy  in terms of capacity c  which has to be no
greater than the bound 
b c  
the following example illustrates the single agent model 
example   let us augment example   as follows  suppose the agents needs to obtain a
truck to perform its delivery actions  a  and a     the truck is also required by the service
and repair actions  a  and a     further  to deliver appliances  the agent needs to acquire
a forklift  and it needs to hire a mechanic to be able to repair the vehicle  a     the noop
action a  requires no resources  this maps to a model with three resources  truck  forklift 
and mechanic   o    ot   of   om    and the following action resource costs  listing only the
non zero ones  
 a    ot         a    ot         a    of         a    ot         a    ot         a    om       
moreover  suppose that the resources  truck ot   forklift of   or mechanic om   have the
following capacity costs  there is only one capacity type  money  c    c    
 ot   c          of   c          om   c        
and the agent has a limited budget of 
b      it can  therefore acquire no more than two of
the three resources  which means that the optimal solution to the unconstrained problem as
in example   is no longer feasible 

let us observe that the mdp based model of agents preferences presented above is
fully general for discrete indivisible resources  i e   any non decreasing utility function over
resource bundles can be represented via the resource constrained mdp model described
above 
   this formulation assumes a stationary policy  which is supported by the argument in section     

   

firesource allocation among agents with mdp induced preferences

theorem   consider a finite set of n indivisible resources o    oi    i      n    with
m  n available units of each resource  then  for any non decreasing utility function
defined over resource bundles f       m n   r  there exists a resource constrained mdp
hs  a  p  r  o    c    
b  i  with the same resource set o  whose induced utility function over
the resource bundles is the same as f   in other words  for every resource bundle z      m n  
the value of the optimal policy among those whose resource requirements do not exceed z
 call this set  z   is the same as f  z  
f       m n   r   hs  a  p  r  o    c    
b  i  
n fi
h
o
x
i
fi
 z      m n    z     fi max  a  oi  h
 s  a   zi   max u        f  z  
a

s

 z 

proof  see appendix a   

let us comment that while theorem   establishes the generality of the mdp based preference model introduced in this section  the construction used in the proof is of little practical interest  as it requires an mdp with an exponentially large state or action space  indeed 
we do not advocate mapping arbitrary unstructured utility functions to exponentially large
mdps as a general solution technique  rather  our contention is that our techniques apply to domains where utility functions are induced by a stochastic decision making process
 modeled as an mdp   thus resulting in well structured preferences over resources that
can be exploited to drastically lower the computational complexity of resource allocation
algorithms 
    properties of the single agent constrained mdp
in this section  we analyze the constrained policy optimization problem      namely  we
show that stationary deterministic policies are optimal for this problem  meaning that it
is not necessary to consider randomized  or history dependent policies  however  solutions
to problem     are not  in general  uniformly optimal  optimal for any initial distribution  
furthermore  we show that     is np hard  unlike the unconstrained mdps  which can be
solved in polynomial time  littman  dean    kaelbling        
we begin by showing optimality of stationary deterministic policies for      in the
following  we use hr to refer to the class of history dependent randomized policies  the
most general policies   and sd  hr to refer to the class of stationary deterministic
policies 
theorem   given an mdp m   hs  a  p  r  o    c    
b  i with resource and capacity
hr
constraints  if there exists a policy   
that is a feasible solution for m   there exists a
stationary deterministic policy  sd  sd that is also feasible  and the expected total reward
of  sd is no less than that of  
   hr     sd  sd   u   sd      u     
proof  see appendix a   

the result of theorem   is not at all surprising  intuitively  stationary deterministic
policies are optimal  because history dependence does not increase the utility of the policy 
   

fidolgov   durfee

and using randomization can only increase resource costs  the latter is true because including an action in a policy incurs the same costs in terms of resources regardless of the
probability of executing that action  or the expected number of times the action will be
executed   this is true because we are dealing with non consumable resources  this property does not hold for mdps with consumable resources  as we discuss in more detail in
section    
we now show that uniformly optimal policies do not always exist for our constrained
problem  this result is well known for another class of constrained mdps  where constraints
are imposed on the total expected costs that are proportional to the expected number of
times the corresponding actions are executed  discussed earlier in section     mdps with
such constraints arise  for example  when bounds are imposed on the expected usage of
consumable resources  and as mentioned in section    these problems can be solved using
linear programming by augmenting the dual lp     with linear constraints on expected
costs      below  we establish the same result for problems with non consumable resources
and capacity constraints 
observation   there do not always exist uniformly optimal solutions to      in other
words  there exist two constrained mdps that differ only in their initial conditions  m  
hs  a  p  r  o    c    
b  i and m     hs  a  p  r  o    c    
b    i  such that there is no policy
that is optimal for both problems simultaneously  i e   for any two policies  and    that are
optimal solutions to m and m     respectively  the following holds 
u        u         

u          u          

    

we demonstrate this observation by example 
example   consider the resource constrained problem as in example    it is easy to see
that if the initial conditions are               the agent starts in state s  with certainty  
the optimal policy for states s  and s  is the same as in example    s   a  and s   a    
which  given the initial conditions  results in zero probability of reaching state s   to which
the noop a  is assigned   this policy requires the truck and the forklift  however  if the
agent starts in state s                  the optimal policy is to fix the truck  execute a  in
s     and to resort to furniture delivery  do a  in s  and assign the noop ao to s    which is
then never visited   this policy requires the mechanic and the truck  these two policies are
uniquely optimal for the corresponding initial conditions  and are suboptimal for other initial
conditions  which demonstrates that no uniformly optimal policy exists for this example  
the intuition behind the fact that uniformly optimal policies do not  in general  exist
for constrained mdps is that since the resource information is not a part of the mdp state
space  and there are constraints imposed on resource usage  the principle of bellman optimality does not hold  optimal actions for different states cannot be chosen independently  
given a constrained mdp  it is possible to construct an equivalent unconstrained mdp with
the standard properties of optimal solutions  by folding the resource information into the
state space  and modeling resource constraints via the transition function   but the resulting
state space will be exponential in the number of resources 
we now analyze the computational complexity of the optimization problem     
   

firesource allocation among agents with mdp induced preferences

theorem   the following decision problem is np complete  given an instance of an mdp
hs  a  p  r  o    c    
b  i with resources and capacity constraints  and a rational number y  
does there exist a feasible policy   whose expected total reward  given   is no less than y  
proof  see appendix a   

note that the above complexity result stems from the limited capacities of the agents
and the fact that we define the resource requirements of a policy as the set of all resources
needed to carry out all actions that have a nonzero probability of being executed  if 
however  we defined constraints on the expected resource requirements  then actions with
low probability of being executed would have lower resource requirements  optimal policies
would be randomized  and the problem would be equivalent to a knapsack with continuously
divisible items  which is solvable in polynomial time via the lp formulation of mdps with
linear constraints       
    milp solution
now that we have analyzed the properties of the optimization problem      we present a
formulation of     as a mixed integer linear program  milp   given that we have established
np completeness of     in the previous section  milp  also np complete  is a reasonable
formulation that allows us to reap the benefits of a vast selection of efficient algorithms and
tools  see  for example  the text by wolsey       and references therein  
in this section and in the rest of the paper we will assume that the resource requirements
of actions are binary  i e    a  o            we make this assumption to simplify the
discussion  and it does not limit the generality of our results  we briefly describe the case
of non binary resource costs in appendix b for completeness  but refer to the work by
dolgov        for a more detailed discussion and examples 
let us rewrite     in the occupation measure coordinates x by adding the constraints
from     to the standard lp in occupancy coordinates      noticing that  for states with
nonzero probability of being visited   s  a  and x s  a  are either zero or nonzero simultaneously 
x
x


x s  a   
a  a 
 s  a    h
h
s

s

and that  when  a  o            the total resource requirements of a policy can be simplified
as follows 
x

n
x
x
o
max  a  o h
x s  a    h
 a  o 
x s  a   
o  o 
    
a

a

s

s

we get the following program in x 
xx
max
x s  a r s  a 
x

s

a

subject to 
x
xx
x   a   
x s  a p  s  a       
a

x

s

 o  c h

o

x
a

  s 

a

 a  o 

x


x s  a   
b c  

c  c 

s

x s  a     

s  s  a  a 
   

    

fidolgov   durfee

the challenge in solving this mathematical program is that the constraints are nonlinear
due to the heaviside function h 
to linearize the heaviside function  we augment the original
variables
 poptimization
 x
p
with a set of  o  binary variables  o           where  o    h
a  a  o 
s x s  a    in
other words   o  is an indicator variable that shows whether the policy requires resource o 
using  o   we can rewrite the resource constraints in      as 
x
 o  c  o   
b c  
c  c 
    
o

which are linear in   we can then synchronize  and x via the following linear inequalities 
x
x
  x
 a  o 
x s  a    o  
o  o 
    
a

s

p
p
where x  maxo a  a  o  s x s  a  is the normalization constant  for which any upper bound on the argument of h   can be used  the bound x is guaranteed
to exist
p
  max
 a 
o  
since
for
discounted
problems 
for
example 
we
can
use
x
 
  

 
o
a
p
 
for any x that is a valid occupation measure for an mdp with
s a x s  a         
 
discount factor  
putting it all together  the problem     of finding optimal policies under resource constraints can be formulated as the following milp 
xx
max
x s  a r s  a 
x 

s

a

subject to 
x
xx
x   a   
x s  a p  s  a       
a

x

s

  s 

a

 o  c  o   
b c  

c  c 

x

o  o 

    

o

  x

 a  o 

a

x

x s  a    o  

s

x s  a     

s  s  a  a 

 o          

o  o 

we illustrate the milp construction with an example 
example   let us formulate the milp for the constrained problem from example    recall that in that problem there are three resources o    ot   of   om    truck  forklift  and
mechanic   one capacity type c    c     money   and actions have the following resource
requirements  again  listing only the nonzero ones  
 a    ot         a    ot         a    of         a    ot         a    ot         a    om      
p
p
   instead of using a single x for all resources  a different x o   a  a  o  s x s  a  can be used for
every resource  leading to more uniform normalization and potentially better numerical stability of the
milp solver 

   

firesource allocation among agents with mdp induced preferences

the resources have the following capacity costs 
 ot   c        

 of   c        

 om   c        

and the agent has a limited budget  i e   a capacity bound  
b c        
to compute the optimal policy for an arbitrary   we can formulate the problem as
an milp as described above  using binary variables  o      ot     of     om     we can
express the constraint on capacity cost as the following inequality 
  ot       of       om      
for the constraints that synchronizep
the occupation measure x and the binary indicators  o  
we can set x          maxo a  a  o              combining this with other
constraints from       we get an milp with    continuous and   binary variables  and
 s     c     o                  constraints  not counting the last two sets of range constraints  

as mentioned earlier  even though solving such programs is  in general  an np complete
task  there is a wide variety of very efficient algorithms and tools for doing so  therefore 
one of the benefits of formulating the optimization problem     as an milp is that it allows
us to make use of the highly efficient existing tools 

   multiagent resource allocation
we now consider the multiagent problem of resource allocation between several agents 
where the resource preferences of the agents are defined by the constrained mdp model
described in the previous section  we reiterate our main assumptions about the problem 
   weak coupling  we assume that agents are weakly coupled  meuleau et al         
i e   they only interact through the shared resources  and once the resources are allocated  the agents transitions and rewards are independent  this assumption is critical
to our results  
   one shot resource allocation  the resources are distributed once before the agents
start executing their mdps  there is no reallocation of resources during the mdp
phase  this assumption is critical to our results  allowing reallocation of resources
would violate the weak coupling assumption 
   initial central control over the resources  we assume that at the beginning of
the resource allocation phase  the resources are controlled by a single authority  this
is the standard sell auction setting  for problems where the resources are distributed
   if agents are cooperative  the assumption about weak coupling can be relaxed  at the expense of an
increase in complexity   and the same milp based algorithm for simultaneously performing policy optimization and resource allocation can be applied if we consider the joint state spaces of the interacting
agents  for self interested agents  a violation of the weakly coupled assumption would mean that the
agents would be playing a markov game  shapley        once the resources are allocated  which would
significantly complicate the strategic analysis of the agents bidding strategies during the initial resource
allocation 

   

fidolgov   durfee

among the agents to begin with  we face the problem of designing a computationallyefficient combinatorial exchange  parkes  kalagnanam    eso         which is a more
complicated problem that is outside the scope of this work  however  many of the
ideas presented in this paper could potentially be applicable to that domain as well 
   binary resource costs  as before  we assume that agents resource costs are binary 
this assumption is not limiting  the case of non binary resources is discussed in
appendix b 
formally  the input to the resource allocation problem consists of the following 
 m is the set of agents  we will use m  m to refer to an agent 
  hs  a  pm   rm   m   m   
bm i  is the collection of weakly coupled single agent mdps 
as defined in the single agent model in section    for simplicity  but without loss of
generality  we assume that all agents have the same state and action spaces s and a 
but each has its own transition and reward functions pm and rm   initial conditions m  
as well as its own resource requirements m   a  o          and capacity bounds

bm   c   r  we also assume that all agents have the same discount factor   but this
assumption can be trivially relaxed 
 o and c are the sets of resources and capacities  defined exactly as in the single agent
model in section   
    o  c   r specifies the capacity costs of the resources  defined exactly as in the
single agent model in section   
 b   o   r specifies the upper bound on the amounts of the shared resources  this
defines the additional bound for the multiagent problem  
given the above  our goal is to design a mechanism for allocating the resources to the
agents in an economically efficient way  i e   in a way that maximizes the social welfare of
the agents  one of the most often used criteria in mechanism design   we would also like
the mechanism to be efficient from the computational standpoint 
example   suppose that there are two delivery agents  the mdp and capacity constraints
of the first agent are exactly as they were defined previously in examples   and    the mdp
of the second agent is almost the same as that of the first agent  with the only difference
that it gets a slightly higher reward for delivering appliances  r   s    a          whereas
r   s    a         for the first agent   suppose there are two trucks  one forklift  and one
mechanic that are shared by the two agents  these bounds are specified as follows 
b ot       

b of       

b om       

then the problem is to decide which agent should get the forklift  and which should get the
mechanic  trucks are plentiful in this example  

   

firesource allocation among agents with mdp induced preferences

    combinatorial auctions
as previously mentioned  the problem of finding an optimal resource allocation among
self interested agents that have complex valuations over combinations of resources arises
in many different domains  e g   ferguson  nikolaou  sairamesh    yemini        wellman
et al         and is often called a combinatorial allocation problem  a natural and widely
used mechanism for solving such problems is a combinatorial auction  ca   e g   de vries
  vohra         in a ca  each agent submits a set of bids for resource bundles to the
auctioneer  who then decides what resources each agent will get and at what price 
consider the problem of allocating among a set of agents m a set of indivisible resources o  where the total quantity of resource o  o is bounded by b o   our earlier
simplifying assumption that actions resource requirements are binary implies that agents
will only be interested in bundles that contain no more than one unit of a particular resource 
in a combinatorial auction  each agent m  m submits a bid bm
w  specifying how much
the agent is willing to pay  for every bundle w  w m that has some value um
w      in some
cases  it is possible to express such bids without enumerating all bundles  for example  using
an xor bidding language  sandholm        where it is necessary to only consider bundles
with strictly positive value  such that no subset of a bundle has the same value   such
techniques often reduce the complexity of the resource allocation problem  but do not  in
general  avoid the exponential blow up in the number of bids  therefore  below we describe
the simplest combinatorial auction with flat bids  but it should be noted that many concise
bidding languages exist that in special cases can reduce the number of explicit bids 
after collecting all the bids  the auctioneer solves the winner determination problem
 wdp   a solution to which prescribes how the resources should be distributed among the
m   its utility is um  q m
agents and at what prices  if agent m wins bundle w at price qw
w
w
 we are assuming risk neutral agents with quasi linear utility functions   thus  the optimal
bidding strategy of an agent depends on how the auctioneer allocates the resources and sets
prices 
vickrey clarke groves  vcg  mechanisms  vickrey        clarke        groves       
are a widely used family of mechanisms that have certain very attractive properties  discussed in more detail below   an instantiation of a vcg mechanism in the context of
combinatorial auctions is the generalized vickrey auction  gva   mackie mason   varian         which allocates resources and sets prices as follows  given the bids bm
w of all
agents  the auctioneer chooses an allocation that maximizes the sum of agents bids  this
problem is np complete  rothkopf et al         and can be expressed as the following inm          are indicator variables that
teger program  where the optimization variables zw
show whether bundle w is assigned to agent m  and nwo          specifies whether bundle w
contains o  

   there are other related algorithms for solving the wdp  e g   sandholm         but we will use the
integer program      as a representative formulation for the class of algorithms that perform a search in
the space of binary decisions on resource bundles 

   

fidolgov   durfee

x

max
z

x

m m
zw
bw

mm ww m

subject to 
x
m
zw
   

m  m 

    

ww m

x

x

mm

ww m

m
zw
nwo  b o  

o  o 

the first constraint in      says that no agent can receive more than one bundle  while the
second constraint ensures that the total amount of resource o assigned to the agents does
not exceed the total amount available  notice that milp      performs the summation over
exponentially large sets of bundles w  w m   as outlined above  in an auction with xor
bidding  these sets would typically be smaller  but  in general  still exponentially large 
a gva assigns resources according to the optimal solution ze to      and sets the payment
for agent m to 
x
m

m  m 
qw
  vm

zew
bw  
    
m    m

where vm
is the value of      if m were to not participate in the auction  the optimal
value if m does not submit any bids   and the second term is the sum of other agents bids
in the solution ze to the wdp with m participating 
a gva has a number of nice properties  it is strategy proof  meaning that the dominant
m
strategy of every agent is to bid its true value for every bundle  bm
w   uw   the auction
is economically efficient  meaning that it allocates the resources to maximize the social
welfare of the agents  because  when agents bid their true values  the objective function
of      becomes the social welfare   finally  a gva satisfies the participation constraint 
meaning that no agent decreases its utility by participating in the auction 
a straightforward way to implement a gva for our mdp based problem is the following 
let each agent m  m enumerate all resource bundles w m that satisfy its local capacity
constraints defined by 
bm  c   this is sufficient because our mdp model implies free disposal
of resources for the agents  and we make the same assumption about the auctioneer   for
each bundle w  w m   agent m would determine the feasible action set a w  and formulate
an mdp m  w    hs  a w   pm  w   rm  w   m i  where pm  w  and rm  w  are the transition
and reward functions defined on the pruned action space a w   every agent would then
solve each m  w  corresponding to a feasible bundle to find the optimal policy 
em  w   whose
m
m
m
expected discounted reward would define the value of bundle w  uw   u  e
  w   m   
this mechanism suffers from two major complexity problems  first  the agents have
to enumerate an exponential number of resource bundles and compute the value of each
by solving the corresponding  possibly large  mdp  second  the auctioneer has to solve
an np complete winner determination problem on exponentially large input  the following
sections are devoted to tackling these complexity problems 

example   consider the two agent problem described in example    where two trucks  one
forklift  and the services of one mechanic are being auctioned off  using the straightforward
version of the combinatorial auction outlined above  each agent would have to consider
   

firesource allocation among agents with mdp induced preferences

  o           possible resource bundles  since resource requirements of both agents are
binary  neither agent is going to bid on a bundle that contains two trucks   for every
resource bundle  each agent will have to formulate and solve the corresponding mdp to
compute the utility of the bundle 
for example  if we assume that both agents start in state s   different initial conditions
would result in different expected rewards  and thus different utility functions   the value of
the null resource bundle to both agents would be    since the only action they would be able
to execute is the noop a     on the other hand  the value of bundle  ot   of   om               that
contains all the resources would be      to the first agent and       to the second one  the
value of bundle           to each agent would be the same as the value of            since their
optimal policies for the initial conditions that put them in s  do not require the mechanic  
once the agents submit their bids to the auctioneer  it will have to solve the wdp via
the integer program      with  m   o               binary variables  given the above  the
optimal way to allocate the resources would be to assign a truck to each of the agents  the
forklift to the second agent  and the mechanic to either  or neither  of the two  thus  the
agents would receive bundles           and            respectively  resulting in social welfare of
                    however  if at least one of the agents had a non zero probability of
starting in state s    the value of the resource bundles involving the mechanic would change
drastically  as would the optimal resource allocation and its social value 

    avoiding bundle enumeration
to avoid enumerating all resource bundles that have non zero value to an agent  two things
are required  i  the mechanism has to support a concise bidding language that allows
the agent to express its preferences to the auctioneer in a compact manner  and ii  the
agents have to be able to find a good representation of their preferences in that language  a
simple way to achieve both in our model is to create an auction where the agents submit the
specifications of their resource parameterized mdps to the auctioneer as bids  the language
is compact and  given our assumption that each agent can formulate its planning problem
as an mdp  this does not require additional computation for the agents  however  this
only changes the communication protocol between the agents and the auctioneer  similarly
to other concise bidding languages  sandholm        nisan        boutilier   hoos       
boutilier         as such  it simply moves the burden of solving the valuation problem from
the agents to the auctioneer  which by itself does not lead to any gains in computational
efficiency  such a mechanism also has implications on information privacy issues  because
the agents have to reveal their local mdps to the auctioneer  which they might not want
to do   nevertheless  we can build on this idea to increase the efficiency of solving both the
valuation and winner determination problems and do so while keeping most of the agents
mdp information private  we address ways of maintaining information privacy in the next
section  and for the moment focus on improving the computational complexity of the agents
valuation and the auctioneers winner determination problems 
the question we pose in this section is as follows  given that the bid of each agent consists of its mdp  its resource information and its capacity bounds hs  a  pm   rm   m   m   
bm i 
can the auctioneer formulate and solve the winner determination problem more efficiently
   

fidolgov   durfee

than by simply enumerating each agents resource bundles and solving the standard integer
program      with an exponential number of binary variables 
therefore  the goal of the auctioneer is to find a joint policy  a collection of single agent
policies under our weak coupling assumption  that maximizes the sum of the expected total
discounted rewards for all agents  under the conditions that  i  no agent m is assigned a set
of resources that violates its capacity bound 
bm  i e   no agent is assigned more resources
than it can carry   and ii  the total amounts of resources assigned to all agents do not exceed
the global resource bounds b o   i e   we cannot allocate to the agents more resources than
are available   this problem can be expressed as the following mathematical program 
x
max
um   m   m  


m

subject to 
x
x

 o  c h m  a  o 
 m  s  a   
bm  c  
o

x

c  c  m  m 

    

s
m

  a  o h

x

m


  s  a   b o  

o  o 

s

m

obviously  the decision version of this problem is np complete  as it subsumes the singleagent mdp with capacity constraints  np completeness of which was shown in section     
moreover  the problem remains np complete even in the absence of single agent capacity
constraints  indeed  the global constraint on the amounts of the shared resources is sufficient
to make the problem np complete  which can be shown with a straightforward reduction
from knapsack  similar to the one used in the single agent case in section     
we can linearize      similarly to the single agent problem from section      yielding
the following milp  which is simply a multiagent version of       recall the assumption of
this section that resource requirements are binary  
xxx
max
xm  s  a rm  s  a 
x 

m

s

a

subject to 
x
xx
xm    a   
xm  s  a pm   s  a    m    
a

x

s

  s  m  m 

a

 o  c  m  o   
bm  c  

c  c  m  m 
    

o

x

m

  o   b o  

o  o 

m

  x

x

m  a  o 

a

x

xm  s  a    m  o  

o  o  m  m 

s

xm  s  a     

s  s  a  a  m  m 

m

  o          

o  o  m  m 

where x  maxo m a  a  o  s xm  s  a  is an upper bound on the argument of h   
used for normalization  as in the single agent case  this bound is guaranteed to exist for
discounted mdps and is easy to obtain 
p

p

   

firesource allocation among agents with mdp induced preferences

the milp      allows the auctioneer to solve the wdp without having to enumerate
the possible resource bundles  as compared to the standard wdp formulation       which
has on the order of  m   o  binary variables       has only  m  o  binary variables  this
exponential reduction is attained by exploiting the knowledge of the agents mdp based
valuations and simultaneously solving the policy optimization and resource allocation problems  given that the worst case solution time for milps is exponential in the number of
integer variables  this reduction has a significant impact on the worst case performance of
the mechanism  the average case running time is also reduced drastically  as demonstrated
by our experiments  presented in section   
example   if we apply the mechanism discussed above to our running example as an alternative to the straightforward combinatorial auction presented in example    the winnerdetermination milp      will look as follows  it will have  m  s  a                   continuous occupation measure variables xm   and  m  o               binary variables  m  o  
it will have  m  s               conservation of flow constraints that involve continuous
variables only  as well as  m  c     o     m  o                            constraints that
involve binary variables 
the capacity constraints for the agents will be exactly as in the single agent case described
in example    and the global resource constraints will be 
    ot         ot      

    of         of      

    om         om      

notice that in this example there is one binary decision variable per resource per agent
 yielding   such variables for this simple problem   this is exponentially fewer than the
number of binary variables in the straightforward ca formulation of example    which
requires one binary variable per resource bundle per agent  yielding    such variables for
this problem   given that milps are np complete in the number of integer variables  this
reduction from    to   variables is noticeable even in a small problem like this one and can
lead to drastic speedup for larger domains 

the mechanism described above is an instantiation of the gva  so by the well known
properties of vcg mechanisms  this auction is strategy proof  the agents have no incentive
to lie to the auctioneer about their mdps   it attains the socially optimal resource allocation 
and no agent decreases its utility by participating in the auction 
to sum up the results of this section  by having the agents submit their mdp information to the auctioneer instead of their valuations over resource bundles  we have essentially
removed all computational burden from the agents and at the same time significantly simplified the auctioneers winner determination problem  the number of integer variables in
the wdp is reduced exponentially  
    distributing the winner determination problem
unlike the straightforward combinatorial auction implementation discussed earlier in section      where the agents shared some computational burden with the auctioneer  in the
mechanism from section      the agents submit their information to the auctioneer and
then just idle while waiting for a solution  this suggests further potential improvements in
computational efficiency  indeed  given the complexity of milps  it would be beneficial to
   

fidolgov   durfee

exploit the computational power of the agents to offload some of the computation from the
auctioneer back to the agents  we assume that agents have no cost for helping out and
would prefer for the outcome to be computed faster    thus  we would like to distribute
the computation of the winner determination problem       the common objective in distributed algorithmic mechanism design  feigenbaum   shenker        parkes   shneidman 
        
for concreteness  we will base the algorithm of this section on the branch and bound
method for solving milps  wolsey         but exactly the same techniques will also work
for other milp algorithms  e g   cutting planes  that perform a search in the space of lp
relaxations of the milp  in branch and bound for milps with binary variables  lp relaxations are created by choosing a binary variable and setting it to either   or    and relaxing
the integrality constraints of other binary variables  if a solution to an lp relaxation happens to be integer valued  it provides a lower bound on the value of the global solution  a
non integer solution provides an upper bound for the current subproblem  which  combined
with other lower bounds  is used to prune the search space 
thus  a simple way for the auctioneer to distribute the branch and bound algorithm is
to simply farm out lp relaxations to other agents and ask them to solve the lps  however 
it is easy to see that this mechanism is not strategy proof  indeed  an agent that is tasked
with performing some computation for determining the optimal resource allocation or the
associated payments could benefit from lying about the outcome of its computation to
the auctioneer  this is a common phenomenon in distributed mechanism implementations 
whenever some wdp calculations are offloaded to an agent participating in the auction  the
agent might be able to benefit from sabotaging the computation  there are several methods
to ensuring the strategy proofness of a distributed implementation  the approach best
suited for our problem is based on the idea of redundant computation  parkes   shneidman 
         where multiple agents are asked to do the same task and any disagreement is
carefully punished to discourage lying  in the rest of this section  we demonstrate that this
is very easy to implement in our case 
the basic idea is very simple  let the auctioneer distribute lp relaxations to the agents 
but check their solutions and re solve the problems if the agents return incorrect solutions
 this would make truthful computation a weakly dominant strategy for the agents  and
a nonzero punishment can be used to achieve strong dominance   this strategy of the
auctioneer removes the incentive for the agents to lie and yields exactly the same solution
as the centralized algorithm  however  in order for this to be beneficial  the complexity of
checking a solution must be significantly lower than the complexity of solving the problem 
fortunately  this is true for lps 
suppose the auctioneer has to solve the following lp  which can be written in two
equivalent ways  let us refer to the one on the left as the primal  and to the one on the right
   as observed by parkes and shneidman         this assumption is a bit controversial  since a desire for
efficient computation implies nonzero cost for computation  while the agents cost for helping out is
not modeled  it is  nonetheless  a common assumption in distributed mechanism implementations 
    we describe one simple way of distributing the mechanism  others are also possible 
    redundant computation is discussed by parkes and shneidman        in the context of ex post nash
equilibria  whereas we are interested in dominant strategies  but the high level idea is very similar 

   

firesource allocation among agents with mdp induced preferences

as the dual  
min t v

max rt x

subject to 

    

subject to 

t

ax    

a v  r 

x    

by the strong duality property  if the primal lp has a solution v   then the dual also has a
solution x   and t v   rt x   furthermore  given a solution to the primal lp  it is easy to
compute a solution to the dual  by complementary slackness  vt   rt b   and x   b    
where b is a square invertible matrix composed of columns of a that correspond to basic
variables of the solution 
these well known properties can be used by the auctioneer to quickly check optimality
of solutions returned by the agents  suppose that an agent returns v as a solution to the
primal lp  the auctioneer can calculate the dual solution vt   rt b   and check whether
rt x   t v  thus  the most expensive operation that the auctioneer has to perform is
the inversion of b  which can be done in sub cubic time  as a matter of fact  from the
implementation perspective  it would be more efficient to ask the agents to return both the
primal and the dual solutions  since many popular algorithms compute both in the process
of solving lps 
thus  we have provided a simple method that allows us to effectively distribute the
winner determination problem  while maintaining strategy proofness of the mechanism and
with a negligible computation overhead for the auctioneer 
    preserving information privacy
the mechanism that we have discussed so far has the drawback that it requires agents
to reveal complete information about their mdps to the auctioneer  the problem is also
exacerbated in the distributed wdp algorithm from the previous section  since not only
does each agent reveal its mdp information to the auctioneer  but that information is then
also spread to other agents via the lp relaxations of the global milp  we now show how
to alleviate this problem 
let us note that  in saying that agents prefer not to reveal their local information  we
are implicitly assuming that there is an external factor that affects agents utilities that is
not captured in the agents mdps  a sensible way to measure the value of information is by
how it changes ones decision making process and its outcomes  since this effect is not part
of our model  in fact  it contradicts our weak coupling assumption   we cannot in a domainindependent manner define what constitutes useful information  and how bad it is for an
agent to reveal too much about its mdp  modeling such effects and carefully analyzing
them is an interesting research task  but it is outside the scope of this paper  thus  for the
purposes of this section  we will be content with a mechanism that hides enough information
to make it impossible for the auctioneer or an agent to uniquely determine the transition
or reward function of any other agent  in fact  information revealed to any agent will map
to infinitely many mdps of other agents     many such transformations are possible  we
present just one to illustrate the concept 
    a more stringent condition would require that agents preferences over resource bundles are not revealed
 parkes         but we set a lower bar here 

   

fidolgov   durfee

the main idea of our approach is to modify the previous mechanism so that the agents
submit their private information to the auctioneer in an encrypted form that allows the
auctioneer to solve the winner determination problem  but does not allow it to infer the
agents original mdps 
first  note that  instead of passing an mdp to the auctioneer  each agent can submit
an equivalent lp      so  the question becomes  can the agent transform its lp in such a
way that the auctioneer will be able to solve it  but will not be able to infer the transition
and reward functions of the originating mdp  in other words  the problem reduces to the
following  given an lp l   created from an mdp    hs  a  p  r  i via       we need to
find a transformation l   l  such that a solution to the transformed lp l  will uniquely
map to a solution to the original lp l    but l  will not reveal the transition or the reward
functions of the original mdp  p or r   we show that a simple change of variables suffices 
suppose agent m  has an mdp originated lp and is going to ask agent m  to solve it  in
order to maintain the linearity of the problem  to keep it simple for m  to solve   m  should
limit itself to linear transformations  consider a linear  invertible transformation of the
primal coordinates u   f v  and a linear  invertible transformation of the dual coordinates
y   dx  then  the lp from      will be transformed  by applying f   switching to the
dual  and then applying d  to an equivalent lp in the new coordinates y 
max rt d  y
subject to 
 f    t ad  y    f    t  

    

d  y    
the value of the optimal solution to      will be the same as the value of the optimal solution
to       and given an optimal solution y to       it is easy to compute the solution to the
original  x   d  y   indeed  from the perspective of the dual  the primal transformation f
is equivalent to a linear transformation of the dual equality constraints ax     which  given
that f is non singular  has no effect on the solution or the objective function  furthermore 
the dual transformation d is equivalent to a change of variables that modifies the solution
but not the value of the objective function 
however  a problem with the above transformations is that it gives away d    indeed 
agent m  will be able to simply read  up to a set of multiplicative constants  the transformation off the constraints d  y     therefore  only diagonal matrices with positive
coefficients  which are equivalent to stretching the coordinate system  are not trivially deduced by m    since they also map to y     choosing a negative multiplier for some xi
 inverting the axis  is pointless  because that flips the non negativity constraints to yi    
immediately revealing the sign to m   
let us demonstrate that  given any mdp  and the corresponding lp l    we can
choose d and f such that it will be impossible for m  to determine the coefficients of l 
 or equivalently the original transition and reward functions p and r   when agent m 
receives l   as in        all it knows is that l  was created from an mdp  so the columns
of the constraint matrix of the original lp l  must sum to a constant 
x
x
aji      
p  s  a        
    
j



   

firesource allocation among agents with mdp induced preferences

a  
p  
r  

a  
p  
r  

s 

s 

a  
p  
r  

a  
p     
r    

a  
p    
r     

a  
p    
r     
a  
p     
r     

a  
p    
r     

a  
p  
r  

s 

s 

a  
p  
r  

a  
p    
r     

 a 

a  
p  
r  

a  
p  
r  

s 

s 

a  
p  
r  

a  
p     
r    

 b 

figure    preserving privacy example  two different mdps that can lead to the same lp
constraint matrix 

this gives m  a system of  s  nonlinear equations for the diagonal d and arbitrary f  
which have a total of  s  a     s   free parameters  for everything but the most degenerate
cases  which can be easily handled by an appropriate choice of d and f    these equations
are hugely under constrained and will have infinitely many solutions  as a matter of fact 
by sacrificing  s  of the free parameters  m  can choose d and f in such a way that the
columns of constraints in l  will also sum to a constant             which would have the
effect of transforming l  to an l  that corresponds to another valid mdp     therefore 
given an l    there are infinitely many original mdps  and transformations d and f that
map to the same lp l   
we also have to consider the connection of resource and capacity costs to agents occupation measures in the global wdp       there are two things that the auctioneer has
to be able to do  i  determine the value of each agents policy  to be able to maximize
the social welfare   and ii  determine the resource requirements of the policies  to check
the resource constraints   so  the question is  how does our transformation affect these 
as noted earlier  the transformation does not change the objective function  so the first
requirement holds  on the other hand  d does change the occupation measure xm  s  a  by
arbitrary multipliers  however  a multiplicative factor of xm  s  a  has no effect on the usage
of non consumable resources  as it only matters whether the corresponding xm  s  a  is zero
or not  step function h nullifies the scaling effect   thus  the second condition also holds 
example   consider the two state mdp depicted in figure  a that represents the decisionmaking problem of a sales company  with the two states corresponding to possible market
conditions  and the two actions  to two possible sales strategies  market conditions in state
s  are much more favorable than in state s   the rewards for both actions are higher   the
transitions between the two states correspond to probabilities of market conditions changing
and the rewards reflect the expected profitability in these two states  obtaining such numbers
in a realistic scenario would require performing costly and time consuming research  and the
company might not want to make this information public 
therefore  if the company were to participate in the resource allocation mechanism described above  it would want to encrypt its mdp before submitting it to the auctioneer 
   

a  
p    
r     

fidolgov   durfee

the mdp has the following reward function
r                            t  
and the following transition function 


 
 
p a     
 
       


p a     


           
 
   
   

    

    

using         this corresponds to the following conservation of flow constraint matrix 


                 
a 
 
    
           
   
before submitting its lp to the auctioneer  the agent applies the following transformations 


 
 
 
    
d   diag                            f  
           
yielding the following new constraint matrix 


   
 
 
 
 
  t
 
a    f   ad  
 
             

    

however  the above constraint matrix a  corresponds to a non transformed conservation
of flow constraint for a different mdp  shown in figure  b  with         the following
reward function 
r               t  
    
and the following transition function 


   
p a     
 
   


p a     


   
 
   

    

therefore  when the auctioneer receives the constraint matrix a    it has no way of knowing whether the agent has an mdp with transition function      that was transformed
using      or the mdp with transition function      that was not transformed  notice that
the dynamics of the two mdps vary significantly  both in transition probabilities and state
connectivity  the second mdp does not reveal any information about the originating mdp
and the corresponding market dynamics 

to sum up  we can  to a large extent  maintain information privacy in our mechanism by
allowing agents to apply linear transformations to their original lps  the information that
is revealed by our mechanism consists of agents resource costs m  a  o   capacity bounds

bm  c   and the sizes of their state and action spaces  the latter can be hidden by adding
dummy states and actions to the mdp  
the revealed information can be used to infer agents preferences and resource requirements  further  numeric policies are revealed  but the lack of information about transition
and reward functions renders this information worthless  as just illustrated in example   
there could be multiple originating mdps with very different properties  
   

firesource allocation among agents with mdp induced preferences

   experimental results
in this section we present an empirical analysis of the computational complexity of the
resource allocation mechanism described in section    we report results on the computational complexity of the mechanism from section      where the agents submit their
mdps to the auctioneer  who then simultaneously solves the resource allocation and policyoptimization problems  as far as the additional speedup achieved by distributing the wdp 
as described in section      we do not report empirical results  since it is well established
in the parallel programming literature that parallel versions of branch and bound milp
solvers consistently achieve linear speedup  eckstein  phillips    hart         this is due
to the fact that branch and bound algorithms require very little inter process communication 
for our experiments  we implemented a multiagent delivery problem  based on the
multiagent rover domain  dolgov   durfee         in this problem  agents operate in a
stochastic grid world with delivery locations randomly placed throughout the grid  each
delivery task requires a set of resources  and there are limited quantities of the resources 
there are random delivery locations on the grid  and each location has a set of deliveries that
it accepts  each resource has some size requirements  capacity cost   and each delivery agent
has bounded space to hold the resources  limited capacity   the agents participate in an
auction where they bid on delivery resources  in this setting  the value of a resource depends
on what other resources the agent acquires and what other deliveries it can make  given a
bundle of resources  an agents policy optimization problem is to find the optimal delivery
plan  the exact parameters used in our experiments are not critical for the trends seen in
the results presented below  but for the sake of reproducibility the domain is described in
detail in appendix c    all of the resource costs in the experiments presented below are
binary 
computational complexity of constrained optimization problems can vary greatly as
constraints are tightened or relaxed  therefore  as our first step in the analysis of empirical
computational complexity of our mechanism  we investigate how its running time depends
on the capacity constraints of each agent and on the bounds on the total amounts of
resources shared by the agents  as is common with other types of constrained optimization
or constraint satisfaction problems  it is natural to expect that the wdp milp will be
easy to solve when the problem is over  or under constrained in either the capacity or the
resource bounds  to empirically verify this  we varied local capacity constraint levels from  
 meaning agents cannot use any resources  to    meaning each agent has the capacity to
use enough resources to execute its optimal unconstrained policy   as well as the global
constraint levels for which   meant that no resources were available to the agents  and  
meant that there were enough resources to assign to each agent its most desired resource
bundle  in all of our experiments  the part of the milp solver was played by cplex    
on a pentium   machine with  gb of ram  ram was not a bottleneck due to the use of
sparse matrix representations   a typical running time profile is shown in figure    the
problem is very easy when it is over constrained  becomes more difficult as the constraints
are relaxed and then abruptly becomes easy again when capacity and resource levels start
to approach utopia 
    we also investigated other  randomly generated domains  and the results were qualitatively the same 

   

fidolgov   durfee

 
   
   
local constraint level

 

t  sec

 

 

 
 

 

   
local constraints

   
 

 

   
   
   
   
   
   
   
 
 

global constraints

   

   
   
global constraint level

   

 

figure    running time for mdp based winner determination milp      for different levels of global  b
  and local  b
m   constraints  the constraint levels are fractions
of utopian levels that are needed to implement optimal unconstrained policies 
problems involved    agents  each operating on a   by   grid  with    shared
resource types  each data point shown is an average of ten runs over randomlygenerated problems 

in all of the following experiments we aim to avoid the easy regions of constraint levels 
therefore  given the complexity profiles  we set the constraint levels to     for both local
capacity and global resource bounds  we also set the discount factor to          this
value was chosen arbitrarily  because our investigations into the effect of the value of  on
the running time of the milp revealed no significant trends 
we begin by comparing the performance of our mdp based auction  section      to the
performance of the straightforward ca with flat preferences  as described in section      
the results are summarized in figure    which compares the time it takes to solve the
standard winner determination problem on the space of all resource bundles      to the
time needed to solve the combined mdp wdp problem      used in our mechanism  as
the number of resources is increased  with   agents  on a   by   grid   despite the fact
that both algorithms have exponential worst case running time  the number of integer
variables in      is exponentially larger than in our milp       the effect of which is clearly
demonstrated in figure    furthermore  this comparison gives an extremely optimistic
view of the performance of the standard ca  as it does not take into account the additional
complexity of the valuation problem  which requires formulating and solving a very large
number of mdps  one per resource bundle   on the other hand  the latter is embedded into
the wdp of our mechanism       thus including the time for solving the valuation problem
in the comparison would only magnify the effect  in fact  in our experiments  the time
required to solve the mdps for the valuation problem was significantly greater than the
time for solving the resulting wdp milp  however  we do not present quantitative results
to that effect here  because of the difference in implementation  iterating over resource
bundles and solving mdps was done via a straightforward implementation in matlab  while
   

firesource allocation among agents with mdp induced preferences

figure    gains in computation efficiency  mdp based wdp versus a wdp in a straightforward ca implementation  the latter does not include the time for solving
the mdps to compute resource bundle values  error bars show the standard
deviation over ten runs 
n     o      
 

  

 

  

 

t  sec

  

 

  

 

  

 

  

 

  
  
number of agents  m 

  

  

figure    scaling the mdp based winner determination milp      to more agents  agents
operated on   by   grids and shared    types of resources 

milps were solved using highly optimized cplex code   no parallelization of the wdp
was performed for these experiments for either algorithm 
below we analyze the performance of our algorithm on larger problems infeasible for
the straightforward ca  figure   illustrates the scaling effect as the number of agents
participating in the auction is increased  here and below  each point on the plot corresponds
to a single run of the experiment  with no less than ten runs performed for every value of
parameters   and the solid line is the mean  recall that the size of the wdp scales linearly
   

fidolgov   durfee

n        m     

n       m      

 

  

 

  

 

 

  
t  sec

t  sec

  

 

  

 

  

 

  

 

  

 

  

  
  
  
number of resource types  o 

 

  

   

 a 

 

  

  
  
  
number of resource types  o 

   

 b 

n       m      
 

  

 

t  sec

  

 

  

 

  

 

  

 

  

 

  

  
  
  
number of resource types  o 

   

 c 

 d 

figure     a  c   scaling of the mdp based winner determination milp      with the
number of resources on three sets of problems with different grid sizes  n  and
different numbers of agents   m     d   a linear scale plot of the tail of the data
in  c  

with the number of agents  the graph therefore reflects a rather standard scaling effect
for an np complete problem  as can be seen from the plot  problems with    agents
and    resource types are well within the reach of the method  on average taking around
   minutes 
next  we analyze how the method scales with the number of resource types  figure  
shows the solution time as a function of the number of resource types for three different
sets of problems  in these problems  the number of actions scaled linearly with the number
of resource types  but each action required a constant number of resources  i e   the number
   

firesource allocation among agents with mdp induced preferences

n       m     

  
  
  

t  sec

  
  
  
  
  
 
 

 

  
  
resources per action

  

figure    complexity of mdp based winner determination milp      as a function of the
number of actions resource requirements 

of nonzero  a  o  per action was constant  two  regardless of the total number of resource
types  these problems exhibit an interesting trait wherein the running time peaks for
relatively low numbers of resource types  then falls quickly  and then increases much more
slowly as the number of resource types increases  as illustrated in figure  d  which uses
a linear scale   this is due to the fact that when the total number of resource types is
much higher than the number of resources required by any action  there is less contention
for a particular resource among the agents and between one agents actions  therefore  the
problems become relatively under constrained and the solution time increases slowly 
to better illustrate this effect  we ran a set of experiments inverse to the ones shown in
figure    we kept the total number of resource types constant and increased the number of
resource types required by each action  the results are shown in figure    the running time
profile is similar to what we observed earlier when we varied the local and global constraints 
when the total number of resources per action is low or high  the problem is under  or overconstrained and is relatively easy to solve  but its complexity increases significantly when
the number of resources required by each resource is in the range of        of the total
number of resource types 
based on the above  we would expect that if the actions resource requirements increased
with the total number of resource types  the problem would not scale as gracefully as in
figure    for example  figure   illustrates the running time for problems where the number
of resources required by each action scales linearly with the total number of resources  there 
the complexity does increase significantly faster  however  it is not unreasonable to assume
that in many domains the number of actions does not  in fact  increase with the total
number of resource types involved  indeed  it is natural to assume that the total number
of resource types increases as the problem becomes more complicated and the number of
tasks the agent can perform increases  however  why should the resource requirements of an
action increase as well  if the delivery agent from our running example acquires the ability
   

fidolgov   durfee

n      m     
 

  

 

  

 

t  sec

  

 

  

 

  

 

  

 

  

 

  

  
  
  
number of resource types  o 

  

figure    complexity when actions resource requirements grow proportionally to the total
number of resource types  the number of resource types needed by each action
is     of the total number of resource types  o  

to deliver pizza  it might need new resources to perform actions related to this new activity 
but one would not expect the resource requirements for delivering furniture or appliances
to change  therefore  we believe that in many real applications  our method will scale up
gracefully with the total number of resource types 
the above experiments illustrate the point that for domains where agents have preferences that are defined by underlying markov decision processes  the resource allocation
mechanism developed in this paper can lead to significant computational advantages  as
shown in figure    the method can be successfully applied to very large problems that  we
argue  are well beyond the reach of combinatorial resource allocation mechanisms with flat
preferences  as our experiments show  figure     even for small problems  combinatorial
resource allocation mechanisms with flat preferences can be time consuming  and our attempts to empirically evaluate those simpler mechanisms on larger problems proved futile 
for instance  our method takes under one minute to solve a problem that  in the standard
ca  requires the agents to enumerate up to      bundles and the auctioneer to solve an
np complete problem with an input of that size 

   generalizations  extensions  and conclusions
there are many possible extensions and generalizations of the work presented here  and we
briefly outline several below 
the treatment in this paper focused on the problem of resource allocation among
self interested agents  but the algorithms also apply to cooperative mdps with weaklyinteracting agents  in the cooperative setting  the concept of resources can be viewed as
a compact way to model inter agent constraints and their inability to include some combinations of joint actions in their policies  such weakly coupled mdps  where agents have
   

firesource allocation among agents with mdp induced preferences

independent transition and reward functions  but certain combinations of joint actions are
not feasible is a widely used model of agents interactions  e g   singh   cohn         our
model was resource centric  but more direct models are also certainly possible  for example  agents can use sat formulas to describe valid combinations of joint actions  this case
can be easily handled via simple modifications to the single and multiagent milps     
and       indeed  any sat formula can be expressed as a set of linear inequalities on
binary variables  a   or m  a  in the multiagent case   which can be directly added to
the corresponding milp  see the case of non binary resources in appendix b for an milp
defined on indicators  a   instead of the  o  used in the binary case  
as mentioned previously  our work can be extended to handle consumable resources that
are used up whenever agents execute actions  in fact  under some conditions  the problem
can be considerably simplified for domains with only these kinds of resources  the most
important change is that we have to redefine the value of a particular resource bundle to
an agent  the difficulty is that  given a policy  the total use of consumable resources is
uncertain  and the definition of the value of a resource bundle becomes ambiguous  one
possibility is to define the value of a bundle as the payoff of the best policy whose expected
resource usage does not exceed the amounts of resources in the bundle  the interpretation
of m  a  o  would also change to mean the amount of resource o consumed by action a
every time it is executed  this would make the constraints in      linear in the occupation
measure  which would tremendously simplify the wdp  making it polynomial   this is
analogous to the models used in constrained mdps  altman   shwartz         as briefly
described earlier in section    information privacy can be handled similarly to the case of
non consumable resources  however  given the transformation y   dx  the resource cost
function m will also have to be scaled by d   since the total consumption of consumable
resources is proportional to the occupation measure   this has the additional benefit of
hiding the resource cost functions  unlike the case of non consumable resources where they
were revealed   a more detailed treatment of the model with consumable resources is
presented in the work by dolgov         including a discussion of risk sensitive cases  where
the value of a resource bundle is defined as the payoff of the best policy whose probability
of exceeding the resource amounts is bounded 
in this work we exploited structure in agents preferences that stems from the underlying
policy optimization problems  however  the latter were modeled using flat mdps that
enumerate all possible states and actions  such flat mdps do not scale well due to the
curse of dimensionality  bellman         to address this  the wdp milp can be modified
to work with factored mdps  boutilier  dearden    goldszmidt        by using a factored
resource allocation algorithm  dolgov   durfee         which is based on the dual alp
method for solving factored mdps as developed by guestrin         this method allows us
to exploit both types of structure in the resource allocation algorithms  structure in agents
preferences induced by the underlying mdps  as well as structure in mdps themselves 
the resource allocation mechanism discussed in this paper assumed a one shot allocation
of resources and a static population of agents  an interesting extension of our work would
be to consider a system where agents and resources arrive and depart dynamically  as in the
online mechanism design work  parkes   singh        parkes  singh    yanovsky        
combining the mdp based model of utility functions with the dynamics of online problems
could be a valuable result and thus appears to be a worthwhile direction of future work  if
   

fidolgov   durfee

the agent population is static  but a periodic re allocation of resources is allowed  techniques
like phasing can be used to solve the resulting problem  wu   durfee        
to summarize the results of this paper  we presented a variant of a combinatorial auction for resource allocation among self interested agents whose valuations of resource bundles
are defined by their weakly coupled constrained mdps  for such problems  our mechanism 
which exploits knowledge of the structure of agents mdp based preferences  achieves an
exponential reduction in the number of integer decision variables  which in turn leads to
tremendous speedup over a straightforward implementation  as confirmed by our experimental results  our mechanism can be implemented to achieve its reduction in computational
complexity without sacrificing any of the nice properties of a vcg mechanism  optimal outcomes  strategy proofness  and voluntary participation   we also discussed a distributed
implementation of the mechanism that retains strategy proofness  using the fact that an
lp solution can be easily verified   and does not reveal agents private mdp information
 using a transformation of agents mdps  
we believe that the models and solution algorithms described in this paper significantly
further the applicability of combinatorial resource allocation mechanisms to practical problems  where the utility functions for resource bundles are defined by sequential stochastic
decision making problems 

   acknowledgments
we thank the anonymous reviewers for their helpful comments  as well as our colleagues
satinder singh  kang shin  michael wellman  demothenis teneketsis  jianhui wu  and
jeffrey cox for the valuable discussions related to this work 
this material is based in part upon work supported by honeywell international  and
by the darpa ipto coordinators program and the air force research laboratory
under contract no  fa      c      the views and conclusions contained in this document are those of the authors  and should not be interpreted as representing the official
policies  either expressed or implied  of the defense advanced research projects agency or
the u s  government 

appendix a  proofs
a   proof of theorem  
theorem   consider a finite set of n indivisible resources o    oi    i      n    with
m  n available units of each resource  then  for any non decreasing utility function
defined over resource bundles f       m n   r  there exists a resource constrained mdp
hs  a  p  r  o    c    
b  i  with the same resource set o  whose induced utility function over
the resource bundles is the same as f   in other words  for every resource bundle z      m n  
the value of the optimal policy among those whose resource requirements do not exceed z
 call this set  z   is the same as f  z  
f q       m n   r   hs  a  p  r  o    c    
b  i  
h
i
o
n fi
x

fi
 z      m n    z     fi max  a  oi  h
 s  a   zi   max u        f  z  
a

s

   

 z 

firesource allocation among agents with mdp induced preferences

s   
a  
s    a
 
a  

a  

a  

a 
r f       

a  

s    a
 
a  

s    a 
a  

a  

s   
a  

s    a 
a  

a  

a 
r f       
a 
r f       

s 

a 
r  

s   

a  

a 
r f       

s   

figure     creating an mdp with resources for an arbitrary non decreasing utility function 
the case shown has three binary resources  all transitions are deterministic 

proof  this statement can be shown via a straightforward construction of an mdp that
has an exponential number  one per resource bundle  of states or actions  below we present
a reduction with a linear number of actions and an exponential number of states  our choice
is due to the fact that  although the reverse mapping requiring two states and exponentially
many actions is even more straightforward  such an mdp feels somewhat unnatural 
given an arbitrary non decreasing utility function f   a corresponding mdp can be
constructed as follows  illustrated in figure    for n     and m       the state space s of
the mdp consists of  m   n    states  one state  sz   for every resource bundle z      m n  
plus a sink state  s    
s
the action space of the mdp a   a   aij    i      n   j      m  consists of mn    
actions  m actions per each resource oi   i      n   plus an additional action a   
the transition function p is deterministic and is defined as follows  action a  is applicable in every state and leads to the sink state s    every other action aij is applicable in
states sz   where zi    j     and leads with certainty to the states where zi   j 

 

  a   aij   s   sz      sz    zi    j      zi   j 
p  s  a      a   a       s   


  otherwise 
in other words  aij only applies in states that have j    units of resource i and leads to the
state where the amount of ith resource increases to j 
the reward function r is defined as follows  there are no rewards in state s    and
action a  is the only action that produces rewards in other states 
 
f    z  a   ao   s   sz   z      m n
r s  a   
 
otherwise 
where f   is a simple transformation of f that compensates for the effects of discounting 
f    z    f  z   
   

p

i zi

 

fidolgov   durfee

p
in other words  it takes i zi transitions to get to state sz   so the contribution of the above
into the total discounted reward will be exactly f  z  
the resource requirements  of actions are as follows  action a  does not require any
resources  while every other action aij requires j units of resource oi  
finally  the initial conditions are  sz          meaning that the agent always starts
in the state that corresponds to the empty resource bundle  state s    in figure      the
capacity costs  and limits 
b are not used  so we set c    
it is easy to see that in the mdp constructed above  given a resource bundle z  any
policy from the feasible set  z  has zero probability of reaching any state sz  for which
z    z  for any component i   furthermore  an optimal policy from the set  z  will be to
transition to state sz  since f  z  is non decreasing  and then use action a    thus obtaining
a total discounted reward of f  z  

a   proof of theorem  
theorem   given an mdp m   hs  a  p  r  o    c    
b  i with resource and capacity
constraints  if there exists a policy   hr that is a feasible solution for m   there exists a
stationary deterministic policy  sd  sd that is also feasible  and the expected total reward
of  sd is no less than that of  
   hr     sd  sd   u   sd      u     
proof  let us label a   a the set of all actions that have a non zero probability of being
executed according to   i e  
a     a s    s  a      
let us also construct an unconstrained mdp  m     hs  a    p    r  i  where p  and r  are the
restricted versions of p and r with the action domain limited to a   
p    s  a   s         
r    s  a    r
p    s  a    p  s  a   r   s  a    r s  a  s  s    s  a  a 
due to a well known property of unconstrained infinite horizon mdps with the total
expected discounted reward optimization criterion  m   is guaranteed to have an optimal
stationary deterministic solution  e g   theorem         puterman         which we label
 sd  
consider  sd as a potential solution to m   clearly   sd is a feasible solution  because
its actions come from the set a  that includes actions that  uses with non zero probability 
which means that the resource requirements  as in      of  sd can be no greater than those
of   indeed 
n
n
x
x
o
o
max   a  o h
 s  a   
    
 sd  s  a   max   a  o    max  a  o h
aa

s

aa

aa

s

where the first inequality is due to the fact that h z     z  and the second equality
follows from the definition of a   
   

firesource allocation among agents with mdp induced preferences

s 

a  
r v u   
o a  o    
o   c z  

a  
v u   
a
 o
   
o    
o   c z  

a  

r 

s 

a  
r   
a      
o o

r 
o

   

s 

a  
r   
a      
o o

sm

m
  mv u   
m

am om   
om  c zm 
a  
r   
a      
o o

sm  

a  
r   
  
o

figure     reduction of knapsack to m oper cmdp  all transitions are deterministic 

furthermore  observe that  sd yields the same total reward under m   and m   additionally  since  sd is a uniformly optimal solution to m     it is  in particular  optimal for
the initial conditions  of the constrained mdp m   therefore   sd constitutes a feasible
solution to m whose expected reward is greater than or equal to the expected reward of
any feasible policy  

a   proof of theorem  
theorem   the following decision problem is np complete  given an instance of an mdp
hs  a  p  r  o    c    
b  i with resources and capacity constraints  and a rational number y  
does there exist a feasible policy   whose expected total reward  given   is no less than y  
proof  as shown in theorem    there always exists an optimal policy for     that is
stationary and deterministic  therefore  the presence in np is obvious  since we can  in
polynomial time  guess a stationary deterministic policy  verify that it satisfies the resource
constraints  and calculate its expected total reward  the latter can be done by solving the
standard system of linear markov equations     on the values of all states  
to show np completeness of the problem  we use a reduction from knapsack  garey
  johnson         recall that knapsack in an np complete problem  which asks
whether  for a given set of items z  z  each of which has a cost c z  and a value v z  
there exists a subset z    z such that the total value of all items in z   is no less than
some
c  i e  
p constant vb  and
pthe total cost of the items is no greater than another constant b
c z 

b
c
and
v z 

v
b
 
our
reduction
is
illustrated
in
figure
  
and
proceeds
 
 
zz
zz
as follows 
given an instance of knapsack with  z    m  let us number all items as zi   i 
    m  as a notational convenience  for such an instance of knapsack  we create an
mdp with m     states  s    s          sm      m     actions  a          am    m types of resources
o    o          om    and a single capacity c    c    
the transition function on these states is defined as follows  every state si   i      m 
has two transitions from it  corresponding to actions ai and a    both actions lead to state
si   with probability    state sm   is absorbing and all transitions from it lead back to
itself 
the reward and the cost functions are defined as follows  we want action ai   i      m 
 which corresponds to item zi in knapsack  to contribute v zi   to the total discounted
   

fidolgov   durfee

reward  hence  we set the immediate reward for every action ai to v zi     i   which  given
that our transition function implies that state si is reached exactly at step i     ensures
that if action ai is ever executed  its contribution to the total discounted reward will be
v zi     i   i    v zi    action a  produces a reward of zero in all states 
the resource requirements of actions are defined as follows  action ai   i      m  only
needs resource oi   i e    ai   oj        i   j  we set the cost of resource oi to be the
cost c zi   of item i in the knapsack problem  the null action a  requires no resources 
in order to complete the construction  we set the initial distribution                  so that
the agent starts in state s  with probability    we also define the decision parameter y   vb
and the upper bound on the single capacity 
b b
c 
essentially  this construction allows the agent to choose action ai or a  at every state si  
choosing action ai is equivalent to putting item zi into the knapsack  while action a 
corresponds to the choice of not including zi in the knapsack  therefore  there exists a
policy that has the expected payoff no less than y   vb and uses no more than 
b   b
c
of the shared resource if and only if there exists a solution to the original instance of
knapsack 


appendix b  non binary resource requirements
below we describe an milp formulation of the capacity constrained single agent optimization problem     for arbitrary resource costs    a  o   r  as opposed to binary costs
that were assumed in the main parts of the paper  the corresponding multiagent winnerdetermination problem  the non binary equivalent of       follows immediately from the
single agent milp 
for arbitrary resource costs  we obtain the following non binary equivalent of the optimization problem      in the occupation measure coordinates 
max
x

xx
s

x s  a r s  a 

a

subject to 
x
xx
x   a   
x s  a p  s  a       
a

x

s

  s 

a

n
x
o
 o  c  max  a  o h
x s  a   
b c  
a

o

c  c 

s

x s  a     

s  s  a  a 

to linearize the sum of max operators in       let us observe that the inequality
n
x
i

g ui   max f  z  ui     g u    max f  z  u              g un   max f  z  un    a
zz

zz

zz

is equivalent to the following system of  z n linear inequalities 
g u   f  z    u      g u   f  z    u              g un  f  zn   un    a 
   

z    z          zn  z 

    

firesource allocation among agents with mdp induced preferences

applying this to the constraints from       we can express the original system of  c  nonlinear
constraints  each of which has a max  
x

n
x
o
 o  c  max  a  o h
x s  a   
b c  
a

o

c  c

s

as the following system of  c  a  o  constraints where the max is removed 
x

 o  c  ao   o h

x

o


x s  a   
b c  

c  c  ao    ao           a 

    

s

notice that this way of eliminating the maximization exponentially increases the number
of constraints  because the above expansion enumerates all possible actions for each resource
 i e   it enumerates policies where each resource o is used by action a    where it is used by
action a    action a    etc   however  in many problems not all resources are used by all
actions  in such cases  most of the above constraints
q become redundant  and the number of
constraints can be reduced from  c  a  o  to  c  o  ao    where ao is the number of actions
that use resource o 
we can linearize the heaviside function analogously to the case of binary resource costs
in section      we create a binary indicator variable that corresponds to the argument of
h   and tie it to the occupation measure x via linear inequalities  the only difference is that
for non binary resource costs  instead of using
p indicators on resources  we use indicators on
actions   a           where  a    h  s x s  a   is an indicator that shows whether
action a is used in the policy  using  and expanding the max as above  we can represent
the optimization problem     as the following milp 
max
x 

xx
s

x s  a r s  a 

a

subject to 
x
xx
x   a   
x s  a p  s  a       
a

x

s

  s 

a

 o  c  ao   o  ao    
b c  

c  c  ao    ao           a 

    

o

x

x s  a  x   a  

a  a 

s

x s  a     

s  s  a  a 

 a          

a  a 

p
where x  max s x s  a  is some constant finite upper bound on the expected number
of times action a is used 
p which exists for any discounted mdp  we can  for example  let
x            since s a x s  a           for any x that is a valid occupation measure
for an mdp with discount factor  
example   let us formulate the milp for the constrained problem from example    recall
that there are three resources o    ot   of   om    truck  forklift  and mechanic   one capacity
   

fidolgov   durfee

type c    c     money   and actions have the following resource requirements  listing only
the nonzero ones  
 a    ot         a    ot         a    of         a    ot         a    ot         a    om      
the resources have the following capacity costs 
 ot   c          of   c          om   c        
and the agent has a limited budget  i e   a capacity bound 
b c        
to compute the optimal policy for an arbitrary   we can formulate the problem as an
milp using the techniques described above  using binary variables   ai       i    
                    we can express the constraint on capacity cost as the following system
of  c  a  o               linear constraints 
                               
                               
                               
                               
                               
   
                               
it is easy to see that most of these constraints are redundant  and the fact that each action
only requires a small subset of the resources allows us to prune many of the constraints  in
fact  the only resource that is used byqmultiple actions is ot   therefore  in accordance with
our earlier discussion  we only need o  ao                 constraints 
                               
                               
                               
                               
where each of the four constraints corresponds to a case where the first resource  ot   is used
by a different action 
as mentioned earlier  we can set x          for the constraints that synchronize the
occupation measure x and the binary indicators   combining this with other constraints
q
from       we get an milp with    continuous and   binary variables  and  s   c  o  ao   
 a                   constraints  not counting the last two sets of range constraints  

finally  let us observe that by expanding the resource and action sets  any problem
can be represented using binary resources only  if the domain contains mostly binary
requirements  it may be more effective to expand the non binary resource requirements 
by augmenting the resource set o  and then use the binary formulation of section     rather
than directly applying the more general formulation described above 
    we do not create a   for the noop action a    as its resource costs are zero  and it drops out of all
expressions 

   

firesource allocation among agents with mdp induced preferences

appendix c  experimental setup
this appendix details how our experimental domains were constructed  for a delivery
domain with  m  agents operating on an n by n grid and sharing  o  resource types  we
used the following parameters 
the resources enable agents to carry out delivery tasks  for a problem with  o  resource
types  there are  o  delivery actions  and performing action i       o   requires a random
subset of resources from o  where the number of resources required by an action is an
important parameter  whose effect on complexity is discussed in section     the probability
that task i       o   can be carried at a location is          o i    o     i e   uniformly
distributed between     and      as a function of the action id  actions with lower ids are
more rewarding  per the definition of the reward function below  but can be executed at
fewer locations  
there are n     possible delivery locations randomly placed on the grid  each delivery
location is assigned a set of delivery tasks that can be executed there  a single location can
be used for multiple delivery tasks  and a single task can be carried out at any of several
locations   the assignment of tasks to locations is done randomly 
each agent has      o  actions  drive in any of the four perpendicular directions and
execute one of the delivery tasks  the drive actions result in movement in the intended
direction with probability of     and with probability of     produce no change of location 
all movement actions incur a negative reward  the amount of which depends on the size of
the agent  for a problem with  m  agents  the movement penalty incurred by agent m 
     m   is      m       m       i e   distributed uniformly on         as a function
of the agents id 
execution of an action corresponding to a delivery task i       o   in a location to which
the task is assigned produces a reward of    i  o  and moves the agent to a new random
location on the grid  the new location is chosen randomly at problem generation  thus
known to agent   but the transition is deterministic  which induces a topology with nearby
and remote locations  attempting execution of a delivery task in an incorrect location does
not change state and produces zero reward 
the agents bid for delivery resources of  o  types  there are cglob  m  units of each
resource  where cglob is the global constraint level  set to     for most of our experiments 
as described in more detail in section     there is one capacity type  size  the size
requirements for making deliveries of type i       o   is i  the capacity limit of agent m
is cloc    o   o        where cloc is the local constraint level  set to     for most of our
experiments  as was described in more detail in section    
the initial location of each agent is randomly selected from a uniform distribution  the
discount factor is         

references
altman  e          constrained markov decision processes with total cost criteria  occupation measures and primal lp  methods and models in operations research         
     
   

fidolgov   durfee

altman  e     shwartz  a          adaptive control of constrained markov chains  criteria and policies  annals of operations research  special issue on markov decision
processes             
altman  e          constrained markov decision processes  chapman and hall crc 
bellman  r          adaptive control processes  a guided tour  princeton university
press 
benazera  m  e   brafman  r  i   meuleau  n     hansen  e          planning with continuous resources in stochastic domains  in proceedings of the nineteenth international
joint conference on artificial intelligence  ijcai      pp           
bererton  c   gordon  g     thrun  s          auction mechanism design for multi robot
coordination  in thrun  s   saul  l     scholkopf  b   eds    proceedings of conference
on neural information processing systems  nips   mit press 
bertsimas  d     tsitsiklis  j  n          introduction to linear optimization  athena
scientific 
boutilier  c          solving concisely expressed combinatorial auction problems  in proceedings of the eighteenth national conference on artificial intelligence  aaai     
pp         
boutilier  c   dearden  r     goldszmidt  m          exploiting structure in policy construction  in proceedings of the fourteenth international joint conference on artificial
intelligence  ijcai      pp           
boutilier  c     hoos  h  h          bidding languages for combinatorial auctions  in proceedings of the seventeenth international joint conference on artificial intelligence
 ijcai      pp           
clarke  e  h          multipart pricing of public goods  public choice           
de vries  s     vohra  r  v          combinatorial auctions  a survey  informs journal
on computing                 
dolgov  d          integrated resource allocation and planning in stochastic multiagent
environments  ph d  thesis  computer science department  university of michigan 
dolgov  d  a     durfee  e  h          optimal resource allocation and policy formulation in loosely coupled markov decision processes  in proceedings of the fourteenth
international conference on automated planning and scheduling  icaps      pp 
       
dolgov  d  a     durfee  e  h          resource allocation among agents with preferences
induced by factored mdps  in proceedings of the fifth international joint conference
on autonomous agents and multiagent systems  aamas      hakodate  japan 
eckstein  j   phillips  c     hart  w          pico  an object oriented framework for parallel
branch and bound  in proceedings of the workshop on inherently parallel algorithms
in optimization and feasibility and their applications 
feigenbaum  j     shenker  s          distributed algorithmic mechanism design  recent
results and future directions  in proceedings of the sixths international workshop
   

firesource allocation among agents with mdp induced preferences

on discrete algorithms and methods for mobile computing and communications  pp 
     acm press  new york 
feldmann  r   gairing  m   lucking  t   monien  b     rode  m          selfish routing in
non cooperative networks  a survey  in proceedings of the twenty eights international
symposium on mathematical foundations of computer science  mfcs      pp    
    springer verlag 
ferguson  d   nikolaou  c   sairamesh  j     yemini  y          economic models for allocating resources in computer systems  in clearwater  s   ed    market based control 
a paradigm for distributed resource allocation  pp          hong kong  world
scientific 
garey  m  r     johnson  d  s          computers and intractability  a guide to the theory
of np completeness  w  h  freeman   co 
groves  t          incentives in teams  econometrica                 
guestrin  c          planning under uncertainty in complex structured environments 
ph d  thesis  computer science department  stanford university 
heyman  d  p     sobel  m  j          volume ii  stochastic models in operations research 
mcgraw hill  new york 
kallenberg  l          linear programming and finite markovian control problems  math 
centrum  amsterdam 
littman  m  l   dean  t  l     kaelbling  l  p          on the complexity of solving markov
decision problems  in proceedings of the eleventh annual conference on uncertainty
in artificial intelligence  uai     pp          montreal 
mackie mason  j  k     varian  h          generalized vickrey auctions  tech  rep  
university of michigan 
mas colell  a   whinston  m  d     green  j  r          microeconomic theory  oxford
university press  new york 
mcafee  r  p     mcmillan  j          analyzing the airwaves auction  journal of economic
perspectives                
mcmillan  j          selling spectrum rights  journal of economic perspectives        
      
meuleau  n   hauskrecht  m   kim  k  e   peshkin  l   kaelbling  l   dean  t     boutilier 
c          solving very large weakly coupled markov decision processes  in proceedings
of the fifteenth national conference on artificial intelligence  aaai      pp     
    
nisan  n          bidding and allocation in combinatorial auctions  in electronic commerce 
parkes  d          iterative combinatorial auctions  achieving economic and computational efficiency  ph d  thesis  department of computer and information science 
university of pennsylvania 
parkes  d  c   kalagnanam  j  r     eso  m          achieving budget balance with
vickrey based payment schemes in exchanges  in proc    th international joint conference on artificial intelligence  ijcai      pp           
   

fidolgov   durfee

parkes  d  c     shneidman  j          distributed implementations of vickrey clarkegroves mechanisms  in proceedings of the third international joint conference on
autonomous agents and multi agent systems  aamas      pp         
parkes  d  c     singh  s          an mdp based approach to online mechanism design  in
proceedings of the seventeenths annual conference on neural information processing
systems  nips     
parkes  d  c   singh  s     yanovsky  d          approximately efficient online mechanism
design  in proceedings of the eighteenths annual conference on neural information
processing systems  nips     
puterman  m  l          markov decision processes  john wiley   sons  new york 
ross  k     chen  b          optimal scheduling of interactive and non interactive traffic in
telecommunication systems  ieee transactions on automatic control             
ross  k     varadarajan  r          markov decision processes with sample path constraints  the communicating case  operations research             
rothkopf  m  h   pekec  a     harstad  r  m          computationally manageable combinational auctions  management science                   
sandholm  t     boutilier  c          preference elicitation in combinatorial auctions  in
cramton  shoham    steinberg  eds    combinatorial auctions  chap      mit press 
sandholm  t          an algorithm for optimal winner determination in combinatorial
auctions  in proceedings of the sixteenth international joint conference on artificial
intelligence  ijcai      pp          san francisco  ca  usa  morgan kaufmann
publishers inc 
sandholm  t          algorithm for optimal winner determination in combinatorial auctions  artificial intelligence                 
shapley  l  s          stochastic games  proceedings of national academy of science  usa 
             
sheffi  y          combinatorial auctions in the procurement of transportation services 
interfaces                 
singh  s     cohn  d          how to dynamically merge markov decision processes  in
jordan  m  i   kearns  m  j     solla  s  a   eds    advances in neural information
processing systems  vol      pp            the mit press 
song  j     regan  a          combinatorial auctions for transportation service procurement  the carrier perspective  transportation research record             
vickrey  w          counterspeculation  auctions and competitive sealed tenders  journal
of finance          
wellman  m  p   walsh  w  e   wurman  p  r     mackie mason  j  k          auction
protocols for decentralized scheduling  games and economic behavior             
wolsey  l          integer programming  john wiley   sons 
   

firesource allocation among agents with mdp induced preferences

wu  j     durfee  e  h          automated resource driven mission phasing techniques for
constrained agents  in proceedings of the fourth international joint conference on
autonomous agents and multiagent systems  aamas      pp         

   

fi