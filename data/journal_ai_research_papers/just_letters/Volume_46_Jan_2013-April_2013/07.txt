journal of artificial intelligence research                 

submitted       published      

incremental clustering and expansion for faster
optimal planning in decentralized pomdps
frans a  oliehoek

frans oliehoek maastrichtuniversity nl

maastricht university
maastricht  the netherlands

matthijs t j  spaan

m t j spaan tudelft nl

delft university of technology
delft  the netherlands

christopher amato

camato csail mit edu

massachusetts institute of technology
cambridge  ma  usa

shimon whiteson

s a whiteson uva nl

university of amsterdam
amsterdam  the netherlands

abstract
this article presents the state of the art in optimal solution methods for decentralized
partially observable markov decision processes  dec pomdps   which are general models for
collaborative multiagent planning under uncertainty  building off the generalized multiagent a    gmaa   algorithm  which reduces the problem to a tree of one shot collaborative
bayesian games  cbgs   we describe several advances that greatly expand the range of decpomdps that can be solved optimally  first  we introduce lossless incremental clustering
of the cbgs solved by gmaa   which achieves exponential speedups without sacrificing
optimality  second  we introduce incremental expansion of nodes in the gmaa  search
tree  which avoids the need to expand all children  the number of which is in the worst case
doubly exponential in the nodes depth  this is particularly beneficial when little clustering
is possible  in addition  we introduce new hybrid heuristic representations that are more
compact and thereby enable the solution of larger dec pomdps  we provide theoretical
guarantees that  when a suitable heuristic is used  both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent  finally 
we present extensive empirical results demonstrating that gmaa  ice  an algorithm that
synthesizes these advances  can optimally solve dec pomdps of unprecedented size 

   introduction
a key goal of artificial intelligence is the development of intelligent agents that interact with
their environment in order to solve problems  achieve goals  and maximize utility  while such
agents sometimes act alone  researchers are increasingly interested in collaborative multiagent
systems  in which teams of agents work together to perform all manner of tasks  multiagent
systems are appealing  not only because they can tackle inherently distributed problems  but
because they facilitate the decomposition of problems too complex to be tackled by a single
c
    
ai access foundation  all rights reserved 

fioliehoek  spaan  amato    whiteson

agent  huhns        sycara        panait   luke        vlassis        busoniu  babuska   
de schutter        
one of the primary challenges of multiagent systems is the presence of uncertainty  even
in single agent systems  the outcome of an action may be uncertain  e g   the action may fail
with some probability  furthermore  in many problems the state of the environment may be
uncertain due to limited or noisy sensors  however  in multiagent settings these problems
are often greatly exacerbated  since agents have access only to their own sensors  typically a
small fraction of those of the complete system  their ability to predict how other agents will
act is limited  complicating cooperation  if such uncertainties are not properly addressed 
arbitrarily bad performance may result 
in principle  agents can use communication to synchronize their beliefs and coordinate
their actions  however  due to bandwidth constraints  it is typically infeasible for all agents
to broadcast the necessary information to all other agents  in addition  in many realistic
scenarios  communication may be unreliable  precluding the possibility of eliminating all uncertainty about other agents actions 
especially in recent years  much research has focused on approaches to  collaborative 
multiagent systems that deal with uncertainty in a principled way  yielding a wide variety
of models and solution methods  pynadath   tambe        goldman   zilberstein       
seuken   zilberstein         this article focuses on the decentralized partially observable
markov decision process  dec pomdp   a general model for collaborative multiagent planning under uncertainty  unfortunately  solving a dec pomdp  i e   computing an optimal
plan  is generally intractable  nexp complete   bernstein  givan  immerman    zilberstein 
      and in fact even computing solutions with absolutely bounded error  i e    approximate
solutions  is also nexp complete  rabinovich  goldman    rosenschein         in particular 
the number of joint policies grows exponentially with the number of agents and observations
and doubly exponentially with respect to the horizon of the problem   though these complexity results preclude methods that are efficient on all problems  developing better optimal
solution methods for dec pomdps is nonetheless an important goal  for several reasons 
first  since the complexity results describe only the worst case  there is still great potential
to improve the performance of optimal methods in practice  in fact  there is evidence that
many problems can be solved much faster than the worst case complexity bound indicates
 allen   zilberstein         in this article  we present experiments that clearly demonstrate
this point  on many problems  the methods we propose scale vastly beyond what would be
expected for a doubly exponential dependence on the horizon 
second  as computer speed and memory capacity increase  a growing set of small and
medium sized problems can be solved optimally  some of these problems arise naturally while
others result from the decomposition of larger problems  for instance  it may be possible
to extrapolate optimal solutions of problems with shorter planning horizons  using them as
the starting point of policy search for longer horizon problems as in the work of eker and
akn         or to use such shorter horizon  no communication solutions inside problems with
communication  nair  roth    yohoo        goldman   zilberstein         more generally 
optimal policies of smaller problems can potentially be used to find good solutions for larger
problems  for instance  transfer planning  oliehoek        oliehoek  whiteson    spaan 
   surprisingly  the number of states in a dec pomdp is less important  e g   brute force search depends on
the number of states only via its policy evaluation routine  which scales linearly in the number of states 

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

      employs optimal solutions to problems with few agents to better solve problems with
many agents  by performing  approximate  influence based abstraction and influence search
 witwicki        oliehoek  witwicki    kaelbling         optimal solutions of component
problems can potentially be used to find  near  optimal solutions of larger problems 
third  optimal methods offer important insights into the nature of specific dec pomdp
problems and their solutions  for instance  the methods introduced in this article enabled the
discovery of certain properties of the broadcastchannel benchmark problem that make
it much easier to solve 
fourth  optimal methods provide critical inspiration for principled approximation methods  in fact  almost all successful approximate dec pomdp methods are based on optimal
ones  see  e g   seuken   zilberstein      b      a  dibangoye  mouaddib    chai draa       
amato  dibangoye    zilberstein        wu  zilberstein    chen      a  oliehoek        or
locally optimal ones  velagapudi  varakantham  scerri    sycara           and the clustering technique presented in this article forms the basis of a recently introduced approximate
clustering technique  wu  zilberstein    chen        
finally  optimal methods are essential for benchmarking approximate methods  in recent
years  there have been huge advances in the approximate solution of dec pomdps  leading
to the development of solution methods that can deal with large horizons  hundreds of agents
and many states  e g   seuken   zilberstein      b  amato et al         wu et al       a 
oliehoek        velagapudi et al         
however  since computing even  approximate
solutions is nexp complete  any method whose complexity is not doubly exponential cannot
have any guarantees on the absolute error of the solution  assuming exp  nexp   as such 
existing effective approximate methods have no quality guarantees  
consequently  it is difficult to meaningfully interpret their empirical performance without
the upper bounds optimal methods supply  while approximate methods can also be benchmarked against lower bounds  e g   other approximate methods   such comparisons cannot
detect when a method fails to find good solutions  doing so requires benchmarking against
upper bounds and  unfortunately  upper bounds that are easier to compute  such as qmdp
and qpomdp  are too loose to be helpful  oliehoek  spaan    vlassis         as such 
benchmarking with respect to optimal solutions is an important part of the verification of any
approximate algorithm  since existing optimal methods can only tackle very small problems 
scaling optimal solutions to larger problems is a critical goal 
    contributions
this article presents the state of the art in optimal solution methods for dec pomdps  in
particular  it describes several advances that greatly expand the horizon to which many decpomdps can be solved optimally  in addition  it proposes and evaluates a complete algorithm
that synthesizes these advances  our approach is based on the generalized multiagent a 
 gmaa   algorithm  oliehoek  spaan    vlassis         which makes it possible to reduce
the problem to a tree of one shot collaborative bayesian games  cbgs   the appeal of this
   the method by velagapudi et al         repeatedly computes best responses in a way similar to dp jesp
 nair  tambe  yokoo  pynadath    marsella         the best response computation  however  exploits
sparsity of interactions 
   note that we refer to methods without quality guarantees as approximate rather than heuristic to avoid
confusion with heuristic search  which is used throughout this article and is exact 

   

fioliehoek  spaan  amato    whiteson

approach is the abstraction layer it introduces  which has led to various insights into decpomdps and  in turn  to the improved solution methods we describe 
the specific contributions of this article are  
   we introduce lossless clustering of cbgs  a technique to reduce the size of the cbgs
for which gmaa  enumerates all possible solutions  while preserving optimality  this
can exponentially reduce the number of child nodes in the gmaa  search tree  leading
to huge increases in efficiency  in addition  by applying incremental clustering  ic  to
gmaa   our gmaa  ic method can avoid clustering exponentially sized cbgs 
   we introduce incremental expansion  ie  of nodes in the gmaa  search tree  although
clustering may reduce the number of children of a search node  this number is in the
worst case still doubly exponential in the nodes depth  gmaa  ice  which applies
ie to gmaa  ic  addresses this problem by creating a next child node only when it
is a candidate for further expansion 
   we provide theoretical guarantees for both gmaa  ic and gmaa  ice  in particular  we show that  when using a suitable heuristic  both algorithms are both complete
and search equivalent 
   we introduce an improved heuristic representation  tight heuristics like those based
on the underlying pomdp solution  qpomdp   or the value function resulting from
assuming   step delayed communication  qbg   are essential for heuristic search methods like gmaa   oliehoek  spaan    vlassis         however  the space needed to
store these heuristics grows exponentially with the problem horizon  we introduce hybrid representations that are more compact and thereby enable the solution of larger
problems 
   we present extensive empirical results that show substantial improvements over the
current state of the art  whereas seuken and zilberstein        argued that gmaa 
can at best optimally solve dec pomdps only one horizon further than brute force
search  our results demonstrate that gmaa  ice can do much better  in addition  we
provide a comparative overview of the results of competitive optimal solution methods
from the literature 
the primary aim of the techniques introduced in this article is to improve scalability
with respect to the horizon  our empirical results confirm that these techniques are highly
successful in this regard  as an added bonus  our experiments also demonstrate improvement
in scalability with respect to the number of agents  in particular  we present the first optimal
results on general  non special case  dec pomdps with more than three agents  extensions
of our techniques to achieve further improvements with respect to the number of agents 
as well as promising ways to combine the ideas behind our methods with state of the art
approximate approaches  are discussed under future work in section   
   this article synthesizes and extends research that was already reported in two conference papers  oliehoek 
whiteson    spaan        spaan  oliehoek    amato        

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

    organization
the article is organized as follows  section   provides background on the dec pomdp model 
the gmaa  heuristic search solution method  as well as suitable heuristics  in section    we
introduce lossless clustering of the cbgs and its integration into gmaa   section   introduces the incremental expansion of search nodes  the empirical evaluation of the proposed
techniques is reported in section    we give a treatment of related work in section    future
work is discussed in section   and conclusions are drawn in section   

   background
in a dec pomdp  multiple agents must collaborate to maximize the sum of the common
rewards they receive over multiple timesteps  their actions affect not only their immediate
rewards but also the state to which they transition  while the current state is not known to
the agents  at each timestep each agent receives a private observation correlated with that
state 


ff
definition    a dec pomdp is a tuple d  s  a  t  o  o  r  b    h   where
 d              n  is the finite set of agents 
 

 s   s           s s  is the finite set of states 

 a   i ai is the set of joint actions a   ha            an i  where ai is the finite set of actions
available to agent i 
 t is a transition function specifying the state transition probabilities pr s  s a  
 o   i oi is the finite set of joint observations  at every stage one joint observation
o   ho       on i is received  each agent i observes only its own component oi  
 o is the observation function  which specifies observation probabilities pr o a s   
 r s a  is the immediate reward function mapping  s a  pairs to real numbers 
 b    s  is the initial state distribution at time t      where  s  denotes the infinite
set of probability distributions over the finite set s 
 h is the horizon  i e   the number of stages  we consider the case where h is finite 
at each stage t           h     each agent takes an individual action and receives an individual
observation 
example    recycling robots   to illustrate the dec pomdp model  consider a team of robots tasked
with removing trash from an office building  depicted in fig     the robots have sensors to find marked
trash cans  motors to move around in order to look for cans  as well as gripper arms to grasp and carry
a can  small trash cans are light and compact enough for a single robot to carry  but large trash cans
require multiple robots to carry them out together  because more people use them  the larger trash
cans fill up more quickly  each robot must also ensure that its battery remains charged by moving
to a charging station before it expires  the battery level for a robot degrades due to the distance the
robot travels and the weight of the item being carried  each robot knows its own battery level but not
that of the other robots and only the location of other robots within sensor range  the goal of this
problem is to remove as much trash as possible in a given time period 
this problem can be represented as a dec pomdp in a natural way  the states  s  consist of
the different locations of each robot  their battery levels and the different amounts of trash in the
cans  the actions  ai   for each robot consist of movements in different directions as well as decisions
   

fioliehoek  spaan  amato    whiteson

figure    illustration of the recycling robots example  in which two robots have to remove
trash in an office environment with three small  blue  trash cans and two large  yellow  ones 
in this situation  the left robot might observe that the large trash can next to it is full  and
the other robot that the small trash can is empty  however  none of them is sure of the trash
cans state due to limited sensing capabilities  nor do they see the state of trash cans further
away  in particular  one robot has no knowledge regarding the observations of the other robot 
to pick up a trash can or recharge the battery  when in range of a can or a charging station   the
observations  oi   of each robot consist of its own battery level  its own location  the locations of other
robots in sensor range and the amount of trash in cans within range  the rewards  r  could consist of
a large positive value for a pair of robots emptying a large  full  trash can  a small positive value for
a single robot emptying a small trash can and negative values for a robot depleting its battery or a
trash can overflowing  an optimal solution is a joint policy that leads to the expected behavior  given
that the rewards are properly specified   that is  it ensures that the robots cooperate to empty the
large trash cans when appropriate and the small ones individually while considering battery usage 

for explanatory purposes  we also consider a much simpler problem  the so called decentralized tiger problem  nair et al         
example    dec tiger   the dec tiger problem concerns two agents that find themselves in a hallway with two doors  behind one door  there is a treasure and behind the other is a tiger  the state
describes which door the tiger is behindleft  sl   or right  sr  each occurring with     probability
 i e   the initial state distribution b  is uniform   each agent can perform three actions  open the left
door  aol    open the right door  aor   or listen  ali    clearly  opening the door to the treasure will
yield a reward  but opening the door to the tiger will result in a severe penalty  a greater reward
is given for both agents opening the correct door at the same time  as such  a good strategy will
probably involve listening first  the listen actions  however  also have a minor cost  negative reward  
at every stage the agents get an observation  the agents can either hear the tiger behind the left
 ohl   or right  ohr   door  but each agent has a     chance of hearing it incorrectly  getting the wrong
observation   moreover  the observation is informative only if both agents listen  if either agent opens
a door  both agents receive an uninformative  uniformly drawn  observation and the problem resets to
sl or sr with equal probability  at this point the problem just continues  such that the agents may be
able to open the door to the treasure multiple times  also note that  since the only two observations
the agents can get are ohl   ohr   the agents have no way of detecting that the problem has been reset 
if one agent opens the door while the other listens  the other agent will not be able to tell that the
door was opened  for a complete specification  see the discussion by nair et al         

given a dec pomdp  the agents common goal is to maximize the expected cumulative
reward or return  the planning task entails finding a joint policy    h           n i from the
space of joint policies   that specifies an individual policy i for each agent i  such an
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

individual policy in general specifies an individual action for each action observation history
 aoh   it    a i  o i          ait   oti    e g   i   it     ati   however  it is possible to restrict our
attention to deterministic or pure policies  in which case i maps each observation history
  t to an action  e g   i   o t     at   the number of such policies is
 oh   o i          oti      oit  o
i
i
i
h      o    
  o
 
i
 ai   i
and the number of joint policies is therefore

n o  h   
o  a    o     
     
where a and o denote the largest individual action and observation sets  the quality of a
particular joint policy is expressed by the expected cumulative reward it induces  also referred
to as its value 
definition    the value v    of a joint policy  is
v      e

h 
hx
t  

fi
i
fi
r st  at  fi b   

     

where the expectation is over sequences of states  actions and observations 
the planning problem for a dec pomdp is to find an optimal joint policy     i e   a joint
policy that maximizes the value      arg max v    
because an individual policy i depends only on the local information  oi available to an
agent  the on line execution phase is truly decentralized  no communication takes place other
than that modeled via actions and observations  the planning itself however  may take place
in an off line phase and be centralized  this is the scenario that we consider in this article  for
a more detailed introduction to dec pomdps see  e g   the work of seuken and zilberstein
       and oliehoek        
    heuristic search methods
in recent years  numerous dec pomdp solution methods have been proposed  most of these
methods fall into one of two categories  dynamic programming and heuristic search methods 
dynamic programming methods take a backwards or bottom up perspective by first considering policies for the last time step t   h    and using them to construct policies for stage
t   h     etc  in contrast  heuristic search methods take a forward or top down perspective
by first constructing plans for t     and extending them to later stages 
in this article  we focus on the heuristic search approach that has shown state of the art
results  as we make clear in this section  this method can be interpreted as searching over a
tree of collaborative bayesian games  cbgs   these cbgs provide a convenient abstraction
layer that facilitates the explanation of the techniques introduced in this article 
this section provides some concise background on heuristic search methods  for a more
detailed description  see the work of oliehoek  spaan  and vlassis         for a further description of dynamic programming methods and their relationship to heuristic search methods 
see the work of oliehoek        
      multiagent a 
szer  charpillet  and zilberstein        introduced a heuristically guided policy search method
called multiagent a   maa    it performs an a  search over partially specified joint policies 
   

fioliehoek  spaan  amato    whiteson

t  

t  

t  

i 

ali
i   

 i

ohr

ohl
aol

i 

aol

ohl

ohr

ohl

ohr

ali

ali

aol

ali

i 

i   
figure    an arbitrary policy for the dec tiger problem  the figure illustrates the different
types of partial policies used in this paper  the shown past policy  i consists of two decision
rules i    i    also shown are two sub tree policies i      i     introduced in section        
pruning joint policies that are guaranteed to be worse than the best  fully specified  joint policy
found so far  oliehoek  spaan  and vlassis        generalized the algorithm by making explicit
the expand and selection operators performed in the heuristic search  the resulting algorithm 
generalized maa   gmaa   offers a unified perspective of maa  and the forward sweep
policy computation method  emery montemerlo         which differ in how they implement
gmaa s expand operator  forward sweep policy computation solves  i e   finds the best
policy for  collaborative bayesian games  while maa  finds all policies for those collaborative
bayesian games  as we describe in section       
the gmaa  algorithm considers joint policies that are partially specified with respect
to time  these partially specified policies can be formalized as follows 
definition    a decision rule it for agent is decision for stage t is a mapping from action  t  ai  
observation histories for stage t to actions it   
i
in this article  we consider only deterministic policies  since such policies need to condition
their actions only on observation histories  they are made up of decision rules that map length  t  ai   a joint decision rule  t   h t          nt i specifies
t observation histories to actions  it   o
 
i
a decision rule for each agent  fig    illustrates this concept  as well as that of a past policy 
which we introduce shortly  as discussed below  decision rules allow partial policies to be
defined and play a crucial role in gmaa  and the algorithms developed in this article 
definition    a partial or past policy for stage t  ti   specifies the part of agent is policy
that relates to stages t   t  that is  it specifies the decision rules for the first t stages 
ti    i   i           it     a past policy for stage h is just a regular  or fully specified  policy
hi   i   a past joint policy t                     t    specifies joint decision rules for the first t
stages 
gmaa  performs a heuristic search over such partial joint policies t by constructing
a search tree as illustrated in fig   a  each node q   ht   vi in the search tree specifies a
past joint policy t and heuristic value v  this heuristic value v of the node represents an
optimistic estimate of the past joint policy vb  t    which can be computed via
vb  t     v     t   t     h t   h   t   
   

     

fiincremental clustering and expansion for faster optimal planning in dec pomdps

b    

 
 

 

  

  
  

  
 

 
 

 

   

b    

 

 

  

   

b    

b    

 

 a  the maa  perspective 

   

b    

  

   

b    

 b  the cbg perspective 

figure    generalized maa   associated with every node is a heuristic value  the search
trees for the two perspectives shown are equivalent under certain assumptions on the heuristic 
as explained in section     
where h t   h  is a heuristic value for the remaining h  t stages and v     t   t   is the actual
expected reward t achieves over the first t stages  for its definition  see appendix a    
clearly  when h t   h  is an admissible heuristica guaranteed overestimationso is vb  t    
algorithm   illustrates gmaa   it starts by creating a node q   for a completely unspecified joint policy   and placing it in an open list l  then  it selects nodes  algorithm    and
expands them  algorithm     repeating this process until it is certain that it has found the
optimal joint policy 
the select operator returns the highest ranked node  as defined by the following comparison operator 
definition    the node comparison operator   is defined for two nodes q   ht  vi  q   
ht  v  i as follows 



  if v    v 
v   v
q   q    depth q    depth q      otherwise if depth q     depth q   
     

 t
t
  
  otherwise 
that is  the comparison operator first compares the heuristic values  if those are equal 
it compares the depth of the nodes  finally  if nodes have equal value and equal depth  it
lexically compares the past joint policies  this ranking leads to a  behavior  i e   selecting the
node from the open list with the highest heuristic value  of gmaa   as well as guaranteeing
the same selection order in our incremental expansion technique  introduced in section    
ranking nodes with greater depth higher in case of equal heuristic value helps find tight
lower bounds early by first expanding deeper nodes  szer et al         and is also useful in
incremental expansion 
   more formally  h should not underestimate the value  note that  unlike classical a  applications such as
path planningin which an admissible heuristic should not overestimatein our setting we maximize reward 
rather than minimize cost 

   

fioliehoek  spaan  amato    whiteson

algorithm   generalized multiagent a  
input  a dec pomdp  an admissible heuristic h  an empty open list l
output  an optimal joint policy  
   vgm aa  
   q    h        v    i
   l insert q    
   repeat
  
q  select l 
  
qexpand  expand q  h 
  
if depth q    h    then
  
  qexpand contains fully specified joint policies  we only are interested in the best one  
  
h  vi  bestjointpolicyandvalue qexpand  
   
if v   vgm aa then
   
  
 found a new best joint policy 
   
vgm aa  v
   
l prune vgm aa  
  optionally  prune the open list 
   
end if
   
else
 

 add expanded children to open list 
   
l insert  q   qexpand   q   v   vgm aa  
   
end if
   
postprocessnode q  l 
    until l is empty
    return  

algorithm   select l   return the highest ranked node from the open list 
input  open list l  total order on nodes  
output  the highest ranked node q 
   q   q  l s t  q l  q     q   q    q 
   return q 

the expand operator constructs qexpand   the set of all child nodes  that is  given a node
that contains partial joint policy t                     t     it constructs t     the set of all
t                       t    t    by appending all possible joint decision rules  t for the next time
step t  for all these t     a heuristic value is computed and a node is constructed 
after expansion  the algorithm checks  line    if the expansion resulted in fully specified
joint policies  if not  all children with sufficient heuristic value are placed in the open list
algorithm   expand q  h   the expand operator of plain maa  
input  q   ht   vi the search node to expand  h the admissible heuristic 
output  qexpand the set containing all expanded child nodes 
   qexpand    
   t     t     t      t    t   
   for t    t   do
  
vb  t      v     t  t       h t    
  
q   ht     vb  t    i
  
qexpand  insert q   
   end for
   return qexpand

   

 create child node 

fiincremental clustering and expansion for faster optimal planning in dec pomdps

algorithm   postprocessnode q l 
input  q the expanded parent node  l the open list 
output  the expanded node is removed 
   l pop q 

 line      if the children are fully specified  bestjointpolicyandvalue returns only the best
joint policy  and its value  from qexpand  see algorithm    in appendix a   for details of
bestjointpolicyandvalue   gmaa  also maintains a lower bound vgm aa which corresponds to the actual value of the best fully specified joint policy found so far  if the newly
found joint policy has a higher value this lower bound is updated  lines    and      also  any
nodes for partial joint policies t   with an upper bound that is lower than the best solution
so far  vb  t       vgm aa   can be pruned  line      this pruning takes additional time  but
can save memory  finally  postprocessnode simply removes the parent node from the open
list  this procedure is augmented for incremental expansion in section     the search ends
when the list becomes empty  at which point an optimal joint policy has been found 
gmaa  is complete  i e   it will search until it finds a solution  therefore  in theory 
gmaa  is guaranteed to eventually produce an optimal joint policy  szer et al         
however  in practice  this is often infeasible for larger problems  a major source of complexity
is the full expansion of a search node  the number of joint decision rules for stage t that can
form the children of a node at depth t in the search tree  is


t
o  a  n  o      
     
which is doubly exponential in t  comparing       with        we see that the worst case
complexity of expanding a node for the deepest level in the tree t   h    is comparable to
that of brute force search for the entire dec pomdp  consequently  seuken and zilberstein
       conclude that maa  can at best solve problems whose horizon is only   greater than
those that can already be solved by nave brute force search 
      the bayesian game perspective
gmaa  makes it possible to interpret maa  as the solution of a collection of collaborative
bayesian games  cbgs   we employ this approach throughout this article  as it facilitates
the improvements to gmaa  that we introduce  each of which results in significant advances
in the state of the art in dec pomdp solutions 
a bayesian game  bg  models a one shot interaction between a number of agents  it is
an extension of the well known strategic game  also known as a normal form game  in which
each agent holds some private information  osborne   rubinstein         a cbg is a bg
in which the agents receive identical payoffs  in the bayesian game perspective  each node q
in the gmaa  search tree  along with its corresponding partial joint policy t   defines a
cbg  oliehoek  spaan    vlassis         that is  given state distribution b    for each t  
it is possible to construct a cbg b b   t   that represents the decision making problem for
stage t given that t was followed for the first t stages starting from b    when it is clear what
b  is  we simply write b t   
   we follow the convention that the root has depth   

   

fioliehoek  spaan  amato    whiteson

definition    a collaborative bayesian game  cbg  b b   t     hd  a    pr    ui modeling
stage t of a dec pomdp  given initial state distribution b  and past joint policy t   consists
of 
 d  the set of agents          n  
 a  the set of joint actions 
   the set of their joint types  each of which specifies a type for each agent   
h           n i 
 pr    a probability distribution over joint types 
 u  a  heuristic  payoff function mapping joint type and action to a real number  u  a  
in any bayesian game  the type i of an agent i represents the private information it holds 
for instance  in a bayesian game modeling of a job recruitment scenario  the type of an agent
may indicate whether that agent is a hard worker  in a cbg for a dec pomdp  an agents
private information is its individual aoh  therefore  the type i of an agent i corresponds to
 it   its history of actions and observations  i   it   similarly  a joint type corresponds to a
joint aoh      t  
consequently  u should provide a  heuristic  estimate for the long term payoff of each
   t  a  pair  in other words  the payoff function corresponds to a heuristic q value  u  a  
b   t  a   we discuss how to compute such heuristics in section      given t   b    and the
q 
correspondence of joint types and aohs  the probability distribution over joint types is 
pr     pr   t  b   t   

     

where the latter probability is the marginal of pr s   t  b   t   as defined by  a    used in the
computation of the value of a partial joint policy v     t   t   in appendix a    note that due
to the correspondence between types and aohs  the size of a cbg b b   t   for a stage t is
exponential in t 
in a cbg  each agent uses a bayesian game policy i that maps individual types to actions 
i  i     ai   because of the correspondence between types and aohs  a  joint  policy for the
cbg  corresponds to a  joint  decision rule     t   in the remainder of this article  we
assume deterministic past joint policies t   which implies that only one   t will have non zero
probability given the observation history  o t   thus   effectively maps observation histories
to actions  the number of such  for b b   t   is given by        the value of a joint cbg
policy  for a cbg b b   t   is 
x
b   t     t    
pr   t  b   t  q 
     
vb     
 
t

where  t    t     hi   it  ii     n denotes the joint action that results from application of the
individual cbg policies to the individual aoh  it specified by   t  
example    consider a cbg for dec tiger given the past joint policy   that specifies to listen at the first two stages  at stage t      each agent has four possible observation histories 
        ohl  ohl     ohl  ohr     ohr  ohl     ohr  ohr    that correspond directly to its possible types  the
o
i
probabilities of these joint types given   are listed in fig   a  since the joint ohs together with
  determine the joint aohs  they also correspond to so called joint beliefs  probability distributions
over states  introduced formally in section       fig   b shows these joint beliefs  which can serve as
the basis for the heuristic payoff function  as further discussed in section      
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

 o  
 ohl  ohl  
 ohl  ohr  
 ohr  ohl  
 ohr  ohr  

 o  
 ohl  ohl  
     
     
     
     

 ohl  ohr  
     
     
     
     

 ohr  ohl  
     
     
     
     

 ohr  ohr  
     
     
     
     

 a  the joint type probabilities 

 o  
 ohl  ohl  
 ohl  ohr  
 ohr  ohl  
 ohr  ohr  

 o  
 ohl  ohl  
     
     
     
   

 ohl  ohr  
     
   
   
     

 ohr  ohl  
     
   
   
     

 ohr  ohr  
   
     
     
     

 b  the induced joint beliefs  listed is the probability pr sl       b    of
the tiger being behind the left door 

figure    illustration for the dec tiger problem with a past joint policy   that specifies
only listen actions for the first two stages 

algorithm   expand cbg q  h   the expand operator of gmaa  that makes use of cbgs 
input  q   ht   vi the search node to expand 
b   a  
input  h the admissible heuristic that is of the form q 
output  qexpand the set containing all expanded child nodes 
b
   b b   t    constructbg b   t   q 
   qexpand  generateallchildrenforcbg b b   t   
   return qexpand

 as explained in section       

a solution to the cbg is a  that maximizes        a cbg is equivalent to a team
decision process and finding a solution is np complete  tsitsiklis   athans         however 
in the bayesian game perspective of gmaa   illustrated in fig   b  the issue of solving a
cbg  i e   finding the highest payoff   is not so relevant because we need to expand all  
that is  the expand operator enumerates all  and appends them to t to form the set of
extended joint policies

 
t      t        is a joint cbg policy of b b   t  
and uses this set to construct qexpand   the set of child nodes  the heuristic value of such a
child node q  qexpand that specifies t      t     is given by
vb  t       v     t   t     vb    

     

the expand operator that makes use of cbgs is summarized in algorithm    which uses the
generateallchildrenforcbg subroutine  algorithm    in appendix a     fig   b illustrates
the bayesian game perspective of gmaa  
   

fioliehoek  spaan  amato    whiteson

    heuristics
to perform heuristic search  gmaa  defines the heuristic value vb  t   using        in contrast  the bayesian game perspective uses        these two formulations are equivalent when
b faithfully represents the expected immediate reward  oliehoek  spaan    vlasthe heuristic q
sis         the consequence is that gmaa  via cbgs is complete  and thus finds optimal
solutions  as stated by the following theorem 
theorem    when using a heuristic of the form
b   t  a    est  r st  a      t     e  t    vb    t         t   a  
q 


     

where vb    t      q    t          t      is an overestimation of the value of an optimal joint
policy     gmaa  via cbgs is complete 
proof  see appendix 
in this theorem  q    t  a  is the q value  i e   the expected future cumulative reward of
performing a from   t under joint policy   oliehoek  spaan p
  vlassis         the expectation
t
 
of the immediate reward will also be written as r   a    ss r s a  pr s   t  b     it can be
computed using pr s   t  b     a quantity we refer to as the joint belief resulting from   t and
that we also denote as b  the joint belief itself can be computed via repeated application of
bayes rule  kaelbling  littman    cassandra         or as the conditional of  a    
the rest of this subsection reviews several heuristics that have been used for gmaa  
      qmdp
b   a  is to solve the underlying mdp  i e   to
one way to obtain an admissible heuristic q 
assume the joint action is chosen by a single puppeteer agent that can observe the true
state  this approach  known as qmdp  littman  cassandra    kaelbling         uses the
t
mdp value function qt 
m  s  a   which can be computed using standard dynamic programming
t
b  t
techniques  puterman         in order to transform the qt 
m  s  a  values to qm    a  values 
we compute 
x t 
b m    t  a   
      
q
q  s a  pr s   t  b    
m

ss

solving the underlying mdp has time complexity that is linear in h  which makes it 
especially compared to the dec pomdp  easy to compute  in addition  it is only necessary
to store a value for each  s a  pair  for each stage t  however  the bound it provides on the
optimal dec pomdp q  value function is loose  oliehoek   vlassis        
      qpomdp
similar to the underlying mdp  one can define the underlying pomdp of a dec pomdp 
i e   assuming the joint action is chosen by a single agent with access to the joint observation  
   alternatively one can view this pomdp as a multiagent pomdp in which the agents can instantaneously
broadcast their private observations 

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

tree

vector

t  
t  
t  
t  
figure    visual comparison of tree and vector based q representations 

the resulting solution can be used as a heuristic  called qpomdp  szer et al         roth 
simmons    veloso         the optimal qpomdp value function satisfies 

qp  bt   a    r bt  a   

x

p  ot    bt  a  max qp  bt    at     
at  

ot   o

      

p
where bt is the joint belief  r bt  a    ss r s a bt  s  is the immediate reward  and bt   is
the joint belief resulting from bt by action a and joint observation ot     to use qpomdp   for
b p    t  a    qt  b t  a  
each   t   we can directly use the value for the induced joint belief  q
p

there are two approaches to computing qpomdp   one is to construct the belief mdp
tree of all joint beliefs  illustrated in fig     left   starting with b   corresponding to the
empty joint aoh        for each a and o we compute the resulting     and corresponding
  
belief b and continue recursively  given this tree  it is possible to compute values for all the
nodes by standard dynamic programming 
another possibility is to apply vector based pomdp techniques  see fig     right    the
q value function for a stage qtp  b a  can be represented using a set of vectors for each joint
t    kaelbling et al          qt  b a  is then defined as the maximum
action v t    v t          v a 
p
inner product 
qtp  b a    max b  vat  
t v t
va
a

given v h    the vector representation of the last stage  we can compute v h    etc  in order
to limit the growth of the number of vectors  dominated vectors can be pruned 
since qmdp is an upper bound on the pomdp value function  hauskrecht         qpomdp
provides a tighter upper bound to q than qmdp   however  it is also more costly to compute
and store  both the tree based and the vector based approach may need to store a number of
values exponential in h 
   

fioliehoek  spaan  amato    whiteson

      qbg
a third heuristic  called qbg   assumes that each agent in the team has access only to its
individual observation but it can communicate with a   step delay   we define qbg as
x
qb    t  a    r   t  a    max
pr ot      t  a qb    t     ot      
      


ot   o

t  
where    h   ot  
        n  on  i is a tuple of individual policies i   oi  ai for the cbg
t
constructed for    a  like qpomdp   qbg can also be represented using vectors  varaiya  
walrand        hsu   marcus        oliehoek  spaan    vlassis        and the same two
manners of computation  tree and vector based  apply  it yields a tighter heuristic than
qpomdp   but its computation has an additional exponential dependence on the maximum
number of individual observations  oliehoek  spaan    vlassis         which is particularly
troubling for the vector based computation  since it precludes effective application of incremental pruning  a  cassandra  littman    zhang         to overcome this problem  oliehoek
and spaan        introduce novel tree based pruning methods 

   clustering
gmaa  solves dec pomdps by repeatedly constructing cbgs and expanding all the joint
bg policies  for them  however  the number of such  is equal to the number of regular
maa  child nodes given by       and thus grows doubly exponentially with the horizon h 
in this section  we propose a new approach for improving scalability with respect to h by
clustering individual aohs  this reduces the number of  and therefore the number of
constructed child nodes in the gmaa  search tree  
previous research has also investigated such clustering  emery montemerlo  gordon 
schneider  and thrun        propose clustering types based on the profiles of the payoff
functions of the cbgs  however  the resulting method is ad hoc  even given bounds on the
error of clustering two types in a cbg  no guarantees can be made about the quality of the
dec pomdp solution  as the bound is with respect to a heuristic payoff function 
in contrast  we propose to cluster histories based on the probability these histories induce
over histories of the other agents and over states  the critical advantage of this criterion 
which we call probabilistic equivalence  pe   is that the resulting clustering is lossless  the
solution for the clustered cbg can be used to construct the solution for the original cbg and
the values of the two cbgs are identical  thus  the criterion allows for clustering of aohs
in cbgs that represent dec pomdps while preserving optimality   
in section      we describe how histories in dec pomdps can be clustered using the
notions of probabilistic and best response equivalence  this allows histories to be clustered
   the name qbg stems from the fact that such a   step delayed communication scenario can be modeled
as a cbg  note  however  that the cbgs used to compute qbg are of a different form than the b b   t  
discussed in section        in the latter  types correspond to length t  action   observation histories  in the
former  types correspond to length   observation histories 
   while cbgs are not essential for clustering  they provide a convenient level of abstraction that simplifies
exposition of our techniques  moreover  this level of abstraction makes it possible to employ our results
concerning cbgs outside the context of dec pomdps 
    the probabilistic equivalence criterion and lossless clustering were introduced by oliehoek et al         
this article presents a new  simpler proof of the optimality of clustering based on pe 

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

when it is rational to always choose the same action  in section      we describe the application
of these results to gmaa   section     introduces improved heuristic representations that
allow for the computation over longer horizons 
    lossless clustering in dec pomdps
in this section  we discuss lossless clustering based on the notion of probabilistic equivalence 
we show that this clustering is lossless by demonstrating that probabilistic equivalence implies
best response equivalence  which describes the conditions that a rational agent will select the
same action for two of its types  to prove this implication  we show that the best response
depends only on the multiagent belief  i e   the probability distribution over states and policies
of the other agents   which is the same for two probabilistically equivalent histories  relations
to other equivalence notions are discussed in section   
      probabilistic equivalence criterion
we first introduce the probabilistic equivalence criterion  which can be used to decide whether
two individual histories  ia   ib can be clustered without loss in value 
criterion    probabilistic equivalence   two aohs  ia   ib for agent i are probabilistically
equivalent  pe   written p e  ia   ib    when the following holds 
   i s

pr s  
   i   ia     pr s  
   i   ib   

     

these probabilities can be computed as the conditional of pr s   t  b   t    defined by  a    
in subsections             we formally prove that pe is a sufficient criterion to guarantee
that clustering is lossless  in the remainder of section       we discuss some key properties of
the pe criterion in order to build intuition 
note that the criterion can be decomposed into the following two criteria 
   i
   i s

pr  
   i   ia     pr  
   i   ib   

     

pr s  
   i   ia     pr s  
   i   ib   

     

these criteria give a natural interpretation  the first says that the probability distribution
over the other agents aohs must be identical for both  ia and  ib   the second demands that
the resulting joint beliefs are identical 
the above probabilities are not well defined without the initial state distribution b  and
past joint policy t   however  since we consider clustering of histories within a particular cbg
 for some stage t  constructed for a particular b   t   they are implicitly specified  therefore
we drop these arguments  clarifying the notation 
example    in example    the types  ohl  ohr   and  ohr  ohl   of each agent are pe  to see this  note
that the rows  columns for the second agent  for these histories are identical in both fig   a and
fig   b  thus  they specify the same distribution over histories of the other agents  cf  equation       
and the induced joint beliefs are the same  cf  equation        

probabilistic equivalence has a convenient property that our algorithms exploit  if it holds
for a particular pair of histories  then it will also hold for all identical extensions of those
histories  i e   it propagates forwards regardless of the policies of the other agents 
   

fioliehoek  spaan  amato    whiteson

definition    identical extensions   given two aohs  ia t   ib t   their respective extensions
  a t        a t  ai  oi   and   b t        b t  a  o   are called identical extensions if and only if
i

i

i

ai   ai and oi   oi  

i

i

i

lemma    propagation of pe   given  ia t   ib t that are pe  regardless of the decision rule
the other agents use   t  i    identical extensions are also pe 
ati ot   t st    t  
i

  i

  i

t  
t
t     t     b t t t   t
    i  i  ai  oi     i        
pr st     
   i   ia t  ati  ot  
i     i     pr s

proof  the proof is listed in the appendix  but holds intuitively because if the probabilities
described above were the same before  they will also be the same after taking the same action
and seeing the same observation 
note that  while the probabilities defined in       superficially resemble beliefs used in
pomdps  they are substantially different  in a pomdp  the single agent can compute its
individual belief using only its aoh  it can then use this belief to determine the value of
any future policy  as it is a sufficient statistic of the history to predict the future rewards
 kaelbling et al         bertsekas         thus  it is trivial to show equivalence of aohs
that induce the same individual belief in a pomdp  unfortunately  dec pomdps are more
problematic  the next section elaborates on this issue by discussing the relation to multiagent
beliefs 
      sub tree policies  multiagent beliefs and expected future value
to describe the relationship between multiagent beliefs and probabilistic equivalence  we
must first discuss the policies an agent may follow and their resulting values  we begin
by introducing the concept of sub tree policies  as illustrated in fig     on page       a
 deterministic  policy i can be represented as a tree with nodes labeled using actions and
edges labeled using observations  the root node corresponds to the first action taken  other
nodes specify the action for the observation history encoded by the path from the root node 
as such  it is possible to define sub tree policies  i   which correspond to sub trees of agent
is policy i  also illustrated in fig      in particular  we write
w
i  o t   i  ht
     
i

for the sub tree policy of i corresponding to w
observation history  oit that specifies the actions
for the last    h  t stages  we refer to  as the policy consumption operator  since it
w
consumes the part of the policy corresponding to  oit   similarly we write i  k  o l   i  kl
i
 note that in        i is just a    h steps to go sub tree policy  and use similar notation 
   k   for joint sub tree policies  for a more extensive treatment of these different forms of
policy  we refer to the discussion by oliehoek        
given these concepts  we can define the value of a    k stages to go joint policy starting
from state s 
xx
w
pr s  o s a v  s      k o   
     
v  s    k     r s a   
s

o

here  a is the joint action specified by the roots of the individual sub tree policies specified
by    k for stage t   h  k 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

from this definition  it follows directly that the probability distribution over states s and
sub tree policies over other agents    i is sufficient to predict the value of a sub tree policy i  
in fact  such a distribution is known as a multiagent belief bi  s    i    hansen  bernstein   
zilberstein         its value is given by
xx
v  bi     max
bi  s    i  v  s hi      i i  
     
i

s

   i

and we refer to the maximizing i as agent is best response for bi   this illustrates that a
multiagent belief is a sufficient statistic  it contains sufficient information to predict the value
of any sub tree policy i  
it is possible to connect action observation histories to multiagent beliefs by fixing the
policies of the other agents  given that the other agents will act according to a profile of
policies    i   agent i has a multiagent belief at the first stage of the dec pomdp  bi  s    i    
b   s   moreover  agent i can maintain such a multiagent belief during execution  as such 
given    i   each history  i induces a multiagent belief  which we will write as bi  s    i   i      i  
to make the dependence on  i      i explicit  the multiagent belief for a history is defined as
bi  s    i   i      i     pr s    i   i   b       i   

     

and induces a best response via       
br  i     i     arg max
i

xx
s

bi  s    i   i      i  v  s    i  i   

     

   i

from this we can conclude that two aohs  ia   ib can be clustered together if they induce the
same multiagent belief 
however  this notion of multiagent belief is clearly quite different from the distributions
used in our notion of pe  in particular  to establish whether two aohs induce the same
multiagent belief  we need a full specification of    i   nevertheless  we show that two aohs
that are pe are also best response equivalent and that we can therefore cluster them  the
crux is that we can show that  if criterion   is satisfied  the aohs will always induce the
same multiagent beliefs for any    i  consistent with the current past joint policy   i   
      best response equivalence allows lossless clustering of histories
we can now relate probabilistic equivalence and the multiagent belief as follows 
lemma    pe implies multiagent belief equivalence   for any    i   probabilistic equivalence
implies multiagent belief equivalence 


p e  ia   ib    s   i bi  s    i   ia      i     bi  s    i   ib      i  
      
proof  see appendix 
this lemma shows that if two aohs are pe  they produce the same multiagent belief 
intuitively  this gives us a justification to cluster such aohs together  since a multiagent
belief is a sufficient statistic we should act the same when we have the same multiagent belief 
but since lemma   shows that  ia   ib induces the same multiagent beliefs for any    i when
they are pe  we can conclude that we will always act the same in those histories  formally 
we prove that  ia   ib are best response equivalent if they are pe 
   

fioliehoek  spaan  amato    whiteson

theorem    pe implies best response equivalence   probabilistic equivalence implies bestresponse equivalence  that is


p e  ia   ib      i br  ia     i     br  ib     i  
proof  assume any arbitrary    i   then
br  ia     i     arg max

xx

bi  s    i   ia  v  s    i  i  

  arg max

xx

bi  s    i   ib  v  s    i  i     br  ib     i   

i

i

s

s

   i

   i

where lemma   is employed to assert the equality of bi    ia   and bi    ib   
this theorem is key because it demonstrates that when two aohs  ia   ib of an agent are
pe  then that agent need not discriminate between them now or in the future  thus  when
searching the space of joint policies  we can restrict our search to those that assign the same
sub tree policy i to  ia and  ib   as such  it directly provides intuition as to why lossless
clustering is possible  formally  we define the clustered joint policy space as follows 
definition    clustered joint policy space   let c   be the subset of joint policies that
is clustered  i e   each i that is part of a   c assigns the same sub tree policy to action
observation histories that are probabilistically equivalent 
corollary    existence of an optimal clustered joint policy   there exists an optimal joint
policy in the clustered joint policy space 
max v      max v   

c



      

proof  it is clear that the left hand side of        is upper bounded by the right hand side 
since c    now suppose that     arg max v    has strictly higher value than the
best clustered joint policy  for at least one agent i and one pair of pe histories  ia    ib     must
assign different sub tree policies ia    ib  otherwise   would be clustered   without loss of
generality we assume that there is only one such pair  it follows directly from theorem   that
from this policy we can construct a clustered policy  c  c  by assigning either ia or ib
to both  ia    ib   that is guaranteed to have value no less than     thereby contradicting the
assumption that   has strictly higher value than the best clustered joint policy 
this formally proves that we can restrict our search to c   the space of clustered joint
policies  without sacrificing optimality 
      clustering with commitment in cbgs
though it is now clear that two aohs that are pe can be clustered  making this result
operational requires an additional step  to this end  we use the abstraction layer provided
by bayesian games  recall that in the cbg for a stage  the aohs correspond to types 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

therefore  we want to cluster these types in the cbg  to accomplish the clustering of two
types ia  ib   we introduce a new type ic to replace them  by defining 
  i pr ic     i     pr ia     i     pr ib     i  
j a

u hic     i i  a 



ff
pr ia     i  u hia     i i  a    pr ib     i  u  ib     i  a 
 
 
pr ia     i     pr ib     i  

      
      

theorem    reduction through commitment   given that agent i in collaborative bayesian
game b is committed to selecting a policy that assigns the same action for two of its types
ia  ib   i e   to selecting a policy i such that i  ia     i  ib    the cbg can be reduced without
loss in value for any agents  that is  the result is a new cbg b  in which agent i employs a
policy i that reflects the clustering and whose expected payoff is the same as in the original

cbg  v b  i     i     v b  i     i   
proof  see appendix 
this theorem shows that  given that agent i is committed to taking the same action for
its types ia  ib   we can reduce the collaborative bayesian game b to a smaller one b  and

translate the joint cbg policy   found
 for b back to a joint cbg policy  in b  this does
not necessarily mean that    i     i is also a solution for b  because the best response of
agent i against    i may not select the same action for ia  ib   rather i is the best response
against    i given that the same action needs to be taken for ia  ib    
even though theorem   only gives a conditional statement that depends on an agent being
committed to select the same action for two of its types  the previous subsection discussed
when a rational agent can make such a commitment  combining these results gives the
following corollary 
corollary    lossless clustering with pe   probabilistically equivalent histories  ia   ib can
be clustered without loss in heuristic value by merging them into a single type in a cbg 
proof  theorem   shows that  given that an agent i is committed to take the same action
for two of its types  those types can be clustered without loss in value  since  ia   ib are pe 
they are best response equivalent  which means that the agent is committed to use the same
sub tree policy i and hence the same action ai   therefore we can directly apply clustering
without loss in expected payoff  which in a cbg for a stage of a dec pomdp means no loss
in expected heuristic value as given by       
intuitively  the maximizing action is the same for  ia and  ib regardless of what  future 
joint policies    i the other agents will use and hence we can cluster them without loss in
heuristic value  note that this does not depend on which heuristic is used and hence also
holds for an optimal heuristic  i e   when using an optimal q value function that gives the
true value   this directly relates probabilistic equivalence with equivalence in optimal value   
    although we focus on cbgs  these results generalize to bgs with individual payoff functions  thus  they
could potentially be exploited by algorithms for general payoff bgs  developing methods that do so is an
interesting avenue for future work 
    the proof originally provided by oliehoek et al         is based on showing that histories that are pe will
induce identical q values 

   

fioliehoek  spaan  amato    whiteson

algorithm   clustercbg b 
input  cbg b
output  losslessly clustered cbg b
   for each agent i do
  
for each individual type i  b i do
  
if pr i       then
  
b i  b i  i
  
continue
  
end if
  
for each individual type i  b i do
  
isprobabilisticallyequivalent  true
  
for all hs    i i do
   
if pr s    i  i      pr s    i  i   then
   
isprobabilisticallyequivalent  false
   
break
   
end if
   
end for
   
if isprobabilisticallyequivalent then
   
b i  b i  i
   
for each a  a do
   
for all    i do
   
u i     i  a   min u i     i  a  u i     i  a  
   
pr i     i    pr i     i     pr i     i  
   
pr i     i     
   
end for
   
end for
   
end if
   
end for
   
end for
    end for
    return b

 prune i from b  

 prune i from b  
  take the lowest upper bound  

note that this result establishes a sufficient  but not necessary condition for lossless clustering 
in particular  given policies for the other agents  many types are best response equivalent and
can be clustered  however  as far as we know  the criterion must hold in order to guarantee
that two histories have the same best response against any policy of the other agents 
    gmaa  with incremental clustering
knowing which individual histories can be clustered together without loss of value has the
potential to speed up many dec pomdp methods  in this article  we focus on its application
within the gmaa  framework 
emery montemerlo et al         showed how clustering can be incorporated at every stage
in their algorithm  when the cbg for a stage t is constructed  a clustering of the individual
histories  types  is performed first and only afterwards is the  reduced  cbg solved  the same
approach can be employed within gmaa  by modifying the expand procedure  algorithm   
to cluster the cbg before calling generateallchildrenforcbg 
algorithm   shows the clustering algorithm  it takes as input a cbg and returns the
clustered cbg  it performs clustering by performing pairwise comparison of all types of each
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

b
algorithm   constructextendedbg b  t    q 

input  a cbg b for stage t     and the joint bg policy followed  t   
b   a  
input  an admissible heuristic of the form q 

output  cbg b for stage t 
   b   b
 make a copy of b that we subsequently alter 
   for each agent i do
  
b   i   constructextendedtypeset i 
 overwrite the individual type sets 
   end for
   b     id i
 the new joint type set  does not have to be explicitly stored  
   for each joint type      t   at   ot    b    do
  
for each state st  s do
  
compute pr st   
 from pr st    t    via bayes rule  
  
end for
   
pr    pr ot   t   at    pr  t   
   
for each a  a do
   
q
   
for each history   t represented by  do
b   t  a  
b we can take the lowest upper bound  
   
q  min q q 
  if q  q
   
end for
   
b   u  a   q
   
end for
    end for
    return b 

agent to see if they satisfy the criterion  yielding o  i      comparisons for each agent i  each
comparison involves looping over all hs    i i  line     if there are many states  some efficiency
could be gained by first checking       and then checking        rather than taking the
average as in         on line    we take the lowest payoff  which can be done if we are using
upper bound heuristic values 
the following theorem demonstrates that  when incorporating clustering into gmaa  
the resulting algorithm is still guaranteed to find an optimal solution 
theorem    when using a heuristic of the form       and clustering the cbgs in gmaa 
using the pe criterion  the resulting search method is complete 
proof  applying clustering does not alter the computation of lower bound values  also 
heuristic values computed for the expanded nodes are admissible and in fact unaltered as
guaranteed by corollary    therefore  the only difference with regular gmaa  is that the
class of considered joint policies is restricted to c   the class of clustered joint policies  not
all possible child nodes are expanded  because clustering effectively prunes away policies that
would specify different actions for aohs that are pe and thus clustered  however  corollary  
guarantees that there exists an optimal joint policy in this restricted class 
the modification of the expand proposed above is rather naive  to construct b b   t  
it must first construct all  oi  t possible aohs for agent i  given the past policy ti    the
subsequent clustering involves pairwise comparison of all these exponentially many types 
clearly  this is not tractable for later stages 
however  because pe of aohs propagates forwards  i e   identical extensions of pe histories are also pe   a more efficient approach is possible  instead of clustering this exponentially
   

fioliehoek  spaan  amato    whiteson

algorithm   expand ic q  h   the expand operator for gmaa  ic 
input  q   ht   vi the search node to expand 
b   a  
input  h the admissible heuristic that is of the form q 
output  qexpand the set containing expanded child nodes 
   b t     t   cbg
 retrieve previous cbg  note t    t     t    
t  b
t
t 
   b     constructextendedbg b 
     q 
   b t    clusterbg b t   
   t  cbg  b t  
 store pointer to this cbg 
   qexpand  generateallchildrenforcbg b t   
   return qexpand

growing set of types  we can simply extend the already clustered types of the previous stages
cbg  as shown in algorithm    that is  given i   the set of types of agent i at the previous
stage t     and it  the policy agent i took at that stage  the set of types at stage t  i   can
be constructed as

 
i   i    i  it   i   oti     i  i  oti  oi  
      
this means that the size of this newly constructed set is  i      i     oi     if the type set i at
the previous stage t    was much smaller than the set of all histories  i     oi  t    then the
new type set i is also much smaller   i     oi  t   in this way  we bootstrap the clustering
at each stage and spend significantly less time clustering  we refer to the algorithm that
implements this type of clustering as gmaa  with incremental clustering  gmaa  ic  
this approach is possible only because we perform an exact  value preserving clustering for
which lemma   guarantees that identical extensions will also be clustered without loss in
value  when performing the same procedure in a lossy clustering scheme  e g   as in emerymontemerlo et al          errors might accumulate  and a better option might be to re cluster
from scratch at every stage 
expansion of a gmaa  ic node takes exponential time with respect to both the number
of agents and types  as there are o  a  n      joint cbg policies and thus child nodes in the
gmaa  ic search tree  a is the largest action set and  is the largest type set   clustering
involves a pairwise comparison of all types of each agent and each of these comparisons needs
to check o    n   s   numbers for equality to verify        the total cost of clustering can
therefore be written as
o n         n   s   
which is only polynomial in the number of types  when clustering decreases the number of
types      it can therefore significantly reduce the number of child nodes and thereby the
overall time needed  however  when no clustering is possible  some overhead will be incurred 
    improved heuristic representation
since clustering can reduce the number of types  gmaa  ic has the potential to scale to
larger horizons  however  doing so has important consequences for the computation of the
heuristics  previous research has shown that the upper bound provided by qmdp is often
too loose for effective heuristic search  oliehoek  spaan    vlassis         however  the
space needed to store tighter heuristics such as qpomdp or qbg grows exponentially with the
horizon  recall from section        see fig     that there are two approaches to computing
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

b with minimum size 
algorithm   compute hybrid q
  
  
  
  
  
  
  
  
  
   
   
   
   
   

qh    r           r a   
z   a    s 
for t   h    to   do
  t     a 
y   
if z   y then
v  vectorbackup qt    
v   prune v 
qt  v 
z   v      s 
end if
if z  y then
qt  treebackup qt    
end if
end for

 vector representation of last stage 
 the size of the  a  vectors 
 size of aoh representation 

 from now on z  y 

qpomdp or qbg   the first constructs a tree of all joint aohs and their heuristic values  which
is simple to implement but requires storing a value for each    t   a  pair  the number of which
grows exponentially with t  the second approach maintains a vector based representation  as
is common for pomdps  though pruning can provide leverage  in the worst case  no pruning
is possible and the number of maintained vectors grows doubly exponentially with h  t  the
number of stages to go  similarly  the initial belief and subsequently reachable beliefs can be
used to reduce the number of vectors retained at each stage  but as the number of reachable
beliefs is exponential in the horizon the exponential complexity remains 
oliehoek  spaan  and vlassis        used a tree based representation for the qpomdp and qbg heuristics  since the
computational cost of solving the dec pomdp was the bottleneck  the inefficiencies in the representation could be overlooked  however  this approach is no longer feasible for the
longer horizons made possible by gmaa  ic 

hybrid

t  

to mitigate this problem  we propose a hybrid represent  
tation for the heuristics  as illustrated in fig     the main
insight is that the exponential growth of the two existing representations occurs in opposite directions  therefore  we can
t  
use the low space complexity side of both representations  the
later stages  which have fewer vectors  use a vector based representation  while the earlier stages  which have fewer histot  
ries  use a history based representation  this is similar to
the idea of utilizing reachable beliefs to reduce the size of the figure    an illustration of
vector representation described above but  rather than stor  the hybrid representation 
ing vectors for the appropriate aohs at each step  only the
values are needed when using the tree based representation 
algorithm   shows how  under mild assumptions  a minimally sized representation can be
computed  starting from the last stage  the algorithm performs vector backups  switching to
tree backups when they become the smaller option  for the last time step h     we represent
   

fioliehoek  spaan  amato    whiteson

qt by the set of immediate reward vectors     and variable z  initialized on line    keeps track
of the number of parameters needed to represent qt as vectors for the time step at hand 
note that z depends on how effective the vector pruning is  i e   how large the parsimonious
representation of the piecewise linear and convex value function is  since this is problem
dependent  z can be updated only after pruning has actually been performed  line     by
contrast y  the number of parameters in a tree representation  can be computed directly from
the dec pomdp  line     when z   y  the algorithm switches to tree backups   

   incremental expansion
the clustering technique presented in the previous section has the potential to significantly
speed up planning if much clustering is possible  however  if little clustering is possible  the
number of children in the gmaa  search tree will still grow super exponentially  this section
presents incremental expansion  a complementary technique to deal with this problem 
incremental expansion exploits recent improvements in effectively solving cbgs  first
note that during the expansion of the last stage t   h    for a particular h    we are only
interested in the best child  h    h      which corresponds to the optimal solution of the
bayesian game  h        as such  for this last stage  we can use new methods for solving
cbgs  kumar   zilberstein      b  oliehoek  spaan  dibangoye    amato        that can
provide speedups of multiple orders of magnitude over brute force search  enumeration    
unfortunately  the improvements to gmaa  afforded by this approach are limited  in order
to guarantee optimality  it still relies on expansion of all  child nodes corresponding to all 
joint cbg policies  for the intermediate stages  thus necessitating a brute force approach 
however  many of the expanded child nodes may have low heuristic values vb and may therefore
never be selected for further expansion 
incremental expansion overcomes this problem because it exploits the following key observation  if we can generate the children in decreasing heuristic order using an admissible
heuristic  we do not have to expand all the children  as before  an a  search is performed
over partially specified policies and each new cbg is constructed by extending the cbg for
the parent node  however  rather than fully expanding  i e   enumerating all the cbg policies
of and thereby constructing all children for  each search node  we instantiate an incremental
cbg solver for the corresponding cbg  this incremental solver returns only one joint cbg
policy at a time  which is then used to construct a single child t      t      by revisiting
the nodes  only the promising child nodes are expanded incrementally 
below  we describe gmaa  ice  an algorithm that combines gmaa  ic with incremental expansion  we establish theoretical guarantees and describe the modifications to
bagabab  the cbg solver that gmaa  ice employs  that are necessary to deliver the
child nodes in decreasing order 
    only in exceptional cases where a short horizon is combined with large state and action spaces will representing the last time step as vectors not be minimal  in such cases  the algorithm can be trivially adapted 
    this assumes that the vector representation will not shrink again for earlier stages  although unlikely in
practice  such cases would prevent the algorithm from computing a minimal representation 
    kumar and zilberstein      b  tackle a slightly different problem  they introduce a weighted constraint satisfaction approach to solving the point based backup in dynamic programming for dec pomdps  however 
this point based backup can be interpreted as a collection of cbgs  oliehoek et al         

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

    gmaa  with incremental clustering and expansion
we begin by formalizing incremental expansion and incorporating it into gmaa  ic  yielding gmaa  with incremental clustering and expansion  gmaa  ice   at the core of
incremental expansion lies the following lemma 
lemma    given two joint cbg policies     for a cbg b b   t    if vb     vb       then
for the corresponding child nodes vb  t      vb  t     
proof  this holds directly by the definition of vb  t   as given by       
vb  t       v      t    t     vb   
 v      t    t     vb        vb  t     

it follows directly that  if for b b   t   we use a cbg solver that can generate a sequence
of policies             such that
vb     vb            

then  for the sequence of corresponding children

vb  t      vb  t             

exploiting this knowledge  we can expand only the first child t   and compute its heuristic
value vb  t     using        since all the unexpanded siblings will have heuristic values less
than or equal to that  we can modify gmaa  ic to reinsert the node q into the open list l
to act as a placeholder for all its non expanded children 
definition    a placeholder is a node for which at least one child has been expanded  a
placeholder has a heuristic value equal to its last expanded child 
thus  after expansion of a search node qs child  we update q v  the heuristic value of the
node  to vb  t      the value of the expanded child  i e   we set q v  vb  t      as such  we
can reinsert q into l as a placeholder  as mentioned above  this is correct because all the
unexpanded siblings  for which the parent node q now is a placeholder  have heuristic values
lower than or equal to vb  t      therefore the next sibling q  represented by the placeholder
is always expanded in time  q  is always created before nodes with lower heuristic value are
selected for further expansion  we keep track of whether a node is a previously expanded
placeholder or not 
as before  gmaa  ice performs an a  search over partially specified policies  as in
gmaa  ic  each new cbg is constructed by extending the cbg for the parent node and
then applying lossless clustering  however  rather than expanding all children  gmaa  ice
requests only the next solution  of an incremental cbg solver  from which a single child
t      t     is constructed  in principle gmaa  ice can use any cbg solver that is able
to incrementally deliver all  in descending order of vb     we propose a modification of the
bagabab algorithm  oliehoek et al          briefly discussed in section     
fig    illustrates the process of incremental expansion in gmaa  ice  with t indexed
by letters  first  a cbg solver for the root node ha   i is created  and the optimal solution   is
computed  with value    this results in a child hb   i  and the root is replaced by a placeholder
node ha   i  as per definition    the node comparison operator   b appears before a in the
   

fioliehoek  spaan  amato    whiteson

legend 
t
v

t

a
 

a
 

root node

a
 




b
 

t  

b
 


new b a   vb   
c
 

t  
ht   vi
in open list

ha   i

a
   

new b b   vb   

ha   i
hc   i
hb   i

hb   i
ha   i

b
 

c
 

d
   
next solution of
b a   vb     

hd     i
ha     i
hc   i
hb   i

figure    illustration of incremental expansion  with the nodes in the open list at the bottom 
past joint policies t are indexed by letters  placeholder nodes are indicated by dashes 
open list and hence is selected for expansion  its best child hc   i is added and hb   i is replaced
by placeholder hb   i  now the search returns to the root node  and the second best solution  
is obtained from the cbg solver  leading to child hd     i  placeholder nodes are retained as
long as they have unexpanded children  only their values are updated 
when using gmaa  ice  we can derive lower and upper bounds for the cbg solution 
which can be exploited by the incremental cbg solver  the incremental cbg solver for
b t   can be initialized with lower bound
vcbg   vgm aa  v      t    t   

     

where vgm aa is the value of the current best solution  and v      t    t   is the true expected
value of t over the first t stages  therefore  vcbg is the minimum value that a candidate
must generate over the remaining h  t stages in order to beat the current best solution  note
that each time the incremental cbg solver is queried for a solution  vcbg is re evaluated
 using         because vgm aa may have changed 
when the used heuristic faitfully represents the immediate reward  i e   is of the form
        then  for the last stage t   h     we can also specify an upper bound for the solution
of the cbg
vcbg   vb  h     v      h    h    
     
if this upper bound is attained  no further solutions will be required from the cbg solver 
the upper bound holds since by      
vb      vb  h    v      h    h   

  v  h    v      h    h   
 vb  h     v      h    h    

in the first step  vb  h     v  h    because h is a fully specified policy and the heuristic value
given by       equals the actual value when a heuristic that faithfully represents the expected
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

algorithm    expand ice q  h   the expand operator for gmaa  ice 
input  q   ht   vi the search node to expand 
b   a  
input  h the admissible heuristic that is of the form q 
output  qexpand the set containing   or   expanded child nodes 
   if isplaceholder q  then
  
b t    t  cbg
 reuse stored cbg 
   else
  
b t     t   cbg
 retrieve previous cbg  note t    t     t    
t 
b
  
b t    constructextendedbg b t       q 
t
t
  
b     clusterbg b    
  
b t   solver  createsolver b t   
  
t  cbg  b t  
 store pointer to this cbg 
   end if
 set lower bound for cbg solution 
    vcbg   vgm aa  v      t    t  
    if t   h    then
   
vcbg   vb  h     v      h    h   
 upper bound only used for last stage cbg 
    else
   
vcbg    
    end if
b   t  i  b t   solver nextsolution vcbg  vcbg  
 compute next cbg solution 
    h t   v
t
    if  then
   
t     t    t  
 create partial joint policy 
t
t  
    t 
t
b
b
   
v      v
      v    
 compute heuristic value 
   
q   ht     vb  t    i
 create child node 
   
qexpand   q   
    else
   
qexpand  
 fully expanded  exists no solution s t  v   h     vcbg  
    end if
    return qexpand

algorithm    postprocessnode ice q  l   post processing of a node in gmaa  ice 
input  q the last expanded node  l the open list 
output  q is either removed or updated 
   l pop q 
   if q is fully expanded or depth q    h    then
  
cleanup q
 delete the node and the associated cbg and solver 
  
return
   else
  
c  last expanded child of q
  
q v  c v
 update heuristic value of parent node 
  
isplaceholder q   true
 remember that q is a placeholder 
  
l insert q 
 reinsert at appropriate position 
    end if

   

fioliehoek  spaan  amato    whiteson

immediate reward is used  this implies that vb    itself is a lower bound  in the second step
v  h    vb  h     because vb  h    is admissible  therefore  we can stop expanding when
we find a  with  lower bound  heuristic value equal to the upper bound vcbg   this applies
only to the last stage because only then the first step is valid 
gmaa  ice can be implemented by replacing the expand and the postprocessnode
procedures of algorithms   and   by algorithms    and     respectively  expand ice first
determines if a placeholder is being used and either reuses the previously constructed incremental cbg solver or constructs a new one  then  new bounds are calculated and the next
cbg solution is obtained  subsequently  only a single child node is generated  rather than
expanding all children as in algorithm      postprocessnode ice removes the last node
that was returned by select only when all its children have been expanded  otherwise  it
updates that nodes heuristic value and reinserts it in the open list  see appendix a   for
gmaa  ice shown as a single algorithm 
    theoretical guarantees
in this section  we prove that gmaa  ic and gmaa  ice are search equivalent  as a direct
result we establish that gmaa  ice is complete  which means that integrating incremental
expansion preserves the optimality guarantees of gmaa  ic 
definition     we call two gmaa  variants search equivalent if they select exactly the
same sequence of non placeholder nodes corresponding to past joint policies to expand in the
search tree using the select operator 
for gmaa  ic and gmaa  ice we show that the set of selected nodes are the same 
however  the set of expanded nodes can be different  in fact  it is precisely these differences
that incremental expansion exploits 
theorem    gmaa  ice and gmaa  ic are search equivalent 
proof  proof is listed in section a   of the appendix 
note that theorem   does not imply that the computational and space requirements
of gmaa  ice and gmaa  ic are identical  on the contrary  for each expansion 
gmaa  ice generates only one child node to be stored on the open list  in contrast 
gmaa  ic generates a number of child nodes that is  in the worst case  doubly exponential
in the depth of the selected node    however  gmaa  ice is not guaranteed to be more
efficient than gmaa  ic  for example  in the case where all child nodes still have to be
generated  gmaa  ice will be slower due to the overhead it incurs 
corollary    when using a heuristic of the form       gmaa  ice is complete 
proof  under the stated conditions  gmaa  ic is complete  see theorem    
gmaa  ice is search equivalent to gmaa  ic  it is also complete 

since

    when a problem allows clustering  the number of child nodes grows less dramatically  see section    

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

    incremental cbg solvers
implementing gmaa  ice requires a cbg solver that can incrementally deliver all  in
descending order of vb     to this end  we propose to modify the bayesian game branch and
bound  bagabab  algorithm  oliehoek et al          bagabab performs an a  search
over  partially specified  cbg policies  thus  when applied within gmaa  ice  it performs
a second  nested a  search  to expand each node in the gmaa  search tree  a nested a 
search computes the next cbg solution    this section briefly summarizes the main ideas
behind bagabab  for more information  see oliehoek et al         and our modifications 
bagabab works by creating a search tree in which the nodes correspond to partially
specified joint cbg policies  in particular  it represents a  as a joint action vector  a vector
h                    i of the joint actions that  specifies for each joint type  each node g in the
bagabab search tree represents a partially specified vector and thus a partially specified
joint cbg policy  for example  a completely unspecified vector h            i corresponds to
the root node  while an internal node

 g at depth d  root beingff at depth    specifies joint
actions for the first d joint types g                    d                    the value of a node v  g 
is the value of the best joint cbg policy consistent with it  since this value is not known in
advance  bagabab performs an a  search guided by an optimistic heuristic 
in particular  we can compute an upper bound on the value achievable for any such
partially specified vector by computing the maximum value of the complete information joint
policy that is consistent with it  i e   a non admissible joint policy that selects the maximizing
joint actions for the remaining joint types   since this value is a guaranteed upper bound on
the maximum value achievable by a consistent joint cbg policy  it is an admissible heuristic 
we propose a modification to bagabab to allow solutions to be incrementally delivered 
the main idea is to retain the search tree after a first call of bagabab on a particular cbg
b t   and update it during subsequent calls  thereby saving computational effort 
standard a  search terminates when a single optimal solution has been found  this
behavior is the same when incremental bagabab is called for the first time on a b t   
however  during standard a   nodes whose upper bound is lower than the best known lower
bound can be safely deleted  as they will never lead to an optimal solution  in contrast  in
an incremental setting such nodes cannot be pruned  as they could possibly result in the k th
best solution and therefore might need to be expanded during subsequent calls to bagabab 
only nodes returned as solutions are pruned in order to avoid returning the same solution
twice  this modification requires more memory but does not affect the a  search process
otherwise 
when asked it for the k th solution  bagabab resets its internal lower bound to the value
of the next best solution that was previously found but not returned  or to vcbg as defined in
      if no such solution was found   then it starts an a  search initialized using the search
tree resulting from the  k     th solution  in essence  this method is similar to searching for
the best k solutions  where k can be incremented on demand  recently it was shown that  for
fixed k  such a modification preserves all the theoretical guarantees  soundness  completeness 
    while gmaa  ice could also use any other incremental cgb solver  there are few that avoid enumerating
all  before providing the first result and thus have the potential to work incrementally  an exception may
be the method of kumar and zilberstein      b   which employs and or branch and bound search with
the edac heuristic  and is thus limited to the two agent case   as a heuristic search method  it may be
amenable to an incremental implementation though to our knowledge this has not been attempted 

   

fioliehoek  spaan  amato    whiteson

optimal efficiency  of the a  algorithm  dechter  flerova    marinescu         but the results
trivially transfer to the setting where k is allowed to increase 

   experiments
in this section  we empirically test and validate all the proposed techniques  lossless clustering
of joint histories  incremental expansion of search nodes  and hybrid heuristic representations 
after introducing the experimental setup  we compare the performance of gmaa  ic and
gmaa  ice to that of gmaa  on a suite of benchmark problems from the literature 
next  we compare the performance of the proposed methods with state of the art optimal
and approximate dec pomdp methods  followed by a case study of the scaling behavior
with respect to the number of agents  finally  we compare memory requirements of the
hybrid heuristic representation to those of the tree and vector representations 
    experimental setup
the most well known dec pomdp benchmarks are the dec tiger  nair et al         and
broadcastchannel  hansen et al         problems  dec tiger was discussed extensively
in section    in broadcastchannel  two agents have to transmit messages over a communication channel  but when both agents transmit at the same time a collision occurs that is
noisily observed by the agents  the firefighting problem models a team of n firefighters
that have to extinguish fires in a row of nh houses  oliehoek  spaan    vlassis         each
agent can choose to move to any of the houses to fight fires at that location  if two agents are
in the same house  they will completely extinguish any fire there  the  negative  reward of
the team of firefighters depends on the intensity of the fire at each house  when all fires have
been extinguished  reward of zero is received  in the hotel   problem  spaan   melo        
travel agents need to assign customers to hotels with limited capacity  they can also send a
customer to a resort but this yields lower reward  in addition  we also use the following problems  recycling robots  amato  bernstein    zilberstein         a scaled down version of
the problem described in section    gridsmall with two observations  amato  bernstein   
zilberstein        and cooperative box pushing  seuken   zilberstein      a   a larger
two robot benchmark  table   summarizes these problems numerically  listing the number of
joint policies for different planning horizons 
experiments were run on an intel core i  cpu running linux  and gmaa   gmaa  ic 
and gmaa  ice were implemented in the same code base using the madp toolbox  c   
 spaan   oliehoek         the vector based qbg representation is computed using a variation of incremental pruning  adapted for computing q functions instead of regular value functions   corresponding to the naiveip method as described by oliehoek and spaan        
to implement the pruning  we employ cassandras pomdp solve software  a  r  cassandra 
      
for the results in sections     and      we limited each process to  gb ram and a
maximum cpu time of      s  reported cpu times are averaged over    independent runs
and have a resolution of     s  timings are given only for the maa  search processes  since
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

problem primitives

dec tiger

num   for h

n

 s 

 ai  

 oi  

 

 

 

 

 

 

 

    e 

    e  

    e  

broadcastchannel

 

 

 

 

    e 

    e 

    e  

gridsmall

 

  

 

 

     e 

     e  

     e  

cooperative box pushing

 

   

 

 

    e 

    e   

    e    

recycling robots

 

 

 

 

    e 

    e  

    e  

hotel  

 

  

 

 

    e 

    e  

    e    

firefighting

 

   

 

 

    e 

    e  

    e  

table    benchmark problem sizes and number of joint policies for different horizons 
computation of the heuristic is the same for both methods and can be amortized over multiple
runs    all problem definitions are available via http   masplan org 
    comparing gmaa   gmaa  ic  and gmaa  ice
we compared gmaa   gmaa  ic  and gmaa  ice using the hybrid qbg representation  while all methods compute an optimal policy  we expect gmaa  ic to be more efficient
than gmaa  when lossless clustering is possible  furthermore  we expect gmaa  ice to
provide further improvements in terms of speedup and scaling to longer planning horizons 
the results are shown in table    for all entries where we report results  the qbg heuristics
could be computed  thanks to the hybrid representation  consequently  the performance of
gmaa  ic is much better than all previously reported results  including those of oliehoek
et al          who were often required to resort to qmdp for larger problems and or horizons 
the entries marked by  show the limits when using qmdp instead of qbg   in most of these
problems we can reach longer horizons with qbg   only for firefighting can gmaa  ice
with qmdp compute solutions for higher h than is possible with qbg  hence the missing  
and showing that gmaa  ice is more efficient using a loose heuristic than gmaa  ic  
furthermore  the  entries indicate that the horizon to which we can solve a problem with
a tree based qbg representation is often much shorter 
these results clearly illustrate that gmaa  ic leads to a significant improvement in
performance  in all problems  gmaa  ic was able to produce a solution more quickly and
to increase the largest solvable horizon over gmaa   in some cases  gmaa  ic is able to
drastically increase the solvable horizon 
furthermore  the results clearly demonstrate that incremental expansion allows for significant additional improvements  in fact  the table demonstrates that gmaa  ice significantly outperforms gmaa  ic  especially in problems where little clustering is possible 
the results in table   also illustrate the efficacy of a hybrid representation  for problems
like gridsmall  cooperative box pushing  firefighting and hotel   neither the
tree nor vector representation is able to provide a compact qbg heuristic for the longer hori    the heuristics computation time ranges from less than a second to hours  for high h in some difficult
problems   table   presents some heuristic computation time results 

   

fioliehoek  spaan  amato    whiteson

h
 
 
 
 
 
 

v  tgmaa   s 
dec tiger
        
     
        
     
        
      
        

         

tic  s 

tice  s 

h

     
     
    
     


     
     
     
    
     


 
 
 
 
 
  
  
  
  
  
  
  
  
  
  

firefighting hnh     nf    i
          
    
     
     
          
    
    
    
          
              
    
          


    
          
    
    
 
 
 
gridsmall
 
        
     
     
     
 
        
    
    
     
 
        
 
          
 
        

    
 
        

    
 
 
 
hotel  
           
    
     
     
           
 
     
     
           
           
           
     
     
           
     
     
           
     
     
           
     
     
           
    
     
  
 
 
cooperative box pushing
 
         
    
     
     
 
         

          
 
         
       
 
 
 

 
 
 
 
 
 
  
  
  
  
  
  
  
   
   
   
   
   
   
   
    

v  tgmaa   s  tic  s  tice  s 
recycling robots
        
           
     
         
           
     
         
            
     
         
            
         
     
     
         
     
     
         
     
     
         
           
         
     
     
         
    
    
          
    
    
          
    
    
          
    
    
          

     


broadcastchannel
        
           
     
        
           
     
        
           
     
        
          
     
        
      
     
        
           
        
     
     
         
     
     
         
     
     
         
     
     
         
     
     
         
     
     
         
           
         
     
     
          
    
    
          
    
    
          
     
     
          
    
    


          
    
     



table    experimental results comparing regular gmaa   gmaa  ic  and gmaa  ice 
listed are the computation times of gmaa   tgmaa     gmaa  ic  tic    and
gmaa  ice  tice    using the hybrid qbg representation  we use the following symbols 
 memory limit violations   time limit overruns    heuristic computation exceeded memory or time limits   maximum planning horizon using qmdp    maximum planning horizon
using tree based qbg   bold entries indicate that only the methods proposed in this article
have computed these results 
zons  apart from dec tiger and firefighting  computing and storing qbg  or another
tight heuristic  for longer horizons is the bottleneck to further scalability 
together  these algorithmic improvements lead to the first optimal solutions for many
problem horizons  in fact  for the vast majority of problems tested  we provide results for
longer horizons than any previous work  the bold entries   these improvements are quite sub   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

h
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 bgh   

 cbgt  
dec tiger
          
                
                       
                              
firefighting hnh     nf    i
          
                 
                       
                               
                                  
gridsmall
          
                  
                        
hotel  
           
                  
                        
                              
    e                                  
    e                                  
    e                                  
    e                                  
cooperative box pushing
           
                  

h
 
 
 
 
  
  
  
  
  
  
  
  
 
 
 
 
 
 
  
  
  
  
  
  
   
   

 bgh     cbgt  
recycling robots
       remaining stages
        remaining stages
        remaining stages
         remaining stages
            remaining stages
    e       remaining stages
    e        remaining stages
    e        remaining stages
    e        remaining stages
     remaining stages
     remaining stages
     remaining stages
broadcastchannel
       for all t 
        for all t 
        for all t 
         for all t 
          for all t 
          for all t 
            for all t 
    e        for all t 
    e        for all t 
    e        for all t 
     for all t 
     for all t 
     for all t 
     for all t 

    
    
    
    
    
    
    
    
    
    
    
    

table    experimental results detailing the effectiveness of clustering  listed are the size of
the cbgs for t   h    without clustering   bgh      and the average cbg size for all stages
with clustering   cbgt    
stantial  especially given that lengthening the horizon by one increases the problem difficulty
exponentially  cf  table    
      analysis of clustering histories
table   provides additional details about the performance of gmaa  ic  by listing the
number of joint types in the gmaa  ic search   cbgt    for each stage t  these are averages
since the algorithm forms cbgs for different past policies  leading to clusterings of different
sizes    to see the impact of clustering  the table also lists  bgh     the number of joint types
in the cbgs constructed for the last stage without clustering  which is constant 
in dec tiger  the time needed by gmaa  ic is more than   orders of magnitude less
than that of gmaa  for horizon h      for h      this test problem has     e   joint
policies  and no other method has been able to optimally solve it  gmaa  ic  however 
is able to do so in reasonable time  in dec tiger  there are clear symmetries between the
    note that in some problem domains we report smaller clusterings than oliehoek et al          due to an
implementation mistake  their clustering was overly conservative  and did not in all cases treat two histories
as probabilistically equivalent  when in fact they were 

   

fioliehoek  spaan  amato    whiteson

observations that allow for clustering  as demonstrated by fig     another key property is
that opening the door resets the problem  which may also facilitate clustering 
in firefighting  for short planning horizons no lossless clustering is possible at any
stage  and as such  the clustering incurs some overhead  however  gmaa  ic is still faster
than gmaa  because constructing the bgs using bootstrapping from the previous cbg
takes less time than constructing a cbg from scratch  interesting counterintuitive results
occur for h      which was solved within memory limits  in contrast to h      in fact  using
qmdp we could compute optimal values v  for h      and it turns out that these are equal
to that for h      the reason is that the optimal joint policy is guaranteed to extinguish all
fires in   stages  for subsequent stages  all the rewards will be    while this itself does not
influence clustering  the further analysis of table   reveals that the cbg instances encountered
during the h     search happen to cluster much better than those in h      which is possible
because the heuristics vary with the horizon  in fact    for h     sends both agents to the
middle house at t      while for h      agents are dispatched to different houses  when both
agents fight fires at the same house  the fire is extinguished completely  and resulting joint
observations do not provide any new information  as a result  different joint types lead to the
same joint belief  which means they can be clustered  if agents visit different houses  their
observations do convey information  leading to different possible joint beliefs  which cannot
be clustered  
hotel   allows for a large amount of clustering  and gmaa  ic outperforms gmaa 
by a large margin  with the former reaching h     and the latter h      this problem
is transition and observation independent  becker  zilberstein  lesser    goldman       
nair  varakantham  tambe    yokoo        varakantham  marecki  yabu  tambe    yokoo 
       which facilitates clustering  as we further discuss in section      unlike methods
specifically designed to exploit transition and observation independence  gmaa  ic exploits
this structure without requiring a predefined explicit representation of it  further scalability
is limited by the computation of the heuristic 
for broadcastchannel  gmaa  ic achieves an even more dramatic increase in performance  allowing the solution of up to horizon h        analysis reveals that the cbgs
constructed for all stages are fully clustered  they contain only one type for each agent  the
reason is as follows  when constructing a cbg for t      there is only one joint type for the
previous cbg so  given      the solution for the previous cbg  there is no uncertainty with
respect to the previous joint action a    the crucial property of broadcastchannel is that
the  joint  observation reveals nothing about the new state  but only about what joint action
was taken  e g   collision if both agents chose to send   as a result  the different individual
histories can be clustered  in a cbg constructed for stage t      there is again only one joint
type in the previous game  therefore  given the past policy  the actions of the other agents
can be perfectly predicted  again the observation conveys no information so this process repeats  thus  the problem has a special property which could be described as non observable
given the past joint policy  gmaa  ic automatically exploits this property  consequently 
the time needed to solve each cbg does not grow with the horizon  the solution time  however  still increases super linearly because of the increased amount of backtracking  as in
firefighting  performance is not monotonic in the planning horizon  in this case however 
clustering is clearly not responsible for the difference  rather  the only explanation is that
for certain horizons  there are many near optimal joint policies  leading to more backtracking
and a higher search cost 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

  

nodes at depth t

  

dectiger  h    full exp 
dectiger  h    inc  exp 
gridsmall  h    full exp 
gridsmall  h    inc  exp 
firefighting  h    full exp 
firefighting  h    inc  exp 

 

  

 

  

 

 

 
t

 

 

figure    number of expanded partial joint policies t for intermediate stages t             h   
 in log scale  

      analysis of incremental expansion
in dec tiger for h      gmaa  ice achieves a speedup of three orders of magnitude and
can compute a solution for h      unlike gmaa  ic  for gridsmall  it achieves a large
speedup for h     and fast solutions for h     and    where gmaa  ic runs out of memory  similar positive results are obtained for firefighting  cooperative box pushing
and recycling robots  in fact  when using qmdp   gmaa  ice is able to compute
solutions well beyond h        for the firefighting problem  which stands in stark contrast to gmaa  ic that only computes solutions to h     with this heuristic  note that
broadcastchannel is the only problem for which gmaa  ic is  slightly  faster than
gmaa  ice  because this problem exhibits clustering to a single joint type  the overhead
of incremental expansion does not pay off 
to further analyze incremental expansion  we examined its impact on the number of nodes
expanded for intermediate stages t             h     fig    shows the number of nodes expanded
in gmaa  ice and the number that would be expanded for gmaa  ic  which can be
easily computed since they are search tree equivalent   there is a clear relationship between
the results from fig    and table    illustrating  e g   why gmaa  ic runs out of memory on
gridsmall h      the plots confirm our hypothesis that  in practice  only a small number
of child nodes are queried 
      analysis of hybrid heuristic representation
fig    illustrates the memory requirements in terms of number of parameters  i e   real numbers  for the tree  vector  and hybrid representations for qbg   where the latter is computed
following algorithm    results for the vector representation are omitted when those representations grew beyond limits  the effectiveness of the vector pruning depends on the problem
and the complexity of the value function  which can increase suddenly  as for instance happens in fig   c  these results show that  for several benchmark dec pomdps  the hybrid
representation allows for significant savings in memory  allowing the computation of tight
heuristics for longer horizons 
   

fioliehoek  spaan  amato    whiteson

h

milp

dp lpc

dp ipg

gmaa  qbg
ic

ice

heur

broadcastchannel  ice solvable to h      
 
    
     
    
     
 
    
    
     
     
 
     

 
     
 
     
     

     
     
     
     

     
     
     
     

dec tiger  ice
 
    
 
     
 

 

     
     
     
    

     
     
    
    

solvable to h    
    
    
     
     

       


     
     
    
     

firefighting    agents    houses    firelevels   ice
 
    
    
     
     
 


      
    
 

      
gridsmall  ice solvable to h    
 
    
     
    
 


    
 
     

    
    
    

recycling robots  ice solvable to h     
 
    
    
    
     
 
 
    
    
     
 
       
     
     
 

       
     
hotel
 
 
 
 
 
  
  

   ice solvable to h    
    
    
    
      
       
    


    
    
    
     
      

cooperative box pushing
 
    
     
 
        
 


     
     
     
     
    
 

solvable to h      
           
    
    
    
    
     
     
     

     
    
     

     
     
     
     

     
     
    
    

     
     
     
     
     
 

    
    
    
    
     

 qpomdp    ice solvable to h    
    
                 
    
    
    
    
       
 
           

table    comparison of runtimes with other methods  total time of the gmaa  methods
is given by taking the time from the method column  ic or ice  and adding the heuristic
computation time  heur   we use the following symbols   memory limit violations   
time limit overruns    heuristic computation exceeded memory or time limits 

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

  

memory required

 

  

 

  

 

  

 

 

 

 
 
horizon

 

  

 

 a  dec tiger 

 

  

 

  

 

 
 
horizon

 

  

 

 d  recycling robots 

  

 

                 
horizon

  

  

tree
vector
hybrid

  

  

 

  

 

                    
horizon

tree
vector
hybrid

 c  hotel   

  

tree
vector
hybrid

memory required

memory required

  

  

 

  

  

 b  firefighting 

  

  

  

tree
vector
hybrid

  

  
memory required

memory required

  

tree
vector
hybrid

memory required

  

  

tree
vector
hybrid

  

  

 

  

 

                    
horizon

 e  broadcastchannel 

  

 

 

 
 
horizon

 

 

 f  gridsmall 

figure    hybrid heuristic representation  the y axis shows number of real numbers stored
for different representations of qbg for several benchmark problems  in log scale  
    comparing to other methods
in this section  we compare gmaa  ic and gmaa  ice to other methods from the literature  we begin by comparing the runtimes of our methods against the following state ofthe art optimal dec pomdp methods  milp    aras   dutech        converts the decpomdp to a mixed integer linear program  for which numerous solvers are available  we
have used mosek version      dp lpc    boularias   chaib draa        performs dynamic programming with lossless policy compression  with cplex      as the lp solver 
dp ipg  amato et al         performs exact dynamic programing with incremental policy
    the results reported here deviate from those reported by aras and dutech         for a number of
problems  aras et al  employed a solution method that solves the milp as a series  a tree  of smaller
milps by branching on the continuous realization weight variables for earlier stages  that is  for each past
joint policy t for some stage t  they solve a different milp involving the subset of consistent sequences 
additionally  for firefighting and gridsmall  we use the benchmark versions standard to the literature
 oliehoek  spaan    vlassis        amato et al          whereas aras and dutech        use non standard
versions  this explains the difference between our results and the ones reported in their article  personal
communication  raghav aras  
    the goal of boularias and chaib draa        was to find non dominated joint policies for all initial beliefs 
the previously reported results concerned run time to compute the non dominated joint policies  without
performing pruning on the full length joint policies  in contrast  we report the time needed to compute the
actual optimal dec pomdp policy  given b     this additionally requires the final round of pruning and
subsequently computing the value for each of the remaining joint policies for the initial belief  this additional overhead explains the differences in run time between what we report here and what was previously
reported  personal communication  abdeslam boularias  

   

fioliehoek  spaan  amato    whiteson

problem
dec tiger
cooperative box pushing
gridsmall

h
 
 
 

m
 
 
 

vmbdp
    
     
    

v
     
     
    

table    comparison of optimal  v    and approximate  vmbdp   values 

generation that exploits known start state and knowledge about what states are reachable in
doing the dp backup 
table    which shows the results of the comparison  demonstrates that  in almost all cases 
the total time of gmaa  ice  given by the sum of heuristic computation time and the time
for the gmaa  phase  is significantly less than that of any other state of the art methods 
moreover  as demonstrated in table    gmaa  ice can compute solutions for longer horizons for all these problems  except for cooperative box pushing and hotel      for
these problems  it is not possible to compute qbg for longer horizons  overcoming this
problem could enable gmaa  ice to scale to further horizons as well 
the dp lpc algorithm proposed by boularias and chaib draa        also improves the
efficiency of optimal solutions by a form of compression  the performance of their algorithm 
however  is weaker than that of gmaa  ic  there are two main explanations for the performance difference  first  dp lpc uses compression to more compactly represent the values
for sets of useful sub tree policies  by using sequence form representation  the policies themselves  however  are not compressed  they still specify actions for every possible observation
history  for each policy it needs to select an exponential amount of sequences that make up
that policy   hence  it cannot compute solutions for long horizons  second  gmaa  ic can
exploit knowledge of the initial state distribution b   
overall  gmaa  ice substantially improves the state of the art in optimally solving
dec pomdps  previous methods typically improved the feasible solution horizon by just
one  or only provided speed ups for horizons that could already be solved   by contrast 
gmaa  ice dramatically extends the feasible solution horizon for many problems 
we also consider mbdp based approaches  the leading family of approximate algorithms 
table    which reports the vmbdp values produced by pbip ipg  amato et al          with
typical maxtrees parameter setting m   demonstrates that the optimal solutions produced by
gmaa  ic or gmaa  ice are of higher quality  pbip ipg was chosen because all other
mbdp algorithms with the same parameters achieve at most the same value  while not
exhaustive  this comparison illustrates that even the best approximate dec pomdp methods
in practice provide inferior joint policies on some problems  conducting such analysis is
possible only if optimal solutions can be computed  clearly  the more data that becomes
available  the more thorough the comparisons that can be made  therefore  scalable optimal
solution methods such as gmaa  ice are critical for improving these analyses 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

problem primitives

num   for h

n

 s 

 a 

 o 

 

 

 

 

  

 

 

  

    e 

    e  

 

  

 

 

   

    e  

    e  

 

   

  

  

    e 

    e  

    e  

 

   

  

  

    e 

    e  

    e  

 

    

  

  

    e 

    e  

    e   

table    firefightinggraph  the number of joint policies for different numbers of agents
and horizons  with   possible fire levels 
    scaling to more agents
all of the benchmark problems in our results presented so far were limited to two agents  here 
we present a case study on firefightinggraph  oliehoek  spaan  whiteson    vlassis 
       a variation of firefighting allowing for more agents  in which each agent can only
fight fires at two houses  instead of at all of them  table   highlights the size of these
problems  including the total number of joint policies for different horizons  we compared
gmaa   gmaa  ic  gmaa  ice  all using a qmdp heuristic   bruteforcesearch 
and dp ipg  with a maximum run time of    hours and running on an intel core i  cpu 
averaged over    runs  bruteforcesearch is a simple optimal algorithm that enumerates
and evaluates all joint policies  and was implemented in the same codebase as the gmaa 
variations  dp ipg results use the original implementation and were run on an intel xeon
computer  hence  while the timing results are not directly comparable  the overall trends are
apparent  also  since the dp ipg implementation is limited to   agents  no results are shown
for more agents 
fig     shows the computation times for firefightinggraph across different numbers of
of agents and planning horizons  while table   lists the optimal values obtained  as expected 
the baseline bruteforcesearch performs very poorly  only scaling beyond h     for  
agents  while dp ipg can only reach h      on the other hand  regular gmaa  performs
relatively well  scaling to a maximum of   agents  however  gmaa  ic and gmaa  ice
improve the efficiency of gmaa  by    orders of magnitude  as such  they substantially
outperform the other three methods  and scale up to   agents  the benefit of incremental expansion is clear for n        where gmaa  ice can reach a higher horizon than gmaa  ic 
hence  although this article focuses on scalability in the horizon  these results show that the
methods we propose can also improve scalability in the number of agents 
    discussion
overall  the empirical results demonstrate that incremental clustering and expansion offers
dramatic performance gains on a diverse set of problems  in addition  the results on broad    in hotel    dp ipg performs particularly well because the problem structure has limited reachability 
that is  each agent can fully observe its local state  but not that of the other agent  and in all local states
except one there is one action that dominates all others  as a result  dp ipg can generate a small number
of possibly optimal policies 

   

fioliehoek  spaan  amato    whiteson

 

 

  

  

 

 

  

 

  

 

  

 

  

 

  

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

  

computation time  s 

computation time  s 

computation time  s 

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

 

 

h

 agents

 

 

 

 

 

 

 

 

 

 

  

  

 

  

 

  

 

  

 

  

 

  

 

  

 

 

h

 agents

 a  gmaa  results 

 

 

 

 

 

 

 

 

 

  

h

 agents

 b  gmaa  ic results 

 

 c  gmaa  ice results 

 

  

  

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  

computation time  s 

 

computation time  s 

 

 

  

 

  

 

  

 

  

 

  

 

  

 

  

 
h

 agents

 

 

 

 

 

 

 

 

 

 

 

 

  

h

 agents

 d  bruteforcesearch results 

 e  dp ipg results 

figure     comparison of gmaa   gmaa  ic  gmaa  ice  bruteforcesearch 
and dp ipg on the firefightinggraph problem  shown are computation time  in log
scale  for various number of agents and horizons  missing bars indicate that the method
exceeded time or memory limits  however  the dp ipg implementation only supports  
agents 

h
 
 
 
 
 

n  
        
        
        
        
        

n  
        
        
        

n  
        
        
        

n  
        

n  
        

table    value v  of optimal solutions to the firefightinggraph problem  for different
horizons and numbers of agents 

castchannel illustrate a key advantage of our approach  when a problem possesses a property that makes a large amount of clustering possible  our clustering method exploits this
property automatically  without requiring a predefined explicit representation of it 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

of course  not all problems admit great reductions via clustering  one domain property
that allows for clustering is when the past joint policy encountered during gmaa  makes the
observations superfluous  as with broadcastchannel and firefighting  in dec tiger 
we see that certain symmetries can lead to clustering  however clustering can occur even
without these properties  in fact  for all problems and nearly all horizons that we tested  the
size of the cbgs can be reduced  moreover  in accordance with the analysis of section     
the improvements in planning efficiency are huge  even for modest reductions in cbg size 
one class of problems where we can say something a priori about the amount of clustering
that is possible is the class of dec pomdps with transition and observation independence
 becker et al          in such problems  the agents have local states and the transitions are
independent  which for two agents can be expressed as
pr s    s   s    s    a    a      pr s   s    a    pr s   s    a    

     

similarly  the observations are assumed to be independent  which means that for each agent
the observation probability depends only on its own action and local state  pr oi  ai   si    for
such problems  the probabilistic equivalence criterion       factors too  in particular  due to
transition and observation independence           holds true for any  ia   ib   moreover       
factors as the product pr s    s               pr s        pr s        and thus holds if pr s     a    
pr s     b    that is  two histories can be clustered if they induce the same local belief  as
such  the size of the cbgs directly corresponds to the product of the number of reachable local
beliefs  since the transition and observation independent hotel   problem is also locally
fully observable  and the local state spaces consist of four states  there are only four possible
local beliefs  which is consistent with the cbg size of    from table     moreover  we see
that this maximum size is typically only reached at the end of search  this is because good
policies defer sending customers to the hotel and thus do not visit local states where the hotel
is filled in the earlier stages 
in more general classes of problems  even other weakly coupled models  e g   becker 
zilberstein    lesser        witwicki   durfee         the criterion       does not factor 
and hence there is no direct correspondence to the number of local beliefs  as such  only
by applying our clustering algorithm can we determine how well such a problem clusters 
this is analogous to  e g   state aggregation in mdps  e g   discussed in givan  dean   
greig        where it is not known how to predict a priori how large a minimized model will
be  fortunately  our empirical results demonstrate that  in domains that admit little or no
clustering  the overhead is small 
as expected  incremental expansion is most helpful for problems which do not allow for
much clustering  however  the results for  e g   dec tiger illustrate that there is a limit to
the amount of scaling that the method can currently provide  the bottleneck is the solution
of the large cbgs for the later stages  the cbg solver has to solve these large cbgs when
returning the first solution in order to guarantee optimality  but this takes takes a long time 
we expect that further improvements to cbg solvers can directly add to the efficacy of
incremental expansion 
our experiments also clearly demonstrate that the dec pomdp complexity results  while
important  are only worst case results  in fact  the scalability demonstrated in our experiments
clearly show that in many problems we successfully scale dramatically beyond what would be
    this assumes no external state variable s   

   

fioliehoek  spaan  amato    whiteson

expected for a doubly exponential dependence on the horizon  even for the smallest problems 
a doubly exponential scaling in the horizon implies that it is impossible to compute solutions
beyond h     at all  as indicated by the following simple calculation  let n       ai      
actions   oi        observations  then
 

 ai   n  oi  

  

 

  ai   n  oi  

  

        e  

thus  even in the simplest possible case  we see an increase of a factor       e   from h    
to h      similarly  the next increment  from h     to h      increases the size of the search
space by a factor       e    however  our experiments clearly indicate that in almost all
cases  things are not so dire  that is  even though matters look bleak in the light of the
complexity results  we are in many cases able to perform substantially better than this worst
case 

   related work
in this section  we discuss a number of methods that are related to those proposed in this
article  some of these methods have already been discussed in earlier sections  in section    we
indicated that our clustering method is closely related to the approach of emery montemerlo
et al         but is also fundamentally different because our method is lossless  in section     
we discussed connections to the approach of boularias and chaib draa        which clusters
policy values  this contrasts with our approach which clusters the histories and thus the
policies themselves  leading to greater scalability 
in section        we discussed the relationship between our notion of probabilistic equivalence  pe  and the multiagent belief  however  there is yet another notion of belief  employed
in the jesp solution method  nair et al          that is superficially more similar to the pe
distribution  a jesp belief for an aoh  i is a probability distribution pr s  o  i   i   b       i  
over states and observation histories of other agents given a  deterministic  full policy of all
the other agents  it is a sufficient statistic  since it induces a multiagent belief  thus it also
allows for the clustering of histories  the crucial difference with  and the utility of  pe lies in
the fact that the pe criterion is specified over states and aohs given only a past joint policy 
that is        does not induce a multiagent belief 
our clustering approach also resembles a number of methods that employ other equivalence
notions  first  several approaches exploit the notion of behavioral equivalence  pynadath  
marsella        zeng et al         zeng   doshi         they consider  from the perspective
of a protagonist agent i  the possible models of another agent j  since j affects i only through
its actions  i e   its behavior  agent i can cluster together all the models of agent j that lead
to the same policy j for that agent  that is  it can cluster all models of agent j that are
behaviorally equivalent  in contrast  we do not cluster models of other agents j  but histories
of this agent i if all the other agents  as well as the environment  are guaranteed to behave the
same in expectation  thus leading to the same best response of agent i  that is  our method
could be seen as clustering histories that are expected environmental behavior equivalent 
the notion of utility equivalence  pynadath   marsella        zeng et al         is closer to
pe because it also takes into account the  value of the  best response of agent i  in particular 
it clusters two models mj and mj if using br mj  the best response against mj  achieves
the same value against mj    however  it remains a form of behavior equivalence in that it
clusters models of other agents  not histories of the protagonist agent 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

there are also connections between pe and work on influence based abstraction  becker et
al         witwicki   durfee        witwicki        oliehoek et al          since the influence
 or point in parameter space  becker et al         is a compact representation of the other
agents policies  models of the other agents can be clustered if they lead to the same influence
on agent i  however  though more fine grained  this is ultimately still a form of behavioral
equivalence 
a final relation to our equivalence notion is the work by dekel  fudenberg  and morris
        which constructs a distance measure and topology on the space of types with the
goal of approximating the infinite universal type space  the space of all possible beliefs about
beliefs about beliefs  etc   for one shot bayesian games  our setting  however  considers a
simple finite type space where the types directly correspond to the private histories  in the
form of aohs  in a sequential problem  thus  we do not need to approximate the universal
type space  instead we want to know which histories lead to the same future dynamics from
the perspective of an agent  dekel et al s topology does not address this question 
our incremental expansion technique is related to approaches extending a to deal with
large branching factors in the context of multiple sequence alignment  ikeda   imai       
yoshizumi  miura    ishida         however  our approach is different because we do not
discard unpromising nodes but rather provide a mechanism to generate only the necessary
ones  also  when proposing maa   szer et al         developed a superficially similar approach that could be applied only to the last stage  in particular  they proposed generating
the child nodes one by one  each time checking if a child is found with value equal to its
parents heuristic value  since the value of such a child specifies a full policy  its value is
a lower bound and therefore expansion of any remaining child nodes can be skipped  unfortunately  a number of issues prevent this approach from providing substantial leverage in
practice  first  it cannot be applied to intermediate stages    t   h    since no lower bound
values for the expanded children are available  second  in many problems it is unlikely that
such a child node exists  third  even if it does  szer et al  did not specify an efficient way of
finding it  incremental expansion overcomes all of these issues  yielding an approach that  as
our experiments demonstrate  significantly increases the size of the dec pomdps that can
be solved optimally 
this article focuses on optimal solutions for dec pomdps over a finite horizon  as part
of our evaluation  we compare against the milp approach  aras   dutech         dpilp  boularias   chaib draa        and dp ipg  amato et al          an extension of the
exact dynamic programming algorithm  hansen et al          research on finite horizon decpomdps has considered many other approaches such as bounded approximations  amato 
carlin    zilberstein         locally optimal solutions  nair et al         varakantham  nair 
tambe    yokoo        and approximate methods without guarantees  seuken   zilberstein 
    b      a  carlin   zilberstein        eker   akn        oliehoek  kooi    vlassis       
dibangoye et al         kumar   zilberstein      b  wu et al       a  wu  zilberstein   
chen      b  
in particular  much research has considered the optimal and or approximate solution of
subclasses of dec pomdps  one such subclass contains only dec pomdps in which the
agents have local states that other agents cannot influence  the resulting models  such as the
toi dec mdp  becker et al         dibangoye  amato  doniec    charpillet        and ndpomdp  nair et al         varakantham et al         marecki  gupta  varakantham  tambe 
  yokoo        kumar   zilberstein         can be interpreted as independent  po mdps for
   

fioliehoek  spaan  amato    whiteson

each agent that are coupled through the reward function  and possibly an unaffectable state
feature   on the other hand  event driven interaction models  becker et al         consider
agents that have individual rewards but can influence each others transitions 
more recently  models that allow for limited transition and reward dependence have been
introduced  examples are interaction driven markov games  spaan   melo         decmdps with sparse interactions  melo   veloso         distributed pomdps with coordination locales  varakantham et al         velagapudi et al          event driven interactions with
complex rewards  edi cr   mostafa   lesser         and transition decoupled dec pomdps
 witwicki   durfee        witwicki         while the methods developed for these models often exhibit better scaling behavior than methods for standard dec  po mdps  they typically
are not suitable when agents have extended interactions  e g   to collaborate in transporting
an item  also  there have been specialized models that consider the timing of actions whose
ordering is already determined  marecki   tambe        beynier   mouaddib        
another body of work addresses infinite horizon problems  amato  bernstein    zilberstein        amato  bonet    zilberstein        bernstein  amato  hansen    zilberstein 
      kumar   zilberstein      a  pajarinen   peltonen         in which it is not possible to
represent a policy as a tree  these approaches represent policies using finite state controllers
that are then optimized in various ways  also  since the infinite horizon case is undecidable
 bernstein et al          the approaches are approximate or optimal given a particular controller size  while there exists a boundedly optimal approach that can theoretically construct
a controller within any  of optimal  it is only feasible for very small problems or a large 
 bernstein et al         
there has also been great interest in dec pomdps that explicitly take into account communication  some approaches try to optimize the meaning of communication actions without
semantics  xuan  lesser    zilberstein        goldman   zilberstein        spaan  gordon 
  vlassis        goldman  allen    zilberstein        while others use fixed semantics  e g  
broadcasting the local observations   ooi   wornell        pynadath   tambe        nair et
al         roth et al         oliehoek  spaan    vlassis        roth  simmons    veloso       
spaan  oliehoek    vlassis        goldman   zilberstein        becker  carlin  lesser    zilberstein        williamson  gerding    jennings        wu et al          since models used
in the first category  e g   the dec pomdp com  can be converted to normal dec pomdps
 seuken   zilberstein         the contributions of this article are applicable to those settings 
finally  there are numerous models closely related to dec pomdps  such as posgs
 hansen et al          interactive pomdps  i pomdps   gmytrasiewicz   doshi        
and their graphical counterparts  doshi  zeng    chen         these models are more general in the sense that they consider self interested settings where each agent has an individual
reward function  i pomdps are conjectured to also require doubly exponential time  seuken
  zilberstein         however  for the i pomdp there have been a number of recent advances
 doshi   gmytrasiewicz         the current paper makes a clear link between best response
equivalence of histories and the notion of best response equivalence of beliefs in i pomdps 
in particular  this article demonstrates that two pe action observation histories  aohs  induce  given only a past joint policy  a distribution over states and aohs of other agents  and
therefore will induce the same multiagent belief for any future policies of other agents  these
induced multiagent beliefs  in turn  can be interpreted as special cases of i pomdp beliefs
where the model of the other agents are sub intentional models in the form of a fixed policy
tree  rabinovich and rosenschein        introduced a method that  rather than optimizing
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

the expected value of a joint policy  selects coordinated actions under uncertainty by tracking
the dynamics of an environment  this approach  however  requires a model of the ideal system
dynamics as input and in many problems  such as those considered in this article  identifying
such dynamics is difficult 

   future work
several avenues for future work are made possible by the research presented in this article 
perhaps the most promising is the development of new approximate dec pomdp algorithms 
while this article focused on optimal methods  gmaa  ice can also be seen as a framework for approximate methods  such methods could be derived by limiting the amount of
backtracking  employing approximate cbg solvers  emery montemerlo  gordon  schneider 
  thrun        kumar   zilberstein      b  wu et al       a   integrating gmaa  methods for factored dec pomdps  oliehoek  spaan  whiteson    vlassis        oliehoek       
oliehoek et al          performing lossy clustering  emery montemerlo        wu et al        
or using bounded approximations for the heuristics  in particular  it seems promising to combine approximate clustering with approximate factored gmaa  methods 
lossy clustering could be achieved by generalizing the probabilistic equivalence criterion 
which is currently so strict that little or no clustering may be possible in many problems  an
obvious approach is to cluster histories for which the distributions over states and histories of
other agents are merely similar  as measured by  e g   kullback leibler divergence  alternately 
histories could be clustered if they induce the same individual belief over states 
pr s  i    

x

pr s  
   i   i   

     

 
   i

while individual beliefs are not sufficient statistics for history  we hypothesize that they
constitute effective metrics for approximate clustering  since the individual belief simply
marginalizes out the other agents histories from the probabilities used in the probabilistic
equivalence criterion  it is an intuitive heuristic metric for approximate clustering 
while this article focuses on increasing scalability with respect to the horizon  developing
techniques to deal with larger number of agents is an important direction of future work  we
plan to further explore performing gmaa  using factored representations  oliehoek  spaan 
whiteson    vlassis         in that previous work  we could only exploit the factorization at
the last stage  since earlier stages required full expansions to guarantee optimality  however 
for such larger problems  the number of joint bg policies  i e   number of child nodes  is
directly very large  earlier stages are more tightly coupled   therefore incremental expansion
is crucial to improving the scalability of optimal solution methods with respect to the number
of agents 
another avenue for future work is to further generalize gmaa  ice  in particular  it
may be possible to flatten the two nested a searches into a single a search  doing so
could lead to significant savings as it would obviate the need to solve an entire cbg before
expanding the next one  in our work  we employed the plain a algorithm as a basis  but a
promising direction of future work is to investigate what a enhancements from the literature
 edelkamp   schrodl        can benefit gmaa  most  in particular  as we described in
our experiments  different past joint policies can lead to cbgs of different sizes  one idea
   

fioliehoek  spaan  amato    whiteson

is to first expand parts of the search tree that lead to small cbgs  by biasing the selection
operator  but not the pruning operator  so as to maintain optimality  
yet another important direction for future work is the development of tighter heuristics 
though few researchers are addressing this topic  the results presented in this article underscore how important such heuristics are for solving larger problems  currently  the heuristic
is the bottleneck in four out of the seven problems we considered  moreover  two of the
problems where this is not the bottleneck can already be solved for long  h       horizons 
therefore  we believe that computing tight heuristics for longer horizons is the single most
important research direction for further improving the scalability of optimal dec pomdp
solution methods with respect to the horizon 
a different direction is to employ our theoretical results on clustering beyond the decpomdp setting to develop new solution methods for cbgs  for instance  a well known
method for computing a local optimum is alternating maximization  am   starting from an
arbitrary joint policy  compute a best response for some agent given that other agents keep
their policies fixed and then select another agents policy to improve  etc  one idea is to start
with a completely clustered cbg  where all agents types are clustered together and thus
a random joint cbg policy has a simple form  each agent just selects a single action  only
when improving the policy of an agent do we consider all its actual possible types to compute
its best response  subsequently  we cluster together all types for which that agent selects the
same action and proceed to the next agent  in addition  since our clustering results are not
restricted to the collaborative setting  it may also be possible to employ them  using a similar
approach  to develop new solution methods for general payoff bgs 
finally  two of our other contributions can have a significant impact beyond the problem of
optimally solving dec pomdps  first  the idea of incrementally expanding nodes introduced
in gmaa  ice can be applied in other a search methods  incremental expansion is most
useful when children can be generated in order of decreasing heuristic value without prohibitive
computational effort  and in problems with a large branching factor such as multiple sequence
alignment problems in computational biology  carrillo   lipman        ikeda   imai        
second  representing pwlc value functions as a hybrid of a tree and a set of vectors can have
wider impact as well  e g   in online search for pomdps  ross  pineau  paquet    chaib draa 
      

   conclusions
this article presented a set of methods that advance the state of the art in optimal solution
methods for dec pomdps  in particular  we presented several advances that aim to extend
the horizon over which optimal solutions can be found  these advances build off the gmaa 
heuristic search approach and include lossless incremental clustering of the cbgs solved by
gmaa   incremental expansion of nodes in the gmaa  search tree  and hybrid heuristic
representations  we provided theoretical guarantees that  when a suitable heuristic is used 
both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent  finally  we presented extensive empirical results demonstrating
that gmaa  ice can optimally solve dec pomdps of unprecedented size  we significanty
increase the planning horizons that can be tackledin some cases by more than an order of
magnitude  given that an increase of the horizon by one results in an exponentially larger
search space  this constitutes a very large improvement  moreover  our techniques also im   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

prove scalability with respect to the number of agents  leading to the first ever solutions of
general dec pomdps with more than three agents  these results also demonstrated how
optimal techniques can yield new insights about particular dec pomdps  as incremental
clustering revealed properties of broadcastchannel that make it much easier to solve  in
addition to facilitating optimal solutions  we hope these advances will inspire new principled
approximation methods  as incremental clustering has already done  wu et al          and
enable them to be meaningfully benchmarked 

acknowledgments
we thank raghav aras and abdeslam boularias for making their code available to us  research supported in part by afosr muri project  fa               and in part by nwo
catch project               m s  is funded by the fp  marie curie actions individual
fellowship          fp  people      ief  

appendix a  appendix
a   auxiliary algorithms
algorithm    implements the bestjointpolicyandvalue function  which prunes all child
nodes that are not fully specified  algorithm    generates all children of a particular cbg 
algorithm    bestjointpolicyandvalue qexpand    prune fully expanded nodes from a set
of nodes qexpand returning only the best one and its value 
input  qexpand a set of nodes for fully specified joint policies 
output  the best full joint policy in the input set and its value 
   v    
   for q  qexpand do
  
qexpand  remove q 
  
h  vi  q
  
if v   v  then
  
v  v
  
  
  
end if
   end for
    return h    v  i

a   detailed gmaa  ice algorithm
the complete gmaa  ice algorithm is shown in algorithm    
a   computation of v     t   t  
the quantity v     t   t   is defined recursively via 
v     t   t     v     t   t      est    t   r st    t     t       b    t   
   

 a   

fioliehoek  spaan  amato    whiteson

algorithm    generateallchildrenforcbg b t    
input  cbg b t   
output  qexpand the set containing all expanded child nodes for this cbg 
   qexpand    
   for all jointp
cbg policies  for b do
  
vb      pr  u     
  
t     t    t  
 create partial joint policy 
t
t  
    t 
t
b
b
  
v      v
      v    
 compute heuristic value 
  
q   ht     vb  t    i
 create child node 
  
qexpand  insert q   
   end for
   return qexpand

the expectation is taken with respect to the joint probability distribution over states and
joint aohs that is induced by t  
x
pr st    t  b   t    
pr ot  at   st   pr st  st   at    pr at   t    t    pr st     t   b   t   
st  s

 a   
here    t      t   at   ot   and pr at   t    t    is the probability that t specifies at  for
aoh   t   which is   or   in case of deterministic past joint policy t   
a   proofs
proof of theorem  
substituting       in       yields
x
b   t   t    t   
vb      vb   t    
pr   t  b   t  q 
 
t

 

x
 
t



pr   t  b   t   est  r st   t    t        t     e t    vb    t         t    t    t   

  est   t  r st   t    t     b    t     e t    vb    t       b    t    t  

 est   t  r st   t    t     b    t     e t    q    t          t        b    t      t    t   
  est   t  r st   t    t     b    t     h  t     h   t     

where h  is an optimal admissible heuristic  substituting this into       we obtain
vb  t      t    t      v     t   t     est   t  r st   t    t     b    t     e t    vb    t       b    t    t  
 v     t   t     est   t  r st   t    t      b    t       h  t     h   t    

 via  a       v     t  t       h  t     h   t     
which demonstrates that the heuristic value vb  t   used by gmaa  via cbgs using heuristic
of a form       is admissible  as it is lower bounded by the actual value for the first t plus
an admissible heuristic  since it performs heuristic search with this admissible heuristic  this
algorithm is also complete 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

algorithm    gmaa  ice
  
  
  
  
  
  
  
  
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

vgm aa  
     
v   
q    h    vi
lie   q    
repeat
q  select lie  
 q   ht   vi 
ie
l  pop q 
if isplaceholder q  then
b t    t  cbg
 reuse stored cbg 
else
 construct extended bg and solver  
b t     t   cbg
 note t    t     t    
t 
t
t 
b     constructextendedbg b      
b t    clusterbg b t   
b t   solver  createsolver b t   
t  cbg  b t  
end if
 expand a single child  
vcbg   vgm aa  v      t    t  
vcbg    
if last stage t   h    then
vcbg   vb  h     v      h    h   
end if
h t   vb   t  i  b t   solver nextsolution vcbg  vcbg  
if not  t then
 fully expanded  no solution s t  v   h     vcbg  
delete q  and its cbg   solver 
continue
  i e  goto line    
end if
t     t    t  
vb  t      v     t   t     vb   t  
if last stage t   h    then
 note that    t     v      vb  t      
if v      vgm aa then
vgm aa  v   
 found new lower bound 
  
lie  prune vgm aa  
end if
delete q  and its cbg   solver 
else
q   ht     vb  t    i
lie  insert q   
q  ht   vb  t    i
  update parent node q  which now is a placeholder  
lie  insert q 
end if
until lie is empty

   

fioliehoek  spaan  amato    whiteson

proof of lemma  
t
t  
t
t   and  
proof  assume an arbitrary ati  ot  
   i  at  i  ot  
   i     
i      i  s
  i     we have that
t  

  a t t t
pr st     
   i  ot  
i  i  ai     i  
x
t
t
t   t t
t  
   i   ia t  
pr ot  
  pr st    st  ati  at  i   pr at  i   
   i   t  i   pr st   
 
i  o  i  ai  a  i  s
st

 

x

t
t
t   t t
t  
   i   ib t  
  pr st    st  ati  at  i   pr at  i   
   i   t  i   pr st   
pr ot  
i  o  i  ai  a  i  s

st

t  
  b t t t
  pr st     
   i  ot  
i  i  ai     i  
t  

because we assumed an arbitrary st     
   i  ot  
i   we have that
st     t    ot  
  i

i

t  
t     t   t     b t t t
  a t t t
    i  oi  i  ai     i  
pr st     
   i  ot  
i  i  ai     i     pr s

 a   

in general we have that
t  

pr s

t  

t  
t
  
   i   it  ati  ot  
i     i    

 t t t
pr st     
   i  ot  
i  i  ai     i  
pr ot      t  at   t  
i

 

i

i

  i

t  
 t t t
pr st     
   i  ot  
i  i  ai     i  
p
t   t   t t t
t     
 

t   pr s
  i  oi  i  ai     i  
t  
 
s
 
  i

now  because of  a     both the numerator and denominator are the same when substituting
 ia t   ib t in this equation  consequently  we can conclude that
t  
t
t     t     b t t t   t
    i  i  ai  oi     i  
pr st     
   i   ia t  ati  ot  
i     i     pr s
t  

t
t     and  
finally  because ati   ot  
   i were all arbitrarily chosen  we can conclude that
i      i  s
      holds 

proof of lemma  
proof  assume an arbitrary    i  s and    i   then we have
bi  s    i   ia      i     pr s    i   ia      i  b   
x
 
pr s    i   
   i   ia     i  b   
 
   i

 factoring the joint distribution 

 

x

pr s  
   i   ia     i  b    pr    i  s  
   i    ia     i  b   

 
   i

   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

    i only depends on  
   i     i    

x

pr s  
   i   ia     i  b    pr    i   
   i     i  

  s  
   i only depend on   i    

x

pr s  
   i   ia    i  b    pr    i   
   i     i  

 due to pe   

x

pr s  
   i   ib    i  b    pr    i   
   i     i  

 
   i

 
   i

 
   i

          pr s    i   ib     i  b      bi  s    i   ib      i  
we can conclude this holds for all    i  s and    i  
proof of theorem    search equivalence 
to prove search equivalence  we explicitly write a node as a tuple q   ht   v  phi  where t
is the past joint policy  v the nodes heuristic value  and ph a boolean indicating whether
it is a placeholder  we consider the equivalence of the maintained open lists  the open list
l maintained by gmaa  ic contains only non expanded nodes q  in contrast  the open
list lie of gmaa  ice contains both non expanded nodes q and placeholders  previously
expanded nodes   q  we denote the ordered subset of lie containing non expanded nodes
with q and that containing placeholders with q  we treat these open lists as ordered sets of
heuristic values and their associated nodes 
definition     l and lie are equivalent  l  lie if 
   q  l 
   the qs have the same ordering  l remove l   q    q   
   nodes q in l but not q have a placeholder q that is the parent of and higher ranked
than q 
q ht  vq  falsei l q 

q ht   vq  trueiq s t   t    t       q   q  

   there are no other placeholders 
fig     illustrates two equivalent lists in which the past joint policies are indexed with letters 
note that the placeholders in lie are ranked higher than the nodes in l that they represent 
let us write it ic l  and it ice lie   for one iteration  i e   one loop of the main repeat
in algorithm    of the respective algorithms  let it ice  denote the operation that repeats
it ice as long as a placeholder was selected  so it ends when a q is expanded  
lemma    if l  lie   then executing it ic l  and it ice  lie   leads to new open lists
that are again equivalent  l   lie  
proof  when it ice  selects a placeholder q  it generates child q  that was already present
in l  due to properties   and   of definition     and inserts it  insertion occurs at the
same relative location as it ic because both algorithms use the same comparison operator
 definition     together these facts guarantee that the insertion preserves properties   and   
    a remove b  removes the elements of b from a without changing as ordering 

   

fioliehoek  spaan  amato    whiteson

lie

l
q
vb

t

 
 
   

c
d
e

 
 
   
 
   

f
g
h
i
j

q

vb

t

 

d

 
 

f
g

vb
 

 

t
a

b



placeholder for  c e j 



same nodes  same position


o

placeholder for  h i 
consistent ordering
for equal values

figure     illustration of equivalent lists  past joint policies are indexed by letters  in this
example  a and b have been expanded earlier  but are not yet fully expanded in the ice case  
if there are remaining unexpanded children of q  it ice  reinserts q with an updated heuristic
value q v  q   v that is guaranteed to be an upper bound on the value of unexpanded siblings
q  since q   v   vb  q      vb  q       q   v  preserving properties   and    
when it ice  finally selects a non placeholder q  it is guaranteed to be the same q as
selected by it ic  due to properties   and     expansion in ice generates one child q   again
inserted at the same relative location as in ic  and inserts placeholder q   hq   q   v  truei for
the other siblings q   again preserving properties   and    
proof of theorem    the fact that gmaa  ice and gmaa  ic are search equivalent follows directly from lemma    search equivalence means that both algorithms select the same
non placeholders q to expand  since both algorithms begin with identical  and therefore trivially equivalent  open lists  they maintain equivalent open lists throughout search  as such 
property   of definition    ensures that every time it ice  selects a non placeholder  it ic
selects it too 

references
allen  m     zilberstein  s          agent influence as a predictor of difficulty for decentralized
problem solving  in proceedings of the twenty second aaai conference on artificial
intelligence 
amato  c   bernstein  d  s     zilberstein  s          optimal fixed size controllers for
decentralized pomdps  in proc  of the aamas workshop on multi agent sequential
decision making in uncertain domains 
amato  c   bernstein  d  s     zilberstein  s          optimizing memory bounded controllers
for decentralized pomdps  in proc  of uncertainty in artificial intelligence 
amato  c   bernstein  d  s     zilberstein  s          optimizing fixed size stochastic controllers for pomdps and decentralized pomdps  autonomous agents and multi agent
systems                 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

amato  c   bonet  b     zilberstein  s          finite state controllers based on mealy
machines for centralized and decentralized pomdps  in proceedings of the twentyfourth aaai conference on artificial intelligence 
amato  c   carlin  a     zilberstein  s          bounded dynamic programming for decentralized pomdps  in proc  of the aamas workshop on multi agent sequential
decision making in uncertain domains 
amato  c   dibangoye  j  s     zilberstein  s          incremental policy generation for
finite horizon dec pomdps  in proc  of the international conference on automated
planning and scheduling 
aras  r     dutech  a          an investigation into mathematical programming for finite
horizon decentralized pomdps  journal of artificial intelligence research          
    
becker  r   carlin  a   lesser  v     zilberstein  s          analyzing myopic approaches for
multi agent communication  computational intelligence               
becker  r   zilberstein  s     lesser  v          decentralized markov decision processes with
event driven interactions  in proc  of the international conference on autonomous
agents and multi agent systems 
becker  r   zilberstein  s   lesser  v     goldman  c  v          transition independent
decentralized markov decision processes  in proc  of the international conference on
autonomous agents and multi agent systems 
bernstein  d  s   amato  c   hansen  e  a     zilberstein  s          policy iteration for
decentralized control of markov decision processes  journal of artificial intelligence
research             
bernstein  d  s   givan  r   immerman  n     zilberstein  s          the complexity of
decentralized control of markov decision processes  mathematics of operations research 
               
bertsekas  d  p          dynamic programming and optimal control   rd ed   vol  i   athena
scientific 
beynier  a     mouaddib  a  i          solving efficiently decentralized mdps with temporal
and resource constraints  autonomous agents and multi agent systems             
    
boularias  a     chaib draa  b          exact dynamic programming for decentralized
pomdps with lossless policy compression  in proc  of the international conference on
automated planning and scheduling 
busoniu  l   babuska  r     de schutter  b          a comprehensive survey of multi agent
reinforcement learning  ieee transactions on systems  man  and cybernetics  part c 
applications and reviews                 
carlin  a     zilberstein  s          value based observation compression for dec pomdps 
in proc  of the international conference on autonomous agents and multi agent systems 
   

fioliehoek  spaan  amato    whiteson

carrillo  h     lipman  d          the multiple sequence alignment problem in biology 
siam journal on applied mathematics                   
cassandra  a   littman  m  l     zhang  n  l          incremental pruning  a simple  fast 
exact method for partially observable markov decision processes  in proc  of uncertainty
in artificial intelligence 
cassandra  a  r          exact and approximate algorithms for partially observable markov
decision processes  unpublished doctoral dissertation  brown university 
dechter  r   flerova  n     marinescu  r          search algorithms for m best solutions for
graphical models  in proceedings of the twenty sixth aaai conference on artificial
intelligence 
dekel  e   fudenberg  d     morris  s          topologies on types  theoretical economics 
              
dibangoye  j  s   amato  c   doniec  a     charpillet  f          producing efficient errorbounded solutions for transition independent decentralized mdps  in proc  of the international conference on autonomous agents and multi agent systems   submitted
for publication 
dibangoye  j  s   mouaddib  a  i     chai draa  b          point based incremental pruning heuristic for solving finite horizon dec pomdps  in proc  of the international
conference on autonomous agents and multi agent systems 
doshi  p     gmytrasiewicz  p          monte carlo sampling methods for approximating
interactive pomdps  journal of artificial intelligence research              
doshi  p   zeng  y     chen  q          graphical models for interactive pomdps  representations and solutions  autonomous agents and multi agent systems                 
edelkamp  s     schrodl  s          heuristic search  theory and applications  morgan
kaufmann 
eker  b     akn  h  l          using evolution strategies to solve dec pomdp problems 
soft computinga fusion of foundations  methodologies and applications            
   
eker  b     akn  h  l          solving decentralized pomdp problems using genetic
algorithms  autonomous agents and multi agent systems                 
emery montemerlo  r          game theoretic control for robot teams  unpublished
doctoral dissertation  carnegie mellon university 
emery montemerlo  r   gordon  g   schneider  j     thrun  s          approximate solutions for partially observable stochastic games with common payoffs  in proc  of the
international conference on autonomous agents and multi agent systems 
emery montemerlo  r   gordon  g   schneider  j     thrun  s          game theoretic
control for robot teams  in proc  of the ieee international conference on robotics and
automation 
givan  r   dean  t     greig  m          equivalence notions and model minimization in
markov decision processes  artificial intelligence                  
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

gmytrasiewicz  p  j     doshi  p          a framework for sequential planning in multi agent
settings  journal of artificial intelligence research            
goldman  c  v   allen  m     zilberstein  s          learning to communicate in a decentralized environment  autonomous agents and multi agent systems               
goldman  c  v     zilberstein  s          optimizing information exchange in cooperative
multi agent systems  in proc  of the international conference on autonomous agents
and multi agent systems 
goldman  c  v     zilberstein  s          decentralized control of cooperative systems 
categorization and complexity analysis  journal of artificial intelligence research      
       
goldman  c  v     zilberstein  s          communication based decomposition mechanisms
for decentralized mdps  journal of artificial intelligence research              
hansen  e  a   bernstein  d  s     zilberstein  s          dynamic programming for partially observable stochastic games  in proc  of the national conference on artificial
intelligence 
hauskrecht  m          value function approximations for partially observable markov decision processes  journal of artificial intelligence research            
hsu  k     marcus  s          decentralized control of finite state markov processes  ieee
transactions on automatic control                  
huhns  m  n   ed            distributed artificial intelligence  pitman publishing ltd 
ikeda  t     imai  h          enhanced a  algorithms for multiple alignments  optimal
alignments for several sequences and k opt approximate alignments for large cases  theoretical computer science                  
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in partially
observable stochastic domains  artificial intelligence                   
kumar  a     zilberstein  s          constraint based dynamic programming for decentralized
pomdps with structured interactions  in proc  of the international conference on
autonomous agents and multi agent systems 
kumar  a     zilberstein  s       a   anytime planning for decentralized pomdps using
expectation maximization  in proc  of uncertainty in artificial intelligence 
kumar  a     zilberstein  s       b   point based backup for decentralized pomdps  complexity and new algorithms  in proc  of the international conference on autonomous
agents and multi agent systems 
littman  m   cassandra  a     kaelbling  l          learning policies for partially observable environments  scaling up  in proc  of the international conference on machine
learning 
marecki  j   gupta  t   varakantham  p   tambe  m     yokoo  m          not all agents are
equal  scaling up distributed pomdps for agent networks  in proc  of the international
conference on autonomous agents and multi agent systems 
   

fioliehoek  spaan  amato    whiteson

marecki  j     tambe  m          on opportunistic techniques for solving decentralized
markov decision processes with temporal constraints  in proc  of the international
conference on autonomous agents and multi agent systems 
melo  f  s     veloso  m          decentralized mdps with sparse interactions  artificial
intelligence                     
mostafa  h     lesser  v          a compact mathematical formulation for problems with
structured agent interactions  in proc  of the aamas workshop on multi agent sequential decision making in uncertain domains 
nair  r   roth  m     yohoo  m          communication for improving policy computation in
distributed pomdps  in proc  of the international conference on autonomous agents
and multi agent systems 
nair  r   tambe  m   yokoo  m   pynadath  d  v     marsella  s          taming decentralized pomdps  towards efficient policy computation for multiagent settings  in proc 
of the international joint conference on artificial intelligence 
nair  r   varakantham  p   tambe  m     yokoo  m          networked distributed pomdps 
a synthesis of distributed constraint optimization and pomdps  in proc  of the national conference on artificial intelligence 
oliehoek  f  a          value based planning for teams of agents in stochastic partially observable environments  amsterdam university press   doctoral dissertation  university
of amsterdam 
oliehoek  f  a          decentralized pomdps  in m  wiering   m  van otterlo  eds   
reinforcement learning  state of the art  vol       springer berlin heidelberg 
oliehoek  f  a   kooi  j  f     vlassis  n          the cross entropy method for policy search
in decentralized pomdps  informatica              
oliehoek  f  a     spaan  m  t  j          tree based solution methods for multiagent
pomdps with delayed communication  in proceedings of the twenty sixth aaai conference on artificial intelligence 
oliehoek  f  a   spaan  m  t  j   dibangoye  j     amato  c          heuristic search for identical payoff bayesian games  in proc  of the international conference on autonomous
agents and multi agent systems 
oliehoek  f  a   spaan  m  t  j     vlassis  n          dec pomdps with delayed communication  in proc  of the aamas workshop on multi agent sequential decision making
in uncertain domains 
oliehoek  f  a   spaan  m  t  j     vlassis  n          optimal and approximate q value
functions for decentralized pomdps  journal of artificial intelligence research      
       
oliehoek  f  a   spaan  m  t  j   whiteson  s     vlassis  n          exploiting locality
of interaction in factored dec pomdps  in proc  of the international conference on
autonomous agents and multi agent systems 
oliehoek  f  a     vlassis  n          q value functions for decentralized pomdps  in proc 
of the international conference on autonomous agents and multi agent systems 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

oliehoek  f  a   whiteson  s     spaan  m  t  j          lossless clustering of histories
in decentralized pomdps  in proc  of the international conference on autonomous
agents and multi agent systems 
oliehoek  f  a   whiteson  s     spaan  m  t  j          approximate solutions for factored dec pomdps with many agents  in proc  of the international conference on
autonomous agents and multi agent systems   submitted for publication 
oliehoek  f  a   witwicki  s     kaelbling  l  p          influence based abstraction for
multiagent systems  in proceedings of the twenty sixth aaai conference on artificial
intelligence 
ooi  j  m     wornell  g  w          decentralized control of a multiple access broadcast
channel  performance bounds  in proc  of the   th conference on decision and control 
osborne  m  j     rubinstein  a          a course in game theory  the mit press 
pajarinen  j     peltonen  j          efficient planning for factored infinite horizon decpomdps  in proc  of the international joint conference on artificial intelligence 
panait  l     luke  s          cooperative multi agent learning  the state of the art 
autonomous agents and multi agent systems                 
puterman  m  l          markov decision processesdiscrete stochastic dynamic programming  john wiley   sons  inc 
pynadath  d  v     marsella  s  c          minimal mental models  in proceedings of the
twenty second aaai conference on artificial intelligence 
pynadath  d  v     tambe  m          the communicative multiagent team decision problem 
analyzing teamwork theories and models  journal of artificial intelligence research 
            
rabinovich  z   goldman  c  v     rosenschein  j  s          the complexity of multiagent
systems  the price of silence  in proc  of the international conference on autonomous
agents and multi agent systems 
rabinovich  z     rosenschein  j  s          multiagent coordination by extended markov
tracking  in proc  of the international conference on autonomous agents and multi
agent systems 
ross  s   pineau  j   paquet  s     chaib draa  b          online planning algorithms for
pomdps  journal of artificial intelligence research              
roth  m   simmons  r     veloso  m          reasoning about joint beliefs for executiontime communication decisions  in proc  of the international conference on autonomous
agents and multi agent systems 
roth  m   simmons  r     veloso  m          exploiting factored representations for decentralized execution in multi agent teams  in proc  of the international conference on
autonomous agents and multi agent systems 
seuken  s     zilberstein  s       a   improved memory bounded dynamic programming for
decentralized pomdps  in proc  of uncertainty in artificial intelligence 
   

fioliehoek  spaan  amato    whiteson

seuken  s     zilberstein  s       b   memory bounded dynamic programming for decpomdps  in proc  of the international joint conference on artificial intelligence 
seuken  s     zilberstein  s          formal models and algorithms for decentralized decision
making under uncertainty  autonomous agents and multi agent systems             
    
spaan  m  t  j   gordon  g  j     vlassis  n          decentralized planning under uncertainty for teams of communicating agents  in proc  of the international conference on
autonomous agents and multi agent systems 
spaan  m  t  j     melo  f  s          interaction driven markov games for decentralized
multiagent planning under uncertainty  in proc  of the international conference on
autonomous agents and multi agent systems 
spaan  m  t  j     oliehoek  f  a          the multiagent decision process toolbox 
software for decision theoretic planning in multiagent systems  in proc  of the aamas
workshop on multi agent sequential decision making in uncertain domains 
spaan  m  t  j   oliehoek  f  a     amato  c          scaling up optimal heuristic search in
dec pomdps via incremental expansion  in proc  of the international joint conference
on artificial intelligence 
spaan  m  t  j   oliehoek  f  a     vlassis  n          multiagent planning under uncertainty
with stochastic communication delays  in proc  of the international conference on
automated planning and scheduling 
sycara  k  p          multiagent systems  ai magazine               
szer  d   charpillet  f     zilberstein  s          maa   a heuristic search algorithm for
solving decentralized pomdps  in proc  of uncertainty in artificial intelligence 
tsitsiklis  j     athans  m          on the complexity of decentralized decision making and
detection problems  ieee transactions on automatic control                  
varaiya  p     walrand  j          on delayed sharing patterns  ieee transactions on
automatic control                  
varakantham  p   kwak  j  young  taylor  m  e   marecki  j   scerri  p     tambe  m         
exploiting coordination locales in distributed pomdps via social model shaping  in
proc  of the international conference on automated planning and scheduling 
varakantham  p   marecki  j   yabu  y   tambe  m     yokoo  m          letting loose a
spider on a network of pomdps  generating quality guaranteed policies  in proc 
of the international conference on autonomous agents and multi agent systems 
varakantham  p   nair  r   tambe  m     yokoo  m          winning back the cup for distributed pomdps  planning over continuous belief spaces  in proc  of the international
conference on autonomous agents and multi agent systems 
velagapudi  p   varakantham  p   scerri  p     sycara  k          distributed model shaping
for scaling to decentralized pomdps with hundreds of agents  in proc  of the international conference on autonomous agents and multi agent systems 
vlassis  n          a concise introduction to multiagent systems and distributed artificial
intelligence  morgan   claypool publishers 
   

fiincremental clustering and expansion for faster optimal planning in dec pomdps

williamson  s  a   gerding  e  h     jennings  n  r          reward shaping for valuing communications during multi agent coordination  in proc  of the international conference
on autonomous agents and multi agent systems 
witwicki  s  j          abstracting influences for efficient multiagent coordination under
uncertainty  unpublished doctoral dissertation  university of michigan  ann arbor 
michigan  usa 
witwicki  s  j     durfee  e  h          influence based policy abstraction for weakly coupled
dec pomdps  in proc  of the international conference on automated planning and
scheduling 
wu  f   zilberstein  s     chen  x       a   point based policy generation for decentralized
pomdps  in proc  of the international conference on autonomous agents and multi
agent systems 
wu  f   zilberstein  s     chen  x       b   rollout sampling policy iteration for decentralized
pomdps  in proc  of uncertainty in artificial intelligence 
wu  f   zilberstein  s     chen  x          online planning for multi agent systems with
bounded communication  artificial intelligence                  
xuan  p   lesser  v     zilberstein  s          communication decisions in multi agent cooperation  model and experiments  in proc  of the international conference on autonomous
agents 
yoshizumi  t   miura  t     ishida  t          a  with partial expansion for large branching
factor problems  in proc  of the national conference on artificial intelligence 
zeng  y     doshi  p          exploiting model equivalences for solving interactive dynamic
influence diagrams  journal of artificial intelligence research              
zeng  y   doshi  p   pan  y   mao  h   chandrasekaran  m     luo  j          utilizing
partial policies for identifying equivalence of behavioral models  in proceedings of the
twenty fifth aaai conference on artificial intelligence 

   

fi