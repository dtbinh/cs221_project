journal artificial intelligence research                 

submitted        published      

risk sensitive reinforcement learning applied control
constraints
peter geibel

pgeibel uos de

institute cognitive science  ai group
university osnabruck  germany

fritz wysotzki

wysotzki cs tu berlin de

faculty electrical engineering computer science  ai group
tu berlin  germany

abstract
paper  consider markov decision processes  mdps  error states  error
states states entering undesirable dangerous  define risk
respect policy probability entering state policy
pursued  consider problem finding good policies whose risk smaller
user specified threshold  formalize constrained mdp two criteria 
first criterion corresponds value function originally given  show
risk formulated second criterion function based cumulative return 
whose definition independent original value function  present model free 
heuristic reinforcement learning algorithm aims finding good deterministic policies 
based weighting original value function risk  weight parameter
adapted order find feasible solution constrained problem good
performance respect value function  algorithm successfully applied
control feed tank stochastic inflows lies upstream distillation
column  control task originally formulated optimal control problem
chance constraints  solved certain assumptions model obtain
optimal solution  power learning algorithm used even
restrictive assumptions relaxed 

   introduction
reinforcement learning  research area  provides range techniques applicable difficult nonlinear stochastic control problems  see e g  sutton   barto       
bertsekas   tsitsiklis         reinforcement learning  rl  agent considered
learns control process  agent able perceive state process 
acts order maximize cumulative return based real valued reward
signal  often  experiences process used improve agents policy instead
previously given analytical model 
notion risk rl related fact  even optimal policy may perform
poorly cases due stochastic nature problem  risk sensitive rl
approaches concerned variance return  worst outcomes 
 e g  coraluppi   marcus        heger        neuneier   mihatsch         see
discussion section    take alternative view risk defined geibel       
concerned variability return  occurrence errors
c
    
ai access foundation  rights reserved 

figeibel   wysotzki

undesirable states underlying markov decision process  mdp   means
address different class problems compared approaches referring variability
return 
paper  consider constrained mdps two criteria usual value function risk second value function  value optimized risk
must remain specified threshold  describe heuristic algorithm based
weighted formulation finds feasible policy original constrained problem 
order offer insight behavior algorithm  investigate application algorithm simple grid world problem discounted criterion function 
apply algorithm stochastic optimal control problem continuous states 
set feasible solutions restricted constraint required hold
certain probability  thus demonstrating practical applicability approach 
consider control feed tank lies upstream distillation column respect
two objectives      outflow tank required stay close specified value
order ensure optimal operation distillation column      tank level
substance concentrations required remain within specified intervals  certain
admissible chance constraint violation 
li  wendt  arellano garcia  wozny        formulate problem quadratic
program chance constraints   e g  kall   wallace         relaxed nonlinear
program case gaussian distributions random input variables systems
whose dynamics given linear equations  nonlinear program solved
sequential quadratic programming 
note approach li et al  involves simulation based estimation
gradients chance constraints  li et al         p         q learning  watkins 
      watkins   dayan        sutton   barto         learning algorithm based
simulating episodes estimating value risk states  tank control task
correspond measure deviation optimal outflow probability
constraint violation  respectively 
contrast approach li et al          rl algorithm applicable systems
continuous state spaces  whose system dynamics governed nonlinear equations
involve randomization noise arbitrary distributions random variables 
makes prior assumptions either aspect  special property
learning algorithm  holds true e g  q learning rl algorithms 
convergence q learning combined function approximation techniques necessary
continuous state spaces cannot guaranteed general  e g  sutton   barto        
holds true algorithm  nevertheless  rl algorithms successfully applied
many difficult problems continuous state spaces nonlinear dynamics  see e g 
sutton   barto        crites   barto        smart   kaelbling        stephan  debes 
gross  wintrich    wintrich        
   constraint seen relation domains variables restricting possible values 
variables constraint c   c x            xn   random  constraint hold certain
probability  chance constrained programming particular approach stochastic programming
considers constrained optimization problems containing random variables so called chance
constraints form p c  p p        formulated 

  

firisk sensitive reinforcement learning

article organized follows  section    rl framework described  section   reviews related work risk sensitive approaches  section   describes approach
risk sensitive rl  section    elucidate heuristic learning algorithm solving
constrained problem using weighted formulation  section    describe application
grid world problem  tank control task described section    section   
experiments feed tank control described  section   concludes short
summary outlook 

   rl framework
rl one considers agent interacts process controlled 
discrete time step  agent observes state x takes action u general
depends x  action agent causes environment change state x
according probability px u  x    section    consider set states  x 
finite set 
action set agent assumed finite  allowed depend
current state  state x  agent uses action set u  x  possible actions 
taking action u u  x   agent receives real valued reinforcement signal rx u  x  
depends action taken successor state x   case random reward
signal  rx u  x   corresponds expected value  markov property mdp requires
probability distribution successor states one rewards depend
current state action only  distributions change additional
information past states  actions rewards considered  i e  independent
path leading current state 
aim agent find policy selecting actions maximizes
cumulative reward  called return  return defined
r 


x

rt  

   

t  

random variable rt denotes reward occurring t th time step
agent uses policy   let x    x    x          denote corresponding probabilistic sequence
states  ui sequence actions chosen according policy  
constant        discount factor allows control influence future
rewards  expectation return 
h



v  x    e r   x    x  

   

defined value x respect   well known exist stationary

deterministic policies v  x  optimal  maximal  every state x  stationary deterministic policy function maps states actions particularly defined
independent time markovian  independent history   work 
use term maximum value policies instead optimal policies distinguish
minimum risk policies optimal sense  see section     
usual  define state action value function



h



q  x  u    e r    v  x    x    x  u    u  
  

   

figeibel   wysotzki

q  x  u  expected return agent first chooses action u  acts according
subsequent time steps  optimal q function q   optimal policies
unique optimal values v derived  x  argmaxu q  x  u  v  x    q  x   x   
q computed using watkins q learning algorithm 
rl one general distinguishes episodic continuing tasks treated
framework  see e g  sutton   barto         episodic tasks  agent may
reach terminal absorbing state time   reaching absorbing state 
agent stays executes dummy action  reward defined rt    
  learning agent restarted according distribution
initial states reached absorbing state 

   related work
p


random variable r  
t   rt  return  used define value state possesses
certain variance  risk averse approaches dynamic programming  dp  reinforcement learning concerned variance r  worst outcomes 
example approach worst case control  e g  coraluppi   marcus        heger 
       worst possible outcome r optimized  risk sensitive control based use exponential utility functions  e g  liu  goodwin    koenig      a 
koenig   simmons        liu  goodwin    koenig      b  borkar         return r
transformed reflect subjective measure utility  instead maximizing
expected value r  objective maximize e g  u     log e er   
parameter r usual return  shown depending parameter
  policies high variance v r  penalized        enforced         value criterion introduced heger        seen extension worst case control
bad outcomes policy occur probability less neglected 
neuneier mihatsch        give model  free rl algorithm based
parameterized transformation temporal difference errors occurring  see mihatsch
  neuneier         parameter transformation allows switch riskaverse risk seeking policies  influence parameter value function cannot
expressed explicitly 
view risk concerned variance return worst possible
outcomes  instead fact processes generally possess dangerous undesirable states  think chemical plant temperature pressure exceeding
threshold may cause plant explode  controlling plant  return corresponds plants yield  seems inappropriate let return reflect
cost explosion  e g  human lives affected 
work  consider processes undesirable terminal states  seemingly straightforward way handle error states system provide high
negative rewards systems enters error state  optimal policy avoid
error states general  drawback approach fact unknown
large risk  probability  entering error state is  moreover  may want provide
threshold probability entering error state must exceeded
agents policy  general  impossible completely avoid error states  risk
controllable extend  precisely  agent placed state

  

firisk sensitive reinforcement learning

x  follow policy whose risk constrained   parameter       
reflects agents risk averseness  is  goal minimization risk 
maximization v risk kept threshold  
markowitz        considers combination different criteria equal discount
factors context portfolio selection  risk selected portfolio related
variance combined  weighted  criteria  markowitz introduces notion
 e  v   space  notion risk related variance v   depends
occurrence error states mdp  therefore risk conceptually independent v  
see e g  tank control problem described section   
idea weighting return risk  markowitz        freund        heger        leads
expected value minus variance criterion  e r  kv r   k parameter 
use idea computing feasible policy problem finding good policy
constrained risk  in regard probability entering error state   value
risk weighted using weight value weight   risk  value
increased  giving value weight compared risk  risk state
becomes larger user specified threshold  
considering ordering relation tuples values  learning algorithm
fixed value related artdp approach gabor  kalmar  szepesvari
        article  gabor et al  additionally propose recursive formulation
mdp constraints may produce suboptimal solutions  applicable
case approach requires nonnegative reward function 
noted aforementioned approaches based variability
return suited problems grid world problem discussed section   
tank control task section   risk related parameters  variables  state
description  example  grid world problem  policies worst case
outcome  regard approaches based variance  found policy leading
error states fast possible higher variance one reaches
goal states fast possible  policy small variance therefore large risk
 with respect probability entering error state   means address
different class control problems  underpin claim section       
fulkerson  littman  keim        sketch approach framework probabilistic planning similar although based complementary notion
safety  fulkerson et al  define safety probability reaching goal state  see
buridan system kushmerick  hanks    weld         fulkerson et al  discuss
problem finding plan minimum cost subject constraint safety  see
blythe         episodic mdp goal states  safety   minus risk 
continuing tasks absorbing states neither goal error states 
safety may correspond smaller value  fulkerson et al         manipulate  scale 
 uniform  step reward undiscounted cost model order enforce agent reach
goal quickly  see koenig   simmons         contrast  consider
discounted mdps  neither require existence goal states  although
change original reward function  algorithm section   seen systematic
approach dealing idea fulkerson et al  consists modification
relative importance original objective  reaching goal  safety  contrast
aforementioned approaches belonging field probabilistic planning 
  

figeibel   wysotzki

operate previously known finite mdp  designed online learning algorithm
uses simulated actual experiences process  use neural network
techniques algorithm applied continuous state processes 
dolgov durfee        describe approach computes policies
constrained probability violating given resource constraints  notion risk
similar described geibel         algorithm given dolgov durfee
       computes suboptimal policies using linear programming techniques require
previously known model and  contrast approach  cannot easily extended
continuous state spaces  dolgov durfee included discussion dp approaches
constrained mdps  e g  altman        generalize continuous state
spaces  as tank control task  require known model  algorithm described
feinberg shwartz        constrained problems two criteria applicable
case  requires discount factors strictly smaller   
limited finite mdps 
downside risk common notion finance refers likelihood security
investment declining price  amount loss could result
potential decline  scientific literature downside risk  e g  bawas        fishburn 
      markowitz        roy        investigates risk measures particularly consider
case return lower mean value  target value encountered 
contrast  notion risk coupled return r  fact state
x error state  example  parameters describing state lie outside
permissible ranges  state lies inside obstacle may occur
robotics applications 

   risk
define notion risk precisely  consider set
x

   

error states  error states terminal states  means control agent
ends reaches state   allow additional set non error terminal states
   
now  define risk x respect probability state sequence
 xi  i  x    x  generated executing policy   terminates error state
x  
definition      risk  let policy  let x state  risk defined




 x    p xi   x    x  

   

definition   x      holds x   x    x         
states     risk depends action choices policy  
following subsection  consider computation minimum risk policies
analogous computation maximum value policies 
  

firisk sensitive reinforcement learning

    risk minimization
risk considered value function defined cost signal r  see this 
augment state space mdp additional absorbing state
agent transfered reaching state   state introduced technical
reasons 
agent reaches state   reward signals r r become zero 
set r     r      agent reaches error state  states
longer absorbing states  new cost function r defined


rx u  x    

 

  x x  
  else 

   

construction cost function r  episode states  actions costs
starting initial state x contains exactly cost r     error state
occurs it  process enter error state  sequence r costs contains
zeros only  therefore  probability defining risk expressed expectation
cumulative return 
proposition     holds


 x    e

 
x
i  

discount factor     

 


ri x    x


   

proof  r    r          probabilistic sequence costs related risk  stated
p

above  holds r  def
i   ri     trajectory leads error state  otherwise
p
i   ri      means return r bernoulli random variable 
probability q r     corresponds risk x respect   bernoulli random
variable holds er   q  see e g  ross         notice introduction together
fact r     occurs transition error state
iwhen

hp  



entering respective error state  ensures correct value e
i   ri x    x
error states x  q e d 
similar q function define state action risk
h

q  x  u    e r     x      x    x  u    u
 

x





px u  x   rx u  x      x    

x



   
   

minimum risk policies obtained variant q learning algorithm  geibel 
      
    maximized value  constrained risk
general  one interested policies minimum risk  instead  want provide
parameter specifies risk willing accept  let x x set
states interested in  e g  x   x       x    x    distinguished
  

figeibel   wysotzki

starting state x    state x x   let px probability selecting starting
state  value
x
v  def
px v  x 
    
xx

corresponds performance states x   consider constrained problem
max v

    

x x    x   

    



subject
policy fulfills      called feasible  depending   set feasible policies
may empty  optimal policies generally depend starting state  nonstationary randomized  feinberg   shwartz        gabor et al         geibel        
restrict considered policy class stationary deterministic policies  constrained
problem generally well defined x singleton  need
stationary deterministic policy optimal states x   feinberg shwartz
       shown case two unequal discount factors smaller  
exist optimal policies randomized markovian time step n  i e 
depend history  may non stationary randomized   stationary
deterministic  particularly markovian  time step n onwards  feinberg shwartz
       give dp algorithm case  cp  feinberg   shwartz         cannot
applied case      generalize continuous state
spaces  case equal discount factors  shown feinberg shwartz       
 for fixed starting state  exist optimal stationary randomized policies
case one constraint consider one action stationary deterministic
policy  i e  one state policy chooses randomly two
actions 

   learning algorithm
reasons efficiency predictability agents behavior
said end last section  restrict consideration stationary deterministic policies  following present heuristic algorithm aims
computing good policy  assume reader familiar watkins q learning
algorithm  watkins        watkins   dayan        sutton   barto        
    weighting risk value
define new  third  value function v state action value function q
weighted sum risk value
v  x    v  x   x 
q  x  u 





  q  x  u  q  x  u   

    
    

parameter   determines influence v  values  q  values  compared
 values  q  values        v corresponds negative   means
  

firisk sensitive reinforcement learning

maximization v  lead minimization     maximization
v leads lexicographically optimal policy unconstrained  unweighted   criteria
problem  one compares performance two policies lexicographically  criteria
ordered  large values   original value function multiplied dominates
weighted criterion 
weight successively adapted starting      see section      adaptation
  discuss learning fixed proceeds 
    learning fixed
fixed value   learning algorithm computes optimal policy using
algorithm resembles q learning based artdp approach gabor
et al         
learning  agent estimates qt   qt time    thus estimate
qt performance current greedy policy  policy selects best
action respect current estimate qt   values updated using example
state transitions  let x current state  u chosen action  x observed
successor state  reward risk signal example state transition given
r r respectively  x   greedy action defined following manner  action
u preferable u qt  x   u    qt  x   u   holds  equality holds  action
higher qt  value preferred  write u u   u preferable u  
let u greedy action x respect ordering   agents
estimates updated according
qt    x  u        qt  x  u     r   qt  x   u   

    

qt    x  u        qt  x  u     r   qt  x   u   

    

t  
qt  
 x  u  qt    x  u 
 x  u    q

    

every time new chosen  learning rate set    afterwards decreases
time  cp  sutton   barto        
fixed   algorithm aims computing good stationary deterministic policy
weighted formulation feasible original constrained problem  existence
optimal stationary deterministic policy weighted problem convergence
learning algorithm guaranteed criteria discount factor  i e 
    even      case     q forms standard criterion function
rewards r r  consider risk second criterion function    implies
       ensure convergence case required either  a 
exists least one proper policy  defined policy reaches absorbing state
probability one   improper policies yield infinite costs  see tsitsiklis          b  
policies proper  case application example  conjecture
case   convergence possibly suboptimal policy guaranteed mdp forms
directed acyclic graph  dag   cases oscillations non convergence may occur 
optimal policies weighted problem generally found considered
policy class stationary deterministic policies  as constrained problem  
  

figeibel   wysotzki

    adaptation
learning starts  agent chooses     performs learning steps lead 
time  approximated minimum risk policy     policy allows agent
determine constrained problem feasible 
afterwards value increased step step risk state x becomes
larger   increasing increases influence q values compared
q values  may cause agent select actions result higher value 
perhaps higher risk  increasing   agent performs learning
steps greedy policy sufficiently stable  aimed producing optimal
deterministic policy   computed q  q values old  i e  estimates




 
q q   used initialization computing  
aim increasing give value function v maximum influence possible 
means value maximized  needs chosen user 
adaptation provides means searching space feasible policies 

    using discounted risk
order prevent oscillations algorithm section     case     may
advisable set   corresponding using discounted risk defined
 x 

 e

 
x
i  

 


ri x    x  


    

values ri positive  holds  x   x  states x  discounted risk  x  gives weight error states occurring near future  depending
value  
finite mdp fixed   convergence algorithm optimal stationary
policy weighted formulation guaranteed q  using  x   forms
standard criterion function rewards r r  terminating adaptation
case risk state x becomes larger   one might still use original
 undiscounted  risk  x  learning done discounted version  x   i e 
learning algorithm maintain two risk estimates every state  major
problem  notice case     effect considering weighted criterion
v corresponds modifying unscaled original reward function r adding
negative reward   agent enters error state  set optimal stationary
deterministic policies equal cases  where added absorbing state
single dummy action neglected  
section    experiments case         x   x        finite
state space found  sections     consider application example
infinite state space  x    x           

   grid world experiment
following study behaviour learning algorithm finite mdp
discounted criterion  contrast continuous state case discussed next
  

firisk sensitive reinforcement learning

e
e
e
a 
e
e g
e e
e
e
e
c 
e
e g
e e

e g
e
e
b 
e
e g
e e e e e e
e g
e
e
d 
e
e g
e e e e e e

g

e





e

e





e

e e
g




e e

figure    a  example grid world  x   horizontal    vertical  explanation see
text  b  minimum risk policy           unsafe states  c  maximum value
policy             unsafe states  d  result algorithm  policy       
   unsafe states 

section  function approximation neural networks needed value
function risk stored table  grid world  chosen  
      x   x   state graph dag  implies
stationary policy optimal every state x   although oscillations therefore
expected  found algorithm stabilizes feasible policy
learning rate tends zero  investigated use discounted risk
prevents oscillatory behaviour 
consider     grid world depicted figure   a   empty field denotes
state  es denote error states  two gs denote two goal states  describe
states pairs  x  y  x                      i e                                    
                                                                             additional absorbing
state depicted 
chosen error states lower  i e  extremal values x
dangerous  one goal states placed next error states  safer
part state space 
agent actions u             action u u takes agent
denoted direction possible  probability       agent transported
desired direction one three remaining directions 
agent receives reward   enters goal state  agent receives reward
  every case  noted explicit punishment entering
error state  implicit one  agent enters error state  current
episode ends  means agent never receive positive reward
reached error state  therefore  try reach one goal states 
     try fast possible 
  

figeibel   wysotzki

chosen x   x               equal probabilities px states 
although convergence algorithm cannot guaranteed case  experimental results show algorithm yields feasible policy 
selected         order illustrate behaviour algorithm
computed minimum risk maximum value policy  figure   b  shows
minimum risk policy  though reward function r defined plays role
minimum risk policy  agent tries reach one two goal states 
goal state probability reaching error state    clearly  respect
value function v   policy figure   b  optimal  e g  state        agent
tries reach distant goal  causes higher discounting goal reward 
minimum risk policy figure   b     safe states  defined states
risk   minimum risk policy estimated mean value v         
figure   c  maximum value policy shown  maximum value policy
optimizes value without considering risk estimated value v        
thus  performs better minimum risk policy figure   b   risk       
       become greater   algorithm starts     computes
minimum risk policy figure   b   increased step step risk state
changes value lower value     algorithm stops        
policy computed shown figure   d   obviously  lies minimum risk
policy figure   b  maximum value policy figure   c  
applied algorithm discounted version risk    grid
world problem  discounted risk used learning  whereas original risk   
used selecting best weight   parameters described above  modified
algorithm produced policy depicted figure   d   seemingly  grid world
example  oscillations present major problem 
tank control task described next section  holds  
   

   stochastic optimal control chance constraints
section  consider solution stochastic optimal control problem chance
constraints  li et al         applying risk sensitive learning method 
    description control problem
following  consider plant depicted figure    task control
outflow tank lies upstream distillation column order fulfill several
objectives described below  purpose distillation column separation
two substances      consider finite number time steps            n   outflow
tank  i e  feedstream distillation column  characterized flowrate
f  t  controlled agent  substance concentrations c   t  c   t   for
  n   
purpose control designed keep outflow rate f  t  near specified
optimal flow rate fspec order guarantee optimal operation distillation column 
  

firisk sensitive reinforcement learning

f 
c  
c  

distillation column

f 
c  
c  

ymax
y  h  c   c 
ymin

f

tank

fspec
c min  c max
c min  c max

figure    plant  see text description 
using quadratic objective function  goal specified
min

f         f  n   

n
 
x

 f  t  fspec     

    

t  

values obey
  n     fmin f  t  fmax  

    

tank characterized tank level y t  holdup h t     a  h
constant footprint tank  tank level y t  concentrations
c   t  c   t  depend two stochastic inflow streams characterized flowrates
f   t  f   t   inflow concentrations c  j  t  c  j  t  substances j        
linear dynamics tank level given
y t        y t    a 

x



fj  t  f  t   

    


a  x
fj  t  cj i  t  ci  t  
 
y t  j    

    

j    

dynamics concentrations given
         ci  t        ci  t   

initial state system characterized
y      y    c        c     c        c    

    

tank level required fulfill constraint ymin y t  ymax   concentrations inside tank correspond concentrations outflow  substance
concentrations c   t  c   t  required remain intervals  c  min   c  max  
  

figeibel   wysotzki

 c  min   c  max    respectively  assume inflows  t  inflow concentrations
ci j  t  random  governed probability distribution  li et al        
assume multivariate gaussian distribution  randomness variables 
tank level feedstream concentrations may violate given constraints 
therefore formulate stochastic constraint




p ymin y t  ymax   ci min ci  t  ci max     n         p

    

expression      called  joint  chance constraint    p corresponds
permissible probability constraint violation  value p given user 
stochastic optimization problem sop yc defined quadratic objective function      describing sum quadratic differences outflow rates fspec  
linear dynamics tank level       nonlinear dynamics concentrations
      initial state given       chance constraint      
li et al  describe simpler problem sop y concentrations considered 
see figure    sop y use cumulative inflow f   f    f  description
tank level dynamics  see       sop y describes dynamics linear system 
li et al  solve sop y relaxing nonlinear program solved sequential
quadratic programming  relaxation possible sop y linear system 
multivariate gaussian distribution assumed  solving nonlinear systems sop yc
non gaussian distributions difficult  e g  wendt  li    wozny        
achieved rl approach 

min

f         f  n   

subject
  n    
y t     

n
 
x

 f  t  fspec   

    

t  

fmin f  t  fmax


  y t    a  f  t  f  t 
y      y 



p ymin y t  ymax     n p

    
    
    
    

figure    problem sop y 
note control f  t  optimization problems depends time step
t  means solutions sop yc sop y yield open loop controls 
dependence initial condition       moving horizon approach taken
design closed loop control  discuss issue  goes beyond scope
paper 
  

firisk sensitive reinforcement learning

    formulation reinforcement learning problem
using rl instead analytical approach advantage probability distribution doesnt gaussian unknown  state equations need
known  nonlinear  learning agent must access simulated
empirical data  i e  samples least random variables 
independent chosen state representation  immediate reward defined
rx u  x      u fspec     

    

u chosen action minus required rl value function
maximized  reward signal depends action chosen  current
successor state 
work consider finite  discretized  action sets  although approach
extended continuous action sets  e g  using actor critic method  sutton  
barto         following  assume interval  fmin   fmax   discretized
appropriate manner 
process reaches error state one constraints       or       respectively  violated  process artificially terminated transferring agent
additional absorbing state giving risk signal r      v  value error states
set zero  controller could choose action fspec first constraint
violation  subsequent constraint violations make things worse respect
chance constraints            respectively 
    definition state space
following consider design appropriate state spaces result either
open loop control  olc  closed loop control  clc  
      open loop control
note sop yc sop y time dependent finite horizon problems
control f  xt     f  t  depends only  means state feedback
resulting controller open looped  respect state definition xt    t  
markov property defined section   clearly holds probabilities rewards defining
v   markov property hold rewards defining   using xt    t 
implies agent information state process  including
information history form past action  agent gets idea
current state process  therefore  inclusion history information changes
probability r      markov property violated  including past actions
state description ensures markov property r  markov property therefore
recovered considering augmented state definition
xt    t  ut            u     

    

past actions  ut            u     first action u  depends fixed initial tank level y 
fixed initial concentrations only  second action depends first action  i e 
initial tank level initial concentrations on  therefore  learning
  

figeibel   wysotzki

states      results open loop control  original problems sop yc
sop y 
noted mdp  risk depend past actions 
future actions only  choice xt    t   hidden state information 
mdp markov property violated  therefore probability
entering error state conditioned time step  i e  p  r      t   changes
additionally conditioned past actions yielding value p  r      t  ut            u   
 corresponding agent remembers past actions   example  agent
remembers past time steps current learning episode always used
action f     corresponding zero outflow  conclude increased
probability tank level exceeds ymax   i e  knowledge increased risk 
if  hand  remember past actions  cannot know increased
risk knows index current time step  carries less information
current state 
well known markov property generally recovered including
complete state history state description  xt    t   state history contains
past time indices  actions r costs  tank control task  action history
relevant part state history previous r costs necessarily zero 
indices past time steps already given actual time known
agent  therefore  past rewards indices past time steps need
included expanded state  although still complete state information
known agent  knowledge past actions suffices recover markov property 
respect state choice      reward signal       expectation
definition value function needed  cp  eq       means
h



v  x    e r   x    x  

n
 
x

 f  t  fspec   

t  

holds  i e  direct correspondence value function objective
function sop yc sop y 
      closed loop control
define alternative state space  expectation needed 
decided use state definition
xt    t  y t   c   t   c   t  

    

xt    t  y t  

    

problem sop yc
simpler problem sop y  result learning state time dependent closed
loop controller  achieve better regulation behavior open loop controller 
reacts actual tank level concentrations  whereas open loop control
not  agent access inflow rates concentrations 
included state vector  yielding improved performance controller 
  

firisk sensitive reinforcement learning

parameter
n
y 
 ymin   ymax  
a 
fspec
 fmin   fmax  
rl yc clc 
c  
c  
 c  min   c  max  
 c  min   c  max  

table    parameter settings
value
explanation
  
number time steps
   
initial tank level
             admissible interval tank level
   
constant  see     
   
optimal action value
             interval actions     discrete values
   
   
          
          

initial concentration subst   
initial concentration subst   
interval concentration  
interval concentration  

    rl problems
definitions  optimization problem defined via          
   p  see             set x  see            defined contain unique
starting state  i e x    x     experiments consider following instantiations
rl problem 
rl y clc reduced problem sop y using states xt    t  y t    x        y    resulting closed loop controller  clc  
rl y olc open loop controller  olc  reduced problem sop y  state space
defined action history time  see eq        starting state x        
rl yc clc closed loop controller full problem sop yc using states xt  
 t  y t   c   t   c   t   x        y    c     c     
solving problem rl y olc yields action vector  problems rl yc clc
rl y clc result state dependent controllers  present results fourth
natural problem rl yc olc  offer additional insights 
interpolation states used      multilayer perceptrons  mlps  e g 
bishop        case rl y olc extremely large state space     dimensions   n     used radial basis function  rbf  networks case
rl yc clc rl y clc  produced faster  stable robust results
compared mlps 
training respective networks  used direct method corresponds
performing one gradient descent step current state action pair new

estimate target value  see e g  baird         new estimate q given

r   qt  x   u    q r   qt  x   u    compare right sides update
equations            
  

figeibel   wysotzki

 a 
 
   
   
   
   
 
   
   
   
   
 

outflow rate

inflow

 b 

 

 

 

 

 
    
   
    
   
    
   
    

             
time

 c 

omega     
   

 

 

 

 

             
time

omega     
   
outflow rate

outflow rate

 d 
 
    
   
    
   
    
   
    
 

 

 

 

             
time

 
    
   
    
   
    
   
    

omega    
   

 

 

 

 

             
time

figure    rl y clc   a  inflow rates f  t     runs   b    c    d  example runs
policies                     i e  p                      holds fspec       

   experiments
section  examine experimental results obtained tank control task
          section     discuss linear case compare results li
et al          linear case  consider closed loop controller obtained solving
rl y clc  sect         open loop controller related rl problem rl y olc
 sect          closed loop controller  discuss problem non zero covariances
variables different time steps  nonlinear case discussed section     
    problems rl y clc rl y olc
start simplified problems  rl y clc rl y olc  derived sop y
discussed li et al          sop y concentrations considered 
one inflow rate f  t    f   t    f   t   parameter settings table    first five
lines  taken li et al          minimum maximum values actions
determined preliminary experiments 
li et al  define inflows  f              f      t gaussian distribution
mean vector
                                                                                t  
  

    

firisk sensitive reinforcement learning

   
   
   
risk
 
    
value

    

weighted
    
    
    
 

 

  

  

  

xi






figure    rl y clc  estimates risk  x     value v  x     v  x     




v  x     x    different values  

covariance matrix given





c 

    r  
  
    r  
   


  n   r  n   


  n   r  n   
   
   


 

n
 







    

        correlation inflows time j defined
rij   rji          j i 

    

  n      j n    from li et al          inflow rates ten example
runs depicted figure   a  
      problem rl y clc  constraints tank level 
start presentation results problem rl y clc  control
 i e  outflow f   depends time tank level  x    x   
overall performance policy defined      corresponds performance x   




v   v  x     


holds x        y     v  x    value respect policy learned
weighted criterion function v   see       respective risk


 x     




figure   estimated  risk  x    estimated value v  x    depicted


different values   estimate risk  x    value v  x   
   values policies presented following estimated learning algorithm  note
order enhance readability  denoted learned policy  

  

figeibel   wysotzki

   
   
   
   
   
 
    
                                                    
xi

figure    rl y clc  difference weighted criteria  explanation see text 

increase   given fixed value p admissible probability constraint violation 

appropriate    p  obtained value risk  x    lower

   p maximum v  x     due variation performance  see
fig     found works better selecting maximum   estimate






weighted criterion v  x      v  x     x    shown figure   
outflow rate f  control variable  different values found figure   bc   note rates certain variance since depend probabilistic tank
level  randomly picked one example run value   found control
values f  t  tend approach fspec increasing values  i e  decreasing values p  
correlations definition covariance matrix           reveals high
correlation inflow rates neighboring time steps  order better account this 
possible include information past time steps state description time t 
level changes according inflow rate f   investigated inclusion
past values y  inflow rates measured  could included
state vector  former rewards need included depend past tank
levels  i e  represent redundant information 
compared performance algorithm augmented state space
defined xt    t  y t   y t     y t      depth   history  normal state space
xt    t  y t    no history   fig    shows












v     y          v     y     
 

 z
x 

 

   z  
x 

i e  difference weighted criteria starting state respect learned
policies  history   no history   note starting state x    past values
defined    curve figure   runs mainly    means using
augmented state space results better performance many values   note
   

firisk sensitive reinforcement learning

 a 

 b 
   

 
risk

   

    

   
outflow rate

   

 
    
value

    

    
   
    

    

   

    

    
 

 

 

 

 

  

  

  

 

xi


 

 

 

 
time

  

  

  



figure    rl y olc   a  estimates risk  x    value v  x    increasing


values    b  learned policy  x          v  x         

larger values original value function overweights risk cases
policy always chooses outflow fspec approximated  means
difference performance tends zero 
similar  quite pronounced effect observed using history
length   only  principle  assume possible achieve even better performance
including full history tank levels state description  tradeoff objective difficulty network training caused number
additional dimensions 
      rl y olc  history control actions 
rl problem rl y olc comprises state descriptions consisting action history
together time  see eq        starting state empty history  i e  x        
result learning time dependent policy implicit dependence y   
learned policy therefore fixed vector actions f              f      forms feasible 
general suboptimal solution problem sop y figure   


progression risk estimate  i e   x     value  v  x    
different values found figure    results good ones

rl y clc figure    estimated minimum risk        risk  x    grows
much faster rl y clc risk figure   

policy risk  x          depicted figure   b   contrast
policies rl y clc  see figure   b c    control values change different runs 
      comparison
table    compared performance approach li et al  rl y clc
rl y olc p       p        rl y clc rl y olc performed   
learning runs  respective learned policy   risk  x    value v  x   
estimated      test runs  rl y clc rl y olc  table shows mean
performance averaged    runs together standard deviation parentheses 
   

figeibel   wysotzki

table    comparison est  squared deviation fspec  i e  v  x     results li et
al  results rl y clc rl y olc p                p      
          smaller values better 
approach
li et al        
rl y clc
rl y olc

p      
      
                 
                 

p      
      
              
               

found that  average  policy determined rl y clc performs better
obtained approach li et al          with respect estimated squared
deviation desired outflow fspec   i e  respect v  x      policy obtained
rl y olc performs better p       worse p        maximal achievable
probability holding constraints      sd      rl y clc        sd        
rl y olc  li et al  report p         approach 
approach neuneier mihatsch        considers worst case outcomes
policy  i e  risk related variability return  neuneier mihatsch show
learning algorithm interpolates risk neutral worst case criterion
limiting behavior exponential utility approach 
   

risk
value

   
   
   

risk

   
value

   
 
    
  

    

 

   

 

kappa

figure    risk value several values
learning algorithm neuneier mihatsch parameter           
allows switch risk averse behavior       risk neutral behavior        
risk seeking behavior       agent risk seeking  prefers policies
good best case outcome  figure   shows risk  probability constraint violation  value
starting state regard policy computed algorithm neuneier
mihatsch  obviously  algorithm able find maximum value policy yielding zero
deviation fspec   corresponding choosing f   fspec       states  learning
result sensitive risk parameter   reason worst case
best case returns policy always chooses outflow     correspond
   

firisk sensitive reinforcement learning

 a 

inflow

 b 
 

 

   

   
ymax
mu t      

   



   

   

   
mu t      

c max
c 

ymin
   

   

c min
 

 
 

 

 

 

 

  

  

  

 

time

 

 

 

 

  

  

  

  

time

figure    rl yc clc   a   t          t        profiles two mode means  
 b  tank level y t  concentration c   t     example runs using
minimum risk policy 

   best return possible  implying zero variance return   approach
neuneier mihatsch variance based approaches therefore unsuited
problem hand 
    problem rl yc clc  constraints tank level concentrations 
following consider full problem rl yc clc  two inflows f  f 
assumed equal gaussian distributions distribution cumulative
inflow f  t    f   t    f   t  described covariance matrix      mean
vector       see figure   a  
order demonstrate applicability approach non gaussian distributions 
chosen bimodal distributions inflow concentrations c  c    underlying
assumption upstream plants either increased output 
lower output  e g  due different hours weekdays 
distribution inflow concentration ci    t  characterized two gaussian
distributions means
 t       k       
k                    value k        chosen beginning
run equal probability outcome  means overall mean value
ci    t  given  t   profiles mean values modes found
figure   a   ci   given ci    t        ci    t   minimum maximum values
concentrations ci  t  found table    figure   b   note
concentrations controlled indirectly choosing appropriate outflow f  
developing risk value starting state shown figure    
resulting curves behave similar problem rl y clc depicted figure   
value risk increase   seen algorithm covers relatively
broad range policies different value risk combinations 
   

figeibel   wysotzki

   
   
   
risk
   
 
    
    

value

    
    
 

 

  

  

  

  

  

  

  

  

  

xi








figure     rl yc clc  estimated risk  x     value v  x     v  x      v  x   


 x    different values  

minimum risk policy  curves tank level concentration
c  found figure   b   bimodal characteristics substance   inflow
concentrations reflected c   t   it holds c   t      c   t    attainable minimum
risk        increasing weight leads curves similar shown figures  
   assume minimum achievable risk decreased inclusion
additional variables  e g  inflow rates concentrations  and or inclusion past
values discussed section        treatment version action history
analogous section        therefore conclude presentation experiments
point 

   conclusion
paper  presented approach learning optimal policies constrained risk
mdps error states  contrast rl dp approaches consider risk
matter variance return worst outcomes  defined risk
probability entering error state 
presented heuristic algorithm aims learning good stationary policies
based weighted formulation problem  weight original value function
increased order maximize return risk required stay
given threshold  fixed weight finite state space  algorithm converges
optimal policy case undiscounted value function  case state
space finite  contains cycles      holds  conjecture convergence
learning algorithm policy  assume suboptimal weighted
formulation  optimal stationary policy exists weighted formulation 
feasible  generally suboptimal solution constrained problem 
   

firisk sensitive reinforcement learning

weighted approach combined adaptation heuristic searching space feasible stationary policies original constrained problem 
us seems relatively intuitive  conjecture better policies could found allowing
state dependent weights  x  modified adaptation strategy  extending
considered policy class 
successfully applied algorithm control outflow feed tank
lies upstream distillation column  started formulation stochastic
optimal control problem chance constraints  mapped risk sensitive learning
problem error states  that correspond constraint violation   latter problem
solved using weighted rl algorithm 
crucial point reformulation rl problem design state space 
found algorithm consistently performed better state information
provided learner  using time action history resulted large state
spaces  poorer learning performance  rbf networks together sufficient state
information facilitated excellent results 
must mentioned use rl together mlp rbf network based
function approximation suffers usual flaws  non optimality learned network 
potential divergence learning process  long learning times  contrast
exact method  priori performance guarantee given  course posteriori
estimate performance learned policy made  main advantage
rl method lies broad applicability  tank control task  achieved good
results compared obtained  mostly  analytical approach 
cases  x          theoretical investigations convergence
experiments required  preliminary experiments shown oscillations may
occur algorithm  behavior tends oscillate sensible policies without
getting bad in between although convergence usefulness policies remains
open issue 
oscillations prevented using discounted risk leads underestimation
actual risk  existence optimal policy convergence learning
algorithm fixed guaranteed case finite mdp  probabilistic
interpretation discounted risk given considering   probability
exiting control mdp  bertsekas         investigation discounted
risk may worthwhile right  example  task long episodes 
continuing  i e  non episodic  natural give larger weight error
states occurring closer current state 
designed learning algorithm online algorithm  means learning accomplished using empirical data obtained interaction simulated
real process  use neural networks allows apply algorithm processes
continuous state spaces  contrast  algorithm described dolgov durfee       
applied case known finite mdp  model obtained
case continuous state process finding appropriate discretization estimating state transition probabilities together reward function  although
discretization prevents application dolgov durfees algorithm rl y olc 
   dimensional state space encountered  probably applied case
rl y olc  plan investigate point future experiments 
   

figeibel   wysotzki

question arises whether approach applied stochastic optimal
control problems types chance constraints  consider conjunction chance
constraints
p c    p            p cn     pn    
    
ct constraint system containing variables time t  pt
respective probability threshold       requires alternative rl formulation risk
state depends next reward  time step  
solution modified version rl algorithm difficult 
ct      allowed constraint system state variables depending t  things get involved several risk functions needed
state  plan investigating cases future 

acknowledgments thank dr  pu li providing application example
helpful comments  thank onder gencaslan conducting first experiments
masters thesis 

references
altman  e          constrained markov decision processes  chapman hall crc 
baird  l          residual algorithms  reinforcement learning function approximation  proc    th international conference machine learning  pp        morgan
kaufmann 
bawas  v  s          optimal rules ordering uncertain prospects  journal finance 
            
bertsekas  d  p          dynamic programming optimal control  athena scientific 
belmont  massachusetts  volumes     
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
belmont  ma 
bishop  c  m          neural networks pattern recognition  oxford university press 
oxford 
blythe  j          decision theoretic planning  ai magazine               
borkar  v          q learning risk sensitive control  mathematics operations research 
               
coraluppi  s     marcus  s          risk sensitive minimax control discrete time 
finite state markov decision processes  automatica             
crites  r  h     barto  a  g          elevator group control using multiple reinforcement
learning agents  machine learning                   
dolgov  d     durfee  e          approximating optimal policies agents limited
execution resources  proceedings eighteenth international joint conference
artificial intelligence  pp            aaai press 
   

firisk sensitive reinforcement learning

feinberg  e     shwartz  a          markov decision models weighted discounted
criteria  math  operations research             
feinberg  e     shwartz  a          constrained discounted dynamic programming  math 
operations research             
feinberg  e     shwartz  a          constrained dynamic programming two discount
factors  applications algorithm  ieee transactions automatic control 
           
fishburn  p  c          mean risk analysis risk associated below target returns 
american economics review                 
freund  r          introduction risk programming model  econometrica     
       
fulkerson  m  s   littman  m  l     keim  g  a          speeding safely  multi criteria
optimization probabilistic planning  proceedings fourteenth national
conference artificial intelligence  p       aaai press mit press 
gabor  z   kalmar  z     szepesvari  c          multi criteria reinforcement learning 
proc    th international conf  machine learning  pp          morgan kaufmann 
san francisco  ca 
geibel  p          reinforcement learning bounded risk  brodley  e     danyluk 
a  p   eds    machine learning   proceedings eighteenth international conference  icml     pp          morgan kaufmann publishers 
heger  m          consideration risk reinforcement learning  proc    th international conference machine learning  pp          morgan kaufmann 
kall  p     wallace  s  w          stochastic programming  wiley  new york 
koenig  s     simmons  r  g          risk sensitive planning probabilistic decision
graphs  doyle  j   sandewall  e     torasso  p   eds    kr    principles knowledge representation reasoning  pp          san francisco  california  morgan
kaufmann 
kushmerick  n   hanks  s     weld  d  s          algorithm probabilistic leastcommitment planning   aaai  pp           
li  p   wendt  m   arellano garcia    wozny  g          optimal operation distillation
processes uncertain inflows accumulated feed tank  aiche journal     
         
liu  y   goodwin  r     koenig  s       a   risk averse auction agents  rosenschein  j  
sandholm  t     wooldridge  m  yokoo  m   eds    proceedings second international joint conference autonomous agents multiagent systems  aamas     pp          acm press 
liu  y   goodwin  r     koenig  s       b   risk averse auction agents   aamas  pp 
       
markowitz  h  m          portfolio selection  journal finance              
markowitz  h  m          portfolio selection  john wiley sons  new york 
   

figeibel   wysotzki

mihatsch  o     neuneier  r          risk sensitive reinforcement learning  machine learning                   
neuneier  r     mihatsch  o          risk sensitive reinforcement learning  michael
s  kearns  sara a  solla  d  a  c   ed    advances neural information processing
systems  vol      mit press 
ross  s  m          introduction probability models  academic press  new york 
roy  a  d          safety first holding assets  econometrica                 
smart  w  d     kaelbling  l  p          effective reinforcement learning mobile robots  proceedings      ieee international conference robotics
automation  icra       
stephan  v   debes  k   gross  h  m   wintrich  f     wintrich  h          new control
scheme combustion processes using reinforcement learning based neural networks  international journal computational intelligence applications        
       
sutton  r  s     barto  a  g          reinforcement learning introduction  mit
press 
tsitsiklis  j  n          asynchronous stochastic approximation q learning  machine
learning                 
watkins  c  j  c  h          learning delayed rewards  ph d  thesis  kings college 
oxford 
watkins  c  j  c  h     dayan  p          q learning  machine learning           special
issue reinforcement learning 
wendt  m   li  p     wozny  g          non linear chance constrained process optimization
uncertainty  ind  eng  chem  res                

   


