journal of artificial intelligence research                 

submitted        published      

risk sensitive reinforcement learning applied to control
under constraints
peter geibel

pgeibel uos de

institute of cognitive science  ai group
university of osnabruck  germany

fritz wysotzki

wysotzki cs tu berlin de

faculty of electrical engineering and computer science  ai group
tu berlin  germany

abstract
in this paper  we consider markov decision processes  mdps  with error states  error
states are those states entering which is undesirable or dangerous  we define the risk
with respect to a policy as the probability of entering such a state when the policy is
pursued  we consider the problem of finding good policies whose risk is smaller than
some user specified threshold  and formalize it as a constrained mdp with two criteria 
the first criterion corresponds to the value function originally given  we will show that
the risk can be formulated as a second criterion function based on a cumulative return 
whose definition is independent of the original value function  we present a model free 
heuristic reinforcement learning algorithm that aims at finding good deterministic policies 
it is based on weighting the original value function and the risk  the weight parameter is
adapted in order to find a feasible solution for the constrained problem that has a good
performance with respect to the value function  the algorithm was successfully applied
to the control of a feed tank with stochastic inflows that lies upstream of a distillation
column  this control task was originally formulated as an optimal control problem with
chance constraints  and it was solved under certain assumptions on the model to obtain an
optimal solution  the power of our learning algorithm is that it can be used even when
some of these restrictive assumptions are relaxed 

   introduction
reinforcement learning  as a research area  provides a range of techniques that are applicable to difficult nonlinear or stochastic control problems  see e g  sutton   barto       
bertsekas   tsitsiklis         in reinforcement learning  rl  an agent is considered that
learns to control a process  the agent is able to perceive the state of the process  and
it acts in order to maximize the cumulative return that is based on a real valued reward
signal  often  experiences with the process are used to improve the agents policy instead
of a previously given analytical model 
the notion of risk in rl is related to the fact  that even an optimal policy may perform
poorly in some cases due to the stochastic nature of the problem  most risk sensitive rl
approaches are concerned with the variance of the return  or with its worst outcomes 
 e g  coraluppi   marcus        heger        neuneier   mihatsch         see also the
discussion in section    we take the alternative view of risk defined by geibel        that
is not concerned with the variability of the return  but with the occurrence of errors or
c
    
ai access foundation  all rights reserved 

figeibel   wysotzki

undesirable states in the underlying markov decision process  mdp   this means that we
address a different class of problems compared to approaches referring to the variability of
the return 
in this paper  we consider constrained mdps with two criteria  the usual value function and the risk as a second value function  the value is to be optimized while the risk
must remain below some specified threshold  we describe a heuristic algorithm based on a
weighted formulation that finds a feasible policy for the original constrained problem 
in order to offer some insight in the behavior of the algorithm  we investigate the application of the algorithm to a simple grid world problem with a discounted criterion function 
we then apply the algorithm to a stochastic optimal control problem with continuous states 
where the set of feasible solutions is restricted by a constraint that is required to hold with
a certain probability  thus demonstrating the practical applicability of our approach  we
consider the control of a feed tank that lies upstream of a distillation column with respect
to two objectives      the outflow of the tank is required to stay close to a specified value in
order to ensure the optimal operation of the distillation column  and     the tank level and
substance concentrations are required to remain within specified intervals  with a certain
admissible chance of constraint violation 
li  wendt  arellano garcia  and wozny        formulate the problem as a quadratic
program with chance constraints   e g  kall   wallace         which is relaxed to a nonlinear
program for the case of gaussian distributions for the random input variables and systems
whose dynamics is given by linear equations  the nonlinear program is solved through
sequential quadratic programming 
note that the approach of li et al  involves the simulation based estimation of the
gradients of the chance constraints  li et al         p         like q learning  watkins 
      watkins   dayan        sutton   barto         our learning algorithm is based on
simulating episodes and estimating value and risk of states  which for the tank control task
correspond to a measure for the deviation from the optimal outflow and the probability of
constraint violation  respectively 
in contrast to the approach of li et al          our rl algorithm is applicable to systems
with continuous state spaces  whose system dynamics is governed by nonlinear equations
and involve randomization or noise with arbitrary distributions for the random variables 
for it makes no prior assumptions of either aspect  this it not a special property of our
learning algorithm  and also holds true for e g  q learning and other rl algorithms  the
convergence of q learning combined with function approximation techniques necessary for
continuous state spaces cannot be guaranteed in general  e g  sutton   barto         the
same holds true for our algorithm  nevertheless  rl algorithms were successfully applied
to many difficult problems with continuous state spaces and nonlinear dynamics  see e g 
sutton   barto        crites   barto        smart   kaelbling        stephan  debes 
gross  wintrich    wintrich        
   a constraint can be seen as a relation on the domains of variables restricting their possible values  if
the variables in a constraint c   c x            xn   are random  the constraint will hold with a certain
probability  chance constrained programming is a particular approach to stochastic programming that
considers constrained optimization problems containing random variables for which so called chance
constraints of the form p c   p with p         are formulated 

  

firisk sensitive reinforcement learning

this article is organized as follows  in section    the rl framework is described  section   reviews related work on risk sensitive approaches  section   describes our approach
to risk sensitive rl  in section    we elucidate a heuristic learning algorithm for solving the
constrained problem using a weighted formulation  in section    we describe its application
to a grid world problem  the tank control task is described in section    in section   
experiments with the feed tank control are described  section   concludes with a short
summary and an outlook 

   the rl framework
in rl one considers an agent that interacts with a process that is to be controlled  at
each discrete time step  the agent observes the state x and takes an action u that in general
depends on x  the action of the agent causes the environment to change its state to x
according to the probability px u  x    until section    we will consider the set of states  x 
to be a finite set 
the action set of the agent is assumed to be finite  and it is allowed to depend on the
current state  in each state x  the agent uses an action from the set u  x  of possible actions 
after taking action u  u  x   the agent receives a real valued reinforcement signal rx u  x  
that depends on the action taken and the successor state x   in the case of a random reward
signal  rx u  x   corresponds to its expected value  the markov property of an mdp requires
that the probability distribution on the successor states and the one on the rewards depend
on the current state and action only  the distributions do not change when additional
information on past states  actions and rewards is considered  i e  they are independent of
the path leading to the current state 
the aim of the agent is to find a policy  for selecting actions that maximizes the
cumulative reward  called the return  the return is defined as
r 


x

 t rt  

   

t  

where the random variable rt denotes the reward occurring in the t th time step when the
agent uses policy   let x    x    x          denote the corresponding probabilistic sequence of
states  and ui the sequence of actions chosen according to policy  
the constant          is a discount factor that allows to control the influence of future
rewards  the expectation of the return 
h

i

v   x    e r   x    x  

   

is defined as the value of x with respect to   it is well known that there exist stationary

deterministic policies   for which v   x  is optimal  maximal  for every state x  a stationary deterministic policy is a function that maps states to actions and is particularly defined
as being independent of time and markovian  independent of history   in this work  we will
use the term of maximum value policies instead of optimal policies just to distinguish them
from minimum risk policies that are also optimal in some sense  see section     
as usual  we define the state action value function
fi
fi

h

i

q  x  u    e r    v   x    fi x    x  u    u  
  

   

figeibel   wysotzki

q  x  u  is the expected return when the agent first chooses action u  and acts according to
 in subsequent time steps  from the optimal q function q   optimal policies   and the
unique optimal values v  are derived by    x   argmaxu q  x  u  and v   x    q  x     x   
q can be computed by using watkins q learning algorithm 
in rl one in general distinguishes episodic and continuing tasks that can be treated
in the same framework  see e g  sutton   barto         in episodic tasks  the agent may
reach some terminal or absorbing state at same time t   after reaching the absorbing state 
the agent stays there and executes some dummy action  the reward is defined by rt    
for t  t   during learning the agent is restarted according to some distribution on the
initial states after it has reached the absorbing state 

   related work
p

t
the random variable r   
t    rt  return  used to define the value of a state possesses
a certain variance  most risk averse approaches in dynamic programming  dp  and reinforcement learning are concerned with the variance of r  or with its worst outcomes  an
example of such an approach is worst case control  e g  coraluppi   marcus        heger 
       where the worst possible outcome of r is to be optimized  in risk sensitive control based on the use of exponential utility functions  e g  liu  goodwin    koenig      a 
koenig   simmons        liu  goodwin    koenig      b  borkar         the return r
is transformed so as to reflect a subjective measure of utility  instead of maximizing the
expected value of r  now the objective is to maximize e g  u      log e er    where  is
a parameter and r is the usual return  it can be shown that depending on the parameter
  policies with a high variance v r  are penalized        or enforced         the value criterion introduced by heger        can be seen as an extension of worst case control
where bad outcomes of a policy that occur with a probability less than  are neglected 
neuneier and mihatsch        give a model  free rl algorithm which is based on a
parameterized transformation of the temporal difference errors occurring  see also mihatsch
  neuneier         the parameter of the transformation allows to switch between riskaverse and risk seeking policies  the influence of the parameter on the value function cannot
be expressed explicitly 
our view of risk is not concerned with the variance of the return or its worst possible
outcomes  but instead with the fact that processes generally possess dangerous or undesirable states  think of a chemical plant where temperature or pressure exceeding some
threshold may cause the plant to explode  when controlling such a plant  the return corresponds to the plants yield  but it seems inappropriate to let the return also reflect the
cost of the explosion  e g  when human lives are affected 
in this work  we consider processes that have such undesirable terminal states  a seemingly straightforward way to handle these error states of the system is to provide high
negative rewards when the systems enters an error state  an optimal policy will then avoid
the error states in general  a drawback of the approach is the fact that it is unknown how
large the risk  probability  of entering an error state is  moreover  we may want to provide
a threshold  for the probability of entering an error state that must not be exceeded by
the agents policy  in general  it is impossible to completely avoid error states  but the risk
should be controllable to some extend  more precisely  if the agent is placed in some state

  

firisk sensitive reinforcement learning

x  then it should follow a policy whose risk is constrained by   the parameter         
reflects the agents risk averseness  that is  our goal is not the minimization of the risk 
but the maximization of v  while the risk is kept below the threshold  
markowitz        considers the combination of different criteria with equal discount
factors in the context of portfolio selection  the risk of the selected portfolio is related to
the variance of the combined  weighted  criteria  markowitz introduces the notion of the
 e  v   space  our notion of risk is not related to the variance of v   but depends on the
occurrence of error states in the mdp  therefore the risk is conceptually independent of v  
see e g  the tank control problem described in section   
the idea of weighting return and risk  markowitz        freund        heger        leads
to the expected value minus variance criterion  e r   kv r   where k is a parameter  we
use this idea for computing a feasible policy for the problem of finding a good policy that
has a constrained risk  in regard to the probability of entering an error state   value and
risk are weighted using a weight  for the value and weight   for the risk  the value of
 is increased  giving the value more weight compared to the risk  until the risk of a state
becomes larger than the user specified threshold  
in considering an ordering relation for tuples of values  our learning algorithm for a
fixed value of  is also related to the artdp approach by gabor  kalmar  and szepesvari
        in their article  gabor et al  additionally propose a recursive formulation for an
mdp with constraints that may produce suboptimal solutions  it is not applicable in our
case because their approach requires a nonnegative reward function 
it should be noted that the aforementioned approaches based on the variability of the
return are not suited for problems like the grid world problem discussed in section    or the
tank control task in section   where risk is related to the parameters  variables  of the state
description  for example  in the grid world problem  all policies have the same worst case
outcome  in regard to approaches based on the variance  we found that a policy leading to
the error states as fast as possible does not have a higher variance than one that reaches the
goal states as fast as possible  a policy with a small variance can therefore have a large risk
 with respect to the probability of entering an error state   which means that we address a
different class of control problems  we underpin this claim in section       
fulkerson  littman  and keim        sketch an approach in the framework of probabilistic planning that is similar to ours although based on the complementary notion of
safety  fulkerson et al  define safety as the probability of reaching a goal state  see also
the buridan system of kushmerick  hanks    weld         fulkerson et al  discuss the
problem of finding a plan with minimum cost subject to a constraint on the safety  see
also blythe         for an episodic mdp with goal states  the safety is   minus risk  for
continuing tasks or if there are absorbing states that are neither goal nor error states  the
safety may correspond to a smaller value  fulkerson et al         manipulate  scale  the
 uniform  step reward of the undiscounted cost model in order to enforce the agent to reach
the goal more quickly  see also koenig   simmons         in contrast  we also consider
discounted mdps  and neither require the existence of goal states  although we do not
change the original reward function  our algorithm in section   can be seen as a systematic
approach for dealing with the idea of fulkerson et al  that consists in modification of the
relative importance of the original objective  reaching the goal  and the safety  in contrast
to the aforementioned approaches belonging to the field of probabilistic planning  which
  

figeibel   wysotzki

operate on an previously known finite mdp  we have designed an online learning algorithm
that uses simulated or actual experiences with the process  by the use of neural network
techniques the algorithm can also be applied to continuous state processes 
dolgov and durfee        describe an approach that computes policies that have a
constrained probability for violating given resource constraints  their notion of risk is
similar to that described by geibel         the algorithm given by dolgov and durfee
       computes suboptimal policies using linear programming techniques that require a
previously known model and  in contrast to our approach  cannot be easily extended to
continuous state spaces  dolgov and durfee included a discussion on dp approaches for
constrained mdps  e g  altman        that also do not generalize to continuous state
spaces  as in the tank control task  and require a known model  the algorithm described
by feinberg and shwartz        for constrained problems with two criteria is not applicable
in our case  because it requires both discount factors to be strictly smaller than    and
because it is limited to finite mdps 
downside risk is a common notion in finance that refers to the likelihood of a security
or other investment declining in price  or the amount of loss that could result from such
potential decline  the scientific literature on downside risk  e g  bawas        fishburn 
      markowitz        roy        investigates risk measures that particularly consider the
case in which a return lower than its mean value  or below some target value is encountered 
in contrast  our notion of risk is not coupled with the return r  but with the fact that a state
x is an error state  for example  because some parameters describing the state lie outside
their permissible ranges  or because the state lies inside an obstacle which may occur in
robotics applications 

   risk
to define our notion of risk more precisely  we consider a set
x

   

of error states  error states are terminal states  this means that the control of the agent
ends when it reaches a state in   we allow an additional set of non error terminal states
 with       
now  we define the risk of x with respect to  as the probability that the state sequence
 xi  i  with x    x  which is generated by executing policy   terminates in an error state
x   
definition      risk  let  be a policy  and let x be some state  the risk is defined as




  x    p i xi     x    x  

   

by definition    x      holds if x    if x    then   x      because of        for
states       the risk depends on the action choices of the policy  
in the following subsection  we will consider the computation of minimum risk policies
analogous to the computation of maximum value policies 
  

firisk sensitive reinforcement learning

    risk minimization
the risk  can be considered a value function defined for a cost signal r  to see this  we
augment the state space of the mdp with an additional absorbing state  to which the
agent is transfered after reaching a state from     the state  is introduced for technical
reasons 
if the agent reaches  from a state in   both the reward signals r and r become zero 
we set r     and r      if the agent reaches  from an error state  then the states in   
are no longer absorbing states  the new cost function r is defined by


rx u  x    

 

  if x   and x   
  else 

   

with this construction of the cost function r  an episode of states  actions and costs
starting at some initial state x contains exactly once the cost of r     if an error state
occurs in it  if the process does not enter an error state  the sequence of r costs contains
zeros only  therefore  the probability defining the risk can be expressed as the expectation
of a cumulative return 
proposition     it holds


  x    e

 
x
i  

with the discount factor      

 
fi
fi
 ri fi x    x
i

   

proof  r    r          is the probabilistic sequence of the costs related to the risk  as stated
p
i
above  it holds that r  def 
i    ri     if the trajectory leads to an error state  otherwise
p i
i    ri      this means that the return r is a bernoulli random variable  and the
probability q of r     corresponds to the risk of x with respect to   for a bernoulli random
variable it holds that er   q  see e g  ross         notice that the introduction of  together
with the fact that r     occurs during the transition from an error state
not iwhen
fi
hp to   and
fi

i
entering the respective error state  ensures the correct value of e
i    ri fi x    x also
for error states x  q e d 
similar to the q function we define the state action risk as
h

q  x  u    e r      x      x    x  u    u
 

x





px u  x   rx u  x       x    

x

i

   
   

minimum risk policies can be obtained with a variant of the q learning algorithm  geibel 
      
    maximized value  constrained risk
in general  one is not interested in policies with minimum risk  instead  we want to provide
a parameter  that specifies the risk we are willing to accept  let x   x be the set of
states we are interested in  e g  x    x         or x     x    for a distinguished
  

figeibel   wysotzki

starting state x    for a state x  x    let px be the probability for selecting it as a starting
state  the value of
x
v   def
px v   x 
    
xx 

corresponds to the performance on the states in x    we consider the constrained problem
max v 

    

for all x  x      x     

    



subject to
a policy that fulfills      will be called feasible  depending on   the set of feasible policies
may be empty  optimal policies generally depend on the starting state  and are nonstationary and randomized  feinberg   shwartz        gabor et al         geibel         if
we restrict the considered policy class to stationary deterministic policies  the constrained
problem is generally only well defined if x  is a singleton  because there need not be a
stationary deterministic policy being optimal for all states in x    feinberg and shwartz
       have shown for the case of two unequal discount factors smaller than   that there
exist optimal policies that are randomized markovian until some time step n  i e  they do
not depend on the history  but may be non stationary and randomized   and are stationary
deterministic  particularly markovian  from time step n onwards  feinberg and shwartz
       give a dp algorithm for this case  cp  feinberg   shwartz         this cannot be
applied in our case because       and also because it does not generalize to continuous state
spaces  in the case of equal discount factors  it is shown by feinberg and shwartz        that
 for a fixed starting state  there also exist optimal stationary randomized policies that in
the case of one constraint consider at most one action more than a stationary deterministic
policy  i e  there is at most one state where the policy chooses randomly between two
actions 

   the learning algorithm
for reasons of efficiency and predictability of the agents behavior and because of what
has been said at the end of the last section  we will restrict our consideration to stationary deterministic policies  in the following we present a heuristic algorithm that aims at
computing a good policy  we assume that the reader is familiar with watkins q learning
algorithm  watkins        watkins   dayan        sutton   barto        
    weighting risk and value
we define a new  third  value function v and a state action value function q that is the
weighted sum of the risk and the value with
v  x    v   x     x 
q  x  u 





  q  x  u   q  x  u   

    
    

the parameter     determines the influence of the v   values  q  values  compared to the
  values  q  values   for       v corresponds to the negative of    this means that
  

firisk sensitive reinforcement learning

the maximization of v  will lead to a minimization of    for     the maximization of
v leads to a lexicographically optimal policy for the unconstrained  unweighted   criteria
problem  if one compares the performance of two policies lexicographically  the criteria are
ordered  for large values of   the original value function multiplied by  dominates the
weighted criterion 
the weight is successively adapted starting with       see section      before adaptation
of   we will discuss how learning for a fixed  proceeds 
    learning for a fixed 
for a fixed value of   the learning algorithm computes an optimal policy  using an
algorithm that resembles q learning and is also based on the artdp approach by gabor
et al         
during learning  the agent has estimates qt   qt for time t     and thus an estimate
qt for the performance of its current greedy policy  which is the policy that selects the best
action with respect to the current estimate qt   these values are updated using example
state transitions  let x be the current state  u the chosen action  and x the observed
successor state  the reward and the risk signal of this example state transition are given by
r and r respectively  in x   the greedy action is defined in the following manner  an action
u is preferable to u if qt  x   u    qt  x   u   holds  if the equality holds  the action with
the higher qt  value is preferred  we write u  u   if u is preferable to u  
let u be the greedy action in x with respect to the ordering   then the agents
estimates are updated according to
qt    x  u        t  qt  x  u    t  r   qt  x   u   

    

qt    x  u        t  qt  x  u    t  r    qt  x   u   

    

t  
qt  
 x  u   qt    x  u 
  x  u    q

    

every time a new  is chosen  the learning rate t is set to    afterwards t decreases over
time  cp  sutton   barto        
for a fixed   the algorithm aims at computing a good stationary deterministic policy 
for the weighted formulation that is feasible for the original constrained problem  existence
of an optimal stationary deterministic policy for the weighted problem and convergence of
the learning algorithm can be guaranteed if both criteria have the same discount factor  i e 
     even when       in the case      q forms a standard criterion function with
rewards r  r  because we consider the risk as the second criterion function      implies
that          to ensure convergence in this case it is also required that either  a  there
exists at least one proper policy  defined as a policy that reaches an absorbing state with
probability one   and improper policies yield infinite costs  see tsitsiklis         or  b   all
policies are proper  this is the case in our application example  we conjecture that in the
case     convergence to a possibly suboptimal policy can be guaranteed if the mdp forms
a directed acyclic graph  dag   in other cases oscillations and non convergence may occur 
because optimal policies for the weighted problem are generally not found in the considered
policy class of stationary deterministic policies  as for the constrained problem  
  

figeibel   wysotzki

    adaptation of 
when learning starts  the agent chooses      and performs learning steps that will lead 
after some time  to an approximated minimum risk policy     this policy allows the agent
to determine if the constrained problem is feasible 
afterwards the value of  is increased step by step until the risk in a state in x  becomes
larger than   increasing  by some  increases the influence of the q values compared to
the q values  this may cause the agent to select actions that result in a higher value 
but perhaps also in a higher risk  after increasing   the agent again performs learning
steps until the greedy policy is sufficiently stable  this is aimed at producing an optimal
deterministic policy    the computed q  and q values for the old   i e  estimates for




  
q and q   are used as the initialization for computing  
the aim of increasing  is to give the value function v the maximum influence possible 
this means that the value of  is to be maximized  and needs not be chosen by the user 
the adaptation of  provides a means for searching the space of feasible policies 

    using a discounted risk
in order to prevent oscillations of the algorithm in section     for the case      it may be
advisable to set     corresponding to using a discounted risk defined as
  x 

 e

 
x
i  

 
fi
fi
 ri fi x    x  
i

    

because the values of the ri are all positive  it holds   x     x  for all states x  the discounted risk   x  gives more weight to error states occurring in the near future  depending
on the value of  
for a finite mdp and a fixed   the convergence of the algorithm to an optimal stationary
policy for the weighted formulation can now be guaranteed because q  using   x   forms
a standard criterion function with rewards r  r  for terminating the adaptation of  in
the case that the risk of a state in x  becomes larger than   one might still use the original
 undiscounted  risk   x  while learning is done with its discounted version   x   i e  the
learning algorithm has to maintain two risk estimates for every state  which is not a major
problem  notice that in the case of      the effect of considering the weighted criterion
v    corresponds to modifying the unscaled original reward function r by adding a
negative reward of    when the agent enters an error state  the set of optimal stationary
deterministic policies is equal in both cases  where the added absorbing state  with its
single dummy action can be neglected  
in section    experiments for the case of          x    x          and a finite
state space can be found  in the sections   and   we will consider an application example
with infinite state space  x     x     and         

   grid world experiment
in the following we will study the behaviour of the learning algorithm for a finite mdp
with a discounted criterion  in contrast to the continuous state case discussed in the next
  

firisk sensitive reinforcement learning

e
e
e
a 
e
e g
e e
e 
e 
e 
c 
e 
e g
e e

e     g
e     
e     
b 
e     
e g    
e e e e e e
e     g
e     
e     
d 
e     
e g    
e e e e e e

g

e





e

e





e

e e
 g
 
 
 
 
e e

figure    a  an example grid world  x   horizontal  y   vertical  for further explanation see
text  b  minimum risk policy        with    unsafe states  c  maximum value
policy          with    unsafe states  d  result of algorithm  policy for        
with    unsafe states 

section  no function approximation with neural networks is needed because both the value
function and the risk can be stored in a table  for the grid world  we have chosen   
      x    x    and a state graph that is not a dag  this implies that there is no
stationary policy which is optimal for every state in x    although oscillations can therefore
be expected  we have found that the algorithm stabilizes at a feasible policy because the
learning rate t tends to zero  we also investigated the use of the discounted risk that
prevents an oscillatory behaviour 
we consider the      grid world that is depicted in figure   a   an empty field denotes
some state  es denote error states  and the two gs denote two goal states  we describe
states as pairs  x  y  where x  y                      i e                                      
                                                                             the additional absorbing
state  is not depicted 
we have chosen the error states as if the lower  i e  extremal values of x and y were
dangerous  one of the goal states is placed next to the error states  the other in a safer
part of the state space 
the agent has the actions u             an action u  u takes the agent in the
denoted direction if possible  with a probability of       the agent is not transported to
the desired direction but to one of the three remaining directions 
the agent receives a reward of   if it enters a goal state  the agent receives a reward of
  in every other case  it should be noted that there is no explicit punishment for entering an
error state  but there is an implicit one  if the agent enters an error state  then the current
episode ends  this means that the agent will never receive a positive reward after it has
reached an error state  therefore  it will try to reach one of the goal states  and because
      it will try to do this as fast as possible 
  

figeibel   wysotzki

we have chosen x    x                  and equal probabilities px for all states 
although the convergence of the algorithm cannot be guaranteed in this case  the experimental results show that the algorithm yields a feasible policy 
we have selected          in order to illustrate the behaviour of the algorithm we
have also computed the minimum risk and maximum value policy  figure   b  shows the
minimum risk policy  though the reward function r defined above plays no role for the
minimum risk policy  the agent tries to reach one of the two goal states  this is so because
from a goal state the probability of reaching an error state is    clearly  with respect to the
value function v   the policy in figure   b  is not optimal  e g  from state        the agent
tries to reach the more distant goal  which causes higher discounting of the goal reward 
the minimum risk policy in figure   b  has    safe states  defined as states for which the
risk is below   the minimum risk policy has an estimated mean value of v          
in figure   c  the maximum value policy is shown  the maximum value policy that
optimizes the value without considering the risk has an estimated value of v         
thus  it performs better than the minimum risk policy in figure   b   but the risk in       
and        has become greater than   our algorithm starts with      and computes the
minimum risk policy in figure   b    is increased step by step until the risk for a state
changes from a value lower than  to a value     our algorithm stops at          the
policy computed is shown in figure   d   obviously  it lies in between the minimum risk
policy in figure   b  and the maximum value policy in figure   c  
we also applied the algorithm with the discounted version of the risk     to the grid
world problem  the discounted risk was used for learning  whereas the original risk    
was used for selecting the best weight   for the parameters described above  the modified
algorithm also produced the policy depicted in figure   d   seemingly  in the grid world
example  oscillations do not present a major problem 
for the tank control task described in the next section  it holds that     because
    

   stochastic optimal control with chance constraints
in this section  we consider the solution of a stochastic optimal control problem with chance
constraints  li et al         by applying our risk sensitive learning method 
    description of the control problem
in the following  we consider the plant depicted in figure    the task is to control the
outflow of the tank that lies upstream of a distillation column in order to fulfill several
objectives that are described below  the purpose of the distillation column is the separation
of two substances   and    we consider a finite number of time steps            n   the outflow
of the tank  i e  the feedstream of the distillation column  is characterized by a flowrate
f  t  which is controlled by the agent  and the substance concentrations c   t  and c   t   for
   t  n   
the purpose of the control to be designed is to keep the outflow rate f  t  near a specified
optimal flow rate fspec in order to guarantee optimal operation of the distillation column 
  

firisk sensitive reinforcement learning

f 
c  
c  

distillation column

f 
c  
c  

ymax
y  h  c   c 
ymin

f

tank

fspec
c min  c max
c min  c max

figure    the plant  see text for description 
using a quadratic objective function  this goal is specified by
min

f         f  n   

n
 
x

 f  t   fspec     

    

t  

where the values obey
for    t  n      fmin  f  t   fmax  

    

the tank is characterized by its tank level y t  and its holdup h t   where y   a  h with
some constant a for the footprint of the tank  the tank level y t  and the concentrations
c   t  and c   t  depend on the two stochastic inflow streams characterized by the flowrates
f   t  and f   t   and the inflow concentrations c  j  t  and c  j  t  for substances j         
the linear dynamics of the tank level is given by
y t        y t    a  t 

 x



fj  t   f  t   

    


a  t x
fj  t  cj i  t   ci  t  
 
y t  j    

    

j    

the dynamics of the concentrations is given by
for i          ci  t        ci  t   

the initial state of the system is characterized by
y      y    c        c     c        c    

    

the tank level is required to fulfill the constraint ymin  y t   ymax   the concentrations inside the tank correspond to the concentrations of the outflow  the substance
concentrations c   t  and c   t  are required to remain in the intervals  c  min   c  max   and
  

figeibel   wysotzki

 c  min   c  max    respectively  we assume that the inflows fi  t  and inflow concentrations
ci j  t  are random  and that they are governed by a probability distribution  li et al        
assume a multivariate gaussian distribution  because of the randomness of the variables 
the tank level and the feedstream concentrations may violate the given constraints  we
therefore formulate the stochastic constraint




p ymin  y t   ymax   ci min  ci  t   ci max      t  n  i         p

    

the expression in      is called a  joint  chance constraint  and    p corresponds to the
permissible probability of constraint violation  the value of p is given by the user 
the stochastic optimization problem sop yc is defined by the quadratic objective function      describing the sum of the quadratic differences of the outflow rates and fspec   the
linear dynamics of the tank level in       the nonlinear dynamics of the concentrations in
      the initial state given in       and the chance constraint in      
li et al  describe a simpler problem sop y where the concentrations are not considered 
see figure    for sop y we use the cumulative inflow f   f    f  in the description
of the tank level dynamics  see       sop y describes the dynamics of a linear system 
li et al  solve sop y by relaxing it to a nonlinear program that is solved by sequential
quadratic programming  this relaxation is possible because sop y is a linear system  and
a multivariate gaussian distribution is assumed  solving of nonlinear systems like sop yc
and non gaussian distributions is difficult  e g  wendt  li    wozny         but can be
achieved with our rl approach 

min

f         f  n   

subject to
for    t  n     
y t     

n
 
x

 f  t   fspec   

    

t  

fmin  f  t   fmax


  y t    a  t  f  t   f  t 
y      y 



p ymin  y t   ymax      t  n  p

    
    
    
    

figure    the problem sop y 
note that the control f  t  in the optimization problems only depends on the time step
t  this means that the solutions of sop yc and sop y yield open loop controls  because of
the dependence on the initial condition in       a moving horizon approach can be taken to
design a closed loop control  we will not discuss this issue  as it goes beyond the scope of
the paper 
  

firisk sensitive reinforcement learning

    formulation as a reinforcement learning problem
using rl instead of an analytical approach has the advantage that the probability distribution doesnt have to be gaussian and it can be unknown  the state equations also need not
be known  and they can be nonlinear  but the learning agent must have access to simulated
or empirical data  i e  samples of at least some of the random variables 
independent of the chosen state representation  the immediate reward is defined by
rx u  x      u  fspec     

    

where u is a chosen action  the minus is required because the rl value function is to be
maximized  the reward signal only depends on the action chosen  not on the current and
the successor state 
in this work we only consider finite  discretized  action sets  although our approach can
also be extended to continuous action sets  e g  by using an actor critic method  sutton  
barto         in the following  we assume that the interval  fmin   fmax   is discretized in an
appropriate manner 
the process reaches an error state if one of the constraints in       or in       respectively  is violated  the process is then artificially terminated by transferring the agent to
the additional absorbing state  giving a risk signal of r      the v   value of error states
is set to zero  because the controller could choose the action fspec after the first constraint
violation  as subsequent constraint violations do not make things worse with respect to the
chance constraints      and       respectively 
    definition of the state space
in the following we consider the design of appropriate state spaces that result either in an
open loop control  olc  or a closed loop control  clc  
      open loop control
we note that sop yc and sop y are time dependent finite horizon problems where the
control f  xt     f  t  depends on t only  this means that there is no state feedback and
the resulting controller is open looped  with respect to the state definition xt    t   the
markov property defined in section   clearly holds for the probabilities and rewards defining
v    but the markov property does not hold for the rewards defining    using xt    t 
implies that the agent has no information about the state of the process  by including
information about the history in the form of its past action  the agent gets an idea about
the current state of the process  therefore  the inclusion of history information changes the
probability for r      and the markov property is violated  including the past actions in
the state description ensures the markov property for r  the markov property is therefore
recovered by considering the augmented state definition
xt    t  ut            u     

    

with past actions  ut            u     the first action u  depends on the fixed initial tank level y 
and the fixed initial concentrations only  the second action depends on the first action  i e 
also on the initial tank level and the initial concentrations and so on  therefore  learning
  

figeibel   wysotzki

with states      results in an open loop control  as in the original problems sop yc and
sop y 
it should be noted that for an mdp  the risk does not depend on past actions  but
on future actions only  for the choice xt    t   there is hidden state information  and we
do not have an mdp because the markov property is violated  therefore the probability
of entering an error state conditioned on the time step  i e  p  r      t   changes if it is
additionally conditioned on the past actions yielding the value p  r      t  ut            u   
 corresponding to an agent that remembers its past actions   for example  if the agent
remembers that in the past time steps of the current learning episode it has always used
action f     corresponding to a zero outflow  it can conclude that there is an increased
probability that the tank level exceeds ymax   i e  it can have knowledge of an increased risk 
if  on the other hand  it does not remember its past actions  it cannot know of an increased
risk because it only knows the index of the current time step  which carries less information
about the current state 
it is well known that the markov property can generally be recovered by including the
complete state history into the state description  for xt    t   the state history contains
the past time indices  actions and r costs  for the tank control task  the action history is
the relevant part of the state history because all previous r costs are necessarily zero  and
the indices of the past time steps are already given with the actual time t that is known
to the agent  therefore  the past rewards and the indices of the past time steps need not
be included into the expanded state  although still not the complete state information is
known to the agent  knowledge of past actions suffices to recover the markov property 
with respect to the state choice      and the reward signal       the expectation from
the definition of the value function is not needed  cp  eq       this means that
h

i

v   x    e r   x    x   

n
 
x

 f  t   fspec   

t  

holds  i e  there is a direct correspondence between the value function and the objective
function of sop yc and sop y 
      closed loop control
we will now define an alternative state space  where the expectation is needed  we have
decided to use the state definition
xt    t  y t   c   t   c   t  

    

xt    t  y t  

    

for the problem sop yc and
for the simpler problem sop y  the result of learning is a state and time dependent closed
loop controller  which can achieve a better regulation behavior than the open loop controller 
because it reacts on the actual tank level and concentrations  whereas an open loop control
does not  if the agent has access to the inflow rates or concentrations  they too can be
included in the state vector  yielding improved performance of the controller 
  

firisk sensitive reinforcement learning

parameter
n
y 
 ymin   ymax  
a  t
fspec
 fmin   fmax  
only rl yc clc 
c  
c  
 c  min   c  max  
 c  min   c  max  

table    parameter settings
value
explanation
  
number of time steps
   
initial tank level
             admissible interval for tank level
   
constant  see     
   
optimal action value
             interval for actions     discrete values
   
   
          
          

initial concentration subst   
initial concentration subst   
interval for concentration  
interval for concentration  

    the rl problems
with the above definitions  the optimization problem is defined via      and      with
    p  see      and        the set x   see      and       is defined to contain the unique
starting state  i e x     x     in our experiments we consider the following instantiations
of the rl problem 
 rl y clc reduced problem sop y using states xt    t  y t    with x        y    resulting in a closed loop controller  clc  
 rl y olc open loop controller  olc  for reduced problem sop y  the state space is
defined by the action history and time  see eq        the starting state is x        
 rl yc clc closed loop controller for full problem sop yc using states xt  
 t  y t   c   t   c   t   with x        y    c     c     
solving the problem rl y olc yields an action vector  the problems rl yc clc and
rl y clc result in state dependent controllers  we do not present results for the fourth
natural problem rl yc olc  because they offer no additional insights 
for interpolation between states we used       multilayer perceptrons  mlps  e g 
bishop        in the case of rl y olc because of the extremely large state space     dimensions for t   n      we used radial basis function  rbf  networks in the case of
rl yc clc and rl y clc  because they produced faster  more stable and robust results
compared to mlps 
for training the respective networks  we used the direct method that corresponds
to performing one gradient descent step for the current state action pair with the new

estimate as the target value  see e g  baird         the new estimate for q is given

by r   qt  x   u    and for q by r    qt  x   u    compare the right sides of the update
equations            
  

figeibel   wysotzki

 a 
 
   
   
   
   
 
   
   
   
   
 

outflow rate

inflow

 b 

 

 

 

 

 
    
   
    
   
    
   
    

             
time

 c 

omega     
   

 

 

 

 

             
time

omega     
   
outflow rate

outflow rate

 d 
 
    
   
    
   
    
   
    
 

 

 

 

             
time

 
    
   
    
   
    
   
    

omega    
   

 

 

 

 

             
time

figure    rl y clc   a  the inflow rates f  t  for    runs   b    c    d  example runs of
policies for                      i e  p                      it holds fspec       

   experiments
in this section  we examine the experimental results obtained for the tank control task
           in section     we discuss the linear case and compare the results to those of li
et al          for the linear case  we consider the closed loop controller obtained by solving
rl y clc  sect         and the open loop controller related to the rl problem rl y olc
 sect          for the closed loop controller  we discuss the problem of non zero covariances
between variables of different time steps  the nonlinear case is discussed in section     
    the problems rl y clc and rl y olc
we start with the simplified problems  rl y clc and rl y olc  derived from sop y that
is discussed by li et al          in sop y the concentrations are not considered  and there
is only one inflow rate f  t    f   t    f   t   the parameter settings in table    first five
lines  were taken from li et al          the minimum and maximum values for the actions
were determined by preliminary experiments 
li et al  define the inflows  f              f      t as having a gaussian distribution with
the mean vector
                                                                                t  
  

    

firisk sensitive reinforcement learning

   
   
   
risk
 
    
value

    

weighted
    
    
    
 

 

  

  

  

xi






figure    rl y clc  estimates of the risk   x     the value v   x     and of v   x     




v   x       x    for different values of  

the covariance matrix is given by





c 

    r  
  
    r  
   


  n   r  n   


     n   r  n   
   
   


 

n
 







    

with i         the correlation of the inflows of time i and j is defined by
rij   rji           j  i 

    

for    i  n     i   j  n     from li et al          the inflow rates for ten example
runs are depicted in figure   a  
      the problem rl y clc  constraints for tank level 
we start with the presentation of the results for the problem rl y clc  where the control
 i e  the outflow f   depends only on the time t and the tank level  because x     x    the
overall performance of the policy as defined in      corresponds to its performance for x   




v    v   x     


it holds that x        y     v   x    is the value with respect to the policy  learned for the
weighted criterion function v   see also       the respective risk is


  x     




in figure   the estimated  risk   x    and the estimated value v   x    are depicted


for different values of   both the estimate for the risk   x    and that for value v   x   
   all values and policies presented in the following were estimated by the learning algorithm  note that
in order to enhance the readability  we have also denoted the learned policy as   

  

figeibel   wysotzki

   
   
   
   
   
 
    
                                                    
xi

figure    rl y clc  difference between the weighted criteria  for an explanation see text 

increase with   given a fixed value p for the admissible probability of constraint violation 

the appropriate     p  can be obtained as the value for which the risk   x    is lower

than     p and has the maximum v   x     due to the variation of the performance  see
fig     we found that this works better than just selecting the maximum   the estimate






of the weighted criterion v   x      v   x       x    is also shown in figure   
the outflow rate f  control variable  for different values of  can be found in figure   bc   note that the rates have a certain variance since they depend on the probabilistic tank
level  we randomly picked one example run for each value of   it is found that the control
values f  t  tend to approach fspec with increasing values of   i e  decreasing values of p  
correlations the definition of the covariance matrix in      and      reveals a high
correlation of the inflow rates in neighboring time steps  in order to better account for this 
it is possible to include information on past time steps in the state description at time t 
because the level y changes according to the inflow rate f   we investigated the inclusion
of past values of y  if the inflow rates were measured  they too could be included in the
state vector  former rewards need not be included because they depend on the past tank
levels  i e  they represent redundant information 
we have compared the performance of the algorithm for the augmented state space
defined by xt    t  y t   y t      y t       depth   history  and the normal state space
xt    t  y t    no history   fig    shows












v      y           v      y     
 

 z
x 

 

   z  
x 

i e  the difference in the weighted criteria for the starting state with respect to the learned
policies   history  and   no history   note that for the starting state x    the past values
have been defined as    the curve in figure   runs mainly above    this means that using
the augmented state space results in a better performance for many values of   note that
   

firisk sensitive reinforcement learning

 a 

 b 
   

 
risk

   

    

   
outflow rate

   

 
    
value

    

    
   
    

    

   

    

    
 

 

 

 

 

  

  

  

 

xi


 

 

 

 
time

  

  

  



figure    rl y olc   a  estimates of the risk   x    and the value v   x    for increasing


values of    b  learned policy  with   x           and v   x          

for larger values of  the original value function overweights the risk so that in both cases
the policy that always chooses the outflow fspec is approximated  this means that the
difference in performance tends to zero 
a similar  but not quite pronounced effect can be observed when using a history of
length   only  in principle  we assume that it is possible to achieve even better performance
by including the full history of tank levels in the state description  but there is a tradeoff between this objective and the difficulty of network training caused by the number of
additional dimensions 
      rl y olc  history of control actions 
the rl problem rl y olc comprises state descriptions consisting of an action history
together with the time  see eq        the starting state has an empty history  i e  x        
the result of learning is a time dependent policy with an implicit dependence on y    the
learned policy  is therefore a fixed vector of actions f              f      that forms a feasible 
but in general suboptimal solution to the problem sop y in figure   


the progression of the risk estimate  i e  of   x     and that for the value  v   x    
for different values of  can be found in figure    the results are not as good as the ones

for rl y clc in figure    the estimated minimum risk is        and the risk   x    grows
much faster than the rl y clc risk in figure   

a policy having a risk   x           is depicted in figure   b   in contrast to the
policies for rl y clc  see figure   b c    the control values do not change in different runs 
      comparison
in table    we have compared the performance of the approach of li et al  with rl y clc
and rl y olc for p       and p        for both rl y clc and rl y olc we performed   
learning runs  for the respective learned policy   the risk   x    and value v   x    were
estimated during      test runs  for rl y clc and rl y olc  the table shows the mean
performance averaged over this    runs together with the standard deviation in parentheses 
   

figeibel   wysotzki

table    comparison of est  squared deviation to fspec  i e  v   x     for results of li et
al  with results for rl y clc and rl y olc for p                and p      
          smaller values are better 
approach
li et al        
rl y clc
rl y olc

p      
      
                 
                 

p      
      
              
               

it is found that  in average  the policy determined for rl y clc performs better than that
obtained through the approach of li et al          with respect to the estimated squared
deviation to the desired outflow fspec   i e  with respect to v   x      the policy obtained
for rl y olc performs better for p       and worse for p        the maximal achievable
probability for holding constraints was      sd      for rl y clc  and and       sd        
for rl y olc  li et al  report p         for their approach 
the approach of neuneier and mihatsch        considers the worst case outcomes of a
policy  i e  risk is related to the variability of the return  neuneier and mihatsch show that
the learning algorithm interpolates between risk neutral and the worst case criterion and
has the same limiting behavior as the exponential utility approach 
   

risk
value

   
   
   

risk

   
value

   
 
    
  

    

 

   

 

kappa

figure    risk and value for several values of 
the learning algorithm of neuneier and mihatsch has a parameter              that
allows to switch between risk averse behavior        risk neutral behavior         and
risk seeking behavior        if the agent is risk seeking  it prefers policies with a
good best case outcome  figure   shows risk  probability of constraint violation  and value
for the starting state in regard to the policy computed with the algorithm of neuneier and
mihatsch  obviously  the algorithm is able to find the maximum value policy yielding a zero
deviation of fspec   corresponding to choosing f   fspec       in all states  but the learning
result is not sensitive to the risk parameter   the reason for this is that the worst case and
the best case returns for the policy that always chooses the outflow     also correspond to
   

firisk sensitive reinforcement learning

 a 

inflow

 b 
 

 

   

   
ymax
mu t      

   

y

   

   

   
mu t      

c max
c 

ymin
   

   

c min
 

 
 

 

 

 

 

  

  

  

 

time

 

 

 

 

  

  

  

  

time

figure    rl yc clc   a   t         and  t         profiles of the two mode means  
 b  the tank level y t  and the concentration c   t  for    example runs using the
minimum risk policy 

   which is the best return possible  implying a zero variance of the return   the approach
of neuneier and mihatsch and variance based approaches are therefore unsuited for the
problem at hand 
    the problem rl yc clc  constraints for tank level and concentrations 
in the following we will consider the full problem rl yc clc  the two inflows f  and f  are
assumed to have equal gaussian distributions such that the distribution of the cumulative
inflow f  t    f   t    f   t  is described by the covariance matrix in      and the mean
vector  in       see also figure   a  
in order to demonstrate the applicability of our approach to non gaussian distributions 
we have chosen bimodal distributions for the inflow concentrations c  and c    the underlying
assumption is that the upstream plants either all have an increased output  or all have a
lower output  e g  due to different hours or weekdays 
the distribution of the inflow concentration ci    t  is characterized by two gaussian
distributions with means
 t       k       
where k        and              the value of k         is chosen at the beginning of
each run with equal probability for each outcome  this means that the overall mean value
of ci    t  is given by  t   the profiles of the mean values of the modes can be found in
figure   a   ci   is given as ci    t         ci    t   the minimum and maximum values for
the concentrations ci  t  can be found in table    and also in figure   b   note that the
concentrations have to be controlled indirectly by choosing an appropriate outflow f  
the developing of the risk and the value of the starting state is shown in figure    
the resulting curves behave similar to that for the problem rl y clc depicted in figure   
both value and risk increase with   it can be seen that the algorithm covers a relatively
broad range of policies with different value risk combinations 
   

figeibel   wysotzki

   
   
   
risk
   
 
    
    

value

    
    
 

 

  

  

  

  

  

  

  

  

  

xi








figure     rl yc clc  estimated risk   x     value v   x     and v   x      v   x    


  x    for different values of  

for the minimum risk policy  the curves of the tank level y and the concentration
c  can be found in figure   b   the bimodal characteristics of the substance   inflow
concentrations are reflected by c   t   it holds c   t       c   t    the attainable minimum
risk is        increasing the weight  leads to curves similar to that shown in the figures  
and    we assume that the minimum achievable risk can be decreased by the inclusion of
additional variables  e g  inflow rates and concentrations  and or by the inclusion of past
values as discussed in section        the treatment of a version with an action history is
analogous to section        we therefore conclude presentation of the experiments at this
point 

   conclusion
in this paper  we presented an approach for learning optimal policies with constrained risk
for mdps with error states  in contrast to other rl and dp approaches that consider risk
as a matter of the variance of the return or of its worst outcomes  we defined risk as the
probability of entering an error state 
we presented a heuristic algorithm that aims at learning good stationary policies that is
based on a weighted formulation of the problem  the weight of the original value function
is increased in order to maximize the return while the risk is required to stay below the
given threshold  for a fixed weight and a finite state space  the algorithm converges to an
optimal policy in the case of an undiscounted value function  for the case that the state
space is finite  contains no cycles  and that      holds  we conjecture the convergence of
the learning algorithm to a policy  but assume that it can be suboptimal for the weighted
formulation  if an optimal stationary policy exists for the weighted formulation  it is a
feasible  but generally suboptimal solution for the constrained problem 
   

firisk sensitive reinforcement learning

the weighted approach that is combined with an adaptation of  is a heuristic for searching the space of feasible stationary policies of the original constrained problem  which to
us seems relatively intuitive  we conjecture that better policies could be found by allowing
state dependent weights  x  with a modified adaptation strategy  and by extending the
considered policy class 
we have successfully applied the algorithm to the control of the outflow of a feed tank
that lies upstream of a distillation column  we started with a formulation as a stochastic
optimal control problem with chance constraints  and mapped it to a risk sensitive learning
problem with error states  that correspond to constraint violation   the latter problem was
solved using the weighted rl algorithm 
the crucial point in reformulation as an rl problem was the design of the state space 
we found that the algorithm consistently performed better when state information was
provided to the learner  using the time and the action history resulted in very large state
spaces  and a poorer learning performance  rbf networks together with sufficient state
information facilitated excellent results 
it must be mentioned that the use of rl together with mlp or rbf network based
function approximation suffers from the usual flaws  non optimality of the learned network 
potential divergence of the learning process  and long learning times  in contrast to an
exact method  no a priori performance guarantee can be given  but of course an a posteriori
estimate of the performance of the learned policy can be made  the main advantage of the
rl method lies in its broad applicability  for the tank control task  we achieved very good
results compared to those obtained through a  mostly  analytical approach 
for the cases  x      or      further theoretical investigations of the convergence and
more experiments are required  preliminary experiments have shown that oscillations may
occur in our algorithm  but the behavior tends to oscillate between sensible policies without
getting too bad in between although the convergence and usefulness of the policies remains
an open issue 
oscillations can be prevented by using a discounted risk that leads to an underestimation
of the actual risk  the existence of an optimal policy and convergence of the learning
algorithm for a fixed  can be guaranteed in the case of a finite mdp  a probabilistic
interpretation of the discounted risk can be given by considering     as the probability of
exiting from the control of the mdp  bertsekas         the investigation of the discounted
risk may be worthwhile in its own right  for example  if the task has long episodes  or if
it is continuing  i e  non episodic  it can be more natural to give a larger weight to error
states occurring closer to the current state 
we have designed our learning algorithm as an online algorithm  this means that learning is accomplished using empirical data obtained through interaction with a simulated or
real process  the use of neural networks allows to apply the algorithm to processes with
continuous state spaces  in contrast  the algorithm described by dolgov and durfee       
can only be applied in the case of a known finite mdp  such a model can be obtained
in the case of a continuous state process by finding an appropriate discretization and estimating the state transition probabilities together with the reward function  although
such discretization prevents the application of dolgov and durfees algorithm to rl y olc 
where a    dimensional state space is encountered  it can probably be applied in the case
of rl y olc  we plan to investigate this point in future experiments 
   

figeibel   wysotzki

the question arises as to whether our approach can also be applied to stochastic optimal
control problems with other types of chance constraints  consider a conjunction of chance
constraints
p c     p            p cn      pn    
    
where each ct is a constraint system containing only variables at time t  and pt is the
respective probability threshold       requires an alternative rl formulation where the risk
of a state only depends on the next reward  and where each time step has its own t   the
solution with a modified version of the rl algorithm is not difficult 
if each of the ct in      is allowed to be a constraint system over state variables depending on t  t  things get more involved because several risk functions are needed for
each state  we plan investigating these cases in the future 

acknowledgments we thank dr  pu li for providing the application example and for his
helpful comments  we thank onder gencaslan for conducting first experiments during his
masters thesis 

references
altman  e          constrained markov decision processes  chapman and hall crc 
baird  l          residual algorithms  reinforcement learning with function approximation  in proc    th international conference on machine learning  pp        morgan
kaufmann 
bawas  v  s          optimal rules for ordering uncertain prospects  journal of finance 
            
bertsekas  d  p          dynamic programming and optimal control  athena scientific 
belmont  massachusetts  volumes   and   
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
belmont  ma 
bishop  c  m          neural networks for pattern recognition  oxford university press 
oxford 
blythe  j          decision theoretic planning  ai magazine               
borkar  v          q learning for risk sensitive control  mathematics of operations research 
               
coraluppi  s     marcus  s          risk sensitive and minimax control of discrete time 
finite state markov decision processes  automatica             
crites  r  h     barto  a  g          elevator group control using multiple reinforcement
learning agents  machine learning                   
dolgov  d     durfee  e          approximating optimal policies for agents with limited
execution resources  in proceedings of the eighteenth international joint conference
on artificial intelligence  pp            aaai press 
   

firisk sensitive reinforcement learning

feinberg  e     shwartz  a          markov decision models with weighted discounted
criteria  math  of operations research             
feinberg  e     shwartz  a          constrained discounted dynamic programming  math 
of operations research             
feinberg  e     shwartz  a          constrained dynamic programming with two discount
factors  applications and an algorithm  ieee transactions on automatic control 
           
fishburn  p  c          mean risk analysis with risk associated with below target returns 
american economics review                 
freund  r          the introduction of risk into a programming model  econometrica     
       
fulkerson  m  s   littman  m  l     keim  g  a          speeding safely  multi criteria
optimization in probabilistic planning  in proceedings of the fourteenth national
conference on artificial intelligence  p       aaai press mit press 
gabor  z   kalmar  z     szepesvari  c          multi criteria reinforcement learning  in
proc    th international conf  on machine learning  pp          morgan kaufmann 
san francisco  ca 
geibel  p          reinforcement learning with bounded risk  in brodley  e     danyluk 
a  p   eds    machine learning   proceedings of the eighteenth international conference  icml     pp          morgan kaufmann publishers 
heger  m          consideration of risk in reinforcement learning  in proc    th international conference on machine learning  pp          morgan kaufmann 
kall  p     wallace  s  w          stochastic programming  wiley  new york 
koenig  s     simmons  r  g          risk sensitive planning with probabilistic decision
graphs  in doyle  j   sandewall  e     torasso  p   eds    kr    principles of knowledge representation and reasoning  pp          san francisco  california  morgan
kaufmann 
kushmerick  n   hanks  s     weld  d  s          an algorithm for probabilistic leastcommitment planning   in aaai  pp           
li  p   wendt  m   arellano garcia    wozny  g          optimal operation of distillation
processes under uncertain inflows accumulated in a feed tank  aiche journal     
         
liu  y   goodwin  r     koenig  s       a   risk averse auction agents  in rosenschein  j  
sandholm  t     wooldridge  m  yokoo  m   eds    proceedings of the second international joint conference on autonomous agents and multiagent systems  aamas     pp          acm press 
liu  y   goodwin  r     koenig  s       b   risk averse auction agents   in aamas  pp 
       
markowitz  h  m          portfolio selection  the journal of finance              
markowitz  h  m          portfolio selection  john wiley and sons  new york 
   

figeibel   wysotzki

mihatsch  o     neuneier  r          risk sensitive reinforcement learning  machine learning                   
neuneier  r     mihatsch  o          risk sensitive reinforcement learning  in michael
s  kearns  sara a  solla  d  a  c   ed    advances in neural information processing
systems  vol      mit press 
ross  s  m          introduction to probability models  academic press  new york 
roy  a  d          safety first and the holding of assets  econometrica                 
smart  w  d     kaelbling  l  p          effective reinforcement learning for mobile robots  in proceedings of the      ieee international conference on robotics and
automation  icra       
stephan  v   debes  k   gross  h  m   wintrich  f     wintrich  h          a new control
scheme for combustion processes using reinforcement learning based on neural networks  international journal of computational intelligence and applications        
       
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit
press 
tsitsiklis  j  n          asynchronous stochastic approximation and q learning  machine
learning                 
watkins  c  j  c  h          learning from delayed rewards  ph d  thesis  kings college 
oxford 
watkins  c  j  c  h     dayan  p          q learning  machine learning           special
issue on reinforcement learning 
wendt  m   li  p     wozny  g          non linear chance constrained process optimization
under uncertainty  ind  eng  chem  res                

   

fi