journal artificial intelligence research                  

submitted        published      

efficiency versus convergence boolean kernels
on line learning algorithms
roni khardon

roni cs tufts edu

department computer science  tufts university
medford       

dan roth

danr cs uiuc edu

department computer science  university illinois
urbana  il       usa

rocco a  servedio

rocco cs columbia edu

department computer science  columbia university
new york  ny      

abstract
paper studies machine learning problems example described using
set boolean features hypotheses represented linear threshold elements 
one method increasing expressiveness learned hypotheses context
expand feature set include conjunctions basic features  done explicitly
possible using kernel function  focusing well known perceptron
winnow algorithms  paper demonstrates tradeoff computational
efficiency algorithm run expanded feature space
generalization ability corresponding learning algorithm 
first describe several kernel functions capture either limited forms conjunctions conjunctions  show kernels used efficiently run
perceptron algorithm feature space exponentially many conjunctions  however show using kernels  perceptron algorithm provably make
exponential number mistakes even learning simple functions 
consider question whether kernel functions analogously used
run multiplicative update winnow algorithm expanded feature space
exponentially many conjunctions  known upper bounds imply winnow algorithm
learn disjunctive normal form  dnf  formulae polynomial mistake bound
setting  however  prove computationally hard simulate winnows
behavior learning dnf feature set  implies kernel functions
correspond running winnow problem efficiently computable 
general construction run winnow kernels 

   introduction
problem classifying objects one two classes positive negative
examples concept often studied machine learning  task machine learning
extract classifier given pre classified examples   problem learning
data  example represented set n numerical features  example
c
    
ai access foundation  rights reserved 

fikhardon  roth    servedio

seen point euclidean space  n   common representation classifiers
case hyperplane dimension  n    splits domain examples
two areas positive negative examples  representation known linear
threshold function  many learning algorithms output hypothesis represented
manner developed  analyzed  implemented  applied practice 
particular interest paper well known perceptron  rosenblatt        block 
      novikoff        winnow  littlestone        algorithms intensively
studied literature 
well known expressiveness linear threshold functions quite limited  minsky   papert         despite fact  perceptron winnow
applied successfully recent years several large scale real world classification problems 
one example  snow system  roth        carlson  cumby  rosen    roth       
successfully applied variations perceptron winnow problems natural language
processing  snow system extracts basic boolean features x            xn labeled pieces
text data order represent examples  thus features numerical values restricted         several ways enhance set basic features x             xn
perceptron winnow  one idea expand set basic features x             xn using
conjunctions  x  x  x    use expanded higher dimensional examples 
conjunction plays role basic feature  examples perceptron
winnow  fact approach snow system takes running perceptron
winnow space restricted conjunctions basic features  idea closely
related use kernel methods  see e g  book cristianini shawe taylor
        feature expansion done implicitly kernel function  approach clearly leads increase expressiveness thus may improve performance 
however  dramatically increases number features  from n   n conjunctions used   thus may adversely affect computation time convergence
rate learning  paper provides theoretical study performance perceptron
winnow run expanded feature spaces these 
    background  on line learning perceptron winnow
describing results  recall necessary background on line learning
model  littlestone        perceptron winnow algorithms 
given instance space x possible examples  concept mapping instances
one two  or more  classes  concept class c  x simply set concepts  on line
learning concept class c fixed advance adversary pick concept c c 
learning modeled repeated game iteration adversary
picks example x x  learner gives guess value c x  told
correct value  count one mistake iteration value predicted
correctly  learning algorithm learns concept class c mistake bound
choice c c  arbitrarily long  sequence examples  learner guaranteed
make mistakes 
paper consider case examples given boolean features 
x         n   two class labels denoted      thus x       n  
labeled example hx   i positive example  labeled example hx   i negative
   

fiefficiency versus convergence boolean kernels

example  concepts consider built using logical combinations n base
features interested mistake bounds polynomial n 
      perceptron
throughout execution perceptron maintains weight vector w   n initially
                upon receiving example x  n algorithm predicts according
linear threshold function w x    prediction   label    false positive
prediction  vector w set w x  prediction   label  
 false negative  w set w   x  change made w prediction correct 
many variants basic algorithm proposed studied particular one
add non zero threshold well learning rate controls size update
w  discussed section   
famous perceptron convergence theorem  rosenblatt        block        novikoff 
      bounds number mistakes perceptron algorithm make 
theorem   let hx    y  i          hxt   yt sequence labeled examples xi  n   kxi k
r yi        i  let u  n       yi  u xi   i 
 
 
mistakes example sequence 
perceptron makes r kuk
 
      winnow
winnow algorithm  littlestone        similar structure  winnow maintains
hypothesis vector w  n initially w                   winnow parameterized
promotion factor     threshold      upon receiving example x        n
winnow predicts according threshold function w x   prediction  
label   xi     value wi set wi    demotion
step  prediction   label   xi     value wi
set wi   promotion step  change made w prediction correct 
purposes following mistake bound  implicit littlestones work        
interest 
theorem   let target function k literal monotone disjunction f  x             xn    
xi  xik   sequence examples       n labeled according f number

prediction mistakes made winnow      
n   k           log   
    results
interested computational efficiency convergence perceptron
winnow algorithms run expanded feature spaces conjunctions  specifically 
study use kernel functions expand feature space thus enhance
learning abilities perceptron winnow  refer enhanced algorithms
kernel perceptron kernel winnow 
first result  cf  papers sadohara        watkins        kowalczyk
et al         uses kernel functions show possible efficiently run kernel
perceptron algorithm exponential number conjunctive features 
   

fikhardon  roth    servedio

result     see theorem    algorithm simulates perceptron   n dimensional feature space conjunctions n basic features  given sequence
labeled examples       n prediction update example take poly n  t  time
steps  prove variants result expanded feature space consists
monotone conjunctions conjunctions bounded size 
result closely related one main open problems learning theory 
efficient learnability disjunctions conjunctions  dnf  disjunctive normal form 
expressions   since linear threshold elements represent disjunctions  e g  x  x  x 
true iff x    x    x      theorem   result   imply kernel perceptron
used learn dnf  however  framework values n r theorem  
exponentially large  note n    n r    n   conjunctions used  
hence mistake bound given theorem   exponential rather polynomial
n  question thus arises whether exponential upper bound implied theorem
  essentially tight kernel perceptron algorithm context dnf learning 
give affirmative answer  thus showing kernel perceptron cannot efficiently learn
dnf 
result    monotone dnf f x            xn sequence examples labeled
according f causes kernel perceptron algorithm make    n  mistakes 
result holds generalized versions perceptron algorithm fixed updated
threshold learning rate used  give variant result showing
kernel perceptron fails probably approximately correct  pac  learning model
 valiant        well 
turning winnow  attractive feature theorem   suitable   bound
logarithmic total number features n  e g        n    therefore 
noted several researchers  maass   warmuth         winnow analogue theorem  
could obtained would imply dnf learned computationally efficient
algorithm poly n  mistake bound  however  give strong evidence
winnow analogue theorem   exist 
result    polynomial time algorithm simulates winnow exponentially many monotone conjunctive features learning monotone dnf unless every problem
complexity class  p solved polynomial time  result holds wide
range parameter settings winnow algorithm 
observe that  contrast negative result  maass warmuth shown
winnow algorithm simulated efficiently exponentially many conjunctive
features learning simple geometric concept classes  maass   warmuth        
results thus indicate tradeoff computational efficiency convergence
kernel algorithms rich classes boolean functions dnf formulas  kernel
   angluin        proved dnf expressions cannot learned efficiently using equivalence queries
whose hypotheses dnf expressions  since model exact learning equivalence
queries equivalent mistake bound model consider paper  result implies
online algorithm uses dnf formulas hypotheses efficiently learn dnf  however 
result preclude efficient learnability dnf using different class hypotheses 
kernel perceptron algorithm generates hypotheses thresholds conjunctions rather dnf
formulas  thus angluins negative results apply here 

   

fiefficiency versus convergence boolean kernels

perceptron algorithm computationally efficient run exponentially slow convergence  whereas kernel winnow rapid convergence seems require exponential
runtime 

   kernel perceptron many features
well known hypothesis w perceptron algorithm linear combination
previous examples mistakes made  cristianini   shaw taylor        
precisely  let l v         denote label example v 
p
w   vm l v v set examples algorithm made mistake 
p
p
thus prediction perceptron x   iff wx     vm l v v x   vm l v  v x 
  
example x       n let  x  denote transformation enhanced feature
space space conjunctions  run perceptron algorithm
enhanced space must predict   iff w  x    w weight vector
p
enhanced space  discussion holds iff vm l v   v   x     
p
denoting k v  x     v   x  holds iff vm l v k v  x    
thus never need construct enhanced feature space explicitly  order run
perceptron need able compute kernel function k v  x  efficiently 
idea behind so called kernel methods  applied algorithm  such
support vector machines  whose prediction function inner products examples 
detailed discussion given book cristianini shawe taylor        
thus next theorem simply obtained presenting kernel function capturing
conjunctions 
theorem   algorithm simulates perceptron feature spaces
    conjunctions      monotone conjunctions      conjunctions size k     
monotone conjunctions size k  given sequence labeled examples        n
prediction update example take poly n  t  time steps 
proof  case        includes  n conjunctions  with positive negative literals 
k x  y  must compute number conjunctions true x y  clearly 
literal conjunction must satisfy x thus corresponding bit
x  must value  thus conjunction true x corresponds
subset bits  counting conjunctions gives k x  y     same x y 
same x  y  number original features value x y  i e 
number bit positions xi   yi   kernel obtained independently
sadohara        
express monotone monomials     take k x  y      xy   x y 
number active features common x y  i e  number bit positions
xi   yi     
similarly  case     number conjunctions satisfy x k x  y   
pk same x y 
  kernel reported watkins         case    
l  
l


p
k x  y    kl    xy 
 
 
l
   

fikhardon  roth    servedio

   kernel perceptron many mistakes
section describe simple monotone dnf target function sequence
labeled examples causes monotone monomials kernel perceptron algorithm
make exponentially many mistakes 
x        n write  x  denote number  s x and  described above 
 xy  denote number bit positions xi   yi      need following
well known tail bound sums independent random variables found in 
e g   section     book kearns vazirani        
fact   let x            xm sequence independent     valued random variables 
p
e xi     p  let x denote
i   xi   e x    pm      

pr x         pm  emp

    



pr x       pm  emp

    

 

use following combinatorial property 
lemma   set n bit strings    x            xt         n   en     
 xi     n       xi xj   n        j t 
proof  use probabilistic method               let xi       n chosen
independently setting bit   probability       clear
e  xi      n     applying fact    pr  xi     n     en      thus
probability xi satisfies  xi     n    ten      similarly     j
e  xi xj      n      applying fact   pr  xi xj     n     en       
thus probability xi   xj
   j satisfies  xi xj     n   
n     
n     
n     
  ten    less    thus
    e
value   e
  e
choice x            xt  xi   n     xi xj   n     xi
 xi     n    set  xi   n     s  s  lemma proved 
 
using previous lemma construct difficult data set kernel perceptron 
theorem   monotone dnf f x            xn sequence examples labeled
according f causes kernel perceptron algorithm make   n  mistakes 
proof  target dnf use simple  single conjunction
x  x        xn   original perceptron algorithm n features x            xn easily
seen make poly n  mistakes target function  show
monotone kernel perceptron algorithm runs feature space   n monotone
monomials make     en      mistakes 
recall beginning perceptron algorithms execution   n coordinates
w    first example negative example  n   monomial true
example empty monomial true every example  since w  x      perceptron incorrectly predicts   example  resulting update causes coefficient
w corresponding empty monomial become    n   coordinates
w remain    next example positive example  n   example
w  x      perceptron incorrectly predicts    since  n monotone conjunctions
   

fiefficiency versus convergence boolean kernels

satisfied example resulting update causes w become    n  
coordinates w become    next en      examples vectors x            xt
described lemma    since example  xi     n    example negative 
however show perceptron algorithm predict   examples 
fix value   en      consider hypothesis vector w example

x received  since  xi     n    value w  xi   sum  n    different
coordinates wt correspond monomials satisfied xi   precisely
p
p
w  xi     ai wt   bi wt ai contains monomials satisfied
xi xj j    bi contains monomials satisfied xi
xj j    i  lower bound two sums separately 
let monomial ai   lemma   ai contains n    variables

pn   
monomials ai   using well known bound
thus r   n   
r


p   
 h   o     
        h p    p log p    p  log   p 
j   j    
binary entropy function  found e g  theorem       book
van lint                 n     o n          n terms ai   moreover
value wt must least en      since wt decreases  
p
example  hence ai wt en            n         n   hand  bi
false examples therefore wt demoted wt     
lemma   r   n    every r variable monomial satisfied xi must belong bi  


p
pn   
hence bi wt r n      n   
        n   combining inequalities
r
w xi       n         n     hence perceptron prediction xi   
 
remark   first sight might seem result limited simple special case
perceptron algorithm  several variations exist use  added feature fixed
value enables algorithm update threshold indirectly  via weight w   non
zero fixed  initial  threshold   learning rate   particular three
used simultaneously  generalized algorithm predicts according hypothesis
w x   w updates w w   x w w   promotions similarly
demotions  show exponential lower bounds number mistakes
derived general algorithm well  first  note since kernel
includes feature empty monomial always true  first parameter
already accounted for  two parameters note degree freedom
learning rate fixed threshold since multiplying factor
change hypothesis therefore suffices consider threshold only 
consider several cases value threshold  satisfies          
use sequence examples  first two examples algorithm makes
promotion  n  it may may update  n important  
p
p
examples sequence bounds ai wt bi wt still valid
final inequality proof becomes w xi       n         n         n true
sufficiently large n          n construct following scenario  use
function f   x  x        xn   sequence examples includes     repetitions
example x first bit   bits    example x satisfies
exactly   monomials therefore algorithm make mistakes examples
sequence      initial hypothesis misclassifies  n   start example
   

fikhardon  roth    servedio

sequence repeating example  n classified correctly  de times 
threshold large absolute value e g          n done  otherwise
continue example  n   since weights except empty monomial zero
stage examples  n  n classified way  n misclassified
therefore algorithm makes promotion  argument rest sequence
 except adding term empty monomial  final inequality becomes
w xi       n       n         n         n examples misclassified  thus
cases kernel perceptron may make exponential number mistakes 
    negative result pac model
proof adapted give negative result kernel perceptron pac
learning model  valiant         model example x independently drawn
fixed probability distribution high probability learner must construct
hypothesis h high accuracy relative target concept c distribution d 
see kearns vazirani text        detailed discussion pac learning model 
let probability distribution       n assigns weight     ex 
en      examples
ample  n   weight     example  n   weight    en     
x            xt  
theorem   kernel perceptron run using sample polynomial size p n 
probability least      error final hypothesis least      
proof  probability       first two examples received  n
 n   thus  probability       two examples  as proof above  perceptron
algorithm w     coefficients w equal   
consider sequence examples following two examples  first note
trial  occurrence example  n  i e  occurrence either xi
p
 n example  decrease  n  wt  n      since first two examples
p
w   n      n  wt    n    follows least    n      examples
must occur  n example incorrectly classified negative example  since
consider performance algorithm p n       n      steps 
may ignore subsequent occurrences  n since change algorithms
hypothesis 
observe first example  n algorithm perform
demotion resulting w      possibly changing coefficients well   since
promotions performed rest sample  get w   rest
learning process  follows future occurrences example   n correctly
classified thus may ignore well 
considering examples xi sequence constructed above  may ignore example correctly classified since update made it  follows
perceptron algorithm gone examples  hypothesis formed demotions
examples sequence xi s  difference scenario
algorithm may make several demotions example occurs multiple times
sample  however  inspection proof shows x
p
p
seen algorithm  bounds ai wt bi wt still valid
   

fiefficiency versus convergence boolean kernels

therefore xi misclassified  since sample size p n  sequence
size en      probability weight examples sample      sufficiently
large n error hypothesis least      
 

   computational hardness kernel winnow
section  x       n let  x  denote   n    element vector whose coordinates nonempty monomials  monotone conjunctions  x            xn   say
sequence labeled examples hx    b  i          hxt   bt monotone consistent consistent
monotone function  i e  xik xjk k              n implies bi bj  
monotone consistent labeled examples clearly monotone dnf
formula consistent contains conjunctions  consider following
problem 
kernel winnow prediction      kwp 
instance  monotone consistent sequence   hx    b  i          hxt   bt labeled examples
xi       m bi         unlabeled example z       m  
question  w  z    w n     m    dimensional hypothesis vector
generated running winnow     example sequence h x     b  i        h xt    bt i 
order run winnow  m   nonempty monomials learn monotone dnf 
one must able solve kwp efficiently  main result section proof
kwp computationally hard wide range parameter settings yield
polynomial mistake bound winnow via theorem   
recall  p class counting problems associated n p decision problems  well known every function  p computable polynomial time
p   n p  see book papadimitriou        paper valiant        details
 p  following problem  p hard  valiant        
monotone   sat  m sat 
instance  monotone   cnf boolean formula f   c  c        cr ci    yi  yi   
yij  y            yn    integer k   k  n  
question   f        k  i e  f least k satisfying assignments       n  
theorem   fix      let n    m    let       m    let  

max   
n              log      poly m   polynomial time algorithm
kwp      every function  p computable polynomial time 
proof  n  described theorem routine calculation shows
      m  poly m 



 m
 poly m   
poly m 

   

proof reduction problem m sat  high level idea proof
simple  let  f  k  instance m sat f defined variables y            yn  
winnow algorithm maintains weight wt monomial variables x            xn  
define     correspondence monomials truth assignments       n
   

fikhardon  roth    servedio

f  give sequence examples winnow causes wt   f  y      
wt     f  y        value w  z  thus related  f         note
could control well would sufficient since could use   k
result follow  however parameter algorithm  therefore make
additional updates w  z      f        k  w  z 
 f        k  details somewhat involved since must track resolution
approximations different values final inner product indeed give
correct result respect threshold 
general setup construction  detail  let
u   n       d dlog  e      log e 
n  
v   log
e     
u   
w   log
e    

let defined
  n   u    v n     u w     

   

since       m    using fact log     x  x       x    
log     m     easily follows specified polynomial
n  describe polynomial time transformation maps n variable instance  f  k 
m sat m variable instance  s  z  kwp       hx     b  i          hxt   bt
monotone consistent  xi z belong       m   w  z 
 f        k 
winnow variables x            xm divided three sets a  b c  
 x            xn    b    xn             xn u   c    xn u              xm    unlabeled example z
 n u  mnu   i e  variables b set   variables c set   
p
p
thus w  z     mb  mab     t wt   mb     t b wt
p
mab   ab t a   t b   wt   refer monomials    type a monomials 
monomials    b type b monomials  monomials ab       b   
type ab monomials 
example sequence divided four stages  stage   results  f        
described n variables correspond n variables cnf formula
f  stage   results q  f        positive integer q specify later 
stages     together result mb   mab q k  thus final value w  z 
approximately   q   f        k   w  z   f        k 
since variables c   z  includes variable c value wt
affect w  z   variables c slack variables  i  make winnow
perform correct promotions demotions  ii  ensure monotone consistent 
stage    setting  f        define following correspondence
truth assignments       n monomials   yit     xi
present t  clause yi  yi  f  stage   contains v negative examples
xi    xi      xi     xi a  show     winnow makes
false positive prediction examples     stage   winnow never
   

fiefficiency versus convergence boolean kernels

promotion example variable set    consider
f  y        since examples include example monomial
demoted least v times  result stage     w    
f  y           wt v f  y        thus    f           
         n v       
show stage   examples cause winnow make false positive prediction
negative examples xi    xi      xi     described
above  negative example stage   six new slack variables x             x   c
used follows  stage   dlog     e repeated instances positive example
x     x       bits    examples cause promotions result
wx     wx     wx   x     hence wx       two groups
similar examples  the first x     x        second x     x        cause
wx      wx       next example negative example
xi    xi       xi     xi a  x     x     x       bits   
example w  x    wx     wx     wx   winnow makes false positive
prediction 
since f n  clauses v negative examples per clause 
construction carried using  v n  slack variables xn u              xn u   v n   
thus         claimed above 
stage    setting q  f        first stage   example positive example
xi     xi a  xn u   v n         bits    since  n
monomials contain xn u   v n     satisfied example wt     
w  x     n    f               n     since    m  poly m     n    recall
equation        n     resulting promotion w  x   
  n    f                 n     let
q   dlog    n    e  

q  n     q    n    

   

stage   consists q repeated instances positive example described above 
promotions w  x    q   n    f                q  n       since    
 f               n
q     q   f                q  n      

   

equation     gives value throughout rest argument 
calculations stages      start stage   type b typeab monomial wt      n variables u variables b
start stage   mb    u   mab     n     u     since example
stages     satisfies xi a  end stage   still q   f             
mab still   n     u     therefore end stage  
w  z    mb   q   f                  n     u    
   

fikhardon  roth    servedio

simplify notation let
    n     u    q k 
ideally end stage   value mb would q   since would imply
w  z      q   f        k  least  f        k  however
necessary mb assume exact value  since  f        must integer
             long
 
   
mb     q
 
get
 
  q   f        k         w  z      q   f        k         
 
 f        k clearly w  z    hand  f          k
since  f        integer value  f        k   get w  z      therefore
remains construct examples stages     b satisfies
equation     
next calculate appropriate granularity d  note k   n   equation     q k       recall equations          
 
n   u    n     m  poly m       n u   n  poly m   n  u   consequently
certainly       equation            q  n       q  
let
c   dlog  e 

 
qc q   d 
 

   

unique smallest positive integer p     satisfies pqc        q  
stage   examples result mb satisfying p   mb   p        that 
 
qc   pqc     q
 
  q

 
q    n    qc
 

qc

 

c   n  

 

   

   
   
   

    holds since k    thus  by definition d    q
equivalent equation      inequality     follows equations         
hence
    p c    n      n   d c    log e      u   

    

second inequality chain follows equation      use
following lemma 
   

fiefficiency versus convergence boolean kernels

lemma           p       monotone cnf f  p
  boolean variables   clauses  exactly p satisfying assignments
          constructed   p poly    time 
proof  proof induction    base case       p     f   p   x   
assuming lemma true                k prove     k      
  p  k   desired cnf fk   p   xk   fk p   since fk p k
clauses fk   p k     clauses   k     p  k     desired cnf
fk   p   xk   fk p k   distributing xk clause fk p k write fk   p
cnf k clauses  p    k fk p   x   
 
stage    setting mb p  let fu p r clause monotone cnf formula
u variables b p satisfying assignments  similar stage    clause
fu p   stage   w negative examples corresponding clause  stage
  slack variables c used ensure winnow makes false positive prediction
negative example  thus examples stage   cause mb   p    
         u w        since six slack variables c used negative example
rw u w negative examples  slack variables xn u   v n               xm 
sufficient stage   
stage    setting mb   mab q k  remains perform q c
promotions examples xi b set    cause mb equal
 p      qc   inequalities established above  give us
 
 
pqc    p      qc   mb     q     qc     q
 
 
desired 
order guarantee q c promotions use two sequences examples length
n
u n
q du
log e log e c respectively  first show positive numbers 
follows directly definitions u   n       d dlog  e      log e c   dlog  e
n
 n   by definition equation      bounded
u
log c  since    
polynomial m  clearly log   n       u n   log    since
n    
u n
n
q   dlog    n    e   implies q   log   
    du
log e  q log e     
log  
n
first q u
log e examples stage   positive example
xi b set   xm       first time example received 
 
w  x     u   p        u      since     n   inspection u  u       
n
winnow performs promotion  similarly  q u
log e occurrences example 

qd u n e
qd u n e
w  x    log   u   p         log  u    q  n    

promotions indeed performed occurrence 
mb  

n
qd u
e
log

 p       

n
remaining examples stage   u
log e c repetitions positive example x
xi b set   xm      promotions occurred repetition

   

fikhardon  roth    servedio

example would w  x   

n
du
ec
log

  u  

n
qd u
e
log

 p         need

show quantity less   reexpress quantity
qc  p       

n
ec u
du
log
 

 
qc  p         pqc   qc
 
  q
 
  q
 
  
  q
 
 

 

    

u n ec

     follows     definition c  finally  log  u
 

  u nc log     u n     
     q   last inequality equation    
 n  
previous inequality inspection values   u   combining two
bounds see indeed w  x     
finally  observe construction example sequence monotone consistent 
since   poly n  contains poly n  examples transformation m sat
kwp     polynomial time computable theorem proved 
  theorem   

   conclusion
linear threshold functions weak representation language interesting learning algorithms  therefore  linear learning algorithms learn expressive
functions  necessary expand feature space applied 
work explores tradeoff computational efficiency convergence using
expanded feature spaces capture conjunctions base features 
shown iteration kernel perceptron algorithm
executed efficiently  algorithm provably require exponentially many updates even
learning function simple f  x    x  x        xn   hand  kernel
winnow algorithm polynomial mistake bound learning polynomial size monotone
dnf  results show widely accepted computational hardness assumption
impossible efficiently simulate execution kernel winnow  latter implies
general construction run winnow using kernel functions 
results indicate additive multiplicative update algorithms lie opposite
extremes tradeoff computational efficiency convergence  believe
fact could significant practical implications  demonstrating provable limitations using kernel functions correspond high degree feature expansions 
results lend theoretical justification common practice using small degree
similar feature expansions well known polynomial kernel  
since publication initial conference version work  khardon  roth   
servedio         several authors explored closely related ideas  one show
construction negative results perceptron extend  either pac
   boolean kernels different standard polynomial kernels conjunctions
weighted equally  allow negations 

   

fiefficiency versus convergence boolean kernels

online setting  related algorithms support vector machines work constructing maximum margin hypothesis consistent examples  paper  khardon
  servedio        gives analysis pac learning performance maximum margin
algorithms monotone monomials kernel  derives several negative results thus
giving negative evidence monomial kernel  paper  cumby   roth 
      kernel expressions description logic  generalizing monomials kernel 
developed successfully applied natural language molecular problems  takimoto warmuth        study use multiplicative update algorithms
winnow  such weighted majority  obtain positive results restricting
type loss function used additive base features  chawla et al        
studied monte carlo estimation approaches approximately simulate winnow algorithms performance run space exponentially many features  use
kernel methods logic learning developing alternative methods feature expansion
multiplicative update algorithms remain interesting challenging problems
investigated 

acknowledgments
work partly done khardon university edinburgh partly
servedio harvard university  authors gratefully acknowledge financial
support work epsrc grant gr n       nsf grant iis         research semester fellowship award tufts university  khardon   nsf grants itr iis          itr iis         iis          roth   nsf grant ccr          nsf
mathematical sciences postdoctoral fellowship  servedio  

references
angluin  d          negative results equivalence queries  machine learning            
block  h          perceptron  model brain functioning  reviews modern
physics             
carlson  a   cumby  c   rosen  j     roth  d          snow learning architecture 
tech  rep  uiucdcs r          uiuc computer science department 
chawla  d   li  l     scott   s          approximating weighted sums exponentially
many terms  journal computer system sciences             
cristianini  n     shaw taylor  j          introduction support vector machines 
cambridge press 
cumby  c     roth  d          kernel methods relational learning  proc 
international conference machine learning 
kearns  m     vazirani  u          introduction computational learning theory 
mit press  cambridge  ma 
   

fikhardon  roth    servedio

khardon  r   roth  d     servedio  r          efficiency versus convergence boolean
kernels on line learning algorithms  dietterich  t  g   becker  s     ghahramani 
z   eds    advances neural information processing systems     cambridge  ma 
mit press 
khardon  r     servedio  r          maximum margin algorithms boolean kernels 
proceedings sixteenth annual conference computational learning theory 
pp        
lint  j  v          introduction coding theory  springer verlag 
littlestone  n          learning quickly irrelevant attributes abound  new linearthreshold algorithm  machine learning            
maass  w     warmuth  m  k          efficient learning virtual threshold gates 
information computation                  
minsky  m     papert  s          perceptrons  introduction computational geometry 
mit press  cambridge  ma 
novikoff  a          convergence proofs perceptrons  proceeding symposium
mathematical theory automata  vol      pp         
papadimitriou  c          computational complexity  addison wesley 
rosenblatt  f          perceptron  probabilistic model information storage
organization brain  psychological review             
roth  d          learning resolve natural language ambiguities  unified approach 
proc  american association artificial intelligence  pp         
sadohara  k          learning boolean functions using support vector machines  proc 
conference algorithmic learning theory  pp          springer  lnai      
takimoto  e     warmuth  m          path kernels multiplicative updates  journal
machine learning research            
valiant  l  g          complexity enumeration reliability problems  siam
journal computing            
valiant  l  g          theory learnable  communications acm          
         
watkins  c          kernels matching operations  tech  rep  csd tr        computer
science department  royal holloway  university london 

   


