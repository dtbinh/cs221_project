journal of artificial intelligence research                  

submitted        published      

efficiency versus convergence of boolean kernels for
on line learning algorithms
roni khardon

roni cs tufts edu

department of computer science  tufts university
medford  ma      

dan roth

danr cs uiuc edu

department of computer science  university of illinois
urbana  il       usa

rocco a  servedio

rocco cs columbia edu

department of computer science  columbia university
new york  ny      

abstract
the paper studies machine learning problems where each example is described using a
set of boolean features and where hypotheses are represented by linear threshold elements 
one method of increasing the expressiveness of learned hypotheses in this context is to
expand the feature set to include conjunctions of basic features  this can be done explicitly
or where possible by using a kernel function  focusing on the well known perceptron
and winnow algorithms  the paper demonstrates a tradeoff between the computational
efficiency with which the algorithm can be run over the expanded feature space and the
generalization ability of the corresponding learning algorithm 
we first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions  we show that these kernels can be used to efficiently run
the perceptron algorithm over a feature space of exponentially many conjunctions  however we also show that using such kernels  the perceptron algorithm can provably make an
exponential number of mistakes even when learning simple functions 
we then consider the question of whether kernel functions can analogously be used
to run the multiplicative update winnow algorithm over an expanded feature space of
exponentially many conjunctions  known upper bounds imply that the winnow algorithm
can learn disjunctive normal form  dnf  formulae with a polynomial mistake bound in
this setting  however  we prove that it is computationally hard to simulate winnows
behavior for learning dnf over such a feature set  this implies that the kernel functions
which correspond to running winnow for this problem are not efficiently computable  and
that there is no general construction that can run winnow with kernels 

   introduction
the problem of classifying objects into one of two classes being positive and negative
examples of a concept is often studied in machine learning  the task in machine learning
is to extract such a classifier from given pre classified examples   the problem of learning
from data  when each example is represented by a set of n numerical features  an example
c
    
ai access foundation  all rights reserved 

fikhardon  roth    servedio

can be seen as a point in euclidean space  n   a common representation for classifiers in
this case is a hyperplane of dimension  n     which splits the domain of examples into
two areas of positive and negative examples  such a representation is known as a linear
threshold function  and many learning algorithms that output a hypothesis represented in
this manner have been developed  analyzed  implemented  and applied in practice  of
particular interest in this paper are the well known perceptron  rosenblatt        block 
      novikoff        and winnow  littlestone        algorithms that have been intensively
studied in the literature 
it is also well known that the expressiveness of linear threshold functions is quite limited  minsky   papert         despite this fact  both perceptron and winnow have been
applied successfully in recent years to several large scale real world classification problems 
as one example  the snow system  roth        carlson  cumby  rosen    roth        has
successfully applied variations of perceptron and winnow to problems in natural language
processing  the snow system extracts basic boolean features x            xn from labeled pieces
of text data in order to represent the examples  thus the features have numerical values restricted to         there are several ways to enhance the set of basic features x             xn
for perceptron or winnow  one idea is to expand the set of basic features x             xn using
conjunctions such as  x   x   x    and use these expanded higher dimensional examples  in
which each conjunction plays the role of a basic feature  as the examples for perceptron or
winnow  this is in fact the approach which the snow system takes running perceptron or
winnow over a space of restricted conjunctions of these basic features  this idea is closely
related to the use of kernel methods  see e g  the book of cristianini and shawe taylor
        where a feature expansion is done implicitly through the kernel function  the approach clearly leads to an increase in expressiveness and thus may improve performance 
however  it also dramatically increases the number of features  from n to   n if all conjunctions are used   and thus may adversely affect both the computation time and convergence
rate of learning  the paper provides a theoretical study of the performance of perceptron
and winnow when run over expanded feature spaces such as these 
    background  on line learning with perceptron and winnow
before describing our results  we recall some necessary background on the on line learning
model  littlestone        and the perceptron and winnow algorithms 
given an instance space x of possible examples  a concept is a mapping of instances into
one of two  or more  classes  a concept class c   x is simply a set of concepts  in on line
learning a concept class c is fixed in advance and an adversary can pick a concept c  c 
the learning is then modeled as a repeated game where in each iteration the adversary
picks an example x  x  the learner gives a guess for the value of c x  and is then told the
correct value  we count one mistake for each iteration in which the value is not predicted
correctly  a learning algorithm learns a concept class c with mistake bound m if for any
choice of c  c and any  arbitrarily long  sequence of examples  the learner is guaranteed
to make at most m mistakes 
in this paper we consider the case where the examples are given by boolean features 
that is x         n   and we have two class labels denoted by   and    thus for x        n  
a labeled example hx   i is a positive example  and a labeled example hx   i is a negative
   

fiefficiency versus convergence of boolean kernels

example  the concepts we consider are built using logical combinations of the n base
features and we are interested in mistake bounds that are polynomial in n 
      perceptron
throughout its execution perceptron maintains a weight vector w    n which is initially
                upon receiving an example x   n the algorithm predicts according to the
linear threshold function w  x     if the prediction is   and the label is    false positive
prediction  then the vector w is set to w  x  while if the prediction is   and the label is  
 false negative  then w is set to w   x  no change is made to w if the prediction is correct 
many variants of this basic algorithm have been proposed and studied and in particular one
can add a non zero threshold as well as a learning rate that controls the size of update to
w  some of these are discussed further in section   
the famous perceptron convergence theorem  rosenblatt        block        novikoff 
      bounds the number of mistakes which the perceptron algorithm can make 
theorem   let hx    y  i          hxt   yt i be a sequence of labeled examples with xi   n   kxi k 
r and yi         for all i  let u   n        be such that yi  u  xi     for all i  then
 
 
mistakes on this example sequence 
perceptron makes at most r kuk
 
      winnow
the winnow algorithm  littlestone        has a very similar structure  winnow maintains
a hypothesis vector w   n which is initially w                   winnow is parameterized by
a promotion factor      and a threshold       upon receiving an example x         n
winnow predicts according to the threshold function w  x    if the prediction is   and the
label is   then for all i such that xi     the value of wi is set to wi    this is a demotion
step  if the prediction is   and the label is   then for all i such that xi     the value of wi
is set to wi   this is a promotion step  no change is made to w if the prediction is correct 
for our purposes the following mistake bound  implicit in littlestones work         is
of interest 
theorem   let the target function be a k literal monotone disjunction f  x             xn    
xi       xik   for any sequence of examples in       n labeled according to f the number

of prediction mistakes made by winnow     is at most  
 n   k           log   
    our results
we are interested in the computational efficiency and convergence of the perceptron and
winnow algorithms when run over expanded feature spaces of conjunctions  specifically 
we study the use of kernel functions to expand the feature space and thus enhance the
learning abilities of perceptron and winnow  we refer to these enhanced algorithms as
kernel perceptron and kernel winnow 
our first result  cf  also the papers of sadohara        watkins        and kowalczyk
et al         uses kernel functions to show that it is possible to efficiently run the kernel
perceptron algorithm over an exponential number of conjunctive features 
   

fikhardon  roth    servedio

result     see theorem    there is an algorithm that simulates perceptron over the   n dimensional feature space of all conjunctions of n basic features  given a sequence of t
labeled examples in       n the prediction and update for each example take poly n  t  time
steps  we also prove variants of this result in which the expanded feature space consists of
all monotone conjunctions or all conjunctions of some bounded size 
this result is closely related to one of the main open problems in learning theory 
efficient learnability of disjunctions of conjunctions  or dnf  disjunctive normal form 
expressions   since linear threshold elements can represent disjunctions  e g  x   x   x 
is true iff x    x    x       theorem   and result   imply that kernel perceptron can be
used to learn dnf  however  in this framework the values of n and r in theorem   can be
exponentially large  note that we have n    n and r    n   if all conjunctions are used  
and hence the mistake bound given by theorem   is exponential rather than polynomial
in n  the question thus arises whether the exponential upper bound implied by theorem
  is essentially tight for the kernel perceptron algorithm in the context of dnf learning 
we give an affirmative answer  thus showing that kernel perceptron cannot efficiently learn
dnf 
result    there is a monotone dnf f over x            xn and a sequence of examples labeled
according to f which causes the kernel perceptron algorithm to make    n  mistakes  this
result holds for generalized versions of the perceptron algorithm where a fixed or updated
threshold and a learning rate are used  we also give a variant of this result showing
that kernel perceptron fails in the probably approximately correct  pac  learning model
 valiant        as well 
turning to winnow  an attractive feature of theorem   is that for suitable    the bound
is logarithmic in the total number of features n  e g       and    n    therefore  as
noted by several researchers  maass   warmuth         if a winnow analogue of theorem  
could be obtained this would imply that dnf can be learned by a computationally efficient
algorithm with a poly n  mistake bound  however  we give strong evidence that no such
winnow analogue of theorem   can exist 
result    there is no polynomial time algorithm which simulates winnow over exponentially many monotone conjunctive features for learning monotone dnf unless every problem
in the complexity class  p can be solved in polynomial time  this result holds for a wide
range of parameter settings in the winnow algorithm 
we observe that  in contrast to this negative result  maass and warmuth have shown
that the winnow algorithm can be simulated efficiently over exponentially many conjunctive
features for learning some simple geometric concept classes  maass   warmuth        
our results thus indicate a tradeoff between computational efficiency and convergence
of kernel algorithms for rich classes of boolean functions such as dnf formulas  the kernel
   angluin        proved that dnf expressions cannot be learned efficiently using equivalence queries
whose hypotheses are themselves dnf expressions  since the model of exact learning from equivalence
queries only is equivalent to the mistake bound model which we consider in this paper  her result implies
that no online algorithm which uses dnf formulas as hypotheses can efficiently learn dnf  however 
this result does not preclude the efficient learnability of dnf using a different class of hypotheses  the
kernel perceptron algorithm generates hypotheses which are thresholds of conjunctions rather than dnf
formulas  and thus angluins negative results do not apply here 

   

fiefficiency versus convergence of boolean kernels

perceptron algorithm is computationally efficient to run but has exponentially slow convergence  whereas kernel winnow has rapid convergence but seems to require exponential
runtime 

   kernel perceptron with many features
it is well known that the hypothesis w of the perceptron algorithm is a linear combination
of the previous examples on which mistakes were made  cristianini   shaw taylor        
more precisely  if we let l v          denote the label of example v  then we have that
p
w   vm l v v where m is the set of examples on which the algorithm made a mistake 
p
p
thus the prediction of perceptron on x is   iff wx     vm l v v x   vm l v  v x  
  
for an example x        n let  x  denote its transformation into an enhanced feature
space such as the space of all conjunctions  to run the perceptron algorithm over the
enhanced space we must predict   iff w    x     where w  is the weight vector in
p
the enhanced space  from the above discussion this holds iff vm l v   v    x      
p
denoting k v  x     v    x  this holds iff vm l v k v  x     
thus we never need to construct the enhanced feature space explicitly  in order to run
perceptron we need only be able to compute the kernel function k v  x  efficiently  this is
the idea behind all so called kernel methods  which can be applied to any algorithm  such
as support vector machines  whose prediction is a function of inner products of examples 
a more detailed discussion is given in the book of cristianini and shawe taylor        
thus the next theorem is simply obtained by presenting a kernel function capturing all
conjunctions 
theorem   there is an algorithm that simulates perceptron over the feature spaces of
    all conjunctions      all monotone conjunctions      conjunctions of size  k  and    
monotone conjunctions of size  k  given a sequence of t labeled examples in        n the
prediction and update for each example take poly n  t  time steps 
proof  for case        includes all  n conjunctions  with positive and negative literals  and
k x  y  must compute the number of conjunctions which are true in both x and y  clearly 
any literal in such a conjunction must satisfy both x and y and thus the corresponding bit
in x  y must have the same value  thus each conjunction true in both x and y corresponds
to a subset of such bits  counting all these conjunctions gives k x  y     same x y  where
same x  y  is the number of original features that have the same value in x and y  i e  the
number of bit positions i which have xi   yi   this kernel has been obtained independently
by sadohara        
to express all monotone monomials as in     we take k x  y      xy  where  x  y  is
the number of active features common to both x and y  i e  the number of bit positions
which have xi   yi     
similarly  for case     the number of conjunctions that satisfy both x and y is k x  y   
pk same x y 
  this kernel is reported also by watkins         for case     we have
l  
l


p
k x  y    kl    xy 
 
 
l
   

fikhardon  roth    servedio

   kernel perceptron with many mistakes
in this section we describe a simple monotone dnf target function and a sequence of
labeled examples which causes the monotone monomials kernel perceptron algorithm to
make exponentially many mistakes 
for x  y        n we write  x  to denote the number of  s in x and  as described above 
 xy  to denote the number of bit positions i which have xi   yi      we need the following
well known tail bound on sums of independent random variables which can be found in 
e g   section     of the book by kearns and vazirani        
fact   let x            xm be a sequence of m independent     valued random variables  each
p
of which has e xi     p  let x denote m
i   xi   so e x    pm  then for         we
have
pr x         pm   emp

    

and

pr x        pm   emp

    

 

we also use the following combinatorial property 
lemma   there is a set s of n bit strings s    x            xt          n with t   en     
such that  xi     n    for    i  t and  xi  xj    n    for    i   j  t 
proof  we use the probabilistic method  for each i              t let xi        n be chosen
by independently setting each bit to   with probability       for any i it is clear that
e  xi      n     applying fact    we have that pr  xi     n      en      and thus the
probability that any xi satisfies  xi     n    is at most ten      similarly  for any i    j we
have e  xi  xj      n      applying fact   we have that pr  xi  xj     n      en       
and thus the probability that any xi   xj with
i    j satisfies  xi  xj     n    is at most
 t  n     
t  n     
n     
  ten    is less than    thus for some
  for t   e
the value of   e
  e
choice of x            xt we have each  xi    n    and  xi  xj    n     for any xi which has
 xi     n    we can set  xi    n    of the  s to  s  and the lemma is proved 
 
now using the previous lemma we can construct a difficult data set for kernel perceptron 
theorem   there is a monotone dnf f over x            xn and a sequence of examples labeled
according to f which causes the kernel perceptron algorithm to make   n  mistakes 
proof  the target dnf with which we will use is very simple  it is the single conjunction
x  x        xn   while the original perceptron algorithm over the n features x            xn is easily
seen to make at most poly n  mistakes for this target function  we now show that the
monotone kernel perceptron algorithm which runs over a feature space of all   n monotone
monomials can make     en      mistakes 
recall that at the beginning of the perceptron algorithms execution all   n coordinates
of w are    the first example is the negative example  n   the only monomial true in this
example is the empty monomial which is true in every example  since w    x      perceptron incorrectly predicts   on this example  the resulting update causes the coefficient
w corresponding to the empty monomial to become   but all  n    other coordinates
of w remain    the next example is the positive example  n   for this example we have
w   x      so perceptron incorrectly predicts    since all  n monotone conjunctions
   

fiefficiency versus convergence of boolean kernels

are satisfied by this example the resulting update causes w to become   and all  n   
other coordinates of w  to become    the next en      examples are the vectors x            xt
described in lemma    since each such example has  xi     n    each example is negative 
however as we now show the perceptron algorithm will predict   on each of these examples 
fix any value    i  en      and consider the hypothesis vector w  just before example
i
x is received  since  xi     n    the value of w    xi   is a sum of the  n    different
coordinates wt which correspond to the monomials satisfied by xi   more precisely we have
p
p
w   xi     t ai wt   t bi wt where ai contains the monomials which are satisfied
by xi and xj for some j    i and bi contains the monomials which are satisfied by xi but
no xj with j    i  we lower bound the two sums separately 
let t be any monomial in ai   by lemma   any t  ai contains at most n    variables

pn    
monomials in ai   using the well known bound
and thus there can be at most r   n   
r


p   
 h   o     
where           and h p    p log p      p  log    p  is
j   j    
the binary entropy function  which can be found e g  as theorem       of the book by
van lint         there can be at most         n     o n          n terms in ai   moreover
the value of each wt must be at least en      since wt decreases by at most   for each
p
example  and hence t ai wt  en            n         n   on the other hand  any t  bi
is false in all other examples and therefore wt has not been demoted and wt      by
lemma   for any r   n    every r variable monomial satisfied by xi must belong to bi  


p
pn   
and hence t bi wt  r n      n   
        n   combining these inequalities we have
r
w  xi        n         n     and hence the perceptron prediction on xi is   
 
remark   at first sight it might seem that the result is limited to a simple special case of
the perceptron algorithm  several variations exist that use  an added feature with a fixed
value that enables the algorithm to update the threshold indirectly  via a weight w   a non
zero fixed  initial  threshold   and a learning rate   and in particular all these three can
be used simultaneously  the generalized algorithm predicts according to the hypothesis
w  x   w   and updates w  w   x and w  w    for promotions and similarly
for demotions  we show here that exponential lower bounds on the number of mistakes
can be derived for the more general algorithm as well  first  note that since our kernel
includes a feature for the empty monomial which is always true  the first parameter is
already accounted for  for the other two parameters note that there is a degree of freedom
between the learning rate  and fixed threshold  since multiplying both by the same factor
does not change the hypothesis and therefore it suffices to consider the threshold only  we
consider several cases for the value of the threshold  if  satisfies              then we
use the same sequence of examples  after the first two examples the algorithm makes a
promotion on  n  it may or may not update on  n but that is not important   for the
p
p
examples in the sequence the bounds on t ai wt and t bi wt are still valid so the
final inequality in the proof becomes w  xi        n         n         n which is true for
sufficiently large n  if          n then we can construct the following scenario  we use the
function f   x   x          xn   and the sequence of examples includes      repetitions of
the same example x where the first bit is   and all other bits are    the example x satisfies
exactly   monomials and therefore the algorithm will make mistakes on all the examples in
the sequence  if      then the initial hypothesis misclassifies  n   we start the example
   

fikhardon  roth    servedio

sequence by repeating the example  n until it is classified correctly  that is de times 
if the threshold is large in absolute value e g           n we are done  otherwise we
continue with the example  n   since all weights except for the empty monomial are zero at
this stage the examples  n and  n are classified in the same way so  n is misclassified and
therefore the algorithm makes a promotion  the argument for the rest of the sequence is as
above  except for adding a term for the empty monomial  and the final inequality becomes
w  xi        n        n         n         n so each of the examples is misclassified  thus
in all cases kernel perceptron may make an exponential number of mistakes 
    a negative result for the pac model
the proof above can be adapted to give a negative result for kernel perceptron in the pac
learning model  valiant         in this model each example x is independently drawn from
a fixed probability distribution d and with high probability the learner must construct a
hypothesis h which has high accuracy relative to the target concept c under distribution d 
see the kearns vazirani text        for a detailed discussion of the pac learning model 
let d be the probability distribution over       n which assigns weight     to the ex 
to each of the en      examples
ample  n   weight     to the example  n   and weight    en     
x            xt  
theorem   if kernel perceptron is run using a sample of polynomial size p n  then with
probability at least      the error of its final hypothesis is at least      
proof  with probability       the first two examples received from d will be  n and then
 n   thus  with probability       after two examples  as in the proof above  the perceptron
algorithm will have w     and all other coefficients of w  equal to   
consider the sequence of examples following these two examples  first note that in any
trial  any occurrence of an example other than  n  i e  any occurrence either of some xi or of
p
the  n example  can decrease t  n  wt by at most  n      since after the first two examples
p
we have w     n     t  n  wt    n     it follows that at least    n       more examples
must occur before the  n example will be incorrectly classified as a negative example  since
we will only consider the performance of the algorithm for p n       n       steps  we
may ignore all subsequent occurrences of  n since they will not change the algorithms
hypothesis 
now observe that on the first example which is not  n the algorithm will perform a
demotion resulting in w      possibly changing other coefficients as well   since no
promotions will be performed on the rest of the sample  we get w    for the rest of
the learning process  it follows that all future occurrences of the example   n are correctly
classified and thus we may ignore them as well 
considering examples xi from the sequence constructed above  we may ignore any example that is correctly classified since no update is made on it  it follows that when the
perceptron algorithm has gone over all examples  its hypothesis is formed by demotions on
examples in the sequence of xi s  the only difference from the scenario above is that the
algorithm may make several demotions on the same example if it occurs multiple times in
the sample  however  an inspection of the proof above shows that for any x i that has not
p
p
been seen by the algorithm  the bounds on t ai wt and t bi wt are still valid and
   

fiefficiency versus convergence of boolean kernels

therefore xi will be misclassified  since the sample is of size p n  and the sequence is of
size en      the probability weight of examples in the sample is at most      for sufficiently
large n so the error of the hypothesis is at least      
 

   computational hardness of kernel winnow
in this section  for x        n we let  x  denote the   n     element vector whose coordinates are all nonempty monomials  monotone conjunctions  over x            xn   we say that
a sequence of labeled examples hx    b  i          hxt   bt i is monotone consistent if it is consistent
with some monotone function  i e  xik  xjk for all k              n implies bi  bj   if s is
monotone consistent and has t labeled examples then clearly there is a monotone dnf
formula consistent with s which contains at most t conjunctions  we consider the following
problem 
kernel winnow prediction      kwp 
instance  monotone consistent sequence s   hx    b  i          hxt   bt i of labeled examples with
each xi        m and each bi          unlabeled example z        m  
question  is w    z     where w  is the n     m     dimensional hypothesis vector
generated by running winnow     on the example sequence h x     b  i        h xt    bt i 
in order to run winnow over all  m    nonempty monomials to learn monotone dnf 
one must be able to solve kwp efficiently  our main result in this section is a proof
that kwp is computationally hard for a wide range of parameter settings which yield a
polynomial mistake bound for winnow via theorem   
recall that  p is the class of all counting problems associated with n p decision problems  it is well known that if every function in  p is computable in polynomial time then
p   n p  see the book of papadimitriou        or the paper of valiant        for details
on  p  the following problem is  p hard  valiant        
monotone   sat  m sat 
instance  monotone   cnf boolean formula f   c   c          cr with ci    yi   yi   
and each yij   y            yn    integer k such that    k   n  
question  is  f         k  i e  does f have at least k satisfying assignments in       n  
theorem   fix any       let n    m     let         m    and let     be such

that max   
 n              log      poly m   if there is a polynomial time algorithm
for kwp      then every function in  p is computable in polynomial time 
proof  for n   and  as described in the theorem a routine calculation shows that
      m     poly m 

and

 m
    poly m   
poly m 

   

the proof is a reduction from the problem m sat  the high level idea of the proof is
simple  let  f  k  be an instance of m sat where f is defined over variables y            yn   the
winnow algorithm maintains a weight wt for each monomial t over variables x            xn   we
define a     correspondence between these monomials t and truth assignments y t        n
   

fikhardon  roth    servedio

for f  and we give a sequence of examples for winnow which causes wt    if f  y t      
and wt     if f  y t        the value of w    z  is thus related to  f         note that
if we could control  as well this would be sufficient since we could use    k and the
result will follow  however  is a parameter of the algorithm  we therefore have to make
additional updates so that w    z        f         k  so that w    z    if and
only if  f         k  the details are somewhat involved since we must track the resolution
of approximations of the different values so that the final inner product will indeed give a
correct result with respect to the threshold 
general setup of the construction  in more detail  let
 u   n       d dlog  e      log e 
n  
 v   d log
 e     
u   
 w   d log
e    

and let m be defined as
m   n   u    v n     u w     

   

since         m    using the fact that log     x   x   for     x     we have that
log       m     and from this it easily follows that m as specified above is polynomial in
n  we describe a polynomial time transformation which maps an n variable instance  f  k 
of m sat to an m variable instance  s  z  of kwp     where s   hx     b  i          hxt   bt i
is monotone consistent  each xi and z belong to       m   and w   z    if and only if
 f         k 
the winnow variables x            xm are divided into three sets a  b and c where a  
 x            xn    b    xn             xn u   and c    xn u              xm    the unlabeled example z
is  n u  mnu   i e  all variables in a and b are set to   and all variables in c are set to   
p
p
we thus have w   z    ma  mb  mab where ma     t a wt   mb     t b wt and
p
mab   t ab t a   t b   wt   we refer to monomials     t  a as type a monomials 
monomials     t  b as type b monomials  and monomials t  ab  t a      t b    
as type ab monomials 
the example sequence s is divided into four stages  stage   results in ma   f        
as described below the n variables in a correspond to the n variables in the cnf formula
f  stage   results in ma  q  f        for some positive integer q which we specify later 
stages   and   together result in mb   mab    q k  thus the final value of w    z  is
approximately    q   f         k   so we have w    z    if and only if  f         k 
since all variables in c are   in z  if t includes a variable in c then the value of wt
does not affect w    z   the variables in c are slack variables which  i  make winnow
perform the correct promotions demotions and  ii  ensure that s is monotone consistent 
stage    setting ma   f        we define the following correspondence between
truth assignments y t        n and monomials t  a   yit     if and only if xi is not
present in t  for each clause yi   yi  in f  stage   contains v negative examples such that
xi    xi      and xi     for all other xi  a  we show below that     winnow makes a
false positive prediction on each of these examples and     in stage   winnow never does a
   

fiefficiency versus convergence of boolean kernels

promotion on any example which has any variable in a set to    consider any y t such that
f  y t        since our examples include an example y s such that y t  y s the monomial t
is demoted at least v times  as a result after stage   we will have that for all t   w t    
if f  y t       and     wt  v if f  y t        thus we will have ma    f            for
some          n v       
we now show how the stage   examples cause winnow to make a false positive prediction
on negative examples which have xi    xi      and xi     for all other i in a as described
above  for each such negative example in stage   six new slack variables x             x    c
are used as follows  stage   has dlog      e repeated instances of the positive example which
has x     x       and all other bits    these examples cause promotions which result
in   wx     wx     wx   x      and hence wx        two other groups of
similar examples  the first with x     x        the second with x     x        cause
wx       and wx        the next example in s is the negative example which has
xi    xi       xi     for all other xi in a  x     x     x       and all other bits   
for this example w    x    wx     wx     wx     so winnow makes a false positive
prediction 
since f has at most n  clauses and there are v negative examples per clause  this
construction can be carried out using  v n  slack variables xn u              xn u   v n    we
thus have     and     as claimed above 
stage    setting ma  q  f        the first stage   example is a positive example
with xi     for all xi  a  xn u   v n         and all other bits    since each of the  n
monomials which contain xn u   v n     and are satisfied by this example have wt     
we have w    x     n    f               n     since     m  poly m     n    recall
from equation     that m    n     after the resulting promotion we have w    x   
  n    f                 n     let
q   dlog    n    e   
so that
q  n       q    n    

   

stage   consists of q repeated instances of the positive example described above  after
these promotions we have w    x    q   n    f                q  n       since    
 f               n we also have
q   ma   q   f                q  n      

   

equation     gives the value which ma will have throughout the rest of the argument 
some calculations for stages   and    at the start of stage   each type b and typeab monomial t has wt      there are n variables in a and u variables in b so at the
start of stage   we have mb    u    and mab     n      u      since no example in
stages   or   satisfies any xi in a  at the end of stage   ma will still be q   f             
and mab will still be   n      u      therefore at the end of stage   we have
w   z    mb   q   f                  n      u     
   

fikhardon  roth    servedio

to simplify notation let
d       n      u      q k 
ideally at the end of stage   the value of mb would be d  q   since this would imply that
w   z       q   f         k  which is at least  if and only if  f         k  however it
is not necessary for mb to assume this exact value  since  f        must be an integer and
             as long as
 
   
d  mb   d    q
 
we get that
 
   q   f         k         w   z       q   f         k         
 
now if  f         k we clearly have w    z     on the other hand if  f          k
then since  f        is an integer value  f         k    and we get w    z      therefore
all that remains is to construct the examples in stages   and   so that that m b satisfies
equation     
we next calculate an appropriate granularity for d  note that k    n   so by equation     we have that   q k       now recall from equations     and     that m  
 
n   u    n  and     m  poly m   so      n u   n  poly m    n  u   consequently we
certainly have that d       and from equation     we have that d         q  n       q  
let
c   dlog  e 
so that we have
 
qc  q   d 
 

   

there is a unique smallest positive integer p     which satisfies d  pqc   d      q   the
stage   examples will result in mb satisfying p   mb   p        we now have that 
 
qc   d  pqc   d   q
 
  q
  
 
 q    n     qc
  

qc

  

c   n  

 

    

   
   
   

here     holds since k     and thus  by definition of d  we have d   q   which is
equivalent to equation      inequality     follows from equations     and     
hence we have that
    p  c    n        n   d c    log e       u    

    

where the second inequality in the above chain follows from equation      we now use the
following lemma 
   

fiefficiency versus convergence of boolean kernels

lemma    for all       for all    p         there is a monotone cnf f  p over
  boolean variables which has at most   clauses  has exactly p satisfying assignments in
          and can be constructed from   and p in poly    time 
proof  the proof is by induction on    for the base case       we have p     and f   p   x   
assuming the lemma is true for                k we now prove it for     k      
if    p   k    then the desired cnf is fk   p   xk    fk p   since fk p has at most k
clauses fk   p has at most k     clauses  if  k      p   k      then the desired cnf is
fk   p   xk    fk p k   by distributing xk over each clause of fk p k we can write fk   p
as a cnf with at most k clauses  if p    k then fk p   x   
 
stage    setting mb  p  let fu p be an r clause monotone cnf formula over the
u variables in b which has p satisfying assignments  similar to stage    for each clause
of fu p   stage   has w negative examples corresponding to that clause  and as in stage
  slack variables in c are used to ensure that winnow makes a false positive prediction
on each such negative example  thus the examples in stage   cause mb   p     where
         u w        since six slack variables in c are used for each negative example
and there are rw  u w negative examples  the slack variables xn u   v n               xm  are
sufficient for stage   
stage    setting mb   mab    q k  all that remains is to perform q  c
promotions on examples which have each xi in b set to    this will cause mb to equal
 p      qc   by the inequalities established above  this will give us
 
 
d  pqc    p      qc   mb   d   q     qc   d   q
 
 
which is as desired 
in order to guarantee q  c promotions we use two sequences of examples of length
n
u n
q  du
log  e and d log  e  c respectively  we first show that these are positive numbers  it
follows directly from the definitions u   n       d dlog   e      log e and c   dlog  e
n
 n   by definition of m and equation      and  is bounded
that u
log   c  since     
by a polynomial in m  we clearly have that log   n       u  n   log    now since
n    
u n
n
q   dlog    n    e    this implies that q   log   
    du
log  e  so that q  d log  e     
log  
n
the first q  d u
log  e examples in stage   are all the same positive example which has
each xi in b set to   and xm       the first time this example is received  we have
 
w   x     u   p        u      since      n   by inspection of u we have  u        so
n
winnow performs a promotion  similarly  after q  d u
log  e occurrences of this example  we
have
qd u n e
qd u n e
w   x     log    u   p          log   u     q  n     

so promotions are indeed performed at each occurrence  and
mb   

n
qd u
e
log 

 p       

n
the remaining examples in stage   are d u
log  e  c repetitions of the positive example x
which has each xi in b set to   and xm      if promotions occurred on each repetition of

   

fikhardon  roth    servedio

this example then we would have w    x    

n
du
ec
log 

  u   

n
qd u
e
log 

 p         so we need

only show that this quantity is less than   we reexpress this quantity as 
qc  p        we have

n
ec u
du
log 
 

 
qc  p         pqc   qc
 
  q
 
      q
 
  
  q
   
 

 

    

d u n ec

where      follows from     and the definition of c  finally  we have that  log   u 
 

    u nc log        u n     
     q   where the last inequality is by equation    
 n  
and the previous inequality is by inspection of the values of    and u   combining the two
bounds above we see that indeed w    x     
finally  we observe that by construction the example sequence s is monotone consistent 
since m   poly n  and s contains poly n  examples the transformation from m sat to
kwp     is polynomial time computable and the theorem is proved 
  theorem   

   conclusion
linear threshold functions are a weak representation language for which we have interesting learning algorithms  therefore  if linear learning algorithms are to learn expressive
functions  it is necessary to expand the feature space over which they are applied  this
work explores the tradeoff between computational efficiency and convergence when using
expanded feature spaces that capture conjunctions of base features 
we have shown that while each iteration of the kernel perceptron algorithm can be
executed efficiently  the algorithm can provably require exponentially many updates even
when learning a function as simple as f  x    x  x        xn   on the other hand  the kernel
winnow algorithm has a polynomial mistake bound for learning polynomial size monotone
dnf  but our results show that under a widely accepted computational hardness assumption
it is impossible to efficiently simulate the execution of kernel winnow  the latter also implies
that there is no general construction that will run winnow using kernel functions 
our results indicate that additive and multiplicative update algorithms lie on opposite
extremes of the tradeoff between computational efficiency and convergence  we believe that
this fact could have significant practical implications  by demonstrating the provable limitations of using kernel functions which correspond to high degree feature expansions  our
results also lend theoretical justification to the common practice of using a small degree in
similar feature expansions such as the well known polynomial kernel  
since the publication of the initial conference version of this work  khardon  roth   
servedio         several authors have explored closely related ideas  one can show that our
construction for the negative results for perceptron does not extend  either in the pac or
   our boolean kernels are different than standard polynomial kernels in that all the conjunctions are
weighted equally  and also in that we allow negations 

   

fiefficiency versus convergence of boolean kernels

online setting  to related algorithms such as support vector machines which work by constructing a maximum margin hypothesis consistent with the examples  the paper  khardon
  servedio        gives an analysis of the pac learning performance of maximum margin
algorithms with the monotone monomials kernel  and derives several negative results thus
giving further negative evidence for the monomial kernel  in the paper  cumby   roth 
      a kernel for expressions in description logic  generalizing the monomials kernel  is
developed and successfully applied for natural language and molecular problems  takimoto and warmuth        study the use of multiplicative update algorithms other than
winnow  such as weighted majority  and obtain some positive results by restricting the
type of loss function used to be additive over base features  chawla et al         have
studied monte carlo estimation approaches to approximately simulate the winnow algorithms performance when run over a space of exponentially many features  the use of
kernel methods for logic learning and developing alternative methods for feature expansion
with multiplicative update algorithms remain interesting and challenging problems to be
investigated 

acknowledgments
this work was partly done while khardon was at the university of edinburgh and partly
while servedio was at harvard university  the authors gratefully acknowledge financial
support for this work by epsrc grant gr n       nsf grant iis         and a research semester fellowship award from tufts university  khardon   nsf grants itr iis          itr iis         and iis          roth   and nsf grant ccr          and nsf
mathematical sciences postdoctoral fellowship  servedio  

references
angluin  d          negative results for equivalence queries  machine learning            
block  h          the perceptron  a model for brain functioning  reviews of modern
physics             
carlson  a   cumby  c   rosen  j     roth  d          the snow learning architecture 
tech  rep  uiucdcs r          uiuc computer science department 
chawla  d   li  l     scott   s          on approximating weighted sums with exponentially
many terms  journal of computer and system sciences             
cristianini  n     shaw taylor  j          an introduction to support vector machines 
cambridge press 
cumby  c     roth  d          on kernel methods for relational learning  in proc  of the
international conference on machine learning 
kearns  m     vazirani  u          an introduction to computational learning theory 
mit press  cambridge  ma 
   

fikhardon  roth    servedio

khardon  r   roth  d     servedio  r          efficiency versus convergence of boolean
kernels for on line learning algorithms  in dietterich  t  g   becker  s     ghahramani 
z   eds    advances in neural information processing systems     cambridge  ma 
mit press 
khardon  r     servedio  r          maximum margin algorithms with boolean kernels  in
proceedings of the sixteenth annual conference on computational learning theory 
pp        
lint  j  v          introduction to coding theory  springer verlag 
littlestone  n          learning quickly when irrelevant attributes abound  a new linearthreshold algorithm  machine learning            
maass  w     warmuth  m  k          efficient learning with virtual threshold gates 
information and computation                  
minsky  m     papert  s          perceptrons  an introduction to computational geometry 
mit press  cambridge  ma 
novikoff  a          on convergence proofs for perceptrons  in proceeding of the symposium
on the mathematical theory of automata  vol      pp         
papadimitriou  c          computational complexity  addison wesley 
rosenblatt  f          the perceptron  a probabilistic model for information storage and
organization in the brain  psychological review             
roth  d          learning to resolve natural language ambiguities  a unified approach  in
proc  of the american association of artificial intelligence  pp         
sadohara  k          learning of boolean functions using support vector machines  in proc 
of the conference on algorithmic learning theory  pp          springer  lnai      
takimoto  e     warmuth  m          path kernels and multiplicative updates  journal of
machine learning research            
valiant  l  g          the complexity of enumeration and reliability problems  siam
journal of computing            
valiant  l  g          a theory of the learnable  communications of the acm          
         
watkins  c          kernels from matching operations  tech  rep  csd tr        computer
science department  royal holloway  university of london 

   

fi