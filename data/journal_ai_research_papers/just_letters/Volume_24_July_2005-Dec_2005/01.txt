journal of artificial intelligence research                

submitted        published      

a framework for sequential planning in multi agent settings
piotr j  gmytrasiewicz
prashant doshi

piotr   cs   uic   edu
pdoshi   cs   uic   edu

department of computer science
university of illinois at chicago
    s  morgan st
chicago  il      

abstract
this paper extends the framework of partially observable markov decision processes  pomdps 
to multi agent settings by incorporating the notion of agent models into the state space  agents
maintain beliefs over physical states of the environment and over models of other agents  and they
use bayesian updates to maintain their beliefs over time  the solutions map belief states to actions 
models of other agents may include their belief states and are related to agent types considered in
games of incomplete information  we express the agents autonomy by postulating that their models are not directly manipulable or observable by other agents  we show that important properties
of pomdps  such as convergence of value iteration  the rate of convergence  and piece wise linearity and convexity of the value functions carry over to our framework  our approach complements a
more traditional approach to interactive settings which uses nash equilibria as a solution paradigm 
we seek to avoid some of the drawbacks of equilibria which may be non unique and do not capture
off equilibrium behaviors  we do so at the cost of having to represent  process and continuously
revise models of other agents  since the agents beliefs may be arbitrarily nested  the optimal solutions to decision making problems are only asymptotically computable  however  approximate
belief updates and approximately optimal plans are computable  we illustrate our framework using
a simple application domain  and we show examples of belief updates and value functions 

   introduction
we develop a framework for sequential rationality of autonomous agents interacting with other
agents within a common  and possibly uncertain  environment  we use the normative paradigm of
decision theoretic planning under uncertainty formalized as partially observable markov decision
processes  pomdps   boutilier  dean    hanks        kaelbling  littman    cassandra       
russell   norvig        as a point of departure  solutions of pomdps are mappings from an
agents beliefs to actions  the drawback of pomdps when it comes to environments populated by
other agents is that other agents actions have to be represented implicitly as environmental noise
within the  usually static  transition model  thus  an agents beliefs about another agent are not part
of solutions to pomdps 
the main idea behind our formalism  called interactive pomdps  i pomdps   is to allow
agents to use more sophisticated constructs to model and predict behavior of other agents  thus 
we replace flat beliefs about the state space used in pomdps with beliefs about the physical
environment and about the other agent s   possibly in terms of their preferences  capabilities  and
beliefs  such beliefs could include others beliefs about others  and thus can be nested to arbitrary
levels  they are called interactive beliefs  while the space of interactive beliefs is very rich and
updating these beliefs is more complex than updating their flat counterparts  we use the value
c
    
ai access foundation  all rights reserved 

fig mytrasiewicz   d oshi

function plots to show that solutions to i pomdps are at least as good as  and in usual cases superior
to  comparable solutions to pomdps  the reason is intuitive  maintaining sophisticated models of
other agents allows more refined analysis of their behavior and better predictions of their actions 
i pomdps are applicable to autonomous self interested agents who locally compute what actions they should execute to optimize their preferences given what they believe while interacting
with others with possibly conflicting objectives  our approach of using a decision theoretic framework and solution concept complements the equilibrium approach to analyzing interactions as used
in classical game theory  fudenberg   tirole         the drawback of equilibria is that there could
be many of them  non uniqueness   and that they describe agents optimal actions only if  and when 
an equilibrium has been reached  incompleteness   our approach  instead  is centered on optimality
and best response to anticipated action of other agent s   rather then on stability  binmore       
kadane   larkey         the question of whether  under what circumstances  and what kind of
equilibria could arise from solutions to i pomdps is currently open 
our approach avoids the difficulties of non uniqueness and incompleteness of traditional equilibrium approach  and offers solutions which are likely to be better than the solutions of traditional
pomdps applied to multi agent settings  but these advantages come at the cost of processing and
maintaining possibly infinitely nested interactive beliefs  consequently  only approximate belief
updates and approximately optimal solutions to planning problems are computable in general  we
define a class of finitely nested i pomdps to form a basis for computable approximations to infinitely nested ones  we show that a number of properties that facilitate solutions of pomdps carry
over to finitely nested i pomdps  in particular  the interactive beliefs are sufficient statistics for the
histories of agents observations  the belief update is a generalization of the update in pomdps  the
value function is piece wise linear and convex  and the value iteration algorithm converges at the
same rate 
the remainder of this paper is structured as follows  we start with a brief review of related
work in section    followed by an overview of partially observable markov decision processes in
section    there  we include a simple example of a tiger game  we introduce the concept of
agent types in section    section   introduces interactive pomdps and defines their solutions  the
finitely nested i pomdps  and some of their properties are introduced in section    we continue
with an example application of finitely nested i pomdps to a multi agent version of the tiger game
in section    there  we show examples of belief updates and value functions  we conclude with
a brief summary and some current research issues in section    details of all proofs are in the
appendix 

   related work
our work draws from prior research on partially observable markov decision processes  which
recently gained a lot of attention within the ai community  smallwood   sondik        monahan 
      lovejoy        hausktecht        kaelbling et al         boutilier et al         hauskrecht 
      
the formalism of markov decision processes has been extended to multiple agents giving rise to
stochastic games or markov games  fudenberg   tirole         traditionally  the solution concept
used for stochastic games is that of nash equilibria  some recent work in ai follows that tradition
 littman        hu   wellman        boutilier        koller   milch         however  as we
mentioned before  and as has been pointed out by some game theorists  binmore        kadane  
  

fia f ramework for s equential p lanning in m ulti  agent s ettings

larkey         while nash equilibria are useful for describing a multi agent system when  and if 
it has reached a stable state  this solution concept is not sufficient as a general control paradigm 
the main reasons are that there may be multiple equilibria with no clear way to choose among them
 non uniqueness   and the fact that equilibria do not specify actions in cases in which agents believe
that other agents may not act according to their equilibrium strategies  incompleteness  
other extensions of pomdps to multiple agents appeared in ai literature recently  bernstein 
givan  immerman    zilberstein        nair  pynadath  yokoo  tambe    marsella         they
have been called decentralized pomdps  dec pomdps   and are related to decentralized control
problems  ooi   wornell         dec pomdp framework assumes that the agents are fully cooperative  i e   they have common reward function and form a team  furthermore  it is assumed that
the optimal joint solution is computed centrally and then distributed among the agents for execution 
from the game theoretic side  we are motivated by the subjective approach to probability in
games  kadane   larkey         bayesian games of incomplete information  see fudenberg  
tirole        harsanyi        and references therein   work on interactive belief systems  harsanyi 
      mertens   zamir        brandenburger   dekel        fagin  halpern  moses    vardi 
      aumann        fagin  geanakoplos  halpern    vardi         and insights from research on
learning in game theory  fudenberg   levine         our approach  closely related to decisiontheoretic  myerson         or epistemic  ambruster   boge        battigalli   siniscalchi       
brandenburger        approach to game theory  consists of predicting actions of other agents given
all available information  and then of choosing the agents own action  kadane   larkey        
thus  the descriptive aspect of decision theory is used to predict others actions  and its prescriptive
aspect is used to select agents own optimal action 
the work presented here also extends previous work on recursive modeling method  rmm 
 gmytrasiewicz   durfee         but adds elements of belief update and sequential planning 

   background  partially observable markov decision processes
a partially observable markov decision process  pomdp   monahan        hausktecht       
kaelbling et al         boutilier et al         hauskrecht        of an agent i is defined as
pomdpi   hs  ai   ti   i   oi   ri i

   

where  s is a set of possible states of the environment  ai is a set of actions agent i can execute  ti is
a transition function  ti   s ai s         which describes results of agent is actions  i is the
set of observations the agent i can make  oi is the agents observation function  oi   s ai i 
       which specifies probabilities of observations given agents actions and resulting states  finally 
ri is the reward function representing the agent is preferences  r i   s  ai    
in pomdps  an agents belief about the state is represented as a probability distribution over s 
initially  before any observations or actions take place  the agent has some  prior  belief  b  i   after
some time steps  t  we assume that the agent has t     observations and has performed t actions    
t
these can be assembled into agent is observation history  h ti    o i   o i       ot 
i   oi   at time t  let
hi denote the set of all observation histories of agent i  the agents current belief  b ti over s  is
continuously revised based on new observations and expected results of performed actions  it turns
   we assume that action is taken at every time step  it is without loss of generality since any of the actions maybe a
no op 

  

fig mytrasiewicz   d oshi

out that the agents belief state is sufficient to summarize all of the past observation history and
initial belief  hence it is called a sufficient statistic  
t 
the belief update takes into account changes in initial belief  b t 
i   due to action  ai   executed
t
t
at time t     and the new observation  oi   the new belief  bi   that the current state is st   is 
bti  st     oi  oti   st   at 
i  

x

bit   st   ti  st   ati   st   

   

st  s

where  is the normalizing constant 
it is convenient to summarize the above update performed for all states in s as
t
bti   se bit    at 
i   oi    kaelbling et al         
    optimality criteria and solutions
the agents optimality criterion  oci   is needed to specify how rewards acquired over time are
handled  commonly used criteria include 
 a finite horizon criterion p
in which the agent maximizes the expected value of the sum of the
following t rewards  e  tt   rt    here  rt is a reward obtained at time t and t is the length
of the horizon  we will denote this criterion as fht  
 anp
infinite horizon criterion with discounting  according to which the agent maximizes

t
e  
t    rt    where          is a discount factor  we will denote this criterion as ih  

 an infinite horizon criterion with averaging  according to which the agent maximizes the
average reward per time step  we will denote this as ihav  

in what follows  we concentrate on the infinite horizon criterion with discounting  but our approach can be easily adapted to the other criteria 
the utility associated with a belief state  bi is composed of the best of the immediate rewards
that can be obtained in bi   together with the discounted expected sum of utilities associated with
belief states following bi  

u  bi     max

ai ai

x

bi  s ri  s  ai     

x

p r oi  ai   bi  u  sei  bi   ai   oi   

oi i

ss



   

value iteration uses the equation   iteratively to obtain values of belief states for longer time
horizons  at each step of the value iteration the error of the current value estimate is reduced by the
factor of at least   see for example russell   norvig        section        the optimal action  a i  
is then an element of the set of optimal actions  op t  bi    for the belief state  defined as 

op t  bi     argmax
ai ai

x

bi  s ri  s  ai     

x

oi i

ss

   see  smallwood   sondik        for proof 

  

p r oi  ai   bi  u  se bi   ai   oi   



   

fia f ramework for s equential p lanning in m ulti  agent s ettings

l
or

ol

  

value function u 

 

 

 

 

 

 

   

   

   

   

 

pp i tl 
i  tl 
pomdp with noise

pomdp

figure    the value function for single agent tiger game with time horizon of length    oc i   fh   
actions are  open right door   or  open left door   ol  and listen   l  for this value of
the time horizon the value function for a pomdp with noise factor is identical to single
agent pomdp 

    example  the tiger game
we briefly review the pomdp solutions to the tiger game  kaelbling et al          our purpose is
to build on the insights that pomdp solutions provide in this simple case to illustrate solutions to
interactive versions of this game later 
the traditional tiger game resembles a game show situation in which the decision maker has
to choose to open one of two doors behind which lies either a valuable prize or a dangerous tiger 
apart from actions that open doors  the subject has the option of listening for the tigers growl
coming from the left  or the right  door  however  the subjects hearing is imperfect  with given
percentages  say       of false positive and false negative occurrences  following  kaelbling et al  
       we assume that the value of the prize is     that the pain associated with encountering the
tiger can be quantified as       and that the cost of listening is    
the value function  in figure    shows values of various belief states when the agents time
horizon is equal to    values of beliefs are based on best action available in that belief state  as
specified in eq     the state of certainty is most valuable  when the agent knows the location of
the tiger it can open the opposite door and claim the prize which certainly awaits  thus  when the
probability of tiger location is   or    the value is     when the agent is sufficiently uncertain  its
best option is to play it safe and listen  the value is then     the agent is indifferent between opening
doors and listening when it assigns probabilities of     or     to the location of the tiger 
note that  when the time horizon is equal to    listening does not provide any useful information
since the game does not continue to allow for the use of this information  for longer time horizons
the benefits of results of listening results in policies which are better in some ranges of initial belief 
since the value function is composed of values corresponding to actions  which are linear in prob  

fig mytrasiewicz   d oshi

l    l  gl  ol  gr 

l    l    

l    or  gl  l  gr 
l    or    
or    l    

l    ol    
ol    l    

 

value function u 

 

 

 

 

  
 

   

   

   

   

 

pp i tl 
i  tl 
pomdp with noise

pomdp

figure    the value function for single agent tiger game compared to an agent facing a noise factor  for horizon of length    policies corresponding to value lines are conditional plans 
actions  l  or or ol  are conditioned on observational sequences in parenthesis  for
example l    l  gl  ol  gr  denotes a plan to perform the listening action  l  at the
beginning  list of observations is empty   and then another l if the observation is growl
from the left  gl   and open the left door  ol  if the observation is gr   is a wildcard
with the usual interpretation 

ability of tiger location  the value function has the property of being piece wise linear and convex
 pwlc  for all horizons  this simplifies the computations substantially 
in figure   we present a comparison of value functions for horizon of length   for a single
agent  and for an agent facing a more noisy environment  the presence of such noise could be
due to another agent opening the doors or listening with some probabilities    since pomdps do
not include explicit models of other agents  these noise actions have been included in the transition
model  t  
consequences of folding noise into t are two fold  first  the effectiveness of the agents optimal
policies declines since the value of hearing growls diminishes over many time steps  figure   depicts
a comparison of value functions for horizon of length    here  for example  two consecutive growls
in a noisy environment are not as valuable as when the agent knows it is acting alone since the noise
may have perturbed the state of the system between the growls  for time horizon of length   the
noise does not matter and the value vectors overlap  as in figure   
second  since the presence of another agent is implicit in the static transition model  the agent
cannot update its model of the other agents actions during repeated interactions  this effect becomes more important as time horizon increases  our approach addresses this issue by allowing
explicit modeling of the other agent s   this results in policies of superior quality  as we show in
section    figure   shows a policy for an agent facing a noisy environment for time horizon of   
we compare it to the corresponding i pomdp policy in section    note that it is slightly different
   we assumed that  due to the noise  either door opens with probabilities of     at each turn  and nothing happens with
the probability      we explain the origin of this assumption in section   

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

l    l     ol  gr gr  l    
l    l  gl  ol  gr  ol  gl gr  l    

l    l     or  gl gl  l    
l    or  gl  l  gr  or  gr gl  l    
l    l     or  gl gl  ol  gr gr  l    
or    l     l    
l    l     or    

ol    l     l    
l    l     ol    
 

value function u 

 
 
 
 
 
 
 
 

   

   

   

   

 

p  tl 
p i tl 
i

pomdp with noise

pomdp

figure    the value function for single agent tiger game compared to an agent facing a noise factor 
for horizon of length    the   in the description of a policy stands for any of the
perceptual sequences not yet listed in the description of the policy 

        
ol
 

                                      
l

l

gr

gl

gr

l
gr

gl

 

ol

l
gr

l
gl

gr

or
gl

 

gl

l
gl

                     

l

gr

ol

            

l
 

l

gr

or
gl

 

or

figure    the policy graph corresponding to value function of pomdp with noise depicted in
fig    

  

fig mytrasiewicz   d oshi

than the policy without noise in the example by kaelbling  littman and cassandra        due to
differences in value functions 

   agent types and frames
the pomdp definition includes parameters that permit us to compute an agents optimal behavior   
conditioned on its beliefs  let us collect these implementation independent factors into a construct
we call an agent is type 
definition    type   a type of an agent i is  i   hbi   ai   i   ti   oi   ri   oci i  where bi is agent is
state of belief  an element of  s    oci is its optimality criterion  and the rest of the elements are
as defined before  let i be the set of agent is types 
given type  i   and the assumption that the agent is bayesian rational  the set of agents optimal
actions will be denoted as op t  i    in the next section  we generalize the notion of type to situations which include interactions with other agents  it then coincides with the notion of type used in
bayesian games  fudenberg   tirole        harsanyi        
it is convenient to define the notion of a frame  bi   of agent i 

b i be the set of
definition    frame   a frame of an agent i is  bi   hai   i   ti   oi   ri   oci i  let 
agent is frames 

for brevity one can write a type as consisting of an agents belief together with its frame   i  
hbi   bi i 
in the context of the tiger game described in the previous section  agent type describes the
agents actions and their results  the quality of the agents hearing  its payoffs  and its belief about
the tiger location 
realistically  apart from implementation independent factors grouped in type  an agents behavior may also depend on implementation specific parameters  like the processor speed  memory
available  etc  these can be included in the  implementation dependent  or complete  type  increasing the accuracy of predicted behavior  but at the cost of additional complexity  definition and use
of complete types is a topic of ongoing work 

   interactive pomdps
as we mentioned  our intention is to generalize pomdps to handle presence of other agents  we
do this by including descriptions of other agents  their types for example  in the state space  for
simplicity of presentation  we consider an agent i  that is interacting with one other agent  j  the
formalism easily generalizes to larger number of agents 
definition    i pomdp   an interactive pomdp of agent i  i pomdpi   is 
i pomdpi   hisi   a  ti   i   oi   ri i

   

   the issue of computability of solutions to pomdps has been a subject of much research  papadimitriou   tsitsiklis 
      madani  hanks    condon         it is of obvious importance when one uses pomdps to model agents  we
return to this issue later 

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

where 
 isi is a set of interactive states defined as isi   s  mj    interacting with agent i  where
s is the set of states of the physical environment  and mj is the set of possible models of agent
j  each model  mj  mj   is defined as a triple mj   hhj   fj   oj i  where fj   hj   aj  
is agent js function  assumed computable  which maps possible histories of js observations to
distributions over its actions  hj is an element of hj   and oj is a function specifying the way the
environment is supplying the agent with its input  sometimes we write model m j as mj   hhj   m
b j i 
where m
b j consists of fj and oj   it is convenient to subdivide the set of models into two classes 
the subintentional models  smj   are relatively simple  while the intentional models  imj   use the
notion of rationality to model the other agent  thus  mj   imj  smj  
simple examples of subintentional models include a no information model and a fictitious play
model  both of which are history independent  a no information model  gmytrasiewicz   durfee 
      assumes that each of the other agents actions is executed with equal probability  fictitious
play  fudenberg   levine        assumes that the other agent chooses actions according to a fixed
but unknown distribution  and that the original agents prior belief over that distribution takes a form
of a dirichlet distribution   an example of a more powerful subintentional model is a finite state
controller 
the intentional models are more sophisticated in that they ascribe to the other agent beliefs 
preferences and rationality in action selection   intentional models are thus js types  j   hbj   bj i 
under the assumption that agent j is bayesian rational   agent js belief is a probability distribution
over states of the environment and the models of the agent i  b j   s  mi    the notion of a type
we use here coincides with the notion of type in game theory  where it is defined as consisting of
all of the agent is private information relevant to its decision making  harsanyi        fudenberg
  tirole         in particular  if agents beliefs are private information  then their types involve
possibly infinitely nested beliefs over others types and their beliefs about others  mertens   zamir 
      brandenburger   dekel        aumann        aumann   heifetz           they are related
to recursive model structures in our prior work  gmytrasiewicz   durfee         the definition of
interactive state space is consistent with the notion of a completely specified state space put forward
by aumann         similar state spaces have been proposed by others  mertens   zamir       
brandenburger   dekel        
 a   ai  aj is the set of joint moves of all agents 
 ti is the transition model  the usual way to define the transition probabilities in pomdps
is to assume that the agents actions can change any aspect of the state description  in case of ipomdps  this would mean actions modifying any aspect of the interactive states  including other
agents observation histories and their functions  or  if they are modeled intentionally  their beliefs
and reward functions  allowing agents to directly manipulate other agents in such ways  however 
violates the notion of agents autonomy  thus  we make the following simplifying assumption 
 
   if there are more agents  say n      then isi   s n
j   mj
   technically  according to our notation  fictitious play is actually an ensemble of models 
   dennet        advocates ascribing rationality to other agent s   and calls it assuming an intentional stance towards
them 
   note that the space of types is by far richer than that of computable models  in particular  since the set of computable
models is countable and the set of types is uncountable  many types are not computable models 
   implicit in the definition of interactive beliefs is the assumption of coherency  brandenburger   dekel        

  

fig mytrasiewicz   d oshi

model non manipulability assumption  mnm   agents actions do not change the other
agents models directly 
given this simplification  the transition model can be defined as t i   s  a  s        
autonomy  formalized by the mnm assumption  precludes  for example  direct mind control 
and implies that other agents belief states can be changed only indirectly  typically by changing the
environment in a way observable to them  in other words  agents beliefs change  like in pomdps 
but as a result of belief update after an observation  not as a direct result of any of the agents
actions   
 i is defined as before in the pomdp model 
 oi is an observation function  in defining this function we make the following assumption 
model non observability  mno   agents cannot observe others models directly 
given this assumption the observation function is defined as o i   s  a  i         
the mno assumption formalizes another aspect of autonomy  agents are autonomous in that
their observations and functions  or beliefs and other properties  say preferences  in intentional
models  are private and the other agents cannot observe them directly    
 ri is defined as ri   isi  a     we allow the agent to have preferences over physical
states and models of other agents  but usually only the physical state will matter 
as we mentioned  we see interactive pomdps as a subjective counterpart to an objective external view in stochastic games  fudenberg   tirole         and also followed in some work in
ai  boutilier        and  koller   milch        and in decentralized pomdps  bernstein et al  
      nair et al          interactive pomdps represent an individual agents point of view on the
environment and the other agents  and facilitate planning and problem solving at the agents own
individual level 
    belief update in i pomdps
we will show that  as in pomdps  an agents beliefs over their interactive states are sufficient
statistics  i e   they fully summarize the agents observation histories  further  we need to show how
beliefs are updated after the agents action and observation  and how solutions are defined 
t 
the new belief state  bti   is a function of the previous belief state  bt 
i   the last action  ai  
t
and the new observation  oi   just as in pomdps  there are two differences that complicate belief
update when compared to pomdps  first  since the state of the physical environment depends on
the actions performed by both agents the prediction of how the physical state changes has to be
made based on the probabilities of various actions of the other agent  the probabilities of others
actions are obtained based on their models  thus  unlike in bayesian and stochastic games  we do
not assume that actions are fully observable by other agents  rather  agents can attempt to infer what
actions other agents have performed by sensing their results on the environment  second  changes in
the models of other agents have to be included in the update  these reflect the others observations
and  if they are modeled intentionally  the update of the other agents beliefs  in this case  the agent
has to update its beliefs about the other agent based on what it anticipates the other agent observes
    the possibility that agents can influence the observational capabilities of other agents can be accommodated by
including the factors that can change sensing capabilities in the set s 
    again  the possibility that agents can observe factors that may influence the observational capabilities of other agents
is allowed by including these factors in s 

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

and how it updates  as could be expected  the update of the possibly infinitely nested belief over
others types is  in general  only asymptotically computable 
proposition     sufficiency  in an interactive pomdp of agent i  is current belief  i e   the probability distribution over the set s  mj   is a sufficient statistic for the past history of is observations 
t 
the next proposition defines the agent is belief update function  b ti  ist     p r ist  oti   at 
i   bi   
where ist  isi is an interactive state  we use the belief state estimation function  se i   as an abt  t
breviation for belief updates for individual states so that bti   sei  bt 
i   ai   oi   
t  t  t t
t 
t 
i  bi   ai   oi   bi   will stand for p r bti  bi   ai   oti    further below we also define the set of
type dependent optimal actions of an agent  op t  i   

proposition     belief update  under the mnm and mno assumptions  the belief update function
for an interactive pomdp hisi   a  ti   i   oi   ri i  when mj in ist is intentional  is 
bti  ist     
ti

p

t   
bt 
i  is

ist   m
b t 
 bjt
j
p
t 
t 
 s   a   st   
otj

p

t 
t t    ot  
p r at 
i
j  j  oi  s   a

at 
j
t  t  t t
t t    ot  
t
 b
j j   aj   oj   bj  oj  s   a
j

   

 m
b tj  
when mj in ist is subintentional the first summation extends over ist    m
b t 
j
t 
t 
t 
t  t  t t
p r at 
j  j   is replaced with p r aj  mj    and jt  bj   aj   oj   bj   is replaced with the
t
t
kronecker delta function k  append ht 
j   oj    hj   

above  bt 
and btj are the belief elements of jt  and jt   respectively   is a normalizing constant 
j
t 
t 
and p r at 
is bayesian rational for agent described by type
j  j   is the probability that aj
t 
t 
 
j   this probability is equal to  op t  j    if aj  op t  j    and it is equal to zero otherwise 
we define op t in section        for the case of js subintentional model  is    s  mj    ht 
and
j
t respectively  o is the observation
htj are the observation histories which are part of mt 
 
and
m
j
j
j
t 
t 
t 
function in mtj   and p r at 
 m
 
is
the
probability
assigned
by
m
to
a
j
j
j
j   append returns
a string with the second argument appended to the first  the proofs of the propositions are in the
appendix 
proposition   and eq    have a lot in common with belief update in pomdps  as should be
expected  both depend on agent is observation and transition functions  however  since agent is
observations also depend on agent js actions  the probabilities of various actions of j have to be
included  in the first line of eq      further  since the update of agent js model depends on what
j observes  the probabilities of various observations of j have to be included  in the second line of
eq      the update of js beliefs is represented by the j term  the belief update can easily be
generalized to the setting where more than one other agents co exist with agent i 
p
    if the agents prior belief over isi is given by a probability density function then the
ist  is replaced by
 
  otj   btj   takes the form of dirac delta function over argument bt 
  at 
an integral  in that case jt  bt 
j
j
j
  otj    btj   
  at 
d  sejt  bt 
j
j

  

fig mytrasiewicz   d oshi

    value function and solutions in i pomdps
analogously to pomdps  each belief state in i pomdp has an associated value reflecting the maximum payoff the agent can expect in this belief state 


p
p
b
eri  is  ai  bi  is    
u  i     max
p r oi  ai   bi  u  hsei  bi   ai   oi    i i 
   
ai ai

oi i

is

p
where  eri  is  ai    
aj ri  is  ai   aj  p r aj  mj    eq    is a basis for value iteration in ipomdps 
agent is optimal action  ai   for the case of infinite horizon criterion with discounting  is an
element of the set of optimal actions for the belief state  op t  i    defined as 


p
p
op t  i     argmax
eri  is  ai  bi  is    
p r oi  ai   bi  u  hsei  bi   ai   oi    bi i 
ai ai

oi i

is

   
as in the case of belief update  due to possibly infinitely nested beliefs  a step of value iteration
and optimal actions are only asymptotically computable 

   finitely nested i pomdps
possible infinite nesting of agents beliefs in intentional models presents an obvious obstacle to
computing the belief updates and optimal solutions  since the models of agents with infinitely
nested beliefs correspond to agent functions which are not computable it is natural to consider
finite nestings  we follow approaches in game theory  aumann        brandenburger   dekel 
      fagin et al          extend our previous work  gmytrasiewicz   durfee         and construct
finitely nested i pomdps bottom up  assume a set of physical states of the world s  and two
agents i and j  agent is   th level beliefs  bi     are probability distributions over s  its   th level
types  i     contain its   th level beliefs  and its frames  and analogously for agent j    level types
are  therefore  pomdps      level models include   level types  i e   intentional models  and the
subintentional models  elements of sm   an agents first level beliefs are probability distributions
over physical states and   level models of the other agent  an agents first level types consist of
its first level beliefs and frames  its first level models consist of the types upto level   and the
subintentional models  second level beliefs are defined in terms of first level models and so on 
formally  define spaces 
isi     s 
j      hbj     bj i   bj     isj       mj     j    smj
isi     s  mj    
j      hbj     bj i   bj     isj       mj     j    mj  
 
 
 
 
 
 
isi l   s  mj l    j l    hbj l   bj i   bj l   isj l     mj l   j l  mj l 
definition     finitely nested i pomdp  a finitely nested i pomdp of agent i  i pomdp i l   is 
i pomdpi l   hisi l   a  ti   i   oi   ri i
    in   level types the other agents actions are folded into the t   o and r functions as noise 

  

   

fia f ramework for s equential p lanning in m ulti  agent s ettings

the parameter l will be called the strategy level of the finitely nested i pomdp  the belief update 
value function  and the optimal actions for finitely nested i pomdps are computed using equation  
and equation    but recursion is guaranteed to terminate at   th level and subintentional models 
agents which are more strategic are capable of modeling others at deeper levels  i e   all levels
up to their own strategy level l   but are always only boundedly optimal  as such  these agents
could fail to predict the strategy of a more sophisticated opponent  the fact that the computability
of an agent function implies that the agent may be suboptimal during interactions has been pointed
out by binmore         and proved more recently by nachbar and zame         intuitively  the
difficulty is that an agents unbounded optimality would have to include the capability to model the
other agents modeling the original agent  this leads to an impossibility result due to self reference 
which is very similar to godels incompleteness theorem and the halting problem  brandenburger 
       on a positive note  some convergence results  kalai   lehrer        strongly suggest that
approximate optimality is achievable  although their applicability to our work remains open 
as we mentioned  the   th level types are pomdps  they provide probability distributions
over actions of the agent modeled at that level to models with strategy level of    given probability
distributions over other agents actions the level   models can themselves be solved as pomdps 
and provide probability distributions to yet higher level models  assume that the number of models
considered at each level is bound by a number  m   solving an i pomdp i l in then equivalent to
solving o m l   pomdps  hence  the complexity of solving an i pomdpi l is pspace hard for
finite time horizons    and undecidable for infinite horizons  just like for pomdps 
    some properties of i pomdps
in this section we establish two important properties  namely convergence of value iteration and
piece wise linearity and convexity of the value function  for finitely nested i pomdps 
      c onvergence

of

value i teration

for an agent i and its i pomdpi l   we can show that the sequence of value functions   u n    where
n is the horizon  obtained by value iteration defined in eq     converges to a unique fixed point  u   
let us define a backup operator h   b  b such that u n   hu n    and b is the set of all
bounded value functions  in order to prove the convergence result  we first establish some of the
properties of h 
lemma    isotonicity   for any finitely nested i pomdp value functions v and u   if v  u   then
hv  hu  
the proof of this lemma is analogous to one due to hauskrecht         for pomdps  it is
also sketched in the appendix  another important property exhibited by the backup operator is the
property of contraction 
lemma    contraction   for any finitely nested i pomdp value functions v   u and a discount
factor             hv  hu       v  u    
the proof of this lemma is again similar to the corresponding one in pomdps  hausktecht 
       the proof makes use of lemma           is the supremum norm 
    usually pspace complete since the number of states in i pomdps is likely to be larger than the time horizon
 papadimitriou   tsitsiklis        

  

fig mytrasiewicz   d oshi

under the contraction property of h  and noting that the space of value functions along with
the supremum norm forms a complete normed space  banach space   we can apply the contraction
mapping theorem  stokey   lucas        to show that value iteration for i pomdps converges
to a unique fixed point  optimal solution   the following theorem captures this result 
theorem    convergence   for any finitely nested i pomdp  the value iteration algorithm starting from any arbitrary well defined value function converges to a unique fixed point 
the detailed proof of this theorem is included in the appendix 
as in the case of pomdps  russell   norvig         the error in the iterative estimates  u n   for
finitely nested i pomdps  i e     u n  u      is reduced by the factor of at least  on each iteration 
hence  the number of iterations  n   needed to reach an error of at most  is 
n   dlog rmax          log    e

    

where rmax is the upper bound of the reward function 
      p iecewise l inearity

and

c onvexity

another property that carries over from pomdps to finitely nested i pomdps is the piecewise
linearity and convexity  pwlc  of the value function  establishing this property allows us to decompose the i pomdp value function into a set of alpha vectors  each of which represents a policy
tree  the pwlc property enables us to work with sets of alpha vectors rather than perform value
iteration over the continuum of agents beliefs  theorem   below states the pwlc property of the
i pomdp value function 
theorem    pwlc   for any finitely nested i pomdp  u is piecewise linear and convex 
the complete proof of theorem   is included in the appendix  the proof is similar to one
due to smallwood and sondik        for pomdps and proceeds by induction  the basis case is
established by considering the horizon   value function  showing the pwlc for the inductive step
requires substituting the belief update  eq     into eq     followed by factoring out the belief from
both terms of the equation 

   example  multi agent tiger game
to illustrate optimal sequential behavior of agents in multi agent settings we apply our i pomdp
framework to the multi agent tiger game  a traditional version of which we described before 
    definition
let us denote the actions of opening doors and listening as or  ol and l  as before  tl and
tr denote states corresponding to tiger located behind the left and right door  respectively  the
transition  reward and observation functions depend now on the actions of both agents  again  we
assume that the tiger location is chosen randomly in the next time step if any of the agents opened
any doors in the current step  we also assume that the agent hears the tigers growls  gr and gl 
with the accuracy of      to make the interaction more interesting we added an observation of
door creaks  which depend on the action executed by the other agent  creak right  cr  is likely due
  

fia f ramework for s equential p lanning in m ulti  agent s ettings

to the other agent having opened the right door  and similarly for creak left  cl  silence  s  is a good
indication that the other agent did not open doors and listened instead  we assume that the accuracy
of creaks is      we also assume that the agents payoffs are analogous to the single agent versions
described in section     to make these cases comparable  note that the result of this assumption is
that the other agents actions do not impact the original agents payoffs directly  but rather indirectly
by resulting in states that matter to the original agent  table   quantifies these factors 

hai   aj i
hol  i
hor  i
h  oli
h  ori
hl  li
hl  li

state
 
 
 
 
tl
tr

tl
   
   
   
   
   
 

tr
   
   
   
   
 
   

hai   aj i
hor  ori
hol  oli
hor  oli
hol  ori
hl  li
hl  ori
hor  li
hl  oli
hol  li

transition function  ti   tj

tl
  
    
  
    
  
  
  
  
    

tr
    
  
    
  
  
  
    
  
  

hai   aj i
hor  ori
hol  oli
hor  oli
hol  ori
hl  li
hl  ori
hor  li
hl  oli
hol  li

tl
  
    
    
  
  
  
  
    
  

tr
    
  
  
    
  
    
  
  
  

reward functions of agents i and j

hai   aj i
hl  li
hl  li
hl  oli
hl  oli
hl  ori
hl  ori
hol  i
hor  i

state
tl
tr
tl
tr
tl
tr



h gl  cl i
         
         
        
        
         
         
   
   

h gl  cr i
         
         
         
         
        
        
   
   

h gl  s i
        
        
         
         
         
         
   
   

h gr  cl i
         
         
        
        
         
         
   
   

h gr  cr i
         
         
         
         
        
        
   
   

h gr  s i
        
        
         
         
         
         
   
   

hai   aj i
hl  li
hl  li
hol  li
hol  li
hor  li
hor  li
h  oli
h  ori

state
tl
tr
tl
tr
tl
tr



h gl  cl i
         
         
        
        
         
         
   
   

h gl  cr i
         
         
         
         
        
        
   
   

h gl  s i
        
        
         
         
         
         
   
   

h gr  cl i
         
         
        
        
         
         
   
   

h gr  cr i
         
         
         
         
        
        
   
   

h gr  s i
        
        
         
         
         
         
   
   

observation functions of agents i and j 
table    transition  reward  and observation functions for the multi agent tiger game 
when an agent makes its choice in the multi agent tiger game  it considers what it believes
about the location of the tiger  as well as whether the other agent will listen or open a door  which in
turn depends on the other agents beliefs  reward function  optimality criterion  etc     in particular 
if the other agent were to open any of the doors the tiger location in the next time step would be
chosen randomly  thus  the information obtained from hearing the previous growls would have to
be discarded  we simplify the situation by considering is i pomdp with a single level of nesting 
assuming that all of the agent js properties  except for beliefs  are known to i  and that js time
horizon is equal to is  in other words  is uncertainty pertains only to js beliefs and not to its
frame  agent is interactive state space is  isi     s  j     where s is the physical state  s  tl 
    we assume an intentional model of the other agent here 

  

fig mytrasiewicz   d oshi

tr   and j   is a set of intentional models of agent js  each of which differs only in js beliefs
over the location of the tiger 
    examples of the belief update
in section    we presented the belief update equation for i pomdps  eq      here we consider
examples of beliefs  bi     of agent i  which are probability distributions over s   j     each   th
level type of agent j  j    j     contains a flat belief as to the location of the tiger  which can be
represented by a single probability assignment  bj     pj  t l  
     
     

     

pr tl p
pr tl b j   
j

pr tl p
pr tl b j 

j

 

     
     
   
     

     
   
     

     
     

     
 

   

   

   

   

 

     
 

   

   

   

   

 

   

 

pb j
j  tl 

     

     

     

     

j

 

     

pr tr p
pr tr b j 

pr tr p
pr tr b j 

j

 

pjb j
 tl 
j tl 

   
     

     
   
     

     
     

     

 

   

           
ppb j
 tl 
 tr 
 tl 

 
     

jj

 

   

   

   

p j  tl 
b j

 i 

 ii 

figure    two examples of singly nested belief states of agent i  in each case i has no information
about the tigers location  in  i  agent i knows that j does not know the location of the
tiger  the single point  star  denotes a dirac delta function which integrates to the height
of the point  here       in  ii  agent i is uninformed about js beliefs about tigers location 

in fig    we show some examples of level   beliefs of agent i  in each case i does not know
the location of the tiger so that the marginals in the top and bottom sections of the figure sum up to
    for probabilities of tl and tr each  in fig    i   i knows that j assigns     probability to tiger
being behind the left door  this is represented using a dirac delta function  in fig    ii   agent i is
uninformed about js beliefs  this is represented as a uniform probability density over all values of
the probability j could assign to state tl 
to make the presentation of the belief update more transparent we decompose the formula in
eq    into two steps 
  

fia f ramework for s equential p lanning in m ulti  agent s ettings

t 
 prediction  when agent i performs an action at 
i   and given that agent j performs aj   the
predicted belief state is 

bbt  ist     p r ist  at    at    bt      p t  bt  bt bt   ist   p r at   t   
i
j
j
i
j
i
is
 j  j i
p
t  st    at    st   oj  st   at    otj  

    

otj

t  t t
jt  bt 
j   a j   o j   bj  

 correction  when agent i perceives an observation  o ti   the predicted belief states 
t  t 
p r  at 
i   aj   bi    are combined according to 
bti  ist     p r ist  oti   ait    bt 
i   

x

t  t 
oi  st   at    oti  p r ist  at 
i   a j   bi  

    

at 
j

where  is the normalizing constant 

t 

t

     
     
 

   

           
pb j tl 

 

j

     
     
           
pb j tl 

 

   

 

   
   
   
   
   
l  gl s 
   
   
l  gl s   
 
   
 

l  gl s 

 

   

   

   

pjb j tl 

 gl s 

 gl s 
   

   

   

   

l  gl s 

   

l  gl s 

    

   

 

           
pb j tl 

 

    

    

     

    

l  gl s 
   

   

pjb j
 tl 

   

pjb j
 tl 

 b 

   

pjb j tl 

     

    

 

 

   

     

    

    
 

   

l  gl s 

    

    
   
    
   
    
   
    
   
    

j

 a 

   

pjb j
 tl 

   
   
   
   
   
   
   
   

j 

j 
   

pr tr pj  

     

pr tr b j 

j 

pr tr p
pr tr b j 

l  l gl 

   

   

   

l  l gr 

     

 

 gl s 

 

     
     

l  gl s 

    
   
    
   
    
   
    
   
    

pr tl p
pr tl b j 

l  l gr 

   
     

bi

pr tr b j 
pr tr p
 
j

     

t  

bi

 gl s 

pr tl p
pr tl b j 

     

pr tl p
pr tl b j   
j

pr tl p
pr tl b j 

j 

     

t

bi

l  l gl 

pr tr p
pr tr b j   
j

bi

 c 

   

 

 
 

   

j

 d 

figure    a trace of the belief update of agent i   a  depicts the prior   b  is the result of prediction
given is listening action  l  and a pair denoting js action and observation  i knows that
j will listen and could hear tigers growl on the right or the left  and that the probabilities
j would assign to tl are      or       respectively   c  is the result of correction after
i observes tigers growl on the left and no creaks  hgl si  the probability i assigns to
tl is now greater than tr   d  depicts the results of another update  both prediction and
correction  after another listen action of i and the same observation  hgl si 
each discrete point above denotes  again  a dirac delta function which integrates to the height of
the point 
in fig     we display the example trace through the update of singly nested belief  in the first
column of fig     labeled  a   is an example of agent is prior belief we introduced before  according
  

fig mytrasiewicz   d oshi

to which i knows that j is uninformed of the location of the tiger     let us assume that i listens and
hears a growl from the left and no creaks  the second column of fig      b   displays the predicted
belief after i performs the listen action  eq       as part of the prediction step  agent i must solve
js model to obtain js optimal action when its belief is      term p r a t 
j  j   in eq       given the
value function in fig     this evaluates to probability of   for listen action  and zero for opening of
any of the doors  i also updates js belief given that j listens and hears the tiger growling from either
t  t t
the left  gl  or right  gr   term jt  bt 
j   aj   oj   bj   in eq       agent js updated probabilities
for tiger being on the left are      and       for js hearing gl and gr  respectively  if the tiger is
on the left  top of fig     b   js observation gl is more likely  and consequently js assigning the
probability of      to state tl is more likely  i assigns a probability of       to this state   when
the tiger is on the right j is more likely to hear gr and i assigns the lower probability         to
js assigning a probability      to tiger being on the left  the third column   c   of fig    shows
the posterior belief after the correction step  the belief in column  b  is updated to account for is
hearing a growl from the left and no creaks  hgl si  the resulting marginalised probability of the
tiger being on the left is higher        than that of the tiger being on the right  if we assume that in
the next time step i again listens and hears the tiger growling from the left and no creaks  the belief
state depicted in the fourth column of fig    results 
in fig    we show the belief update starting from the prior in fig     ii   according to which
agent i initially has no information about what j believes about the tigers location 
the traces of belief updates in fig    and fig    illustrate the changing state of information agent
i has about the other agents beliefs  the benefit of representing these updates explicitly is that  at
each stage  is optimal behavior depends on its estimate of probabilities of js actions  the more
informative these estimates are the more value agent i can expect out of the interaction  below  we
show the increase in the value function for i pomdps compared to pomdps with the noise factor 
    examples of value functions
this section compares value functions obtained from solving a pomdp with a static noise factor 
accounting for the presence of another agent    to value functions of level   i pomdp  the advantage of more refined modeling and update in i pomdps is due to two factors  first is the ability to
keep track of the other agents state of beliefs to better predict its future actions  the second is the
ability to adjust the other agents time horizon as the number of steps to go during the interaction
decreases  neither of these is possible within the classical pomdp formalism 
we continue with the simple example of i pomdpi   of agent i  in fig    we display is
value function for the time horizon of    assuming that is initial belief as to the value j assigns
to tl  pj  t l   is as depicted in fig     ii   i e  i has no information about what j believes about
tigers location  this value function is identical to the value function obtained for an agent using
a traditional pomdp framework with noise  as well as single agent pomdp which we described
in section      the value functions overlap since agents do not have to update their beliefs and
the advantage of more refined modeling of agent j in is i pomdp does not become apparent  put
another way  when agent i models j using an intentional model  it concludes that agent j will open
each door with probability     and listen with probability      this coincides with the noise factor
we described in section     
    the points in fig    again denote dirac delta functions which integrate to the value equal to the points height 
    the pomdp with noise is the same as level   i pomdp 

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

t 

t
bi

bi

l l gr 
l l gl 
   

   

l l gl 
   

   

l l gr 

   

   

   

   

     
   

   

l l gr 

     
     
 

   

   

   

   

 

l ol or   

 

pjb j tl 

 

     
     

     
 

   

   

   

   

 

   

 

 

 

pr tl b j 

l l gr 

pjb j tl 

   

   

   

 

pjb j
 tl 
       
      
       

      

      

       
      

       

       
      
       

      

      

       

       

      

l ol or   

   

       

pr tl  pj  

l ol or   

     

   

      

l ol or   

     

   

       

l l gl 

   

   

pjb j
 tl 

l l gl 

     

pr tr 
pj  
pr tr b j 

   

pr tr b j 
pr tr p
j 

pr tl p
 
pr tl b j 
j

     

pr tr b j 
pr tr p
j 

     

pr tl b j 
pr tl 
pj  

     

      

       

       
 

   

   

   

   

 

 

pjb j
 tl 

 a 

   

   

   

   

 

pjb j
 tl 

 b 
 gl s 

t

bi

   

   

   
    

pr tr pj  

   

pr tr b j 

pr tl p
 
pr tl b j 
j

   

 

   

   

    

   

   

   
    
   
 

 
 

   

   

   

pjb j
 tl 

   

 

 

   

   

   

   

 

pj b j
 tl 

 c 

figure    a trace of the belief update of agent i   a  depicts the prior according to which i is
uninformed about js beliefs   b  is the result of the prediction step after is listening
action  l   the top half of  b  shows is belief after it has listened and given that j also
listened  the two observations j can make  gl and gr  each with probability dependent
on the tigers location  give rise to flat portions representing what i knows about js belief
in each case  the increased probability i assigns to js belief between       and       is
due to js updates after it hears gl and after it hears gr resulting in the same values in
this interval  the bottom half of  b  shows is belief after i has listened and j has opened
the left or right door  plots are identical for each action and only one of them is shown   i
knows that j has no information about the tigers location in this case   c  is the result of
correction after i observes tigers growl on the left and no creaks hgl si  the plots in  c 
are obtained by performing a weighted summation of the plots in  b   the probability i
assigns to tl is now greater than tr  and information about js beliefs allows i to refine
its prediction of js action in the next time step 

  

fig mytrasiewicz   d oshi

l
or

ol

  

value function  u 

 

 

 

 

 

 

   

   

   

   

 

pp i tl 
i  tl 
level   i pomdp

pomdp with noise

figure    for time horizon of   the value functions obtained from solving a singly nested i pomdp
and a pomdp with noise factor overlap 
l    ol   gr s   l    

l    or   gl s   l    

l    l   gl     ol   gr    

ol    l    

l    l    

l    or   gl     l   gr    

l    l  gl  ol  gr 

l    or  gl  l  gr 

or    l    

 

value function  u 

 

 

 

 

  
 

   

   

   

   

 

pp i tl 
i  tl 
level   i pomdp

pomdp with noise

figure    comparison of value functions obtained from solving an i pomdp and a pomdp with
noise for time horizon of    i pomdp value function dominates due to agent i adjusting
the behavior of agent j to the remaining steps to go in the interaction 

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

 

value function  u 

 
 
 
 
 
 
 
 

   

   

   

   

 

p i  tl 
p i tl 
level   i pomdp

pomdp with noise

figure     comparison of value functions obtained from solving an i pomdp and a pomdp with
noise for time horizon of    i pomdp value function dominates due to agent is adjusting js remaining steps to go  and due to is modeling js belief update  both factors
allow for better predictions of js actions during interaction  the descriptions of individual policies were omitted for clarity  they can be read off of fig     

in fig    we display is value functions for the time horizon of    the value function of
i pomdpi   is higher than the value function of a pomdp with a noise factor  the reason is
not related to the advantages of modeling agent js beliefs  this effect becomes apparent at the time
horizon of   and longer  rather  the i pomdp solution dominates due to agent i modeling js time
horizon during interaction  i knows that at the last time step j will behave according to its optimal
policy for time horizon of    while with two steps to go j will optimize according to its   steps to go
policy  as we mentioned  this effect cannot be modeled using a pomdp with a static noise factor
included in the transition function 
fig     shows a comparison between the i pomdp and the noisy pomdp value functions for
horizon    the advantage of more refined agent modeling within the i pomdp framework has
increased    both factors  is adjusting js steps to go and is modeling js belief update during
interaction are responsible for the superiority of values achieved using the i pomdp  in particular 
recall that at the second time step is information as to js beliefs about the tigers location is as
depicted in fig     c   this enables i to make a high quality prediction that  with two steps left to
go  j will perform its actions ol  l  and or with probabilities                   and         
respectively  recall that for pomdp with noise these probabilities remained unchanged at          
and      respectively  
fig     shows agent is policy graph for time horizon of    as usual  it prescribes the optimal
first action depending on the initial belief as to the tigers location  the subsequent actions depend
on the observations received  the observations include creaks that are indicative of the other agents
    note that i pomdp solution is not as good as the solution of a pomdp for an agent operating alone in the environment shown in fig    

  

fig mytrasiewicz   d oshi

          
ol

              

              

l
 

l

 gr s 

 gl cl cr 
 gr   

              

              

l

 

ol

or
 

 gr s 
 gl cl cr 

l

l
 gr   

          

l

 gr cl cr 
 gr cl cr 
 gl cl cr 
 gl s 
 gr     gl   
 gl   
 gr s 
 gl s 

 gl s 
 gr cl cr 

ol

              

l

 gl   

l
 

 gr   

or
 gl   

 

or

l

figure     the policy graph corresponding to the i pomdp value function in fig     
having opened a door  the creaks contain valuable information and allow the agent to make more
refined choices  compared to ones in the noisy pomdp in fig     consider the case when agent i
starts out with fairly strong belief as to the tigers location  decides to listen  according to the four
off center top row l nodes in fig      and hears a door creak  the agent is then in the position to
open either the left or the right door  even if that is counter to its initial belief  the reason is that the
creak is an indication that the tigers position has likely been reset by agent j and that j will then
not open any of the doors during the following two time steps  now  two growls coming from the
same door lead to enough confidence to open the other door  this is because the agent is hearing
of tigers growls are indicative of the tigers position in the state following the agents actions 
note that the value functions and the policy above depict a special case of agent i having no
information as to what probability j assigns to tigers location  fig     ii    accounting for and
visualizing all possible beliefs i can have about js beliefs is difficult due to the complexity of the
space of interactive beliefs  as our ongoing work indicates  a drastic reduction in complexity is
possible without loss of information  and consequently representation of solutions in a manageable
number of dimensions is indeed possible  we will report these results separately 

   conclusions
we proposed a framework for optimal sequential decision making suitable for controlling autonomous
agents interacting with other agents within an uncertain environment  we used the normative
paradigm of decision theoretic planning under uncertainty formalized as partially observable markov
decision processes  pomdps  as a point of departure  we extended pomdps to cases of agents
interacting with other agents by allowing them to have beliefs not only about the physical environment  but also about the other agents  this could include beliefs about the others abilities  sensing
capabilities  beliefs  preferences  and intended actions  our framework shares numerous properties
with pomdps  has analogously defined solutions  and reduces to pomdps when agents are alone
in the environment 
in contrast to some recent work on dec pomdps  bernstein et al         nair et al         
and to work motivated by game theoretic equilibria  boutilier        hu   wellman        koller
  

fia f ramework for s equential p lanning in m ulti  agent s ettings

  milch        littman         our approach is subjective and amenable to agents independently
computing their optimal solutions 
the line of work presented here opens an area of future research on integrating frameworks for
sequential planning with elements of game theory and bayesian learning in interactive settings  in
particular  one of the avenues of our future research centers on proving further formal properties of
i pomdps  and establishing clearer relations between solutions to i pomdps and various flavors
of equilibria  another concentrates on developing efficient approximation techniques for solving
i pomdps  as for pomdps  development of approximate approaches to i pomdps is crucial for
moving beyond toy problems  one promising approximation technique we are working on is particle
filtering  we are also devising methods for representing i pomdp solutions without assumptions
about whats believed about other agents beliefs  as we mentioned  in spite of the complexity of the
interactive state space  there seem to be intuitive representations of belief partitions corresponding
to optimal policies  analogous to those for pomdps  other research issues include the suitable
choice of priors over models    and the ways to fulfill the absolute continuity condition needed for
convergence of probabilities assigned to the alternative models during interactions  kalai   lehrer 
      

acknowledgments
this research is supported by the national science foundation career award iri          and
nsf award iri         

appendix a  proofs
proof of propositions   and    we start with proposition    by applying the bayes theorem 

t 
bti  ist     p r ist  oti   at 
i   bi    

 
 bt 
p r ist  oti  at 
i
i
t  t 
t
p r oi  ai  bi  

p
t   
   ist  bit   ist   p r ist   oti  at 
i   is
p
p
t 
t   p r at   at    ist   
   ist  bit   ist    at  p r ist   oti  at 
i   aj   is
j
i
j
p
p
    
t 
t   p r at   ist   
 
is
 
a
   ist  bit   ist    at  p r ist   oti  at 
j
j
i
j
p
p
t 
i
t t    ist   p r ist  at    ist   
   ist  bit   ist    at  p r at 
j  mj  p r ot  is   a
j
p
p
t 
i
t t   p r ist  at    ist   
   ist  bit   ist    at  p r at 
j  mj  p r ot  is   a
j
p
p
t 
t t    ot  p r ist  at    ist   
   ist  bit   ist    at  p r at 
i
j  mj  oi  s   a
j

    we are looking at kolmogorov complexity  li   vitanyi        as a possible way to assign priors 

  

fig mytrasiewicz   d oshi

to simplify the term p r ist  at    ist    let us substitute the interactive state ist with its components  when mj in the interactive states is intentional  ist    st   jt      st   btj   bjt   

p r ist  at    ist      p r st   btj   bjt  at    ist   
  p r btj  st   bjt   at    ist   p r st   bjt  at    ist   
  p r btj  st   bjt   at    ist   p r bjt  st   at    ist   p r st  at    ist   
  p r btj  st   bjt   at    ist   i bjt    bjt  ti  st    at    st  
    
b tj   
when mj is subintentional  ist    st   mtj      st   htj   m

p r ist  at    ist      p r st   htj   m
b tj  at    ist   
b tj  at    ist   
b tj   at    ist   p r st   m
  p r htj  st   m
b tj   at    ist   p r bjt  st   at    ist   p r st  at    ist   
  p r htj  st   m
t
t
  p r hj  s   m
b tj   at    ist   i m
b tj  ti  st    at    st  
b t 
    
j  m

the joint action pair  at    may change the physical state  the third term on the right hand
side of eqs     and     above captures this transition  we utilized the mnm assumption to replace
the second terms of the equations with boolean identity functions  i  bjt    bjt   and i m
b t 
b tj  
j  m
respectively  which equal   if the two frames are identical  and   otherwise  let us turn our attention
to the first terms  if mj in ist and ist  is intentional 
p
p r btj  st   bjt   at    ist      ot p r btj  st   bjt   at    ist    otj  p r otj  st   bjt   at    ist   
pj
  ot p r btj  st   bjt   at    ist    otj  p r otj  st   bjt   at   
pj
t  t t
t    ot  
  ot jt  bt 
j
j   aj   oj   bj  oj  st   a

    

j

else if it is subintentional 

p r htj  st   m
b tj   at    ist     

 

 

p

t

po j
t

po j
otj

b tj   at    ist   
b tj   at    ist    otj  p r otj  st   m
p r htj  st   m

b tj   at   
b tj   at    ist    otj  p r otj  st   m
p r htj  st   m

t
t
t    ot  
k  append ht 
j
j   oj    hj  oj  st   a

    

t  t
in eq      the first term on the right hand side is   if agent js belief update  se j  bt 
j   a j   oj  
generates a belief state equal to btj   similarly  in eq        the first term is   if appending the otj
to ht 
results in htj   k is the kronecker delta function  in the second terms on the right hand
j
side of the equations  the mno assumption makes it possible to replace p r o t  st   bt   at    with
j

j

oj  st   at    otj    and p r otj  st   m
b tj   at    with oj  st   at    otj   respectively 
let us now substitute eq     into eq     
p
t  t t
t t    ot  i 
bt    bt  ti  st    at    st  
p r ist  at    ist      ot jt  bt 
j
j
j   aj   oj   bj  oj  s   a
j
j
    
 
 
substituting eq     into eq     we get 
p
t
t
t t    ot  i m
p r ist  at    ist      ot k  append ht 
b t 
b tj  
j
j   oj    hj  oj  s   a
j  m
j

ti  st    at    st  

    

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

replacing eq     into eq     we get 
p
p
t  t t
t 
t   
p r at 
 jt   oi  st   at    oti   ot jt  bt 
ist  bi  is
j   a j   o j   bj  
j
at 
j
j
oj  st   at    otj  i bjt    bjt  ti  st    at    st  

bti  ist     

p

    

similarly  replacing eq      into eq     we get 

p
p
t 
t t    ot  
t   
p r at 
bti  ist      ist  bt 
i
j  mj  oi  s   a
i  is
at 
j
p
t  t
t 
t
t
t 
t
bj  m
b tj  ti  st    at    st  
 ot k  append hj   oj    hj  oj  s   a   oj  i m
j

we arrive at the final expressions for the belief update by removing the terms
i m
b t 
b tj   and changing the scope of the first summations 
j  m
when mj in the interactive states is intentional 

i  bjt    bjt  

p
p
t 
t t    ot  
bt   ist    at  p r at 
bti  ist      ist   m
i
j  j  oi  s   a
b t 
 bjt i
j
j
p
t  t  t t
t 
t 
t
t
t 
t
 ot jt  bj   aj   oj   bj  oj  s   a   oj  ti  s   a   s  

      
and

    

j

else  if it is subintentional 
p
p
t 
t 
t t    ot  
 ist    at  p r at 
bti  ist      ist   m
t bi
i
j  mj  oi  s   a
 
m
b
b t 
j
j
j
p
t    ht  o  st   at    ot  t  st    at    st  
 
o
 ot k  append ht 
j
j
j
j i
j

    

j

since proposition   expresses the belief bti  ist   in terms of parameters of the previous time step
only  proposition   holds as well 
before we present the proof of theorem   we note that the equation    which defines value
iteration in i pomdps  can be rewritten in the following form  u n   hu n    here  h   b  b
is a backup operator  and is defined as 
hu n   i     max h i   ai   u n   
ai ai

where h   i  ai  b  r is 
h i   ai   u    

p
is

bi  is eri  is  ai     

p

oi

p r oi  ai   bi  u  hsei  bi   ai   oi    i i 

and where b is the set of all bounded value functions u   lemmas   and   establish important
properties of the backup operator  proof of lemma   is given below  and proof of lemma   follows
thereafter 
proof of lemma    select arbitrary value functions v and u such that v   i l    u  i l   i l 
i l   let i l be an arbitrary type of agent i 
  

fig mytrasiewicz   d oshi





p

p

hv  i l     max
oi p r oi  ai   bi  v  hsei l  bi   ai   oi    i i 
is bi  is eri  is  ai     
ai ai
p
p
  is bi  is eri  is  ai      oi p r oi  ai   bi  v  hsei l  bi   ai   oi    i i 
p
p



 is b
i  is eri  is  ai     
oi p r oi  ai   bi  u  hsei l  bi   ai   oi    i i 

p
p
 max
oi p r oi  ai   bi  u  hsei l  bi   ai   oi    i i 
is bi  is eri  is  ai     
ai ai

  hu  i l  

since i l is arbitrary  hv  hu  
proof of lemma    assume two arbitrary well defined value functions v and u such that v  u  
from lemma   it follows that hv  hu   let i l be an arbitrary type of agent i  also  let ai be
the action that optimizes hu  i l   
   hu  i l    hv  i l  



p

  max sumis bi  is eri  is  ai      oi p r oi  ai   bi  u  sei l  bi   ai   oi    hi i  
ai ai

p
p
max
is bi  is eri  is  ai     
oi p r oi  ai   bi  v  sei l  bi   ai   oi    hi i 
ai ai
p
p
 is bi  is eri  is  ai      oi p r oi  ai   bi  u  sei l  bi   ai   oi    hi i  
p
p



oi p r oi  ai   bi  v  sei l  bi   ai   oi    hi i 
is bi  is eri  is  ai    
p


   oi p r oi  ai   bi  u  sei l  bi   ai   oi    hi i 
p

 oi p r oi  ai   bi  v  se

 i l  bi   ai   oi    hi i 
p



   oi p r oi  ai   bi   u  sei l  bi   ai   oi    hi i   v  sei l  bi   ai   oi    hi i 
p
  oi p r oi  ai   bi    u  v   
    u  v   

as the supremum norm is symmetrical  a similar result can be derived for hv   i l    hu  i l   
since i l is arbitrary  the contraction property follows  i e    hv  hu       v  u    
lemmas   and   provide the stepping stones for proving theorem    proof of theorem   follows
from a straightforward application of the contraction mapping theorem  we state the contraction
mapping theorem  stokey   lucas        below 
theorem    contraction mapping theorem   if  s    is a complete metric space and t   s  s
is a contraction mapping with modulus   then
   t has exactly one fixed point u  in s  and
   the sequence  u n   converges to u   
proof of theorem   follows 
  

fia f ramework for s equential p lanning in m ulti  agent s ettings

proof of theorem    the normed space  b          is complete w r t the metric induced by the supremum norm  lemma   establishes the contraction property of the backup operator  h  using theorem    and substituting t with h  convergence of value iteration in i pomdps to a unique fixed
point is established 
we go on to the piecewise linearity and convexity  pwlc  property of the value function 
we follow the outlines of the analogous proof for pomdps in  hausktecht        smallwood  
sondik        
let    is  r be a real valued and bounded function  let the space of such real valued
bounded functions be b is   we will now define an inner product 
definition    inner product   define the inner product  h  i   b is    is   r  by
x
h  bi i  
bi  is  is 
is

the next lemma establishes the bilinearity of the inner product defined above 
lemma    bilinearity   for any s  t  r  f  g  b is   and b     is  the following equalities
hold 
hsf   tg  bi   shf  bi   thg  bi
hf  sb   ti   shf  bi   thf  i
we are now ready to give the proof of theorem    theorem   restates theorem   mathematically  and its proof follows thereafter 
theorem    pwlc   the value function  u n   in finitely nested i pomdp is piece wise linear and
convex  pwlc   mathematically 
u n  i l     max
n


x

bi  is n  is 

n            

is

proof of theorem    basis step  n    
from bellmans dynamic programming equation 
u    i     max
ai

x

bi  is er is  ai  

    

is

p
where eri  is  ai     aj r is  ai   aj  p r aj  mj    here  eri    represents the expectation of
r w r t  agent js actions  eq     represents an inner product and using lemma    the inner product
is linear in bi   by selecting the maximum of a set of linear vectors  hyperplanes   we obtain a pwlc
horizon   value function 
inductive hypothesis  suppose that u n   i l   is pwlc  formally we have 
u n   i l     max
n 

 

p

max

n   

is bi  is 

n 



p

n   is 

is mj imj bi

 is n   is 
  

 

p

is mj smj bi

 is n   is 



    

fig mytrasiewicz   d oshi

inductive proof  to show that u n  i l   is pwlc 

u n  i l     max
at 
i

 

x

t 
bt 
 eri  ist    at 
i  is
i   

x

t 
n 
p r oti  at 
 i l  
i   bi  u

oti

ist 

from the inductive hypothesis 
 
p
t 
t   er  ist    at   
u n  i l     max
i
ist  bi  is
i
at 
i

 

p

oti

t 
p r oti  at 
i   bi  

max

n  n 

p

t
t n   ist  
ist bi  is  

 

 

t  t
t  t  t
t
let l bt 
i   ai   oi   be the index of the alpha vector that maximizes the value at b i   se bi   ai   oi   
then 
 
p
t 
t   er  ist    at   
u n  i l     max
i
ist  bi  is
i
t 
ai
 
p
p
t 
t
t n 
  ot p r oti  at 
ist bi  is  l bt   at   ot  
i   bi  
i

i

i

i

from the second equation in the inductive hypothesis 
 
p
p
t 
t   er  ist    at      
n
t t  t 
u  i l     max
i
ot p r oi  ai   bi  
ist  bi  is
i
at 
i

i





p

t
t n 
ist  mtj imj bi  is  l bt   at   ot  
i
i
i

 

p

t
t n 
ist  mtj smj bi  is  l bt   at   ot  
i
i
i

substituting bti with the appropriate belief updates from eqs     and      we get 
 
p
p
t 
t t  t 
t   er  ist    at      
u n  i l     max
i
oti p r oi  ai   bi  
ist  bi  is
i
t 
ai
 


p
p
p
t 
t  t 
t 

 
p r aj  j   oi  st   at    oti  
ist  mtj imj
ist  bi  is
at 
j


p
t  t  t t
t 
t
t
t 
t
t
t 
t 
t
 ot oj  s   a   oj   jt  bj   aj   oj   bj  i bj   bj  ti  s   a   s  

 

j

n 
t
l b
t  t  t  is  
 ai  oi  
i


p
p
p
t 
t 
t 
t 
  ist  mt smj ist  bi  is  
p r aj  mj   oi  st   at    oti  
at 
j
j


p
t  t
t 
t
t
t 
t
t
t
t 
t 
t
 ot oj  s   a   oj   k  append hj   oj    hj  i m
bj  m
b j  ti  s   a   s  
j
  
n 
t
l b
t  t  t  is  
 a
 o  
i

i

i

  

fia f ramework for s equential p lanning in m ulti  agent s ettings

and further
u n  i l     max
at 
i




p

p

 

p

t 
t   er  ist    at   
i
ist  bi  is
i

t 
t   
ist  bi  is



t t t    ot  
j
otj oj  s   a

p



at 
j

 

p

oti

 

p

ist  mtj imj


t 
p r at 
 
 
oi  st   at    oti  
j
j

t  t t
t    at    st  
bt  bt
jt  bt 
j   aj   oj   bj  i j   j  ti  s



n 
t
l b
t  t  t  is  
 ai  oi  
i


p
p
p
t 
t 
t 
t 
p r aj  mj   oi  st   at    oti  
  ist  mt smj ist  bi  is  
at 
j
j


p
t 
t    ht  i m
t  t  st    at    st  
 ot ojt  st   at    otj   k  append ht 
 
o
b
 
m
b
j
j
j i
j
j
j
  
n 
t
l b
t  t  t  is  
 a
 o  
i

i

i

rearranging the terms of the equation 
u n  

 


p
p p
t 
t    er  ist    at      
 
 
max
b
 is
t 
t   m
i
i l
oti
ist  mtj imj
i
i
is
im
j
j
at 
i


p
p
t  t 

p r aj  j   oi  st   at    oti   ot ojt  st   at    otj  
at 
j
j



n 
t  t  t t
t 
t
t 
t 
t
t
 jt  bj   aj   oj   bj  i bj   bj  ti  s   a   s  
l bt   at   ot    is  
i
i
i

p
p
p
p
  ist   mt  smj bit   ist    eri  ist    at 
oti
ist  mtj smj
oti
i   
j


p
p
p r ajt   mt 
  oi  st   at    oti   ot ojt  st   at    otj  

j
at 
j
j

 

n 
t
t
t
l b
b t 
b tj  ti  st    at    st  
 k  append ht 
t  t  t  is  
j  m
j   oj    hj  i m
 ai  oi  
i

p
t 
t   n  ist   
  max
ai
imj bi  is
ist   mt 
j
at 
i

p
t 
t 
t 
n
  ist   mt  smj bi  is  ai  is  
j

therefore 
u n  

i l  

  max
n
n
   

 
 

p



p

t 
t   n  ist   
ist   mt 
imj bi  is
j



t 
t   n  ist   
smj bi  is
ist   mt 
j
p
t 
t   n  ist      maxhbt    n i
max
ist  bi  is
i
n

n

  

    

fig mytrasiewicz   d oshi

where  if mjt  in ist  is intentional then n   n  
n  ist   

eri  ist    at 
i  



p p

p

t 
p r at 
j  j  



oi  ist   at    oti  
   ot ist  mt imj
at 
j
j
 i

p
t  t  t t
t 
t
t
t 
t
t
t 
t 
t
 ot oj  isj   a   oj   jt  bj   aj   oj   bj  i bj   bj  ti  s   a   s  

 

j

n 
t
l b
t  t  t  is  
 o  
 a
i

i

i

and  if mjt  is subintentional then n   n  
n  ist   

eri  ist    at 
i  



p p

p

t 
p r at 
j  j  



oi  ist   at    oti  
   ot ist  mt smj
at 
i
j
j


p
t 
t  t
t 
t 
t
t
t
t 
t
t
t
bj  m
b j  ti  s   a   s  
 ot oj  isj   a   oj   k  append hj   oj    hj  i m

 

j

n 
t
l b
t  t  t  is  
 ai  oi  
i

eq     is an inner product and using lemma    the value function is linear in b t 
i   furthermore 
maximizing over a set of linear vectors  hyperplanes  produces a piecewise linear and convex value
function 

references
ambruster  w     boge  w          bayesian game theory  in moeschlin  o     pallaschke  d   eds    game
theory and related topics  north holland 
aumann  r  j          interactive epistemology i  knowledge  international journal of game theory  pp 
       
aumann  r  j     heifetz  a          incomplete information  in aumann  r     hart  s   eds    handbook
of game theory with economic applications  volume iii  chapter     elsevier 
battigalli  p     siniscalchi  m          hierarchies of conditional beliefs and interactive epistemology in
dynamic games  journal of economic theory  pp         
bernstein  d  s   givan  r   immerman  n     zilberstein  s          the complexity of decentralized control
of markov decision processes  mathematics of operations research                
binmore  k          essays on foundations of game theory  blackwell 
boutilier  c          sequential optimality and coordination in multiagent systems  in proceedings of the
sixteenth international joint conference on artificial intelligence  pp         
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research          
brandenburger  a          the power of paradox  some recent developments in interactive epistemology 
tech  rep   stern school of business  new york university  http   pages stern nyu edu  abranden  
brandenburger  a     dekel  e          hierarchies of beliefs and common knowledge  journal of economic
theory             
dennett  d          intentional systems  in dennett  d   ed    brainstorms  mit press 
fagin  r  r   geanakoplos  j   halpern  j  y     vardi  m  y          a hierarchical approach to modeling
knowledge and common knowledge  international journal of game theory  pp         
fagin  r  r   halpern  j  y   moses  y     vardi  m  y          reasoning about knowledge  mit press 
fudenberg  d     levine  d  k          the theory of learning in games  mit press 
  

fia f ramework for s equential p lanning in m ulti  agent s ettings

fudenberg  d     tirole  j          game theory  mit press 
gmytrasiewicz  p  j     durfee  e  h          rational coordination in multi agent environments  autonomous agents and multiagent systems journal               
harsanyi  j  c          games with incomplete information played by bayesian players  management
science                
hauskrecht  m          value function approximations for partially observable markov decision processes 
journal of artificial intelligence research  pp       
hausktecht  m          planning and control in stochastic domains with imperfect information  ph d  thesis 
mit 
hu  j     wellman  m  p          multiagent reinforcement learning  theoretical framework and an algorithm  in fifteenth international conference on machine learning  pp         
kadane  j  b     larkey  p  d          subjective probability and the theory of games  management science 
              
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in partially observable
stochastic domains  artificial intelligence                
kalai  e     lehrer  e          rational learning leads to nash equilibrium  econometrica  pp           
koller  d     milch  b          multi agent influence diagrams for representing and solving games  in seventeenth international joint conference on artificial intelligence  pp            seattle  washington 
li  m     vitanyi  p          an introduction to kolmogorov complexity and its applications  springer 
littman  m  l          markov games as a framework for multi agent reinforcement learning  in proceedings
of the international conference on machine learning 
lovejoy  w  s          a survey of algorithmic methods for partially observed markov decision processes 
annals of operations research                
madani  o   hanks  s     condon  a          on the undecidability of probabilistic planning and related
stochastic optimization problems  artificial intelligence           
mertens  j  f     zamir  s          formulation of bayesian analysis for games with incomplete information 
international journal of game theory          
monahan  g  e          a survey of partially observable markov decision processes  theory  models  and
algorithms  management science      
myerson  r  b          game theory  analysis of conflict  harvard university press 
nachbar  j  h     zame  w  r          non computable strategies and discounted repeated games  economic
theory            
nair  r   pynadath  d   yokoo  m   tambe  m     marsella  s          taming decentralized pomdps  towards
efficient policy computation for multiagent settings  in proceedings of the eighteenth international
joint conference on artificial intelligence  ijcai     
ooi  j  m     wornell  g  w          decentralized control of a multiple access broadcast channel  in
proceedings of the   th conference on decision and control 
papadimitriou  c  h     tsitsiklis  j  n          the complexity of markov decision processes  mathematics
of operations research                
russell  s     norvig  p          artificial intelligence  a modern approach  second edition   prentice hall 
smallwood  r  d     sondik  e  j          the optimal control of partially observable markov decision
processes over a finite horizon  operations research  pp           
stokey  n  l     lucas  r  e          recursive methods in economic dynamics  harvard univ  press 

  

fi