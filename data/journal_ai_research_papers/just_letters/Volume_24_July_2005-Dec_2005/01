journal artificial intelligence research                

submitted        published      

framework sequential planning multi agent settings
piotr j  gmytrasiewicz
prashant doshi

piotr   cs   uic   edu
pdoshi   cs   uic   edu

department computer science
university illinois chicago
    s  morgan st
chicago  il      

abstract
paper extends framework partially observable markov decision processes  pomdps 
multi agent settings incorporating notion agent models state space  agents
maintain beliefs physical states environment models agents 
use bayesian updates maintain beliefs time  solutions map belief states actions 
models agents may include belief states related agent types considered
games incomplete information  express agents autonomy postulating models directly manipulable observable agents  show important properties
pomdps  convergence value iteration  rate convergence  piece wise linearity convexity value functions carry framework  approach complements
traditional approach interactive settings uses nash equilibria solution paradigm 
seek avoid drawbacks equilibria may non unique capture
off equilibrium behaviors  cost represent  process continuously
revise models agents  since agents beliefs may arbitrarily nested  optimal solutions decision making problems asymptotically computable  however  approximate
belief updates approximately optimal plans computable  illustrate framework using
simple application domain  show examples belief updates value functions 

   introduction
develop framework sequential rationality autonomous agents interacting
agents within common  possibly uncertain  environment  use normative paradigm
decision theoretic planning uncertainty formalized partially observable markov decision
processes  pomdps   boutilier  dean    hanks        kaelbling  littman    cassandra       
russell   norvig        point departure  solutions pomdps mappings
agents beliefs actions  drawback pomdps comes environments populated
agents agents actions represented implicitly environmental noise
within the  usually static  transition model  thus  agents beliefs another agent part
solutions pomdps 
main idea behind formalism  called interactive pomdps  i pomdps   allow
agents use sophisticated constructs model predict behavior agents  thus 
replace flat beliefs state space used pomdps beliefs physical
environment agent s   possibly terms preferences  capabilities 
beliefs  beliefs could include others beliefs others  thus nested arbitrary
levels  called interactive beliefs  space interactive beliefs rich
updating beliefs complex updating flat counterparts  use value
c
    
ai access foundation  rights reserved 

fig mytrasiewicz   oshi

function plots show solutions i pomdps least good as  usual cases superior
to  comparable solutions pomdps  reason intuitive maintaining sophisticated models
agents allows refined analysis behavior better predictions actions 
i pomdps applicable autonomous self interested agents locally compute actions execute optimize preferences given believe interacting
others possibly conflicting objectives  approach using decision theoretic framework solution concept complements equilibrium approach analyzing interactions used
classical game theory  fudenberg   tirole         drawback equilibria could
many  non uniqueness   describe agents optimal actions if  when 
equilibrium reached  incompleteness   approach  instead  centered optimality
best response anticipated action agent s   rather stability  binmore       
kadane   larkey         question whether  circumstances  kind
equilibria could arise solutions i pomdps currently open 
approach avoids difficulties non uniqueness incompleteness traditional equilibrium approach  offers solutions likely better solutions traditional
pomdps applied multi agent settings  advantages come cost processing
maintaining possibly infinitely nested interactive beliefs  consequently  approximate belief
updates approximately optimal solutions planning problems computable general 
define class finitely nested i pomdps form basis computable approximations infinitely nested ones  show number properties facilitate solutions pomdps carry
finitely nested i pomdps  particular  interactive beliefs sufficient statistics
histories agents observations  belief update generalization update pomdps 
value function piece wise linear convex  value iteration algorithm converges
rate 
remainder paper structured follows  start brief review related
work section    followed overview partially observable markov decision processes
section    there  include simple example tiger game  introduce concept
agent types section    section   introduces interactive pomdps defines solutions 
finitely nested i pomdps  properties introduced section    continue
example application finitely nested i pomdps multi agent version tiger game
section    there  show examples belief updates value functions  conclude
brief summary current research issues section    details proofs
appendix 

   related work
work draws prior research partially observable markov decision processes 
recently gained lot attention within ai community  smallwood   sondik        monahan 
      lovejoy        hausktecht        kaelbling et al         boutilier et al         hauskrecht 
      
formalism markov decision processes extended multiple agents giving rise
stochastic games markov games  fudenberg   tirole         traditionally  solution concept
used stochastic games nash equilibria  recent work ai follows tradition
 littman        hu   wellman        boutilier        koller   milch         however 
mentioned before  pointed game theorists  binmore        kadane  
  

fia f ramework equential p lanning ulti  agent ettings

larkey         nash equilibria useful describing multi agent system when  if 
reached stable state  solution concept sufficient general control paradigm 
main reasons may multiple equilibria clear way choose among
 non uniqueness   fact equilibria specify actions cases agents believe
agents may act according equilibrium strategies  incompleteness  
extensions pomdps multiple agents appeared ai literature recently  bernstein 
givan  immerman    zilberstein        nair  pynadath  yokoo  tambe    marsella        
called decentralized pomdps  dec pomdps   related decentralized control
problems  ooi   wornell         dec pomdp framework assumes agents fully cooperative  i e   common reward function form team  furthermore  assumed
optimal joint solution computed centrally distributed among agents execution 
game theoretic side  motivated subjective approach probability
games  kadane   larkey         bayesian games incomplete information  see fudenberg  
tirole        harsanyi        references therein   work interactive belief systems  harsanyi 
      mertens   zamir        brandenburger   dekel        fagin  halpern  moses    vardi 
      aumann        fagin  geanakoplos  halpern    vardi         insights research
learning game theory  fudenberg   levine         approach  closely related decisiontheoretic  myerson         epistemic  ambruster   boge        battigalli   siniscalchi       
brandenburger        approach game theory  consists predicting actions agents given
available information  choosing agents action  kadane   larkey        
thus  descriptive aspect decision theory used predict others actions  prescriptive
aspect used select agents optimal action 
work presented extends previous work recursive modeling method  rmm 
 gmytrasiewicz   durfee         adds elements belief update sequential planning 

   background  partially observable markov decision processes
partially observable markov decision process  pomdp   monahan        hausktecht       
kaelbling et al         boutilier et al         hauskrecht        agent defined
pomdpi   hs  ai   ti     oi   ri

   

where  set possible states environment  ai set actions agent execute  ti
transition function ti   ai        describes results agent actions 
set observations agent make  oi agents observation function oi   ai
       specifies probabilities observations given agents actions resulting states  finally 
ri reward function representing agent preferences r   ai   
pomdps  agents belief state represented probability distribution s 
initially  observations actions take place  agent  prior  belief  b  i  
time steps  t  assume agent     observations performed actions    

assembled agent observation history  h ti    o i   o i       ot 
  oi   time t  let
hi denote set observation histories agent i  agents current belief  b ti s 
continuously revised based new observations expected results performed actions  turns
   assume action taken every time step  without loss generality since actions maybe
no op 

  

fig mytrasiewicz   oshi

agents belief state sufficient summarize past observation history
initial belief  hence called sufficient statistic  
t 
belief update takes account changes initial belief  b t 
  due action  ai   executed


time    new observation  oi   new belief  bi   current state st   is 
bti  st     oi  oti   st   at 
 

x

bit   st   ti  st   ati   st   

   

st 

normalizing constant 
convenient summarize update performed states

bti   se bit    at 
  oi    kaelbling et al         
    optimality criteria solutions
agents optimality criterion  oci   needed specify rewards acquired time
handled  commonly used criteria include 
finite horizon criterion p
agent maximizes expected value sum
following rewards  e  tt   rt    here  rt reward obtained time length
horizon  denote criterion fht  
anp
infinite horizon criterion discounting  according agent maximizes


e 
t   rt            discount factor  denote criterion ih  

infinite horizon criterion averaging  according agent maximizes
average reward per time step  denote ihav  

follows  concentrate infinite horizon criterion discounting  approach easily adapted criteria 
utility associated belief state  bi composed best immediate rewards
obtained bi   together discounted expected sum utilities associated
belief states following bi  

u  bi     max

ai ai

x

bi  s ri  s  ai    

x

p r oi  ai   bi  u  sei  bi   ai   oi   

oi

ss



   

value iteration uses equation   iteratively obtain values belief states longer time
horizons  step value iteration error current value estimate reduced
factor least  see example russell   norvig        section        optimal action   
element set optimal actions  op  bi    belief state  defined as 

op  bi     argmax
ai ai

x

bi  s ri  s  ai    

x

oi

ss

   see  smallwood   sondik        proof 

  

p r oi  ai   bi  u  se bi   ai   oi   



   

fia f ramework equential p lanning ulti  agent ettings

l


ol

  

value function u 

 

 

 

 

 

 

   

   

   

   

 

pp i tl 
 tl 
pomdp noise

pomdp

figure    value function single agent tiger game time horizon length    oc   fh   
actions are  open right door   or  open left door   ol  listen   l  value
time horizon value function pomdp noise factor identical single
agent pomdp 

    example  tiger game
briefly review pomdp solutions tiger game  kaelbling et al          purpose
build insights pomdp solutions provide simple case illustrate solutions
interactive versions game later 
traditional tiger game resembles game show situation decision maker
choose open one two doors behind lies either valuable prize dangerous tiger 
apart actions open doors  subject option listening tigers growl
coming left  right  door  however  subjects hearing imperfect  given
percentages  say       false positive false negative occurrences  following  kaelbling et al  
       assume value prize     pain associated encountering
tiger quantified       cost listening    
value function  figure    shows values various belief states agents time
horizon equal    values beliefs based best action available belief state 
specified eq     state certainty valuable agent knows location
tiger open opposite door claim prize certainly awaits  thus 
probability tiger location      value     agent sufficiently uncertain 
best option play safe listen  value     agent indifferent opening
doors listening assigns probabilities         location tiger 
note that  time horizon equal    listening provide useful information
since game continue allow use information  longer time horizons
benefits results listening results policies better ranges initial belief 
since value function composed values corresponding actions  linear prob  

fig mytrasiewicz   oshi

l    l  gl  ol  gr 

l    l    

l    or  gl  l  gr 
l    or    
or    l    

l    ol    
ol    l    

 

value function u 

 

 

 

 

  
 

   

   

   

   

 

pp i tl 
 tl 
pomdp noise

pomdp

figure    value function single agent tiger game compared agent facing noise factor  horizon length    policies corresponding value lines conditional plans 
actions  l  ol  conditioned observational sequences parenthesis 
example l    l  gl  ol  gr  denotes plan perform listening action  l 
beginning  list observations empty   another l observation growl
left  gl   open left door  ol  observation gr  wildcard
usual interpretation 

ability tiger location  value function property piece wise linear convex
 pwlc  horizons  simplifies computations substantially 
figure   present comparison value functions horizon length   single
agent  agent facing noisy environment  presence noise could
due another agent opening doors listening probabilities    since pomdps
include explicit models agents  noise actions included transition
model   
consequences folding noise two fold  first  effectiveness agents optimal
policies declines since value hearing growls diminishes many time steps  figure   depicts
comparison value functions horizon length    here  example  two consecutive growls
noisy environment valuable agent knows acting alone since noise
may perturbed state system growls  time horizon length  
noise matter value vectors overlap  figure   
second  since presence another agent implicit static transition model  agent
cannot update model agents actions repeated interactions  effect becomes important time horizon increases  approach addresses issue allowing
explicit modeling agent s   results policies superior quality  show
section    figure   shows policy agent facing noisy environment time horizon   
compare corresponding i pomdp policy section    note slightly different
   assumed that  due noise  either door opens probabilities     turn  nothing happens
probability      explain origin assumption section   

  

fia f ramework equential p lanning ulti  agent ettings

l    l     ol  gr gr  l    
l    l  gl  ol  gr  ol  gl gr  l    

l    l     or  gl gl  l    
l    or  gl  l  gr  or  gr gl  l    
l    l     or  gl gl  ol  gr gr  l    
or    l     l    
l    l     or    

ol    l     l    
l    l     ol    
 

value function u 

 
 
 
 
 
 
 
 

   

   

   

   

 

p  tl 
p i tl 


pomdp noise

pomdp

figure    value function single agent tiger game compared agent facing noise factor 
horizon length      description policy stands
perceptual sequences yet listed description policy 

        
ol
 

                                      
l

l

gr

gl

gr

l
gr

gl

 

ol

l
gr

l
gl

gr


gl

 

gl

l
gl

                     

l

gr

ol

            

l
 

l

gr


gl

 



figure    policy graph corresponding value function pomdp noise depicted
fig    

  

fig mytrasiewicz   oshi

policy without noise example kaelbling  littman cassandra        due
differences value functions 

   agent types frames
pomdp definition includes parameters permit us compute agents optimal behavior   
conditioned beliefs  let us collect implementation independent factors construct
call agent type 
definition    type   type agent is    hbi   ai     ti   oi   ri   oci i  bi agent
state belief  an element  s    oci optimality criterion  rest elements
defined before  let set agent types 
given type    assumption agent bayesian rational  set agents optimal
actions denoted op  i    next section  generalize notion type situations include interactions agents  coincides notion type used
bayesian games  fudenberg   tirole        harsanyi        
convenient define notion frame  bi   agent i 

b set
definition    frame   frame agent is  bi   hai     ti   oi   ri   oci i  let
agent frames 

brevity one write type consisting agents belief together frame   
hbi   bi i 
context tiger game described previous section  agent type describes
agents actions results  quality agents hearing  payoffs  belief
tiger location 
realistically  apart implementation independent factors grouped type  agents behavior may depend implementation specific parameters  processor speed  memory
available  etc  included  implementation dependent  complete  type  increasing accuracy predicted behavior  cost additional complexity  definition use
complete types topic ongoing work 

   interactive pomdps
mentioned  intention generalize pomdps handle presence agents 
including descriptions agents  their types example  state space 
simplicity presentation  consider agent i  interacting one agent  j 
formalism easily generalizes larger number agents 
definition    i pomdp   interactive pomdp agent i  i pomdpi   is 
i pomdpi   hisi   a  ti     oi   ri

   

   issue computability solutions pomdps subject much research  papadimitriou   tsitsiklis 
      madani  hanks    condon         obvious importance one uses pomdps model agents 
return issue later 

  

fia f ramework equential p lanning ulti  agent ettings

where 
isi set interactive states defined isi   mj    interacting agent i 
set states physical environment  mj set possible models agent
j  model  mj mj   defined triple mj   hhj   fj   oj i  fj   hj  aj  
agent js function  assumed computable  maps possible histories js observations
distributions actions  hj element hj   oj function specifying way
environment supplying agent input  sometimes write model j mj   hhj  
b j i 

b j consists fj oj   convenient subdivide set models two classes 
subintentional models  smj   relatively simple  intentional models  imj   use
notion rationality model agent  thus  mj   imj smj  
simple examples subintentional models include no information model fictitious play
model  history independent  no information model  gmytrasiewicz   durfee 
      assumes agents actions executed equal probability  fictitious
play  fudenberg   levine        assumes agent chooses actions according fixed
unknown distribution  original agents prior belief distribution takes form
dirichlet distribution   example powerful subintentional model finite state
controller 
intentional models sophisticated ascribe agent beliefs 
preferences rationality action selection   intentional models thus js types  j   hbj   bj i 
assumption agent j bayesian rational   agent js belief probability distribution
states environment models agent i  b j  s mi    notion type
use coincides notion type game theory  defined consisting
agent private information relevant decision making  harsanyi        fudenberg
  tirole         particular  agents beliefs private information  types involve
possibly infinitely nested beliefs others types beliefs others  mertens   zamir 
      brandenburger   dekel        aumann        aumann   heifetz           related
recursive model structures prior work  gmytrasiewicz   durfee         definition
interactive state space consistent notion completely specified state space put forward
aumann         similar state spaces proposed others  mertens   zamir       
brandenburger   dekel        
  ai aj set joint moves agents 
ti transition model  usual way define transition probabilities pomdps
assume agents actions change aspect state description  case ipomdps  would mean actions modifying aspect interactive states  including
agents observation histories functions  or  modeled intentionally  beliefs
reward functions  allowing agents directly manipulate agents ways  however 
violates notion agents autonomy  thus  make following simplifying assumption 
 
   agents  say n      isi   n
j   mj
   technically  according notation  fictitious play actually ensemble models 
   dennet        advocates ascribing rationality agent s   calls assuming intentional stance towards
them 
   note space types far richer computable models  particular  since set computable
models countable set types uncountable  many types computable models 
   implicit definition interactive beliefs assumption coherency  brandenburger   dekel        

  

fig mytrasiewicz   oshi

model non manipulability assumption  mnm   agents actions change
agents models directly 
given simplification  transition model defined         
autonomy  formalized mnm assumption  precludes  example  direct mind control 
implies agents belief states changed indirectly  typically changing
environment way observable them  words  agents beliefs change  pomdps 
result belief update observation  direct result agents
actions   
defined pomdp model 
oi observation function  defining function make following assumption 
model non observability  mno   agents cannot observe others models directly 
given assumption observation function defined          
mno assumption formalizes another aspect autonomy agents autonomous
observations functions  beliefs properties  say preferences  intentional
models  private agents cannot observe directly    
ri defined ri   isi    allow agent preferences physical
states models agents  usually physical state matter 
mentioned  see interactive pomdps subjective counterpart objective external view stochastic games  fudenberg   tirole         followed work
ai  boutilier         koller   milch        decentralized pomdps  bernstein et al  
      nair et al          interactive pomdps represent individual agents point view
environment agents  facilitate planning problem solving agents
individual level 
    belief update i pomdps
show that  pomdps  agents beliefs interactive states sufficient
statistics  i e   fully summarize agents observation histories  further  need show
beliefs updated agents action observation  solutions defined 
t 
new belief state  bti   function previous belief state  bt 
  last action  ai  

new observation  oi   pomdps  two differences complicate belief
update compared pomdps  first  since state physical environment depends
actions performed agents prediction physical state changes
made based probabilities various actions agent  probabilities others
actions obtained based models  thus  unlike bayesian stochastic games 
assume actions fully observable agents  rather  agents attempt infer
actions agents performed sensing results environment  second  changes
models agents included update  reflect others observations
and  modeled intentionally  update agents beliefs  case  agent
update beliefs agent based anticipates agent observes
    possibility agents influence observational capabilities agents accommodated
including factors change sensing capabilities set s 
    again  possibility agents observe factors may influence observational capabilities agents
allowed including factors s 

  

fia f ramework equential p lanning ulti  agent ettings

updates  could expected  update possibly infinitely nested belief
others types is  general  asymptotically computable 
proposition     sufficiency  interactive pomdp agent i  current belief  i e   probability distribution set mj   sufficient statistic past history observations 
t 
next proposition defines agent belief update function  b ti  ist     p r ist  oti   at 
  bi   
ist isi interactive state  use belief state estimation function  se   abt 
breviation belief updates individual states bti   sei  bt 
  ai   oi   
t  t 
t 
t 
 bi   ai   oi   bi   stand p r bti  bi   ai   oti    define set
type dependent optimal actions agent  op  i   

proposition     belief update  mnm mno assumptions  belief update function
interactive pomdp hisi   a  ti     oi   ri i  mj ist intentional  is 
bti  ist    
ti

p

t   
bt 
 is

ist   m
b t 
 bjt
j
p
t 
t 
 s     st  
otj

p

t 
t    ot  
p r at 

j  j  oi  s  

at 
j
t  t 
t    ot  

 b
j j   aj   oj   bj  oj  s  
j

   

 m
b tj  
mj ist subintentional first summation extends ist   
b t 
j
t 
t 
t 
t  t 
p r at 
j  j   replaced p r aj  mj    jt  bj   aj   oj   bj   replaced


kronecker delta function k  append ht 
j   oj    hj   

above  bt 
btj belief elements jt  jt   respectively  normalizing constant 
j
t 
t 
p r at 
bayesian rational agent described type
j  j   probability aj
t 
t 
 
j   probability equal  op  j    aj op  j    equal zero otherwise 
define op section        case js subintentional model     s  mj    ht 

j
respectively  observation
htj observation histories part mt 
 


j
j
j
t 
t 
t 
function mtj   p r at 
 m
 


probability
assigned




j
j
j
j   append returns
string second argument appended first  proofs propositions
appendix 
proposition   eq    lot common belief update pomdps 
expected  depend agent observation transition functions  however  since agent
observations depend agent js actions  probabilities various actions j
included  in first line eq      further  since update agent js model depends
j observes  probabilities various observations j included  in second line
eq      update js beliefs represented j term  belief update easily
generalized setting one agents co exist agent i 
p
    agents prior belief isi given probability density function
ist  replaced
 
  otj   btj   takes form dirac delta function argument bt 
  at 
integral  case jt  bt 
j
j
j
  otj   btj   
  at 
 sejt  bt 
j
j

  

fig mytrasiewicz   oshi

    value function solutions i pomdps
analogously pomdps  belief state i pomdp associated value reflecting maximum payoff agent expect belief state 


p
p
b
eri  is  ai  bi  is   
u  i     max
p r oi  ai   bi  u  hsei  bi   ai   oi    i 
   
ai ai

oi



p
where  eri  is  ai    
aj ri  is  ai   aj  p r aj  mj    eq    basis value iteration ipomdps 
agent optimal action  ai   case infinite horizon criterion discounting 
element set optimal actions belief state  op  i    defined as 


p
p
op  i     argmax
eri  is  ai  bi  is   
p r oi  ai   bi  u  hsei  bi   ai   oi    bi i 
ai ai

oi



   
case belief update  due possibly infinitely nested beliefs  step value iteration
optimal actions asymptotically computable 

   finitely nested i pomdps
possible infinite nesting agents beliefs intentional models presents obvious obstacle
computing belief updates optimal solutions  since models agents infinitely
nested beliefs correspond agent functions computable natural consider
finite nestings  follow approaches game theory  aumann        brandenburger   dekel 
      fagin et al          extend previous work  gmytrasiewicz   durfee         construct
finitely nested i pomdps bottom up  assume set physical states world s  two
agents j  agent   th level beliefs  bi     probability distributions s    th level
types  i     contain   th level beliefs  frames  analogously agent j    level types
are  therefore  pomdps      level models include   level types  i e   intentional models 
subintentional models  elements sm   agents first level beliefs probability distributions
physical states   level models agent  agents first level types consist
first level beliefs frames  first level models consist types upto level  
subintentional models  second level beliefs defined terms first level models on 
formally  define spaces 
isi     s 
j      hbj     bj   bj    isj       mj     j   smj
isi     mj    
j      hbj     bj   bj    isj       mj     j   mj  
 
 
 
 
 
 
isi l   mj l    j l    hbj l   bj   bj l  isj l     mj l   j l mj l 
definition     finitely nested i pomdp  finitely nested i pomdp agent i  i pomdp i l   is 
i pomdpi l   hisi l   a  ti     oi   ri
      level types agents actions folded   r functions noise 

  

   

fia f ramework equential p lanning ulti  agent ettings

parameter l called strategy level finitely nested i pomdp  belief update 
value function  optimal actions finitely nested i pomdps computed using equation  
equation    recursion guaranteed terminate   th level subintentional models 
agents strategic capable modeling others deeper levels  i e   levels
strategy level l   always boundedly optimal  such  agents
could fail predict strategy sophisticated opponent  fact computability
agent function implies agent may suboptimal interactions pointed
binmore         proved recently nachbar zame         intuitively 
difficulty agents unbounded optimality would include capability model
agents modeling original agent  leads impossibility result due self reference 
similar godels incompleteness theorem halting problem  brandenburger 
       positive note  convergence results  kalai   lehrer        strongly suggest
approximate optimality achievable  although applicability work remains open 
mentioned    th level types pomdps  provide probability distributions
actions agent modeled level models strategy level    given probability
distributions agents actions level   models solved pomdps 
provide probability distributions yet higher level models  assume number models
considered level bound number    solving i pomdp i l equivalent
solving o m l   pomdps  hence  complexity solving i pomdpi l pspace hard
finite time horizons    undecidable infinite horizons  pomdps 
    properties i pomdps
section establish two important properties  namely convergence value iteration
piece wise linearity convexity value function  finitely nested i pomdps 
      c onvergence



value teration

agent i pomdpi l   show sequence value functions   u n   
n horizon  obtained value iteration defined eq     converges unique fixed point  u  
let us define backup operator h   b b u n   hu n    b set
bounded value functions  order prove convergence result  first establish
properties h 
lemma    isotonicity   finitely nested i pomdp value functions v u   v u  
hv hu  
proof lemma analogous one due hauskrecht         pomdps 
sketched appendix  another important property exhibited backup operator
property contraction 
lemma    contraction   finitely nested i pomdp value functions v   u discount
factor           hv hu      v u    
proof lemma similar corresponding one pomdps  hausktecht 
       proof makes use lemma          supremum norm 
    usually pspace complete since number states i pomdps likely larger time horizon
 papadimitriou   tsitsiklis        

  

fig mytrasiewicz   oshi

contraction property h  noting space value functions along
supremum norm forms complete normed space  banach space   apply contraction
mapping theorem  stokey   lucas        show value iteration i pomdps converges
unique fixed point  optimal solution   following theorem captures result 
theorem    convergence   finitely nested i pomdp  value iteration algorithm starting arbitrary well defined value function converges unique fixed point 
detailed proof theorem included appendix 
case pomdps  russell   norvig         error iterative estimates  u n  
finitely nested i pomdps  i e     u n u     reduced factor least iteration 
hence  number iterations  n   needed reach error is 
n   dlog rmax         log    e

    

rmax upper bound reward function 
      p iecewise l inearity



c onvexity

another property carries pomdps finitely nested i pomdps piecewise
linearity convexity  pwlc  value function  establishing property allows us decompose i pomdp value function set alpha vectors  represents policy
tree  pwlc property enables us work sets alpha vectors rather perform value
iteration continuum agents beliefs  theorem   states pwlc property
i pomdp value function 
theorem    pwlc   finitely nested i pomdp  u piecewise linear convex 
complete proof theorem   included appendix  proof similar one
due smallwood sondik        pomdps proceeds induction  basis case
established considering horizon   value function  showing pwlc inductive step
requires substituting belief update  eq     eq     followed factoring belief
terms equation 

   example  multi agent tiger game
illustrate optimal sequential behavior agents multi agent settings apply i pomdp
framework multi agent tiger game  traditional version described before 
    definition
let us denote actions opening doors listening or  ol l  before  tl
tr denote states corresponding tiger located behind left right door  respectively 
transition  reward observation functions depend actions agents  again 
assume tiger location chosen randomly next time step agents opened
doors current step  assume agent hears tigers growls  gr gl 
accuracy      make interaction interesting added observation
door creaks  depend action executed agent  creak right  cr  likely due
  

fia f ramework equential p lanning ulti  agent ettings

agent opened right door  similarly creak left  cl  silence  s  good
indication agent open doors listened instead  assume accuracy
creaks      assume agents payoffs analogous single agent versions
described section     make cases comparable  note result assumption
agents actions impact original agents payoffs directly  rather indirectly
resulting states matter original agent  table   quantifies factors 

hai   aj
hol 
hor 
h  oli
h  ori
hl  li
hl  li

state
 
 
 
 
tl
tr

tl
   
   
   
   
   
 

tr
   
   
   
   
 
   

hai   aj
hor  ori
hol  oli
hor  oli
hol  ori
hl  li
hl  ori
hor  li
hl  oli
hol  li

transition function  ti   tj

tl
  
    
  
    
  
  
  
  
    

tr
    
  
    
  
  
  
    
  
  

hai   aj
hor  ori
hol  oli
hor  oli
hol  ori
hl  li
hl  ori
hor  li
hl  oli
hol  li

tl
  
    
    
  
  
  
  
    
  

tr
    
  
  
    
  
    
  
  
  

reward functions agents j

hai   aj
hl  li
hl  li
hl  oli
hl  oli
hl  ori
hl  ori
hol 
hor 

state
tl
tr
tl
tr
tl
tr



h gl  cl
         
         
        
        
         
         
   
   

h gl  cr
         
         
         
         
        
        
   
   

h gl 
        
        
         
         
         
         
   
   

h gr  cl
         
         
        
        
         
         
   
   

h gr  cr
         
         
         
         
        
        
   
   

h gr 
        
        
         
         
         
         
   
   

hai   aj
hl  li
hl  li
hol  li
hol  li
hor  li
hor  li
h  oli
h  ori

state
tl
tr
tl
tr
tl
tr



h gl  cl
         
         
        
        
         
         
   
   

h gl  cr
         
         
         
         
        
        
   
   

h gl 
        
        
         
         
         
         
   
   

h gr  cl
         
         
        
        
         
         
   
   

h gr  cr
         
         
         
         
        
        
   
   

h gr 
        
        
         
         
         
         
   
   

observation functions agents j 
table    transition  reward  observation functions multi agent tiger game 
agent makes choice multi agent tiger game  considers believes
location tiger  well whether agent listen open door 
turn depends agents beliefs  reward function  optimality criterion  etc     particular 
agent open doors tiger location next time step would
chosen randomly  thus  information obtained hearing previous growls would
discarded  simplify situation considering i pomdp single level nesting 
assuming agent js properties  except beliefs  known i  js time
horizon equal is  words  uncertainty pertains js beliefs
frame  agent interactive state space is  isi     j     physical state  s  tl 
    assume intentional model agent here 

  

fig mytrasiewicz   oshi

tr   j   set intentional models agent js  differs js beliefs
location tiger 
    examples belief update
section    presented belief update equation i pomdps  eq      consider
examples beliefs  bi     agent i  probability distributions j       th
level type agent j  j   j     contains flat belief location tiger 
represented single probability assignment bj     pj  t l  
     
     

     

pr tl p
pr tl b j   
j

pr tl p
pr tl b j 

j

 

     
     
   
     

     
   
     

     
     

     
 

   

   

   

   

 

     
 

   

   

   

   

 

   

 

pb j
j  tl 

     

     

     

     

j

 

     

pr tr p
pr tr b j 

pr tr p
pr tr b j 

j

 

pjb j
 tl 
j tl 

   
     

     
   
     

     
     

     

 

   

           
ppb j
 tl 
 tr 
 tl 

 
     

jj

 

   

   

   

p j  tl 
b j

 i 

 ii 

figure    two examples singly nested belief states agent i  case information
tigers location   i  agent knows j know location
tiger  single point  star  denotes dirac delta function integrates height
point         ii  agent uninformed js beliefs tigers location 

fig    show examples level   beliefs agent i  case know
location tiger marginals top bottom sections figure sum
    probabilities tl tr each  fig    i   knows j assigns     probability tiger
behind left door  represented using dirac delta function  fig    ii   agent
uninformed js beliefs  represented uniform probability density values
probability j could assign state tl 
make presentation belief update transparent decompose formula
eq    two steps 
  

fia f ramework equential p lanning ulti  agent ettings

t 
prediction  agent performs action at 
  given agent j performs aj  
predicted belief state is 

bbt  ist     p r ist  at    at    bt      p t  bt  bt bt   ist   p r at   t   

j
j

j


 j  j
p
 st    at    st   oj  st   at    otj  

    

otj

t 
jt  bt 
j   j   j   bj  

correction  agent perceives observation  ti   predicted belief states 
t  t 
p r  at 
  aj   bi    combined according to 
bti  ist     p r ist  oti   ait    bt 
  

x

t  t 
oi  st   at    oti  p r ist  at 
  j   bi  

    

at 
j

normalizing constant 

t 



     
     
 

   

           
pb j tl 

 

j

     
     
           
pb j tl 

 

   

 

   
   
   
   
   
l  gl s 
   
   
l  gl s   
 
   
 

l  gl s 

 

   

   

   

pjb j tl 

 gl s 

 gl s 
   

   

   

   

l  gl s 

   

l  gl s 

    

   

 

           
pb j tl 

 

    

    

     

    

l  gl s 
   

   

pjb j
 tl 

   

pjb j
 tl 

 b 

   

pjb j tl 

     

    

 

 

   

     

    

    
 

   

l  gl s 

    

    
   
    
   
    
   
    
   
    

j

 a 

   

pjb j
 tl 

   
   
   
   
   
   
   
   

j 

j 
   

pr tr pj  

     

pr tr b j 

j 

pr tr p
pr tr b j 

l  l gl 

   

   

   

l  l gr 

     

 

 gl s 

 

     
     

l  gl s 

    
   
    
   
    
   
    
   
    

pr tl p
pr tl b j 

l  l gr 

   
     

bi

pr tr b j 
pr tr p
 
j

     

t  

bi

 gl s 

pr tl p
pr tl b j 

     

pr tl p
pr tl b j   
j

pr tl p
pr tl b j 

j 

     



bi

l  l gl 

pr tr p
pr tr b j   
j

bi

 c 

   

 

 
 

   

j

 d 

figure    trace belief update agent i   a  depicts prior   b  result prediction
given listening action  l  pair denoting js action observation  knows
j listen could hear tigers growl right left  probabilities
j would assign tl            respectively   c  result correction
observes tigers growl left creaks  hgl si  probability assigns
tl greater tr   d  depicts results another update  both prediction
correction  another listen action observation  hgl si 
discrete point denotes  again  dirac delta function integrates height
point 
fig     display example trace update singly nested belief  first
column fig     labeled  a   example agent prior belief introduced before  according
  

fig mytrasiewicz   oshi

knows j uninformed location tiger     let us assume listens
hears growl left creaks  second column fig      b   displays predicted
belief performs listen action  eq       part prediction step  agent must solve
js model obtain js optimal action belief      term p r a t 
j  j   eq       given
value function fig     evaluates probability   listen action  zero opening
doors  updates js belief given j listens hears tiger growling either
t 
left  gl  right  gr   term jt  bt 
j   aj   oj   bj   eq       agent js updated probabilities
tiger left            js hearing gl gr  respectively  tiger
left  top fig     b   js observation gl likely  consequently js assigning
probability      state tl likely  i assigns probability       state  
tiger right j likely hear gr assigns lower probability        
js assigning probability      tiger left  third column   c   fig    shows
posterior belief correction step  belief column  b  updated account
hearing growl left creaks  hgl si  resulting marginalised probability
tiger left higher        tiger right  assume
next time step listens hears tiger growling left creaks  belief
state depicted fourth column fig    results 
fig    show belief update starting prior fig     ii   according
agent initially information j believes tigers location 
traces belief updates fig    fig    illustrate changing state information agent
agents beliefs  benefit representing updates explicitly that 
stage  optimal behavior depends estimate probabilities js actions 
informative estimates value agent expect interaction  below 
show increase value function i pomdps compared pomdps noise factor 
    examples value functions
section compares value functions obtained solving pomdp static noise factor 
accounting presence another agent    value functions level   i pomdp  advantage refined modeling update i pomdps due two factors  first ability
keep track agents state beliefs better predict future actions  second
ability adjust agents time horizon number steps go interaction
decreases  neither possible within classical pomdp formalism 
continue simple example i pomdpi   agent i  fig    display
value function time horizon    assuming initial belief value j assigns
tl  pj  t l   depicted fig     ii   i e  information j believes
tigers location  value function identical value function obtained agent using
traditional pomdp framework noise  well single agent pomdp described
section      value functions overlap since agents update beliefs
advantage refined modeling agent j i pomdp become apparent  put
another way  agent models j using intentional model  concludes agent j open
door probability     listen probability      coincides noise factor
described section     
    points fig    denote dirac delta functions integrate value equal points height 
    pomdp noise level   i pomdp 

  

fia f ramework equential p lanning ulti  agent ettings

t 


bi

bi

l l gr 
l l gl 
   

   

l l gl 
   

   

l l gr 

   

   

   

   

     
   

   

l l gr 

     
     
 

   

   

   

   

 

l ol or   

 

pjb j tl 

 

     
     

     
 

   

   

   

   

 

   

 

 

 

pr tl b j 

l l gr 

pjb j tl 

   

   

   

 

pjb j
 tl 
       
      
       

      

      

       
      

       

       
      
       

      

      

       

       

      

l ol or   

   

       

pr tl  pj  

l ol or   

     

   

      

l ol or   

     

   

       

l l gl 

   

   

pjb j
 tl 

l l gl 

     

pr tr 
pj  
pr tr b j 

   

pr tr b j 
pr tr p
j 

pr tl p
 
pr tl b j 
j

     

pr tr b j 
pr tr p
j 

     

pr tl b j 
pr tl 
pj  

     

      

       

       
 

   

   

   

   

 

 

pjb j
 tl 

 a 

   

   

   

   

 

pjb j
 tl 

 b 
 gl s 



bi

   

   

   
    

pr tr pj  

   

pr tr b j 

pr tl p
 
pr tl b j 
j

   

 

   

   

    

   

   

   
    
   
 

 
 

   

   

   

pjb j
 tl 

   

 

 

   

   

   

   

 

pj b j
 tl 

 c 

figure    trace belief update agent i   a  depicts prior according
uninformed js beliefs   b  result prediction step listening
action  l   top half  b  shows belief listened given j
listened  two observations j make  gl gr  probability dependent
tigers location  give rise flat portions representing knows js belief
case  increased probability assigns js belief            
due js updates hears gl hears gr resulting values
interval  bottom half  b  shows belief listened j opened
left right door  plots identical action one shown  
knows j information tigers location case   c  result
correction observes tigers growl left creaks hgl si  plots  c 
obtained performing weighted summation plots  b   probability
assigns tl greater tr  information js beliefs allows refine
prediction js action next time step 

  

fig mytrasiewicz   oshi

l


ol

  

value function  u 

 

 

 

 

 

 

   

   

   

   

 

pp i tl 
 tl 
level   i pomdp

pomdp noise

figure    time horizon   value functions obtained solving singly nested i pomdp
pomdp noise factor overlap 
l    ol   gr s   l    

l    or   gl s   l    

l    l   gl     ol   gr    

ol    l    

l    l    

l    or   gl     l   gr    

l    l  gl  ol  gr 

l    or  gl  l  gr 

or    l    

 

value function  u 

 

 

 

 

  
 

   

   

   

   

 

pp i tl 
 tl 
level   i pomdp

pomdp noise

figure    comparison value functions obtained solving i pomdp pomdp
noise time horizon    i pomdp value function dominates due agent adjusting
behavior agent j remaining steps go interaction 

  

fia f ramework equential p lanning ulti  agent ettings

 

value function  u 

 
 
 
 
 
 
 
 

   

   

   

   

 

p  tl 
p i tl 
level   i pomdp

pomdp noise

figure     comparison value functions obtained solving i pomdp pomdp
noise time horizon    i pomdp value function dominates due agent adjusting js remaining steps go  due modeling js belief update  factors
allow better predictions js actions interaction  descriptions individual policies omitted clarity  read fig     

fig    display value functions time horizon    value function
i pomdpi   higher value function pomdp noise factor  reason
related advantages modeling agent js beliefs effect becomes apparent time
horizon   longer  rather  i pomdp solution dominates due agent modeling js time
horizon interaction  knows last time step j behave according optimal
policy time horizon    two steps go j optimize according   steps go
policy  mentioned  effect cannot modeled using pomdp static noise factor
included transition function 
fig     shows comparison i pomdp noisy pomdp value functions
horizon    advantage refined agent modeling within i pomdp framework
increased    factors  adjusting js steps go modeling js belief update
interaction responsible superiority values achieved using i pomdp  particular 
recall second time step information js beliefs tigers location
depicted fig     c   enables make high quality prediction that  two steps left
go  j perform actions ol  l  probabilities                           
respectively  recall pomdp noise probabilities remained unchanged          
     respectively  
fig     shows agent policy graph time horizon    usual  prescribes optimal
first action depending initial belief tigers location  subsequent actions depend
observations received  observations include creaks indicative agents
    note i pomdp solution good solution pomdp agent operating alone environment shown fig    

  

fig mytrasiewicz   oshi

         
ol

             

             

l
 

l

 gr s 

 gl cl cr 
 gr   

             

             

l

 

ol


 

 gr s 
 gl cl cr 

l

l
 gr   

         

l

 gr cl cr 
 gr cl cr 
 gl cl cr 
 gl s 
 gr     gl   
 gl   
 gr s 
 gl s 

 gl s 
 gr cl cr 

ol

             

l

 gl   

l
 

 gr   


 gl   

 



l

figure     policy graph corresponding i pomdp value function fig     
opened door  creaks contain valuable information allow agent make
refined choices  compared ones noisy pomdp fig     consider case agent
starts fairly strong belief tigers location  decides listen  according four
off center top row l nodes fig      hears door creak  agent position
open either left right door  even counter initial belief  reason
creak indication tigers position likely reset agent j j
open doors following two time steps  now  two growls coming
door lead enough confidence open door  agent hearing
tigers growls indicative tigers position state following agents actions 
note value functions policy depict special case agent
information probability j assigns tigers location  fig     ii    accounting
visualizing possible beliefs js beliefs difficult due complexity
space interactive beliefs  ongoing work indicates  drastic reduction complexity
possible without loss information  consequently representation solutions manageable
number dimensions indeed possible  report results separately 

   conclusions
proposed framework optimal sequential decision making suitable controlling autonomous
agents interacting agents within uncertain environment  used normative
paradigm decision theoretic planning uncertainty formalized partially observable markov
decision processes  pomdps  point departure  extended pomdps cases agents
interacting agents allowing beliefs physical environment  agents  could include beliefs others abilities  sensing
capabilities  beliefs  preferences  intended actions  framework shares numerous properties
pomdps  analogously defined solutions  reduces pomdps agents alone
environment 
contrast recent work dec pomdps  bernstein et al         nair et al         
work motivated game theoretic equilibria  boutilier        hu   wellman        koller
  

fia f ramework equential p lanning ulti  agent ettings

  milch        littman         approach subjective amenable agents independently
computing optimal solutions 
line work presented opens area future research integrating frameworks
sequential planning elements game theory bayesian learning interactive settings 
particular  one avenues future research centers proving formal properties
i pomdps  establishing clearer relations solutions i pomdps various flavors
equilibria  another concentrates developing efficient approximation techniques solving
i pomdps  pomdps  development approximate approaches i pomdps crucial
moving beyond toy problems  one promising approximation technique working particle
filtering  devising methods representing i pomdp solutions without assumptions
whats believed agents beliefs  mentioned  spite complexity
interactive state space  seem intuitive representations belief partitions corresponding
optimal policies  analogous pomdps  research issues include suitable
choice priors models    ways fulfill absolute continuity condition needed
convergence probabilities assigned alternative models interactions  kalai   lehrer 
      

acknowledgments
research supported national science foundation career award iri         
nsf award iri         

appendix a  proofs
proof propositions      start proposition    applying bayes theorem 

t 
bti  ist     p r ist  oti   at 
  bi    

 
 bt 
p r ist  oti  at 


t  t 

p r oi  ai  bi  

p
t   
  ist  bit   ist   p r ist   oti  at 
 
p
p
t 
t   p r at   at    ist   
  ist  bit   ist    at  p r ist   oti  at 
  aj  
j

j
p
p
    
t 
t   p r at   ist   
 

 

  ist  bit   ist    at  p r ist   oti  at 
j
j

j
p
p
t 

t    ist   p r ist  at    ist   
  ist  bit   ist    at  p r at 
j  mj  p r ot  is  
j
p
p
t 

t   p r ist  at    ist   
  ist  bit   ist    at  p r at 
j  mj  p r ot  is  
j
p
p
t 
t    ot  p r ist  at    ist   
  ist  bit   ist    at  p r at 

j  mj  oi  s  
j

    looking kolmogorov complexity  li   vitanyi        possible way assign priors 

  

fig mytrasiewicz   oshi

simplify term p r ist  at    ist    let us substitute interactive state ist components  mj interactive states intentional  ist    st   jt      st   btj   bjt   

p r ist  at    ist      p r st   btj   bjt  at    ist   
  p r btj  st   bjt   at    ist   p r st   bjt  at    ist   
  p r btj  st   bjt   at    ist   p r bjt  st   at    ist   p r st  at    ist   
  p r btj  st   bjt   at    ist   i bjt    bjt  ti  st    at    st  
    
b tj   
mj subintentional  ist    st   mtj      st   htj  

p r ist  at    ist      p r st   htj  
b tj  at    ist   
b tj  at    ist   
b tj   at    ist   p r st  
  p r htj  st  
b tj   at    ist   p r bjt  st   at    ist   p r st  at    ist   
  p r htj  st  


  p r hj  s  
b tj   at    ist   i m
b tj  ti  st    at    st  
b t 
    
j  m

joint action pair  at    may change physical state  third term right hand
side eqs         captures transition  utilized mnm assumption replace
second terms equations boolean identity functions  i  bjt    bjt   i m
b t 
b tj  
j  m
respectively  equal   two frames identical    otherwise  let us turn attention
first terms  mj ist ist  intentional 
p
p r btj  st   bjt   at    ist      ot p r btj  st   bjt   at    ist    otj  p r otj  st   bjt   at    ist   
pj
  ot p r btj  st   bjt   at    ist    otj  p r otj  st   bjt   at   
pj
t 
t    ot  
  ot jt  bt 
j
j   aj   oj   bj  oj  st  

    

j

else subintentional 

p r htj  st  
b tj   at    ist     

 

 

p



po j


po j
otj

b tj   at    ist   
b tj   at    ist    otj  p r otj  st  
p r htj  st  

b tj   at   
b tj   at    ist    otj  p r otj  st  
p r htj  st  



t    ot  
k  append ht 
j
j   oj    hj  oj  st  

    

t 
eq      first term right hand side   agent js belief update  se j  bt 
j   j   oj  
generates belief state equal btj   similarly  eq        first term   appending otj
ht 
results htj   k kronecker delta function  second terms right hand
j
side equations  mno assumption makes possible replace p r o  st   bt   at   
j

j

oj  st   at    otj    p r otj  st  
b tj   at    oj  st   at    otj   respectively 
let us substitute eq     eq     
p
t 
t    ot  i 
bt    bt  ti  st    at    st  
p r ist  at    ist      ot jt  bt 
j
j
j   aj   oj   bj  oj  s  
j
j
    
 
 
substituting eq     eq     get 
p


t    ot  i m
p r ist  at    ist      ot k  append ht 
b t 
b tj  
j
j   oj    hj  oj  s  
j  m
j

ti  st    at    st  

    

  

fia f ramework equential p lanning ulti  agent ettings

replacing eq     eq     get 
p
p
t 
t 
t   
p r at 
 jt   oi  st   at    oti   ot jt  bt 
ist  bi  is
j   j   j   bj  
j
at 
j
j
oj  st   at    otj  i bjt    bjt  ti  st    at    st  

bti  ist    

p

    

similarly  replacing eq      eq     get 

p
p
t 
t    ot  
t   
p r at 
bti  ist     ist  bt 

j  mj  oi  s  
 is
at 
j
p
t 
t 


t 

bj  m
b tj  ti  st    at    st  
ot k  append hj   oj    hj  oj  s     oj  i m
j

arrive final expressions belief update removing terms
i m
b t 
b tj   changing scope first summations 
j  m
mj interactive states intentional 

i  bjt    bjt  

p
p
t 
t    ot  
bt   ist    at  p r at 
bti  ist     ist   m

j  j  oi  s  
b t 
 bjt
j
j
p
t  t 
t 
t 


t 

ot jt  bj   aj   oj   bj  oj  s     oj  ti  s      

      


    

j

else  subintentional 
p
p
t 
t 
t    ot  
 ist    at  p r at 
bti  ist     ist   m
bi

j  mj  oi  s  
 

b
b t 
j
j
j
p
   ht  o  st   at    ot  t  st    at    st  
 

ot k  append ht 
j
j
j
j
j

    

j

since proposition   expresses belief bti  ist   terms parameters previous time step
only  proposition   holds well 
present proof theorem   note equation    defines value
iteration i pomdps  rewritten following form  u n   hu n    here  h   b b
backup operator  defined as 
hu n   i     max h i   ai   u n   
ai ai

h   ai b r is 
h i   ai   u    

p


bi  is eri  is  ai    

p

oi

p r oi  ai   bi  u  hsei  bi   ai   oi    i 

b set bounded value functions u   lemmas     establish important
properties backup operator  proof lemma   given below  proof lemma   follows
thereafter 
proof lemma    select arbitrary value functions v u v   i l   u  i l   i l
i l   let i l arbitrary type agent i 
  

fig mytrasiewicz   oshi





p

p

hv  i l     max
oi p r oi  ai   bi  v  hsei l  bi   ai   oi    i 
bi  is eri  is  ai    
ai ai
p
p
  bi  is eri  is  ai     oi p r oi  ai   bi  v  hsei l  bi   ai   oi    i 
p
p



b
 is eri  is  ai    
oi p r oi  ai   bi  u  hsei l  bi   ai   oi    i 

p
p
max
oi p r oi  ai   bi  u  hsei l  bi   ai   oi    i 
bi  is eri  is  ai    
ai ai

  hu  i l  

since i l arbitrary  hv hu  
proof lemma    assume two arbitrary well defined value functions v u v u  
lemma   follows hv hu   let i l arbitrary type agent i  also  let ai
action optimizes hu  i l   
  hu  i l   hv  i l  



p

  max sumis bi  is eri  is  ai     oi p r oi  ai   bi  u  sei l  bi   ai   oi    hi i 
ai ai

p
p
max
bi  is eri  is  ai    
oi p r oi  ai   bi  v  sei l  bi   ai   oi    hi i 
ai ai
p
p
bi  is eri  is  ai     oi p r oi  ai   bi  u  sei l  bi   ai   oi    hi i 
p
p



oi p r oi  ai   bi  v  sei l  bi   ai   oi    hi i 
bi  is eri  is  ai  
p


  oi p r oi  ai   bi  u  sei l  bi   ai   oi    hi i 
p

oi p r oi  ai   bi  v  se

i l  bi   ai   oi    hi i 
p



  oi p r oi  ai   bi   u  sei l  bi   ai   oi    hi i  v  sei l  bi   ai   oi    hi i 
p
oi p r oi  ai   bi    u v   
    u v   

supremum norm symmetrical  similar result derived hv   i l   hu  i l   
since i l arbitrary  contraction property follows  i e    hv hu      v u    
lemmas     provide stepping stones proving theorem    proof theorem   follows
straightforward application contraction mapping theorem  state contraction
mapping theorem  stokey   lucas        below 
theorem    contraction mapping theorem    s    complete metric space  
contraction mapping modulus  
   exactly one fixed point u s 
   sequence  u n   converges u  
proof theorem   follows 
  

fia f ramework equential p lanning ulti  agent ettings

proof theorem    normed space  b         complete w r t metric induced supremum norm  lemma   establishes contraction property backup operator  h  using theorem    substituting h  convergence value iteration i pomdps unique fixed
point established 
go piecewise linearity convexity  pwlc  property value function 
follow outlines analogous proof pomdps  hausktecht        smallwood  
sondik        
let   r real valued bounded function  let space real valued
bounded functions b is   define inner product 
definition    inner product   define inner product  h    b is   is  r 
x
h  bi  
bi  is  is 


next lemma establishes bilinearity inner product defined above 
lemma    bilinearity   s  r  f  g b is   b   is  following equalities
hold 
hsf   tg  bi   shf  bi   thg  bi
hf  sb   ti   shf  bi   thf 
ready give proof theorem    theorem   restates theorem   mathematically  proof follows thereafter 
theorem    pwlc   value function  u n   finitely nested i pomdp piece wise linear
convex  pwlc   mathematically 
u n  i l     max
n


x

bi  is n  is 

n            



proof theorem    basis step  n    
bellmans dynamic programming equation 
u    i     max
ai

x

bi  is er is  ai  

    



p
eri  is  ai     aj r is  ai   aj  p r aj  mj    here  eri    represents expectation
r w r t  agent js actions  eq     represents inner product using lemma    inner product
linear bi   selecting maximum set linear vectors  hyperplanes   obtain pwlc
horizon   value function 
inductive hypothesis  suppose u n   i l   pwlc  formally have 
u n   i l     max
n 

 

p

max

n   

bi  is 

n 



p

n   is 

is mj imj bi

 is n   is 
  

 

p

is mj smj bi

 is n   is 



    

fig mytrasiewicz   oshi

inductive proof  show u n  i l   pwlc 

u n  i l     max
at 


 

x

t 
bt 
 eri  ist    at 
 is
  

x

t 
n 
p r oti  at 
 i l  
  bi  u

oti

ist 

inductive hypothesis 
 
p
t 
t   er  ist    at   
u n  i l     max

ist  bi  is

at 


 

p

oti

t 
p r oti  at 
  bi  

max

n  n 

p


n   ist  
ist bi  is  

 

 

t 
t  t 

let l bt 
  ai   oi   index alpha vector maximizes value b   se bi   ai   oi   
then 
 
p
t 
t   er  ist    at   
u n  i l     max

ist  bi  is

t 
ai
 
p
p
t 

n 
  ot p r oti  at 
ist bi  is  l bt   at   ot  
  bi  








second equation inductive hypothesis 
 
p
p
t 
t   er  ist    at     
n
t  t 
u  i l     max

ot p r oi  ai   bi  
ist  bi  is

at 








p


n 
ist  mtj imj bi  is  l bt   at   ot  




 

p


n 
ist  mtj smj bi  is  l bt   at   ot  




substituting bti appropriate belief updates eqs          get 
 
p
p
t 
t  t 
t   er  ist    at     
u n  i l     max

oti p r oi  ai   bi  
ist  bi  is

t 
ai
 


p
p
p
t 
t  t 
t 

 
p r aj  j   oi  st   at    oti  
ist  mtj imj
ist  bi  is
at 
j


p
t  t 
t 


t 


t 
t 

ot oj  s     oj   jt  bj   aj   oj   bj  i bj   bj  ti  s      

 

j

n 

l b
t  t   is  
 ai  oi  



p
p
p
t 
t 
t 
t 
  ist  mt smj ist  bi  is  
p r aj  mj   oi  st   at    oti  
at 
j
j


p
t 
t 


t 



t 
t 

ot oj  s     oj   k  append hj   oj   hj  i m
bj  m
b j  ti  s      
j
  
n 

l b
t  t   is  
 a
 o  






  

fia f ramework equential p lanning ulti  agent ettings


u n  i l     max
at 





p

p

 

p

t 
t   er  ist    at   

ist  bi  is


t 
t   
ist  bi  is



t    ot  
j
otj oj  s  

p



at 
j

 

p

oti

 

p

ist  mtj imj


t 
p r at 
 
 
oi  st   at    oti  
j
j

t 
t    at    st  
bt  bt
jt  bt 
j   aj   oj   bj  i j   j  ti  s



n 

l b
t  t   is  
 ai  oi  



p
p
p
t 
t 
t 
t 
p r aj  mj   oi  st   at    oti  
  ist  mt smj ist  bi  is  
at 
j
j


p
t 
  ht  i m
 t  st    at    st  
ot ojt  st   at    otj   k  append ht 
 

b
 

b
j
j
j
j
j
j
  
n 

l b
t  t   is  
 a
 o  






rearranging terms equation 
u n  

 


p
p p
t 
t    er  ist    at     
 
 
max
b
 is
t 
t   m

i l
oti
ist  mtj imj



im
j
j
at 



p
p
t  t 

p r aj  j   oi  st   at    oti   ot ojt  st   at    otj  
at 
j
j



n 
t  t 
t 

t 
t 


jt  bj   aj   oj   bj  i bj   bj  ti  s      
l bt   at   ot    is  




p
p
p
p
  ist   mt  smj bit   ist    eri  ist    at 
oti
ist  mtj smj
oti
  
j


p
p
p r ajt   mt 
  oi  st   at    oti   ot ojt  st   at    otj  

j
at 
j
j

 

n 



l b
b t 
b tj  ti  st    at    st  
k  append ht 
t  t   is  
j  m
j   oj   hj  i m
 ai  oi  


p
t 
t   n  ist   
  max
ai
imj bi  is
ist   mt 
j
at 


p
t 
t 
t 
n
  ist   mt  smj bi  is  ai  is  
j

therefore 
u n  

i l  

  max
n
n
 

 
 

p



p

t 
t   n  ist   
ist   mt 
imj bi  is
j



t 
t   n  ist   
smj bi  is
ist   mt 
j
p
t 
t   n  ist      maxhbt    n
max
ist  bi  is

n

n

  

    

fig mytrasiewicz   oshi

where  mjt  ist  intentional n   n  
n  ist   

eri  ist    at 
 



p p

p

t 
p r at 
j  j  



oi  ist   at    oti  
  ot ist  mt imj
at 
j
j


p
t  t 
t 


t 


t 
t 

ot oj  isj     oj   jt  bj   aj   oj   bj  i bj   bj  ti  s      

 

j

n 

l b
t  t   is  
 o  
 a






and  mjt  subintentional n   n  
n  ist   

eri  ist    at 
 



p p

p

t 
p r at 
j  j  



oi  ist   at    oti  
  ot ist  mt smj
at 

j
j


p
t 
t 
t 
t 



t 



bj  m
b j  ti  s      
ot oj  isj     oj   k  append hj   oj   hj  i m

 

j

n 

l b
t  t   is  
 ai  oi  


eq     inner product using lemma    value function linear b t 
  furthermore 
maximizing set linear vectors  hyperplanes  produces piecewise linear convex value
function 

references
ambruster  w     boge  w          bayesian game theory  moeschlin  o     pallaschke  d   eds    game
theory related topics  north holland 
aumann  r  j          interactive epistemology i  knowledge  international journal game theory  pp 
       
aumann  r  j     heifetz  a          incomplete information  aumann  r     hart  s   eds    handbook
game theory economic applications  volume iii  chapter     elsevier 
battigalli  p     siniscalchi  m          hierarchies conditional beliefs interactive epistemology
dynamic games  journal economic theory  pp         
bernstein  d  s   givan  r   immerman  n     zilberstein  s          complexity decentralized control
markov decision processes  mathematics operations research                
binmore  k          essays foundations game theory  blackwell 
boutilier  c          sequential optimality coordination multiagent systems  proceedings
sixteenth international joint conference artificial intelligence  pp         
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  journal artificial intelligence research          
brandenburger  a          power paradox  recent developments interactive epistemology 
tech  rep   stern school business  new york university  http   pages stern nyu edu  abranden  
brandenburger  a     dekel  e          hierarchies beliefs common knowledge  journal economic
theory             
dennett  d          intentional systems  dennett  d   ed    brainstorms  mit press 
fagin  r  r   geanakoplos  j   halpern  j  y     vardi  m  y          hierarchical approach modeling
knowledge common knowledge  international journal game theory  pp         
fagin  r  r   halpern  j  y   moses  y     vardi  m  y          reasoning knowledge  mit press 
fudenberg  d     levine  d  k          theory learning games  mit press 
  

fia f ramework equential p lanning ulti  agent ettings

fudenberg  d     tirole  j          game theory  mit press 
gmytrasiewicz  p  j     durfee  e  h          rational coordination multi agent environments  autonomous agents multiagent systems journal               
harsanyi  j  c          games incomplete information played bayesian players  management
science                
hauskrecht  m          value function approximations partially observable markov decision processes 
journal artificial intelligence research  pp       
hausktecht  m          planning control stochastic domains imperfect information  ph d  thesis 
mit 
hu  j     wellman  m  p          multiagent reinforcement learning  theoretical framework algorithm  fifteenth international conference machine learning  pp         
kadane  j  b     larkey  p  d          subjective probability theory games  management science 
              
kaelbling  l  p   littman  m  l     cassandra  a  r          planning acting partially observable
stochastic domains  artificial intelligence                
kalai  e     lehrer  e          rational learning leads nash equilibrium  econometrica  pp           
koller  d     milch  b          multi agent influence diagrams representing solving games  seventeenth international joint conference artificial intelligence  pp            seattle  washington 
li  m     vitanyi  p          introduction kolmogorov complexity applications  springer 
littman  m  l          markov games framework multi agent reinforcement learning  proceedings
international conference machine learning 
lovejoy  w  s          survey algorithmic methods partially observed markov decision processes 
annals operations research                
madani  o   hanks  s     condon  a          undecidability probabilistic planning related
stochastic optimization problems  artificial intelligence           
mertens  j  f     zamir  s          formulation bayesian analysis games incomplete information 
international journal game theory          
monahan  g  e          survey partially observable markov decision processes  theory  models 
algorithms  management science      
myerson  r  b          game theory  analysis conflict  harvard university press 
nachbar  j  h     zame  w  r          non computable strategies discounted repeated games  economic
theory            
nair  r   pynadath  d   yokoo  m   tambe  m     marsella  s          taming decentralized pomdps  towards
efficient policy computation multiagent settings  proceedings eighteenth international
joint conference artificial intelligence  ijcai     
ooi  j  m     wornell  g  w          decentralized control multiple access broadcast channel 
proceedings   th conference decision control 
papadimitriou  c  h     tsitsiklis  j  n          complexity markov decision processes  mathematics
operations research                
russell  s     norvig  p          artificial intelligence  modern approach  second edition   prentice hall 
smallwood  r  d     sondik  e  j          optimal control partially observable markov decision
processes finite horizon  operations research  pp           
stokey  n  l     lucas  r  e          recursive methods economic dynamics  harvard univ  press 

  


