journal of artificial intelligence research                  

submitted        published      

stochastic enforced hill climbing
jia hong wu

jw   alumni   purdue   edu

institute of statistical science 
academia sinica  taipei      taiwan roc

rajesh kalyanam
robert givan

rkalyana   purdue   edu
givan   purdue   edu

electrical and computer engineering 
purdue university  w  lafayette  in        usa

abstract
enforced hill climbing is an effective deterministic hill climbing technique that deals with local optima using breadth first search  a process called basin flooding   we propose and evaluate
a stochastic generalization of enforced hill climbing for online use in goal oriented probabilistic planning problems  we assume a provided heuristic function estimating expected cost to the
goal with flaws such as local optima and plateaus that thwart straightforward greedy action choice 
while breadth first search is effective in exploring basins around local optima in deterministic problems  for stochastic problems we dynamically build and solve a heuristic based markov decision
process  mdp  model of the basin in order to find a good escape policy exiting the local optimum 
we note that building this model involves integrating the heuristic into the mdp problem because
the local goal is to improve the heuristic 
we evaluate our proposal in twenty four recent probabilistic planning competition benchmark
domains and twelve probabilistically interesting problems from recent literature  for evaluation 
we show that stochastic enforced hill climbing  seh  produces better policies than greedy heuristic
following for value cost functions derived in two very different ways  one type derived by using
deterministic heuristics on a deterministic relaxation and a second type derived by automatic learning of bellman error features from domain specific experience  using the first type of heuristic 
seh is shown to generally outperform all planners from the first three international probabilistic
planning competitions 

   introduction
heuristic estimates of distance to the goal have long been used in deterministic search and deterministic planning  such estimates typically have flaws such as local extrema and plateaus that limit
their utility  methods such as simulated annealing  kirkpatrick  gelatt    vecchi        cerny 
      and a   nilsson        search have been developed for handling flaws in heuristics  more
recently  excellent practical results have been obtained by flooding local optima using breadth first
search  this method is called enforced hill climbing  hoffmann   nebel        
deterministic enforced hill climbing  deh  is proposed in the work of hoffmann and nebel
       as a core element of the successful deterministic planner fast forward  ff   deh is an
extension of the basic hill climbing approach of simply selecting actions greedily by looking
ahead one action step  and terminating when reaching a local optimum  deh extends basic hillclimbing by replacing termination at local optima with breadth first search to find a successor state
with strictly better heuristic value  the planner then moves to that descendant and repeats this
c
    
ai access foundation  all rights reserved 

fiw u   k alyanam     g ivan

process  deh is guaranteed to find a path to the goal if the problem itself is deadend free  so that
every state has such a path   while that relatively weak guarantee applies independent of the quality
of the heuristic function  the intent of deh is to remediate flaws in a generally accurate heuristic
in order to leverage that heuristic in finding short paths to the goal  in domains where the basin
size  search depth needed to escape any optimum  is bounded  deh can provide a polynomial time
solution method  hoffmann        
enforced hill climbing is not defined for probabilistic problems  due to the stochastic outcomes
of actions  in the presence of stochastic outcomes  finding descendants of better values no longer
implies the existence of a policy that reaches those descendants with high probability  one may argue that ff replan  yoon  fern    givan       a top performer for recent probabilistic planning
benchmarksuses enforced hill climbing during its call to ff  however  the enforced hill climbing
process is used on a determinized problem  and ff replan does not use any form of hill climbing
directly in the stochastic problem  in fact  ff replan does not consider the outcome probabilities
at all 
one problem to consider in generalizing enforced hill climbing to stochastic domains is that the
solution to a deterministic problem is typically concise  a sequential plan  in contrast  the solution
to a stochastic problem is a policy  action choice  for all possibly reached states  the essential
motivation for hill climbing is to avoid storing exponential information during search  and even the
explicit solution to a stochastic problem cannot be directly stored while respecting this motivation 
for this reason  we limit consideration to the online setting  where the solution to the problem is a
local policy around the current state  after this local policy is committed to and executed until the
local region is exited  the planner then has a new online problem to solve  possibly retaining some
information from the previous solution   our approach generalizes directly to the construction of
offline policies in situations where space to store such policies is available  note that  in contrast 
deterministic enforced hill climbing is easily implemented as an offline solution technique 
we propose a novel tool for stochastic planning by generalizing enforced hill climbing to goalbased stochastic domains  rather than seeking a sequence of actions deterministically leading to a
better state  our method uses a finite horizon mdp analysis around the current state to seek a policy
that expects to improve on the heuristic value of the current state  critical to this process is the direct
incorporation of both the probabilistic model and the heuristic function in finding the desired policy 
therefore  for the finite horizon analysis  the heuristic function is integrated into the mdp problem
in order to represent the temporary  greedy goal of improving on the current heuristic value  this
integration is done by building a novel heuristic based mdp in which any state has a new exit
action available that terminates execution with cost equal to the heuristic estimate for that state  and
all other action costs are removed    in a heuristic based mdp  finite horizon policies are restricted
by a requirement that at horizon one  the exit action must be selected  but can also be selected at
other horizons   in this heuristic based mdp  the cost of any policy  at a state s is the expected
value of the heuristic upon exit  or horizon  if  is executed from s 
thus  we find the desired local policy using value iteration on the heuristic based mdp around
the current state  with deepening horizon  until a policy is found with cost improving on the heuristic
estimate at the current state  the restriction of selecting the exit action at horizon one corresponds
to initializing value iteration with the provided heuristic function  when such a policy is found  the
   the motivation for the removal of action costs in the heuristic based mdp is discussed in section     

   

fis tochastic e nforced h ill  c limbing

method executes the policy until an exiting action is indicated  or to the horizon used in computing
the policy  
the resulting method  stochastic enforced hill climbing  seh   simply generalizes depth k
breadth first search for a state with improved heuristic value  from deh  to a k horizon value iteration computation seeking a policy that expects improvement in heuristic value  note that although
stochastic enforced hill climbing is an explicit statespace technique  it can be suitable for use in astronomically large statespaces if the heuristic used is informative enough to limit the effective size
of the horizon k needed to find expected heuristic improvement  our empirical results in this work
demonstrate this behavior successfully 
    applicability and limitations
stochastic enforced hill climbing  seh  can be applied to any heuristic function  however  the
applicability  and likewise the limitations  of seh greatly depends on the characteristics of the
heuristic function  seh is appropriate in any goal oriented problem given a strong enough heuristic
function  and we demonstrate empirically that seh generally outperforms greedy following of the
same heuristic for a variety of heuristics in a variety of domains  even in presence of probabilistically interesting features  little   thiebaux        and deadends  seh can rely upon the heuristic
function for identification of dead ends and appropriate handling of probabilistically interesting features that require non local analysisseh simply provides local search that often can correct other
flaws in the heuristic function  seh is thus intended as a possible improvement over stochastic
solution methods that construct a cost to go  cost  function and follow it greedily when using the
constructed cost function as a search heuristic  many methods for constructing value cost functions
have been proposed and evaluated in the literature  all of which can potentially be improved for
goal based domains by using seh in place of greedy following  sutton        fahlman   lebiere 
      bertsekas        gordon        mahadevan   maggioni        sanner   boutilier          
we prove the correctness of seh in section     by showing that in deadend free domains  seh
finds the goal with probability one  i e  seh does not get stuck in local optima  
while seh is a search technique that leverages a heuristic estimate of distance to go  it must
be emphasized that  unlike many other such search techniques  seh makes no promises about the
optimality of the solution path found  seh is a greedy  local technique and can only promise to
repeatedly find a policy that reduces the heuristic value  and only when that is possible  as such 
seh is an inappropriate technique for use when optimal solutions are required 
stochastic enforced hill climbing can be ineffective in the presence of huge plateaus or valleys
in the heuristic functions  due to extreme resource consumption in finding desired local policies 
heuristic functions with huge plateaus result from methods that have failed to find any useful information about the problem in those state regions  seh is inappropriate as the only tool for solving
a stochastic planning problemother tools are needed to construct a useful heuristic function that
manages deadends and avoids huge plateaus  this weakness mirrors the weakness of enforced hillclimbing in deterministic domains  seh can also fail to find the goals when avoidable dead ends
are present but not recognized early enough by the heuristic  in fact  effective dead end detection is
a central goal in heuristic design when any greedy technique will be applied to the heuristic 
   for applicability of seh  a cost function must be non negative and must identify goals by assigning zero to a state if
and only if it is a goal state  however  more general value cost functions can be normalized to satisfy these requirements 

   

fiw u   k alyanam     g ivan

further insight into the usefulness of seh can be gained by comparison with recent determinizing replanners  as mentioned above  one way to exploit deterministic planning techniques such as
deh for stochastic problems is to determinize the planning problem and use a deterministic planner to select an action sequence  executing this action sequence in the problem is not guaranteed
to reach the goal due to the determinization approximation  so replanning is needed to augment
this technique  in this paper  we call stochastic planners that use this technique determinizing replanners  determinizing replanners using a determinization  called all outcomes  that retains
all possible state transitions can be shown to reach the goal with probability one in the absence of
dead end states 
in contrast to determinizing replanners  seh at no point relies on any determinization of the
problem  but instead analyzes increasing size local probabilistic approximations to the problem 
seh conducts a full probabilistic analysis within the horizon  seeking the objective of reducing the
provided heuristic  using value iteration  in this way  seh leverages the probabilistic parameters
that are ignored by determinizing replanners  as well as the provided heuristic function  which can
be based upon substantial probabilistic analysis  as a result  seh successfully handles probabilistic
problem aspects that cause major problems for determinizing replanners  however  at this point 
we have no theoretical results characterizing its gains over determinizing replanners  instead  we
have an extensive empirical evaluation showing advantages over ff replan  yoon et al        
and rff  teichteil konigsbuch  kuter    infantes         two determinizing replanners   as well
as substantial gains compared to greedy following of the heuristic  which also uses the transition
probability parameters  
    evaluation
we test seh on a broad range of domains from the first three international probabilistic planning
competitions  as well as the probabilistically interesting domains in little   thiebaux        
using two very different methods to generate heuristic functions  first  we test seh on a heuristic function based on the ideas of the successful re planner ff replan  yoon et al          this
new controlled randomness ff  cr ff  heuristic is the deterministic ff heuristic  hoffmann
  nebel        computed on the simple determinization of the probabilistic problem that makes
available a deterministic transition wherever a probabilistic transition was possible  we note that
ff replan itself does not use this  or any  heuristic function in the stochastic problem  instead  ffreplan relies on ff to construct a plan in the deterministic problem  and these calls to ff in turn use
deterministic enforced hill climbing with exactly this heuristic  here  we consider the performance
of this heuristic directly in the stochastic problem  comparing greedy heuristic following with sehbased search around the heuristic  the latter method using seh constitutes a novel method for
combining determinization  that removes the probabilistic parameters  with probabilistic reasoning 
our experiments show that this new method substantially outperforms ff replan across our broad
evaluation 
we have also performed a second evaluation of our technique on heuristic functions learned
from domain specific experience by the relational feature learning method presented in the work
of wu and givan               these heuristic functions have already been shown to give good
performance when used to construct a simple greedy policy  and are further improved by seh 
the seh technique can be seen to perform well in a domain by domain analysis across the
broad set of competition planning domains  and full domain by domain results are available in an
   

fis tochastic e nforced h ill  c limbing

online appendix  however  to compress and summarize the extensive per problem results  we have
divided all the evaluation domains into experimenter defined categories and have aggregated performance measurement within each problem category  while some categories are single domains 
more generally  multiple closely related domains may be aggregated within a single category  for
example  multiple domains from the competitions have been variants of the blocks world  and
problems in these domains are aggregated as a b locksworld category 
in order to fairly compare seh with ff based planners  such as rff  as described in teichteilkonigsbuch et al         and ff replan  that exploit the blocksworld targeted planning heuristics
added goal deletion and goal agenda  we have provided these heuristics as extensions to seh 
the resulting planner is called seh    described in detail in section      our results show that seh 
performs nearly identically to seh on non blocksworld categories when using the cr ff heuristic 
we employ these extensions when comparing seh with the cr ff heuristic to other planners 
using experimenter defined categories  we are able to show that seh exploits the heuristic
functions more effectively than greedy following of the heuristic  seh statistically significantly
outperforms greedy following in thirteen out of seventeen categories using the cr ff heuristics
while losing in one category  seh also outperforms greedy following in six out of seven categories using the learned heuristics   in both cases  the other categories showed similar performance
between the compared planners  
we show that seh    when using the cr ff heuristics  outperforms ff replan on ten out of
fifteen categories  with similar performance on two more categories  losing on only three categories 
our aggregate results show that seh   using the cr ff heuristics  has a particularly strong performance advantage over ff replan in probabilistically interesting categories  little   thiebaux 
      
finally  we compare the performance of seh  against that of rff bg  teichteil konigsbuch
et al          one winner of the fully observable track of the third international probabilistic planning competition  seh  outperforms rff bg on twelve out of fifteen categories  with similar
performance on one more category  losing on only two categories 
in summary  our empirical work demonstrates that seh provides a novel automatic technique
for improving on a heuristic function using limited searches  and that simply applying seh to
reasonable heuristic functions produces a state of the art planner 

   technical background  markov decision processes
we give a brief review of markov decision processes  mdps  specialized to goal region objectives 
for more detail on mdps  see the work of bertsekas         puterman         and sutton and barto
       
    goal oriented markov decision processes
a markov decision process  mdp  m is a tuple  s  a  c  t  sinit    here  s is a finite state space
containing initial state sinit   and a selects a non empty finite available action set a s  for each state
s in s  the action cost function c assigns a non negative real action cost to each state action state
triple  s  a  s   where action a is enabled in state s  i e   a is in a s   the transition probability
function t maps state action pairs  s  a  to probability distributions over s  p s   where a is in
a s  
   

fiw u   k alyanam     g ivan

to represent the goal  we include in s a zero cost absorbing state   i e   such that c   a  s   
  and t    a        for all s  s and a  a    goal oriented mdps are mdps where there is
a subset g  s of the statespace  containing   such that      c g  a  s   is zero whenever g  g
and one otherwise  and     t  g  a    is one for all g  g and all a  a g   the set g can thus be
taken to define the action cost function c  as well as constrain the transition probabilities t  
a  stochastic  policy for an mdp    s  n  p a  specifies a distribution over actions for
each state at each finite horizon  the cost to go function j   s  k  gives the expected cumulative
cost for k steps of execution starting at state s selecting actions according to    at each state
encountered  for any horizon k  there is at least one  deterministic  optimal policy      k  for

which j   s  k   abbreviated j   s  k   is no greater than j   s  k  at every state s  for any other
policy   the following q function evaluates an action a by using a provided cost to go function
j to estimate the value after action a is applied 
x
q s  a  j   
t  s  a  s   c s  a  s     j s    
s s

recursive bellman equations use q   to describe j  and j  as follows 
j   s  k    e  q s   s  k   j     k       and
j   s  k    min q s  a  j     k      
aa s 

taking the expectation over the random choice made by the possibly stochastic policy  s  k   in
both cases  the zero step cost to go function is zero everywhere  so that j   s       j   s        
for all s  value iteration computes j   s  k  for each k in increasing order starting at zero  note that
when a policy or cost function does not depend on k  we may drop k from its argument list 
also using q    we can select an action greedily relative to any cost function  the policy
greedy j  selects  at any state s and horizon k  a uniformly randomly selected action from
argminaa s  q s  a  j   k      
while goal based mdp problems can be directly specified as above  they may also be specified
exponentially more compactly using planning languages such as ppddl  younes  littman  weissman    asmuth         as used in our experiments  our technique below avoids converting the
entire ppddl problem explicitly into the above form  for resource reasons  but instead constructs a
sequence of smaller problems of explicit mdp form modeling heuristic flaws 
a dead end state is a state for which every policy has zero probability of reaching the goal at any
horizon  we say that a policy reaches a region of states with probability one if following that policy
to horizon k has a probability of entering the region at some point that converges to one as k goes to
infinity  we say dead ends are unavoidable in a problem whenever there is no policy from sinit that
reaches the goal region with probability one   we then say a domain has unavoidable dead ends if
any problem in that domain has unavoidable dead ends   we note that greedy techniques such as
hill climbing can be expected to perform poorly in domains that have dead end states with attractive
heuristic values  application of seh thus leaves the responsibility for detecting and avoiding deadend states in the design of the heuristic function 
a heuristic h   s  r may be provided  intended as an estimate of the cost function j for large
horizons  with h s      for s  g  and h s      otherwise  the heuristic may indicate dead end
states by returning a large positive value v which we assume is selected by the experimenter to
exceed the expected steps to the goal from any state that can reach the goal  in our experiments  we
   

fis tochastic e nforced h ill  c limbing

add trivial  incomplete dead end detection  described in section      to each heuristic function that
we evaluate 
we note that some domains evaluated in this paper do contain unavoidable deadends  so that
there may be no policy with success ratio one  the choice of the large value used for recognized
dead end states effects a trade off between optimizing success ratio and optimizing expected cost
incurred to the goal when successful 
    determinizing stochastic planning problems
some stochastic planners and heuristic computation techniques  including some used in our experiments  rely on computing deterministic approximations of stochastic problems  one such planner 
the all outcomes ff replan  yoon et al          determinizes a stochastic planning problem and
invokes the deterministic planner ff  hoffmann   nebel        on the determinized problem  the
determinization used in ff replan is constructed by creating a new deterministic action for each
possible outcome of a stochastic action while ignoring the probability of that outcome happening  this effectively allows the planner to control the randomness in executing actions  making
this determinization a kind of relaxation of the problem  in section      we define a domainindependent heuristic function  the controlled randomness ff heuristic  cr ff   as the deterministic ff heuristic  hoffmann   nebel        computed on the all outcomes ff replan determinization of the probabilistic problem    a variety of relaxations have previously been combined with a
variety of deterministic heuristics in order to apply deterministic planning techniques to stochastic problems  bonet   geffner         more generally  deterministic relaxations provide a general
technique for transferring techniques from deterministic planning for use in solution of stochastic
problems 

   stochastic enforced hill climbing
deterministic enforced hill climbing  deh   hoffmann   nebel        searches for a successor
state of strictly better heuristic value and returns a path from the current state to such a successor 
this path is an action sequence that guarantees reaching the desired successor  we illustrate the
behavior of deh as compared to greedy policy using the example in figure    in a stochastic
environment  there may be no single better descendant that can be reached with probability one 
since actions may have multiple stochastic outcomes  if we simply use breadth first search as in
deh to find a single better descendant and ignore the other possible outcomes  we might end up
selecting an action with very low probability of actually leading to any state of better heuristic value 
as illustrated in figure    as shown in this figure  our algorithm  stochastic enforced hill climbing
 seh   accurately analyzes the probabilistic dynamics of the problem of improving the heuristic
value 
in this section  we give details of seh  we note that in deh  the local breadth first search
gives a local policy in a state region surrounding the current state in a deterministic environment 
the value of following this policy is the heuristic value of the improved descendant found during
breadth first search  in seh  we implement these same ideas in a stochastic setting 
   the deterministic ff heuristic  described in the work of hoffmann and nebel         from ff planner version    
available at http   www loria fr hoffmanj ff html  efficiently computes a greedy plan length in a problem relaxation
where state facts are never deleted  the plan found in the relaxed problem is referred to as a relaxed plan for the
problem 

   

fiw u   k alyanam     g ivan

h  

h  

h  

h  

h  
h  

h  

h  

h  

h  

h  

h  
h  

h  

h     

h     

 a  behavior of greedy policy 

 b  behavior of deh 

figure    comparison between the behavior of deh and greedy policy when a local optimum is
encountered  the solid black circle represents the current state  and the shaded circle represents the
goal state  with heuristic value zero   in  a  the greedy policy keeps selecting actions indicated by
the wide arrow and cannot reach the goal state  on the other hand  deh uses breadth first search
and finds the goal state that is two steps away from the current state  as shown in  b  

h  
h  
h  
h  

h  

p     
p     

h  

h  

h  

h     

p     

h  

h  

p     
h  

 a  behavior of deh in stochastic environments 

h  

h  

h  

h     

 b  behavior of seh in stochastic environments 

figure    comparison between the behavior of seh and deh in a stochastic example  we assume
deh first determinizes the problem  creating one deterministic action for each possible stochastic
outcome  the solid black circle represents the current state  and the shaded circle represents the
goal state  with heuristic value zero   in  a  deh looks one step ahead and selects the action drawn
with double lines  as one of the outcomes leads to a state with h      which is better than the current
state  however  this action choice has a higher probability of going to the state with h      than
the one with h      in  b  seh first decides there is no policy with better value than   when the
horizon in the mdp only includes states reachable from the current state in one step  seh then
extends the horizon to two so that all states are considered  it then selects the actions indicated in
the wide arrows that lead to the goal state 

   

fis tochastic e nforced h ill  c limbing

online planning using a local planner
   repeat
  
s  current state
  
local  find local policy s h 
  
follow local until a is selected
   until the goal is reached
table    pseudo code for an online planning framework  the policy local may be non stationary 
in which case the local planner also returns the initial horizon for execution of the policy and termination in line   can also happen by reaching that specified horizon 

we present seh in two steps  first  we present a simple general framework for online planning that repeatedly calls a local planner that selects a policy around the current state  second 
we present a local planner based on the enforced hill climbing idea  when the online planning
framework is instantiated with this local planner  the resulting algorithm is seh  the combination
of these two steps constitute the central algorithmic contribution of this paper  finally  we present
some analytical properties of our algorithm 
    a simple online planning framework
a familiar direct approach to online planning is to call the planner at the current state and have the
planner select an action  that action is then executed in the environment  resulting in a new current
state  this process can then be repeated 
here  we present a simple generalization of this approach that allows the planner to select more
than one action during each call  before any action is executed  the idea is that the planner makes a
plan for the local context surrounding the current state  and then that plan is executed until the local
context is exited  when the local context is exited  we have a new current state and the process is
repeated 
more formally  we augment the action space with a new terminate action  called a    indicating that the planned for local context has been exited  we then define a local policy around a state
s to be a partial mapping from states to the augmented action space that is defined at s and at every
state reachable from s under the policy    an online planner can then be built by repeatedly seeking
and executing a local policy around the current state using a planning subroutine  the local policy
is executed until the terminate action is called  which has no effect on the state   at which point a
new local policy must be sought  these ideas are reflected in the pseudo code shown in table   
we note that the notion of local context in our discussion here is informal  the precise
notion is given by the use of the terminate action  a local policy is executed until it selects the
terminate action  the find local policy routine is free to use any method to decide when a state
will be assigned the terminate action  previously published envelope methods  dean  kaelbling 
kirman    nicholson        provide one way to address this issue  so that termination will be
assigned to every state outside some envelope of states  however  this framework is more general
than envelope methods  and allows for local policies that are not selected based upon pre existing
   the local policy returned can be non stationary and finite horizon  but must then select the terminate action at the
final stage  in all reachable states 

   

fiw u   k alyanam     g ivan

envelopes of states  though we can always  post planning  interpret the set of reachable states as
an envelope   the general intuition is that selecting the action for the current states may involve
analysis that is sufficient to select actions for many surrounding states  so our framework allows the
find local policy routine to return a policy specifying all such action selections 
also  we note that this online planning framework includes recent re planners such as ffreplan  yoon et al         and rff  teichteil konigsbuch et al          however  replanning
because the current plan failed  e g  because the determinization used to generate it was naive 
is quite different in character from seh  which constructs plans to improve the heuristic value  and
replans each time such a plan terminates  thus  seh uses the heuristic function to define subgoals
and plan for the original goal incrementally 
it remains to present the local planner we combine with this online planning framework to
define stochastic enforced hill climbing  our local planner analyzes the mdp problem around the
current state  but with the heuristic function integrated into the problem so as to embody the subgoal
of improving on the heuristic value of the current state  we describe this simple integration of the
heuristic function into the problem next  and then discuss the local planner based on this integration 
    heuristic based markov decision processes
our method relies on finite horizon analyses of a transformed mdp problem  with increasing horizons  we transform the mdp problem with a novel heuristic achievement transform before analysis
in order to represent the goal of finding and executing a policy that expects to improve on the initial
 current  states heuristic value 
the heuristic achievement transform is very straightforward  and applies to any goal oriented
mdp problem  first  all action costs are removed from the problem  second  the terminate action
a is assigned the action cost h s  and transitions deterministically to the absorbing state   when
a policy is executed  the selection of the action a at any state will result in replanning  as discussed
in the online planning framework just presented  the actions a can be thought of as heuristic
achievement actions  allowing the immediate achievement of the value promised by the heuristic
function 
analyzing the mdp transformed by the heuristic achievement transform at a finite horizon
around s  represents the problem of finding a policy for improving on the heuristic value of s 
without regard to the cost of achieving such improvement in the heuristic  allowing the heuristic
achievement action a to be selected at any point at any state reflects the greedy nature of this goal 
the planner is not forced to look further once an improvement is found  so long as there is a policy
from the initial state that expects to see improvement 
formally  given mdp m    s  a  c  t  s    and non negative heuristic function h   s  r  the
heuristic achievement transform of m under h  written mh   is given by  s  a   c    t    s     where
a   c    and t  are as follows  let s  s    and s  be arbitrary states from s  we define a  s  to be
a s    a    and we take c   s    a  s        and t   s    a  s      t  s    a  s    for each a  a s    
finally  we define t   s  a         and c   s  a       h s  
the transformed mdp will have zero cost policies at all states  and as such is not of immediate use  however  policies that are required to select a as their final action  at horizon one 
represent policies that are seeking to get to regions with low heuristic value  whatever the cost 
an increasing horizon search for such policies corresponds roughly to a breadth first search for an
improved heuristic value in deterministic enforced hill climbing  formally  we define the class of
   

fis tochastic e nforced h ill  c limbing

heuristic achievement policies h as the class of policies  s  k  that satisfy  s       a for all s 
we define jh  s  k  to be the value minh j   s  k  in the heuristic transform mdp for h and h to
be a policy that achieves this value  we note that  due to the zeroing of non terminal action costs 
jh  s  k  represents the expected heuristic value achieved at the next execution of a   where a is
required at horizon k if not before  formally  if we define the random variable s to be the state at
which h first executes a in a trajectory from s  we have jh  s  k    e h s    
the rough motivation for setting action costs to zero for the analysis of the heuristic based
mdp is that such actions are being considered by our method to remediate a flawed heuristic  the
cumulative action cost required to reach a state of improved heuristic value is a measure of the
magnitude of the flaw in the heuristic  here  we remove this cost from the analysis in order to
directly express the subgoal of reaching a state with lower heuristic value  including action costs
might  for example  lead to preferring cheap paths to higher heuristic values  i e   to states worse
than s    when expensive paths to lower heuristic values have been found  the basic motivation
for enforced hill climbing is to strongly seek improved heuristic values  instead of diluting this
subgoal by adding in action costs  our methods seek the shortest path to a heuristic improvement
by analyzing the heuristic based mdp with an iteratively deepened finite horizon  as discussed in
the next subsection  this approach is most reasonable in settings where each action has the same
cost  so that the finite horizon value iteration is a stochastic setting analogue to uniform cost search 
in settings with varying action cost  future work is needed to adapt seh to usefully consider that
cost without excessively diluting the focus on improving the heuristic 
      h euristic achievement value i teration
following the formalism of value iteration in section      we compute jh  s  k  with heuristic
achievement value iteration as follows 
jh  s       h s   and
jh  s  k    min q s  a  jh    k      for k    
aa  s 

a non stationary policy achieving the cost to go given by jh    k  can also be computed using the
following definition 
h  s       a   and
h  s  k    argminaa  s  q s  a  jh    k      for k    
note that q   is computed on the heuristic achievement transformed mdp mh in both equations 
for technical reasons that arise when zero cost loops are present  we require that tie breaking in the
argmin for h  s  k  favors the action selected by h  s  k     whenever it is one of the options  this
is to prevent the selection of looping actions over shorter  more direct routes of the same value 
    a local planner
we consider a method to be stochastic enforced hill climbing if it uses an online planning framework  such as that presented in table    together with a local policy selection method that solves
the heuristic achievement mdp  exactly  approximately  or heuristically   here  we describe one
straightforward method of local policy selection by defining seh find local policy using finitehorizon value iteration  this method generalizes the breadth first search used in deterministic
   

fiw u   k alyanam     g ivan

enforced hill climbing  and seeks an expected heuristic improvement rather than a deterministic
path to an improved heuristic value  more sophisticated and heuristic methods than finite horizon
value iteration should be considered if the implementation presented here finds the local mdp problems intractable  our analytical results in section     apply to any method that exactly solves the
heuristic achievement mdp  such as the method presented in table    our experimental results are
conducted using the implementation in table   as well 
we present pseudo code for seh find local policy in table    a heuristic function h respects
goals if h s      iff s  g  the algorithm assumes a non negative heuristic function h   s  r
that respects goals  as input  seh find local policy s   h  returns the policy h and a horizon k 
the policy is only computed for states and horizons needed in order to execute h from s  using
horizon k until the policy terminates 
thus  in lines   to    of table     heuristic achievement value iteration is conducted for increasing horizons around s    seeking a policy improving h s     note that for a given horizon k      only
states reachable within k steps need to be included in the value iteration 
      e arly t ermination
the primary termination condition for repeated local policy construction is the discovery of a policy
improving on the heuristic estimate of the initial state  as discussed below in proposition    in
domains without deadends  seh find local policy will always find a policy improving on h s    
given sufficient resources 
however  for badly flawed heuristic functions the large enough horizons that are analyzed in
seh find local policy may be unacceptably large given resource constraints  moreover  in domains with unavoidable deadends  there may be no horizon  however large  with a policy improving
on the heuristic at the initial state  for these reasons  in practice  the algorithm stops enlarging the
horizon for heuristic based mdp analysis when user specified resource limits are exceeded 
when horizon limited analysis of the heuristic transform mdp construction does not yield the
desired results inexpensively  biased random walk is used to seek a new initial state  as an example  consider a problem in which the provided heuristic labels all states reachable in k steps
with cost to go estimates that are very similar  forming a very large plateau  if analysis of this large
plateau exceeds the resources available  biased random walk is indicated  for lack of useful heuristic
guidance 
so  once a horizon k is found for which jh  s    k    h s     the system executes h from s 
at horizon k until the terminate action a is selected  if the resource limit is exceeded without
finding such a horizon  the system executes a biased random walk of length   with the terminating
action a imposed in all states s reachable by such biased random walk with h s    h s     this
additional biased random walk allows our method to retain some of the beneficial properties of
random exploration in domains where heuristic flaws are too large for mdp analysis  the resource
consumption threshold at which random walk is triggered can be viewed as a parameter controlling
the blend of random walk and mdp based search that is used in overcoming heuristic flaws  we
currently do not have a principled way of analyzing the tradeoff between resource consumption and
the cost of switching to biased random walk  or determining when to do such switching  instead 
we use domain independent resource limits as described in section      which are determined after
some experimentation 
   

fis tochastic e nforced h ill  c limbing

seh find local policy s   h 
  
s  is the current state 
  
h   s      r the heuristic function  extended with h       
  
we assume global variable mh is the heuristic achievement
transform of the original problem m under h 
  
we seek a policy in the problem mh achieving cost less than h s    
   k    
   repeat
  
k  k  
  
   compute jh  s    k  in mh using value iteration
  
jh         h    h         a   n    
  
repeat
  
n n  
  
for each s reachable from s  in mh using at most k  n steps
  
jh  s  n    minaa  s  q s  a  jh    n     
   
h  s  n    argminaa  s  q s  a  jh    n     
   
until n   k
    until jh  s    k    h s    or resource consumption exceeds user set limits
    if jh  s    k    h s   
   
then
   
return h and horizon k
   
else
   
   return a  step biased random walk policy 
   
   note  implementations should compute  lazily online
   
for n     to 
   
for each state s
   
if h s    h s   
   
then
   
 s  n  selects a with probability one
   
else
   
 s  n  selects each action a  a s  with probability
   

p

eq s a h 
ai a s   e

q s ai  h   

return  and horizon 

table    pseudo code for the local planner used to implement stochastic enforced hill climbing 

   

fiw u   k alyanam     g ivan

horizon limited analysis of the heuristic transform mdp may also terminate without finding
a horizon k such that jh  s    k    h s    when the entire reachable statespace has been explored 
in the presence of deadends  this may happen without exceeding the available resources  and in
this case we fall back to a fixed number of iterations of standard vi on the original mdp model
 including action costs and without the heuristic transform  for the reachable states 
    analytical properties of stochastic enforced hill climbing
in deterministic settings  given sufficient resources and no dead ends  enforced hill climbing can
guarantee finding a deterministic path to an improved heuristic value  if nothing else  a goal state
will suffice   given the finite state space  this guarantee implies a guarantee that repeated enforced
hill climbing will find the goal 
the situation is more subtle for stochastic settings  in a problem with no dead ends  for every
state s the optimal policy reaches the goal with probability one  it follows that in such problems  for
any h assigning zero to every goal state  for every state s and real value       there is some horizon
k such that jh  s  k       recall that jh analyzes the heuristic transform mdp wherein action costs
are dropped except that h   must be realized at horizon one   because seh find local policy s h 
considers each k in turn until jh  s  k    h s  we then have 

proposition    given non goal state s  no dead ends  non negative heuristic function
h   s  r respecting goals  and sufficient resources  the routine seh find localpolicy s h  returns the policy h and a horizon k with expected return jh  s  k  strictly
less than h s  
however  unlike the deterministic setting  the policy found by the routine seh find local policy
only expects some improvement in the heuristic value  so particular executions of the policy from
the current state may result in a degraded heuristic value 
here  we prove that even in stochastic settings  in spite of this possibility of poor results from
one iteration  seh will reach the goal region with probability one  in the absence of dead end states
and with sufficient resources  in practice  the provision of sufficient resources is a serious hurdle 
and must be addressed by providing a base heuristic with modest sized flaws 

theorem    in dead end free domains  with unbounded memory resources  seh reaches the goal region with probability one 
proof  let x    x    x            xm         be random variables representing the sequence of
states assigned to s in line   of table   when we execute seh on a planning problem 
with x  being the initial state sinit   if the algorithm achieves x  g for some    and
thus terminates  we take xj     xj for all j      note that as a result xj  g implies
xj    g  where g is the goal region of states  
we show that for arbitrary m     the probability that xm is in the goal region is
h sinit  
at least    m
  for a real value      defined below  this expression goes to one
   

fis tochastic e nforced h ill  c limbing

as m goes to infinity  and so we can conclude that seh reaches the goal region with
probability one 
by proposition    from any non goal state s  absent dead ends and with sufficient
resources  one iteration of seh is guaranteed to return a policy for some finite horizon
ks with value jh  s  ks   improving on h s   let s   h s   jh  s  ks       be the value
of the improvement from s at horizon ks   because there are finitely many non goal
states  there exists    minssg s     such that the improvement h s   jh  s  ks  
is at least   consider an arbitrary i such that xi 
  g  noting that jh  xi   kxi    
e h xi      due to zero action costs in mh   it follows immediately then that e h xi   
h xi     xi 
  g     where g is the goal region of states  using that xi  g implies
both xi    g and h xi     h xi          we write this as
e h xi    h xi     
 e h xi    h xi     xi 
  g qi
  e h xi    h xi     xi  g     qi  

   

qi   for      
defining qj to be the probability that xj 
  g 
now  we lower bound the expected heuristic improvement e h x     h xm    after m calls to seh find local policy  for m      we can decompose this expected
improvement over m calls to seh find local policy as the sum of the expected improvements for the individual calls  then  lower bounding this sum using its smallest
term  we get
e h x     h xm   
 


m 
x

i  
m 
x

e h xi    h xi     
   
qi   from inequality   

i  

 mqm  
as qm is non increasing  since xm   g implies xm  g 
next  we combine this lower bound with the natural upper bound h sinit    since h
is assumed to be non negative  so e h x     h xm     h sinit    and x    sinit   thus 
h sinit    qm m 
h s

 

init   converging to zero with
therefore the probability qm that xm 
  g is at most m
large m and so seh reaches the goal with probability one 

while the theorem above assumes the absence of dead ends  problems with dead ends are covered by this theorem as well if the dead ends are both avoidable and identified by the heuristic 
specifically  we may require that the heuristic function assigns  to a state if and only if there
is no policy to reach the goal from that state with probability one  in this case  the problem can
be converted to the form required by our theorem by simply removing all states assigned  from
consideration  either in pre processing or during local mdp construction  
   

fiw u   k alyanam     g ivan

    variants and extensions of seh
seh is based on finite horizon analysis of the mdp transformed by the heuristic achievement transform around the current state s    the particular heuristic achievement transform we describe is of
course not the only option for incorporating the heuristic in a local search around s    while we
have already considered a number of related alternatives in arriving at the choice we describe  other
options can and should be considered in future research  one notable restriction in our transform
is the removal of action costs  which is discussed in section      it is important for the method
to retain the actual heuristic value in the analysis so that it can trade off large  small  positive and
negative changes in heuristic value according to their probabilities of arising  for this reason  we
do not have the heuristic transform abstract away from the value and simply assign rewards of   or
  according to whether the state improves on h s     our choice to remove action costs during local
expansion can lead to poor performance in domains with flawed heuristics interacting badly with
high variations in action costs  this is a subject for future research on the method 
also  the mdp models we describe in this paper are limited in some obvious ways  these
limitations include that the state space is discrete and finite  the problem setting lacks discounting 
and the objective is goal oriented  we have yet to implement any extension to relax these limitations 
and leave consideration of the issues that arise to future work  we note that it would appear that
the method is fundamentally goal oriented  given the goal of repeatedly reducing the heuristic value
of the current state  however  it is possible to contemplate infinite horizon discounted non goaloriented variants that seek policies that maintain the current heuristic estimate 
    incorporating ff goal ordering techniques in seh
the planner ff contains heuristic elements inspired by ordering issues that arise in the blocksworld
problems  hoffmann   nebel         these heuristic elements improve performance on the blocksworld problems significantly  to assist in a fair comparison of seh with ff replan  we have
implemented two of the heuristic elements  namely goal agenda and added goal deletion  in a variant
of seh that we call seh   
the implementation of seh  is as follows  the stochastic planning problem is first determinized using the all outcomes determinization described in section      the goal agenda technique of ff is then invoked on the determinized problem to extract a sequence of temporary goals
g            gm   where each gi is a set of goal facts and gm is the original problem goal  seh with a
stochastic version of added goal deletion  described next in this subsection  is then invoked repeatedly to compute a sequence of states s            sm   where s  is the initial state and for i     each si
is defined as the state reached by invoking seh in state si  with goal gi  thus satisfying gi   
added goal deletion is the idea of pruning the state search space by avoiding repetitive addition
and deletion of the same goal fact along searched paths  in ff  for a search state s  if a goal fact
is achieved by the action arriving at s  but is deleted by an action in the relaxed plan found from s 
then s is not expanded further  hoffmann   nebel        
for our stochastic adaptation of added goal deletion  we define the set of facts added by any
state transition  s  a  s   to be those facts true in s but not in s and represent it as the set difference
s  s  then  the set of added goal facts for the transition are those added facts which are also true in
the current temporary goal gi   i e    s  s   gi   we prune any state transition  s  a  s   whenever
the relaxed plan computed from s to the current temporary goal gi contains an action that deletes
any of the added goal facts  a transition  s  a  s   is pruned by modifying the bellman update at
   

fis tochastic e nforced h ill  c limbing

state s so that s contributes the dead end state value  v   to the q value for a at s  weighted by
the transition probability  instead of contributing the cost to go at s    more formally  we define a
modified q function when using added goal deletion  qagd  s  a  j  as follows 
 
   if f   s  s   gi is deleted by an action in relaxed plan s  gi   

i s    
   otherwise
x
qagd  s  a  j   
t  s  a  s   i s  v       i s   j s   
s

qagd    then replaces q   in the definition of the cost to go function jh    in section      also 
reachability in line   of table   does not use pruned transitions 
in some problems  subsequent deletion of newly added goals is unavoidable for any valid plan 
added goal deletion prunes all routes leading to the goal region for such problems even though
no actual deadend is present  hence  this is an incomplete technique as discussed in the work
of hoffmann and nebel         ff falls back to best first search if deh is not able to find a valid
plan due to pruning  similarly  when unable to find an improved policy  seh falls back to either
value iteration or biased random walk as described in section     
preliminary exploration of incorporating stochastic variants of ffs helpful action pruning
 hoffmann   nebel        into seh did not improve performance  much like the effect of added
goal deletion on all domains except the blocksworld    as a result  we do not report on helpfulaction pruning methods here 

   related work
in this section we discuss planning techniques that are close to our work in one or more dimensions 
    fast foward  ff  planner and deterministic enforced hill climbing
for an introduction to deterministic enforced hill climbing  deh  and its relation to our technique 
please see section    here  we additionally note that there are several lines of work that directly
extend the ff planner to allow planning with numeric state variables  hoffmann        and planning with uncertainty  hoffmann   brafman              domshlak   hoffmann         although
these techniques involve significant changes to the computation of the relaxed plan heuristic and the
possible addition of the use of belief states to handle uncertainty  enforced hill climbing is still the
primary search technique used in these lines of work  we note that although in the work of domshlak and hoffmann actions with probabilistic outcome are handled  the planner  probabilistic ff 
is designed for probabilistic planning with no observability  whereas our planner is designed for
probabilistic planning with full observability 
    envelope based planning techniques
stochastic enforced hill climbing dynamically constructs local mdps to find a local policy leading
to heuristically better state regions  the concept of forming local mdps  or envelopes  and using
   relaxed plan s  gi   computes the relaxed plan between states s and gi as defined in the work of hoffmann and
nebel        using the all outcomes problem determinization defined in section     
   we explored ideas based on defining the helpfulness of an action to be the expectation of the helpfulness of its
deterministic outcomes 

   

fiw u   k alyanam     g ivan

them to facilitate probabilistic planning has been used in previous research such as the work of bonet
and geffner         and dean et al          which we briefly review here 
the envelope based methods in the work of dean et al         and gardiol and kaelbling       
start with a partial policy in a restricted area of the problem  the envelope   then iteratively improves the solution quality by extending the envelope and recomputing the partial policy  the
typical assumption when implementing this method is that the planner has an initial trajectory from
the starting state to the goal  generated by some stochastic planner  to use as the initial envelope 
another line of work  including rtdp  barto  bradtke    singh         lao   hansen  
zilberstein         and ldfs  bonet   geffner         starts with an envelope containing only the
initial state  and then iteratively expands the envelope by expanding states  states are expanded
according to state values and dynamic programming methods are used to backup state values from
newly added states  until some convergence criterion is reached  stochastic enforced hill climbing
can be viewed as repeatedly deploying the envelope method with the goal  each time  of improving
on the heuristic estimate of distance to go  for a good h function  most invocations result in trivial
one step envelopes  however  when local optima or plateaus are encountered  the envelope may
need to grow to locate a stochastically reachable set of exits 
all of the above referenced previous search methods have constructed envelopes seeking a high
quality policy to the goal rather than our far more limited and relatively inexpensive goal of basin
escape  our results derive from online greedy exploitation of the heuristic rather than the more
expensive offline computation of converged values proving overall  near  optimality  ldfs  for
example  will compute check values for at least all states reachable under the optimal policy  even
if given j  as input  and possibly vastly many others as well during the computation 
some of these previous methods are able to exploit properties  such as admissibility  of the
heuristic function to guarantee avoiding state expansions in some regions of the state space  clearly 
seh exploits the heuristic function in a way that can avoid expanding regions of the statespace 
however  we have not at this point conducted any theoretical analysis of what regions can be guaranteed unexpanded for particular kinds of heuristic  and such analyses may be quite difficult 
    policy rollout
the technique of policy rollout  tesauro   galperin        bertsekas   tsitsiklis        uses a
provided base policy  to make online decisions  the technique follows the policy greedy vf   
where vf is computed online by sampling simulations of the policy  
the computation of the optimal heuristic transform policy h in seh has similarities to policy
rollout  in each case  online decisions are made by local probabilistic analysis that leverages provided information to manage longer range aspects of the local choice  for seh  a heuristic function
is provided while  for policy rollout  a base policy is provided  in this view  policy rollout does local
analysis under the assumption that non local execution will use the base policy   whereas seh
does local analysis under the assumption that non local execution will achieve the base heuristic
cost estimate h 
in fact  for our goal oriented setting  when the provided heuristic function h is stochastic  a simple generalization of what we describe in this paper   and equal to a sampled simulation evaluation
of v  for some policy   then seh executes the same policy as policy rollout  assuming uniform
action costs and sufficient sampling to correctly order the action choices  this claim follows because when h   v  there is always some action to yield an expected improvement in h in one step 
   

fis tochastic e nforced h ill  c limbing

in our goal oriented setting  the need for uniform action costs in this claim may be relaxed if a
variant of seh is developed that retains action costs in the heuristic transform 
in policy rollout  only horizon one greedy use of the sampled heuristic is needed  but the main
substance of seh is to enable the repair and use of heuristic functions with flaws that cannot be
repaired at horizon one  thus the central differences between the techniques are reflected in the
ability of seh to leverage arbitrary heuristic functions and repair flaws in those functions at larger
horizons 
policy rollout provides an elegant guarantee that the online policy selected improves on the base
policy  given sufficient sampling  this result follows intuitively because the computed policy is the
policy iteration improvement of the base policy  unfortunately  no similar guarantee is known to
apply for seh for an arbitrary heuristic function  however  policy rollout cannot be used to improve
an arbitrary heuristic function either 
    local search in optimization
stochastic enforced hill climbing can be regarded as one of many local search techniques designed
to improve on greedy one step lookahead  the most naive form of local search optimization  here
we briefly discuss connections to the method of simulated annealing  one of a large family of related
local search techniques  for more detail  please see the work of aarts and lenstra        
simulated annealing  kirkpatrick et al         cerny        allows the selection of actions with
inferior expected outcome with a probability that is monotone in the action q value  the probability
that an inferior action will be selected often starts high and decreases over time according to a cooling schedule  the ability to select inferior actions leads to a non zero probability of escaping local
optima  however  this method does not systematically search for a policy that does so  in contrast 
stochastic enforced hill climbing analyzes a heuristic based mdp at increasing horizons to systematically search for policies that give improved expected value  hence leaving the local extrema   in
our substantial preliminary experiments  we could not find successful parameter settings to control
simulated annealing for effective application to online action selection in goal directed stochastic
planning  to our knowledge  simulated annealing has not otherwise been tested on direct forwardsearch action selection in planning  although variants have been applied with some success to other
planning as search settings  selman  kautz    cohen        kautz   selman        gerevini  
serina        such as planning via boolean satisfiability search 

   setup for empirical evaluation
here  we describe the parameters used in evaluating our method  the heuristics we will test the
method on  the problem categories in which the tests will be conducted  the random variables for
aggregated evaluation  and issues arising in interpreting the results and their statistical significance 
we run our experiments on intel xeon    ghz machines with     mhz bus speed and    kb
cache 
    implementation details
if at any horizon increase no new states are reachable  our implementation of seh simply switches
to an explicit statespace method to solve the mdp formed by the reachable states  more specifically 
   

fiw u   k alyanam     g ivan

if the increase in k at line   in table   does not lead to new reachable states in line    we trigger
value iteration on the states reachable from s   
throughout our experiments  the thresholds used to terminate local planning in line    of table  
are set at          states and one minute  we set the biased random walk length  to ten  this work
makes the assumption that the heuristic functions used assign large values to easily recognized deadends  as hill climbing works very poorly in the presence of dead end attractor states  we enforce
this requirement by doing very simple dead end detection on the front end of each heuristic function
 described next in section     for each heuristic  and assigning the value          to recognized
dead end states 
we denote this implementation running on heuristic h with seh h  
    heuristics evaluated
we describe two different types of heuristic functions used in our evaluation and the associated
dead end detection mechanisms 
      t he c ontrolled  r andomness ff h euristic
for use in our evaluations  we define a domain independent heuristic function  the controlledrandomness ff heuristic  cr ff   we define cr ff on a state s to be the ff distance to goal
estimate  hoffmann   nebel        of s computed on the all outcomes determinization as described
in section      we denote the resulting heuristic function f   while computing the cr ff heuristic 
we use the reachability analysis built into the ff planner for the detection of deadends 
      l earned h euristics
we also test stochastic enforced hill climbing on automatically generated heuristic functions from
the work of wu and givan         which on their own perform at the state of the art when used to
construct a greedy policy  we shift these heuristic functions to fit the non negative range requirement of h discussed previously  these learned heuristic functions are currently available for only
seven of our test categories  and so are only tested in those categories 
we note that these heuristics were learned for a discounted setting without action costs and
so are not a direct fit to the distance to go formalization adopted here  we are still able to get
significant improvements from applying our technique  we denote these heuristics l  only states
with no valid action choice available are labeled as deadends when applying seh to the learned
heuristics 
    goals of the evaluation
our primary empirical goal is to show that stochastic enforced hill climbing generally improves
significantly upon greedy following of the same heuristic  using the policy greedy h  as described
in the technical background above   we will show that this is true for both of the heuristics defined in
section      we show empirically the applicability and limitation of seh discussed in section     
in different types of problems including probabilistically interesting ones  little   thiebaux        
a secondary goal of our evaluation is to show that for some base heuristics the resulting performance is strong in comparison to the deterministic replanners ff replan  yoon et al         and
rff  teichteil konigsbuch et al          while both ff replan and rff use the fast forward  ff 
   

fis tochastic e nforced h ill  c limbing

base planner  rff uses a most probable outcome determinization in contrast to the all outcomes
determinization used by ff replan  the primary other difference between rff and ff replan is
that before executing the plan  rff grows policy trees to minimize the probability of having to
replan  while ff replan does not 
    adapting ippc domains for our experiments
we conduct our empirical evaluation using all problems from the first three international probabilistic planning competitions  ippcs  as well as all twelve probabilistically interesting problems
from the work of little and thiebaux         we omit some particular problems or domains from
particular comparisons for any of several practical reasons  detailed in an online appendix 
because enforced hill climbing is by nature a goal oriented technique and seh is formulated
for the goal oriented setting  we ignore the reward structure  including action and goal rewards 
in any of the evaluated problems and assume an uniform action cost of one for those problems 
transforming any reward oriented problem description into a goal oriented one 
we provide detailed per problem results in an online appendix for each planner evaluated in this
work  however  in support of our main conclusions  we limit our presentation here to aggregations
comparing pairs of planners over sets of related problems  for this purpose  we define seventeen
problem categories and aggregate within each problem category  while some categories are single
domains  more generally  multiple closely related domains may be aggregated within a single category  for example  the blocksworld category aggregates all blocksworld problems from the three
competitions  even though the action definitions are not exactly the same in every such problem  for
some paired comparisons  we have aggregated the results of all problems labeled as or constructed
to be probabilistically interesting by the ippc  organizers or by the work of little and thiebaux
       into a combined category pi problems 
in table    we list all evaluated categories  including the combined category pi problems  
as well as the planning competitions or literature the problems in each category are from  the
evaluated problems in each category are identified in an online appendix 
the reward oriented s ysadmin domain from ippc  was a stochastic longest path problem
where best performance required avoiding the goal so as to continue accumulating reward as long
as possible  bryce   buffet          note that contrary to the organizers report  the domains goal
condition is all servers up rather than all servers down   our goal oriented adaptation removes
the longest path aspect of the domain  converting it to a domain where the goal is to get all the
servers up 
the b locksworld problems from ippc  contain flawed definitions that may lead to a block
stacking on top of itself  nevertheless  the goal of these problems is well defined and is achievable
using valid actions  hence the problems are included in the b locksworld category 
we have discovered that the five rectangle tireworld problems  p   to p   from ippc   t ireworld  have an apparent bugno requirement to remain alive is included in the goal condition  the domain design provides a powerful teleport action to non alive agents intended only to
increase branching factor  buffet         however  lacking a requirement to be alive in the goal 
this domain is easily solved by deliberately becoming non alive and then teleporting to the goal  we
have modified these problems to require the predicate alive in the goal region  we have merged
these modified rectangle tireworld problems with triangle tireworld problems from both ippc  and
   

fiw u   k alyanam     g ivan

category

problem source s 

b locksworld

ippc   ippc   ippc 

b oxworld

ippc   ippc 

b usfare

little and thiebaux       

d rive

ippc 

e levator

ippc 

e xploding b locksworld

ippc   ippc   ippc 

f ileworld

ippc 

p itchcatch

ippc 

r andom

ippc 

r iver

little and thiebaux       

s chedule

ippc   ippc 

s earch and r escue

ippc 

s ysadmin

ippc 

s ystematic   tire

triangle tireworld  ippc    tireworld p  to p    little and thiebaux         
rectangle tireworld  ippc    tireworld p   to p    with bug fixed

t ireworld

ippc   ippc 

t owers of h anoi

ippc 

z enotravel

ippc   ippc 

pi problems

b usfare  d rive  e xploding b locksworld
p itchcatch  r iver  s chedule  s ystematic   tire  t ireworld

table    list of categories and the planning competitions or literature from which the problems in
each category are taken 

   

fis tochastic e nforced h ill  c limbing

the work of little and thiebaux        into a category s ystematic   tire  as these problems have
been systematically constructed to emphasize pi features 
    aggregating performance measurements
for our experiments  we have designed repeatable aggregate measurements that we can then sample
many times in order to evaluate statistical significance  we now define the random variables representing these aggregate measurements and describe our sampling process  as well as our method for
evaluating statistical significance 
      d efining and s ampling aggregate  m easurement r andom variables
for each pair of compared planners  we define four random variables representing aggregate performance comparisons over the problems in each category  each random variable is based upon a
sampling process that runs each planner five times on all problems in a category  and aggregates the
per problem result by computing the mean  we use five trial runs to reduce the incidence of lowsuccess planners failing to generate a plan length comparison  each mean value from a five trial run
is a sample value of the respective random variable 
first  the per problem success ratio  sr  is the fraction of the five runs that succeed for each
problem  the success ratio random variable for each category and planner is then the mean sr
across all problems in the category 
second  the per problem successful plan length  slen  is the mean plan length of all successful
runs among the five runs  in order to compare two planners on plan length  we then define the perproblem ratio of jointly successful plan lengths  jslen ratio  for the two compared planners as
follows  if both planners have positive sr among the five trials on the problem  jslen ratio is
the ratio of the slen values for the two planners  otherwise  jslen ratio is undefined for that
problem  we use ratio of lengths to emphasize small plan length differences more in short solutions
than in long solutions  and to decrease sensitivity to the granularity of the action definitions 
the mean jslen ratio random variable for each category and pair of planners is then the
geometric mean of the jslen ratio across all problems in the category for which jslen ratio
is well defined  in this manner we ensure that the two planners are compared on exactly the same set
of problems  note then that  unlike sr  jslen ratio depends on the pair of compared planners 
rather than being a measurement on any single planner  it is the ratio of successful plan length on
the jointly solved problems for the two planners 
similarly  the per problem ratio of jointly successful runtimes  jstime ratio  is defined in
the same manner used for comparing plan lengths  the mean jstime ratio is again computed
as the geometric mean of well defined per problem jstime ratio values 
because jslen ratio and jstime ratio are ratios of two measurements  we use the geometric mean to aggregate per problem results to generate a sample value  whereas we use arithmetic
mean for the sr variables  note that geometric mean has the desired property that when the planners are tied overall  so that the geometric mean is one   the mean is insensitive to which planner is
given the denominator of the ratio 
thus  to draw a single sample of all four aggregate random variables  sr for each planner 
jslen ratio  and jstime ratio  in comparing two planners  we run the two planners on each
problem five times  computing per problem values for the four variables  and then take the  arith   

fiw u   k alyanam     g ivan

metic or geometric  means of the per problem variables to get one sample of each aggregate variable  this process is used repeatedly to draw as many samples as needed to get significant results 
we use a plan length cutoff of      for each attempt  each attempt is given a time limit of   
minutes 
      s ignificance of p erformance d ifferences b etween p lanners
our general goal is to order pairs of planners in overall performance on each category of problem 
to do this  we must trade off success rate and plan length  we take the position that a significant
advantage in success rate is our primary goal  with plan length used only to determine preference
among planners when success rate differences are not found to be significant 
we determine significance for each of the three performance measurements  sr  jslenratio  and jstime ratio  using t tests  ascribing significance to the results when the p value
is less than       the exact hypothesis tested and form of t test used depends on the performance
measurement  as follows 
   sr  we use a paired one sided t test on the hypothesis that the difference in true means is
larger than      
   jslen ratio  we use a one sample one sided t test on the hypothesis that the true geometric mean of jslen ratio exceeds       log of the true mean of jslen ratio exceeds
log        
   jstime ratio  we use a one sample one sided t test on the hypothesis that the true
geometric mean of jstime ratio exceeds       log of the true mean of jstime ratio
exceeds log        
we stop sampling the performance variables when we have achieved one of the following criteria  representing an sr winner is determined or sr appears tied 
   thirty samples have been drawn and the p value for sr difference is below      or above     
   sixty samples have been drawn and the p value for sr difference is below      or above     
   one hundred and fifty samples have been drawn 
in all the experiments we present next  this stopping rule leads to only    samples being drawn
unless otherwise mentioned  upon stopping  we conclude a ranking between the planners  naming a
winner  if either the sr difference or the jslen ratio has p value below       with significant
sr differences being used first to determine the winner  if neither measure is significant upon
stopping  we deem the experiment inconclusive 
combining categories for some of our evaluations  we aggregate results across multiple categories of problem  e g   the combined category pi problems  in such cases  we have effectively
defined one larger category  and all our techniques for defining performance measurements and determining statistical significance are the same as in section      however  we do not actually re run
planners for such combined category measurements  instead  we re use the planner runs used for
the single category experiments  rather than use the stopping rule just described  we compute the
maximum number of runs available in all the combined categories and use that many samples of
   

fis tochastic e nforced h ill  c limbing

the combined category performance measurements  to avoid double counting problem results  we
treat combined categories separately when analyzing the results and counting wins and losses 

   empirical results
we present the performance evaluation of stochastic enforced hill climbing  seh  in this section 
the experiments underlying the results presented here involve         planner runs in    categories 
    summary of comparison
the results in table   show that  for the cr ff heuristic  seh with the goal ordering and addedgoal deletion enhancements  seh   f    improves significantly over the baseline seh technique
 seh f    in the category b locksworld  but does not show significant changes in the aggregated
performance for non blocksworld problems    for the remainder of the experiments involving crff  we evaluate only seh   f    noting that both of our comparison planners  ff replan and rff 
benefit from the goal ordering and added goal deletion enhancements of their base planner  ffplan 
the results we present next for seh   f   show 
 seh   f   significantly outperforms greedy f   in    categories  but is outperformed by
greedy f   in s chedule  there were three categories where the comparison was inconclusive  b usfare  r iver and t ireworld   see table   for details 
 ff replan was inapplicable in two categories  ippc  s earch   and  r escue and ippc 
s ysadmin   seh   f   significantly outperforms ff replan in    categories  but is outperformed by ff replan in three categories  e xploding b locksworld  p itchcatch  and
z enotravel   there were two categories where the comparison was inconclusive  f ile world and r iver    seh   f   also significantly outperforms ff replan on the combined
category pi problems  although the winner varied between the aggregated categories  see
table   for details 
 rff bg was inapplicable in two categories  b usfare and ippc  f ileworld   seh   f  
significantly outperforms rff bg in    categories  but is outperformed by rff bg in two
categories  e xploding b locksworld and s ystematic   tire   there was one category
where the comparison was inconclusive  s ysadmin   seh   f   also significantly outperforms rff bg on the combined category pi problems  although the winner varied between
the aggregated categories  see table   for details 
the learned heuristic from the work of wu and givan        has been computed only in
a subset of the domains  hence only seven categories are applicable for the evaluation using the
learned heuristic  see an online appendix for details   the results we present next for seh with the
learned heuristic  seh l   show 
 seh l  significantly outperforms greedy l  in six categories  there was one category
 t ireworld  where the comparison was inconclusive  see table   for details 
   we show p values rounded to two decimal places  for example  we show p      when the value of p rounded to two
decimal places is   

   

fiw u   k alyanam     g ivan

sr of
sr of
seh   f   seh f  

category

jslenratio
 seh 
seh   

jstimeratio
 seh 
seh   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

n on   blocksworld

    

    

    

    

no

 p      

no

 p      

inconclusive

table    aggregated comparison of seh   f   against seh f   

 seh l  significantly outperforms ff replan in five categories  but is outperformed by ffreplan in two categories  e xploding b locksworld and z enotravel   see table   for
details 
    discussion
we now discuss the results for comparisons between pairs of planners  including seh versus greedy
heuristic following  seh versus ff replan  and seh versus rff bg 
      seh seh  v ersus g reedy
our primary evaluation goal was to show that stochastic enforced hill climbing generally improves
significantly upon greedy following of the same heuristic  using the policy greedy h  as described
in the technical background above   this was demonstrated by evaluating seh with two different
heuristics in tables   and    where seh h  significantly outperforms greedy h  in nineteen out of
twenty four heuristic category pairs  only losing in s chedule for seh   f   against greedy f   
we now discuss the only category where greedy outperforms seh techniques significantly 
in s chedule  there are multiple classes of network packets with different arrival rates  packets have deadlines  and if a packet is not served before its deadline  the agent encounters a classdependent risk of death as well as a delay while the packet is cleaned up  to reach the goal of
serving a packet from every class  the agent must minimize the dropping related risk of dying while
waiting for an arrival in each low arrival rate class  the all outcomes determinization underlying
the cr ff heuristic gives a deterministic domain definition where dying is optional  never chosen 
and unlikely packet arrivals happen by choice  leading to a very optimistic heuristic value  when
using a very optimistic heuristic value  the basic local goal of seh  which is to improve on the
current state heuristic  leads to building very large local mdps for analysis  in the presence of
dead ends  death  as above   even arbitrarily large local mdps may not be able to achieve a local
improvement  and so in s chedule  seh  will typically hit the resource limit for mdp size at
every action step 
in contrast  greedy local decision making is well suited to packet scheduling  many well known
packet scheduling policies  e g  earliest deadline first or static priority in the work of liu  
layland        make greedy local decisions and are practically quite effective  in our experiments 
the greedy policy applied to cr ff benefits from locally seeking to avoid the incidental delays of
dropped packet cleanup  even though the heuristic sees no risk of dying cost to dropping  it still
recognizes the delay of cleaning up lost dropped packets  thus  greedy f   is a class insensitive
   

fis tochastic e nforced h ill  c limbing

jslensr of
ratio
sr of
seh   f   greedy f    greedy 
seh   

category

jstimeratio
 greedy 
seh   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b usfare

    

    

    

    

no

 p      

no

 p      

inconclusive

d rive

    

    

    

    

yes

 p      

yes

 p      

seh   f  

e levator

    

    

    

    

yes

 p      

yes

 p      

seh   f  

e xploding
b locksworld

    

    

    

    

yes

 p      

no

 p      

seh   f  

f ileworld

    

    

    

    

yes

 p      

no

 p      

seh   f  

p itchcatch

    

    





yes

 p      

r andom

    

    

    

    

yes

 p      

yes

 p      

seh   f  

r iver

    

    

    

    

no

 p      

no

 p      

inconclusive

s chedule

    

    

    

    

yes

 p      

yes

 p      

greedy f  

s earch
and r escue

    

    

    

    

no

 p      

yes

 p      

seh   f  

s ysadmin

    

    

    

    

no

 p      

yes

 p      

seh   f  

s ystematic
  tire

    

    

    

    

yes

 p      

no

 p      

seh   f  

t ireworld

    

    

    

    

no

 p      

no

 p      

inconclusive

t owers
h anoi

    

    





yes

 p      

    

    

    

    

yes

 p      

of

z enotravel




yes

 p      

seh   f  

seh   f  
seh   f  

table    aggregated comparison of seh   f   against greedy f    the r iver domain evaluation required extending sampling to    samples as per the experimental protocol described in section        the values and p values of jslen ratio and jstime ratio in p itchcatch and
t owers of h anoi are not available due to the zero success ratio of greedy f   in these categories 

   

fiw u   k alyanam     g ivan

jslensr of
sr of
ratio
seh   f   ff replan  ffr 
seh   f   

category

jstimeratio
 ffr 
seh   f   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b usfare

    

    

    

    

yes

 p      

d rive

    

    

    

    

yes

 p      

yes

 p      

seh   f  

e levator

    

    

    

    

yes

 p      

no

 p      

seh   f  

e xploding
b locksworld

    

    

    

    

no

 p      

yes

 p      

ff replan

f ileworld

    

    

    

    

no

 p      

no

 p      

inconclusive

p itchcatch

    

    

    

    

yes

 p      

yes

 p      

ff replan

r andom

    

    

    

    

yes

 p      

yes

 p      

seh   f  

r iver

    

    

    

    

no

 p      

no

 p      

inconclusive

s chedule

    

    

    

    

yes

 p      

no

 p      

seh   f  

s ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

seh   f  

t ireworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

t owers
h anoi

    

    

    

    

yes

 p      

yes

 p      

seh   f  

z enotravel

    

    

    

    

yes

 p      

yes

 p      

ff replan

pi
p roblems

    

    

    

    

yes

 p      

no

 p      

seh   f  

of



seh   f  

table    aggregated comparison of seh   f   against ff replan  ffr   the r andom and r iver
domains required extending sampling to    samples and the t owers of h anoi domain required
extending sampling to     samples as per the experimental protocol described in section       
the p value of jslen ratio in b usfare is not available because the extremely low success rate
of ffr leads to only one sample of jslen being gathered in    attempts  yielding no estimated
variance 

   

fis tochastic e nforced h ill  c limbing

sr of
sr of
seh   f   rff bg

category

jslenratio
 rff bg 
seh   f   

jstimeratio
 rff bg 
seh   f   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

no

 p      

seh   f  

d rive

    

    

    

    

yes

 p      

no

 p      

seh   f  

e levator

    

    

    

    

no

 p      

yes

 p      

seh   f  

e xploding
b locksworld

    

    

    

    

no

 p      

yes

 p      

rff bg

p itchcatch

    

    





yes

 p      

r andom

    

    

    

    

yes

 p      

yes

 p      

seh   f  

r iver

    

    

    

    

yes

 p      

yes

 p      

seh   f  

s chedule

    

    

    

    

yes

 p      

no

 p      

seh   f  

s earch
and r escue

    

    

    

    

yes

 p      

yes

 p      

seh   f  

s ysadmin

    

    

    

    

no

 p      

no

 p      

inconclusive

s ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

rff bg

t ireworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

t owers
h anoi

    

    

    

    

yes

 p      

yes

 p      

seh   f  

z enotravel

    

    

    

    

yes

 p      

no

 p      

seh   f  

pi
p roblems

    

    

    

    

yes

 p      

yes

 p      

seh   f  

of



seh   f  

table    aggregated comparison of seh   f   against rff bg  the r iver and t owers of
h anoi domains required extending sampling to    samples as per the experimental protocol described in section        the values and p values of jslen ratio and jstime ratio in p itch catch are not available due to the zero success ratio of rff bg in this category 

   

fiw u   k alyanam     g ivan

sr of
seh l 

category

jslensr of
ratio
greedy l   greedy 
seh 

jstimeratio
 greedy 
seh 

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

no

 p      

yes

 p      

seh l 

b oxworld

    

    

    

    

no

 p      

yes

 p      

seh l 

e xploding
b locksworld

    

    

    

    

yes

 p      

no

 p      

seh l 

s ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

seh l 

t ireworld

    

    

    

    

no

 p      

no

 p      

inconclusive

t owers
h anoi

    

    





yes

 p      

    

    

     

    

yes

 p      

of

z enotravel


yes

 p      

seh l 
seh l 

table    aggregated comparison of seh l  against greedy l   the values of jslen ratio and
jstime ratio and p value of jslen ratio in t owers of h anoi are not available due to the
zero success ratio of greedy l  in this category 

sr of
seh l 

category

jslensr of
ratio
ff replan  ffr 
seh l  

jstimeratio
 ffr 
seh l  

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

no

 p      

seh l 

b oxworld

    

    

    

    

no

 p      

yes

 p      

seh l 

e xploding
b locksworld

    

    

    

    

yes

 p      

yes

 p      

ff replan

s ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

seh l 

t ireworld

    

    

    

    

yes

 p      

yes

 p      

seh l 

t owers
h anoi

    

    

    

    

yes

 p      

yes

 p      

seh l 

    

    

    

    

yes

 p      

yes

 p      

ff replan

of

z enotravel

table    aggregated comparison of seh l  against ff replan  ffr  

   

fis tochastic e nforced h ill  c limbing

policy that greedily seeks to avoid dropping  similar to earliest deadline first  the problems
seh encounters in our evaluation in s chedule suggest future work in automatically recognizing
domains where large mdp construction is proving futile and automatically reducing mdp size
limits to adapt performance towards the behavior of a greedy policy  we note that across all tested
benchmark domains and both heuristics  there is only one domain heuristic combination where this
phenomenon arose in practice 
      seh seh  v ersus ff r eplan

and

rff bg

we have also demonstrated performance improvement of seh   f   over the best performing planners in the first three international probabilistic planning competitions  outperforming ff replan in
ten out of fifteen categories while losing in three  e xploding b locksworld  p itchcatch  and
z enotravel   and outperforming rff bg in    out of    categories while losing in e xploding
b locksworld and s ystematic   tire  additionally  seh l  outperforms ff replan in five out
of seven categories while losing in e xploding b locksworld and z enotravel  in this section
we discuss the categories where seh   f   and seh l  lose to ff replan and rff bg 
z enotravel is a logistics domain where people are transported between cities via airplanes
and each load unload fly action has a non zero probability of having no effect  as a result  it takes
an uncertain number of attempts to complete each task  in domains where the only probabilistic effect is a choice between change and no change  the all outcome determinization leads to a
safe determinized plan for ff replanone in which no replanning is needed to reach the goal 
in such domains  including z enotravel  all outcomes determinization can provide an effective
way to employ deterministic enforced hill climbing on the problem  we note though though  that
determinization still ignores the probabilities on the action outcomes  which can lead to very bad
choices in some domains  not z enotravel   while both deterministic and stochastic enforced
hill climbing must climb out of large basins in z enotravel  the substantial overhead of stochastic backup computations during basin expansion leads to at least a constant factor advantage for deterministic expansion  an extension to seh that might address this problem successfully in future
research would detect domains where the only stochastic choice is between change and non change 
and handle such domains with more emphasis on determinization 
e xploding b locksworld is a variant of the blocks world with two new predicates detonated and destroyed  each block can detonate once  during put down  with some probability 
destroying the object it is being placed upon  the state resulting from the action depicted in figure   has a delete relaxed path to the goal  but no actual path  so this state is a dead end attractor
for delete relaxation heuristics such as cr ff  ff replan or rff bg will never select this action
because there is no path to the goal including this action  seh   f   with the weak dead end detection used in these experiments will select the dead action shown  resulting in poor performance
when this situation arises  it would be possible to use all outcomes determinization as an improved
dead end detector in conjunction with seh   f   in order to avoid selecting such actions  any such
dead end detection would have to be carefully implemented and managed to control the run time
costs incurred as seh relies critically on being able to expand sufficiently large local mdp regions
during online action selection 
in p itchcatch  there are unavoidable dead end states  used by the domain designers to simulate cost penalties   however  the cr ff heuristic  because it is based on all outcomes determinization  assigns optimistic values that correspond to assumed avoidance of the dead end states  as a
   

fiw u   k alyanam     g ivan

current
state

b 

b 

b 

b 

b 

b 

b 

b 

b 

goal state

no path
destroyed table

pick up from table b 

b 

b 

b 

b 

b 

destroyed table

figure    an illustration of a critical action choice of seh   f   in an e xploding b locksworld
problem  ippc  p    the middle state has no actual path to the goal but has a delete relaxed path to
the goal  due to the table having been exploded  no block can be placed onto the table  resulting in
the middle state being a dead end state  the middle state is a dead end with an attractive heuristic
value without regard to whether the blocks shown have remaining explosive charge or not  so this
state feature is not shown 
result  local search by seh   f   is unable to find any expected improvement on the cr ff values 
and falls back to biased random walk in this domain  this domain suggests  as do the other domains where seh   f   performs weakly  that further work is needed on managing domains with
unavoidable deadend states 
the two categories where seh l  loses to ff replan  e xploding b locksworld and
z enotravel  are also categories where seh   f   loses to ff replan  greedily following the
learned heuristics in these two categories leads to lower success ratio than greedily following crff  suggesting more significant flaws in the learned heuristics than in cr ff  although seh is able
to give at least a five fold improvement over greedy following  in success ratio in these two categories  this improvement is not large enough for seh l  to match the performance of seh   f   or
ff replan  both based on the relaxed plan heuristic of ff 
seh  loses to rff in s ystematic   tire due to weak performance in triangle tireworld problems  triangle tireworld provides a map of connected locations arranged so that there is a single
safe path from the source to the destination  but exponentially many shorter unsafe paths   
determinizing heuristics do not detect the risk in the unsafe paths and so greedy following of such
heuristics will lead planners  such as seh    to take unsafe paths  lowering their success rate  while
our results above show that seh  can often repair a flawed heuristic  in the triangle tireworld domain the heuristic attracts seh  to apparent improvements that are actually dead ends 
in contrast  rff is designed to increase robustness for determinized plans with a high probability of failure  rff will continue planning to avoid such failure rather than relying on replanning
after failure  because the initial determinized plan has a high probability of failure  relative to rffs
   the safe path can be drawn as following two sides of a triangular map  with many unsafe paths through the interior
of the triangle  safety in this domain is represented by the presence of spare tires to repair a flat tire that has    
chance of occurring on every step 

   

fis tochastic e nforced h ill  c limbing

jslenratio
 ffr 
seh   

jstimeratio
 ffr 
seh   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

sr of
seh   f  

sr of
ffreplan

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

category

winner

table     aggregated comparison of seh   f   against ff replan in scaled up problems 

category

sr of
seh   f  

sr of
rffbg

jslenratio
 rffbg 
seh   

jstimeratio
 rffbg 
seh   

sr
difference
significant 
 p value 

b locksworld

    

    

    

    

yes

 p      

b oxworld

    

    

    

     

yes

 p      

jslenratio
significant 
 p value 

yes

 p      


winner

seh   f  
seh   f  

table     aggregated comparison of seh   f   against rff bg in scaled up problems 

threshold   rff extends the plan before execution and can often detect the need to use the longer 
safe route 
      p erformance on l arge p roblems
in order to demonstrate that the advantages of seh are emphasized as problem size grows  we
present aggregated performance of seh   f   on additional large sized problems we have generated using generators provided by the first ippc  as such scaling experiments are computationally
very expensive  we have only run two domains that have been most widely evaluated in the planning literature  b locksworld and b oxworld  which is a stochastic version of logistics   for
b locksworld  we generated    problems each for     and    block problems  for b oxworld 
we generated    problems for the size of    cities and    boxes   only one problem across the three
competitions reached this size in b oxworld  and that problem was unsolved by the competition
winner  rff   the aggregated results against ff replan and rff bg are presented in tables   
and     the experiments for these scaled up problems consumed       hours of cpu time and
show that seh   f   successfully completed a majority of the attempts while ff replan and rff
succeeded substantially less often   
note that although the ff heuristic is very good on b oxworld and other logistics domains  the
failure of all outcomes determinization to take into account the probabilities on action outcomes is
quite damaging to ffr in b oxworld  leading the planner to often select an action hoping for its
   our statistical protocol requires    samples of a random variable averaging performance over   solution attempts  for
each planner for each problem  with    problems and   planners  this yields                  solution attempts 
each taking approximately    cpu minutes on these large problems 

   

fiw u   k alyanam     g ivan

low probability error outcome  we note that rff uses a most probable outcome determinization
and will not suffer from the same issues as ffr in the boxworld  given the high accuracy of the
ff heuristic in the boxworld  we believe that the ideas in rff can likely be re implemented and or
tuned to achieve better scalability in the boxworld problems  we leave this possibility as a direction
for future work on understanding the scalability of rff 

   summary
we have proposed and evaluated stochastic enforced hill climbing  a novel generalization of the
deterministic enforced hill climbing method used in the planner ff  hoffmann   nebel        
generalizing deterministic search for a descendant that is strictly better than the current state in
heuristic value  we analyze a heuristic based mdp around any local optimum or plateau reached at
increasing horizons to seek a policy that expects to exit this mdp with a better valued state  we
have demonstrated that this approach provides substantial improvement over greedy hill climbing
for heuristics created using two different styles for heuristic definition  we have also demonstrated
that one resulting planner is a substantial improvement over ff replan  yoon et al         and
rff  teichteil konigsbuch et al         in our experiments 
we find that the runtime of stochastic enforced hill climbing can be a concern in some domains 
one reason for the long runtime is that the number and size of local optima basins or plateaus may
be large  currently  long runtime is managed primarily by reducing to biased random walk when
resource consumption exceeds user set thresholds  a possible future research direction regarding
this issue is how to prune the search space automatically by state or action pruning 

acknowledgments
this material is based upon work supported in part by the national science foundation  united
states under grant no          and by the national science council  republic of china         m         and         m          

references
aarts  e     lenstra  j   eds            local search in combinatorial optimization  john wiley  
sons  inc 
barto  a  g   bradtke  s  j     singh  s  p          learning to act using real time dynamic programming  artificial intelligence            
bertsekas  d  p          dynamic programming and optimal control  athena scientific 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
bonet  b     geffner  h          mgpt  a probabilistic planner based on heuristic search  journal
of artificial intelligence research             
bonet  b     geffner  h          learning depth first search  a unified approach to heuristic search
in deterministic and non deterministic settings  and its application to mdps  in proceedings
of the sixteenth international conference on automated planning and scheduling  pp     
    
   

fis tochastic e nforced h ill  c limbing

bryce  d     buffet  o          international planning competition uncertainty part  benchmarks
and results   http   ippc      loria fr wiki images      results pdf 
buffet  o         personal communication 
cerny  v          thermodynamical approach to the traveling salesman problem  an efficient simulation algorithm  j  optim  theory appl            
dean  t   kaelbling  l  p   kirman  j     nicholson  a          planning under time constraints in
stochastic domains  artificial intelligence           
domshlak  c     hoffmann  j          probabilistic planning via heuristic forward search and
weighted model counting  journal of artificial intelligence research             
fahlman  s     lebiere  c          the cascade correlation learning architecture  in advances in
neural information processing systems    pp           
gardiol  n  h     kaelbling  l  p          envelope based planning in relational mdps  in proceedings of the seventeenth annual conference on advances in neural information processing
systems 
gerevini  a     serina  i          planning as propositional csp  from walksat to local search
techniques for action graphs  constraints               
gordon  g          stable function approximation in dynamic programming  in proceedings of the
twelfth international conference on machine learning  pp         
hansen  e     zilberstein  s          lao   a heuristic search algorithm that finds solutions with
loops  artificial intelligence            
hoffmann  j          the metric ff planning system  translating ignoring delete lists to numeric
state variables  journal of artificial intelligence research             
hoffmann  j     brafman  r          contingent planning via heuristic forward search with implicit
belief states  in proceedings of the   th international conference on automated planning and
scheduling 
hoffmann  j     brafman  r          conformant planning via heuristic forward search  a new
approach  artificial intelligence                     
hoffmann  j          where ignoring delete lists works  local search topology in planning benchmarks  journal of artificial intelligence research             
hoffmann  j     nebel  b          the ff planning system  fast plan generation through heuristic
search  journal of artificial intelligence research             
kautz  h     selman  b          planning as satisfiability  in proceedings of the tenth european
conference on artificial intelligence  ecai    
kirkpatrick  s   gelatt  jr  c     vecchi  m          optimization by simulated annealing  science 
            
little  i     thiebaux  s          probabilistic planning vs replanning  in workshop on international
planning competition  past  present and future  icaps  
liu  c     layland  j          scheduling algorithms for multiprogramming in a hard real time
environment  journal of the association for computing machinery           
   

fiw u   k alyanam     g ivan

mahadevan  s     maggioni  m          proto value functions  a laplacian framework for learning representation and control in markov decision processes  journal of machine learning
research              
nilsson  n          principles of artificial intelligence  tioga publishing  palo alto  ca 
puterman  m  l          markov decision processes  discrete stochastic dynamic programming 
john wiley   sons  inc 
sanner  s     boutilier  c          practical solution techniques for first order mdps  artificial
intelligence                   
selman  b   kautz  h     cohen  b          local search strategies for satisfiability testing  in
dimacs series in discrete mathematics and theoretical computer science  pp         
sutton  r  s          learning to predict by the methods of temporal differences  machine learning 
       
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
teichteil konigsbuch  f   kuter  u     infantes  g          incremental plan aggregation for generating policies in mdps  in proceedings of the ninth international conference on autonomous
agents and multiagent systems  aamas        pp           
tesauro  g     galperin  g          on line policy improvement using monte carlo search  in
nips 
wu  j     givan  r          discovering relational domain features for probabilistic planning 
in proceedings of the seventeenth international conference on automated planning and
scheduling  pp         
wu  j     givan  r          automatic induction of bellman error features for probabilistic planning  journal of artificial intelligence research             
yoon  s   fern  a     givan  r          ff replan  a baseline for probabilistic planning  in proceedings of the seventeenth international conference on automated planning and scheduling  pp         
younes  h   littman  m   weissman  d     asmuth  j          the first probabilistic track of the
international planning competition  journal of artificial intelligence research             

   

fi