journal artificial intelligence research                  

submitted        published      

stochastic enforced hill climbing
jia hong wu

jw   alumni   purdue   edu

institute statistical science 
academia sinica  taipei      taiwan roc

rajesh kalyanam
robert givan

rkalyana   purdue   edu
givan   purdue   edu

electrical computer engineering 
purdue university  w  lafayette         usa

abstract
enforced hill climbing effective deterministic hill climbing technique deals local optima using breadth first search  a process called basin flooding   propose evaluate
stochastic generalization enforced hill climbing online use goal oriented probabilistic planning problems  assume provided heuristic function estimating expected cost
goal flaws local optima plateaus thwart straightforward greedy action choice 
breadth first search effective exploring basins around local optima deterministic problems  stochastic problems dynamically build solve heuristic based markov decision
process  mdp  model basin order find good escape policy exiting local optimum 
note building model involves integrating heuristic mdp problem
local goal improve heuristic 
evaluate proposal twenty four recent probabilistic planning competition benchmark
domains twelve probabilistically interesting problems recent literature  evaluation 
show stochastic enforced hill climbing  seh  produces better policies greedy heuristic
following value cost functions derived two different ways  one type derived using
deterministic heuristics deterministic relaxation second type derived automatic learning bellman error features domain specific experience  using first type heuristic 
seh shown generally outperform planners first three international probabilistic
planning competitions 

   introduction
heuristic estimates distance to the goal long used deterministic search deterministic planning  estimates typically flaws local extrema plateaus limit
utility  methods simulated annealing  kirkpatrick  gelatt    vecchi        cerny 
      a   nilsson        search developed handling flaws heuristics 
recently  excellent practical results obtained flooding local optima using breadth first
search  method called enforced hill climbing  hoffmann   nebel        
deterministic enforced hill climbing  deh  proposed work hoffmann nebel
       core element successful deterministic planner fast forward  ff   deh
extension basic hill climbing approach simply selecting actions greedily looking
ahead one action step  terminating reaching local optimum  deh extends basic hillclimbing replacing termination local optima breadth first search find successor state
strictly better heuristic value  planner moves descendant repeats
c
    
ai access foundation  rights reserved 

fiw u   k alyanam     g ivan

process  deh guaranteed find path goal problem deadend free  so
every state path   relatively weak guarantee applies independent quality
heuristic function  intent deh remediate flaws generally accurate heuristic
order leverage heuristic finding short paths goal  domains basin
size  search depth needed escape optimum  bounded  deh provide polynomial time
solution method  hoffmann        
enforced hill climbing defined probabilistic problems  due stochastic outcomes
actions  presence stochastic outcomes  finding descendants better values longer
implies existence policy reaches descendants high probability  one may argue ff replan  yoon  fern    givan       a top performer recent probabilistic planning
benchmarksuses enforced hill climbing call ff  however  enforced hill climbing
process used determinized problem  ff replan use form hill climbing
directly stochastic problem  fact  ff replan consider outcome probabilities
all 
one problem consider generalizing enforced hill climbing stochastic domains
solution deterministic problem typically concise  sequential plan  contrast  solution
stochastic problem policy  action choice  possibly reached states  essential
motivation hill climbing avoid storing exponential information search  even
explicit solution stochastic problem cannot directly stored respecting motivation 
reason  limit consideration online setting  solution problem
local policy around current state  local policy committed executed
local region exited  planner new online problem solve  possibly retaining
information previous solution   approach generalizes directly construction
offline policies situations space store policies available  note that  contrast 
deterministic enforced hill climbing easily implemented offline solution technique 
propose novel tool stochastic planning generalizing enforced hill climbing goalbased stochastic domains  rather seeking sequence actions deterministically leading
better state  method uses finite horizon mdp analysis around current state seek policy
expects improve heuristic value current state  critical process direct
incorporation probabilistic model heuristic function finding desired policy 
therefore  finite horizon analysis  heuristic function integrated mdp problem
order represent temporary  greedy goal improving current heuristic value 
integration done building novel heuristic based mdp state new exit
action available terminates execution cost equal heuristic estimate state 
action costs removed    heuristic based mdp  finite horizon policies restricted
requirement horizon one  exit action must selected  but selected
horizons   heuristic based mdp  cost policy state expected
value heuristic upon exit  or horizon  executed s 
thus  find desired local policy using value iteration heuristic based mdp around
current state  deepening horizon  policy found cost improving heuristic
estimate current state  restriction selecting exit action horizon one corresponds
initializing value iteration provided heuristic function  policy found 
   motivation removal action costs heuristic based mdp discussed section     

   

fis tochastic e nforced h ill  c limbing

method executes policy exiting action indicated  or horizon used computing
policy  
resulting method  stochastic enforced hill climbing  seh   simply generalizes depth k
breadth first search state improved heuristic value  from deh  k horizon value iteration computation seeking policy expects improvement heuristic value  note although
stochastic enforced hill climbing explicit statespace technique  suitable use astronomically large statespaces heuristic used informative enough limit effective size
horizon k needed find expected heuristic improvement  empirical results work
demonstrate behavior successfully 
    applicability limitations
stochastic enforced hill climbing  seh  applied heuristic function  however 
applicability  and likewise limitations  seh greatly depends characteristics
heuristic function  seh appropriate goal oriented problem given strong enough heuristic
function  demonstrate empirically seh generally outperforms greedy following
heuristic variety heuristics variety domains  even presence probabilistically interesting features  little   thiebaux        deadends  seh rely upon heuristic
function identification dead ends appropriate handling probabilistically interesting features require non local analysisseh simply provides local search often correct
flaws heuristic function  seh thus intended possible improvement stochastic
solution methods construct cost to go  cost  function follow greedily using
constructed cost function search heuristic  many methods constructing value cost functions
proposed evaluated literature  potentially improved
goal based domains using seh place greedy following  sutton        fahlman   lebiere 
      bertsekas        gordon        mahadevan   maggioni        sanner   boutilier          
prove correctness seh section     showing deadend free domains  seh
finds goal probability one  i e  seh get stuck local optima  
seh search technique leverages heuristic estimate distance go  must
emphasized that  unlike many search techniques  seh makes promises
optimality solution path found  seh greedy  local technique promise
repeatedly find policy reduces heuristic value  possible  such 
seh inappropriate technique use optimal solutions required 
stochastic enforced hill climbing ineffective presence huge plateaus valleys
heuristic functions  due extreme resource consumption finding desired local policies 
heuristic functions huge plateaus result methods failed find useful information problem state regions  seh inappropriate tool solving
stochastic planning problemother tools needed construct useful heuristic function
manages deadends avoids huge plateaus  weakness mirrors weakness enforced hillclimbing deterministic domains  seh fail find goals avoidable dead ends
present recognized early enough heuristic  fact  effective dead end detection
central goal heuristic design greedy technique applied heuristic 
   applicability seh  cost function must non negative must identify goals assigning zero state
goal state  however  general value cost functions normalized satisfy requirements 

   

fiw u   k alyanam     g ivan

insight usefulness seh gained comparison recent determinizing replanners  mentioned above  one way exploit deterministic planning techniques
deh stochastic problems determinize planning problem use deterministic planner select action sequence  executing action sequence problem guaranteed
reach goal due determinization approximation  replanning needed augment
technique  paper  call stochastic planners use technique determinizing replanners  determinizing replanners using determinization  called outcomes  retains
possible state transitions shown reach goal probability one absence
dead end states 
contrast determinizing replanners  seh point relies determinization
problem  instead analyzes increasing size local probabilistic approximations problem 
seh conducts full probabilistic analysis within horizon  seeking objective reducing
provided heuristic  using value iteration  way  seh leverages probabilistic parameters
ignored determinizing replanners  well provided heuristic function 
based upon substantial probabilistic analysis  result  seh successfully handles probabilistic
problem aspects cause major problems determinizing replanners  however  point 
theoretical results characterizing gains determinizing replanners  instead 
extensive empirical evaluation showing advantages ff replan  yoon et al        
rff  teichteil konigsbuch  kuter    infantes         two determinizing replanners   well
substantial gains compared greedy following heuristic  which uses transition
probability parameters  
    evaluation
test seh broad range domains first three international probabilistic planning
competitions  as well probabilistically interesting domains little   thiebaux        
using two different methods generate heuristic functions  first  test seh heuristic function based ideas successful re planner ff replan  yoon et al         
new controlled randomness  cr ff  heuristic deterministic heuristic  hoffmann
  nebel        computed simple determinization probabilistic problem makes
available deterministic transition wherever probabilistic transition possible  note
ff replan use  or any  heuristic function stochastic problem  instead  ffreplan relies construct plan deterministic problem  calls turn use
deterministic enforced hill climbing exactly heuristic  here  consider performance
heuristic directly stochastic problem  comparing greedy heuristic following sehbased search around heuristic  latter method using seh constitutes novel method
combining determinization  that removes probabilistic parameters  probabilistic reasoning 
experiments show new method substantially outperforms ff replan across broad
evaluation 
performed second evaluation technique heuristic functions learned
domain specific experience relational feature learning method presented work
wu givan               heuristic functions already shown give good
performance used construct simple greedy policy  improved seh 
seh technique seen perform well domain by domain analysis across
broad set competition planning domains  full domain by domain results available
   

fis tochastic e nforced h ill  c limbing

online appendix  however  compress summarize extensive per problem results 
divided evaluation domains experimenter defined categories aggregated performance measurement within problem category  categories single domains 
generally  multiple closely related domains may aggregated within single category 
example  multiple domains competitions variants blocks world 
problems domains aggregated b locksworld category 
order fairly compare seh ff based planners  such rff  described teichteilkonigsbuch et al         ff replan  exploit blocksworld targeted planning heuristics
added goal deletion goal agenda  provided heuristics extensions seh 
resulting planner called seh    described detail section      results show seh 
performs nearly identically seh non blocksworld categories using cr ff heuristic 
employ extensions comparing seh cr ff heuristic planners 
using experimenter defined categories  able show seh exploits heuristic
functions effectively greedy following heuristic  seh statistically significantly
outperforms greedy following thirteen seventeen categories using cr ff heuristics
losing one category  seh outperforms greedy following six seven categories using learned heuristics   in cases  categories showed similar performance
compared planners  
show seh    using cr ff heuristics  outperforms ff replan ten
fifteen categories  similar performance two categories  losing three categories 
aggregate results show seh   using cr ff heuristics  particularly strong performance advantage ff replan probabilistically interesting categories  little   thiebaux 
      
finally  compare performance seh  rff bg  teichteil konigsbuch
et al          one winner fully observable track third international probabilistic planning competition  seh  outperforms rff bg twelve fifteen categories  similar
performance one category  losing two categories 
summary  empirical work demonstrates seh provides novel automatic technique
improving heuristic function using limited searches  simply applying seh
reasonable heuristic functions produces state of the art planner 

   technical background  markov decision processes
give brief review markov decision processes  mdps  specialized goal region objectives 
detail mdps  see work bertsekas         puterman         sutton barto
       
    goal oriented markov decision processes
markov decision process  mdp  tuple  s  a  c  t  sinit    here  finite state space
containing initial state sinit   selects non empty finite available action set a s  state
s  action cost function c assigns non negative real action cost state action state
triple  s  a    action enabled state s  i e   a s   transition probability
function maps state action pairs  s  a  probability distributions s  p s  
a s  
   

fiw u   k alyanam     g ivan

represent goal  include zero cost absorbing state   i e   c   a  s   
     a        a    goal oriented mdps mdps
subset g statespace  containing   that      c g  a    zero whenever g g
one otherwise       g  a    one g g a g   set g thus
taken define action cost function c  well constrain transition probabilities  
 stochastic  policy mdp   n p a  specifies distribution actions
state finite horizon  cost to go function j  s  k  gives expected cumulative
cost k steps execution starting state selecting actions according    state
encountered  horizon k  least one  deterministic  optimal policy    k 

j  s  k   abbreviated j  s  k   greater j  s  k  every state s 
policy   following q function evaluates action using provided cost to go function
j estimate value action applied 
x
q s  a  j   
 s  a    c s  a      j s    


recursive bellman equations use q   describe j j follows 
j  s  k    e  q s   s  k   j    k     
j  s  k    min q s  a  j    k     
aa s 

taking expectation random choice made possibly stochastic policy  s  k  
cases  zero step cost to go function zero everywhere  j  s       j  s        
s  value iteration computes j  s  k  k increasing order starting zero  note
policy cost function depend k  may drop k argument list 
using q    select action greedily relative cost function  policy
greedy j  selects  state horizon k  uniformly randomly selected action
argminaa s  q s  a  j   k     
goal based mdp problems directly specified above  may specified
exponentially compactly using planning languages ppddl  younes  littman  weissman    asmuth         used experiments  technique avoids converting
entire ppddl problem explicitly form  resource reasons  instead constructs
sequence smaller problems explicit mdp form modeling heuristic flaws 
dead end state state every policy zero probability reaching goal
horizon  say policy reaches region states probability one following policy
horizon k probability entering region point converges one k goes
infinity  say dead ends unavoidable problem whenever policy sinit
reaches goal region probability one   we say domain unavoidable dead ends
problem domain unavoidable dead ends   note greedy techniques
hill climbing expected perform poorly domains dead end states attractive
heuristic values  application seh thus leaves responsibility detecting avoiding deadend states design heuristic function 
heuristic h   r may provided  intended estimate cost function j large
horizons  h s      g  h s      otherwise  heuristic may indicate dead end
states returning large positive value v assume selected experimenter
exceed expected steps goal state reach goal  experiments 
   

fis tochastic e nforced h ill  c limbing

add trivial  incomplete dead end detection  described section      heuristic function
evaluate 
note domains evaluated paper contain unavoidable deadends 
may policy success ratio one  choice large value used recognized
dead end states effects trade off optimizing success ratio optimizing expected cost
incurred goal successful 
    determinizing stochastic planning problems
stochastic planners heuristic computation techniques  including used experiments  rely computing deterministic approximations stochastic problems  one planner 
all outcomes ff replan  yoon et al          determinizes stochastic planning problem
invokes deterministic planner  hoffmann   nebel        determinized problem 
determinization used ff replan constructed creating new deterministic action
possible outcome stochastic action ignoring probability outcome happening  effectively allows planner control randomness executing actions  making
determinization kind relaxation problem  section      define domainindependent heuristic function  controlled randomness heuristic  cr ff   deterministic heuristic  hoffmann   nebel        computed all outcomes ff replan determinization probabilistic problem    variety relaxations previously combined
variety deterministic heuristics order apply deterministic planning techniques stochastic problems  bonet   geffner         generally  deterministic relaxations provide general
technique transferring techniques deterministic planning use solution stochastic
problems 

   stochastic enforced hill climbing
deterministic enforced hill climbing  deh   hoffmann   nebel        searches successor
state strictly better heuristic value returns path current state successor 
path action sequence guarantees reaching desired successor  illustrate
behavior deh compared greedy policy using example figure    stochastic
environment  may single better descendant reached probability one 
since actions may multiple stochastic outcomes  simply use breadth first search
deh find single better descendant ignore possible outcomes  might end
selecting action low probability actually leading state better heuristic value 
illustrated figure    shown figure  algorithm  stochastic enforced hill climbing
 seh   accurately analyzes probabilistic dynamics problem improving heuristic
value 
section  give details seh  note deh  local breadth first search
gives local policy state region surrounding current state deterministic environment 
value following policy heuristic value improved descendant found
breadth first search  seh  implement ideas stochastic setting 
   deterministic heuristic  described work hoffmann nebel         planner version    
available http   www loria fr hoffmanj ff html  efficiently computes greedy plan length problem relaxation
state facts never deleted  plan found relaxed problem referred relaxed plan
problem 

   

fiw u   k alyanam     g ivan

h  

h  

h  

h  

h  
h  

h  

h  

h  

h  

h  

h  
h  

h  

h     

h     

 a  behavior greedy policy 

 b  behavior deh 

figure    comparison behavior deh greedy policy local optimum
encountered  solid black circle represents current state  shaded circle represents
goal state  with heuristic value zero    a  greedy policy keeps selecting actions indicated
wide arrow cannot reach goal state  hand  deh uses breadth first search
finds goal state two steps away current state  shown  b  

h  
h  
h  
h  

h  

p     
p     

h  

h  

h  

h     

p     

h  

h  

p     
h  

 a  behavior deh stochastic environments 

h  

h  

h  

h     

 b  behavior seh stochastic environments 

figure    comparison behavior seh deh stochastic example  assume
deh first determinizes problem  creating one deterministic action possible stochastic
outcome  solid black circle represents current state  shaded circle represents
goal state  with heuristic value zero    a  deh looks one step ahead selects action drawn
double lines  one outcomes leads state h      better current
state  however  action choice higher probability going state h     
one h       b  seh first decides policy better value  
horizon mdp includes states reachable current state one step  seh
extends horizon two states considered  selects actions indicated
wide arrows lead goal state 

   

fis tochastic e nforced h ill  c limbing

online planning using local planner
   repeat
  
current state
  
local find local policy s h 
  
follow local selected
   goal reached
table    pseudo code online planning framework  policy local may non stationary 
case local planner returns initial horizon execution policy termination line   happen reaching specified horizon 

present seh two steps  first  present simple general framework online planning repeatedly calls local planner selects policy around current state  second 
present local planner based enforced hill climbing idea  online planning
framework instantiated local planner  resulting algorithm seh  combination
two steps constitute central algorithmic contribution paper  finally  present
analytical properties algorithm 
    simple online planning framework
familiar direct approach online planning call planner current state
planner select action  action executed environment  resulting new current
state  process repeated 
here  present simple generalization approach allows planner select
one action call  action executed  idea planner makes
plan local context surrounding current state  plan executed local
context exited  local context exited  new current state process
repeated 
formally  augment action space new terminate action  called    indicating planned for local context exited  define local policy around state
partial mapping states augmented action space defined every
state reachable policy    online planner built repeatedly seeking
executing local policy around current state using planning subroutine  local policy
executed terminate action called  which effect state   point
new local policy must sought  ideas reflected pseudo code shown table   
note notion local context discussion informal precise
notion given use terminate action  local policy executed selects
terminate action  find local policy routine free use method decide state
assigned terminate action  previously published envelope methods  dean  kaelbling 
kirman    nicholson        provide one way address issue  termination
assigned every state outside envelope states  however  framework general
envelope methods  allows local policies selected based upon pre existing
   local policy returned non stationary finite horizon  must select terminate action
final stage  reachable states 

   

fiw u   k alyanam     g ivan

envelopes states  though always  post planning  interpret set reachable states
envelope   general intuition selecting action current states may involve
analysis sufficient select actions many surrounding states  framework allows
find local policy routine return policy specifying action selections 
also  note online planning framework includes recent re planners ffreplan  yoon et al         rff  teichteil konigsbuch et al          however  replanning
current plan failed  e g  determinization used generate naive 
quite different character seh  constructs plans improve heuristic value 
replans time plan terminates  thus  seh uses heuristic function define subgoals
plan original goal incrementally 
remains present local planner combine online planning framework
define stochastic enforced hill climbing  local planner analyzes mdp problem around
current state  heuristic function integrated problem embody subgoal
improving heuristic value current state  describe simple integration
heuristic function problem next  discuss local planner based integration 
    heuristic based markov decision processes
method relies finite horizon analyses transformed mdp problem  increasing horizons  transform mdp problem novel heuristic achievement transform analysis
order represent goal finding executing policy expects improve initial
 current  states heuristic value 
heuristic achievement transform straightforward  applies goal oriented
mdp problem  first  action costs removed problem  second  terminate action
assigned action cost h s  transitions deterministically absorbing state  
policy executed  selection action state result replanning  discussed
online planning framework presented  actions thought heuristic
achievement actions  allowing immediate achievement value promised heuristic
function 
analyzing mdp transformed heuristic achievement transform finite horizon
around s  represents problem finding policy improving heuristic value s 
without regard cost achieving improvement heuristic  allowing heuristic
achievement action selected point state reflects greedy nature goal 
planner forced look improvement found  long policy
initial state expects see improvement 
formally  given mdp    s  a  c  t  s    non negative heuristic function h   r 
heuristic achievement transform h  written mh   given  s    c     s    
  c   follows  let s  s    s  arbitrary states s  define  s 
a s   a    take c  s    a  s         s    a  s       s    a  s    a s    
finally  define  s          c  s        h s  
transformed mdp zero cost policies states  immediate use  however  policies required select final action  at horizon one 
represent policies seeking get regions low heuristic value  whatever cost 
increasing horizon search policies corresponds roughly breadth first search
improved heuristic value deterministic enforced hill climbing  formally  define class
   

fis tochastic e nforced h ill  c limbing

heuristic achievement policies h class policies  s  k  satisfy  s       s 
define jh  s  k  value minh j  s  k  heuristic transform mdp h h
policy achieves value  note that  due zeroing non terminal action costs 
jh  s  k  represents expected heuristic value achieved next execution  
required horizon k before  formally  define random variable state
h first executes trajectory s  jh  s  k    e h s    
rough motivation setting action costs zero analysis heuristic based
mdp actions considered method remediate flawed heuristic 
cumulative action cost required reach state improved heuristic value measure
magnitude flaw heuristic  here  remove cost analysis order
directly express subgoal reaching state lower heuristic value  including action costs
might  example  lead preferring cheap paths higher heuristic values  i e   states worse
s    expensive paths lower heuristic values found  basic motivation
enforced hill climbing strongly seek improved heuristic values  instead diluting
subgoal adding action costs  methods seek shortest path heuristic improvement
analyzing heuristic based mdp iteratively deepened finite horizon  discussed
next subsection  approach reasonable settings action
cost  finite horizon value iteration stochastic setting analogue uniform cost search 
settings varying action cost  future work needed adapt seh usefully consider
cost without excessively diluting focus improving heuristic 
      h euristic achievement value teration
following formalism value iteration section      compute jh  s  k  heuristic
achievement value iteration follows 
jh  s       h s  
jh  s  k    min q s  a  jh    k     k   
aa  s 

non stationary policy achieving cost to go given jh    k  computed using
following definition 
h  s        
h  s  k    argminaa  s  q s  a  jh    k     k   
note q   computed heuristic achievement transformed mdp mh equations 
technical reasons arise zero cost loops present  require tie breaking
argmin h  s  k  favors action selected h  s  k    whenever one options 
prevent selection looping actions shorter  direct routes value 
    local planner
consider method stochastic enforced hill climbing uses online planning framework  presented table    together local policy selection method solves
heuristic achievement mdp  exactly  approximately  heuristically   here  describe one
straightforward method local policy selection defining seh find local policy using finitehorizon value iteration  method generalizes breadth first search used deterministic
   

fiw u   k alyanam     g ivan

enforced hill climbing  seeks expected heuristic improvement rather deterministic
path improved heuristic value  sophisticated heuristic methods finite horizon
value iteration considered implementation presented finds local mdp problems intractable  analytical results section     apply method exactly solves
heuristic achievement mdp  method presented table    experimental results
conducted using implementation table   well 
present pseudo code seh find local policy table    heuristic function h respects
goals h s      iff g  algorithm assumes non negative heuristic function h   r
respects goals  input  seh find local policy s   h  returns policy h horizon k 
policy computed states horizons needed order execute h s  using
horizon k policy terminates 
thus  lines      table     heuristic achievement value iteration conducted increasing horizons around s    seeking policy improving h s     note given horizon k     
states reachable within k steps need included value iteration 
      e arly ermination
primary termination condition repeated local policy construction discovery policy
improving heuristic estimate initial state  discussed proposition   
domains without deadends  seh find local policy always find policy improving h s    
given sufficient resources 
however  badly flawed heuristic functions large enough horizons analyzed
seh find local policy may unacceptably large given resource constraints  moreover  domains unavoidable deadends  may horizon  however large  policy improving
heuristic initial state  reasons  practice  algorithm stops enlarging
horizon heuristic based mdp analysis user specified resource limits exceeded 
horizon limited analysis heuristic transform mdp construction yield
desired results inexpensively  biased random walk used seek new initial state  example  consider problem provided heuristic labels states reachable k steps
cost to go estimates similar  forming large plateau  analysis large
plateau exceeds resources available  biased random walk indicated  lack useful heuristic
guidance 
so  horizon k found jh  s    k    h s     system executes h s 
horizon k terminate action selected  resource limit exceeded without
finding horizon  system executes biased random walk length   terminating
action imposed states reachable biased random walk h s    h s    
additional biased random walk allows method retain beneficial properties
random exploration domains heuristic flaws large mdp analysis  resource
consumption threshold random walk triggered viewed parameter controlling
blend random walk mdp based search used overcoming heuristic flaws 
currently principled way analyzing tradeoff resource consumption
cost switching biased random walk  determining switching  instead 
use domain independent resource limits described section      determined
experimentation 
   

fis tochastic e nforced h ill  c limbing

seh find local policy s   h 
  
s  current state 
  
h      r heuristic function  extended h       
  
assume global variable mh heuristic achievement
transform original problem h 
  
seek policy problem mh achieving cost less h s    
   k    
   repeat
  
k  k  
  
   compute jh  s    k  mh using value iteration
  
jh         h    h           n    
  
repeat
  
n n  
  
reachable s  mh using k n steps
  
jh  s  n    minaa  s  q s  a  jh    n    
   
h  s  n    argminaa  s  q s  a  jh    n    
   
n   k
    jh  s    k    h s    resource consumption exceeds user set limits
    jh  s    k    h s   
   

   
return h horizon k
   
else
   
   return  step biased random walk policy
   
   note  implementations compute lazily online
   
n    
   
state
   
h s    h s   
   

   
 s  n  selects probability one
   
else
   
 s  n  selects action a s  probability
   

p

eq s a h 
ai a s   e

q s ai  h   

return horizon

table    pseudo code local planner used implement stochastic enforced hill climbing 

   

fiw u   k alyanam     g ivan

horizon limited analysis heuristic transform mdp may terminate without finding
horizon k jh  s    k    h s    entire reachable statespace explored 
presence deadends  may happen without exceeding available resources 
case fall back fixed number iterations standard vi original mdp model
 including action costs without heuristic transform  reachable states 
    analytical properties stochastic enforced hill climbing
deterministic settings  given sufficient resources dead ends  enforced hill climbing
guarantee finding deterministic path improved heuristic value  if nothing else  goal state
suffice   given finite state space  guarantee implies guarantee repeated enforced
hill climbing find goal 
situation subtle stochastic settings  problem dead ends  every
state optimal policy reaches goal probability one  follows problems 
h assigning zero every goal state  every state real value      horizon
k jh  s  k       recall jh analyzes heuristic transform mdp wherein action costs
dropped except h   must realized horizon one   seh find local policy s h 
considers k turn jh  s  k    h s  have 

proposition    given non goal state s  dead ends  non negative heuristic function
h   r respecting goals  sufficient resources  routine seh find localpolicy s h  returns policy h horizon k expected return jh  s  k  strictly
less h s  
however  unlike deterministic setting  policy found routine seh find local policy
expects improvement heuristic value  particular executions policy
current state may result degraded heuristic value 
here  prove even stochastic settings  spite possibility poor results
one iteration  seh reach goal region probability one  absence dead end states
sufficient resources  practice  provision sufficient resources serious hurdle 
must addressed providing base heuristic modest sized flaws 

theorem    dead end free domains  unbounded memory resources  seh reaches goal region probability one 
proof  let x    x    x            xm         random variables representing sequence
states assigned line   table   execute seh planning problem 
x  initial state sinit   algorithm achieves x g  
thus terminates  take xj     xj j    note result xj g implies
xj   g  g goal region states  
show arbitrary     probability xm goal region
h sinit  
least  
  real value     defined below  expression goes one
   

fis tochastic e nforced h ill  c limbing

goes infinity  conclude seh reaches goal region
probability one 
proposition    non goal state s  absent dead ends sufficient
resources  one iteration seh guaranteed return policy finite horizon
ks value jh  s  ks   improving h s   let   h s  jh  s  ks       value
improvement horizon ks   finitely many non goal
states  exists   minssg     improvement h s  jh  s  ks  
least   consider arbitrary xi
  g  noting jh  xi   kxi    
e h xi      due zero action costs mh   follows immediately e h xi  
h xi     xi
  g    g goal region states  using xi g implies
xi   g h xi     h xi          write
e h xi   h xi     
 e h xi   h xi     xi
  g qi
  e h xi   h xi     xi g    qi  

   

qi       
defining qj probability xj
  g 
now  lower bound expected heuristic improvement e h x    h xm    calls seh find local policy       decompose expected
improvement calls seh find local policy sum expected improvements individual calls  then  lower bounding sum using smallest
term  get
e h x    h xm   
 


m 
x

i  
m 
x

e h xi   h xi     
   
qi  from inequality   

i  

mqm  
qm non increasing  since xm  g implies xm g 
next  combine lower bound natural upper bound h sinit    since h
assumed non negative  so e h x    h xm    h sinit    x    sinit   thus 
h sinit   qm m 
h s

 

init   converging zero
therefore probability qm xm
  g
large seh reaches goal probability one 

theorem assumes absence dead ends  problems dead ends covered theorem well dead ends avoidable identified heuristic 
specifically  may require heuristic function assigns state
policy reach goal state probability one  case  problem
converted form required theorem simply removing states assigned
consideration  either pre processing local mdp construction  
   

fiw u   k alyanam     g ivan

    variants extensions seh
seh based finite horizon analysis mdp transformed heuristic achievement transform around current state s    particular heuristic achievement transform describe
course option incorporating heuristic local search around s   
already considered number related alternatives arriving choice describe 
options considered future research  one notable restriction transform
removal action costs  discussed section      important method
retain actual heuristic value analysis trade large  small  positive
negative changes heuristic value according probabilities arising  reason 
heuristic transform abstract away value simply assign rewards  
  according whether state improves h s     choice remove action costs local
expansion lead poor performance domains flawed heuristics interacting badly
high variations action costs  subject future research method 
also  mdp models describe paper limited obvious ways 
limitations include state space discrete finite  problem setting lacks discounting 
objective goal oriented  yet implement extension relax limitations 
leave consideration issues arise future work  note would appear
method fundamentally goal oriented  given goal repeatedly reducing heuristic value
current state  however  possible contemplate infinite horizon discounted non goaloriented variants seek policies maintain current heuristic estimate 
    incorporating goal ordering techniques seh
planner contains heuristic elements inspired ordering issues arise blocksworld
problems  hoffmann   nebel         heuristic elements improve performance blocksworld problems significantly  assist fair comparison seh ff replan 
implemented two heuristic elements  namely goal agenda added goal deletion  variant
seh call seh   
implementation seh  follows  stochastic planning problem first determinized using outcomes determinization described section      goal agenda technique invoked determinized problem extract sequence temporary goals
g            gm   gi set goal facts gm original problem goal  seh
stochastic version added goal deletion  described next subsection  invoked repeatedly compute sequence states s            sm   s  initial state     si
defined state reached invoking seh state si  goal gi  thus satisfying gi   
added goal deletion idea pruning state search space avoiding repetitive addition
deletion goal fact along searched paths  ff  search state s  goal fact
achieved action arriving s  deleted action relaxed plan found s 
expanded  hoffmann   nebel        
stochastic adaptation added goal deletion  define set facts added
state transition  s  a    facts true represent set difference
s  then  set added goal facts transition added facts true
current temporary goal gi   i e    s s  gi   prune state transition  s  a    whenever
relaxed plan computed current temporary goal gi contains action deletes
added goal facts  transition  s  a    pruned modifying bellman update
   

fis tochastic e nforced h ill  c limbing

state contributes dead end state value  v   q value s  weighted
transition probability  instead contributing cost to go    formally  define
modified q function using added goal deletion  qagd  s  a  j  follows 
 
   f  s s  gi deleted action relaxed plan s  gi   

i s    
   otherwise
x
qagd  s  a  j   
 s  a    i s  v      i s   j s   


qagd    replaces q   definition cost to go function jh    section      also 
reachability line   table   use pruned transitions 
problems  subsequent deletion newly added goals unavoidable valid plan 
added goal deletion prunes routes leading goal region problems even though
actual deadend present  hence  incomplete technique discussed work
hoffmann nebel         falls back best first search deh able find valid
plan due pruning  similarly  unable find improved policy  seh falls back either
value iteration biased random walk described section     
preliminary exploration incorporating stochastic variants ffs helpful action pruning
 hoffmann   nebel        seh improve performance  much effect added
goal deletion domains except blocksworld    result  report helpfulaction pruning methods here 

   related work
section discuss planning techniques close work one dimensions 
    fast foward  ff  planner deterministic enforced hill climbing
introduction deterministic enforced hill climbing  deh  relation technique 
please see section    here  additionally note several lines work directly
extend planner allow planning numeric state variables  hoffmann        planning uncertainty  hoffmann   brafman              domshlak   hoffmann         although
techniques involve significant changes computation relaxed plan heuristic
possible addition use belief states handle uncertainty  enforced hill climbing still
primary search technique used lines work  note although work domshlak hoffmann actions probabilistic outcome handled  planner  probabilistic ff 
designed probabilistic planning observability  whereas planner designed
probabilistic planning full observability 
    envelope based planning techniques
stochastic enforced hill climbing dynamically constructs local mdps find local policy leading
heuristically better state regions  concept forming local mdps  envelopes  using
   relaxed plan s  gi   computes relaxed plan states gi defined work hoffmann
nebel        using all outcomes problem determinization defined section     
   explored ideas based defining helpfulness action expectation helpfulness
deterministic outcomes 

   

fiw u   k alyanam     g ivan

facilitate probabilistic planning used previous research work bonet
geffner         dean et al          briefly review here 
envelope based methods work dean et al         gardiol kaelbling       
start partial policy restricted area problem  the envelope   iteratively improves solution quality extending envelope recomputing partial policy 
typical assumption implementing method planner initial trajectory
starting state goal  generated stochastic planner  use initial envelope 
another line work  including rtdp  barto  bradtke    singh         lao   hansen  
zilberstein         ldfs  bonet   geffner         starts envelope containing
initial state  iteratively expands envelope expanding states  states expanded
according state values dynamic programming methods used backup state values
newly added states  convergence criterion reached  stochastic enforced hill climbing
viewed repeatedly deploying envelope method goal  time  improving
heuristic estimate distance to go  good h function  invocations result trivial
one step envelopes  however  local optima plateaus encountered  envelope may
need grow locate stochastically reachable set exits 
referenced previous search methods constructed envelopes seeking high
quality policy goal rather far limited relatively inexpensive goal basin
escape  results derive online greedy exploitation heuristic rather
expensive offline computation converged values proving overall  near  optimality  ldfs 
example  compute check values least states reachable optimal policy  even
given j input  possibly vastly many others well computation 
previous methods able exploit properties  such admissibility 
heuristic function guarantee avoiding state expansions regions state space  clearly 
seh exploits heuristic function way avoid expanding regions statespace 
however  point conducted theoretical analysis regions guaranteed unexpanded particular kinds heuristic  analyses may quite difficult 
    policy rollout
technique policy rollout  tesauro   galperin        bertsekas   tsitsiklis        uses
provided base policy make online decisions  technique follows policy greedy vf   
vf computed online sampling simulations policy  
computation optimal heuristic transform policy h seh similarities policy
rollout  case  online decisions made local probabilistic analysis leverages provided information manage longer range aspects local choice  seh  heuristic function
provided while  policy rollout  base policy provided  view  policy rollout local
analysis assumption non local execution use base policy   whereas seh
local analysis assumption non local execution achieve base heuristic
cost estimate h 
fact  goal oriented setting  provided heuristic function h stochastic  a simple generalization describe paper   equal sampled simulation evaluation
v policy   seh executes policy policy rollout  assuming uniform
action costs sufficient sampling correctly order action choices  claim follows h   v always action yield expected improvement h one step 
   

fis tochastic e nforced h ill  c limbing

goal oriented setting  need uniform action costs claim may relaxed
variant seh developed retains action costs heuristic transform 
policy rollout  horizon one greedy use sampled heuristic needed  main
substance seh enable repair use heuristic functions flaws cannot
repaired horizon one  thus central differences techniques reflected
ability seh leverage arbitrary heuristic functions repair flaws functions larger
horizons 
policy rollout provides elegant guarantee online policy selected improves base
policy  given sufficient sampling  result follows intuitively computed policy
policy iteration improvement base policy  unfortunately  similar guarantee known
apply seh arbitrary heuristic function  however  policy rollout cannot used improve
arbitrary heuristic function either 
    local search optimization
stochastic enforced hill climbing regarded one many local search techniques designed
improve greedy one step lookahead  naive form local search optimization 
briefly discuss connections method simulated annealing  one large family related
local search techniques  detail  please see work aarts lenstra        
simulated annealing  kirkpatrick et al         cerny        allows selection actions
inferior expected outcome probability monotone action q value  probability
inferior action selected often starts high decreases time according cooling schedule  ability select inferior actions leads non zero probability escaping local
optima  however  method systematically search policy so  contrast 
stochastic enforced hill climbing analyzes heuristic based mdp increasing horizons systematically search policies give improved expected value  hence leaving local extrema  
substantial preliminary experiments  could find successful parameter settings control
simulated annealing effective application online action selection goal directed stochastic
planning  knowledge  simulated annealing otherwise tested direct forwardsearch action selection planning  although variants applied success
planning as search settings  selman  kautz    cohen        kautz   selman        gerevini  
serina        planning via boolean satisfiability search 

   setup empirical evaluation
here  describe parameters used evaluating method  heuristics test
method on  problem categories tests conducted  random variables
aggregated evaluation  issues arising interpreting results statistical significance 
run experiments intel xeon    ghz machines     mhz bus speed    kb
cache 
    implementation details
horizon increase new states reachable  implementation seh simply switches
explicit statespace method solve mdp formed reachable states  specifically 
   

fiw u   k alyanam     g ivan

increase k line   table   lead new reachable states line    trigger
value iteration states reachable s   
throughout experiments  thresholds used terminate local planning line    table  
set         states one minute  set biased random walk length ten  work
makes assumption heuristic functions used assign large values easily recognized deadends  hill climbing works poorly presence dead end attractor states  enforce
requirement simple dead end detection front end heuristic function
 described next section     heuristic  assigning value         recognized
dead end states 
denote implementation running heuristic h seh h  
    heuristics evaluated
describe two different types heuristic functions used evaluation associated
dead end detection mechanisms 
      c ontrolled  r andomness h euristic
use evaluations  define domain independent heuristic function  controlledrandomness heuristic  cr ff   define cr ff state distance to goal
estimate  hoffmann   nebel        computed all outcomes determinization described
section      denote resulting heuristic function f   computing cr ff heuristic 
use reachability analysis built planner detection deadends 
      l earned h euristics
test stochastic enforced hill climbing automatically generated heuristic functions
work wu givan         perform state of the art used
construct greedy policy  shift heuristic functions fit non negative range requirement h discussed previously  learned heuristic functions currently available
seven test categories  tested categories 
note heuristics learned discounted setting without action costs
direct fit distance to go formalization adopted here  still able get
significant improvements applying technique  denote heuristics l  states
valid action choice available labeled deadends applying seh learned
heuristics 
    goals evaluation
primary empirical goal show stochastic enforced hill climbing generally improves
significantly upon greedy following heuristic  using policy greedy h  described
technical background above   show true heuristics defined
section      show empirically applicability limitation seh discussed section     
different types problems including probabilistically interesting ones  little   thiebaux        
secondary goal evaluation show base heuristics resulting performance strong comparison deterministic replanners ff replan  yoon et al        
rff  teichteil konigsbuch et al          ff replan rff use fast forward  ff 
   

fis tochastic e nforced h ill  c limbing

base planner  rff uses most probable outcome determinization contrast all outcomes
determinization used ff replan  primary difference rff ff replan
executing plan  rff grows policy trees minimize probability
replan  ff replan not 
    adapting ippc domains experiments
conduct empirical evaluation using problems first three international probabilistic planning competitions  ippcs  well twelve probabilistically interesting problems
work little thiebaux         omit particular problems domains
particular comparisons several practical reasons  detailed online appendix 
enforced hill climbing nature goal oriented technique seh formulated
goal oriented setting  ignore reward structure  including action goal rewards 
evaluated problems assume uniform action cost one problems 
transforming reward oriented problem description goal oriented one 
provide detailed per problem results online appendix planner evaluated
work  however  support main conclusions  limit presentation aggregations
comparing pairs planners sets related problems  purpose  define seventeen
problem categories aggregate within problem category  categories single
domains  generally  multiple closely related domains may aggregated within single category  example  blocksworld category aggregates blocksworld problems three
competitions  even though action definitions exactly every problem 
paired comparisons  aggregated results problems labeled constructed
probabilistically interesting ippc  organizers work little thiebaux
       combined category pi problems 
table    list evaluated categories  including combined category pi problems  
well planning competitions literature problems category from 
evaluated problems category identified online appendix 
reward oriented ysadmin domain ippc  stochastic longest path problem
best performance required avoiding goal continue accumulating reward long
possible  bryce   buffet          note contrary organizers report  domains goal
condition servers rather servers down   goal oriented adaptation removes
longest path aspect domain  converting domain goal get
servers up 
b locksworld problems ippc  contain flawed definitions may lead block
stacking top itself  nevertheless  goal problems well defined achievable
using valid actions  hence problems included b locksworld category 
discovered five rectangle tireworld problems  p   p   ippc   t ireworld  apparent bugno requirement remain alive included goal condition  domain design provides powerful teleport action non alive agents intended
increase branching factor  buffet         however  lacking requirement alive goal 
domain easily solved deliberately becoming non alive teleporting goal 
modified problems require predicate alive goal region  merged
modified rectangle tireworld problems triangle tireworld problems ippc 
   

fiw u   k alyanam     g ivan

category

problem source s 

b locksworld

ippc   ippc   ippc 

b oxworld

ippc   ippc 

b usfare

little thiebaux       

rive

ippc 

e levator

ippc 

e xploding b locksworld

ippc   ippc   ippc 

f ileworld

ippc 

p itchcatch

ippc 

r andom

ippc 

r iver

little thiebaux       

chedule

ippc   ippc 

earch r escue

ippc 

ysadmin

ippc 

ystematic   tire

triangle tireworld  ippc    tireworld p  p    little thiebaux         
rectangle tireworld  ippc    tireworld p   p    bug fixed

ireworld

ippc   ippc 

owers h anoi

ippc 

z enotravel

ippc   ippc 

pi problems

b usfare  rive  e xploding b locksworld
p itchcatch  r iver  chedule  ystematic   tire  ireworld

table    list categories planning competitions literature problems
category taken 

   

fis tochastic e nforced h ill  c limbing

work little thiebaux        category ystematic   tire  problems
systematically constructed emphasize pi features 
    aggregating performance measurements
experiments  designed repeatable aggregate measurements sample
many times order evaluate statistical significance  define random variables representing aggregate measurements describe sampling process  well method
evaluating statistical significance 
      efining ampling aggregate  m easurement r andom variables
pair compared planners  define four random variables representing aggregate performance comparisons problems category  random variable based upon
sampling process runs planner five times problems category  aggregates
per problem result computing mean  use five trial runs reduce incidence lowsuccess planners failing generate plan length comparison  mean value five trial run
sample value respective random variable 
first  per problem success ratio  sr  fraction five runs succeed
problem  success ratio random variable category planner mean sr
across problems category 
second  per problem successful plan length  slen  mean plan length successful
runs among five runs  order compare two planners plan length  define perproblem ratio jointly successful plan lengths  jslen ratio  two compared planners
follows  planners positive sr among five trials problem  jslen ratio
ratio slen values two planners  otherwise  jslen ratio undefined
problem  use ratio lengths emphasize small plan length differences short solutions
long solutions  decrease sensitivity granularity action definitions 
mean jslen ratio random variable category pair planners
geometric mean jslen ratio across problems category jslen ratio
well defined  manner ensure two planners compared exactly set
problems  note that  unlike sr  jslen ratio depends pair compared planners 
rather measurement single planner  ratio successful plan length
jointly solved problems two planners 
similarly  per problem ratio jointly successful runtimes  jstime ratio  defined
manner used comparing plan lengths  mean jstime ratio computed
geometric mean well defined per problem jstime ratio values 
jslen ratio jstime ratio ratios two measurements  use geometric mean aggregate per problem results generate sample value  whereas use arithmetic
mean sr variables  note geometric mean desired property planners tied overall  so geometric mean one   mean insensitive planner
given denominator ratio 
thus  draw single sample four aggregate random variables  sr planner 
jslen ratio  jstime ratio  comparing two planners  run two planners
problem five times  computing per problem values four variables  take  arith   

fiw u   k alyanam     g ivan

metic geometric  means per problem variables get one sample aggregate variable  process used repeatedly draw many samples needed get significant results 
use plan length cutoff      attempt  attempt given time limit   
minutes 
      ignificance p erformance ifferences b etween p lanners
general goal order pairs planners overall performance category problem 
this  must trade success rate plan length  take position significant
advantage success rate primary goal  plan length used determine preference
among planners success rate differences found significant 
determine significance three performance measurements  sr  jslenratio  jstime ratio  using t tests  ascribing significance results p value
less       exact hypothesis tested form t test used depends performance
measurement  follows 
   sr use paired one sided t test hypothesis difference true means
larger      
   jslen ratio use one sample one sided t test hypothesis true geometric mean jslen ratio exceeds       log true mean jslen ratio exceeds
log        
   jstime ratio use one sample one sided t test hypothesis true
geometric mean jstime ratio exceeds       log true mean jstime ratio
exceeds log        
stop sampling performance variables achieved one following criteria  representing sr winner determined sr appears tied 
   thirty samples drawn p value sr difference          
   sixty samples drawn p value sr difference          
   one hundred fifty samples drawn 
experiments present next  stopping rule leads    samples drawn
unless otherwise mentioned  upon stopping  conclude ranking planners  naming
winner  either sr difference jslen ratio p value       significant
sr differences used first determine winner  neither measure significant upon
stopping  deem experiment inconclusive 
combining categories evaluations  aggregate results across multiple categories problem  e g   combined category pi problems  cases  effectively
defined one larger category  techniques defining performance measurements determining statistical significance section      however  actually re run
planners combined category measurements  instead  re use planner runs used
single category experiments  rather use stopping rule described  compute
maximum number runs available combined categories use many samples
   

fis tochastic e nforced h ill  c limbing

combined category performance measurements  avoid double counting problem results 
treat combined categories separately analyzing results counting wins losses 

   empirical results
present performance evaluation stochastic enforced hill climbing  seh  section 
experiments underlying results presented involve         planner runs    categories 
    summary comparison
results table   show that  cr ff heuristic  seh goal ordering addedgoal deletion enhancements  seh   f    improves significantly baseline seh technique
 seh f    category b locksworld  show significant changes aggregated
performance non blocksworld problems    remainder experiments involving crff  evaluate seh   f    noting comparison planners  ff replan rff 
benefit goal ordering added goal deletion enhancements base planner  ffplan 
results present next seh   f   show 
seh   f   significantly outperforms greedy f      categories  outperformed
greedy f   chedule  three categories comparison inconclusive  b usfare  r iver ireworld   see table   details 
ff replan inapplicable two categories  ippc  earch    r escue ippc 
ysadmin   seh   f   significantly outperforms ff replan    categories  outperformed ff replan three categories  e xploding b locksworld  p itchcatch 
z enotravel   two categories comparison inconclusive  f ile world r iver    seh   f   significantly outperforms ff replan combined
category pi problems  although winner varied aggregated categories  see
table   details 
rff bg inapplicable two categories  b usfare ippc  f ileworld   seh   f  
significantly outperforms rff bg    categories  outperformed rff bg two
categories  e xploding b locksworld ystematic   tire   one category
comparison inconclusive  s ysadmin   seh   f   significantly outperforms rff bg combined category pi problems  although winner varied
aggregated categories  see table   details 
learned heuristic work wu givan        computed
subset domains  hence seven categories applicable evaluation using
learned heuristic  see online appendix details   results present next seh
learned heuristic  seh l   show 
seh l  significantly outperforms greedy l  six categories  one category
 t ireworld  comparison inconclusive  see table   details 
   show p values rounded two decimal places  example  show p      value p rounded two
decimal places   

   

fiw u   k alyanam     g ivan

sr
sr
seh   f   seh f  

category

jslenratio
 seh 
seh   

jstimeratio
 seh 
seh   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

n   blocksworld

    

    

    

    



 p      



 p      

inconclusive

table    aggregated comparison seh   f   seh f   

seh l  significantly outperforms ff replan five categories  outperformed ffreplan two categories  e xploding b locksworld z enotravel   see table  
details 
    discussion
discuss results comparisons pairs planners  including seh versus greedy
heuristic following  seh versus ff replan  seh versus rff bg 
      seh seh  v ersus g reedy
primary evaluation goal show stochastic enforced hill climbing generally improves
significantly upon greedy following heuristic  using policy greedy h  described
technical background above   demonstrated evaluating seh two different
heuristics tables      seh h  significantly outperforms greedy h  nineteen
twenty four heuristic category pairs  losing chedule seh   f   greedy f   
discuss category greedy outperforms seh techniques significantly 
chedule  multiple classes network packets different arrival rates  packets deadlines  packet served deadline  agent encounters classdependent risk death well delay packet cleaned up  reach goal
serving packet every class  agent must minimize dropping related risk dying
waiting arrival low arrival rate class  all outcomes determinization underlying
cr ff heuristic gives deterministic domain definition dying optional  never chosen 
unlikely packet arrivals happen choice  leading optimistic heuristic value 
using optimistic heuristic value  basic local goal seh  improve
current state heuristic  leads building large local mdps analysis  presence
dead ends  death  above   even arbitrarily large local mdps may able achieve local
improvement  chedule  seh  typically hit resource limit mdp size
every action step 
contrast  greedy local decision making well suited packet scheduling  many well known
packet scheduling policies  e g  earliest deadline first static priority work liu  
layland        make greedy local decisions practically quite effective  experiments 
greedy policy applied cr ff benefits locally seeking avoid incidental delays
dropped packet cleanup  even though heuristic sees risk of dying cost dropping  still
recognizes delay cleaning lost dropped packets  thus  greedy f   class insensitive
   

fis tochastic e nforced h ill  c limbing

jslensr
ratio
sr
seh   f   greedy f    greedy 
seh   

category

jstimeratio
 greedy 
seh   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b usfare

    

    

    

    



 p      



 p      

inconclusive

rive

    

    

    

    

yes

 p      

yes

 p      

seh   f  

e levator

    

    

    

    

yes

 p      

yes

 p      

seh   f  

e xploding
b locksworld

    

    

    

    

yes

 p      



 p      

seh   f  

f ileworld

    

    

    

    

yes

 p      



 p      

seh   f  

p itchcatch

    

    





yes

 p      

r andom

    

    

    

    

yes

 p      

yes

 p      

seh   f  

r iver

    

    

    

    



 p      



 p      

inconclusive

chedule

    

    

    

    

yes

 p      

yes

 p      

greedy f  

earch
r escue

    

    

    

    



 p      

yes

 p      

seh   f  

ysadmin

    

    

    

    



 p      

yes

 p      

seh   f  

ystematic
  tire

    

    

    

    

yes

 p      



 p      

seh   f  

ireworld

    

    

    

    



 p      



 p      

inconclusive

owers
h anoi

    

    





yes

 p      

    

    

    

    

yes

 p      



z enotravel




yes

 p      

seh   f  

seh   f  
seh   f  

table    aggregated comparison seh   f   greedy f    r iver domain evaluation required extending sampling    samples per experimental protocol described section        values p values jslen ratio jstime ratio p itchcatch
owers h anoi available due zero success ratio greedy f   categories 

   

fiw u   k alyanam     g ivan

jslensr
sr
ratio
seh   f   ff replan  ffr 
seh   f   

category

jstimeratio
 ffr 
seh   f   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b usfare

    

    

    

    

yes

 p      

rive

    

    

    

    

yes

 p      

yes

 p      

seh   f  

e levator

    

    

    

    

yes

 p      



 p      

seh   f  

e xploding
b locksworld

    

    

    

    



 p      

yes

 p      

ff replan

f ileworld

    

    

    

    



 p      



 p      

inconclusive

p itchcatch

    

    

    

    

yes

 p      

yes

 p      

ff replan

r andom

    

    

    

    

yes

 p      

yes

 p      

seh   f  

r iver

    

    

    

    



 p      



 p      

inconclusive

chedule

    

    

    

    

yes

 p      



 p      

seh   f  

ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

seh   f  

ireworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

owers
h anoi

    

    

    

    

yes

 p      

yes

 p      

seh   f  

z enotravel

    

    

    

    

yes

 p      

yes

 p      

ff replan

pi
p roblems

    

    

    

    

yes

 p      



 p      

seh   f  





seh   f  

table    aggregated comparison seh   f   ff replan  ffr   r andom r iver
domains required extending sampling    samples owers h anoi domain required
extending sampling     samples per experimental protocol described section       
p value jslen ratio b usfare available extremely low success rate
ffr leads one sample jslen gathered    attempts  yielding estimated
variance 

   

fis tochastic e nforced h ill  c limbing

sr
sr
seh   f   rff bg

category

jslenratio
 rff bg 
seh   f   

jstimeratio
 rff bg 
seh   f   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      



 p      

seh   f  

rive

    

    

    

    

yes

 p      



 p      

seh   f  

e levator

    

    

    

    



 p      

yes

 p      

seh   f  

e xploding
b locksworld

    

    

    

    



 p      

yes

 p      

rff bg

p itchcatch

    

    





yes

 p      

r andom

    

    

    

    

yes

 p      

yes

 p      

seh   f  

r iver

    

    

    

    

yes

 p      

yes

 p      

seh   f  

chedule

    

    

    

    

yes

 p      



 p      

seh   f  

earch
r escue

    

    

    

    

yes

 p      

yes

 p      

seh   f  

ysadmin

    

    

    

    



 p      



 p      

inconclusive

ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

rff bg

ireworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

owers
h anoi

    

    

    

    

yes

 p      

yes

 p      

seh   f  

z enotravel

    

    

    

    

yes

 p      



 p      

seh   f  

pi
p roblems

    

    

    

    

yes

 p      

yes

 p      

seh   f  





seh   f  

table    aggregated comparison seh   f   rff bg  r iver owers
h anoi domains required extending sampling    samples per experimental protocol described section        values p values jslen ratio jstime ratio p itch catch available due zero success ratio rff bg category 

   

fiw u   k alyanam     g ivan

sr
seh l 

category

jslensr
ratio
greedy l   greedy 
seh 

jstimeratio
 greedy 
seh 

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    



 p      

yes

 p      

seh l 

b oxworld

    

    

    

    



 p      

yes

 p      

seh l 

e xploding
b locksworld

    

    

    

    

yes

 p      



 p      

seh l 

ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

seh l 

ireworld

    

    

    

    



 p      



 p      

inconclusive

owers
h anoi

    

    





yes

 p      

    

    

     

    

yes

 p      



z enotravel


yes

 p      

seh l 
seh l 

table    aggregated comparison seh l  greedy l   values jslen ratio
jstime ratio p value jslen ratio owers h anoi available due
zero success ratio greedy l  category 

sr
seh l 

category

jslensr
ratio
ff replan  ffr 
seh l  

jstimeratio
 ffr 
seh l  

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

winner

b locksworld

    

    

    

    

yes

 p      



 p      

seh l 

b oxworld

    

    

    

    



 p      

yes

 p      

seh l 

e xploding
b locksworld

    

    

    

    

yes

 p      

yes

 p      

ff replan

ystematic
  tire

    

    

    

    

yes

 p      

yes

 p      

seh l 

ireworld

    

    

    

    

yes

 p      

yes

 p      

seh l 

owers
h anoi

    

    

    

    

yes

 p      

yes

 p      

seh l 

    

    

    

    

yes

 p      

yes

 p      

ff replan



z enotravel

table    aggregated comparison seh l  ff replan  ffr  

   

fis tochastic e nforced h ill  c limbing

policy greedily seeks avoid dropping  similar earliest deadline first  problems
seh encounters evaluation chedule suggest future work automatically recognizing
domains large mdp construction proving futile automatically reducing mdp size
limits adapt performance towards behavior greedy policy  note across tested
benchmark domains heuristics  one domain heuristic combination
phenomenon arose practice 
      seh seh  v ersus ff r eplan



rff bg

demonstrated performance improvement seh   f   best performing planners first three international probabilistic planning competitions  outperforming ff replan
ten fifteen categories losing three  e xploding b locksworld  p itchcatch 
z enotravel   outperforming rff bg       categories losing e xploding
b locksworld ystematic   tire  additionally  seh l  outperforms ff replan five
seven categories losing e xploding b locksworld z enotravel  section
discuss categories seh   f   seh l  lose ff replan rff bg 
z enotravel logistics domain people transported cities via airplanes
load unload fly action non zero probability effect  result  takes
uncertain number attempts complete task  domains probabilistic effect choice change change  all outcome determinization leads
safe determinized plan ff replanone replanning needed reach goal 
domains  including z enotravel  all outcomes determinization provide effective
way employ deterministic enforced hill climbing problem  note though though 
determinization still ignores probabilities action outcomes  lead bad
choices domains  not z enotravel   deterministic stochastic enforced
hill climbing must climb large basins z enotravel  substantial overhead stochastic backup computations basin expansion leads least constant factor advantage deterministic expansion  extension seh might address problem successfully future
research would detect domains stochastic choice change non change 
handle domains emphasis determinization 
e xploding b locksworld variant blocks world two new predicates detonated destroyed  block detonate once  put down  probability 
destroying object placed upon  state resulting action depicted figure   delete relaxed path goal  actual path  state dead end attractor
delete relaxation heuristics cr ff  ff replan rff bg never select action
path goal including action  seh   f   weak dead end detection used experiments select dead action shown  resulting poor performance
situation arises  would possible use all outcomes determinization improved
dead end detector conjunction seh   f   order avoid selecting actions 
dead end detection would carefully implemented managed control run time
costs incurred seh relies critically able expand sufficiently large local mdp regions
online action selection 
p itchcatch  unavoidable dead end states  used domain designers simulate cost penalties   however  cr ff heuristic  based all outcomes determinization  assigns optimistic values correspond assumed avoidance dead end states 
   

fiw u   k alyanam     g ivan

current
state

b 

b 

b 

b 

b 

b 

b 

b 

b 

goal state

path
destroyed table

pick table b 

b 

b 

b 

b 

b 

destroyed table

figure    illustration critical action choice seh   f   e xploding b locksworld
problem  ippc  p    middle state actual path goal delete relaxed path
goal  due table exploded  block placed onto table  resulting
middle state dead end state  middle state dead end attractive heuristic
value without regard whether blocks shown remaining explosive charge not 
state feature shown 
result  local search seh   f   unable find expected improvement cr ff values 
falls back biased random walk domain  domain suggests  domains seh   f   performs weakly  work needed managing domains
unavoidable deadend states 
two categories seh l  loses ff replan  e xploding b locksworld
z enotravel  categories seh   f   loses ff replan  greedily following
learned heuristics two categories leads lower success ratio greedily following crff  suggesting significant flaws learned heuristics cr ff  although seh able
give least five fold improvement greedy following  success ratio two categories  improvement large enough seh l  match performance seh   f  
ff replan  based relaxed plan heuristic ff 
seh  loses rff ystematic   tire due weak performance triangle tireworld problems  triangle tireworld provides map connected locations arranged single
safe path source destination  exponentially many shorter unsafe paths   
determinizing heuristics detect risk unsafe paths greedy following
heuristics lead planners  such seh    take unsafe paths  lowering success rate 
results show seh  often repair flawed heuristic  triangle tireworld domain heuristic attracts seh  apparent improvements actually dead ends 
contrast  rff designed increase robustness determinized plans high probability failure  rff continue planning avoid failure rather relying replanning
failure  initial determinized plan high probability failure  relative rffs
   safe path drawn following two sides triangular map  many unsafe paths interior
triangle  safety domain represented presence spare tires repair flat tire    
chance occurring every step 

   

fis tochastic e nforced h ill  c limbing

jslenratio
 ffr 
seh   

jstimeratio
 ffr 
seh   

sr
difference
significant 
 p value 

jslenratio
significant 
 p value 

sr
seh   f  

sr
ffreplan

b locksworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

b oxworld

    

    

    

    

yes

 p      

yes

 p      

seh   f  

category

winner

table     aggregated comparison seh   f   ff replan scaled up problems 

category

sr
seh   f  

sr
rffbg

jslenratio
 rffbg 
seh   

jstimeratio
 rffbg 
seh   

sr
difference
significant 
 p value 

b locksworld

    

    

    

    

yes

 p      

b oxworld

    

    

    

     

yes

 p      

jslenratio
significant 
 p value 

yes

 p      


winner

seh   f  
seh   f  

table     aggregated comparison seh   f   rff bg scaled up problems 

threshold   rff extends plan execution often detect need use longer 
safe route 
      p erformance l arge p roblems
order demonstrate advantages seh emphasized problem size grows 
present aggregated performance seh   f   additional large sized problems generated using generators provided first ippc  scaling experiments computationally
expensive  run two domains widely evaluated planning literature  b locksworld b oxworld  which stochastic version logistics  
b locksworld  generated    problems        block problems  b oxworld 
generated    problems size    cities    boxes   only one problem across three
competitions reached size b oxworld  problem unsolved competition
winner  rff   aggregated results ff replan rff bg presented tables   
    experiments scaled up problems consumed       hours cpu time
show seh   f   successfully completed majority attempts ff replan rff
succeeded substantially less often   
note although heuristic good b oxworld logistics domains 
failure all outcomes determinization take account probabilities action outcomes
quite damaging ffr b oxworld  leading planner often select action hoping
   statistical protocol requires    samples random variable averaging performance   solution attempts 
planner problem     problems   planners  yields                  solution attempts 
taking approximately    cpu minutes large problems 

   

fiw u   k alyanam     g ivan

low probability error outcome  note rff uses most probable outcome determinization
suffer issues ffr boxworld  given high accuracy
heuristic boxworld  believe ideas rff likely re implemented and or
tuned achieve better scalability boxworld problems  leave possibility direction
future work understanding scalability rff 

   summary
proposed evaluated stochastic enforced hill climbing  novel generalization
deterministic enforced hill climbing method used planner  hoffmann   nebel        
generalizing deterministic search descendant strictly better current state
heuristic value  analyze heuristic based mdp around local optimum plateau reached
increasing horizons seek policy expects exit mdp better valued state 
demonstrated approach provides substantial improvement greedy hill climbing
heuristics created using two different styles heuristic definition  demonstrated
one resulting planner substantial improvement ff replan  yoon et al        
rff  teichteil konigsbuch et al         experiments 
find runtime stochastic enforced hill climbing concern domains 
one reason long runtime number size local optima basins plateaus may
large  currently  long runtime managed primarily reducing biased random walk
resource consumption exceeds user set thresholds  possible future research direction regarding
issue prune search space automatically state action pruning 

acknowledgments
material based upon work supported part national science foundation  united
states grant no          national science council  republic china         m                 m          

references
aarts  e     lenstra  j   eds            local search combinatorial optimization  john wiley  
sons  inc 
barto  a  g   bradtke  s  j     singh  s  p          learning act using real time dynamic programming  artificial intelligence            
bertsekas  d  p          dynamic programming optimal control  athena scientific 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
bonet  b     geffner  h          mgpt  probabilistic planner based heuristic search  journal
artificial intelligence research             
bonet  b     geffner  h          learning depth first search  unified approach heuristic search
deterministic non deterministic settings  application mdps  proceedings
sixteenth international conference automated planning scheduling  pp     
    
   

fis tochastic e nforced h ill  c limbing

bryce  d     buffet  o          international planning competition uncertainty part  benchmarks
results   http   ippc      loria fr wiki images      results pdf 
buffet  o         personal communication 
cerny  v          thermodynamical approach traveling salesman problem  efficient simulation algorithm  j  optim  theory appl            
dean  t   kaelbling  l  p   kirman  j     nicholson  a          planning time constraints
stochastic domains  artificial intelligence           
domshlak  c     hoffmann  j          probabilistic planning via heuristic forward search
weighted model counting  journal artificial intelligence research             
fahlman  s     lebiere  c          cascade correlation learning architecture  advances
neural information processing systems    pp          
gardiol  n  h     kaelbling  l  p          envelope based planning relational mdps  proceedings seventeenth annual conference advances neural information processing
systems 
gerevini  a     serina  i          planning propositional csp  walksat local search
techniques action graphs  constraints               
gordon  g          stable function approximation dynamic programming  proceedings
twelfth international conference machine learning  pp         
hansen  e     zilberstein  s          lao   heuristic search algorithm finds solutions
loops  artificial intelligence            
hoffmann  j          metric ff planning system  translating ignoring delete lists numeric
state variables  journal artificial intelligence research             
hoffmann  j     brafman  r          contingent planning via heuristic forward search implicit
belief states  proceedings   th international conference automated planning
scheduling 
hoffmann  j     brafman  r          conformant planning via heuristic forward search  new
approach  artificial intelligence                    
hoffmann  j          ignoring delete lists works  local search topology planning benchmarks  journal artificial intelligence research             
hoffmann  j     nebel  b          planning system  fast plan generation heuristic
search  journal artificial intelligence research             
kautz  h     selman  b          planning satisfiability  proceedings tenth european
conference artificial intelligence  ecai    
kirkpatrick  s   gelatt  jr  c     vecchi  m          optimization simulated annealing  science 
            
little  i     thiebaux  s          probabilistic planning vs replanning  workshop international
planning competition  past  present future  icaps  
liu  c     layland  j          scheduling algorithms multiprogramming hard real time
environment  journal association computing machinery           
   

fiw u   k alyanam     g ivan

mahadevan  s     maggioni  m          proto value functions  laplacian framework learning representation control markov decision processes  journal machine learning
research              
nilsson  n          principles artificial intelligence  tioga publishing  palo alto  ca 
puterman  m  l          markov decision processes  discrete stochastic dynamic programming 
john wiley   sons  inc 
sanner  s     boutilier  c          practical solution techniques first order mdps  artificial
intelligence                   
selman  b   kautz  h     cohen  b          local search strategies satisfiability testing 
dimacs series discrete mathematics theoretical computer science  pp         
sutton  r  s          learning predict methods temporal differences  machine learning 
       
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
teichteil konigsbuch  f   kuter  u     infantes  g          incremental plan aggregation generating policies mdps  proceedings ninth international conference autonomous
agents multiagent systems  aamas        pp           
tesauro  g     galperin  g          on line policy improvement using monte carlo search 
nips 
wu  j     givan  r          discovering relational domain features probabilistic planning 
proceedings seventeenth international conference automated planning
scheduling  pp         
wu  j     givan  r          automatic induction bellman error features probabilistic planning  journal artificial intelligence research             
yoon  s   fern  a     givan  r          ff replan  baseline probabilistic planning  proceedings seventeenth international conference automated planning scheduling  pp         
younes  h   littman  m   weissman  d     asmuth  j          first probabilistic track
international planning competition  journal artificial intelligence research             

   


