journal of artificial intelligence research                  

submitted        published      

combining evaluation metrics via the unanimous
improvement ratio and its application to clustering tasks
enrique amigo
julio gonzalo

enrique lsi uned es
julio lsi uned es

uned nlp   ir group  juan del rosal   
madrid        spain

javier artiles

javier artiles qc cuny edu

blender lab  queens college  cuny  
      kissena blvd  ny        usa

felisa verdejo

felisa lsi uned es

uned nlp   ir group  juan del rosal   
madrid        spain

abstract
many artificial intelligence tasks cannot be evaluated with a single quality criterion
and some sort of weighted combination is needed to provide system rankings  a problem of
weighted combination measures is that slight changes in the relative weights may produce
substantial changes in the system rankings  this paper introduces the unanimous improvement ratio  uir   a measure that complements standard metric combination criteria
 such as van rijsbergens f measure  and indicates how robust the measured differences
are to changes in the relative weights of the individual metrics  uir is meant to elucidate
whether a perceived difference between two systems is an artifact of how individual metrics
are weighted 
besides discussing the theoretical foundations of uir  this paper presents empirical
results that confirm the validity and usefulness of the metric for the text clustering problem  where there is a tradeoff between precision and recall based metrics and results are
particularly sensitive to the weighting scheme used to combine them  remarkably  our
experiments show that uir can be used as a predictor of how well differences between
systems measured on a given test bed will also hold in a different test bed 

   introduction
many artificial intelligence tasks cannot be evaluated with a single quality criterion  and
some sort of weighted combination is needed to provide system rankings  many problems 
for instance  require considering both precision  p  and recall  r  to compare systems
performance  perhaps the most common combining function is the f measure  van rijsbergen         which includes a parameter  that sets the relative weight of metrics  when
        both metrics have the same relative weight and f computes their harmonic mean 
a problem of weighted combination measures is that relative weights are established
intuitively for a given task  but at the same time a slight change in the relative weights may
produce substantial changes in the system rankings  the reason for this behavior is that
an overall improvement in f often derives from an improvement in one of the individual
c
    
ai access foundation  all rights reserved 

fiamigo  gonzalo  artiles   verdejo

metrics at the expense of a decrement in the other  for instance  if a system a improves a
system b in precision with a loss in recall  f may say that a is better than b or viceversa 
depending on the relative weight of precision and recall  i e  the  value  
this situation is more common than one might expect  table   shows evaluation results
for different tasks extracted from the acl       su et al         conference proceedings 
in which p and r are combined using the f measure  for each paper we have considered
three evaluation results  the one that maximizes f  which is presented as the best result
in the paper  the baseline  and an alternative method that is also considered  note that in
all cases  the top ranked system improves the baseline according to the f measure  but at
the cost of decreasing one of the metrics  for instance  in the case of the paper on word
alignment  the average r grows from       to        while p decreases from       to       
in the paper on sentiment analysis  p increases in four points but r decreases in five points 
how reasonable is to assume that the contrastive system is indeed improving the baseline 
the evaluation results for the alternative approach are also controversial  in all cases 
the alternative approach improves the best system according to one metric  and it is improved according to the other  therefore  depending on the relative metric weighting  the
alternative approach could be considered better or worse than the best scored system 
the conclusion is that the  parameter is crucial when comparing real systems  in
practice  however  most authors set         equal weights for precision and recall  as a
standard  agnostic choice that requires no further justification  thus  without a notion of
how much a perceived difference between systems depends on the relative weights between
metrics  the interpretation of results with f  or any other combination scheme  can be
misleading 
our goal is  therefore  to find a way of estimating to what extent a perceived difference
using a metric combination scheme  f or any other  is robust to changes in the relative
weights assigned to each individual metric 
in this paper we propose a novel measure  the unanimity improvement ratio  uir  
which relies on a simple observation  when a system a improves other system b according to
all individual metrics  the improvement is unanimous   a is better than b for any weighting
scheme  given a test collection with n test cases  the more test cases where improvements
are unanimous  the more robust the perceived difference  average difference in f or any
other combination scheme  will be 
in other words  as well as statistical significance tests provide information about the
robustness of the evaluation across test cases  is the perceived difference between two systems
an artifact of the set of test cases used in the test collection     uir is meant to provide
information about the robustness of the evaluation across variations of the relative metric
weightings  is the perceived difference between two systems an artifact of the relative metric
weighting chosen in the evaluation metric    
our experiments on clustering test collections show that uir contributes to the analysis
of evaluation results in two ways 
 it allows to detect system improvements that are biased by the metric weighting
scheme  in such cases  experimenters should carefully justify a particular choice of
relative weights and check whether results are swapped in their vicinity 
   

ficombining evaluation metrics via the unanimous improvement ratio

systems
precision recall
f
task  word alignment  huang      
baseline
bm
     
           
max  f
link select
     
           
alternative
me
     
           
task  opinion question answering  li et al       
baseline
system  
    
    
    
max  f
ophit
    
    
    
alternative
oppagerank
    
    
  
task  sentiment analysis  kim et al      
baseline
baseline
    
    
    
max  f
vs lsa dtp
    
    
  
alternative
vs pmi
    
    
    
task  lexical reference rule extraction  shnarch et al      
baseline
no expansion
  
  
  
max  f
wordnet wiki
  
  
  
alternative all rules   dice filter
  
  
  
table    a few three way system comparisons taken from acl      conference proceedings  su et al        

 it increases substantially the consistency of evaluation results across datasets  a result
that is supported by a high unanimous improvement ratio is much more likely to hold
in a different test collection  this is  perhaps  the most relevant practical application
of uir  as a predictor of how much a result can be replicable across test collections 
although most of the work presented in this paper applies to other research areas  here
we will focus on the clustering task as one of the most relevant examples because clustering
tasks are specially sensitive to the metric relative weightings  our research goals are 
   to investigate empirically whether clustering evaluation can be biased by precision
and recall relative weights in f  we will use one of the most recent test collections
focused on a text clustering problem  artiles  gonzalo    sekine        
   to introduce a measure that quantifies the robustness of evaluation results across
metric combining criteria  which leads us to propose the uir measure  which is derived
from the conjoint measurement theory  luce   tukey        
   to analyze empirically how uir and f measure complement each other 
   to illustrate the application of uir when comparing systems in the context of a shared
task  and measure how uir serves as a predictor of the consistency of evaluation
results across different test collections 

   

fiamigo  gonzalo  artiles   verdejo

figure    evaluation results for semantic labeling in conll     

   combining functions for evaluation metrics
in this section we briefly review different metrics combination criteria  we present the
rationale behind each metric weighting approach as well as its effects on the systems ranking 
    f measure
the most frequent way of combining two evaluation metrics is the f measure  van rijsbergen         it was originally proposed for the evaluation of information retrieval systems
 ir   but its use has expanded to many other tasks  given two metrics p and r  e g 
precision and recall  purity and inverse purity  etc    van rijsbergens f measure combines
them into a single measure of efficiency as follows 
f  r  p    

  p   

 
         r   

f assumes that the  value is set for a particular evaluation scenario  this parameter
represents the relative weight of metrics  in some cases the  value is not crucial  in
particular  when metrics are correlated  for instance  figure   shows the precision and
recall levels obtained at the conll      shared task for evaluating semantic role labeling
systems  carreras   marquez         except for one system  every substantial improvement
in precision involves also an increase in recall  in this case  the relative metric weighting
does not substantially modify the system ranking 
in cases where the metrics are not completely correlated  the decreasing marginal effectiveness property  van rijsbergen        ensures a certain robustness across  values  f
satisfies this property  which states that a large decrease in one metric cannot be compensated by a large increase in the other metric  therefore  systems with very low precision
or recall will obtain low f values for any  value  this is discussed in more detail in section      however  as we will show in section      in other cases the decreasing marginal
   

ficombining evaluation metrics via the unanimous improvement ratio

effectiveness property does not prevent the f measure from being overly sensitive to small
changes in the  value 

figure    an example of two systems evaluated by break even point

    precision and recall break even point
another way of combining metrics consists of evaluating the system at the point where one
metric equals the other  tao li   zhu         this method is applicable when each system
is represented by a trade off between both metrics  for instance  a precision recall curve 
this method relies on the idea that increasing both metrics implies necessarily a overall
quality increase  for instance  it assumes that obtaining a     of precision at the recall
point     is better than obtaining a     of precision at the recall point     
actually  the break even point assumes the same relevance for both metrics  it considers
the precision recall point where the system distributes its efforts equitably between both
metrics  indeed  we could change the relative relevance of metrics when computing the
break even point 
figure   illustrates this idea  the continuous curve represents the trade off between
precision and recall for system s   the straight diagonal represents the points where both
metrics return the same score  the quality of the system corresponds therefore with the
intersection between this diagonal and the precision recall curve  on the other hand  the
discontinuous curve represents another system s  which achieves an increase of precision
at low recall levels at the cost of decreasing precision at high recall levels  according to the
break even points  the second system is superior than the first one 
however  we could give more relevance to recall identifying the point where recall doubles
precision  in that case  we would obtain the intersection points q   and q   shown in the
figure  which reverses the quality order between systems  in conclusion  the break even
point also assumes an arbitrary relative relevance for the combined metrics 
    area under the precision recall curve
some approaches average scores over every potential parameterization of the metric combining function  for instance  mean average precision  map  is oriented to ir systems 
   

fiamigo  gonzalo  artiles   verdejo

and computes the average precision across a number of recall levels  another example is
the receiver operating characteristic  roc  function used to evaluate binary classifiers
 cormack   lynam         roc computes the probability that a positive sample receives
a confidence score higher than a negative sample  independently from the threshold used to
classify the samples  both functions are related with the area auc that exists under the
precision recall curve  cormack   lynam        
in both map and roc the low and high recall regions have the same relative relevance
when computing this area  again  we could change the measures in order to assign different
weights to high and low recall levels  indeed in  weng   poon        a weighted area under
the curve is proposed  something similar would happen if we average f across different 
values 
note that these measures can only be applied in certain kinds of problem  such as binary
classification or document retrieval  where the system output can be seen as a ranking  and
different cutoff points in the ranking give different precision recall values  they are not
directly applicable  in particular  to the clustering problem which is the focus of our work
here 

   combining metrics in clustering tasks
in this section we present metric combination experiments on a specific clustering task  our
results corroborate the importance of quantifying the robustness of systems across different
weighting schemes 
    the clustering task
clustering  grouping similar items  has applications in a wide range of artificial intelligence
problems  in particular  in the context of textual information access  clustering algorithms
are employed for information retrieval  clustering text documents according to their content
similarity   document summarization  grouping pieces of text in order to detect redundant
information   topic tracking  opinion mining  e g  grouping opinions about a specific topic  
etc 
in such scenarios  clustering distributions produced by systems are usually evaluated
according to their similarity to a manually produced gold standard  extrinsic evaluation  
there is a wide set of metrics that measure this similarity  amigo  gonzalo  artiles   
verdejo         but all of them rely on two quality dimensions   i  to what extent items
in the same cluster also belong to the same group in the gold standard  and  ii  to what
extent items in different clusters also belong to different groups in the gold standard  a
wide set of extrinsic metrics has been proposed  entropy and class entropy  steinbach 
karypis    kumar        ghosh         purity and inverse purity  zhao   karypis        
precision and recall bcubed metrics  bagga   baldwin         metrics based on counting
pairs  halkidi  batistakis    vazirgiannis        meila         etc  

   see the work of amigo et al         for a detailed overview 

   

ficombining evaluation metrics via the unanimous improvement ratio

    dataset
weps  web people search  campaigns are focused on the task of disambiguating person
names in web search results  the input for systems is a ranked list of web pages retrieved
from a web search engine using a person name as a query  e g  john smith   the
challenge is to correctly estimate the number of different people sharing the name in the
search results and group documents referring to the same individual  for every person
name  weps datasets provide around     web pages from the top search results  using the
quoted person name as query  in order to provide different ambiguity scenarios  person
names were sampled from the us census  wikipedia  and listings of program committee
members of computer science conferences 
systems are evaluated comparing their output with a gold standard  a manual grouping
of documents produced by two human judges in two rounds  first they annotated the corpus
independently and then they discussed the disagreements together   note that a single
document can be assigned to more than one cluster  an amazon search results list  for
instance  may refer to books written by different authors with the same name  the weps
task is  therefore  an overlapping clustering problem  a more general case of clustering where
items are not restricted to belong to one single cluster  both the weps datasets and the
official evaluation metrics reflect this fact 
for our experiments we have focused on the evaluation results obtained in the weps  
 artiles  gonzalo    sekine        and weps    artiles et al         evaluation campaigns 
the weps   corpus also includes data from the web   test bed  mann         which was
used for trial purposes and follows similar annotation guidelines  although the number of
document per ambiguous name is more variable  we will refer to these corpora as weps  a
 trial   weps  b and weps      
    thresholds and stopping criteria
the clustering task involves three main aspects that determine the systems output quality 
the first one is the method used for measuring similarity between documents  the second is
the clustering algorithm  k neighbors  hierarchical agglomerative clustering  etc    and the
third aspect to be considered usually consists of a couple of related variables to be fixed  a
similarity threshold  above which two pages will be considered as related  and a stopping
criterion which determines when the clustering process stops and  consequently  the number
of clusters produced by the system 
figure   shows how purity and inverse purity values change for different clustering
stopping points  for one of the systems evaluated on the weps  b corpus     purity focuses
on the frequency of the most common category into each cluster  amigo et al          being
c the set of clusters to be evaluated  l the set of categories  reference distribution  and
   the weps datasets were selected for our experiments because  i  they address a relevant and well defined
clustering task   ii  its use is widespread  weps datasets have been used in hundreds of experiments
since the first weps evaluation in        iii  runs submitted by participants to weps   and weps   were
available to us  which was essential to experiment with different evaluation measures  weps datasets
are freely available from http   nlp uned es weps 
   this system is based on bag of words  tf idf word weighting  stopword removal  cosine distance and
a hierarchical agglomerative clustering algorithm 

   

fiamigo  gonzalo  artiles   verdejo

n the number of clustered items  purity is computed by taking the weighted average of
maximal precision values 
purity  

x  ci  
i

n

maxj precision ci   lj  

where the precision of a cluster ci for a given category lj is defined as 
 ci lj  
precision ci   lj    
 ci  
t

purity penalizes the noise in a cluster  but it does not reward grouping items from
the same category together  if we simply make one cluster per item  we reach trivially a
maximum purity value  inverse purity focuses on the cluster with maximum recall for each
category  inverse purity is defined as 
inverse purity  

x  li  
i

n

maxj precision li   cj  

inverse purity rewards grouping items together  but it does not penalize mixing items from
different categories  we can reach a maximum value for inverse purity by making a single
cluster with all items 
any change in the stopping point implies an increase in purity at the cost of a decrease in
inverse purity  or viceversa  therefore  each possible  value in f rewards different stopping
points  this phenomenon produces a high dependency between clustering evaluation results
and the metric combining function 

figure    an example of the trade off between purity and inverse purity when optimizing
the grouping threshold

   

ficombining evaluation metrics via the unanimous improvement ratio

    robustness across  values
determining the appropriate  value for a given scenario is not trivial  for instance  from a
users point of view in the weps task  it is easier to discard a few irrelevant documents from
the good cluster  because its precision is not perfect but it has a high recall  than having
to check for additional relevant documents in all clusters  because its precision is high but
its recall is not   therefore  it seems that inverse purity should have priority over purity 
i e   the value of  should be below      from the point of view of a company providing a
web people search service  however  the situation is quite different  their priority is having
a very high precision  because mixing the profiles of  say  a criminal and a doctor may result
in the company being sued  from their perspective   should receive a high value  the
weps campaign decided to be agnostic and set a neutral        value 
table   shows the resulting system ranking in weps  b according to f with  set at    
and      this ranking includes two baseline systems  b  consists of grouping each document
in a separate cluster  and b    consists of grouping all documents into one single cluster 
b  maximizes purity  and b    maximizes inverse purity 
b  and b    may obtain a high or low f measure depending on the  value  as the
table shows  for        b  outperforms b    and also a considerable number of systems 
the reason for this result is that  in the weps  b test set  there are many singleton clusters
 people which are referred to in only one web page   this means that a default strategy
of making one cluster per document will not only achieve maximal purity  but also an
acceptable inverse purity         however  if  is fixed at      b  goes down to the bottom
of the ranking and it is outperformed by all systems  including the other baseline b     
note that outperforming a trivial baseline system such as b  is crucial to optimize
systems  given that the optimization cycle could otherwise lead to a baseline approach like
b    the drawback of b  is that it is not informative  the output does not depend on the
input  and  crucially  it is very sensitive to variations in   in other words  its performance
is not robust to changes in the metric combination criterion  remarkably  the top scoring
system  s    is the best for both  values  our primary motivation in this article is to
quantify the robustness across  values in order to complement the information given by
traditional system ranking 
    robustness across test beds
the average size of the clusters in the gold standard may change from one test bed to
another  as this affects the purity and inverse purity trade off  the same clustering system
may obtain a different balance between both metrics in different corpora  and this may
produce contradictory evaluation results when comparing systems across different corpora 
even for the same  value 
for instance  in the weps  b test bed  artiles et al          b  substantially outperforms
b          vs       using f        in the weps   data set  artiles et al          however 
b    outperforms b        versus        the reason is that singletons are less common in
weps    in other words  the comparison between b    and b  depends both on the  value
and of the particular distribution of reference cluster sizes in the test bed 
our point is that system improvements that are robust across  values  which is not the
case of b  and b      should not be affected by this phenomenon  therefore  estimating the
   

fiamigo  gonzalo  artiles   verdejo

f    
ranked systems f result
s 
    
s 
    
s 
    
s 
    
s 
    
s 
    
s 
    
b 
    
s 
    
s 
    
s  
    
s  
    
s  
    
s  
    
s  
    
s  
    
b   
   
s  
   

f   
ranked systems
s 
s 
s 
s 
s 
s 
s  
s 
s  
s  
s  
s 
s  
s 
s  
b   
s  
b 

f result
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   
    
    
    

table    weps  b system ranking according to f     vs f     using purity and inverse
purity

robustness of system improvements to changes in  should prevent reaching contradictory
results for different test beds  indeed  evidence for this is presented in section   

   proposal
our primary motivation in this article is to quantify the robustness across  values in
order to complement the information given by traditional system rankings  to this end we
introduce in this section the unanimous improvement ratio 
    unanimous improvements
the problem of combining evaluation metrics is closely related with the theory of conjoint
measurement  see section     for a detailed discussion   van rijsbergen        argued that
it is not possible to determine empirically which metric combining function is the most
adequate in the context of information retrieval evaluation  however  starting from the
measurement theory principles  van rijsbergen described the set of properties that a metric
combining function should satisfy  this set includes the independence axiom  also called
single cancellation   from which the monotonicity property derives  the monotonicity
property states that the quality of a system that surpasses or equals another one according
to all metrics is necessarily equal or better than the other  in other words  one system is
   

ficombining evaluation metrics via the unanimous improvement ratio

better than the other with no dependence whatsoever on how the relative importance of
each metric is set 
we will define a combination procedure for metrics  unanimous improvement  which is
based on this property 
qx  a   qx  b  if and only if x  x qx  a   qx  b 
where qx  a  is the quality of a according to a set of metrics x 
this relationship has no dependence on how metrics are scaled or weighted  or on their
degree of correlation in the metric set  equality      can be derived directly from    the
unanimous equality implies that both systems obtain the same score for all metrics 
qx  a    qx  b    qx  a   qx  b     qx  b   qx  a  

the strict unanimous improvement implies that one system improves the other strictly
at least for one metric  and it is not improved according to any metric 
qx  a    qx  b    qx  a   qx  b     qx  a    qx  b   
 qx  a   qx  b     qx  b   qx  a  

non comparability k can also derived from here  it occurs when some metrics favor one
system and some other metrics favor the other  we refer to this cases as metric biased
improvements 
qx  a k qx  b    qx  a   qx  b     qx  b   qx  a  

the theoretical properties of the unanimous improvement are described in depth in
section      the most important property is that the unanimous improvement is the only
relational structure that does not depend on relative metric weightings  while satisfying
the independence  monotonicity  axiom  in other words  we can claim that  a system improvement according to a metric combining function does not depend whatsoever on metric
weightings if and only if there is no quality decrease according to any individual metric  the
theoretical justification of this assertion is developed in section       
    unanimous improvement ratio
according to the unanimous improvement  our unique observable over each test case is a
three valued function  unanimous improvement  equality or biased improvement   we need 
however  a way of quantitatively comparing systems 
given two systems  a and b  and the unanimous improvement relationship over a set of
test cases t   we have samples where a improves b  qx  a   qx  b    samples where b improves a  qx  b   qx  a   and also samples with biased improvements  qx  a k qx  b   
we will refer to these sets as ta b   tb a and tak b   respectively  the unanimous improvement ratio  uir  is defined according to three formal restrictions 
   

fiamigo  gonzalo  artiles   verdejo

test cases  t
 
 
 
 
 
 
 
 
 
  

precision
system a system b
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

recall
system a system b
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

a  b
yes
yes
yes
yes
yes
yes
no
no
no
no

b  a
yes
yes
no
no
no
no
yes
yes
no
no

table    example of experiment input to compute uir
   uir a  b  should decrease with the number of biased improvements  tak b    in the
boundary condition where all samples are biased improvements  tak b   t    then
uir a  b  should be   
   if a improves b as much as b improves a  ta b   tb a   then uir a  b      
   given a fixed number of biased improvements  tak b    uir a  b  should be proportional
to ta b and inversely proportional to tb a  
given these restrictions  we propose the following uir definition 
uirx t  a  b   

 ta b     tb a  
 
 t  

 t  t  qx  a   qx  b     t  t  qx  b   qx  a  
 t  
which can be alternatively formulated as 
uirx t  a  b    p a  b   p b  a 
where these probabilities are estimated in a frequentist manner 
uir range is        and is not symmetric  uirx t  a  b    uirx t  b  a   as an
illustration of how uir is computed  consider the experiment outcome in table    systems
a and b are compared in terms of precision and recall for    test cases  for test case    for
instance  a has an unanimous improvement over b  it is better both in terms of precision
            and recall              from the table  uir value is 
uirx t  a  b   

 ta b     tb a  
  
 
        uirx t  b  a 
 t  
  

uir has two formal limitations  first  it is not transitive  see section       therefore 
it is not possible to define a linear system ranking based on uir  this is  however  not
   

ficombining evaluation metrics via the unanimous improvement ratio

necessary  uir is not meant to provide a ranking  but to complement the ranking provided
by the f measure  or other metric combining function   indicating how robust results are to
changes in   section     illustrates how uir can be integrated with the insights provided
by a system ranking 
the second limitation is that uir does not consider improvement ranges  therefore 
it is less sensitive than the f measure  our empirical results  however  show that uir is
sensitive enough to discriminate robust improvements versus metric biased improvements 
and in section   we make an empirical comparison of our non parametric definition of uir
with a parametric version  with results that make the non parametric definition preferable 

   theoretical foundations
in this section we discuss the theoretical foundations of the unanimous improvement ratio
in the framework of the conjoint measurement theory  then we proceed to describe the
formal properties of uir and their implications from the point of view of the evaluation
methodology  readers interested solely in the practical implications of using uir may
proceed directly to section   
    conjoint measurement theory
the problem of combining evaluation metrics is closely related with the conjoint measurement theory  which was independently discovered by the economist debreu        and the
mathematical psychologist r  duncan luce and statistician john tukey  luce   tukey 
       the theory of measurement defines the necessary conditions to state an homomorphism between an empirical relational structure  e g  john is bigger than bill  and a
numeric relational structure  johns height is      meters and bills height is      meters  
in the case of the conjoint measurement theory  the relational structure is factored into
two  or more  ordered substructures  e g  height and weight  
in our context  the numerical structures are given by the evaluation metric scores  e g 
purity and inverse purity   however  we do not have an empirical quality ordering for
clustering systems  different human assessors could assign more relevance to purity than
to inverse purity or viceversa  nevertheless  the conjoint measurement theory does provide
mechanisms that state what kind of numerical structures can produce an homomorphism
assuming that the empirical structure satisfies certain axioms  van rijsbergen        used
this idea to analyze the problem of combining evaluation metrics  these axioms shape an
additive conjoint structure  being  r  p   the quality of a system according to two evaluation
metrics r and p   these axioms are 
connectedness  all systems should be comparable to each other  formally   r  p   
 r    p     or  r    p       r  p   
transitivity   r  p     r    p     and  r    p       r     p      implies that  r  p     r     p      
the axioms transitivity and connectedness shape a weak order 
thomsen condition   r    p      r    p    and  r    p      r    p    imply that  r    p    
 r    p     where  indicates equal effectiveness  
   

fiamigo  gonzalo  artiles   verdejo

independence  the two components contribute their effects independently to the effectiveness  formally   r    p     r    p   implies that  r    p       r    p     for all p     and
 r  p      r  p    implies that  r    p      r    p    for all r    this property implies
monotonicity  narens   luce        which states that an improvement in both metrics necessarily produces an improvement according to the metric combining function 
restricted solvability  a property which is     concerned with the continuity of each
component  it makes precise what intuitively we would expect when considering the
existence of intermediate levels  formally  whenever  r    p       r  p     r    p    
then exists r such that  r    p        r  p   
essential components  variation in one while leaving the other constant gives a variation in effectiveness  there exists r  r  and p such that it is not the case that
 r  p      r    p    and there exists p   p   and r such that it is not the case that
 r  p      r  p     
archimedean property  which merely ensures that the intervals on a component are
comparable 
the f measure proposed by van rijsbergen        and the arithmetic mean of p r
satisfy all these axioms  according to these restrictions  indeed  an unlimited set of acceptable combining functions for evaluation metrics can be defined  the f relational structure 
however  satisfies another property which is not satisfied by other functions such as the
arithmetic mean  this property is the decreasing marginal effectiveness  the basic idea
is that increasing one unit in one metric and decreasing one unit in the other metric can
improve the overall quality  i e  if the first metric has more weight in the combining function   but this does not imply that a great loss in one metric can be compensated by a great
increase in the other  it can be defined as 
r  p      n     such that   p   n  r  n     r  p   
according to this  high values in both metrics are required to obtain a high overall
improvement  this makes measures observing this property   such as f   more robust to
arbitrary metric weightings 
    formal properties of the unanimous improvement
the unanimous improvement x trivially satisfies most of the desirable properties proposed by van rijsbergen        for metric combining functions  transitivity  independence 
thomsen condition  restricted solvability  essential components and decreasing marginal
effectiveness  the exception being the connectedness property    given that the non comparability k  biased improvements  see section      is derived from the unanimous improvement  it is possible to find system pairs where neither qx  a   qx  b  nor qx  b   qx  a 
hold  therefore  connectedness is not satisfied 
formally  the limitation of the unanimous improvement is that it does not represent a
weak order  because it cannot satisfy transitivity and connectedness simultaneously  let
us elaborate on this issue 
   for the sake of simplicity  we consider here the combination of two metrics  r  p   

   

ficombining evaluation metrics via the unanimous improvement ratio

systems
a
b
c

metric x 
   
   
    

metric x 
   
   
    

table    a counter sample for transitivity in unanimous improvement
we could satisfy connectedness by considering that biased improvements represent
equivalent system pairs       but in this case  transitivity would not be satisfied  see  for
instance  table    according to the table 
qx  b k qx  a  and qx  c k qx  b 
therefore  considering that k represents equivalence  we have 
qx  b   qx  a  and qx  c   qx  b 
but not
qx  c   qx  a 
in summary  we can choose to satisfy transitivity or connectedness  but not both  the
unanimous improvement can not derive a weak order 
      uniqueness of the unanimous improvement
the unanimous improvement has the interesting property that is does not contradict any
evaluation result given by the f measure  regardless of the  value used in f 
qx  a   qx  b   f  a   f  b 
this is due to the fact that the f measure  for any  value  satisfies the monotonicity
axiom  in which the unanimous improvement is grounded  this property is essential for
the purpose of checking the robustness of system improvements across  values  and
crucially  the unanimous improvement is the only function that satisfies this property  more
precisely  the unanimous improvement is the only relational structure that  while satisfying
monotonicity  does not contradict any additive conjoint structure  see section      
in order to prove this assertion  we need to define the concept of compatibility with any
additive conjoint structure  let add be any additive conjoint structure and let r be any
relational structure  we will say that r is compatible with any conjoint structure if and
only if 
ha  b  add i  qx  a  r qx  b     qx  a  add qx  b  
in other words  if r holds  then any other additive conjoint holds  we want to prove
that the unanimous improvement is the only relation that satisfies this property  therefore 
we have to prove that if r is a monotonic and compatible relational structure  then it
necessarily matches the unanimous improvement definition 
   

fiamigo  gonzalo  artiles   verdejo

r is monotonic and compatible    qx  a  r qx  b   xi  a   xi  b xi  x 
which can be split in 
    r monotonic and compatible    qx  a  r qx  b   xi  a   xi  b xi  x 
    r monotonic and compatible    qx  a  r qx  b   xi  a   xi  b xi  x 
proving     is immediate  since the rightmost component corresponds with the monotonicity property definition  let us prove     by reductio ad absurdum  assuming that there
exists a relational structure o such that 
 o monotonic and compatible    qx  a  o qx  b     xi  x xi  a    xi  b  
in this case  we could define an additive conjoint structure over the combined measure
q x  a      x   a    i xi  a    n xn  a  with i big enough such that q x  a    q x  b   the
q  additive conjoint structure would contradict o   therefore  o would not be compatible
 contradiction   in conclusion  predicate     is true and the unanimous improvement x
is the only monotonic and compatible relational structure 
an interesting corollary can be derived from this analysis  if the unanimous improvement is the only compatible relational structure  then we can formally conclude that the
measurement of system improvements without dependence on metric weighting schemes can
not derive a weak order  i e  one that satisfies both transitivity and connectedness   this
corollary has practical implications  it is not possible to establish a system ranking which is
independent on metric weighting schemes 
a natural way to proceed is  therefore  to use the unanimous improvement as an addition
to the standard f measure  for a suitable  value  which provides additional information
about the robustness of system improvements across  values 

   f versus uir  empirical study
in this section we perform a number of empirical studies on the weps corpora in order to
find out how uir behaves in practice  first  we focus on a number of empirical results that
show how uir rewards robustness across  values  and how this information is complementary to the information provided by f  second  we examine to what extent  and why  f
and uir are correlated 
    uir  rewarding robustness
figure   shows three examples of system comparisons in weps  b corpus using the metrics
purity and inverse purity  each curve represents the f value obtained for one system
according to different  values  system s   black curves  is compared with s    s  and
s    grey curves  in each of the three graphs  in all cases there is a similar quality increase
according to f       uir  however  ranges between      and       depending on how robust
the difference is to changes in   the highest difference in uir is for the  s  s    system
pair  rightmost graph   because these systems do not swap their f values for any  value 
   

ficombining evaluation metrics via the unanimous improvement ratio

    f      
 uir 

improvements
for all 
    system pairs 
    
    

other cases
     system pairs 
    
    

table    uir and f     increase when f increases for all  values

figure    f measure vs  uir  rewarding robustness
the smallest uir value is for  s  s     where s  is better than s   for  values below
     and worse when  is larger  this comparison illustrates how uir captures  for similar
increments in f  which ones are less dependent of the relative weighting scheme between
precision and recall 
let us now consider all two system combinations in the weps  b corpus  dividing them
in two sets   i  system pairs for which f increases for all  values  i e  both purity and
inverse purity increases   and  ii  pairs for which the relative systems performance swaps
at some  value  i e  f increases for some  values and decreases for the rest 
one would expect that the average increase in f should be larger for those system pairs
where one beats the other for every  value  surprisingly  this is not true  table   shows the
average increments for uir and f     for both sets  uir behaves as expected  its average
value is substantially larger for the set where different  do not lead to contradictory results
      vs         but the average relative increase of f       however  is very similar in both
sets       vs        
the conclusion is that a certain f     improvement range does not say anything about
whether both purity and inverse purity are being simultaneously improved or not  in other
words  no matter how large is a measured improvement in f is  it can still be extremely
dependent on how we are weighting the individual metrics in that measurement 
this conclusion can be corroborated by considering independently both metrics  purity and inverse purity   according to the statistical significance of the improvements for
independent metrics  we can distinguish three cases 
   opposite significant improvements  one of the metrics  purity or inverse purity 
increases and the other decreases  and both changes are statistically significant 
   

fiamigo  gonzalo  artiles   verdejo

    f      
 uir 

significant
concordant
improvements
   pairs
    
    

significant
opposite
improvements
   pairs
    
    

non
significant
improvements
   pairs
    
     

table    uir and f     increases vs  statistical significance tests
   concordant significant improvements  both metrics improve significantly or at least
one improves significantly and the other does not decrease significantly 
   non significant improvements  there is no statistically significant differences between
both systems for any metric 
we use the wilcoxon test with p        to detect statistical significance  table  
shows the average uir and     f       values in each of the three cases  remarkably  the
f     average increase is even larger for the opposite improvements set        than for
the concordant improvements set         according to these results  it would seem that
f     rewards individual metric improvements which are obtained at the cost of  smaller 
decreases in the other metric  uir  on the other hand  has a sharply different behavior 
strongly rewarding the concordant improvements set       versus       
all these results confirm that uir provides essential information about the experimental
outcome of two system comparisons  which is not provided by the main evaluation metric
f  
    correlation between f and uir
the fact that uir and f offer different information about the outcome of an experiment
does not imply that uir and f are orthogonal  in fact  there is some correlation between
both values 
figure   represents f     differences and uir values for each possible system pair in the
weps   test bed  the general trends are  i  high uir values imply a positive difference in
f  ii  high   f       values do not imply anything on uir values   iii  low uir do not seem
to imply anything on   f       values  overall  the figure suggest a triangle relationship 
which gives a pearson correlation of      
      reflecting improvement ranges
when there is a consistent difference between two systems for most  values  uir rewards
larger improvement ranges  let us illustrate this behavior considering three sample system
pairs taken from the weps   test bed 
figure   represents the f      values for three system pairs  in all cases  one system
improves the other for all  values  however  uir assigns higher values to larger improvements in f  larger distance between the black and the grey curves   the reason is that a
   

ficombining evaluation metrics via the unanimous improvement ratio

figure      f       vs uir

figure    f vs  uir  reflecting improvement ranges
larger average improvement over test cases makes less likely the cases where individual test
cases  which are the ones that uir considers  contradict the average result 
another interesting finding is that  when both metrics are improved  the metric that
has the weakest improvement determines the behavior of uir  figure   illustrates this
relationship for the ten system pairs with a largest improvement  the pearson correlation in
this graph is       in other words  when both individual metrics improve  uir is sensitive
to the weakest improvement 
      analysis of boundary cases
in order to have a better understanding of the relationship between uir and f  we will now
examine in detail two cases of system improvements in which uir and f produce drastically
different results  these two cases are marked as a and b in figure   
the point marked as case a in the figure corresponds with the comparison of systems
s  and s     there exists a substantial  and statistically significant  difference between both
systems according to f       however  uir has a low value  i e   the improvement is not
robust to changes in  according to uir 
   

fiamigo  gonzalo  artiles   verdejo

figure    correlation between uir and the weakest single metric improvement 

figure    purity and inverse purity per test case  systems s  and s   
a visual explanation of these results can be seen in figure    it shows the purity and
inverse purity results of systems s    s   for every test case  in most test cases  s  has
an important advantage in purity at the cost of a slight  but consistent  loss in inverse
purity  given that f     compares purity and inverse purity ranges  it states that there
exists an important and statistically significant improvement from s   to s    however  the
slight but consistent decrease in inverse purity affects uir  which decreases because in most
test cases the improvements in f are metric biased  k in our notation  
case b  see figure    is the opposite example  there is a small difference between systems
s  and s   according to f       because differences in both purity and inverse purity are
also small  s   however  gives small but consistent improvements both for purity and inverse
purity  all test cases to the right of the vertical line in the figure   these are unanimous
improvements  therefore  uir considers that there exists a robust overall improvement in
this case 
   

ficombining evaluation metrics via the unanimous improvement ratio

figure    purity and inverse purity per test case  systems s    and s 
again  both cases show how uir gives additional valuable information on the comparative behavior of systems 
    a significance threshold for uir
we mentioned earlier that uir has a parallelism with statistical significance tests  which
are typically used in information retrieval to estimate the probability p that an observed
difference between two systems is obtained by chance  i e   the difference is an artifact of
the test collection rather than a true difference between the systems  when computing
statistical significance  it is useful to establish a threshold that allows for a binary decision 
for instance  a result is often said to be statistically significant if p         and not significant
otherwise  choosing the level of significance is arbitrary  but it nevertheless helps reporting
and summarizing significance tests  stricter thresholds increase confidence of the test  but
run an increased risk of failing to detect a significant result 
the same situation applies to uir  we would like to establish an uir threshold that
decides whether an observed difference is reasonably robust to changes in   how can we set
such a threshold  we could be very restrictive and decide  for instance  that an improvement
is significantly robust when uir        this condition  however  is so hard that it would
never be satisfied in practice  and therefore the uir test would not be informative  on
the other hand  if we set a very permissive threshold it will be satisfied by most system
pairs and  again  it will not be informative  the question now is whether there exists a
threshold for uir values such that obtaining a uir above the threshold guarantees that an
improvement is robust  and  at the same time  is not too strong to be satisfied in practice 
given the set of two system combinations for which uir surpasses a certain candidate
threshold  we can think of some desirable features 
   it must be able to differentiate between two types of improvements  robust vs  nonrobust   in other words  if one of the two types is usually empty or almost empty  the
threshold is not informative 
   

fiamigo  gonzalo  artiles   verdejo

   the robust set should contain a high ratio of two system combinations such that the
average f increases for all  values  f  a    f  b   
   the robust set should contain a high ratio of significant concordant improvements and
a low ratio of significant opposite improvements  see section      
   the robust set should contain a low ratio of cases where f contradicts uir  the dots
in figure   in the region   f            

figure     improvement detected across uir thresholds
figure    shows how these conditions are met for every threshold in the range          
a uir threshold of      accepts around     of all system pairs  with a low      ratio of
significant opposite improvements and a high       ratio of significant concordant improvements  at this threshold  in half of the robust cases f increases for all  values  and in
most cases       f     increases  it seems  therefore  that uir       can be a reasonable
threshold  at least for this clustering task  note  however  that this is a rough rule of thumb
that should be revised adjusted when dealing with clustering tasks other than weps 
    uir and system rankings
all results presented so far are focused on pairwise system comparisons  according to the
nature of uir  we now turn to the question of how can we use uir as a component in the
analysis of the results of an evaluation campaign 
in order to answer this question we have applied uir to the results of the weps  
evaluation campaign  artiles et al          in this campaign  the best runs for each system
were ranked according to bcubed precision and recall metrics  combined with f       in
addition to all participant systems  three baseline approaches were included in the ranking 
   

ficombining evaluation metrics via the unanimous improvement ratio

all documents in one cluster  b       each document in one cluster  b    and the union of
both  bcomb     
table   shows the results of applying uir to the weps   participant systems   robust
improvements are represented in the third column  improved systems   for every system 
it displays the set of systems that it improves with uir        the fourth column is the
reference system  which is defined as follows  given a system a  its reference system is the
one that improves a with maximal uir 
sref  a    argmaxs  uir s  a  
in other words  sref  a  represents the system with which a should be replaced in order
to robustly improve results across different  values  finally  the last column  uir for the
reference system  displays the uir between the system and its reference  uir sref   si    
note that uir adds new insights into the evaluation process  let us highlight two
interesting facts 
 although the three top scoring systems  s   s   s   have a similar performance in
terms of f             and        s  is consistently the best system according to uir 
because it is the reference for    other systems  s   s   s   s   s    s    s    s   
s   and the baseline b     in contrast  s  is reference for s  only  and s  is reference
for s   only  therefore  f and uir together strongly point towards s  as the best
system  while f alone was only able to discern a set of three top scoring systems 
 although the non informative baseline b     all documents in one cluster  is better
than five systems according to f  this improvement is not robust according to uir 
note that uir will signal near baseline behaviors in participant systems with a low
value  while they can receive a large f depending on the nature of the test collection 
when the average cluster is large or small  systems that tend to cluster everything or
nothing can be artificially rewarded  this is  in our opinion  a substantial improvement
over using f alone 

   uir as predictor of the stability of results across test collections
a common issue when evaluating systems that deal with natural language is that results on
different test collections are often contradictory  in the particular case of text clustering 
a factor that contributes to this problem is that the average size of clusters can vary across
different test beds  and this variability modifies the optimal balance between precision and
recall  a system which tends to favor precision  creating small clusters  may have good
results in a dataset with a small average cluster size and worse results in a test collection
with a larger average cluster size 
therefore  if we only apply f to combine single metrics  we can reach contradictory
results over different test beds  as uir does not depend on metric weighting criteria  our
hypothesis is that a high uir value ensures robustness of evaluation results across test beds 
   see the work of artiles et al         for an extended explanation 

   

fiamigo  gonzalo  artiles   verdejo

system

f   

s 
s 
s 
s 
s 
s 
s 
s 
s 
s  
s  
s  
b   
s  

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

bcomb
s  
s  
s  
b 
s  

improved systems
 uir        
s  s  s  s  s  s    s   b 
s  s  s  s  s    s   b 
s  s  s  s  s    s   b 
s   s    s  
s    s  
s  s  s   s    s   b 
s   s    s  
s    s  
s  s   s   s  
s    s  
s    s  
s   s  
bcomb
s   s  
s  
s  
 

reference
system
s 
s 
s 
s 
s 
s 
s 
s 
b   
s 
s 
s 
s 
s 

uir for the
reference system
    
    
    
    
    
    
    
   
    
   
    
    
    
    

table    weps   results with bcubed precision and recall  f and uir measures
in other words  given a particular test bed  a high uir value should be a good predictor that
an observed difference between two systems will still hold in other test beds 
the following experiment is designed to verify our hypothesis  we have implemented
four different systems for the weps problem  all based on an agglomerative clustering algorithm  hac  which was used by the best systems in weps    each system employs a
certain cluster linkage technique  complete link or single link  and a certain feature extraction criterion  word bigrams or unigrams   for each system we have experimented with   
stopping criteria  therefore  we have used   x  system variants overall  we have evaluated
these systems over weps  a  weps  b and weps   corpora   
the first observation is that  given all system pairs  f     only gives consistent results
for all three test beds in     of the cases  for all other system pairs  the best system
is different depending of the test collection  a robust evaluation criterion should predict 
given a single test collection  whether results will still hold in other collections 
we now consider two alternative ways of predicting that an observed difference  system
a is better than system b  in one test bed will still hold in all three test beds 
 the first is using f  a   f  b   the larger this value is on the reference test bed  the
more likely that f  a   f  b  will still be positive in a different test collection 
   weps  a was originally used for training in the first weps campaign  and weps  b was used for testing 

   

ficombining evaluation metrics via the unanimous improvement ratio

 the second is using u ir a  b  instead of f  the larger uir is  the more likely that
f  a   f  b  is also positive in a different test bed 
in summary  we want to compare f and uir as predictors of how robust is a result to
a change of test collection  this is how we tested it 
   we select a reference corpus out of weps  a  weps  b and weps   test beds 
cref   weps  a weps  b weps   
   for each system pair in the reference corpus  we compute the improvement of one
system with respect to the other according to f and uir  we take those system pairs
such that one improves the other over a certain threshold t  being uirc  s    s    the
uir results for systems s  and s  in the test bed c  and being fc  s  the results of f
for the system s in the test bed c 
su ir t  c      s    s    uirc  s    s      t 
sf t  c     s    s    fc  s     fc  s       t  
for every threshold t  su ir t and sf t represent the set of robust improvements as
predicted by uir and f  respectively 
   then  we consider the system pairs such that one improves the other according to f
for all the three test collections simultaneously 
t    s    s   fc  s      fc  s   c 
t is the gold standard to be compared with predictions su ir t and sf t  
   for every threshold t  we can compute precision and recall of uir and f predictions
 su ir t  c  and sf t  c   versus the actual set of robust results across all collections
 t   
p recision su ir t  c    

 su ir t  c   t  
 su ir t  

p recision sf t  c    

 sf t  c   t  
 sf t  c  

recall su ir t  c    

recall sf t  c    

 su ir t  c   t  
 t  

 sf t  c   t  
 t  

we can now trace the precision recall curve for each of the predictors f  uir and
compare their results  figures        and     show precision recall values for f  triangles 
and uir  rhombi   each figure displays results for one of the reference test beds  weps a weps  b and weps     
altogether  the figures show how uir is much more effective than f as a predictor 
note that f suffers a sudden drop in performance for low recall levels  which suggests that
   the curve parametric uir refers to an alternative definition of uir which is explained in section  

   

fiamigo  gonzalo  artiles   verdejo

figure     predictive power of uir and f from weps  a

figure     predictive power of uir and f from weps  b

   

ficombining evaluation metrics via the unanimous improvement ratio

figure     predictive power of uir and f from weps  
big improvements in f tend to be due to the peculiarities of the test collection rather than
to a real superiority of one system versus the other 
this is  in our opinion  a remarkable result  differences in uir are better indicators of
the reliability of a measured difference in f than the amount of the measured difference 
therefore  uir is not only useful to know how stable are results to changes in   but also
to changes in the test collection  i e   it is an indicator of how reliable a perceived difference
is 
note that we have not explicitly tested the dependency  and reliability  of uir results
with the number of test cases in the reference collection  however  as working with a
collection of less than    test cases is unlikely  in practical terms the usability of uir is
granted for most test collections  at least with respect of the number of test cases 

   parametric versus non parametric uir
according to our analysis  see section       given two measures p and r  the only relational
structure over pairs hpi   ri i that does not depend on weighting criteria is the unanimous
improvement 
a  b  pa  pb  ra  rb
when comparing systems  our uir measure counts the unanimous improvement results
across test cases 
uirx t  a  b   

 ta b     tb a  
 t  

alternatively  this formulation can be expressed in terms of probabilities 
   

fiamigo  gonzalo  artiles   verdejo

uirx t  a  b    prob a  b   prob b  a 
where these probabilities are estimated in a frequentist manner 
as we said  the main drawback of the unanimous improvement is that it is a threevalued function which does not consider metric ranges  uir inherits this drawback  as a
consequence  uir is less sensitive than other combining schemes such as the f measure  in
order to solve this drawback  we could estimate uir parametrically  however  the results
in this section seem to indicate that this is not the best option 
one way of estimating p rob a  b  and p rob b  a  consists of assuming that the
metric differences  p  r  between two systems across test cases follow a normal bivariate
distribution  we can then estimate this distribution from the case samples provided in each
test bed  after estimating the density function p rob p  r   we can estimate p rob a 
b  as   
p rob a  b    p rob p     r      

z p    r  

p rob p  r  dp dr
p    r  

this expression can be used to compute uirx t  a  b    prob a  b   prob b  a  
and leads to a parametric version of uir 
in order to compare the effectiveness of the parametric uir versus the original uir  we
repeated the experiment described in section    adding uirparam to the precision recall
curves in figures        and     the squares in that figures represent the results for the
parametric version of uir  note that its behavior lies somewhere between f and the nonparametric uir  for low levels of recall  it behaves like the original uir  for intermediate
levels  it is in general worse than the original definition but better than f  and in the
recall high end  it overlaps with the results of f  this is probably due to the fact that the
parametric uir estimation considers ranges  and becomes sensitive to the unreliability of
high improvements in f 

   conclusions
our work has addressed the practical problem of the strong dependency  and usually some
degree of arbitrariness  on the relative weights assigned to metrics when applying metric
combination criteria  such as f  
based on the theory of measurement  we have established some relevant theoretical results  the most fundamental is that there is only one monotonic relational structure that
does not contradict any additive conjoint structure  and that this unique relationship is
not transitive  this implies that it is not possible to establish a ranking  a complete ordering  of systems without assuming some arbitrary relative metric weighting  a transitive
relationship  however  is not necessary to ensure the robustness of specific pairwise system
comparisons 
based on this theoretical analysis  we have introduced the unanimous improvement ratio  uir   which estimates the robustness of measured system improvements across potential
metric combining schemes  uir is a measure complementary to any metric combination
   for this computation we have employed the matlab tool

   

ficombining evaluation metrics via the unanimous improvement ratio

scheme and it works similarly to a statistical relevance test  indicating if a perceived difference between two systems is reliable or biased by the particular weighting scheme used to
evaluate the overall performance of systems 
our empirical results on the text clustering task  which is particularly sensitive to this
problem  confirm that uir is indeed useful as an analysis tool for pairwise system comparisons   i  for similar increments in f  uir captures which ones are less dependent of the
relative weighting scheme between precision and recall   ii  unlike f  uir rewards system
improvements that are corroborated by statistical significance tests over each single measure   iii  in practice  a high uir tends to imply a large f increase  while a large increase
in f does not imply a high uir  in other words  a large increase in f can be completely
biased by the weighting scheme  and therefore uir is an essential information to add to f 
when looking at results of an evaluation campaign  uir has proved useful to  i  discern
which is the best system among a set of systems with similar performance according to f  
 ii  penalize trivial baseline strategies and systems with a baseline like behavior 
perhaps the most relevant result is a side effect on how our proposed measure is defined 
uir is a good estimator of how robust a result is to changes in the test collection  in other
words  given a measured increase in f in a test collection  a high uir value makes more
likely that an increase will also be observed in other test collections  remarkably  uir
estimates cross collection robustness of f increases much better than the absolute value of
the f increase 
a limitation of our present study is that we have only tested uir on the text clustering
problem  while its usefulness for clustering problems already makes uir a useful analysis
tool  its potential goes well beyond this particular problem  most natural language problems  and  in general  many problems in artificial intelligence  are evaluated in terms of
many individual measures which are not trivial to combine  uir should be a powerful tool
in many of those scenarios 
an uir evaluation package is available for download at http   nlp uned es 

acknowledgments
this research has been partially supported by the spanish government  grant holopedia 
tin           c    and the regional government of madrid under the research network
ma vicmr  s     tic       

references
amigo  e   gonzalo  j   artiles  j     verdejo  f          a comparison of extrinsic clustering
evaluation metrics based on formal constraints  information retrieval                 
artiles  j   gonzalo  j     sekine  s          weps   evaluation campaign  overview of the
web people search clustering task  in proceedings of the  nd web people search
evaluation workshop  weps       
artiles  j   gonzalo  j     sekine  s          the semeval      weps evaluation  establishing a benchmark for the web people search task  in proceedings of the  th
international workshop on semantic evaluations  semeval     pp       stroudsburg  pa  usa  association for computational linguistics 
   

fiamigo  gonzalo  artiles   verdejo

bagga  a     baldwin  b          entity based cross document coreferencing using the
vector space model  in proceedings of the   th annual meeting of the association for
computational linguistics and the   th international conference on computational
linguistics  coling acl     pp       
carreras  x     marquez  l          introduction to the conll      shared task  semantic
role labeling  in ng  h  t     riloff  e   eds    hlt naacl      workshop  eighth
conference on computational natural language learning  conll        pp      
boston  massachusetts  usa  association for computational linguistics 
cormack  g  v     lynam  t  r          trec      spam track overview  in proceedings
of the fourteenth text retrieval conference  trec       
debreu  g          topological methods in cardinal utility theory  mathematical methods
in the social sciences  stanford university press               
ghosh  j          scalable clustering methods for data mining  in ye  n   ed    handbook
of data mining  lawrence erlbaum 
halkidi  m   batistakis  y     vazirgiannis  m          on clustering validation techniques 
journal of intelligent information systems                   
luce  r     tukey  j          simultaneous conjoint measurement  a new scale type of
fundamental measurement  journal of mathematical psychology        
mann  g  s          multi document statistical fact extraction and fusion  ph d  thesis 
johns hopkins university 
meila  m          comparing clusterings  in proceedings of colt    
narens  l     luce  r  d          measurement  the theory of numerical assignments  psychological bulletin     
steinbach  m   karypis  g     kumar  v          a comparison of document clustering
techniques  in kdd workshop on text mining      
su  k  y   su  j   wiebe  j     li  h   eds            proceedings of the joint conference of
the   th annual meeting of the acl and the  th international joint conference on
natural language processing of the afnlp  association for computational linguistics  suntec  singapore 
tao li  c  z     zhu  s          empirical studies on multilabel classification  in proceedings of the   th ieee international conference on tools with artificial intelligence
 ictai       
van rijsbergen  c  j          foundation of evaluation  journal of documentation         
       
weng  c  g     poon  j          a new evaluation measure for imbalanced datasets  in
roddick  j  f   li  j   christen  p     kennedy  p  j   eds    seventh australasian
data mining conference  ausdm        vol     of crpit  pp       glenelg  south
australia  acs 
zhao  y     karypis  g          criterion functions for document clustering  experiments
and analysis  technical report tr       department of computer science  university of minnesota  minneapolis  mn 
   

fi