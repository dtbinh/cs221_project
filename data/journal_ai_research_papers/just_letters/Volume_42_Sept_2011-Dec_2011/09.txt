journal of artificial intelligence research                  

submitted       published      

learning to make predictions in partially observable
environments without a generative model
erik talvitie

erik talvitie fandm edu

mathematics and computer science
franklin and marshall college
lancaster  pa             usa

satinder singh

baveja umich edu

computer science and engineering
university of michigan
ann arbor  mi             usa

abstract
when faced with the problem of learning a model of a high dimensional environment  a
common approach is to limit the model to make only a restricted set of predictions  thereby
simplifying the learning problem  these partial models may be directly useful for making
decisions or may be combined together to form a more complete  structured model  however  in partially observable  non markov  environments  standard model learning methods
learn generative models  i e  models that provide a probability distribution over all possible futures  such as pomdps   it is not straightforward to restrict such models to make
only certain predictions  and doing so does not always simplify the learning problem  in
this paper we present prediction profile models  non generative partial models for partially
observable systems that make only a given set of predictions  and are therefore far simpler
than generative models in some cases  we formalize the problem of learning a prediction
profile model as a transformation of the original model learning problem  and show empirically that one can learn prediction profile models that make a small set of important
predictions even in systems that are too complex for standard generative models 

   introduction
learning a model of the dynamics of an environment through experience is a critical capability for an artificial agent  agents that can learn to make predictions about future events
and anticipate the consequences of their own actions can use these predictions to plan and
make better decisions  when the agents environment is very complex  however  this learning problem can pose serious challenges  one common approach to dealing with complex
environments is to learn partial models  focusing the model learning problem on making
a restricted set of particularly important predictions  often when only a few predictions
need to be made  much of the complexity of the dynamics being modeled can be safely ignored  sometimes a partial model can be directly useful for making decisions  for instance
if the model makes predictions about the agents future rewards  e g   see mccallum       
mahmud         in other cases  many partial models making restricted predictions are
combined to form a more complete model as in  for instance  factored mdps  boutilier 
dean    hanks         factored psrs  wolfe  james    singh         or collections of
local models  talvitie   singh      b  
c
    
ai access foundation  all rights reserved 

fitalvitie   singh

the most common approach to learning a partial model is to apply an abstraction
 whether learned or supplied by a domain expert  that filters out detail from the training data that is irrelevant to making the important predictions  model learning methods
can then be applied to the abstract data  and typically the learning problem will be more
tractable as a result  however  especially in the case of partially observable systems  abstraction alone may not sufficiently simplify the learning problem  even  as we will see in
subsequent examples  when the model is being asked to make intuitively simple predictions 
the counter intuitive complexity of learning a partial model in the partially observable case
is a direct result of the fact that standard model learning approaches for partially observable systems learn generative models that attempt to make every possible prediction about
the future and cannot be straightforwardly restricted to making only a few particularly
important predictions 
in this paper we present an alternative approach that learns non generative models that
make only the specified predictions  conditioned on history  in the following illustrative
example  we will see that sometimes a small set of predictions is all that is necessary for
good control performance but that learning to make these predictions in a high dimensional
environment using standard generative models can pose serious challenges  by contrast we
will see that there exists a simple  non generative model that can make and maintain these
predictions and this will form the learning target of our method 
    an example
consider the simple game of three card monte  the dealer  perhaps on a crowded street 
has three cards  one of which is an ace  the dealer shows the location of the ace  flips over
the cards  and then mixes them up by swapping two cards at every time step  a player of
the game must keep track of location of the ace  eventually the dealer stops mixing up the
cards and asks for a guess  if a player correctly guesses where the ace is  they win some
money  if they guess wrong  they lose some money 
consider an artificial agent attempting to learn a model of the dynamics of this game
from experience  it takes a sequence of actions and perceives a sequence of observations 
the raw data received by the agent includes a rich  high dimensional scene including the
activities of the crowd  the movement of cars  the weather  as well as the game itself  the
dealer swapping cards   clearly  learning a model that encompasses all of these complex
phenomena is both infeasible and unnecessary  in order to win the game  the agent needs
only focus on making predictions about the cards  and need not anticipate the future behavior of the city scene around it  in particular  the agent need only make three predictions 
if i flip over card    will it be the ace  and the corresponding predictions for cards  
and    one can safely ignore much of the detail in the agents experience and still make
these important predictions accurately  once one filters out the irrelevant detail  the agents
experience might look like this 
bet pos  watch swap     watch swap            
where the agent takes the bet action  starting the game  and observes the dealer showing
the card in position    then the agent takes the watch action  observes the dealer swapping
cards   and    takes the watch action again  observes the dealer swapping cards   and    and
   

filearning to make predictions without a generative model

so on until the dealer prompts the agent for a guess  note that this is not an uncontrolled
system  watch is indeed an action that the agent must select over  say  reaching out and
flipping the cards itself  which in a real game of three card monte would certainly result
in negative utility   now the data reflects only the movement of the cards  one could learn
a model using this new data set and the learning problem would be far simpler than before
since complex and irrelevant phenomena like the crowd and the weather have been ignored 
in the markov case  the agent directly observes the entire state of the environment and
can therefore learn to make predictions as a direct function of state  abstraction simplifies
the representation of state and thereby simplifies the learning problem  note  however  that
the three card monte problem is partially observable  non markov   the agent cannot
directly observe the state of the environment  the location of the ace and the state of the
dealers mind are both hidden to the agent   in the partially observable case  the agent
must learn to maintain a compact representation of state as well as learn the dynamics of
that state  the most common methods to achieve this  such as expectation maximization
 em  for learning pomdps  baum  petrie  soules    weiss         learn generative models
which provide a probability distribution over all possible futures 
in three card monte  even when all irrelevant details have been ignored and the data
contains only information about the cards movement  a generative model will still be intractably complex  a generative model makes predictions about all future events  this
includes the predictions the model is meant to make  such as whether flipping over card  
in the next time step will reveal the ace  but also many irrelevant predictions  a generative
model  will also predict  for instance  whether flipping over card   in    time steps will
reveal the ace or whether cards   and   will be swapped in the next time step  to make
these predictions  the model must capture not only the dynamics of the cards but also of
the dealers decision making process  if the dealer decides which cards to swap using some
complex process  as a human dealer might  then the problem of learning a generative model
of this abstract system will be correspondingly complex 
of course  in three card monte  predicting the dealers future behavior is entirely
unnecessary to win  all that is required is to maintain the aces current location over time 
as such  learning a model that devotes most of its complexity to anticipating the dealers
decisions is counter intuitive at best  a far more reasonable model can be seen in figure   
here the states of the model are labeled with predictions about the aces location  the
transitions are labeled with observations of the dealers behavior  as an agent plays the
game  it could use such a model to maintain its predictions about the location of the ace
over time  taking the dealers behavior into account  but not predicting the dealers future
behavior  note that this is a non generative model  it does not provide a distribution
over all possible futures and it cannot be used to simulate the world because it does not
predict the dealers next move  it only provides a limited set of conditional predictions
about the future  given the history of past actions and observations  on the other hand 
it is far simpler than a generative model would be  because it does not model the dealers
decision making process  this model has only   states  regardless of the underlying process
used by the dealer 
the model in figure   is an example of what we term a prediction profile model  this
paper will formalize prediction profile models and present an algorithm for learning them
from data  under some assumptions  to be specified once we have established some necessary
   

fitalvitie   singh

figure    maintaining predictions about the location of the ace in three card monte  transitions are labeled with the dealers swaps  states are labeled with the predicted
position of the special card 

terminology   we will empirically demonstrate that in some partially observable systems
that prove too complex for standard generative model learning methods  it is possible to
learn a prediction profile model that makes a small set of important predictions that allow
the agent to make good decisions  the next sections will formally describe the setting
and establish some notation and terminology and formalize the general learning problem
being addressed  subsequent sections will formally present prediction profile models and
an algorithm for learning them  as well as several relevant theoretical and empirical results 
    discrete dynamical systems
we focus on discrete dynamical systems  the agent has a finite set a of actions that it can
take and the environment has a finite set o of observations that it can produce  at every
time step i  the agent chooses an action ai  a and the environment stochastically emits
an observation oi  o 
definition    at time step i  the sequence of past actions and observations since the
beginning of time hi   a  o  a  o        ai oi is called the history at time i 
the history at time zero  before the agent has taken any actions or seen any observations
h    is called the null history 
      predictions
an agent uses its model to make conditional predictions about future events  given the history of actions and observations and given its own future behavior  because the environment
is assumed to be stochastic  predictions are probabilities of future events  the primitive
building block used to describe future events is called a test  after rivest   schapire       
littman  sutton    singh         a test t is simply a sequence of actions and observations
   

filearning to make predictions without a generative model

that could possibly occur  t   a  o        ak ok   if the agent actually takes the action sequence
in t and observes the observation sequence in t  we say that test t succeeded  a prediction
p t   h  is the probability that test t succeeds after history h  assuming the agent takes the
actions in the test  essentially  the prediction of a test is the answer to the question if
i were to take this particular sequence of actions  with what probability would i see this
particular sequence of observations  given the history so far  formally 
def

p t   h    pr o    h  a   pr o    ha  o    a          pr ok   ha  o  a  o        ak  ok    ak   

   

let t be the set of all tests  that is  the set of all possible action observation sequences
of all lengths   then the set of all possible histories h is the set of all action observation
sequences that could possibly occur starting from the null history  and the null history itself 
def
h    t  t   p t   h           h    
a model that can make a prediction p t   h  for all t  t and h  h can make any
conditional prediction about the future  littman et al          because it represents a
probability distribution over all futures  such a model can be used to sample from that
distribution in order to simulate the world  or sample possible future trajectories  as
such  we call a model that makes all predictions a generative model 
note that the use of the word generative here is closely related to its broader sense
in general density estimation  if one is attempting to represent the conditional probability
distribution pr a   b   the generative approach would be to represent the full joint distribution pr a  b  from which the conditional probabilities can be computed as pr a b 
pr b    that is
to say  a generative model in this sense makes predictions even about variables we only wish
to condition on  the non generative or  in some settings  discriminitive approach would
instead directly represent the conditional distribution  taking the value of b as un modeled
input  the non generative approach can sometimes result in significant savings if pr b  is
very difficult to represent learn  but pr a   b  is relatively simple  so long as one is truly
disinterested in modeling the joint distribution  
in our particular setting  a generative model is one that provides a probability distribution over all futures  given the agents actions   as such  one would use a generative model
  
  in fact  from equation   one
to compute p t   h  for some particular t and h as p ht h
p h h   
can see that the prediction for any multi step test can be computed from the predictions of
one step tests 
p a  o  a  o        ak ok   h    p a  o    h p a  o    ha  o          p ak ok   ha  o  a  o        ak ok   
this leads to a simple definition of a generative model 
definition    any model that can provide the predictions p ao   h  for all actions a  a 
observations o  o and histories h  h is a generative model 
a non generative model  then  would not make all one step predictions in all histories
and  consequently  would have to directly represent the prediction p t   h  with the history h
as an un modeled input  it would condition on a given history  but not necessarily be capable
of computing the probability of that history sequence  as we saw in the three card monte
example  this can be beneficial if making and maintaining predictions for t is substantially
simpler than making predictions for every possible action observation sequence 
   

fitalvitie   singh

note that a test describes a very specific future event  a sequence of specific actions
and observations   in many cases one might wish to make predictions about more abstract
events  this can be achieved by composing the predictions of many tests  for instance set
tests  wingate  soni  wolfe    singh        are a sequence of actions and a set of observation
sequences  a set test succeeds when the agent takes the specified action sequence and sees
any observation sequence contained within the set occur  while traditional tests allow an
agent  for instance to express the question if i go outside  what is the probability i will see
this exact sequence of images  a set test can express the far more useful  abstract question
if i go outside  what is the probability that it will be sunny  by grouping together all
observations of a sunny day  even more generally  option tests  wolfe   singh        soni
  singh        express future events where the agents behavior is described abstractly as
well as the resulting observations  these types of abstract predictions can be computed as
the linear combination of a set of concrete predictions 
      system dynamics matrix and linear dimension
it is sometimes useful to describe a dynamical system using a conceptual object called the
system dynamics matrix  singh  james    rudary         the system dynamics matrix
contains the values of all possible predictions  and therefore fully encodes the dynamics of
the system  specifically 
definition    the system dynamics matrix of a dynamical system is an infinity by infinity
matrix  there is a column corresponding to every test t  t   there is a row corresponding
to every history h  h  the ijth entry of the system dynamics matrix is the prediction
p tj   hi   of the test corresponding to column j at the history corresponding to row i and
there is an entry for every history test pair 
though the system dynamics matrix has infinitely many entries  in many cases it has
finite rank  the rank of the system dynamics matrix can be thought of as a measure of the
complexity of the system  singh et al         
definition    the linear dimension of a dynamical system is the rank of the corresponding
system dynamics matrix 
for some popular modeling representations  the linear dimension is a major factor in the
complexity of representing and learning a generative model of the system  for instance  in
pomdps  the number of hidden states required to represent the system is lower bounded
by the linear dimension  in this work we adopt linear dimension as our measure of the
complexity of a dynamical system  when we say a system is simpler than another  we
mean it has a lower linear dimension 
      the markov property
a dynamical system is markov if all that one needs to know about history in order to make
predictions about future events is the most recent observation 
definition    a system is markov if for any two histories h and h  that may be the null
history   any two actions a and a   any observation o  and any test t  p t   hao    p t   h a o  
   

filearning to make predictions without a generative model

in the markov case we will use the notational shorthand p t   o  to indicate the prediction
of t at any history that ends in observation o  in the markov case  because observations
contain all the information needed to make any prediction about the future  they are often
called state  because they describe the state of the world   when a system is not markov 
it is partially observable  in partially observable systems predictions can depend arbitrarily
on the entire history  we focus on the partially observable case 

   learning to make predictions
in this work we assume that  as in three card monte  though the agent may live in a
complex environment  it has only a small set of important predictions to make  these
predictions could have been identified as important by a designer  or by some other learning
process  we do not address the problem of identifying which predictions should be made 
but rather focus on the problem of learning to make predictions  once they are identified 
in general  we imagine that we are given some finite set t i    t    t            tm   of tests of
interest for which we would like our model to make accurate predictions  here the term
test should be construed broadly  possibly including abstract tests in addition to raw
sequences of actions and observations  the tests of interest are the future events the model
should predict  for instance  in the three card monte problem  in order to perform well
the agent must predict whether it will see the ace when it flips over each card  so it will
have three one step tests of interest  f lip  ace  f lip  ace  and f lip  ace  representing the
future events where the agent flips over card       and    respectively  and sees the ace   if
the agent can learn to maintain the probability of these events over time  it can win the
game 
as such  the general problem is to learn a function    h        m where
def

 h    hp t    h   p t    h           p tm   h i 

   

that is  a function from histories to the predictions for the test of interest  which we will refer
to as the predictions of interest  at that history  note that the output of  is not necessarily
a probability distribution  the tests of interest may be selected arbitrarily and therefore
need not represent mutually exclusive or exhaustive events  we will call a particular vector
of predictions for the tests of interest a prediction profile 
definition    we call  h  the prediction profile at history h 
we now describe two existing general approaches to learning   learning a direct function from history to predictions  most common in the markov case   and learning a fully
generative model that maintains a finite dimensional summary of history  common in the
partially observable case   both have strengths and weaknesses as approaches to learning
  section     will contrast these with our approach  which combines some of the strengths
of both approaches 
    direct function approximation
when the system is markov  learning  is conceptually straightforward  essentially it is
a problem of learning a function from observation  state  to predictions  rather than
   

fitalvitie   singh

learning  which takes histories as input  one can instead learn a function m arkov   o 
      m   which maps an observation to the predictions for the tests of interest resulting from
all histories that end in that observation  note that  as an immediate consequence  in
discrete markov systems there is a finite number of distinct prediction profiles  in fact 
there can be no more distinct prediction profiles than there are observations 
when the number of observations and the number of tests of interest are small enough 
m arkov can be represented as a  o    t i   look up table  and the entries estimated using
sample averages   
p ti   o   

  times t succeeds from histories ending in o
 
  times acts t  taken from histories ending in o

   

the main challenge of learning markov models arises when the number of observations is
very large  then it becomes necessary to generalize across observations  using data gathered
about one observation to learn about many others  specifically  one may be able to exploit
the fact that some observations will be associated with very similar  or identical  prediction
profiles  that is  the same predictions for the tests of interest  and share data amongst them 
restricting a models attention to only a few predictions can afford more generalization 
which is why learning a partial model can be beneficial in the markov setting 
even when the system is partially observable  one can still attempt to learn  directly 
typically by performing some sort of regression over a set of features of entire histories  for
instance  u tree  mccallum        takes a set of history features and learns a decision tree
that attempts to distinguish histories that result in different expected asymptotic return
under optimal behavior  wolfe and barto        apply a u tree like algorithm but rather
than restricting the model to predicting future rewards  they learn to make predictions
about some pre selected set of features of the next observation  a special case of the more
general concept of tests of interest   dinculescu and precup        learn the expected value
of a given feature of the future as a direct function of a given real valued feature of history
by clustering futures and histories that have similar associated values 
because they directly approximate  these types of models only make predictions for t i
and are therefore non generative  and therefore able  for instance  to avoid falling into the
trap of predicting the dealers decisions in three card monte   though this approach has
demonstrated promise  it also faces a clear pragmatic challenge  especially in the partially
observable setting  feature selection  because  is a function of history  an ever expanding
sequence of actions and observations  finding a reasonable set of compactly represented features that collectively capture all of the history information needed to make the predictions
of interest is a significant challenge  in a sense  even in the partially observable setting 
this type of approach takes only a small step away from the markov case  it still requires
a good idea a priori of what information should be extracted from history  in the form of
features  in order to make the predictions of interest 
   bowling  mccracken  james  neufeld  and wilkinson        showed that this estimator is unbiased only
in the case where the data is collected using a blind policy  in which action selection does not depend on
the history of observations and provided an alternative estimator that is unbiased for all policies  for
simplicitys sake  however  we will assume throughout that the data gathering policy is blind 

   

filearning to make predictions without a generative model

    generative models
if one does not have a good idea a priori of what features should be extracted from history
to make accurate predictions  one faces the additional challenge of learning to summarize
the relevant information from history in a compact sufficient statistic 
there exist methods that learn from training data to maintain a finite dimensional
statistic of history from which any prediction can be computed  in analogy to the markov
case  this statistic is called the state vector  clearly any model that can maintain state
can be used to compute   since it can make all predictions   we briefly mention two
examples of this approach that are particularly relevant to the development and analysis of
our method 
pomdps by far the most popular representation for models of partially observable
systems is the partially observable markov decision process  pomdp   monahan        
a pomdp posits an underlying mdp  puterman        with a set s of hidden states that
the agent never observes  at any given time step i  the system is in some particular hidden
state si   s  unknown to the agent   the agent takes some action ai  a and the system
transitions to the next state si according to the transition probability pr si   si    ai    an
observation oi  o is then emitted according to a probability distribution that in general
may depend upon si    ai   and si   pr oi   si    ai   si   
because the agent does not observe the hidden states  it cannot know which hidden
state the system is in at any given moment  the agent can however maintain a probability
distribution that represents the agents current beliefs about the hidden state  this probability distribution is called the belief state  if the belief state associated with history h is
known  then it is straightforward to compute the prediction of any test t 
x
p t   h   
pr s   h pr t   s  
ss

where pr t   s  can be computed using the transition and observation emission probabilities 
the belief state is a finite summary of history from which any prediction about the
future can be computed  so  the belief state is the state vector for a pomdp  given the
transition probabilities and the observation emission probabilities  it is possible to maintain
the belief state over time using bayes rule  if at the current history h one knows pr s   h 
for all hidden states s and the agent takes action a and observes observation o  then one
can compute the probability of any hidden state s at the new history 
p



s s pr s   h pr s   s   ai  pr oi   s   ai   s 
p
pr s   hao    p
 
   





s s
s s pr s   h pr s   s   ai  pr oi   s   ai   s  
the parameters of a pomdp that must be learned in order to be able to maintain
state are the transition probabilities and the observation emission probabilities  given these
parameters  the belief state corresponding to any given history can be recursively computed
and the model can thereby make any prediction at any history  pomdp parameters are
typically learned using the expectation maximization  em  algorithm  baum et al         
given some training data and the number of actions  observations  and hidden states as
input  em essentially performs gradient ascent to find transition and emission distributions
that  locally  maximize the likelihood of the provided data 
   

fitalvitie   singh

psrs another more recently introduced modeling representation is the predictive state
representation  psr   littman et al          instead of hidden states  psrs are defined more
directly in terms of the system dynamics matrix  described in section         specifically 
psrs find a set of core tests q whose corresponding columns in the system dynamics matrix
form a basis  recall that the system dynamics matrix often has finite rank  for instance  the
matrix associated with any pomdp with finite hidden states has finite linear dimension 
and thus q is finite for many systems of interest  since the predictions of q are a basis 
the prediction for any other test at some history can be computed as a linear combination
of the predictions of q at that history 
the vector of predictions for q is called the predictive state  while the belief state was
the state vector for pomdps  the predictive state is the state vector for psrs  it can also
be maintained by application of bayes rule  specifically  if at some history h  p q   h 
is known for all core tests q and the agent takes some action a  a and observes some
observation o  o  then one can compute the prediction of any core test q at the new
history 
p


p aoq   h 
q  q p q   h maoq  q  
p
p q   hao   
 
 
   


p ao   h 
q  q p q   h mao  q  
where maoq  q    is the coefficient of p q    h  in the linear combination that computes the
prediction p aoq   h  
so  given a set of core tests  the parameters of a psr that must be learned in order
to maintain state are the coefficients mao for every action a and observation o and the
coefficients maoq for every action a  observation o  and core tests q  given these parameters
the predictive state at any given history can be recursively computed and used to make any
prediction about the future  psrs are learned by directly estimating the system dynamics matrix  james   singh        wolfe  james    singh        or  more recently  some
sub matrix or derived matrix thereof  boots  siddiqi    gordon              using sample
averages in the training data  the estimated matrix is used to find a set of core tests and
the parameters are then estimated using linear regression 
note that both of these types of models are inherently generative  they both rely
upon the maintenance of the state vector in order to make predictions and  as can be
seen in equations   and    the state update equations of these models rely upon access to
one step predictions to perform the bayesian update  as such  unlike the direct function
approximation approach  one cannot simply choose a set of predictions for the model to
make  these models by necessity make all predictions 
there are many reasons to desire a complete  generative model  because it makes all
possible predictions  such a model can be used to sample possible future trajectories which is
a useful capability for planning  a generative model is also  by definition  very flexible about
what predictions it can be used to make  on the other hand  in many cases a complete 
generative model may be difficult to obtain  both psr and pomdp training methods scale
very poorly with the linear dimension of the system being learned  the linear dimension
lower bounds the number of hidden states needed to represent a system as a pomdp and
is precisely the number of core tests needed to represent it as a psr  the learning methods
for pomdps and psrs are rarely successfully applied to systems with a linear dimension of
   

filearning to make predictions without a generative model

figure    size     d ball bounce
more than a few hundred  though the work of boots et al  is pushing these limits further  
most systems of interest will have several orders of magnitude higher linear dimension 
furthermore  a complete  generative model is overkill for the problem at hand  recall
that we do not seek to make all predictions  we are focused on making some particularly
important predictions t i   even in problems where learning to make all predictions might
be intractable  it should still be possible to make some simple but important predictions 
      abstract generative models
as discussed earlier  when there is a restricted set of tests of interest  the learning problem
can often be simplified by ignoring irrelevant details through abstraction  of course  having
an abstraction does not solve the problem of partial observability  what is typically done
is to apply the abstraction to the training data  discarding the irrelevant details  as we
did in the three card monte example  and then to apply model learning methods like the
ones described above to the abstract data set  just as in the markov setting  in some cases
observation abstraction can greatly simplify the learning problem  certainly learning about
only the cards in three card monte is easier than learning about the cards and the crowd
and weather and so on  
ignoring details irrelevant to making the predictions of interest is intuitive and can
significantly simplify the learning problem  on the other hand  because they are generative
models  an abstract pomdp or psr will still make all abstract predictions  this typically
includes predictions other than those that are directly of interest  if these extra predictions
require a complex model  even an abstract generative model can be intractible to learn  this
is true of the three card monte example  where a generative model ends up modeling the
dealer as well as the cards   the following is another simple example of this phenomenon 
example  consider the uncontrolled system pictured in figure    called the  d ball
bounce system  the agent observes a strip of pixels that can be black or white  the
black pixel represents the position of a ball that moves around on the strip  the ball has
a current direction and every time step it moves one pixel in that direction  whenever it
reaches an edge pixel  its current direction changes to move away from the edge  in figure
  a  a complete pomdp model of a    pixel version of this system is pictured  if there
are k pixels  the pomdp has  k    hidden states  because the ball can have one of  
possible directions in one of k possible positions  except the two ends  where there is only
one possible direction  
now say the agent wishes only to predict whether the ball will be in the position marked
with the x in the next time step  clearly this prediction can be made by only paying
attention to the immediate neighborhood of the x  the details of what happens to the
ball while it is far away do not matter for making these predictions  so  one could apply
   

fitalvitie   singh

 a 

 b 

figure    pomdp model of the size     d ball bounce system  a  and of the abstracted
 d ball bounce system  b  

an abstraction that lumps together all observations in which the neighborhood about x
looks the same  the problem is that an abstract generative model of this system makes
predictions not only about x  but also about the pixels surrounding x  specifically  the
model still makes predictions about whether the ball will enter the neighborhood in the near
future  this of course depends on how long it has been since the ball left the neighborhood 
so  the pomdp model of the abstract system  pictured in figure   b   has exactly the same
state diagram as the original system  though its observations have changed to reflect the
abstraction  the abstract system and the primitive system have the same linear dimension 
in order to make predictions about x  one must condition on information about the
pixels surrounding x  consequently  a generative model also makes predictions about these
pixels  counterintuitively  the abstract models complexity is mainly devoted to making
predictions other than the predictions of interest  in general  while learning an abstract
model can drastically simplify the learning problem by ignoring irrelevant details  an abstract generative model still learns to make predictions about any details that are relevant 
even if they are not directly of interest 

    prediction profile models
the contribution of this paper  prediction profile models  seek to combine the main strengths
of the two model learning approaches discussed above  as with a direct approximation of
  a prediction profile model will only make the predictions of interest  and no others 
as such  it can be far simpler than a generative model  which will typically make many
extraneous predictions  however  the learning method for prediction profile models will
not require a set of history features to be given a priori  by leveraging existing generative
model learning methods  prediction profile models learn to maintain the state information
necessary for making the predictions of interest 
   

filearning to make predictions without a generative model

figure    prediction profile model for the  d ball bounce system
a typical model learns to make predictions about future observations emitted by the
system  the main idea behind prediction profile models is to instead model the values of
the predictions themselves as they change over time  conditioned on both the actions chosen
by the agent and the observations emitted by the system 
we have already seen an example of this in three card monte  the prediction profile
model  shown in figure    takes observations of the dealers behavior as input and outputs
predictions for the tests of interest  it does not predict the dealers behavior  but it takes it
into account when updating the predictions of interest  recall that  though the three card
monte system can be arbitrarily complicated  depending on the dealer   this prediction
profile system has three states  regardless of the dealers decision making process 
another example is shown in figure    this is the prediction profile system for the  d
ball bounce system  figure     where the model must predict whether the ball will enter
position x in the next time step  each state of the prediction profile model is labeled with a
prediction for pixel x  white or black   the transitions are labeled with observations of the
  pixel neighborhood centered on position x  in this case the transitions capture the ball
entering the neighborhood  moving to position x  leaving the neighborhood  staying away
for some undetermined amount of time  and returning again  recall that a pomdp model
of this system has  k    hidden states  where k is the number of pixels  even after ignoring
all pixels irrelevant to making predictions about pixel x  by contrast  the prediction profile
model always has three states  regardless of the number of pixels 
the next section will formally describe prediction profile models as models of a dynamical system that results from a transformation of the original system  subsequent sections
will discuss how to learn prediction profile models from data  by converting data from the
original system into data from the transformed system and learning a model from the converted data set  and present results that help to characterize the conditions under which
prediction profile models are best applied 

   the prediction profile system
we now formally describe a theoretical dynamical system  defined in terms of both the
dynamics of the original system and the given tests of interest  we call this constructed
system the prediction profile system  a prediction profile model  which it is our goal to
   

fitalvitie   singh

construct  is a model of the prediction profile system  that is  the system is an ideal 
theoretical construct  a model may be imperfect  approximate  etc    as such  our analysis of
the problem of learning a prediction profile model will depend a great deal on understanding
properties of the prediction profile system 
in this paper we make the restrictive assumption that  as in the markov case  there is
a finite number of distinct prediction profiles  that is  the predictions of interest take on
only a finite number of distinct values   this is certainly not true of all partially observable
systems and all sets of tests of interest  though it is true in many interesting examples 
formally  this assumption requires that  map histories to a finite set of prediction profiles 
assumption    assume there exists a finite set of prediction profiles p                    k   
      m such that for every history h   h   p  
this assumption allows the definition of the prediction profile system  or p p for short 
as a discrete dynamical system that captures the sequence of prediction profiles over time 
given an action observation sequence  the prediction profile systems actions  observations 
and dynamics are defined in terms of quantities associated with the original system 
definition    the prediction profile system is defined by a set of observations  a set of
actions  and a rule governing its dynamics 
   observations  the set of prediction profile observations  op p   is defined to be the set
def
of distinct prediction profiles  that is  op p   p                k   
   actions  the set of prediction profile actions  ap p   is defined to be the set of actiondef
observation pairs in the original system  that is  ap p   a  o 
   dynamics  the dynamics of the prediction profile system are deterministically governed by   at any prediction profile history  ha    o  i  ha    o  i        haj   oj ij   and
for any next p p  action  haj     oj   i  the prediction profile system deterministically
emits the p p  observation  a  o  a  o        aj oj aj   oj     
we now present some key facts about the prediction profile system  specifically  it
will be noted that the prediction profile system is always deterministic  also  though the
prediction profile system may be markov  as it is in the three card monte example   in
general it is partially observable 
proposition    even if the original system is stochastic  the prediction profile system is
always deterministic 
proof  this follows immediately from the definition  every history corresponds to exactly
one prediction profile  so a p p  history  action observation profile sequence  and a p p action  action observation pair  fully determine the next p p  observation  prediction profile   the stochastic observations in the original system have been folded into the unmodeled actions of the prediction profile system 
proposition     if the original system is markov  the prediction profile system is markov 
   

filearning to make predictions without a generative model

proof  by definition  if the original system is markov the prediction profile at any time
step depends only on the most recent observation  so  if at time step t  the current profile
is t   the agent takes action at   and observes observation ot     the next profile is simply
t     m arkov  ot      so  in fact  when the original system is markov  the prediction profile
system satisfies an even stronger condition  the next p p  observation is fully determined
by the p p  action and has no dependence on history whatsoever  including the most recent
p p  observation  
proposition     even if the original system is partially observable  the prediction profile
system may be markov 
proof  consider the three card monte example  the original system is clearly non markov
 the most recent observation  that is the dealers most recent swap  tells one very little about
the location of the ace   however  the prediction profile system for the tests of interest
regarding the location of the special card  pictured in figure    is markov  the next profile
is fully determined by the current profile and the p p  action 
in general  however  the p p system may be partially observable  though in the three
card monte example the current prediction profile and the next action observation pair
together fully determine the next prediction profile  in general the next prediction profile is
determined by the history of action observation pairs  and prediction profiles  
proposition     the prediction profile system may be partially observable 
proof  recall the  d ball bounce example  the corresponding prediction profile system is
shown in figure    note that two distinct states in the update graph are associated with
the same prediction profile  pixel x will be white   given only the current prediction profile
 pixel x will be white  and the p p  action  observe the ball in a neighboring pixel on the left
or right   one cannot determine whether the ball is entering or leaving the neighborhood 
and thus cannot uniquely determine the next profile  this prediction profile system is
partially observable 
so  in general  the prediction profile system is a deterministic  partially observable dynamical system  a model of the prediction profile system can only be used to make the
predictions of interest  as such  if one wishes to use a prediction profile model as a generative
model  one must select the tests of interest carefully  for instance 
proposition     if the tests of interest include the set of one step primitive tests  that is
if  ao   a  a  o  o   t i   then a model of the prediction profile system can be used as a
generative model of the original system 
proof  this follows immediately from the definition of generative model 
while in this special case a prediction profile model can be a complete  generative
model of the system  it will be shown in section   that if one desires a generative model 
it is essentially never preferable to learn a prediction profile model over a more traditional
representation  a prediction profile model is best applied when it is relatively simple to make
and maintain the predictions of interest in comparison to making all predictions  in general 
   

fitalvitie   singh

figure    flow of the algorithm 
a prediction profile model conditions on the observations  but it does not necessarily predict
the next observation  as such  a model of the prediction profile system cannot typically be
used for the purposes of model based planning control like a generative model could  the
experiments in section   will demonstrate that the output of prediction profile models can 
however  be useful for model free control methods 

   learning a prediction profile model
the definition of the prediction profile system straightforwardly suggests a method for
learning prediction profile models  estimate the prediction profiles  and learn a model of
their dynamics using a standard model learning technique   this section will present such
a learning algorithm  discussing some of the main practical challenges that arise 
let s be a training data set of trajectories of experience with the original system  actionobservation sequences  and let t i    t    t            tk   be the set of tests of interest  the
algorithm presented in this section will learn a model of the prediction profile system from
the data s  the algorithm has three main steps  pictured in figure     first the training
data is used to estimate the prediction profiles  both the number of unique profiles and their
values   next  the learned set of prediction profiles is used to translate the training data into
trajectories of experience with the prediction profile system  finally  any applicable model
learning method can be trained on the transformed data to learn a model of the prediction
profile system  ultimately  in our experiments  the learned prediction profile models will be
evaluated by how useful their predictions are as features for control 
    estimating the prediction profiles
given s and t i   the first step of learning a prediction profile model is to determine how
many distinct prediction profiles there are  as well as their values  the estimated prediction
for a test of interest t at a history h is 
p t   h   

  times t succeeds from h
 
  times acts t  taken from h
def

   

one could  at this point  directly estimate  by letting  h    hp t    h   p t    h           p tk  
h i  of course  due to sampling error  it is unlikely that any of these estimated profiles
will be exactly the same  even if the true underlying prediction profiles are identical  so 
   

filearning to make predictions without a generative model

to estimate the number of distinct underlying profiles  statistical tests will be used to find
histories that have significantly different prediction profiles 
to compare the profiles of two histories  a likelihood ratio test of homogeneity is performed on the counts for each test of interest in the two histories  if the statistical test
associated with any test of interest rejects the null hypothesis that the prediction is the
same in both histories  then the two histories have different prediction profiles 
in order to find the set of distinct prediction profiles  we greedily cluster the estimated
prediction profiles  specifically  an initially empty set of exemplar histories is maintained 
the algorithm searches over all histories in the agents experience  comparing each historys
estimated profile to the exemplar histories estimated profiles  if the candidate historys
profile is significantly different from the profiles of all exemplar histories  the candidate is
added as a new exemplar  in the end  the estimated profiles corresponding to the exemplar
histories are used as the set of prediction profiles  in order to obtain the best estimates
possible  the search is ordered so as to prioritize histories with lots of associated data 
the prediction profile estimation procedure has two main sources of complexity  the
first is the sample complexity of estimating the prediction profiles  it can take a great
deal of exploration to see each history enough times to obtain good statistics  especially if
the number of actions and observations is large  this issue could be addressed by adding
generalization to the estimation procedure  so that data from one sample trajectory could
improve the estimates of many similar histories  in one of the experiments in section   
observation abstraction will be employed as a simple form of generalization  the second
bottleneck is the computational complexity of searching for prediction profiles  as this involves exhaustively enumerating all histories in the agents experience  it would be valuable
to develop heuristics to identify the histories most likely to provide new profiles  in order
to avoid searching over all histories  in the experiments in section    a simple heuristic
of limiting the search to short histories is employed  long histories will tend to have less
associated data  and will therefore be less likely to provide distinguishably new profiles 
    generating prediction profile trajectories
having generated a finite set of distinct prediction profiles  the next step is to translate the
agents experience into sequences of action observation pairs and prediction profiles  these
trajectories will be used to train a model of the prediction profile system 
the process of translating an action observation sequence s into a prediction profile trajectory s is straightforward and  apart from a few practical concerns  follows directly from
definition    recall that  for an action observation sequence s   a  o  a  o        ak ok   the corresponding p p  action sequence is ha    o  iha    o  i       hak   ok i  the corresponding sequence of
profiles is  a  o    a  o  a  o           a  o        ak ok    thus  in principle  every primitive actionobservation sequence can be translated into an action observation profile sequence 
of course  is not available to generate the sequence of prediction profiles  so  it is
necessary to use an approximation   generated from the training data  specifically  the
estimated predictions for the tests of interest at each history h  computed using equation
   are compared  using statistical tests  to the set of distinct estimated prediction profiles
from section      if there is only one estimated profile  that is not statistically significantly
different from the estimated predictions at h  then let  h     
   

fitalvitie   singh

given sufficient data  the statistical tests will uniquely identify the correct match with
high probability  in practice  however  some histories will not have very much associated
data  it is possible in such a case for the test of homogeneity to fail to reject the null
hypothesis for two or more profiles  this indicates that there is not enough data to distinguish between multiple possible matches  in the experiments in section    two different
heuristic strategies for handling this situation are employed  the first strategy lets  h 
be the matching profile that has the smallest empirical kl divergence from the estimated
predictions  summed over all tests of interest   this is a heuristic choice that may lead to
noise in the prediction profile labeling  which could in turn affect the accuracy of the learned
model  the second strategy is to simply cut off any trajectory at the point where multiple
matches occur  rather than risk assigning an incorrect labeling  this ensures that labels
only appear in the prediction profile trajectories if there is a reasonable level of confidence
in their correctness  however  it is wasteful to throw out training data in this way 
    learning a prediction profile model
the translation step produces a set s  of trajectories of interaction with the prediction
profile system  recall that the prediction profile system is a deterministic  partially observable  discrete dynamical system and these trajectories can be used to train a model of the
prediction profile system using  in principle  any applicable model learning method 
there is an issue faced by models of the prediction profile system that is not present in
the usual discrete dynamical systems modeling setting  while the prediction profile labels
are present in the training data  when actually using the model they are not available  say
the current history is h  and an action a  is taken and an observation o  is emitted  together 
this action observation pair constitutes a p p  action  being a model of the prediction profile
system  a prediction profile model can identify the next profile    this profile can be used
to compute predictions p t   ha  o    for the tests of interest at the history ha  o    now
another action a  and observation o  occur  it is now necessary to update the pp models
state in order to obtain the next prediction profile  a typical dynamical systems model
makes predictions about the next observation  but is then able to update its state with the
actual observation that occurred  a prediction profile models observations are prediction
profiles themselves  which are not observable when interacting with the world  as such 
the prediction profile model will update its state with prediction profile it itself predicted
    once updated  the prediction profile model can obtain the profile that follows ha    o  i
which gives the predictions for the tests of interest at the new history ha  o  a  o   
if the prediction profile model is a perfect model of the prediction profile system  this
poses no problems  because the prediction profile system is deterministic  there is no need
to observe the true prediction profile label  it is fully determined by the history  in practice 
of course  the model will be imperfect and different modeling representations will require
different considerations when performing the two functions of providing predictions for the
tests of interest  and providing a profile for the sake of updating the model 
      pp pomdps
since the prediction profile system is partially observable it is natural to model it using a pomdp  unfortunately  even when the training data is from a deterministic sys   

filearning to make predictions without a generative model

tem  pomdp training using the em algorithm will generally not provide a deterministic
pomdp  thus  at any given history  a learned pomdp model of the prediction profile
system  pp pomdp  will provide a distribution over prediction profiles instead of deterministically providing the one profile associated with that history  the implementation
used in section   simply takes the most likely profile from the distribution to be the profile
associated with the history and uses it to make predictions for the tests of interest  as well
as to update the pomdp model 
      pp lpsts
another natural choice of representation for a prediction profile model is a looping predictive
suffix tree  lpst   holmes   isbell         lpsts are specialized to deterministic  partially
observable systems  as such  they could not be used to model the original system  which
is assumed to be stochastic in general   but they do apply to the prediction profile system
 and they do not have to be determinized like a pomdp  
briefly  an lpst captures the parts of recent history relevant to predicting the next
observation  every node in the tree corresponds to an action observation pair  a node
may be a leaf  may have children  or it may loop to one of its ancestors  every leaf of the
tree corresponds to a history suffix that has a deterministic prediction of an observation
for every action  in order to predict the next observation from a particular history  one
reads the history in reverse order  following the corresponding links on the tree until a leaf
is reached  which gives the prediction  holmes and isbell provide a learning algorithm that 
under certain conditions on the training data  is guaranteed to produce an optimal tree 
the reader is referred to the work of holmes and isbell        for details 
one weakness of lpsts  however  is that they fail to make a prediction for the next
observation if the current history does not lead to a leaf node in the tree  or if the leaf
node reached does not have a prediction for the action being queried   this typically occurs
when some history suffixes do not occur in the training data but do occur while using the
model  for a pp lpst  this can mean that in some histories the model cannot uniquely
determine the corresponding prediction profile  when this happens the implementation
used in section   simply finds the longest suffix of the current history that does occur in the
data  this suffix will be associated with multiple prediction profiles  otherwise the lpst
would have provided a prediction   to make predictions for the tests of interest  the model
provides the average prediction over this set of profiles  the profile used to update the
model is picked out of the set uniformly randomly 
      pp psrs
applying psr learning algorithms to prediction profile data poses a practical concern 
specifically  methods that attempt to estimate the system dynamics matrix  james   singh 
      wolfe et al         implicitly presume that every action sequence could in principle
be taken from every history  if some action sequences can be taken from some histories but
not from others  then the matrix will have undefined entries  this poses challenges to rank
estimation  and  indeed  the very definition of the model representation   unfortunately 
this can be the case for the prediction profile system since p p  actions  action observation
pairs  are not completely under the agents control  they are partly selected by the environ   

fitalvitie   singh

ment itself  the recent spectral learning algorithms presented by boots et al         may
be able to side step this issue  as they have more flexibility in selecting which predictions
are estimated for use in the model learning process  though we have not investigated this
possibility in this work 
note that  though our method for learning a prediction profile model involves standard
model learning methods for partially observable environments  the result is not a generative
model of the original system  a prediction profile model is a generative model of the
prediction profile system and  as such  cannot be used to make any predictions about the
original system  other than the predictions of interest 

   complexity of the prediction profile system
the learning algorithm we have presented will be evaluated empirically in section    first 
however  we analyze the complexity of the prediction profile system in relation to the complexity of the original system  this will give some indication of how difficult it is to learn a
prediction profile model and provide insight into when it is appropriate to learn a prediction
profile model over a more typical generative model approach 
there are many factors that affect the complexity of learning a model  this section
will largely focus on linear dimension as the measure of complexity  taking the view that 
generally speaking  systems with lower linear dimension are easier to learn than systems with
larger linear dimension  as discussed in section        this is generally true for pomdps 
where the linear dimension lower bounds the number of hidden states  so comparing the
linear dimension of the prediction profile system to that of the original system can give
some idea of whether it would be easier to learn a pp pomdp or just to learn a standard
pomdp of the original system  of course  there are other model learning methods for
which other complexity measures would be more appropriate  for instance it is not known
precisely how lpsts interact with linear dimension   extending some of these results to
other measures of complexity may be an interesting topic of future investigation 
    linear dimension comparison
this section will discuss how the linear dimension of the prediction profile system relates
to that of the original system  the first result is a proof of concept that simply states
that there exist problems in which the prediction profile system is vastly more simple than
the original system  in fact  such a problem has already been presented 
proposition     the prediction profile system can have linear dimension that is arbitrarily
lower than that of the original system 
proof  recall the three card monte example  thus far the domain has been described
without describing the dealers behavior  however  note that the prediction profile system
for the tests of interest relating to the location of the special card  pictured in figure    has
a linear dimension of    regardless of how the dealers swaps are chosen  if a very complex
dealer is chosen  the original system will have high linear dimension  but the prediction
profile systems linear dimension will remain constant  for instance  in the experiments in
section    the dealer chooses which cards to swap stochastically  but is more likely to choose
   

filearning to make predictions without a generative model

the swap that has been selected the least often so far  thus  in order to predict the dealers
next decision  one must count how many times each swap has been chosen in history and
as a result the system effectively has infinite linear dimension 
on the other hand  prediction profile models are not a panacea  the following results
indicate that there are problems for which learning a prediction profile model would not
be advisable over learning a standard generative model  in that the linear dimension of the
prediction profile system can be far greater than that of the original system  later in the
section some special cases will be characterized where prediction profile models are likely to
be useful  the next result shows that the linear dimension of the prediction profile model
can be infinite when the original system has finite linear dimension  via a lower bound on
linear dimension that is true of all deterministic dynamical systems 
proposition     for any deterministic dynamical system with actions a  and observations
o  the linear dimension  n  log  a    log  o    
 
log  a 
proof  see appendix a   
because proposition    applies to all deterministic dynamical systems  it certainly applies to the prediction profile system  though it is a very loose bound  the basic implication
is that as the number of prediction profiles  the observations of p p   increases in comparison to the number of action observation pairs  the actions of p p    the linear dimension of
the prediction profile system necessarily increases  this bound also clearly illustrates the
importance of the assumption that there is a finite number of distinct prediction profiles 
corollary     if there are infinitely many distinct prediction profiles  the prediction profile
system has infinite linear dimension 
proof  clearly  ap p      a  o  is finite so long as there are finitely many actions and
observations  so  from the last result it follows immediately that as the number of distinct
prediction profiles  op p   approaches infinity  then so must the linear dimension of the
prediction profile system 
hence  so long as prediction profile models are represented using methods that rely
on a finite linear dimension  it is critical that there be finitely many prediction profiles 
note that this is not a fundamental barrier  but a side effect of the representational choice 
model learning methods that are not as sensitive to linear dimension  such as those designed
to model continuous dynamical systems  may be able to effectively capture systems with
infinitely many prediction profiles 
one conclusion to be drawn from the last few results is that knowing the linear dimension
of the original system does not  in itself  necessarily say much about the complexity of the
prediction profile system  the prediction profile system may be far simpler or far more
complex than the original system  thus it may be more informative to turn to other factors
when trying to characterize the complexity of the prediction profile system 
   

fitalvitie   singh

    bounding the complexity of the prediction profile system
the results in the previous section do not take into account an obviously important aspect
of the prediction profile system  the predictions it is asked to make  some predictions
of interest can be made very simply by keeping track of very little information  other
predictions will rely on a great deal of history information and will therefore require a more
complex model  the next result identifies the worst case set of tests of interest for any
system  the tests of interest whose corresponding prediction profile model has the highest
linear dimension  ultimately this section will present some  non exhaustive  conditions
under which the prediction profile system is likely to be simpler than the original system 
proposition     for a given system and set of tests of interest  the linear dimension of
the corresponding prediction profile system is no greater than that of the prediction profile
system associated with any set of core tests for the system  as described in section      
proof  see appendix a   
with this worst case identified  one can immediately obtain bounds on how complex
any prediction profile system can possibly be 
corollary     for any system and any set of tests of interest  the corresponding prediction
profile system has linear dimension no greater than the number of distinct predictive states
for the original system 
proof  the prediction profile system for a set of core tests q is a deterministic mdp where
the observations are prediction profiles for q  that is  predictive states   that is  each state
is associated with a unique prediction profile  the linear dimension of an mdp is never
greater than the number of observations  singh et al          therefore  by the previous
result the prediction profile system for any set of tests of interest can have linear dimension
no greater than the number of predictive states 
corollary     if the original system is a pomdp  the prediction profile system for any set
of tests of interest has linear dimension no greater than the number of distinct belief states 
proof  this follows immediately from the previous result and the fact that the number of
distinct predictive states is no greater than the number of distinct belief states  littman
et al         
the bounds presented so far help explain why the prediction profile system can be more
complex than the original system  however  because they are focused on the worst possible
choice of tests of interest  they do little to illuminate when the opposite is true  a prediction
profile model is at its most complex when it is asked to perform the same task as a generative
model  keep track of as much information from history as is necessary to make all possible
predictions  or equivalently  the predictive state or the belief state   these results indicate
that  generally speaking  if one desires a generative model  standard approaches would be
preferable to learning a prediction profile model 
on the other hand  our stated goal is not to learn a generative model  but instead to
focus on some particular predictions that will hopefully be far simpler to make than all
predictions  the examples we have seen make it clear that in some cases  some predictions
   

filearning to make predictions without a generative model

can be made by a prediction profile model far simpler than a generative model of the original
system  in general one might expect the prediction profile model to be simple when the
predictions of interest rely on only a small amount of the state information required to
maintain a generative model  the next bound aligns with this intuitive reasoning 
essentially what this result points out is that often much of the hidden state information
in a pomdp will be irrelevant to the predictions of interest  the linear dimension of the
prediction profile system is bounded only by the number of distinct beliefs over the relevant
parts of the hidden state  rather than the number of distinct beliefs states overall  the idea
of the result is that if one can impose an abstraction over the hidden states of a pomdp
 not the observations  that still allows the predictions of interest to be made accurately
and that allows abstract belief states to be computed accurately  then the prediction profile
systems linear dimension is bounded by the number of abstract belief states 
proposition     consider a pomdp with hidden states s  actions a  and observations
o  let t i be the set of tests of interest  let ai be the action taken at time step i  si be the
hidden state reached after taking action ai   and oi be the observation emitted by si   now 
consider any surjection    s  s  mapping hidden states to a set of abstract states with
the following properties 
   for any pair of primitive states s    s   s  if  s       s     then for any time step i
and any test of interest t  t i   p t   si   s      p t   si   s    
   for any pair of primitive states s    s   s  if  s       s     then for any time step i 
abstract state s  s    observation o  o  and action a  a 
pr  si       s   si   s   ai     a  oi     o   
pr  si       s   si   s    ai     a  oi     o  
for any such   the prediction profile system for t i has linear dimension no greater than
the number of distinct beliefs over abstract states  s   
proof  see appendix a  
there are a few things to note about this result  first  a surjection  always exists that
def
has properties   and    one can always define    s  s with  s    s  this degenerate
case trivially satisfies the requirements of proposition    and recovers the bound given in
corollary     however  proposition    applies to all surjections that satisfy the conditions 
there must be a surjection that satisfies the conditions and results in the smallest number of
beliefs over abstract states  essentially  this is the one that ignores as much state information
as possible while still allowing the predictions of interest to be made accurately and it is this
surjection that most tightly bounds the complexity of the prediction profile system  even if
 is not known  
of course  there may still be a large or even infinite number of distinct beliefs  even over
abstract states  so other factors must come into play to ensure a simple prediction profile
system  furthermore  this result does not characterize all settings in which the prediction
profile system will be simple  that said  this result does support the intuition that the
   

fitalvitie   singh

prediction profile system will tend to be simple when the predictions it is asked to make
depend on small amounts of state information 
in order to build intuition about how this result relates to earlier examples  recall the
three card monte problem  in three card monte there are two sources of hidden state 
the aces unobserved position and whatever hidden mechanism the dealer uses to make its
decisions  clearly the agents predictions of interest depend only on the first part of the
hidden state  so  in this case one can satisfy property   with a surjection  that maps
two hidden states to the same abstract state if the ace is in the same position  regardless
of the dealers state  under this  there are only   abstract states  one for each possible
position   even though there might be infinitely many true hidden states  now  different
states corresponding to the same ace position will have different distributions over the aces
next position  this distribution does  after all  depend upon the dealers state  however 
property   is a statement about the distribution over the next abstract state given the
observation that is emitted after entering the abstract state  if one knows the current
abstract state and observes what the dealer does  the next abstract state is fully determined 
so property   holds as well  in fact  since the aces position is known at the beginning of the
game  this means the current abstract state is always known with absolute certainty  even
though beliefs about the dealers state will in general be uncertain  hence  there are only  
distinct beliefs about the abstract states  one for each state   as such  the prediction profile
models linear dimension is upper bounded by    regardless of the dealers complexity  and
in this case the bound is met  
    bounding the number of prediction profiles
the previous section describes some conditions under which the prediction profile system
may have a lower linear dimension than the original system  also of concern is the number
of prediction profiles  and whether that number is finite  this section will briefly discuss
some  non exhaustive  cases in which the number of prediction profiles is bounded 
one case that has already been discussed is when the original system is markov  in that
case the number of prediction profiles is bounded by the number of observations  states  
of course  when the original system is markov  there is little need to use prediction profile
models  another  similar case is when the system is partially observable  but completely
deterministic  that is  the next observation is completely determined by history and the
selected action   if the system is a deterministic pomdp then at any given history the
current hidden state is known  as such  the number of belief states is bounded by the
number of hidden states  since there cannot be more prediction profiles than belief states 
the number of prediction profiles are bounded as well 
one can move away from determinism in a few different ways  first  note that the key
property of a deterministic pomdp is that the hidden state is fully determined by history 
it is possible to satisfy this property even in stochastic systems  as long as one can uniquely
determine the hidden state  given the observation that was emitted when arriving there  in
that case  observations can be emitted stochastically  but the number of belief states  and
the number of prediction profiles  is still bounded by the number of hidden states 
another step away from determinism is a class of systems  introduced by littman        
called det pomdps  a det pomdp is a pomdp where the transition function and
   

filearning to make predictions without a generative model

the observation function are both deterministic  but the initial state distribution may be
stochastic  a det pomdp is not a deterministic dynamical system  as there is uncertainty
about the hidden state  because of this uncertainty  the system appears to emit observations
stochastically  it is only the underlying dynamics that are deterministic  littman showed
that a det pomdp with n hidden states and an initial state distribution with m states in
its support has at most  n     m    distinct belief states  so  this bounds the number of
prediction profiles as well 
finally  and most importantly  if the hidden state can be abstracted as in proposition    
then these properties only really need to hold for abstract beliefs  that is  the environment
itself may be complex and stochastic in arbitrary ways  but if the abstract hidden state
described in proposition    is fully determined by history  then the number of prediction
profiles is bounded by the number of abstract states  as was the case in three card monte  
similarly  det pomdp like properties can be imagined for abstract hidden states as well 
these cases by no means cover all situations where the number of prediction profiles
can be bounded  but they do seem to indicate that the class of problems where the number
of prediction profiles is finite is quite broad  and may contain many interesting examples 

   experiments
this section will empirically evaluate the prediction profile model learning procedure developed in section    in each experiment an agent faces an environment for which a generative
model would be a challenge to learn due to its high linear dimension  however  in each
problem the agent could make good decisions if it could only have the predictions to a small
number of important tests  a prediction profile model is learned for these important tests
and the accuracy of the learned predictions is evaluated 
these experiments also demonstrate one possible use of prediction profile models  and
partial models in general  for control  because they are not generative  prediction profile
models cannot typically be used directly by offline  model based planning methods  however  their output may be useful for model free methods of control  specifically  in these
experiments  the predictions made by the learned prediction profile models are provided as
features to a policy gradient algorithm 
    predictive features for policy gradient
policy gradient methods  e g   williams        baxter   bartlett        peters   schaal 
      have been very successful as viable options for model free control in partially observable domains  though there are differences between various algorithms  the common
thread is that they assume a parametric form for the agents policy and then attempt to
alter those parameters in the direction of the gradient with respect to expected average reward  these experiments will make use of online gpomdp with average reward baseline
 weaver   tao         or olgarb  readers are referred to the original paper for details  
olgarb assumes there is some set of features of history  and that the agents policy takes
the parametric form 
pr a   h  w 
   p

e

p

a

   

wi a fi  h 
p
i wi a fi  h 

i

e

fitalvitie   singh

where fi  h  is the ith feature and each parameter wi a is a weight specific to the feature
and the action being considered 
typically the features used in policy gradient are features that can be directly read
from history  e g   features of the most recent few observations or the presence absence of
some event in history   it can be difficult to know a priori which historical features will be
important for making good control decisions  in contrast  the idea in these experiments is
to provide the values of some predictions as features  these predictive features have direct
consequences for control  as they provide information about the effects of possible behaviors
the agent might engage in  as such  it may be easier to select a set of predictive features
that are likely to be informative about the optimal action to take  e g   will the agent
reach the goal state when it takes this action  or will taking this action damage the
agent    furthermore  information may be expressed compactly in terms of a prediction
that would be complex to specify purely in terms of past observations  as seen in the
discussion of psrs in section      an arbitrary length history can be fully captured by
a finite set of short term predictions  for these reasons it seems reasonable to speculate
that predictive features  as maintained by a prediction profile model  may be particularly
valuable to model free control methods like policy gradient 
    experimental setup
the learning algorithm will be applied to two example problems  in each problem prediction
profile models are learned with various amounts of training data  using both lpsts and
pomdps as the representation and using both strategies for dealing with multiple matches 
as described in section       the prediction accuracy of the models is evaluated  as well as
how useful their predictions are as features for control  the training data is generated by
executing a uniform random policy in the environment 
the free parameter of the learning algorithm is the significance value of the statistical
tests    given the large number of contingency tests that will be performed on the same
data set  which can compound the probability of a false negative   should be set fairly
low  in these experiments we use             though several reasonable values were tried
with similar results  as discussed in section    there will also be a maximum length of
histories to consider during the search for prediction profiles  this cutoff allows the search
to avoid considering long histories  as there are many long histories to search over and they
are unlikely to provide new prediction profiles 
after a prediction profile model is learned  its predictions are evaluated as features for
the policy gradient algorithm olgarb  specifically  for each test of interest t the unit
interval is split up into    equally sized bins b and a binary feature ft b is provided that is
  if the prediction of t lies in bin b  and   otherwise  also provided are binary features fo  
for each possible observation o  the feature fo     if o is the most recent observation and
   otherwise  the parameters of olgarb  the learning rate and discount factor  are set
to      and       respectively in all experiments 
to evaluate a prediction profile model olgarb is run for           steps  the average
reward obtained and the root mean squared error  rmse  of the predictions for the tests
of interest accrued by the model along the way are reported  prediction performance is
compared to that obtained by learning a pomdp on the training data and using it to
   

filearning to make predictions without a generative model

prediction performance

control performance
   

avg  reward     trials 

avg  rmse     trials 

 
   
   
flat pomdp

   
pplpst kld 
pplpst cut 

   
 
 

pppomdp kld 
pppomdp cut 

 

 

  training trajectories

true

    
    

pppomdp kld 
pppomdp cut 

    

pplpst kld 
pplpst cut 

 
flat pomdp

    
    
    
 

 

expert

    

som

 

 

 

  training trajectories x    

 

x   

figure    results in the three card monte domain 
make the predictions of interest  because these problems are too complex to feasibly train
a pomdp with the correct number of underlying states     state pomdps were used
 stopping em after a maximum of    iterations     control performance is compared to
that obtained by olgarb using the predictions provided by a learned pomdp model as
features  as well as olgarb using the true predictions as features  the best the prediction
profile model could hope to do   olgarb using second order markov features  the two
most recent observations  as well as the action between them  but no predictive features at
all  and a hand coded expert policy 
    three card monte
the first domain is the three card monte example  the agent is presented with three
cards  initially  the card in the middle  card    is the ace  the agent has four actions
available to it  watch  f lip   f lip   and f lip   if the agent chooses a flip action  it observes
whether the card it flipped over is the special card  if the agent chooses the watch action 
the dealer can swap the positions of two cards  in which case the agent observes which two
cards were swapped  or the dealer can ask for a guess  if the dealer has not asked for a
guess  then watch results in   reward and any flip action results in    reward  if the dealer
asks for a guess and the agent flips over the special card  the agent gets reward of    if the
agent flips over one of the other two cards  or doesnt flip a card  by selecting watch   it
gets reward of     the agent has three tests of interest  and they take the form f lipx ace 
for each card x  that is  if i flip card x  will i see the ace   
as discussed previously  the complexity of this system is directly related to the complexity of the dealers decision making process  in this experiment  when the agent chooses
watch the dealer swaps the pair of cards it has swapped the least so far with probability
     with probability     it chooses uniformly amongst the other pairs of cards  otherwise
it asks for a guess  since the dealer is keeping a count of how many times each swap was
made  the process governing its dynamics effectively has an infinite linear dimension  the
   similar results were obtained with                and    states 

   

fitalvitie   singh

prediction profile system  on the other hand  has only   states  regardless of the dealers
complexity  see figure    
training trajectories were of length     figure   shows the results for various amounts of
training data  averaged over    trials  both pp pomdps and pp lpsts learned to make
accurate predictions for the tests of interest  eventually achieving zero prediction error  in
this case  pp pomdps did so using less data  this is likely because a pomdp model is
more readily able to take advantage of the fact that the prediction profile system for three
card monte is markov  as expected  the standard pomdp model was unable to accurately
predict the tests of interest 
also compared are the two different strategies for dealing with multiple matches discussed in section      recall that the first one  marked kld in the graph  picks the
matching profile with the smallest empirical kl divergence from the estimated predictions 
the second  marked cut in the graph  simply cuts off the trajectory at the point of
a multiple match to avoid any incorrect labels  in this problem these two strategies result in almost exactly the same performance  this is likely because the profiles in three
card monte are deterministic  and are therefore quite easy to distinguish  making multiple
matches unlikely   the next experiment will have stochastic profiles 
the predictive features provided by the prediction profile models are clearly useful for
control  as the control performance of olgarb using their predictions approaches  and
eventually exactly matches that of olgarb using the true predictions  marked true  
the inaccurate predictions provided by the pomdp were not very useful for control  olgarb using the pomdp provided predictions does not even break even  meaning it loses
the game more often than it wins  the pomdp features did  however  seem to contain
some useful information beyond that provided by the second order markov features  marked
som  which  as one might expect  performed very poorly 
    shooting gallery
the second example is called the shooting gallery  pictured in figure   a   the agent
has a gun aimed at a fixed position on an    grid  marked by the x    a target moves
diagonally  bouncing off of the boundaries of the image and    obstacles  an example
trajectory is pictured   the agents task is to shoot the target  the agent has two actions 
watch and shoot  when the agent chooses watch  it gets   reward  if the agent chooses
shoot and the target is in the crosshairs in the step after the agent shoots  the agent gets
reward of     otherwise it gets a reward of     whenever the agent hits the target  the
shooting range resets  the agent receives a special reset observation  each      square on
the range is made an obstacle with probability      and the target is placed in a random
position  there is also a      probability that the range will reset at every time step  the
difficulty is that the target is sticky  every time step with probability     it moves in its
current direction  but with probability     it sticks in place  thus  looking only at recent
history  the agent may not be able to determine the targets current direction  the agent
needs to know the probability that the target will be in its sights in the next step  so clearly
the single test of interest is  watch target  that is if i choose the watch action  will the
target enter the crosshairs    when the target is far from the crosshairs  the prediction of
this test will be    when it target is in the crosshairs  it will be      when the target is
   

filearning to make predictions without a generative model

 a 

 b 

figure    the shooting gallery domain   a  a possible arrangement of obstacles and trajectory for the target  lighter is further back in time   in this case the target will
definitely not enter the agents crosshairs  since it will bounce off of the obstacle 
 b  the abstraction applied to the most recent observation 

near the crosshairs  the model must determine whether the prediction is     or    based on
the targets previous behavior  its direction  and the configuration of nearby obstacles 
this problem has stochastic prediction profiles  so it is expected that more data will
be required to differentiate them  also  due to the number of possible configurations of
obstacles and positions of the target  this system has roughly           observations and
even more latent states  this results in a large number of possible histories  each with only
a small probability of occurring  as discussed in section    this can lead to a large sample
complexity for obtaining good estimates of prediction profiles  here this is addressed with
a simple form of generalization  observation abstraction  two observations are treated as
the same if the target is in the same position and if the configuration of obstacles in the
immediate vicinity of the target is the same  in other words  each abstract observation
contains information only about the targets position and the obstacles surrounding the
target  and not the placement of obstacles far away from the target  see figure   b   for an
example  under this abstraction  the abstract observations still provide enough detail to
make accurate predictions  that is  two histories do indeed have the same prediction profile
if they have the same action sequence and their observation sequences correspond to the
same sequence of aggregate observations  this enables one sample trajectory to improve
the estimates for several histories  though  even with this abstraction  there are still over
     action observation pairs  the same observation abstraction was applied when training
the pomdp model 
training trajectories were length   and the search for profiles was restricted to length
  histories  results are shown in figure    perhaps the most eye catching feature of
the results is the upward trending curve in the prediction error graph  corresponding to
the pp pomdp with the kl divergence based matching  labeled pp pomdp kld   
recall that the danger of the kl divergence based matching strategy is that it may produce
incorrect labels in the training data  apparently these errors were severe enough in this
problem to drastically mislead the pomdp model  with a small amount of data it obtained
   

fitalvitie   singh

prediction performance

control performance
     

avg  reward     trials 

avg  rmse     trials 

    
   
pppomdp cut 

    
flat pomdp

   

pppomdp kld 

    

pplpst cut 
pplpst kld 

 
 

 

 

 

 

  training trajectories

    
     
    

pplpst kld 
pppomdp kld 

pplpst cut 

     
pppomdp cut 

 
     
 

  
 
x   

expert
true

som
flat pomdp

 

 

 

 

  

  training trajectories x    

figure    results in the shooting gallery domain 
very good prediction error  but with more data came more misleading labelings  and the
performance suffered  the pp pomdp trained with the other matching method  pppomdp cut   displays a more typical learning curve  more data results in better error  
though it takes a great deal of data before it begins to make reasonable predictions  this
is because cutting off trajectories that have multiple matches throws away data that might
have been informative to the model  the pp lpsts generally outperform the pp pomdps
in this problem  with the trajectory cutting method  the pp lpst  pp lpst cut  
quickly outperforms the flat pomdp and  with enough data  outperforms both versions of
pp pomdp  the pp lpst with the kl divergence based matching  pp lpst kld  
is by far the best performer  quickly achieving small prediction error  clearly the incorrect
labels in the training data did not have as dramatic an effect on the lpst learning  possibly
because  as a suffix tree  an lpst mostly makes its predictions based on recent history 
limiting the effects of labeling errors to a few time steps 
control performance essentially mirrors prediction performance  with some interesting
exceptions  note that even though pp pomdp kld  obtains roughly the same prediction
error as the flat pomdp at           training trajectories  the predictive features it provides
still result in substantially better control performance  this indicates that  even though the
pp pomdp is making errors in the exact values of the predictions  it has still captured more
of the important dynamics of the predictions than the flat pomdp has  the flat pomdp
itself provides features that are roughly as useful as second order markov features  which
do not result in good performance  again  olgarb using these features does not break
even  meaning it is wasting bullets when the target is not likely to enter the crosshairs  the
best performing prediction profile model  pp lpst kld  approaches the performance of
olgarb using the true predictions with sufficient data 

   related work
the idea of modeling only some aspects of the observations of a dynamical system has
certainly been raised before  for instance  in a recent example rudary        learned linear   

filearning to make predictions without a generative model

gaussian models of continuous partially observable environments where some dimensions of
the observation were treated as unmodeled exogenous input  these inputs were assumed
to have a linear effect on state transition  along somewhat similar lines  but in the context of
model minimization  taking a given  complete model and deriving a simpler  abstract model
that preserves the value function  wolfe        constructed both an abstract model and a
shadow model that predicts observation details that are ignored by the abstraction  the
shadow model takes the abstract observations of the abstract model as unmodeled input 
splitting the observation into modeled and un modeled components and then learning a
generative model is certainly related to our approach  in that case  a model would make all
conditional predictions about the modeled portion of the observation  given the exogenous
inputs  as well as actual actions and the history   prediction profile models take this to an
extreme  by treating the entire observation as input  instead of predicting future sequences
of some piece of the next observation conditioned on another piece  prediction profile models
predict the values of an arbitrary set of predictions of interest at the next time step  given
the entire action and observation  this allows significantly more freedom in choosing which
predictions the model will make  and  more importantly  will not make  
one modeling method closely related to prediction profiles is causal state splitting
reconstruction  cssr   shalizi   klinker         cssr is an algorithm for learning generative models of discrete  partially observable  uncontrolled dynamical systems  the basic
idea is to define an equivalence relation over histories where two histories are considered
equivalent if they are associated with identical distributions over possible futures  the
equivalence classes under this relation are called causal states  the cssr algorithm learns
the number of causal states  the distribution over next observations associated with each
causal state  and the transitions from one causal state to the next  given an observation 
it is straightforward to see that there is a one to one correspondance between causal states
and the predictive states of a psr  as such  a causal state model is precisely the prediction
profile model where the set of tests of interest is q  some set of core tests  with this correspondance in hand  the results in section     show that in many cases the number of causal
states will greatly exceed the linear dimension of the original system and that therefore
cssr may be inadvisable in many problems  in comparison to more standard modeling
approaches  it is possible that the cssr algorithm could be adapted to the more general
setting of arbitrary sets of tests of interest  however the algorithm does rely heavily on the
fact that a prediction profile model with q as the tests of interest is markov  which is not
generally the case for other sets of tests of interest 
as mentioned in section    mccallum        presented utree  a suffix tree based algorithm for learning value functions in partially observable environments  because utree
learns only the value function  a prediction about future rewards   and does not make any
predictions about observations  utree does learn a non generative partial model  wolfe
and barto        extend utree to make one step predictions about particular observation
features rather than limiting predictions to the value function  because it learns a suffix
tree  utree is able to operate on non episodic domains  whereas our method requires seeing
histories multiple times  and is not required to explicitly search for distinct prediction profiles  utree also directly incorporates abstraction learning  learning simultaneously which
observation features are important  and where in the history suffix to attend to them  that
said  the main drawback of the suffix tree approach is that the tree only takes into account
   

fitalvitie   singh

information from relatively recent history  a suffix of the history   it cannot remember
important information for an arbitrary number of steps as a recurrent state based model
can  in the three card monte example  for instance  having access to a depth limited suffix
of history would be of little help  in order to track the ace  one must take into account
every move the dealer has made since the beginning of the game  utree would essentially
forget where the card was if the games length surpassed the depth of its memory 
mccallum        and mahmud        both provide methods for learning state machines
that predict the immediate reward resulting from any given action observation pair in partially observable control tasks  and thus do not suffer from the issue of finite depth memory
that suffix trees do   thus  their learning problem is a special case of ours  where they
restrict their models to make one step predictions about the immediate reward  in both
cases  a simple model is incrementally and greedily elaborated by proposing states to be split
and evaluating the results  via statistical tests in the case of mccallum and via likelihood
hill climbing in the case of mahmud   mccallum expressed concern that his approach had
difficulty extracting long range dependencies  for instance  learning to attend to an event
that does not appear to affect the distribution of rewards until many steps later   it is not
clear the extent to which mahmuds approach addresses this issue  these methods have
some of the advantages of utree  most notably that they can be applied to non episodic
domains  that said  our approach has advantages as well  by re casting the problem of
learning a non generative model as a standard generative model learning problem  we have
been able to gain deeper understanding of the complexity and applicability of prediction
profile models compared to more standard generative models  furthermore  this has allowed us to incorporate standard  well studied generative model learning methods into our
learning algorithm  thereby leveraging their strengths in the non generative setting  most
specifically  this has resulting in a principled  albeit heuristic  learning algorithm  that does
not rely on guess and check or stochastic local search 
the prediction profile system is also similar in spirit to finite state controllers for
pomdps  sondik        noted that in some cases  it is possible to represent the optimal policy for a pomdp as a finite state machine  these finite state controllers are very
much like prediction profile models in that they take action observation pairs as inputs 
but instead of outputting predictions associated with the current history  they output the
optimal action to take  multiple authors  e g   hansen        poupart   boutilier       
provide techniques for learning finite state controllers  however  these algorithms typically
require access to a complete pomdp model of the world to begin with which  in our setting 
is assumed to be impractical 

   conclusions and future directions
the most standard methods for learning models in partially observable environments learn
generative models  if one has only a small set of predictions of interest to make  and
therefore does not require the full power of a generative model   one can ignore irrelevant
detail via abstraction to simplify the learning problem  even so  a generative model will
necessarily make predictions about any relevant details  even if they are not directly of
interest  we have seen by example that the resulting model can be counter intuitively
complex  even if the predictions the model is being asked to make are quite simple 
   

filearning to make predictions without a generative model

we presented prediction profile models  which are non generative models for partially
observable systems that make only the predictions of interest and no others  the main idea
of prediction profile models is to learn a model of the dynamics of the predictions themselves
as they change over time  rather than a model of the dynamics of the system  the learning
method for prediction profile models learns a transformation of the training data and then
applies standard methods to the transformed data  assuming that the predictions of interest
take on only a finite number of distinct values   as a result  it retains advantages of methods
like em for pomdps that learn what information from history must be maintained in order
to make predictions  rather than requiring a set of history features a priori   we showed
that a prediction profile model can be far simpler than a generative model  though it can
also be far more complex  depending on what predictions it is asked to make  however  if
the predictions of interest depend on relatively little state information  prediction profile
models can provide substantial savings over standard modeling methods such as pomdps 
while the experiments in section   demonstrate that it is possible to learn prediction
profile models in contrived systems too complex for pomdps  the specific learning algorithm presented here is not likely to scale to more natural domains without modification 
the most critical scaling issues for prediction profile models are the sample complexity of
estimating the prediction profiles  and the computational complexity of searching for prediction profiles and translating the data  in both cases  the critical source of complexity is
essentially how many distinct histories there are in the training data  more distinct histories
means the data is spread thin amongst them and there are more estimated profiles to search
through   as such  generalization of prediction estimates across many histories would be
a key step toward applying these ideas to more realistic domains  we are currently developing learning algorithms that combine the ideas behind prediction profile models with
methods for learning abstractions that allow many essentially equivalent histories to be
lumped together for the purposes of estimating the predictions of interest 
another limitation of the prediction profile model learning method presented here is its
reliance on the assumption of a finite number of prediction profiles  while this assumption
does hold in many cases  an ideal method would be able to deal gracefully with a very large
or infinite number of prediction profiles  one possibility is to simply cluster the predictions
in other ways  for instance  one may only desire a certain level of prediction accuracy and
may therefore be willing to lump some distinct prediction profiles together in exchange for a
simpler prediction profile system  another idea would be to learn a prediction profile model
using continuous valued representations such as kalman filters  kalman        or plgs
 rudary  singh    wingate         or their nonlinear variants  e g   julier   uhlmann 
      wingate         these representations and learning algorithms explicitly deal with
systems with an infinite number of observations  prediction profiles in this case   even
when there are finitely many prediction profiles  methods for learning non linear continuous
models may still be able to  approximately  capture the discrete dynamics 
additionally  though our results have focused on discrete systems  the main motivation
behind prediction profile models also has purchase in the continuous setting  typical methods for learning models of partially observable systems in continuous systems  much like
their discrete valued counterparts  learn generative models  as such  the non generative
approach of prediction profile models may provide similar benefits in the continuous setting
if not all predictions need be made  in this setting  prediction profiles might be represented
   

fitalvitie   singh

in a parametric form  for instance  the mean and variance of a gaussian   the main idea of
prediction profile models  though not the specific method presented here  could still then
be applied  learn a model of the dynamics of these distribution parameters  rather than the
dynamics of the system itself 
finally  we have not discussed in this work how the tests of interest should be determined  only how to predict them once they are selected  automatically selecting interesting important predictive features as targets for partial models would certainly be an
interesting research challenge  of course  this would depend on what the predictions will
be used for  if the predictions will be used as features for control  as we have done in
our experiments  then it would certainly seem intuitive to start with predictive features
regarding the reward signal  and perhaps observation features that strongly correlate with
reward  as we have intuitively done by hand in our experiments   it may also be useful to
consider making predictions about those predictions in the style of td networks  sutton
  tanner         for instance  one could imagine learning models that make predictions
about which profile another model will emit  in this way models could be chained together
to make predictions about more extant rewards  rather than focusing solely on predicting
the immediate reward signal  which is not always a particularly good feature for temporal
decision problems   another common use of partial models is to decompose a large modeling
problem into many small ones  as in  for instance  factored mdps  boutilier et al         
factored psrs  wolfe et al          or collections of local models  talvitie   singh      b  
in this setting  choosing tests of interest would be an example of the structure learning
problem  decomposing one step predictions into relatively independent components and
then assigning them to different models 

acknowledgments
erik talvitie was supported under the nsf grfp  satinder singh was supported by nsf
grant iis          any opinions  findings  and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily reflect the views of the nsf 
the work presented in this paper is an extension of work presented at ijcai  talvitie  
singh      a   we are grateful to the anonymous reviewers whose helpful comments have
improved the presentation of this work 

appendix a 
a   proof of proposition   
this result will follow straightforwardly from a general fact about dynamical systems  let
h i   j  be the sequence of actions and observations from h starting with the ith time step in
the sequence and ending with the jth time step in the sequence  for conveniences sake  if
i   j let h i   j    h    the null sequence  the following two results will show that if some
test t ever has positive probability  then it must have positive probability at some history
with length less than the linear dimension of the system 

   

filearning to make predictions without a generative model

figure    the matrix constructed in lemma    is full rank  a contradiction  
lemma     if the linear dimension of a dynamical system is n  then for any test t and
history h with length h    k  n and p t   h       i  j with    i   j     k such that
p t   h     i  h j   k        
proof  note that because p t   h       p h  i      k  t   h     i      p t   h p h  i      k    h     i     
  for all    i  k  now assume for all i  j with    i   j     k that p h j   k  t   h     i     
p t   h     i  h j   k   p h j   k    h     i        and seek a contradiction  consider a submatrix of
the system dynamics matrix  the rows of this submatrix correspond to prefixes of h  h     i 
for all    i  k  the columns correspond to suffixes of h pre pended to the test t  h j   k  t
for all    j  k      this is a k      k     matrix  under the above assumption  this
matrix is triangular with positive entries along the diagonal  figure   shows this matrix
when k       as such  this matrix is full rank  rank k       this is a contradiction since
k  n and a submatrix can never have higher rank than the matrix that contains it 
the next result follows immediately from lemma    
corollary     if the system has linear dimension n and for some test t and history h
p t   h       then there exists a  possibly non consecutive  subsequence h of h such that
length h     n with p t   h       
proof  by lemma     every history h with length k  n such that p t   h      must have
a subsequence h  with length k    k such that p t   h       if k   n  then h  must have a
subsequence h  with length k    k    this argument can be repeated until the subsequence
has length less than n 
the consequence of corollary    is that every test that ever has positive probability 
must have positive probability following some history of length less than n  with this fact
in hand  proposition    can now be proven 
proposition     for any deterministic dynamical system with actions a  and observa 
tions o  the linear dimension  n  log  a    log  o    
log  a 
   

fitalvitie   singh

proof  since the system is deterministic  each history and action correspond to exactly one
resulting observation  a history is a sequence of actions and observations  however  since
the sequence of observations is fully determined by the sequence of actions in a deterministic
system  the number of distinct histories of length k is simply  a k   at each history there
are  a  action choices that could each result in a different observation  so  the number
of observations that could possibly occur after histories of length k is simply  a k     by
corollary     if the linear dimension is n  all observations must occur after some history h
with length h   n     thus  the number of observations that can possibly follow histories
of length less than n is 
 o  

n 
x

 a i    

i  

 a n     
   
 a    

solving for n yields the bound on linear dimension in terms of the number of actions and
the number of observations 
a   proof of proposition   
proposition     for a given system and set of tests of interest  the linear dimension of
the corresponding prediction profile system is no greater than that of the prediction profile
system associated with any set of core tests for the system  as described in section      
proof  recall from the discussion of psrs in section     that a set of core tests  q  is
a set of tests whose corresponding columns in the system dynamics matrix constitute a
basis  the predictions for the core tests at a given history form the predictive state at that
history  so  the predictive state is precisely the prediction profile for the core tests q  the
prediction for any other test can be computed as a linear function of the prediction profile
for q  note that the prediction profile system for q is itself an mdp  it was shown in
section     how to compute the next predictive state given the current predictive state and
an action observation pair 
now consider some other set of tests of interest t i   because the predictions for q
can be used to compute the prediction for any other test  it must be that there is some
function  that maps the prediction profiles for q to the prediction profiles for t i   in
general  multiple predictive states may map to the same prediction profile for t i so  is a
surjection  now it is easy to see that the prediction profile system for t i is the result of
applying the observation abstraction  to the prediction profile system for q  performing
observation abstraction on an mdp generally produces a pomdp  but never increases the
linear dimension  talvitie         hence  the prediction profile system for any set of tests
of interest t i has linear dimension no greater than that of the prediction profile system for
any set of core tests  q 
a   proof of proposition   
proposition     consider a pomdp with hidden states s  actions a  and observations
o  let t i be the set of tests of interest  let ai be the action taken at time step i  si be the
hidden state reached after taking action ai   and oi be the observation emitted by si   now 
   

filearning to make predictions without a generative model

consider any surjection    s  s  mapping hidden states to a set of abstract states with
the following properties 
   for any pair of primitive states s    s   s  if  s       s     then for any time step i
and any test of interest t  t i   p t   si   s      p t   si   s    
   for any pair of primitive states s    s   s  if  s       s     then for any time step i 
abstract state s  s    observation o  o  and action a  a 
pr  si       s   si   s   ai     a  oi     o   
pr  si       s   si   s    ai     a  oi     o  
if such a  exists  then the prediction profile system for t i has linear dimension no greater
than the number of distinct beliefs over abstract states  s   
proof  the proof follows similar reasoning to the proof of proposition     note that  because
of property   the belief over abstract states at a given history is sufficient to compute the
prediction profile  for any history h and any test of interest t  t i  
x
x x
p t   h   
pr s   h p t   s   
pr s   h p t   s 
ss

 

x

ss 

p t   s 

x

ss  ss

pr s   h   

x

p t   s pr s   h  

ss 

ss

where the third equality follows from property    for any s  s    all hidden states s  s
have the same associated probabilities for the tests of interest 
now  consider the dynamical system with beliefs over abstract states as observations
and action observation pairs as actions  call this the abstract belief system  just as
with the predictive state  because it is possible to compute the prediction profile from
the abstract beliefs  the prediction profile model for t i can be seen as the result of an
observation aggregation of the abstract belief system  as a result  the prediction profile
system has linear dimension no greater than that of the abstract belief system 
the rest of the proof shows that  because of property    the abstract belief system is
an mdp  and therefore has linear dimension no greater than the number of distinct beliefs
over abstract states 
given the probability distribution over abstract states at a given history h  and the agent
takes an action a and observes and observation o  it is possible to compute the probability
of an abstract state s  s  at the new history 
x
x x
pr s   hao   
pr s   h pr s   s  a  o   
pr s   h pr s   s  a  o 
ss

 

x

s  s 

pr s   s    a  o 

x

s  s  ss 

pr s   h   

x

pr s   s    a  o pr s    h  

s  s 

ss 

where the third equality follows from property    for any s  s    all hidden states s  s
have the same associated conditional distribution over next abstract states  given the action
and observation 
   

fitalvitie   singh

so  because one can compute the next abstract beliefs from the previous abstract beliefs 
the abstract belief system is an mdp  and therefore has linear dimension no greater than
the number of observations  the number of distinct abstract beliefs   because one can
compute the prediction profile from the abstract beliefs  the prediction profile system can
be constructed by applying an observation abstraction to the abstract belief system  thus 
the prediction profile system has linear dimension no greater than the number of distinct
abstract beliefs 

references
baum  l  e   petrie  t   soules  g     weiss  n          a maximization technique occuring
in the statistical analysis of probabilistic functions of markov chains  the annals of
mathematical statistics                 
baxter  j     bartlett  p  l          reinforcement learning in pomdps via direct gradient ascent  in proceedings of the eighteenth international conference on machine
learning  icml   pp       
boots  b   siddiqi  s     gordon  g          closing the learning planning loop with predictive state representations  in proceedings of robotics  science and systems  zaragoza 
spain 
boots  b   siddiqi  s     gordon  g          an online spectral learning algorithm for
partially observable nonlinear dynamical systems  in proceedings of the twenty fifth
national conference on artificial intelligence  aaai  
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research 
        
bowling  m   mccracken  p   james  m   neufeld  j     wilkinson  d          learning
predictive state representations using non blind policies  in proceedings of the twentythird international conference on machine learning  icml   pp         
dinculescu  m     precup  d          approximate predictive representations of partially
observable systems  in proceedings of the twenty seventh international conference
on machine learning  icml   pp         
hansen  e          finite memory control of partially observable systems  ph d  thesis 
university of massachussetts  amherst  ma 
holmes  m     isbell  c          looping suffix tree based inference of partially observable hidden state  in proceedings of the twenty third international conference on
machine learning  icml   pp         
james  m     singh  s          learning and discovery of predictive state representations
in dynamical systems with reset  in proceedings of the twenty first international
conference on machine learning  icml   pp         
julier  s  j     uhlmann  j  k          a new extension of the kalman filter to nonlinear
systems  in proceedings of aerosense  the eleventh international symposium on
aerospace defense sensing  simulation and controls  pp         
   

filearning to make predictions without a generative model

kalman  r  e          a new approach to linear filtering and prediction problems  transactions of the asme  journal of basic engineering           
littman  m   sutton  r     singh  s          predictive representations of state  in advances
in neural information processing systems     nips   pp           
littman  m  l          algorithms for sequential decision making  ph d  thesis  brown
university  providence  ri 
mahmud  m  m  h          constructing states for reinforcement learning  in proceedings
of the twenty seventh international conference on machine learning  icml   pp 
       
mccallum  a  k          reinforcement learning with selective perception and hidden
state  ph d  thesis  rutgers university 
mccallum  r  a          overcoming incomplete perception with utile distinction memory 
in proceedings of the tenth international conference on machine learning  icml  
pp         
monahan  g  e          a survey of partially observable markov decisions processes  theory 
models  and algorithms  management science              
peters  j     schaal  s          natural actor critic  neurocomputing               
poupart  p     boutilier  c          bounded finite state controllers  in advances in neural
information processing systems     nips  
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  john wiley and sons  new york  ny 
rivest  r  l     schapire  r  e          diversity based inference of finite automata  journal
of the association for computing machinery                 
rudary  m          on predictive linear gaussian models  ph d  thesis  university of
michigan 
rudary  m   singh  s     wingate  d          predictive linear gaussian models of stochastic dynamical systems  in uncertainty in artificial intelligence  proceedings of the
twenty first conference  uai   pp         
shalizi  c  r     klinker  k  l          blind construction of optimal nonlinear recursive
predictors for discrete sequences  in proceedings of the twentieth conference on
uncertainty in artificial intelligence  uai   pp         
singh  s   james  m  r     rudary  m  r          predictive state representations  a
new theory for modeling dynamical systems  in uncertainty in artificial intelligence 
proceedings of the twentieth conference  uai   pp         
sondik  e  j          the optimal control of partially observable markov processes over the
infinite horizon  discounted costs  operations research             
soni  v     singh  s          abstraction in predictive state representations  in proceedings
of the twenty second national conference on artificial intelligence  aaai   pp     
    
   

fitalvitie   singh

sutton  r  s     tanner  b          temporal difference networks  in advances in neural
information processing systems     nips   pp           
talvitie  e          simple partial models for complex dynamical systems  ph d  thesis 
university of michigan  ann arbor  mi 
talvitie  e     singh  s       a   maintaining predictions over time without a model  in
proceedings of the twenty first international joint conference on artificial intelligence  ijcai   pp           
talvitie  e     singh  s       b   simple local models for complex dynamical systems  in
advances in neural information processing systems     nips   pp           
weaver  l     tao  n          the optimal reward baseline for gradient based reinforcement learning  in uncertainty in artificial intelligence  proceedings of the seventeenth
conference  uai   pp         
williams  r          simple statistical gradient following algorithms for connectionist reinforcement learning  machine learning            
wingate  d          exponential family predictive representations of state  ph d  thesis 
university of michigan 
wingate  d   soni  v   wolfe  b     singh  s          relational knowledge with predictive
state representations  in proceedings of the twentieth international joint conference
on artificial intelligence  ijcai   pp           
wolfe  a  p          paying attention to what matters  observation abstraction in partially
observable environments  ph d  thesis  university of massachussetts  amherst  ma 
wolfe  a  p     barto  a  g          decision tree methods for finding reusable mdp
homomorphisms  in proceedings of the twenty first national conference on artificial
intelligence  aaai  
wolfe  b   james  m     singh  s          approximate predictive state representations  in
proceedings of the seventh conference on autonomous agents and multiagent systems
 aamas  
wolfe  b   james  m  r     singh  s          learning predictive state representations in
dynamical systems without reset  in proceedings of the twenty second international
conference on machine learning  icml   pp         
wolfe  b     singh  s          predictive state representations with options  in proceedings of the twenty third international conference on machine learning  icml   pp 
         

   

fi