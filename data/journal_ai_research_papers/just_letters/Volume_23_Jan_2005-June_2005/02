journal artificial intelligence research                 

submitted       published     

reinforcement learning agents many sensors
actuators acting categorizable environments
josep porta

porta science uva nl

ias group  informatics institute
university amsterdam
kruislaan          sj  amsterdam  netherlands

enric celaya

celaya iri upc edu

institut de robotica informatica industrial
spanish council scientific research  csic 
llorens artigas             barcelona  spain

abstract
paper  confront problem applying reinforcement learning agents
perceive environment many sensors perform parallel actions using
many actuators case complex autonomous robots  argue reinforcement
learning successfully applied case strong assumptions made
characteristics environment learning performed 
relevant sensor readings motor commands readily identified  introduction
assumptions leads strongly biased learning systems eventually lose
generality traditional reinforcement learning algorithms 
line  observe that  realistic situations  reward received robot
depends reduced subset executed actions reduced subset
sensor inputs  possibly different situation action  relevant
predict reward  formalize property called categorizability assumption
present algorithm takes advantage categorizability environment 
allowing decrease learning time respect existing reinforcement learning
algorithms  results application algorithm couple simulated realisticrobotic problems  landmark based navigation six legged robot gait generation 
reported validate approach compare existing flat generalizationbased reinforcement learning approaches 

   introduction
division knowledge based behavior based artificial intelligence
fundamental achieving successful applications within field autonomous robots  arkin 
       however  now  division repercussions reinforcement learning  within artificial intelligence  reinforcement learning formalized
general way borrowing ideas dynamic programming decision theory fields 
within formalization  objective reinforcement learning methods establish
correct mapping set abstract observations  formalized states  set
high level actions  without worried sets states actions
defined  for introduction reinforcement learning check kaelbling  littman 
  moore        sutton   barto        among many others   algorithms developed within
general framework used different fields without modification 
c
    
ai access foundation  rights reserved 

fiporta   celaya

particular application  definition sets states actions responsibility
programmer supposed part reinforcement learning problem 
however  clearly pointed brooks         autonomous robots major hurdles
related perception action representations  reason  robotic
task  traditional reinforcement learning research assumes major problem
 connecting states actions  simpler assumes given  the definition
states actions   consequence existing reinforcement learning methods
best suited problems fall symbolic artificial intelligence domain
belong robotics  due generality existing reinforcement learning
algorithms  robotic problem analyzed re formulated tackled
available reinforcement learning tools but  many cases  re formulation
awkward introducing unnecessary complexity learning process  alternative
explore paper new reinforcement learning algorithm applied
robotic problems are  without re formulation 
brooks        remarked  dealing real environment necessarily problem
since real environments properties exploited reduce complexity
robots controller  brooks works  find simple robot controllers achieve
good performance particular environments  clearly contrast generality
pursued within reinforcement learning  following idea parallel brooks 
paper  present new reinforcement learning algorithm takes advantage specific
environment related property  that call categorizability  efficiently learn achieve
given task  formalize categorizability property present representation
system  partial rules  exploit property  remarkable feature representation
system allows generalization spaces sensors actions  using
uniform mechanism  ability generalize state action spaces
fundamental successfully apply reinforcement learning autonomous robots 
paper organized follows  first  section    formalize reinforcement learning point view use field autonomous robotics describe
problems make flat  and  cases  generalization based  reinforcementlearning algorithms adequate case  section   presents categorizability assumption plausible robotics environments  then  section    describe
alternative reinforcement learning algorithm exploits categorizability assumption circumvent problems present existing approaches  section    analyze
points contact proposal already existing work  next  section   
present experiments validate approach  experiments performed
simulations mimic realistic robotic applications categorizability assumption
likely valid  finally  section    conclude analyzing strengths
weaknesses proposed learning system 
additionally  appendix provides detailed description partial rule learning
algorithm introduced paper  appendix b devoted enhancement
algorithm make execution efficient  appendix c summarizes notation
use throughout paper 
  

fireinforcement learning categorizable environments

   problem formalization
simplicity  assume robot perceives environment set binary
feature detectors  f    fdi        nf    feature detector devised process
identifies specific combinations present  and possibly past  sensor readings 
use feature detectors common robotics  field  feature detectors
defined programmer attending special characteristics environment 
robot sensors  task executed order extract potentially useful information
 presence landmarks obstacles          raw sensor readings 
similar way  instead working directly space actions provided
robot motors  that define low level way controlling robot   common
practice define set elementary actions ea    eai  i      ne    elementary action
specific sequence combination motor commands defined programmer attending
characteristics robot task achieved  simplify  assume
elementary actions form  mi k   i     nm    mi motor k
value range valid inputs motor mi   framework quite flexible since
motor mi either one physical motors robot high level  abstract
motor combines movements actual motors  formalization  given
moment  robot execute parallel many elementary actions available motors 
robot controller seen procedure executes  combinations elementary 
actions response specific situations  i e   activation specific feature detectors 
objective achieving given task  reinforcement learning approaches automatically
define controller using information provided reward signal  context
reinforcement learning  controller called policy learner 
objective value function based reinforcement learning algorithms  the
common reinforcement learning algorithms  predict reward directly
indirectly obtained execution action  i e   combination elementary
actions  possible situation  described combination active inactive feature
detectors  prediction available  action executed situation
one maximum reward expected 
predict reward  classic reinforcement learning algorithms rely markov
assumption  requires state signal carry enough information determine effects
actions given situation   additionally  non generalizing reinforcement learning
algorithms assume states system must learned independently  so 
information gathered effects action given state s  denoted q s  a  
cannot safely transferred similar states actions  assumption  cost
reinforcement learning algorithm general problem
 ns na   
ns number states na number actions 
action tried least state  since state defined observed
   non binary feature detectors providing discrete range values readily binarized 
   non markovian problems  confronted  converted markovian ones 
scope paper  although one relevant points achieve successful
real world reinforcement learning application 

  

fiporta   celaya

combination feature detectors  potential number states
ns     nf  
nf number feature detectors  consequently 
 ns na       nf na   
exponential number feature detectors  since number feature detectors used robotic applications tends high  non generalizing reinforcement learning
becomes impractical realistic problems  well known curse dimensionality
introduced bellman         whose research presaged work reinforcement
learning 
although size action set  na   important size state set  ns  
curse dimensionality  less attention paid actions reinforcement learning
literature  however  robot many degrees freedom execute many elementary
actions simultaneously makes cost learning algorithms increase
exponentially number motors robot  nm   
suppose address task two different sets feature detectors f  
f d  f d  f d    using plain reinforcement learning algorithm  cost
finding proper policy would larger using larger set features  f     
even one features f d  f d  stronger correlation reward
features f d    non generalizing reinforcement learning algorithms
able take advantage situation  and  even better input information 
performance decreases  similar argument made actions addition
feature detectors 
generalizing reinforcement learning algorithms using gradient descent
techniques  widrow   hoff         coarse codings  hinton  mcclelland    rumelhart 
       radial basis functions  poggio   girosi         tile coding  sutton        decision
trees  chapman   kaelbling        mccallum        partially palliate problem
since deal large state spaces  however  approach complex realistic
problems  number dimensions state space grows point making use
generalization techniques impractical function approximation
techniques must used  sutton   barto        page      
adding relevant inputs actions task make task easier least
difficult  methods whose complexity depends relevance available inputs actions number would scale well real domain problems 
examples systems fulfilling property are  instance  kanerva coding system presented kanerva        random representation method sutton whitehead
        systems rely large collections fixed prototypes  i e   combinations
feature detectors  selected random  proposal search appropriate prototypes  using strong bias search performed reasonable time 
strong bias based categorizability assumption plausible assumption
case autonomous robots  allows large speed learning process 
additionally  existing systems address problem determining relevance
actions  since assume learning agent single actuator  that is  obviously 
  

fireinforcement learning categorizable environments

relevant one   simple set adequate robotics  approach  presented below   combinations feature detectors elementary actions considered
using unified framework 

   categorizability assumption
experience developing controllers autonomous robots  observe that  many
realistic situations  reward received robot depends reduced subset
actions executed robot sensor inputs irrelevant
predict reward  thus  example  value resulting action grasping
object front robot depend object is  object robot
bring user  electrified cable  unimportant object  however  result
probably whether robot moving cameras grasping
object  day night  robot is  time  checking distance
nearest wall  see red light nearby  aspects  them  may
become important circumstances  
agent observes acts environment reduced fraction available inputs actuators considered time  say agent
categorizable environment 
categorizability binary predicate graded property  completely
categorizable case  would necessary pay attention one sensor motor
situation  extreme spectrum  motors carefully
coordinated achieve task effect action could predicted
taking account value feature detectors  would say environment
categorizable all 
since robots large collection sensors providing heterogeneous collection
inputs many actuators affecting quite different degrees freedom  hypothesis
that  robotic problems  environments highly categorizable and  cases 
algorithm biased categorizability assumption would result advantageous 

   reinforcement learning categorizable environments  partial
rule approach
implement algorithm able exploit potential categorizability environment 
need representation system able transfer information similar situations
similar actions 
clustering techniques successive subdivisions state space  as  instance 
presented mccallum        focus perception side problem aim
determining reward expected given state considering
feature detectors perceived state  subset relevant feature detectors
used compute expected reward state possible action  the q s  a 
function   however  way posing problem curse dimensionality problem
completely avoided since features relevant one action
another produces unnecessary  from point view action 
differentiation equivalent situations  decreasing learning speed  problem
  

fiporta   celaya

avoided finding specific set relevant feature detectors action 
case  q function computed q fs  a   a   state definition function
action consideration  technique used  instance  mahadevan
connell         unfortunately  problem confronting  enough since 
case  actions composed combinations elementary actions want
transfer reward information similar combinations actions  therefore 
estimate q fs  a   a  taking account elementary actions compose
a  however  principle  relevance elementary actions function situation  or 
equivalently  state   given elementary action relevant situations
others  reason  function approximate becomes q f  a   fa  s  
cross dependency state defined function action  f  a  
action defined function state  fa  s   proposal detail next solves
cross dependency working cartesian product spaces feature detectors
elementary actions combinations 
formalize proposal  introduce definitions 
say agent perceives  or observes  partial view order k  v fd i            fdik   
k nf whenever predicate fdi      fdik holds   obviously  many partial views
perceived time 
given moment  agent executes action issues different command
one agents motors    ea            eanm    nm number motors 
partial command order k  noted c eai            eaik    k nm   executed whenever
elementary actions  eai            eaik   executed simultaneously  say partial
command c action accordance c subset a  note execution
given action supposes execution partial commands accordance
it 
partial rule w defined pair w    v  c   v partial view c
partial command  say partial rule w    v  c  active v observed  w
used whenever partial view v perceived partial command c executed 
partial rule covers sub area cartesian product feature detectors elementary
actions and  thus  defines situation action rule used partially determine
actions robot many situations  all partial view rule
active   order partial rule defined sum order partial view
order partial command compose rule 
associate quantiy qw partial rule  qw estimation value  i e  
discounted cumulative reward  obtained executing c v observed
time t 

x
qw  
t i rt i  
i  

rt i reward received learner time step   rule w used time
t  so  partial rule interpreted as  partial view v observed execution
partial command c results value qw  
   partial view include negations feature detectors since non detection feature
relevant detection 

  

fireinforcement learning categorizable environments

objective learning process deriving set partial rules adjusting
corresponding qw values desired task properly achieved 
apparent drawback partial rule representation number possible
partial rules much larger number state action pairs  number
partial rules defined set nf binary feature detectors nm binary
motors  nf  nm   number different states action pairs  nf  nm  
arbitrary problems confronted  as case synthetic learning situations  
partial rule approach could useful  however  problems confronted robots
arbitrary since  mentioned  environments present regularities properties  as
categorizability  exploited reduce complexity controller necessary
achieve given task 
using partial rule framework  categorizability assumption formally defined
as 
definition   say environment task highly categorizable exists set
low order partial rules allows us predict reward accuracy
statistics possible state action combination considered  lower order
rules controller higher categorizability environment task 
extent categorizability assumption fulfilled  number partial rules
necessary control robot becomes much smaller number state action pairs
defined using sets feature detectors elementary actions
partial views partial commands based  additionally  categorizability implies
rules necessary controller mostly lower order
easily exploited bias search space partial rules  so  environment
categorizable  use partial rule approach suppose important increase
learning speed reduction use memory respect traditional
non generalizing reinforcement learning algorithms 
following sections  describe possible estimate effect
action given fixed set partial rules  evaluation  repeated actions  used
determine best action executed given moment  next  detail
possible adjust value predictions fixed set partial rules  finally  describe
categorizability assumption allows us use incremental strategy generation
new partial rules  strategy results faster learning existing generalizing
non generalizing reinforcement learning algorithms  procedures described highlevel form make explanation clear  details implementation found
appendix a 
    value prediction using partial rules
given situation  many partial views simultaneously active triggering subset
partial rules controller c  call subset active partial rules denote
c     evaluate given action take account rules c  
partial command accordance a  denote subset c    a   note that 
approach  refer action  mean corresponding set elementary actions
 one per motor  single element  general case reinforcement learning 
  

fiporta   celaya

every rule w    v  c  c    a  provides value prediction a  qw associated
partial rule  averaged value provides information accuracy
prediction  pointed wilson         favor use partial
rules high accuracy value prediction or  say it  rules high relevance 
seems clear relevance rule  w   depends distribution values
around qw   distributions low dispersion indicative coherent value predictions
and  so  highly relevant rule  measure dispersion maintain error estimation
ew approximation qw   another factor  not used wilson        taken
account relevance determination confidence qw ew statistics  low
confidence  i e   insufficiently sampled  measures qw ew reduce relevance
rule  confidence value prediction given rule  cw   number
interval         initialized    increasing partial rule used  i e   rule
active partial command executed   confidence would decrease
value model given partial rule consistently wrong 
using confidence  approximate real error value prediction partial
rule w
w   ew cw   e    cw   
value e average error value prediction  observe importance
e reduced confidence increases and  consequently  w converges ew  
definitions  relevance partial rule defined
w  

 
 
    w

note exact formula relevance important far w  w 
w  w    formula provides value range        could directly
used scale factor  necessary 
problem then  derive single value prediction using qw statistics
rules c    a  corresponding relevance value  w   two possible solutions
come mind  using weighted sum values predicted partial rules using
relevance weighting factor  using competitive approach 
relevant partial rule used determine predicted value  weighted sum assumes
linear relation inputs  the value prediction provided individual rule 
output  the value prediction a   assumption proved powerful many
systems but  general  compatible categorizability assumption since 
although one partial rules involved sum low order  taking
account means using large set different feature detectors elementary
actions predict effect given action  reason  learning system uses
winner take all solution value prediction relevant partial rule
taken account predict value action  so  action determine
winner rule
w  winner  c     a   arg

max  w    

w  c    a 

use range likely value rule  iw    qw  w   qw    w    randomly
determine value prediction action a  probability distribution inside interval
depends distribution assume value 
  

fireinforcement learning categorizable environments

procedure outlined used time step obtain value prediction
action  action maximal value one want robot execute
next 
observe obtain probabilistic value prediction  situation
statistics  get different value predictions action  way 
action obtains maximal evaluation always one maximal q w
and  consequently  favor exploration promising actions  probabilistic action selection provides exploratory mechanism uses information typical
reinforcement learning exploration mechanisms  the error confidence value predictions available reinforcement learning algorithms  result
sophisticated exploration schema  see wilson        survey different exploration
mechanisms reinforcement learning  
    partial rules value adjustment
adjust value predictions rules c    a  last executed action 
rule adjusted  update qw   ew   cw statistics 
effect action accordance partial command c attending
partial rule w    v  c  defined  using bellman like equation 

qw
  rw  

x

p w  c     v  c     

c  

r w average reward obtained immediately executing c v observed 
discount factor used balance importance immediate respect delayed
reward  v  c     represents goodness  or value  situation rules c   active 
p w  c     probability reaching situation execution c v
observed  value situation assessed using best action executable
situation

v  c       max
 qw
 w   winner c     a     
 


since gives us information well robot perform  at most 
situation 
many existing reinforcement learning approaches  values q w ew
rules adjusted modified using temporal difference rule
error measure  rules direct relation
progressively approach qw
received reward would provide value prediction  qw   coherent actually
obtained one and  consequently  statistics adjustment  prediction error
decreased  contrariwise  rules related observed reward would predict value
different obtained one error statistics increased  way 
rule really important generation received reward  relevance increased
decreased  rules low relevance chances used drive
robot and  extreme cases  could removed controller 
confidence cw adjusted  adjustment depends confidence measured  related number samples used qw ew
statistics  cw simply slightly incremented every time statistics rule w
  

fiporta   celaya

updated  however  decrease confidence value model given partial
rule consistently wrong  i e   value observed systematically interval w   
observe learning rule equivalent used state based reinforcementlearning methods  instance  q learning  watkins   dayan         q  s  a  
state action  defined
x
p s  a  s    v  s    
q  s  a    r w  
s 

p s  a  s    probability transition s  executed
v  s      max
 q  s    a    
 


approach  set rules active given situation c   plays role state
instead
and  thus  v  c     v  s    equivalent  hand  estimate qw

q  s  a   rule w includes information  partial  state actions
q  s  a  play similar role  value prediction given rule  q  
making qw
w
corresponds average value predictions cells cartesian product
feature detectors elementary actions covered rule  case complete
rules  i e   rules involving feature detectors actions motors   sub area
covered rule includes one cell cartesian product and  therefore 
controller includes complete rules  described learning rule exactly
used q learning  particular case  c    a  one rule that  consequently 
winner rule  statistics rule  and updated
way  q s  a  entry table used q learning  thus  learning rule
generalization learning rule normally used reinforcement learning 
    controller initialization partial rule creation elimination
since assume working categorizable environment  use incremental
strategy learn adequate set partial rules  initialize controller rules
lowest order generate new partial rules necessary  i e   cases
correctly categorized using available set rules   so  initial controller contain 
instance  rules order two include one feature detector one elementary
action   v fdi    c aej      v fdi    c aej    i  j   case  sensible include
empty rule  the rule order    w   initial controller  rule always active
provides average value average error value prediction  additionally 
knowledge user task achieved easily introduced initial
controller form partial rules  available  estimation value predictions
user defined rules included  hand crafted rules  and value
predictions  correct learning process accelerated  correct 
learning algorithm would take care correcting them 
create new rule large error value prediction detected  new
rule defined combination two rules c    a   rules forecast
effects last executed action  a  current situation  selecting couple
rules combined  favor selection value prediction close
  

fireinforcement learning categorizable environments

actually observed one  since likely involve features elementary actions
 partially  relevant value prediction try refine 
problem possible determine priori whether incorrectly
predicted value would correctly predicted rule adjustments really
necessary create new partial rule account received reward  so  create new
rules large error value prediction  possible create unnecessary
rules  existence  almost  redundant rules necessarily negative  since
provide robustness controller  called degeneracy effect introduced edelman
        must avoided generate rule twice  since useful
all  two rules identical respect lexicographic criteria  they contain
feature detectors elementary actions  respect semantic ones  they
get active situations propose equivalent actions   identical rules
created  detected removed soon possible  preserving
rules proved useful avoids number rules controller growing
reasonable limit 
since create new rules significant error value prediction 
necessary  could end generating complete rules  provided limit
number rules controller   case  assuming specific
rule accurate value prediction  system would behave normal tablebased reinforcement learning algorithm  specific rules  i e   relevant
ones  would used evaluate actions and  explained before  statistics
rules would exactly table based reinforcement learning algorithms 
thus  limit  system deal type problems non generalizing
reinforcement learning algorithms  however  regard limit situation improbable impose limits number rules controllers  observe
asymptotic convergence table based reinforcement learning possible
use winner takes all strategy action evaluation  weighted sum strategy 
value estimation non complete rules possibly present controller would
added complete rules leading action evaluation different
table based reinforcement learning algorithms 

   partial rule approach context
categorizability assumption closely related complexity theory principles
minimum description length  mdl  used authors schmidhuber        bias learning algorithms  complexity results try formalize
well known occams razor principle enforces choosing simplest model
set otherwise equivalent models 
boutilier  dean  hanks        presents good review representation methods
reduce computational complexity planning algorithms exploiting particular
characteristics given environment  representation based partial rules seen
another representation systems  however  partial rule representation
formalism that  without bias introduced categorizability assumption  would
efficient enough applied realistic applications 
  

fiporta   celaya

partial rule formalism seen generalization xcs classifier
systems described wilson         xcs learning system aims determining set
classifiers  that combinations features associated action  associated value relevance predictions  main difference approach
wilsons work pursues generic learner bias learning process using
categorizability assumption  allows us use incremental rule generation strategy
likely efficient robotic problems  additionally  categorizability assumption modifies way value given action evaluated  wilsons
approach uses weighted sum predictions classifier advocating action
determine expected effect action  while  fulfill categorizability assumption  i e   minimize number feature detectors elementary actions involved
given evaluation   propose use winner takes all strategy  critical point
since winner takes all strategy takes full advantage categorizability assumption
allows partial rule system asymptotically converge table based
reinforcement learning system  case weighted sum strategy used 
furthermore  xcs formalism generalization action space and 
already commented  requirement robotic like applications 
general  reinforcement learning pay attention necessity generalizing
space actions  although exceptions exists  instance  work maes
brooks        includes possible execution elementary actions parallel  however
system include mechanism detecting interactions actions and 
thus  coordination actions relies sensory conditions  instance  system
difficulties detecting execution two actions results always  i e   independently
active inactive feature detectors  positive negative reward 
cascade algorithm kaelbling        learns bit complex action
separately  algorithm presents clear sequential structure learning
given action bit depends previously learned ones  approach
predefined order learning outputs result flexible learning
schema 
multiagent learning  claus   boutilier        sen        tan        objective
learn optimal behavior group agents trying cooperatively solve given
task  thus  field  case  multiple actions issued parallel considered  however  one main issues multiagent learning  coordination
different learners irrelevant case since one learner 
finally  way define complex actions elementary actions
points common works reinforcement learning macro actions defined
learner confronts different tasks  sutton  precup    singh        drummond        
however  useful combinations elementary actions detected algorithm
guaranteed relevant task hand  although likely relevant
related tasks  

   experiments
show results applying learning algorithm two robotics like simulated problems  robot landmark based navigation legged robot walking  first problem
  

fireinforcement learning categorizable environments

flowers

bushes

boat

tree

lake

goal





a 
a 



a 






a 



ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff








































































fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
a 
fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi
a 


a 
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi








fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi




rock

bushes

start

north

bush

figure    landscape simple landmark based navigation task  landscape divided areas  the dashed ovals  subsets landmarks visible 

simpler  although includes delayed reward  use clearly describe
workings algorithm  second problem approaches realistic robotic application 
objective long term  use two examples compare performance
learning system generalizing non generalizing reinforcement learning
algorithms  confronted problems different enough show generality
proposed learning system 
    simulated landmark based navigation
confront simple simulated landmark based navigation task forest like environment shown figure    objective learner go start position
 marked cross bottom figure  goal position
food  marked cross top right corner environment   agent
neither walk lake escape depicted terrain 
agent make use binary landmark  i e   feature  detectors identify
position environment decide action execute next  example 
landmark detectors agent are 
   rock detector  active rock seen 
   boat detector  active boat seen 
   flower detector  active bunch flowers seen 
  

fiporta   celaya

   tree detector  active tree seen 
   bush detector  active whenever bush seen 
   water detector  active water nearby 
   bird detector  active bird flying agent 
   cow detector  active cow nearby 
   sun detector  active sun shining 
    cloud detector  active cloudy 
detectors  first   relevant task  water detector always
active  rest landmark detectors become active random     landmark
detectors differentiate            situations 
simplify problem clustering possible positions learner environment   areas  shown figure     area includes positions
set relevant landmarks seen 
far actions concerned  use three actions west east movement
robot  move west  denoted w    stay place     move east  e  
three indicate movement along north south dimension  move north
n   stay latitude   move south s   two independent groups
three actions combined giving rise   different actions  move north west  north 
north east  etc    assume agent executes one actions 
stop nearest area terrain direction movement reached 
agent tries move lake terrain  remains
position was  figure   shows possible transitions contiguous areas
environment 
described landmark detectors elementary actions maximum possible order given rule     define                  syntactically different
partial rules  taking account rules one feature detector one elementary action  that ones initially included controller     different
partial rules 
agent receives reward  with value      reaches goal  consequently 
problem delayed reward since agent must transmit information provided reward signal actions situations directly related
observation reward 
parameters partial rule learning algorithm used task       
                               and          see appendix detailed
description parameters   observe that  maximum number partial rules
      initial controller containing    rules  little room left generation
rules order higher   
learning organized sequence trials  trial consists placing
learner starting position letting move goal reached  allowing
execution     actions reach goal  performing optimally  three
actions required reach objective starting position 
  

fireinforcement learning categorizable environments

   
   

steps goal

   
   
   
  
  
  
  
 

 

  

   

   

   

   

trial
pr algorithm

xcs

figure    performance landmark based navigation task  results shown average    runs 

figure   shows that     learning trials  agent approaches optimal behavior
 represented flat dashed line      
dashed line figure   performance xcs problem  perform
test  used implementation wilsons xcs developed butz        
make xcs work search space partial rule algorithm  modified
xcs implementation able deal non binary actions  modification 
parameter adjustment  introduced original code  results presented
corresponds average    runs using set parameters gave better
result  nominally  parameters were  learning rate        decay rate       
maximum number classifiers        however  initial set empty   genetic
algorithm applied average every   time steps  deletion experience    subsume
experience     fall rate      minimum error       prediction threshold
     crossover probability      mutation probability      initial dont
care probability      prediction fitness new classifiers initialized   
error    detailed explanation meaning parameters provided
wilson        comments code butz        
see xcs reaches performance partial rule approach 
using four times trials  difference performance partially explained
xcss lack generalization action space  however factor relevant
case since action space two dimensions  main factor explains
better performance partial rule approach bias introduced categorizability
  

fiporta   celaya


 

position
a 

v
  

 

a 

  

 

a 

   

action
 w  n  
 w   
   n  
 e  n  
 e   
 e   

winner rule
w     v rock  boat   c w  n   
w     v rock  w ater   c w   
w     v boat  ree   c n   
w     v t ree   c e  n   
w     v rock  boat   c e  
w     v bush   c e    

qw
     
     
     
    
     
     

ew
    
    
    
   
    
   

guess
     
     
     
     
     
     

table    partial execution trace landmark based navigation task  elementary action means movement along corresponding dimension  time step
t  action highest guess executed  time step    goal
reached 

assumption present xcs system that  case  allows
efficient learning process  xcs powerful partial rule approach sense
xcs makes assumption categorizability environment 
assume high  result xcs learning process includes identification
degree categorizability environment case is  sense 
pre defined  generality xcs  however  produces slower learning process 
initialize classifiers xcs high dont care probability initialize
rules partial rule algorithm generalization used action space
 i e   rules include command motor   two systems become closer 
case  main  but only  difference two approaches
assumption relation inputs value  xcs assumes linear
relation  assume environment categorizable  or  same  assume
value depend inputs  due difference  confronted
problem  two systems would learn policy values
action  values would computed using different rules different associated
values  independently parameter rule initialization used case 
system smaller learning time would assumption closer
reality  results obtained particular example presented show
categorizability assumption valid hypothesis would case
robotics like applications 
table   shows evaluation actions different situations agent encounters path start goal    learning trials  analyzing trace 
extract insight partial rule learning algorithm works 
instance  time step    see rule w     v rock  w ater   c w    used
determine value action  w     since landmark detector water always active 
rule equivalent w    v rock   c w     one rules used generate w    
examine statistics w find qw         ew          obviously 
value distributions qw qw  look different        vs              vs        
w  generated later stages learning and  thus  statistics
updated using subsample values used adjusts statistics w 
  

fireinforcement learning categorizable environments

particular case  qw updated     times qw  updated   
times  learning continues  distributions become similar rule w  
eventually eliminated 
table    see sometimes non optimal actions get
evaluation close optimal ones  reason  agent executes  times 
non optimal actions increases number steps necessary reach goal 
general  adjustment statistics rules solve problem but 
particular case  need create new rules fix situation  instance  time step   
value rule w  increased towards     value rules active time step
proposing actions accordance action rule w  converge toward    
so  long term  rule proposing action  n   get value close    
absence specific rules  rule used estimate value action
   n   and  due probabilistic nature action selection procedure 
action can  eventually  executed delaying agent reaching goal   time
step  however  execution    n   results error value prediction and  thus 
creation new rules better characterize situation  soon specific rule
action    n   generated  error longer repeated 
time step    see rule w     v bush   c e     value     error
  guess rule        maximum confidence    lower
          case  makes agent keep always certain degree
exploration 
agent receives reward task totally achieved  function value
situation computed v  s    n  r n distance  in actions 
situation target one r reward finally obtained  table    see
situations get correct evaluation                          a                
a       a  
observe problem solved using     partial rules     
possible situation action combinations domain  so  say problem
certainly categorizable  main conclusion extract toy example
that  particular case confronted problem categorizable  presented
algorithm able determine relevant rules adjust values  including
effect delayed reward  optimal action determined
situation 
    gait generation six legged robot
applied algorithm task learning generate appropriate gait  i e  
sequence steps  six legged robot  figure     apply learning algorithm
real robot would possible  dangerous  initial phases learning robot
would fall many times damaging motors  reason used simulator
learning and  afterward  applied learned policy real robot 
problem learning walk six legged robot chosen many authors
paradigmatic robotic learning problem  instance  maes brooks       
implemented specific method based immediate reward derive preconditions
leg perform step  pendrith ryan        used simplified version
  

fiporta   celaya

six legged walking problem test algorithm able deal non markovian spaces
states kirchner        presented hierarchical version q learning learn
low level movements leg  well coordination scheme low level
learned behaviors  ilg  muhlfriedel  berns        introduced learning architecture
based self organizing neural networks  kodjabachia meyer        proposed
evolutionary strategy develop neural network control gait robot  vallejo
ramos        used parallel genetic algorithm architecture parker        described
evolutionary computation robot executes best controller found
given moment new optimal controller computed off line simulation 
algorithms usually tested flat terrain aim generating periodic gaits  i e  
gaits sequence steps repeated cyclically   however  general locomotion
 turns  irregular terrain  etc  problem free gait generation needs considered 

figure    genghis ii walking robot  d simulation environment 
simulator  see figure    allows controller command leg robot
two independent degrees freedom  horizontal vertical  able detect
robot unstable position  in robot happens two neighboring legs
air simultaneously   using simulator  implemented behaviors described
celaya porta        except charge gait generation  therefore 
task learned consists deciding every moment legs must step  that is  leave
ground move advanced position   must descend stay
ground support propel body 
defined set    feature detectors that  due experience legged robots 
knew could useful different situations gait generation task 
air x   active leg x air 
advanced x   active leg x advanced neighboring leg clockwise
circuit around robot 
attending activation non activation    feature detectors 
differentiate      different situations 
action side  work two different elementary actions per leg  one
issues step leg another descends leg touches ground 
  

fireinforcement learning categorizable environments

thus  cardinality set elementary actions    and  time step  robot
issues action containing   elementary elements  one per leg   thus  think
leg virtual motor accepts two possible values    remain contact
ground   perform step 
reward signal includes two aspects 
stability  action causes robot fall down  reward    given 
efficiency  robot fall down  reward equal distance
advanced robot given  observe legs descend recover contact
ground advance robot obtained movement necessary
able get reward next time steps  so  problem delayed
reward 
efficient stable gait tripod gait two sets three non adjacent
legs step alternately  using gait  robot would obtain reward    when one group
three legs lifted advanced  followed reward     when legs contact
ground move backward reaction advance legs moved previous
time step   thus  optimal average reward    
experiments  robot set initial posture legs contact
ground random advance position 
figure   shows results applying partial rule algorithm compared obtained using standard q learning      distinct states    different actions 
partial rule algorithm  used following set parameters          
                                  and          see appendix description
parameters   q learning  learning rate set       use
action selection rule performs exploratory actions probability     
figure    see stability subproblem  i e   falling down 
corresponds getting reward greater zero  learned quickly  because 
stability subproblem  take advantage generalization provided using
separate elementary actions and  single rule  avoid executing several dangerous
actions  however  advance subproblem  i e   getting reward close     learned
slowly  little generalization possible learning system must generate
specific rules  words  sub problem less categorizable stability
one 
landmark based navigation example discussed previous section 
observe controller contains  slightly  overly general rules responsible
non optimal performance robot  however  dont regard problem
since interested efficiently learning correct enough policy
frequent situations finding optimal behaviors particular cases 
figure   shows performance q learning longer run using different exploration rates  shows q learning eventually converge optimal policy
many iterations approach  about factor      observe lower
exploration rate allows algorithm achieve higher performance  around   
learning rate     around    learning rate       using longer period 
careful adjustment exploration rate combine initial faster learning
  

fiporta   celaya

  

average reward

  
 
   
   
   
   
   

 

    

    
    
time slice

pr algorithm

    

    

qlearning

figure    performance partial rule approach compared standard q learning 
results smoothed average    experiments 

better convergence long term  experiments q learning using learning rates
    showed insignificant differences compared results shown here 
advantage algorithm non generalizing ones increased problems
sensors provide information related task  test point 
set experiment   feature detectors become active randomly
added    initial ones  new features  number possible combinations
feature activations increases  number states considered q learning 
figure   shows comparison algorithm q learning problem 
q learning able learn reasonable gait strategy      time steps shown
figure  performance partial rule algorithm almost
before  means partial rule algorithm able detect sets features
relevant use effectively determine robots behavior  remarkable
that  case  ratio memory used algorithm respect used
non generalizing algorithms       exemplifies performance
non generalizing algorithms degrades number features increases 
necessarily case using partial rule approach 
importance generation partial rules improvement categorization seen comparing results obtained problem without
mechanism  figure     results show task cannot learned using
partial rules order    aspect gait generation problem learned
rules order   avoid lifting leg one neighboring legs already
  

fireinforcement learning categorizable environments

  

average reward

  
 
   
   
   
   
   

 

     

exploration    

      
      
time slice

      

exploration     

      

references

figure    performance q learning algorithm different exploration rates 
reference values       upper bound performance attainable
using exploration rate          

  

average reward

  
 
   
   
   
   
   

 

    

    
    
time slice

pr algorithm

    

    

qlearning

figure    performance algorithm compared q learning irrelevant
features 

  

fiporta   celaya

  

average reward

  
 
   
   
   
   
   

 

    

    
    
time slice

without generation

    

    

generation

figure    performance without partial rule generation procedure 
air  instance  rule
v in air     c step     
forecasts highly relevant negative reward prevents leg   raised
leg   air 
rules order higher    i e   provided robot initial controller 
necessary  instance  avoid raising two neighboring legs simultaneously  rule
v in air     c step     step    
becomes active robot evaluates action implies raising leg   leg  
time  since value prediction rule negative relevance
high  action evaluation would discarded  preventing robot falling
down  similar rules generated pair neighboring legs  make
robot advance  need generate rules even higher order 
figure    see performance algorithm start learning
process correct rule set  i e   rule set learned previous experiment  
statistics initialized    experiment  compare complexity
learning values rules compared complexity learning rules
value time  see values rules need
learned learning process two times faster normal application
algorithm 
final experiment  issue frequent changes heading direction robot
 generated randomly every    time steps   way  periodic gaits become suboptimal
   

fireinforcement learning categorizable environments

  

average reward

  
 
   
   
   
   
   

 

   

    

    

    
    
time slice

pr correct rule set

    

    

    

pr algorithm

figure    performance partial rule approach learning started correct
rule set compared standard approach rules learned 

controller produce free gait  i e   gait includes sequence steps
without periodic repetition 
case  focus advance subproblem and  thus  introduced handcrafted rules initial controller prevent robot falling down  rules
form 
leg lifted execution action results value    confidence   
actions lift one two legs contiguous i 
set parameters used case was                            
               and         
figure   shows average results obtained using partial rule learning algorithm
compared obtained best hand coded gait generation strategy  figure  horizontal dashed line shows average performance using best gait generation
strategy implemented  celaya   porta         seen learned gaitgeneration strategy  the increasing continuous line  produces performance similar
best hand coded strategy that  cases  even outperforms it  figure   
shows situation learned controller produces better behavior hand
coded one  using hand coded strategy  robot starts walk raising two legs   
   and  time steps reaches state tripod gait generated  initially 
leg   advanced legs     and  general  suboptimal execute step
leg neighboring legs less advances itself  particular case
however  general rule hold  learned strategy detects exception
   

fiporta   celaya

  

average reward

  
  
  
 
 

 

    

    
    
time slice

pr algorithm

    

    

hand coded

figure    performance partial rule approach learning free gait 
generates tripod gait beginning resulting larger advance robot
initial stages movement 

   conclusions
paper  introduced categorizability assumption states robot
driven achieve given task using simple rules  i e   rules including reduced
set feature detectors elementary actions  assumption supported
experience within behavior based approach controllers formed sets rules
relatively simple conditions actions  shown learning algorithm
based categorizability assumption allows large speed learning process
many realistic robotic applications respect existing algorithms 
exploit categorizability assumption observations action spaces 
introduced new representation formalism based concept partial rules
concepts independent states independent actions kernel
many existing reinforcement learning approaches 
introduction partial rule concept provides large flexibility problems
formalized  structure algorithms  confront problems
generalization perception side  usually considered reinforcement learning  
action side  usually considered   them 
generalization possible via partial rules  use complete rules 
rules involving available inputs outputs  case  partial rule approach
equivalent non generalizing reinforcement learning  algorithm presented
   

fireinforcement learning categorizable environments

leg numbering
 
 

 

 

step    

step      

  

  

 

 

 

 

step      

step      

  

  

step      

step      

   

   

figure     hand programmed gait strategy  top sequence  vs  learned one  bottom
sequence   advance position robot snapshot indicated
picture 

can  necessary  generate complete rules and  consequently  can  principle  solve
problem solved using traditional reinforcement learning algorithm  however 
take categorizability assumption valid so  generation complete rules
extreme case likely occur limit situation  therefore 
approach  forego generality order increase efficiently learning process
class problems want address 
another advantage partial rule framework allows easy robust
introduction initial knowledge learning process form rules easily understood programmer  contrast usual reinforcement learning
algorithms introduction initial knowledge is  general  rather difficult 
partial rule approach  subtle change emphasis main goal
learning  work reinforcement learning emphasis learning
value action state  main purpose learn relevance  subsets of 
elementary actions feature detectors  relevant subsets elementary actions
feature detectors identified  learning becomes straightforward 
   

fiporta   celaya

main limitation work possible know priori  except trivial cases  whether environment categorizable given robot  non generalizing
reinforcement learning implicitly assumes environment non categorizable
that  consequently  possible combination features actions taken
account separately  approach assumes opposite  environment
categorizable and  so  reduced combinations features actions need taken
account  drawback using non generalizing approach robotic tasks
become intractable curse dimensionality  generalization techniques
problem partially alleviated  enough general  approach take
radical approach order much less affected curse dimensionality 
introduce strong bias learning process drastically limit use combinations
features actions 
tested partial rule learning algorithm many robotic inspired problems
two discussed paper  landmark based navigation sixlegged robot gait generation  categorizability assumption proved valid
cases tested  algorithm out performs generalizing non generalizing reinforcementlearning algorithms memory requirements convergence time  additionally 
shown approach scales well number inputs increases 
performance existing algorithms largely degraded  important result
lets us think could possible use approach control complex robots 
use existing approaches discarded 
work presented paper  extract two main proposals  first 
apply reinforcement learning agents many sensors actuators 
concentrate efforts determining relevance inputs outputs and  second 
achieve efficient learning complex environments could necessary introduce
additional assumptions reinforcement learning algorithms  even risk losing
generality 

acknowledgments
authors would express gratitude anonymous reviewers paper 
contributions toward improving quality paper relevant enough
considered  sense  co authors paper  shortcomings still paper
attributed nominal authors 
second author partially supported spanish ministerio de ciencia tecnologa feder funds  project dpi           c      plan
nacional de i d i 

   

fireinforcement learning categorizable environments

appendix a  partial rule learning algorithm
appendix  describe detail approach described main body
paper 

partial rule learning algorithm
 initialize 
f set features detectors
ea set elementary actions
c  w     v fd   c ea     v fd   c ea   fd f d  ea ea 
w c
qw  
ew  
iw  
endfor
e 
episode
c    w c w active 
repeat  for step episode  
 action selection 
action evaluation
 computes guess a    a   
 
arg max
 guess a   
 


execute
 system update 
ra reward generated
 
cant
c 
 
c  w c w active 
statistics update
partial rule management
terminal situation
enddo

figure     partial rule learning algorithm  text inside parentheses comments 
action evaluation  statistics update  partial rule management procedures
described next 

partial rule learning algorithm  whose top level form shown figure     stores
following information partial rule
value  i e   discounted cumulative reward  estimation qw  
error estimation ew  
confidence index iw  
   

fiporta   celaya

   
   





   
   

cw

   
   
   
   
   
   
 

 

 

 

 

iw

 

 



 

 

  

figure     confidence function         
estimate confidence qw ew use confidence index iw that  roughly
speaking  keeps track number times partial rule used  confidence
derived iw using confidence function following way 
cw  confidence function iw   
confidence function non decreasing function range       
less   since  way  system always keeps certain degree exploration and 
consequently  able adapt changes environment  different confidence schemes
implemented changing confidence function  implementation  use
sigmoid like function  see figure     increases slowly low values w reducing
confidence provided first obtained rewards  way avoid premature
increase confidence  and  thus  decrease error exploration 
insufficiently sampled rules  parameter    determines point function
reaches top value  
additionally  confidence index used define learning rate  i e   weight
new observed rewards statistics update   purpose implement mam
function  venturini        rule 
mw   max      iw       
using mam based updating rule  that  lower confidence  higher
effect last observed rewards statistics  faster adaptation
statistics  adaptive learning rate strategy related presented sutton       
kaelbling         contrasts traditional reinforcement learning algorithms
constant learning rate used 
initialization phase  algorithm enters continuous loop task
episode consisting estimating possible effects actions  executing promis   

fireinforcement learning categorizable environments

action evaluation
action a 
w winner c     a   
guess a    qw     random w   w  
endfor

figure     action evaluation procedure 
ing one  updating system performance improves future  system
update includes statistics update partial rule management 
action evaluation
simplest procedure get estimated value actions brute force approach
consisting independent evaluation one them  simple cases  approach
would enough but  number valid combinations elementary actions  i e  
actions  large  separate evaluation action would take long time  increasing
time robot decision decreasing reactivity control  avoid this 
appendix b presents efficient procedure get value action 
figure    summarizes action evaluation procedure using partial rules  value
action guessed using relevant rule action  i e   winner rule  
winner rule computed
winner  c     a   arg

max  w   

wc    a 

w relevance rule w
w  

 
 
    w

value estimation using winner rule selected random  uniformly 
interval
iw    qw  w   qw    w   

w   ew cw   e    cw   
here  e average error value prediction  i e   value error prediction
empty rule  w   
statistics update
statistics update procedure  figure      qw ew adjusted rules
active previous time step proposed partial command accordance
 the last executed action  
   

fiporta   celaya

statistics update
terminal situation
v 
else
v max
 qw  w   winner c     a    
 


endif
q ra   v
 
w    v  c  cant
c accordance
q iw
iw iw    
else
iw min     iw   
endif
qw qw    mw     q mw
ew ew    mw      qw q  mw
endif
endfor
e ew

figure     statistics update procedure 

qw ew updated using learning rate  mw   computed using mam
function  initially    consequently  initial values qw ew
influence future values variables  initial values become relevant
using constant learning rate  many existing reinforcement learning algorithms do 
observed effects last executed action agree current estimated
interval value  iw    confidence index increased one unit  otherwise 
confidence index decreased allowing faster adaptation statistics last
obtained  surprising values reward 
partial rule management
procedure  figure     includes generation new partial rules removal
previously generated ones proved useless 
implementation  apply heuristic produces generation new partial
rules value prediction error exceeds e  way  concentrate efforts
improve categorization situations larger errors value prediction 
every time wrong prediction made  new partial rules generated
   a   recall set includes
combination pairs rules included set cant
rules active previous time step accordance executed action a  thus 
rules related situation action whose value prediction need
improve 
   

fireinforcement learning categorizable environments

combination two partial rules w  w  consists new partial rule partial
view includes features included partial views either w  w 
partial command includes elementary actions partial commands either
w  w    words  feature set w  w  union feature sets w 
w  elementary actions w  w  union w 
   a   simultaneously active
w    note that  since w  w  cant
accordance action and  thus  incompatible
 i e   include inconsistent features elementary actions  
partial rule creation  bias system favor combination rules
 wi   whose value prediction  qwi   closer observed one  q   finally  generation
rules lexicographically equivalent already existing ones allowed 
according categorizability assumption  low order partial rules required
achieve task hand  reason  improve efficiency  limit number
partial rules maximum   however  partial rule generation procedure always
generating new rules  concentrating situations larger error   therefore 
need create new rules room them  must eliminate less useful
partial rules 
partial rule removed value prediction similar rule
situations 
similarity two rules measured using normalized degree intersection value distributions number times rules used
simultaneously 
similarity w  w      

u  w w   
kiw iw  k
 
max kiw k  kiw  k  min u  w   u  w     

u  w  indicates number times rule w actually used 
similarity assessment pair partial rules controller expensive
and  general  determining similarity rule respect
generated  that rules tried refine new rule created 
sufficient  thus  based similarity measure  define redundancy
partial rule w    w  w    as 
redundancy w    max similarity w  w     similarity w  w     
observe w    w  w     w w    w u  w  u  w    
therefore
u  w w   
u  w 
u  w 
 
 
    
min u  w   u  w    
min u  w   u  w    
u  w 
reasoning done w  and  consequently 
redundancy w    max 

kiw iw  k
kiw iw  k
 
  
max kiw k  kiw  k  max kiw k  kiw  k 

need create new rules maximum number rules   
reached  partial rules redundancy given threshold    eliminated 
since redundancy partial rule estimated observing number
   

fiporta   celaya

partial rule management
 
w winner cant
  a 
 qw q    e
 if time create new rules 
 partial rule elimination 
 test room new rules 
kck  
 rule elimination based redundancy 
c c  w c   redundancy w     
 rule elimination based creation error 
kck    if still room 
sc partial rules c with 
  lowest creation error w  
  creation error w     qw q 
c c sc
endif
endif
 partial rule generation 
t 
kck    
 create new rule w    
 
 a 
select two different rules w    w  cant
preferring minimize
 qwi q  cwi   e    cwi  
w     w  w   
creation error w      qw q 
 insert new rule controller 
c c  w    
tt  
endwhile
endif

figure     partial rule management procedure  value q calculated statistics
update procedure last executed action 

times  redundancy partial rules low confidence indexes set   
immediately removed creation 
observe that  compute redundancy rule w  use partial rules
w derived  reason  rule w   cannot removed controller c
exists rule w c w   w   w     additionally  way eliminate
first useless rules higher order 

   

fireinforcement learning categorizable environments

appendix b  efficient action evaluation
non generalizing reinforcement learning cost executing single learning step
neglected  however  algorithms generalization spaces sensors and or actuators
simple execution time iteration increased substantially 
extreme case  increase limit reactivity learner
dangerous working autonomous robot 
expensive procedure algorithm computing value
actions  i e   valid combinations elementary actions   cost procedure
especially critical since used twice step  get guess action
 in action evaluation procedure detailed figure     get goodness
new achieved situation action execution  when computing v value
statistics update procedure detailed figure      trivial re order algorithm
avoid double use expensive procedure learning step  select
action executed next time evaluate goodness new
achieved situation  drawback re order action selected without
taking account information provided last reward value  the goodness
situation assessed value adjustment   however  problem tasks
require many learning steps 
even use action evaluation procedure per learning step 
optimize much possible since brute force approach described before 
evaluates action sequentially  feasible simple problems 
action evaluation method presented next based observation many
actions would value since highest relevant partial rule given
moment would provide value actions accordance partial
command rule  separate computation value two actions would end
evaluated using rule waste time  avoided performing
action evaluation attending set active rules first place set
possible actions  brute force approach does 
figure    shows general form algorithm propose  algorithm  partial
rules considered one time  ordered relevant rule least relevant
one  partial command rule consideration  cow   used process
actions accordance partial command  already processed sub set
actions need considered action evaluation procedure 
rules processed  update current situation assessment  v  action
executed next  a  attending  respectively  value prediction  qw   guess  gw  
rules 
observe partial rules maintained sorted relevance statistics update
procedure  since procedure rule relevance modified  relevance
rule changed  position list modified accordingly  way
re sort list rules every time want apply procedure
described 
elementary actions form  m k  motor k value
range possible values motor  algorithm implemented
especially efficient way since need explicitly compute set actions a 
   

fiporta   celaya

action evaluation
 initialization 
l list active rules sorted relevance 
ea set elementary actions
set combinations ea
v
 situation assessment 

 optimal action 
g
 optimal action value prediction 
 process 
w first element l 

cow partial command w
gw qw     random w   w  
aw  a a cow accordance a 
qw   v
v qw
endif
gw   g
g gw
cow
endif
aw
w next element l 
  

figure     general form proposed situation assessment action selection procedure 

case  see figure         construct decision tree using motors decision
attributes groups leaf actions evaluated partial
rule  all actions removed set iteration algorithm figure     
internal node tree classifies action according one motor commands included action  internal nodes store following information 
partial command  partial command accordance action classified node  partial command constructed collecting
motors whose values fixed nodes root tree node
consideration 
motor  motor used node classify actions  node open  i e  
still decided motor attend  motor value set  
node closed deciding motor pay attention  and adding
corresponding subtrees  converting node leaf 
   

fireinforcement learning categorizable environments

action evaluation
 initialization 
l list active rules sorted relevance 
v

g
tree new node c  
open  
closed  
 process 
w first element l 

gw qw     random w   w  
include rule tree  w  gw  
w next element l 
closed   open

figure     top level algorithm efficient action evaluation algorithm  end
algorithm  v goodness current situation used statistics
update algorithm  see figure      action executed next guess
expected value  include rule procedure detailed next figure 

subtrees  list subtrees start node  subtree
associated value corresponds one possible actions executable motor node  actions included given subtree elementary action
 m k  motor node k value corresponding
subtree 
leaves tree information value actions classified
leaf  information represented following set attributes leaf 
value  expected value actions classified leaf  maximum
value leaves used assess goodness  v  new achieved situation 
guess  value altered noise exploratory reasons  leaf maximal
guess set actions select action executed next 
relevance  relevance value predictions  of value guess  
partial command  partial command accordance actions
classified leaf  case internal nodes  partial command
constructed collecting motors whose values fixed root
tree leaf consideration 
   

fiporta   celaya

include rule n  w  gw  
not is leaf n  
cow command w 
con command n 
motor n    
 closed node  search compatible sub nodes 
ea cow motor ea    motor n 
include rule get subtree value ea   n   w  gw  
else
subtrees n 
include rule s  w  gw  
endfor
endif
else
 open node  specialize node 
cow con   
 extend node 
ea action in cow con  
set motor n  motor ea  
closed closed    
k values motor ea  
new subtree n   k  new node con  motor ea  k    
open open    
endfor
include rule n  w  gw  
else
 transform node leaf  
transform leaf n  qw   gw   w   cow  
closed closed    
qw   v
v qw
endif
gw   guess
g gw
cow
endif
endif
endif
endif

figure     include rule algorithm searches nodes node n partial command compatible partial command rule w extends nodes
insert leave tree 

given moment  inclusion new partial rule tree produces specialization open nodes compatible rule  see figure      say open
node n compatible given rule w partial command node con
partial command rule cow assign different values motor 
specialization open node result extension node  i e   new branches
   

fireinforcement learning categorizable environments

partial rules
partial view
partial command
ru ev
 m  v     m  v   
ru ev
 m  v   
ru ev
 m  v     m  v   
ru ev
 m  v   
ru ev
 m  v   
ru ev
 m  v   
ru ev
 m  v   
ru ev
 m  v   

q

e
 
 
 
 
 
  
 
 


   
   
   
   
   
   
   
   

    
    
    
    
    
    
    
    

guess
   
   
   
   
   
   
   
    

table    set rules controller  values q e stored guess
computed them  define partial views ru ev indicate
active current time step 

added tree node  transformation node leaf 
node extended partial command rule affects motors included
partial command node  means motor values taken
account tree used action evaluation according
rule consideration  node extended  one motors present
layers tree used generate layer open nodes current node 
that  node considered closed inclusion rule procedure repeated
node  with different effects node closed   motors affected
partial command rule affected partial command node 
node transformed leaf storing value  guess  relevance attributes
extracted information associated rule 
process stopped soon detect nodes closed  i e 
external nodes tree leaves   case  rules still processed
effect tree form and  consequently useful action evaluation  rule
consistently used action evaluation  removed controller 
toy size example illustrate tree based action evaluation algorithm  suppose
robot three motors accept two different values  named v  
v     produces set   different actions  suppose that  given moment  robot
controller includes set rules shown table    action evaluation algorithm
 figure      rules processed least relevant one expanding
initially empty tree using algorithm figure     inclusion rule tree results
extension tree  see stages b  e figure     closing branches
converting open nodes leaves  stages c f   particular case tree becomes
completely closed processing   rules   active rules controller 
end process  tree five leaves  three include two actions
two represent single action  using tree say value
situation tree constructed  v     this given leaf circled
solid line figure   additionally  next action executed form
   

fiporta   celaya

motor  m 
command 
true c

b


open
node

v 

v 

motor  m 
command 
 m  v  
v 

open
node

v 
value   
guess     
relevance      
command 
 m  v  
 m  v  

open
node

c

motor  m 
command 
 m  v  
v 

open
node

value   
guess     
relevance      
command 
 m  v  
 m  v  

e

open
node

v 

v 

v 

open
node

value   
guess     
relevance      
command 
 m  v  
 m  v  

motor  m 
command 
 m  v  
 m  v  
v 
v 

v 
motor  m 
command 
 m  v  
 m  v  
v 
value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

v 

v 

v 

value   
guess     
relevance      
command 
 m  v  
 m  v  

motor  m 
command 
 m  v  
v 

motor  m 
command 
 m  v  
v 

v 
value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  

motor  m 
command 
true c

v 
motor  m 
command 
 m  v  
v 

v 

value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

open
node

f

motor  m 
command 
 m  v  

motor  m 
command 
 m  v  
v 

motor  m 
command 
 m  v  

motor  m 
command 
true c
v 

v 

v 

v 

v 

v 

motor  m 
command 
true c



motor  m 
command 
true c

v 
motor  m 
command 
 m  v  
 m  v  
v 
value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

v 

value   
guess     
relevance      
command 
 m  v  
 m  v  

v 
value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

figure     six different stages construction tree action evaluation 
stage corresponds insertion one rule table   

   

fireinforcement learning categorizable environments

 
 

log time 

 
 
 
 
 
 
 

 

 

 
 
number void motors

brute force evaluation

 

 

treebased evaluation

figure     log execution time  in seconds  brute force approach vs  treebased one 

 m  v    m  v    m       represents possible action  optimal action
given leaf circled dashed line leaf larger guess value 
cost algorithm largely depends specific set partial rules
processed  worst case  cost algorithm is 
o nr lnm   
nr number rules  nm number motors and  l maximal range values
accepted motors  because  worst case  insert given rule 
visit nodes maximally expanded tree  i e   tree node l subtrees
final nodes branches still opened   number nodes
tree
nm
x
lnm     
li  
  o lnm   
l 
i  

transform cost expression taking account l nm total number
possible combinations elementary actions  nc   or  words  total amount
actions  therefore  cost presented algorithm
o nr nc   
hand  cost brute force approach always
 nr nc   
   

fiporta   celaya

so  worst case  cost presented algorithm order cost
brute force approach  however  since l rules would enough close
maximally expanded tree  one rule different values motor used last
still open layer tree   cost tree based algorithm would be  average 
much smaller brute force approach 
figure    exemplifies different performance brute force action evaluation procedure tree based one  figure shows time taken execution
toy example section      experiment  defined void motors motors
whose actions effect environment  seen  number void
motors increases  cost tree based evaluation significantly less
brute force approach 

   

fireinforcement learning categorizable environments

appendix c  notation
uppercase used sets  greek letters represent parameters algorithms 

set states 
 
s 
individual states  full views 
ns
number states 
f    fdi        nf  
set feature detectors 
partial view order k 
v fdi            fdik  

set actions robot 
na
number actions 
ea    eai        ne  
set elementary actions 
nm
number motors robot 
eai    mi k 
elementary action assigns value k motor mi  
c eai            eaik  
partial command order k 
   ea            eanm  
action  combination elementary actions  full command 
w    v  c 
partial rule composed partial view v partial command c 
w
empty partial rule 
w  w  
composition two partial rules 
c    wi        nr  
controller set partial rules 

maximum number elements c 
 
 
c   cant
subset rules active given time step previous one 
c    a 
active rules partial command accordance a 
qw
expected value partial rule w 
ew
expected error value estimation partial rule w 
e
average error value prediction 
iw
confidence index 
cw
confidence statistics partial rule w 

top value confidence 

index confidence function reaches value  
w   ew cw   e    cw   error return prediction partial rule w 
w          w  
relevance rule w 
iw    qw  w  
value interval partial rule w 
mw
updating ratio statistics partial rule w 

learning rate  top value mw  
u  w 
number times rule w used 
 
winner c   a 
relevant active partial rule w r t  action a 
guess a 
reliable value estimation action a 
ra
reward received execution a 

discount factor 
v
goodness given situation 
q   ra   v
value executing action given situation 

number new partial rules created time 

redundancy threshold used partial rule elimination 

   

fiporta   celaya

references
arkin  r  c          behavior based robotics  intelligent robotics autonomous agents 
mit press 
bellman  r  e          dynamic programming  princeton university press  princeton 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  journal artificial intelligence research 
        
brooks  r  a          intelligence without representation  artificial intelligence         
    
butz 

m         
c xcs  implementation
 http   www cs bath ac uk  amb lcsweb computer htm  



xcs



c 

celaya  e     porta  j  m          control six legged robot walking abrupt terrain 
proceedings ieee international conference robotics automation 
pp           
celaya  e     porta  j  m          control structure locomotion legged
robot difficult terrain  ieee robotics automation magazine  special issue
walking robots              
chapman  d     kaelbling  l  p          input generalization delayed reinforcement
learning  algorithm performance comparisons  proceedings international joint conference artificial intelligence  pp         
claus  c     boutilier  c          dynamics reinforcement learning cooperative
multiagent systems  proceedings fifteenth national conference artificial
intelligence  pp          american association artificial intelligence 
drummond  c          accelerating reinforcement learning composing solutions automatically identified subtasks  journal artificial intelligence research            
edelman  g  m          neuronal darwinism  oxford university press 
hinton  g   mcclelland  j     rumelhart  d          parallel distributed processing  explorations microstructure cognition  volume    foundations  chap  distributed
representations  mit press  cambridge  ma 
ilg  w   muhlfriedel  t     berns  k          hybrid learning architecture based neural
networks adaptive control walking machine  proceedings      ieee
international conference robotics automation  pp           
kaelbling  l  p          learning embedded systems  bradford book  mit press 
cambridge ma 
kaelbling  l  p   littman  m  l     moore  a  w          reinforcement learning  survey 
journal artificial intelligence research             
kanerva  p          sparse distributed memory  mit press  cambridge  ma 
kirchner  f          q learning complex behaviors six legged walking machine 
robotics autonomous systems             
   

fireinforcement learning categorizable environments

kodjabachia  j     meyer  j  a          evolution development modular control
architectures   d locomotion six legged animats  connection science        
    
maes  p     brooks  r  a          learning coordinate behaviors  proceedings
aaai     pp         
mahadevan  s     connell  j  h          automatic programming behavior based robots
using reinforcement learning  artificial intelligence             
mccallum  a  k          reinforcement learning selective perception hidden
state  ph d  thesis  department computer science 
parker  g  b          co evolving model parameters anytime learning evolutionary
robotics  robotics autonomous systems           
pendrith  m  d     ryan  m  r  k          c trace  new algorithm reinforcement
learning robotic control  proceedings      international workshop
learning autonomous robots  robotlearn    
poggio  t     girosi  f          regularization algorithms learning equivalent
multilayer networks  science  pp         
schmidhuber  j          speed prior  new simplicity measure yielding near optimal
computable predictions  proceedings   th annual conference computational learning theory  colt  oo    lecture notes artificial intelligence 
springer   pp         
sen  s          learning coordinate without sharing information  proceedings
twelfth national conference artificial intelligence  pp          american
association artificial intelligence 
sutton  r  s          reinforcement learning architectures animats  meyer  j  a    
wilson  s  w   eds    proceedings first international conference simulation adaptive behavior  animals animats  pp          mit press 
bradford books 
sutton  r  s     barto  a  g          reinforcement learning  introduction  bradford
book  mit press 
sutton  r  s     whitehead  s  d          online learning random representations 
proceedings eleventh international conference machine learning  pp 
        morgan kaufman  san francisco  ca 
sutton  r          generalization reinforcement learning  successful examples using
sparse coarse coding  proceedings      conference advances neural
information processing  pp           
sutton  r   precup  d     singh  s          mdps semi mdps  framework
temporal abstraction reinforcement learning  artificial intelligence             
tan  m          multi agent reinforcement learning  independent vs  cooperative agents 
reading agents  pp          morgan kaufmann publishers inc 
   

fiporta   celaya

vallejo  e  e     ramos  f          distributed genetic programming architecture
evolution robust insect locomotion controllers  meyer  j  a   berthoz  a  
floreano  d   roitblat  h  l     wilson  s  w   eds    supplement proceedings
sixth international conference simulation adaptive behavior  animals
animats  pp          international society adaptive behavior 
venturini  g          apprentissage adaptatif et apprentissage supervise par algorithme
genetique  ph d  thesis 
watkins  c  j  c  h     dayan  p          q learning  machine learning            
widrow  b     hoff  m          adaptive switching circuits  western electronic show
convention  volume    pp         institute radio engineers  now ieee  
wilson  s  w          classifier fitness based accuracy  evolutionary computation    
       
wilson  s  w          explore exploit strategies autonomy  animals animats    proceedings  th international conference simulation adaptive
behavior  pp         

   


