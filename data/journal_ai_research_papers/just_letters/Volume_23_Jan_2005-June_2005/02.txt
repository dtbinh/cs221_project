journal of artificial intelligence research                 

submitted       published     

reinforcement learning for agents with many sensors and
actuators acting in categorizable environments
josep m porta

porta science uva nl

ias group  informatics institute
university of amsterdam
kruislaan          sj  amsterdam  the netherlands

enric celaya

celaya iri upc edu

institut de robotica i informatica industrial
spanish council of scientific research  csic 
llorens i artigas             barcelona  spain

abstract
in this paper  we confront the problem of applying reinforcement learning to agents that
perceive the environment through many sensors and that can perform parallel actions using
many actuators as is the case in complex autonomous robots  we argue that reinforcement
learning can only be successfully applied to this case if strong assumptions are made on
the characteristics of the environment in which the learning is performed  so that the
relevant sensor readings and motor commands can be readily identified  the introduction
of such assumptions leads to strongly biased learning systems that can eventually lose the
generality of traditional reinforcement learning algorithms 
in this line  we observe that  in realistic situations  the reward received by the robot
depends only on a reduced subset of all the executed actions and that only a reduced subset
of the sensor inputs  possibly different in each situation and for each action  are relevant to
predict the reward  we formalize this property in the so called categorizability assumption
and we present an algorithm that takes advantage of the categorizability of the environment 
allowing a decrease in the learning time with respect to existing reinforcement learning
algorithms  results of the application of the algorithm to a couple of simulated realisticrobotic problems  landmark based navigation and the six legged robot gait generation  are
reported to validate our approach and to compare it to existing flat and generalizationbased reinforcement learning approaches 

   introduction
the division between knowledge based and behavior based artificial intelligence has been
fundamental to achieving successful applications within the field of autonomous robots  arkin 
       however  up to now  this division has had few repercussions for reinforcement learning  within artificial intelligence  reinforcement learning has been formalized in a very
general way borrowing ideas from the dynamic programming and decision theory fields 
within this formalization  the objective of reinforcement learning methods is to establish
a correct mapping from a set of abstract observations  formalized as states  to a set of
high level actions  without being worried about how these sets of states and actions are
defined  for an introduction to reinforcement learning you can check kaelbling  littman 
  moore        sutton   barto        among many others   algorithms developed within
this general framework can be used in different fields without any modification  for each
c
    
ai access foundation  all rights reserved 

fiporta   celaya

particular application  the definition of the sets of states and actions is the responsibility
of the programmer and is not supposed to be part of the reinforcement learning problem 
however  as clearly pointed by brooks         in autonomous robots the major hurdles
are those related with perception and action representations  for this reason  in a robotic
task  what traditional reinforcement learning research assumes to be the major problem
 connecting states and actions  is simpler than what it assumes as given  the definition
of states and actions   the consequence is that existing reinforcement learning methods
are best suited for problems that fall into the symbolic artificial intelligence domain than
for those that belong to robotics  due to the generality of existing reinforcement learning
algorithms  a robotic problem can be analyzed and re formulated so that it can be tackled
with the available reinforcement learning tools but  in many cases  this re formulation is
too awkward introducing unnecessary complexity in the learning process  the alternative
we explore in this paper is a new reinforcement learning algorithm that can be applied to
robotic problems as they are  without any re formulation 
as brooks        remarked  dealing with a real environment is not necessarily a problem
since real environments have properties that can be exploited to reduce the complexity of the
robots controller  in brooks works  we can find simple robot controllers that achieve very
good performance in particular environments  this is clearly in contrast with the generality
pursued within reinforcement learning  following an idea parallel to that of brooks  in this
paper  we present a new reinforcement learning algorithm that takes advantage of a specific
environment related property  that we call categorizability  to efficiently learn to achieve
a given task  we formalize the categorizability property and we present a representation
system  partial rules  to exploit this property  a remarkable feature of this representation
system is that it allows generalization in both the spaces of sensors and actions  using
a uniform mechanism  this ability to generalize in both the state and action spaces is
fundamental to successfully apply reinforcement learning to autonomous robots 
this paper is organized as follows  first  in section    we formalize reinforcement learning from the point of view of its use in the field of autonomous robotics and we describe
the problems that make flat  and  in most cases  also generalization based  reinforcementlearning algorithms not adequate for this case  section   presents the categorizability assumption which is plausible in most robotics environments  then  in section    we describe
an alternative reinforcement learning algorithm that exploits the categorizability assumption to circumvent the problems present in existing approaches  in section    we analyze
the points of contact between our proposal and already existing work  next  in section   
we present experiments that validate our approach  the experiments are performed in
simulations that mimic realistic robotic applications where the categorizability assumption
is likely to be valid  finally  in section    we conclude by analyzing the strengths and
weaknesses of the proposed learning system 
additionally  appendix a provides a detailed description of the partial rule learning
algorithm introduced in this paper  appendix b is devoted to an enhancement on this
algorithm to make its execution more efficient  and appendix c summarizes the notation
we use throughout the paper 
  

fireinforcement learning in categorizable environments

   problem formalization
for simplicity  we assume that the robot perceives its environment through a set of binary
feature detectors  f d    fdi   i      nf    a feature detector can be devised as a process
that identifies specific combinations of present  and possibly past  sensor readings  the
use of feature detectors is very common in robotics  in this field  feature detectors are
defined by the programmer attending to the special characteristics of the environment  the
robot sensors  and the task to be executed in order to extract potentially useful information
 presence of landmarks or obstacles          from raw sensor readings 
in a similar way  instead of working directly with the space of actions provided by
the robot motors  that define a too low level way of controlling the robot   it is a common
practice to define a set of elementary actions ea    eai  i      ne    an elementary action is
a specific sequence combination of motor commands defined by the programmer attending
to the characteristics of the robot and the task to be achieved  to simplify  we can assume
that elementary actions are of the form  mi  k   i      nm    where mi is a motor and k
a value in the range of valid inputs for the motor mi   this framework is quite flexible since
a motor mi can be either one of the physical motors of the robot or a high level  abstract
motor that combines movements of the actual motors  with this formalization  at a given
moment  the robot can execute in parallel as many elementary actions as available motors 
a robot controller can be seen as a procedure that executes  combinations of elementary 
actions in response to specific situations  i e   activation of specific feature detectors  with
the objective of achieving a given task  reinforcement learning approaches automatically
define such a controller using the information provided by the reward signal  in the context
of reinforcement learning  the controller is called the policy of the learner 
the objective of the value function based reinforcement learning algorithms  the most
common reinforcement learning algorithms  is to predict the reward that can be directly or
indirectly obtained from the execution of each action  i e   of each combination of elementary
actions  in each possible situation  described as a combination of active and inactive feature
detectors  if this prediction is available  the action to be executed in each situation is the
one from which maximum reward is expected 
to predict the reward  classic reinforcement learning algorithms rely on the markov
assumption  that requires a state signal to carry enough information to determine the effects
of all actions in a given situation   additionally  non generalizing reinforcement learning
algorithms assume that the states of the system must be learned about independently  so 
the information gathered about the effects of an action a in a given state s  denoted q s  a  
cannot be safely transferred to similar states or actions  with this assumption  the cost of
a reinforcement learning algorithm in a general problem is
 ns na   
where ns is the number of states and na is the number of actions  this is because each
action has to be tried as least once in each state  since the state is defined as the observed
   non binary feature detectors providing a discrete range of values can be readily binarized 
   non markovian problems  when confronted  should be converted into markovian ones  how to do that
is out of the scope of this paper  although it is one of the most relevant points to achieve a successful
real world reinforcement learning application 

  

fiporta   celaya

combination of feature detectors  we have that the potential number of states is
ns     nf  
with nf the number of feature detectors  consequently  we have that
 ns na       nf na   
which is exponential in the number of feature detectors  since the number of feature detectors used in robotic applications tends to be high  non generalizing reinforcement learning
becomes impractical for realistic problems  this is the well known curse of dimensionality
introduced by bellman         whose research presaged some of the work in reinforcement
learning 
although the size of the action set  na   is as important as the size of the state set  ns  
in the curse of dimensionality  less attention is paid to actions in the reinforcement learning
literature  however  a robot with many degrees of freedom can execute many elementary
actions simultaneously and this makes the cost of the learning algorithms also increase
exponentially with the number of motors of the robot  nm   
suppose we address the same task but with two different sets of feature detectors f d  
and f d  such that f d   f d    using a plain reinforcement learning algorithm  the cost
of finding a proper policy would be larger using the larger set of features  f d      and this
is so even if one of the features in f d   f d  has a stronger correlation with the reward
than any of the features in f d    non generalizing reinforcement learning algorithms are
not able to take advantage of this situation  and  even having better input information 
their performance decreases  a similar argument can be made for actions in addition to
feature detectors 
generalizing reinforcement learning algorithms such as those using gradient descent
techniques  widrow   hoff         coarse codings  hinton  mcclelland    rumelhart 
       radial basis functions  poggio   girosi         tile coding  sutton        or decision
trees  chapman   kaelbling        mccallum        can partially palliate this problem
since they can deal with large state spaces  however  as we approach complex realistic
problems  the number of dimensions of the state space grows to the point of making the use
of some of these generalization techniques impractical and other function approximation
techniques must be used  sutton   barto        page      
adding relevant inputs or actions to a task should make this task easier or at least
not more difficult  only methods whose complexity depends on the relevance of the available inputs and actions and not on their number would scale well to real domain problems 
examples of systems fulfilling this property are  for instance  the kanerva coding system presented by kanerva        and the random representation method by sutton and whitehead
        while those systems rely on large collections of fixed prototypes  i e   combinations
of feature detectors  selected at random  our proposal is to search for the appropriate prototypes  but using a strong bias so that the search can be performed in a reasonable time 
this strong bias is based on the categorizability assumption that is a plausible assumption
for the case of autonomous robots  which allows a large speed up in the learning process 
additionally  existing systems do not address the problem of determining the relevance of
actions  since they assume the learning agent has a single actuator  that is  obviously  the
  

fireinforcement learning in categorizable environments

only relevant one   this simple set up is not adequate for robotics  in our approach  presented below   combinations of both feature detectors and elementary actions are considered
using a unified framework 

   the categorizability assumption
from our experience developing controllers for autonomous robots  we observe that  in many
realistic situations  the reward received by the robot depends only on a reduced subset of
all the actions executed by the robot and that most of the sensor inputs are irrelevant to
predict that reward  thus  for example  the value resulting from the action of grasping
the object in front of the robot will depend on what the object is  the object the robot
should bring to the user  an electrified cable  or an unimportant object  however  the result
will probably be the same whether or not the robot is moving its cameras while grasping
the object  if it is day or night  if the robot is  at the same time  checking the distance to
the nearest wall  or if it can see a red light nearby or not  aspects  all of them  that may
become important in other circumstances  
if an agent observes and acts in an environment where a reduced fraction of the available inputs and actuators have to be considered at a time  we say that the agent is in a
categorizable environment 
categorizability is not a binary predicate but a graded property  in the completely
categorizable case  it would be necessary to pay attention to only one sensor motor in
each situation  on the other extreme of the spectrum  if all motors have to be carefully
coordinated to achieve the task and the effect of each action could only be predicted by
taking into account the value of all feature detectors  we would say that the environment is
not categorizable at all 
since robots have large collection of sensors providing a heterogeneous collection of
inputs and many actuators affecting quite different degrees of freedom  our hypothesis is
that  in robotic problems  environments are highly categorizable and  in those cases  an
algorithm biased by the categorizability assumption would result advantageous 

   reinforcement learning in categorizable environments  the partial
rule approach
to implement an algorithm able to exploit the potential categorizability of the environment 
we need a representation system able to transfer information between similar situations and
also between similar actions 
clustering techniques or successive subdivisions of the state space  as  for instance  that
presented by mccallum        focus on the perception side of the problem and aim at
determining the reward that can be expected in a given state s considering only some of
the feature detectors perceived in that state  this subset of relevant feature detectors is
used to compute the expected reward in this state for any possible action a  the q s  a 
function   however  with this way of posing the problem the curse of dimensionality problem
is not completely avoided since some of the features can be relevant for one action but
not for another and this produces an unnecessary  from the point of view of each action 
differentiation between equivalent situations  decreasing the learning speed  this problem
  

fiporta   celaya

can be avoided by finding the specific set of relevant feature detectors for each action  in
this case  the q function is computed as q fs  a   a   with a state definition that is function
of the action under consideration  this technique is used  for instance  by mahadevan and
connell         unfortunately  in the problem we are confronting  this is not enough since 
in our case  actions are composed by combinations of elementary actions and we also want to
transfer reward information between similar combinations of actions  therefore  we have to
estimate q fs  a   a  only taking into account some of the elementary actions that compose
a  however  in principle  the relevance of elementary actions is function of the situation  or 
equivalently  of the state   a given elementary action can be relevant in some situations but
not in others  for this reason  the function to approximate becomes q f s  a   fa  s   where
there is a cross dependency between the state defined as a function of the action  f s  a   and
the action defined as a function of the state  fa  s   the proposal we detail next solves this
cross dependency by working in the cartesian product of the spaces of feature detectors
and elementary actions combinations 
to formalize our proposal  we introduce some definitions 
we say that the agent perceives  or observes  a partial view of order k  v fd i            fdik   
k  nf whenever the predicate fdi        fdik holds   obviously  many partial views can be
perceived at the same time 
at a given moment  the agent executes an action a that issues a different command for
each one of the agents motors a    ea            eanm    with nm the number of motors 
a partial command of order k  noted as c eai            eaik    k  nm   is executed whenever
the elementary actions  eai            eaik   are executed simultaneously  we say that a partial
command c and an action a are in accordance if c is a subset of a  note that the execution
of a given action a supposes the execution of all the partial commands in accordance with
it 
a partial rule w is defined as a pair w    v  c   where v is a partial view and c is a
partial command  we say that a partial rule w    v  c  is active if v is observed  and that w
is used whenever the partial view v is perceived and the partial command c is executed  a
partial rule covers a sub area of the cartesian product of feature detectors and elementary
actions and  thus  it defines a situation action rule that can be used to partially determine
the actions of the robot in many situations  all those where the partial view of the rule is
active   the order of a partial rule is defined as the sum of the order of the partial view
and the order of the partial command that compose the rule 
we associate a quantiy qw to each partial rule  qw is an estimation of the value  i e   the
discounted cumulative reward  that can be obtained after executing c when v is observed
at time t 

x
qw  
 t i rt i  
i  

with rt i the reward received by the learner at time step t   i after rule w is used at time
t  so  a partial rule can be interpreted as  if partial view v is observed then the execution
of partial command c results in value qw  
   a partial view can also include negations of feature detectors since the non detection of a feature can be
as relevant as its detection 

  

fireinforcement learning in categorizable environments

the objective of the learning process is that of deriving a set of partial rules and adjusting
the corresponding qw values so that the desired task can be properly achieved 
the apparent drawback of the partial rule representation is that the number of possible
partial rules is much larger than the number of state and action pairs  the number of
partial rules that can be defined on a set of nf binary feature detectors and nm binary
motors is  nf  nm   while the number of different states and action pairs is only  nf  nm  
if arbitrary problems have to be confronted  as is the case in synthetic learning situations  
the partial rule approach could not be useful  however  problems confronted by robots
are not arbitrary since  as mentioned  environments present regularities or properties  as
categorizability  that can be exploited to reduce the complexity of the controller necessary
to achieve a given task 
using the partial rule framework  the categorizability assumption can be formally defined
as 
definition   we say that an environment task is highly categorizable if there exists a set
of low order partial rules that allows us to predict the reward with the same accuracy as if
statistics for each possible state action combination were considered  the lower the order
of the rules in the controller the higher the categorizability of the environment task 
to the extent the categorizability assumption is fulfilled  the number of partial rules
necessary to control the robot becomes much smaller than the number of state action pairs
that can be defined using the same sets of feature detectors and elementary actions in which
the partial views and partial commands are based  additionally  categorizability implies
that the rules necessary in the controller are mostly those with lower order and this can
be easily exploited to bias the search in the space of partial rules  so  if the environment
is categorizable  the use of the partial rule approach can suppose an important increase
in the learning speed and a reduction in the use of memory with respect to traditional
non generalizing reinforcement learning algorithms 
in the following sections  we describe how it is possible to estimate the effect of an
action given a fixed set of partial rules  this evaluation  repeated for all actions  is used
to determine the best action to be executed at a given moment  next  we detail how it is
possible to adjust the value predictions of a fixed set of partial rules  finally  we describe how
the categorizability assumption allows us to use an incremental strategy in the generation
of new partial rules  this strategy results in faster learning than existing generalizing and
non generalizing reinforcement learning algorithms  all procedures are described in highlevel form to make the explanation more clear  details of their implementation can be found
in appendix a 
    value prediction using partial rules
in a given situation  many partial views are simultaneously active triggering a subset of the
partial rules of the controller c  we call this subset the active partial rules and we denote
it as c     to evaluate a given action a we only take into account the rules in c   with a
partial command in accordance with a  we denote this subset as c    a   note that  in our
approach  when we refer to an action  we mean the corresponding set of elementary actions
 one per motor  and not a single element  as it is the general case in reinforcement learning 
  

fiporta   celaya

every rule w    v  c  in c    a  provides a value prediction for a  the qw associated with
the partial rule  this is an averaged value that provides no information about the accuracy
of this prediction  as also pointed by wilson         we should favor the use of the partial
rules with a high accuracy in value prediction or  as we say it  rules with a high relevance 
it seems clear that the relevance of a rule  w   depends on the distribution of values
around qw   distributions with low dispersion are indicative of coherent value predictions
and  so  of a highly relevant rule  to measure this dispersion we maintain an error estimation
ew on the approximation of qw   another factor  not used by wilson        to be taken into
account in the relevance determination is the confidence on the qw and ew statistics  low
confidence  i e   insufficiently sampled  measures of qw and ew should reduce the relevance
of the rule  the confidence on the value prediction for a given rule  cw   is a number in
the interval         initialized as    and increasing as the partial rule is used  i e   the rule
is active and its partial command is executed   the confidence would only decrease if the
value model for a given partial rule is consistently wrong 
using the confidence  we approximate the real error in the value prediction for a partial
rule w as
w   ew cw   e     cw   
where value e is the average error on the value prediction  observe that the importance of
e is reduced as the confidence increases and  consequently  w converges to ew  
with the above definitions  the relevance of a partial rule can be defined as
w  

 
 
    w

note that the exact formula for the relevance is not that important as far as  w   w  
w   w    the above formula provides a value in the range        that could be directly
used as a scale factor  if necessary 
the problem is then  how can we derive a single value prediction using the qw statistics
of all the rules in c    a  and its corresponding relevance value  w   two possible solutions
come to mind  using a weighted sum of the values predicted by all these partial rules using
the relevance as a weighting factor  or using a competitive approach  in which only the most
relevant partial rule is used to determine the predicted value  the weighted sum assumes
a linear relation between the inputs  the value prediction provided by each individual rule 
and the output  the value prediction for a   this assumption has proved powerful in many
systems but  in general  it is not compatible with the categorizability assumption since 
although each one of the partial rules involved in the sum can be of low order  taking all
of them into account means using a large set of different feature detectors and elementary
actions to predict the effect of a given action  for this reason  our learning system uses a
winner take all solution where only the value prediction of the most relevant partial rule is
taken into account to predict the value of an action  so  for each action we determine the
winner rule
w  winner  c     a   arg

max  w    

w  c    a 

and we use the range of likely value for this rule  iw    qw   w   qw    w    to randomly
determine the value prediction for action a  the probability distribution inside this interval
depends on the distribution we assume for the value 
  

fireinforcement learning in categorizable environments

the procedure just outlined can be used at each time step to obtain a value prediction
for each action  the action with the maximal value is the one we want the robot to execute
next 
observe that we obtain a probabilistic value prediction  in the same situation with the
same statistics  we can get different value predictions for the same action  in this way 
the action that obtains the maximal evaluation is not always the one with maximal q w
and  consequently  we favor the exploration of promising actions  this probabilistic action selection provides an exploratory mechanism that uses more information than typical
reinforcement learning exploration mechanisms  the error and confidence of value predictions is not available in most reinforcement learning algorithms  and the result is a more
sophisticated exploration schema  see wilson        for a survey of different exploration
mechanisms in reinforcement learning  
    partial rules value adjustment
we adjust the value predictions for all the rules in c    a  where a is the last executed action 
for each rule to be adjusted  we have to update its qw   ew   and cw statistics 
the effect of any action a in accordance with the partial command c attending to a
partial rule w    v  c  can be defined  using a bellman like equation  as

qw
  rw   

x

p w  c     v   c     

c  

where r w is the average reward obtained immediately after executing c when v is observed 
 is the discount factor used to balance the importance of immediate with respect to delayed
reward  v   c     represents the goodness  or value  of the situation where rules c   are active 
and p w  c     is the probability of reaching that situation after the execution of c when v
is observed  the value of a situation is assessed using the best action executable in that
situation

v   c       max
 qw
 w   winner c     a     
 
a

since this gives us information about how well the robot can perform  at most  from that
situation 
as in many of the existing reinforcement learning approaches  the values of q w and ew
for the rules to be adjusted are modified using a temporal difference rule so that they
 and the error on this measure  rules that have a direct relation
progressively approach qw
with the received reward would provide a value prediction  qw   coherent with the actually
obtained one and  consequently  after the statistics adjustment  their prediction error will
be decreased  contrariwise  rules not related to the observed reward would predict a value
different from the obtained one and their error statistics will be increased  in this way  if a
rule is really important for the generation of the received reward  its relevance is increased
and if not it is decreased  rules with low relevance have few chances of being used to drive
the robot and  in extreme cases  they could be removed from the controller 
the confidence cw should also be adjusted  this adjustment depends on how the confidence is measured  if it is only related to the number of samples used in the qw and ew
statistics  then cw should be simply slightly incremented every time the statistics of rule w
  

fiporta   celaya

are updated  however  we also decrease the confidence if the value model for a given partial
rule is consistently wrong  i e   the value observed is systematically out of the interval i w   
observe that our learning rule is equivalent to those used in state based reinforcementlearning methods  for instance  in q learning  watkins   dayan         q   s  a   with s a
state and a an action  is defined as
x
p s  a  s    v   s    
q  s  a    r w   
s 

with p s  a  s    the probability of a transition from s to s  when a is executed and
v   s      max
 q  s    a    
 
a

in our approach  the set of rules active in a given situation c   plays the role of a state
 instead
and  thus  v   c     and v   s    are equivalent  on the other hand  we estimate qw

of q  s  a   but the rule w includes information about both  partial  state and actions
 and q  s  a  to play a similar role  the value prediction for a given rule  q  
making qw
w
corresponds to the average of value predictions for the cells of the cartesian product of
feature detectors and elementary actions covered by that rule  in the case of complete
rules  i e   rules involving all the feature detectors and actions for all motors   the sub area
covered by the rule includes only one cell of the cartesian product and  therefore  if the
controller only includes complete rules  the just described learning rule is exactly the same
as that used in q learning  in this particular case  c    a  is just one rule that  consequently 
is the winner rule  the statistics for this rule are the same  and are updated in the same
way  as those for the q s  a  entry of the table used in q learning  thus  our learning rule
is a generalization of the learning rule normally used in reinforcement learning 
    controller initialization and partial rule creation elimination
since we assume we are working in a categorizable environment  we can use an incremental
strategy to learn an adequate set of partial rules  we initialize the controller with rules of
the lowest order and we generate new partial rules only when necessary  i e   for cases not
correctly categorized using the available set of rules   so  the initial controller can contain 
for instance  all the rules of order two that include one feature detector and one elementary
action   v fdi    c aej      v fdi    c aej    i  j   in any case  it is sensible to include the
empty rule  the rule of order    w   in the initial controller  this rule is always active and it
provides the average value and the average error in the value prediction  additionally  any
knowledge the user has about the task to be achieved can be easily introduced in the initial
controller in the form of partial rules  if available  an estimation of the value predictions
for the user defined rules can also be included  if the hand crafted rules  and their value
predictions  are correct the learning process will be accelerated  if they are not correct  the
learning algorithm would take care of correcting them 
we create a new rule when a large error in the value prediction is detected  the new
rule is defined as a combination of two of the rules in c    a   that are the rules that forecast
the effects of the last executed action  a  in the current situation  when selecting a couple
of rules to be combined  we favor the selection of those with a value prediction close to
  

fireinforcement learning in categorizable environments

the actually observed one  since they are likely to involve features and elementary actions
 partially  relevant for the value prediction we try to refine 
the problem is that it is not possible to determine a priori whether an incorrectly
predicted value would be correctly predicted after some rule adjustments or if it is really
necessary to create a new partial rule to account for the received reward  so  if we create new
rules when there is a large error in the value prediction  it is possible to create unnecessary
rules  the existence of  almost  redundant rules is not necessarily negative  since they
provide robustness to the controller  the so called degeneracy effect introduced by edelman
        what must be avoided is to generate the same rule twice  since this is not useful at
all  two rules can be identical with respect to lexicographic criteria  they contain the same
feature detectors and elementary actions  but also with respect to semantic ones  they
get active in the same situations and propose equivalent actions   if identical rules are
created  then they have to be detected and removed as soon as possible  preserving only
the rules that proved to be useful avoids the number of rules in the controller growing above
a reasonable limit 
since we create new rules while there is a significant error in the value prediction 
if necessary  we could end up generating complete rules  provided we do not limit the
number of rules in our controller   in this case  and assuming that the more specific the
rule the more accurate the value prediction  our system would behave as a normal tablebased reinforcement learning algorithm  only the most specific rules  i e   the most relevant
ones  would be used to evaluate actions and  as explained before  the statistics for these
rules would be exactly the same as those in table based reinforcement learning algorithms 
thus  in the limit  our system can deal with the same type of problems as non generalizing
reinforcement learning algorithms  however  we regard this limit situation as very improbable and we impose limits to the number of rules in our controllers  observe that this
asymptotic convergence to a table based reinforcement learning is only possible because we
use a winner takes all strategy in the action evaluation  with a weighted sum strategy 
the value estimation for the non complete rules possibly present in the controller would
be added to that of complete rules leading to an action evaluation different from that of
table based reinforcement learning algorithms 

   the partial rule approach in context
the categorizability assumption is closely related with complexity theory principles such as
the minimum description length  mdl  that has been used by authors such as schmidhuber        to bias learning algorithms  all these complexity results try to formalize the
well known occams razor principle that enforces choosing the simplest model from a
set of otherwise equivalent models 
boutilier  dean  and hanks        presents a good review on representation methods
to reduce the computational complexity of planning algorithms by exploiting the particular
characteristics of a given environment  the representation based on partial rules can be seen
as another of these representation systems  however  the partial rule is just a representation
formalism that  without the bias introduced by the categorizability assumption  would not
be efficient enough to be applied to realistic applications 
  

fiporta   celaya

the partial rule formalism can be seen as a generalization of that of the xcs classifier
systems described by wilson         this xcs learning system aims at determining a set
of classifiers  that are combinations of features with an associated action  with their associated value and relevance predictions  the main difference between this approach and ours
is that wilsons work pursues a generic learner and we bias the learning process using the
categorizability assumption  this allows us to use an incremental rule generation strategy
that is likely to be more efficient in robotic problems  additionally  the categorizability assumption also modifies the way in which the value for a given action is evaluated  wilsons
approach uses a weighted sum of the predictions of the classifier advocating for each action
to determine the expected effect of that action  while  to fulfill with the categorizability assumption  i e   to minimize the number of feature detectors and elementary actions involved
in a given evaluation   we propose to use a winner takes all strategy  this is a critical point
since the winner takes all strategy takes full advantage of the categorizability assumption
and because it allows the partial rule system to asymptotically converge to a table based
reinforcement learning system  this is not the case when a weighted sum strategy is used 
furthermore  in the xcs formalism there is no generalization in the action space and  as
already commented  this is a requirement in robotic like applications 
in general  reinforcement learning does not pay attention to the necessity of generalizing
in the space of actions  although some exceptions exists  for instance  the work of maes and
brooks        includes the possible execution of elementary actions in parallel  however
this system does not include any mechanism detecting interactions between actions and 
thus  the coordination of actions relies on sensory conditions  for instance  this system has
difficulties detecting that the execution of two actions results always  i e   independently of
the active inactive feature detectors  in positive negative reward 
the cascade algorithm by kaelbling        learns each bit of a complex action
separately  this algorithm presents a clear sequential structure where the learning of a
given action bit depends on all the previously learned ones  in our approach there is not
a predefined order in the learning of the outputs and the result is a more flexible learning
schema 
in multiagent learning  claus   boutilier        sen        tan        the objective
is to learn an optimal behavior for a group of agents trying to cooperatively solve a given
task  thus  in this field  as in our case  multiple actions issued in parallel have to be considered  however  one of the main issues in multiagent learning  the coordination between
the different learners is irrelevant in our case since we only have one learner 
finally  the way in which we define complex actions from elementary actions has some
points in common with the works in reinforcement learning where macro actions are defined
as the learner confronts different tasks  sutton  precup    singh        drummond        
however  the useful combinations of elementary actions detected by our algorithm are only
guaranteed to be relevant for the task at hand  although they are likely to be also relevant
for related tasks  

   experiments
we show the results of applying our learning algorithm to two robotics like simulated problems  robot landmark based navigation and legged robot walking  the first problem is
  

fireinforcement learning in categorizable environments

flowers

bushes

boat

tree

lake

goal

     
     
     
     
a 
a 
     
                                                 
                                             
a      
                                                
                                                
                                                
                                      
                                      

a 
                                      

                                           
ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 ffff

 
                                
                                           
ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff
 ff
ff
ff

                     
ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff
 ff
ff

fififififififififififififififififififififififififififififififififififififififififififififi
a 
fifififi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi
a 


 a 
fifififififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
fifififififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi








fifififi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi

    


rock

bushes

start

north

bush

figure    landscape for the simple landmark based navigation task  the landscape is divided in areas  the dashed ovals  where the same subsets of landmarks are visible 

simpler  although it includes more delayed reward  and we use it to clearly describe the
workings of the algorithm  the second problem approaches a realistic robotic application 
our objective in the long term  we use the two examples to compare the performance of
our learning system with that of generalizing and non generalizing reinforcement learning
algorithms  the confronted problems are different enough to show the generality of the
proposed learning system 
    simulated landmark based navigation
we confront a simple simulated landmark based navigation task in the forest like environment shown in figure    the objective for the learner is to go from the start position
 marked with a cross at the bottom of the figure  to the goal position where there is the
food  marked with a cross at the top right corner of the environment   the agent can
neither walk into the lake nor escape from the depicted terrain 
the agent can make use of some binary landmark  i e   feature  detectors to identify its
position in the environment and to decide which action to execute next  in the example 
the landmark detectors of the agent are 
   rock detector  active when the rock is seen 
   boat detector  active when the boat is seen 
   flower detector  active when the bunch of flowers is seen 
  

fiporta   celaya

   tree detector  active when the tree is seen 
   bush detector  active whenever a bush is seen 
   water detector  active when there is water nearby 
   bird detector  active when there is a bird flying over the agent 
   cow detector  active when there is a cow nearby 
   sun detector  active when the sun is shining 
    cloud detector  active when it is cloudy 
of these detectors  only the first   are relevant for the task  the water detector is always
active  and the rest of landmark detectors become active at random  these    landmark
detectors can differentiate between            situations 
we simplify the problem by clustering the possible positions of the learner in the environment in   areas  shown in figure     each area includes the positions from which the
same set of relevant landmarks can be seen 
as far as actions is concerned  we use three actions for the west east movement of the
robot  move to the west  denoted as w    stay in the same place     move to the east  e  
the other three indicate movement along the north south dimension  move to the north
n   stay in the same latitude   move to the south s   these two independent groups of
three actions can be combined giving rise to   different actions  move north west  north 
north east  etc    we assume that when the agent executes one of these actions  it does
not stop until the nearest area of the terrain in the direction of the movement is reached 
when the agent tries to move into the lake or out of the terrain  it remains in the same
position it was  figure   shows all the possible transitions between contiguous areas in the
environment 
with the just described landmark detectors and elementary actions the maximum possible order of a given rule is     and we can define up to                  syntactically different
partial rules  only taking into account all the rules with one feature detector and one elementary action  that are the ones initially included in the controller  we have    different
partial rules 
the agent only receives reward  with value      when it reaches the goal  consequently 
this is a problem with delayed reward since the agent must transmit the information provided by the reward signal to those actions and situations not directly related with the
observation of reward 
the parameters of the partial rule learning algorithm we used for this task were        
                                    and           see appendix a for a detailed
description of the parameters   observe that  with a maximum number of partial rules
       and an initial controller containing    rules  little room is left for the generation
of rules with order higher that   
the learning is organized in a sequence of trials  each trial consists in placing the
learner in the starting position and letting it move until the goal is reached  allowing the
execution of at most     actions to reach the goal  when performing optimally  only three
actions are required to reach the objective from the starting position 
  

fireinforcement learning in categorizable environments

   
   

steps to goal

   
   
   
  
  
  
  
 

 

  

   

   

   

   

trial
pr algorithm

xcs

figure    performance on the landmark based navigation task  results shown are the average over    runs 

figure   shows that  after    learning trials  the agent approaches the optimal behavior
 represented by the flat dashed line at y      
the dashed line in figure   is the performance of a xcs in this problem  to perform
this test  we used the implementation of wilsons xcs developed by butz         to
make xcs work in the same search space as the partial rule algorithm  we modified the
xcs implementation to be able to deal with non binary actions  no other modification 
but parameter adjustment  were introduced in the original code  the results presented
here corresponds to the average of    runs using the set of parameters that gave a better
result  nominally  these parameters were  learning rate         decay rate        
maximum number of classifiers         however  the initial set is empty   the genetic
algorithm is applied in average every   time steps  the deletion experience is    the subsume
experience is     the fall off rate is      the minimum error       a prediction threshold
of      the crossover probability is      the mutation probability      and the initial dont
care probability      the prediction and the fitness of new classifiers are initialized to   
and the error to    a detailed explanation of the meaning of those parameters is provided
by wilson        and also by the comments in the code of butz        
we can see that the xcs reaches the same performance of the partial rule approach  but
using about four times more trials  this difference in performance is partially explained by
xcss lack of generalization in the action space  however this factor is not that relevant in
this case since the action space has only two dimensions  the main factor that explains the
better performance of the partial rule approach is the bias introduced by the categorizability
  

fiporta   celaya

t
 

position
a 

v
  

 

a 

  

 

a 

   

action
 w  n  
 w   
   n  
 e  n  
 e   
 e   

winner rule
w     v rock  boat   c w  n   
w     v rock  w ater   c w   
w     v boat  t ree   c n   
w     v t ree   c e  n   
w     v rock  boat   c e  
w     v bush   c e    

qw
     
     
     
    
     
     

ew
    
    
    
   
    
   

guess
     
     
     
     
     
     

table    partial execution trace for the landmark based navigation task  elementary action  means no movement along the corresponding dimension  at each time step
t  the action with the highest guess is executed  after time step    the goal is
reached 

assumption that is not present in the xcs system and that  in this case  allows a more
efficient learning process  xcs in more powerful than our partial rule approach in the sense
that xcs makes no assumption about the categorizability of the environment  while we
assume it as high  the result of the xcs learning process includes the identification of
the degree of categorizability of the environment and in our case this is  in some sense 
pre defined  the generality of the xcs  however  produces a slower learning process 
if we initialize the classifiers in a xcs with a high dont care probability and we initialize
the rules in the partial rule algorithm so that no generalization is used in the action space
 i e   if all rules include a command for each motor   then the two systems become closer 
in this case  the main  but not the only  difference between the two approaches is the
assumption on the relation between the inputs and the value  while xcs assumes a linear
relation  we assume the environment to be categorizable  or  what is the same  we assume
the value to depend on only few of the inputs  due to this difference  when confronted to
the same problem  the two systems would learn the same policy and the same values for
each action  but the values would be computed using different rules with different associated
values  and this is so independently of the parameter rule initialization used in each case 
the system with a smaller learning time would be that with an assumption closer to the
reality  the results obtained in the particular example presented above show that the
categorizability assumption is more valid and our hypothesis is that this would be the case
in most robotics like applications 
table   shows the evaluation of some actions in the different situations the agent encounters on its path from the start to the goal after    learning trials  analyzing this trace 
we can extract some insight about how the partial rule learning algorithm works 
for instance  at time step    we see that rule w     v rock  w ater   c w    is used to
determine the value of action  w     since the landmark detector water is always active 
this rule is equivalent to w    v rock   c w     which is one of the rules used to generate w    
if we examine the statistics for w we find that qw         and ew          obviously  the
value distributions of qw and qw  look different        vs        and       vs         this
is because w  has been generated on later stages of the learning and  thus  its statistics
have been updated using a subsample of the values used to adjusts the statistics of w  in
  

fireinforcement learning in categorizable environments

this particular case  qw has been updated     times while qw  has been updated only   
times  as learning continues  both distributions will become more similar and rule w   will
be eventually eliminated 
in table    we can see that sometimes there are some non optimal actions that get
an evaluation close to the optimal ones  for this reason  the agent executes  some times 
non optimal actions and this increases the number of steps necessary to reach the goal 
in general  the adjustment of the statistics of the rules can solve this problem but  in this
particular case  we need to create new rules to fix the situation  for instance  at time step   
when the value for rule w  is increased towards     the value of rules active at this time step
and proposing actions in accordance with the action of rule w  also converge toward    
so  in the long term  a rule proposing just action  n   can get a value close to     in the
absence of more specific rules  this rule can be used to estimate the value of an action
such as    n   and  due to the probabilistic nature of the action selection procedure  this
action can  eventually  be executed delaying the agent from reaching the goal by   time
step  however  the execution of    n   results in an error in the value prediction and  thus 
in the creation of new rules to better characterize the situation  as soon as a specific rule
for action    n   is generated  the error is no longer repeated 
at time step    we see that rule w     v bush   c e     has a value of     with error
  but the guess for this rule is        this is because the maximum confidence    is lower
than           in this case  and this makes the agent to keep always a certain degree of
exploration 
if the agent only receives reward when the task is totally achieved  the function value
for each situation can be computed as v  s     n  r with n the distance  in actions  from
situation s to the target one and r the reward finally obtained  in table    we can see that
situations get the correct evaluation                           for a                  for
a   and     for a  
observe that this problem can be solved using only     partial rules out of the     
possible situation action combinations in this domain  so  we can say that the problem
is certainly categorizable  the main conclusion we can extract from this toy example is
that  in a particular case in which the confronted problem was categorizable  the presented
algorithm has been able to determine the relevant rules and to adjust their values  including
the effect of the delayed reward  so that the optimal action can be determined for each
situation 
    gait generation for a six legged robot
we also applied our algorithm to the task of learning to generate an appropriate gait  i e   the
sequence of steps  for a six legged robot  figure     to apply the learning algorithm to the
real robot would be possible  but dangerous  in the initial phases of the learning the robot
would fall down many times damaging the motors  for this reason we used a simulator
during the learning and  afterward  we applied the learned policy to the real robot 
the problem of learning to walk with a six legged robot has been chosen by many authors
before as a paradigmatic robotic learning problem  for instance  maes and brooks       
implemented a specific method based on immediate reward to derive the preconditions for
each leg to perform the step  pendrith and ryan        used a simplified version of the
  

fiporta   celaya

six legged walking problem to test an algorithm able to deal with non markovian spaces
of states and kirchner        presented a hierarchical version of q learning to learn the
low level movements of each leg  as well as a coordination scheme between the low level
learned behaviors  ilg  muhlfriedel  and berns        introduced a learning architecture
based on self organizing neural networks  and kodjabachia and meyer        proposed an
evolutionary strategy to develop a neural network to control the gait of the robot  vallejo
and ramos        used a parallel genetic algorithm architecture and parker        described
an evolutionary computation where the robot executes the best controller found up to a
given moment while a new optimal controller is computed in an off line simulation  all these
algorithms are usually tested on flat terrain with the aim of generating periodic gaits  i e  
gaits where the sequence of steps is repeated cyclically   however  for general locomotion
 turns  irregular terrain  etc  the problem of free gait generation needs to be considered 

figure    the genghis ii walking robot and its  d simulation environment 
our simulator  see figure    allows the controller to command each leg of the robot in
two independent degrees of freedom  horizontal and vertical  and it is able to detect when
the robot is in an unstable position  in our robot this happens when two neighboring legs are
in the air simultaneously   using this simulator  we implemented the behaviors described
by celaya and porta        except those in charge of the gait generation  therefore  the
task to be learned consists in deciding at every moment which legs must step  that is  leave
the ground and move to an advanced position   and which must descend or stay on the
ground to support and propel the body 
we defined a set of    feature detectors that  due to our experience on legged robots 
we knew could be useful in different situations for the gait generation task 
 in the air x   active if the leg x is in the air 
 advanced x   active if leg x is more advanced than its neighboring leg in a clockwise
circuit around the robot 
attending to the activation and non activation of these    feature detectors  we can
differentiate between      different situations 
on the action side  we work with two different elementary actions per leg  one that
issues the step of the leg and another that descends the leg until it touches the ground 
  

fireinforcement learning in categorizable environments

thus  the cardinality of the set of elementary actions is    and  at each time step  the robot
issues an action containing   elementary elements  one per leg   thus  we can think of each
leg as a virtual motor that accepts two possible values    to remain in contact with the
ground and   to perform the step 
the reward signal includes two aspects 
 stability  if an action causes the robot to fall down  a reward of    is given 
 efficiency  when the robot does not fall down  a reward equal to the distance
advanced by the robot is given  observe that when legs descend to recover contact
with the ground no advance of the robot is obtained but this movement is necessary
to be able to get reward in next time steps  so  we have again a problem with delayed
reward 
the most efficient stable gait is the tripod gait in which two sets of three non adjacent
legs step alternately  using this gait  the robot would obtain a reward of    when one group
of three legs are lifted and advanced  followed by a reward of     when the legs in contact
with the ground move backward as a reaction to the advance of legs moved in the previous
time step   thus  the optimal average reward is    
in the experiments  the robot is set in an initial posture with all the legs in contact with
the ground but in a random advance position 
figure   shows results of applying the partial rule algorithm compared with those obtained using standard q learning with      distinct states and    different actions 
for the partial rule algorithm  we used the following set of parameters            
                                      and           see appendix a for a description
of these parameters   for q learning  the learning rate is set to        and we use an
action selection rule that performs exploratory actions with probability     
in figure    we can see that the stability subproblem  i e   not falling down  which
corresponds to getting a reward greater than zero  is learned very quickly  this is because 
in the stability subproblem  we can take advantage of the generalization provided by using
separate elementary actions and  with a single rule  we can avoid executing several dangerous
actions  however  the advance subproblem  i e   getting a reward close to     is learned
slowly  this is because little generalization is possible and the learning system must generate
very specific rules  in other words  this sub problem is less categorizable than the stability
one 
as in the landmark based navigation example discussed in the previous section  we
observe that the controller contains some  slightly  overly general rules that are responsible
for the non optimal performance of the robot  however  we dont regard this as a problem
since we are more interested in efficiently learning a correct enough policy for the most
frequent situations than in finding optimal behaviors for all particular cases 
figure   shows the performance of q learning over a longer run using different exploration rates  this shows that q learning can eventually converge to an optimal policy but
with many more iterations than our approach  about a factor of      observe that a lower
exploration rate allows the algorithm to achieve higher performance  around    with a
learning rate of     and around    with learning rate       but using a longer period  with
a careful adjustment of the exploration rate we can combine an initial faster learning with
  

fiporta   celaya

  

average reward

  
 
   
   
   
   
   

 

    

    
    
time slice

pr algorithm

    

    

qlearning

figure    performance of the partial rule approach compared with standard q learning 
results are the smoothed average of    experiments 

a better convergence in the long term  experiments with q learning using learning rates
other than     showed insignificant differences compared to the results shown here 
the advantage of our algorithm over non generalizing ones is increased in problems in
which some of the sensors provide information not related to the task  to test this point 
we set up an experiment in which   feature detectors that become active randomly were
added to the    initial ones  with these new features  the number of possible combinations
of feature activations increases  and so does the number of states considered by q learning 
figure   shows the comparison between our algorithm and q learning for this problem 
q learning is not able to learn a reasonable gait strategy in the      time steps shown
in the figure  while the performance of the partial rule algorithm is almost the same as
before  this means that the partial rule algorithm is able to detect which sets of features
are relevant and use them effectively to determine the robots behavior  it is remarkable
that  in this case  the ratio of memory used by our algorithm with respect to that used by
non generalizing algorithms is below       this exemplifies how the performance of the
non generalizing algorithms degrades as the number of features increases  while this is not
necessarily the case using the partial rule approach 
the importance of the generation of partial rules in the improvement of the categorization can be seen comparing the results obtained for the same problem with and without
this mechanism  figure     the results show that the task cannot be learned using only
partial rules of order    the only aspect of the gait generation problem that can be learned
with rules of order   is to avoid lifting a leg if one of its neighboring legs is already in the
  

fireinforcement learning in categorizable environments

  

average reward

  
 
   
   
   
   
   

 

     

exploration    

      
      
time slice

      

exploration     

      

references

figure    performance of the q learning algorithm with different exploration rates  the
reference values    and    are the upper bound of the performance attainable
when using exploration rate     and      

  

average reward

  
 
   
   
   
   
   

 

    

    
    
time slice

pr algorithm

    

    

qlearning

figure    performance of our algorithm compared with q learning when there are irrelevant
features 

  

fiporta   celaya

  

average reward

  
 
   
   
   
   
   

 

    

    
    
time slice

without generation

    

    

with generation

figure    performance with and without the partial rule generation procedure 
air  for instance  a rule such as
v in the air      c step     
forecasts a highly relevant negative reward and this prevents leg   from being raised when
leg   is in the air 
rules with order higher than    i e   not provided to the robot in the initial controller 
are necessary  for instance  to avoid raising two neighboring legs simultaneously  a rule like
v in the air      c step     step    
becomes active when the robot evaluates any action that implies raising leg   and leg  
at the same time  since the value prediction of this rule is very negative and its relevance
is high  the action under evaluation would be discarded  preventing the robot from falling
down  similar rules have to be generated for each pair of neighboring legs  to make the
robot advance  we need to generate rules with even higher order 
in figure    we can see the performance of the algorithm when we start the learning
process from a correct rule set  i e   a rule set learned in a previous experiment   but with
all the statistics initialized to    in this experiment  we can compare the complexity of
learning only the values for the rules compared with the complexity of learning the rules
and their value at the same time  we can see that when only the values for the rules need
to be learned the learning process is about two times faster than in the normal application
of the algorithm 
in a final experiment  we issue frequent changes in the heading direction of the robot
 generated randomly every    time steps   in this way  periodic gaits become suboptimal
   

fireinforcement learning in categorizable environments

  

average reward

  
 
   
   
   
   
   

 

   

    

    

    
    
time slice

pr from a correct rule set

    

    

    

pr algorithm

figure    performance of the partial rule approach when learning is started from a correct
rule set compared with the standard approach where rules are also learned 

and the controller should produce a free gait  i e   a gait that includes a sequence of steps
without any periodic repetition 
in this case  we focus on the advance subproblem and  thus  we introduced some handcrafted rules to the initial controller to prevent the robot from falling down  these rules
are of the form 
if leg i is lifted then execution of action a results in value    with confidence   
where a is any of the actions that lift one of the two legs that are contiguous to i 
the set of parameters we used in this case was                                
                 and          
figure   shows the average results obtained using the partial rule learning algorithm
compared with those obtained with our best hand coded gait generation strategy  in the figure  the horizontal dashed line shows the average performance using the best gait generation
strategy we have implemented  celaya   porta         it can be seen that the learned gaitgeneration strategy  the increasing continuous line  produces a performance similar to that
of our best hand coded strategy and that  in some cases  it even outperforms it  figure   
shows a situation where a learned controller produces a better behavior than our hand
coded one  using the hand coded strategy  the robot starts to walk raising two legs    and
   and  in few time steps it reaches a state from which the tripod gait is generated  initially 
leg   is more advanced than legs   and   and  in general  it is suboptimal to execute a step
with a leg when its neighboring legs are less advances that itself  in this particular case
however  this general rule does not hold  the learned strategy detects this exception and
   

fiporta   celaya

  

average reward

  
  
  
 
 

 

    

    
    
time slice

pr algorithm

    

    

hand coded

figure    performance of the partial rule approach when learning a free gait 
generates the tripod gait from the very beginning resulting in a larger advance of the robot
in the initial stages of the movement 

   conclusions
in this paper  we have introduced the categorizability assumption that states that a robot
can be driven to achieve a given task using only simple rules  i e   rules including a reduced
set of feature detectors and elementary actions  this assumption is supported by our
experience within the behavior based approach where controllers are formed by sets of rules
with relatively simple conditions and actions  we have shown that a learning algorithm
based on the categorizability assumption allows a large speed up in the learning process in
many realistic robotic applications with respect to existing algorithms 
to exploit the categorizability assumption both in the observations and action spaces 
we have introduced a new representation formalism based on the concept of partial rules
and not on the concepts of independent states and independent actions that are the kernel
of many existing reinforcement learning approaches 
the introduction of the partial rule concept provides a large flexibility on how problems
are formalized  with the same structure and algorithms  we can confront problems with
generalization in the perception side  usually considered in reinforcement learning   in the
action side  usually not considered   or in both of them 
when no generalization is possible at all via partial rules  we have to use complete rules 
rules involving all the available inputs and outputs  in this case  the partial rule approach is
equivalent to the non generalizing reinforcement learning  the algorithm we have presented
   

fireinforcement learning in categorizable environments

leg numbering
 
 

 

 

step    

step      

  

  

 

 

 

 

step      

step      

  

  

step      

step      

   

   

figure     the hand programmed gait strategy  top sequence  vs  a learned one  bottom
sequence   the advance position of the robot at each snapshot is indicated below
each picture 

can  if necessary  generate complete rules and  consequently  it can  in principle  solve any
problem that can be solved using a traditional reinforcement learning algorithm  however 
we take the categorizability assumption as valid and so  the generation of complete rules
is an extreme case that is only likely to occur in a very limit situation  therefore  in our
approach  we forego generality in order to increase the efficiently of the learning process in
the class of problems we want to address 
another advantage of the partial rule framework is that it allows the easy and robust
introduction of initial knowledge in the learning process in the form of rules that are easily understood by the programmer  this is in contrast with usual reinforcement learning
algorithms where the introduction of initial knowledge is  in general  rather difficult 
in the partial rule approach  there is a subtle change of emphasis as to the main goal
of learning  while in most work in reinforcement learning the emphasis is on learning the
value of each action in each state  our main purpose is to learn the relevance of  subsets of 
elementary actions and feature detectors  if the relevant subsets of elementary actions and
feature detectors are identified  the learning becomes straightforward 
   

fiporta   celaya

the main limitation of our work is that it is not possible to know a priori  except for trivial cases  whether or not an environment is categorizable by a given robot  non generalizing
reinforcement learning implicitly assumes that the environment is non categorizable and
that  consequently  all the possible combination of features and actions have to be taken
into account separately  our approach assumes just the opposite  that the environment is
categorizable and  so  only reduced combinations of features and actions need to be taken
into account  the drawback of using the non generalizing approach is that robotic tasks
become intractable because of the curse of dimensionality  with generalization techniques
this problem can be partially alleviated  but not enough in general  in our approach we take
a more radical approach in order to be much less affected by the curse of dimensionality  we
introduce a strong bias in the learning process to drastically limit the use of combinations
of features and actions 
we have tested the partial rule learning algorithm in many robotic inspired problems
and two of them have been discussed in this paper  landmark based navigation and sixlegged robot gait generation  and the categorizability assumption proved to be valid in all
cases we tested  the algorithm out performs generalizing and non generalizing reinforcementlearning algorithms in both memory requirements and convergence time  additionally  we
have shown that our approach scales well when the number of inputs increases  while the
performance of existing algorithms is largely degraded  this is a very important result that
lets us think that it could be possible to use our approach to control more complex robots 
while the use of existing approaches has to be discarded 
from the work presented in this paper  we can extract two main proposals  first 
to apply reinforcement learning to agents with many sensors and actuators  we should
concentrate our efforts in determining the relevance of inputs and outputs and  second 
to achieve efficient learning in complex environments it could be necessary to introduce
additional assumptions into the reinforcement learning algorithms  even at the risk of losing
generality 

acknowledgments
the authors would like to express their gratitude to the anonymous reviewers of the paper 
their contributions toward improving the quality of this paper are relevant enough to be
considered  in some sense  as a co authors of the paper  the shortcomings still in the paper
can only be attributed to the nominal authors 
the second author has been partially supported by the spanish ministerio de ciencia y tecnologa and feder funds  under the project dpi           c      of the plan
nacional de i d i 

   

fireinforcement learning in categorizable environments

appendix a  the partial rule learning algorithm
in this appendix  we describe in detail the approach described in the main body of the
paper 

partial rule learning algorithm
 initialize 
f d  set of features detectors
ea  set of elementary actions
c   w      v fd   c ea     v fd   c ea   fd  f d  ea  ea 
for each w in c
qw   
ew   
iw   
endfor
e 
do for each episode
c     w  c w is active 
repeat  for each step in the episode  
 action selection 
action evaluation
 computes guess a    a   
 
a  arg max
 guess a   
 
a

execute a
 system update 
ra  reward generated by a
 
cant
 c 
 
c   w  c w is active 
statistics update
partial rule management
until terminal situation
enddo

figure     the partial rule learning algorithm  text inside parentheses are comments  the
action evaluation  statistics update  and partial rule management procedures
are described next 

the partial rule learning algorithm  whose top level form is shown in figure     stores
the following information for each partial rule
 the value  i e   the discounted cumulative reward  estimation qw  
 the error estimation ew   and
 the confidence index iw  
   

fiporta   celaya

   
   





   
   

cw

   
   
   
   
   
   
 

 

 

 

 

iw

 

 



 

 

  

figure     confidence function with    and      
to estimate the confidence on qw and ew we use a confidence index iw that  roughly
speaking  keeps track of the number of times the partial rule is used  the confidence is
derived from iw using a confidence function in the following way 
cw  confidence function iw   
where the confidence function is a non decreasing function in the range         should be
less than   since  in this way  the system always keeps a certain degree of exploration and 
consequently  it is able to adapt to changes in the environment  different confidence schemes
can be implemented by changing the confidence function  in our implementation  we use
a sigmoid like function  see figure     that increases slowly for low values of i w reducing
the confidence provided by the first obtained rewards  in this way we avoid a premature
increase of the confidence  and  thus  a decrease in the error and in the exploration  for
insufficiently sampled rules  a parameter    determines the point at which this function
reaches the top value  
additionally  the confidence index is used to define the learning rate  i e   the weight
of new observed rewards in the statistics update   for this purpose we implement a mam
function  venturini        for each rule 
mw   max      iw       
using a mam based updating rule  we have that  the lower the confidence  the higher
the effect of the last observed rewards on the statistics  and the faster the adaptation of the
statistics  this adaptive learning rate strategy is related to those presented by sutton       
and by kaelbling         and contrasts with traditional reinforcement learning algorithms
where a constant learning rate is used 
after the initialization phase  the algorithm enters in a continuous loop for each task
episode consisting in estimating the possible effects of all actions  executing the most promis   

fireinforcement learning in categorizable environments

action evaluation
for each action a 
w  winner c     a   
guess a     qw     random w   w  
endfor

figure     action evaluation procedure 
ing one  and updating the system so that its performance improves in the future  the system
update includes the statistics update and the partial rule management 
action evaluation
the simplest procedure to get the estimated value for actions is a brute force approach
consisting of the independent evaluation of each one of them  in simple cases  this approach
would be enough but  when the number of valid combinations of elementary actions  i e  
of actions  is large  the separate evaluation of each action would take long time  increasing
the time of each robot decision and decreasing the reactivity of the control  to avoid this 
appendix b presents a more efficient procedure to get the value of any action 
figure    summarizes the action evaluation procedure using partial rules  the value for
each action is guessed using the most relevant rule for this action  i e   the winner rule  
this winner rule is computed as
winner  c     a   arg

max  w   

wc    a 

where w is the relevance of rule w
w  

 
 
    w

the value estimation using the winner rule is selected at random  uniformly  from the
interval
iw    qw   w   qw    w   
with
w   ew cw   e     cw   
here  e is the average error on the value prediction  i e   the value error prediction of the
empty rule  w   
statistics update
in the statistics update procedure  figure      qw and ew are adjusted for all rules that
were active in the previous time step and proposed a partial command in accordance with
a  the last executed action  
   

fiporta   celaya

statistics update
if terminal situation then
v 
else
v  max
 qw  w   winner c     a    
 
a

endif
q  ra    v
 
for each w    v  c  in cant
if c is in accordance with a then
if q  iw then
iw  iw    
else
iw  min      iw    
endif
qw  qw     mw     q mw
ew  ew     mw      qw  q  mw
endif
endfor
e  ew

figure     statistics update procedure 

both qw and ew are updated using a learning rate  mw   computed using the mam
function  which initially is    and consequently  the initial values of qw and ew have no
influence on the future values of these variables  these initial values become relevant when
using a constant learning rate  as many existing reinforcement learning algorithms do 
if the observed effects of the last executed action agree with the current estimated
interval for the value  iw    then the confidence index is increased by one unit  otherwise 
the confidence index is decreased allowing a faster adaptation of the statistics to the last
obtained  surprising values of reward 
partial rule management
this procedure  figure     includes the generation of new partial rules and the removal of
previously generated ones that proved to be useless 
in our implementation  we apply a heuristic that produces the generation of new partial
rules when the value prediction error exceeds e  in this way  we concentrate our efforts to
improve the categorization on those situations with larger errors in the value prediction 
every time a wrong prediction is made  at most  new partial rules are generated by
   a   recall that this set includes the
combination of pairs of rules included in the set cant
rules active in the previous time step and in accordance with the executed action a  thus 
these are the rules related with the situation action whose value prediction we need to
improve 
   

fireinforcement learning in categorizable environments

the combination of two partial rules w   w  consists of a new partial rule with a partial
view that includes all the features included in the partial views of either w  or w  and with a
partial command that includes all the elementary actions of the partial commands of either
w  or w    in other words  the feature set of w   w  is the union of the feature sets in w 
and in w  and the elementary actions in w   w  are the union of those in w  and those in
   a   they have been simultaneously active
w    note that  since both w  and w  are in cant
and they are in accordance with the same action and  thus  they can not be incompatible
 i e   they can not include inconsistent features or elementary actions  
in the partial rule creation  we bias our system to favor the combination of those rules
 wi   whose value prediction  qwi   is closer to the observed one  q   finally  the generation
of rules lexicographically equivalent to already existing ones is not allowed 
according to the categorizability assumption  only low order partial rules are required
to achieve the task at hand  for this reason  to improve efficiency  we limit the number of
partial rules to a maximum of   however  our partial rule generation procedure is always
generating new rules  concentrating on those situations with larger error   therefore  when
we need to create new rules and there is no room for them  we must eliminate the less useful
partial rules 
a partial rule can be removed if its value prediction is too similar to some other rule in
the same situations 
the similarity between two rules can be measured using the normalized degree of intersection between their value distributions and the number of times both rules are used
simultaneously 
similarity w  w      

u  w  w   
kiw  iw  k
 
max kiw k  kiw  k  min u  w   u  w     

where u  w  indicates the number of times rule w is actually used 
the similarity assessment for any pair of partial rules in the controller is too expensive
and  in general  determining the similarity of each rule with respect to those from which
it was generated  that are the rules we tried to refine when the new rule was created  is
sufficient  thus  based on the above similarity measure  we define the redundancy of a
partial rule w    w   w    as 
redundancy w    max similarity w  w     similarity w  w     
observe that with w    w   w     we have that w  w    w and u  w   u  w    
therefore
u  w  w   
u  w 
u  w 
 
 
    
min u  w   u  w    
min u  w   u  w    
u  w 
the same reasoning can be done with w  and  consequently 
redundancy w    max 

kiw  iw  k
kiw  iw  k
 
  
max kiw k  kiw  k  max kiw k  kiw  k 

when we need to create new rules but the maximum number of rules    has been
reached  the partial rules with a redundancy above a given threshold    are eliminated 
since the redundancy of a partial rule can only be estimated after observing it a number of
   

fiporta   celaya

partial rule management
 
w  winner cant
  a 
if  qw  q    e
 if it is time to create new rules 
 partial rule elimination 
 test if there is no room for new rules 
if kck      then
 rule elimination based on redundancy 
c  c   w  c   redundancy w     
 rule elimination based on creation error 
if kck      then  if there is still no room 
sc  the  partial rules from c with 
  lowest creation error w   and
  creation error w     qw  q 
c  c  sc
endif
endif
 partial rule generation 
t 
while kck    and t   
 create a new rule w    
 
 a 
select two different rules w    w  from cant
preferring those that minimize
 qwi  q  cwi   e     cwi  
w     w   w   
creation error w       qw  q 
 insert the new rule in the controller 
c  c   w    
tt  
endwhile
endif

figure     partial rule management procedure  the value of q is calculated in the statistics
update procedure and a is the last executed action 

times  the redundancy of the partial rules with low confidence indexes is set to    so that
they are not immediately removed after creation 
observe that  to compute the redundancy of a rule w  we use the partial rules from
which w was derived  for this reason  a rule w   cannot be removed from a controller c if
there exists any rule w  c such that w   w    w     additionally  in this way we eliminate
first the useless rules with higher order 

   

fireinforcement learning in categorizable environments

appendix b  efficient action evaluation
in non generalizing reinforcement learning the cost of executing a single learning step can be
neglected  however  algorithms with generalization in the spaces of sensors and or actuators
are not so simple and the execution time of each iteration can be increased substantially 
in an extreme case  this increase can limit the reactivity of the learner and this is very
dangerous when working with an autonomous robot 
the most expensive procedure of our algorithm is that of computing the value of all
actions  i e   all valid combinations of elementary actions   the cost of this procedure is
especially critical since it is used twice in each step  once to get the guess of each action
 in the action evaluation procedure detailed in figure     and again to get the goodness
of the new achieved situation after the action execution  when computing the v value in
the statistics update procedure detailed in figure      a trivial re order of the algorithm
can avoid the double use of this expensive procedure at each learning step  we can select
the action to be executed next at the same time that we evaluate the goodness of the new
achieved situation  the drawback of this re order is that the action is selected without
taking into account the information provided by the last reward value  the goodness of the
situation is assessed before the value adjustment   however  this is not a problem in tasks
that require many learning steps 
even if we use the action evaluation procedure only once per learning step  we have
to optimize it as much as possible since the brute force approach described before  which
evaluates each action sequentially  is only feasible for simple problems 
the action evaluation method presented next is based on the observation that many
of the actions would have the same value since the highest relevant partial rule at a given
moment would provide the value to all actions that are in accordance with the partial
command of the rule  the separate computation of the value of two actions that would end
up evaluated using the same rule is a waste of time  this can be avoided by performing the
action evaluation attending to the set of active rules in the first place and not to the set of
possible actions  as the brute force approach does 
figure    shows a general form of the algorithm we propose  in this algorithm  partial
rules are considered one at a time  ordered from the most relevant rule to the least relevant
one  the partial command of the rule under consideration  cow   is used to process all the
actions that are in accordance with that partial command  this already processed sub set
of actions need not to be considered any more in the action evaluation procedure  while
the rules are processed  we update the current situation assessment  v  and the action to be
executed next  a  attending  respectively  to the value prediction  qw   and the guess  gw  
of the rules 
observe that partial rules can be maintained sorted by relevance by the statistics update
procedure  since it is in this procedure where rule relevance is modified  when the relevance
of a rule is changed  its position in the list can be also modified accordingly  in this way
we do not have to re sort the list of rules every time we want to apply the procedure just
described 
when elementary actions are of the form  m  k  with m a motor and k a value in
the range of possible values for that motor  the above algorithm can be implemented in an
especially efficient way since there is no need to explicitly compute the set of actions a 
   

fiporta   celaya

action evaluation
 initialization 
l  list of active rules sorted by relevance 
ea  set of elementary actions
a  set of combinations of ea
v  
 situation assessment 
a
 optimal action 
g  
 optimal action value prediction 
 process 
w first element l 
do
cow  partial command of w
gw  qw     random w   w  
aw   a  a cow in accordance with a 
if qw   v then
v  qw
endif
if gw   g then
g  gw
a  cow
endif
a  a  aw
w next element l 
until a    

figure     general form of the proposed situation assessment and action selection procedure 

in this case  see figure    and      we construct a decision tree using motors as decision
attributes and that groups in the same leaf all those actions evaluated by the same partial
rule  all actions removed from the set a in each iteration of the algorithm in figure     
each internal node of the tree classifies the action according to one of the motor commands included in the action  these internal nodes store the following information 
 partial command  a partial command that is in accordance with all the action classified under the node  this partial command can be constructed by collecting all the
motors whose values are fixed in the nodes from the root of the tree to the node under
consideration 
 motor  the motor used in this node to classify actions  when a node is open  i e  
we have still not decided to which motor to attend  the motor value is set to a  
a node can be closed by deciding which motor to pay attention to  and adding the
corresponding subtrees  or by converting the node into a leaf 
   

fireinforcement learning in categorizable environments

action evaluation
 initialization 
l  list of active rules sorted by relevance 
v  
a
g  
tree  new node c  
open   
closed   
 process 
w first element l 
do
gw  qw     random w   w  
include rule tree  w  gw  
w next element l 
until closed   open

figure     top level algorithm of the efficient action evaluation algorithm  at the end of the
algorithm  v is the goodness of the current situation to be used in the statistics
update algorithm  see figure      a is the action to be executed next and guess
its expected value  the include rule procedure is detailed in next figure 

 subtrees  this is a list of the subtrees that start in that node  each subtree has an
associated value that corresponds to one of the possible actions executable by the motor of the node  all the actions included in a given subtree have an elementary action
such as  m  k  where m is the motor of the node and k is the value corresponding
to this subtree 
the leaves of the tree have information about the value of the actions classified in that
leaf  this information is represented with the following set of attributes for each leaf 
 value  the expected value for all the actions classified in this leaf  the maximum of
this value for all leaves is used to assess the goodness  v  of a new achieved situation 
 guess  the value altered with noise for exploratory reasons  the leaf with a maximal
guess is the set of actions from where to select the action to be executed next 
 relevance  the relevance of the value predictions  of both the value and the guess  
 partial command  a partial command that is in accordance with all the actions
classified in that leaf  as in the case of internal nodes  this partial command can be
constructed by collecting all the motors whose values are fixed from the root of the
tree to the leaf under consideration 
   

fiporta   celaya

include rule n  w  gw  
if not is leaf n   then
cow  command w 
con  command n 
if motor n     then
 closed node  search for compatible sub nodes 
if ea  cow with motor ea    motor n  then
include rule get subtree value ea   n   w  gw  
else
for all s in subtrees n  do
include rule s  w  gw  
endfor
endif
else
 open node  specialize the node 
if cow  con     then
 extend a node 
ea  action in cow  con  
set motor n  motor ea  
closed  closed    
for all k in values motor ea   do
new subtree n   k  new node con   motor ea   k    
open  open    
endfor
include rule n  w  gw  
else
 transform a node into a leaf  
transform to leaf n  qw   gw   w   cow  
closed  closed    
if qw   v then
v  qw
endif
if gw   guess then
g  gw
a  cow
endif
endif
endif
endif

figure     the include rule algorithm searches for nodes from node n with a partial command compatible with the partial command of rule w and extends those nodes
to insert a leave in the tree 

at a given moment  the inclusion of a new partial rule in the tree produces the specialization of all open nodes compatible with the rule  see figure      we say that an open
node n is compatible with a given rule w if the partial command of the node con and the
partial command of the rule cow does not assign different values to the same motor  the
specialization of an open node can result in the extension of the node  i e   new branches
   

fireinforcement learning in categorizable environments

partial rules
partial view
partial command
t ru ev
 m   v      m   v   
t ru ev
 m   v   
t ru ev
 m   v      m   v   
t ru ev
 m   v   
t ru ev
 m   v   
t ru ev
 m   v   
t ru ev
 m   v   
t ru ev
 m   v   

q

e
 
 
 
 
 
  
 
 


   
   
   
   
   
   
   
   

    
    
    
    
    
    
    
    

guess
   
   
   
   
   
   
   
    

table    set of rules of the controller  the values q and e are stored and the  and guess
are computed from them  we define all partial views as t ru ev to indicate that
they are active in the current time step 

are added to the tree under that node  or in the transformation of this node into a leaf  a
node is extended when the partial command of the rule affects some motors not included in
the partial command of the node  this means that there are some motor values not taken
into account in the tree but that have to be used in the action evaluation according to the
rule under consideration  when a node is extended  one of the motors not present in the
above layers of the tree is used to generate a layer of open nodes in the current node  after
that  the node is considered as closed and the inclusion rule procedure is repeated for this
node  with different effects because now the node is closed   when all the motors affected
by the partial command of the rule are also affected by the partial command of the node 
then the node is transformed into a leaf storing the value  guess  and relevance attributes
extracted from the information associated with the rule 
the process is stopped as soon as we detect that all nodes have been closed  i e  all the
external nodes of the tree are leaves   in this case  the rules still to be processed can have
no effect in the tree form and  consequently are not useful for action evaluation  if a rule is
consistently not used for action evaluation  it can be removed from the controller 
a toy size example can illustrate this tree based action evaluation algorithm  suppose
that we have a robot with three motors that accept two different values  named v   and
v     this produces a set of   different actions  suppose that  at a given moment  the robot
controller includes the set of rules shown in table    in the action evaluation algorithm
 figure      rules are processed from the most to the least relevant one expanding an
initially empty tree using algorithm in figure     the inclusion of a rule in the tree results
in an extension of the tree  see stages b  d and e in figure     or in closing branches by
converting open nodes into leaves  stages c and f   in this particular case the tree becomes
completely closed after processing   rules out of the   active rules in the controller  at
the end of the process  we have a tree with five leaves  three of them include two actions
and the other two only represent a single action  using the tree we can say that the value
of the situation in which the tree is constructed  v  is    this is given by the leaf circled
with a solid line in the figure   additionally  the next action to be executed is of the form
   

fiporta   celaya

motor  m 
command 
true c

b

a
open
node

v 

v 

motor  m 
command 
 m  v  
v 

open
node

v 
value   
guess     
relevance      
command 
 m  v  
 m  v  

open
node

c

motor  m 
command 
 m  v  
v 

open
node

value   
guess     
relevance      
command 
 m  v  
 m  v  

e

open
node

v 

v 

v 

open
node

value   
guess     
relevance      
command 
 m  v  
 m  v  

motor  m 
command 
 m  v  
 m  v  
v 
v 

v 
motor  m 
command 
 m  v  
 m  v  
v 
value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

v 

v 

v 

value   
guess     
relevance      
command 
 m  v  
 m  v  

motor  m 
command 
 m  v  
v 

motor  m 
command 
 m  v  
v 

v 
value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  

motor  m 
command 
true c

v 
motor  m 
command 
 m  v  
v 

v 

value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

open
node

f

motor  m 
command 
 m  v  

motor  m 
command 
 m  v  
v 

motor  m 
command 
 m  v  

motor  m 
command 
true c
v 

v 

v 

v 

v 

v 

motor  m 
command 
true c

d

motor  m 
command 
true c

v 
motor  m 
command 
 m  v  
 m  v  
v 
value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

v 

value   
guess     
relevance      
command 
 m  v  
 m  v  

v 
value   
guess     
relevance      
command 
 m  v  
 m  v  

value   
guess     
relevance      
command 
 m  v  
 m  v  
 m  v  

figure     six different stages during the construction of the tree for action evaluation 
each stage corresponds to the insertion of one rule from table   

   

fireinforcement learning in categorizable environments

 
 

log time 

 
 
 
 
 
 
 

 

 

 
 
number of void motors

brute force evaluation

 

 

treebased evaluation

figure     log of the execution time  in seconds  of the brute force approach vs  the treebased one 

 m   v    m   v    m      where   represents any possible action  this optimal action
is given by the leaf circled with a dashed line that is the leaf with a larger guess value 
the cost of our algorithm largely depends on the specific set of partial rules to be
processed  in the worst case  the cost of our algorithm is 
o nr lnm   
with nr the number of rules  nm the number of motors and  l the maximal range of values
accepted by the motors  this is because  in the worst case  to insert a given rule  we have to
visit all the nodes of a maximally expanded tree  i e   a tree where each node has l subtrees
and where all the final nodes of the branches are still opened   the number of nodes of
such a tree is
nm
x
lnm      
li  
  o lnm   
l 
i  

we can transform the cost expression taking into account that l nm is the total number of
possible combinations of elementary actions  nc   or  in other words  the total amount of
actions  therefore  the cost of the presented algorithm is
o nr nc   
on the other hand  the cost of the brute force approach is always
 nr nc   
   

fiporta   celaya

so  in the worst case  the cost of the presented algorithm is of the same order as the cost
of the brute force approach  however  since at most l rules would be enough to close a
maximally expanded tree  one rule for the different values of the motor used in the last
still open layer of the tree   the cost of the tree based algorithm would be  on average 
much smaller than that of the brute force approach 
figure    exemplifies the different performance of the brute force action evaluation procedure and the tree based one  the figure shows the time taken in the execution of the
toy example of section      for this experiment  we defined some void motors or motors
whose actions have no effect in the environment  as it can be seen  as the number of void
motors increases  the cost of the tree based evaluation is significantly less than that of the
brute force approach 

   

fireinforcement learning in categorizable environments

appendix c  notation
uppercase are used for sets  and greek letters represent parameters of the algorithms 
s
set of states 
 
s  s
individual states  full views 
ns
number of states 
f d    fdi   i      nf  
set of feature detectors 
partial view of order k 
v fdi            fdik  
a
set of actions of the robot 
na
number of actions 
ea    eai   i      ne  
set of elementary actions 
nm
number of motors of the robot 
eai    mi  k 
elementary action that assigns value k to motor mi  
c eai            eaik  
partial command of order k 
a    ea            eanm  
action  combination of elementary actions  full command 
w    v  c 
partial rule composed by partial view v and partial command c 
w
the empty partial rule 
w   w  
composition of two partial rules 
c    wi   i      nr  
controller or set of partial rules 

maximum number of elements of c 
 
 
c   cant
subset of rules active at a given time step and at the previous one 
c    a 
active rules with a partial command in accordance with a 
qw
expected value of the partial rule w 
ew
expected error in the value estimation of the partial rule w 
e
average error in the value prediction 
iw
confidence index 
cw
confidence on the statistics of the partial rule w 

top value of the confidence 

index where the confidence function reaches the value  
w   ew cw   e     cw   error in the return prediction of the partial rule w 
w          w  
relevance of rule w 
iw    qw   w  
value interval of the partial rule w 
mw
updating ratio for the statistics of the partial rule w 

learning rate  top value of mw  
u  w 
number of times rule w has been used 
 
winner c   a 
most relevant active partial rule w r t  action a 
guess a 
most reliable value estimation for action a 
ra
reward received after the execution of a 

discount factor 
v
goodness of a given situation 
q   ra   v
value of executing action a in given situation 

number of new partial rules created at a time 

redundancy threshold used for partial rule elimination 

   

fiporta   celaya

references
arkin  r  c          behavior based robotics  intelligent robotics and autonomous agents 
the mit press 
bellman  r  e          dynamic programming  princeton university press  princeton 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research 
        
brooks  r  a          intelligence without representation  artificial intelligence         
    
butz 

m         
c xcs  an implementation of
 http   www cs bath ac uk  amb lcsweb computer htm  

the

xcs

in

c 

celaya  e     porta  j  m          control of a six legged robot walking on abrupt terrain 
in proceedings of the ieee international conference on robotics and automation 
pp           
celaya  e     porta  j  m          a control structure for the locomotion of a legged
robot on difficult terrain  ieee robotics and automation magazine  special issue on
walking robots              
chapman  d     kaelbling  l  p          input generalization in delayed reinforcement
learning  an algorithm and performance comparisons  in proceedings of the international joint conference on artificial intelligence  pp         
claus  c     boutilier  c          the dynamics of reinforcement learning in cooperative
multiagent systems  in proceedings of the fifteenth national conference on artificial
intelligence  pp          american association for artificial intelligence 
drummond  c          accelerating reinforcement learning by composing solutions of automatically identified subtasks  journal of artificial intelligence research            
edelman  g  m          neuronal darwinism  oxford university press 
hinton  g   mcclelland  j     rumelhart  d          parallel distributed processing  explorations in the microstructure of cognition  volume    foundations  chap  distributed
representations  mit press  cambridge  ma 
ilg  w   muhlfriedel  t     berns  k          hybrid learning architecture based on neural
networks for adaptive control of a walking machine  in proceedings of the      ieee
international conference on robotics and automation  pp           
kaelbling  l  p          learning in embedded systems  a bradford book  the mit press 
cambridge ma 
kaelbling  l  p   littman  m  l     moore  a  w          reinforcement learning  a survey 
journal of artificial intelligence research              
kanerva  p          sparse distributed memory  mit press  cambridge  ma 
kirchner  f          q learning of complex behaviors on a six legged walking machine 
robotics and autonomous systems             
   

fireinforcement learning in categorizable environments

kodjabachia  j     meyer  j  a          evolution and development of modular control
architectures for   d locomotion in six legged animats  connection science        
    
maes  p     brooks  r  a          learning to coordinate behaviors  in proceedings of the
aaai     pp         
mahadevan  s     connell  j  h          automatic programming of behavior based robots
using reinforcement learning  artificial intelligence             
mccallum  a  k          reinforcement learning with selective perception and hidden
state  ph d  thesis  department of computer science 
parker  g  b          co evolving model parameters for anytime learning in evolutionary
robotics  robotics and autonomous systems           
pendrith  m  d     ryan  m  r  k          c trace  a new algorithm for reinforcement
learning of robotic control  in proceedings of the      international workshop on
learning for autonomous robots  robotlearn    
poggio  t     girosi  f          regularization algorithms for learning that are equivalent
to multilayer networks  science  pp         
schmidhuber  j          the speed prior  a new simplicity measure yielding near optimal
computable predictions  in proceedings of the   th annual conference on computational learning theory  colt  oo    lecture notes in artificial intelligence 
springer   pp         
sen  s          learning to coordinate without sharing information  in proceedings of
the twelfth national conference on artificial intelligence  pp          american
association for artificial intelligence 
sutton  r  s          reinforcement learning architectures for animats  in meyer  j  a    
wilson  s  w   eds    proceedings of the first international conference on simulation of adaptive behavior  from animals to animats  pp          the mit press 
bradford books 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  a bradford
book  the mit press 
sutton  r  s     whitehead  s  d          online learning with random representations 
in proceedings of the eleventh international conference on machine learning  pp 
        morgan kaufman  san francisco  ca 
sutton  r          generalization in reinforcement learning  successful examples using
sparse coarse coding  in proceedings of the      conference on advances in neural
information processing  pp           
sutton  r   precup  d     singh  s          between mdps and semi mdps  a framework
for temporal abstraction in reinforcement learning  artificial intelligence             
tan  m          multi agent reinforcement learning  independent vs  cooperative agents 
in reading in agents  pp          morgan kaufmann publishers inc 
   

fiporta   celaya

vallejo  e  e     ramos  f          a distributed genetic programming architecture for
the evolution of robust insect locomotion controllers  in meyer  j  a   berthoz  a  
floreano  d   roitblat  h  l     wilson  s  w   eds    supplement proceedings of the
sixth international conference on simulation of adaptive behavior  from animals to
animats  pp          the international society for adaptive behavior 
venturini  g          apprentissage adaptatif et apprentissage supervise par algorithme
genetique  ph d  thesis 
watkins  c  j  c  h     dayan  p          q learning  machine learning            
widrow  b     hoff  m          adaptive switching circuits  in western electronic show
and convention  volume    pp         institute of radio engineers  now ieee  
wilson  s  w          classifier fitness based on accuracy  evolutionary computation    
       
wilson  s  w          explore exploit strategies in autonomy  in from animals to animats    proceedings of the  th international conference on simulation of adaptive
behavior  pp         

   

fi