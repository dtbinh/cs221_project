journal of artificial intelligence research                  

submitted       published     

an improved search algorithm
for optimal multiple sequence alignment
stefan schroedl

stefan schroedl gmx de

      th st
san francisco ca      
                 

abstract
multiple sequence alignment  msa  is a ubiquitous problem in computational biology 
although it is np  hard to find an optimal solution for an arbitrary number of sequences 
due to the importance of this problem researchers are trying to push the limits of exact
algorithms further  since msa can be cast as a classical path finding problem  it is attracting a growing number of ai researchers interested in heuristic search algorithms as a
challenge with actual practical relevance 
in this paper  we first review two previous  complementary lines of research  based on
hirschbergs algorithm  dynamic programming needs o kn k    space to store both the
search frontier and the nodes needed to reconstruct the solution path  for k sequences of
length n   best first search  on the other hand  has the advantage of bounding the search
space that has to be explored using a heuristic  however  it is necessary to maintain all
explored nodes up to the final solution in order to prevent the search from re expanding
them at higher cost  earlier approaches to reduce the closed list are either incompatible
with pruning methods for the open list  or must retain at least the boundary of the closed
list 
in this article  we present an algorithm that attempts at combining the respective
advantages  like a it uses a heuristic for pruning the search space  but reduces both
the maximum open and closed size to o kn k     as in dynamic programming  the
underlying idea is to conduct a series of searches with successively increasing upper bounds 
but using the dp ordering as the key for the open priority queue  with a suitable choice
of thresholds  in practice  a running time below four times that of a can be expected 
in our experiments we show that our algorithm outperforms one of the currently most
successful algorithms for optimal multiple sequence alignments  partial expansion a   both
in time and memory  moreover  we apply a refined heuristic based on optimal alignments
not only of pairs of sequences  but of larger subsets  this idea is not new  however  to
make it practically relevant we show that it is equally important to bound the heuristic
computation appropriately  or the overhead can obliterate any possible gain 
furthermore  we discuss a number of improvements in time and space efficiency with
regard to practical implementations 
our algorithm  used in conjunction with higher dimensional heuristics  is able to calculate for the first time the optimal alignment for almost all of the problems in reference  
of the benchmark database balibase  

   introduction  multiple sequence alignment
the multiple sequence alignment problem  msa  in computational biology consists in aligning several sequences  e g  related genes from different organisms  in order to reveal simic
    
ai access foundation  all rights reserved 

fischroedl

larities and differences across the group  either dna can be directly compared  and the
underlying alphabet  consists of the set  c g a t  for the four standard nucleotide bases
cytosine  guanine  adenine and thymine  or we can compare proteins  in which case 
comprises the twenty amino acids 
roughly speaking  we try to write the sequences one above the other such that the
columns with matching letters are maximized  thereby gaps  denoted here by an additional
letter    may be inserted into either of them in order to shift the remaining characters into
better corresponding positions  different letters in the same column can be interpreted as
being caused by point mutations during the course of evolution that substituted one amino
acid by another one  gaps can be seen as insertions or deletions  since the direction of
change is often not known  they are also collectively referred to as indels   presumably  the
alignment with the fewest mismatches or indels constitutes the biologically most plausible
explanation 
there is a host of applications of msa within computational biology  e g   for determining the evolutionary relationship between species  for detecting functionally active sites
which tend to be preserved best across homologous sequences  and for predicting threedimensional protein structure 
formally  one associates a cost with an alignment and tries to find the  mathematically 
optimal alignment  i e   that one with minimum cost  when designing a cost function 
computational efficiency and biological meaning have to be taken into account  the most
widely used definition is the sum of pairs cost function  first  we are given a symmetric
          matrix containing penalties  scores  for substituting a letter with another one
 or a gap   in the simplest case  this could be one for a mismatch and zero for a match 
but more biologically relevant scores have been developed  dayhoff  schwartz  and orcutt
       have proposed a model of molecular evolution where they estimate the exchange
probabilities of amino acids for different amounts of evolutionary divergence  this gives rise
to the so called pam matrices  where pam    is generally the most widely used  jones 
taylor  and thornton        refined the statistics based on a larger body of experimental
data  based on such a substitution matrix  the sum of pairs cost of an alignment is defined
as the sum of penalties between all letter pairs in corresponding column positions 
a pairwise alignment can be conveniently depicted as a path between two opposite
corners in a two dimensional grid  needleman and wunsch         one sequence is placed
on the horizontal axis from left to right  the other one on the vertical axis from top to
bottom  if there is no gap in either string  the path moves diagonally down and right  a gap
in the vertical  horizontal  string is represented as a horizontal  vertical  move right  down  
since a letter is consumed in only one of the strings  the alignment graph is directed and
acyclic  where a  non border  vertex has incoming edges from the left  top  and top left
adjacent vertices  and outgoing edges to the right  bottom  and bottom right vertices 
pairwise alignment can be readily generalized to the simultaneous alignment of multiple
sequences  by considering higher dimensional lattices  for example  an alignment of three
sequences can be visualized as a path in a cube  fig    illustrates an example for the strings
abcb  bcd  and db  it also shows the computation of the sum of pairs cost  for a hypothetical
substitution matrix  a real example  problem  trx of balibase   see sec       is given in
fig    
   

fian improved search algorithm for optimal multiple sequence alignment

alignment 

substitution matrix 

a b c   b
  b c d  
      d b

a b c d  
a          
b
       
c
     
d
   
 
 

cost                

end

d

c

b

b

d

b

a

c

b

start

figure    fictitious alignment problem  column representation  cost matrix  threedimensional visualization of the alignment path through the cube 

a number of improvements can be integrated into the sum of pairs cost  like associating
weights with sequences  and using different substitution matrices for sequences of varying
evolutionary distance  a major issue in multiple sequence alignment algorithms is their
ability to handle gaps  gap penalties can be made dependent on the neighbor letters 
moreover  it has been found  altschul        that assigning a fixed score for each indel
sometimes does not produce the biologically most plausible alignment  since the insertion
of a sequence of x letters is more likely than x separate insertions of a single letter  gap cost
functions have been introduced that depend on the length of a gap  a useful approximation
are affine gap costs  which distinguish between opening and extension of a gap and charge
a   b  x for a gap of length x  for appropriate a and b  another frequently used modification
is to waive the penalties for gaps at the beginning or end of a sequence 
technically  in order to deal with affine gap costs we can no longer identify nodes in the
search graph with lattice vertices  since the cost associated with an edge depends on the
preceding edge in the path  therefore  it is more suitable to store lattice edges in the priority
   

fischroedl

 thx
 grx
 erv
 trcp

 aeqpvlvyfwaswcgpcqlmsplinlaantysdrlkvvkleidpnpttvkkyk      vegvpal
  mqtvi  fgrsgcpysvrakdlaeklsnerdd fqyqyvdiraegitkedlqqkagkpvetvp  
agdklvvvdfsatwcgpckmikpffhslsekysn viflevdvddcqdvasece      vksmptf
 kvttivvniyedgvrgcdalnssleclaaeypm vkfckira sntgagdrfs      sdvlptl

 thx
 grx
 erv
 trcp

rlvkgeqildstegvis  kdkllsf ldthln         
qifvdqqhiggytdfaawvken     lda            
qffkkgqkvgefsgan   kek     leatine  lv    
lvykggelisnfisvaeqfaedffaadvesflneygllper 

figure    alignment of problem  trx of balibase   computed with algorithm settings as
described in sec      

a

b

c

d

a
g     

cost   c   
gap penalty    
g     

b

a
cost a     
gap penalty    
g     

cost a c   
gap penalty    
g     

d

figure    example of computing path costs with affine gap function  the substitution matrix
of fig    and a gap opening penalty of   is used 

queue  and let the transition costs for u  v  v  w be the sum of pairs substitution costs
for using one character from each sequence or a gap  plus the incurred gap penalties for
v  w followed by u  v  this representation was adopted in the program msa  gupta 
kececioglu    schaeffer         note that the state space in this representation grows by
a factor of  k   an example of how successor costs are calculated  with the cost matrix of
fig    and a gap opening penalty of    is shown in fig    
for convenience of terminology in the sequel we will still refer to nodes when dealing
with the search algorithm 
   

fian improved search algorithm for optimal multiple sequence alignment

   overview
wang and jiang        have shown that the optimal multiple sequence alignment problem is
np  hard  therefore  we cannot hope to achieve an efficient algorithm for an arbitrary number
of sequences  as a consequence  alignment tools most widely used in practice sacrifice the
sound theoretical basis of exact algorithms  and are heuristic in nature  chan  wong   
chiu         a wide variety of techniques has been developed  progressive methods build
up the alignment gradually  starting with the closest sequences and successively adding
more distant ones  iterative strategies refine an initial alignment through a sequence of
improvement steps 
despite their limitation to moderate number of sequences  however  the research into
exact algorithms is still going on  trying to push the practical boundaries further  they still
form the building block of heuristic techniques  and incorporating them into existing tools
could improve them  for example  an algorithm iteratively aligning two groups of sequences
at a time could do this with three or more  to better avoid local minima  moreover  it is
theoretically important to have the gold standard available for evaluation and comparison 
even if not for all problems 
since msa can be cast as a minimum cost path finding problem  it turns out that it is
amenable to heuristic search algorithms developed in the ai community  these are actually
among the currently best approaches  therefore  while many researchers in this area have
often used puzzles and games in the past to study heuristic search algorithms  recently there
has been a rising interest in msa as a testbed with practical relevance  e g    korf       
korf   zhang        yoshizumi  miura    ishida        zhou   hansen      b   its study
has also led to major improvements of general search techniques 
it should be pointed out that the definition of the msa problem as given above is not the
only one  it competes with other attempts at formalizing biological meaning  which is often
imprecise or depends on the type of question the biologist investigator is pursuing  e g   in
this paper we are only concerned with global alignment methods  which find an alignment of
entire sequences  local methods  in contrast  are geared towards finding maximally similar
partial sequences  possibly ignoring the remainder 
in the next section  we briefly review previous approaches  based on dynamic programming and incorporating lower and upper bounds  in sec     we describe a new algorithm
that combines and extends some of these ideas  and allows to reduce the storage of closed
nodes by partially recomputing the solution path at the end  sec      moreover  it turns out
that our algorithms iterative deepening strategy can be transferred to find a good balance
between the computation of improved heuristics and the main search  sec      an issue that
has previously been a major obstacle for their practical application  sec    presents an
experimental comparison with partial expansion a  yoshizumi  miura    ishida        
one of the currently most successful approaches  we also solve all but two problems of
reference   of the widely used benchmark database balibase  thompson  plewniak   
poch         to the best of our knowledge  this has not been achieved previously with an
exact algorithm 
   

fischroedl

   previous work
a number of exact algorithms have been developed previously that can compute alignments
of a moderate number of sequences  some of them are mostly constrained by available
memory  some by the required computation time  and some on both  we can roughly
group them into two categories  those based on the dynamic programming paradigm  which
proceed primarily in breadth first fashion  and best first search  utilizing lower and upper
bounds to prune the search space  some recent research  including our new algorithm
introduced in sec     attempts to beneficially combine these approaches 
    dijkstras algorithm and dynamic programming
dijkstra        presented a general algorithm for finding the shortest  resp  minimum cost 
path in a directed graph  it uses a priority queue  heap  to store nodes v together with
the shortest found distance from the start node s  i e   the top left corner of the grid  to v
 also called the g value of v   starting with only s in the priority queue  in each step  an
edge with the minimum g value is removed from the priority queue  its expansion consists
in generating all of its successors  vertices to the right and or below  reachable in one step 
computing their respective g value by adding the edge cost to the previous g value  and
inserting them in turn into the priority queue in case this newly found distance is smaller
than their previous g value  by the time a node is expanded  the g value is guaranteed to
be the minimal path cost from the start node  g   v    d s  v   the procedure runs until the
priority queue becomes empty  or the target node t  the bottom right corner of the grid 
has been reached  its g value then constitutes the optimal solution cost g   t    d s  t  of
the alignment problem  in order to trace back the path corresponding to this cost  we move
backwards to the start node choosing predecessors with minimum cost  the nodes can either
be stored in a fixed matrix structure corresponding to the grid  or they can be dynamically
generated  in the latter case  we can explicitly store at each node a backtrack pointer to
this optimal parent 
for integer edge costs  the priority queue can be implemented as a bucket array pointing
to doubly linked lists  dial         so that all operations can be performed in constant time
 to be precise  the deletemin operation also needs a pointer that runs through all different
g values once  however  we can neglect this in comparison to the number of expansions  
to expand a vertex  at most  k    successor vertices have to be generated  since we have
the choice of introducing a gap in each sequence  thus  dijkstras algorithm can solve the
multiple sequence alignment problem in o  k n k   time and o n k   space for k sequences of
length  n  
a means to reduce the number of nodes that have to be stored for path reconstruction
is by associating a counter with each node that maintains the number of children whose
backtrack pointer refers to them  gupta et al          since each node can be expanded at
most once  after this the number of referring backtrack pointers can only decrease  namely 
whenever a cheaper path to one of its children is found  if a nodes reference count goes
to zero  whether immediately after its expansion or when it later loses a child  it can
be deleted for good  this way  we only keep nodes in memory that have at least one
descendant currently in the priority queue  moreover  auxiliary data structures for vertices
   

fian improved search algorithm for optimal multiple sequence alignment

and coordinates are most efficiently stored in tries  prefix trees   they can be equipped with
reference counters as well and be freed accordingly when no longer used by any edge 
the same complexity as for dijkstras algorithm holds for dynamic programming  dp  
it differs from the former one in that it scans the nodes in a fixed order that is known
beforehand  hence  contrary to the name the exploration scheme is actually static   the
exact order of the scan can vary  e g   row wise or column wise   as long as it is compatible
with the topological ordering of the graph  e g   for two sequences that the cells left  top 
and diagonally top left have been explored prior to a cell   one particular such ordering is
that of antidiagonals  diagonals running from upper right to lower left  the calculation of
the antidiagonal of a node merely amounts to summing up its k coordinates 
hirschberg        noticed that in order to determine only the cost of the optimal alignment g   t   it would not be necessary to store the whole matrix  instead  when proceeding
e g  by rows it suffices to keep track of only k of them at a time  deleting each row as soon
as the next one is completed  this reduces the space requirement by one dimension from
o n k   to o kn k     in order to recover the solution path at the end  re computation of
the lost cell values is needed  a divide and conquer  strategy applies the algorithm twice
to half the grid each  once in forward and once in backward direction  meeting at a fixed
middle row  by adding the corresponding forward and backward distances in this middle
row and finding the minimum  one cell lying on an optimal path can be recovered  this
cell essentially splits the problem into two smaller subproblems  one from the upper left
corner to it  and the other one to the lower right corner  they can be recursively solved
using the same method  in two dimensions  the computation time is at most doubled  and
the overhead reduces even more in higher dimensions 
the fastlsa algorithm  davidson        further refines hirschbergs algorithm by exploiting additionally available memory to store more than one node on an optimal path 
thereby reducing the number of re computations 
    algorithms utilizing bounds
while dijkstras algorithm and dynamic programming can be viewed as variants of breadthfirst search  we achieve best first search if we expand nodes v in the order of an estimate
 lower bound  of the total cost of a path from s to the t passing through v  rather than using
the g value as in dijkstras algorithm  we use f  v     g v    h v  as the heap key  where
h v  is a lower bound on the cost of an optimal path from v to t  if h is indeed admissible 
then the first solution found is guaranteed to be optimal  hart  nilsson    raphael        
this is the classical best first search algorithm  the a algorithm  well known in the artificial
intelligence community  in this context  the priority queue maintaining the generated nodes
is often also called the open list  while the nodes that have already been expanded and
removed from it constitute the closed list  fig    schematically depicts a snapshot during a
two dimensional alignment problem  where all nodes with f  value no larger than the current
fmin have been expanded  since the accuracy of the heuristic decreases with the distance to
the goal  the typical onion shaped distribution results  with the bulk being located closer
to the start node  and tapering out towards higher levels 
the a algorithm can significantly reduce the total number of expanded and generated
nodes  therefore  in higher dimensions it is clearly superior to dynamic programming  how   

fischroedl

m
i
ax
um
m
m
ia

d
er
et

closed
open
possible back leak

le
ve
ls

 a
n

tid

ia
go

na
ls 

x

start
x x
x
x
x
x

x
x
x
x
x
end

figure    snapshot during best first search in pairwise alignment  schematically  

ever  in contrast to the hirschberg algorithm  it still stores all of the explored nodes in the
closed list  apart from keeping track of the solution path  this is necessary to prevent the
search from leaking back  in the following sense 
a heuristic h is called consistent if h x   h x    c x  x     for any node x and its child x   
a consistent heuristic ensures that  as in the case of dijkstras algorithm  at the time a node
is expanded  its g value is optimal  and hence it is never expanded again  however  if we try
to delete the closed nodes  then there can be topologically smaller nodes in open with a
higher f  value  when those are expanded at a later stage  they can lead to the re generation
of the node at a non optimal g value  since the first instantiation is no longer available for
duplicate checking  in fig     nodes that might be subject to spurious re expansion are
marked x 
researchers have tried to avoid these leaks  while retaining the basic a search scheme 
korf proposed to store a list of forbidden operators with each node  or to place the parents
of a deleted node on open with f  value infinity  korf        korf   zhang         however 
as zhou and hansen      a  remark  it is hard to combine this algorithm with techniques
for reduction of the open list  and moreover the storage of operators lets the size of the
nodes grow exponentially with the number of sequences  in their algorithm  they keep
track of the kernel of the closed list  which is defined as the set of nodes that have only
closed nodes as parents  otherwise a closed node is said to be in the boundary  the key
idea is that only the boundary nodes have to be maintained  since they shield the kernel
from re expansions  only when the algorithm gets close to the memory limit nodes from
the kernel are deleted  the backtrack pointer of the children is changed to the parents of
   

fian improved search algorithm for optimal multiple sequence alignment

the deleted nodes  which become relay nodes for them  for the final reconstruction of the
optimal solution path  the algorithm is called recursively for each relay node to bridge the
gap of missing edges 
in addition to the closed list  also the open list can grow rapidly in sequence alignment
problems  particularly  since in the original a algorithm the expansion of a node generates
all of its children at once  those whose f  value is larger than the optimal cost g   t  are kept
in the heap up to the end  and waste much of the available space 
if an upper bound u on the optimal solution cost g   t  is known  then nodes v with
f  v    u can be pruned right away  this idea is used in several articles  spouge        gupta
et al          one of the most successful approaches is yoshizumi et al s        partial
expansion a  pea    each node stores an additional value f   which is the minimum
f  value of all of its yet ungenerated children  in each step  only a node with minimum
f  value is expanded  and only those children with f   f are generated  this algorithm
clearly only generates nodes with f value no larger than the optimal cost  which cannot
be avoided altogether  however  the overhead in computation time is considerable  in the
straightforward implementation  if we want to maintain nodes of constant size  generating
one edge requires determining the f  values of all successors  such that for an interior node
which eventually will be fully expanded the computation time is of the order of the square
of the number of successors  which grows as o  k   with the number of sequences k  as a
remedy  in the paper it is proposed to relax the condition by generating all children with
f  f   c  for some small c 
an alternative general search strategy to a that uses only linear space is iterative
deepening a  ida    korf         the basic algorithm conducts a depth first search up to
a pre determined threshold for the f  value  during the search  it keeps track of the smallest
f  value of a generated successor that is larger than the threshold  if no solution is found 
this provides an increased threshold to be used in the next search iteration 
wah and shang        suggested more liberal schemes for determining the next threshold dynamically in order to minimize the number of recomputations  ida is most efficient
in tree structured search spaces  however  it is difficult to detect duplicate expansions without additional memory  therefore  unfortunately it is not applicable in lattice structured
graphs like in the sequence alignment problem due to the combinatorially explosive number
of paths between any two given nodes 
a different line of research tries to restrict the search space of the breadth first approaches by incorporating bounds  ukkonen        presented an algorithm for the pairwise
alignment problem which is particularly efficient for similar sequences  its computation time
scales as o dm   where d is the optimal solution cost  first consider the problem of deciding
whether a solution exists whose cost is less than some upper threshold u   we can restrict
the evaluation of the dp matrix to a band of diagonals where the minimum number of
indels required to reach the diagonal  times the minimum indel cost  does not exceed u  
in general  starting with a minimum u value  we can successively double g until the test
returns a solution  the increase of computation time due to the recomputations is then also
bounded by a factor of   
another approach for multiple sequence alignment is to make use of the lower bounds h
from a   the key idea is the following  since all nodes with an f  value lower than g   t  have
to be expanded anyway in order to guarantee optimality  we might as well explore them in
   

fischroedl

any reasonable order  like that of dijkstras algorithm or dp  if we only knew the optimal
cost  even slightly higher upper bounds will still help pruning  spouge        proposed to
bound dp to vertices v where g v    h v  is smaller than an upper bound for g   t  
linear bounded diagonal alignment  lbd align   davidson        uses an upper
bound in order to reduce the computation time and memory in solving a pairwise alignment
problem by dynamic programming  the algorithm calculates the dp matrix one antidiagonal at a time  starting in the top left corner  and working down towards bottom right 
while a would have to check the bound in every expansion  lbd align only checks the
top and bottom cell of each diagonal  if e g  the top cell of a diagonal has been pruned  all
the remaining cells in that row can be pruned as well  since they are only reachable through
it  this means that the pruning frontier on the next row can be shifted down by one  thus 
the pruning overhead can be reduced from a quadratic to a linear amount in terms of the
sequence length 
    obtaining heuristic bounds
up to now we have assumed lower and upper bounds  without specifying how to derive them 
obtaining an inaccurate upper bound on g   t  is fairly easy  since we can use the cost of any
valid path through the lattice  better estimates are e g  available from heuristic linear time
alignment programs such as fasta and blast  altschul  gish  miller  myers    lipman 
       which are a standard method for database searches  davidson        employed a
local beam search scheme 
gusfield        proposed an approximation called the star alignment  out of all the
sequences to be aligned  one consensus sequence is chosen such that the sum of its pairwise
alignment costs to the rest of the sequences is minimal  using this best sequence as
the center  the other ones are aligned using the once a gap  always a gap rule  gusfield
showed that the cost of the optimal alignment is greater or equal to the cost of this star
alignment  divided by       k  
for use in heuristic estimates  lower bounds on the k alignment are often based on
optimal alignments of subsets of m   k sequences  in general  for a vertex v in k space  we
are looking for a lower bound for a path from v to the target corner t  consider first the
case m      the cost of such a path is  by definition  the sum of its edge costs  where each
edge cost in turn is the sum of all pairwise  replacement or gap  penalties  each multiple
sequence alignment induces a pairwise alignment for sequences i and j  by simply copying
rows i and j and ignoring columns with a   in both rows  these pairwise alignments can
be visualized as the projection of an alignment onto its faces  cf  fig    
by interchange of the summation order  the sum of pairs cost is the sum of all pairwise
alignment costs of the respective paths projected on a face  each of which cannot be smaller
than the optimal pairwise path cost  thus  we can construct an admissible heuristic hpair
by computing  for each pairwise alignment and for each cell in a pairwise problem  the
cheapest path cost to the goal node 
the optimal solutions to all pairwise alignment problems needed for the lower bound h
values are usually computed prior to the main search in a preprocessing step  ikeda   imai 
       to this end  it suffices to apply the ordinary dp procedure  however  since this time
we are interested in the lowest cost of a path from v to t  it runs in backward direction 
   

fian improved search algorithm for optimal multiple sequence alignment

proceeding from the lower right corner to the upper left  expanding all possible parents of
a vertex in each step 
let u be an upper bound on the cost of an optimal multiple sequence alignment g 
the sum of all optimal alignment costs lij   d sij   tij   for pairwise subproblems i  j 
            k   i   j  call it l  is a lower bound on g  carrillo and lipman        pointed out
that by the additivity of the sum of pairs cost function  any pairwise alignment induced
by the optimal multiple sequence alignment can at most be    u  l larger than the
respective optimal pairwise alignment  this bound can be used to restrict the number of
values that have to be computed in the preprocessing stage and have to be stored for the
calculation of the heuristic  for the pair of sequences i  j  only those nodes v are feasible
such that a path from the start node si j to the goal node ti j exists with total cost no more
than li j     to optimize the storage requirements  we can combine the results of two
searches  first  a forward pass determines for each relevant node v the minimum distance
d sij   v  from the start node  the subsequent backward pass uses this distance like an exact
heuristic and stores the distance d v  tij   from the target node only for those nodes with
d sij   v    d v  tij    d s  t        
still  for larger alignment problems the required storage size can be extensive  the
program msa  gupta et al         allows the user to adjust  to values below the carrillolipman bound individually for each pair of sequences  this makes it possible to generate
at least heuristic alignments if time or memory doesnt allow for the complete solution 
moreover  it can be recorded during the search if the  bound was actually reached  in the
negative case  optimality of the found solution is still guaranteed  otherwise  the user can
try to run the program again with slightly increased bounds 
the general idea of precomputing simplified problems and storing the solutions for use as
a heuristic has been explored under the name of pattern databases  culberson   schaeffer 
       however  these approaches implicitly assume that the computational cost can be
amortized over many search instances to the same target  in contrast  in the case of msa 
the heuristics are instance specific  so that we have to strike a balance  we will discuss this
in greater depth in sec      

   iterative deepening dynamic programming
as we have seen  a fixed search order as in dynamic programming can have several advantages over pure best first selection 
 since closed nodes can never be reached more than once during the search  it is safe to
delete useless ones  those that are not part of any shortest path to the current open
   a slight technical complication arises for affine gap costs  recall that dp implementations usually charge
the gap opening penalty to the g value of the edge e starting the gap  while the edge e  ending the gap
carries no extra penalty at all  however  since the sum of pairs heuristics h is computed in backward
direction  using the same algorithm we would assign the penalty for the same path instead to e    this
means that the heuristic f   g   h would no longer be guaranteed to be a lower bound  since it
contains the penalty twice  as a remedy  it is necessary to make the computation symmetric by charging
both the beginning and end of a gap with half the cost each  the case of the beginning and end of
the sequences can be handled most conveniently by starting the search from a dummy diagonal edge
                                  and defining the target edge to be the dummy diagonal edge   n          n     n  
           n        similar to the arrows shown in fig    

   

fischroedl

nodes  and to apply path compression schemes  such as the hirschberg algorithm 
no sophisticated schemes for avoiding back leaks are required  such as the abovementioned methods of core set maintenance and dummy node insertion into open 
 besides the size of the closed list  the memory requirement of the open list is determined by the maximum number of nodes that are open simultaneously at any
time while the algorithm is running  when the f  value is used as the key for the
priority queue  the open list usually contains all nodes with f  values in some range
 fmin   fmin      this set of nodes is generally spread across all over the search space 
since g  and accordingly h    f  g   can vary arbitrarily between   and fmin     as
opposed to that  if dp proceeds along levels of antidiagonals or rows  at any iteration
at most k levels have to be maintained at the same time  and hence the size of the
open list can be controlled more effectively  in fig     the pairwise alignment is partitioned into antidiagonals  the maximum number of open nodes in any two adjacent
levels is four  while the total amounts to seventeen   
 for practical purposes  the running time should not only be measured in terms of the
number of node expansions  but one should also take into account the execution time
needed for an expansion  by arranging the exploration order such that edges with
the same head node  or more generally  those sharing a common coordinate prefix 
are dealt with one after the other  much of the computation can be cached  and edge
generation can be sped up significantly  we will come back to this point in sec    
the remaining issue of a static exploration scheme consists in adequately bounding the
search space using the h values  a is known to be minimal in terms of the number of node
expansions  if we knew the cost g   t  of a cheapest solution path beforehand  we could
simply proceed level by level of the grid  however only immediately prune generated edges
e whenever f  e    g   t   this would ensure that we only generate those edges that would
have been generated by algorithm a   as well  an upper threshold would additionally help
reduce the size of the closed list  since a node can be pruned if all of its children lie beyond
the threshold  additionally  if this node is the only child of its parent  this can give rise to
a propagating chain of ancestor deletions 
we propose to apply a search scheme that carries out a series of searches with successively larger thresholds  until a solution is found  or we run out of memory or patience  
the use of such an upper bound parallels that in the ida algorithm 
the resulting algorithm  which we will refer to as iterative deepening dynamic programming  iddp   is sketched in fig     the outer loop initializes the threshold with a
lower bound  e g   h s    and  unless a solution is found  increases it up to an upper bound 
in the same manner as in the ida algorithm  in order to make sure that at least one additional edge is explored in each iteration the threshold has to be increased correspondingly
at least to the minimum cost of a fringe edge that exceeded the previous threshold  this
fringe increment is maintained in the variable minnextthresh  initially estimated as the
upper bound  and repeatedly decreased in the course of the following expansions 
   contrary to what the figure might suggest  a can open more than two nodes per level in pairwise
alignments  if the set of nodes no worse than some fmin contains holes 

   

fian improved search algorithm for optimal multiple sequence alignment

procedure iddp edge startedge  edge targetedge  int lowerbound  int upperbound 
int thresh   lowerbound
 outer loop  iterative deepening phases 
while  thresh  upperbound  do
heap h     startedge     
int minnextthresh   upperbound
 inner loop  bounded dynamic programming 
while  not h isempty    do
edge e   h deletemin    find and remove an edge with minimum level 
if  e    targetedge  then
 optimal alignment found 
return tracebackpath startedge  targetedge 
end if
expand e  thresh  minnextthresh 
end while
int threshincr   computethreshincr    compute search threshold for next iteration  see text 
thresh   max thresh   threshincr  minnextthresh 
end while
print no alignment with cost at most upperbound found 

figure    algorithm iterative deepening dynamic programming 
in each step of the inner loop  we select and remove a node from the priority queue
whose level is minimal  as explained later in sec     it is favorable to break ties according
to the lexicographic order of target nodes  since the total number of possible levels is
comparatively small and known in advance  the priority queue can be implemented using
an array of linked lists  dial         this provides constant time operations for insertion and
deletion 
the expansion of an edge e is partial  fig      a child edge might already exist from an
earlier expansion of an edge with the same head vertex  we have to test if we can decrease
the g value  otherwise  we generate a new edge  if only temporarily for the sake of calculating its f  value  that is  if its f  value exceeds the search threshold of the current iteration 
its memory is immediately reclaimed  moreover  in this case the fringe threshold minnextthresh is updated  in a practical implementation  we can prune unnecessary accesses to
partial alignments inside the calculation of the heuristic e geth   as soon as as the search
threshold has already been reached 
the relaxation of a child edge within the threshold is performed by the subprocedure
updateedge  cf  fig      this is similar to the corresponding relaxation step in a   updating
the childs g  and f values  its parent pointers  and inserting it into open  if not already
contained  however  in contrast to best first search  it is inserted into the heap according to
the antidiagonal level of its head vertex  note that in the event that the former parent loses
its last child  propagation of deletions  fig     can ensure that only those closed nodes
continue to be stored that belong to some solution path  edge deletions can also ensue
deletion of dependent vertex and coordinate data structures  not shown in the pseudocode  
the other situation that gives rise to deletions is if immediately after the expansion of a
node no children are pointing back to it  the children might either be reachable more cheaply
from different nodes  or their f  value might exceed the threshold  
   

fischroedl

procedure expand edge e  int thresh  int minnextthresh 
for all edge child  succ e  do
 retrieve child or tentatively generate it if not yet existing  set boolean variable created
accordingly 
int newg   e getg     gapcost e  child 
  child getcost  
int newf   newg   child geth  
if  newf  thresh and newg   child getg    then
 shorter path than current best found  estimate within threshold 
child setg newg 
updateedge e  child  h   update search structures 
else if  newf   thresh  then
minnextthresh  
min minnextthresh  newf 
 record minimum of pruned edges 
if  created  then
delete child   make sure only promising edges are stored 
end if
end if
end for
if  e ref       then
deleterec e   no promising children could be inserted into the heap 
end if

figure    edge expansion in iddp 
procedure updateedge edge parent  edge child  heap h 
parent ref  
child getbacktrack   ref
if  child getbacktrack   ref       then
deleterec child getbacktrack     the former parent has lost its last child and becomes useless 
end if
child setbacktrack parent 
if  not h contains child   then
h insert child  child gethead   getlevel   
end if

figure    edge relaxation in iddp 

the correctness of the algorithm can be shown analogously to the soundness proof of a  
if the threshold is smaller than g   t   the dp search will terminate without encountering
a solution  otherwise  only nodes are pruned that cannot be part of an optimal path  the
invariant holds that there is always a node in each level which lies on an optimal path and
is in the open list  therefore  if the algorithm terminates only when the heap runs empty 
the best found solution will indeed be optimal 
the iterative deepening strategy results in an overhead computation time due to reexpansions  and we are trying to restrict this overhead as much as possible  more precisely 
   

fian improved search algorithm for optimal multiple sequence alignment

procedure deleterec edge e 
if  e getbacktrack      nil  then
e getbacktrack   ref
if  e getbacktrack   ref       then
deleterec e getbacktrack   
end if
end if
delete e 

figure    recursive deletion of edges that are no longer part of any solution path 
procedure traceback edge startedge  edge e 
if  e    startedge  then
return  end of recursion 
end if
if  e getbacktrack   gettarget      e getsource    then
 relay node  recursive path reconstruction 
iddp  e getbacktrack    e  e getf    e getf   
end if
outputedge e 
traceback startedge  e getbacktrack   

figure    divide and conquer solution reconstruction in reverse order 
we want to minimize the ratio
 

niddp
 
na

where niddp and na denote the number of expansions in iddp and a   respectively  one
way to do so  wah   shang        is to choose a threshold sequence               such that
the number of expansions ni in stage i satisfies
ni   rni   
for some fixed ratio r  if we choose r too small  the number of re expansions and hence
the computation time will grow rapidly  if we choose it too big  then the threshold of the
last iteration can exceed the optimal solution cost significantly  and we will explore many
irrelevant edges  suppose that n  rp   na  n  rp     then the algorithm performs p    
iterations  in the worst case  the overshoot will be maximal if a finds the optimal solution
just above the previous threshold  na   n  rp      the total number of expansions is
p
r r p     
r 
i
n  p  
  and the ratio  becomes approximately r 
  by setting the
i   r   n 
r 
derivative of this expression to zero  we find that the optimal value for r is    the number
of expansions should double from one search stage to the next  if we achieve doubling  we
will expand at most four times as many nodes as a  
like in wah and shangs        scheme  we dynamically adjust the threshold using runtime information  procedure computethreshincr stores the sequence of expansion numbers
and thresholds from the previous search stages  and then uses curve fitting for extrapolation
 in the first few iterations without sufficient data available  a very small default threshold
is applied   we found that the distribution of nodes n   with f  value smaller or equal to
   

fischroedl

threshold  can be modeled very accurately according to the exponential approach
n     a  b   
consequently  in order to attempt to double the number of expansions  we choose the next
threshold according to
 
i     i  
 
log  b

   sparse representation of solution paths
when the search progresses along antidiagonals  we do not have to fear back leaks  and
are free to prune closed nodes  similarly as in zhou and hansens      a  work  however 
we only want to delete them lazily and incrementally when being forced by the algorithm
approaching the computers memory limit 
when deleting an edge e  the backtrack pointers of its child edges that refer to it are
redirected to the respective predecessor of e  whose reference count is increased accordingly 
in the resulting sparse solution path representation  backtrack pointers can point to any
optimal ancestors 
after termination of the main search  we trace back the pointers starting with the goal
edge  this is outlined in procedure traceback  fig      which prints out the solution path
in reverse order  whenever an edge e points back to an ancestor e  which is not its direct
parent  we apply an auxiliary search from start edge e  to goal edge e in order to reconstruct
the missing links of the optimal solution path  the search threshold can now be fixed at the
known solution cost  moreover  the auxiliary search can prune those edges that cannot be
ancestors of e because they have some coordinate greater than the corresponding coordinate
in e  since also the shortest distance between e and e  is known  we can stop at the first path
that is found at this cost  to improve the efficiency of the auxiliary search even further 
the heuristic could be recomputed to suit the new target  therefore  the cost of restoring
the solution path is usually marginal compared to that of the main search 
which edges are we going to prune  in which order  for simplicity  assume for the
moment that the closed list consists of a single solution path  according to the hirschberg
approach  we would keep only one edge  preferably lying near the center of the search
space  e g   on the longest anti diagonal   in order to minimize the complexity of the two
auxiliary searches  with additional available space allowing to store three relay edges  we
would divide the search space into four subspaces of about equal size  e g   additionally
storing the antidiagonals half way between the middle antidiagonal and the start node resp 
the target node   by extension  in order to incrementally save space under diminishing
resources we would first keep only every other level  then every fourth  and so on  until only
the start edge  the target edge  and one edge half way on the path would be left 
since in general the closed list contains multiple solution paths  more precisely  a tree
of solution paths   we would like to have about the same density of relay edges on each of
them  for the case of k sequences  an edge reaching level l with its head node can originate
with its tail node from level l             l  k  thus  not every solution path passes through
each level  and deleting every other level could result in leaving one path completely intact 
while extinguishing another totally  thus  it is better to consider contiguous bands of k
   

fian improved search algorithm for optimal multiple sequence alignment

procedure sparsifyclosed  
for  int sparse     to blog  n c  do
while  usedmemory     maxmemory and exists  edge e  open   e getlastsparse    
sparse   do
edge pred   e getbacktrack  
 trace back solution path 
while  pred    nil and e getlastsparse     sparse  do
e setlastsparse sparse   mark to avoid repeated trace back 
if  bpred gethead   getlevel     kc mod  sparse       then
 pred lies in prunable band  redirect pointer 
e setbacktrack pred getbacktrack   
e getbacktrack   ref  
pred ref
if  pred ref       then
 e is the last remaining edge referring to pred 
deleterec pred 
end if
else
 not in prunable band  continue traversal 
e   e getbacktrack  
end if
pred   e getbacktrack  
end while
end while
end for

figure     sparsification of closed list under restricted memory 

levels each  instead of individual levels  bands of this size cannot be skipped by any path 
the total number of antidiagonals in an alignment problem of k sequences of length n is
k  n     thus  we can decrease the density in blog  n c steps 
a technical implementation issue concerns the ability to enumerate all edges that reference some given prunable edge  without explicitly storing them in a list  however  the
reference counting method described above ensures that any closed edge can be reached by
following a path bottom up from some edge in open  the procedure is sketched in fig     
the variable sparse denotes the interval between level bands that are to be maintained in
memory  in the inner loop  all paths to open nodes are traversed in backward direction 
for each edge e  that falls into a prunable band  the pointer of the successor e on the path
is redirected to its respective backtrack pointer  if e was the last edge referencing e    the
latter one is deleted  and the path traversal continues up to the start edge  when all open
nodes have been visited and the memory bound is still exceeded  the outer loop tries to
double the number of prunable bands by increasing sparse 
procedure sparsifyclosed is called regularly during the search  e g   after each expansion 
however  a naive version as described above would incur a huge overhead in computation
time  particularly when the algorithms memory consumption is close to the limit  therefore  some optimizations are necessary  first  we avoid tracing back the same solution path
at the same  or lower  sparse interval by recording for each edge the interval when it was
   

fischroedl

traversed the last time  initially zero   only for an increased variable sparse there can be
anything left for further pruning  in the worst case  each edge will be inspected blog  n c
times  secondly  it would be very inefficient to actually inspect each open node in the inner
loop  just to find that its solution path has been traversed previously  at the same or higher
sparse value  however  with an appropriate bookkeeping strategy it is possible to reduce the
time for this search overhead to o k  

   use of improved heuristics
as we have seen  the estimator hpair   the sum of optimal pairwise goal distances  gives
a lower bound on the actual path length  however  more powerful heuristics are also
conceivable  while their computation will require more resources  the trade off can prove
itself worthwhile  the tighter the estimator is  the smaller is the space that the main search
needs to explore 
    beyond pairwise alignments
kobayashi and imai        suggested to generalize hpair by considering optimal solutions
for subproblems of size m      they proved that the following heuristics are admissible
and more informed than the pairwise estimate 
 hall m is the sum of all m dimensional optimal costs  divided by

k  
m   

 hone m splits the sequences into two sets of sizes m and k  m  the heuristic is the sum
of the optimal cost of the first subset  plus that of the second one  plus the sum of all
  dimensional optimal costs of all pairs of sequences in different subsets  usually  m
is chosen close to k   
these improved heuristics can reduce the main search effort by orders of magnitudes 
however  in contrast to pairwise sub alignments  time and space resources devoted to compute and store higher dimensional heuristics are in general no longer negligible compared
to the main search  kobayashi and imai        noticed that even for the case m     of
triples of sequences  it can be impractical to compute the entire subheuristic hall m   as one
reduction  they show that it suffices to restrict oneself to nodes where the path cost does
not exceed the optimal path cost of the subproblem by more than
 

 

x
k 
u
d si       im   ti       im   
m 
i      i
 

m

this threshold can be seen as a generalization of the carrillo lipman bound  however 
it can still
incur excessive overhead in space and computation time for the computation of
k
the m
lower dimensional subproblems  a drawback is that it requires an upper bound
u   on whose accuracy also the algorithms efficiency hinges  we could improve this bound
by applying more sophisticated heuristic methods  but it seems counterintuitive to spend
more time doing so which we would rather use to calculate the exact solution  in spite of
its advantages for the main search  the expensiveness of the heuristic calculation appears
as a major obstacle 
   

fian improved search algorithm for optimal multiple sequence alignment

mcnaughton  lu  schaeffer  and szafron        suggested to partition the heuristic
into  hyper   cubes using a hierarchical oct tree data structure  in contrast to full cells 
empty cells only retain the values at their surface  when the main search tries to use one
of them  its interior values are recomputed on demand  still  this work assumes that each
node in the entire heuristic is calculated at least once using dynamic programming 
we see one cause of the dilemma in the implicit assumption that a complete computation
is necessary  the bound  above refers to the worst case  and can generally include many
more nodes than actually required in the main search  however  since we are only dealing
with the heuristic  we can actually afford to miss some values occasionally  while this might
slow down the main search  it cannot compromise the optimality of the final solution 
therefore  we propose to generate the heuristics with a much smaller bound   whenever
the attempt to retrieve a value of the m dimensional subheuristic
fails during the main
m
search  we simply revert to replacing it by the sum of the   optimal pairwise goal distances
it covers 
we believe that the iddp algorithm lends itself well to make productive use of higherdimensional heuristics  firstly and most importantly  the strategy of searching to adaptively
increasing thresholds can be transferred to the  bound as well  this will be addressed in
more detail in the next section 
secondly  as far as a practical implementation is concerned  it is important to take into
account not only how a higher dimensional heuristic affects the number of node expansions 
but also their time complexity  this time is dominated by the number of accesses to subalignments  with k sequences  in the worst case an edge has  k    successors  leading to
a total of
 
k
k
      
m
evaluations for hall m   one possible improvement is to enumerate all edges emerging from a
given vertex in lexicographic order  and to store partial sums of heuristics of prefix subsets
of sequences for later re use  in this way  if we allow for a cache of linear size  the number
of accesses is reduced to
 
i k
x
i i 
 
 
m 
i m
correspondingly  for a quadratic cache we only need
i k
x
i m

 

 

i

i 
m 

evaluations  for instance  in aligning    sequences using hall     a linear cache reduces the
evaluations to about    percent within one expansion 
as mentioned above  in contrast to a   iddp gives us the freedom to choose any
particular expansion order of the edges within a given level  therefore  when we sort edges
lexicographically according to the target nodes  much of the cached prefix information can
be shared additionally across consecutively expanded edges  the higher the dimension of
the subalignments  the larger are the savings  in our experiments  we experienced speedups
of up to eighty percent in the heuristic evaluation 
   

fischroedl

execution time  s 

   

main search
heuristic
total time

  

 

   
 

  

  

              
heuristic miss ratio r    

  

  

   

figure     trade off between heuristic and main search  execution times for problem  tvxa
as a function of heuristic miss ratio 

    trade off between computation of heuristic and main search
as we have seen  we can control the size of the precomputed sub alignments by choosing
the bound  up to which f  values of edges are generated beyond the respective optimal
solution cost  there is obviously a trade off between the auxiliary and main searches  it
is instructive to consider the heuristic miss ratio r  i e   the fraction of calculations of
the heuristic h during the main search when a requested entry in a partial msa has not
been precomputed  the optimum for the main search is achieved if the heuristic has been
computed for every requested edge  r       going beyond that point will generate an
unnecessarily large heuristic containing many entries that will never be actually used  on
the other hand  we are free to allocate less effort to the heuristic  resulting in r     and
consequently decreasing performance of the main search  generally  the dependence has
an s shaped form  as exemplified in fig     for the case of problem  tvxa of balibase
 cf  next section   here  the execution time of one iteration of the main search at a fixed
threshold of    above the lower bound is shown  which includes the optimal solution 
fig     illustrates the overall time trade off between auxiliary and main search  if we fix
 at different levels  the minimum total execution time  which is the sum of auxiliary and
main search  is attained at about r              seconds   the plot for the corresponding
memory usage trade off has a very similar shape 
unfortunately  in general we do not know in advance the right amount of auxiliary search 
as mentioned above  choosing  according to the carrillo lipman bound will ensure that
   

fian improved search algorithm for optimal multiple sequence alignment

execution time  s 

   

  

 

   
 

  

  

  

  
  
  
  
heuristic miss ratio r    

  

  

   

figure     time of the last iteration in the main search for problem  tvxa as a function of
heuristic miss ratio 

every requested sub alignment cost will have been precomputed  however  in general we will
considerably overestimate the necessary size of the heuristic 
as a remedy  our algorithm iddp gives us the opportunity to recompute the heuristic in
each threshold iteration in the main search  in this way  we can adaptively strike a balance
between the two 
when the currently experienced miss rate r rises above some threshold  we can suspend
the current search  recompute the pairwise alignments with an increased threshold   and
resume the main search with the improved heuristics 
like for the main search  we can accurately predict the auxiliary computation time
and space at threshold  using exponential fitting  due to the lower dimensionality  it
will generally increase less steeply  however  the constant factor might be higher for the
k
heuristic  due to the combinatorial number of m
alignment problems to be solved 
a doubling scheme as explained above can bound the overhead to within a constant
factor of the effort in the last iteration  in this way  when also limiting the heuristic
computation time by a fixed fraction of the main search  we can ensure as an expected
upper bound that the overall execution time stays within a constant factor of the search
time that would be required using only the pairwise heuristic 
if we knew the exact relation between   r  and the speedup of the main search  an ideal
strategy would double the heuristic whenever the expected computation time is smaller than
the time saved in the main search  however  as illustrated in fig      this dependence is
more complex than simple exponential growth  it varies with the search depth and specifics
of the problem  either we would need a more elaborate model of the search space  or the
   

fischroedl

algorithm would have to conduct exploratory searches in order to estimate the relation 
we leave this issue to future work  and restrict ourselves here to a simplified  conservative
heuristic  we hypothesize that the main search can be made twice as fast by a heuristic
doubling if the miss rate r rises above    percent  in our experiments  we found that this
assumption is almost always true  in this event  since the effective branching factor of the
main search is reduced by the improved heuristic  we also ignore the history of main search
times in the exponential extrapolation procedure for subsequent iterations 

   experimental results
in the following  we compare iddp to one of the currently most successful approaches 
partial expansion a   we empirically explore the benefit of higher dimensional heuristics 
finally  we show its feasibility by means of the benchmark database balibase  
    comparison to partial expansion a
for the first series of evaluations  we ran iddp on the same set of sequences as chosen by
yoshizumi et al          elongation factors ef tu and ef   from various species  with a
high degree of similarity   as in this work  substitution costs were chosen according to the
pam     matrix  the applied heuristic was the sum of optimal pairwise goal distances  the
expansion numbers do not completely match with their results  however  since we applied
the biologically more realistic affine gap costs  gaps of length x were charged    x  except
at the beginning and end of a sequence  where the penalty was    x 
all of the following experiments were run under redhat linux     on an intel xeont m
cpu with      ghz  and main memory of   gigabytes  we used the gcc      compiler 
the total space consumption of a search algorithm is determined by the peak number of
open and closed edges over the entire running time  table   and fig     give these values
for the series of successively larger sets of input sequences  with the sequences numbered as
defined in yoshizumi et al                                  
with our implementation  the basic a algorithm could be carried out only up to  
sequences  before exhausting our computers main memory 
confirming the results of yoshizumi et al          partial expansion requires only about
one percent of this space  interestingly  during the iteration with the peak in total numbers
of nodes held in memory  no nodes are actually closed except in problem    this might
be explained with the high degree of similarity between sequences in this example  recall
that pea only closes a node if all of its successors have an f  value of no more than the
optimal solution cost  if the span to the lower bound is small  each node can have at least
one bad successor that exceeds this difference 
iddp reduces the memory requirements further by a factor of about    the diagram
also shows the maximum size of the open list alone  for few sequences  the difference
between the two is dominated by the linear length to store the solution path  as the
problem size increases  however  the proportion of the closed list of the total memory drops
to about only    percent for    sequences  the total number of expansions  including all
search stages  is slightly higher than in pea   however  due to optimizations made possible
by the control of the expansion order  the execution time at    sequences is reduced by
about a third 
   

fian improved search algorithm for optimal multiple sequence alignment

num
exp

time
 sec 

max
open

max
open  
closed

   
    
    
     
      
      

a
    
    
    
    
     
      

    
     
      
      
       
        

    
     
      
      
       
        

 
 
 
 
 
 
 
  
  
  

   
   
    
    
     
      
      
       
        
        

pea
    
    
    
    
    
     
      
       
        
         

   
   
    
    
     
      
      
       
       
       

   
   
    
    
     
      
      
       
       
       

 
 
 
 
 
 
 
  
  
  

   
    
    
     
     
      
      
       
        
        

iddp
    
    
    
    
    
     
      
       
        
         

 
 
   
   
   
     
     
      
      
       

   
   
   
   
    
     
     
      
      
       

 
 
 
 
 
 

table    algorithm comparison for varying number of input sequences  elongation factors
ef tu and ef    

since pea does not prune edges  its maximum space usage is always the total number
of edges with f  value smaller than g   t   call these edges the relevant edges  since they have
to be inspected by each admissible algorithm   in iddp  on the other hand  the open list
can only comprise k adjacent levels out of those edges  not counting the possible threshold
overshoot  which would contribute a factor of at most     thus  the improvement of iddp
over pea will tend to increase with the overall number of levels  which is the sum of
   

fischroedl

 e   
 e   

edges in memory

 e   
      
     
    
   
a  max open closed
pea  max open closed
iddp max open closed
iddp max open

  
 
 

 

 

 

 
 
 
number of sequences

  

  

  

figure     memory requirements for a   iddp  and pea  elongation factors ef tu and
ef    

all string lengths   divided by the number of sequences  in other words  with the average
sequence length 
moreover  the ratio depends on how well the heuristic suits the particular problem 
fig     shows the distribution of all edges with f value smaller or equal to g   t   for the
case of   of the example sequences  this problem is quite extreme as the bulk of these edges
is concentrated in a small level band between      and       as an example with a more
even distribution  fig     depicts the situation for problem  cpt from reference   in the
benchmark set balibase  thompson et al         with heuristic hall     in this case  the
proportion of the overall          relevant edges that are maximal among all   adjacent
levels amounts to only     percent  the maximum open size in iddp is       while the
total number of edges generated by pea is         an improvement by about a factor of
   
    multidimensional heuristics
on the same set of sequences  we compared different improved heuristics in order to get an
impression for their respective potential  specifically  we ran iddp with heuristics hpair  
hall     hall     and hone k   at various thresholds   fig     shows the total execution time
for computing the heuristics  and performing the main search  in each case  we manually
selected a value for  which minimized this time  it can be seen that the times for hone k  
lie only a little bit below hpair   for few sequences  less than six   the computation of the
heuristics hall   and hall   dominates their overall time  with increasing dimensions  how   

fian improved search algorithm for optimal multiple sequence alignment

     

open edges   sum open edges    

     
     
    
     
     
     
     
 
 

   

    

    

    
level

    

    

    

    

figure     distribution of relevant edges over levels  elongation factors ef tu and ef    
compare to the schematic projection in fig    

ever  this investment starts to yield growing returns  with hall   being the fastest algorithm 
requiring only   percent of the time of hpair at    sequences 
as far as memory is concerned  fig     reveals that the maximum size of the open and
closed list  for the chosen  values  is very similar for hpair and hone k   on the one hand 
and hall   and hall   on the other hand 
at    sequences  hone   saves only about    percent of edges  while hall   only needs    
percent and hall   only     percent of the space required by the pairwise heuristic  using
iddp  we never ran out of main memory  even larger test sets could be aligned  the range
of the shown diagrams was limited by our patience to wait for the results for more than two
days 
based on the experienced burden of computing the heuristic  kobayashi and imai       
concluded that hone m should be preferred to hall m   we do not quite agree with this judgment  we see that the heuristic hall m is able to reduce the search space of the main search
considerably stronger than hone m   so that it can be more beneficial with an appropriate
amount of heuristic computation 
    the benchmark database balibase
balibase  thompson et al         is a widely used database of manually refined multiple
sequence alignments specifically designed for the evaluation and comparison of multiple sequence alignment programs  the alignments are classified into   reference sets  reference  
contains alignments of up to six about equidistant sequences  all the sequences are of sim   

fischroedl

open edges   sum open edges    

       

      

 e   

 e   

 e   

 e   

 
 

   

   

   

   
level

    

    

    

    

figure     distribution of relevant edges over levels  problem  cpt from balibase  
ilar length  they are grouped into   classes  indexed by sequence length and the percentage
of identical amino acids in the same columns  note that many of these problems are indeed much harder than the elongation factor examples from the previous section  despite
consisting of fewer sequences  their dissimilarities are much more pronounced 
we applied our algorithm to reference    with substitution costs according to the pet  
matrix  jones et al         and affine gap costs of  x    except for leading and trailing gaps 
where no gap opening penalty was charged  for all instances  we precomputed the pairwise
sub alignments up to a fixed bound of     above the optimal solution  the optimal solution
was found within this bound in all cases  and the effort is generally marginal compared to
the overall computation  for all problems involving more than three sequences  the heuristic
hall   was applied 
out of the    alignment problems in reference    our algorithm could solve all but  
problems  namely   pama and gal    on our computer  detailed results are listed in tables  
through    
thompson  plewniak  and poch        compared a number of widely used heuristic
alignment tools using the so called sp  score  their software calculates the percentage of
correctly aligned pairs within the biologically significant motifs  they found that all programs perform about equally well for the sequences with medium and high amino acid
identity  differences only occurred for the case of the more distant sequences with less
than    percent identity  the so called twilight zone  particularly challenging was the
group of short sequences  in this subgroup  the three highest scoring programs are prrp 
clustalx  and saga  with respective median scores of               and        the
medium score for the alignments found in our experiments amounts to        hence  it is
about as good as prrp  and only beaten by clustalx  while we focused in our exper   

fian improved search algorithm for optimal multiple sequence alignment

 e   
      
     

total time  sec 

    
   
  
 
   
  fold heuristic
div conq heuristic
  fold heuristic
  fold heuristic

    
     
 

 

 

 
  
number of sequences

  

  

figure     comparison of execution times  including calculation of heuristics   elongation
factors ef tu and ef   

iments on algorithmic feasibility rather than on solution quality  it would be worthwhile
to attempt to improve the alignments found by these program using their more refined
penalty functions  clustalx  for example  uses different pam matrices depending on
the evolutionary distance of sequences  moreover  it assigns weights to sequences  based on
a phylogenetic tree   and gap penalties are made position specific  all of these improvements can be easily integrated into the basic sum of pairs cost function  so that we could
attempt to compute an optimal alignment with respect to these metrics  we leave this line
of research for future work 
fig     shows the maximum number of edges that have to be stored in open during the
search  in dependence of the search threshold in the final iteration  for better comparability 
we only included those problems in the diagram that consist of   sequences  the logarithmic
scale emphasizes that the growth fits an exponential curve quite well  roughly speaking  an
increase of the cost threshold by    leads to a ten fold increase in the space requirements 
this relation is similarly applicable to the number of expansions  fig      
fig     depicts the proportion between the maximum open list size and the combined
maximum size of open and closed  it is clearly visible that due to the pruning of edges
outside of possible solution paths  the closed list contributes less and less to the overall
space requirements the more difficult the problems become 
finally  we estimate the reduction in the size of the open list compared to all relevant
edges by the ratio of the maximum open size in the last iteration of iddp to the total
number of expansions in this stage  which is equal to the number of edges with f  value
less or equal to the threshold  considering possible overshoot of iddp  algorithm pea
   

fischroedl

maximum size of open   closed

 e   

 e   

      

     

    

  fold heuristic
div conq heuristic
  fold heuristic
  fold heuristic

   
 

 

 

 
  
number of sequences

  

  

figure     combined maximum size of open and closed  for different heuristics  elongation
factors ef tu and ef    

 e   
 e   

max open

      
     
    
   
  

short
medium length
long

 
 

  

   
   
   
threshold   lower bound

   

   

figure     maximum size of open list  dependent on the final search threshold  balibase   

   

fian improved search algorithm for optimal multiple sequence alignment

 e   
 e   

expansions

 e   
      
     
    
   

short
medium length
long

  
 

  

   
   
   
threshold   lower bound

   

   

figure     number of expansions in the final search iteration  balibase   

  

max open  max open   closed    

  
  
  
  
  
  
  

short
medium length
long

 
 

  

   
   
   
threshold   lower bound

   

   

figure     maximum number of open edges  divided by combined maximum of open and
closed  balibase   

   

fischroedl

 

short
medium length
long

max open   expansions    

 

 

 

 

 
 

  

   
   
   
threshold   lower bound

   

   

figure     percentage of reduction in open size  balibase   
would expand at least half of these nodes  the proportion ranges between     to   percent
 cf  fig       its considerable scatter indicates the dependence on individual problem properties  however  a slight average decrease can be noticed for the more difficult problems 

   

fian improved search algorithm for optimal multiple sequence alignment

   conclusion and discussion
we have presented a new search algorithm for optimal multiple sequence alignment that
combines the effective use of a heuristic bound as in best first search with the ability of the
dynamic programming approach to reduce the maximum size of the open and closed lists
by up to one order of magnitude of the sequence length  the algorithm performs a series
of searches with successively increasing bounds that explore the search space in dp order 
the thresholds are chosen adaptively so that the expected overhead in recomputations is
bounded by a constant factor 
we have demonstrated that the algorithm can outperform one of the currently most
successful algorithms for optimal multiple sequence alignments  partial expansion a   both
in terms of computation time and memory consumption  moreover  the iterative deepening
strategy alleviates the use of partially computed higher dimensional heuristics  to the best
of our knowledge  the algorithm is the first one that is able to solve standard benchmark
alignment problems in balibase with a biologically realistic cost function including affine
gap costs without end gap penalties  the quality of the alignment is in the range of the
best heuristic programs  while we have concentrated on algorithmic feasibility  we deem it
worthwhile to incorporate their refined cost metrics for better results  we will study this
question in future work 
recently  we learned about related approaches developed simultaneously and independently by zhou and hansen      b         sweepa explores a search graph according
to layers in a partial order  but still uses the f  value for selecting nodes within one layer 
breadth first heuristic search implicitly defines the layers in a graph with uniform costs
according to the breadth first traversal  both algorithms incorporate upper bounds on the
optimal solution cost for pruning  however  the idea of adaptive threshold determination to
limit re expansion overhead to a constant factor is not described  moreover  they do not
consider the flexible use of additional memory to minimize the divide and conquer solution
reconstruction phase 
although we described our algorithm entirely within the framework of the msa problem 
it is straightforward to transfer it to any domain in which the state space graph is directed
and acyclic  natural candidates include applications where such an ordering is imposed by
time or space coordinates  e g   finding the most likely path in a markov model 
two of the balibase benchmark problems could still not be solved by our algorithm
within the computers main memory limit  future work will include the integration of
techniques exploiting secondary memory  we expect that the level wise exploration scheme
of our algorithm lends itself naturally to external search algorithms  another currently very
active research topic in artificial intelligence and theoretical computer science 

acknowledgments
the author would like to thank the reviewers of this article whose comments have helped
in significantly improving it 

   

fischroedl

appendix a

table    results for balibase reference    group of short sequences with low amino acid
identity  the columns denote  s  number of aligned sequences    upper
bound for precomputing optimal solutions for partial problems in last iteration of
main search  g   t   optimal solution cost  h s   lower bound for solution cost 
using heuristics   exp  total number of expansions in all iterations of the main
search   op  peak number of edges in open list over the course of the search 
 op cl  peak combined number of edges in either open or closed list during
search   heu  peak number of subalignment edge costs stored as heuristic 
time  total running time including auxiliary and main search  in seconds  mem
 peak total memory usage for face alignments  heuristic  and main search  in
kb 
 aboa
 idy
 r  
 tvxa
 ubi
 wit
 trx

s
 
 
 
 
 
 
 


  
  
  
  
  
  
  

g   t 
    
    
    
    
    
     
    

h s 
    
    
    
    
    
     
    

 exp
       
       
      
       
       
       
     

 op
      
     
     
     
     
      
    

 op cl
      
      
     
     
     
      
    

 heu
       
      
     
      
      
       
      

time
       
       
      
      
      
       
     

mem
     
     
    
    
    
     
    

table    short sequences  medium similarity 
 aab
 fjla
 hfh
 hpi
 csy
 pfc
 tgxa
 ycc
 cyr
   c

s
 
 
 
 
 
 
 
 
 
 


  
  
  
  
  
  
  
  
  
  

g   t 
    
     
     
    
     
     
    
    
    
     

h s 
    
     
     
    
     
     
    
    
    
     

 exp
   
   
      
    
     
      
     
     
      
       

 op
  
   
    
  
    
    
   
    
     
     

   

 op cl
  
   
    
   
    
    
    
    
     
     

 heu
    
     
     
    
     
     
    
     
      
      

time
     
     
      
     
     
      
     
     
      
       

mem
   
    
    
   
    
    
   
    
    
    

fian improved search algorithm for optimal multiple sequence alignment

table    short sequences  high similarity 
s
 
 
 
 
 
 
 
 
 
 

 aho
 csp
 dox
 fkj
 fmb
 krn
 plc
 fxb
 mhr
 rnt


  
  
  
  
  
  
  
  
  
  

g   t 
    
    
    
     
    
    
     
    
     
     

h s 
    
    
    
     
    
    
     
    
     
     

 exp
     
  
   
    
   
   
   
  
   
   

 op
    
 
  
   
 
 
  
 
 
  

 op cl
    
  
   
   
   
  
   
  
   
   

 heu
     
    
    
     
    
    
     
    
    
    

time
     
     
     
     
     
     
     
     
     
     

mem
    
   
   
    
   
    
    
   
    
    

table    medium length sequences  low similarity 

 bbt 
 sbp
 hava
 uky
 hsda
 pia
 grs
kinase

s
 
 
 
 
 
 
 
 


   
   
   
  
  
   
   
   

g   t 
     
     
     
     
     
     
     
     

h s 
     
     
     
     
     
     
     
     

 exp
         
          
          
         
        
        
         
          

 op
        
       
        
      
      
      
      
        

 op cl
        
        
        
       
      
       
       
        

 heu
        
        
        
        
        
        
        
        

time
         
          
          
        
        
        
        
          

mem
      
      
      
      
     
      
      
      

table    medium length sequences  medium similarity 

 ad 
 aym 
 gdoa
 ldg
 mrj
 pgta
 pii
 ton
 cba

s
 
 
 
 
 
 
 
 
 


  
  
  
  
  
  
  
   
   

g   t 
     
     
     
     
     
     
     
     
     

h s 
     
     
     
     
     
     
     
     
     

 exp
   
      
        
      
      
       
     
        
        

 op
  
    
     
    
    
     
   
      
       

   

 op cl
   
    
      
    
    
     
    
      
       

 heu
     
     
       
      
     
      
      
        
        

time
     
      
       
      
     
      
     
        
        

mem
    
    
     
    
    
    
    
     
      

fischroedl

table    medium length sequences  high similarity 
s
 
 
 
 
 
 
 
 
 
 

 amk
 ar a
 ezm
 led
 ppn
 pysa
 thm
 tis
 zin
 ptp


  
  
  
  
  
  
  
  
  
  

g   t 
     
     
     
     
     
     
     
     
     
     

h s 
     
     
     
     
     
     
     
     
     
     

 exp
   
    
   
     
     
     
   
     
   
    

 op
 
   
 
    
   
   
 
   
  
   

 op cl
   
   
   
    
   
   
   
   
   
   

 heu
     
     
     
     
     
     
    
     
    
     

time
     
     
     
     
     
     
     
     
     
     

mem
    
    
    
    
    
    
    
    
    
    

table    long sequences  low similarity 

 ajsa
 cpt
 lvl
 ped
 myr
 enl

s
 
 
 
 
 
 


   
   
   
  
   
  

g   t 
     
     
     
     
     
     

h s 
     
     
     
     
     
     

 exp
         
      
         
       
          
       

 op
       
    
       
    
       
    

 op cl
       
     
       
     
        
     

 heu
        
        
        
 
         
 

time
        
       
         
      
          
      

 heu
        
     
         
        
       
       
        
       
       
        
        
        

time
        
     
        
       
       
       
        
       
       
         
         
       

mem
      
     
      
    
      
    

table    long sequences  medium similarity 

 ac 
 adj
 bgl
 dlc
 eft
 fiea
 gowa
 pkm
 sesa
 ack
arp
glg

s
 
 
 
 
 
 
 
 
 
 
 
 


  
  
   
   
  
  
   
  
  
   
   
   

g   t 
     
     
     
     
     
     
     
     
     
     
     
     

h s 
     
     
     
     
     
     
     
     
     
     
     
     

 exp
         
      
         
        
       
       
        
        
       
         
         
       

 op
      
    
      
     
     
     
      
     
     
       
       
     

   

 op cl
       
    
       
      
     
     
      
      
      
        
       
      

mem
      
    
      
     
     
     
     
     
     
      
      
     

fian improved search algorithm for optimal multiple sequence alignment

table     long sequences  high similarity 

 ad 
 gpb
 gtr
 lcf
 rtha
 taq
 pmg
actin

s
 
 
 
 
 
 
 
 


  
  
  
   
   
   
  
  

g   t 
     
      
     
      
     
      
     
     

h s 
     
      
     
      
     
      
     
     

 exp
      
       
       
         
        
          
       
      

   

 op
    
     
     
       
     
       
    
     

 op cl
    
     
     
       
      
        
     
     

 heu
     
       
       
        
        
         
      
      

time
     
       
       
         
        
        
      
      

mem
    
     
     
      
     
       
    
     

fischroedl

references
altschul  s   gish  w   miller  w   myers  e     lipman  d          basic local alignment
search tool  journal of molecular biology              
altschul  s  f          gap costs for multiple sequence alignment  journal of theoretical
biology              
carrillo  h     lipman  d          the multiple sequence alignment problem in biology 
siam journal of applied mathematics                   
chan  s  c   wong  a  k  c     chiu  d  k  y          a survey of multiple sequence
comparison techniques  bulletin of mathematical biology                 
culberson  j  c     schaeffer  j          pattern databases  computational intelligence 
               
davidson  a          a fast pruning algorithm for optimal sequence alignment  in proceedings of the  nd ieee international symposium on bioinformatics and bioengineering
 bibe       pp       
dayhoff  m  o   schwartz  r  m     orcutt  b  c          a model of evolutionary change
in proteins  in dayhoff  m  o   ed    atlas of protein sequence and structure  pp 
        washington  d c  national biomedical research foundation 
dial  r  b          shortest path forest with topological ordering  comm  acm          
       
dijkstra  e  w          a note on two problems in connection with graphs   numerische
mathematik            
gupta  s   kececioglu  j     schaeffer  a          improving the practical space and time
efficiency of the shortest paths approach to sum of pairs multiple sequence alignment 
j  computational biology                
gusfield  d          efficient methods for multiple sequence alignment with guaranteed
error bounds  bull  of math  biol                  
hart  p  e   nilsson  n  j     raphael  b          a formal basis for heuristic determination
of minimum path cost  ieee trans  on systems science and cybernetics            
hirschberg  d  s          a linear space algorithm for computing maximal common subsequences  comm  acm                 
ikeda  t     imai  h          fast a  algorithms for multiple sequence alignment  in
proceedings of the genome informatics workshop  pp       
jones  d  t   taylor  w  r     thornton  j  m          the rapid generation of mutation
data matrices from protein sequences  cabios            
kobayashi  h     imai  h          improvement of the a  algorithm for multiple sequence
alignment  in miyano  s     takagi  t   eds    genome informatics  pp         
tokyo  universal academy press 
korf  r  e          depth first iterative deepening  an optimal admissible tree search 
artificial intelligence                
   

fian improved search algorithm for optimal multiple sequence alignment

korf  r  e          divide and conquer bidirectional search  first results  in proceedings
of the sixteenth international conference on artificial intelligence  ijcai      pp 
          stockholm  sweden 
korf  r  e     zhang  w          divide and conquer frontier search applied to optimal
sequence alignment  in proceedings of the eighteenth national conference on artificial
intelligence  aaai      pp         
mcnaughton  m   lu  p   schaeffer  j     szafron  d          memory efficient a  heuristics
for multiple sequence alignment  in proceedings of the eighteenth national conference
on artificial intelligence  aaai      edmonton  alberta  canada 
spouge  j  l          speeding up dynamic programming algorithms for finding optimal
lattice paths  siam j  applied mathematics                   
thompson  j  d   plewniak  f     poch  o          a comprehensive comparison of multiple
sequence alignment programs  nucleic acids res                     
ukkonen  e          algorithms for approximate string matching  information and control 
           
wah  b  w     shang  y          a comparison of a class of ida  search algorithms 
international journal of tools with artificial intelligence                
wang  l     jiang  t          on the complexity of multiple sequence alignment  journal
of computational biology            
yoshizumi  t   miura  t     ishida  t          a  with partial expansion for large branching
factor problems  in aaai iaai  pp         
zhou  r     hansen  e  a       a   sparse memory graph search  in   th international
joint conference on artificial intelligence  ijcai      acapulco  mexico 
zhou  r     hansen  e  a       b   sweep a   space efficient heuristic search in partiallyordered graphs  in   th ieee international conference on tools with artificial intelligence  sacramento  ca 
zhou  r     hansen  e  a          breadth first heuristic search  in fourteenth international conference on automated planning and scheduling  icaps      whistler  bc 
canada 

   

fi