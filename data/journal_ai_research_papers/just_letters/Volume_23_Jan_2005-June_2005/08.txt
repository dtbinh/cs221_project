journal of artificial intelligence research                  

submitted        published      

hybrid bdi pomdp framework for multiagent teaming
ranjit nair

ranjit nair honeywell com

automation and control solutions
honeywell laboratories  minneapolis  mn      

milind tambe

tambe usc edu

department of computer science
university of southern california  los angeles  ca      

abstract
many current large scale multiagent team implementations can be characterized as
following the belief desire intention  bdi  paradigm  with explicit representation of team
plans  despite their promise  current bdi team approaches lack tools for quantitative
performance analysis under uncertainty  distributed partially observable markov decision
problems  pomdps  are well suited for such analysis  but the complexity of finding optimal
policies in such models is highly intractable  the key contribution of this article is a
hybrid bdi pomdp approach  where bdi team plans are exploited to improve pomdp
tractability and pomdp analysis improves bdi team plan performance 
concretely  we focus on role allocation  a fundamental problem in bdi teams  which
agents to allocate to the different roles in the team  the article provides three key contributions  first  we describe a role allocation technique that takes into account future
uncertainties in the domain  prior work in multiagent role allocation has failed to address
such uncertainties  to that end  we introduce rmtdp  role based markov team decision problem   a new distributed pomdp model for analysis of role allocations  our
technique gains in tractability by significantly curtailing rmtdp policy search  in particular  bdi team plans provide incomplete rmtdp policies  and the rmtdp policy search
fills the gaps in such incomplete policies by searching for the best role allocation  our
second key contribution is a novel decomposition technique to further improve rmtdp
policy search efficiency  even though limited to searching role allocations  there are still
combinatorially many role allocations  and evaluating each in rmtdp to identify the best
is extremely difficult  our decomposition technique exploits the structure in the bdi team
plans to significantly prune the search space of role allocations  our third key contribution
is a significantly faster policy evaluation algorithm suited for our bdi pomdp hybrid approach  finally  we also present experimental results from two domains  mission rehearsal
simulation and robocuprescue disaster rescue simulation 

   introduction
teamwork  whether among software agents  or robots  and people  is a critical capability
in a large number of multiagent domains ranging from mission rehearsal simulations  to
robocup soccer and disaster rescue  to personal assistant teams  already a large number of multiagent teams have been developed for a range of domains  pynadath   tambe 
      yen  yin  ioerger  miller  xu    volz        stone   veloso        jennings       
grosz  hunsberger    kraus        decker   lesser        tambe  pynadath    chauvat 
      da silva   demazeau         these existing practical approaches can be characterized as situated within the general belief desire intention  bdi  approach  a paradigm
c
    
ai access foundation  all rights reserved 

finair   tambe

for designing multiagent systems  made increasingly popular due to programming frameworks  tambe et al         decker   lesser        tidhar      b  that facilitate the design
of large scale teams  within this approach  inspired explicitly or implicitly by bdi logics 
agents explicitly represent and reason with their team goals and plans  wooldridge        
this article focuses on analysis of such bdi teams  to provide feedback to aid human
developers and possibly to agents participating in a team  on how the team performance
in complex dynamic domains can be improved  in particular  it focuses on the critical
challenge of role allocation in building teams  tidhar  rao    sonenberg        hunsberger
  grosz         i e  which agents to allocate to the various roles in the team  for instance 
in mission rehearsal simulations  tambe et al          we need to select the numbers and
types of helicopter agents to allocate to different roles in the team  similarly  in disaster
rescue  kitano  tadokoro  noda  matsubara  takahashi  shinjoh    shimada         role
allocation refers to allocating fire engines and ambulances to fires and it can greatly impact
team performance  in both these and other such domains  the performance of the team is
linked to important metrics such as loss of human life and property and thus it is critical
to analyze team performance and suggest improvements 
while bdi frameworks facilitate human design of large scale teams  the key difficulty
in analyzing role allocation in these teams is due to the uncertainty that arises in complex
domains  for example  actions may fail and the world state may be only partially observable
to the agents owing to physical properties of the environment or imperfect sensing  role
allocation demands such future uncertainties be taken into account  e g  the fact that an
agent may fail during execution and may or may not be replaced by another must be taken
into account when determining the role allocation  yet most current role allocation algorithms do not address such uncertainty  see section       indeed  such uncertainty requires
quantitative comparison of different role allocations  however  tools for such quantitative
evaluations of bdi teams are currently absent  thus  given these uncertainties  we may be
required to experimentally recreate a large number of possible scenarios  in a real domain
or in simulations  to evaluate and compare different role allocations 
fortunately  the emergence of distributed partially observable markov decision problems  pomdps  provides models  bernstein  zilberstein    immerman        boutilier 
      pynadath   tambe        xuan  lesser    zilberstein        that can be used for
quantitative analysis of agent teams in uncertain domains  distributed pomdps represent a class of formal models that are powerful enough to express the uncertainty in these
dynamic domains arising as a result of non determinism and partial observability and in
principle  can be used to generate and evaluate complete policies for the multiagent team 
however  there are two shortcomings in these models that prevents their application in
the analysis of role allocation  first  previous work on analysis has focused on communication  pynadath   tambe        xuan et al          rather than role allocation or any
other coordination decisions  second  as shown by bernstein et al          the problem
of deriving the optimal policy is generally computationally intractable  the corresponding
decision problem is nexp complete   thus  applying optimal policies for analysis is highly
intractable 
to address the first difficulty  we derive rmtdp  role based multiagent team decision
problem   a distributed pomdp framework for quantitatively analyzing role allocations 
using this framework  we show that  in general  the problem of finding the optimal role
   

fihybrid bdi pomdp framework for multiagent teaming

completed policy  
additions to bdi team plan

bdi team plan
rmtdp
search policy space

incomplete policy
bdi interpreter

domain
rmtdp model

figure    integration of bdi and pomdp 

allocation policy is computationally intractable  the corresponding decision problem is still
nexp complete   this shows that improving the tractability of analysis techniques for role
allocation is a critically important issue 
therefore  in order to make the quantitative analysis of multiagent teams using rmtdp
more tractable  our second contribution provides a hybrid bdi pomdp approach that
combines the native strengths of the bdi and pomdp approaches  i e   the ability in bdi
frameworks to encode large scale team plans and the pomdp ability to quantitatively
evaluate such plans  this hybrid approach is based on three key interactions that improve
the tractability of rmtdp and the optimality of bdi agent teams  the first interaction is
shown in figure    in particular  suppose we wish to analyze a bdi agent team  each agent
consisting of a bdi team plan and a domain independent interpreter that helps coordinate
such plans  acting in a domain  then as shown in figure    we model the domain via an
rmtdp  and rely on the bdi team plan and interpreter for providing an incomplete policy
for this rmtdp  the rmtdp model evaluates different completions of this incomplete
policy and provides an optimally completed policy as feedback to the bdi system  thus 
the rmtdp fills in the gaps in an incompletely specified bdi team plan optimally  here
the gaps we concentrate on are the role allocations  but the method can be applied to other
key coordination decisions  by restricting the optimization to only role allocation decisions
and fixing the policy at all other points  we are able to come up with a restricted policy
space  we then use rmtdps to effectively search this restricted space in order to find the
optimal role allocation 
while the restricted policy search is one key positive interaction in our hybrid approach 
the second interaction consists of a more efficient policy representation used for converting
a bdi team plan and interpreter into a corresponding policy  see figure    and a new
algorithm for policy evaluation  in general  each agents policy in a distributed pomdp
is indexed by its observation history  bernstein et al         pynadath   tambe        
   

finair   tambe

however  in a bdi system  each agent performs its action selection based on its set of
privately held beliefs which is obtained from the agents observations after applying a belief
revision function  in order to evaluate the teams performance  it is sufficient in rmtdp to
index the agents policies by their belief state  represented here by their privately held beliefs 
instead of their observation histories  this shift in representation results in considerable
savings in the amount of time needed to evaluate a policy and in the space required to
represent a policy 
the third key interaction in our hybrid approach further exploits bdi team plan structure for increasing the efficiency of our rmtdp based analysis  even though rmtdp
policy space is restricted to filling in gaps in incomplete policies  many policies may result
given the large number of possible role allocations  thus enumerating and evaluating each
possible policy for a given domain is difficult  instead  we provide a branch and bound algorithm that exploits task decomposition among sub teams of a team to significantly prune
the search space and provide a correctness proof and worst case analysis of this algorithm 
in order to empirically validate our approach  we have applied rmtdp for allocation
in bdi teams in two concrete domains  mission rehearsal simulations  tambe et al        
and robocuprescue  kitano et al          we first present the  significant  speed up
gained by our three interactions mentioned above  next  in both domains  we compared
the role allocations found by our approach with state of the art techniques that allocate
roles without uncertainty reasoning  this comparison shows the importance of reasoning
about uncertainty when determining the role allocation for complex multiagent domains  in
the robocuprescue domain  we also compared the allocations found with allocations chosen
by humans in the actual robocuprescue simulation environment  the results showed that
the role allocation technique presented in this article is capable of performing at human
expert levels in the robocuprescue domain 
the article is organized as follows  section   presents background and motivation  in
section    we introduce the rmtdp model and present key complexity results  section
  explains how a bdi team plan can be evaluated using rmtdp  section   describes the
analysis methodology for finding the optimal role allocation  while section   presents an
empirical evaluation of this methodology  in section    we present related work and in
section    we list our conclusions 

   background
this section first describes the two domains that we consider in this article  an abstract
mission rehearsal domain  tambe et al         and the robocuprescue domain  kitano
et al          each domain requires us to allocate roles to agents in a team  next  teamoriented programming  top   a framework for describing team plans is described in the
context of these two domains  while we focus on top  as discussed further in section     
our techniques would be applicable in other frameworks for tasking teams  stone   veloso 
      decker   lesser        
    domains
the first domain that we consider is based on mission rehearsal simulations  tambe et al  
       for expository purposes  this has been intentionally simplified  the scenario is as
   

fihybrid bdi pomdp framework for multiagent teaming

follows  a helicopter team is executing a mission of transporting valuable cargo from point
x to point y through enemy terrain  see figure     there are three paths from x to y of
different lengths and different risk due to enemy fire  one or more scouting sub teams must
be sent out  one for each path from x to y   and the larger the size of a scouting sub team
the safer it is  when scouts clear up any one path from x to y  the transports can then
move more safely along that path  however  the scouts may fail along a path  and may
need to be replaced by a transport at the cost of not transporting cargo  owing to partial
observability  the transports may not receive an observation that a scout has failed or that
a route has been cleared  we wish to transport the most amount of cargo in the quickest
possible manner within the mission deadline 
the key role allocation decision here is given a fixed number of helicopters  how should
they be allocated to scouting and transport roles  allocating more scouts means that the
scouting task is more likely to succeed  but there will be fewer helicopters left that can
be used to transport the cargo and consequently less reward  however  allocating too few
scouts could result in the mission failing altogether  also  in allocating the scouts  which
routes should the scouts be sent on  the shortest route would be preferable but it is more
risky  sending all the scouts on the same route decreases the likelihood of failure of an
individual scout  however  it might be more beneficial to send them on different routes  e g 
some scouts on a risky but short route and others on a safe but longer route 
thus there are many role allocations to consider  evaluating each is difficult because
role allocation must look ahead to consider future implications of uncertainty  e g  scout
helicopters can fail during scouting and may need to be replaced by a transport  furthermore  failure or success of a scout may not be visible to the transport helicopters and hence
a transport may not replace a scout or transports may never fly to the destination 
scout
transports

x

route  

route  

y

enemy gun
route  

figure    mission rehearsal domain 

the second example scenario  see figure     set up in the robocuprescue disaster
simulation environment  kitano et al          consists of five fire engines at three different
fire stations  two each at stations       and the last at station    and five ambulances
stationed at the ambulance center  two fires  in top left and bottom right corners of the
map  start that need to be extinguished by the fire engines  after a fire is extinguished 
ambulance agents need to save the surviving civilians  the number of civilians at each
   

finair   tambe

location is not known ahead of time  although the total number of civilians in known  as
time passes  there is a high likelihood that the health of civilians will deteriorate and fires
will increase in intensity  yet the agents need to rescue as many civilians as possible with
minimal damage to the buildings  the first part of the goal in this scenario is therefore to
first determine which fire engines to assign to each fire  once the fire engines have gathered
information about the number of civilians at each fire  this is transmitted to the ambulances 
the next part of the goal is then to allocate the ambulances to a particular fire to rescue
the civilians trapped there  however  ambulances cannot rescue civilians until fires are fully
extinguished  here  partial observability  each agent can only view objects within its visual
range   and uncertainty related to fire intensity  as well as location of civilians and their
health add significantly to the difficulty 

c 
f 
f 
f 
c 
a

figure    robocuprescue scenario  c  and c  denote the two fire locations  f   f  and
f  denote fire stations      and   respectively and a denotes the ambulance
center 

    team oriented programming
the aim of the team oriented programming  top   pynadath   tambe        tambe et al  
      tidhar      b  framework is to provide human developers  or automated symbolic
planners  with a useful abstraction for tasking teams  for domains such as those described
in section      it consists of three key aspects of a team   i  a team organization hierarchy
consisting of roles   ii  a team  reactive  plan hierarchy  and  iii  an assignment of roles
to sub plans in the plan hierarchy  the developer need not specify low level coordination
details  instead  the top interpreter  the underlying coordination infrastructure  automatically enables agents to decide when and with whom to communicate and how to reallocate
   

fihybrid bdi pomdp framework for multiagent teaming

roles upon failure  the top abstraction enables humans to rapidly provide team plans for
large scale teams  but unfortunately  only a qualitative assessment of team performance is
feasible  thus  a key top weakness is the inability to quantitatively evaluate and optimize
team performance  e g   in allocating roles to agents only a qualitative matching of capabilities may be feasible  as discussed later  our hybrid bdi pomdp model addresses this
weakness by providing techniques for quantitative evaluation 
as a concrete example  consider the top for the mission rehearsal domain  we first
specify the team organization hierarchy  see figure   a    task force is the highest level
team in this organization and consists of two roles scouting and transport  where the
scouting sub team has roles for each of the three scouting sub sub teams  next we specify
a hierarchy of reactive team plans  figure   b    reactive team plans explicitly express
joint activities of the relevant team and consist of   i  pre conditions under which the plan
is to be proposed   ii  termination conditions under which the plan is to be ended  and  iii 
team level actions to be executed as part of the plan  an example plan will be discussed
shortly   in figure   b   the highest level plan execute mission has three sub plans 
doscouting to make one path from x to y safe for the transports  dotransport to
move the transports along a scouted path  and remainingscouts for the scouts which
have not reached the destination yet to get there 
execute mission  task force 
doscouting
 task force 

remainingscouts
dotransport
 scouting team   transport team 

task force
scoutroutes
waitatbase
 transport team   scouting team 

scouting team

transport team

sctteama sctteamb sctteamc

scoutroute  scoutroute  scoutroute 
 sctteama   sctteamb   sctteamc 

 a 

 b 

figure    top for mission rehearsal domain a  organization hierarchy  b  plan hierarchy 

figure   b  also shows coordination relationships  an and relationship is indicated
with a solid arc  while an or relationship is indicated with a dashed arc  thus  waitatbase and scoutroutes must both be done while at least one of scoutroute  
scoutroute  or scoutroute  need be performed  there is also a temporal dependence relationship among the sub plans  which implies that sub teams assigned to perform
dotransport or remainingscouts cannot do so until the doscouting plan has completed  however  dotransport and remainingscouts execute in parallel  finally  we
assign roles to plans  figure   b  shows the assignment in brackets adjacent to the plans 
for instance  task force team is assigned to jointly perform execute mission while sctteama is assigned to scoutroute  
the team plan corresponding to execute mission is shown in figure    as can be
seen  each team plan consists of a context  pre conditions  post conditions  body and constraints  the context describes the conditions that must be fulfilled in the parent plan while
the pre conditions are the particular conditions that will cause this sub plan to begin exe   

finair   tambe

cution  thus  for execute mission  the pre condition is that the team mutually believes
 mb   that they are the start location  the post conditions are divided into achieved 
unachievable and irrelevant conditions under which this sub plan will be terminated  the
body consists of sub plans that exist within this team plan  lastly  constraints describe any
temporal constraints that exist between sub plans in the body  the description of all the
plans in the plan hierarchy of figure   b  is given in appendix a 
executemission 
context 
pre conditions   mb  taskforce  location taskforce    start 
achieved   mb  taskforce   achieved doscouting   achieved dotransport      time
  t   mb  taskforce 
achieved remainingscouts     helo  scoutingteam  alive helo  
location helo     end   
unachievable   mb  taskforce  unachievable doscouting     mb  taskforce 
unachievable dotransport  
 achieved remainingscouts    helo  scoutingteam  alive helo  
location helo     end   
irrelevant  
body 
doscouting
dotransport
remainingscouts
constraints  doscouting  dotransport  doscouting  remainingscouts

figure    example team plan  mb refers to mutual belief 
just as in htn  dix  muoz avila  nau    zhang        erol  hendler    nau         the
plan hierarchy of a top gives a decomposition of the task into smaller tasks  however  the
language of tops is richer than the language of early htn planning  erol et al         which
contained just simple ordering constraints  as seen in the above example  the plan hierarchy
in tops can also contain relationships like and and or  in addition  just like more recent
work in htn planning  dix et al          sub plans in tops can contain pre conditions and
post conditions  thus allowing for conditional plan execution  the main differences between
tops and htn planning are   i  tops contain an organization hierarchy in addition to a
plan hierarchy   ii  the top interpreter ensures that the team executes its plans coherently 
as seen later  tops will be analyzed with all of this expressiveness including conditional
execution  however  since our analysis will focus on a fixed time horizon  any loops in the
task description will be unrolled up to the time horizon 
   mutual belief  wooldridge         shown as  mb hteami x  in figure    refers to a private belief held
by each agent in the team that they each believe that a fact x is true  and that each of the other agents
in the team believe that x is true  and that every agent believes that every other agent believes that x
is true and so on  such infinite levels of nesting are difficult to realize in practice  thus  as in practical
bdi implementations  for the purposes of this article  a mutual belief is approximated to be a private
belief held by an agent that all the agents in the team believe that x is true 

   

fihybrid bdi pomdp framework for multiagent teaming

new observation for
agent i

belief update
function

private beliefs of
agent i

figure    mapping of observations to beliefs 

during execution  each agent has a copy of the top  the agent also maintains a set
of private beliefs  which are a set of propositions that the agent believes to be true  see
figure     when an agent receives new beliefs  i e  observations  including communication  
the belief update function is used to update its set of privately held beliefs  for instance 
upon seeing the last scout crashed  a transport may update its privately held beliefs to
include the belief criticalfailure doscouting   in practical bdi systems  such belief
update computation is of low complexity  e g  constant or linear time   once beliefs
are updated  an agent selects which plan to execute by matching its beliefs with the preconditions in the plans  the basic execution cycle is similar to standard reactive planning
systems such as prs  georgeff   lansky        
during team plan execution  observations in the form of communications often arise
because of the coordination actions executed by the top interpreter  for instance  top
interpreters have exploited bdi theories of teamwork  such as levesque et al s theory
of joint intentions  levesque  cohen    nunes        which require that when an agent
comes to privately believe a fact that terminates the current team plan  i e  matches the
achievement or unachievability conditions of a team plan   then it communicates this fact
to the rest of the team  by performing such coordination actions automatically  the top
interpreter enables coherence at the initiation and termination of team plans within a top 
some further details and examples of tops can be seen in the work of pynadath and
tambe         tambe et al         and tidhar      b  
we can now more concretely illustrate the key challenges in role allocation mentioned
earlier  first  a human developer must allocate available agents to the organization hierarchy  figure   a    to find the best role allocation  however  there are combinatorially many
allocations to choose from  hunsberger   grosz        tambe et al          for instance 
starting with just   homogeneous helicopters results in    different ways of deciding how
many agents to assign to each scouting and transport sub team  this problem is exacerbated by the fact that the best allocation varies significantly based on domain variations 
for example  figure   shows three different assignments of agents to the team organization hierarchy  each found in our analysis to be the best for a given setting of failure and
observation probabilities  details in section     for example  increasing the probability of
failures on all routes resulted in the number of transports in the best allocation changing
from four  see figure   b   to three  see figure   a    where an additional scout was added
to sctteamb  if failures were not possible at all  the number of transports increased to
five  see figure   c    our analysis takes a step towards selecting the best among such
allocations 
   

finair   tambe

task force
scouting team

task force

transport team  

scouting team

sctteama   sctteamb   sctteamc  

transport team  

sctteama   sctteamb   sctteamc  

 a  medium probability

 b  low probability
task force

scouting team

transport team  

sctteama   sctteamb   sctteamc  

 c  zero probability

figure    best role allocations for different probabilities of scout failure 

figure   shows the top for the robocuprescue scenario  as can be seen  the plan hierarchy for this scenario consists of a pair of extinguishfire and rescuecivilians plans
done in parallel  each of which further decompose into individual plans   these individual plans get the fire engines and ambulances to move through the streets using specific
search algorithms  however  these individual plans are not relevant for our discussions in
this article  interested readers should refer to the description of our robocuprescue team
entered into the robocup competitions of       nair  ito  tambe    marsella          the
organizational hierarchy consists of task force comprising of two engine sub teams  one
for each fire and an ambulance team  where the engine teams are assigned to extinguishing
the fires while the ambulance team is assigned to rescuing civilians  in this particular top 
the assignment of ambulances to ambulanceteama and ambulanceteamb is conditioned
on the communication c  indicated by ambulanceteama c and ambulanceteamb c 
c is not described in detail in this figure  but it refers to the communication that is received from the fire engines that describes the number of civilians present at each fire  the
problem is which engines to assign to each engine team and for each possible value of c 
which ambulances to assign to each ambulance team  note that engines have differing
capabilities owing to differing distances from fires while all the ambulances have identical
capabilities 
task force
engineteama

engineteamb

ambulanceteam

ambulanceteama  c

ambulanceteamb  c

 a 
executemission
 task force 
extinguishfire 
 engineteama 

rescuecivilians 
 ambulanceteama 

extinguishfire 
 engineteamb 

rescuecivilians 
 ambulanceteamb 

 b 

figure    top for robocuprescue scenario a  organization hierarchy  b  plan hierarchy 

   

fihybrid bdi pomdp framework for multiagent teaming

   role based multiagent team decision problem
multiagent team decision problem  mtdp   pynadath   tambe        is inspired by
the economic theory of teams  marschak   radner        ho        yoshikawa        
in order to do quantitative analysis of key coordination decisions in multiagent teams  we
extend mtdp for the analysis of the coordination actions of interest  for example  the
com mtdp  pynadath   tambe        is an extension of mtdp for the analysis of communication  in this article  we illustrate a general methodology for analysis of other aspects
of coordination and present the rmtdp model for quantitative analysis of role allocation
and reallocation as a concrete example  in contrast to bdi systems introduced in the previous section  rmtdp enables explicit quantitative optimization of team performance  note
that  while we use mtdp  other possible distributed pomdp models could potentially also
serve as a basis  bernstein et al         xuan et al         
    multiagent team decision problem
given a team of n agents  an mtdp  pynadath   tambe        is defined as a tuple 
hs  a  p    o  ri  it consists of a finite set of states s          m where each j  
   j  m  is a feature of the world state  each agent i can perform an action from its
set of actions ai   where  in ai   a  p  s    a            an    s   gives the probability of
transitioning from state s to state s given that the agents perform the actions   a            an  
jointly  each agent i receives an observation i  i   in i     based on the function
o s    a            an                n    which gives the probability that the agents receive the
observations              n given that the world state is s and they perform   a            an  
jointly  the agents receive a single joint reward r s    a            an    based on the state s
and their joint action   a            an    this joint reward is shared equally by all members
and there is no other private reward that individual agents receive for their actions  thus 
the agents are motivated to behave as a team  taking the actions that jointly yield the
maximum expected reward 
each agent i in an mtdp chooses its actions based on its local policy  i   which is a
mapping of its observation history to actions  thus  at time t  agent i will perform action
i  i            it    this contrasts with a single agent pomdp  where we can index an agents
policy by its belief state  a probability distribution over the world state  kaelbling  littman 
  cassandra         which is shown to be a sufficient statistic in order to compute the
optimal policy  sondik         unfortunately  we cannot directly use single agent pomdp
techniques  kaelbling et al         for maintaining or updating belief states  kaelbling et al  
      in a mtdp  unlike in a single agent pomdp  in mtdp  an agents observation
depends not only on its own actions  but also on unknown actions of other agents  thus 
as with other distributed pomdp models  bernstein et al         xuan et al          in
mtdp  local policies i are indexed by observation histories                  n   refers to
the joint policy of the team of agents 
    extension for explicit coordination
beginning with mtdp  the next step in our methodology is to make an explicit separation
between domain level actions and the coordination actions of interest  earlier work intro   

finair   tambe

duced the com mtdp model  pynadath   tambe         where the coordination action
was fixed to be the communication action  and got separated out  however  other coordination actions could also be separated from domain level actions in order to investigate their
impact  thus  to investigate role allocation and reallocations  actions for allocating agents
to roles and to reallocate such roles are separated out  to that end  we define rmtdp
 role based multiagent team decision problem  as a tuple hs  a  p    o  r  rli with a
new component  rl  in particular  rl    r            rs   is a set of all roles that the agents
can undertake  each instance of role rj may be assigned some agent i to fulfill it  the
actions of each agent are now distinguishable into two types 
role taking actions  i    irj   contains the role taking actions for agent i  irj  i
means that agent i takes on the role rj  rl 
s
role execution actions  i   rj rl irj contains the execution actions for agent i
where irj is the set of agent is actions for executing role rj  rl
in addition we define the set of states as s          m  roles  where the feature roles  a vector  gives the current role that each agent has taken on  the reason for
introducing this new feature is to assist us in the mapping from a bdi team plan to an
rmtdp  thus each time an agent performs a new role taking action successfully  the value
of the feature roles will be updated to reflect this change  the key here is that we not only
model an agents initial role taking action but also subsequent role reallocation  modeling
both allocation and reallocation is important for an accurate analysis of bdi teams  note
that an agent can observe the part of this feature pertaining to its own current role but it
may not observe the parts pertaining to other agents roles 
the introduction of roles allows us to represent the specialized behaviors associated
with each role  e g  a transport vs  a scout role  while filling a particular role  rj   agent
i can perform only role execution actions    irj   which may be different from the roleexecution actions irl for role rl   thus  the feature roles is used to filter actions such that
only those role execution actions that correspond to the agents current role are permitted 
in the worst case  this filtering does not affect the computational complexity  see theorem  
below  but in practice  it can significantly improve performance when trying to find the
optimal policy for the team  since the number of domain actions that each agent can choose
from is restricted by the role that the agent has taken on  also  these different roles can
produce varied effects on the world state  modeled via transition probabilities  p   and the
teams reward  thus  the policies must ensure that agents for each role have the capabilities
that benefit the team the most 
just as in mtdp  each agent chooses which action to perform by indexing its local policy
i by its observation history  in the same epoch some agents could be doing role taking
actions while others are doing role execution actions  thus  each agents local policy i can
be divided into local role taking and role execution policies such that for all observation
histories  i            it   either i  i            it     null or i  i            it     null     
            n   refers to the joint role taking policy of the team of agents while    
            n   refers to the joint role execution policy 
   

fihybrid bdi pomdp framework for multiagent teaming

in this article we do not explicitly model communicative actions as a special action 
thus communication is treated like any other role execution action and the communication
received from other agents are treated as observations  
    complexity results with rmtdp
while section     qualitatively emphasized the difficulty of role allocation  rmtdp helps
us in understanding the complexity more precisely  the goal in rmtdp is to come up with
joint policies  and  that will maximize the total expected reward over a finite horizon
t   note that agents can change their roles according to their local role taking policies  the
agents role execution policy subsequent to this change would contain actions pertaining to
this new role  the following theorem illustrates the complexity of finding such optimal joint
policies 
theorem   the decision problem of determining if there exist policies   and    for an
rmtdp  that yield an expected reward of at least k over some finite horizon t is nexpcomplete 
proof sketch  proof follows from the reduction of mtdp  pynadath   tambe       
to from rmtdp  to reduce mtdp to rmtdp  we set rmtdps role taking actions    
to null and set the rmtdps role execution actions     to the mtdps set of actions  a 
to reduce rmtdp
to mtdp  we generate a new mtdp such that its set of actions  a
s
is equal to    finding the required policy in mtdp is nexp complete  pynadath  
tambe        
as this theorem shows us  solving the rmtdp for the optimal joint role taking and roleexecution policies over even a finite horizon is highly intractable  hence  we focus on the
complexity of just determining the optimal role taking policy  given a fixed role execution
policy  by fixed role execution policy  we mean that the action selection of an agent is
predetermined by the role it is executing 
theorem   the decision problem of determining if there exists a role taking policy     for
an rmtdp  that yields an expected reward of at least k together with a fixed role execution
policy    over some finite horizon t is nexp complete 
proof sketch  we reduce an mtdp to an rmtdp with a different role taking and a
role execution action corresponding to each action in the mtdp  hence  in the rmtdp we
have a role taking action irj for agent i to take on role rj created for each action aj  ai in
the mtdp and each such role rj contains a single role execution action  i e   irj        for
the rmtdp  construct the transition function to be such that a role taking action always
succeeds and the only affected state feature is roles   for the role execution action   irj  
the transition probability is the same as that of the mtdp action  aj  ai corresponding
to the last role taking action irj   the fixed role execution policy is to simply perform
the action    irj   corresponding to the last successful role taking action  irj   thus 
the decision problem for an rmtdp with a fixed role execution policy is at least as hard
   for a more explicit analysis of communication please refer to work done by pynadath and tambe       
and goldman et al         

   

finair   tambe

as the decision problem for an mtdp  furthermore  given theorem    we can conclude
nexp completeness 
this result suggests that even by fixing the role execution policy  solving the rmtdp for
the optimal role taking policy is still intractable  note that theorem   refers to a completely
general globally optimal role taking policy  where any number of agents can change roles at
any point in time  given the above result  in general the globally optimal role taking policy
will likely be of doubly exponential complexity  and so we may be left no choice but to run
a brute force policy search  i e  to enumerate all the role taking policies and then evaluate
them  which together determinethe run time of finding the globally optimal policy  the
number of policies is

  

  t  
   

n

  i e  doubly exponential in the number of observation

histories and the number of agents  thus  while rmtdp enables quantitative evaluation of
teams policies  computing optimal policies is intractable  furthermore  given its low level of
abstraction  in contrast to top  it is difficult for a human to understand the optimal policy 
this contrast between rmtdp and top is at the root of our hybrid model described in
the following section 

   hybrid bdi pomdp approach
having explained top and rmtdp  we can now present a more detailed view of our
hybrid methodology to quantitatively evaluate a top  we first provide a more detailed
interpretation of figure    bdi team plans are essentially top plans  while the bdi
interpreter is the top coordination layer  as shown in figure    an rmtdp model is
constructed corresponding to the domain and the top and its interpreter are converted
into a corresponding  incomplete  rmtdp policy  we can then analyze the top using
analysis techniques that rely on evaluating the rmtdp policy using the rmtdp model
of the domain 
thus  our hybrid approach combines the strengths of the tops  enabling humans to
specify tops to coordinate large scale teams  with the strengths of rmtdp  enabling
quantitative evaluation of different role allocations   on the one hand  this synergistic
interaction enables rmtdps to improve the performance of top based bdi teams  on
the other hand  we have identified at least six specific ways in which tops make it easier
to build rmtdps and to efficiently search rmtdp policies  two of which are discussed in
this section  and four in the next section  in particular  the six ways are 
   tops are exploited in constructing rmtdp models of the domain  section      
   tops are exploited to present incomplete policies to rmtdps  restricting the rmtdp
policy search  section      
   top belief representation is exploited in enabling faster rmtdp policy evaluation
 section      
   top organization hierarchy is exploited in hierarchically grouping rmtdp policies
 section      
   top plan hierarchy is exploited in decomposing rmtdps  section      
   

fihybrid bdi pomdp framework for multiagent teaming

   top plan hierarchies are also exploited in cutting down the observation or belief
histories in rmtdps  section      
the end result of this efficient policy search is a completed rmtdp policy that improves
top performance  while we exploit the top framework  other frameworks for tasking
teams  e g  decker and lesser        and stone and veloso        could benefit from a
similar synergistic interaction 
    guidelines for constructing an rmtdp
as shown in figure    our analysis approach uses as input an rmtdp model of the domain 
as well as an incomplete rmtdp policy  fortunately  not only does the top serve as a
direct mapping to the rmtdp policy  but it can also be utilized in actually constructing
the rmtdp model of the domain  in particular  the top can be used to determine which
domain features are important to model  in addition  the structure in the top can be
exploited in decomposing the construction of the rmtdp 
the elements of the rmtdp tuple  hs  a  p    o  r  rli  can be defined using a procedure that relies on both the top as well as the underlying domain  while this procedure
is not automated  our key contribution is recognizing the exploitation of top structures
in constructing the rmtdp model  first  in order to determine the set of states  s  it
is critical to model the variables tested in the pre conditions  termination conditions and
context of all the components  i e  sub plans  in the top  note that a state only needs
to model the features tested in the top  if a top pre condition expresses a complex test
on the feature  that test is not modeled in the state  but instead gets used in defining the
incomplete policy input to rmtdp  next we define the set of roles  rl  as the leaf level
roles in the organization hierarchy of the top  furthermore  as specified in section      we
define a state feature roles as a vector containing the current role for each agent  having
defined rl and roles   we now define the actions  a as follows  for each role rj  rl  we
define a corresponding role taking action  irj which will succeed or fail depending on the
agent i that performs the action and the state s that the action was performed in  the
role execution actions  irj for agent i in role rj   are those allowed for that role according
to the top 
thus  we have defined s  a and rl based on the top  to illustrate these steps  consider
the plans in figure   b   the pre conditions of the leaf level plan scoutroute   see
appendix a   for instance  tests start location of the helicopters to be at start location x 
while the termination conditions test that scouts are at end location y  thus  the locations
of the helicopters are modeled as features in the set of states in the rmtdp  using the
organization hierarchy  we define the set of roles rl with a role corresponding to each of the
four different kinds of leaf level roles  i e  rl    membersctt eama  membersctt eamb 
membersctt eamc  membert ransportt eam   the role taking and role execution actions
can be defined as follows 
 a role taking action is defined corresponding to each of the four roles in rl  i e 
becoming a member of one of the three scouting teams or of the transport team  the
domain specifies that only a transport can change to a scout and thus the role taking
action  jointtransportteam  will fail for agent i  if the current role of agent i is a scout 
   

finair   tambe

 role execution actions are obtained from the top plans corresponding to the agents
role  in the mission rehearsal scenario  an agent  fulfilling a scout role  members
of sctteama  sctteamb or sctteamc   always goes forward  making the current
position safe  until it reaches the destination and so the only execution action we will
consider is move making safe  an agent in a transport role  members of transport
team  waits at x until it obtains observation of a signal that one scouting sub team
has reached y and hence the role execution actions are wait and move forward 
we must now define   p  o  r  we obtain the set of observations i for each agent i
directly from the domain  for instance  the transport helos may observe the status of scout
helos  normal or destroyed   as well as a signal that a path is safe  finally  determining
the functions  p  o  r requires some combination of human domain expertise and empirical
data on the domain behavior  however  as shown later in section    even an approximate
model of transitional and observational uncertainty is sufficient to deliver significant benefits  defining the reward and transition function may sometimes require additional state
variables to be modeled  if they were only implicitly modeled in the top  in the mission
rehearsal domain  the time at which the scouting and transport mission were completed
determined the amount of reward  thus  time was only implicitly modeled in the top and
needed to be explicitly modeled in the rmtdp 
since we are interested in analyzing a particular top with respect to uncertainty  the
procedure for constructing an rmtdp model can be simplified by exploiting the hierarchical decomposition of the top in order to decompose the construction of the rmtdp
model  the high level components of a top often represent plans executed by different
sub teams  which may only loosely interact with each other  within a component  the
sub team members may exhibit a tight interaction  but our focus is on the loose coupling
across components  where only the end results of one component feed into another  or the
components independently contribute to the team goal  thus  our procedure for constructing an rmtdp exploits this loose coupling between components of the plan hierarchy in
order to build an rmtdp model represented as a combination of smaller rmtdps  factors   note that if such decomposition is infeasible  our approach still applies except that
the benefits of the hierarchical decomposition will be unavailable 
we classify sibling components as being either parallel or sequentially executed  contains a temporal constraint   components executed in parallel could be either independent
or dependent  for independent components  we can define rmtdps for each of these
components such that the sub team executing one component cannot affect the transitions  observations and reward obtained by the sub teams executing the other components  the procedure for determining the elements of the rmtdp tuple for component k 
hsk   ak   pk   k   ok   rk   rlk i  is identical to the procedure described earlier for constructing
the overall rmtdp  however  each such component has a smaller set of relevant variables
and roles and hence specifying the elements of its corresponding rmtdp is easier 
we can now combine the rmtdps of the independent components to obtain the
rmtdp corresponding to the higher level component  for a higher level component l 
whose child
s components are independent  the set of states  sl   x fsl x such that
fsl   k s t  child k l  true fsk where fsl and fsk are the sets of features for the set
of states sl and set of states sk   a state sl  sl is said to correspond to the state
sk  sk if x  fsk   sl  x     sk  x    i e  the state sl has the same value as state sk
   

fihybrid bdi pomdp framework for multiagent teaming

for
is defined as follows  pl  sl   al   sl    
q all features of state sk   the transition function

k s t  child k l  true pk  sk   ak   sk    where sl and sl of component l corresponds to states
sk and sk of component k and ak is the joint action performed by the sub team assigned to component k corresponding to the joint action al performed by the sub team
assigned
to component l  the observation function is defined similarly as ol  sl   al   l    
q
ok  sk   ak   k    the reward function for component l is defined as
k s t  child k l  true
p
rl  sl   al     k s t  child k l  true rk  sk   ak   

in the case of sequentially executed components  those connected by a temporal constraint   the components are loosely coupled since the end states of the preceding component
specify the start states of the succeeding component  thus  since only one component is
active at a time  the transition function is defined as follows  pl  sl   al   sl     pk  sk   ak   sk   
where component k is the only active child component  sk and sk represent the states of
component k corresponding to states sl and sl of component l and ak is the joint action
performed by the sub team assigned to component k corresponding to the joint action
al performed by the sub team corresponding to component l  similarly  we can define
ol  sl   al   l     ok  sk   ak   k   and rl  sl   al     rk  sk   ak    where k is the only active child
component 

consider the following example from the mission rehearsal domain where components
exhibit both sequential dependence and parallel independence  concretely  the component
doscouting is executed first followed by dotransport and remainingscouts  which
are parallel and independent and hence  either doscouting is active or dotransport and
remainingscouts are active at any point in the execution  hence  the transition  observation and reward functions of their parent execute mission is given by the corresponding
functions of either doscouting or by the combination of the corresponding functions of
dotransport and remainingscouts 
we use a top down approach in order to determine how to construct a factored rmtdp
from the plan hierarchy  as shown in algorithm    we replace a particular sub plan by its
constituent sub plans if they are either independent or sequentially executed  if not  then
the rmtdp is defined using that particular sub plan  this process is applied recursively
starting at the root component of the plan hierarchy  as a concrete example  consider
again our mission rehearsal simulation domain and the hierarchy illustrated in figure   b  
given the temporal constraints between doscouting and dotransport  and doscouting and remainingscouts  we exploited sequential decomposition  while dotransport
and remainingscouts were parallel and independent components  hence  we can replace
executemission by doscouting  dotransport and remainingscouts  we then apply the same process to doscouting  the constituent components of doscouting are
neither independent nor sequentially executed and thus doscouting cannot be replaced by
its constituent components  thus  rmtdp for the mission rehearsal domain is comprised
of smaller rmtdps for doscouting  dotransport and remainingscouts 
thus  using the top to identify relevant variables and building a factored rmtdp
utilizing the structure of top to decompose the construction procedure  reduce the load
on the domain expert for model construction  furthermore  as shown in section      this
factored model greatly improves the performance of the search for the best role allocation 
   

finair   tambe

algorithm   build rmtdp top top  sub plan subplan 
   children  subplanchildren    subplanchildren   returns the sub plans within subplan 
   if children   null or children are not  loosely coupled or independent  then
  
rmtdp  define rmtdp subplan   not automated 
  
return rmtdp
   else
  
for all child in children do
  
factors child   build rmtdp top child 
  
rmtdp  constructfromfactors factors 
  
return rmtdp

    exploiting top beliefs in evaluation of rmtdp policies
we now present a technique for exploiting tops in speeding up evaluation of rmtdp
policies  before we explain our improvement  we first describe the original algorithm for
determining the expected reward of a joint policy  where the local policies of each agent
are indexed by its entire observation histories  pynadath   tambe        nair  pynadath 
yokoo  tambe    marsella      a   here  we obtain an rmtdp policy from a top as
follows  we obtain i   
it    i e  the action performed by agent i for each observation history
t
 i   as the action a performed by the agent i following the top when it has a set of privately
held beliefs corresponding to the observation history   it   we compute the expected reward
for the rmtdp policy by projecting the teams execution over all possible branches on
different world states and different observations  at each time step  we can compute the
expected value of a joint policy                  n    for a team starting in a given state  st  
with a given set of past observations  
   t           
  nt   as follows 
x


ff
ff





ff

  nt     r st       
vt  st     t           
 t            n   nt      
p s t     
   t           n 
  nt   st  
st   s

x



ff 

ff


ff
 o st      
   t           n 
  nt    t            nt    vt   st       t             
  nt  

   

t   

the expected reward of a joint policy  is given by v   s      null          null    where s 
is the start state  at each time step t  the computation of vt performs a summation over all
possible world states and agent observations and so has a time complexity of o   s       
this computation
is repeated for all states and all observation histories of length t  i e 

o  s     t times  therefore 
given a time horizon t   the overall complexity of this algo
 
t
  
rithm is o  s     
 
as discussed in section      in a team oriented program  each agents action selection
is based on just its currently held private beliefs  note that mutual beliefs are modeled
as privately held beliefs about all agents as per footnote     a similar technique can be
exploited when mapping top to an rmtdp policy  indeed  the evaluation of a rmtdp
policy that corresponds to a top can be speeded up if each agents local policy is indexed
by its private beliefs  i t   we refer to i t   as the top congruent belief state of agent i
   

fihybrid bdi pomdp framework for multiagent teaming

in the rmtdp  note that this belief state is not a probability distribution over the world
states as in a single agent pomdp  but rather the privately held beliefs  from the bdi
program  of agent i at time t  this is similar to the idea of representing a policy by a
finite state controller  hansen   zhou        poupart   boutilier         in this case  the
private beliefs would map to the states of the finite state controller 
belief based rmtdp policy evaluation leads to speedup because multiple observation
histories map to the same belief state  i t   this speedup is a key illustration of exploitation
of synergistic interactions of top and rmtdp  in this instance  belief representation techniques used in top are reflected in rmtdp  and the resulting faster policy evaluation can
help us optimize top performance  a detailed example of belief state is presented later
after a brief explanation of how such belief based rmtdp policies can be evaluated 
just as with evaluation using observation histories  we compute the expected reward
of a belief based policy by projecting the teams execution over all possible branches on
different world states and different observations  at each time step  we can compute the
expected value of a joint policy                  n    for a team starting in a given state  st  
with a given team belief state     t           nt   as follows 


ff


ff x

ff



vt  st    t       nt     r st       t            n  nt       p st      t           n nt   st  
st   s

x
ff 


ff




ff
 o st       t           n nt    t            nt    vt   st      t             nt  

t   

   
where i

t  

  beliefupdatefunction

t

i   it  



the complexity of computing this function  expression    is o   s        bf   where bf
represents the complexity of the belief update function  beliefupdatefunction  at each
time step the computation of the value function is done for every state and for all possible
reachable belief states  let  i     max tt   it    represent the maximum number of
possible belief states that agent i can be in at any point in time  where  it   is the number
of belief states that agent i can be in at t  therefore the complexity of this algorithm is
given by o  s                       n     t    bf   note that  in this algorithm t is not in
the exponent unlike in the algorithm in expression    thus  this evaluation method will
give large time savings if   i  the quantity                n     t is much less than   t and
 ii  the belief update cost is low  in practical bdi systems  multiple observation histories
map often onto the same belief state  and thus usually                 n     t is much less
than   t   furthermore  since the belief update function mirrors practical bdi systems 
its complexity is also a low polynomial or a constant  indeed  our experimental results
show that significant speedups result from switching to our top congruent belief states
i t   however  in the absolute worst case  the belief update function may simply append
the new observation to the history of past observations  i e   top congruent beliefs will
be equivalent to keeping entire observation histories  and thus belief based evaluation will
have the same complexity as the observation history based evaluation 
we now turn to an example of belief based policy evaluation from the mission rehearsal
domain  at each time step  the transport helicopters may receive an observation about
   

finair   tambe

whether a scout has failed based on some observation function  if we use the observationhistory representation of the policy  then each transport agent would maintain a complete
history of the observations that it could receive at each time step  for example  in a setting
with two scout helicopters  one on route   and the other on route    a particular transport
helicopter may have several different observation histories of length two  at every time step 
the transports may receive an observation about each scout being alive or having failed 
thus  at time t      a transport helicopter might have one of the following observation histories of length two     sct onroute alive  sct onroute alive      sct onroute f ailed 
sct onroute f ailed         sct onroute alive  sct onroute f ailed      sct onroute 
f ailed         sct onroute f ailed  sct onroute alive      sct onroute f ailed     
etc  however  the action selection of the transport helicopters depends on only whether
a critical failure  i e  the last remaining scout has crashed  has taken place to change its
role  whether a failure is critical can be determined by passing each observation through
a belief update function  the exact order in which the observations are received or the
precise times at which the failure or non failure observations are received are not relevant
to determining if a critical failure has taken place and consequently whether a transport
should change its role to a scout  thus  many observation histories map onto the same
belief states  for example  the above three observation histories all map to the same belief
criticalf ailure doscouting  i e  a critical failure has taken place  this results in significant speedups using belief based evaluation  as equation   needs to be executed over a
smaller number of belief states  linear in t in our domains  as opposed to the observation
history based evaluation  where equation   is executed over an exponential number of observation histories    t    the actual speedup obtained in the mission rehearsal domain is
demonstrated empirically in section   

   optimizing role allocation
while section   focused on mapping a domain of interest onto rmtdp and algorithms for
policy evaluation  this section focuses on efficient techniques for rmtdp policy search  in
service of improving bdi top team plans  the top in essence provides an incomplete 
fixed policy  and the policy search optimizes decisions left open in the incomplete policy  the
policy thus completed optimizes the original top  see figure     by enabling the rmtdp
to focus its search on incomplete policies  and by providing ready made decompositions 
tops assist rmtdps in quickly searching through the policy space  as illustrated in this
section  we focus  in particular  on the problem of role allocation  hunsberger   grosz 
      modi  shen  tambe    yokoo        tidhar et al         fatima   wooldridge        
a critical problem in teams  while the top provides an incomplete policy  keeping open
the role allocation decision for each agent  the rmtdp policy search provides the optimal
role taking action at each of the role allocation decision points  in contrast to previous
role allocation approaches  our approach determines the best role allocation  taking into
consideration the uncertainty in the domain and future costs  although demonstrated for
solving the role allocation problem  the methodology is general enough to apply to other
coordination decisions 
   

fihybrid bdi pomdp framework for multiagent teaming

    hierarchical grouping of rmtdp policies
as mentioned earlier  to address role allocation  the top provides a policy that is complete 
except for the role allocation decisions  rmtdp policy search then optimally fills in the
role allocation decisions  to understand the rmtdp policy search  it is useful to gain an
understanding of the role allocation search space  first  note that role allocation focuses on
deciding how many and what types of agents to allocate to different roles in the organization
hierarchy  this role allocation decision may be made at time t     or it may be made at a
later time conditioned on available observations  figure   shows a partially expanded role
allocation space defined by the top organization hierarchy in figure   a  for six helicopters 
each node of the role allocation space completely specifies the allocation of agents to roles
at the corresponding level of the organization hierarchy  ignore for now  the number to the
right of each node   for instance  the root node of the role allocation space specifies that
six helicopters are assigned to the task force  level one  of the organization hierarchy while
the leftmost leaf node  at level three  in figure   specifies that one helicopter is assigned
to sctteama  zero to sctteamb  zero to sctteamc and five helicopters to transport team 
thus  as we can see  each leaf node in the role allocation space is a complete  valid role
allocation of agents to roles in the organization hierarchy 
in order to determine if one leaf node  role allocation  is superior to another we evaluate
each using the rmtdp by constructing an rmtdp policy for each  in this particular
example  the role allocation specified by the leaf node corresponds to the role taking actions
that each agent will execute at time t      for example  in the case of the leftmost leaf in
figure    at time t      one agent  recall from section     that this is a homogeneous team
and hence which specific agent does not matter  will become a member of sctteama while
all other agents will become members of transport team  thus  for one agent i  the roletaking policy will include i  null    joinsctt eama and for all other agents  j  j    i  it
will include j  null    joint ransportt eam  in this case  we assume that the rest of the
role taking policy  i e  how roles will be reallocated if a scout fails  is obtained from the role
reallocation algorithm in the bdi top interpreter  such as the steam algorithm  tambe
et al          thus for example  if the role reallocation is indeed performed by the steam
algorithm  then steams reallocation policy is included into the incomplete policy that
the rmtdp is initially provided  thus  the best role allocation is computed keeping in
mind steams reallocation policy  in steam  given a failure of an agent playing rolef  
an agent playing roler will replace it if 
criticality  rolef    criticality  roler      
criticality  x      if x is critical      otherwise

thus  if based on the agents observations  a critical failure has taken place  then the
replacing agents decision to replace or not will be computed using the above expression
and then included in the incomplete policy input to the rmtdp  since such an incomplete
policy is completed by the role allocation at each leaf node using the technique above  we
have been able to construct a policy for the rmtdp that corresponds to the role allocation 
in some domains like robocuprescue  not all allocation decisions are made at time
t      in such domains  it is possible for the role allocation to be conditioned on observations
 or communication  that are obtained during the course of the execution  for instance  as
shown in figure   a   in the robocuprescue scenario  the ambulances are allocated to the
sub team ambulanceteama or ambulanceteamb only after information about the location
   

finair   tambe

 

 

 

   
 

 

 

      
 

 

 

      
 

 

 

      
 

 

 

      
 

 

 

      
 

 

 

     
 

         
         
         
        
   
   
   
   
     
     
     
     

figure    partially expanded role allocation space for mission rehearsal domain six helos  

of civilians is conveyed to them by the fire engines  the allocation of the ambulances is
then conditioned on this communication  i e  on the number of civilians at each location 
figure    shows the partially expanded role allocation for a scaled down rescue scenario
with three civilians  two ambulances and two fire engines  one at station   and the other at
station     in the figure        depicts the fact that there are two ambulances  while there
is one fire engine at each station  as shown  there is a level for the allocation of fire engines
to engineteama and engineteamb which gives the number of engines assigned to each
engineteam from each station  the next level  leaf level  has different leaf nodes for each
possible assignment of ambulances to ambulanceteama and ambulanceteamb depending
upon the value of communication c  since there are three civilians and we exclude the
case where no civilians are present at a particular fire  there are two possible messages i e 
one civilian at fire   or two civilians at fire    c     or    
taskforce      
     

     

engineteama     engineteamb     ambteam  

c  

engineteama     engineteamb     ambteam  

     

     

         

         

c  

c  

ambteama   ambteamb   ambteama   ambteamb  

c  

ambteama   ambteamb   ambteama   ambteamb  

figure     partially expanded role allocation space for rescue domain  one fire engine at
station    one fire engine at station    two ambulances  three civilians  
we are thus able to exploit the top organization hierarchy to create a hierarchical
grouping of rmtdp policies  in particular  while the leaf node represents a complete
rmtdp policy  with the role allocation as specified by the leaf node   a parent node
represents a group of policies  evaluating a policy specified by a leaf node is equivalent to
evaluating a specific role allocation while taking future uncertainties into account  we could
   

fihybrid bdi pomdp framework for multiagent teaming

do a brute force search through all role allocations  evaluating each in order to determine
the best role allocation  however  the number of possible role allocations is exponential in
the leaf roles in the organization hierarchy  thus  we must prune the search space 
    pruning the role allocation space
we prune the space of valid role allocations using upper bounds  maxestimates  for the
parents of the leaves of the role allocation space as admissible heuristics  section       each
leaf in the role allocation space represents a completely specified policy and the maxestimate is an upper bound of maximum value of all the policies under the same parent node
evaluated using the rmtdp  once we obtain maxestimates for all the parent nodes  shown
in brackets to the right of each parent node in figure     we use branch and bound style
pruning  see algorithm     while we discuss algorithm   below  we note that in essence
it performs branch and bound style pruning  the key novelty is step   which we discuss in
section     
the branch and bound algorithm works as follows  first  we sort the parent nodes by
their estimates and then start evaluating children of the parent with the highest maxestimate  algorithm    steps        evaluate rmtdp  child  refers to the evaluation of the
leaf level policy  child  using the rmtdp model  this evaluation of leaf level policies  step
    can be done using either of the methods described in section    in the case of the
role allocation space in figure    we would start with evaluating the leaves of the parent
node that has one helicopter in scouting team and five in transport team  the value of
evaluating each leaf node is shown to the right of the leaf node  once we have obtained
the value of the best leaf node  algorithm    steps         in this case          we compare
this with the maxestimates of the other parents of the role allocation space  algorithm   
steps         as we can see from figure   this would result in pruning of three parent nodes
 leftmost parent and right two parents  and avoid the evaluation of    of the    leaf level
policies  next  we would then proceed to evaluate all the leaf nodes under the parent with
two helos in scouting team and four in transport team  this would result in pruning of all
the remaining unexpanded parent nodes and we will return the leaf with the highest value 
which in this case is the node corresponding to two helos allocated to sctteama and four
to transport team  although demonstrated for a   level hierarchy  the methodology for
applying to deeper hierarchies is straightforward 
    exploiting top to calculate upper bounds for parents
we will now discuss how the upper bounds of parents  called maxestimates  can be calculated for each parent  the maxestimate of a parent is defined as a strict upper bound of
the maximum of the expected reward of all the leaf nodes under it  it is necessary that the
maxestimate be an upper bound or else we might end up pruning potentially useful role
allocations  in order to calculate the maxestimate of each parent we could evaluate each of
the leaf nodes below it using the rmtdp  but this would nullify the benefit of any subsequent pruning  we  therefore  turn to the top plan hierarchy  see figure   b   to break up
this evaluation of the parent node into components  which can be evaluated separately thus
decomposing the problem  in other words  our approach exploits the structure of the bdi
program to construct small scale rmtdps unlike other decomposition techniques which
   

finair   tambe

algorithm   branch and bound algorithm for policy search 
   parents  list of parent nodes
   compute maxexp parents   algorithm   
   sort parents in decreasing order of maxexp
   bestval  
   for all parent  parents do
  
done parent   false  pruned parent   false
   for all parent  parents do
  
if done parent    false and pruned parent    false then
  
child  parentnextchild    child is a leaf level policy under parent 
   
if child   null then
   
done parent   true
   
else
   
childval  evaluate rmtdp child 
   
if childval   bestval then
   
bestval  childval best  child
   
for all parent  in parents do
   
if maxexp parent     bestval then
   
pruned parent    true
    return best

just assume decomposition or ultimately rely on domain experts to identify interactions in
the agents reward and transition functions  dean   lin        guestrin  venkataraman 
  koller        
for each parent in the role allocation space  we use these small scale rmtdps to evaluate the values for each top component  fortunately  as discussed in section      we
exploited small scale rmtdps corresponding to top components in constructing larger
scale rmtdps  we put these small scale rmtdps to use again  evaluating policies within
each component to obtain upper bounds  note that just like in evaluation of leaf level
policies  the evaluation of components for the parent node can be done using either the
observation histories  see equation    or belief states  see equation     we will describe
this section using the observation history based evaluation method for computing the values
of the components of each parent  which can be summed up to obtain its maxestimate  an
upper bound on its childrens values   thus  whereas a parent in the role allocation space
represents a group of policies  the top components  sub plans  allow a component wise
evaluation of such a group to obtain an upper bound on the expected reward of any policy
within this group 
algorithm   exploits the smaller scale rmtdp components  discussed in section     
to obtain upper bounds of parents  first  in order to evaluate the maxestimate for each
parent node in the role allocation space  we identify the start states for each component from
which to evaluate the rmtdps  we explain this step using a parent node from figure   
scouting team   two helos  transport team   four helos  see figure      for the very first
component which does not have any preceding components  the start states corresponds
to the start states of the policy that the top was mapped onto  for each of the next
   

fihybrid bdi pomdp framework for multiagent teaming

components  where the next component is one linked by a sequential dependence  the
start states are the end states of the preceding component  however  as explained later in
this section  we can significantly reduce this list of start states from which each component
can be evaluated 
algorithm   maxexp method for calculating upper bounds for parents in the role allocation space 
   for all parent in search space do
  
maxexp parent    
  
for all component i corresponding to factors in the rmtdp from section     do
  
if component i has a preceding component j then
  
obtain start states  states i   endstates j 
  
states i   removeirrelevantfeatures states i    discard features not present
in si  
  
obtain corresponding observation histories at start ohistories i  
endohistories j 
  
ohistories i   removeirrelevantobservations ohistories i  
  
else
   
obtain start states  states i 
   
observation histories at start ohistories i   null
   
maxeval i    
   
for all leaf level policies  under parent do
   
maxeval i   max maxeval i   maxsi states i  ohi ohistories i  evaluate rm t dpi  
si   ohi      
 
   
maxexp parent   maxeval i 
similarly  the starting observation histories for a component are the observation histories on completing the preceding component  no observation history for the very first
component   bdi plans do not normally refer to entire observation histories but rely only
on key beliefs which are typically referred to in the pre conditions of the component  each
starting observation history can be shortened to include only these relevant observations 
thus obtaining a reduced list of starting observation sequences  divergence of private observations is not problematic  e g  will not cause agents to trigger different team plans 
this is because as indicated earlier in section      top interpreters guarantee coherence
in key aspects of observation histories  for instance  as discussed earlier  top interpreter
ensures coherence in key beliefs when initiating and terminating team plans in a top  thus
avoiding such divergence of observation histories 
in order to compute the maximum value for a particular component  we evaluate all
possible leaf level policies within that component over all possible start states and observation histories and obtain the maximum  algorithm   steps         during this evaluation 
we store all the end states and ending observation histories so that they can be used in
the evaluation of subsequent components  as shown in figure     for the evaluation of
doscouting component for the parent node where there are two helicopters assigned to
scouting team and four helos to transport team  the leaf level policies correspond to all
possible ways these helicopters could be assigned to the teams sctteama  sctteamb  sct   

finair   tambe

teamc and transport team  e g  one helo to sctteamb  one helo to sctteamc and four
helos to transport team  or two helos to sctteama and four helos to transport team  etc 
the role allocation tells the agents what role to take in the first step  the remainder of the
role taking policy is specified by the role replacement policy in the top infrastructure and
role execution policy is specified by the doscouting component of the top 
to obtain the maxestimate for a parent node of the role allocation space  we simply
sum up the maximum values obtained for each component  algorithm   steps      e g 
the maximum values of each component  see right of each component in figure     were
summed to obtain the maxestimate                          as seen in figure    third
node from the left indeed has an upper bound of      
the calculation of the maxestimate for a parent nodes should be much faster than
evaluating the leaf nodes below it in most cases for two reasons  firstly  parent nodes are
evaluated component wise  thus  if multiple leaf level policies within one component result
in the same end state  we can remove duplicates to get the start states of the next component  since each component only contains the state features relevant to it  the number of
duplicates is greatly increased  such duplication of the evaluation effort cannot be avoided
for leaf nodes  where each policy is evaluated independently from start to finish  for instance  in the doscouting component  the role allocations  sctteama    sctteamb   
sctteamc    transportteam   and sctteama    sctteamb    sctteamc    transportteam    will have end states in common after eliminating irrelevant features when the
scout in sctteamb for the former allocation and the scout in sctteamc for the latter allocation fail  this is because through feature elimination  algorithm   steps     the only
state features retained for dotransport are the scouted route and number of transports
 some transports may have replaced failed scouts  as shown in figure    
the second reason computation of maxestimates for parents is much faster is that the
number of starting observation sequences will be much less than the number of ending observation histories of the preceding components  this is because not all the observations in
the observation histories of a component are relevant to its succeeding components  algorithm   steps     thus  the function removeirrelevantobservations reduces the number
of starting observation histories from the observation histories of the preceding component 
we refer to this methodology of obtaining the maxestimates of each parent as maxexp  a variation of this  the maximum expected reward with no failures  nofail   is
obtained in a similar fashion except that we assume that the probability of any agent failing is    we are able to make such an assumption in evaluating the parent node  since we
focus on obtaining upper bounds of parents  and not on obtaining their exact value  this
will result in less branching and hence evaluation of each component will proceed much
quicker  the nofail heuristic only works if the evaluation of any policy without failures
occurring is higher than the evaluation of the same policy with failures possible  this should
normally be the case in most domains  the evaluation of the nofail heuristics for the
role allocation space for six helicopters is shown in square brackets in figure   
the following theorem shows that the maxexp method for finding the upper bounds
indeed finds an upper bound and thus yields an admissible search heuristic for the branchand bound search of the role allocation space 
theorem   the maxexp method will always yield an upper bound 
   

fihybrid bdi pomdp framework for multiagent teaming

    
doscouting
 scoutingteam   transportteam   

alloc 
sctteama  
sctteamb  
sctteamc  
transportteam  

alloc 
sctteama  
sctteamb  
sctteamc  
transportteam  

      
dotransport
 transportteam   

startstate 
routescouted  
transports  

    
remainingscouts
 scoutteam   

startstate 
routescouted  
transports  

startstate 
routescouted  
transports  

figure     component wise decomposition of a parent by exploiting top 

proof  see appendix c 
from theorem    we can conclude that our branch and bound policy search algorithm
will always find the best role allocation  since the maxestimates of the parents are true
upper bounds  also  with the help of theorem    we show that in the worst case  our
branch and bound policy search has the same complexity as doing a brute force search 
theorem   worst case complexity for evaluating a single parent node using maxexp is
the same as that of evaluating every leaf node below it within a constant factor 
proof sketch 
 the worst case complexity for maxexp arises when 
   let esj be the end states of component j executing policy  after removing
features that are irrelevant to the succeeding component k  similarly  let esj
be the end states of component j executing policy   after
tremoving features that
are irrelevant to the succeeding component k  if esj esj   null then no
duplication in the end states will occur 
   let ohj be the ending observation histories of component j executing policy
 after removing observations that are irrelevant to the succeeding component
k  similarly  let ohj be the ending observation histories of component j executing policy   after removing observation
histories that are irrelevant to the
t
succeeding component k  if ohj ohj   null then no duplication in the
observation histories will occur  note that if the belief based evaluation was used
then we would replace observation histories by the top congruent belief states
 see sect    
 in such a case  there is no computational advantage to evaluating each components
maxestimate separately  thus  it is equivalent to evaluating each child node of the
parent  thus  in the worst case  maxexp computation for the parent is the same as
the evaluating all its children within a constant factor  
in addition  in the worst case  no pruning will result using maxexp and each and every
leaf node will need to be evaluated  this is equivalent to evaluating each leaf node twice 
   

finair   tambe

thus  the worst case complexity of doing the branch and bound search using maxexp is
the same as that of finding the best role allocation by evaluating every leaf node  we refer
to this brute force approach as noprune  thus  the worst case complexity of maxexp
is the same as noprune  however  owing to pruning and the savings through decomposition in the computation of maxestimates  significant savings are likely in the average
case  section   highlights these savings for the mission rehearsal and the robocuprescue
domains 

   experimental results
this section presents four sets of results in the context of the two domains introduced
in section      viz  mission rehearsal and robocuprescue  kitano et al          first 
we investigated empirically the speedups that result from using the top congruent belief
states i  belief based evaluation  over observation history based evaluation and from using
the algorithm from section   over a brute force search  here we focus on determining
the best assignment of agents to roles  but assume a fixed top and top infrastructure 
second  we conducted experiments to investigate the benefits of considering uncertainty in
determining role allocations  for this  we compared the allocations found by the rmtdp
role allocation algorithm with  i  allocations which do not consider any kind of uncertainty 
and  ii  allocations which do not consider observational uncertainty but consider action
uncertainty  third  we conducted experiments in both domains to determine the sensitivity
of the results to changes in the model  fourth  we compare the performance of allocations
found by the rmtdp role allocation algorithm with allocations of human subjects in the
more complex of our domains  robocuprescue simulations 
    results in mission rehearsal domain
for the mission rehearsal domain  the top is the one discussed in section      as can be
seen in figure   a   the organization hierarchy requires determining the number of agents
to be allocated to the three scouting sub teams and the remaining helos must be allocated
to the transport sub team  different numbers of initial helicopters were attempted  varying
from three to ten  the details on how the rmtdp is constructed for this domain are given
appendix b  the probability of failure of a scout at each time step on routes      and  
are           and      respectively  the probability of a transport observing an alive scout
on routes      and   are            and       respectively  false positives are not possible 
i e  a transport will not observe a scout as being alive if it has failed  the probability of a
transport observing a scout failure on routes      and   are            and       respectively 
here too  false positives are not possible and hence a transport will not observe a failure
unless it has actually taken place 
figure    shows the results of comparing the different methods for searching the role
allocation space  we show four methods  each method adds new speedup techniques to
the previous 
   noprune obs  a brute force evaluation of every role allocation to determine the
best  here  each agent maintains its complete observation history and the evaluation
algorithm in equation   is used  for ten agents  the rmtdp is projected to have in
   

fihybrid bdi pomdp framework for multiagent teaming

the order of        reachable states and in the order of         observation histories
per role allocation evaluated  thus the largest experiment in this category was limited
to seven agents  
   noprune bel  a brute force evaluation of every role allocation  the only difference
between this method and noprune obs is the use of the belief based evaluation
algorithm  see equation    
   maxexp  the branch and bound search algorithm described in section     that
uses upper bounds of the evaluation of the parent nodes to find the best allocation 
evaluation of the parent and leaf nodes uses the belief based evaluation 
   nofail  the modification to branch and bound heuristic mentioned in section     
in essence it is same as maxexp  except that the upper bounds are computed making
the assumption that agents do not fail  this heuristic is correct in those domains where
the total expected reward with failures is always less than if no failures were present
and will give significant speedups if agent failures is one of the primary sources of
stochasticity  in this method  too  the evaluation of the parent and leaf nodes uses
the belief based evaluation   note that only upper bounds are computed using the
no failure assumption  no changes are assumed in the actual domains  
in figure    a   the y axis is the number of nodes in the role allocation space evaluated
 includes leaf nodes as well as parent nodes   while in figure    b  the y axis represents
the runtime in seconds on a logarithmic scale  in both figures  we vary the number of agents
on the x axis  experimental results in previous work using distributed pomdps are often
restricted to just two agents  by exploiting hybrid models  we are able to vary the number of
agents from three to ten as shown in figure    a   as clearly seen in figure    a   because
of pruning  significant reductions are obtained by maxexp and nofail over noprunebel in terms of the numbers of nodes evaluated  this reduction grows quadratically to
about    fold at ten agents   noprune obs is identical to noprune bel in terms of
number of nodes evaluated  since in both methods all the leaf level policies are evaluated 
only the method of evaluation differs  it is important to note that although nofail and
maxexp result in the same number of nodes being evaluated for this domains  this is
not necessarily true always  in general  nofail will evaluate at least as many nodes as
maxexp since its estimate is at least as high as the maxexp estimate  however  the
upper bounds are computed quicker for nofail 
figure    b  shows that the noprune bel method provides a significant speedup
over noprune obs in actual run time  for instance  there was a    fold speedup using
noprune bel instead of noprune obs for the seven agent case  noprune obs
could not be executed within a day for problem settings with greater than seven agents  
this empirically demonstrates the computational savings possible using belief based evaluation instead of observation history based evaluation  see section     for this reason  we
use only belief based evaluation for the maxexp and nofail approaches and also for all
   the number of nodes for noprune up to eight agents were obtained from experiments  the rest can
be calculated using the formula  m n  n     m   n             m n   where m represents the number of
heterogeneous role types and n is the number of homogeneous agents   m n    m   n             m is
referred to as a rising factorial 

   

finair   tambe

the remaining experiments in this paper  maxexp heuristic results in a    fold speedup
over noprune bel in the eight agent case 
the nofail heuristic which is very quick to compute the upper bounds far outperforms
the maxexp heuristic     fold speedup over maxexp for ten agents   speedups of
maxexp and nofail continually increase with increasing number of agents  the speedup
of the nofail method over maxexp is so marked because  in this domain  ignoring
failures results in much less branching 
   

nofail  maxexp

number of nodes

   

noprune obs 
noprune bel

   
   
   
   
  
 

 

 

 

 

 

 

 

  

number of agents
      

maxexp
nofail
noprune bel
noprune obs

time in secs  log scale 

     
    
   
  
 
   
    

 

 

 

 

 

 

 

  

number of agents

figure     performance of role allocation space search in mission rehearsal domain  a   left 
number of nodes evaluated  b   right run time in seconds on a log scale 

next  we conducted experiments illustrating the importance of rmtdps reasoning
about action and observation uncertainties on role allocations  for this  we compared the
allocations found by the rmtdp role allocation algorithm with allocations found using two
different methods  see figure     
   role allocation via constraint optimization  cop   modi et al         mailler   lesser 
      allocation approach  in the cop approach    leaf level sub teams from the or   modi et al s work        focused on decentralized cop  but in this investigation our emphasis is on the
resulting role allocation generated by the cop  and not on the decentralization per se 

   

fihybrid bdi pomdp framework for multiagent teaming

ganization hierarchy are treated as variables and the number of helicopters as the
domain of each such variable  thus  the domain may be           helicopters   the
reward for allocating agents to sub teams is expressed in terms of constraints 
 allocating a helicopter to scout a route was assigned a reward corresponding to
the routes distance but ignoring the possibility of failure  i e  ignoring transition
probability   allocating more helicopters to this subteam obtained proportionally higher reward 
 allocating a helicopter a transport role was assigned a large reward for transporting cargo to the destination  allocating more helicopters to this subteam
obtained proportionally higher reward 
 not allocating at least one scout role was assigned a reward of negative infinity
 exceeding the total number of agents was assigned a reward of negative infinity
   rmtdp with complete observability  in this approach  we consider the transition
probability  but ignore partial observability  achieved by assuming complete observability in the rmtdp  an mtdp with complete observability is equivalent to a
markov decision problem  mdp   pynadath   tambe        where the actions are
joint actions  we  thus  refer to this allocation method as the mdp method 
figure    a  shows a comparison of the rmtdp based allocation with the mdp allocation and the cop allocation for increasing number of helicopters  x axis   we compare
using the expected number of transports that get to the destination  y axis  as the metric
for comparison since this was the primary objective of this domain  as can be seen  considering both forms of uncertainty  rmtdp  performs better than just considering transition
uncertainty  mdp  which in turn performs better than not considering uncertainty  cop  
figure    b  shows the actual allocations found by the three methods with four helicopters
and with six helicopters  in the case of four helicopters  first three bars   rmtdp and mdp
are identical  two helicopters scouting route   and two helicopters taking on transport role 
the cop allocation however consists of one scout on route   and three transports  this
allocation proves to be too myopic and results in fewer transports getting to the destination
safely  in the case of six helicopters  cop chooses just one scout helicopter on route   
the shortest route  the mdp approach results in two scouts both on route    which was
longest route albeit the safest  the rmtdp approach  which also considers observational
uncertainty chooses an additional scout on route    in order to take care of the cases where
failures of scouts go undetected by the transports 
it should be noted that the performance of the rmtdp based allocation will depend
on the values of the elements of the rmtdp model  however  as our next experiment
revealed  getting the values exactly correct is not necessary  in order to test the sensitivity
of the performance of the allocations to the actual model values  we introduced error in the
various parameters of the model to see how the allocations found using the incorrect model
would perform in the original model  without any errors   this emulates the situation where
the model does not correctly represent the domain  figure    shows the expected number
of transports that reach the destination  y axis  in the mission rehearsal scenario with six
helicopters as error  x axis  is introduced to various parameters in the model  for instance 
   

finair   tambe

 

number of transports

 
 

rmtdp
cop
mdp

 
 
 
 
 

 

 

 

 

 

number of agents
 
  helos

rm

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

rt 
rt 
xxx
xxxrt 
xxx
transports
xxxx
xxxx
xxxx
xxxx
xxxx

xxxx
xxxx
xxxx
xxxx
xxxx

dp

td

p

 

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxx

m

 

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

td
p

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx

rm

 

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

m
dp

 

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

p

 

co

number of helos

 

p

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

co

  helos

 

figure     a  comparison of performance of different allocation methods  b allocations
found using different allocation methods 

   

fihybrid bdi pomdp framework for multiagent teaming

when the percentage error in failure rate on route    route  failure rate  was between     
 i e  erroneous failure rate is     of actual failure rate  and      there was no difference
in the number of transports that reached their destination          however when the
percentage error was greater than      the allocation found was too conservative resulting
in fewer transports getting to the destination  similarly  when the percentage error was less
than       the allocation found was too risky  with too few scouts assigned  resulting in
more failures  in general  figure    shows that the model is insensitive to errors of   to    
in the model parameters for the mission rehearsal domain  but if the model parameters were
outside this range  non optimal allocations would result  in comparing these non optimal
allocations with cop  we find that they always perform better than cop for the range of
errors tested          for both failure rate as well as observability of routes  for instance 
at an error of     in failure rate on route    rmtdp managed to have       transports
safely reach the destination  and cop only managed to get       transports reach safely  in
comparing the non optimal allocations with mdp  we also find that they performed better
than mdp within the range of         for error in the observability of the routes  thus 
although the allocations found using an incorrect model were non optimal they performed
better than cop and mdp for large ranges of errors in the model  this shows that getting
the model exactly correct is not necessary to find good allocations  we are thus able to
obtain benefits from rmtdp even without insisting on an accurate model 
 

route  failure rate
route   failure rate
route  failure rate
route  observability

number of transports

   
 
   
 
   
 
   
 
                  

 

 

  

  

  

  

percentage error

figure     model sensitivity in mission rehearsal domain 

    results in robocuprescue domain
      speedups in robocuprescue domain
in our next set of experiments  we highlight the computational savings obtained in the
robocuprescue domain  the scenario for this experiment consisted of two fires at different
locations in the city  each of these fires has a different initially unknown number of civilians
in it  however the total number of civilians and the distribution from which the locations
of the civilians is chosen is known ahead of time  for this experiment  we fix the number of
civilians at five and set the distribution used to choose the civilians locations to be uniform 
the number of fire engines is set at five  located in three different fire stations as described
   

finair   tambe

in section     and vary the number of ambulances  all co located at an ambulance center 
from two to seven  the reason we chose to change only the number of ambulances is because
small number of fire engines are unable to extinguish fires  changing the problem completely 
the goal is to determine which fire engines to allocate to which fire and once information
about civilians is transmitted  how many ambulances to send to each fire location 
figure    highlights the savings in terms of the number of nodes evaluated and the actual
runtime as we increase the number of agents  we show results only from noprune bel
and maxexp  noprune obs could not be run because of slowness  here the nofail
heuristic is identical to maxexp since agents cannot fail in this scenario  the rmtdp
in this case had about        reachable states 
in both figures    a  and    b   we increase the number of ambulances along the xaxis  in figure    a   we show the number of nodes evaluated  parent nodes   leaf nodes  
on a logarithmic scale  as can be seen  the maxexp method results in about a    fold
decrease in the number of nodes evaluated when compared to noprune bel for seven
ambulances  and this decrease becomes more pronounced as the number of ambulances is
increased  figure    b  shows the time in seconds on a logarithmic scale on the y axis and
compares the run times of the maxexp and noprune bel methods for finding the best
role allocation  the noprune bel method could not find the best allocation within a
day when the number of ambulances was increased beyond four  for four ambulances  and
five fire engines   maxexp resulted in about a    fold speedup over noprune bel 
      allocation in robocuprescue
our next set of experiments shows the practical utility of our role allocation analysis in
complex domains  we are able to show significant performance improvements in the actual
robocuprescue domain using the role allocations generated by our analysis  first  we
construct an rmtdp for the rescue scenario  described in section      by taking guidance
from the top and the underlying domain  as described in section       we then use
the maxexp heuristic to determine the best role allocation  we compared the rmtdp
allocation with the allocations chosen by human subjects  our goal in comparing rmtdp
allocations with human subjects was mainly to show that rmtdp is capable at performing
at or near human expert levels for this domain  in addition  in order to determine that
reasoning about uncertainty actually impacts the allocations  we compared the rmtdp
allocations with allocations determined by two additional allocation methods 
   rescueisi  allocations used by the our robocuprescue agents that were entered in
the robocuprescue competitions of      rescueisi   nair et al          where they
finished in third place  these agents used local reasoning for their decision making 
ignoring transitional as well and observational uncertainty 
   rmtdp with complete observability  as discussed earlier  complete observability in
rmtdp leads to an mdp  and we refer to this method as the mdp method 
   the number of nodes evaluated using noprune bel can be computed as  f         f         f       
 a     c     where f    f  and f  are the number of fire engines are station      and    respectively  a is
the number of ambulances and c is the number of civilians  each node provides a complete conditional
role allocation  assuming different numbers of civilians at each fire station 

   

fihybrid bdi pomdp framework for multiagent teaming

number of nodes  log scale 

        

maxexp
noprune

       
      
     
    
   
  
 

 

 

 

 

 

 

number of ambulances

run time in secs  log scale 

      

maxexp
noprune

     

    

   

  

 

 

 

 

 

number of ambulances

 

 

figure     performance of role allocation space search in robocuprescue  a   left  number
of nodes evaluated on a log scale  and b   right  run time in seconds on a log
scale 

   

finair   tambe

note that these comparisons were performed using the robocuprescue simulator with
multiple runs to deal with stochasticity    the scenario is as described in section        we
fix the number of fire engines  ambulances and civilians at five each  for this experiment 
we consider two settings  where the location of civilians is drawn from 
 uniform distribution      of the cases have four civilians at fire   and one civilian
at fire        with three civilians at fire   and two at fire        with two civilians
at fire   and three at fire   and the remaining     with one civilian at fire   and
four civilians at fire    the speedup results of section       were obtained using this
distribution 
 skewed distribution      of the cases have four civilians at fire   and one civilian at
fire   and the remaining     with one civilian at fire   and four civilians at fire   
note that we do not consider the case where all civilians are located at the same fire as
the optimal ambulance allocation is simply to assign all ambulances to the fire where the
civilians are located  a skewed distribution was chosen to highlight the cases where it
becomes difficult for humans to reason about what allocation to choose 
the three human subjects used in this experiment were researchers at usc  all three
were familiar with robocuprescue  they were given time to study the setup and were not
given any time limit to provide their allocations  each subject was told that the allocations
were going to be judged first on the basis of the number of civilian lives lost and next on the
damage sustained due to fire  these are exactly the criteria used in robocuprescue  kitano
et al         
we then compared rmtdp allocation with those of the human subjects in the
robocuprescue simulator and with rescueisi and mdp  in figure     we compared the
performance of the allocations on the basis of the number of civilians who died and the
average damage to the two buildings  lower values are better for both criteria   these two
criteria are the main two criteria used in robocuprescue  kitano et al          the values shown in figure    were obtained by averaging forty simulator runs for the uniform
distribution and twenty runs for the skewed distribution for each allocation  the average
values were plotted to account for the stochasticity in the domain  error bars are provided
to show the standard error for each allocation method 
as can be seen in figure    a   the rmtdp allocation did better than the other five
allocations in terms of a lower number of civilians dead  although human  was quite close  
for example  averaging forty runs  the rmtdp allocation resulted in      civilian deaths
while human s allocation resulted in      civilian deaths  in terms of the average building
damage  the six allocations were almost indifferentiable  with the humans actually performing marginally better  using the skewed distribution  the difference between the allocations
was much more perceptible  see figure    b    in particular  we notice how the rmtdp
allocation does much better than the humans in terms of the number of civilians dead  here 
human  did particularly badly because of a bad allocation for fire engines  this resulted in
more damage to the buildings and consequently to the number of civilians dead 
   for the mission rehearsal domain  we could run on the actual mission rehearsal simulator since that
simulator is not public domain and no longer accessible  and hence the difference in how we tested role
allocations in the mission rehearsal and the robocuprescue domains 

   

fihybrid bdi pomdp framework for multiagent teaming

comparing rmtdp with rescueisi and the mdp approach showed that reasoning
about transitional uncertainty  mdp  does better than a static reactive allocation method
 rescueisi  but not as well as reasoning about both transitional and observational uncertainty  in the uniform distribution case  we found that rmtdp does better than both mdp
and rescueisi  with the mdp method performing better than rescueisi  in the skewed distribution case  the improvement in allocations using rmtdp is greater  averaging twenty
simulation runs  rmtdp allocations resulted in      civilians deaths while mdp resulted
in      and rescueisi in       the allocation method used by rescueisi often resulted
in one of the fires being allocated too few fire engines  the allocations determined by the
mdp approach turned out to be the same as human  
a two tailed t test was performed in order to test the statistical significance of the means
for the allocations in figure     the means of number of civilians dead for the rmtdp
allocation and the human allocations were found to be statistically different  confidence
       for both the uniform as well as the skewed distributions  the difference in the fire
damage was not statistically significant in the uniform case  however  the difference between
the rmtdp allocation and human  for fire damage was statistically significant         in
the skewed case 
 

civilians casualties
building damage

 
 
 
 
 

dp

ue

m

is

i

 
sc

m

an

 

re

hu

hu

m

an

 
an
m
hu

rm

td

p

 

 

civilians casualties
building damage

 
 
 
 
 

dp
m

i
re

sc
ue

is

 
an
m
hu

 
an
m
hu

 
an
m
hu

rm
td

p

 

figure     comparison of performance in robocuprescue  a   left  uniform  and b   right 
skewed 

   

finair   tambe

considering just the average performance of these different allocations does not highlight
the individual cases where marked differences were seen in the performance  in figure     we
present the comparison of particular settings where the other allocation methods showed a
bigger difference from rmtdp in terms of their allocations  the standard error is shown in
error bars for each allocation  figures    a  and    b  compare the allocations for uniform
civilian distributions in the setting where there was one civilian at fire   and four civilians at
fire        civilian setting  and four civilians at fire   and one at fire        civilian setting 
respectively  as can be seen in these figure  the rmtdp allocation results in fewer civilian
casualties but in slightly more damage to the buildings due to fire  difference in fire damage
was not statistically significant because the damage values were very close   figures    c 
and    d  compare the allocations for the skewed civilian distribution  the key difference
arises for human   as can be seen  human  results in more damage due to fire  this is
because human  allocated too few fire engines to one of the buildings  which in turn resulted
in that building being burnt down completely  consequently  civilians located at this fire
location could not be rescued by the ambulances  thus  we see specific instances where
the allocation done using the rmtdp based allocation algorithm is superior to allocations
that a human comes up with 

   

civilians casualties
building damage

 

   

civilians casualties
building damage

 
   

   

 

 

   
 

   

   

 

   

dp

i
is
ue

m

 
sc

 

an
m

re

hu

an
m
hu

p

an
m

td

civilians casualties
building damage

 

hu

rm

i

dp
m

is

 

ue

an
re

hu

sc

m

an
hu

m

an
m
hu

td
rm

 

 
 

 

p

   
 

 
   

   

civilians casualties
building damage

 
   

   

 
 

   
 

   

   

 

 
   

   
dp
m

is
i
sc
ue

m
an
 

re

hu

m
an
 
hu

hu

p
rm
td

dp
m

i
ue
is

 
an
m

re
sc

hu

 
an
m
hu

 
an
m
hu

td
p
rm

m
an
 

 

 

figure     comparison of performance in robocuprescue for particular settings  a   topleft  uniform     civilian setting b  top right  uniform     civilian setting  c 
 bottom left  skewed     civilian setting d  bottom right  skewed     civilian
setting 

   

fihybrid bdi pomdp framework for multiagent teaming

table   shows the allocations to fire    agents not assigned to fire   are allocated to fire
   found by the rmtdp role allocation algorithm and those used by the human subjects for
the skewed     civilian setting  we consider this case since it shows the most difference   in
particular  this table highlights the differences between the various allocators for the skewed
    civilian setting and helps account for the differences seen in their performance in the
actual simulator  as can be seen from figure    d   the main difference in performance
was in terms of the number of civilians saved  recall that in this scenario  there are four
civilians at fire    and one at fire    here all the human subjects and mdp chose to
send only one ambulance to fire    number of ambulances allocated to f ire       
number of ambulances allocated to f ire     this lone ambulance was unable to rescue the
civilian at fire    resulting in the humans and mdp saving fewer civilians  rescueisi chose to
send all the ambulances to fire   using a greedy selection method based on proximity to the
civilians resulting in all the civilians at fire   dying    in terms of the fire engine allocation 
human  sent in four fire engines to fire   where more civilians were likely to be located
 number of engines allocated to f ire        number of engines allocated to f ire    
unfortunately  this backfired since the lone fire engine at fire   was not able to extinguish
the fire there  causing the fire to spread to other parts of the city 
distribution
skewed    

engines from station  
engines from station  
engines from station  
ambulances

rmtdp
 
 
 
 

human 
 
 
 
 

human 
 
 
 
 

human 
 
 
 
 

rescueisi
 
 
 
 

mdp
 
 
 
 

table    allocations of ambulances and fire engines to fire   
these experiments show that the allocations found by the rmtdp role allocation algorithm performs significantly better than allocations chosen by human subjects and rescueisi
and mdp in most cases  and does not do significantly worse in any case   in particular
when the distribution of civilians is not uniform  it is more difficult for humans to come up
with an allocation and the difference between human allocations and the rmtdp allocation
becomes more significant  from this we can conclude that the rmtdp allocation performs
at near human expertise 
in our last experiment done using the robocuprescue simulator  we introduced error
in the rmtdp model in order to determine how sensitive the model was to errors in the
parameters of the model  figure    compares the allocations found  when there were five
ambulances    fire engines and   civilians  in terms of the number of civilian casualties  yaxis  when error  x axis  was introduced to the probability of fire spread and the probability
of civilian health deterioration  as can be seen increasing the error in the probability of fire
spread to     and higher results in allocations that save fewer civilians as the fire brigades
choose to concentrate their effort on only one of the fires  the resulting allocation was
found to have the same value in terms of the number of civilians casualties as that used by
rescueisi  which did not consider any uncertainty  reducing the error in the probability of
fire did not have an impact on the allocations found  increasing the error in probability of
   this strategy of ambulances going to the closest civilian worked fairly well because the ambulances were
usually well spread out

   

finair   tambe

civilian health deterioration to     and higher caused some civilians to be sacrificed  this
allocation was found to have the same value in terms of the number of civilians casualties as
that used by rescueisi  decreasing the error in probability of civilian health deterioration
    and lower  more negative  caused the number of ambulances to be allocated to a fire
to be the same as the number of civilians at that fire  same as human   
 

civilian casualties

   
 

fire rate
civilian health

   
 
   
 

                  

 

 

  

  

  

  

percentage error

figure     model sensitivity in the robocuprescue scenario 

   related work
there are four related areas of research  that we wish to highlight  first  there has been
a considerable amount of work done in the field of multiagent teamwork  section      
the second related area of research is the use of decision theoretic models  in particular
distributed pomdps  section       the third area of related work we describe  section     
are hybrid systems that used markov decision process and bdi approaches  finally  in
section      the related work in role allocation and reallocation in multiagent teams is
described 
    bdi based teamwork
several formal teamwork theories such as joint intentions  cohen   levesque        
sharedplans  grosz   kraus        were proposed that tried to capture the essence of
multiagent teamwork in the logic of beliefs desires intentions  next  practical models
of teamwork such as collagen  rich   sidner         grate   jennings        
steam  tambe        built on these teamwork theories  cohen   levesque        grosz
  kraus        and attempted to capture the aspects of teamwork that were reusable
across domains  in addition  to complement the practical teamwork models  the teamoriented programming approach  pynadath   tambe        tidhar      a      b  was
introduced to allow large number of agents to be programmed as teams  this approach
was then expanded on and applied to a variety of domains  pynadath   tambe        yen
et al         da silva   demazeau         other approaches for building practical multia   

fihybrid bdi pomdp framework for multiagent teaming

gent systems  stone   veloso        decker   lesser         while not explicitly based on
team oriented programming  could be considered in the same family 
the research reported in this article complements this research on teamwork by introducing hybrid bdi pomdp models that exploit the synergy between bdi and pomdp
approaches  in particular  top and teamwork models have traditionally not addressed
uncertainty and cost  our hybrid model provides this capability  and we have illustrated
the benefits of this reasoning via detailed experiments 
while this article uses team oriented programming  tambe et al         da silva  
demazeau        tidhar      a      b  as an example bdi approach  it is relevant to
other similar techniques of modeling and tasking collectives of agents  such as decker and
lessers        taems approach  in particular  the taems language provides an abstraction for tasking collaborative groups of agents similar to top  while the gpgp infrastructure used in executing taems based tasks is analogous to the top interpreter
infrastructure shown in figure    while lesser et al  have explored the use of distributed
mdps in analyses of gpgp coordination  xuan   lesser         they have not exploited
the use of taems structures in decomposition or abstraction for searching optimal policies
in distributed mdps  as suggested in this article  thus  this article complements lesser
et al s work in illustrating a significant avenue for further efficiency improvements in such
analyses 
    distributed pomdp models
distributed pomdp models represent a collection of formal models that are expressive
enough to capture the uncertainty in the domain and the costs and rewards associated
with states and actions  given a group of agents  the problem of deriving separate policies for them that maximize some joint reward can be modeled using distributed pomdp
models  in particular  the dec pomdp  decentralized pomdp   bernstein et al        
and mtdp  multiagent team decision problem   pynadath   tambe        are generalizations of pomdps to the case where there are multiple  distributed agents  basing
their actions on their separate observations  these frameworks allow us to formulate what
constitutes an optimal policy for a multiagent team and in principle derive that policy 
however  with a few exceptions  effective algorithms for deriving policies for distributed
pomdps have not been developed  significant progress has been achieved in efficient
single agent pomdp policy generation algorithms  monahan        cassandra  littman 
  zhang        kaelbling et al          however  it is unlikely such research can be directly
carried over to the distributed case  finding optimal policies for distributed pomdps is
nexp complete  bernstein et al          in contrast  finding an optimal policy for a single
agent pomdp is pspace complete  papadimitriou   tsitsiklis         as bernstein et
al  note  bernstein et al          this suggests a fundamental difference in the nature of the
problems  the distributed problem cannot be treated as one of separate pomdps in which
individual policies can be generated for individual agents because of possible cross agent
interactions in the reward  transition or observation functions   for any one action of one
agent  there may be many different rewards possible  based on the actions that other agents
may take  
   

finair   tambe

three approaches have been used to solve distributed pomdps  one approach that
is typically taken is to make simplifying assumptions about the domain  for instance  in
guestrin et al          it is assumed that each agent can completely observe the world state 
in addition  it is assumed that the reward function  and transition function  for the team
can be expressed as the sum  product  of the reward  transition  functions of the agents
in the team  becker et al         assume that the domain is factored such that each agent
has a completely observable local state and also that the domain is transition independent
 one agent cannot affect another agents local state  
the second approach taken is to simplify the nature of the policies considered for each
of the agents  for example  chades et al         restrict the agent policies to be memoryless
 reactive  policies  thereby simplifying the problem to solving multiple mdps  peshkin et
al         take a different approach by using gradient descent search to find local optimum
finite controllers with bounded memory  nair et al       a  present an algorithm for finding
a locally optimal policy from a space of unrestricted finite horizon policies  the third
approach  taken by hansen et al          involves trying to determine the globally optimal
solution without making any simplifying assumptions about the domain  in this approach 
they attempt to prune the space of possible complete policies by eliminating dominated
policies  although a brave frontal assault on the problem  this method is expected to
face significant difficulties in scaling up due to the fundamental complexity of obtaining a
globally optimal solution 
the key difference with our work is that our research is focused on hybrid systems where
we leverage the advantages of bdi team plans  which are used in practical systems  and
distributed pomdps that quantitatively reason about uncertainty and cost  in particular 
we use tops to specify large scale team plans in complex domains and use rmtdps for
finding the best role allocation for these teams 
    hybrid bdi pomdp approaches
pomdp models have been used in the context of analysis of both single agent  schut 
wooldridge    parsons        and multiagent  pynadath   tambe        xuan et al        
behavior  schut et al  compare various strategies for intention reconsideration  deciding
when to deliberate about its intentions  by modeling a bdi system using a pomdp  the
key differences with this work and our approach are that they apply their analysis to a single
agent case and do not consider the issues of exploiting bdi system structure in improving
pomdp efficiency 
xuan and lesser        and pynadath and tambe         both analyze multiagent
communication  while xuan and lesser dealt with finding and evaluating various communication policies  pynadath and tambe used the com mtdp model to deal with the problem of comparing various communication strategies both empirically and analytically  our
approach is more general in that we explain an approach for analyzing any coordination actions including communication  we concretely demonstrate our approach for analysis of role
allocation  additional key differences from the earlier work by pynadath and tambe       
are as follows   i  in rmtdp  we illustrate techniques to exploit team plan decomposition
in speeding up policy search  absent in com mtdp   ii  we also introduce techniques for
belief based evaluation absent from previous work  nonetheless  combining rmtdp with
   

fihybrid bdi pomdp framework for multiagent teaming

com mtdp is an interesting avenue for further research and some preliminary steps in
this direction are presented in nair  tambe and marsella      b  
among other hybrid systems not focused on analysis  scerri et al         employ markov
decision processes within team oriented programs for adjustable autonomy  the key difference between that work and ours is that the mdps were used to execute a particular
sub plan within the tops plan hierarchy and not for making improvements to the top 
dtgolog  boutilier  reiter  soutchanski    thrun        provides a first order language
that limits mdp policy search via logical constraints on actions  although it shares with
our work the key idea of synergistic interactions in mdps and golog  it differs from our
work in that it focuses on single agent mdps in fully observable domains  and does not
exploit plan structure in improving mdp performance  isaac  nair  tambe  marsella 
  raines         a system for analyzing multiagent teams  also employs decision theoretic
methods for analyzing multiagent teams  in that work  a probabilistic finite automaton
 pfa  that represents the probability distribution of key patterns in the teams behavior
are learned from logs of the teams behaviors  the key difference with that work is that the
analysis is performed without having access to the actual team plans that the agents are
executing and hence the advice provided cannot directly be applied to improving the team 
but will need a human developer to change the team behavior as per the advice generated 

    role allocation and reallocation
there are several different approaches to the problem of role allocation and reallocation 
for example  tidhar et al         and tambe et al         performed role allocation based
on matching of capabilities  while hunsberger and grosz        proposed the use of combinatorial auctions to decide on how roles should be assigned  modi et al         showed
how role allocation can be modeled as a distributed constraint optimization problem and
applied it to the problem of tracking multiple moving targets using distributed sensors 
shehory and kraus        suggested the use of coalition formation algorithms for deciding
quickly which agent took on which role  fatima and wooldridge        use auctions to
decide on task allocation  it is important to note that these competing techniques are not
free of the problem of how to model the problem  even though they do not have to model
transition probabilities  other approaches to reforming a team are reconfiguration methods due to dunin keplicz and verbrugge         self adapting organizations by horling
and lesser        and dynamic re organizing groups  barber   martin         scerri et
al         present a role  re allocation algorithm that allows autonomy of role reallocation
to shift between a human supervisor and the agents 
the key difference with all this prior work is our use of stochastic models  rmtdps 
to evaluate allocations  this enables us to compute the benefits of role allocation  taking
into account uncertainty and costs of reallocation upon failure  for example  in the mission
rehearsal domain  if uncertainties were not considered  just one scout would have been
allocated  leading to costly future reallocations or even in mission failure  instead  with
lookahead  depending on the probability of failure  multiple scouts were sent out on one or
more routes  resulting in fewer future reallocations and higher expected reward 
   

finair   tambe

   conclusion
while the bdi approach to agent teamwork has provided successful applications  tools and
techniques that provide quantitative analyses of team coordination and other team behaviors under uncertainty are lacking  the emerging field of distributed pomdps provides
a decision theoretic method for quantitatively obtaining the optimal policy for a team of
agents  but faces a serious intractability challenge  therefore  this article leverages the
benefits of both the bdi and pomdp approaches to analyze and improve key coordination
decisions within bdi based team plans using pomdp based methods  in order to demonstrate these analysis methods  we concentrated on role allocation  a fundamental aspect
of agent teamwork  and provided three key contributions  first  we introduced rmtdp 
a distributed pomdp based framework  for analysis of role allocation  second  this article
presented an rmtdp based methodology for optimizing key coordination decisions within
a bdi team plan for a given domain  concretely  the article described a methodology for
finding the best role allocation for a fixed team plan  given the combinatorially many
role allocations  we introduced methods to exploit task decompositions among sub teams
to significantly prune the search space of role allocations 
third  our hybrid bdi pomdp approach uncovered several synergistic interactions
between bdi team plans and distributed pomdps 
   tops were useful in constructing the rmtdp model for the domain  in identifying
the features that need to be modeled as well as in decomposing the model construction
according to the structure of the top  the rmtdp model could then be used to
evaluate the top 
   tops restricted the policy search by providing rmtdps with incomplete policies
with a limited number of open decisions 
   the bdi approach helped in coming up with a novel efficient belief based representation of policies suited for this hybrid bdi pomdp approach and a corresponding
algorithm for evaluating such policies  this resulted in faster evaluation and also a
more compact policy representation 
   the structure in the top was exploited to decompose the problem of evaluating
abstract policies  resulting in significant pruning in the search for the optimal role
allocations 
we constructed rmtdps for two domains  robocuprescue and mission rehearsal
simulation  and determined the best role allocation in these domains  furthermore  we
illustrated significant speedups in rmtdp policy search due to the techniques introduced
in this article  detailed experiments revealed the advantages of our approach over state ofthe art role allocation approaches that failed to reason with uncertainty 
our key agenda for future work is to continue scale up of rmtdps to even larger
scale agent teams  such scale up will require further efficiency improvements  we propose
to continue to exploit the interaction in the bdi and pomdp approaches in achieving
such scale up  for instance  besides disaster rescue  distributed sensor nets and large area
monitoring applications could benefit from such a scale up 
   

fihybrid bdi pomdp framework for multiagent teaming

acknowledgments
this research was supported by nsf grant           we would like to thank jim blythe 
anthony cassandra  hyuckchul jung  spiros kapetanakis  sven koenig  michael littman 
stacy marsella  david pynadath and paul scerri for discussions related to this article 
we would also like to thank the reviewers of this article whose comments have helped in
significantly improving this article 

appendix a  top details
in this section  we will describe the top for the helicopter scenario  the details of each
subplan in figure   b  are shown below 
executemission 
context 
pre conditions   mb  taskforce  location taskforce    start 
achieved   mb  taskforce   achieved doscouting   achieved dotransport   
  time   t   mb  taskforce  achieved remainingscouts  
  helo  scoutingteam  alive helo   location helo     end   
unachievable   mb  taskforce  unachievable doscouting  
  mb  taskforce   unachievable dotransport 
  achieved remainingscouts 
  helo  scoutingteam  alive helo   location helo     end    
irrelevant  
body 
doscouting
dotransport
remainingscouts
constraints 
doscouting  dotransport
doscouting  remainingscouts
doscouting 
context executemission  taskforce 
pre conditions  
achieved  
unachievable  
irrelevant 
body 
waitatbase
scoutroutes
constraints 
waitatbase and scoutroutes
waitatbase 
context  doscouting  taskforce 
pre conditions  
achieved  
unachievable   mb  transportteam   helo  transportteam  alive helo  
   

finair   tambe

irrelevant 
body 
no op



scoutroutes 
context  doscouting  taskforce 
achieved  
unachievable  
irrelevant  mb  scoutingteam   helo  transportteam  alive helo  
body 
scoutroute 
scoutroute 
scoutroute 
constraints 
scoutroute  or scoutroute  or scoutroute 
scoutroute  
context  scoutroutes  scoutingteam 
pre conditions  
achieved   mb  sctteama   helo  sctteama  location helo    end 
unachievable  time   t   mb  sctteama   helo  sctteama  alive helo  
irrelevant  
body 
if  location sctteama    start  then route sctteama    
if  location sctteama     end  then move forward
scoutroute  
context  scoutroutes  scoutingteam 
pre conditions  
achieved   mb  sctteamb   helo  sctteamb  location helo    end 
unachievable  time   t   mb  sctteamb   helo  sctteamb  alive helo  
irrelevant  
body 
if  location sctteamb    start  then route sctteamb    
if  location sctteamb     end  then move forward
scoutroute  
context  scoutroutes  scoutingteam 
pre conditions  
achieved   mb  sctteama   helo  sctteama  location helo    end 
unachievable  time   t   mb  sctteama   helo  sctteama  alive helo  
irrelevant  
body 
if  location sctteama    start  then route sctteama    
if  location sctteama     end  then move forward
dotransport 
context  executemission  taskforce 
pre conditions  
   

fihybrid bdi pomdp framework for multiagent teaming

achieved   mb  transportteam  location transportteam    end 
unachievable  time   t   mb  transportteam   helo  transportteam  alive helo  
irrelevant  
body 
if  location transportteam    start  then
if  mb  transportteam  achieved scoutroute    then
route transportteam    
elseif  mb  transportteam  achieved scoutroute    then
route transportteam    
elseif  mb  transportteam  achieved scoutroute    then
route transportteam    
if  route transportteam     null  and  location transportteam     end  then
move forward
remainingscouts 
context  executemission  taskforce 
pre conditions  
achieved   mb  scoutingteam  location scoutingteam    end 
unachievable  time   t   mb  scoutingteam    helo  scoutingteam
alive helo   location helo     end  
irrelevant  
body 
if  location scoutingteam     end  then move forward

the predicate achieved tplan  is true if the achieved conditions of tplan are true  similarly  the predicates unachievable tplan  and irrelevant tplan  are true if the the unachievable conditions and the irrelevant conditions of tplan are true  respectively  the predicate
 location team    end  is true if all members of team are at end 
figure   b  also shows coordination relationships  an and relationship is indicated
with a solid arc  while an or relationship is indicated with a dotted arc  these coordination relationships indicate unachievability  achievability and irrelevance conditions that
are enforced by the top infrastructure  an and relationship between team sub plans
means that if any of the team sub plans fail  then the parent team plan will fail  also  for
the parent team plan to be achieved  all the child sub plans must be achieved  thus  for
doscouting  waitatbase and scoutroutes must both be done 
achieved   mb  taskforce  achieved waitatbase   achieved scoutroutes  
unachievable   mb  taskforce  unachievable waitatbase 
 unachievable scoutroutes  

an or relationship means that all the subplans must fail for the parent to fail and success of
any of the subplans means that the parent plan has succeeded  thus  for scoutingroutes 
at least one of scoutroute   scoutroute  or scoutroute  need be performed 
 mb  scoutingteam  achieved scoutroute   
achieved scoutroute   achieved scoutroute   
unachievable   mb  taskforce  unachievable scoutroute   
unachievable scoutroute    unachievable scoutroute   
achieved 

   

finair   tambe

also an and relationship affects the irrelevance conditions of the subplans that it joins  if
the parent is unachievable then all its subplans that are still executing become irrelevant 
thus  for waitatbase 
irrelevant 

 mb  taskforce  unachievable scoutroutes  

similarly for scoutingroutes 
irrelevant 

 mb  taskforce  unachievable scoutroutes  

 
finally  we assign roles to plans  figure   b  shows the assignment in brackets adjacent to the plans  for instance  task force team is assigned to jointly perform execute
mission 

appendix b  rmtdp details
in this section  we present details of the rmtdp constructed for the top in figure   
 s  we get the features of the state from the attributes tested in the preconditions
and achieved  unachievable and irrelevant conditions and the body of the team plans
and individual agent plans  thus the relevant state variables are location of each
helicopter  role of each helicopter route of each helicopter  status of each helicopter
 alive or not  and time  for a team of n helicopters  the state is given by the tuple
  time  role            rolen   loc            locn   route            routen   status            statusn   
 a  we consider actions to be the primitive actions that each agent can perform
within its individual plans  the top infrastructure enforces mutual belief through
communication actions  since analyzing the cost of these is not the focus of this
research we consider communication to be implicit and we model the effect of this
communication directly in the observation function 
we consider   kinds of actions role taking and role execution actions  we assume
that the initial allocation will specify roles for all agents  this specifies whether the
agent is a scout or a transport and if a scout which scout team it is assigned to  a
scout cannot become a transport or change its team after its initial allocation while a
transport can change its role by taking one of the role taking actions the role taking
and role execution actions for each agent i are given by 
i membert ransportt eam    joinsctt eama  joinsctt eamb  joinsctt eamc 
i membersctt eama   i membersctt eamb   i membersctt eamcx   
i membert ransportt eam    chooseroute  movef orward 
i membersctt eama   i membersctt eamb   i membersctt eamc    movef orward 
 p   we obtain the transition function with the help of a human expert or through
simulations if a simulator is available  in this domain  helicopters can crash  be shot
down  if they are not at start  end or an already scouted location  the probability
that scouts will get shot down depends on which route they are on  i e  probability
of crash on route  is p    probability of crash on route  is p  and probability of crash
on route  is p  and how many scouts are on the same spot  we assume that the
   

fihybrid bdi pomdp framework for multiagent teaming

probability of a transport being shot down in an unscouted location to be   and in
a scouted location to be    the probability of multiple crashes can be obtained by
multiplying the probabilities of individual crashes 
the action  moveforward  will have no effect if routei   null or loci   end or if
statusi   dead  in all other cases  the location of the agent gets incremented  we
assume that the role taking actions scoutroutex will always succeed if the role of the
performing agent is transport and it has not been assigned a route already 
   each transport at start can observe the status of the other agents with some
probability depending on their positions  each helicopter on a particular route can
observe all the helicopters on that route completely and cannot observe helicopters
on other routes 
 o  the observation function gives the probability for a group of agents to receive a
particular joint observation  in this domain we assume that observations of one agent
are independent of the observations of other agents  given the current state and the
previous joint action  thus the probability of a joint observation can be computed by
multiplying the probabilities of each individual agents observations 
the probability of a transport at start observing the status of an alive scout on
route   is       the probability of a transport at start observing nothing about
that alive scout is      since we dont have false negatives  similarly if a scout on
route   crashes  the probability that this is visible to a transport at start is     
and the probability that the transport doesnt see this failure is       similarly the
probabilities for observing an alive scout on route   and route   and      and     
respectively and the probabilities for observing a crash on route   and route   and
     and      respectively 
 r  the reward function is obtained with the help of a human expert who helps
assign value to the various states and the cost of performing various actions  for
this analysis  we assume that actions moveforward and chooseroute have no cost 
we consider the negative reward  cost  for the replacement action  scoutroutex  to
be r   the negative reward for a failure of a helicopter to be rf   the reward for a
scout reaching end to be rscout and the reward for a transport reaching end to be
rtransport   e g  r       rf       rscout      rtransport      
 rl  these are the roles that individual agents can take in top organization hierarchy 
rl    transport  scoutonroute   scoutonroute   scoutonroute   

appendix c  theorems
theorem   the maxexp method will always yield an upper bound 
proof sketch 
 let policy   be the leaf level policy with the highest expected reward under a particular parent node  i  in the restricted policy space 
v   maxchildren i  v
   

   

finair   tambe

 since the reward function is specified separately for each component  we can separate the expected reward v into the rewards from the constituent components given
the starting states and starting observation histories of these components  let the
team plan be divided into m components such that the components are parallel and
independent or sequentially executed 
x
v 
maxstates j  ohistories j vj
 jm

 the expected value obtained for any component j     j  m for   cannot be greater
than that of the highest value obtained for j using any policy 
maxstates j  ohistories j vj  maxchildren i  maxstates j  ohistories j  vj  

   

 hence 
v  

x

maxchildren i  maxstates j  ohistories j  vj  

 jm

v  maxestimate i 

   



references
barber  s     martin  c          dynamic reorganization of decision making groups  in
proceedings of the fifth international conference on autonomous agents  agents     
pp         
becker  r   zilberstein  s   lesser  v     goldman  c  v          transition independent
decentralized markov decision processes  in proceedings of the second international
joint conference on autonomous agents and multi agent systems  aamas      pp 
     
bernstein  d  s   zilberstein  s     immerman  n          the complexity of decentralized control of mdps  in proceedings of the sixteenth conference on uncertainty in
artificial intelligence uai      pp       
boutilier  c          planning  learning   coordination in multiagent decision processes  in
proceedings of the sixth conference on theoretical aspects of rationality and knowledge  tark      pp         
boutilier  c   reiter  r   soutchanski  m     thrun  s          decision theoretic  highlevel agent programming in the situation calculus  in proceedings of the seventeenth
national conference on artificial intelligence  aaai      pp         
cassandra  a   littman  m     zhang  n          incremental pruning  a simple  fast 
exact method for partially observable markov decision processes  in proceedings of
the thirteenth annual conference on uncertainty in artificial intelligence  uai     
pp       
   

fihybrid bdi pomdp framework for multiagent teaming

chades  i   scherrer  b     charpillet  f          a heuristic approach for solving
decentralized pomdp  assessment on the pursuit problem  in proceedings of the     
acm symposium on applied computing  sac      pp       
cohen  p  r     levesque  h  j          teamwork  nous                 
da silva  j  l  t     demazeau  y          vowels co ordination model  in proceedings
of the first international joint conference on autonomous agents and multiagent
systems  aamas        pp           
dean  t     lin  s  h          decomposition techniques for planning in stochastic domains  in proceedings of the fourteenth international joint conference on artificial
intelligence  ijcai      pp           
decker  k     lesser  v          quantitative modeling of complex computational task
environments  in proceedings of the eleventh national conference on artificial intelligence  aaai      pp         
dix  j   muoz avila  h   nau  d  s     zhang  l          impacting shop  putting an
ai planner into a multi agent environment  annals of mathematics and artificial
intelligence                 
dunin keplicz  b     verbrugge  r          a reconfiguration algorithm for distributed
problem solving  engineering simulation             
erol  k   hendler  j     nau  d  s          htn planning  complexity and expressivity  in
proceedings of the twelfth national conference on artificial intelligence  aaai     
pp           
fatima  s  s     wooldridge  m          adaptive task and resource allocation in multiagent systems  in proceedings of the fifth international conference on autonomous
agents  agents      pp         
georgeff  m  p     lansky  a  l          procedural knowledge  proceedings of the ieee
special issue on knowledge representation               
goldman  c  v     zilberstein  s          optimizing information exchange in cooperative
multi agent systems  in proceedings of the second international joint conference on
autonomous agents and multi agent systems  aamas      pp         
grosz  b   hunsberger  l     kraus  s          planning and acting together  ai magazine 
             
grosz  b     kraus  s          collaborative plans for complex group action  artificial
intelligence                 
guestrin  c   venkataraman  s     koller  d          context specific multiagent coordination and planning with factored mdps  in proceedings of the eighteenth national
conference on artificial intelligence  aaai      pp         
hansen  e     zhou  r          synthesis of hierarchical finite state controllers for pomdps 
in proceedings of the thirteenth international conference on automated planning and
scheduling  icaps      pp         
   

finair   tambe

hansen  e  a   bernstein  d  s     zilberstein  s          dynamic programming for partially
observable stochastic games  in proceedings of the nineteenth national conference
on artificial intelligence  aaai      pp         
ho  y  c          team decision theory and information structures  proceedings of the
ieee                 
horling  b   benyo  b     lesser  v          using self diagnosis to adapt organizational
structures  in proceedings of the fifth international conference on autonomous
agents  agents      pp         
hunsberger  l     grosz  b          a combinatorial auction for collaborative planning  in
proceedings of the fourth international conference on multiagent systems  icmas       pp         
jennings  n          controlling cooperative problem solving in industrial multi agent
systems using joint intentions  artificial intelligence                 
kaelbling  l   littman  m     cassandra  a          planning and acting in partially
observable stochastic domains  artificial intelligence                 
kitano  h   tadokoro  s   noda  i   matsubara  h   takahashi  t   shinjoh  a     shimada 
s          robocup rescue  search and rescue for large scale disasters as a domain
for multiagent research  in proceedings of ieee conference on systems  men  and
cybernetics  smc      pp         
levesque  h  j   cohen  p  r     nunes  j          on acting together  in proceedings of the
national conference on artificial intelligence  pp        menlo park  calif   aaai
press 
mailler  r  t     lesser  v          solving distributed constraint optimization problems using cooperative mediation  in proceedings of the third international joint conference
on agents and multiagent systems  aamas      pp         
marschak  j     radner  r          the economic theory of teams  cowles foundation
and yale university press  new haven  ct 
modi  p  j   shen  w  m   tambe  m     yokoo  m          an asynchronous complete
method for distributed constraint optimization  in proceedings of the second international joint conference on agents and multiagent systems  aamas      pp 
       
monahan  g          a survey of partially observable markov decision processes  theory 
models and algorithms  management science               
nair  r   ito  t   tambe  m     marsella  s          task allocation in the rescue simulation
domain  in robocup       robot soccer world cup v  vol       of lecture notes in
computer science  pp          springer verlag  heidelberg  germany 
nair  r   pynadath  d   yokoo  m   tambe  m     marsella  s       a   taming decentralized
pomdps  towards efficient policy computation for multiagent settings  in proceedings
of the eighteenth international joint conference on artificial intelligence  ijcai     
pp         
   

fihybrid bdi pomdp framework for multiagent teaming

nair  r   tambe  m     marsella  s       b   team formation for reformation in multiagent domains like robocuprescue  in kaminka  g   lima  p     roja  r   eds   
proceedings of robocup      international symposium  pp          lecture notes
in computer science  springer verlag 
nair  r   tambe  m   marsella  s     raines  t          automated assistants to analyze
team behavior  journal of autonomous agents and multi agent systems           
    
papadimitriou  c     tsitsiklis  j          complexity of markov decision processes  mathematics of operations research                 
peshkin  l   meuleau  n   kim  k  e     kaelbling  l          learning to cooperate via
policy search  in proceedings of the sixteenth conference in uncertainty in artificial
intelligence  uai      pp         
poupart  p     boutilier  c          bounded finite state controllers  in proceedings of
advances in neural information processing systems     nips  
pynadath  d  v     tambe  m          the communicative multiagent team decision
problem  analyzing teamwork theories and models  journal of artificial intelligence
research             
pynadath  d  v     tambe  m          automated teamwork among heterogeneous software agents and humans  journal of autonomous agents and multi agent systems
 jaamas            
rich  c     sidner  c          collagen  when agents collaborate with people  in
proceedings of the first international conference on autonomous agents  agents     pp         
scerri  p   johnson  l   pynadath  d   rosenbloom  p   si  m   schurr  n     tambe  m 
        a prototype infrastructure for distributed robot  agent  person teams  in
proceedings of the second international joint conference on agents and multiagent
systems  aamas      pp         
scerri  p   pynadath  d  v     tambe  m          towards adjustable autonomy for the
real world  journal of artificial intelligence  jair              
schut  m  c   wooldridge  m     parsons  s          reasoning about intentions in uncertain domains  in proceedings of the sixth european conference on symbolic and
quantitative approaches to reasoning with uncertainty  ecsqaru        pp    
   
shehory  o     kraus  s          methods for task allocation via agent coalition formation 
artificial intelligence                    
sondik  e  j          the optimal control of partially observable markov processes  ph d 
thesis  stanford 
stone  p     veloso  m          task decomposition  dynamic role assignment  and lowbandwidth communication for real time strategic teamwork  artificial intelligence 
                
   

finair   tambe

tambe  m          towards flexible teamwork  journal of artificial intelligence research 
         
tambe  m   pynadath  d     chauvat  n          building dynamic agent organizations in
cyberspace  ieee internet computing              
tidhar  g       a   team oriented programming  preliminary report  tech  rep      australian artificial intelligence institute 
tidhar  g       b   team oriented programming  social structures  tech  rep      australian artificial intelligence institute 
tidhar  g   rao  a     sonenberg  e          guided team selection  in proceedings of the
second international conference on multi agent systems  icmas      pp         
wooldridge  m          an introduction to multiagent systems  john wiley   sons 
xuan  p     lesser  v          multi agent policies  from centralized ones to decentralized ones  in proceedings of the first international joint conference on agents and
multiagent systems  aamas      pp           
xuan  p   lesser  v     zilberstein  s          communication decisions in multiagent
cooperation  in proceedings of the fifth international conference on autonomous
agents  agents      pp         
yen  j   yin  j   ioerger  t  r   miller  m  s   xu  d     volz  r  a          cast  collaborative agents for simulating teamwork  in proceedings of the seventeenth international
joint conference on artificial intelligence  ijcai      pp           
yoshikawa  t          decomposition of dynamic team decision problems  ieee transactions on automatic control  ac                

   

fi