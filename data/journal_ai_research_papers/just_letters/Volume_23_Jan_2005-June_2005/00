journal artificial intelligence research               

submitted        published      

finding approximate pomdp solutions belief
compression
nicholas roy

nickroy mit edu

massachusetts institute technology 
computer science artificial intelligence laboratory
cambridge 

geoffrey gordon

ggordon cs cmu edu

carnegie mellon university  school computer science
pittsburgh  pa

sebastian thrun

thrun stanford edu

stanford university  computer science department
stanford  ca

abstract
standard value function approaches finding policies partially observable markov
decision processes  pomdps  generally considered intractable large models 
intractability algorithms large extent consequence computing
exact  optimal policy entire belief space  however  real world pomdp
problems  computing optimal policy full belief space often unnecessary
good control even problems complicated policy classes  beliefs experienced
controller often lie near structured  low dimensional subspace embedded
high dimensional belief space  finding good approximation optimal value function
subspace much easier computing full value function 
introduce new method solving large scale pomdps reducing dimensionality belief space  use exponential family principal components analysis  collins  dasgupta    schapire        represent sparse  high dimensional belief spaces
using small sets learned features belief state  plan terms
low dimensional belief features  planning low dimensional space  find
policies pomdp models orders magnitude larger models
handled conventional techniques 
demonstrate use algorithm synthetic problem mobile robot
navigation tasks 

   introduction
decision making one central problems artificial intelligence robotics 
robots deployed world accomplish specific tasks  real world
difficult place actactions serious consequences  figure   a  depicts
mobile robot  pearl  designed operate environment shown figure   b  
longwood retirement facility pittsburgh  real world environments longwood
characterized uncertainty  sensors cameras range finders noisy
entire world always observable  large number state estimation techniques
explicitly recognize impossibility correctly identifying true state world
 gutmann  burgard  fox    konolige        olson        gutmann   fox        kanazawa 
c
    
ai access foundation  rights reserved 

firoy  gordon    thrun

koller    russell        isard   blake        using probabilistic techniques track
location robot  state estimators kalman filter  leonard   durrantwhyte        markov localization  fox  burgard    thrun        thrun  fox  burgard 
  dellaert        provide  possibly factored  boyen   koller        distribution
possible states world instead single  possibly incorrect  state estimate 

 a 

figure   

 b 

planner mobile robot pearl  shown  a   must able navigate
reliably real environments longwood oakmont retirement facility 
shown  b   white areas map free space  black pixels
obstacles  grey areas regions map uncertainty  notice
large open spaces  many symmetries lead ambiguity robots
position  map     m     m  resolution    m    m per pixel 

contrast  controllers motion planners  dialogue systems  etc  rarely model
notions uncertainty  state estimate full probability distribution 
controller often uses heuristic extract single best state  distributions
mean mode  planners compensate inevitable estimation errors robust control  chen        bagnell   schneider         deployed systems incorporate
full probabilistic state estimate planning  although most likely state method
simple used successfully real applications  nourbakhsh  powers   
birchfield         substantial control errors result distribution possible
states uncertain  single state estimate wrong  planner likely choose
unreasonable action 
figure   illustrates difference conventional controllers model
uncertainty  figure  robot must navigate bottom right corner top
left  limited range sensing  up  m  noisy dead reckoning   impoverished
   purposes example sensing dead reckoning artificially poor 
phenomenon would occur naturally larger scale environments 

 

fifinding approximate pomdp solutions belief compression

sensor data cause robots state estimate become quite uncertain strays
far environmental structures use localize itself  left  figure  a 
example trajectory motion planner knowledge uncertainty
state estimate mechanism taking uncertainty account  robots
trajectory diverges desired path  robot incorrectly believes arrived
goal  shown state estimates reflect high uncertainty
robot position  right  figure  b  example trajectory controller
model positional uncertainty  take action keep uncertainty small following
walls  arrive reliably goal 

goal

goal

measured path

measured path
true path
true path

start

start

 a  conventional controller

figure   

 b  robust controller

two possible trajectories navigation longwood oakmont environment  robot limited range sensing  up  m  poor dead reckoning
odometry   a  trajectory conventional motion planner uses
single state estimate  minimizes travel distance   b  trajectory
robust controller models state uncertainty minimize travel
distance uncertainty 

controller figure   b  derived representation called partially observable markov decision process  pomdp   pomdps technique making decisions
based probabilistic estimates state world  rather absolute knowledge
true state  pomdp uses priori model world together history
actions taken observations received order infer probability distribution 
belief  possible states world  controller chooses actions  based upon
current belief  maximize reward expects receive time 
advantage using pomdps decision making resulting policies
handle uncertainty well  pomdp planning process take advantage actions
implicitly reduce uncertainty  even problem specification  e g   reward function 
explicitly reward actions  disadvantage pomdps finding
optimal policy computationally intractable  existing techniques finding exact optimal
 

firoy  gordon    thrun

plans pomdps typically cannot handle problems hundred states
 hauskrecht        zhang   zhang         planning problems involving real  physical
systems cannot expressed compactly  would deploy robots plan
thousands possible states world  e g   map grid cells   thousands possible
observations  e g   laser range measurements  actions  e g   velocities  
paper  describe algorithm finding approximate solutions realworld pomdps  algorithm arises insight exact pomdp policies use
unnecessarily complex  high dimensional representations beliefs controller
expect experience  finding low dimensional representations  planning process
becomes much tractable 
first describe find low dimensional representations beliefs realworld pomdps  use variant common dimensionality reduction technique
called principal components analysis  particular variant use modifies loss
function pca order better model data probability distributions  using
low dimensional representations  describe plan low dimensional space 
conclude experimental results robot control tasks 

   partially observable markov decision processes
partially observable markov decision process  pomdp  model deciding
act accessible  stochastic environment known transition model  russell
norvig         pg        pomdp described following 









set states    s    s          s s   
set actions    a    a            a a   
set observations z    z    z            z z   
set transition probabilities  si   a  sj     p sj  si   a 
set observation probabilities o zi   a  sj     p zi  sj   a 
set rewards r     r
discount factor       
initial belief p   s 

transition probabilities describe state evolves actions  represent markov assumption  next state depends current  unobservable 
state action independent preceding  unobserved  states actions 
reward function describes objective control  discount factor used
ensure reasonable behaviour face unlimited time  optimal policy known
always exist discounted        case bounded immediate reward  howard 
      
pomdp policies often computed using value function belief space 
value function v  b  given policy defined long term expected reward
controller receive starting belief b executing policy horizon time 
may infinite  optimal pomdp policy maximizes value function 
value function pomdp policy finite horizon described using piecewise linear function space beliefs  many algorithms compute value function
iteratively  evaluating refining current value function estimate
 

fifinding approximate pomdp solutions belief compression

refinements improve expected reward policy belief  figure   a 
shows belief space three state problem  belief space two dimensional 
shaded simplex  point simplex corresponds particular belief  a threedimensional vector   corners simplex represent beliefs state
known      certainty  value function shown figure   b  gives long term
expected reward policy  starting belief simplex 

 

  
 

   

 
 

   

 
 

   

 
 

   

 
 
 

 
 
   

   
   
   
   
 

 

   

   

   

   

   

 

   
   
 

 a  belief space
figure   

 

   

   

   

   

 

 b  value function

 a  belief space three state problem two dimensional  shaded
simplex   b  value function defined belief space  purposes
visualization  set beliefs constitutes belief space shown  a 
projected onto xy plane  b   value function rises along
positive z axis  point belief space corresponds specific
distribution  value function point gives expected reward
policy starting belief  belief space  and therefore value
function  one fewer dimension total number states
problem 

process evaluating refining value function core solving
pomdps considered intractable  value function defined space
beliefs  continuous high dimensional  belief space one fewer
dimension number states model  navigation problem map
thousands possible states  computing value function optimization problem
continuous space many thousands dimensions  feasible existing
algorithms 
however  careful consideration real world problems suggests possible approach
finding approximate value functions  examine beliefs navigating mobile
robot encounters  beliefs share common attributes  beliefs typically
small number modes  particular shape modes fairly generic  modes
move change variance  ways modes change relatively
constrained  fact  even real world navigation problems large belief spaces 
beliefs degrees freedom 
figure   a  illustrates idea  shows typical belief mobile robot might
experience navigating nursing home environment figure   b   visualize
distribution sample set poses  also called particles  according distribution
 

firoy  gordon    thrun

plot particles map  distribution unimodal probability mass
mostly concentrated small area  figure   b  shows different kind belief 
probability mass spread wide area  multiple modes  locations
particles bear little relationship map  would difficult find sequence
actions observations would result belief 

particles


 a  common belief

figure   

 b  unlikely belief

two example probability distributions robot pose  small black dots
particles drawn distribution discrete grid positions  left
distribution robots location relatively certain  kind compact 
unimodal distribution common robot navigation  right
different  implausible distribution  right hand distribution sufficiently
unlikely afford ignore it  even unable distinguish
belief belief result fail identify optimal action 
quality controller unaffected 

real world beliefs degrees freedom  concentrated near
low dimensional subset high dimensional belief spacethat is  beliefs experienced
controller lie near structured  low dimensional surface embedded belief
space  find surface  representation belief state terms
small set bases features  one benefit representation need
plan terms small set features  finding value functions low dimensional
spaces typically easier finding value functions high dimensional spaces 
two potential disadvantages sort representation  first
contains approximation  longer finding complete  optimal pomdp policy 
instead  as suggested figure    trying find representations belief
rich enough allow good control sufficiently parsimonious make
 

fifinding approximate pomdp solutions belief compression

planning problem tractable  second disadvantage technical one 
making nonlinear transformation belief space  pomdp planning algorithms
assume convex value function longer work  discuss problem detail
section   
conventional
path planner

pomdp

tractable
robust

figure   

intractable
robust

useful planner lies somewhere continuum mdp style
approximations full pomdp solution 

   dimensionality reduction
order find low dimensional representation beliefs  use statistical dimensionality reduction algorithms  cox   cox         algorithms search projection
original high dimensional representation beliefs lower dimensional compact representation  is  search low dimensional surface  embedded
high dimensional belief space  passes near sample beliefs  consider
evolution beliefs pomdp trajectory inside belief space  assumption trajectories large  real world pomdps lie near low dimensional
surface embedded belief space  figure   depicts example low dimensional surface
embedded belief space three state pomdp described previous section 
 

   

   

   

   

 
 
   
   
   
   
 

figure   

 

   

   

   

   

 

one dimensional surface  black line  embedded two dimensional belief space
 gray triangle   black dot represents single belief probability distribution
experienced controller  beliefs lie near low dimensional surface 

ideally  dimensionality reduction involves information lossall aspects data
recovered equally well low dimensional representation highdimensional one  practice  though  see use lossy representations
belief  that is  representations may allow original data beliefs
recovered without error  still get good control  but  see finding
representations probability distributions require careful trade off
 

firoy  gordon    thrun

preserving important aspects distributions using dimensions possible 
measure quality representation penalizing reconstruction errors
loss function  collins et al          loss function provides quantitative way
measure errors representing data  different loss functions result different
low dimensional representations 
principal components analysis
one common forms dimensionality reduction principal components analysis  joliffe         given set data  pca finds linear lower dimensional representation data variance reconstructed data preserved  intuitively 
pca finds low dimensional hyperplane that  project data onto
hyperplane  variance data changed little possible  transformation
preserves variance seems appealing maximally preserve ability distinguish beliefs far apart euclidean norm  see below  however 
euclidean norm appropriate way measure distance beliefs
goal preserve ability choose good actions 
first assume data set n beliefs  b            bn   b  belief bi
b  high dimensional belief space  write beliefs column vectors
matrix b    b           bn    b r s n   use pca compute low dimensional
representation beliefs factoring b matrices u b 
b   u b  

   

equation      u r s l corresponds matrix bases span low dimensional
space l    s  dimensions  b rnl represents data low dimensional space  
geometric perspective  u comprises set bases span hyperplane b
high dimensional space b  b co ordinates data hyperplane 
hyperplane dimensionality l exists contains data exactly  pca find surface given dimensionality best preserves variance data  projecting
data onto hyperplane reconstructing it  minimizing change variance original data b reconstruction u b equivalent minimizing
sum squared error loss 
l b  u  b    kb u b k f  

   

pca performance
figure   shows toy problem use evaluate success pca finding
low dimensional representations  abstract model two dimensional state space 
one dimension position along one two circular corridors  one binary variable
determines corridor in  states s        s    inclusive correspond one corridor 
states s          s    correspond other  reward known position
different corridor  therefore  agent needs discover corridor  move
   many descriptions pca based factorization u sv   u v column orthonormal
diagonal  could enforce similar constraint identifying b   v s  case columns u
would orthonormal b would orthogonal 

 

fifinding approximate pomdp solutions belief compression

appropriate position  declare arrived goal  goal declared
system resets  regardless whether agent actually goal   agent
  actions  left  right  sense corridor  declare goal  observation
transition probabilities given discretized von mises distributions  mardia   jupp 
      shatkay   kaelbling         exponential family distribution defined       
von mises distribution wrapped analog gaussian  accounts fact
two ends corridor connected  sum two von mises variates
another von mises variate  product two von mises likelihoods
scaled von mises likelihood  guarantee true belief distribution always
von mises distribution corridor action observation 
instance problem consists     states    actions     observations 
actions     move controller left right  with von mises noise  action
  returns observation uniquely correctly identifies half maze
agent  the top half bottom half   observations returned actions    
identify current state modulo      probability observation von mises
distribution mean equal true state  modulo       is  observations
indicate approximately agent horizontally 
max prob 
obs      
 

max prob 
obs      
 

max prob 
obs      

 

 

   

   

 

 

   

   

 

reward
   

   

   

   
   

max prob 
obs        
   

observation  top 
action    prob   

reward

observation  bottom 
action    prob   

   

figure    toy maze     states 

maze interesting relatively large pomdp standards      states 
contains particular kind uncertaintythe agent must use action   point
uniquely identify half maze in  remaining actions result observations
contain information corridor agent in  problem large
solved conventional pomdp value iteration  structured heuristic
policies perform poorly 
collected data set     beliefs assessed performance pca beliefs
problem  data collected using hand coded controller  alternating
random exploration actions mdp solution  taking current state
maximum likelihood state belief  figure   shows   sample beliefs data
set  notice beliefs essentially two discretized von mises distributions
different weights  one half maze  starting belief state
left most distribution figure    equal probability top bottom corridors 
position along corridor following discretized von mises distribution concentration
parameter      meaning p state  falls   e maximum value move    
way around corridor likely state  
 

firoy  gordon    thrun

sample belief

sample belief

    

    

     

     

    

    

     

     

    
     
    

    
     
    

     

     

 

 

      

 

  

  

  

      

                          
state

probability

    
     

probability

probability

sample belief
    
     

    
     
    
     
 

 

  

  

  

      

                          
state

 

  

  

  

                          
state

sample belief
    
    

probability

   
    
    
    
    
 

figure   

 

  

  

  

  

                       
state

sample beliefs toy problem  sample set      different  noncontiguous  points time  left most belief initial belief state 

figure   examines performance pca representing beliefs data set
computing average error original beliefs b reconstructions u b  
figure   a  see average squared error  squared l    compared number
bases  figure   b   see average kullbach leibler  kl  divergence 
kl divergence belief b reconstruction r   u b low dimensional
representation b given
kl b k r   

 s 
x

b si   ln

i  



b si  
r si  



   

minimizing squared l  error explicit objective pca  kl divergence
appropriate measure much two probability distributions differ   
unfortunately  pca performs poorly representing probability distributions  despite
fact probability distributions collected data set   degrees
freedom  reconstruction error remains relatively high somewhere   
   basis functions  examine reconstruction sample belief  see
kinds errors pca making  figure    shows sample belief  the solid line 
reconstruction  the dotted line   notice reconstructed belief strange
artifacts  contains ringing  multiple small modes   negative regions 
pca purely geometric process  notion original data probability
distributions  therefore free generate reconstructions data contain
negative numbers sum   
   use popular implementation pca based golub reinsche algorithm  golub   reinsch 
      available gnu scientific library  galassi  davies  theiler  gough  jungman  booth 
  rossi        
   note computing kl divergence reconstruction original belief 
shift reconstruction non negative  rescale sum   

  

fifinding approximate pomdp solutions belief compression

average l  error vs  number bases

average kl divergence vs  number bases

    

   
average kl divergence

   
average l  error

     
    
     
 

   
 
   
   
   
   
 

 

 

  
  
  
number bases

  

  

 

 a  average squared l   error
figure   

 

  

  
  
number bases

  

  

 b  average kl divergence

average error original sample set b reconstructions u b 
 a  squared l  error  explicitly minimized pca   b  kl divergence 
error bars represent standard deviation mean error
    beliefs 
example belief reconstruction
     

original belief
reconstructed belief

    

probability

     
    
     
    
     
    
     
 
      

 

  

  

  

  

                       
state

figure     example belief reconstruction  using    bases 

notice pca process making significant errors lowprobability regions belief  particularly unfortunate real world probability distributions tend characterized compact masses probability  surrounded
large regions zero probability  e g   figure  a   therefore need modify
pca ensure reconstructions probability distributions  improve representation sparse probability distributions reducing errors made low probability
events 
question answered loss functions available instead sum
squared errors  equation      would loss function better reflects
need represent probability distributions 
  

firoy  gordon    thrun

   exponential family pca
conventional view pca geometric one  finding low dimensional projection
minimizes squared error loss  alternate view probabilistic one 
data consist samples drawn probability distribution  pca algorithm
finding parameters generative distribution maximize likelihood
data  squared error loss function corresponds assumption data
generated gaussian distribution  collins et al         demonstrated pca
generalized range loss functions modeling data different exponential
families probability distributions gaussian  binomial  poisson 
exponential family distribution corresponds different loss function variant
pca  collins et al         refer generalization pca arbitrary exponential
family data likelihood models exponential family pca e pca 
e pca model represents reconstructed data using low dimensional weight
vector b  basis matrix u   link function f  
b f  u b 

   

e pca model uses different link function  derived data
likelihood model  and corresponding error distribution loss function   link
function mapping data space another space data
linearly represented 
link function f mechanism e pca generalizes dimensionality reduction non linear models  example  identity link function corresponds
gaussian errors reduces e pca regular pca  sigmoid link function
corresponds bernoulli errors produces kind logistic pca     valued data 
nonlinear link functions correspond non gaussian exponential families
distributions 
find parameters e pca model maximizing log likelihood
data model  shown  collins et al         equivalent
minimizing generalized bregman divergence
bf  b k u b    f  u b  b u b   f  b 

   

low dimensional high dimensional representations  solve using
convex optimization techniques   here f convex function whose derivative f  
f convex dual f   ignore f purpose minimizing equation  
since value b fixed   relationship pca e pca link
functions reminiscent relationship linear regression generalized linear
models  mccullagh   nelder        
apply e pca belief compression  need choose link function accurately reflects fact beliefs probability distributions  choose link
function
f  u b    eu b
   
p u b
p
hard verify f  u b   
e f  b    b ln b b  so  equation  
becomes
x
x
bf  b k u b   
eu b b u b   b ln b
b
   
  

fifinding approximate pomdp solutions belief compression

write b   f  u b   equation   becomes
bf  b k u b    b ln b b ln b  

x

b

x

b   u kl b k b 

u kl unnormalized kl divergence  thus  choosing exponential link function     corresponds minimizing unnormalized kl divergence original
belief reconstruction  loss function intuitively reasonable choice measuring error reconstructing probability distribution   exponential link function
corresponds poisson error model component reconstructed belief 
choice loss link functions two advantages  first  exponential link
function constrains low dimensional representation eu b positive  second  error
model predicts variance belief component proportional expected
value  since pca makes significant errors close    wish increase penalty
errors small probabilities  error model accomplishes that 
compute loss bi   ignoring terms depend data b  then 

l b  u  b   

 b 
x
i  


eu bi bi u bi  

   

introduction link function raises question  instead using complex
machinery e pca  could choose non linear function project data
space linear  use conventional pca  difficulty
approach course identifying function  general  good link functions e pca
related good nonlinear functions application regular pca  so 
might appear reasonable use pca find low dimensional representation log
beliefs  rather use e pca exponential link function find representation
beliefs directly  approach performs poorly surface locally
well approximated log projection  e pca viewed minimizing weighted
least squares chooses distance metric appropriately local  using conventional
pca log beliefs performs poorly situations beliefs contain extremely
small zero probability entries 
p
   chosen link function eu b   eu b would arrived normalized kl divergence 
perhaps even intuitively reasonable way measure error reconstructing
probability distribution  more complicated link function would made difficult
derive newton equations following pages  impossible  experimented
resulting algorithm found produces qualitatively similar results algorithm described
here  using normalized kl divergence one advantage  allow us get away
one fewer basis function planning  since unnormalized kl divergence e pca optimization
must learn basis explicitly represent normalization constant 
   e pca related lee seungs        non negative matrix factorization  one nmf loss
functions presented lee seung        penalizes kl divergence matrix
reconstruction  equation    but  nmf loss incorporate link function
e pca loss  another nmf loss function presented lee seung        penalizes squared
error constrains factors nonnegative  resulting model example  gl    m 
generalization e pca described gordon        

  

firoy  gordon    thrun

finding e pca parameters
algorithms conventional pca guaranteed converge unique answer independent initialization  general  e pca property  loss function    
may multiple distinct local minima  however  problem finding best b given
b u convex  convex optimization problems well studied unique global
solutions  rockafellar         similarly  problem finding best u given b b
convex  so  possible local minima joint space u b highly constrained 
finding u b require solving general non convex optimization problem 
gordon        describes fast  newtons method approach computing u b
summarize here  algorithm related iteratively reweighted least squares 
popular algorithm generalized linear regression  mccullagh   nelder         order
use newtons method minimize equation      need derivative respect u
b 

 u b 

l b  u  b   
e

b u b
   
u
u
u
  e u b  b b b
    
   e u b  b b

    


 u b 


l b  u  b   
e

b u b
b
b
b
  u e u b  u b


  u  e

 u b 

b  

    
    
    

set right hand side equation      zero  iteratively compute bj  
column b  newtons method  let us set q bj     u  e u bj   bj    linearize
bj find roots q    gives
j th

bjnew   bj
bjnew bj

 

q bj  
q    bj  

u  e u bj   bj  
q    bj  

    
    

note equation    formulation newtons method finding roots q  typically
written
f  xn  
xn     xn  
 
    
f  xn  
need expression q    
q
bj

 
 


u  e u bj   bj  
bj

u e u bj  
bj

  u dj u
  

    
    
    

fifinding approximate pomdp solutions belief compression

define dj terms diag operator returns diagonal matrix 



dj   diag eu bj   

    


b s         
 

    
  
diag b       
 
 
 
      b s s   

    



combining equation      equation       get
 u dj u   bjnew bj     u  bj eu bj  

    

u dj u bjnew    u dj u  bj   u dj dj   bj eu bj  
  u dj  u bj   dj   bj eu bj   

    
    

weighted least squares problem solved standard linear algebra
techniques  order ensure solution numerically well conditioned  typically
add regularizer divisor 
bjnew  

u dj  u bj   dj   bj eu bj  
 u dj u       il  

 

    

il l l identity matrix  similarly  compute new u computing ui  
ith row u  
 ui b    bi eui b  di   di b
 
    
uinew  
 bdi b       il  
e pca algorithm
algorithm automatically finding good low dimensional representation
b high dimensional belief set b  algorithm given table    optimization
iterated termination condition reached  finite number iterations 
minimum error achieved 
steps     raise one issue  although solving row u column b
separately convex optimization problem  solving two matrices simultaneously
not  therefore subject potential local minima  experiments
find problem  expect need find ways address local
minimum problem order scale even complicated domains 
bases u found  finding low dimensional representation highdimensional belief convex problem  compute best answer iterating equation       recovering full dimensional belief b low dimensional representation b
straightforward 
x   eu b  
    
definition pca explicitly factor data u   b many
presentations do  three part representation pca  contains singular values
  

firoy  gordon    thrun

   collect set sample beliefs high dimensional belief space
   assemble samples data matrix b    b           b b   
   choose appropriate loss function  l b  u  b 
   fix initial estimate b u randomly
  
  

column bj b 
compute bjnew using current u estimate equation     

  
  

row ui u  

  

compute uinew using new b estimate equation     

    l b  u  b   

table   

e pca algorithm finding low dimensional representation pomdp 
including gordons newtons method        

decomposition  u b orthonormal  use two part representation
b f  u b  quantity e pca decomposition corresponds
singular values pca  result  u b general orthonormal 
desired  though  possible orthonormalize u additional step optimization
using conventional pca adjust b accordingly 

   e pca performance
using loss function equation     iterative optimization procedure described
equation      equation      find low dimensional factorization  look
well dimensionality reduction procedure performs pomdp examples 
toy problem
recall figure   unable find good representations data
fewer       bases  even though domain knowledge indicated data
  degrees freedom  horizontal position mode along corridor  concentration
mode  probability top bottom corridor   examining one
sample beliefs figure     saw representation worst lowprobability regions  take data set toy example  use e pca
find low dimensional representation compare performance pca e pca 
figure    a  shows e pca substantially efficient representing data 
see kl divergence falling close     bases  additionally  squared l  
error   bases             we need   bases perfect reconstruction  rather
   since must include constant basis function  small amount reconstruction
  

fifinding approximate pomdp solutions belief compression

error   bases remains stopped optimization procedure fully
converged  

average kl divergence vs  number bases  e pca 

example belief reconstruction using   bases

   

     
    
     
    

   
 

probability

average kl divergence

   

   
   
   
 
 

 

  

  
  
number bases

  

  

 a  reconstruction performance

probability

probability

     
     
     
    
     

  

  

  

                       
state

 e   
   e   
 e   
 e   

     

 
   

   

   
state

   

   

  

 c  belief reconstruction near
peak

figure    

  

example belief reconstruction using   bases
 e   
original belief
reconstructed belief
   e   

original belief
reconstructed belief

     

 

 b  example belief reconstruction

example belief reconstruction using   bases
    

     
    
     
    
     
 
      

   

original belief
reconstructed belief

  

   
state

   

   

 d  belief reconstruction lowprobability region

 a  average kl divergence original sample set reconstructions  kl divergence         bases  error bars represent
standard deviation mean     beliefs   b  example
belief figure    reconstruction using   bases  reconstruction
shows small errors peak mode  shown reconstruction
using   bases  original belief reconstruction indistinguishable naked eye   c   d  show fine detail original belief
reconstruction two parts state space  although reconstruction
perfect  low probability area  see error approximately
       

  

firoy  gordon    thrun

figure    b  shows e pca reconstruction example belief figure    
see many artifacts present pca reconstruction absent  using
  bases  see e pca reconstruction already substantially better pca
using    bases  although small errors peaks  e g   figure   c 
two modes   using   bases  e pca reconstruction indistinguishable naked eye
original belief   kind accuracy     bases typical
data set 
robot beliefs
although performance e pca finding good representations abstract problem
compelling  would ideally able use algorithm real world problems 
robot navigation problem figure    figures       show results two
robot navigation problems  performed using physically realistic simulation  although
artificially limited sensing dead reckoning   collected sample set     beliefs
moving robot around environment using heuristic controller  computed
low dimensional belief space b according algorithm table    full state space
    m   m  discretized resolution  m  m per pixel  total     states 
figure    a  shows sample belief  figure    b  reconstruction using   bases 
figure    c  see average reconstruction performance e pca approach 
measured average kl divergence sample belief reconstruction 
comparison  performance pca e pca plotted  e pca error falls
       bases  suggesting   bases sufficient good reconstruction 
substantial reduction  allowing us represent beliefs problem using
  parameters  rather     parameters  notice many states lie regions
outside map  is  states never receive probability mass
removed  removing states would trivial operation  e pca correctly
able automatically 
figure     similar results shown different environment  sample set    
beliefs collected using heuristic controller  low dimensional belief space
b computed using e pca  full state space     m     m  resolution
  m   m per pixel  example belief shown figure    a   reconstruction
using   bases shown figure    b   reconstruction performance measured
average kl divergence shown figure    c   error falls close   around  
bases  minimal improvement thereafter 

   computing pomdp policies
exponential family principal components analysis model gives us way find
low dimensional representation beliefs occur particular problem 
two real world navigation problems tried  algorithm proved effective
finding low dimensional representations  showing reductions     states
       states     bases      dimensional belief space allow much
tractable computation value function  able solve much
larger pomdps could solved previously 
  

fifinding approximate pomdp solutions belief compression

kl divergence sampled beliefs reconstructions

particles form
bimodal distribution

  

e pca
pca

  

kl divergence

  

 a  original belief

  
  
  
  
  
 
 
 

 

 

 

 

 

 

 

 

number bases

 c  reconstruction performance

 b  reconstruction

figure    

 a  sample belief robot navigation task   b  reconstruction
belief learned e pca representation using   bases   c  average
kl divergence sample beliefs reconstructions
number bases used  notice e pca error falls close     bases 
whereas conventional pca much worse reconstruction error even   bases 
improving rapidly 

kl divergence sampled beliefs reconstructions
 
   
kl divergence

 
   
 
   
 
   
 

 a  sample belief

figure    

 b  reconstruction

 

 

 

 
 
  
number bases

 c  average
performance

  

  

  

reconstruction

 a  sample belief navigation problem longwood  cf  figure     b 
reconstruction learned e pca representation using   bases   c 
average kl divergence sample beliefs reconstructions
number bases used 

unfortunately  longer use conventional pomdp value iteration find
optimal policy given low dimensional set belief space features  pomdp value iteration depends fact value function convex belief space 
  

firoy  gordon    thrun

compute non linear transformation beliefs recover coordinates
low dimensional belief surface  lose convexity value function  compare figure   figure   see why   result  value function cannot expressed
supremum set hyperplanes low dimensional belief space 
so  instead using pomdp value iteration  build low dimensional discrete
belief space mdp use mdp value iteration  since know form
value function  turn function approximation  gordon        proved
fitted value iteration algorithm guaranteed find bounded error approximation
 possibly discounted  mdps value function  long use combination
function approximator averager  averagers function approximators
non expansions max norm  is  exaggerate errors training data 
experiments below  use regular grids well irregular  variable resolution grids
based   nearest neighbour discretization  represented set low dimensional beliefs
b  
b    b    b            b b     
    
approximations averagers  averagers include linear interpolation  knearest neighbours  local weighted averaging  focus detail exact
mechanism discretizing low dimensional space  outside scope
paper  resolution regular grid cases chosen empirically  section  
describe specific variable resolution discretization scheme worked well empirically 
reader consult munos moore        zhou hansen       
sophisticated representations 
fitted value iteration algorithm uses following update rule compute t step
lookahead value function v  t    step lookahead value function v t   


 b  
x
v  bi     max r  bi   a   
 bi   a  bj   v t   bj  
    


j  

r approximate reward transition functions based dynamics
pomdp  result e pca  finite set low dimensional belief samples
b using function approximator  note problems described
paper  problem require discounting         following sections describe
compute model parameters r  
computing reward function
original reward function r s  a  represents immediate reward taking action
state s  cannot know  given either low dimensional high dimensional belief 
immediate reward be  compute expected reward  therefore
represent reward expected value immediate reward full model 
current belief 
r  b   a    eb  r s  a  

    

 s 

 

x
i  

  

r si   a b si   

    

fifinding approximate pomdp solutions belief compression

equation      requires us recover high dimensional belief b low dimensional
representation b   shown equation      
many problems  reward function r effect giving low immediate
reward belief states high entropy  is  many problems planner
driven towards beliefs centred high reward states low uncertainty 
property intuitively desirable  beliefs robot worry
immediate bad outcome 
computing transition function
computing low dimensional transition function   p bj  a  bi   simple
computing low dimensional reward function r   need consider pairs lowdimensional beliefs  bi bj   original high dimensional belief space  transition
prior belief bi posterior belief bj described bayes filter equation 
bj  s    o s  a  z 

 s 
x

 sk   a  s bi  sk  

    

k  

action selected z observation saw  original pomdp
transition probability distribution  original pomdp observation probability
distribution 
equation      describes deterministic transition conditioned upon prior belief 
action observation  transition posterior bj stochastic observation known  is  transition bi bj occurs specific z
generated  probability transition probability generating observation
z  so  separate full transition process deterministic transition b  
belief acting sensing  stochastic transition b j   full posterior 
ba  s   

 s 
x

 sj   a  s bi  sj  

    

j  

bj  s    o s  a  z ba  s  

    

equations       describe transitions high dimensional beliefs
original pomdp  based high dimensional transitions  compute transitions low dimensional approximate belief space mdp  figure    depicts process 
figure shows  start low dimensional belief bi   bi reconstruct
high dimensional belief b according equation       apply action
observation z described equation      equation      find new belief
b    b  compress low dimensional representation b  iterating
equation       finally  since b  may member sample b low dimensional
belief states  map b  nearby bj b according function approximator 
function approximator grid  last step means replacing b 
prototypical bj shares grid cell  generally  function approximator may
represent b  combination several states  putting weight w bj   b    bj    for
example  approximator k nearest neighbour  w bj   b      k  closest k
  

firoy  gordon    thrun

  
bi

 
b

  
bj
lowdimensional

action


b

ba

b

highdimensional

observation
z

figure     process computing single transition probability 

samples b    case replace transition bi b  several transitions 
bi bj   scale probability one w bj   b    
transition bi b ba b  b  bj assign probability
p z  j i  a   

p z ba   w bj   b   

 

w bj   b   

 s 
x

p z sl  ba  sl  

    

l  

total transition probability  bi   a  bj   sum  observations z  p z  j i  a  
step   table   performs computation  shares work computation
 bi   a  bj   different posterior beliefs bj reachable prior belief
bi action a 
computing value function
reward transition functions computed previous sections  use
value iteration compute value function belief space mdp  full algorithm
given table   

   solving large pomdps
section  present application algorithm finding policies large
pomdps 
toy problem
first tested e pca belief features using regular grid representation version
toy problem described earlier  ensure needed small set belief
samples bi   made goal region larger  used coarser discretization
underlying state space     states instead      allow us compute low dimensional
model quickly 
figure    shows comparison policies different algorithms  e pca
approximately twice well maximum likelihood heuristic  heuristic guesses
corridor  correct half time  amdp heuristic algorithm
augmented mdp algorithm reported roy thrun         controller attempts
  

fifinding approximate pomdp solutions belief compression

   generate discrete low dimensional belief space b using e pca  cf  table   
   compute low dimensional reward function r  
b b  
 a  recover b b
 b  compute r  b  a   

p s 

i   r si   a b si   

   compute low dimensional transition function  
bi b  
 a  bj    bi   a  bj      
 b  recover bi bi
 c  observation z
 d 

compute bj bayes filter equation      b 

 e 

compute b  bj iterating equation      

 f 

bj w bj   b       
add p z  j i  a  equation       bi   a  bj  

 g 

   compute value function b
 a     
 b  bi b   v    bi      
 c 
 d 

change    

 e 

bi b  


p b  
v  bi     maxa r  bi   a    j    bi   a  bj   v t   bj  

change   change   v  bi   v t   bi  
 f  change    

table    value iteration e pca pomdp

find policy result lowest entropy belief reaching goal 
controller poorly unable distinguish unimodal belief
knows corridor position within corridor  bimodal
belief knows position corridor  results figure    averaged
       trials 
noted problem sufficiently small conventional pca fares
reasonably well  next sections  see problems pca representation
poorly compared e pca 
  

firoy  gordon    thrun

average reward vs  number bases
      

e pca
pca

average reward

      
     
     
     

mdp heuristic

     
 
      

amdp heuristic
 

 

 

 

number bases

figure    

comparison policy performance using different numbers bases        
trials  regular grid discretization  policy performance given total
reward accumulated trials 

robot navigation
tested e pca pomdp algorithm simulated robot navigation problems two
example environments  wean hall corridor shown figure    longwood retirement facility shown figure   b   model parameters given robot navigation
models  see fox et al         
evaluated policy relatively simple problem depicted figure     set
robots initial belief may one two locations corridor 
objective get within    m goal state  each grid cell    m   m  
controller received reward       arriving goal state taking at goal
action  reward      given  incorrectly  taking action non goal state 
reward   motion  states used planning example
    states along corridor  actions forward backward motion 
figure    shows sample robot trajectory using e pca policy   basis functions 
notice robot drives past goal lab door order verify orientation
returning goal  robot know true position  cannot know
fact passing goal  robot started end corridor 
orientation would become apparent way goal 
figure    shows average policy performance three different techniques 
maximum likelihood heuristic could distinguish orientations  therefore approximately     time declared goal wrong place  evaluated policy
learned using best   bases conventional pca  policy performed substantially
better maximum likelihood heuristic controller incorrectly declare robot arrived goal  however  representation could detect
robot goal  chose sub optimal  with respect e pca
policy  motion actions regularly  e pca outperformed techniques example able model belief accurately  contrast result figure   
pca sufficient representation perform well better e pca 
  

fifinding approximate pomdp solutions belief compression

true position

goal state
final estimated
position

true start state

figure    

goal position

start position

example robot trajectory  using policy learned using   basis functions 
left start conditions goal  right robot
trajectory  notice robot drives past goal lab door localize
itself  returning goal 

policy perfomance mobile robot navigation
      

average reward

      
      
      
 

         
       

       

       
       
       

figure    

ml heuristic

pca

e pca

comparison policy performance using e pca  conventional pca
maximum likelihood heuristic        trials 

figure    a  shows second example navigation simulation  notice initial
belief problem bi modal  good policy take actions disambiguate
modes proceeding goal  using sample set     beliefs  computed
low dimensional belief space b  figure    b  shows average kl divergence
original reconstructed beliefs  improvement kl divergence error measure
slowed substantially around   bases  therefore used   bases represent belief
space 
figure    c  shows example execution policy computed using e pca 
reward parameters previous navigation example  robot
parameters maximum laser range  m  high motion model variance  first
action policy chose turn robot around move closer nearest wall 
effect eliminating second distribution mode right  robot
followed essentially coastal trajectory left hand wall order stay localized 
although uncertainty direction became relatively pronounced  see
uncertainty eventually resolved top image  robot moved
goal 
  

firoy  gordon    thrun

kl divergence sampled beliefs reconstructions
 
   

true  hidden  start

 
kl divergence

goal

   
 
   
 
   
 

start distribution modes

 a  initial distribution

 

 

 

 
 
  
number bases

  

  

  

 b  reconstruction performance

positional accuracy goal

 
distance goal metres

 

 

     

 
 
 
 
 

 c  complete trajectory

figure    

     

 

     
e pca

amdp

mdp

 d  policy performance

 a  sample navigation problem longwood  cf  figure    problem
involves multi modal distributions   c  average kl divergence
sample beliefs reconstructions number bases used     
samples beliefs navigating mobile robot environment   d  comparison policy performance using e pca  conventional mdp amdp
heuristic 

interesting note policy contains similar coastal attribute
heuristic policies  e g   entropy heuristic amdp  cassandra  kaelbling   
kurien        roy   thrun         however  unlike heuristics  e pca representation able reach goal accurately  that is  get closer goal  
representation successful able accurately represent beliefs
effects actions beliefs 
  

fifinding approximate pomdp solutions belief compression

finding people

 a  original belief

 b  reconstruction pca

 c  reconstruction e pca

figure    

performance pca e pca sample belief  map       
grid cells     m resolution   a  sample belief   b  pca reconstruction 
using    bases   c  e pca reconstruction  using   bases 

addition synthetic problem robot navigation problems described
previous sections  tested algorithm complicated pomdp problem 
finding person object moving around environment  problem
motivated nursebot domain  residents experiencing cognitive decline
sometimes become disoriented start wander  order make better use
health care providers time  would use robot pearl  figure  a  find
residents quickly  assume person adversarial 
state space problem much larger previous robot navigation problems  cross product persons position robots position  however 
assume simplicity robots position known  therefore belief distribution persons position  transitions person state feature
modelled brownian motion fixed  known velocity  models persons
motion random  independent robot position   if person moving avoid
captured robot  different transition model would required   assume
position person unobservable robot close enough see
person  when robot line of sight person  maximum range  usually
  metres   observation model    false negatives false positives  reward
function maximal person robot location 
  

firoy  gordon    thrun

figure    a  shows example probability distribution occur problem
 not shown robots position   grey dots particles drawn distribution
person could environment  distribution initially uniform
reachable areas  inside black walls   robot receives sensor data 
probability mass extinguished within sensor range robot  robot
moves around  probability mass extinguished  focusing distribution
remaining places person be  however  probability distribution starts
recover mass places robot visits leaves  particle filter 
visualized particles leaking areas previously emptied out 
collected set     belief samples using heuristic controller given driving
robot maximum likelihood location person  used e pca find good
low dimensional representation beliefs  figure    b  shows reconstruction
example belief figure    a   using conventional pca    bases  figure
reinforce idea pca performs poorly representing probability distributions  figure    c  shows reconstruction using e pca   bases  qualitatively better
representation original belief 
recall section   use function approximator representing value
function  preceding examples used regular grid low dimensional surface
performed well finding good policies  however  problem finding people empirically requires finer resolution representation would computationally tractable
regular grid  therefore turn different function approximator    nearestneighbour variable resolution representation  add new low dimensional belief states
model periodically re evaluating model grid cell  splitting gridcell smaller discrete cells statistic predicted model disagrees
statistic computed experience  number different statistics suggested
testing model data real world  munos   moore        
reduction reward variance  value function disagreement  opted instead
simpler criterion transition probability disagreement  examine policy computed
using fixed representation  policy computed using incrementally refined
representation  note fully explored effect different variable resolution representations value function  e g   using k nearest neighbour interpolations
described hauskrecht         experiments beyond scope
paper  focus utility e pca decomposition  variable resolution
representation value function shown scale effectively beyond tens
dimensions best  munos   moore        
problem shares many attributes robot navigation problem  see
figure    figures       problem generates spatial distributions higher
complexity  somewhat surprising e pca able find good representation
beliefs using   bases  indeed average kl divergence generally higher
robot navigation task  regardless  able find good controllers 
example problem pca performs poorly even large number
bases 
figure    shows example trajectory heuristic control strategy  driving
robot maximum likelihood location person time step  open circle
robot position  starting far right  solid black circle position
  

fifinding approximate pomdp solutions belief compression

figure    

 a 

 b 

 c 

 d 

 e 

 f 

example suboptimal person finding policy  grey particles drawn
distribution person might be  initially uniformly distributed  a   black dot true  unobservable  position person 
open circle observable position robot  robots poor
action selection  person able escape previously explored areas 

person  unobservable robot within  m range  person starts
room corridor  a   moves corridor robot
moved far end corridor  b   robot returns search inside room  c 
 d   person moves unobserved previously searched corridor  e   although
deliberately chosen example heuristic performs poorly  person
following unlikely adversarial trajectory  times solid black circle remains
regions high probability  robots belief accurately reflects possibility
person slip past  heuristic control algorithm way take possibility
account 
using policy found low dimensional belief space described previous
sections  able find much better controller  sample trajectory controller
  

firoy  gordon    thrun

figure    

 a 

 b 

 c 

 d 

 e 

 f 

policy computed using e pca representation  initial conditions
panel  a  figure     notice that  unlike previous figure 
strategy ensures probability mass located one place  allowing
robot find person significantly higher probability 

shown figure     robot travels right most position corridor  a 
part way corridor  b   returns explore room  c 
 d   example  persons starting position different one given
previous examplethe e pca policy would find person point  starting
initial conditions previous example   exploring room eliminating
possibility person inside room  e   policy reduced possible
locations person left hand end corridor  able find
person reliably location 
note figures       target person worst case start position
planner  person start position figure    figure    
policy would found person panel  d   similarly  person started
  

fifinding approximate pomdp solutions belief compression

end corridor figure     policy shown figure    would found
person panel  b  
performance different policies

average   actions find person

   

   

   

   
fully observable policy
  

 

figure    

closest

densest

mdp

pca

e pca refined e pca

comparison   policies person finding simple environment 
baseline fully observable  i e   cheating  solution  the solid line   epca policy fixed  variable resolution  discretization  refined e pca
discretization additional belief samples added  pca
policy approximately   times worse best e pca policy 

figure    shows quantitative comparison performance e pca
number heuristic controllers simulation  comparing average time find
person different controllers  solid line depicts baseline performance 
using controller access true state person times  i e   fully
observable lower bound best possible performance   travel time case
solely function distance person  searching necessary performed 
course  realizable controller reality  controllers are 
closest  robot driven nearest state non zero probability 
densest  robot driven location probability mass
visible 
mdp  robot driven maximum likelihood state 
pca  controller found using pca representation fixed discretization
low dimensional surface 
e pca  e pca controller using fixed discretization low dimensional surface
compute value function 
refined e pca  e pca controller using incrementally refined variable resolution
discretization surface computing value function 
performance best e pca controller surprisingly close theoretical best
performance  terms time find person  result demonstrates need
careful choice discretization belief space computing value function 
  

firoy  gordon    thrun

initial variable resolution representation proved poor function approximator  however  using iteratively refined variable resolution discretization  able improve
performance substantially  controller using conventional pca representation
case computed fixed discretization low dimensional representation using
   bases     grid points  quality belief representation pca poor
investigate complex policy approximators 

   discussion
experiments demonstrate e pca algorithm scale finding low dimensional surfaces embedded high dimensional spaces 
time complexity
algorithm iterative therefore simple expression total running time
available  data set  b  samples dimensionality n  computing surface size
l  iteration algorithm o  b nl      b l    nl     step newtons
algorithm dominated set matrix multiplies final step inverting l l
matrix  o l     u step consists  b  iterations  iteration o nl 
multiplies o l    inversion  v step consists n iterations  iteration
o  b l  multiplies o l     inversion  leading total complexity given above 
figure    shows time compute e pca bases     sample beliefs 
       states  implementation used java       colt          ghz athlon
cpu    m ram  shown computation times conventional pca
decomposition  small state space problems  e pca decomposition faster
pca small number bases  implementation pca always computes
full decomposition  l   n  l reduced dimensionality n full
dimensionality  
exponential family pca running time
     
     
time secs

     
     
     
     
conventional pca       sec

    
 

figure    

 

 

 

 
 
number bases

  

  

time compute e pca representations different discretizations
state space 

  

fifinding approximate pomdp solutions belief compression

far dominant term running time algorithm time compute
e pca bases  bases found low dimensional space
discretized  running time required value iteration converge policy
problems described order       ms 
sample belief collection
example problems addressed  used standard sample size    
sample beliefs  additionally  used hand coded heuristic controllers sample beliefs
model  practice  found     sample beliefs collected using semi random controller sufficient example problems  however  may able improve overall
performance algorithm future problems iterating phases building
belief space representation  i e   collecting beliefs generating low dimensional
representation  computing good controller  initial set beliefs
collected used build initial set bases corresponding policy  continue
evaluate error representation  e g   k l divergence current belief
low dimensional representation   initial representation learned
beliefs  representation may over fit beliefs  detect situation
noticing representation poor job representing new beliefs  validation
techniques cross validation may useful determining enough beliefs
acquired 
model selection
one open questions addressed far choosing appropriate
number bases representation  unless problem specific information 
true number degrees freedom belief space  as toy example
section     difficult identify appropriate dimensionality underlying surface
control  one common approach examine eigenvalues decomposition 
recovered using orthonormalization step algorithm table   
 this assumes particular link function capable expressing surface
data lies on   eigenvalues conventional pca often used determine
appropriate dimensionality underlying surface  certainly reconstruction
lossless use many bases non zero eigenvalues 
unfortunately  recall description e pca section   generate
set singular values  eigenvalues  non linear projection introduced link
function causes eigenvalues u matrix uninformative contribution
basis representation  instead using eigenvalues choose appropriate
surface dimensionality  use reconstruction quality  figure     using reconstruction
quality estimate appropriate dimensionality common choice pca
dimensionality reduction techniques  tenenbaum  de silva    langford         one
alternate choice would evaluate reward policies computed different dimensionalities choose compact representation achieves highest reward 
essentially using control error rather reconstruction quality determine dimensionality 
  

firoy  gordon    thrun

recall discussion section   using dimensionality reduction
represent beliefs pomdps specific kind structure  particular  e pca
representation useful representing beliefs relatively sparse
small number degrees freedom  however  e pca unable find good lowdimensional representations pomdp models exhibit kind structure
is  beliefs cannot represented lying low dimensional hyperplane linked
full belief space via appropriate link function  one additional problem
know priori whether specific pomdp appropriate structure  unlikely
general technique determine usefulness e pca 
take advantage model selection techniques determine whether e pca
find usefully low dimensional representation specific pomdp  example 
kl divergence set sample beliefs reconstructions large even using
large number bases  problem may right structure 

   related work
many attempts made use reachability analysis constrain set beliefs
planning  washington        hauskrecht        zhou   hansen        pineau  gordon 
  thrun      a   reachable set beliefs relatively small  forward search
find set perfectly reasonable approach  policy computed beliefs course optimal  although relatively rare real world problems able
enumerate reachable beliefs  reachability analysis used
success heuristic guiding search methods  especially focusing computation
finding function approximators  washington        hansen         approach 
problem still remains compute low dimensional representation given finite
set representative beliefs  discretization belief space explored
number times  regular grid based discretization  lovejoy         regular variable
resolution approaches  zhou   hansen        non regular variable resolution representations  brafman        hauskrecht         vein  state abstraction  boutilier  
poole        explored take advantage factored state spaces  particular
interest algorithm hansen feng        perform state abstraction
absence prior factorization  far  however  approaches fallen
victim curse dimensionality failed scale dozen
states most 
value directed pomdp compression algorithm poupart boutilier       
dimensionality reduction technique closer spirit ours  technique 
algorithm computes low dimensional representation pomdp directly model
parameters r    finding krylov subspace reward function belief
propagation  krylov subspace vector matrix smallest subspace
contains vector closed multiplication matrix  pomdps 
authors use smallest subspace contains immediate reward vector closed
set linear functions defined state transitions observation model 
major advantage approach optimizes correct criterion  value directed
compression distinguish beliefs different value  major
disadvantage approach krylov subspace constrained linear  using
  

fifinding approximate pomdp solutions belief compression

algorithm pca instead e pca  realize much compression
poupart boutilier        method  take advantage regularities
transition matrices a z reward function r  unfortunately 
seen  beliefs unlikely lie low dimensional hyperplane  results reported
section   indicate linear compression scale size problems wish
address 
possibly promising approaches finding approximate value functions
point based methods  instead optimizing value function entire
belief space  specific beliefs  cheng        described method backing
value function specific belief points procedure called point based dynamic
programming  pb dp   pb dp steps interleaved standard backups
full value iteration  zhang zhang        improved method choosing witness
points backup belief points  iteratively increasing number points 
essential idea point based backups significantly cheaper full backup steps 
indeed  algorithm described zhang zhang        out performs hansens exact
policy search method order magnitude small problems  however  need
periodic backups across full belief space still limits applicability algorithms
small abstract problems 
recently  pineau et al       a  abandoned full value function backups
favour point based backups point based value iteration  pbvi  algorithm 
backing discrete belief points  backup operator polynomial instead
exponential  as value iteration   and  even importantly  complexity
value function remains constant  pbvi uses fundamentally different approach finding
pomdp policies  still remains constrained curse dimensionality large state
spaces  however  applied successfully problems least order magnitude
larger predecessors  another example algorithms used make
large pomdps tractable 
e pca possible technique non linear dimensionality reduction 
exists large body work containing different techniques self organizing maps  kohonen         generative topographic mapping  bishop  svensen    williams        
stochastic neighbour embedding  hinton   roweis         two successful
algorithms emerge recently isomap  tenenbaum et al         locally linear embedding  roweis   saul         isomap extends pca like methods non linear
surfaces using geodesic distances distance metric data samples  rather
euclidean distances  locally linear embedding  lle  considered local alternative
global reduction isomap represents point weighted combination neighbours operates two phases  computing weights k nearest
neighbours high dimensional point  reconstructing data lowdimensional co ordinate frame weights  however  algorithms contain
explicit models kind data  e g   probability distributions  attempting
model  one interesting line research  however  may extend algorithms using
different loss functions manner pca extended e pca 
  

firoy  gordon    thrun

    conclusion
partially observable markov decision processes considered intractable finding
good controllers real world domains  particular  best algorithms date
finding approximate value function full belief space scaled beyond
hundred states  pineau et al       a   however  demonstrated real world
pomdps contain structured belief spaces  finding using structure 
able solve pomdps order magnitude larger solved conventional
value iteration techniques  additionally  able solve different kinds pomdps 
simple highly structured synthetic problem robot navigation problem
problem factored belief space relatively complicated probability distributions 
algorithm used find structure related principal components analysis
loss function specifically chosen representing probability distributions  real
world pomdps able solve characterized sparse distributions 
exponential family pca algorithm particularly effective compressing data 
exist pomdp problems structure 
dimensionality reduction technique work well  however  question
investigation other  related dimensionality reduction techniques  e g   isomap locallylinear embedding  tenenbaum et al         roweis  saul    hinton        applied 
number interesting possibilities extending algorithm order
improve efficiency increase domain applicability  loss function
chose dimensionality reduction based reconstruction error 
l b  u  b    e u b  b u b 

    

 cf  equation     minimizing reconstruction error allow near optimal policies
learned  however  would ideally find compact representation
minimizes control errors  could possibly better approximated taking advantage
transition probability structure  example  dimensionality reduction minimizes
prediction errors would correspond loss function 
l b  u  b      e u b  b u b   kb     n b     n  k 

    

b     n  l n   matrix first n   column vectors b  b     n
l n   matrix n   column vectors v starting second vector 
effect finding representation allows bt   predicted bt  
caveat b must arranged action  plan address issue
future work 
another shortcoming approach described work contains
assumption beliefs described using low dimensional representation 
however  relatively easy construct example problem generates beliefs
lie two distinct low dimensional surfaces  current formulation would make
apparent dimensionality beliefs appear much higher set beliefs sampled
one surface alone 
work largely motivated finding better representations beliefs 
approach solving large pomdps  policy search methods  meuleau 
  

fifinding approximate pomdp solutions belief compression

peshkin  kim    kaelbling        hierarchical methods  pineau  gordon    thrun 
    b  able solve large pomdps  interesting note controllers
based e pca representations often essentially independent policy complexity
strongly dependent belief complexity  whereas policy search hierarchical
methods strongly dependent policy complexity largely independent belief
space complexity  seems likely progress solving large pomdps general lie
combination approaches 
e pca algorithm finds low dimensional representation b full belief space b
sampled data  demonstrated reliance sampled data obstacle
real world problems  furthermore  using sampled beliefs could asset
large problems generating tracking beliefs considerably easier
planning  may however preferable try compute low dimensional representation
directly model parameters  poupart boutilier        use notion krylov
subspace this  subspace computed algorithm may correspond exactly
conventional pca seen instances pca poor job finding
low dimensional representations  likely explanation real world beliefs
lie low dimensional planes problems  instead curved surfaces 
extremely useful algorithm would one finds subset belief space closed
transition observation function  constrained find planes 

acknowledgements
thanks tom mitchell  leslie kaelbling  reid simmons  drew bagnell  aaron courville 
mike montemerlo joelle pineau useful comments insight work  nicholas
roy funded national science foundation itr grant   iis          geoffrey gordon funded afrl contract f       c      darpas mica program 
afrl contract f              darpas coabs program 

references
bagnell  j  a     schneider  j          autonomous helicopter control using reinforcement
learning policy search methods  proceedings ieee international conference
robotics automation  icra   pp            seoul  south korea  ieee press 
bishop  c   svensen  m     williams  c          gtm  generative topographic mapping 
neural computation                 
boutilier  c     poole  d          computing optimal policies partially observable
markov decision processes using compact representations  proceedings   th
national conference artificial intelligence  aaai      pp           
boyen  x     koller  d          tractable inference complex stochastic processes 
proceedings   th annual conference uncertainty ai  uai   pp       
madison  wisconsin 
brafman  r  i          heuristic variable grid solution method pomdps  kuipers 
b  k     webber  b   eds    proceedings   th national conference artificial
intelligence  aaai   pp          providence  ri 
  

firoy  gordon    thrun

cassandra  a  r   kaelbling  l     kurien  j  a          acting uncertainty  discrete bayesian models mobile robot navigation  proceedings ieee rsj
international conference intelligent robots systems 
chen  b  m          robust h  control  springer verlag 
cheng  h  t          algorithms partially observable markov decision processes  ph d 
thesis  university british columbia  vancouver  canada 
collins  m   dasgupta  s     schapire  r          generalization principal components
analysis exponential family  dietterich  t  g   becker  s     ghahramani  z 
 eds    advances neural information processing systems     nips   cambridge 
ma  mit press 
cox  t     cox  m          multidimensional scaling  chapman   hall  london 
fox  d   burgard  w     thrun  s          markov localization mobile robots dynamic
environments  journal artificial intelligence research             
galassi  m   davies  j   theiler  j   gough  b   jungman  g   booth  m     rossi 
f         
gnu scientific library reference manual   rd edition edition  
http   www gnu org software gsl  
golub  g     reinsch  c          singular value decomposition least squares solutions 
numerische mathematik  pp         
gordon  g          stable function approximation dynamic programming  prieditis 
a     russell  s   eds    proceedings    international conference machine
learning  icml   pp          san francisco  ca  morgan kaufmann 
gordon  g          generalized  linear  models  becker  s   thrun  s     obermayer  k 
 eds    advances neural information processing systems     nips   mit press 
gutmann  j  s   burgard  w   fox  d     konolige  k          experimental comparison
localization methods  proceedings ieee rsj international conference
intelligent robots systems  victoria  canada 
gutmann  j  s     fox  d          experimental comparison localization methods
continued  proceedings ieee rsj international conference intelligent
robots systems  lausanne  switzerland 
hansen  e     feng  z          dynamic programming pomdps using factored state
representation  proceedings fifth international conference artificial
intelligence planning scheduling  aips      breckenridge  co 
hansen  e          solving pomdps searching policy space  proceedings
  th conference uncertainty artifical intelligence  uai   pp          madison 
wi 
hauskrecht  m          value function approximations partially observable markov
decision processes  journal artificial intelligence research           
hinton  g     roweis  s          stochastic neighbor embedding  becker  s   thrun 
s     obermayer  k   eds    advances neural information processing systems   
 nips   mit press 
  

fifinding approximate pomdp solutions belief compression

howard  r  a          dynamic programming markov processes  mit 
isard  m     blake  a          condensation conditional density propagation
visual tracking  international journal computer vision              
joliffe  i  t          principal component analysis  springer verlag 
kanazawa  k   koller  d     russell  s          stochastic simulation algorithms dynamic
probabilistic networks  proceedings   th annual conference uncertainty
ai  uai   pp          montreal  canada 
kohonen  t          self organized formation topologically correct feature maps  biological cybernetics           
lee  d  d     seung  h  s          learning parts objects non negative matrix
factorization  nature              
leonard  j     durrant whyte  h          mobile robot localization tracking geometric
beacons  ieee transactions robotics automation                
lovejoy  w  s          computationally feasible bounds partially observable markov
decison processes  operations research             
mardia  k  v     jupp  p  e          directional statistics   nd edition   wiley  chichester 
ny 
mccullagh  p     nelder  j  a          generalized linear models   nd edition   chapman
hall  london 
meuleau  n   peshkin  l   kim  k  e     kaelbling  l  p          learning finite state controllers partially observable environments  laskey  k  b     prade  h   eds   
proceedings fifteenth international conference uncertainty artificial intelligence  pp          stockholm  sweden  morgan kaufmann 
munos  r     moore  a          variable resolution discretization high accuracy solutions optimal control problems  dean  t   ed    proceedings   th international joint conference artificial intelligence  ijcai   pp            stockholm
sweden  morgan kaufmann 
munos  r     moore  a          variable resolution discretization optimal control  machine learning                   
nourbakhsh  i   powers  r     birchfield  s          dervish office navigating robot 
ai magazine               
olson  c  f          probabilistic self localization mobile robots  ieee transactions
robotics automation               
pineau  j   gordon  g     thrun  s       a   point based value iteration  anytime
algorithm pomdps  proceedings   th international joint conference
artificial intelligence  ijcai        acapulco  mexico 
pineau  j   gordon  g     thrun  s       b   policy contingent abstraction robust robot
control  meek  c     kjlruff  u   eds    proceedings   th annual conference
uncertainty artificial intelligence  uai   acapulco  mexico 
  

firoy  gordon    thrun

poupart  p     boutilier  c          value directed compression pomdps  becker 
s   thrun  s     obermayer  k   eds    advances neural information processing
systems     nips   vancouver  canada  mit press 
rockafellar  r  t          convex analysis  princeton university press  new jersey 
roweis  s     saul  l          nonlinear dimensionality reduction locally linear embedding   science                       
roweis  s  t   saul  l  k     hinton  g  e          global coordination local linear
models  dietterich  t  g   becker  s     ghahramani  z   eds    advances
neural information processing systems  vol      cambridge  ma  mit press 
roy  n     thrun  s          coastal navigation mobile robots  solla  s  a   todd
k  leen    muller  k  r   eds    advances neural processing systems     nips  
pp            denver  co  mit press 
russell  s     norvig  p          artificial intelligence  modern approach  prentice hall 
shatkay  h     kaelbling  l  p          learning geometrically constrained hidden markov
models robot navigation  bridging geometrical topological gap  journal ai
research 
tenenbaum  j  b   de silva  v     langford  j  c          global geometric framework
nonlinear dimensionality reduction  science                       
thrun  s   fox  d   burgard  w     dellaert  f          robust monte carlo localization
mobile robots  artificial intelligence                   
washington  r          bi pomdp  bounded  incremental partially observable markovmodel planning  proceedings  th european conference planning  ecp  
zhang  n  l     zhang  w          speeding convergence value iteration partially observable markov decision processes  journal artificial intelligence research 
        
zhou  r     hansen  e          improved grid based approximation algorithm
pomdps  nebel  b   ed    proceedings   th international joint conference artificial intelligence  ijcai   pp          seattle  washington  morgan
kaufmann 

  


