journal artificial intelligence research                  

submitted        published      

hybrid bdi pomdp framework multiagent teaming
ranjit nair

ranjit nair honeywell com

automation control solutions
honeywell laboratories  minneapolis  mn      

milind tambe

tambe usc edu

department computer science
university southern california  los angeles  ca      

abstract
many current large scale multiagent team implementations characterized
following belief desire intention  bdi  paradigm  explicit representation team
plans  despite promise  current bdi team approaches lack tools quantitative
performance analysis uncertainty  distributed partially observable markov decision
problems  pomdps  well suited analysis  complexity finding optimal
policies models highly intractable  key contribution article
hybrid bdi pomdp approach  bdi team plans exploited improve pomdp
tractability pomdp analysis improves bdi team plan performance 
concretely  focus role allocation  fundamental problem bdi teams 
agents allocate different roles team  article provides three key contributions  first  describe role allocation technique takes account future
uncertainties domain  prior work multiagent role allocation failed address
uncertainties  end  introduce rmtdp  role based markov team decision problem   new distributed pomdp model analysis role allocations 
technique gains tractability significantly curtailing rmtdp policy search  particular  bdi team plans provide incomplete rmtdp policies  rmtdp policy search
fills gaps incomplete policies searching best role allocation 
second key contribution novel decomposition technique improve rmtdp
policy search efficiency  even though limited searching role allocations  still
combinatorially many role allocations  evaluating rmtdp identify best
extremely difficult  decomposition technique exploits structure bdi team
plans significantly prune search space role allocations  third key contribution
significantly faster policy evaluation algorithm suited bdi pomdp hybrid approach  finally  present experimental results two domains  mission rehearsal
simulation robocuprescue disaster rescue simulation 

   introduction
teamwork  whether among software agents  robots  and people  critical capability
large number multiagent domains ranging mission rehearsal simulations 
robocup soccer disaster rescue  personal assistant teams  already large number multiagent teams developed range domains  pynadath   tambe 
      yen  yin  ioerger  miller  xu    volz        stone   veloso        jennings       
grosz  hunsberger    kraus        decker   lesser        tambe  pynadath    chauvat 
      da silva   demazeau         existing practical approaches characterized situated within general belief desire intention  bdi  approach  paradigm
c
    
ai access foundation  rights reserved 

finair   tambe

designing multiagent systems  made increasingly popular due programming frameworks  tambe et al         decker   lesser        tidhar      b  facilitate design
large scale teams  within approach  inspired explicitly implicitly bdi logics 
agents explicitly represent reason team goals plans  wooldridge        
article focuses analysis bdi teams  provide feedback aid human
developers possibly agents participating team  team performance
complex dynamic domains improved  particular  focuses critical
challenge role allocation building teams  tidhar  rao    sonenberg        hunsberger
  grosz         i e  agents allocate various roles team  instance 
mission rehearsal simulations  tambe et al          need select numbers
types helicopter agents allocate different roles team  similarly  disaster
rescue  kitano  tadokoro  noda  matsubara  takahashi  shinjoh    shimada         role
allocation refers allocating fire engines ambulances fires greatly impact
team performance  domains  performance team
linked important metrics loss human life property thus critical
analyze team performance suggest improvements 
bdi frameworks facilitate human design large scale teams  key difficulty
analyzing role allocation teams due uncertainty arises complex
domains  example  actions may fail world state may partially observable
agents owing physical properties environment imperfect sensing  role
allocation demands future uncertainties taken account  e g  fact
agent may fail execution may may replaced another must taken
account determining role allocation  yet current role allocation algorithms address uncertainty  see section       indeed  uncertainty requires
quantitative comparison different role allocations  however  tools quantitative
evaluations bdi teams currently absent  thus  given uncertainties  may
required experimentally recreate large number possible scenarios  in real domain
simulations  evaluate compare different role allocations 
fortunately  emergence distributed partially observable markov decision problems  pomdps  provides models  bernstein  zilberstein    immerman        boutilier 
      pynadath   tambe        xuan  lesser    zilberstein        used
quantitative analysis agent teams uncertain domains  distributed pomdps represent class formal models powerful enough express uncertainty
dynamic domains arising result non determinism partial observability
principle  used generate evaluate complete policies multiagent team 
however  two shortcomings models prevents application
analysis role allocation  first  previous work analysis focused communication  pynadath   tambe        xuan et al          rather role allocation
coordination decisions  second  shown bernstein et al          problem
deriving optimal policy generally computationally intractable  the corresponding
decision problem nexp complete   thus  applying optimal policies analysis highly
intractable 
address first difficulty  derive rmtdp  role based multiagent team decision
problem   distributed pomdp framework quantitatively analyzing role allocations 
using framework  show that  general  problem finding optimal role
   

fihybrid bdi pomdp framework multiagent teaming

completed policy  
additions bdi team plan

bdi team plan
rmtdp
search policy space

incomplete policy
bdi interpreter

domain
rmtdp model

figure    integration bdi pomdp 

allocation policy computationally intractable  the corresponding decision problem still
nexp complete   shows improving tractability analysis techniques role
allocation critically important issue 
therefore  order make quantitative analysis multiagent teams using rmtdp
tractable  second contribution provides hybrid bdi pomdp approach
combines native strengths bdi pomdp approaches  i e   ability bdi
frameworks encode large scale team plans pomdp ability quantitatively
evaluate plans  hybrid approach based three key interactions improve
tractability rmtdp optimality bdi agent teams  first interaction
shown figure    particular  suppose wish analyze bdi agent team  each agent
consisting bdi team plan domain independent interpreter helps coordinate
plans  acting domain  shown figure    model domain via
rmtdp  rely bdi team plan interpreter providing incomplete policy
rmtdp  rmtdp model evaluates different completions incomplete
policy provides optimally completed policy feedback bdi system  thus 
rmtdp fills gaps incompletely specified bdi team plan optimally 
gaps concentrate role allocations  method applied
key coordination decisions  restricting optimization role allocation decisions
fixing policy points  able come restricted policy
space  use rmtdps effectively search restricted space order find
optimal role allocation 
restricted policy search one key positive interaction hybrid approach 
second interaction consists efficient policy representation used converting
bdi team plan interpreter corresponding policy  see figure    new
algorithm policy evaluation  general  agents policy distributed pomdp
indexed observation history  bernstein et al         pynadath   tambe        
   

finair   tambe

however  bdi system  agent performs action selection based set
privately held beliefs obtained agents observations applying belief
revision function  order evaluate teams performance  sufficient rmtdp
index agents policies belief state  represented privately held beliefs 
instead observation histories  shift representation results considerable
savings amount time needed evaluate policy space required
represent policy 
third key interaction hybrid approach exploits bdi team plan structure increasing efficiency rmtdp based analysis  even though rmtdp
policy space restricted filling gaps incomplete policies  many policies may result
given large number possible role allocations  thus enumerating evaluating
possible policy given domain difficult  instead  provide branch and bound algorithm exploits task decomposition among sub teams team significantly prune
search space provide correctness proof worst case analysis algorithm 
order empirically validate approach  applied rmtdp allocation
bdi teams two concrete domains  mission rehearsal simulations  tambe et al        
robocuprescue  kitano et al          first present  significant  speed up
gained three interactions mentioned above  next  domains  compared
role allocations found approach state of the art techniques allocate
roles without uncertainty reasoning  comparison shows importance reasoning
uncertainty determining role allocation complex multiagent domains 
robocuprescue domain  compared allocations found allocations chosen
humans actual robocuprescue simulation environment  results showed
role allocation technique presented article capable performing human
expert levels robocuprescue domain 
article organized follows  section   presents background motivation 
section    introduce rmtdp model present key complexity results  section
  explains bdi team plan evaluated using rmtdp  section   describes
analysis methodology finding optimal role allocation  section   presents
empirical evaluation methodology  section    present related work
section    list conclusions 

   background
section first describes two domains consider article  abstract
mission rehearsal domain  tambe et al         robocuprescue domain  kitano
et al          domain requires us allocate roles agents team  next  teamoriented programming  top   framework describing team plans described
context two domains  focus top  discussed section     
techniques would applicable frameworks tasking teams  stone   veloso 
      decker   lesser        
    domains
first domain consider based mission rehearsal simulations  tambe et al  
       expository purposes  intentionally simplified  scenario
   

fihybrid bdi pomdp framework multiagent teaming

follows  helicopter team executing mission transporting valuable cargo point
x point enemy terrain  see figure     three paths x
different lengths different risk due enemy fire  one scouting sub teams must
sent  one path x y   larger size scouting sub team
safer is  scouts clear one path x y  transports
move safely along path  however  scouts may fail along path  may
need replaced transport cost transporting cargo  owing partial
observability  transports may receive observation scout failed
route cleared  wish transport amount cargo quickest
possible manner within mission deadline 
key role allocation decision given fixed number helicopters 
allocated scouting transport roles  allocating scouts means
scouting task likely succeed  fewer helicopters left
used transport cargo consequently less reward  however  allocating
scouts could result mission failing altogether  also  allocating scouts 
routes scouts sent on  shortest route would preferable
risky  sending scouts route decreases likelihood failure
individual scout  however  might beneficial send different routes  e g 
scouts risky short route others safe longer route 
thus many role allocations consider  evaluating difficult
role allocation must look ahead consider future implications uncertainty  e g  scout
helicopters fail scouting may need replaced transport  furthermore  failure success scout may visible transport helicopters hence
transport may replace scout transports may never fly destination 
scout
transports

x

route  

route  



enemy gun
route  

figure    mission rehearsal domain 

second example scenario  see figure     set robocuprescue disaster
simulation environment  kitano et al          consists five fire engines three different
fire stations  two stations       last station    five ambulances
stationed ambulance center  two fires  in top left bottom right corners
map  start need extinguished fire engines  fire extinguished 
ambulance agents need save surviving civilians  number civilians
   

finair   tambe

location known ahead time  although total number civilians known 
time passes  high likelihood health civilians deteriorate fires
increase intensity  yet agents need rescue many civilians possible
minimal damage buildings  first part goal scenario therefore
first determine fire engines assign fire  fire engines gathered
information number civilians fire  transmitted ambulances 
next part goal allocate ambulances particular fire rescue
civilians trapped there  however  ambulances cannot rescue civilians fires fully
extinguished  here  partial observability  each agent view objects within visual
range   uncertainty related fire intensity  well location civilians
health add significantly difficulty 

c 
f 
f 
f 
c 


figure    robocuprescue scenario  c  c  denote two fire locations  f   f 
f  denote fire stations        respectively denotes ambulance
center 

    team oriented programming
aim team oriented programming  top   pynadath   tambe        tambe et al  
      tidhar      b  framework provide human developers  or automated symbolic
planners  useful abstraction tasking teams  domains described
section      consists three key aspects team   i  team organization hierarchy
consisting roles   ii  team  reactive  plan hierarchy   iii  assignment roles
sub plans plan hierarchy  developer need specify low level coordination
details  instead  top interpreter  the underlying coordination infrastructure  automatically enables agents decide communicate reallocate
   

fihybrid bdi pomdp framework multiagent teaming

roles upon failure  top abstraction enables humans rapidly provide team plans
large scale teams  unfortunately  qualitative assessment team performance
feasible  thus  key top weakness inability quantitatively evaluate optimize
team performance  e g   allocating roles agents qualitative matching capabilities may feasible  discussed later  hybrid bdi pomdp model addresses
weakness providing techniques quantitative evaluation 
concrete example  consider top mission rehearsal domain  first
specify team organization hierarchy  see figure   a    task force highest level
team organization consists two roles scouting transport 
scouting sub team roles three scouting sub sub teams  next specify
hierarchy reactive team plans  figure   b    reactive team plans explicitly express
joint activities relevant team consist of   i  pre conditions plan
proposed   ii  termination conditions plan ended   iii 
team level actions executed part plan  an example plan discussed
shortly   figure   b   highest level plan execute mission three sub plans 
doscouting make one path x safe transports  dotransport
move transports along scouted path  remainingscouts scouts
reached destination yet get there 
execute mission  task force 
doscouting
 task force 

remainingscouts
dotransport
 scouting team   transport team 

task force
scoutroutes
waitatbase
 transport team   scouting team 

scouting team

transport team

sctteama sctteamb sctteamc

scoutroute  scoutroute  scoutroute 
 sctteama   sctteamb   sctteamc 

 a 

 b 

figure    top mission rehearsal domain a  organization hierarchy  b  plan hierarchy 

figure   b  shows coordination relationships  relationship indicated
solid arc  relationship indicated dashed arc  thus  waitatbase scoutroutes must done least one scoutroute  
scoutroute  scoutroute  need performed  temporal dependence relationship among sub plans  implies sub teams assigned perform
dotransport remainingscouts cannot doscouting plan completed  however  dotransport remainingscouts execute parallel  finally 
assign roles plans figure   b  shows assignment brackets adjacent plans 
instance  task force team assigned jointly perform execute mission sctteama assigned scoutroute  
team plan corresponding execute mission shown figure   
seen  team plan consists context  pre conditions  post conditions  body constraints  context describes conditions must fulfilled parent plan
pre conditions particular conditions cause sub plan begin exe   

finair   tambe

cution  thus  execute mission  pre condition team mutually believes
 mb   start location  post conditions divided achieved 
unachievable irrelevant conditions sub plan terminated 
body consists sub plans exist within team plan  lastly  constraints describe
temporal constraints exist sub plans body  description
plans plan hierarchy figure   b  given appendix a 
executemission 
context 
pre conditions   mb  taskforce  location taskforce    start 
achieved   mb  taskforce   achieved doscouting  achieved dotransport     time
   mb  taskforce 
achieved remainingscouts    helo scoutingteam  alive helo 
location helo     end   
unachievable   mb  taskforce  unachievable doscouting    mb  taskforce 
unachievable dotransport 
 achieved remainingscouts    helo scoutingteam  alive helo 
location helo     end   
irrelevant 
body 
doscouting
dotransport
remainingscouts
constraints  doscouting dotransport  doscouting remainingscouts

figure    example team plan  mb refers mutual belief 
htn  dix  muoz avila  nau    zhang        erol  hendler    nau        
plan hierarchy top gives decomposition task smaller tasks  however 
language tops richer language early htn planning  erol et al        
contained simple ordering constraints  seen example  plan hierarchy
tops contain relationships or  addition  recent
work htn planning  dix et al          sub plans tops contain pre conditions
post conditions  thus allowing conditional plan execution  main differences
tops htn planning are   i  tops contain organization hierarchy addition
plan hierarchy   ii  top interpreter ensures team executes plans coherently 
seen later  tops analyzed expressiveness including conditional
execution  however  since analysis focus fixed time horizon  loops
task description unrolled time horizon 
   mutual belief  wooldridge         shown  mb hteami x  figure    refers private belief held
agent team believe fact x true  agents
team believe x true  every agent believes every agent believes x
true on  infinite levels nesting difficult realize practice  thus  practical
bdi implementations  purposes article  mutual belief approximated private
belief held agent agents team believe x true 

   

fihybrid bdi pomdp framework multiagent teaming

new observation
agent

belief update
function

private beliefs
agent

figure    mapping observations beliefs 

execution  agent copy top  agent maintains set
private beliefs  set propositions agent believes true  see
figure     agent receives new beliefs  i e  observations  including communication  
belief update function used update set privately held beliefs  instance 
upon seeing last scout crashed  transport may update privately held beliefs
include belief criticalfailure doscouting   practical bdi systems  belief
update computation low complexity  e g  constant linear time   beliefs
updated  agent selects plan execute matching beliefs preconditions plans  basic execution cycle similar standard reactive planning
systems prs  georgeff   lansky        
team plan execution  observations form communications often arise
coordination actions executed top interpreter  instance  top
interpreters exploited bdi theories teamwork  levesque et al s theory
joint intentions  levesque  cohen    nunes        require agent
comes privately believe fact terminates current team plan  i e  matches
achievement unachievability conditions team plan   communicates fact
rest team  performing coordination actions automatically  top
interpreter enables coherence initiation termination team plans within top 
details examples tops seen work pynadath
tambe         tambe et al         tidhar      b  
concretely illustrate key challenges role allocation mentioned
earlier  first  human developer must allocate available agents organization hierarchy  figure   a    find best role allocation  however  combinatorially many
allocations choose  hunsberger   grosz        tambe et al          instance 
starting   homogeneous helicopters results    different ways deciding
many agents assign scouting transport sub team  problem exacerbated fact best allocation varies significantly based domain variations 
example  figure   shows three different assignments agents team organization hierarchy  found analysis best given setting failure
observation probabilities  details section     example  increasing probability
failures routes resulted number transports best allocation changing
four  see figure   b   three  see figure   a    additional scout added
sctteamb  failures possible all  number transports increased
five  see figure   c    analysis takes step towards selecting best among
allocations 
   

finair   tambe

task force
scouting team

task force

transport team  

scouting team

sctteama   sctteamb   sctteamc  

transport team  

sctteama   sctteamb   sctteamc  

 a  medium probability

 b  low probability
task force

scouting team

transport team  

sctteama   sctteamb   sctteamc  

 c  zero probability

figure    best role allocations different probabilities scout failure 

figure   shows top robocuprescue scenario  seen  plan hierarchy scenario consists pair extinguishfire rescuecivilians plans
done parallel  decompose individual plans   these individual plans get fire engines ambulances move streets using specific
search algorithms  however  individual plans relevant discussions
article  interested readers refer description robocuprescue team
entered robocup competitions       nair  ito  tambe    marsella         
organizational hierarchy consists task force comprising two engine sub teams  one
fire ambulance team  engine teams assigned extinguishing
fires ambulance team assigned rescuing civilians  particular top 
assignment ambulances ambulanceteama ambulanceteamb conditioned
communication c  indicated ambulanceteama c ambulanceteamb c 
c described detail figure  refers communication received fire engines describes number civilians present fire 
problem engines assign engine team possible value c 
ambulances assign ambulance team  note engines differing
capabilities owing differing distances fires ambulances identical
capabilities 
task force
engineteama

engineteamb

ambulanceteam

ambulanceteama  c

ambulanceteamb  c

 a 
executemission
 task force 
extinguishfire 
 engineteama 

rescuecivilians 
 ambulanceteama 

extinguishfire 
 engineteamb 

rescuecivilians 
 ambulanceteamb 

 b 

figure    top robocuprescue scenario a  organization hierarchy  b  plan hierarchy 

   

fihybrid bdi pomdp framework multiagent teaming

   role based multiagent team decision problem
multiagent team decision problem  mtdp   pynadath   tambe        inspired
economic theory teams  marschak   radner        ho        yoshikawa        
order quantitative analysis key coordination decisions multiagent teams 
extend mtdp analysis coordination actions interest  example 
com mtdp  pynadath   tambe        extension mtdp analysis communication  article  illustrate general methodology analysis aspects
coordination present rmtdp model quantitative analysis role allocation
reallocation concrete example  contrast bdi systems introduced previous section  rmtdp enables explicit quantitative optimization team performance  note
that  use mtdp  possible distributed pomdp models could potentially
serve basis  bernstein et al         xuan et al         
    multiagent team decision problem
given team n agents  mtdp  pynadath   tambe        defined tuple 
hs  a  p    o  ri  consists finite set states     j  
  j m  feature world state  agent perform action
set actions ai    in ai   a  p  s    a                 gives probability
transitioning state state given agents perform actions   a             
jointly  agent receives observation   in     based function
o s    a                           n    gives probability agents receive
observations              n given world state perform   a             
jointly  agents receive single joint reward r s    a               based state
joint action   a               joint reward shared equally members
private reward individual agents receive actions  thus 
agents motivated behave team  taking actions jointly yield
maximum expected reward 
agent mtdp chooses actions based local policy   
mapping observation history actions  thus  time t  agent perform action
 i               contrasts single agent pomdp  index agents
policy belief state probability distribution world state  kaelbling  littman 
  cassandra         shown sufficient statistic order compute
optimal policy  sondik         unfortunately  cannot directly use single agent pomdp
techniques  kaelbling et al         maintaining updating belief states  kaelbling et al  
      mtdp unlike single agent pomdp  mtdp  agents observation
depends actions  unknown actions agents  thus 
distributed pomdp models  bernstein et al         xuan et al         
mtdp  local policies indexed observation histories                 n   refers
joint policy team agents 
    extension explicit coordination
beginning mtdp  next step methodology make explicit separation
domain level actions coordination actions interest  earlier work intro   

finair   tambe

duced com mtdp model  pynadath   tambe         coordination action
fixed communication action  got separated out  however  coordination actions could separated domain level actions order investigate
impact  thus  investigate role allocation reallocations  actions allocating agents
roles reallocate roles separated out  end  define rmtdp
 role based multiagent team decision problem  tuple hs  a  p    o  r  rli
new component  rl  particular  rl    r            rs   set roles agents
undertake  instance role rj may assigned agent fulfill it 
actions agent distinguishable two types 
role taking actions     irj   contains role taking actions agent i  irj
means agent takes role rj rl 

role execution actions    rj rl irj contains execution actions agent
irj set agent actions executing role rj rl
addition define set states     roles  feature roles  a vector  gives current role agent taken on  reason
introducing new feature assist us mapping bdi team plan
rmtdp  thus time agent performs new role taking action successfully  value
feature roles updated reflect change  key
model agents initial role taking action subsequent role reallocation  modeling
allocation reallocation important accurate analysis bdi teams  note
agent observe part feature pertaining current role
may observe parts pertaining agents roles 
introduction roles allows us represent specialized behaviors associated
role  e g  transport vs  scout role  filling particular role  rj   agent
perform role execution actions  irj   may different roleexecution actions irl role rl   thus  feature roles used filter actions
role execution actions correspond agents current role permitted 
worst case  filtering affect computational complexity  see theorem  
below  practice  significantly improve performance trying find
optimal policy team  since number domain actions agent choose
restricted role agent taken on  also  different roles
produce varied effects world state  modeled via transition probabilities  p  
teams reward  thus  policies must ensure agents role capabilities
benefit team most 
mtdp  agent chooses action perform indexing local policy
observation history  epoch agents could role taking
actions others role execution actions  thus  agents local policy
divided local role taking role execution policies observation
histories  i              either  i                null  i                null    
            n   refers joint role taking policy team agents   
            n   refers joint role execution policy 
   

fihybrid bdi pomdp framework multiagent teaming

article explicitly model communicative actions special action 
thus communication treated role execution action communication
received agents treated observations  
    complexity results rmtdp
section     qualitatively emphasized difficulty role allocation  rmtdp helps
us understanding complexity precisely  goal rmtdp come
joint policies maximize total expected reward finite horizon
  note agents change roles according local role taking policies 
agents role execution policy subsequent change would contain actions pertaining
new role  following theorem illustrates complexity finding optimal joint
policies 
theorem   decision problem determining exist policies   
rmtdp  yield expected reward least k finite horizon nexpcomplete 
proof sketch  proof follows reduction mtdp  pynadath   tambe       
to from rmtdp  reduce mtdp rmtdp  set rmtdps role taking actions   
null set rmtdps role execution actions    mtdps set actions  a 
reduce rmtdp
mtdp  generate new mtdp set actions 

equal   finding required policy mtdp nexp complete  pynadath  
tambe        
theorem shows us  solving rmtdp optimal joint role taking roleexecution policies even finite horizon highly intractable  hence  focus
complexity determining optimal role taking policy  given fixed role execution
policy  fixed role execution policy  mean action selection agent
predetermined role executing 
theorem   decision problem determining exists role taking policy   
rmtdp  yields expected reward least k together fixed role execution
policy   finite horizon nexp complete 
proof sketch  reduce mtdp rmtdp different role taking
role execution action corresponding action mtdp  hence  rmtdp
role taking action irj agent take role rj created action aj ai
mtdp role rj contains single role execution action  i e   irj       
rmtdp  construct transition function role taking action always
succeeds affected state feature roles   role execution action irj  
transition probability mtdp action  aj ai corresponding
last role taking action irj   fixed role execution policy simply perform
action  irj   corresponding last successful role taking action  irj   thus 
decision problem rmtdp fixed role execution policy least hard
   explicit analysis communication please refer work done pynadath tambe       
goldman et al         

   

finair   tambe

decision problem mtdp  furthermore  given theorem    conclude
nexp completeness 
result suggests even fixing role execution policy  solving rmtdp
optimal role taking policy still intractable  note theorem   refers completely
general globally optimal role taking policy  number agents change roles
point time  given result  general globally optimal role taking policy
likely doubly exponential complexity  may left choice run
brute force policy search  i e  enumerate role taking policies evaluate
them  together determinethe run time finding globally optimal policy 
number policies

  

  t  
   

n

  i e  doubly exponential number observation

histories number agents  thus  rmtdp enables quantitative evaluation
teams policies  computing optimal policies intractable  furthermore  given low level
abstraction  contrast top  difficult human understand optimal policy 
contrast rmtdp top root hybrid model described
following section 

   hybrid bdi pomdp approach
explained top rmtdp  present detailed view
hybrid methodology quantitatively evaluate top  first provide detailed
interpretation figure    bdi team plans essentially top plans  bdi
interpreter top coordination layer  shown figure    rmtdp model
constructed corresponding domain top interpreter converted
corresponding  incomplete  rmtdp policy  analyze top using
analysis techniques rely evaluating rmtdp policy using rmtdp model
domain 
thus  hybrid approach combines strengths tops  enabling humans
specify tops coordinate large scale teams  strengths rmtdp  enabling
quantitative evaluation different role allocations   one hand  synergistic
interaction enables rmtdps improve performance top based bdi teams 
hand  identified least six specific ways tops make easier
build rmtdps efficiently search rmtdp policies  two discussed
section  four next section  particular  six ways are 
   tops exploited constructing rmtdp models domain  section      
   tops exploited present incomplete policies rmtdps  restricting rmtdp
policy search  section      
   top belief representation exploited enabling faster rmtdp policy evaluation
 section      
   top organization hierarchy exploited hierarchically grouping rmtdp policies
 section      
   top plan hierarchy exploited decomposing rmtdps  section      
   

fihybrid bdi pomdp framework multiagent teaming

   top plan hierarchies exploited cutting observation belief
histories rmtdps  section      
end result efficient policy search completed rmtdp policy improves
top performance  exploit top framework  frameworks tasking
teams  e g  decker lesser        stone veloso        could benefit
similar synergistic interaction 
    guidelines constructing rmtdp
shown figure    analysis approach uses input rmtdp model domain 
well incomplete rmtdp policy  fortunately  top serve
direct mapping rmtdp policy  utilized actually constructing
rmtdp model domain  particular  top used determine
domain features important model  addition  structure top
exploited decomposing construction rmtdp 
elements rmtdp tuple  hs  a  p    o  r  rli  defined using procedure relies top well underlying domain  procedure
automated  key contribution recognizing exploitation top structures
constructing rmtdp model  first  order determine set states  s 
critical model variables tested pre conditions  termination conditions
context components  i e  sub plans  top  note state needs
model features tested top  top pre condition expresses complex test
feature  test modeled state  instead gets used defining
incomplete policy input rmtdp  next define set roles  rl  leaf level
roles organization hierarchy top  furthermore  specified section     
define state feature roles vector containing current role agent 
defined rl roles   define actions  follows  role rj rl 
define corresponding role taking action  irj succeed fail depending
agent performs action state action performed in 
role execution actions  irj agent role rj   allowed role according
top 
thus  defined s  rl based top  illustrate steps  consider
plans figure   b   pre conditions leaf level plan scoutroute   see
appendix a   instance  tests start location helicopters start location x 
termination conditions test scouts end location y  thus  locations
helicopters modeled features set states rmtdp  using
organization hierarchy  define set roles rl role corresponding
four different kinds leaf level roles  i e  rl    membersctt eama  membersctt eamb 
membersctt eamc  membert ransportt eam   role taking role execution actions
defined follows 
role taking action defined corresponding four roles rl  i e 
becoming member one three scouting teams transport team 
domain specifies transport change scout thus role taking
action  jointtransportteam  fail agent i  current role agent scout 
   

finair   tambe

role execution actions obtained top plans corresponding agents
role  mission rehearsal scenario  agent  fulfilling scout role  members
sctteama  sctteamb sctteamc   always goes forward  making current
position safe  reaches destination execution action
consider move making safe  agent transport role  members transport
team  waits x obtains observation signal one scouting sub team
reached hence role execution actions wait move forward 
must define   p  o  r  obtain set observations agent
directly domain  instance  transport helos may observe status scout
helos  normal destroyed   well signal path safe  finally  determining
functions  p  o  r requires combination human domain expertise empirical
data domain behavior  however  shown later section    even approximate
model transitional observational uncertainty sufficient deliver significant benefits  defining reward transition function may sometimes require additional state
variables modeled  implicitly modeled top  mission
rehearsal domain  time scouting transport mission completed
determined amount reward  thus  time implicitly modeled top
needed explicitly modeled rmtdp 
since interested analyzing particular top respect uncertainty 
procedure constructing rmtdp model simplified exploiting hierarchical decomposition top order decompose construction rmtdp
model  high level components top often represent plans executed different
sub teams  may loosely interact other  within component 
sub team members may exhibit tight interaction  focus loose coupling
across components  end results one component feed another 
components independently contribute team goal  thus  procedure constructing rmtdp exploits loose coupling components plan hierarchy
order build rmtdp model represented combination smaller rmtdps  factors   note decomposition infeasible  approach still applies except
benefits hierarchical decomposition unavailable 
classify sibling components either parallel sequentially executed  contains temporal constraint   components executed parallel could either independent
dependent  independent components  define rmtdps
components sub team executing one component cannot affect transitions  observations reward obtained sub teams executing components  procedure determining elements rmtdp tuple component k 
hsk   ak   pk   k   ok   rk   rlk i  identical procedure described earlier constructing
overall rmtdp  however  component smaller set relevant variables
roles hence specifying elements corresponding rmtdp easier 
combine rmtdps independent components obtain
rmtdp corresponding higher level component  higher level component l 
whose child
components independent  set states  sl   x fsl x
fsl   k s t  child k l  true fsk fsl fsk sets features set
states sl set states sk   state sl sl said correspond state
sk sk x fsk   sl  x     sk  x    i e  state sl value state sk
   

fihybrid bdi pomdp framework multiagent teaming


defined follows  pl  sl   al   sl    
q features state sk   transition function

k s t  child k l  true pk  sk   ak   sk    sl sl component l corresponds states
sk sk component k ak joint action performed sub team assigned component k corresponding joint action al performed sub team
assigned
component l  observation function defined similarly ol  sl   al   l    
q
ok  sk   ak   k    reward function component l defined
k s t  child k l  true
p
rl  sl   al     k s t  child k l  true rk  sk   ak   

case sequentially executed components  those connected temporal constraint   components loosely coupled since end states preceding component
specify start states succeeding component  thus  since one component
active time  transition function defined follows  pl  sl   al   sl     pk  sk   ak   sk   
component k active child component  sk sk represent states
component k corresponding states sl sl component l ak joint action
performed sub team assigned component k corresponding joint action
al performed sub team corresponding component l  similarly  define
ol  sl   al   l     ok  sk   ak   k   rl  sl   al     rk  sk   ak    k active child
component 

consider following example mission rehearsal domain components
exhibit sequential dependence parallel independence  concretely  component
doscouting executed first followed dotransport remainingscouts 
parallel independent hence  either doscouting active dotransport
remainingscouts active point execution  hence  transition  observation reward functions parent execute mission given corresponding
functions either doscouting combination corresponding functions
dotransport remainingscouts 
use top down approach order determine construct factored rmtdp
plan hierarchy  shown algorithm    replace particular sub plan
constituent sub plans either independent sequentially executed  not 
rmtdp defined using particular sub plan  process applied recursively
starting root component plan hierarchy  concrete example  consider
mission rehearsal simulation domain hierarchy illustrated figure   b  
given temporal constraints doscouting dotransport  doscouting remainingscouts  exploited sequential decomposition  dotransport
remainingscouts parallel independent components  hence  replace
executemission doscouting  dotransport remainingscouts  apply process doscouting  constituent components doscouting
neither independent sequentially executed thus doscouting cannot replaced
constituent components  thus  rmtdp mission rehearsal domain comprised
smaller rmtdps doscouting  dotransport remainingscouts 
thus  using top identify relevant variables building factored rmtdp
utilizing structure top decompose construction procedure  reduce load
domain expert model construction  furthermore  shown section     
factored model greatly improves performance search best role allocation 
   

finair   tambe

algorithm   build rmtdp top top  sub plan subplan 
   children subplanchildren    subplanchildren   returns sub plans within subplan 
   children   null children  loosely coupled independent 
  
rmtdp define rmtdp subplan   not automated 
  
return rmtdp
   else
  
child children
  
factors child  build rmtdp top child 
  
rmtdp constructfromfactors factors 
  
return rmtdp

    exploiting top beliefs evaluation rmtdp policies
present technique exploiting tops speeding evaluation rmtdp
policies  explain improvement  first describe original algorithm
determining expected reward joint policy  local policies agent
indexed entire observation histories  pynadath   tambe        nair  pynadath 
yokoo  tambe    marsella      a   here  obtain rmtdp policy top
follows  obtain   
   i e  action performed agent observation history

 i   action performed agent following top set privately
held beliefs corresponding observation history   it   compute expected reward
rmtdp policy projecting teams execution possible branches
different world states different observations  time step  compute
expected value joint policy                 n    team starting given state  st  
given set past observations 
   t          
  nt   follows 
x











  nt     r st       
vt  st     t          
 t            n   nt      
p    
   t           n
  nt   st  
st  

x









st     
   t           n
  nt    t            nt   vt   st       t            
  nt  

   

t  

expected reward joint policy given v   s      null          null    s 
start state  time step t  computation vt performs summation
possible world states agent observations time complexity   s      
computation
repeated states observation histories length t  i e 

 s    t times  therefore 
given time horizon   overall complexity algo
 

  
rithm  s    
 
discussed section      team oriented program  agents action selection
based currently held private beliefs  note mutual beliefs modeled
privately held beliefs agents per footnote     similar technique
exploited mapping top rmtdp policy  indeed  evaluation rmtdp
policy corresponds top speeded agents local policy indexed
private beliefs    refer   top congruent belief state agent
   

fihybrid bdi pomdp framework multiagent teaming

rmtdp  note belief state probability distribution world
states single agent pomdp  rather privately held beliefs  from bdi
program  agent time t  similar idea representing policy
finite state controller  hansen   zhou        poupart   boutilier         case 
private beliefs would map states finite state controller 
belief based rmtdp policy evaluation leads speedup multiple observation
histories map belief state    speedup key illustration exploitation
synergistic interactions top rmtdp  instance  belief representation techniques used top reflected rmtdp  resulting faster policy evaluation
help us optimize top performance  detailed example belief state presented later
brief explanation belief based rmtdp policies evaluated 
evaluation using observation histories  compute expected reward
belief based policy projecting teams execution possible branches
different world states different observations  time step  compute
expected value joint policy                 n    team starting given state  st  
given team belief state     t           nt   follows 





x





vt  st    t       nt     r st       t            n  nt       p st      t           n nt   st  
st  

x









st       t           n nt    t            nt   vt   st      t             nt  

t  

   


t  

  beliefupdatefunction



  it  



complexity computing function  expression      s      bf   bf
represents complexity belief update function  beliefupdatefunction 
time step computation value function done every state possible
reachable belief states  let  i     max tt   it    represent maximum number
possible belief states agent point time   it   number
belief states agent t  therefore complexity algorithm
given o  s                   n      bf   note that  algorithm
exponent unlike algorithm expression    thus  evaluation method
give large time savings if   i  quantity              n    much less   t
 ii  belief update cost low  practical bdi systems  multiple observation histories
map often onto belief state  thus usually               n    much less
  t   furthermore  since belief update function mirrors practical bdi systems 
complexity low polynomial constant  indeed  experimental results
show significant speedups result switching top congruent belief states
  however  absolute worst case  belief update function may simply append
new observation history past observations  i e   top congruent beliefs
equivalent keeping entire observation histories  thus belief based evaluation
complexity observation history based evaluation 
turn example belief based policy evaluation mission rehearsal
domain  time step  transport helicopters may receive observation
   

finair   tambe

whether scout failed based observation function  use observationhistory representation policy  transport agent would maintain complete
history observations could receive time step  example  setting
two scout helicopters  one route   route    particular transport
helicopter may several different observation histories length two  every time step 
transports may receive observation scout alive failed 
thus  time      transport helicopter might one following observation histories length two     sct onroute alive  sct onroute alive      sct onroute f ailed 
sct onroute f ailed         sct onroute alive  sct onroute f ailed      sct onroute 
f ailed         sct onroute f ailed  sct onroute alive      sct onroute f ailed     
etc  however  action selection transport helicopters depends whether
critical failure  i e  last remaining scout crashed  taken place change
role  whether failure critical determined passing observation
belief update function  exact order observations received
precise times failure non failure observations received relevant
determining critical failure taken place consequently whether transport
change role scout  thus  many observation histories map onto
belief states  example  three observation histories map belief
criticalf ailure doscouting  i e  critical failure taken place  results significant speedups using belief based evaluation  equation   needs executed
smaller number belief states  linear domains  opposed observation
history based evaluation  equation   executed exponential number observation histories    t    actual speedup obtained mission rehearsal domain
demonstrated empirically section   

   optimizing role allocation
section   focused mapping domain interest onto rmtdp algorithms
policy evaluation  section focuses efficient techniques rmtdp policy search 
service improving bdi top team plans  top essence provides incomplete 
fixed policy  policy search optimizes decisions left open incomplete policy 
policy thus completed optimizes original top  see figure     enabling rmtdp
focus search incomplete policies  providing ready made decompositions 
tops assist rmtdps quickly searching policy space  illustrated
section  focus  particular  problem role allocation  hunsberger   grosz 
      modi  shen  tambe    yokoo        tidhar et al         fatima   wooldridge        
critical problem teams  top provides incomplete policy  keeping open
role allocation decision agent  rmtdp policy search provides optimal
role taking action role allocation decision points  contrast previous
role allocation approaches  approach determines best role allocation  taking
consideration uncertainty domain future costs  although demonstrated
solving role allocation problem  methodology general enough apply
coordination decisions 
   

fihybrid bdi pomdp framework multiagent teaming

    hierarchical grouping rmtdp policies
mentioned earlier  address role allocation  top provides policy complete 
except role allocation decisions  rmtdp policy search optimally fills
role allocation decisions  understand rmtdp policy search  useful gain
understanding role allocation search space  first  note role allocation focuses
deciding many types agents allocate different roles organization
hierarchy  role allocation decision may made time     may made
later time conditioned available observations  figure   shows partially expanded role
allocation space defined top organization hierarchy figure   a  six helicopters 
node role allocation space completely specifies allocation agents roles
corresponding level organization hierarchy  ignore now  number
right node   instance  root node role allocation space specifies
six helicopters assigned task force  level one  organization hierarchy
leftmost leaf node  at level three  figure   specifies one helicopter assigned
sctteama  zero sctteamb  zero sctteamc five helicopters transport team 
thus  see  leaf node role allocation space complete  valid role
allocation agents roles organization hierarchy 
order determine one leaf node  role allocation  superior another evaluate
using rmtdp constructing rmtdp policy each  particular
example  role allocation specified leaf node corresponds role taking actions
agent execute time      example  case leftmost leaf
figure    time      one agent  recall section     homogeneous team
hence specific agent matter  become member sctteama
agents become members transport team  thus  one agent i  roletaking policy include  null    joinsctt eama agents  j  j    i 
include j  null    joint ransportt eam  case  assume rest
role taking policy  i e  roles reallocated scout fails  obtained role
reallocation algorithm bdi top interpreter  steam algorithm  tambe
et al          thus example  role reallocation indeed performed steam
algorithm  steams reallocation policy included incomplete policy
rmtdp initially provided  thus  best role allocation computed keeping
mind steams reallocation policy  steam  given failure agent playing rolef  
agent playing roler replace if 
criticality  rolef   criticality  roler      
criticality  x      x critical      otherwise

thus  based agents observations  critical failure taken place 
replacing agents decision replace computed using expression
included incomplete policy input rmtdp  since incomplete
policy completed role allocation leaf node using technique above 
able construct policy rmtdp corresponds role allocation 
domains robocuprescue  allocation decisions made time
     domains  possible role allocation conditioned observations
 or communication  obtained course execution  instance 
shown figure   a   robocuprescue scenario  ambulances allocated
sub team ambulanceteama ambulanceteamb information location
   

finair   tambe

 

 

 

   
 

 

 

      
 

 

 

      
 

 

 

      
 

 

 

      
 

 

 

      
 

 

 

     
 

         
         
         
        
   
   
   
   
     
     
     
     

figure    partially expanded role allocation space mission rehearsal domain six helos  

civilians conveyed fire engines  allocation ambulances
conditioned communication  i e  number civilians location 
figure    shows partially expanded role allocation scaled down rescue scenario
three civilians  two ambulances two fire engines  one station  
station     figure        depicts fact two ambulances 
one fire engine station  shown  level allocation fire engines
engineteama engineteamb gives number engines assigned
engineteam station  next level  leaf level  different leaf nodes
possible assignment ambulances ambulanceteama ambulanceteamb depending
upon value communication c  since three civilians exclude
case civilians present particular fire  two possible messages i e 
one civilian fire   two civilians fire    c        
taskforce      
     

     

engineteama     engineteamb     ambteam  

c  

engineteama     engineteamb     ambteam  

     

     

         

         

c  

c  

ambteama   ambteamb   ambteama   ambteamb  

c  

ambteama   ambteamb   ambteama   ambteamb  

figure     partially expanded role allocation space rescue domain  one fire engine
station    one fire engine station    two ambulances  three civilians  
thus able exploit top organization hierarchy create hierarchical
grouping rmtdp policies  particular  leaf node represents complete
rmtdp policy  with role allocation specified leaf node   parent node
represents group policies  evaluating policy specified leaf node equivalent
evaluating specific role allocation taking future uncertainties account  could
   

fihybrid bdi pomdp framework multiagent teaming

brute force search role allocations  evaluating order determine
best role allocation  however  number possible role allocations exponential
leaf roles organization hierarchy  thus  must prune search space 
    pruning role allocation space
prune space valid role allocations using upper bounds  maxestimates 
parents leaves role allocation space admissible heuristics  section      
leaf role allocation space represents completely specified policy maxestimate upper bound maximum value policies parent node
evaluated using rmtdp  obtain maxestimates parent nodes  shown
brackets right parent node figure     use branch and bound style
pruning  see algorithm     discuss algorithm   below  note essence
performs branch and bound style pruning  key novelty step   discuss
section     
branch and bound algorithm works follows  first  sort parent nodes
estimates start evaluating children parent highest maxestimate  algorithm    steps        evaluate rmtdp  child  refers evaluation
leaf level policy  child  using rmtdp model  evaluation leaf level policies  step
    done using either methods described section    case
role allocation space figure    would start evaluating leaves parent
node one helicopter scouting team five transport team  value
evaluating leaf node shown right leaf node  obtained
value best leaf node  algorithm    steps         case          compare
maxestimates parents role allocation space  algorithm   
steps         see figure   would result pruning three parent nodes
 leftmost parent right two parents  avoid evaluation       leaf level
policies  next  would proceed evaluate leaf nodes parent
two helos scouting team four transport team  would result pruning
remaining unexpanded parent nodes return leaf highest value 
case node corresponding two helos allocated sctteama four
transport team  although demonstrated   level hierarchy  methodology
applying deeper hierarchies straightforward 
    exploiting top calculate upper bounds parents
discuss upper bounds parents  called maxestimates  calculated parent  maxestimate parent defined strict upper bound
maximum expected reward leaf nodes it  necessary
maxestimate upper bound else might end pruning potentially useful role
allocations  order calculate maxestimate parent could evaluate
leaf nodes using rmtdp  would nullify benefit subsequent pruning  we  therefore  turn top plan hierarchy  see figure   b   break
evaluation parent node components  evaluated separately thus
decomposing problem  words  approach exploits structure bdi
program construct small scale rmtdps unlike decomposition techniques
   

finair   tambe

algorithm   branch and bound algorithm policy search 
   parents list parent nodes
   compute maxexp parents   algorithm   
   sort parents decreasing order maxexp
   bestval
   parent parents
  
done parent  false  pruned parent  false
   parent parents
  
done parent    false pruned parent    false
  
child parentnextchild    child leaf level policy parent 
   
child   null
   
done parent  true
   
else
   
childval evaluate rmtdp child 
   
childval   bestval
   
bestval childval best child
   
parent  parents
   
maxexp parent     bestval
   
pruned parent   true
    return best

assume decomposition ultimately rely domain experts identify interactions
agents reward transition functions  dean   lin        guestrin  venkataraman 
  koller        
parent role allocation space  use small scale rmtdps evaluate values top component  fortunately  discussed section     
exploited small scale rmtdps corresponding top components constructing larger
scale rmtdps  put small scale rmtdps use again  evaluating policies within
component obtain upper bounds  note evaluation leaf level
policies  evaluation components parent node done using either
observation histories  see equation    belief states  see equation     describe
section using observation history based evaluation method computing values
components parent  summed obtain maxestimate  an
upper bound childrens values   thus  whereas parent role allocation space
represents group policies  top components  sub plans  allow component wise
evaluation group obtain upper bound expected reward policy
within group 
algorithm   exploits smaller scale rmtdp components  discussed section     
obtain upper bounds parents  first  order evaluate maxestimate
parent node role allocation space  identify start states component
evaluate rmtdps  explain step using parent node figure  
scouting team   two helos  transport team   four helos  see figure      first
component preceding components  start states corresponds
start states policy top mapped onto  next
   

fihybrid bdi pomdp framework multiagent teaming

components next component one linked sequential dependence
start states end states preceding component  however  explained later
section  significantly reduce list start states component
evaluated 
algorithm   maxexp method calculating upper bounds parents role allocation space 
   parent search space
  
maxexp parent   
  
component corresponding factors rmtdp section    
  
component preceding component j
  
obtain start states  states i  endstates j 
  
states i  removeirrelevantfeatures states i    discard features present
si  
  
obtain corresponding observation histories start ohistories i 
endohistories j 
  
ohistories i  removeirrelevantobservations ohistories i  
  
else
   
obtain start states  states i 
   
observation histories start ohistories i  null
   
maxeval i   
   
leaf level policies parent
   
maxeval i  max maxeval i   maxsi states i  ohi ohistories i  evaluate rm dpi  
si   ohi      
 
   
maxexp parent  maxeval i 
similarly  starting observation histories component observation histories completing preceding component  no observation history first
component   bdi plans normally refer entire observation histories rely
key beliefs typically referred pre conditions component 
starting observation history shortened include relevant observations 
thus obtaining reduced list starting observation sequences  divergence private observations problematic  e g  cause agents trigger different team plans 
indicated earlier section      top interpreters guarantee coherence
key aspects observation histories  instance  discussed earlier  top interpreter
ensures coherence key beliefs initiating terminating team plans top  thus
avoiding divergence observation histories 
order compute maximum value particular component  evaluate
possible leaf level policies within component possible start states observation histories obtain maximum  algorithm   steps         evaluation 
store end states ending observation histories used
evaluation subsequent components  shown figure     evaluation
doscouting component parent node two helicopters assigned
scouting team four helos transport team  leaf level policies correspond
possible ways helicopters could assigned teams sctteama  sctteamb  sct   

finair   tambe

teamc transport team  e g  one helo sctteamb  one helo sctteamc four
helos transport team  two helos sctteama four helos transport team  etc 
role allocation tells agents role take first step  remainder
role taking policy specified role replacement policy top infrastructure
role execution policy specified doscouting component top 
obtain maxestimate parent node role allocation space  simply
sum maximum values obtained component  algorithm   steps      e g 
maximum values component  see right component figure    
summed obtain maxestimate                          seen figure    third
node left indeed upper bound      
calculation maxestimate parent nodes much faster
evaluating leaf nodes cases two reasons  firstly  parent nodes
evaluated component wise  thus  multiple leaf level policies within one component result
end state  remove duplicates get start states next component  since component contains state features relevant it  number
duplicates greatly increased  duplication evaluation effort cannot avoided
leaf nodes  policy evaluated independently start finish  instance  doscouting component  role allocations  sctteama    sctteamb   
sctteamc    transportteam   sctteama    sctteamb    sctteamc    transportteam    end states common eliminating irrelevant features
scout sctteamb former allocation scout sctteamc latter allocation fail  feature elimination  algorithm   steps    
state features retained dotransport scouted route number transports
 some transports may replaced failed scouts  shown figure    
second reason computation maxestimates parents much faster
number starting observation sequences much less number ending observation histories preceding components  observations
observation histories component relevant succeeding components  algorithm   steps     thus  function removeirrelevantobservations reduces number
starting observation histories observation histories preceding component 
refer methodology obtaining maxestimates parent maxexp  variation this  maximum expected reward failures  nofail  
obtained similar fashion except assume probability agent failing    able make assumption evaluating parent node  since
focus obtaining upper bounds parents  obtaining exact value 
result less branching hence evaluation component proceed much
quicker  nofail heuristic works evaluation policy without failures
occurring higher evaluation policy failures possible 
normally case domains  evaluation nofail heuristics
role allocation space six helicopters shown square brackets figure   
following theorem shows maxexp method finding upper bounds
indeed finds upper bound thus yields admissible search heuristic branchand bound search role allocation space 
theorem   maxexp method always yield upper bound 
   

fihybrid bdi pomdp framework multiagent teaming

    
doscouting
 scoutingteam   transportteam   

alloc 
sctteama  
sctteamb  
sctteamc  
transportteam  

alloc 
sctteama  
sctteamb  
sctteamc  
transportteam  

      
dotransport
 transportteam   

startstate 
routescouted  
transports  

    
remainingscouts
 scoutteam   

startstate 
routescouted  
transports  

startstate 
routescouted  
transports  

figure     component wise decomposition parent exploiting top 

proof  see appendix c 
theorem    conclude branch and bound policy search algorithm
always find best role allocation  since maxestimates parents true
upper bounds  also  help theorem    show worst case 
branch and bound policy search complexity brute force search 
theorem   worst case complexity evaluating single parent node using maxexp
evaluating every leaf node within constant factor 
proof sketch 
worst case complexity maxexp arises when 
   let esj end states component j executing policy removing
features irrelevant succeeding component k  similarly  let esj
end states component j executing policy
tremoving features
irrelevant succeeding component k  esj esj   null
duplication end states occur 
   let ohj ending observation histories component j executing policy
removing observations irrelevant succeeding component
k  similarly  let ohj ending observation histories component j executing policy removing observation
histories irrelevant

succeeding component k  ohj ohj   null duplication
observation histories occur  note belief based evaluation used
would replace observation histories top congruent belief states
 see sect    
case  computational advantage evaluating components
maxestimate separately  thus  equivalent evaluating child node
parent  thus  worst case  maxexp computation parent
evaluating children within constant factor 
addition  worst case  pruning result using maxexp every
leaf node need evaluated  equivalent evaluating leaf node twice 
   

finair   tambe

thus  worst case complexity branch and bound search using maxexp
finding best role allocation evaluating every leaf node  refer
brute force approach noprune  thus  worst case complexity maxexp
noprune  however  owing pruning savings decomposition computation maxestimates  significant savings likely average
case  section   highlights savings mission rehearsal robocuprescue
domains 

   experimental results
section presents four sets results context two domains introduced
section      viz  mission rehearsal robocuprescue  kitano et al          first 
investigated empirically speedups result using top congruent belief
states  belief based evaluation  observation history based evaluation using
algorithm section   brute force search  focus determining
best assignment agents roles  assume fixed top top infrastructure 
second  conducted experiments investigate benefits considering uncertainty
determining role allocations  this  compared allocations found rmtdp
role allocation algorithm  i  allocations consider kind uncertainty 
 ii  allocations consider observational uncertainty consider action
uncertainty  third  conducted experiments domains determine sensitivity
results changes model  fourth  compare performance allocations
found rmtdp role allocation algorithm allocations human subjects
complex domains robocuprescue simulations 
    results mission rehearsal domain
mission rehearsal domain  top one discussed section     
seen figure   a   organization hierarchy requires determining number agents
allocated three scouting sub teams remaining helos must allocated
transport sub team  different numbers initial helicopters attempted  varying
three ten  details rmtdp constructed domain given
appendix b  probability failure scout time step routes       
               respectively  probability transport observing alive scout
routes                         respectively  false positives possible 
i e  transport observe scout alive failed  probability
transport observing scout failure routes                         respectively 
too  false positives possible hence transport observe failure
unless actually taken place 
figure    shows results comparing different methods searching role
allocation space  show four methods  method adds new speedup techniques
previous 
   noprune obs  brute force evaluation every role allocation determine
best  here  agent maintains complete observation history evaluation
algorithm equation   used  ten agents  rmtdp projected
   

fihybrid bdi pomdp framework multiagent teaming

order        reachable states order         observation histories
per role allocation evaluated  thus largest experiment category limited
seven agents  
   noprune bel  brute force evaluation every role allocation  difference
method noprune obs use belief based evaluation
algorithm  see equation    
   maxexp  branch and bound search algorithm described section    
uses upper bounds evaluation parent nodes find best allocation 
evaluation parent leaf nodes uses belief based evaluation 
   nofail  modification branch and bound heuristic mentioned section     
essence maxexp  except upper bounds computed making
assumption agents fail  heuristic correct domains
total expected reward failures always less failures present
give significant speedups agent failures one primary sources
stochasticity  method  too  evaluation parent leaf nodes uses
belief based evaluation   note upper bounds computed using
no failure assumption changes assumed actual domains  
figure    a   y axis number nodes role allocation space evaluated
 includes leaf nodes well parent nodes   figure    b  y axis represents
runtime seconds logarithmic scale  figures  vary number agents
x axis  experimental results previous work using distributed pomdps often
restricted two agents  exploiting hybrid models  able vary number
agents three ten shown figure    a   clearly seen figure    a  
pruning  significant reductions obtained maxexp nofail noprunebel terms numbers nodes evaluated  reduction grows quadratically
   fold ten agents   noprune obs identical noprune bel terms
number nodes evaluated  since methods leaf level policies evaluated 
method evaluation differs  important note although nofail
maxexp result number nodes evaluated domains 
necessarily true always  general  nofail evaluate least many nodes
maxexp since estimate least high maxexp estimate  however 
upper bounds computed quicker nofail 
figure    b  shows noprune bel method provides significant speedup
noprune obs actual run time  instance     fold speedup using
noprune bel instead noprune obs seven agent case  noprune obs
could executed within day problem settings greater seven agents  
empirically demonstrates computational savings possible using belief based evaluation instead observation history based evaluation  see section     reason 
use belief based evaluation maxexp nofail approaches
   number nodes noprune eight agents obtained experiments  rest
calculated using formula  m n  n     m   n          m n   represents number
heterogeneous role types n number homogeneous agents   m n    m   n         
referred rising factorial 

   

finair   tambe

remaining experiments paper  maxexp heuristic results    fold speedup
noprune bel eight agent case 
nofail heuristic quick compute upper bounds far outperforms
maxexp heuristic     fold speedup maxexp ten agents   speedups
maxexp nofail continually increase increasing number agents  speedup
nofail method maxexp marked because  domain  ignoring
failures results much less branching 
   

nofail  maxexp

number nodes

   

noprune obs 
noprune bel

   
   
   
   
  
 

 

 

 

 

 

 

 

  

number agents
      

maxexp
nofail
noprune bel
noprune obs

time secs  log scale 

     
    
   
  
 
   
    

 

 

 

 

 

 

 

  

number agents

figure     performance role allocation space search mission rehearsal domain  a   left 
number nodes evaluated  b   right run time seconds log scale 

next  conducted experiments illustrating importance rmtdps reasoning
action observation uncertainties role allocations  this  compared
allocations found rmtdp role allocation algorithm allocations found using two
different methods  see figure     
   role allocation via constraint optimization  cop   modi et al         mailler   lesser 
      allocation approach  cop approach    leaf level sub teams or   modi et al s work        focused decentralized cop  investigation emphasis
resulting role allocation generated cop  decentralization per se 

   

fihybrid bdi pomdp framework multiagent teaming

ganization hierarchy treated variables number helicopters
domain variable  thus  domain may           helicopters  
reward allocating agents sub teams expressed terms constraints 
allocating helicopter scout route assigned reward corresponding
routes distance ignoring possibility failure  i e  ignoring transition
probability   allocating helicopters subteam obtained proportionally higher reward 
allocating helicopter transport role assigned large reward transporting cargo destination  allocating helicopters subteam
obtained proportionally higher reward 
allocating least one scout role assigned reward negative infinity
exceeding total number agents assigned reward negative infinity
   rmtdp complete observability  approach  consider transition
probability  ignore partial observability  achieved assuming complete observability rmtdp  mtdp complete observability equivalent
markov decision problem  mdp   pynadath   tambe        actions
joint actions  we  thus  refer allocation method mdp method 
figure    a  shows comparison rmtdp based allocation mdp allocation cop allocation increasing number helicopters  x axis   compare
using expected number transports get destination  y axis  metric
comparison since primary objective domain  seen  considering forms uncertainty  rmtdp  performs better considering transition
uncertainty  mdp  turn performs better considering uncertainty  cop  
figure    b  shows actual allocations found three methods four helicopters
six helicopters  case four helicopters  first three bars   rmtdp mdp
identical  two helicopters scouting route   two helicopters taking transport role 
cop allocation however consists one scout route   three transports 
allocation proves myopic results fewer transports getting destination
safely  case six helicopters  cop chooses one scout helicopter route   
shortest route  mdp approach results two scouts route   
longest route albeit safest  rmtdp approach  considers observational
uncertainty chooses additional scout route    order take care cases
failures scouts go undetected transports 
noted performance rmtdp based allocation depend
values elements rmtdp model  however  next experiment
revealed  getting values exactly correct necessary  order test sensitivity
performance allocations actual model values  introduced error
various parameters model see allocations found using incorrect model
would perform original model  without errors   emulates situation
model correctly represent domain  figure    shows expected number
transports reach destination  y axis  mission rehearsal scenario six
helicopters error  x axis  introduced various parameters model  instance 
   

finair   tambe

 

number transports

 
 

rmtdp
cop
mdp

 
 
 
 
 

 

 

 

 

 

number agents
 
  helos

rm

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

rt 
rt 
xxx
xxxrt 
xxx
transports
xxxx
xxxx
xxxx
xxxx
xxxx

xxxx
xxxx
xxxx
xxxx
xxxx

dp

td

p

 

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxx



 

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

td
p

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx

rm

 

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx


dp

 

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

p

 

co

number helos

 

p

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

co

  helos

 

figure     a  comparison performance different allocation methods  b allocations
found using different allocation methods 

   

fihybrid bdi pomdp framework multiagent teaming

percentage error failure rate route    route  failure rate      
 i e  erroneous failure rate     actual failure rate       difference
number transports reached destination          however
percentage error greater      allocation found conservative resulting
fewer transports getting destination  similarly  percentage error less
      allocation found risky  scouts assigned  resulting
failures  general  figure    shows model insensitive errors      
model parameters mission rehearsal domain  model parameters
outside range  non optimal allocations would result  comparing non optimal
allocations cop  find always perform better cop range
errors tested          failure rate well observability routes  instance 
error     failure rate route    rmtdp managed       transports
safely reach destination  cop managed get       transports reach safely 
comparing non optimal allocations mdp  find performed better
mdp within range         error observability routes  thus 
although allocations found using incorrect model non optimal performed
better cop mdp large ranges errors model  shows getting
model exactly correct necessary find good allocations  thus able
obtain benefits rmtdp even without insisting accurate model 
 

route  failure rate
route   failure rate
route  failure rate
route  observability

number transports

   
 
   
 
   
 
   
 
                  

 

 

  

  

  

  

percentage error

figure     model sensitivity mission rehearsal domain 

    results robocuprescue domain
      speedups robocuprescue domain
next set experiments  highlight computational savings obtained
robocuprescue domain  scenario experiment consisted two fires different
locations city  fires different initially unknown number civilians
it  however total number civilians distribution locations
civilians chosen known ahead time  experiment  fix number
civilians five set distribution used choose civilians locations uniform 
number fire engines set five  located three different fire stations described
   

finair   tambe

section     vary number ambulances  co located ambulance center 
two seven  reason chose change number ambulances
small number fire engines unable extinguish fires  changing problem completely 
goal determine fire engines allocate fire information
civilians transmitted  many ambulances send fire location 
figure    highlights savings terms number nodes evaluated actual
runtime increase number agents  show results noprune bel
maxexp  noprune obs could run slowness  nofail
heuristic identical maxexp since agents cannot fail scenario  rmtdp
case        reachable states 
figures    a     b   increase number ambulances along xaxis  figure    a   show number nodes evaluated  parent nodes   leaf nodes  
logarithmic scale  seen  maxexp method results    fold
decrease number nodes evaluated compared noprune bel seven
ambulances  decrease becomes pronounced number ambulances
increased  figure    b  shows time seconds logarithmic scale y axis
compares run times maxexp noprune bel methods finding best
role allocation  noprune bel method could find best allocation within
day number ambulances increased beyond four  four ambulances  and
five fire engines   maxexp resulted    fold speedup noprune bel 
      allocation robocuprescue
next set experiments shows practical utility role allocation analysis
complex domains  able show significant performance improvements actual
robocuprescue domain using role allocations generated analysis  first 
construct rmtdp rescue scenario  described section      taking guidance
top underlying domain  as described section       use
maxexp heuristic determine best role allocation  compared rmtdp
allocation allocations chosen human subjects  goal comparing rmtdp
allocations human subjects mainly show rmtdp capable performing
near human expert levels domain  addition  order determine
reasoning uncertainty actually impacts allocations  compared rmtdp
allocations allocations determined two additional allocation methods 
   rescueisi  allocations used robocuprescue agents entered
robocuprescue competitions      rescueisi   nair et al         
finished third place  agents used local reasoning decision making 
ignoring transitional well observational uncertainty 
   rmtdp complete observability  discussed earlier  complete observability
rmtdp leads mdp  refer method mdp method 
   number nodes evaluated using noprune bel computed  f        f        f      
 a     c     f    f  f  number fire engines station         respectively 
number ambulances c number civilians  node provides complete conditional
role allocation  assuming different numbers civilians fire station 

   

fihybrid bdi pomdp framework multiagent teaming

number nodes  log scale 

        

maxexp
noprune

       
      
     
    
   
  
 

 

 

 

 

 

 

number ambulances

run time secs  log scale 

      

maxexp
noprune

     

    

   

  

 

 

 

 

 

number ambulances

 

 

figure     performance role allocation space search robocuprescue  a   left  number
nodes evaluated log scale  b   right  run time seconds log
scale 

   

finair   tambe

note comparisons performed using robocuprescue simulator
multiple runs deal stochasticity    scenario described section       
fix number fire engines  ambulances civilians five each  experiment 
consider two settings  location civilians drawn from 
uniform distribution     cases four civilians fire   one civilian
fire        three civilians fire   two fire        two civilians
fire   three fire   remaining     one civilian fire  
four civilians fire    speedup results section       obtained using
distribution 
skewed distribution     cases four civilians fire   one civilian
fire   remaining     one civilian fire   four civilians fire   
note consider case civilians located fire
optimal ambulance allocation simply assign ambulances fire
civilians located  skewed distribution chosen highlight cases
becomes difficult humans reason allocation choose 
three human subjects used experiment researchers usc  three
familiar robocuprescue  given time study setup
given time limit provide allocations  subject told allocations
going judged first basis number civilian lives lost next
damage sustained due fire  exactly criteria used robocuprescue  kitano
et al         
compared rmtdp allocation human subjects
robocuprescue simulator rescueisi mdp  figure     compared
performance allocations basis number civilians died
average damage two buildings  lower values better criteria   two
criteria main two criteria used robocuprescue  kitano et al          values shown figure    obtained averaging forty simulator runs uniform
distribution twenty runs skewed distribution allocation  average
values plotted account stochasticity domain  error bars provided
show standard error allocation method 
seen figure    a   rmtdp allocation better five
allocations terms lower number civilians dead  although human  quite close  
example  averaging forty runs  rmtdp allocation resulted      civilian deaths
human s allocation resulted      civilian deaths  terms average building
damage  six allocations almost indifferentiable  humans actually performing marginally better  using skewed distribution  difference allocations
much perceptible  see figure    b    particular  notice rmtdp
allocation much better humans terms number civilians dead  here 
human  particularly badly bad allocation fire engines  resulted
damage buildings consequently number civilians dead 
   mission rehearsal domain  could run actual mission rehearsal simulator since
simulator public domain longer accessible  hence difference tested role
allocations mission rehearsal robocuprescue domains 

   

fihybrid bdi pomdp framework multiagent teaming

comparing rmtdp rescueisi mdp approach showed reasoning
transitional uncertainty  mdp  better static reactive allocation method
 rescueisi  well reasoning transitional observational uncertainty  uniform distribution case  found rmtdp better mdp
rescueisi  mdp method performing better rescueisi  skewed distribution case  improvement allocations using rmtdp greater  averaging twenty
simulation runs  rmtdp allocations resulted      civilians deaths mdp resulted
     rescueisi       allocation method used rescueisi often resulted
one fires allocated fire engines  allocations determined
mdp approach turned human  
two tailed t test performed order test statistical significance means
allocations figure     means number civilians dead rmtdp
allocation human allocations found statistically different  confidence
       uniform well skewed distributions  difference fire
damage statistically significant uniform case  however  difference
rmtdp allocation human  fire damage statistically significant        
skewed case 
 

civilians casualties
building damage

 
 
 
 
 

dp

ue







 
sc





 



hu

hu





 


hu

rm

td

p

 

 

civilians casualties
building damage

 
 
 
 
 

dp





sc
ue



 


hu

 


hu

 


hu

rm
td

p

 

figure     comparison performance robocuprescue  a   left  uniform  b   right 
skewed 

   

finair   tambe

considering average performance different allocations highlight
individual cases marked differences seen performance  figure    
present comparison particular settings allocation methods showed
bigger difference rmtdp terms allocations  standard error shown
error bars allocation  figures    a     b  compare allocations uniform
civilian distributions setting one civilian fire   four civilians
fire        civilian setting  four civilians fire   one fire        civilian setting 
respectively  seen figure  rmtdp allocation results fewer civilian
casualties slightly damage buildings due fire  difference fire damage
statistically significant damage values close   figures    c 
   d  compare allocations skewed civilian distribution  key difference
arises human   seen  human  results damage due fire 
human  allocated fire engines one buildings  turn resulted
building burnt completely  consequently  civilians located fire
location could rescued ambulances  thus  see specific instances
allocation done using rmtdp based allocation algorithm superior allocations
human comes with 

   

civilians casualties
building damage

 

   

civilians casualties
building damage

 
   

   

 

 

   
 

   

   

 

   

dp



ue



 
sc

 






hu



hu

p




td

civilians casualties
building damage

 

hu

rm



dp




 

ue




hu

sc




hu





hu

td
rm

 

 
 

 

p

   
 

 
   

   

civilians casualties
building damage

 
   

   

 
 

   
 

   

   

 

 
   

   
dp




sc
ue



 



hu



 
hu

hu

p
rm
td

dp



ue


 




sc

hu

 


hu

 


hu

td
p
rm



 

 

 

figure     comparison performance robocuprescue particular settings  a   topleft  uniform     civilian setting b  top right  uniform     civilian setting  c 
 bottom left  skewed     civilian setting d  bottom right  skewed     civilian
setting 

   

fihybrid bdi pomdp framework multiagent teaming

table   shows allocations fire    agents assigned fire   allocated fire
   found rmtdp role allocation algorithm used human subjects
skewed     civilian setting  we consider case since shows difference  
particular  table highlights differences various allocators skewed
    civilian setting helps account differences seen performance
actual simulator  seen figure    d   main difference performance
terms number civilians saved  recall scenario  four
civilians fire    one fire    human subjects mdp chose
send one ambulance fire    number ambulances allocated f ire      
number ambulances allocated f ire     lone ambulance unable rescue
civilian fire    resulting humans mdp saving fewer civilians  rescueisi chose
send ambulances fire   using greedy selection method based proximity
civilians resulting civilians fire   dying    terms fire engine allocation 
human  sent four fire engines fire   civilians likely located
 number engines allocated f ire       number engines allocated f ire    
unfortunately  backfired since lone fire engine fire   able extinguish
fire there  causing fire spread parts city 
distribution
skewed    

engines station  
engines station  
engines station  
ambulances

rmtdp
 
 
 
 

human 
 
 
 
 

human 
 
 
 
 

human 
 
 
 
 

rescueisi
 
 
 
 

mdp
 
 
 
 

table    allocations ambulances fire engines fire   
experiments show allocations found rmtdp role allocation algorithm performs significantly better allocations chosen human subjects rescueisi
mdp cases  and significantly worse case   particular
distribution civilians uniform  difficult humans come
allocation difference human allocations rmtdp allocation
becomes significant  conclude rmtdp allocation performs
near human expertise 
last experiment done using robocuprescue simulator  introduced error
rmtdp model order determine sensitive model errors
parameters model  figure    compares allocations found  five
ambulances    fire engines   civilians  terms number civilian casualties  yaxis  error  x axis  introduced probability fire spread probability
civilian health deterioration  seen increasing error probability fire
spread     higher results allocations save fewer civilians fire brigades
choose concentrate effort one fires  resulting allocation
found value terms number civilians casualties used
rescueisi  consider uncertainty  reducing error probability
fire impact allocations found  increasing error probability
   strategy ambulances going closest civilian worked fairly well ambulances
usually well spread

   

finair   tambe

civilian health deterioration     higher caused civilians sacrificed 
allocation found value terms number civilians casualties
used rescueisi  decreasing error probability civilian health deterioration
    lower  more negative  caused number ambulances allocated fire
number civilians fire  same human   
 

civilian casualties

   
 

fire rate
civilian health

   
 
   
 

                  

 

 

  

  

  

  

percentage error

figure     model sensitivity robocuprescue scenario 

   related work
four related areas research  wish highlight  first 
considerable amount work done field multiagent teamwork  section      
second related area research use decision theoretic models  particular
distributed pomdps  section       third area related work describe  section     
hybrid systems used markov decision process bdi approaches  finally 
section      related work role allocation reallocation multiagent teams
described 
    bdi based teamwork
several formal teamwork theories joint intentions  cohen   levesque        
sharedplans  grosz   kraus        proposed tried capture essence
multiagent teamwork logic beliefs desires intentions  next  practical models
teamwork collagen  rich   sidner         grate   jennings        
steam  tambe        built teamwork theories  cohen   levesque        grosz
  kraus        attempted capture aspects teamwork reusable
across domains  addition  complement practical teamwork models  teamoriented programming approach  pynadath   tambe        tidhar      a      b 
introduced allow large number agents programmed teams  approach
expanded applied variety domains  pynadath   tambe        yen
et al         da silva   demazeau         approaches building practical multia   

fihybrid bdi pomdp framework multiagent teaming

gent systems  stone   veloso        decker   lesser         explicitly based
team oriented programming  could considered family 
research reported article complements research teamwork introducing hybrid bdi pomdp models exploit synergy bdi pomdp
approaches  particular  top teamwork models traditionally addressed
uncertainty cost  hybrid model provides capability  illustrated
benefits reasoning via detailed experiments 
article uses team oriented programming  tambe et al         da silva  
demazeau        tidhar      a      b  example bdi approach  relevant
similar techniques modeling tasking collectives agents  decker
lessers        taems approach  particular  taems language provides abstraction tasking collaborative groups agents similar top  gpgp infrastructure used executing taems based tasks analogous top interpreter
infrastructure shown figure    lesser et al  explored use distributed
mdps analyses gpgp coordination  xuan   lesser         exploited
use taems structures decomposition abstraction searching optimal policies
distributed mdps  suggested article  thus  article complements lesser
et al s work illustrating significant avenue efficiency improvements
analyses 
    distributed pomdp models
distributed pomdp models represent collection formal models expressive
enough capture uncertainty domain costs rewards associated
states actions  given group agents  problem deriving separate policies maximize joint reward modeled using distributed pomdp
models  particular  dec pomdp  decentralized pomdp   bernstein et al        
mtdp  multiagent team decision problem   pynadath   tambe        generalizations pomdps case multiple  distributed agents  basing
actions separate observations  frameworks allow us formulate
constitutes optimal policy multiagent team principle derive policy 
however  exceptions  effective algorithms deriving policies distributed
pomdps developed  significant progress achieved efficient
single agent pomdp policy generation algorithms  monahan        cassandra  littman 
  zhang        kaelbling et al          however  unlikely research directly
carried distributed case  finding optimal policies distributed pomdps
nexp complete  bernstein et al          contrast  finding optimal policy single
agent pomdp pspace complete  papadimitriou   tsitsiklis         bernstein et
al  note  bernstein et al          suggests fundamental difference nature
problems  distributed problem cannot treated one separate pomdps
individual policies generated individual agents possible cross agent
interactions reward  transition observation functions   for one action one
agent  may many different rewards possible  based actions agents
may take  
   

finair   tambe

three approaches used solve distributed pomdps  one approach
typically taken make simplifying assumptions domain  instance 
guestrin et al          assumed agent completely observe world state 
addition  assumed reward function  and transition function  team
expressed sum  product  reward  transition  functions agents
team  becker et al         assume domain factored agent
completely observable local state domain transition independent
 one agent cannot affect another agents local state  
second approach taken simplify nature policies considered
agents  example  chades et al         restrict agent policies memoryless
 reactive  policies  thereby simplifying problem solving multiple mdps  peshkin et
al         take different approach using gradient descent search find local optimum
finite controllers bounded memory  nair et al       a  present algorithm finding
locally optimal policy space unrestricted finite horizon policies  third
approach  taken hansen et al          involves trying determine globally optimal
solution without making simplifying assumptions domain  approach 
attempt prune space possible complete policies eliminating dominated
policies  although brave frontal assault problem  method expected
face significant difficulties scaling due fundamental complexity obtaining
globally optimal solution 
key difference work research focused hybrid systems
leverage advantages bdi team plans  used practical systems 
distributed pomdps quantitatively reason uncertainty cost  particular 
use tops specify large scale team plans complex domains use rmtdps
finding best role allocation teams 
    hybrid bdi pomdp approaches
pomdp models used context analysis single agent  schut 
wooldridge    parsons        multiagent  pynadath   tambe        xuan et al        
behavior  schut et al  compare various strategies intention reconsideration  deciding
deliberate intentions  modeling bdi system using pomdp 
key differences work approach apply analysis single
agent case consider issues exploiting bdi system structure improving
pomdp efficiency 
xuan lesser        pynadath tambe         analyze multiagent
communication  xuan lesser dealt finding evaluating various communication policies  pynadath tambe used com mtdp model deal problem comparing various communication strategies empirically analytically 
approach general explain approach analyzing coordination actions including communication  concretely demonstrate approach analysis role
allocation  additional key differences earlier work pynadath tambe       
follows   i  rmtdp  illustrate techniques exploit team plan decomposition
speeding policy search  absent com mtdp   ii  introduce techniques
belief based evaluation absent previous work  nonetheless  combining rmtdp
   

fihybrid bdi pomdp framework multiagent teaming

com mtdp interesting avenue research preliminary steps
direction presented nair  tambe marsella      b  
among hybrid systems focused analysis  scerri et al         employ markov
decision processes within team oriented programs adjustable autonomy  key difference work mdps used execute particular
sub plan within tops plan hierarchy making improvements top 
dtgolog  boutilier  reiter  soutchanski    thrun        provides first order language
limits mdp policy search via logical constraints actions  although shares
work key idea synergistic interactions mdps golog  differs
work focuses single agent mdps fully observable domains 
exploit plan structure improving mdp performance  isaac  nair  tambe  marsella 
  raines         system analyzing multiagent teams  employs decision theoretic
methods analyzing multiagent teams  work  probabilistic finite automaton
 pfa  represents probability distribution key patterns teams behavior
learned logs teams behaviors  key difference work
analysis performed without access actual team plans agents
executing hence advice provided cannot directly applied improving team 
need human developer change team behavior per advice generated 

    role allocation reallocation
several different approaches problem role allocation reallocation 
example  tidhar et al         tambe et al         performed role allocation based
matching capabilities  hunsberger grosz        proposed use combinatorial auctions decide roles assigned  modi et al         showed
role allocation modeled distributed constraint optimization problem
applied problem tracking multiple moving targets using distributed sensors 
shehory kraus        suggested use coalition formation algorithms deciding
quickly agent took role  fatima wooldridge        use auctions
decide task allocation  important note competing techniques
free problem model problem  even though model
transition probabilities  approaches reforming team reconfiguration methods due dunin keplicz verbrugge         self adapting organizations horling
lesser        dynamic re organizing groups  barber   martin         scerri et
al         present role  re allocation algorithm allows autonomy role reallocation
shift human supervisor agents 
key difference prior work use stochastic models  rmtdps 
evaluate allocations  enables us compute benefits role allocation  taking
account uncertainty costs reallocation upon failure  example  mission
rehearsal domain  uncertainties considered  one scout would
allocated  leading costly future reallocations even mission failure  instead 
lookahead  depending probability failure  multiple scouts sent one
routes  resulting fewer future reallocations higher expected reward 
   

finair   tambe

   conclusion
bdi approach agent teamwork provided successful applications  tools
techniques provide quantitative analyses team coordination team behaviors uncertainty lacking  emerging field distributed pomdps provides
decision theoretic method quantitatively obtaining optimal policy team
agents  faces serious intractability challenge  therefore  article leverages
benefits bdi pomdp approaches analyze improve key coordination
decisions within bdi based team plans using pomdp based methods  order demonstrate analysis methods  concentrated role allocation fundamental aspect
agent teamwork provided three key contributions  first  introduced rmtdp 
distributed pomdp based framework  analysis role allocation  second  article
presented rmtdp based methodology optimizing key coordination decisions within
bdi team plan given domain  concretely  article described methodology
finding best role allocation fixed team plan  given combinatorially many
role allocations  introduced methods exploit task decompositions among sub teams
significantly prune search space role allocations 
third  hybrid bdi pomdp approach uncovered several synergistic interactions
bdi team plans distributed pomdps 
   tops useful constructing rmtdp model domain  identifying
features need modeled well decomposing model construction
according structure top  rmtdp model could used
evaluate top 
   tops restricted policy search providing rmtdps incomplete policies
limited number open decisions 
   bdi approach helped coming novel efficient belief based representation policies suited hybrid bdi pomdp approach corresponding
algorithm evaluating policies  resulted faster evaluation
compact policy representation 
   structure top exploited decompose problem evaluating
abstract policies  resulting significant pruning search optimal role
allocations 
constructed rmtdps two domains robocuprescue mission rehearsal
simulation determined best role allocation domains  furthermore 
illustrated significant speedups rmtdp policy search due techniques introduced
article  detailed experiments revealed advantages approach state ofthe art role allocation approaches failed reason uncertainty 
key agenda future work continue scale up rmtdps even larger
scale agent teams  scale up require efficiency improvements  propose
continue exploit interaction bdi pomdp approaches achieving
scale up  instance  besides disaster rescue  distributed sensor nets large area
monitoring applications could benefit scale up 
   

fihybrid bdi pomdp framework multiagent teaming

acknowledgments
research supported nsf grant           would thank jim blythe 
anthony cassandra  hyuckchul jung  spiros kapetanakis  sven koenig  michael littman 
stacy marsella  david pynadath paul scerri discussions related article 
would thank reviewers article whose comments helped
significantly improving article 

appendix a  top details
section  describe top helicopter scenario  details
subplan figure   b  shown below 
executemission 
context 
pre conditions   mb  taskforce  location taskforce    start 
achieved   mb  taskforce   achieved doscouting  achieved dotransport   
 time    mb  taskforce  achieved remainingscouts 
  helo scoutingteam  alive helo  location helo     end   
unachievable   mb  taskforce  unachievable doscouting  
 mb  taskforce   unachievable dotransport 
 achieved remainingscouts 
  helo scoutingteam  alive helo  location helo     end    
irrelevant 
body 
doscouting
dotransport
remainingscouts
constraints 
doscouting dotransport
doscouting remainingscouts
doscouting 
context executemission  taskforce 
pre conditions 
achieved 
unachievable 
irrelevant 
body 
waitatbase
scoutroutes
constraints 
waitatbase scoutroutes
waitatbase 
context  doscouting  taskforce 
pre conditions 
achieved 
unachievable   mb  transportteam  helo transportteam  alive helo  
   

finair   tambe

irrelevant 
body 
no op



scoutroutes 
context  doscouting  taskforce 
achieved 
unachievable 
irrelevant  mb  scoutingteam  helo transportteam  alive helo  
body 
scoutroute 
scoutroute 
scoutroute 
constraints 
scoutroute  scoutroute  scoutroute 
scoutroute  
context  scoutroutes  scoutingteam 
pre conditions 
achieved   mb  sctteama  helo sctteama  location helo    end 
unachievable  time    mb  sctteama  helo sctteama  alive helo  
irrelevant 
body 
 location sctteama    start  route sctteama   
 location sctteama     end  move forward
scoutroute  
context  scoutroutes  scoutingteam 
pre conditions 
achieved   mb  sctteamb  helo sctteamb  location helo    end 
unachievable  time    mb  sctteamb  helo sctteamb  alive helo  
irrelevant 
body 
 location sctteamb    start  route sctteamb   
 location sctteamb     end  move forward
scoutroute  
context  scoutroutes  scoutingteam 
pre conditions 
achieved   mb  sctteama  helo sctteama  location helo    end 
unachievable  time    mb  sctteama  helo sctteama  alive helo  
irrelevant 
body 
 location sctteama    start  route sctteama   
 location sctteama     end  move forward
dotransport 
context  executemission  taskforce 
pre conditions 
   

fihybrid bdi pomdp framework multiagent teaming

achieved   mb  transportteam  location transportteam    end 
unachievable  time    mb  transportteam  helo transportteam  alive helo  
irrelevant 
body 
 location transportteam    start 
 mb  transportteam  achieved scoutroute   
route transportteam   
elseif  mb  transportteam  achieved scoutroute   
route transportteam   
elseif  mb  transportteam  achieved scoutroute   
route transportteam   
 route transportteam     null   location transportteam     end 
move forward
remainingscouts 
context  executemission  taskforce 
pre conditions 
achieved   mb  scoutingteam  location scoutingteam    end 
unachievable  time    mb  scoutingteam    helo scoutingteam
alive helo  location helo     end  
irrelevant 
body 
 location scoutingteam     end  move forward

predicate achieved tplan  true achieved conditions tplan true  similarly  predicates unachievable tplan  irrelevant tplan  true unachievable conditions irrelevant conditions tplan true  respectively  predicate
 location team    end  true members team end 
figure   b  shows coordination relationships  relationship indicated
solid arc  relationship indicated dotted arc  coordination relationships indicate unachievability  achievability irrelevance conditions
enforced top infrastructure  relationship team sub plans
means team sub plans fail  parent team plan fail  also 
parent team plan achieved  child sub plans must achieved  thus 
doscouting  waitatbase scoutroutes must done 
achieved   mb  taskforce  achieved waitatbase  achieved scoutroutes  
unachievable   mb  taskforce  unachievable waitatbase 
unachievable scoutroutes  

relationship means subplans must fail parent fail success
subplans means parent plan succeeded  thus  scoutingroutes 
least one scoutroute   scoutroute  scoutroute  need performed 
 mb  scoutingteam  achieved scoutroute  
achieved scoutroute   achieved scoutroute   
unachievable   mb  taskforce  unachievable scoutroute  
unachievable scoutroute   unachievable scoutroute   
achieved 

   

finair   tambe

relationship affects irrelevance conditions subplans joins 
parent unachievable subplans still executing become irrelevant 
thus  waitatbase 
irrelevant 

 mb  taskforce  unachievable scoutroutes  

similarly scoutingroutes 
irrelevant 

 mb  taskforce  unachievable scoutroutes  

 
finally  assign roles plans figure   b  shows assignment brackets adjacent plans  instance  task force team assigned jointly perform execute
mission 

appendix b  rmtdp details
section  present details rmtdp constructed top figure   
s  get features state attributes tested preconditions
achieved  unachievable irrelevant conditions body team plans
individual agent plans  thus relevant state variables are location
helicopter  role helicopter route helicopter  status helicopter
 alive not  time  team n helicopters  state given tuple
  time  role            rolen   loc            locn   route            routen   status            statusn   
a  consider actions primitive actions agent perform
within individual plans  top infrastructure enforces mutual belief
communication actions  since analyzing cost focus
research consider communication implicit model effect
communication directly observation function 
consider   kinds actions role taking role execution actions  assume
initial allocation specify roles agents  specifies whether
agent scout transport scout scout team assigned to 
scout cannot become transport change team initial allocation
transport change role taking one role taking actions the role taking
role execution actions agent given by 
i membert ransportt eam    joinsctt eama  joinsctt eamb  joinsctt eamc 
i membersctt eama   i membersctt eamb   i membersctt eamcx  
i membert ransportt eam    chooseroute  movef orward 
i membersctt eama   i membersctt eamb   i membersctt eamc    movef orward 
p   obtain transition function help human expert
simulations simulator available  domain  helicopters crash  be shot
down  start  end already scouted location  probability
scouts get shot depends route on  i e  probability
crash route  p    probability crash route  p  probability crash
route  p  many scouts spot  assume
   

fihybrid bdi pomdp framework multiagent teaming

probability transport shot unscouted location  
scouted location    probability multiple crashes obtained
multiplying probabilities individual crashes 
action  moveforward  effect routei   null loci   end
statusi   dead  cases  location agent gets incremented 
assume role taking actions scoutroutex always succeed role
performing agent transport assigned route already 
  transport start observe status agents
probability depending positions  helicopter particular route
observe helicopters route completely cannot observe helicopters
routes 
o  observation function gives probability group agents receive
particular joint observation  domain assume observations one agent
independent observations agents  given current state
previous joint action  thus probability joint observation computed
multiplying probabilities individual agents observations 
probability transport start observing status alive scout
route         probability transport start observing nothing
alive scout      since dont false negatives  similarly scout
route   crashes  probability visible transport start     
probability transport doesnt see failure       similarly
probabilities observing alive scout route   route            
respectively probabilities observing crash route   route  
          respectively 
r  reward function obtained help human expert helps
assign value various states cost performing various actions 
analysis  assume actions moveforward chooseroute cost 
consider negative reward  cost  replacement action  scoutroutex 
r   negative reward failure helicopter rf   reward
scout reaching end rscout reward transport reaching end
rtransport   e g  r       rf       rscout      rtransport      
rl  roles individual agents take top organization hierarchy 
rl    transport  scoutonroute   scoutonroute   scoutonroute   

appendix c  theorems
theorem   maxexp method always yield upper bound 
proof sketch 
let policy leaf level policy highest expected reward particular parent node  i  restricted policy space 
v   maxchildren i  v
   

   

finair   tambe

since reward function specified separately component  separate expected reward v rewards constituent components given
starting states starting observation histories components  let
team plan divided components components parallel
independent sequentially executed 
x
v
maxstates j  ohistories j vj
 jm

expected value obtained component j    j cannot greater
highest value obtained j using policy 
maxstates j  ohistories j vj maxchildren i  maxstates j  ohistories j  vj  

   

hence 
v

x

maxchildren i  maxstates j  ohistories j  vj  

 jm

v maxestimate i 

   



references
barber  s     martin  c          dynamic reorganization decision making groups 
proceedings fifth international conference autonomous agents  agents     
pp         
becker  r   zilberstein  s   lesser  v     goldman  c  v          transition independent
decentralized markov decision processes  proceedings second international
joint conference autonomous agents multi agent systems  aamas      pp 
     
bernstein  d  s   zilberstein  s     immerman  n          complexity decentralized control mdps  proceedings sixteenth conference uncertainty
artificial intelligence uai      pp       
boutilier  c          planning  learning   coordination multiagent decision processes 
proceedings sixth conference theoretical aspects rationality knowledge  tark      pp         
boutilier  c   reiter  r   soutchanski  m     thrun  s          decision theoretic  highlevel agent programming situation calculus  proceedings seventeenth
national conference artificial intelligence  aaai      pp         
cassandra  a   littman  m     zhang  n          incremental pruning  simple  fast 
exact method partially observable markov decision processes  proceedings
thirteenth annual conference uncertainty artificial intelligence  uai     
pp       
   

fihybrid bdi pomdp framework multiagent teaming

chades  i   scherrer  b     charpillet  f          heuristic approach solving
decentralized pomdp  assessment pursuit problem  proceedings     
acm symposium applied computing  sac      pp       
cohen  p  r     levesque  h  j          teamwork  nous                 
da silva  j  l  t     demazeau  y          vowels co ordination model  proceedings
first international joint conference autonomous agents multiagent
systems  aamas        pp           
dean  t     lin  s  h          decomposition techniques planning stochastic domains  proceedings fourteenth international joint conference artificial
intelligence  ijcai      pp           
decker  k     lesser  v          quantitative modeling complex computational task
environments  proceedings eleventh national conference artificial intelligence  aaai      pp         
dix  j   muoz avila  h   nau  d  s     zhang  l          impacting shop  putting
ai planner multi agent environment  annals mathematics artificial
intelligence                 
dunin keplicz  b     verbrugge  r          reconfiguration algorithm distributed
problem solving  engineering simulation             
erol  k   hendler  j     nau  d  s          htn planning  complexity expressivity 
proceedings twelfth national conference artificial intelligence  aaai     
pp           
fatima  s  s     wooldridge  m          adaptive task resource allocation multiagent systems  proceedings fifth international conference autonomous
agents  agents      pp         
georgeff  m  p     lansky  a  l          procedural knowledge  proceedings ieee
special issue knowledge representation               
goldman  c  v     zilberstein  s          optimizing information exchange cooperative
multi agent systems  proceedings second international joint conference
autonomous agents multi agent systems  aamas      pp         
grosz  b   hunsberger  l     kraus  s          planning acting together  ai magazine 
             
grosz  b     kraus  s          collaborative plans complex group action  artificial
intelligence                 
guestrin  c   venkataraman  s     koller  d          context specific multiagent coordination planning factored mdps  proceedings eighteenth national
conference artificial intelligence  aaai      pp         
hansen  e     zhou  r          synthesis hierarchical finite state controllers pomdps 
proceedings thirteenth international conference automated planning
scheduling  icaps      pp         
   

finair   tambe

hansen  e  a   bernstein  d  s     zilberstein  s          dynamic programming partially
observable stochastic games  proceedings nineteenth national conference
artificial intelligence  aaai      pp         
ho  y  c          team decision theory information structures  proceedings
ieee                 
horling  b   benyo  b     lesser  v          using self diagnosis adapt organizational
structures  proceedings fifth international conference autonomous
agents  agents      pp         
hunsberger  l     grosz  b          combinatorial auction collaborative planning 
proceedings fourth international conference multiagent systems  icmas       pp         
jennings  n          controlling cooperative problem solving industrial multi agent
systems using joint intentions  artificial intelligence                 
kaelbling  l   littman  m     cassandra  a          planning acting partially
observable stochastic domains  artificial intelligence                 
kitano  h   tadokoro  s   noda  i   matsubara  h   takahashi  t   shinjoh  a     shimada 
s          robocup rescue  search rescue large scale disasters domain
multiagent research  proceedings ieee conference systems  men 
cybernetics  smc      pp         
levesque  h  j   cohen  p  r     nunes  j          acting together  proceedings
national conference artificial intelligence  pp        menlo park  calif   aaai
press 
mailler  r  t     lesser  v          solving distributed constraint optimization problems using cooperative mediation  proceedings third international joint conference
agents multiagent systems  aamas      pp         
marschak  j     radner  r          economic theory teams  cowles foundation
yale university press  new haven  ct 
modi  p  j   shen  w  m   tambe  m     yokoo  m          asynchronous complete
method distributed constraint optimization  proceedings second international joint conference agents multiagent systems  aamas      pp 
       
monahan  g          survey partially observable markov decision processes  theory 
models algorithms  management science               
nair  r   ito  t   tambe  m     marsella  s          task allocation rescue simulation
domain  robocup       robot soccer world cup v  vol       lecture notes
computer science  pp          springer verlag  heidelberg  germany 
nair  r   pynadath  d   yokoo  m   tambe  m     marsella  s       a   taming decentralized
pomdps  towards efficient policy computation multiagent settings  proceedings
eighteenth international joint conference artificial intelligence  ijcai     
pp         
   

fihybrid bdi pomdp framework multiagent teaming

nair  r   tambe  m     marsella  s       b   team formation reformation multiagent domains robocuprescue  kaminka  g   lima  p     roja  r   eds   
proceedings robocup      international symposium  pp          lecture notes
computer science  springer verlag 
nair  r   tambe  m   marsella  s     raines  t          automated assistants analyze
team behavior  journal autonomous agents multi agent systems           
    
papadimitriou  c     tsitsiklis  j          complexity markov decision processes  mathematics operations research                 
peshkin  l   meuleau  n   kim  k  e     kaelbling  l          learning cooperate via
policy search  proceedings sixteenth conference uncertainty artificial
intelligence  uai      pp         
poupart  p     boutilier  c          bounded finite state controllers  proceedings
advances neural information processing systems     nips  
pynadath  d  v     tambe  m          communicative multiagent team decision
problem  analyzing teamwork theories models  journal artificial intelligence
research             
pynadath  d  v     tambe  m          automated teamwork among heterogeneous software agents humans  journal autonomous agents multi agent systems
 jaamas            
rich  c     sidner  c          collagen  agents collaborate people 
proceedings first international conference autonomous agents  agents     pp         
scerri  p   johnson  l   pynadath  d   rosenbloom  p   si  m   schurr  n     tambe  m 
        prototype infrastructure distributed robot  agent  person teams 
proceedings second international joint conference agents multiagent
systems  aamas      pp         
scerri  p   pynadath  d  v     tambe  m          towards adjustable autonomy
real world  journal artificial intelligence  jair              
schut  m  c   wooldridge  m     parsons  s          reasoning intentions uncertain domains  proceedings sixth european conference symbolic
quantitative approaches reasoning uncertainty  ecsqaru        pp    
   
shehory  o     kraus  s          methods task allocation via agent coalition formation 
artificial intelligence                    
sondik  e  j          optimal control partially observable markov processes  ph d 
thesis  stanford 
stone  p     veloso  m          task decomposition  dynamic role assignment  lowbandwidth communication real time strategic teamwork  artificial intelligence 
                
   

finair   tambe

tambe  m          towards flexible teamwork  journal artificial intelligence research 
         
tambe  m   pynadath  d     chauvat  n          building dynamic agent organizations
cyberspace  ieee internet computing              
tidhar  g       a   team oriented programming  preliminary report  tech  rep      australian artificial intelligence institute 
tidhar  g       b   team oriented programming  social structures  tech  rep      australian artificial intelligence institute 
tidhar  g   rao  a     sonenberg  e          guided team selection  proceedings
second international conference multi agent systems  icmas      pp         
wooldridge  m          introduction multiagent systems  john wiley   sons 
xuan  p     lesser  v          multi agent policies  centralized ones decentralized ones  proceedings first international joint conference agents
multiagent systems  aamas      pp           
xuan  p   lesser  v     zilberstein  s          communication decisions multiagent
cooperation  proceedings fifth international conference autonomous
agents  agents      pp         
yen  j   yin  j   ioerger  t  r   miller  m  s   xu  d     volz  r  a          cast  collaborative agents simulating teamwork  proceedings seventeenth international
joint conference artificial intelligence  ijcai      pp           
yoshikawa  t          decomposition dynamic team decision problems  ieee transactions automatic control  ac                

   


