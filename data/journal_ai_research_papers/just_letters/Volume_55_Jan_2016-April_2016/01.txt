journal of artificial intelligence research                   

submitted        published      

exploiting causality for selective belief filtering in
dynamic bayesian networks
stefano v  albrecht

svalb cs utexas edu

department of computer science
the university of texas at austin
austin  tx        usa

subramanian ramamoorthy

s ramamoorthy ed ac uk

school of informatics
the university of edinburgh
edinburgh  eh   ab  uk

abstract
dynamic bayesian networks  dbns  are a general model for stochastic processes with
partially observed states  belief filtering in dbns is the task of inferring the belief state  i e 
the probability distribution over process states  based on incomplete and noisy observations 
this can be a hard problem in complex processes with large state spaces  in this article  we
explore the idea of accelerating the filtering task by automatically exploiting causality in
the process  we consider a specific type of causal relation  called passivity  which pertains
to how state variables cause changes in other variables  we present the passivity based
selective belief filtering  psbf  method  which maintains a factored belief representation
and exploits passivity to perform selective updates over the belief factors  psbf produces
exact belief states under certain assumptions and approximate belief states otherwise  where
the approximation error is bounded by the degree of uncertainty in the process  we show
empirically  in synthetic processes with varying sizes and degrees of passivity  that psbf is
faster than several alternative methods while achieving competitive accuracy  furthermore 
we demonstrate how passivity occurs naturally in a complex system such as a multi robot
warehouse  and how psbf can exploit this to accelerate the filtering task 

   introduction
dynamic bayesian networks  dbns   dean   kanazawa        are a general model for
stochastic processes with partially observed states  the topology of a dbn is a compact
specification of how variables in the process interact during transitions  cf  figure     given
the possible incompleteness and noise in observations  it may not generally be possible to
infer the state of the process with absolute certainty  instead  we may infer beliefs about the
process state based on the history of observations  in the form of a probability distribution
over the state space of the process  this is often called a belief state and the task of calculating
belief states is commonly referred to as belief filtering 
a number of exact and approximate inference methods exist for bayesian networks  see 
e g   koller   friedman        pearl        which can be used for filtering in dbns  by
applying them to the unrolled dbn in which the t     slice is repeated for each observed
time step  or via a successive update in which the current posterior  belief state  is used
c
    
ai access foundation  all rights reserved 

fialbrecht   ramamoorthy

xt 

xt  
 

y t  

xt 

xt  
 

y t  

t

t  

figure    example of a dynamic bayesian network  dbn  with two state variables and two
observation variables  the xti and xt  
variables represent the process states at time t and
i
t      respectively  while the yit   variables  shaded  represent the observation at time t     
the arrows describe how the variables interact 

as the prior in the next time step  see also murphy         however  it is clear that the
unrolled variant becomes intractable as the network grows unboundedly with time  even
in the successive update  exact methods become intractable in high dimensional process
states and approximate methods may propagate growing errors over time  therefore  filtering
methods were developed which utilise the special structure of dbns and maintain the errors
propagated over time   we defer a detailed discussion of such methods to section    
often  the key to developing efficient filtering methods is to identify structure in the
process which can be leveraged for inference  in this article  we are interested in the application
of dbns as representations of actions in partially observed decision processes  such as
pomdps  kaelbling  littman    cassandra        sondik        and their many variants 
dbns can be used to represent the effects of actions on the decision process  by specifying
how variables interact and what information the decision maker observes  in many cases 
decision processes exhibit high degrees of causal structure  pearl         by which we mean
that a change in one part of the process may cause a change in another part  our experience
with such processes is that this causal structure may be used to make the filtering task more
tractable  because it can tell us that beliefs need only be revised for certain aspects of the
process state  for example  if the variable x  in figure   changes its value only if variable x 
changed its value  i e  a change in x  causes a change in x     then it seems intuitive to use
this causal relation when deciding whether to revise ones belief about x    unfortunately 
current filtering methods do not take such causal structure into account 
we refer to the above type of causal relation  between x  and x    as passivity  intuitively 
we say that a state variable xi is passive in a given action if  when executing that action 
there is a subset of the state variables that directly affect xi  i e  xi s parents in the dbn 
such that xi may change its value only if at least one of the variables in this subset changed
its value  it is worth pointing out that passivity occurs naturally and frequently in many
planning domains  especially in robotic and other physical systems  mainzer         the
following example  illustrates this in a simple robot arm 

   we mark the end of an example with a solid black square 

    

fiexploiting causality for selective belief filtering in dbns

 
 

 

 

 

a

xa

b

xb

 

 a  robot arm with gripper

c
 b  holding blocks b and a

figure    robot arm with three rotational joints and gripper  the variables i represent the
absolute orientations of the corresponding joints 
example    robot arm   consider a robot arm with three rotational joints and a gripper 
as shown in figure  a  the joints are denoted by           and may take any values from the
discrete set                     which indicate their absolute orientations  e g  i     means
that joint i points exactly to the right  i       means that it points to the left   for each
joint i  let there be two actions cwi and ccwi which rotate the joint by   clockwise and
counter clockwise  respectively  the uncertainty in this system could be due to stochastic
joint movements or unreliable sensor readings for the joint orientations 
for any action cwi or ccwi   the variable i is not passive because its value is directly
modified by the action  however  the variables j  i are passive because they change their
values only if the corresponding preceding variable j  changed its value  since a changed
orientation of joint j    causes a changed orientation of joint j  recall that the orientations
are absolute   note that this also accounts for chains of such causal effects  as indicated by
the arrows  the orientation of joint   changes if the orientation of joint   changes  since joint
  causes joint   to change  which in turn causes joint   to change 
further examples of passivity can be seen in the context of object manipulation  such as
in the blocks planning domain  e g  pasula  zettlemoyer    kaelbling         figure  b
shows the arm holding blocks b and a  with a on top of b  here  the position of b  xb   is
passive with respect to the joint orientations since it will only change if any of the orientations
changed  furthermore  there is a causal chain from the joint orientations to the position of
block a  xa    since as position will change if bs position changes 

how can passivity be exploited to accelerate the filtering task in the above example  the
fact that the state variables are passive means that some aspects of the state may remain
unchanged  depending on which action we choose  for example  if we choose to rotate joint
   then the fact that joints   and   are passive means that they are unaffected by this action 
thus  it seems redundant to revise beliefs for the orientations of joints   and    however 
this is precisely what current filtering methods do  cf  section    
more concretely  assume we use a factored belief representation p                p          
p          and choose to rotate   in any direction  then  it is easy to see that we will need
to update the factor p           since   changes its value  but not the factor p           since
the variables       are both passive  since the parents of        if any  do not change their
values  we know that       will not change their values either  as we will show later  skipping
    

fialbrecht   ramamoorthy

over p          does not result in a loss of information in such cases  and similarly for chains
of such causal connections  cf  example     a more complex example of a planning domain
involving passivity  and how it can be exploited  is discussed in section     
in addition to guiding belief revision  there are several features which make passivity
an interesting example of a causal relation  first of all  passivity is a latent causal relation 
meaning that it can be readily extracted from the process dynamics without additional
annotation by an expert   in section    we give a procedure which identifies passive variables
based on their conditional probability tables   furthermore  passivity is not a deterministic
relation since passive variables may have any stochastic behaviour when changing their
values  finally  passivity is a relatively simple example of a causal relation  and the idea of
exploiting passivity in order to accelerate the filtering task is intuitive  yet  to the best of
our knowledge  this has not been formalised and explored rigorously before 
the purpose of the present article is to formalise and evaluate the idea of automatically
exploiting causal structure for efficient belief filtering in dbns  using passivity as a concrete
example of a causal relation  specifically  our hypothesis is that in large processes with
high degrees of passivity  this structure can be exploited to accelerate the filtering task 
after discussing related work in section   and technical preliminaries in section    our
contributions can be grouped into the following parts 
 in section    we give a formally concise definition of passivity and discuss various
aspects of this definition  our definition assumes a decision process which is specified
as a set of dynamic bayesian networks  one for each action   we also discuss a nonexample of passivity  by which we mean variables which appear to be passive but really
are not passive  finally  we give a simple procedure which can detect passive variables
based on their conditional probability tables 
 in section    we present the passivity based selective belief filtering  psbf  method 
following the idea outlined above  psbf uses a factored belief representation in which
the belief factors are defined over clusters of correlated state variables  psbf follows
a   step update procedure wherein the belief state is first propagated through the
process dynamics  the transition step  and then conditioned on the observation  the
observation step   the interesting novelty of psbf is the way in which it performs
the transition step  rather than updating all belief factors  psbf updates only those
factors whose variables it suspects to have changed  which is possible by exploiting
passivity  to be made precise shortly   similarly  in the observation step  psbf updates
only those belief factors which it determines to be structurally connected with the
observation  and it uses only those parts of the observation which are relevant to the
belief factor  thus allowing for a more efficient incorporation of observations  psbf
produces exact belief states under certain assumptions and approximate belief states
otherwise  we also discuss the computational complexity and error bounds of psbf 
 in section    we evaluate psbf in two experimental domains  we first evaluate psbf in
synthetic  i e  randomly generated  processes of varying sizes and degrees of passivity 
the process sizes vary from one thousand to one trillion states  and the passivity
degrees vary from     to      passivity  our results show that psbf is faster than
several alternative methods while maintaining competitive accuracy  in particular  our
    

fiexploiting causality for selective belief filtering in dbns

results indicate that the computational gains grow significantly with both the degree of
passivity and the size of the process  we then evaluate psbf in a complex simulation
of a multi robot warehouse system in the style of kiva  wurman  dandrea    mountz 
       we show how passivity occurs in this system and how psbf can exploit this to
accelerate the filtering task  again outperforming alternative methods 
finally  we discuss the strengths and weaknesses of psbf in section    and we conclude
our work in section    all proofs can be found in the appendix 

   related work
there exists a substantial body of work on belief filtering in partially observed stochastic
processes  in this section  we review filtering methods that utilise the special structure of
dbns and situate our work within this and other related literature 
    approximate belief filtering in dbns
several authors proposed filtering methods wherein the belief state is represented as a set of
state samples  specifically  the probability that the process is in state s is the normalised
frequency with which the state samples correspond to s  these methods are now commonly
referred to as particle filters  pf   see the work of doucet  de freitas  and gordon       
for a survey  in a common variant of pf  gordon  salmond    smith         the filtering
task consists of propagating the current state samples through the process dynamics and a
subsequent resampling step based on the probabilities with which the new state samples
would have produced the observation  two interesting features of pf are that it can be
applied to processes with discrete and continuous variables  and that the approximation
error converges to zero as we increase the number of state samples 
a known problem of pf is the fact that the number of samples needed for acceptable
approximations can grow drastically with the variance in the process dynamics  as shown
in our experiments  cf  section     rao blackwellised pf  rbpf   doucet  de freitas 
murphy    russell        was developed to address this problem  rbpf assumes that the
state variables can be grouped into sets r and x such that the distribution over x can be
efficiently calculated from r during the filtering  hence  a sample in rbpf consists of a
sample of r and a corresponding marginal distribution over x  rbpf is useful when the
variance in r is relatively low and the variance in x is high  since this reduces the number
of samples needed for acceptable approximations 
boyen and koller              recognised that if a process consists of several independent
or weakly interacting subcomponents  then the belief state can be represented more efficiently
as a product of smaller beliefs about these individual subcomponents  their seminal contribution is to show that the approximation error due to this factored representation is essentially
bounded by the degree of uncertainty  or mixing rates  in the process  more precisely  they
prove that the relative entropy  or kl divergence  kullback   leibler        between two belief states contracts at an exponential rate when propagated through a stochastic transition
process  based on this observation  they propose a filtering method  bk  wherein the belief
state is represented in factored form and the belief factors are updated using an exact inference method  such as the junction tree algorithm  lauritzen   spiegelhalter         since
    

fialbrecht   ramamoorthy

the internal cliques used in the junction tree algorithm may not correspond to the belief
state representation of bk  a final projection step will typically have to be performed in
which the original factorisation is restored  the performance of this method depends crucially on whether the relevant correlations between state variables can be captured in small
clusters  and whether the projection step can be performed efficiently 
factored particle filtering  fp   ng  peshkin    pfeffer        addresses the main drawbacks of pf  many samples needed  and bk  small clusters required  by approximating the
belief factors using a set of factored state samples  the samples are factored in the sense
that they only assign values to the variables in the corresponding factor  this allows fp to
represent belief factors which are too large for bk  and it reduces the number of samples
needed due to the smaller number of variables in each factor  the authors provide different methods of updating the factored state samples  but the generic idea is to first perform
a join operation in which full state samples are reconstructed from the factored samples 
which are then updated as in standard pf  the updated samples are then projected down
into factored form using a project operation  the main drawback of fp is that these join
and project operations essentially correspond to standard relational database operations 
which can be very expensive 
murphy and weiss        propose a filtering method called factored frontier  ff   ff
uses a fully factored representation of belief states  that is  the belief state is a product of
marginals for each individual state variable  this allows for a very compact representation
of beliefs  the algorithm works by moving a set of state variables  the frontier  forward
and backward in the dbn topology  this requires a certain variable ordering  which can
be difficult to attain if intra correlations between state variables  i e  edges within the t    
slice of the dbn  are allowed  the authors show that their method is equivalent to a single
iteration of loopy belief propagation  lbp   pearl         thus  similar to lbp  ff can be
applied in successive iterations to improve the approximation accuracy 
none of the works discussed above explicitly address the question of how causal relations
between state variables can be exploited to accelerate the filtering task  or  alternatively  how
the filtering methods proposed therein implicitly benefit from causal structure  our method 
psbf  is related to bk and fp in that psbf  too  uses a factored belief representation 
where the belief factors are defined over clusters of correlated state variables  therefore  the
analysis of approximation errors by boyen and koller        also applies to psbf  as we
show in section   as well as in our experiments  however  in contrast to bk and fp  psbf
does not perform inference over the complete factorisation  but rather over the individual
factors  as a consequence  psbf does not require a join or project operation  which is one
of the main disadvantages of bk and fp 
    belief filtering in decision processes
the methods discussed in the preceding subsection can be used for belief filtering in decision
processes  including pomdps  kaelbling et al         sondik         in this regard  these
methods can be viewed as pure filters in that they are only concerned with belief filtering
and not with the control of the decision process  this is in contrast to combined filtering
methods  which interleave the filtering and control tasks in decision processes and make
specific assumptions regarding solutions thereof  there exists a large body of literature on such
    

fiexploiting causality for selective belief filtering in dbns

combined methods  including reachability based methods  hauskrecht        washington 
       grid based methods  zhou   hansen        brafman        lovejoy         pointbased methods  smith   simmons        pineau  gordon    thrun         and compression
methods  roy  gordon    thrun        poupart   boutilier        
a potential advantage of such combined methods is that they have access to additional
structure and may  therefore  utilise synergies between the filtering and control tasks  one
such synergy is the use of decision quality to guide belief filtering  rather than metrics such
as relative entropy  poupart and boutilier              propose a filtering method  called
value directed approximation  which chooses different approximation schemes for different
decisions so as to minimise the expected loss in decision quality  i e  accumulated rewards  
the method assumes that the pomdp has been solved exactly and that the value function
is provided in the form of  vectors which represent the available actions in the pomdp 
based on the value function  their algorithm computes a switching set and alternative
plans to determine the error bounds of approximation schemes  this is used to search for
an optimal approximation scheme in a tree based manner  where the search traverses from
approximate to exact schemes 
while the idea of using decision quality to guide belief filtering is appealing  their method
involves a series of optimisation problems and an exhaustive tree search  which can be very
costly in complex systems  the advantage of pure filtering methods  including our proposed
method psbf  is that they can filter processes which are too complex for combined methods 
such as the multi robot warehouse system studied in section    the actual control task can
then be done via domain specific solutions  cf  section        
    substructure in parameterisation
bayesian networks  and hence dbns  allow for a compact parameterisation  i e  specification
of probabilities  and efficient inference via conditional independence relations  in addition 
there has been considerable work in identifying substructure in the parameterisation to
further simplify knowledge acquisition and enhance inference  koller   friedman       
boutilier  dean    hanks         the property studied in this work  passivity  is one example
of substructure in the parameterisation  other notable examples include causal independence  e g  heckerman   breese        heckerman        and context specific independence
 boutilier  friedman  goldszmidt    koller        
causal independence is the assumption that the effects of individual causes on a common
variable  i e  the parents of that variable  are independent of one another  this allows for
a compact parameterisation via operators such as noisy or  srinivas        pearl        
and it can be used to enhance inference  zhang   poole         note that passivity is a
conceptually much simpler property than causal independence  because passivity is neither
concerned with the strength of individual causes nor the extent to which they depend on each
other  moreover  passivity can be read directly from the parameterisation  cf  section     
whereas causal independence is usually imposed by the designer 
context specific independence  csi  is a property which states that a variable is independent of some of its parents given a certain assignment of values  i e  context  to some of
its other parents  non local csi statements follow similarly to d separation  geiger  verma 
  pearl         this can allow for a further reduction of parameters  boutilier et al        
    

fialbrecht   ramamoorthy

and enhancement of inference  poole   zhang         as we will discuss in section    passivity can be viewed as a special kind of csi applied to dbns  in that the parents with respect
to which the variable is passive provide the context for csi  however  in contrast to csi 
passivity does not assume that the context is actually observed 

   technical preliminaries
this section introduces the basic concepts and notation used in our work  we begin with
a brief discussion of decision processes to provide the context for our work  followed by a
discussion of dynamic bayesian networks as the model over which we perform inference 
    decision processes  belief states  exact updates
we consider a stochastic decision process wherein  at each time t  the process is in state
st  s and a decision maker  or agent  is choosing an action at   after executing at in st   the
t
process transitions into state st    s with probability t a  st   st     and the agent receives an
t
observation ot    o with probability a  st     ot      we assume factored representations of
the state space s and observation space o  such that s   x        xn and o   y        ym  
where the domains xi   yj are finite  the notation si is used to denote the value of xi in
state s  s  and analogously for oj with o  o  moreover  we assume that the process is
time invariant  meaning that t a and a are independent of t  this framework is compatible
with many decision models used in the artificial intelligence literature  including pomdps
 kaelbling et al         sondik        and its many variants 
the agent chooses action at based on its belief state bt  also known as information state  
which represents the agents beliefs about the likelihood of states at time t  formally  a
belief state is a probability distribution over the state space s of the process  belief filtering
is the task of calculating a belief state based on the history of observations  ideally  the
resulting belief state should be exact in that it retains all relevant information from the past
observations  this is sometimes referred to as sufficient statistic  cf  astrom         the
exact update rule is a simple procedure that produces exact belief states 
definition    exact update rule   the exact update rule is defined as follows  after taking
action at and observing ot     the belief state bt is updated to bt   via
bt    s     

x

t

bt  s  t a  s  s   

   

ss
t

bt    s       bt    s    a  s    ot    

   

where  is a normalisation constant 
we sometimes refer to the step bt  bt   as the transition step and to the step bt    bt  
as the observation step  unfortunately  the space complexity of storing exact belief states
and the time complexity of updating them using the exact update rule are both exponential
in the number of state variables  making it infeasible for complex systems with large state
spaces  hence  more efficient approximate methods are required 
    

fiexploiting causality for selective belief filtering in dbns

    dynamic bayesian networks
a dynamic bayesian network  dbn   dean   kanazawa        is a bayesian network with
a special temporal semantics that specifies how a stochastic process transitions from one
state into another  dbns can be used to model the effects of actions in a stochastic decision
process  specifically  they are a compact representation of the transition function t a and
observation function oa of action a 
definition    dbn   a dynamic bayesian network for action a  denoted a   is an acyclic
directed graph consisting of 

 

 
t   with xt   xt    x  
 state variables x t   xt         xtn and x t     xt  
i
i i
         xn
representing the states of the process at time t and t      respectively 

 
t   with y t    y   representing the obser observation variables y t     y t          ym
j
j
vation received at time t     




 directed edges ea  x t  x t    x t    x t    x t    y t    y t    y t    
specifying the network topology and dependencies between variables 
 conditional probability distributions pa  z   paa  z   for each variable z  x t    y t    
specifying the probability that z assumes a certain value given a specific assignment
to its parents paa  z     z      z     z   ea    for convenience  we also define pata  z   
t    pa  z   where pa  z    
x t  paa  z  and pat  
a
a
zz paa  z  
a  z    x
the edges ea and distributions pa define the functions t a and a as
a

 

t  s  s    

n
y

 
pa xt  
  s i   paa  xt  
i
i      s  s  



   

i  

a  s    o   

m
y



pa yjt     oj   paa  yjt        s    o 

   

j  
t  
 
where we use the notation paa  xt  
in
i      s  s   to specify that the parents of xi
t  
 
and x   respectively  assume their corresponding values from s and s   formally  if
t  
t  
t
xtl  pata  xit     and xt  
 pat  
  s l    similarly  we use
a  xi    then xl   sl and xl 
l 
t  
 
the notation paa  yj      s   o  to specify that the parents of yjt   in x t   and y t    
respectively  assume corresponding values from s  and o 

xt

example    dbn representation of robot arm   we can represent the robot arm from example   as a set of dbns  where we have one dbn a for each action
  cw
i   ccwi    the
  at  
 t  
 
t
t
t
t
state and observation
variables
       t      t    
n
o in the dbns are x               x

and y t      t      t      t     to make our example more realistic  let us assume that the
joint orientations are bounded relative to the orientation of the immediately preceding joint
 e g  in the form of a cone   where the first joint is bounded relative to the ground  this
means that the joint movement depends on its own as well as the preceding joint orientation  as shown in figure    moreover  the joint orientations are correlated  i e  edges within
    

fialbrecht   ramamoorthy

 t

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

xt

x t  

y t  

figure    dbn representation of robot arm 
x t     such that no joint can exceed the bound given by the preceding joint  finally  the observation variables depend solely on the corresponding joint variable  the actions in this
example would differ in their variable distributions pa  

    additional definitions
it will be useful to define the following 
 xt  
for
 the binary order  is defined over x t  x t   such that xti  xtj and xt  
i
j
t  
t
all    i   j  n  and xi  xj for all    i  j  n 
 given a set z  x t  x t     we write z  to denote the tuple that contains all variables
of z  ordered by  
 given the ordered tuple z     zi         zi z     we define the set s z    xi         xi z 
to contain all value tuples for the variables in z 
 given a value tuple sz    si         si z     s z   we use the notation z   sz as an
abbreviation for zil   sil for each zil  z   i e  the variables in z assume their
corresponding values from sz   

   passivity
this section introduces a formal definition of passivity  which will then be used as the basis
for the remainder of this article  we also provide a simple procedure to detect passive
variables from the process dynamics 
    formal definition
as outlined in section    a state variable xt  
is called passive in action a if there exists a
i
t  in the dbn a   such that xt   may change its value only
subset of xt  
s
parents
in
x
i
i
    

fiexploiting causality for selective belief filtering in dbns

if at least one of the variables in this subset changed its value  conversely  xt  
does not
i
change if the variables in the subset did not change  formally  we define passivity as follows 
t  
a
definition    passivity   let action a be given by a dbn
 t
    a state variable xi is
t  
a
t
called passive in  if there exists
a set a i  paa  xi     xi such that 

t  
 i  xtj  a i   xt  
 ea
j   xi
and
 ii  for any two states st and st   with t a  st   st          


 sti   st  
xtj  a i   stj   st  
j
i

   

a state variable which is not passive is called active 
the set a i corresponds to the subset of variables described above  it contains all those
variables which directly affect xit    i e  they are parents of xt  
in x t   such that xt  
may
i
i
change its value only if any of the variables in a i changed its value  we will sometimes say
that a variable xt  
is passive in a with respect to another variable xtj if it is the case that
i
xtj  a i   furthermore  we will omit in a  if it is obvious from context 
clause  i  in definition   requires that xt  
is intra correlated with the variables in a i  
i
specifically  that there is an edge from xt  
to
xt  
for all xtj  a i   as an example  see
j
i
figure   in which we assumed that the variable xt  
was passive with respect to the variable
 
xt     we will discuss the purpose of this clause in the next subsection   clause  ii  defines
the core semantics of passivity by requiring that xt  
remains unchanged if all variables in
i
a i remain unchanged  note that this means that the distribution pa for xt  
may specify
i
any deterministic or stochastic behaviour if the variables in a i change their values  this
includes that xt  
may not change its value at all 
i
a state variable xit   can be passive even if it has no parents in x t   or none other than
xti   in this case  the set a i would be empty and clause  i  as well as the premise in    
would trivially hold true  however  such a variable can only be passive if it does not change
its value under any circumstances  in other words  it would have to be a constant  in that
case  one should consider removing the variable from the state description in order to reduce
computational costs 
as noted in section      passivity can be shown to be a special kind of context specific
independence  csi   boutilier et al         applied to dbns  here  the associated set a i of
a passive variable xt  
provides the context  given any assignment of values to xtj  a i  i e 
i
t  
context  such that xtj   xt  
is independent of all xtk   xt  
with xtk  pata  xt  
j   xi
i     a i
k
and k    i  however  besides this similarity  there is an important difference between passivity
and csi  which is that passivity does not actually assume that the context is observed  thus 
passivity can be viewed as a kind of csi for unobserved contexts  this will become clear in
section    when we describe a filtering method that exploits passivity 
    non example of passivity
what is the purpose of clause  i  in the definition of passivity  after all  and as discussed
previously  clause  ii  captures the core idea of passivity  which is that a variable may only
change its value if any of the variables with respect to which it is passive changed its value 
    

fialbrecht   ramamoorthy

xt 

xt  
 

xt 

xt  
 

figure    example of a process for which clause  ii  is insufficient 
however  while it may seem intuitive that clause  ii  be sufficient for passivity  there are
in fact processes in which clause  ii  alone does not suffice  in other words  clause  ii  is
necessary but not sufficient for passivity  we illustrate this in the following example 
example    non example of passivity   consider a process with two binary state variables 
x    x    and a single action  a  shown in figure     we omit the observation variables for
clarity   the dynamics of the process are such that xt  
takes the value of xt  and xt  
takes
 
 
the value of xt   i e  x  and x  swap their values at each time step   in this process  both
state variables satisfy clause  ii  of definition    if we set x     x    i e  same initial values  
then t a  st   st     is positive only for states st   st     and hence     is true  if we set x      x    
then t a  st   st     is positive only for states st   st   with sti    st  
i   i          and hence    
is trivially true since its premise is false 

despite satisfying clause  ii   the state variables xt  
and xt  
from example   are in
 
 
fact not passive  for the following two reasons  firstly  passivity is a causal relation and as
such it must imply a causal order  pearl         however  there is no causal order between
x  and x    because there is no edge between xt  
and xt  
 
    secondly  passivity means that
a variable may change its value only if another variable with respect to which it is passive  a
variable in a i   changed its value  in other words  whether or not a passive variable xt  
i
may change its value depends on both the past values of a i  at time t  and the new values
of a i  at time t       however  the variables in example   only depend on the values at
time t  hence their own values at time t     are predetermined and do not depend on whether
the variables in a i change values 
the first issue  namely that of the causal order  can be addressed by adding the corresponding edges in x t     for instance  in example   we could add an edge from xt  
to xt  
 
 
to establish a causal order  however  this does not generally solve the second issue  which
is that every passive variable xit   must depend on both past and new values of the variables in a i   in other words  xit   must be both inter correlated as well as intra correlated
with the variables in a i   the former is given by definition  since every variable in a i is
a parent of xt  
i   and the latter is precisely what is required by clause  i  in definition   
therefore  clauses  i  and  ii  together define the formal meaning of passivity 
    detecting passive variables
as mentioned in section    passivity is a latent causal property in the sense that it can be
extracted from the process dynamics without additional information  and with no additional
assumptions regarding the representation of variable distributions  in order to determine if a
    

fiexploiting causality for selective belief filtering in dbns

a
algorithm   passive xt  
i    

  

a
input  state variable xt  
i   dbn 

output  a i if xt  
is passive in a   else false
i
  
t  
   q  orderedqueue p pata  xi     xti
   in ascending order of  a i  
  

  

while q     do

  

a i  nextelement q 

  

q  q    a i  

  

for all xtj  a i do


t  
if xt  
  ea then
j   xi

  
  
   
   
   
   
   
   
   

go to line      clause  i  violated
  
a i  paa  xt  
    a i  xti
i
n
o
t  
t 
t  

x
 
x
a i
j
a i
j
for all s  s a i    s  s a i    si  xi do


t  
t  s   
if pa xt  
 
s
 
x
s
 

s
 

s
i
i
a i
a i


     then
i
i
a i
go to line      clause  ii  violated
return a i
return false

variable xit   is passive in a   one has to find a set a i such that both clauses of definition  
are satisfied  a simple procedure which does this for any representation of the variable
distributions is given in algorithm    the algorithm takes as inputs a variable xt  
and a
i
t  
a
a
dbn    and checks whether xi is passive in  by searching for a set a i which satisfies
both clauses of definition    note that the power set p in line   includes the empty set  
hence it also accounts for a i     lines   to   check if clause  i  is satisfied while lines
   to    check if clause  ii  is satisfied  line    essentially checks if     holds true  if both
clauses are satisfied  then xt  
is passive in a with respect to the variables in a i   and the
i
algorithm returns the set a i   otherwise  the algorithm returns a logical false  
the time complexity of algorithm   is exponential in the worst case  in which xt  
is
i
not passive  specifically  the time requirements of line   grow exponentially with the number
of parents of xt  
in x t   and the time requirements of line    grow exponentially with the
i
cardinality of a i and a i   however  these time requirements can be reduced significantly
when committing to specific representations for the variable distributions pa   for example 
if the distributions are represented in tabular form  then one can utilise arrays of indices
to perform sweeping tests of      i e  line     moreover  it is important to realise that the
algorithm needs to be performed only once for each state variable  prior to the start of the
   strictly speaking  algorithm   checks for a property which is stronger than passivity because it does not
check for t a  st   st          cf  clause  ii   in line     however  the algorithm can be modified to include
such a check  we omit this in our exposition in order to highlight the core ideas behind the algorithm 

    

fialbrecht   ramamoorthy

process or on demand  this is since passivity is invariant of the process states  in other
words  if a variable is passive in a   then it will always be passive in a   therefore  it suffices
to check once in advance for passivity 
note that the set a i is not necessarily unique  for example  consider
a variable xt  
 

which is passive in a with respect to variables xt  and xt    i e  a     xt    xt    and assume
that xt  
changes if and only
if  x t   changes i e 
 

  they change at the same time   then  it is
 
t
  
t
easy to verify that a     x  and a     x  also satisfy clauses  i  and  ii   and hence
a      a       a   are all valid sets under our definition of passivity  the guiding principle in
such cases is occams razor  which  intuitively speaking  states that the simplest explanation
suffices  in our case  this means that it suffices to use the smallest set a i in terms of the
cardinality  a i     hence  line   in algorithm   sorts the queue q in ascending order of
 a i     the rationale is that if there exist multiple causal explanations for a passive variable
xt  
i   then the one involving the fewest key variables is to be favoured since it reduces
 compared to the alternative explanations  the number of cases in which we would have to
revise our beliefs about xit     in our earlier example  if we accept a   as a causal explanation
t  
for xt  
every time xt  
or xt  
may have
    then we would have to revise our beliefs for x 
 
 
 
changed their values  however  if we accept a   as a causal explanation  then we would
have to revise our belief for x t   only if xt  
may have changed its value  this difference
 
will become more obvious in section      which explains how passivity can be exploited to
reduce computational costs 

   passivity based selective belief filtering
this section presents the passivity based selective belief filtering  psbf  method  which
exploits passivity for efficient filtering  as discussed in section    we assume that the process
is specified as a set of dynamic bayesian networks which contains one dbn a for each
action a  a  therefore  whenever we refer to an action a  e g  t a   a   pa   paa    this is
assumed to be in the context of a  
psbf follows the general two step update procedure in which the belief state is first
propagated through the process dynamics  transition step  and then conditioned on the
observation  observation step   thus  it is natural to divide the exposition of psbf into
three parts      the belief state representation      the transition step  and     the observation
step  these are discussed in sections           and      respectively  a summary of psbf
is given in section      we also discuss the computational complexity and error bounds of
psbf in sections     and      respectively 
    belief state representation
recall from section   that the principal idea behind psbf is to maintain separate beliefs
about individual aspects of the process  and to exploit passivity in order to perform selective
updates over these separate beliefs  the union of all individual aspects constitutes a complete
state description of the process  therefore  the belief state can be represented as the product
of all separate beliefs about the individual aspects 
we capture the informal notion of individual aspects formally in the form of clusters 
which are defined as follows 
    

fiexploiting causality for selective belief filtering in dbns

c 
 t

 t  

c 

c 
 t

 t  

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

 t  

c 
 t

 t  
c 

 t

 t  

c 
 a  c    c    c 

 b  c 

 c  c    c 

figure    three clusterings for the robot arm dbn 
definition    cluster   a clustering of x t   is a set c    c         ck   which satisfies
k   ck  x t   and c        ck   x t     we refer to the elements ck  c as clusters 
the underlying idea behind the concept of clusters is that the variables in a cluster ck
are connected in some important sense  specifically  if two or more variables are in a common
cluster  then there exists some relation between these variables regarding the likelihood of
values which they may assume  in other words  the variables are correlated in x t    
the number k and the concrete choice of clusters ck can be specified by the user or
generated automatically  for example  they may be specified manually by a domain expert
who is familiar with the structure of the modelled system  or generated automatically using
methods such as the ones described in section      it should be stressed  however  that in
order to reduce computational costs  it is advisable to follow the general rule as small
as possible  as large as necessary when choosing clusters  see section     for a discussion
about computational complexity   therefore  if two variables are strongly correlated  then
they should presumably be in a common cluster  whereas if they are not or only weakly
correlated  weakly meaning that the correlation can be ignored safely   then they should
be in separate clusters in order to reduce computational costs  this is illustrated in the
following example 
example    clusters in robot dbn   recall the robot arm dbn from example    specifit   is given by the three clusters
cally figure
to cluster the
state
 t       one way
 t  
 t  
  variables in x
c     
  c     
  c      
  as shown in figure  a  this clustering is most
efficient since it minimises the size of each cluster  however  the clusters fail to capture
the important correlation that the joint orientation i is restricted by the preceding joint
orientation
i    another

  way to cluster the state variables is given by the single cluster
c     t      t      t     as shown in figure  b  this clustering captures all correlations
between variables  however  this is the largest possible cluster
the
 and  therefore 
 
 least effi 
cient one  a compromise is given by the two clusters c     t      t     c     t      t    
which are shown in figure  c  this clustering captures the correlation of the joint orientations with the immediately preceding joint orientations  and it is more efficient than the
previous clustering since it has smaller clusters 

    

fialbrecht   ramamoorthy

given the definition of clusters  we capture the informal notion of separate beliefs in
the form of belief factors 
definition    belief factor   given a cluster ck   the corresponding belief factor bk is a
probability distribution over the set s ck   
intuitively  a belief factor bk represents the agents beliefs as to the likelihood of values
for the variables in the corresponding cluster ck   an analogy to this is to view a belief factor
as a smaller belief state  and to view b as the full belief state which is a combination of
the smaller belief states  however  to distinguish the two  we refer to b simply as the belief
state and to bk as a belief factor 
finally  given the clusters ck and their corresponding belief factors bk   the belief state b
is represented in factored form as
b s   

k
y

bk  sk  

k  


 
where we use the notation sk to refer to the tuple  si  xt   ck    e g   if ck   xt  
  xt  
 
 
i
and s    s    s    s    s     then sk    s    s     
    exploiting passivity in the transition step
in order to perform selective updates over the belief factors bk   we require a procedure which
performs the transition step independently for each factor   we obtain such a procedure by
introducing two assumptions which allow us to modify the transition step     of the exact
update rule  the assumptions guarantee that the transition step is performed exactly  in the
sense of      however  as we will discuss shortly  the assumptions can be violated to obtain
approximate belief states 
the first assumption   a    states that the clusters must be uncorrelated  i e  there are
no edges in x t   between clusters   and the second assumption   a    states that the clusters
must be disjoint  formally  these are defined as follows 
t  
 a   a   xt  
 ck  pat  
a  xi    ck
i

 a   k    k     ck  ck    
note that neither assumption implies the other  that is  it may be the case that  a  
is satisfied while  a   is violated  and vice versa  assuming both  a   and  a    we can
reformulate     to
x
y
t
 
bt  
tka  s  s k  
btk   sk   
   
k  sk      
s  s pat t  ck   
a

k    xt  
ck    xti  pat t  ck   
i
a

where   is a normalisation constant and
y

 
tka  s  s k    
pa xt  
   s k  i   paa  xt  
i
i      s  sk    
xt  
 ck
i

   this also has the advantage that the belief factors can be updated in parallel  which is a useful feature
considering that many platforms use parallel processing techniques 

    

fiexploiting causality for selective belief filtering in dbns

this procedure performs the transition step independently for each belief factor bk   hence
they can be updated in any order and in parallel 
assumption  a   is what allows us to bring     into a form which updates the belief
factors bk independently of each other  specifically   a   allows us to define the cluster based
transition function tka   which in turn enables the summation in      assumption  a    on
the other hand  guarantees that the product in     is correct  in particular  it may be the
case that  sk       ck     i e  there are fewer elements in sk  than in ck    if there are variables
in ck  which are not in patat  ck    i e  xt  
 ck  but xti 
  patat  ck     in such cases  btk  is
i
t  
taken to be the marginal distribution over variables xi  ck  with xti  patat  ck    where
 a   guarantees that the marginalisation introduces no errors 
as mentioned previously  each assumption may be violated to obtain approximate belief
states  however  there is an important distinction between  a   and  a   in this regard 
if  a   is violated  then     is still well defined in the sense that it can still be executed 
except that the product in     may degrade the accuracy of the results  this is in contrast to
 a    which is a structural requirement of tka in the sense that tka is ill defined without  a   
this is since  if  a   is violated  the variables in ck may have parents in x t   which are
 
not in ck   in which case paa  xt  
i      s  sk   would be ill defined  thus  if  a   is violated 
we have to enforce it by modifying the distributions pa of all xt  
 ck to marginalise out
i
t  
all variables in pat  
 x
 
which
are
not
in
c
 
for
all
clusters
c
  this means that each
t
k
k
i
a
variable has a separate distribution for every cluster which contains the variable  thereby
possibly introducing an approximation error 
given the modified transition step      we can exploit passivity to perform selective
updates over the belief factors bk   recall from section     that a variable xt  
is passive
i
t  
a
in  if there exists a set a i of variables such that xi may change its value only if any
of the variables in a i changed its value  this causal connection can be used to decide
whether or not the values of the variables in a cluster ck may have changed  in which case the
corresponding belief factor bk should be updated  theorem   provides the formal foundation 
t

theorem    if  a   and  a   hold  and if all xt  
 ck are passive in a   then
i
t
s  s   bt  
k  sk     bk  sk   

proof  proof in appendix a 
theorem   states that if the clusters c         ck are disjoint and uncorrelated  and if all
t
variables in cluster ck are passive in a   then the transition step for the corresponding
belief factor btk  bt  
can be omitted without loss of information 
k
how does theorem   translate into situations in which  a   or  a    or both  are violated 
the key assumption is again  a    which states that the clusters must be uncorrelated  as
discussed earlier  we can enforce this by modifying the variable distributions pa in each cluster 
however  if a passive variable xit    ck is correlated with a  passive or active  variable
t  
t  
xt  
 ck    where xt  
 pat  
in the distribution pa of
a  xi    then marginalising out xj
j
j
t  
t  
xi will typically cause xi to lose its passivity  in the sense that it would no longer satisfy
the clauses in definition    consequently  we would always have to perform the transition
step for ck   even if the unmodified variables in ck are all passive  this is problematic not
only because of the unnecessary computations  but also because the modified distributions
will introduce an error every time the transition step is performed 
    

fialbrecht   ramamoorthy

 t

 t  

 t  
c 

 t

 t  

 t  
c 

 t

 t  

 t  

xt

x t  

y t  

figure    robot arm dbn implementing the action cw    dashed circles mark passive state
variables  the coloured ellipses represent the clusters c  and c   
to alleviate this effect  one can check if there is a chance that the unmodified variables
in the cluster would change their values  it can be shown that this is the case whenever there
is a causal path from any active variable to a variable in the cluster 
definition    causal path   a causal path in a   from an active variable xt  
to another
i
t  
t    q 
t  
   
   
 q 
   
variable xj   is a sequence hx   x        x i such that x   xi   x
  xj   and for
all for all    q   q  
 i  x q   x t   
 ii  x q    x q     ea
 iii  x q    is passive in a with respect to x q 
intuitively  a causal path defines a chain of causal effects  such as between joints   and  
in example     since the active variable x    may have changed its value and x    is passive
with respect to x      x    may also have changed its value  since x    may have changed
its value and x    is passive with respect to x      x    may also have changed its value  etc 
hence  in the absence of observing these changes  the mere existence of a causal path from
x    to x q  is reason to revise our beliefs about x q    therefore  as a general update rule  we
can omit the transition step btk  bt  
if all unmodified variables in cluster ck are passive
k
t
t
in a   and if there is no causal path from any active variable in a to any variable in ck  
this is demonstrated in the following example 
example    psbf update rule in robot arm dbn   let us again consider the robot arm
from the previous examples  figure   shows a dbn which implements the action cw   
this action rotates joint   of the robot arm by   clock wise  i e  the joint orientation  t  
is a direct target of the action   therefore  the variable  t   is active while the variables
 t   and  t   are passive  shown as dashed  circles   
 
we use the clustering c     t      t     c     t      t   for reasons given in example    since  t   is a parent of  t     psbf will have to enforce assumption  a   by
    

fiexploiting causality for selective belief filtering in dbns

algorithm   skippableclusters c  a  
  

input  clustering c    c         ck    dbn a

  

output  set of clusters c  c which can be skipped in transition step

  

c  c

  

q  orderedqueue x t    

  

while c      q     do

  
  
  
  
   
   
   
   
   

xt  
 nextelement q 
i

 
q  q   xt  
i
a
if passive xt  
i      then

 
c  c   ck  c   xt  
 ck
i

for all xt  
 q do
j
t  
a
if causalpath xt  
i   xj      then
n
o
c  c   ck  c   xt  
 ck
j
n
o
q  q   xt  
j

return c

marginalising  t   out of the variable distribution pa of  t   in cluster c    while the modified variable distribution loses the passivity property  both clauses of definition   are
violated   the unmodified distribution of  t   is still passive 
when performing the transition step  psbf has to update the belief factor b  because
the corresponding cluster c  contains the active variable  t     however  since all variables
in cluster c  are passive  there are no modified variables in c     and since there is no causal
path from  t   to any variable in c    psbf can omit the update for the belief factor b   
intuitively  this makes sense since a change in the orientation of joint   cannot cause a
change in the orientations of the preceding joints  note that this corresponds to a saving of
    in the transition step 


algorithm   defines a procedure which utilises this rule to find clusters for which the
transition step can be skipped  the algorithm takes as inputs a clustering c and a dbn
a   and returns a set c of skippable clusters  it essentially searches through all active
variables xt  
in a and removes all clusters ck from c which contain variables to which
i
there is a causal path from xit     the function orderedqueue x t     returns an ordered
queue q with all variables in x t     the performance of algorithm   depends on the order
of the queue  in our experiments  we obtained good performance by ordering the variables in
descending order of their number of outgoing edges  the function nextelement q  returns
a
the next element in the queue  the function passive xt  
i      is defined in algorithm   
t   t  
a
and the function causalpath xi   xj      returns a logical true if and only if there is a
    

fialbrecht   ramamoorthy

causal path from xt  
to xjt   in a    note that  given the invariance of passivity to process
i
states  cf  section       it suffices to call algorithm   only once  in advance or as needed  to
determine which of the clusters to omit in the transition step 
    efficient incorporation of observations
psbf can perform the observation step similarly to the exact update rule      which
conditions the propagated belief state bt   on the observation ot   to obtain a fully updated
belief state bt     however  given the factored belief state representation used by psbf  we
require a procedure which respects this factorisation in the observation step  assuming that
 a   and  a   both hold  we can bring     into a form which updates the belief factors bk
independently of each other
x
y
t
t    
 
 
bt  
a  s  ot    
bt  
   
k  sk       bk  sk  
k   sk  
t        s   s  k      k   c    pat    y t        
s  s pat  
k
k
t  y
t
k
a

a

where   is a normalisation constant  note that  analogously to      if there are variables
t      then bt   is taken to be the marginal distribution
in ck  which are not in pat  
k 
at  y
t  
t  
over ck   paat  y
   assumption  a   guarantees that the marginalisation introduces no
errors  if  a   and  a   both hold  then the transition step     and observation step    
produce exact belief states in the sense of     and      regardless of how many clusters were
skipped in the transition step  cf  theorem    
the observation step     updates all belief states and uses all observation variables in
the process  in other words  it ignores the internal structure of the observation variables 
however  it is clear that if the variables in a cluster ck are marginally independent of the
observation variables y t    this can be determined using d separation  geiger et al         
or simply by checking if there is a directed path from ck to y t      then there is no need
to perform the observation step for the corresponding belief factor bk   this is expressed
formally in theorem   
t

theorem    if all xt  
 ck are marginally independent of all yjt    y t   in a   then
i
t  
s  s   bt  
k  sk     bk  sk   

proof  proof in appendix b 
theorem   states that if the variables in ck are independent of those in y t     then the
observation step for bk can be skipped  however  even if ck is not independent of y t     it may
be the case that the variables in ck depend only on a subset yk  y t   of the observation
variables  clearly  in such cases  it suffices to use yk rather than y t   in the observation
step  to account for this  we first note that the variables in y t   may be correlated with
each other  to preserve the correlations  we subdivide y t   into clusters cl  y t   and
introduce the following assumptions 


 a   a   yjt    cl  paa  yjt      y t    cl
 a   l    l    cl  cl    
   a simple way to implement this function is to modify a standard graph search method  such as breath first
search  to check for  iii  in definition    and to apply it to the variables in x t   with edges ea from a  

    

fiexploiting causality for selective belief filtering in dbns

assumptions  a   and  a   are analogous to  a   and  a    respectively  and essentially
serve the same purposes for the observation step  to distinguish the clusters ck and cl  
we sometimes refer to the former as state cluster and to the latter as observation cluster 
assuming that  a   and  a   both hold  we can redefine the observation step to
x
y
y
t    
 
at
t  
 
bt  
 s
 
 

b
 s
 

 s 
o
 
bt  
   
 
k
k
l
k
k
k   sk  
t  
 
 
l  cl yk    s  s pat  
t  cl      sk   sk k    k   ck   pa t  cl      
a

where
al  s  ot  
l    

y

a



t  
t  
pa yjt     ot  
 
 
pa
 y
 
 s 
o
 
j
a j
l
l

yjt    cl

and yk  y t   is the set of observation variables which are not marginally independent of
the variables in ck  
given theorem    one can see that     is equivalent to     if the observation variables
are not clustered  or  equivalently  there is a single observation cluster cl   y t      however 
it is important to note that if the observation variables are clustered  i e  there are multiple
observation clusters cl    then     is notqnecessarily
equivalent to
p
p    
qmto see this  it is helpful
to compare the abstract formulations m

 o
 
b
and
j  
s s j s
s
j   s  oj   bs   where the
former corresponds to     and the latter to      therein   o         om    o is an observation  bs
is the probability of being in state s  s  and s  oj   is the probability of observing yj   oj
in s  these abstract formulations are equivalent for m     or if bs     for some s  but in all
other cases they may not be equivalent  nonetheless  if we fix the number of observation
variables m  then     approximates     closely as we increase the number of state variables
n  our experiments indicate that it often suffices to use just a few more state variables than
observation variables in order to obtain good approximations 
finally  to show that it suffices to perform the observation step for bk using only those
clusters cl whose variables are not independent of the variables in ck   we observe that    
is in fact a repeated application of     for every cl   where the updated belief factor bt  
is
k
t  
used in place of bk in the subsequent application  since every application has the same
form as      with y t     cl    we conclude that theorem   holds  and hence the observation
step can be skipped for clusters cl which are independent of ck  
    summary of psbf
the preceding sections can be summarised as follows 
 representation  the belief state bt is represented as a product of k belief factors btk  
q
t
t
such that bt  s    k
k   bk  s   each belief factor bk is a probability distribution over
the set s ck    where ck  x t   is a cluster of correlated state variables 
 transition step  the transition step btk  bt  
is performed using      for all clusters
k
t
a
ck which include active variables in    or to which there is a causal path from an
t
active variable in a   all other clusters are skipped 
 observation step  the observation step bt  
 bt  
is performed using      for all
k
k
clusters ck which are dependent on the observation variables y t     using only those
observation clusters cl which are relevant for ck   all other clusters are skipped 
    

fialbrecht   ramamoorthy

algorithm   psbf at   ot      btk  ck c   c  c   a  aa  
  

input  action at   observation ot     belief factors  btk  ck c

  

parameters  state clustering c  observation clustering c  dbns  a  aa

  

output  updated belief factors  bt  
k  ck c

  

   transition step

  

c  skippableclusters c  a  

  

for all ck  c do

  
  
  
   
   

t

if ck  c then
bt  
 btk
k
else
for all s k  s ck   do
x t
y
 
bt  
tka  s  s k  
btk   sk   
k  sk     
s  s patat  ck   

   

k    xt  
ck    xti  patat  ck   
i

   observation step

for all ck  c do
n
o
t
   
yk  yjt    y t     there is a directed path from ck to yjt   in a
   

   

if yk    then

   

 bt  
bt  
k
k

   
   
   

else
for all s k  s ck   do
t    
 
bt  
k  sk      bk  sk  

y

x

t

al  s  ot    

y

 
bt  
k   sk  

 cl      
cl  c   cl yk    s  s pat  
 cl      sk   s k k     k   ck   pat  
at
at

   

return

 bt  
k  ck c

algorithm   provides a procedural specification of psbf  the algorithm takes as inputs
the action at time t  at   the subsequent observation at time t      ot     and the belief factors
at time t  btk   the internal parameters are the state clustering c  the observation clustering
c  and the set of dbns  a  aa which define the process  lines   to    implement the
transition step while lines    to    implement the observation step  note that it suffices to
execute lines   and    once in advance  or on demand  and to remember the results for
future reference  the algorithm returns the updated belief factors bt  
k  
    

fiexploiting causality for selective belief filtering in dbns

    space and time complexity
a belief factor bk has one elementp
bk  sk   for each sk  s ck     thus  the total space required
to maintain k belief factors bk is k
k    s ck     furthermore  the size of the set s ck   grows
exponentially with the number of variables in ck   hence the dominant growth factor in the
space requirement is given by the largest cluster ck such that  ck     maxk   ck     therefore 
the space complexity of psbf is in o exp maxk  ck     hence the representation is feasible
for reasonably small clusters ck  
similarly  the numberpof operations required to perform the transition and observation
steps is in the order of   k
k    s ck    in the worst case  i e  all clusters need to be updated
in both steps   specifically  line    and line    in algorithm   are each executed once for
every sk  ck   the dominant growth factor is again given by the largest cluster ck   hence
the time complexity of psbf is in o   exp maxk  ck      o exp maxk  ck     note that this
assumes that the analysis performed by lines   and    in algorithm   is done in advance 
the above time complexity is for the worst case  in which all clusters need to be updated
in the transition and observation steps  it is difficult to derive the time complexity for the
average case because it is unclear what the average case is in terms of passivity  even if we
stipulate a certain average degree of passivity  e g      of all variables are passive   it would
still be difficult to make a general statement about time requirements since this depends
crucially on how the passive variables are distributed across the clusters  for example  even
if a process has on average     passivity  if there is one active variable in each cluster
then every cluster would need to be updated in the transition step  thus  the only general
statement we can make with regards to passivity is that the time complexity of psbf can
be refined to o exp maxck  ct  co  ck     where ct and co include only those clusters that
need to be updated in the transition and observation step  respectively 
    error bounds
there are five possible sources of approximation errors in psbf 
 if the clusters are correlated  i e   a   or  a   are violated 
 if the clusters are overlapping  i e   a   or  a   are violated 
 generally in     if multiple observation clusters cl are used
in the first two cases  the approximation error depends on the amount of correlation
and overlap  if there is only little correlation and overlap between the clusters  then the
approximation error can be expected to be small  conversely  if the clusters are strongly
correlated and overlapping  then the approximation error can be expected to be large 
boyen and koller        provide a useful analysis of the error bound of any filtering
method which uses a factored belief state representation  since psbf uses a factored
representation  their analysis applies directly to psbf  the purpose of this section is to
restate the main result of their analysis in the context of our work 
their analysis uses the concept of relative entropy  kullback   leibler        as a
measure of similarity for belief states 
   in practice  it suffices to store only  s ck       elements  but this is irrelevant in our analysis 

    

fialbrecht   ramamoorthy

definition    relative entropy   let  and  be two probability distributions defined over
a set x  the relative entropy from  to  is defined as
kl      

x
xx

 x  ln

 x 
 x 

where  x        x      
similar to boyen and koller         we define the approximation error incurred by psbf
relative to the exact belief state  however  since we consider a decision process with multiple
actions a  a  represented by the dbns a    we define the error for each action respectively 
definition    approximation error   let b be an exact belief state and b be the approximation by psbf  after taking action a  let b  be the exact update of b  using     and     
and b  be the psbf update of b  using     and       furthermore  let b  be the exact update
of b  using     and       we say that psbf incurs error a in a relative to b  if
kl b    b     kl b    b     a  
the analysis also relies on the concept of mixing rates  intuitively  the mixing rate  a of
a dbn a quantifies the degree of stochasticity in a   it depends on the mixing rates ka of
the individual clusters ck  
definition    mixing rate   the mixing rate of a cluster ck  x t   in a is defined as
ka    min
  

s  s s

x



min tka  s    s   tka  s     s   

ss ck  

if all ck satisfy  a   and  a    and if all observation variables y t   are in one observation
cluster  then the mixing rate of a is given by  a    mink ka  r q where each cluster ck
depends on at most r and influences at most q other clusters ck    k  boyen   koller        
in the worst case  that is  all  a a   are violated   the minimal mixing rate is given by ka
for the single cluster ck   x t    
finally  the main result in the work of boyen and koller         here restated in the
context of our work in theorem    essentially states that the approximation error of psbf
 measured in terms of relative entropy  is bounded by the mixing rates of the process 
theorem    boyen   koller         let bt be an exact belief state and bt be the approximation by psbf using clusters ck   then  for any t with states  s    s         st   and actions
 a    a         at     we have
h
i max
a
a   a 
eo       ot kl bt   bt   
mina   a  a
where the expectation e is q
taken over all possible sequences of observations o         ot with
 
t
a       o       and where a and  a are defined as above 
probabilities p  o        o     t 
      s
    

fiexploiting causality for selective belief filtering in dbns

process size

  of x vars  n 

  of y vars  m 

  of states   s  

  of obs    o  

s

  

 

  one thousand

 

m

  

 

  one million

  

l

  

 

  one billion

   

xl

  

  

  one trillion

    

table    synthetic process sizes  all variables are binary 

   experimental evaluation
we evaluated psbf in two experimental domains  in section      we evaluated psbf in
synthetic  i e  randomly generated  processes with varying sizes and degrees of passivity  in
section      we evaluated psbf in a simulation of a multi robot warehouse system  a brief
summary of the experimental results is given in section     
    synthetic processes
we first evaluated psbf in a series of synthetic processes  psbf is compared with a selection
of alternative methods  including pf  gordon et al          rbpf  doucet et al          bk
 boyen   koller         and ff  murphy   weiss         see section   for a discussion of
these methods  the algorithms were implemented in matlab       where we used the matlab
toolbox bnt  murphy        to implement bk and ff 
      specification of synthetic processes
we generated synthetic processes of four different sizes which are specified in table    each
process was generated as follows 
first  each variable xit   is chosen to be passive with probability p  in which case we
also add the edge  xti   xt  
i    we refer to p as the degree of passivity  to sample further
t
t  
edges from x  x
to x t     we generate a mixture of gaussians g using algorithm    see
appendix c   figure   shows an example of g generated for a process of size m  the set
g is used to produce areas of correlated variables  i e  the gaussians   which will then
constitute natural candidates for state clusters 
let  be the vector of maximum densities for each gaussian in g  and let i be the
vector of densities at value i  n  then  for every combination of i and j  the edge  xti   xt  
j  
 
is added with probability equal to the maximum element in i j     in which all operators
are point wise  if xt  
was chosen to be passive  then the edge  xti   xt  
i
j   is only added if
t   t  
t   t  
i   j  in that case  we also add the edge  xi   xj    edges  xi   xj   are added similarly
t  
for each i   j   where we also add the edge  xti   xt  
j   for passive xj   to ensure that every
variable has an effect in the generated process  each xti is connected to at least one xt  
j
t  
t or x t    adding
 adding  xti   xt  
 
if
necessary 
and
each
x
has
at
least
one
parent
in
x
i
j
   the condition i   j in both cases is to ensure that the resulting dbn is acyclic 

    

fialbrecht   ramamoorthy

    
   

density

    
   
    
   
    
 

 

 

 

 

  

  

  

  

  

  

i  j

figure    example of mixture of gaussians generated for a process of size m and consisting
t t  
of three gaussians  the closer two variables xi
and xt  
are under the peak of a common
j
gaussian  the higher the probability that an edge will be added between them 
t   t  
 xtj   xt  
j   if necessary   finally  edges  xi   yj   are added with probability      for each
i  j  while ensuring that each yjt   has at least one parent in x t    
all variables in the process are binary  passive variables are assumed to be passive with
respect to all of their parents in x t   the distributions pa of xt  
 x t   are generated
i
t  
uniformly randomly without bias  for passive variables xi   we modify pa to satisfy clause
 ii  in definition    the distributions pa of yjt    y t   are generated with each probability
sampled uniformly from either            or             to obtain meaningful observations 
finally  every process consists of two actions  these are obtained by randomly choosing
between one and three variables xt  
whose distributions pa are resampled as above and
i
edges from x t added with probability      passive variables chosen in this way are no longer
passive   during simulations  these actions are chosen uniformly randomly 
each process starts in a random initial state  and all algorithms are tested on the same
sequence of processes  initial states  chosen actions  and random numbers 

      clustering methods
we used three different clustering methods  denoted hpci  hmorali  and hmodisi  the methods
were applied to the variables in x t   without edges involving x t or y t    
 hpci drops the directions of the edges  i e  for any edge xt  
 xt  
it ads the reverse
i
j
t  
t  
edge xj  xi   and puts all variables between which there is a  undirected  path
into one cluster  by definition  the resulting clusters satisfy all assumptions  a a   
 hmorali connects all parents of a variable and drops the directions  it moralises the
variables  and then extracts clusters of fully connected variables  maximum cliques  
the resulting clusters may not satisfy any of the assumptions  a a   
 hmodisi is similar to hmorali but truncates the resulting clusters to make them disjoint
 clusters are removed if they become a subset of another cluster   by definition  the
resulting clusters satisfy  a  a    but not necessarily  a  a   
as an example  consider figure   from section      here  hpci would produce the cluster
c  from figure  b  since all variables are connected by an undirected path  furthermore 
    

fiexploiting causality for selective belief filtering in dbns

hmorali would produce the two clusters c  and c  from figure  c  which correspond to the
two maximum cliques after moralising the variables in x t     finally  hmodisi would produce
the cluster c  from figure  c and the cluster c  from figure  a 
psbf used the same clustering method to generate clusters of state variables  ck  
and observation variables  cl    moreover  psbf enforced  a  a   whenever necessary by
modifying the variable distributions as described in section     
      accuracy
in order to compare the accuracy of the tested algorithms  we computed the relative entropy
 cf  definition    from exact belief states obtained using the exact update rule  cf  definition   
to the approximate belief states produced by the tested algorithms  however  since exact
belief states and relative entropy are hard to compute for large processes  we were able to
compare the accuracy of algorithms in processes of size s only  all algorithms were initialised
with uniform belief states  or uniformly sampled particles 
we first compared the accuracy of psbf and bk  since they use the same factorisation
in their belief state representations  figure   shows the relative entropy of psbf and bk averaged over      processes with                         and      passivity  respectively 
the results show that psbf hpc modisi produced a lower relative entropy  i e  higher accuracy  than bk hpc modisi  and that psbf hmorali produced a relative entropy comparable
to that of bk hmorali  this indicates that violations of  a  a   introduce smaller errors
than violations of  a  a    note that psbf and bk had the same convergent behaviour in
their relative entropy  which shows that the approximation error due to the factorisation was
bounded  as discussed in section      this is interesting since psbf and bk obtain approximation errors from the factorisation in different ways  psbf loses accuracy by modifying
the variable distributions to ensure that the state clusters are independent  cf  section      
while bk loses accuracy by marginalising out the original factorisation after the inference  i e 
the projection step  cf  section       nevertheless  as shown in our results  the resulting
approximation errors were bounded in both cases  with similar convergence 
note that the relative entropy of both methods increased with the degree of passivity in
the process  this is explained by the fact that higher passivity implies higher determinacy
and  therefore  lower mixing rates  cf  definition     which are a crucial factor in the error
bounds of psbf and bk  cf  theorem     finally  note that psbf did not produce exact
belief states  i e  zero relative entropy  when using hpci clustering  despite the fact that the
clusters generated by hpci satisfy all assumptions  a a    however  as discussed in detail in
sections     and      another possible source of approximation errors is if multiple observation
clusters are used  which was often the case when using hpci to produce observation clusters 
to compare the accuracy of pf rbpf with psbf bk  the number of samples used in
pf rbpf was chosen automatically in each process such that they required approximately
as much time per belief update as psbf hmorali and bk hmorali  respectively  in our
experiments  this meant that pf  rbpf  was only able to process between     and        
and     samples  however  since each process has over      states  this was not nearly enough
to represent a uniform belief state  hence  pf rbpf produced much higher relative entropy
than psbf bk  moreover  the fact that the processes have very high variance means that
pf rbpf would require many more samples to achieve the same accuracy as psbf bk  as
    

fialbrecht   ramamoorthy

 

   
   

bk  pc 

psbf  pc 

bk  moral 

psbf  moral 

bk  modis 

psbf  modis 

relative entropy

relative entropy

   

   
 

 

   

    

    
    
transition

    

   
 
   
 

    

 

   

 a     passivity

    

    
    
transition

    

    

    

    

    

    

 b      passivity

 
relative entropy

relative entropy

 

 

 

 
 
 
 

 

 

   

    

    
    
transition

    

 

    

 

   

 

 

 

 

 

    
    
transition

 d      passivity

relative entropy

relative entropy

 c      passivity

    

 

   

    

    
    
transition

    

    

 e      passivity

 
 
 
 

 

   

    

    
    
transition

 f       passivity

figure    accuracy results for psbf and bk  plots show relative entropy from exact to
algorithms belief states  lower is better   results are averaged over      processes of size
s  n       m       where on average        of non target variables were passive  cf 
section         psbf bk used clustering methods hpci  hmorali  and hmodisi 
shown in the next section   one would expect that this latter issue was alleviated by the use
of exact inference in rbpf  cf  section       however  this is only the case if much of the
variance in the process can be captured in the marginal distributions used in the particles in
rbpf  in contrast  our synthetic processes exhibit high variance across all variables  and our
    

fiexploiting causality for selective belief filtering in dbns

automatic grouping  of state variables into sampled and exact variables still contained
much variance in the sampled variables  hence  rbpf required significantly more samples
than the number it could process in the time provided 
finally  in order to compare the accuracy of ff with psbf bk  the number of iterations
used in ff  more precisely  the number of iterations in loopy belief propagation  cf  murphy  
weiss        was chosen automatically in each process such that ff required approximately
as much time per belief update as psbf hmorali and bk hmorali  respectively  however 
while ff was often able to perform several iterations in the provided time  the resulting
relative entropy was again substantially higher than that of psbf bk  the problem is
that ff was designed for a specific class of dbn topologies  namely those containing no
edges within x t    called regular dbns by murphy   weiss         this is what allows
ff to use a fully factored representation of belief states  in which each variable is its own
belief factor  however  the processes used in our experiments have high intra correlation
between state variables  i e  many edges in x t      especially with increasing passivity  these
correlations cannot be captured in the belief state representation of ff  resulting in a
significantly higher relative entropy than psbf bk 
      timing
we measured computation times in processes of sizes s  m  l  xl with passivities of     
                respectively  psbf and bk used hmorali clustering  which seemed most
appropriate for a fair comparison since it produced consistently similar accuracy for both
algorithms  the number of samples used in pf was chosen automatically in each process
such that pf achieved an average accuracy approximately as good as that of psbf and
bk  respectively  in the final     of the process  as this involved computing exact belief
states and relative entropies  we were able to use pf in processes of size s only  we omit
rbpf and ff in this section as they were shown in the previous section to be unsuitable
for the processes we consider  psbf was tested with       and   parallel processes  which
were allocated approximately the same number of belief factors 
figures  a   d show the times for      transitions averaged over      processes  and
figure  e shows the average percentage of belief factors that were updated in the transition
and observation steps of psbf  the timing reported for psbf includes the time taken to
modify variable distributions  in case of overlapping clusters  and to detect skippable clusters
in the transition and observation steps  both of which were done once in advance for each
action  the results show that psbf was able to minimise the time requirements significantly
by exploiting passivity  first  we note that there were only marginal gains from     to    
passivity  despite the fact that psbf updated     fewer clusters in the transition step  this
is because these clusters were mostly very small  however  there were significant gains from
    to     passivity with average speed ups of      s        m        l        xl   and
   it is an open question how to group state variables into sampled and exact variables  doucet et al  
       we used a simple heuristic whereby the set of sampled variables contained all variables xt  
that
i
had no parents in x t t   or none other than xti   the remaining variables in x t   constituted the set of
exact variables  to ensure that the resulting grouping was valid for all actions  i e  dbns  in a process 
we considered edges in all involved dbns  that is  we performed the grouping over the union of ea for
all a  moreover  to improve efficiency  we further subdivided the set of exact variables into clusters of
variables that were connected by undirected edges in x t   without edges involving the sampled variables 

    

fi   
pf bk
pf psbf
bk

  

 

   

   
   
passivity

   

   

   

psbf     
psbf 
psbf 
  

    

 a  s  n     m   

 

   

   
   
passivity

seconds for      transitions

   

seconds for      transitions

seconds for      transitions

seconds for      transitions

albrecht   ramamoorthy

   
   
   
   
   
   
  

    

 

 b  m  n     m   

   

    

 c  l  n     m   

   
  updated belief factors

   
   
passivity

   
   
   
   
   
   
   
 

   

   
   
passivity

    

 d  xl  n     m    

s  trans 
s  obs 
m  trans 
m  obs 

  
  
  
l  trans 
l  obs 
xl  trans 
xl  obs 

  
 

   

   
   
passivity

    

 e  updated belief factors

figure    timing results   ad  average number of seconds required for      transitions
on a unix dual core machine with     ghz  for sizes s  m  l  xl  passivity of p  means
that on average p  of non target variables were passive  cf  section         psbf and bk
used hmorali clustering  pf was optimised for binary variables and used number of samples
to achieve accuracy of psbf and bk  respectively  psbf was run with    psbf      
 psbf        psbf    parallel processes   e  average percentage of belief factors which
were updated in the transition and observation steps  respectively 
from     to      passivity with further average speed ups of      s        m        l  
     xl   this shows that the computational gains can grow significantly with both the
degree of passivity and the size of the process 
our results show that psbf consistently outperformed bk in all process sizes  there
are two main computational savings in psbf relative to bk  firstly  by skipping over belief
factors in the transition and observation steps  and secondly  by not having to perform a
potentially expensive projection step to restore the original factorisation after the inference 
however  while the times of both algorithms grew exponentially in the size of the process 
we note that the relative difference between psbf and bk decreased significantly for lower
degrees of passivity  this is an instance of no free lunch  see section   for a discussion  
which means that psbf performs best in processes with high passivity but can suffer in
performance in processes that lack passivity  specifically  the computational overhead of
modifying variable distributions and detecting skippable belief factors does not amortise
    

fiexploiting causality for selective belief filtering in dbns

as effectively in large processes with low passivity  furthermore  with low passivity  psbf
often has to perform full transition and observation steps  i e  update all belief factors in
each step   which can be costly in large processes 
how were bk and pf affected by passivity  not surprisingly  the performance of bk was
nearly unaffected by the increasing degrees of passivity  the junction tree algorithm used in
bk benefited marginally from an increased sparsity in the process  but the computational
gains were minimal  we were at first unable to use pf as it required too many samples
 between   k and    k  to achieve comparable accuracy to psbf bk  due to the very
high variance in the processes  in order to investigate the effect of passivity on pf  we
implemented a version of pf which was strictly optimised for binary variables  interestingly 
we found that passivity had an adverse effect on the performance of pf  requiring it to use
exponentially more samples with increased passivity  see figure  a   this makes sense if we
view pf as a factored approximation method  such as psbf and bk  which means that the
analysis in section     applies  however  because pf puts all variables into a single cluster
 since it is not actually a factored method   the mixing rate of the process will be much lower
than for psbf and bk  as discussed in section      and  thus  the error bounds are less
tight  to compensate for this  pf requires significantly more samples for increased passivity 
    multi robot warehouse system
in this section  we demonstrate how passivity can occur naturally in a more complex system
and how psbf can exploit this to accelerate the filtering task  to this end  we consider
a multi robot warehouse system in the style of kiva  wurman et al          in which the
robots task is to transport goods within the warehouse  cf  figure   a  
      specification of warehouse system
figure   b shows the initial state of the warehouse simulation  the warehouse consists of
  workstations  w   w      robots  r r    and    inventory pods  i i     each robot
can move forward and backward  turn left and right  load and unload an inventory pod  if
positioned under the pod   or do nothing  as in kiva  robots can move under inventory pods
unless they are carrying a pod  in which case the other pods become obstacles  the move
and turn operations are stochastic in that the robot may move turn too far     chance  or
do nothing     chance   each robot possesses two sensors  one telling it which inventory
pod it has loaded  if any  and one for the direction it is facing  the direction sensor is noisy
in that a random direction may be reported     chance  
each robot maintains a list of tasks in the form of bring inventory pod i to workstation
w  yellow area around w  and bring inventory pod i to position  x y   how these tasks
are executed depends on the control mode  of which we use two in our simulations  
   our control modes are ad hoc and often make suboptimal decisions  however  we found that current
solution techniques for  dec  pomdps  including approximate methods  were infeasible in this setting 
nonetheless  the quality of the decisions made by our control modes largely depends on the accuracy
of the belief states  hence it is important that the belief states are updated accurately  therefore  the
control modes were sufficient for our purposes 

    

fialbrecht   ramamoorthy

 a  kiva warehouse system

 b  initial state of simulation

figure      a  kiva warehouse system  image reproduced from dandrea   wurman        
robots  orange coloured  transport shelfs with goods to and from workstations   b  initial
state of the warehouse simulation  the warehouse consists of   workstations  w   w     
robots  r r    and    inventory pods  i i    
centralised mode  a central controller maintains a belief state bt about the state of
the warehouse system  at each time t  it samples     states from bt and removes all

duplicate states  resulting in the set
p s t   s    s          it then resamples a state s  s

t

with probabilities w s     b  s    q b  sq    based on s and the current task of each
robot  it performs an a search  hart  nilsson    raphael         with manhattan
distance  in the space of joint actions to find the optimal action for each robot  after
executing their actions  the robots send their sensor readings to the controller  and the
controller updates its belief state using the sensor readings 
decentralised mode  each robot maintains its own belief state and there is no communication between the robots  the only knowledge the robots have about each other are
their current tasks  communicated by the task allocation module  at each time t  each
robot samples the set s and state s as is done in the centralised mode  treating the
other robots as static obstacles  it performs an a search based on s and its current
task to find an action at   this is repeated for each other robot r in all states sq  s 
resulting in actions ar q which are used
p to obtain distributions r   a          a is
the set of all actions  with r  a    q   ar q  a w sq    the robot then executes its action at and updates its belief state using its sensor readings and the distributions r
to average over the other robots actions 
the tasks are generated by an external scheduler in time intervals sampled from u         
each generated task is assigned to one of the robots through a sequential auction  dias  zlot 
kalra    stentz         the robots bids are calculated as their total number of steps needed
to solve all of their current tasks and the auctioned task  in a simplified model in which the
other robots are removed   averaged over all states in s  the robot with the lowest bid is
assigned the task 
    

fiexploiting causality for selective belief filtering in dbns

figure     example dbn of a smaller warehouse system consisting of only one inventory
pod  i   and two robots  r   r    the dbn implements the joint action in which r  moves
and r  turns  dashed circles mark passive state variables  the coloured areas represent the
state clusters c  to c   

      dbn topology and clustering
figure    shows an example dbn for a smaller warehouse with one inventory pod and two
robots  each inventory pod i is represented by two variables  i x and i y  which correspond
to the x and y position of the inventory pod  each robot r is represented by four variables 
r x r y for its x y position  r d for its direction  and r s for its status  the status of a
robot r is either r s    unloaded  or r s i  loaded with inventory pod i   constants such
as the size of the warehouse and the positions of the workstations are omitted in the dbn 
there are four types of clusters  the i clusters  c c   preserve the correlation that if
r is loaded with i  then i must always have the same position as r  there are two i clusters
for each  i r  pair   the r clusters  c   and s clusters  c    respectively  preserve the
correlation that no two robots can have the same position or carry the same inventory pod
 there is one r s cluster for each  ra rb  pair with a   b   and  finally  the d clusters  c  
c    psbf uses singleton observation clusters  i e  one cluster for each observation variable  
there are some differences between the dbns for the centralised and decentralised modes
 figure    uses the centralised mode   in the centralised mode  there is one dbn for each
action combination of the robots  since the controller observes all r s noise free  it can add
edges from r x r y to i x i y if r s i or remove them otherwise to simplify the inference
 thus  in figure     r  is loaded with i  and r  is unloaded   in the decentralised mode 
each robot only observes its own sensor readings  hence it can add or remove edges only for
itself  while edges for all other robots must be permanently added  this also means that the
other robots status variables  r s  must be linked to all i x i y and  therefore  included in
the i clusters  to preserve the correlation that i must have the same position as r if r is
loaded with i   moreover  since each robot only knows its own action  there is one dbn for
    

fiseconds per transition

albrecht   ramamoorthy

centralised
    decentralised
   
   
   
   
  
  
bk

psbf

pf

figure     results of the warehouse simulation  using the centralised and decentralised
control modes  timing measured on a unix dual core machine with     ghz and averaged
over    different simulations with     transitions each 
each of its own actions  and all variables associated with the other robots are active  the
distributions r defined in the previous section are used to average over their actions  
      results
we implemented psbf  bk  and pf in c   using the framework infer net  minka  winn 
guiver    knowles        to implement bk  this allowed bk to exploit sparsity in the
process and offered improved memory handling  psbf was optimised for sparsity in     and
     respectively  by summing over states s for which all btk    bt  
k  are positive  pf naturally
benefits from sparsity as it allows it to concentrate the samples on fewer states  the number
of samples used in pf was set in such a way that the controller decisions were invariant of
the random numbers used in the sampling process of pf  this was done to ensure that the
results were repeatable  finally  to maintain sparsity in the process  each probability in the
belief states lower than      was set to    all tested algorithms were initialised with an exact
belief state  shown in figure   b 
figure    shows the time per transition averaged over    different simulations with    
transitions each  the timing reported for psbf includes the time needed to modify variable
distributions  for overlapping clusters  and to detect skippable belief factors for the transition
and observation steps  both of which were done once on demand for every previously unseen
dbn  in the centralised mode  psbf was able to outperform bk on average by     and
pf by      pf needed        samples to produce consistent  i e  repeatable  results  in
the decentralised mode  psbf outperformed bk on average by     and pf by      pf
now needed        samples to produce consistent results  due to the increased variance in
the process  all differences were statistically significant  based on paired t tests with a   
significance level  note that psbf and bk were slower in the decentralised mode since the
corresponding dbns had much higher inter connectivity  in addition  psbf updated more
belief factors since there were more active variables 
as expected  psbf was able to exploit the high degree of passivity in the process to
accelerate the filtering task  in many cases  this meant that psbf needed to update less than
half of the belief factors  precisely how many belief factors had to be updated depends on the
    

fiexploiting causality for selective belief filtering in dbns

performed action  to illustrate this  consider the smaller warehouse dbn shown in figure   
 for the centralised mode   in which r  is moving and r  is turning  here  r  x  r  y  and
r  d are active variables while all other variables are passive  dashed circles   corresponding
to a passivity of      in this dbn  psbf updates the belief factors corresponding to clusters
c   c   c   and c   since they each contain active variables  and it also updates the belief
factors for c  and c   since there are directed paths from active variables  r  x and r  y 
to each of them  therefore  the only factors which are not updated are for c  and c   now
consider the full warehouse in our experiment  which contains    inventory pods and   robots 
resulting in    variables with     i clusters    r clusters    s clusters  and   d clusters 
assume a similar situation in which one robot moves with an inventory pod  say r  with i  
while the r   turn  in this case  psbf updates only   of   r clusters  those containing
r      of   s clusters  since no status change     of   d clusters  for r     and    of    
i clusters     i clusters containing r  plus   i clusters from r   for i    amounting to a
total saving of        of belief factors which do not need to be updated 
the number of states in the warehouse system  including invalid states  exceeded     
states  therefore  we were unable to compare the accuracy of the tested algorithms in terms
of relative entropy  instead  we compared their accuracy based on the results of the task
auctions and the number of completed tasks by the end of each simulation  this gives a
good indication of the algorithms accuracy  since both the outcome of the auction and the
number of completed tasks depend on the accuracy of the belief states  in the centralised
mode  the algorithms generated over     identical task auctions and completed       bk  
      psbf   and       pf  tasks on average  in the decentralised mode  they generated
over     identical auctions and completed       bk         psbf   and       pf  tasks on
average  in both modes  none of these differences were statistically significant  therefore 
this indicates that psbf achieved an accuracy similar to that of bk and pf 
    summary of experimental evaluation
the experimental results show that psbf produces belief states with competitive accuracy 
in the synthetic processes  psbf achieved an accuracy which on average was better or
comparable to the accuracy of the alternative methods  in the warehouse system  psbf
was able to complete a statistically equivalent number of tasks as compared to the other
methods  which indicates that its accuracy was equivalent or comparable 
furthermore  the experimental results show that psbf performed the belief updates
significantly faster than the alternative methods  in the synthetic processes  psbf using no
parallel processes outperformed bk by up to     in the largest process  xl   while pf took
too much time to achieve an accuracy comparable to psbf  in particular  the results show
that the computational gains can grow significantly with both the degree of passivity and the
size of the process  in the warehouse system  psbf outperformed the alternative methods by
up to      which is a substantial saving considering the size of the state space  more than
     states   furthermore  the computational gains where much higher in the centralised
control mode than in the decentralised control mode  since the latter had a significantly
lower degree of passivity  therefore  this again shows that high degrees of passivity can bear
great potential for the filtering task 
    

fialbrecht   ramamoorthy

   no free lunch for psbf
our view is that no belief filtering method is generally suited for all types of processes 
instead  each method assumes a certain structure in the process  explicitly or implicitly 
which it attempts to exploit in order to render the filtering task more tractable  typically  the
methods are tailored in such a way with respect to this structure that they perform well if the
structure is present in the process  but suffer a significant loss in performance if the structure
is absent  for instance  pf works best in processes with low degrees of uncertainty  since
this means that fewer state samples are needed for acceptable approximations  on the other
hand  the number of samples needed for acceptable approximations can grow substantially
with the degree of uncertainty in the process  as shown in our experiments   as another
example  bk works best in processes with little correlation between state variables  since
this means that the belief factors will be small and can be processed efficiently  however  if
there are many variables which are strongly correlated  then bk typically becomes infeasible 
therefore  these structural assumptions have to be taken into account when choosing a
filtering method for a specific process 
a formal account of this view is given by the no free lunch theorems  wolpert
  macready              which state that  intuitively speaking  any two algorithms have
equivalent performance when averaged over all possible instances of the problem  in other
words  if there are classes of problem instances for which algorithm a has better performance
than algorithm b  then there must be other classes of problem instances for which a has
worse performance than b  then  the question is  for what class of problem instances  that
is  processes  can psbf be expected to achieve good performance  this class is essentially
described by the following three criteria 
degree of passivity  psbf attempts to accelerate the filtering task by omitting the
transition step for as many belief factors as possible  this depends on the passivity of
the variables in the state clusters  in the ideal case  the process exhibits a high degree
of passivity such that psbf can omit the transition step for many belief factors  in
the worst case  the process has no passive variables at all  and psbf has to update all
belief factors in the transition step  however  as discussed in section      a high degree
of passivity is not necessarily sufficient to infer that many clusters can be skipped
in the transition step  since the passive variables could be distributed in such a way
that no cluster can be skipped  e g  if the passive variables are distributed uniformly
amongst the state clusters   therefore  in an optimal case  the passivity is concentrated
on correlated state variables such that passive variables end up in the same clusters 
size of state clusters  the space and time complexity of the belief state representation
in psbf is exponential in the size of the largest state cluster  cf  section       therefore 
in the ideal case  the relevant variable correlations can be captured in small state
clusters and the cost of storing the belief factors and performing the update procedures
is small  in the worst case  large state clusters are required to retain the variable
correlations and the cost of storing and updating belief factors is large  another reason
why the state clusters should be small is because of the way in which psbf performs
the transition step  one pre requisite for omitting the transition step for a belief factor
is that all variables in the corresponding cluster are passive  if there are many variables
    

fiexploiting causality for selective belief filtering in dbns

in one cluster  then it is less likely that all variables in the cluster are passive  and 
therefore  it is less likely that the cluster can be skipped 
structure of observations  a third criterion  though arguably less important than the
other criteria  is the structure of the observations  i e  the way in which the observation
variables depend on the state variables  and the size of the observation clusters  cl   
psbf attempts to accelerate the observation step by skipping over all those state
clusters whose variables are structurally independent of the observation  and  if a
cluster cannot be skipped  by incorporating only those observation clusters which are
relevant to the update  therefore  in the ideal case  only a fraction of the state clusters
depend on the observation  and the relevant correlations between observation variables
can be captured in small observation clusters  in the worst case  all state clusters
depend on the observation in some sense  and the structure of the observation does
not allow for an efficient clustering 
thus  in summary  psbf is most suitable for processes with high degrees of passivity and
in which the relevant variable correlations can be captured in small state and observation
clusters  on the other hand  psbf may not be suitable if there is no or only low degrees
of passivity  and if large state and observation clusters are necessary to retain the relevant
variable correlations in the process 
in addition to identifying the class of processes for which a filtering method is suitable  it
is also important to justify the practical relevance of this class  in this work  we are interested
in robotic and other physical decision processes  as shown by our examples and experiments  
such systems typically exhibit a number of features  first of all  robotic systems usually
have some causal structure  e g  mainzer        pearl         passivity  as a specific type of
causality  can be observed in many robotic systems  including the robot arm used in our
examples and the multi robot warehouse system in section      furthermore  robotic systems
most typically have a modular structure  in which each module is responsible for a specific
subtask and may interact with other modules  this modular structure often allows for an
efficient clustering  in the sense that each module corresponds to a cluster of correlated state
variables  finally  the sensors used in robotic systems typically only provide information
about certain aspects of the system  and some components of the system may not benefit
from some of the sensor information  in other words  there are independencies between state
and observation variables  these features correspond to the criteria  above  which specify
the class of processes for which psbf is a suitable filtering method  therefore  we believe
that this class is practically justified 

   conclusion
inferring the state of a stochastic process can be a difficult technical challenge in complex
systems with large state spaces  the key to developing efficient solutions is to identify special
structure in the process  e g  in the topology and parameterisation of dynamic bayesian
networks  which can be leveraged to render the filtering task more tractable 
to this end  the present article explored the idea of automatically detecting and exploiting
causal structure in order to accelerate the belief filtering task  we considered a specific type of
causal relation  termed passivity  which pertains to how state variables cause changes in other
    

fialbrecht   ramamoorthy

state variables  to demonstrate the potential of exploiting passivity  we developed a novel
filtering method  psbf  which uses a factored belief state representation and exploits passivity
to perform selective updates over the belief factors  psbf produces exact belief states under
certain assumptions and approximate belief states otherwise  we showed empirically  in
synthetic processes with varying sizes and degrees of passivity as well as in an example of a
complex multi robot system  that psbf can be faster than several alternative methods while
achieving competitive accuracy  in particular  our results showed that the computational
gains can grow significantly with the size of the process and the degree of passivity 
our work demonstrates that if a system exhibits much causal structure  then there can
be great potential in exploiting this structure to render the filtering task more tractable  in
particular  our experiments support our initial hypothesis that factored beliefs and passivity
can be a useful combination in large processes  this insight is relevant for complex processes
with high degrees of causality  such as robots used in homes  offices  and industrial factories 
where the filtering task may constitute a major impediment due to the often very large state
space of the system 
there are several potential directions for future work  for example  it would be useful
to know if the definition of passivity could be relaxed such that more variables fall under
this definition  and such that the principal idea behind psbf is still applicable  one such
relaxation could be in the form of approximate passivity  which allows for small probabilities
that passive variables change values even if the relevant parents remain unchanged  in
addition  it would be interesting to know if the idea of performing selective updates over
belief factors  via passivity  could also be applied to other existing methods that use a
factored belief state representation  cf  section       finally  another useful avenue for future
work would be to formulate additional types of causal relations which can be exploited in
ways similar to how psbf exploits passivity  or perhaps in ways other than that 

acknowledgements
this article is the result of a long debate on the presented topic  and in the process benefited
from a number of discussions and suggestions  in particular  the authors wish to thank
anonymous reviewers from the nips   and uai   conferences as well as the journal of ai
research  attendees of the workshop on advances in causal inference held at uai    and
our colleagues in the school of informatics at the university of edinburgh  furthermore  the
authors acknowledge the financial support of the german national academic foundation 
the uk engineering and physical sciences research council  grant number ep h          
and the european commission  tomsy grant agreement         

    

fiexploiting causality for selective belief filtering in dbns

appendix a  proof of theorem  
to prove theorem    it will be useful to first establish the following lemma 
lemma    if  a   holds and all xt  
 ck are passive in a   then
i
s  s    tka  s  s k        sk   s k  
proof 
  the fact of  a   means that a i  ck for all xt  
 ck   since all xt  
 ck are
i
i
passive in a   it follows that all xtj  a i are passive in a   for all a i   therefore  given
tka  s  s k       and clause  ii  in definition    it follows that sk   s k  
  follows directly by  a   and the fact that all xt  
 ck are passive in a  
i

using lemma    we can give a compact proof of theorem   
t

theorem    if  a   and  a   hold  and if all xt  
 ck are passive in a   then
i
t
s   bt  
k  sk     bk  sk   

proof 
 
bt  
k  sk  

 

 

x

t

tka  s  s k  

s  s pat t  ck   

 

 

x

btk   sk   

k    xt  
ck    xti  pat t  ck   
i

a

lem 

y

a

t

tka  s  s k  

y

btk   sk   

s  s pat t  ck    sk  s k k    xt  
ck    xti  pat t  ck   
i
a
a

 

  btk  sk  

x

t

tka  s  s k  

y

btk   sk   

s  s pat t  ck    sk  s k k     k  xt  
ck    xti  pat t  ck   
i
a
a

 z

 

 a  

   

 

  btk  sk  

 

btk  sk           since btk normalised 

    

 

fialbrecht   ramamoorthy

appendix b  proof of theorem  
to prove theorem    we first note the following proposition 

t

proposition    if all xt  
 ck are marginally independent of all yjt    y t   in a   then
i

s  s    k    k sk    s k   a  s  ot     a  s    ot   

this proposition follows directly by definition 

using proposition    we can give a compact proof of theorem   
t

theorem    if all xt  
 ck are marginally independent of all yjt    y t   in a   then
i
t  
s   bt  
k  sk     bk  sk   

proof 
x

t    
 
bt  
k  sk       bk  sk  

t

y

a  s  ot    

 
bt  
k   sk  

t        s   s  k      k   c    pat    y t        
s  s pat  
k
k
t
t  y
k
a

a

 

prop 

 z

  constant   independent of

 

bt    s k   
p k t     
s   bk  sk   
k

 

bt    s k  
p k t     
s   bk  sk  
k

 
  bt  
k  sk   

    

 
s k

fiexploiting causality for selective belief filtering in dbns

appendix c  mixture of gaussians
algorithm   provides a simple procedure that randomly generates a mixture of gaussians
 i e  a set of normal distributions  for the synthetic processes in section      the algorithm
takes as input the number n of state variables and returns a set g of gaussians whose means
are in the set          n   the number of gaussians  their means  and their variances are
chosen automatically so as to achieve good coverage of state variables while minimising
the  visual  overlap of gaussians  see figure   for an example 

algorithm   mixtureofgaussians n 
  

input  number of state variables n

  

parameters       min      max 

  

output  mixture of gaussians g

  

g

  

r            n  

  

while r     do

n
  

  

r  next element of r

  

r  r    r 

  

  r drand   r e     rand returns random number from       

   

    min   r     r  r     

   
   

  min max   max min   rand    

 
g  g             mean and variance of gaussian

   

r   r     r          r p   such that r p      

   

r    r q   r q            r  r    such that r q       

   

if r     then

   
   
   
   

r  r   r  
if r      then

r  r   r   
return g

    

fialbrecht   ramamoorthy

references
astrom  k          optimal control of markov processes with incomplete state information 
journal of mathematical analysis and applications             
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research         
    
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence in bayesian networks  in proceedings of the   th conference on uncertainty in
artificial intelligence  pp         
boyen  x     koller  d          tractable inference for complex stochastic processes  in
proceedings of the   th conference on uncertainty in artificial intelligence  pp       
boyen  x     koller  d          exploiting the architecture of dynamic systems  in proceedings
of the   th national conference on artificial intelligence  pp         
brafman  r          a heuristic variable grid solution method for pomdps  in proceedings
of the   th national conference on artificial intelligence  pp         
dandrea  r     wurman  p          future challenges of coordinating hundreds of autonomous vehicles in distribution facilities  in proceedings of the ieee international
conference on technologies for practical robot applications  pp       
dean  t     kanazawa  k          a model for reasoning about persistence and causation 
computational intelligence            
dias  m   zlot  r   kalra  n     stentz  a          market based multirobot coordination  a
survey and analysis  proceedings of the ieee                   
doucet  a   de freitas  n     gordon  n          sequential monte carlo methods in practice 
springer science   business media 
doucet  a   de freitas  n   murphy  k     russell  s          rao blackwellised particle
filtering for dynamic bayesian networks  in proceedings of the   th conference on
uncertainty in artificial intelligence  pp         
geiger  d   verma  t     pearl  j          d separation  from theorems to algorithms  in
proceedings of the  th conference on uncertainty in artificial intelligence  pp     
    
gordon  n   salmond  d     smith  a          novel approach to nonlinear non gaussian
bayesian state estimation  in iee proceedings f  radar and signal processing   vol 
     pp         
hart  p   nilsson  n     raphael  b          a formal basis for the heuristic determination
of minimum cost paths  in ieee transactions on systems science and cybernetics 
vol     pp         
hauskrecht  m          value function approximations for partially observable markov
decision processes  journal of artificial intelligence research           
    

fiexploiting causality for selective belief filtering in dbns

heckerman  d          causal independence for knowledge acquisition and inference  in
proceedings of the  th conference on uncertainty in artificial intelligence  pp     
    
heckerman  d     breese  j          a new look at causal independence  in proceedings of
the   th conference on uncertainty in artificial intelligence  pp         
kaelbling  l   littman  m     cassandra  a          planning and acting in partially
observable stochastic domains  artificial intelligence                 
koller  d     friedman  n          probabilistic graphical models  principles and techniques 
the mit press 
kullback  s     leibler  r          on information and sufficiency  the annals of mathematical statistics               
lauritzen  s     spiegelhalter  d          local computations with probabilities on graphical
structures and their application to expert systems  journal of the royal statistical
society  series b  methodological                  
lovejoy  w          computationally feasible bounds for partially observed markov decision
processes  operations research             
mainzer  k          causality in natural  technical  and social systems  european review 
           
minka  t   winn  j   guiver  j     knowles  d          infer net       microsoft research
cambridge  http   research microsoft com infernet 
murphy  k          the bayes net toolbox for matlab  computing science and statistics 
                  https   code google com p bnt  
murphy  k     weiss  y          the factored frontier algorithm for approximate inference in
dbns  in proceedings of the   th conference on uncertainty in artificial intelligence 
pp         
murphy  k          dynamic bayesian networks  representation  inference and learning 
ph d  thesis  university of california  berkeley 
ng  b   peshkin  l     pfeffer  a          factored particles for scalable monitoring  in
proceedings of the   th conference on uncertainty in artificial intelligence  pp     
    
pasula  h   zettlemoyer  l     kaelbling  l          learning symbolic models of stochastic
domains  journal of artificial intelligence research             
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann 
pearl  j          causality  models  reasoning  and inference  cambridge university press 
pineau  j   gordon  g     thrun  s          point based value iteration  an anytime algorithm
for pomdps  in proceedings of the   th international joint conference on artificial
intelligence  vol      pp           
poole  d     zhang  n          exploiting contextual independence in probabilistic inference 
journal of artificial intelligence research             
    

fialbrecht   ramamoorthy

poupart  p     boutilier  c          value directed belief state approximation for pomdps 
in proceedings of the   th conference on uncertainty in artificial intelligence  pp 
       
poupart  p     boutilier  c          vector space analysis of belief state approximation
for pomdps  in proceedings of the   th conference on uncertainty in artificial
intelligence  pp         
poupart  p     boutilier  c          value directed compression of pomdps  in advances
in neural information processing systems  pp           
roy  n   gordon  g     thrun  s          finding approximate pomdp solutions through
belief compression  journal of artificial intelligence research          
smith  t     simmons  r          point based pomdp algorithms  improved analysis and
implementation  in proceedings of the   st conference on uncertainty in artificial
intelligence  pp         
sondik  e          the optimal control of partially observable markov processes  ph d 
thesis  stanford university 
srinivas  s          a generalization of noisy or model  in proceedings of the  th conference
on uncertainty in artificial intelligence  pp         
washington  r          bi pomdp  bounded  incremental partially observable markovmodel planning  in recent advances in ai planning  pp          springer 
wolpert  d     macready  w          no free lunch theorems for search  tech  rep  sfi tr           santa fe institute 
wolpert  d     macready  w          no free lunch theorems for optimization  ieee
transactions on evolutionary computation              
wurman  p   dandrea  r     mountz  m          coordinating hundreds of cooperative 
autonomous vehicles in warehouses  ai magazine            
zhang  n     poole  d          exploiting causal independence in bayesian network inference 
journal of artificial intelligence research            
zhou  r     hansen  e          an improved grid based approximation algorithm for
pomdps  in proceedings of the   th international joint conference on artificial
intelligence  pp         

    

fi