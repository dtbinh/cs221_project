journal of artificial intelligence research                  

submitted        published      

automatic description generation from images  a survey
of models  datasets  and evaluation measures
raffaella bernardi

bernardi disi unitn it

university of trento  italy

ruket cakici

ruken ceng metu edu tr

middle east technical university  turkey

desmond elliott

d elliott uva nl

university of amsterdam  netherlands

aykut erdem
erkut erdem
nazli ikizler cinbis

aykut cs hacettepe edu tr
erkut cs hacettepe edu tr
nazli cs hacettepe edu tr

hacettepe university  turkey

frank keller

keller inf ed ac uk

university of edinburgh  uk

adrian muscat

adrian muscat um edu mt

university of malta  malta

barbara plank

bplank cst dk

university of copenhagen  denmark

abstract
automatic description generation from natural images is a challenging problem that
has recently received a large amount of interest from the computer vision and natural language processing communities  in this survey  we classify the existing approaches based
on how they conceptualize this problem  viz   models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational
space  we provide a detailed review of existing models  highlighting their advantages and
disadvantages  moreover  we give an overview of the benchmark image datasets and the
evaluation measures that have been developed to assess the quality of machine generated
image descriptions  finally we extrapolate future directions in the area of automatic image
description generation 

   introduction
over the past two decades  the fields of natural language processing  nlp  and computer
vision  cv  have seen great advances in their respective goals of analyzing and generating
text  and of understanding images and videos  while both fields share a similar set of methods rooted in artificial intelligence and machine learning  they have historically developed
separately  and their scientific communities have typically interacted very little 
recent years  however  have seen an upsurge of interest in problems that require a
combination of linguistic and visual information  a lot of everyday tasks are of this nature 
e g   interpreting a photo in the context of a newspaper article  following instructions in
conjunction with a diagram or a map  understanding slides while listening to a lecture  in
c
    
ai access foundation  all rights reserved 

fibernardi et al 

addition to this  the web provides a vast amount of data that combines linguistic and visual
information  tagged photographs  illustrations in newspaper articles  videos with subtitles 
and multimodal feeds on social media  to tackle combined language and vision tasks and to
exploit the large amounts of multimodal data  the cv and nlp communities have moved
closer together  for example by organizing workshops on language and vision that have been
held regularly at both cv and nlp conferences over the past few years 
in this new language vision community  automatic image description has emerged as a
key task  this task involves taking an image  analyzing its visual content  and generating
a textual description  typically a sentence  that verbalizes the most salient aspects of the
image  this is challenging from a cv point of view  as the description could in principle
talk about any visual aspect of the image  it can mention objects and their attributes  it
can talk about features of the scene  e g   indoor outdoor   or verbalize how the people
and objects in the scene interact  more challenging still  the description could even refer to
objects that are not depicted  e g   it can talk about people waiting for a train  even when
the train is not visible because it has not arrived yet  and provide background knowledge
that cannot be derived directly from the image  e g   the person depicted is the mona lisa  
in short  a good image description requires full image understanding  and therefore the
description task is an excellent test bed for computer vision systems  one that is much more
comprehensive than standard cv evaluations that typically test  for instance  the accuracy
of object detectors or scene classifiers over a limited set of classes 
image understanding is necessary  but not sufficient for producing a good description 
imagine we apply an array of state of the art detectors to the image to localize objects
 e g   felzenszwalb  girshick  mcallester    ramanan        girshick  donahue  darrell 
  malik         determine attributes  e g   lampert  nickisch    harmeling        berg 
berg    shih        parikh   grauman         compute scene properties  e g   oliva  
torralba        lazebnik  schmid    ponce         and recognize human object interactions  e g   prest  schmid    ferrari        yao   fei fei         the result would be a
long  unstructured list of labels  detector outputs   which would be unusable as an image
description  a good image description  in contrast  has to be comprehensive but concise
 talk about all and only the important things in the image   and has to be formally correct 
i e   consists of grammatically well formed sentences 
from an nlp point of view  generating such a description is a natural language generation  nlg  problem  the task of nlg is to turn a non linguistic representation into
human readable text  classically  the non linguistic representation is a logical form  a
database query  or a set of numbers  in image description  the input is an image representation  e g   the detector outputs listed in the previous paragraph   which the nlg
model has to turn into sentences  generating text involves a series of steps  traditionally
referred to as the nlp pipeline  reiter   dale         we need to decide which aspects
of the input to talk about  content selection   then we need to organize the content  text
planning  and verbalize it  surface realization   surface realization in turn requires choosing the right words  lexicalization   using pronouns if appropriate  referential expression
generation   and grouping related information together  aggregation  
in other words  automatic image description requires not only full image understanding 
but also sophisticated natural language generation  this is what makes it such an interesting
   

fiautomatic description generation from images  a survey

task that has been embraced by both the cv and the nlp communities   note that
the description task can become even more challenging when we take into account that
good descriptions are often user specific  for instance  an art critic will require a different
description than a librarian or a journalist  even for the same photograph  we will briefly
touch upon this issue when we talk about the difference between descriptions and captions
in section   and discuss future directions in section   
given that automatic image description is such an interesting task  and it is driven by the
existence of mature cv and nlp methods and the availability of relevant datasets  a large
image description literature has appeared over the last five years  the aim of this survey
article is to give a comprehensive overview of this literature  covering models  datasets  and
evaluation metrics 
we sort the existing literature into three categories based on the image description
models used  the first group of models follows the classical pipeline we outlined above  they
first detect or predict the image content in terms of objects  attributes  scene types  and
actions  based on a set of visual features  then  these models use this content information
to drive a natural language generation system that outputs an image description  we will
term these approaches direct generation models 
the second group of models cast the problem as a retrieval problem  that is  to create a
description for a novel image  these models search for images in a database that are similar to
the novel image  then they build a description for the novel image based on the descriptions
of the set of similar images that was retrieved  the novel image is described by simply
reusing the description of the most similar retrieved image  transfer   or by synthesizing a
novel description based on the description of a set of similar images  retrieval based models
can be further subdivided based on what type of approach they use to represent images
and compute similarity  the first subgroup of models uses a visual space to retrieve images 
while the second subgroup uses a multimodal space that represents images and text jointly 
for an overview of the models that will be reviewed in this survey  and which category they
fall into  see table   
generating natural language descriptions from videos presents unique challenges over
and above image based description  as it additionally requires analyzing the objects and
their attributes and actions in the temporal dimension  models that aim to solve description generation from videos have been proposed in the literature  e g   khan  zhang   
gotoh        guadarrama  krishnamoorthy  malkarnenkar  venugopalan  mooney  darrell 
  saenko        krishnamoorthy  malkarnenkar  mooney  saenko    guadarrama       
rohrbach  qiu  titov  thater  pinkal    schiele        thomason  venugopalan  guadarrama  saenko    mooney        rohrbach  rohrback  tandon    schiele        yao  torabi 
cho  ballas  pal  larochelle    courville        zhu  kiros  zemel  salakhutdinov  urtasun 
torralba    fidler         however  most existing work on description generation has used
static images  and this is what we will focus on in this survey  
in this survey article  we first group automatic image description models into the three
categories outlined above and provide a comprehensive overview of the models in each
   though some image description approaches circumvent the nlg aspect by transferring human authored
descriptions  see sections     and     
   an interesting intermediate approach involves the annotation of image streams with sequences of sentences  see the work of park and kim        

   

fibernardi et al 

category in section    we then examine the available multimodal image datasets used for
training and testing description generation models in section    furthermore  we review
evaluation measures that have been used to gauge the quality of generated descriptions in
section    finally  in section    we discuss future research directions  including possible
new tasks related to image description  such as visual question answering 

   image description models
generating automatic descriptions from images requires an understanding of how humans
describe images  an image description can be analyzed in several different dimensions  shatford        jaimes   chang         we follow hodosh  young  and hockenmaier       
and assume that the descriptions that are of interest for this survey article are the ones
that verbalize visual and conceptual information depicted in the image  i e   descriptions
that refer to the depicted entities  their attributes and relations  and the actions they are
involved in  outside the scope of automatic image description are non visual descriptions 
which give background information or refer to objects not depicted in the image  e g   the
location at which the image was taken or who took the picture   also  not relevant for
standard approaches to image description are perceptual descriptions  which capture the
global low level visual characteristics of images  e g   the dominant color in the image or
the type of the media such as photograph  drawing  animation  etc   
in the following subsections  we give a comprehensive overview of state of the art approaches to description generation  table   offers a high level summary of the field  using
the three categories of models outlined in the introduction  direct generation models  retrieval models from visual space  and retrieval model from multimodal space 
    description as generation from visual input
the general approach of the studies in this group is to first predict the most likely meaning
of a given image by analyzing its visual content  and then generate a sentence reflecting
this meaning  all models in this category achieve this using the following general pipeline
architecture 
   computer vision techniques are applied to classify the scene type  to detect the objects present in the image  to predict their attributes and the relationships that hold
between them  and to recognize the actions taking place 
   this is followed by a generation phase that turns the detector outputs into words or
phrases  these are then combined to produce a natural language description of the
image  using techniques from natural language generation  e g   templates  n grams 
grammar rules  
the approaches reviewed in this section perform an explicit mapping from images to
descriptions  which differentiates them from the studies described in section     and     
which incorporate implicit vision and language models  an illustration of a sample model is
shown in figure    an explicit pipeline architecture  while tailored to the problem at hand 
constrains the generated descriptions  as it relies on a predefined sets of semantic classes of
scenes  objects  attributes  and actions  moreover  such an architecture crucially assumes
   

fiautomatic description generation from images  a survey

reference

generation

farhadi et al        
kulkarni et al        
li et al        
ordonez et al        
yang et al        
gupta et al        
kuznetsova et al        
mitchell et al        
elliott and keller       
hodosh et al        
gong et al        
karpathy et al        
kuznetsova et al        
mason and charniak       
patterson et al        
socher et al        
verma and jawahar       
yatskar et al        
chen and zitnick       
donahue et al        
devlin et al        
elliott and de vries       
fang et al        
jia et al        
karpathy and fei fei       
kiros et al        
lebret et al        
lin et al        
mao et al       a 
ortiz et al        
pinheiro et al        
ushiku et al        
vinyals et al        
xu et al        
yagcioglu et al        

retrieval from
visual space multimodal space
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x

x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x

x
x
x

table    an overview of existing approaches to automatic image description  we have
categorized the literature into approaches that directly generate a description of an image
 section       approaches that retrieve images via visual similarity and transfer their description to the new image  section       and approaches that frame the task as retrieving
descriptions and images from a multimodal space  section      

   

fibernardi et al 

figure    the automatic image description generation system proposed by kulkarni et al 
       
the accuracy of the detectors for each semantic class  an assumption that is not always met
in practice 
approaches to description generation differ along two main dimensions   a  which image
representations they derive descriptions from  and  b  how they address the sentence generation problem  in terms of the representations used  existing models have conceptualized
images in a number of different ways  relying on spatial relationships  farhadi et al         
corpus based relationships  yang et al          or spatial and visual attributes  kulkarni
et al          another group of papers utilizes an abstract image representation in the
form of meaning tuples which capture different aspects of an image  the objects detected 
the attributes of those detections  the spatial relations between them  and the scene type
 farhadi et al         yang et al         kulkarni et al         li et al         mitchell et al  
       more recently  yatskar et al         proposed to generate descriptions from denselylabeled images  which incorporate object  attribute  action  and scene annotations  similar
in spirit is the work by fang et al          which does not rely on prior labeling of objects 
attributes  etc  rather  the authors train word detectors directly from images and their
associated descriptions using multi instance learning  a weakly supervised approach for the
training of object detectors   the words returned by these detectors are then fed into a
language model for sentence generation  followed by a re ranking step 
the first framework to explicitly represent how the structure of an image relates to
the structure of its description is the visual dependency representations  vdr  method
proposed by elliott and keller         a vdr captures the spatial relations between the
objects in an image in the form of a dependency graph  this graph can then be related to the
syntactic dependency tree of the description of the image   while initial work using vdrs
has relied on a corpus of manually annotated vdrs for training  more recent approaches
induce vdrs automatically based on the output of an object detector  elliott   de vries 
      or the labels present in abstract scenes  ortiz et al           the idea of explicitly
representing image structure and using it for description generation has been picked up
   vdrs have proven useful not only for description generation  but also for image retrieval  elliott 
lavrenko    keller        
   abstract scenes are schematic images  typically constructed using clip art  they are employed to avoid
the need for an object detector  as the labels and positions of all objects are know  an example is zitnick
and parikhs        dataset  see section   for details 

   

fiautomatic description generation from images  a survey

by lin et al          who parse images into scene graphs  which are similar to vdrs and
represent the relations between the objects in a scene  they then generate from scene
graphs using a semantic grammar  
existing approaches also vary along the second dimension  viz   in how they approach
the sentence generation problem  at the one end of the scale  there are approaches that use
n gram based language models  examples include the works by kulkarni et al         and
li et al          which both generate descriptions using n gram language models trained on
a subset of wikipedia  these approaches first determine the attributes and relationships
between regions in an image as regionprepositionregion triples  the n gram language
model is then used to compose an image description that is fluent  given the language model 
the approach of fang et al         is similar  but uses a maximum entropy language model
instead of an n gram model to generate descriptions  this gives the authors more flexibility
in handling the output of the word detectors that are at the core of their model 
recent image description work using recurrent neural networks  rnns  can also be
regarded as relying on language modeling  a classical rnn is a language model  it captures
the probability of generating a given word in a string  given the words generated so far  in
an image description setup  the rnn is trained to generate the next word given not only
the string so far  but also a set of image features  in this setting  the rnn is therefore not
purely a language model  as in the case of an n gram model  for instance   but it is a hybrid
model that relies on a representation that incorporates both visual and linguistic features 
we will return to this in more detail in section     
a second set of approaches use sentence templates to generate descriptions  these
are  typically manually  pre defined sentence frames in which open slots need to be filled
with labels for objects  relations  or attributes  for instance  yang et al         fill in
a sentence template by selecting the likely objects  verbs  prepositions  and scene types
based on a hidden markov model  verbs are generated by finding the most likely pairing
of object labels in the gigaword external corpus  the generation model of elliott and
keller        parses an image into a vdr  and then traverses the vdrs to fill the slots
of sentence templates  this approach also performs a limited from of content selection by
learning associations between vdrs and syntactic dependency trees at training time  these
associations then allow to select the most appropriate verb for a description at test time 
other approaches have used more linguistically sophisticated approaches to generation 
mitchell et al         over generate syntactically well formed sentence fragments and then
recombine these using a tree substitution grammar  a related approach has been pursued
by kuznetsova et al          where tree fragments are learnt from a training set of existing
descriptions and then these fragments are combined at test time to form new descriptions 
another linguistically expressive model has recently been proposed by ortiz et al         
the authors model image description as machine translation over vdrsentence pairs and
perform explicit content selection and surface realization using an integer linear program
over linguistic constraints 
the systems presented so far aimed at directly generating novel descriptions  however 
as argued by hodosh et al          framing image description as a natural language generation  nlg  task makes it difficult to objectively evaluate the quality of novel descriptions
   note that graphs are also used for image retrieval by johnson  krishna  stark  li  shamma  bernstein 
and fei fei        and schuster  krishna  chang  fei fei  and manning        

   

fibernardi et al 

figure    the description model based on retrieval from visual space proposed by ordonez
et al         
as it introduces a number of linguistic difficulties that detract attention from the underlying image understanding problem  hodosh et al          at the same time  evaluation
of generation systems is known to be difficult  reiter   belz         hodosh et al  therefore propose an approach that makes it possible to evaluate the mapping between images
and sentences independently of the generation aspect  models that follow this approach
conceptualize image description as a retrieval problem  they associate an image with a
description by retrieving and ranking a set of similar images with candidate descriptions 
these candidate descriptions can then either be used directly  description transfer  or a
novel description can be synthesized from the candidates  description generation  
the retrieval of images and ranking of their descriptions can be carried out in two ways 
either from a visual space or from a multimodal space that combines textual and visual
information space  in the following subsections  we will survey work that follows these two
approaches 
    description as a retrieval in visual space
the studies in this group pose the problem of automatically generating the description
of an image by retrieving images similar to the query image  i e   the new image to be
described   this is illustrated in figure    in other words  these systems exploit similarity
in the visual space to transfer descriptions to the query images  compared to models that
generate descriptions directly  section       retrieval models typically require a large amount
of training data in order to provide relevant descriptions 
in terms of their algorithmic components  visual retrieval approaches typically follow a
pipeline of three main steps 
   represent the given query image by specific visual features 
   retrieve a candidate set of images from the training set based on a similarity measure
in the feature space used 
   re rank the descriptions of the candidate images by further making use of visual
and or textual information contained in the retrieval set  or alternatively combine
fragments of the candidate descriptions according to certain rules or schemes 
one of the first model to follow this approach was the im text model of ordonez et al 
        gist  oliva   torralba        and tiny image  torralba  fergus    freeman       
   

fiautomatic description generation from images  a survey

descriptors are employed to represent the query image and to determine the visually similar
images in the first retrieval step  most of the retrieval based models consider the result of
this step as a baseline  for the re ranking step  a range of detectors  e g   object  stuff 
pedestrian  action detectors  and scene classifiers specific to the entities mentioned in the
candidate descriptions are first applied to the images to better capture their visual content 
and the images are represented by means of these detector and classifier responses  finally 
the re ranking is carried out via a classifier trained over these semantic features 
the model proposed by kuznetsova et al         first runs the detectors and the classifiers used in the re ranking step of the im text model on a query image to extract and
represent its semantic content  then  instead of performing a single retrieval by combining
the responses of these detectors and classifiers as the im text model does  it carries out a
separate image retrieval step for each visual entity present in the query image to collect related phrases from the retrieved descriptions  for instance  if a dog is detected in the given
image  then the retrieval process returns the phrases referring to visually similar dogs in the
training set  more specifically  this step is used to collect three different kinds of phrases 
noun and verb phrases are extracted from descriptions in the training set based on the
visual similarity between object regions detected in the training images and in the query
image  similarly  prepositional phrases are collected for each stuff detection in the query
image by measuring the visual similarity between the detections in the query and training
images based on their appearance and geometric arrangements  prepositional phrases are
additionally collected for each scene context detection by measuring the global scene similarity computed between the query and training images  finally  a description is generated
from these collected phrases for each detected object via integer linear programming  ilp 
which considers factors such as word ordering  redundancy  etc 
the method of gupta et al         is another phrase based approach  to retrieve
visually similar images  the authors employ simple rgb and hsv color histograms 
gabor and haar descriptors  gist and sift  lowe        descriptors as image features  then  instead of using visual object detectors or scene classifiers  they rely only
on the textual information in the descriptions of the visually similar images to extract
the visual content of the input image  specifically  the candidate descriptions are segmented into phrases of a certain type such as  subject  verb    subject  prep  object  
 verb  prep  object    attribute  object   etc  those that best describe the input image are determined according to a joint probability model based on image similarity and google search counts  and the image is represented by triplets of the form
   attribute   object    verb    verb  prep   attribute   object      object   prep  object     in
the end  the description is generated using the three top scoring triplets based on a fixed
template  to increase the quality of the descriptions  the authors also apply syntactic
aggregation and some subject and predicate grouping rules before the generation step 
patterson et al         were the first to present a large scale scene attribute dataset
in the computer vision community  the dataset includes        images from     scene
categories  which are annotated with certain attributes from a list of     discriminative
attributes related to materials  surface properties  lighting  affordances  and spatial layout 
this allows them to train attribute classifiers from this dataset  in their paper  the authors
also demonstrate that the responses of these attribute classifiers can be used as a global
image descriptor which captures the semantic content better than the standard global image
   

fibernardi et al 

descriptors such as gist  as an application  they extended the baseline model of im text
by replacing the global features with automatically extracted scene attributes  giving better
image retrieval and description results 
mason and charniaks        description generation approach differs from the models
discussed above in that it formulates description generation as an extractive summarization
problem  and it selects the output description by considering only the textual information
in the final re ranking step  in particular  the authors represented images by using the scene
attributes descriptor of patterson et al          once the visually similar images are identified from the training set  in the next step  the conditional probabilities of observing a word
in the description of the query image are estimated via non parametric density estimation
using the descriptions of the retrieved images  the final output description is then determined by using two different extractive summarization techniques  one depending on the
sumbasic model  nenkova   vanderwende        and the other based on kullback leibler
divergence between the word distributions of the query and the candidate descriptions 
yagcioglu et al         proposed an average query expansion approach which is based on
compositional distributed semantics  to represent images  they use features extracted from
the recently proposed visual geometry group convolutional neural network  vgg cnn 
chatfield  simonyan  vedaldi    zisserman         these features are the activations of
the last layer of a deep neural network trained on imagenet  which have been proven to
be effective in many computer vision problems  then  the original query is expanded as
the average of the distributed representations of retrieved descriptions  weighted by their
similarity to the input image 
the approach of devlin et al         also utilizes cnn activations as the global image
descriptor and performs k nearest neighbor retrieval to determine the images from the
training set that are visually similar to the query image  it then selects a description
from the candidate descriptions associated with the retrieved images that best describes
the images that are similar to the query image  just like the approaches by mason and
charniak        and yagcioglu et al          their approach differs in terms of how they
represent the similarity between description and how they select the best candidate over
the whole set  specifically  they propose to compute the description similarity based on
the n gram overlap f score between the descriptions  they suggest to choose the output
description by finding the description that corresponds to the description with the highest
mean n gram overlap with the other candidate descriptions  k nearest neighbor centroid
description  estimated via an n gram similarity measure 
    description as a retrieval in multimodal space
the third group of studies casts image description generation again as a retrieval problem 
but from a multimodal space  hodosh et al         socher et al         karpathy et al         
the intuition behind these models is illustrated in figure    and the overall approach can
be characterized as follows 

   learn a common multimodal space for the visual and textual data using a training
set of imagedescription pairs 
   

fiautomatic description generation from images  a survey

   given a query  use the joint representation space to perform cross modal  image
sentence  retrieval 

figure    image descriptions as a retrieval task as proposed in the works by hodosh et al 
        socher et al          and karpathy et al           
in contrast to the retrieval models that work on a visual space  section       where
unimodal image retrieval is followed by ranking of the retrieved descriptions  here image
and sentence features are projected into a common multimodal space  then  the multimodal
space is used to retrieve descriptions for a given image  the advantage of this approach is
that it allows bi directional models  i e   the common space can also be used for the other
direction  retrieving the most appropriate image for a query sentence 
in this section  we first discuss the seminal paper of hodosh et al         on description
retrieval  and then present more recent approaches that combine a retrieval approach with
some form of natural language generation  hodosh et al  map both images and sentences
into a common space  the joint space can be used for both image search  find the most
plausible image given a sentence  and image annotation  find the sentence that describes
the image well   see figure    in an earlier study the authors proposed to learn a common meaning space  farhadi et al         consisting of a triple representation of the form
hobject  action  scenei  the representation was thus limited to a set of pre defined discrete
slot fillers  which was given as training information  instead  hodosh et al  use kcca  a
kernelized version of cca  canonical correlation analysis  hotelling         to learn the
joint space  cca takes a training dataset of image sentence pairs  i e   dtrain    hi  si  
thus input from two different feature spaces  and finds linear projections into a newly induced common space  in kcca  kernel functions map the original items into higher order
space in order to capture the patterns needed to associate image and text  kcca has been
shown previously to be successful in associating images  hardoon  szedmak    shawetaylor        or image regions  socher   fei fei        with individual words or set of
tags 
hodosh et al         compare their kcca approach to a nearest neighbor  nn  baseline
that uses unimodal text and image spaces  without constructing a joint space  a drawback of
kcca is that it is only applicable to smaller datasets  as it requires the two kernel matrices
   source http   nlp cs illinois edu hockenmaiergroup framing image description 

   

fibernardi et al 

to be kept in memory during training  this becomes prohibitive for very large datasets 
some attempts have been made to circumvent the computational burden of kcca  e g  
by resorting to linear models  hodosh   hockenmaier         alternatively  sun  gan  and
nevatia        have used automatically discovered concepts from images to form a semantic
space  and performed sentence retrieval accordingly  however  recent work on description
retrieval has instead utilized neural networks to construct a joint space for image description
generation 
socher et al         use neural networks for building sentence and image vector representations that are then mapped into a common embedding space  a novelty of their
work is that they use compositional sentence vector representations  first  image and word
representations are learned in their single modalities  and finally mapped into a common
multimodal space  in particular  they use a dt rnn  dependency tree recursive neural
network  for composing language vectors to abstract over word order and syntactic difference that are semantically irrelevant  this results in    dimensional word embeddings  for
the image space  the authors use a nine layer neural network trained on imagenet data 
using unsupervised pre training  image embeddings are derived by taking the output of the
last layer        dimensions   the two spaces are then projected into a multi modal space
through a max margin objective function that intuitively trains pairs of correct image and
sentence vectors to have a high inner product  the authors show that their model outperforms previously used kcca approaches such as the work by hodosh and hockenmaier
       
karpathy et al         extend the previous multi modal embeddings model  rather
than directly mapping entire images and sentences into a common embedding space  their
model embeds more fine grained units  i e   fragments of images  objects  and sentences
 dependency tree fragments   into a common space  their final model integrates both global
 sentence and image level  as well as finer grained information and outperforms previous
approaches  such as dt rnn  socher et al          a similar approach is pursued by
pinheiro et al          who propose a bilinear phrase based model that learns a mapping
between image representations and sentences  a constrained language model is then used
to generate from this representation  a conceptually related approach is pursued by ushiku
et al          the authors use a common subspace model which maps all feature vectors
associated with the same phrase into nearby regions of the space  for generation  a beamsearch based decoder or templates are used 
description generation systems are difficult to evaluate  therefore the studies reviewed
above treat the problem as a retrieval and ranking task  hodosh et al         socher et al  
       while such an approach has been valuable because it enables comparative evaluation 
retrieval and ranking is limited by the availability of existing datasets with descriptions  to
alleviate this problem  recent models have been developed that are extensions of multimodal
spaces  they are able to not only rank sentences  but can also generate them  chen   zitnick 
      donahue et al         karpathy   fei fei        kiros et al         lebret et al        
mao et al       a  vinyals et al         xu et al         
kiros et al         introduced a general encoder decoder framework for image description
ranking and generation  illustrated in figure    intuitively the method works as follows 
the encoder first constructs a joint multimodal space  this space can be used to rank
images and descriptions  the second stage  decoder  then uses the shared multimodal
   

fiautomatic description generation from images  a survey

figure    the encoder decoder model proposed by kiros et al         

representation to generate novel descriptions  their model  directly inspired by recent
work in machine translation  encodes sentences using a longshort term memory  lstm 
recurrent neural network  and image features using a deep convolutional network  cnn  
lstm is an extension of the recurrent neural network  rnn  that incorporates builtin memory to store information and exploit long range context  in kiros et al s       
encoder decoder model  the vision space is projected into the embedding space of the lstm
hidden states  a pairwise ranking loss is minimized to learn the ranking of images and their
descriptions  the decoder  a neural network based language model  is able to generate novel
descriptions from this multimodal space 
another work that has been carried out at the same time and is similar to the latter is
described in the paper by donahue et al          the authors propose a model that is also
based on the lstm neural architecture  however  rather than projecting the vision space
into the embedding space of the hidden states  the model takes a copy of the static image
and the previous word directly as input  that is then fed to a stack of four lstms  another
lstm based model is proposed by jia et al          who added semantic image information
as additional input to the lstm  the model by kiros et al         outperforms the prior
dt rnn model  socher et al          in turn  donahue et al  report that they outperform
the work of kiros et al         on the task of image description retrieval  subsequent work
includes the rnn based architectures by mao et al       a  and vinyals et al         
who are very similar to the one proposed by kiros et al         and achieve comparable
results on standard datasets  mao  wei  yang  wang  huang  and yuille      b  propose an
interesting extension of mao et al s      a  model for the learning of novel visual concepts 
karpathy and fei fei        improve on previous models by proposing a deep visualsemantic alignment model with a simpler architecture and objective function  their key
insight is to assume that parts of the sentence refer to particular but unknown regions in the
image  their model tries to infer the alignments between segments of sentences and regions
of images and is based on convolutional neural networks over image regions  bidirectional
rnn over sentences and a structured objective that aligns the two modalities  words
and image regions are mapped into a common multimodal embedding  the multimodal
recurrent neural network architecture uses the inferred alignments to learn and generate
   

fibernardi et al 

novel descriptions  here  the image is used as condition for the first state in the recurrent
neural network  which then generates image descriptions 
another model that can generate novel sentences is proposed in  chen   zitnick        
in contrast to the previous work  their model dynamically builds a visual representation of
the scene as a description is being generated  that is  a word is read or generated and the
visual representation is updated to reflect the new information  they accomplish this with
a simple rnn  the model achieves comparable or better results than most prior studies 
except for the recently proposed deep visual semantic alignment model  karpathy   fei fei 
       the model of xu et al         is closely related in that it also uses an rnn based
architecture in which the visual representations are dynamically updated  xu et al s       
model incorporates an attentional component  which gives it a way of determining which
regions in an image are salient  and it can focus its description on those regions  while
resulting in an improvement in description accuracy  it also makes it possible to analyze
model behavior by visualizing the regions that were attended to during each word that was
generated by the model 
the general rnn based ranking and generation approach is also followed by lebret
et al          here  the main innovation is on the linguistic side  they employ a bilinear
model to learn a common space of image features and syntactic phrases  noun phrases  verb
phrases  and prepositional phrases   a markov model is then utilized to generate sentences
from these phrase embedding  on the visual side  standard cnn based features are used 
this results in an elegant modeling framework  whose performance is broadly comparable
to the state of the art 
finally  two important directions that are less explored are  portability and weakly supervised learning  verma and jawahar        evaluate the portability of a bi directional
model based on topic models  showing that performance significantly degrades  they highlight the importance of cross dataset image description retrieval evaluation  another interesting observation is that all of the above models require a training set of fully annotated
image sentence pairs  however  obtaining such data in large quantities is prohibitively expensive  gong et al         propose an approach based on weak supervision that transfers
knowledge from millions of weakly annotated images to improve the accuracy of description
retrieval 
    comparison of existing approaches
the discussion in the previous subsections makes it clear that each approach to image
description has its particular strengths and weaknesses  for example  the methods that
cast the task as a generation problem  section      have an advantage over other types of
approaches in that they can produce novel sentences to describe a given image  however 
their success relies heavily on how accurately they estimate the visual content and how
well they are able to verbalize this content  in particular  they explicitly employ computer
vision techniques to predict the most likely meaning of a given image  these methods have
limited accuracy in practice  hence if they fail to identify the most important objects and
their attributes  then no valid description can be generated  another difficulty lies in the
final description generation step  sophisticated natural language generation is crucial to
   

fiautomatic description generation from images  a survey

guarantee fluency and grammatical correctness of the generated sentences  this can come
at the price of considerable algorithmic complexity 
in contrast  image description methods that cast the problem as a retrieval from a
visual space problem and transfer the retrieved descriptions to a novel image  section     
always produce grammatically correct descriptions  this is guaranteed by design  as these
systems fetch human generated sentences from visually similar images  the main issue with
this approach is that it requires large amounts of images with human written descriptions 
that is  the accuracy  but not the grammaticality  of the descriptions reduces as the size
of the training set decreases  the training set also needs to be diverse  in addition to being
large   in order for visual retrieval based approaches to produce image descriptions that are
adequate for novel test images  devlin et al          though this problem can be mitigated
by re synthesizing a novel description from the retrieved ones  see section      
approaches that cast image description as a retrieval from a multimodal space problem
 section      also have the advantage of generating human like descriptions as they are
able to retrieve the most appropriate ones from a pre defined large pool of descriptions 
however  ranking these descriptions requires a cross modal similarity metric that compares
images and sentences  such metrics are difficult to define  compared to the unimodal
image to image similarity metrics used by retrieval models that work on a visual space 
additionally  training a common space for images and sentences requires a large training
set of images annotated with human generated descriptions  on the plus side  such a
multimodal embedding space can also be used for the reverse problem  i e   for retrieving
the most appropriate image for a query sentence  this is something generation based or
visual retrieval based approaches are not capable of 

   datasets and evaluation
there is a wide range of datasets for automatic image description research  the images in
these datasets are associated with textual descriptions and differ from each other in certain
aspects such as in size  the format of the descriptions and in how the descriptions were collected  here we review common approaches for collecting datasets  the datasets themselves 
and evaluation measures for comparing generated descriptions with ground truth texts  the
datasets are summarized in table    and examples of images and descriptions are given in
figure    the readers can also refer to the dataset survey by ferraro  mostafazadeh  huang 
vanderwende  devlin  galley  and mitchell        for an analysis similar to ours  it provides
a basic comparison of some of the existing language and vision datasets  it is not limited
to automatic image description  and it reports some simple statistics and quality metrics
such as perplexity  syntactic complexity  and abstract to concrete word ratios 
    image description datasets
the pascal k sentence dataset  rashtchian et al         is a dataset which is commonly
used as a benchmark for evaluating the quality of description generation systems  this
medium scale dataset  consists of       images that were selected from the pascal     
object recognition dataset  everingham  van gool  williams  winn    zisserman       
and includes objects from different visual classes  such as humans  animals  and vehicles 
   

fibernardi et al 

images

texts

judgments

objects

pascal k  rashtchian et al        
vlt k  elliott   keller       
flickr k  hodosh   hockenmaier       
flickr  k  young et al        
abstract scenes  zitnick   parikh       
iapr tc    grubinger et al        
ms coco  lin et al        

     
     
     
      
      
      
       

 
 
 
 
 
  
 

no
partial
yes
no
no
no
collected

partial
partial
no
no
complete
segmented
partial

bbc news  feng   lapata       
sbu m captions  ordonez et al        
deja image captions  chen et al        

     
         
         

 
 
varies

no
collected 
no

no
no
no

table    image datasets for the automatic description generation models  we have split
the overview into image description datasets  top  and caption datasets  bottom   see the
main text for an explanation of this distinction 
each image is associated with five descriptions generated by humans on amazon mechanical
turk  amt  service 
the visual and linguistic treebank  vlt k  elliott   keller        makes use of images
from the pascal      action recognition dataset  it augments these images with three  twosentence descriptions per image  these descriptions were collected on amt with specific
instructions to verbalize the main action depicted in the image and the actors involved  first
sentence   while also mentioning the most important background objects  second sentence  
for a subset of     images of the visual and linguistic treebank  object annotation is
available  in the form of polygons around all objects mentioned in the descriptions   for
this subset  manually created visual dependency representations  see section      are also
included  three vdrs per images  i e   a total of       
the flickr k dataset  hodosh et al         and its extended version flickr  k
dataset  young et al         contain images from flickr  comprising approximately      
and        images  respectively  the images in these two datasets were selected through
user queries for specific objects and actions  these datasets contain five descriptions per image which were collected from amt workers using a strategy similar to that of the pascal k
dataset 
the abstract scenes dataset  zitnick   parikh        zitnick  parikh    vanderwende 
      consists of        clip art images and their descriptions  the images were created
through amt  where workers were asked to place a fixed vocabulary of    clip art objects
into a scene of their choosing  the descriptions were then sourced for these worker created
scenes  the authors provided these descriptions in two different forms  while the first
group contains a single sentence description for each image  the second group includes two
alternative descriptions per image  each of these two descriptions consist of three simple
sentences with each sentence describing a different aspect of the scene  the main advantage
of this dataset is it affords the opportunity to explore image description generation without
   kuznetsova et al         ran a human judgments study on       images from this dataset 

   

fiautomatic description generation from images  a survey

   one jet lands at an airport while another takes off
next to it 
   two airplanes parked in an airport 
   two jets taxi past each other 
   two parked jet airplanes facing opposite directions 
   two passenger planes on a grassy plain

   there are several people in chairs and a small child
watching one of them play a trumpet
   a man is playing a trumpet in front of a little boy 
   people sitting on a sofa with a man playing an
instrument for entertainment 

 a  pascal k 

 b  vlt k 

   a man is snowboarding over a structure on a snowy
hill 
   a snowboarder jumps through the air on a snowy
hill 
   a snowboarder wearing green pants doing a trick
on a high bench
   someone in yellow pants is on a ramp over the
snow 
   the man is performing a trick on a snowboard high
in the air 

   a yellow building with white columns in the background
   two palm trees in front of the house
   cars are parking in front of the house
   a woman and a child are walking over the square

 c  flickr k  

 d  iapr tc    

   a cat anxiously sits in the park and stares at an
unattended hot dog that someone has left on a
yellow bench

   a blue smart car parked in a parking lot 
   some vehicles on a very wet wide city street 
   several cars and a motorcycle are on a snow covered street 
   many vehicles drive down an icy street 
   a small smart car driving in the city 

 e  abstract scenes  

 f  ms coco  

figure    example images and descriptions from the benchmark image datasets 

   

fibernardi et al 

the need for automatic object recognition  thus avoiding the associated noise  a more
recent version of this dataset has been created as a part of the visual question answering
 vqa  dataset  antol  agrawal  lu  mitchell  batra  zitnick    parikh         it contains
       different scene images with more realistic human models and with five single sentence
descriptions 
the iapr tc   dataset introduced by grubinger et al         is one of the earliest
multi modal datasets and contains        images with descriptions  the images were originally retrieved via search engines such as google  bing and yahoo  and the descriptions
were produced in multiple languages  predominantly english and german   each image is
associated with one to five descriptions  where each description refers to a different aspect
of the image  where applicable  the dataset also contains complete pixel level segmentation
of the objects 
the ms coco dataset  lin et al         currently consists of         images with five
different descriptions per image  images in this dataset are annotated for    object categories  which means that bounding boxes around all instances in one of these categories
are available for all images  the ms coco dataset has been widely used for image description  something that is facilitated by the standard evaluation server that has recently
become available     extensions of ms coco are currently under development  including
the addition of questions and answers  antol et al         
one paper  lin et al         uses an the nyu dataset  silberman  kohli  hoiem   
fergus         which contains       indoor scenes with  d object segmentation  this
dataset has been augmented with five descriptions per image by lin et al 
    image caption datasets
image descriptions verbalize what can be seen in the image  i e   they refer to the objects 
actions  and attributes depicted  mention the scene type  etc  captions  on the other hand 
are typically texts associated with images that verbalize information that cannot be seen
in the image  a caption provides personal  cultural  or historical context for the image
 panofsky         images shared through social networking or photo sharing websites can
be accompanied by descriptions or captions  or a mixtures of both types of text  the images
in a newspaper or a museum will typically contain cultural or historical texts  i e   captions
not descriptions 
the bbc news dataset  feng   lapata        was one of the earliest collections of
images and co occurring texts  feng and lapata        harvested       news articles from
the british broadcasting corporation news website  with the constraint that the article
includes an image and a caption 
  
  
   
   
   

source http   nlp cs illinois edu hockenmaiergroup pascal sentences index html
source http   github com elliottd vlt
source https   illinois edu fb sec        
source http   imageclef org photodata
source http   research microsoft com en us um people larryz clipart semanticclassesrender
 classes v  html
    source http   mscoco org explore
    source http   mscoco org dataset  captions eval

   

fiautomatic description generation from images  a survey

the sbu m captions dataset introduced by ordonez et al         differs from the
previous datasets in that it is a web scale dataset containing approximately one million
captioned images  it is compiled from data available on flickr with user provided image
descriptions  the images were downloaded and filtered from flickr with the constraint that
an image contained at least one noun and one verb on predefined control lists  the resulting
dataset is provided as a csv file of urls 
the deja image captions dataset  chen et al         contains           images with
        near identical captions harvested from flickr      million images were downloaded
from flickr during the calendar year      using a set of     nouns as queries  the image
captions are normalized through lemmatization and stop word removal to create a corpus
of the near identical texts  for instance  the sentences the bird flies in blue sky and a bird
flying into the blue sky were normalized to bird fly in blue sky  chen et al          image
caption pairs are retained if the captions are repeated by more than one user in normalized
form 

    collecting datasets
collecting new imagetext datasets is typically performed through crowd sourcing or harvesting data from the web  the images for these datasets have either been sourced from
an existing task in the computer vision community  the pascal challenge  everingham
et al         was used to the pascal k and vlt k datasets  directly from flickr  in the
case of flickr k   k  ms coco  sbu m captions  and deja image captions datasets  or
crowdsourced  in the case of the abstract scenes dataset  the texts in imagedescription
datasets are usually crowd sourced from amazon mechanical turk or crowdflower  whereas
the texts in imagecaption datasets have been harvested from photo sharing sites  such as
flickr  or from news providers  captions are usually collected without financial incentive
because they are written by the people sharing their own images  or by journalists 
crowd sourcing the descriptions of images involves defining a simple task that can be
performed by untrained workers  examples of the task guidelines used by hodosh et al 
       and elliott and keller        are given in figure    in both instances  care was taken
to clearly inform the potential workers about the expectations for the task  in particular 
explicit instructions were given on how the descriptions should be written  and examples of
good texts were provided  in addition  hodosh et al  provided more extensive examples to
explain what would constitute unsatisfactory texts  further options are available to control
the quality of the collected texts  a minimum performance rate for workers is a common
choice  and a pre task selection quiz may be used to determine whether workers have a
sufficient grasp of the english language  hodosh et al         
the issue of remuneration for crowd sourced workers is controversial  and higher payments do not always lead to better quality in a crowd sourced environment  mason   watts 
       rashtchian et al         paid       description  elliott and keller        paid      
for an average of    seconds of work to produce a two sentence description  to the best of
our knowledge  such information is not available for the other datasets 
   

fibernardi et al 

 a  mechanical turk interface used to collect flickr k dataset    

 b  mechanical turk interface used to collect vlt k dataset 

figure    examples of mechanical turk interfaces for collecting descriptions 

    evaluation measures
evaluating the output of a natural language generation  nlg  system is a fundamentally
difficult task  dale   white        reiter   belz         the most common way to assess
the quality of automatically generated texts is the subjective evaluation by human experts 
    source appendix of the work by hodosh et al        

   

fiautomatic description generation from images  a survey

nlg produced text is typically judged in terms of grammar and content  indicating how
syntactically correct and how relevant the text is  respectively  fluency of the generated
text is sometimes tested as well  especially when a surface realization technique is involved
during the generation process  automatically generated descriptions for images can be
evaluated using the same nlg techniques  typically  judges are provided with the image
as well as with the description during evaluation tasks  subjective human evaluations of
machine generated image descriptions are often performed on mechanical turk with the help
of questions  so far  the following likert scale questions have been used to test datasets
and user groups of various sizes 
 the description accurately describes the image  kulkarni et al         li et al        
mitchell et al         kuznetsova et al         elliott   keller        hodosh et al  
      
 the description is grammatically correct  yang et al         mitchell et al        
kuznetsova et al         elliott   keller        inter alia  
 the description has no incorrect information  mitchell et al         
 the description is relevant for this image  li et al         yang et al         
 the description is creatively constructed  li et al         
 the description is human like  mitchell et al         
another approach for evaluating descriptions is to use automatic measures  such as
bleu  papineni  roukos  ward    zhu         rouge  lin   hovy         translation
error rate  feng   lapata         meteor  denkowski   lavie         or cider  vedantam  lawrence zitnick    parikh         these measures were originally developed to evaluate the output of machine translation engines or text summarization systems  with the
exception of cider  which was developed specifically for image description evaluation  all
these measures compute a score that indicates the similarity between the system output and
one or more human written reference texts  e g   ground truth translations or summaries  
this approach to evaluation has been subject to much discussion and critique  kulkarni
et al         hodosh et al         elliott   keller         kulkarni et al  found weakly
negative or no correlation between human judgments and unigram bleu on the pascal  k
dataset  pearsons          and        hodosh et al  studied the cohens  correlation of
expert human judgments and binarized unigram bleu and unigram rouge of retrieved
descriptions on the flickr k dataset  they found the best agreement between humans and
bleu           or rouge           when the system retrieved the sentences originally associated with the images  agreement dropped when only one reference sentence
was available  or when the reference sentences were disjoint from the proposal sentences 
they concluded that neither measure was appropriate for image description evaluation and
subsequently proposed imagesentence ranking experiments  discussed in more detail below  elliott and keller analyzed the correlation between human judgments and automatic
evaluation measures for retrieved and system generated image descriptions in the flickr k
and vlt k datasets  they showed that sentence level unigram bleu  which at that point
   

fibernardi et al 

in time was the de facto standard measure for image description evaluation  is only weakly
correlated with human judgments  meteor  banerjee   lavie         a less frequently used
translation evaluation measure  exhibited the highest correlation with human judgments 
however  kuznetsova et al         found that unigram bleu was more strongly correlated
with human judgments than meteor for image caption generation 
the first large scale image description evaluation took place during the ms coco
captions challenge         featuring    teams with a dataset of         training images
and        images in a withheld test dataset  the number of reference texts for each testing
image was either five or     based on the insight that some measures may benefit from
larger reference sets  vedantam et al          when automatic evaluation measures were
used  some of the image description systems outperformed a humanhuman upper bound   
whether five or    reference descriptions were provided  however  none of the systems
outperformed humanhuman evaluation when a judgment elicitation task was used  meteor
was found to be the most robust measure  with the systems beating the human text on one
and two submissions  depending on the number of references   the systems outperformed
humans seven or five times measured with cider  according to rouge and bleu  the
system nearly always outperformed the humans  further confirming the unsuitability of
these evaluation measures 
the models that approach the description generation problem from a cross modal retrieval perspective  hodosh   hockenmaier        hodosh et al         socher et al        
gong et al         karpathy et al         verma   jawahar        are also able to use measures from information retrieval  such as median rank  mrank   precision at k  s k   or
recall at k  r k  to evaluate the descriptions they return  in addition to the text similarity
measures reported above  this evaluation paradigm was first proposed by hodosh et al  
who reported high correlation with human judgments for imagesentence based ranking
evaluations 
in table    we summarize all the image description approaches discussed in this survey 
and list the datasets and evaluation measures employed by each of these approaches  it
can be seen that more recent systems  starting in       have converged on the use of
large description datasets  flickr k   k  ms coco  and employ evaluation measures that
perform well in terms of correlation with human judgments  meteor  cider   however  the
use of bleu  despite its limitations  is still widespread  also the use of human evaluation
is by no means universal in the literature 

   future directions
as this survey demonstrates  the cv and nlp communities have witnessed an upsurge in
interest in automatic image description systems  with the help of recent advances in deep
learning models for images and text  substantial improvements in the quality of automatically generated descriptions has been registered  nevertheless  a series of challenges for
image description research remain  in the following  we discuss future directions that this
line of research is likely to benefit from 
    source http   mscoco org dataset cap    
    calculated by collecting an additional human written description  which was then compared to the
reference descriptions 

   

fiautomatic description generation from images  a survey

reference

approach

farhadi et al        
kulkarni et al        
li et al        
ordonez et al        
yang et al        

multretrieval
generation
generation
visretrieval
generation

datasets

gupta et al        
kuznetsova et al        
mitchell et al        
elliott and keller       
hodosh et al        

pascal k
pascal k
pascal k
sbu m
iapr 
flickr k   k 
coco
visretrieval
pascal k  iapr
visretrieval
sbu m
generation
pascal k
generation
vlt k
multretrieval pascal k  flickr k

gong et al        
karpathy et al        
kuznetsova et al        
mason and charniak       
patterson et al        
socher et al        
verma and jawahar       
yatskar et al        
chen and zitnick       

multretrieval
multretrieval
generation
visretrieval
visretrieval
multretrieval
multretrieval
generation
multretrieval

donahue et al        

multretrieval

devlin et al        
elliott and de vries       
fang et al        

visretrieval
generation
generation

jia et al        
generation
karpathy and fei fei        multretrieval
kiros et al        
lebret et al        
lin et al        
mao et al       a 
ortiz et al        
pinheiro et al        
ushiku et al        

multretrieval
multretrieval
generation
multretrieval
generation
multretrieval
generation

vinyals et al        

multretrieval

xu et al        
yagcioglu et al        

multretrieval
visretrieval

measures
bleu
human  bleu
human  bleu

bleu  rouge  meteor 
cider  r k
human  bleu  rouge
human  bleu
human
human  bleu
human  bleu  rouge 
mrank  r k
sbu m  flickr  k
r k
flickr k   k  coco
bleu  meteor  cider
sbu m
human  bleu  meteor
sbu m
human  bleu
sbu m
bleu
pascal k
mrank  r k
iapr  sbu m  pascal k bleu  rouge  p k
own data
human  bleu
flickr k   k  coco
bleu  meteor  cider 
mrank  r k
flickr  k  coco
human  bleu  mrank 
r k
coco
bleu  meteor
vlt k  pascal k
bleu  meteor
coco
human  bleu  rouge 
meteor  cider
flickr k   k  coco
bleu  meteor
flickr k   k  coco
bleu  meteor  cider 
mrank  r k
flickr k   k
r k
flickr  k  coco
bleu  r k
nyu
rouge
iapr  flickr  k  coco bleu  mrank  r k
abstract scenes
human  bleu  meteor
coco
bleu
pascal k  iapr  sbu m  bleu
coco
pascal k 
sbu m  bleu  meteor  cider 
flickr k   k
mrank  r k
flickr k   k  coco
bleu  meteor
flickr k   k  coco
human  bleu  meteor 
cider

table    an overview of the approaches  datasets  and evaluation measures reviewed in this
survey and organised in chronological order 

   

fibernardi et al 

    datasets
the earliest work on image description used relatively small datasets  farhadi et al        
kulkarni et al         elliott   keller         recently  the introduction of flickr  k 
ms coco and other large datasets has enabled the training of more complex models such
as neural networks  still  the area is likely to benefit from larger and diversified datasets
that share a common  unified  comprehensive vocabulary  vinyals et al         argue that
the collection process and the quality of the descriptions in the datasets affect performance
significantly  and make transfer learning between datasets not as effective as expected 
they show that learning a model from ms coco and applying it to datasets collected in
different settings such as sbu m captions or pascal k  leads to a degradation in bleu
performance  this is surprising  since ms coco offers a much larger amount of training
data than pascal k  as vinyals et al  put it  this is largely due to the differences in
vocabulary and in the quality of descriptions  most learning approaches are likely to suffer
from such situations  collecting larger and comprehensive datasets and developing more
generic approaches that are capable of generating naturalistic descriptions across domains
therefore is an open challenge 
while supervised algorithms are likely to take advantage of carefully collected large
datasets  lowering the amount of supervision in exchange of access to larger unsupervised
data is also an interesting avenue for future research  leveraging unsupervised data for
building richer representations and description models is another open research challenge
in this context 
    measures
designing automatic measures that can mimic human judgments in evaluating the suitability of image descriptions is perhaps the most urgent need in the area of image description
 elliott   keller         this need can be dramatically observed at the latest evaluation results of ms coco challenge  according to existing measures  including the latest cider
measure  vedantam et al          several automatic methods outperform the human upper bound  this upper bound indicates how similar human descriptions are to each other  
the counterintuitive nature of this result is confirmed by the fact that when human judgments are used for evaluation  the output of even the best system is judged as worse than
a human generated description for most of the time  fang et al          however  since
conducting human judgment experiments is costly  there is a major need for improved automatic measures that are more highly correlated with human judgments  figure   plots the
epanechnikov probability density estimate  a non parametric optimal estimator  for bleu 
meteor  rouge  and cider scores per subjective judgment in flickr k dataset  the human judgments were obtained from human experts  hodosh et al          bleu is once
again confirmed to be unable to sufficiently discriminate between the lowest three human
judgments  while meteor and cider show signs of moving towards a useful separation 
    diversity and originality
current algorithms often rely on direct representations of the descriptions they see at training time  making the descriptions generated at test time very similar  this results in many
   

fiautomatic description generation from images  a survey

human judgement

human judgement
    

perfect
minor mistakes
some aspects
no relation

    

    

    

    

    

    

    

    

    

perfect
minor mistakes
some aspects
no relation

 

  

  

  

  

   

 

  

bleu

  

  

  

   

meteor
human judgement
perfect
minor mistakes
some aspects
no relation

 

 

 

   

 

 

   

 

 

   

 

perfect
minor mistakes
some aspects
no relation

   

 

human judgement

   

   

   

   

   

    

   

    

    

    

    

cider

rouge

figure    probability density estimates of bleu  meteor  rouge  and cider scores
against human judgments in the flickr k dataset  the y axis shows the probability density 
and the x axis is the score computed by the measure 

   

fibernardi et al 

repetitions and limits the diversity of the generated descriptions  making it difficult to reach
human levels of performance  this situation has been demonstrated by devlin et al         
who show that their best model is able to generate only       of unique descriptions  systems that generate diverse and original descriptions that do not just repeat what is already
seen  but also infer the underlying semantics therefore remain as an open challenge  chen
and zitnick        and related approaches take a step towards addressing such limitations
by coupling description and visual representation generation 
jas and parikh        introduces the notion of image specificity  arguing that the domain of image descriptions is not uniform  certain images being more specific than others 
descriptions of non specific images tend to vary a lot as people tend to describe a nonspecific scene from different aspects  this notion and its effects to description systems and
measures should be investigated in further detail 

    further tasks
another open challenge is visual question answering  vqa   while natural language
question answering based on text has been a significant goal of nlp research for a long
time  e g   liang  jordan    klein        fader  zettlemoyer    etzioni        richardson  burges    renshaw        fader  zettlemoyer    etzioni         answering questions
about images is a task that has recently emerged  towards achieving this goal  malinowski
and fritz      a  propose a bayesian framework that connects natural language questionanswering with the visual information extracted from image parts  more recently  image
question answering methods based on neural networks have been developed  gao  mao 
zhou  huang    yuille        ren  kiros    zemel        malinowski  rohrbach    fritz 
      ma  lu    li         following this effort  several datasets on this task are being
released  daquar  malinowski   fritz      a  was compiled from scene depth images
and mainly focuses on questions about the type  quantity and color of objects  cocoqa  ren et al         was constructed by converting image descriptions to vqa format
over a subset of images from the ms coco dataset  the freestyle multilingual image question answering  fm iqa  dataset  gao et al          visual madlibs dataset  yu  park 
berg    berg        and the vqa dataset  antol et al          were again built for images
from ms coco  but this time question answer pairs are collected via human annotators
in a freestyle paradigm  research in this emerging field is likely to flourish in the near future  the ultimate goal of vqa is to build systems that can pass the  recently developed 
visual turing test by being able to answer arbitrary questions about images with the same
precision as a human observer  malinowski   fritz      b  geman  geman  hallonquist   
younes        
having multilingual repositories for image description is an interesting direction to
explore  currently  among the available benchmark datasets  only the iapr tc  
dataset  grubinger et al         has multilingual descriptions  in english and german  
future work should investigate whether transferring multimodal features between monolingual description models results in improved descriptions compared to monolingual baselines 
   

fiautomatic description generation from images  a survey

it would be interesting to study different models and new tasks in a multilingual multimodal
setting using larger and more syntactically diverse multilingual description corpora   
overall  image understanding is the ultimate goal of computer vision and natural language generation is one of the ultimate goals of nlp  image description is where these both
goals are interconnected and this topic is therefore likely to benefit from individual advances
in each of these two fields 

   conclusions
in this survey  we discuss recent advances in automatic image description and closely related
problems  we review and analyze a large body of the existing work by highlighting common
characteristics and differences between existing research  in particular  we categorize the
related work into three groups   i  direct description generation from images   i  retrieval
of images from a visual space  and  iii  retrieval of images from multimodal  joint visual
and linguistic  space  in addition  we provided a brief review of the existing corpora and
automatic evaluation measures  and discussed some future directions for vision and language
research 
compared to traditional keyword based image annotation  using object recognition 
attribute detection  scene labeling  etc    automatic image description systems produce more
human like explanations of visual content  providing a more complete picture of the scene 
advancements in this field could lead to more intelligent artificial vision systems  which
can make inferences about the scenes through the generated grounded image descriptions
and therefore interact with their environments in a more natural manner  they could also
have a direct impact on technological applications from which visually impaired people can
benefit through more accessible interfaces 
despite the remarkable increase in the number of image description systems in recent
years  experimental results suggest that system performance still falls short of human performance  a similar challenge lies in the automatic evaluation of systems using reference
descriptions  the measures and the tools currently in use are not sufficiently highly correlated with human judgments  indicating a need for measures that can deal with the
complexity of the image description problem adequately 

acknowledgments
we thank the anonymous reviewers for their useful comments  this work has been partially supported by the european commission ict cost action iv l net  the european network on integrating vision and language  ic       rc  ae  ee  nic was
funded by the scientific and technological research council of turkey  tubitak  research grant    e     fk would like to acknowledge erc funding through starting grant
       synchronous linguistic and visual processing  de was supported by ercim
abcde fellowship         
    the multimodal translation shared task at the      workshop on machine translation will use an
english and german translated version of the flickr  k corpora  see http   www statmt org wmt   
multimodal task html for more details 

   

fibernardi et al 

references
antol  s   agrawal  a   lu  j   mitchell  m   batra  d   zitnick  c  l     parikh  d         
vqa  visual question answering  in international conference on computer vision 
banerjee  s     lavie  a          meteor  an automatic metric for mt evaluation with
improved correlation with human judgments  in annual meeting of the association for computational linguistics workshop on intrinsic and extrinsic evaluation
measures for mt and or summarization 
berg  t  l   berg  a  c     shih  j          automatic attribute discovery and characterization from noisy web data  in european conference on computer vision 
chatfield  k   simonyan  k   vedaldi  a     zisserman  a          return of the devil in the
details  delving deep into convolutional nets  in british machine vision conference 
chen  j   kuznetsova  p   warren  d     choi  y          deja image captions  a corpus of
expressive descriptions in repetition  in north american chapter of the association
for computational linguistics 
chen  x     zitnick  c  l          minds eye  a recurrent visual representation for image
caption generation  in ieee conference on computer vision and pattern recognition 
dale  r     white  m  e   eds            workshop on shared tasks and comparative
evaluation in natural language generation  position papers 
denkowski  m     lavie  a          meteor universal  language specific translation evaluation for any target language  in conference of the european chapter of the association for computational linguistics workshop on statistical machine translation 
devlin  j   cheng  h   fang  h   gupta  s   deng  l   he  x   zweig  g     mitchell  m 
        language models for image captioning  the quirks and what works  in
annual meeting of the association for computational linguistics 
donahue  j   hendricks  l  a   guadarrama  s   rohrbach  m   venugopalan  s   saenko 
k     darrell  t          long term recurrent convolutional networks for visual recognition and description  in ieee conference on computer vision and pattern recognition 
elliott  d     de vries  a  p          describing images using inferred visual dependency
representations  in annual meeting of the association for computational linguistics 
elliott  d     keller  f          image description using visual dependency representations  in conference on empirical methods in natural language processing 
elliott  d     keller  f          comparing automatic evaluation measures for image
description  in annual meeting of the association for computational linguistics 
elliott  d   lavrenko  v     keller  f          query by example image retrieval using
visual dependency representations  in international conference on computational
linguistics 
everingham  m   van gool  l   williams  c  k  i   winn  j     zisserman  a          the
pascal visual object classes  voc  challenge  international journal of computer
vision                 
   

fiautomatic description generation from images  a survey

fader  a   zettlemoyer  l     etzioni  o          paraphrase driven learning for open question answering  in annual meeting of the association for computational linguistics 
fader  a   zettlemoyer  l     etzioni  o          open question answering over curated and
extracted knowledge bases  in acm sigkdd conference on knowledge discovery
and data mining 
fang  h   gupta  s   iandola  f   srivastava  r   deng  l   dollar  p   gao  j   he  x  
mitchell  m   platt  j   zitnick  c  l     zweig  g          from captions to visual
concepts and back  in ieee conference on computer vision and pattern recognition 
farhadi  a   hejrati  m   sadeghi  m  a   young  p   rashtchian  c   hockenmaier  j    
forsyth  d          every picture tells a story  generating sentences from images  in
european conference on computer vision 
felzenszwalb  p  f   girshick  r  b   mcallester  d     ramanan  d          object detection with discriminatively trained part based models  ieee transactions on pattern
analysis and machine intelligence                   
feng  y     lapata  m          automatic image annotation using auxiliary text information  in annual meeting of the association for computational linguistics 
feng  y     lapata  m          automatic caption generation for news images  ieee
transactions on pattern analysis and machine intelligence                 
ferraro  f   mostafazadeh  n   huang  t   vanderwende  l   devlin  j   galley  m    
mitchell  m          a survey of current datasets for vision and language research  in
conference on empirical methods in natural language processing 
gao  h   mao  j   zhou  j   huang  z     yuille  a          are you talking to a machine 
dataset and methods for multilingual image question answering  in international
conference on learning representations 
geman  d   geman  s   hallonquist  n     younes  l          visual turing test for computer
vision systems  proceedings of the national academy of sciences                     
girshick  r   donahue  j   darrell  t     malik  j          rich feature hierarchies for accurate object detection and semantic segmentation  in ieee conference on computer
vision and pattern recognition 
gong  y   wang  l   hodosh  m   hockenmaier  j     lazebnik  s          improving imagesentence embeddings using large weakly annotated photo collections  in european
conference on computer vision 
grubinger  m   clough  p   muller  h     deselaers  t          the iapr tc    benchmark 
a new evaluation resource for visual information systems  in international conference
on language resources and evaluation 
guadarrama  s   krishnamoorthy  n   malkarnenkar  g   venugopalan  s   mooney  r   darrell  t     saenko  k          youtube text  recognizing and describing arbitrary
activities using semantic hierarchies and zero shot recognition  in international conference on computer vision 
gupta  a   verma  y     jawahar  c  v          choosing linguistics over vision to describe
images  in aaai conference on artificial intelligence 
   

fibernardi et al 

hardoon  d  r   szedmak  s     shawe taylor  j          canonical correlation analysis 
an overview with application to learning methods  neural computation          
         
hodosh  m     hockenmaier  j          sentence based image description with scalable 
explicit models  in ieee conference on computer vision and pattern recognition
workshops 
hodosh  m   young  p     hockenmaier  j          framing image description as a ranking task  data  models and evaluation metrics  journal of artificial intelligence
research             
hotelling  h          relations between two sets of variates  biometrika            
jaimes  a     chang  s  f          a conceptual framework for indexing visual information
at multiple levels  in ist spie internet imaging 
jas  m     parikh  d          image specificity  in ieee conference on computer vision
and pattern recognition 
jia  x   gavves  e   fernando  b     tuytelaars  t          guiding the long short term
memory model for image caption generation  in international conference on computer vision 
johnson  j   krishna  r   stark  m   li  l  j   shamma  d  a   bernstein  m     fei fei  l 
        image retrieval using scene graphs  in ieee conference on computer vision
and pattern recognition 
karpathy  a     fei fei  l          deep visual semantic alignments for generating image
descriptions  in ieee conference on computer vision and pattern recognition 
karpathy  a   joulin  a     fei fei  l          deep fragment embeddings for bidirectional
image sentence mapping  in advances in neural information processing systems 
khan  m  u  g   zhang  l     gotoh  y          towards coherent natural language description of video streams  in international conference on computer vision workshops 
kiros  r   salakhutdinov  r     zemel  r  s          unifying visual semantic embeddings
with multimodal neural language models  in advances in neural information processing systems deep learning workshop 
krishnamoorthy  n   malkarnenkar  g   mooney  r   saenko  k     guadarrama  s         
generating natural language video descriptions using text mined knowledge  in
annual conference of the north american chapter of the association for computational linguistics  human language technologies 
kulkarni  g   premraj  v   dhar  s   li  s   choi  y   berg  a  c     berg  t  l          baby
talk  understanding and generating simple image descriptions  in ieee conference
on computer vision and pattern recognition 
kuznetsova  p   ordonez  v   berg  a  c   berg  t  l     choi  y          collective
generation of natural image descriptions  in annual meeting of the association for
computational linguistics 
   

fiautomatic description generation from images  a survey

kuznetsova  p   ordonezz  v   berg  t  l     choi  y          treetalk  composition
and compression of trees for image descriptions  in conference on empirical methods
in natural language processing 
lampert  c  h   nickisch  h     harmeling  s          learning to detect unseen object
classes by between class attribute transfer  in ieee conference on computer vision
and pattern recognition 
lazebnik  s   schmid  c     ponce  j          beyond bags of features  spatial pyramid
matching for recognizing natural scene categories  in ieee conference on computer
vision and pattern recognition 
lebret  r   pinheiro  p  o     collobert  r          phrase based image captioning  in
international conference on machine learning 
li  s   kulkarni  g   berg  t  l   berg  a  c     choi  y          composing simple image
descriptions using web scale n grams  in the signll conference on computational
natural language learning 
liang  p   jordan  m  i     klein  d          learning dependency based compositional
semantics  computational linguistics                 
lin  c  y     hovy  e          automatic evaluation of summaries using n gram cooccurrence statistics  in annual conference of the north american chapter of the
association for computational linguistics  human language technologies 
lin  d   fidler  s   kong  c     urtasun  r          generating multi sentence natural
language descriptions of indoor scenes  in british machine vision conference 
lin  t  y   maire  m   belongie  s   hays  j   perona  p   ramanan  d   dollar  p     zitnick 
c  l          microsoft coco  common objects in context  in european conference
on computer vision 
lowe  d          distinctive image features from scale invariant keypoints  international
journal of computer vision                
ma  l   lu  z     li  h          learning to answer questions from image using convolutional
neural network  in aaai conference on artificial intelligence 
malinowski  m     fritz  m       a   a multi world approach to question answering about
real world scenes based on uncertain input  in advances in neural information processing systems 
malinowski  m     fritz  m       b   towards a visual turing challenge  in advances in
neural information processing systems workshop on learning semantics 
malinowski  m   rohrbach  m     fritz  m          ask your neurons  a neural based
approach to answering questions about images  in international conference on computer vision 
mao  j   xu  w   yang  y   wang  j     yuille  a  l       a   deep captioning with multimodal recurrent neural networks  m rnn   in international conference on learning
representations 
   

fibernardi et al 

mao  j   wei  x   yang  y   wang  j   huang  z     yuille  a  l       b   learning like
a child  fast novel visual concept learning from sentence descriptions of images  in
international conference on computer vision 
mason  r     charniak  e          nonparametric method for data driven image captioning  in annual meeting of the association for computational linguistics 
mason  w  a     watts  d  j          financial incentives and the performance of crowds 
in acm sigkdd workshop on human computation 
mitchell  m   han  x   dodge  j   mensch  a   goyal  a   berg  a  c   yamaguchi  k  
berg  t  l   stratos  k   daume  iii  h     iii         midge  generating image
descriptions from computer vision detections  in conference of the european chapter
of the association for computational linguistics 
nenkova  a     vanderwende  l          the impact of frequency on summarization  tech 
rep   microsoft research 
oliva  a     torralba  a          modeling the shape of the scene  a holistic representation
of the spatial envelope  international journal of computer vision                 
ordonez  v   kulkarni  g     berg  t  l          im text  describing images using   million
captioned photographs  in advances in neural information processing systems 
ortiz  l  m  g   wolff  c     lapata  m          learning to interpret and describe
abstract scenes  in conference of the north american chapter of the association of
computational linguistics 
panofsky  e          studies in iconology  oxford university press 
papineni  k   roukos  s   ward  t     zhu  w  j          bleu  a method for automatic evaluation of machine translation  in annual meeting of the association for
computational linguistics 
parikh  d     grauman  k          relative attributes  in international conference on
computer vision 
park  c     kim  g          expressing an image stream with a sequence of natural
sentences  in advances in neural information processing systems 
patterson  g   xu  c   su  h     hays  j          the sun attribute database  beyond categories for deeper scene understanding  international journal of computer vision 
                
pinheiro  p   lebret  r     collobert  r          simple image description generator via a
linear phrase based model  in international conference on learning representations
workshop 
prest  a   schmid  c     ferrari  v          weakly supervised learning of interactions
between humans and objects  ieee transactions on pattern analysis and machine
intelligence                 
rashtchian  c   young  p   hodosh  m     hockenmaier  j          collecting image annotations using amazons mechanical turk  in north american chapter of the association
for computational linguistics  human language technologies workshop on creating
speech and language data with amazons mechanical turk 
   

fiautomatic description generation from images  a survey

reiter  e     belz  a          an investigation into the validity of some metrics for automatically evaluating natural language generation systems  computational linguistics 
               
reiter  e     dale  r          building natural language generation systems  cambridge
university press 
ren  m   kiros  r     zemel  r          image question answering  a visual semantic embedding model and a new dataset  in international conference on machine learningt
deep learning workshop 
richardson  m   burges  c  j     renshaw  e          mctest  a challenge dataset for the
open domain machine comprehension of text  in conference on empirical methods in
natural language processing 
rohrbach  a   rohrback  m   tandon  n     schiele  b          a dataset for movie description  in international conference on computer vision 
rohrbach  m   qiu  w   titov  i   thater  s   pinkal  m     schiele  b          translating
video content to natural language descriptions  in international conference on
computer vision 
schuster  s   krishna  r   chang  a   fei fei  l     manning  c  d          generating
semantically precise scene graphs from textual descriptions for improved image retrieval  in conference on empirical methods in natural language processing vision
and language workshop 
shatford  s          analyzing the subject of a picture  a theoretical approach  cataloging
  classification quarterly          
silberman  n   kohli  p   hoiem  d     fergus  r          indoor segmentation and support
inference from rgbd images  in european conference on computer vision 
socher  r     fei fei  l          connecting modalities  semi supervised segmentation
and annotation of im  ages using unaligned text corpora  in ieee conference on
computer vision and pattern recognition 
socher  r   karpathy  a   le  q  v   manning  c  d     ng  a          grounded compositional semantics for finding and describing images with sentences  transactions
of the association for computational linguistics            
sun  c   gan  c     nevatia  r          automatic concept discovery from parallel text
and visual corpora  in international conference on computer vision 
thomason  j   venugopalan  s   guadarrama  s   saenko  k     mooney  r          integrating language and vision to generate natural language descriptions of videos
in the wild  in international conference on computational linguistics 
torralba  a   fergus  r     freeman  w  t             million tiny images  a large data
set for nonparametric object and scene recognition  ieee transactions on pattern
analysis and machine intelligence                    
ushiku  y   yamaguchi  m   mukuta  y     harada  t          common subspace for model
and similarity  phrase learning for caption generation from images  in international
conference on computer vision 
   

fibernardi et al 

vedantam  r   lawrence zitnick  c     parikh  d          cider  consensus based image description evaluation  in ieee conference on computer vision and pattern
recognition 
verma  y     jawahar  c  v          im text and text im  associating images and texts
for cross modal retrieval  in british machine vision conference 
vinyals  o   toshev  a   bengio  s     erhan  d          show and tell  a neural image
caption generator  in ieee conference on computer vision and pattern recognition 
xu  k   ba  j   kiros  r   cho  k   courville  a   salakhutdinov  r   zemel  r     bengio  y 
        show  attend and tell  neural image caption generation with visual attention 
in international conference on machine learning 
yagcioglu  s   erdem  e   erdem  a     cakici  r          a distributed representation
based query expansion approach for image captioning  in annual meeting of the
association for computational linguistics 
yang  y   teo  c  l   daume  iii  h     aloimonos  y          corpus guided sentence generation of natural images  in conference on empirical methods in natural language
processing 
yao  b     fei fei  l          grouplet  a structured image representation for recognizing
human and object interactions  in ieee conference on computer vision and pattern
recognition 
yao  l   torabi  a   cho  k   ballas  n   pal  c   larochelle  h     courville  a         
describing videos by exploiting temporal structure  in international conference on
computer vision 
yatskar  m   galley  m   vanderwende  l     zettlemoyer  l          see no evil  say no
evil  description generation from densely labeled images  in joint conference on
lexical and computation semantics 
young  p   lai  a   hodosh  m     hockenmaier  j          from image descriptions to visual
denotations  new similarity metrics for semantic inference over event descriptions 
transactions of the association for computational linguistics          
yu  l   park  e   berg  a  c     berg  t  l          visual madlibs  fill in the blank
description generation and question answering  in international conference on computer vision 
zhu  y   kiros  r   zemel  r   salakhutdinov  r   urtasun  r   torralba  a     fidler  s 
        aligning books and movies  towards story like visual explanations by watching
movies and reading books  in international conference on computer vision 
zitnick  c  l   parikh  d     vanderwende  l          learning the visual interpretation of
sentences  in international conference on computer vision 
zitnick  c  l     parikh  d          bringing semantics into focus using visual abstraction 
in ieee conference on computer vision and pattern recognition 

   

fi