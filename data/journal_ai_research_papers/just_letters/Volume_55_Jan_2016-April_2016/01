journal artificial intelligence research                   

submitted        published      

exploiting causality selective belief filtering
dynamic bayesian networks
stefano v  albrecht

svalb cs utexas edu

department computer science
university texas austin
austin  tx        usa

subramanian ramamoorthy

s ramamoorthy ed ac uk

school informatics
university edinburgh
edinburgh  eh   ab  uk

abstract
dynamic bayesian networks  dbns  general model stochastic processes
partially observed states  belief filtering dbns task inferring belief state  i e 
probability distribution process states  based incomplete noisy observations 
hard problem complex processes large state spaces  article 
explore idea accelerating filtering task automatically exploiting causality
process  consider specific type causal relation  called passivity  pertains
state variables cause changes variables  present passivity based
selective belief filtering  psbf  method  maintains factored belief representation
exploits passivity perform selective updates belief factors  psbf produces
exact belief states certain assumptions approximate belief states otherwise 
approximation error bounded degree uncertainty process  show
empirically  synthetic processes varying sizes degrees passivity  psbf
faster several alternative methods achieving competitive accuracy  furthermore 
demonstrate passivity occurs naturally complex system multi robot
warehouse  psbf exploit accelerate filtering task 

   introduction
dynamic bayesian networks  dbns   dean   kanazawa        general model
stochastic processes partially observed states  topology dbn compact
specification variables process interact transitions  cf  figure     given
possible incompleteness noise observations  may generally possible
infer state process absolute certainty  instead  may infer beliefs
process state based history observations  form probability distribution
state space process  often called belief state task calculating
belief states commonly referred belief filtering 
number exact approximate inference methods exist bayesian networks  see 
e g   koller   friedman        pearl        used filtering dbns 
applying unrolled dbn     slice repeated observed
time step  via successive update current posterior  belief state  used
c
    
ai access foundation  rights reserved 

fialbrecht   ramamoorthy

xt 

xt  
 

y t  

xt 

xt  
 

y t  



t  

figure    example dynamic bayesian network  dbn  two state variables two
observation variables  xti xt  
variables represent process states time

     respectively  yit   variables  shaded  represent observation time     
arrows describe variables interact 

prior next time step  see murphy         however  clear
unrolled variant becomes intractable network grows unboundedly time  even
successive update  exact methods become intractable high dimensional process
states approximate methods may propagate growing errors time  therefore  filtering
methods developed utilise special structure dbns maintain errors
propagated time   we defer detailed discussion methods section    
often  key developing efficient filtering methods identify structure
process leveraged inference  article  interested application
dbns representations actions partially observed decision processes 
pomdps  kaelbling  littman    cassandra        sondik        many variants 
dbns used represent effects actions decision process  specifying
variables interact information decision maker observes  many cases 
decision processes exhibit high degrees causal structure  pearl         mean
change one part process may cause change another part  experience
processes causal structure may used make filtering task
tractable  tell us beliefs need revised certain aspects
process state  example  variable x  figure   changes value variable x 
changed value  i e  change x  causes change x     seems intuitive use
causal relation deciding whether revise ones belief x    unfortunately 
current filtering methods take causal structure account 
refer type causal relation  between x  x    passivity  intuitively 
say state variable xi passive given action if  executing action 
subset state variables directly affect xi  i e  xi parents dbn 
xi may change value least one variables subset changed
value  worth pointing passivity occurs naturally frequently many
planning domains  especially robotic physical systems  mainzer        
following example  illustrates simple robot arm 

   mark end example solid black square 

    

fiexploiting causality selective belief filtering dbns

 
 

 

 

 



xa

b

xb

 

 a  robot arm gripper

c
 b  holding blocks b

figure    robot arm three rotational joints gripper  variables represent
absolute orientations corresponding joints 
example    robot arm   consider robot arm three rotational joints gripper 
shown figure  a  joints denoted           may take values
discrete set                     indicate absolute orientations  e g      means
joint points exactly right        means points left  
joint i  let two actions cwi ccwi rotate joint   clockwise
counter clockwise  respectively  uncertainty system could due stochastic
joint movements unreliable sensor readings joint orientations 
action cwi ccwi   variable passive value directly
modified action  however  variables j  i passive change
values corresponding preceding variable j  changed value  since changed
orientation joint j   causes changed orientation joint j  recall orientations
absolute   note accounts chains causal effects  indicated
arrows  orientation joint   changes orientation joint   changes  since joint
  causes joint   change  turn causes joint   change 
examples passivity seen context object manipulation 
blocks planning domain  e g  pasula  zettlemoyer    kaelbling         figure  b
shows arm holding blocks b a  top b  here  position b  xb  
passive respect joint orientations since change orientations
changed  furthermore  causal chain joint orientations position
block  xa    since position change bs position changes 

passivity exploited accelerate filtering task example 
fact state variables passive means aspects state may remain
unchanged  depending action choose  example  choose rotate joint
   fact joints     passive means unaffected action 
thus  seems redundant revise beliefs orientations joints      however 
precisely current filtering methods  cf  section    
concretely  assume use factored belief representation p                p         
p          choose rotate   direction  then  easy see need
update factor p           since   changes value  factor p           since
variables       passive  since parents        if any  change
values  know       change values either  show later  skipping
    

fialbrecht   ramamoorthy

p          result loss information cases  similarly chains
causal connections  cf  example     complex example planning domain
involving passivity  exploited  discussed section     
addition guiding belief revision  several features make passivity
interesting example causal relation  first all  passivity latent causal relation 
meaning readily extracted process dynamics without additional
annotation expert   in section    give procedure identifies passive variables
based conditional probability tables   furthermore  passivity deterministic
relation since passive variables may stochastic behaviour changing
values  finally  passivity relatively simple example causal relation  idea
exploiting passivity order accelerate filtering task intuitive  yet  best
knowledge  formalised explored rigorously before 
purpose present article formalise evaluate idea automatically
exploiting causal structure efficient belief filtering dbns  using passivity concrete
example causal relation  specifically  hypothesis large processes
high degrees passivity  structure exploited accelerate filtering task 
discussing related work section   technical preliminaries section   
contributions grouped following parts 
section    give formally concise definition passivity discuss various
aspects definition  definition assumes decision process specified
set dynamic bayesian networks  one action   discuss nonexample passivity  mean variables appear passive really
passive  finally  give simple procedure detect passive variables
based conditional probability tables 
section    present passivity based selective belief filtering  psbf  method 
following idea outlined above  psbf uses factored belief representation
belief factors defined clusters correlated state variables  psbf follows
  step update procedure wherein belief state first propagated
process dynamics  the transition step  conditioned observation  the
observation step   interesting novelty psbf way performs
transition step  rather updating belief factors  psbf updates
factors whose variables suspects changed  possible exploiting
passivity  to made precise shortly   similarly  observation step  psbf updates
belief factors determines structurally connected
observation  uses parts observation relevant
belief factor  thus allowing efficient incorporation observations  psbf
produces exact belief states certain assumptions approximate belief states
otherwise  discuss computational complexity error bounds psbf 
section    evaluate psbf two experimental domains  first evaluate psbf
synthetic  i e  randomly generated  processes varying sizes degrees passivity 
process sizes vary one thousand one trillion states  passivity
degrees vary          passivity  results show psbf faster
several alternative methods maintaining competitive accuracy  particular 
    

fiexploiting causality selective belief filtering dbns

results indicate computational gains grow significantly degree
passivity size process  evaluate psbf complex simulation
multi robot warehouse system style kiva  wurman  dandrea    mountz 
       show passivity occurs system psbf exploit
accelerate filtering task  outperforming alternative methods 
finally  discuss strengths weaknesses psbf section    conclude
work section    proofs found appendix 

   related work
exists substantial body work belief filtering partially observed stochastic
processes  section  review filtering methods utilise special structure
dbns situate work within related literature 
    approximate belief filtering dbns
several authors proposed filtering methods wherein belief state represented set
state samples  specifically  probability process state normalised
frequency state samples correspond s  methods commonly
referred particle filters  pf   see work doucet  de freitas  gordon       
survey  common variant pf  gordon  salmond    smith         filtering
task consists propagating current state samples process dynamics
subsequent resampling step based probabilities new state samples
would produced observation  two interesting features pf
applied processes discrete continuous variables  approximation
error converges zero increase number state samples 
known problem pf fact number samples needed acceptable
approximations grow drastically variance process dynamics  as shown
experiments  cf  section     rao blackwellised pf  rbpf   doucet  de freitas 
murphy    russell        developed address problem  rbpf assumes
state variables grouped sets r x distribution x
efficiently calculated r filtering  hence  sample rbpf consists
sample r corresponding marginal distribution x  rbpf useful
variance r relatively low variance x high  since reduces number
samples needed acceptable approximations 
boyen koller              recognised process consists several independent
weakly interacting subcomponents  belief state represented efficiently
product smaller beliefs individual subcomponents  seminal contribution show approximation error due factored representation essentially
bounded degree uncertainty  or mixing rates  process  precisely 
prove relative entropy  or kl divergence  kullback   leibler        two belief states contracts exponential rate propagated stochastic transition
process  based observation  propose filtering method  bk  wherein belief
state represented factored form belief factors updated using exact inference method  junction tree algorithm  lauritzen   spiegelhalter         since
    

fialbrecht   ramamoorthy

internal cliques used junction tree algorithm may correspond belief
state representation bk  final projection step typically performed
original factorisation restored  performance method depends crucially whether relevant correlations state variables captured small
clusters  whether projection step performed efficiently 
factored particle filtering  fp   ng  peshkin    pfeffer        addresses main drawbacks pf  many samples needed  bk  small clusters required  approximating
belief factors using set factored state samples  samples factored sense
assign values variables corresponding factor  allows fp
represent belief factors large bk  reduces number samples
needed due smaller number variables factor  authors provide different methods updating factored state samples  generic idea first perform
join operation full state samples reconstructed factored samples 
updated standard pf  updated samples projected
factored form using project operation  main drawback fp join
project operations essentially correspond standard relational database operations 
expensive 
murphy weiss        propose filtering method called factored frontier  ff  
uses fully factored representation belief states  is  belief state product
marginals individual state variable  allows compact representation
beliefs  algorithm works moving set state variables  the frontier  forward
backward dbn topology  requires certain variable ordering 
difficult attain intra correlations state variables  i e  edges within    
slice dbn  allowed  authors show method equivalent single
iteration loopy belief propagation  lbp   pearl         thus  similar lbp 
applied successive iterations improve approximation accuracy 
none works discussed explicitly address question causal relations
state variables exploited accelerate filtering task  or  alternatively 
filtering methods proposed therein implicitly benefit causal structure  method 
psbf  related bk fp psbf  too  uses factored belief representation 
belief factors defined clusters correlated state variables  therefore 
analysis approximation errors boyen koller        applies psbf 
show section   well experiments  however  contrast bk fp  psbf
perform inference complete factorisation  rather individual
factors  consequence  psbf require join project operation  one
main disadvantages bk fp 
    belief filtering decision processes
methods discussed preceding subsection used belief filtering decision
processes  including pomdps  kaelbling et al         sondik         regard 
methods viewed pure filters concerned belief filtering
control decision process  contrast combined filtering
methods  interleave filtering control tasks decision processes make
specific assumptions regarding solutions thereof  exists large body literature
    

fiexploiting causality selective belief filtering dbns

combined methods  including reachability based methods  hauskrecht        washington 
       grid based methods  zhou   hansen        brafman        lovejoy         pointbased methods  smith   simmons        pineau  gordon    thrun         compression
methods  roy  gordon    thrun        poupart   boutilier        
potential advantage combined methods access additional
structure may  therefore  utilise synergies filtering control tasks  one
synergy use decision quality guide belief filtering  rather metrics
relative entropy  poupart boutilier              propose filtering method  called
value directed approximation  chooses different approximation schemes different
decisions minimise expected loss decision quality  i e  accumulated rewards  
method assumes pomdp solved exactly value function
provided form  vectors represent available actions pomdp 
based value function  algorithm computes switching set alternative
plans determine error bounds approximation schemes  used search
optimal approximation scheme tree based manner  search traverses
approximate exact schemes 
idea using decision quality guide belief filtering appealing  method
involves series optimisation problems exhaustive tree search 
costly complex systems  advantage pure filtering methods  including proposed
method psbf  filter processes complex combined methods 
multi robot warehouse system studied section    actual control task
done via domain specific solutions  cf  section        
    substructure parameterisation
bayesian networks  hence dbns  allow compact parameterisation  i e  specification
probabilities  efficient inference via conditional independence relations  addition 
considerable work identifying substructure parameterisation
simplify knowledge acquisition enhance inference  koller   friedman       
boutilier  dean    hanks         property studied work  passivity  one example
substructure parameterisation  notable examples include causal independence  e g  heckerman   breese        heckerman        context specific independence
 boutilier  friedman  goldszmidt    koller        
causal independence assumption effects individual causes common
variable  i e  parents variable  independent one another  allows
compact parameterisation via operators noisy or  srinivas        pearl        
used enhance inference  zhang   poole         note passivity
conceptually much simpler property causal independence  passivity neither
concerned strength individual causes extent depend
other  moreover  passivity read directly parameterisation  cf  section     
whereas causal independence usually imposed designer 
context specific independence  csi  property states variable independent parents given certain assignment values  i e  context 
parents  non local csi statements follow similarly d separation  geiger  verma 
  pearl         allow reduction parameters  boutilier et al        
    

fialbrecht   ramamoorthy

enhancement inference  poole   zhang         discuss section    passivity viewed special kind csi applied dbns  parents respect
variable passive provide context csi  however  contrast csi 
passivity assume context actually observed 

   technical preliminaries
section introduces basic concepts notation used work  begin
brief discussion decision processes provide context work  followed
discussion dynamic bayesian networks model perform inference 
    decision processes  belief states  exact updates
consider stochastic decision process wherein  time t  process state
st decision maker  agent  choosing action   executing st  

process transitions state st   probability  st   st     agent receives

observation ot   probability  st     ot      assume factored representations
state space observation space o    x      xn   y      ym  
domains xi   yj finite  notation si used denote value xi
state s  analogously oj o  moreover  assume process
time invariant  meaning independent t  framework compatible
many decision models used artificial intelligence literature  including pomdps
 kaelbling et al         sondik        many variants 
agent chooses action based belief state bt  also known information state  
represents agents beliefs likelihood states time t  formally 
belief state probability distribution state space process  belief filtering
task calculating belief state based history observations  ideally 
resulting belief state exact retains relevant information past
observations  this sometimes referred sufficient statistic  cf  astrom        
exact update rule simple procedure produces exact belief states 
definition    exact update rule   exact update rule defined follows  taking
action observing ot     belief state bt updated bt   via
bt    s     

x



bt  s   s  s   

   

ss


bt    s      bt    s     s    ot    

   

normalisation constant 
sometimes refer step bt bt   transition step step bt   bt  
observation step  unfortunately  space complexity storing exact belief states
time complexity updating using exact update rule exponential
number state variables  making infeasible complex systems large state
spaces  hence  efficient approximate methods required 
    

fiexploiting causality selective belief filtering dbns

    dynamic bayesian networks
dynamic bayesian network  dbn   dean   kanazawa        bayesian network
special temporal semantics specifies stochastic process transitions one
state another  dbns used model effects actions stochastic decision
process  specifically  compact representation transition function
observation function oa action a 
definition    dbn   dynamic bayesian network action a  denoted   acyclic
directed graph consisting of 




t   xt   xt   x  
state variables x   xt         xtn x t     xt  


         xn
representing states process time      respectively 


t   t     representing obser observation variables t     y t          ym
j
j
vation received time     




directed edges ea x x t   x t   x t   x t   t   t   t    
specifying network topology dependencies variables 
conditional probability distributions pa  z   paa  z   variable z x t   t    
specifying probability z assumes certain value given specific assignment
parents paa  z     z      z     z  ea    convenience  define pata  z   
t   pa  z   pa  z   
x paa  z  pat  


zz paa  z  
 z    x
edges ea distributions pa define functions


 

 s     

n


 
pa xt  
  s i   paa  xt  

     s   



   

i  

 s    o   






pa yjt     oj   paa  yjt        s    o 

   

j  
t  
 
use notation paa  xt  

     s    specify parents xi
t  
 
x   respectively  assume corresponding values   formally 
t  
t  

xtl pata  xit     xt  
pat  
  s l    similarly  use
 xi    xl   sl xl 
l 
t  
 
notation paa  yj      s   o  specify parents yjt   x t   t    
respectively  assume corresponding values s  o 

xt

example    dbn representation robot arm   represent robot arm example   set dbns  one dbn action
 cw
  ccwi   
at  
t  





state observation
variables
       t      t    
n
dbns x               x

t      t      t      t     make example realistic  let us assume
joint orientations bounded relative orientation immediately preceding joint
 e g  form cone   first joint bounded relative ground 
means joint movement depends well preceding joint orientation  shown figure    moreover  joint orientations correlated  i e  edges within
    

fialbrecht   ramamoorthy

 t

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

xt

x t  

t  

figure    dbn representation robot arm 
x t     joint exceed bound given preceding joint  finally  observation variables depend solely corresponding joint variable  actions
example would differ variable distributions pa  

    additional definitions
useful define following 
xt  

binary order defined x x t   xti xtj xt  

j
t  

    j n  xi xj   i  j n 
given set z x x t     write z denote tuple contains variables
z  ordered  
given ordered tuple z    zi         zi z     define set s z    xi       xi z 
contain value tuples variables z 
given value tuple sz    si         si z    s z   use notation z   sz
abbreviation zil   sil zil z  i e  variables z assume
corresponding values sz   

   passivity
section introduces formal definition passivity  used basis
remainder article  provide simple procedure detect passive
variables process dynamics 
    formal definition
outlined section    state variable xt  
called passive action exists

 in dbn   xt   may change value
subset xt  

parents

x


    

fiexploiting causality selective belief filtering dbns

least one variables subset changed value  conversely  xt  


change variables subset change  formally  define passivity follows 
t  

definition    passivity   let action given dbn

  state variable xi
t  


called passive exists
set a i paa  xi     xi that 

t  
 i  xtj a i   xt  
ea
j   xi

 ii  two states st st    st   st          


sti   st  
xtj a i   stj   st  
j


   

state variable passive called active 
set a i corresponds subset variables described above  contains
variables directly affect xit    i e  parents xt  
x   xt  
may


change value variables a i changed value  sometimes say
variable xt  
passive respect another variable xtj case

xtj a i   furthermore  omit obvious context 
clause  i  definition   requires xt  
intra correlated variables a i  

specifically  edge xt  

xt  
xtj a i   example  see
j

figure   assumed variable xt  
passive respect variable
 
xt     we discuss purpose clause next subsection   clause  ii  defines
core semantics passivity requiring xt  
remains unchanged variables

a i remain unchanged  note means distribution pa xt  
may specify

deterministic stochastic behaviour variables a i change values 
includes xt  
may change value all 

state variable xit   passive even parents x   none
xti   case  set a i would empty clause  i  well premise    
would trivially hold true  however  variable passive change
value circumstances  words  would constant 
case  one consider removing variable state description order reduce
computational costs 
noted section      passivity shown special kind context specific
independence  csi   boutilier et al         applied dbns  here  associated set a i
passive variable xt  
provides context  given assignment values xtj a i  i e 

t  
context  xtj   xt  
independent xtk   xt  
xtk pata  xt  
j   xi
    a i
k
k    i  however  besides similarity  important difference passivity
csi  passivity actually assume context observed  thus 
passivity viewed kind csi unobserved contexts  become clear
section    describe filtering method exploits passivity 
    non example passivity
purpose clause  i  definition passivity  all  discussed
previously  clause  ii  captures core idea passivity  variable may
change value variables respect passive changed value 
    

fialbrecht   ramamoorthy

xt 

xt  
 

xt 

xt  
 

figure    example process clause  ii  insufficient 
however  may seem intuitive clause  ii  sufficient passivity 
fact processes clause  ii  alone suffice  words  clause  ii 
necessary sufficient passivity  illustrate following example 
example    non example passivity   consider process two binary state variables 
x    x    single action  a  shown figure     we omit observation variables
clarity   dynamics process xt  
takes value xt  xt  
takes
 
 
value xt   i e  x  x  swap values time step   process 
state variables satisfy clause  ii  definition    set x     x    i e  initial values  
 st   st     positive states st   st     hence     true  set x      x    
 st   st     positive states st   st   sti    st  
          hence    
trivially true since premise false 

despite satisfying clause  ii   state variables xt  
xt  
example  
 
 
fact passive  following two reasons  firstly  passivity causal relation
must imply causal order  pearl         however  causal order
x  x    edge xt  
xt  
 
    secondly  passivity means
variable may change value another variable respect passive  a
variable a i   changed value  words  whether passive variable xt  

may change value depends past values a i  at time t  new values
a i  at time       however  variables example   depend values
time t  hence values time     predetermined depend whether
variables a i change values 
first issue  namely causal order  addressed adding corresponding edges x t     instance  example   could add edge xt  
xt  
 
 
establish causal order  however  generally solve second issue 
every passive variable xit   must depend past new values variables a i   words  xit   must inter correlated well intra correlated
variables a i   former given definition  since every variable a i
parent xt  
  latter precisely required clause  i  definition   
therefore  clauses  i   ii  together define formal meaning passivity 
    detecting passive variables
mentioned section    passivity latent causal property sense
extracted process dynamics without additional information  additional
assumptions regarding representation variable distributions  order determine
    

fiexploiting causality selective belief filtering dbns


algorithm   passive xt  
   

  


input  state variable xt  
  dbn

output  a i xt  
passive   else false


t  
   q orderedqueue p pata  xi     xti
   ascending order  a i  
  

  

q   

  

a i nextelement q 

  

q q    a i  

  

xtj a i


t  
xt  
  ea
j   xi

  
  
   
   
   
   
   
   
   

go line      clause  i  violated

a i paa  xt  
    a i xti

n

t  

t  

x
 
x
a i
j
a i
j
s a i    s a i    si xi


t  
 s  
pa xt  
 

 
x

 


 




a i
a i


   


a i
go line      clause  ii  violated
return a i
return false

variable xit   passive   one find set a i clauses definition  
satisfied  simple procedure representation variable
distributions given algorithm    algorithm takes inputs variable xt  


t  


dbn   checks whether xi passive searching set a i satisfies
clauses definition    note power set p line   includes empty set  
hence accounts a i     lines     check clause  i  satisfied lines
      check clause  ii  satisfied  line    essentially checks     holds true 
clauses satisfied  xt  
passive respect variables a i  

algorithm returns set a i   otherwise  algorithm returns logical false  
time complexity algorithm   exponential worst case  xt  


passive  specifically  time requirements line   grow exponentially number
parents xt  
x   time requirements line    grow exponentially

cardinality a i a i   however  time requirements reduced significantly
committing specific representations variable distributions pa   example 
distributions represented tabular form  one utilise arrays indices
perform sweeping tests      i e  line     moreover  important realise
algorithm needs performed state variable  prior start
   strictly speaking  algorithm   checks property stronger passivity
check  st   st          cf  clause  ii   line     however  algorithm modified include
check  omit exposition order highlight core ideas behind algorithm 

    

fialbrecht   ramamoorthy

process demand  since passivity invariant process states 
words  variable passive   always passive   therefore  suffices
check advance passivity 
note set a i necessarily unique  example  consider
variable xt  
 

passive respect variables xt  xt    i e  a     xt    xt    assume
xt  
changes
x t   changes i e 
 

change time   then 
 

  

easy verify a     x  a     x  satisfy clauses  i   ii   hence
a      a       a   valid sets definition passivity  guiding principle
cases occams razor  which  intuitively speaking  states simplest explanation
suffices  case  means suffices use smallest set a i terms
cardinality  a i     hence  line   algorithm   sorts queue q ascending order
 a i     rationale exist multiple causal explanations passive variable
xt  
  one involving fewest key variables favoured since reduces
 compared alternative explanations  number cases would
revise beliefs xit     earlier example  accept a   causal explanation
t  
xt  
every time xt  
xt  
may
    would revise beliefs x 
 
 
 
changed values  however  accept a   causal explanation  would
revise belief x t   xt  
may changed value  difference
 
become obvious section      explains passivity exploited
reduce computational costs 

   passivity based selective belief filtering
section presents passivity based selective belief filtering  psbf  method 
exploits passivity efficient filtering  discussed section    assume process
specified set dynamic bayesian networks contains one dbn
action a  therefore  whenever refer action  e g      pa   paa   
assumed context  
psbf follows general two step update procedure belief state first
propagated process dynamics  transition step  conditioned
observation  observation step   thus  natural divide exposition psbf
three parts      belief state representation      transition step      observation
step  discussed sections                respectively  summary psbf
given section      discuss computational complexity error bounds
psbf sections          respectively 
    belief state representation
recall section   principal idea behind psbf maintain separate beliefs
individual aspects process  exploit passivity order perform selective
updates separate beliefs  union individual aspects constitutes complete
state description process  therefore  belief state represented product
separate beliefs individual aspects 
capture informal notion individual aspects formally form clusters 
defined follows 
    

fiexploiting causality selective belief filtering dbns

c 
 t

 t  

c 

c 
 t

 t  

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

 t  

 t

 t  

 t  

 t

 t  

 t  

 t  

c 
 t

 t  
c 

 t

 t  

c 
 a  c    c    c 

 b  c 

 c  c    c 

figure    three clusterings robot arm dbn 
definition    cluster   clustering x t   set c    c         ck   satisfies
k   ck x t   c      ck   x t     refer elements ck c clusters 
underlying idea behind concept clusters variables cluster ck
connected important sense  specifically  two variables common
cluster  exists relation variables regarding likelihood
values may assume  words  variables correlated x t    
number k concrete choice clusters ck specified user
generated automatically  example  may specified manually domain expert
familiar structure modelled system  generated automatically using
methods ones described section      stressed  however 
order reduce computational costs  advisable follow general rule small
possible  large necessary choosing clusters  see section     discussion
computational complexity   therefore  two variables strongly correlated 
presumably common cluster  whereas weakly
correlated  weakly meaning correlation ignored safely  
separate clusters order reduce computational costs  illustrated
following example 
example    clusters robot dbn   recall robot arm dbn example    specifit   given three clusters
cally figure
cluster
state
t      one way
t  
t  
variables x
c     
  c     
  c     
  shown figure  a  clustering
efficient since minimises size cluster  however  clusters fail capture
important correlation joint orientation restricted preceding joint
orientation
i    another

way cluster state variables given single cluster
c     t      t      t     shown figure  b  clustering captures correlations
variables  however  largest possible cluster

and  therefore 

least effi
cient one  compromise given two clusters c     t      t     c     t      t    
shown figure  c  clustering captures correlation joint orientations immediately preceding joint orientations  efficient
previous clustering since smaller clusters 

    

fialbrecht   ramamoorthy

given definition clusters  capture informal notion separate beliefs
form belief factors 
definition    belief factor   given cluster ck   corresponding belief factor bk
probability distribution set s ck   
intuitively  belief factor bk represents agents beliefs likelihood values
variables corresponding cluster ck   analogy view belief factor
smaller belief state  view b full belief state combination
smaller belief states  however  distinguish two  refer b simply belief
state bk belief factor 
finally  given clusters ck corresponding belief factors bk   belief state b
represented factored form
b s   

k


bk  sk  

k  



use notation sk refer tuple  si  xt   ck    e g   ck   xt  
  xt  
 
 

   s    s    s    s     sk    s    s     
    exploiting passivity transition step
order perform selective updates belief factors bk   require procedure
performs transition step independently factor   obtain procedure
introducing two assumptions allow us modify transition step     exact
update rule  assumptions guarantee transition step performed exactly 
sense      however  discuss shortly  assumptions violated obtain
approximate belief states 
first assumption   a    states clusters must uncorrelated  i e 
edges x t   clusters   second assumption   a    states clusters
must disjoint  formally  defined follows 
t  
 a     xt  
ck pat  
 xi   ck


 a   k    k     ck ck   
note neither assumption implies other  is  may case  a  
satisfied  a   violated  vice versa  assuming  a    a   
reformulate    
x


 
bt  
tka  s  s k  
btk   sk   
   
k  sk      
s pat  ck   


k    xt  
ck    xti pat  ck   



  normalisation constant


 
tka  s  s k    
pa xt  
   s k  i   paa  xt  

     s  sk    
xt  
ck


   advantage belief factors updated parallel  useful feature
considering many platforms use parallel processing techniques 

    

fiexploiting causality selective belief filtering dbns

procedure performs transition step independently belief factor bk   hence
updated order parallel 
assumption  a   allows us bring     form updates belief
factors bk independently other  specifically   a   allows us define cluster based
transition function tka   turn enables summation      assumption  a   
hand  guarantees product     correct  particular  may
case  sk       ck     i e  fewer elements sk  ck    variables
ck  patat  ck    i e  xt  
ck  xti
  patat  ck     cases  btk 

t  
taken marginal distribution variables xi ck  xti patat  ck   
 a   guarantees marginalisation introduces errors 
mentioned previously  assumption may violated obtain approximate belief
states  however  important distinction  a    a   regard 
 a   violated      still well defined sense still executed 
except product     may degrade accuracy results  contrast
 a    structural requirement tka sense tka ill defined without  a   
since   a   violated  variables ck may parents x t  
 
ck   case paa  xt  
     s  sk   would ill defined  thus   a   violated 
enforce modifying distributions pa xt  
ck marginalise

t  
variables pat  
 x
 




c
 


clusters
c
  means

k
k


variable separate distribution every cluster contains variable  thereby
possibly introducing approximation error 
given modified transition step      exploit passivity perform selective
updates belief factors bk   recall section     variable xt  
passive

t  

exists set a i variables xi may change value
variables a i changed value  causal connection used decide
whether values variables cluster ck may changed  case
corresponding belief factor bk updated  theorem   provides formal foundation 


theorem     a    a   hold  xt  
ck passive  


  bt  
k  sk     bk  sk   

proof  proof appendix a 
theorem   states clusters c         ck disjoint uncorrelated 

variables cluster ck passive   transition step corresponding
belief factor btk bt  
omitted without loss information 
k
theorem   translate situations  a    a    both  violated 
key assumption  a    states clusters must uncorrelated 
discussed earlier  enforce modifying variable distributions pa cluster 
however  passive variable xit   ck correlated  passive active  variable
t  
t  
xt  
ck    xt  
pat  
distribution pa
 xi    marginalising xj
j
j
t  
t  
xi typically cause xi lose passivity  sense would longer satisfy
clauses definition    consequently  would always perform transition
step ck   even unmodified variables ck passive  problematic
unnecessary computations  modified distributions
introduce error every time transition step performed 
    

fialbrecht   ramamoorthy

 t

 t  

 t  
c 

 t

 t  

 t  
c 

 t

 t  

 t  

xt

x t  

t  

figure    robot arm dbn implementing action cw    dashed circles mark passive state
variables  coloured ellipses represent clusters c  c   
alleviate effect  one check chance unmodified variables
cluster would change values  shown case whenever
causal path active variable variable cluster 
definition    causal path   causal path   active variable xt  
another

t  
t    q 
t  
   
   
 q 
   
variable xj   sequence hx   x        x x   xi   x
  xj  
  q   q  
 i  x q  x t  
 ii  x q    x q    ea
 iii  x q    passive respect x q 
intuitively  causal path defines chain causal effects  such joints    
example     since active variable x    may changed value x    passive
respect x      x    may changed value  since x    may changed
value x    passive respect x      x    may changed value  etc 
hence  absence observing changes  mere existence causal path
x    x q  reason revise beliefs x q    therefore  general update rule 
omit transition step btk bt  
unmodified variables cluster ck passive
k


  causal path active variable variable ck  
demonstrated following example 
example    psbf update rule robot arm dbn   let us consider robot arm
previous examples  figure   shows dbn implements action cw   
action rotates joint   robot arm   clock wise  i e  joint orientation  t  
direct target action   therefore  variable  t   active variables
 t    t   passive  shown dashed circles  

use clustering c     t      t     c     t      t   reasons given example    since  t   parent  t     psbf enforce assumption  a  
    

fiexploiting causality selective belief filtering dbns

algorithm   skippableclusters c   
  

input  clustering c    c         ck    dbn

  

output  set clusters c c skipped transition step

  

c c

  

q orderedqueue x t    

  

c    q   

  
  
  
  
   
   
   
   
   

xt  
nextelement q 



q q   xt  


passive xt  
   


c c   ck c   xt  
ck


xt  
q
j
t  

causalpath xt  
  xj    
n

c c   ck c   xt  
ck
j
n

q q   xt  
j

return c

marginalising  t   variable distribution pa  t   cluster c    modified variable distribution loses passivity property  both clauses definition  
violated   unmodified distribution  t   still passive 
performing transition step  psbf update belief factor b 
corresponding cluster c  contains active variable  t     however  since variables
cluster c  passive  there modified variables c     since causal
path  t   variable c    psbf omit update belief factor b   
intuitively  makes sense since change orientation joint   cannot cause
change orientations preceding joints  note corresponds saving
    transition step 


algorithm   defines procedure utilises rule find clusters
transition step skipped  algorithm takes inputs clustering c dbn
  returns set c skippable clusters  essentially searches active
variables xt  
removes clusters ck c contain variables

causal path xit     function orderedqueue x t     returns ordered
queue q variables x t     performance algorithm   depends order
queue  experiments  obtained good performance ordering variables
descending order number outgoing edges  function nextelement q  returns

next element queue  function passive xt  
    defined algorithm   
t   t  

function causalpath xi   xj     returns logical true
    

fialbrecht   ramamoorthy

causal path xt  
xjt      note that  given invariance passivity process

states  cf  section       suffices call algorithm    in advance needed 
determine clusters omit transition step 
    efficient incorporation observations
psbf perform observation step similarly exact update rule     
conditions propagated belief state bt   observation ot   obtain fully updated
belief state bt     however  given factored belief state representation used psbf 
require procedure respects factorisation observation step  assuming
 a    a   hold  bring     form updates belief factors bk
independently
x


t    
 
 
bt  
 s  ot    
bt  
   
k  sk       bk  sk  
k   sk  
t          s  k      k   c   pat    y t       
s pat  
k
k
 y

k




  normalisation constant  note that  analogously      variables
t      bt   taken marginal distribution
ck  pat  
k 
 y
t  
t  
ck  paat  y
   assumption  a   guarantees marginalisation introduces
errors   a    a   hold  transition step     observation step    
produce exact belief states sense          regardless many clusters
skipped transition step  cf  theorem    
observation step     updates belief states uses observation variables
process  words  ignores internal structure observation variables 
however  clear variables cluster ck marginally independent
observation variables t    this determined using d separation  geiger et al         
simply checking directed path ck t      need
perform observation step corresponding belief factor bk   expressed
formally theorem   


theorem    xt  
ck marginally independent yjt   t    

t  
  bt  
k  sk     bk  sk   

proof  proof appendix b 
theorem   states variables ck independent t    
observation step bk skipped  however  even ck independent t     may
case variables ck depend subset yk t   observation
variables  clearly  cases  suffices use yk rather t   observation
step  account this  first note variables t   may correlated
other  preserve correlations  subdivide t   clusters cl t  
introduce following assumptions 


 a     yjt   cl paa  yjt     t   cl
 a   l    l    cl cl   
   simple way implement function modify standard graph search method  such breath first
search  check  iii  definition    apply variables x t   edges ea  

    

fiexploiting causality selective belief filtering dbns

assumptions  a    a   analogous  a    a    respectively  essentially
serve purposes observation step  distinguish clusters ck cl  
sometimes refer former state cluster latter observation cluster 
assuming  a    a   hold  redefine observation step
x


t    
 

t  
 
bt  
 s
 
 

b
 s
 

 s 

 
bt  
   
 
k
k
l
k
k
k   sk  
t  
 
 
l  cl yk    s pat  
 cl      sk   sk k    k   ck  pa  cl     



al  s  ot  
l    







t  
t  
pa yjt     ot  
 
 
pa
 y
 
 s 

 
j
j
l
l

yjt   cl

yk t   set observation variables marginally independent
variables ck  
given theorem    one see     equivalent     observation variables
clustered  or  equivalently  single observation cluster cl   t      however 
important note observation variables clustered  i e  multiple
observation clusters cl        notqnecessarily
equivalent
p
p    
qmto see this  helpful
compare abstract formulations

 o
 
b

j  
j

j    oj   bs  
former corresponds     latter      therein   o         om   observation  bs
probability state s   oj   probability observing yj   oj
s  abstract formulations equivalent     bs     s 
cases may equivalent  nonetheless  fix number observation
variables m      approximates     closely increase number state variables
n  experiments indicate often suffices use state variables
observation variables order obtain good approximations 
finally  show suffices perform observation step bk using
clusters cl whose variables independent variables ck   observe    
fact repeated application     every cl   updated belief factor bt  

k
t  
used place bk subsequent application  since every application
form      with t     cl    conclude theorem   holds  hence observation
step skipped clusters cl independent ck  
    summary psbf
preceding sections summarised follows 
representation  belief state bt represented product k belief factors btk  
q


bt  s    k
k   bk  s   belief factor bk probability distribution
set s ck    ck x t   cluster correlated state variables 
transition step  transition step btk bt  
performed using      clusters
k


ck include active variables   causal path

active variable   clusters skipped 
observation step  observation step bt  
bt  
performed using     
k
k
clusters ck dependent observation variables t     using
observation clusters cl relevant ck   clusters skipped 
    

fialbrecht   ramamoorthy

algorithm   psbf at   ot      btk  ck c   c  c   a  aa  
  

input  action   observation ot     belief factors  btk  ck c

  

parameters  state clustering c  observation clustering c  dbns  a  aa

  

output  updated belief factors  bt  
k  ck c

  

   transition step

  

c skippableclusters c   

  

ck c

  
  
  
   
   



ck c
bt  
btk
k
else
s k s ck  
x

 
bt  
tka  s  s k  
btk   sk   
k  sk    
s patat  ck   

   

k    xt  
ck    xti patat  ck   


   observation step

ck c
n


   
yk yjt   t     directed path ck yjt  
   

   

yk  

   

bt  
bt  
k
k

   
   
   

else
s k s ck  
t    
 
bt  
k  sk     bk  sk  



x



al  s  ot    



 
bt  
k   sk  

 cl     
cl c   cl yk    s pat  
 cl      sk   s k k     k   ck  pat  



   

return

 bt  
k  ck c

algorithm   provides procedural specification psbf  algorithm takes inputs
action time t    subsequent observation time      ot     belief factors
time t  btk   internal parameters state clustering c  observation clustering
c  set dbns  a  aa define process  lines      implement
transition step lines       implement observation step  note suffices
execute lines      advance  or demand  remember results
future reference  algorithm returns updated belief factors bt  
k  
    

fiexploiting causality selective belief filtering dbns

    space time complexity
belief factor bk one elementp
bk  sk   sk s ck     thus  total space required
maintain k belief factors bk k
k    s ck     furthermore  size set s ck   grows
exponentially number variables ck   hence dominant growth factor
space requirement given largest cluster ck  ck     maxk   ck     therefore 
space complexity psbf o exp maxk  ck     hence representation feasible
reasonably small clusters ck  
similarly  numberpof operations required perform transition observation
steps order   k
k    s ck    worst case  i e  clusters need updated
steps   specifically  line    line    algorithm   executed
every sk ck   dominant growth factor given largest cluster ck   hence
time complexity psbf o   exp maxk  ck      o exp maxk  ck     note
assumes analysis performed lines      algorithm   done advance 
time complexity worst case  clusters need updated
transition observation steps  difficult derive time complexity
average case unclear average case terms passivity  even
stipulate certain average degree passivity  e g      variables passive   would
still difficult make general statement time requirements since depends
crucially passive variables distributed across clusters  example  even
process average     passivity  one active variable cluster
every cluster would need updated transition step  thus  general
statement make regards passivity time complexity psbf
refined o exp maxck ct co  ck     ct co include clusters
need updated transition observation step  respectively 
    error bounds
five possible sources approximation errors psbf 
clusters correlated  i e   a    a   violated 
clusters overlapping  i e   a    a   violated 
generally     multiple observation clusters cl used
first two cases  approximation error depends amount correlation
overlap  little correlation overlap clusters 
approximation error expected small  conversely  clusters strongly
correlated overlapping  approximation error expected large 
boyen koller        provide useful analysis error bound filtering
method uses factored belief state representation  since psbf uses factored
representation  analysis applies directly psbf  purpose section
restate main result analysis context work 
analysis uses concept relative entropy  kullback   leibler       
measure similarity belief states 
   practice  suffices store  s ck      elements  irrelevant analysis 

    

fialbrecht   ramamoorthy

definition    relative entropy   let two probability distributions defined
set x  relative entropy defined
kl      

x
xx

 x  ln

 x 
 x 

 x       x      
similar boyen koller         define approximation error incurred psbf
relative exact belief state  however  since consider decision process multiple
actions  represented dbns    define error action respectively 
definition    approximation error   let b exact belief state b approximation psbf  taking action a  let b  exact update b  using         
b  psbf update b  using           furthermore  let b  exact update
b  using           say psbf incurs error relative b 
kl b    b    kl b    b     
analysis relies concept mixing rates  intuitively  mixing rate
dbn quantifies degree stochasticity   depends mixing rates ka
individual clusters ck  
definition    mixing rate   mixing rate cluster ck x t   defined
ka    min
  

 s

x



min tka  s    s   tka  s     s   

ss ck  

ck satisfy  a    a    observation variables t   one observation
cluster  mixing rate given    mink ka  r q cluster ck
depends r influences q clusters ck    k  boyen   koller        
worst case  that is   a a   violated   minimal mixing rate given ka
single cluster ck   x t    
finally  main result work boyen koller         restated
context work theorem    essentially states approximation error psbf
 measured terms relative entropy  bounded mixing rates process 
theorem    boyen   koller         let bt exact belief state bt approximation psbf using clusters ck   then  states  s    s         st   actions
 a    a         at    
h
max

 a
eo       ot kl bt   bt  
mina  a
expectation e q
taken possible sequences observations o         ot
 

           defined above 
probabilities p  o            t 
    s
    

fiexploiting causality selective belief filtering dbns

process size

  x vars  n 

  vars  m 

  states   s  

  obs    o  



  

 

  one thousand

 



  

 

  one million

  

l

  

 

  one billion

   

xl

  

  

  one trillion

    

table    synthetic process sizes  variables binary 

   experimental evaluation
evaluated psbf two experimental domains  section      evaluated psbf
synthetic  i e  randomly generated  processes varying sizes degrees passivity 
section      evaluated psbf simulation multi robot warehouse system  brief
summary experimental results given section     
    synthetic processes
first evaluated psbf series synthetic processes  psbf compared selection
alternative methods  including pf  gordon et al          rbpf  doucet et al          bk
 boyen   koller          murphy   weiss         see section   discussion
methods  algorithms implemented matlab       used matlab
toolbox bnt  murphy        implement bk ff 
      specification synthetic processes
generated synthetic processes four different sizes specified table   
process generated follows 
first  variable xit   chosen passive probability p  case
add edge  xti   xt  
   refer p degree passivity  sample

t  
edges x  x
x t     generate mixture gaussians g using algorithm    see
appendix c   figure   shows example g generated process size m  set
g used produce areas correlated variables  i e  gaussians  
constitute natural candidates state clusters 
let vector maximum densities gaussian g  let
vector densities value n  then  every combination j  edge  xti   xt  
j  
 
added probability equal maximum element j     operators
point wise  xt  
chosen passive  edge  xti   xt  

j   added
t   t  
t   t  
  j  case  add edge  xi   xj    edges  xi   xj   added similarly
t  
  j   add edge  xti   xt  
j   passive xj   ensure every
variable effect generated process  xti connected least one xt  
j
t  
x t    adding
 adding  xti   xt  
 

necessary 


x


least
one
parent

x

j
   condition   j cases ensure resulting dbn acyclic 

    

fialbrecht   ramamoorthy

    
   

density

    
   
    
   
    
 

 

 

 

 

  

  

  

  

  

  

i  j

figure    example mixture gaussians generated process size consisting
t t  
three gaussians  closer two variables xi
xt  
peak common
j
gaussian  higher probability edge added them 
t   t  
 xtj   xt  
j   necessary   finally  edges  xi   yj   added probability     
i  j  ensuring yjt   least one parent x t    
variables process binary  passive variables assumed passive
respect parents x   distributions pa xt  
x t   generated

t  
uniformly randomly without bias  passive variables xi   modify pa satisfy clause
 ii  definition    distributions pa yjt   t   generated probability
sampled uniformly either                        obtain meaningful observations 
finally  every process consists two actions  obtained randomly choosing
one three variables xt  
whose distributions pa resampled

edges x added probability      passive variables chosen way longer
passive   simulations  actions chosen uniformly randomly 
process starts random initial state  algorithms tested
sequence processes  initial states  chosen actions  random numbers 

      clustering methods
used three different clustering methods  denoted hpci  hmorali  hmodisi  methods
applied variables x t   without edges involving x t    
hpci drops directions edges  i e  edge xt  
xt  
ads reverse

j
t  
t  
edge xj xi   puts variables  undirected  path
one cluster  definition  resulting clusters satisfy assumptions  a a   
hmorali connects parents variable drops directions  it moralises
variables  extracts clusters fully connected variables  maximum cliques  
resulting clusters may satisfy assumptions  a a   
hmodisi similar hmorali truncates resulting clusters make disjoint
 clusters removed become subset another cluster   definition 
resulting clusters satisfy  a  a    necessarily  a  a   
example  consider figure   section      here  hpci would produce cluster
c  figure  b  since variables connected undirected path  furthermore 
    

fiexploiting causality selective belief filtering dbns

hmorali would produce two clusters c  c  figure  c  correspond
two maximum cliques moralising variables x t     finally  hmodisi would produce
cluster c  figure  c cluster c  figure  a 
psbf used clustering method generate clusters state variables  ck  
observation variables  cl    moreover  psbf enforced  a  a   whenever necessary
modifying variable distributions described section     
      accuracy
order compare accuracy tested algorithms  computed relative entropy
 cf  definition    exact belief states obtained using exact update rule  cf  definition   
approximate belief states produced tested algorithms  however  since exact
belief states relative entropy hard compute large processes  able
compare accuracy algorithms processes size only  algorithms initialised
uniform belief states  uniformly sampled particles 
first compared accuracy psbf bk  since use factorisation
belief state representations  figure   shows relative entropy psbf bk averaged      processes                              passivity  respectively 
results show psbf hpc modisi produced lower relative entropy  i e  higher accuracy  bk hpc modisi  psbf hmorali produced relative entropy comparable
bk hmorali  indicates violations  a  a   introduce smaller errors
violations  a  a    note psbf bk convergent behaviour
relative entropy  shows approximation error due factorisation
bounded  discussed section      interesting since psbf bk obtain approximation errors factorisation different ways  psbf loses accuracy modifying
variable distributions ensure state clusters independent  cf  section      
bk loses accuracy marginalising original factorisation inference  i e 
projection step  cf  section       nevertheless  shown results  resulting
approximation errors bounded cases  similar convergence 
note relative entropy methods increased degree passivity
process  explained fact higher passivity implies higher determinacy
and  therefore  lower mixing rates  cf  definition     crucial factor error
bounds psbf bk  cf  theorem     finally  note psbf produce exact
belief states  i e  zero relative entropy  using hpci clustering  despite fact
clusters generated hpci satisfy assumptions  a a    however  discussed detail
sections          another possible source approximation errors multiple observation
clusters used  often case using hpci produce observation clusters 
compare accuracy pf rbpf psbf bk  number samples used
pf rbpf chosen automatically process required approximately
much time per belief update psbf hmorali bk hmorali  respectively 
experiments  meant pf  rbpf  able process            
    samples  however  since process      states  nearly enough
represent uniform belief state  hence  pf rbpf produced much higher relative entropy
psbf bk  moreover  fact processes high variance means
pf rbpf would require many samples achieve accuracy psbf bk  as
    

fialbrecht   ramamoorthy

 

   
   

bk  pc 

psbf  pc 

bk  moral 

psbf  moral 

bk  modis 

psbf  modis 

relative entropy

relative entropy

   

   
 

 

   

    

    
    
transition

    

   
 
   
 

    

 

   

 a     passivity

    

    
    
transition

    

    

    

    

    

    

 b      passivity

 
relative entropy

relative entropy

 

 

 

 
 
 
 

 

 

   

    

    
    
transition

    

 

    

 

   

 

 

 

 

 

    
    
transition

 d      passivity

relative entropy

relative entropy

 c      passivity

    

 

   

    

    
    
transition

    

    

 e      passivity

 
 
 
 

 

   

    

    
    
transition

 f       passivity

figure    accuracy results psbf bk  plots show relative entropy exact
algorithms belief states  lower better   results averaged      processes size
 n             average        non target variables passive  cf 
section         psbf bk used clustering methods hpci  hmorali  hmodisi 
shown next section   one would expect latter issue alleviated use
exact inference rbpf  cf  section       however  case much
variance process captured marginal distributions used particles
rbpf  contrast  synthetic processes exhibit high variance across variables 
    

fiexploiting causality selective belief filtering dbns

automatic grouping  state variables sampled exact variables still contained
much variance sampled variables  hence  rbpf required significantly samples
number could process time provided 
finally  order compare accuracy psbf bk  number iterations
used  more precisely  number iterations loopy belief propagation  cf  murphy  
weiss        chosen automatically process required approximately
much time per belief update psbf hmorali bk hmorali  respectively  however 
often able perform several iterations provided time  resulting
relative entropy substantially higher psbf bk  problem
designed specific class dbn topologies  namely containing
edges within x t    called regular dbns murphy   weiss         allows
use fully factored representation belief states  variable
belief factor  however  processes used experiments high intra correlation
state variables  i e  many edges x t      especially increasing passivity 
correlations cannot captured belief state representation ff  resulting
significantly higher relative entropy psbf bk 
      timing
measured computation times processes sizes s  m  l  xl passivities     
                respectively  psbf bk used hmorali clustering  seemed
appropriate fair comparison since produced consistently similar accuracy
algorithms  number samples used pf chosen automatically process
pf achieved average accuracy approximately good psbf
bk  respectively  final     process  involved computing exact belief
states relative entropies  able use pf processes size only  omit
rbpf section shown previous section unsuitable
processes consider  psbf tested         parallel processes 
allocated approximately number belief factors 
figures  a  d show times      transitions averaged      processes 
figure  e shows average percentage belief factors updated transition
observation steps psbf  timing reported psbf includes time taken
modify variable distributions  in case overlapping clusters  detect skippable clusters
transition observation steps  done advance
action  results show psbf able minimise time requirements significantly
exploiting passivity  first  note marginal gains        
passivity  despite fact psbf updated     fewer clusters transition step 
clusters mostly small  however  significant gains
        passivity average speed ups      s        m        l        xl  
   open question group state variables sampled exact variables  doucet et al  
       used simple heuristic whereby set sampled variables contained variables xt  


parents x t t   none xti   remaining variables x t   constituted set
exact variables  ensure resulting grouping valid actions  i e  dbns  process 
considered edges involved dbns  is  performed grouping union ea
a  moreover  improve efficiency  subdivided set exact variables clusters
variables connected undirected edges x t   without edges involving sampled variables 

    

fi   
pf bk
pf psbf
bk

  

 

   

   
   
passivity

   

   

   

psbf     
psbf 
psbf 
  

    

 a   n     m   

 

   

   
   
passivity

seconds      transitions

   

seconds      transitions

seconds      transitions

seconds      transitions

albrecht   ramamoorthy

   
   
   
   
   
   
  

    

 

 b   n     m   

   

    

 c  l  n     m   

   
  updated belief factors

   
   
passivity

   
   
   
   
   
   
   
 

   

   
   
passivity

    

 d  xl  n     m    

 trans 
 obs 
 trans 
 obs 

  
  
  
l  trans 
l  obs 
xl  trans 
xl  obs 

  
 

   

   
   
passivity

    

 e  updated belief factors

figure    timing results   ad  average number seconds required      transitions
unix dual core machine     ghz  sizes s  m  l  xl  passivity p  means
average p  non target variables passive  cf  section         psbf bk
used hmorali clustering  pf optimised binary variables used number samples
achieve accuracy psbf bk  respectively  psbf run    psbf      
 psbf        psbf    parallel processes   e  average percentage belief factors
updated transition observation steps  respectively 
         passivity average speed ups      s        m        l  
     xl   shows computational gains grow significantly
degree passivity size process 
results show psbf consistently outperformed bk process sizes 
two main computational savings psbf relative bk  firstly  skipping belief
factors transition observation steps  secondly  perform
potentially expensive projection step restore original factorisation inference 
however  times algorithms grew exponentially size process 
note relative difference psbf bk decreased significantly lower
degrees passivity  instance free lunch  see section   discussion  
means psbf performs best processes high passivity suffer
performance processes lack passivity  specifically  computational overhead
modifying variable distributions detecting skippable belief factors amortise
    

fiexploiting causality selective belief filtering dbns

effectively large processes low passivity  furthermore  low passivity  psbf
often perform full transition observation steps  i e  update belief factors
step   costly large processes 
bk pf affected passivity  surprisingly  performance bk
nearly unaffected increasing degrees passivity  junction tree algorithm used
bk benefited marginally increased sparsity process  computational
gains minimal  first unable use pf required many samples
 between   k    k  achieve comparable accuracy psbf bk  due
high variance processes  order investigate effect passivity pf 
implemented version pf strictly optimised binary variables  interestingly 
found passivity adverse effect performance pf  requiring use
exponentially samples increased passivity  see figure  a   makes sense
view pf factored approximation method  such psbf bk  means
analysis section     applies  however  pf puts variables single cluster
 since actually factored method   mixing rate process much lower
psbf bk  as discussed section      and  thus  error bounds less
tight  compensate this  pf requires significantly samples increased passivity 
    multi robot warehouse system
section  demonstrate passivity occur naturally complex system
psbf exploit accelerate filtering task  end  consider
multi robot warehouse system style kiva  wurman et al         
robots task transport goods within warehouse  cf  figure   a  
      specification warehouse system
figure   b shows initial state warehouse simulation  warehouse consists
  workstations  w   w      robots  r r       inventory pods  i i     robot
move forward backward  turn left right  load unload inventory pod  if
positioned pod   nothing  kiva  robots move inventory pods
unless carrying pod  case pods become obstacles  move
turn operations stochastic robot may move turn far     chance 
nothing     chance   robot possesses two sensors  one telling inventory
pod loaded  if any  one direction facing  direction sensor noisy
random direction may reported     chance  
robot maintains list tasks form bring inventory pod workstation
w  yellow area around w  bring inventory pod position  x y   tasks
executed depends control mode  use two simulations  
   control modes ad hoc often make suboptimal decisions  however  found current
solution techniques  dec  pomdps  including approximate methods  infeasible setting 
nonetheless  quality decisions made control modes largely depends accuracy
belief states  hence important belief states updated accurately  therefore 
control modes sufficient purposes 

    

fialbrecht   ramamoorthy

 a  kiva warehouse system

 b  initial state simulation

figure      a  kiva warehouse system  image reproduced dandrea   wurman        
robots  orange coloured  transport shelfs goods workstations   b  initial
state warehouse simulation  warehouse consists   workstations  w   w     
robots  r r       inventory pods  i i    
centralised mode  central controller maintains belief state bt state
warehouse system  time t  samples     states bt removes

duplicate states  resulting set
p t   s    s          resamples state



probabilities w s     b  s    q b  sq    based current task
robot  performs search  hart  nilsson    raphael         with manhattan
distance  space joint actions find optimal action robot 
executing actions  robots send sensor readings controller 
controller updates belief state using sensor readings 
decentralised mode  robot maintains belief state communication robots  knowledge robots
current tasks  communicated task allocation module  time t 
robot samples set state done centralised mode  treating
robots static obstacles  performs search based current
task find action   repeated robot r states sq s 
resulting actions ar q used
p obtain distributions r           a
set actions  r  a    q   ar q  a w sq    robot executes action updates belief state using sensor readings distributions r
average robots actions 
tasks generated external scheduler time intervals sampled u         
generated task assigned one robots sequential auction  dias  zlot 
kalra    stentz         robots bids calculated total number steps needed
solve current tasks auctioned task  in simplified model
robots removed   averaged states s  robot lowest bid
assigned task 
    

fiexploiting causality selective belief filtering dbns

figure     example dbn smaller warehouse system consisting one inventory
pod  i   two robots  r   r    dbn implements joint action r  moves
r  turns  dashed circles mark passive state variables  coloured areas represent
state clusters c  c   

      dbn topology clustering
figure    shows example dbn smaller warehouse one inventory pod two
robots  inventory pod represented two variables  i x i y  correspond
x position inventory pod  robot r represented four variables 
r x r y x y position  r d direction  r s status  status
robot r either r s    unloaded  r s i  loaded inventory pod i   constants
size warehouse positions workstations omitted dbn 
four types clusters  i clusters  c c   preserve correlation
r loaded i  must always position r  there two i clusters
 i r  pair   r clusters  c   s clusters  c    respectively  preserve
correlation two robots position carry inventory pod
 there one r s cluster  ra rb  pair   b   and  finally  d clusters  c  
c    psbf uses singleton observation clusters  i e  one cluster observation variable  
differences dbns centralised decentralised modes
 figure    uses centralised mode   centralised mode  one dbn
action combination robots  since controller observes r s noise free  add
edges r x r y i x i y r s i remove otherwise simplify inference
 thus  figure     r  loaded i  r  unloaded   decentralised mode 
robot observes sensor readings  hence add remove edges
itself  edges robots must permanently added  means
robots status variables  r s  must linked i x i y and  therefore  included
i clusters  to preserve correlation must position r r
loaded i   moreover  since robot knows action  one dbn
    

fiseconds per transition

albrecht   ramamoorthy

centralised
    decentralised
   
   
   
   
  
  
bk

psbf

pf

figure     results warehouse simulation  using centralised decentralised
control modes  timing measured unix dual core machine     ghz averaged
   different simulations     transitions each 
actions  variables associated robots active  the
distributions r defined previous section used average actions  
      results
implemented psbf  bk  pf c   using framework infer net  minka  winn 
guiver    knowles        implement bk  allowed bk exploit sparsity
process offered improved memory handling  psbf optimised sparsity    
     respectively  summing states btk    bt  
k  positive  pf naturally
benefits sparsity allows concentrate samples fewer states  number
samples used pf set way controller decisions invariant
random numbers used sampling process pf  done ensure
results repeatable  finally  maintain sparsity process  probability
belief states lower      set    tested algorithms initialised exact
belief state  shown figure   b 
figure    shows time per transition averaged    different simulations    
transitions each  timing reported psbf includes time needed modify variable
distributions  for overlapping clusters  detect skippable belief factors transition
observation steps  done demand every previously unseen
dbn  centralised mode  psbf able outperform bk average    
pf      pf needed        samples produce consistent  i e  repeatable  results 
decentralised mode  psbf outperformed bk average     pf      pf
needed        samples produce consistent results  due increased variance
process  differences statistically significant  based paired t tests   
significance level  note psbf bk slower decentralised mode since
corresponding dbns much higher inter connectivity  addition  psbf updated
belief factors since active variables 
expected  psbf able exploit high degree passivity process
accelerate filtering task  many cases  meant psbf needed update less
half belief factors  precisely many belief factors updated depends
    

fiexploiting causality selective belief filtering dbns

performed action  illustrate this  consider smaller warehouse dbn shown figure   
 for centralised mode   r  moving r  turning  here  r  x  r  y 
r  d active variables variables passive  dashed circles   corresponding
passivity      dbn  psbf updates belief factors corresponding clusters
c   c   c   c   since contain active variables  updates belief
factors c  c   since directed paths active variables  r  x r  y 
them  therefore  factors updated c  c  
consider full warehouse experiment  contains    inventory pods   robots 
resulting    variables     i clusters    r clusters    s clusters    d clusters 
assume similar situation one robot moves inventory pod  say r  i  
r   turn  case  psbf updates     r clusters  those containing
r        s clusters  since status change       d clusters  for r           
i clusters     i clusters containing r  plus   i clusters r   i    amounting
total saving        belief factors need updated 
number states warehouse system  including invalid states  exceeded     
states  therefore  unable compare accuracy tested algorithms terms
relative entropy  instead  compared accuracy based results task
auctions number completed tasks end simulation  gives
good indication algorithms accuracy  since outcome auction
number completed tasks depend accuracy belief states  centralised
mode  algorithms generated     identical task auctions completed       bk  
      psbf         pf  tasks average  decentralised mode  generated
    identical auctions completed       bk         psbf         pf  tasks
average  modes  none differences statistically significant  therefore 
indicates psbf achieved accuracy similar bk pf 
    summary experimental evaluation
experimental results show psbf produces belief states competitive accuracy 
synthetic processes  psbf achieved accuracy average better
comparable accuracy alternative methods  warehouse system  psbf
able complete statistically equivalent number tasks compared
methods  indicates accuracy equivalent comparable 
furthermore  experimental results show psbf performed belief updates
significantly faster alternative methods  synthetic processes  psbf using
parallel processes outperformed bk     largest process  xl   pf took
much time achieve accuracy comparable psbf  particular  results show
computational gains grow significantly degree passivity
size process  warehouse system  psbf outperformed alternative methods
     substantial saving considering size state space  more
     states   furthermore  computational gains much higher centralised
control mode decentralised control mode  since latter significantly
lower degree passivity  therefore  shows high degrees passivity bear
great potential filtering task 
    

fialbrecht   ramamoorthy

   free lunch psbf
view belief filtering method generally suited types processes 
instead  method assumes certain structure process  explicitly implicitly 
attempts exploit order render filtering task tractable  typically 
methods tailored way respect structure perform well
structure present process  suffer significant loss performance structure
absent  instance  pf works best processes low degrees uncertainty  since
means fewer state samples needed acceptable approximations 
hand  number samples needed acceptable approximations grow substantially
degree uncertainty process  as shown experiments   another
example  bk works best processes little correlation state variables  since
means belief factors small processed efficiently  however 
many variables strongly correlated  bk typically becomes infeasible 
therefore  structural assumptions taken account choosing
filtering method specific process 
formal account view given free lunch theorems  wolpert
  macready              state that  intuitively speaking  two algorithms
equivalent performance averaged possible instances problem 
words  classes problem instances algorithm better performance
algorithm b  must classes problem instances
worse performance b  then  question is  class problem instances  that
is  processes  psbf expected achieve good performance  class essentially
described following three criteria 
degree passivity psbf attempts accelerate filtering task omitting
transition step many belief factors possible  depends passivity
variables state clusters  ideal case  process exhibits high degree
passivity psbf omit transition step many belief factors 
worst case  process passive variables all  psbf update
belief factors transition step  however  discussed section      high degree
passivity necessarily sufficient infer many clusters skipped
transition step  since passive variables could distributed way
cluster skipped  e g  passive variables distributed uniformly
amongst state clusters   therefore  optimal case  passivity concentrated
correlated state variables passive variables end clusters 
size state clusters space time complexity belief state representation
psbf exponential size largest state cluster  cf  section       therefore 
ideal case  relevant variable correlations captured small state
clusters cost storing belief factors performing update procedures
small  worst case  large state clusters required retain variable
correlations cost storing updating belief factors large  another reason
state clusters small way psbf performs
transition step  one pre requisite omitting transition step belief factor
variables corresponding cluster passive  many variables
    

fiexploiting causality selective belief filtering dbns

one cluster  less likely variables cluster passive  and 
therefore  less likely cluster skipped 
structure observations third criterion  though arguably less important
criteria  structure observations  i e  way observation
variables depend state variables  size observation clusters  cl   
psbf attempts accelerate observation step skipping state
clusters whose variables structurally independent observation  and 
cluster cannot skipped  incorporating observation clusters
relevant update  therefore  ideal case  fraction state clusters
depend observation  relevant correlations observation variables
captured small observation clusters  worst case  state clusters
depend observation sense  structure observation
allow efficient clustering 
thus  summary  psbf suitable processes high degrees passivity
relevant variable correlations captured small state observation
clusters  hand  psbf may suitable low degrees
passivity  large state observation clusters necessary retain relevant
variable correlations process 
addition identifying class processes filtering method suitable 
important justify practical relevance class  work  interested
robotic physical decision processes  as shown examples experiments  
systems typically exhibit number features  first all  robotic systems usually
causal structure  e g  mainzer        pearl         passivity  specific type
causality  observed many robotic systems  including robot arm used
examples multi robot warehouse system section      furthermore  robotic systems
typically modular structure  module responsible specific
subtask may interact modules  modular structure often allows
efficient clustering  sense module corresponds cluster correlated state
variables  finally  sensors used robotic systems typically provide information
certain aspects system  components system may benefit
sensor information  words  independencies state
observation variables  features correspond criteria  above  specify
class processes psbf suitable filtering method  therefore  believe
class practically justified 

   conclusion
inferring state stochastic process difficult technical challenge complex
systems large state spaces  key developing efficient solutions identify special
structure process  e g  topology parameterisation dynamic bayesian
networks  leveraged render filtering task tractable 
end  present article explored idea automatically detecting exploiting
causal structure order accelerate belief filtering task  considered specific type
causal relation  termed passivity  pertains state variables cause changes
    

fialbrecht   ramamoorthy

state variables  demonstrate potential exploiting passivity  developed novel
filtering method  psbf  uses factored belief state representation exploits passivity
perform selective updates belief factors  psbf produces exact belief states
certain assumptions approximate belief states otherwise  showed empirically 
synthetic processes varying sizes degrees passivity well example
complex multi robot system  psbf faster several alternative methods
achieving competitive accuracy  particular  results showed computational
gains grow significantly size process degree passivity 
work demonstrates system exhibits much causal structure 
great potential exploiting structure render filtering task tractable 
particular  experiments support initial hypothesis factored beliefs passivity
useful combination large processes  insight relevant complex processes
high degrees causality  robots used homes  offices  industrial factories 
filtering task may constitute major impediment due often large state
space system 
several potential directions future work  example  would useful
know definition passivity could relaxed variables fall
definition  principal idea behind psbf still applicable  one
relaxation could form approximate passivity  allows small probabilities
passive variables change values even relevant parents remain unchanged 
addition  would interesting know idea performing selective updates
belief factors  via passivity  could applied existing methods use
factored belief state representation  cf  section       finally  another useful avenue future
work would formulate additional types causal relations exploited
ways similar psbf exploits passivity  perhaps ways that 

acknowledgements
article result long debate presented topic  process benefited
number discussions suggestions  particular  authors wish thank
anonymous reviewers nips   uai   conferences well journal ai
research  attendees workshop advances causal inference held uai   
colleagues school informatics university edinburgh  furthermore 
authors acknowledge financial support german national academic foundation 
uk engineering physical sciences research council  grant number ep h          
european commission  tomsy grant agreement         

    

fiexploiting causality selective belief filtering dbns

appendix a  proof theorem  
prove theorem    useful first establish following lemma 
lemma     a   holds xt  
ck passive  

s  s    tka  s  s k       sk   s k  
proof 
  fact  a   means a i ck xt  
ck   since xt  
ck


passive   follows xtj a i passive   a i   therefore  given
tka  s  s k       clause  ii  definition    follows sk   s k  
  follows directly  a   fact xt  
ck passive  


using lemma    give compact proof theorem   


theorem     a    a   hold  xt  
ck passive  


  bt  
k  sk     bk  sk   

proof 
 
bt  
k  sk  

 

 

x



tka  s  s k  

s pat  ck   

 

 

x

btk   sk   

k    xt  
ck    xti pat  ck   




lem 







tka  s  s k  



btk   sk   

s pat  ck    sk  s k k    xt  
ck    xti pat  ck   




 

  btk  sk  

x



tka  s  s k  



btk   sk   

s pat  ck    sk  s k k     k  xt  
ck    xti pat  ck   




 z

 

 a  

   

 

  btk  sk  

 

btk  sk           since btk normalised 

    

 

fialbrecht   ramamoorthy

appendix b  proof theorem  
prove theorem    first note following proposition 



proposition    xt  
ck marginally independent yjt   t    


s  s    k    k sk    s k   s  ot      s    ot   

proposition follows directly definition 

using proposition    give compact proof theorem   


theorem    xt  
ck marginally independent yjt   t    

t  
  bt  
k  sk     bk  sk   

proof 
x

t    
 
bt  
k  sk       bk  sk  





 s  ot    

 
bt  
k   sk  

t          s  k      k   c   pat    y t       
s pat  
k
k

 y
k




 

prop 

 z

  constant   independent

 

bt    s k  
p k t     
s   bk  sk  
k

 

bt    s k  
p k t     
s   bk  sk  
k

 
  bt  
k  sk   

    

 
s k

fiexploiting causality selective belief filtering dbns

appendix c  mixture gaussians
algorithm   provides simple procedure randomly generates mixture gaussians
 i e  set normal distributions  synthetic processes section      algorithm
takes input number n state variables returns set g gaussians whose means
set          n   number gaussians  means  variances
chosen automatically achieve good coverage state variables minimising
 visual  overlap gaussians  see figure   example 

algorithm   mixtureofgaussians n 
  

input  number state variables n

  

parameters     min     max

  

output  mixture gaussians g

  

g

  

r           n  

  

r   

n
  

  

r next element r

  

r r    r 

  

r drand  r e     rand returns random number       

   

  min  r     r  r    

   
   

min max   max min   rand   


g g           mean variance gaussian

   

r  r     r          r p   r p   

   

r   r q   r q            r  r    r q     

   

r   

   
   
   
   

r r  r  
r    

r r  r   
return g

    

fialbrecht   ramamoorthy

references
astrom  k          optimal control markov processes incomplete state information 
journal mathematical analysis applications             
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  journal artificial intelligence research         
    
boutilier  c   friedman  n   goldszmidt  m     koller  d          context specific independence bayesian networks  proceedings   th conference uncertainty
artificial intelligence  pp         
boyen  x     koller  d          tractable inference complex stochastic processes 
proceedings   th conference uncertainty artificial intelligence  pp       
boyen  x     koller  d          exploiting architecture dynamic systems  proceedings
  th national conference artificial intelligence  pp         
brafman  r          heuristic variable grid solution method pomdps  proceedings
  th national conference artificial intelligence  pp         
dandrea  r     wurman  p          future challenges coordinating hundreds autonomous vehicles distribution facilities  proceedings ieee international
conference technologies practical robot applications  pp       
dean  t     kanazawa  k          model reasoning persistence causation 
computational intelligence            
dias  m   zlot  r   kalra  n     stentz  a          market based multirobot coordination 
survey analysis  proceedings ieee                   
doucet  a   de freitas  n     gordon  n          sequential monte carlo methods practice 
springer science   business media 
doucet  a   de freitas  n   murphy  k     russell  s          rao blackwellised particle
filtering dynamic bayesian networks  proceedings   th conference
uncertainty artificial intelligence  pp         
geiger  d   verma  t     pearl  j          d separation  theorems algorithms 
proceedings  th conference uncertainty artificial intelligence  pp     
    
gordon  n   salmond  d     smith  a          novel approach nonlinear non gaussian
bayesian state estimation  iee proceedings f  radar signal processing   vol 
     pp         
hart  p   nilsson  n     raphael  b          formal basis heuristic determination
minimum cost paths  ieee transactions systems science cybernetics 
vol     pp         
hauskrecht  m          value function approximations partially observable markov
decision processes  journal artificial intelligence research           
    

fiexploiting causality selective belief filtering dbns

heckerman  d          causal independence knowledge acquisition inference 
proceedings  th conference uncertainty artificial intelligence  pp     
    
heckerman  d     breese  j          new look causal independence  proceedings
  th conference uncertainty artificial intelligence  pp         
kaelbling  l   littman  m     cassandra  a          planning acting partially
observable stochastic domains  artificial intelligence                 
koller  d     friedman  n          probabilistic graphical models  principles techniques 
mit press 
kullback  s     leibler  r          information sufficiency  annals mathematical statistics               
lauritzen  s     spiegelhalter  d          local computations probabilities graphical
structures application expert systems  journal royal statistical
society  series b  methodological                  
lovejoy  w          computationally feasible bounds partially observed markov decision
processes  operations research             
mainzer  k          causality natural  technical  social systems  european review 
           
minka  t   winn  j   guiver  j     knowles  d          infer net       microsoft research
cambridge  http   research microsoft com infernet 
murphy  k          bayes net toolbox matlab  computing science statistics 
                  https   code google com p bnt  
murphy  k     weiss  y          factored frontier algorithm approximate inference
dbns  proceedings   th conference uncertainty artificial intelligence 
pp         
murphy  k          dynamic bayesian networks  representation  inference learning 
ph d  thesis  university california  berkeley 
ng  b   peshkin  l     pfeffer  a          factored particles scalable monitoring 
proceedings   th conference uncertainty artificial intelligence  pp     
    
pasula  h   zettlemoyer  l     kaelbling  l          learning symbolic models stochastic
domains  journal artificial intelligence research             
pearl  j          probabilistic reasoning intelligent systems  networks plausible inference  morgan kaufmann 
pearl  j          causality  models  reasoning  inference  cambridge university press 
pineau  j   gordon  g     thrun  s          point based value iteration  anytime algorithm
pomdps  proceedings   th international joint conference artificial
intelligence  vol      pp           
poole  d     zhang  n          exploiting contextual independence probabilistic inference 
journal artificial intelligence research             
    

fialbrecht   ramamoorthy

poupart  p     boutilier  c          value directed belief state approximation pomdps 
proceedings   th conference uncertainty artificial intelligence  pp 
       
poupart  p     boutilier  c          vector space analysis belief state approximation
pomdps  proceedings   th conference uncertainty artificial
intelligence  pp         
poupart  p     boutilier  c          value directed compression pomdps  advances
neural information processing systems  pp           
roy  n   gordon  g     thrun  s          finding approximate pomdp solutions
belief compression  journal artificial intelligence research          
smith  t     simmons  r          point based pomdp algorithms  improved analysis
implementation  proceedings   st conference uncertainty artificial
intelligence  pp         
sondik  e          optimal control partially observable markov processes  ph d 
thesis  stanford university 
srinivas  s          generalization noisy or model  proceedings  th conference
uncertainty artificial intelligence  pp         
washington  r          bi pomdp  bounded  incremental partially observable markovmodel planning  recent advances ai planning  pp          springer 
wolpert  d     macready  w          free lunch theorems search  tech  rep  sfi tr           santa fe institute 
wolpert  d     macready  w          free lunch theorems optimization  ieee
transactions evolutionary computation              
wurman  p   dandrea  r     mountz  m          coordinating hundreds cooperative 
autonomous vehicles warehouses  ai magazine            
zhang  n     poole  d          exploiting causal independence bayesian network inference 
journal artificial intelligence research            
zhou  r     hansen  e          improved grid based approximation algorithm
pomdps  proceedings   th international joint conference artificial
intelligence  pp         

    


