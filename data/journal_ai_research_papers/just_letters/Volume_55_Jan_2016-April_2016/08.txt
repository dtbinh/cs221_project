journal of articial intelligence research                    

submitted          published        

semantic visualization with
neighborhood graph regularization
tuan m  v  le
hady w  lauw

vmtle      phdis smu edu sg
hadywlauw smu edu sg

school of information systems
singapore management university
   stamford road  singapore       

abstract
visualization of high dimensional data  such as text documents  is useful to map out
the similarities among various data points  in the high dimensional space  documents
are commonly represented as bags of words  with dimensionality equal to the vocabulary
size  classical approaches to document visualization directly reduce this into visualizable
two or three dimensions  recent approaches consider an intermediate representation in
topic space  between word space and visualization space  which preserves the semantics
by topic modeling  while aiming for a good t between the model parameters and the
observed data  previous approaches have not considered the local consistency among data
instances  we consider the problem of semantic visualization by jointly modeling topics
and visualization on the intrinsic document manifold  modeled using a neighborhood graph 
each document has both a topic distribution and visualization coordinate  specically  we
propose an unsupervised probabilistic model  called semafore  which aims to preserve the
manifold in the lower dimensional spaces through a neighborhood regularization framework
designed for the semantic visualization task  to validate the ecacy of semafore  our
comprehensive experiments on a number of real life text datasets of news articles and
web pages show that the proposed methods outperform the state of the art baselines on
objective evaluation metrics 

   introduction
text documents come in various avors  such as web pages  news articles  blog posts  emails 
or messages on social media such as twitter  while much is in english  there are increasing
amounts of content in various languages as well  with the backdrop of the growth in volume  diversity  and complexity of various corpora  we need more useful tools to analyze the
wealth of text content  one form of analysis which we will look into in this paper is visualization  there are dierent types of visualizations  be it of the temporal or longitudinal 
networked  or other natures  what we are interested in is a form of visualization where we
can represent a collection of documents as coordinates on the same low dimensional space 
so as to learn of the similarities and dierences among documents based on their distances
on the visualization space 
visualization of high dimensional data is an important exploratory data analysis task 
which is actively studied by various academic communities  while the hci community is
interested in the presentation of information  as well as other interface aspects  chi        
the machine learning community is interested in the quality of dimensionality reduction
c
    
ai access foundation  all rights reserved 

file   lauw

 van der maaten   hinton         i e   how to transform the high dimensional representation into a lower dimensional representation that can be shown on a scatterplot  this
visualization form is simple  and widely applicable across various domains 
consider therefore the problem of visualizing documents on a scatterplot  commonly 
a document is represented as a bag of words  i e   a vector of word counts  this highdimensional representation would be reduced into coordinates on a visualizable  d  or  d 
space  one pioneering technique is multidimensional scaling  mds   kruskal         the
goal is to preserve the distances in the high dimensional space in the low dimensional embedding  when applied to documents  a visualization technique for generic high dimensional
data  e g   mds  may not necessarily preserve the topical semantics  words are often ambiguous  with issues such as polysemy  when the same word carries multiple senses  and
synonymy  when dierent words carry the same sense  because the dimensions in the original representation  which are words  may not accurately capture this ambiguity  this aects
the quality of the reduced representation  which is the visualization space  as well 
to model semantics in documents in a way that can resolve some of this ambiguity  the
current popular approach is by topic modeling  such as plsa  hofmann        or lda
 blei  ng    jordan         each document is associated with a probability distribution
over a set of topics  each topic is a probability distribution over words in the vocabulary  in
this way  polysemous words can be separated into dierent topics  and synonymous words
can be grouped into the same topic 
topic modeling itself is another form of dimensionality reduction  from word space to
topic space  the word space refers to a documents original representation  which is usually
a bag of words  the topic space refers to the simplex of topic distributions  a documents
probability distribution over topics is eectively the representation of this document in
this topic space  however  a topic model by itself is not designed for visualization  while
one possible visualization is to plot documents topic distributions on a simplex  a  d
visualization space could express only three topics  which is very limiting 
given its success in modeling semantics in documents  we therefore ask the question
of whether and how best to do both forms of dimensionality reductions  visualization and
topic modeling  for documents  the end goal is to arrive at a visualization of documents
that is consistent with both the semantic representation  topics   as well as the original
representation  words   this coupling is a distinct task from topic modeling or visualization
respectively  as it enables novel capabilities  for one thing  topic modeling helps to create
a richer visualization  as we can now associate each coordinate on the visualization space
with both topic and word distributions  providing semantics to the visualization space 
for another  the tight integration potentially allows the visualization to serve as a way
to explore and tune topic models  allowing users to introduce feedback  hu  boyd graber 
satino    smith        to the model through a visual interface  choo  lee  reddy    park 
       these capabilities support several use case scenarios  one potential use case is a
document organizer system  the visualization could potentially help in assigning categories
to documents  by showing how closely related documents have been labeled  another is
an augmented retrieval system  given a query  the results may include not just relevant
documents  but also other similar documents  neighbors in the visualization  
    

fisemantic visualization with neighborhood graph regularization

    problem statement
we refer to the task of jointly modeling topics and visualization as semantic visualization 
the input is a set of documents d  for a specied number of topics z and visualization
dimensionality  assumed to be  d  without losing any generality   the goal is to derive 
for every document in d  a latent coordinate on the visualization space  and a probability
distribution over the z topics  while we focus on documents in our description  the same
approach would apply to visualization of other data types for which latent factor modeling 
i e   topic model  makes sense 
a straightforward way is to undergo two step reductions  in the rst reduction  the
original representation for documents are reduced into topic distributions using topic modeling  in the second reduction  documents topic distributions are further reduced into
visualization coordinates  this approach may have some value compared to direct reduction from word space to visualization space  however  it is not ideal  because the disjoint
reductions could mean that errors may propagate from the rst to the second reduction 
and the resulting visualization may not faithfully capture the original representation 
a better way to solve this problem is to join up the two reductions into a single  joint
process that produces both topic distributions and visualization coordinates  this approach
was rst pioneered by plsv  iwata  yamada    ueda         which also showed that the
joint approach outperformed the disjoint approach  plsv derives the latent parameters
by maximizing the likelihood of observing the documents  this goal is concerned with the
error between the model and the observation 
in the literature  it is found that algorithms that ensure smoothness tend to perform
better at learning tasks  zhou  bousquet  lal  weston    scholkopf         smoothness
concerns preserving the observed proximity between documents  this objective arises naturally from the assumption that the intrinsic geometry of the data is a low rank  non linear
subspace within the high dimensional space  therefore  preserving neighborhood structure
is important for learning tasks  this assumption is well accepted in the machine learning
community  laerty   wasserman         and nds application in both supervised and
unsupervised learning  belkin   niyogi        zhou et al         zhu  ghahramani  lafferty  et al          recently  there is a preponderance of evidence that this assumption also
applies to text data in particular  cai  mei  han    zhai        cai  wang    he       
huh   fienberg         we therefore propose to incorporate this assumption into a new
unsupervised  semantic visualization model 
    overview
we propose an unsupervised probabilistic model that jointly derives topic distributions and
visualization coordinates on the intrinsic geometry of the data  our proposed model is called
semafore  which stands for semantic visualization with manifold regularization  we
build a neighborhood regularization framework into a semantic visualization model  the
framework involves new issues to resolve  including the regularization function  and the
space in which regularization should take place 
the model is evaluated on a series of real life  publicly available datasets  which are
also benchmark datasets used in document classication task  an advantage of a statistical
method  such as ours  is that it is not dependent on a specic language  two of the datasets
    

file   lauw

are in english  and one is in brazilian portuguese  while our model is unsupervised  class
label is neither required nor used in learning   to objectively quantify the visualization quality  we leverage on the class label information  it is a common assumption that documents
of the same class are expected to be neighbors on the original space  belkin  niyogi   
sindhwani        zhou et al         zhu et al          which suggests that they should also
be close on the visualization space  we investigate the eectiveness of semafore in placing
documents of the same class nearby on the visualization space  and systematically compare
it to existing baselines without one or more of our properties  namely  joint modeling of
topic and visualization  or neighborhood regularization 
    contributions
while visualization and topic modeling are  separately  well studied problems  the interface
between the two  semantic visualization  is a relatively new problem  with very few previous
work  in this work  we make the following contributions 
 we propose incorporating neighborhood structure in semantic visualization  in this
respect  we propose a probabilistic model semafore  with two integrated components  one is a kernelized semantic visualization model  enabling the substitution of
the kernel functions that relate visualization coordinates to topic distributions  see
section       the other is a neighborhood graph regularization framework for semantic
visualization as described in section     
 realizing the neighborhood graph regularization involves an exploration of how to
incorporate the appropriate forms of the neighborhood structure  in this respect 
we investigate the eects of neighborhood graph construction techniques such as knearest neighbors  k nn    ball  and disjoint minimum spanning trees  dmst   as
well as dierent edge weight estimations such as heat kernel  see section      in the
context of semantic visualization 
 in section    we describe the requisite learning algorithms based on maximum a
posteriori  map  estimation using expectation maximization  em   in order to t
the parameters for the various regularization functions and kernels that we propose 
 our nal contribution is the evaluation of semafores eectiveness on a series of reallife  public datasets described in section    which shows that semafore outperforms
existing baselines on a well established and objective visualization metric 
in our prior work  le   lauw      b   we proposed the problem and described the preliminary model  in this extended article  there are signicant technical changes that provide
a signicantly more comprehensive discussion of the model  for instance  we now discuss
the student t kernel  in addition to the previously introduced gaussian kernel  furthermore  we investigate the ecacies of dierent neighborhood graph constructions  including
the  ball and dmst graphs  in addition to the previously introduced knn graph  the
graph weights are also enhanced through investigation of heat kernel  in addition to the
simple minded binary scheme previously  as discussed in section      these enhancements
collectively result in statistically signicant improvements over the previous model  beyond
    

fisemantic visualization with neighborhood graph regularization

the technical enhancements  we also provide more comprehensive model analysis and empirical validation  including richer quantitative and qualitative discussions of the visualizations
and the resulting topic models  as well as a metric to measure topic interpretability based
on pairwise mutual information 

   related work
in this section  we discuss the dierent aspects of our work  identify the related papers in
the literature  and point out the key conceptual dierences 
    visualization and dimensionality reduction
one way to perform visualization is by using a generic dimensionality reduction technique 
such techniques come in several avors  depending on the objective  principal component
analysis  pca   jollie        identies the components that explain most of the variance
in the data  related to pca is singular value decomposition  svd   golub   van loan 
       comparatively  independent component analysis  ica   comon        identies
the components that are independent of one another  whereas linear discriminant analysis
 fishers lda   fisher        identies the components that most discriminate between
known class labels  being generic  these techniques are more frequently applied to feature
extraction  as they are not optimized for visualization  they focus more on the properties of
the components  e g   orthogonality  independence  rather than on the intrinsic relationship
among data instances  furthermore  as they are based on linear projections  they may not
capture non linearities in the data well 
another category of techniques  which is more directly related to visualization  is the
embedding approach  it aims to preserve the high dimensional similarities or dierences
in the low dimensional embedding  one pioneering such work is multidimensional scaling
 mds   kruskal         given a set of pairwise distances ij between data points i and j 
mds determines coordinates xi and xj respectively  such that the embedded visualization
distance   xi  xj    approximates ij as much as possible  for mds  the distance to be
preserved ij is frequently the linear distance  measuring the distance along a straight line
between two points in the input space  instead of this linear distance  isomap  tenenbaum 
de silva    langford        seeks to preserve the geodesic distance  by nding shortest paths
in a graph with edges connecting neighboring data points  lle  roweis   saul        seeks
to preserve linear distances  but only among the neighboring points and avoiding the need to
estimate pairwise distances between widely separated data points  recently there are also
works applying a similar concept to embedding but using probabilistic modeling  such as
pe  iwata  saito  ueda  stromsten  griths    tenenbaum         sne  hinton   roweis 
       t sne  van der maaten   hinton         and gtm  bishop  svensen    williams 
       yet others are based on semi denite programming  shaw   jebara              
alternatively  several embedding techniques do not aim to preserve relationship among
data instances  but rather other properties such as local minima  kim   torre        
importantly  all these techniques are not optimized for semantic visualization  as they do
not model topics at all  the coordinates do not reect any semantic meaning  other than
reecting the optimization objective 
    

file   lauw

there are only a few related works so far that seek to address the semantic visualization
task directly  the closest previous work that does both topic modeling and visualization
in a single generative process is probabilistic latent semantic visualization  plsv   iwata
et al          which also shows that a joint approach outperforms a separate approach  just
as plsv builds upon the foundation of the topic modeling technique probabilistic latent
semantic analysis  plsa   hofmann        by incorporating visualization coordinates  so
do we build upon the foundation of plsv by incorporating rbf kernels  section      and
neighborhood structure  section    
there are also related works that share a similar objective  but do not share the same
paradigm of visualization or topic modeling  for instance  lda som  millar  peterson   
mendenhall        rst conducts topic modeling using latent dirichlet allocation  lda 
 blei et al          and then separately embeds the documents topic distributions on a
self organizing map  som   kohonen         however  this is not a joint model  and
som uses a dierent visualization space than the euclidean space that we are interested
in  for another instance  sse  le   lauw      a  builds on the spherical admixture
model  sam   reisinger  waters  silverthorn    mooney        belonging to the class of
spherical topic models targeted at spherical  unit vector  reprepresentations of topics and
documents  which are not directly comparable or equivalent with the simplex representation
and multinomial modeling  probability distribution over words  adopted in this work as well
as plsv 
by semantic visualization  we refer to the task of joining visualization and topic modeling  a related  but dierent  task is topic visualization  where the objective is to visualize
the topics  in terms of which keywords are dominant for each topic  chaney   blei       
chuang  manning    heer         which topics are dominant in a corpus  wei  liu  song 
pan  zhou  qian  shi  tan    zhang         and how topics are related to one another
 gretarsson  odonovan  bostandjiev  hollerer  asuncion  newman    smyth        
    topic modeling
topic model involves statistical modeling of text  documents and words  in order to discover
some abstract concepts or topics that occur in a corpus  beginning with latent semantic
indexing  dumais  furnas  landauer  deerwester  deerwester  et al          topic model
evolves into the modern probabilistic treatments  such as probabilistic latent semantic
analysis  plsa   hofmann        and latent dirichlet allocation  lda   blei et al  
       intuitively  a topic captures a collection of words that tend to co occur because
they describe the same concept  this has the appeal of producing highly interpretable
statistical models that let users make semantic sense of the corpus  other than text only
document corpora  topic models have also been applied to cases where links are observed
in addition to text  mccallum  wang    corrada emmanuel        
meanwhile  the assumption that the intrinsic geometry of the data is a non linear low
dimensional subspace within the high dimensional space nds application in both supervised
and unsupervised  belkin   niyogi        learning algorithms  it is especially prevalent in
semi supervised learning  zhou et al         zhu et al         as a way to bridge labeled
and unlabeled data  regularization as a technique to realize this assumption has a long
history  belkin et al          the specic form of the regularization function varies among
    

fisemantic visualization with neighborhood graph regularization

applications  the study of this assumption for unsupervised topic models begins with
lapplsi  cai et al          which introduces regularization to plsa  hofmann        
by minimizing the euclidean distance between neighboring documents topic distributions 
follow up work introduce other distance functions  cai et al         wu  bu  chen  zhu 
zhang  liu  wang    cai         while these previous work focus on maintaining proximity
of similar documents  dtm  huh   fienberg        adds a new criterion to also maintain
the distance among dierent documents  our work is dierent in that we also need to
contend with the visualization aspects  and not just topic modeling 
    semantic similarity
other than topic models  there are alternative mechanisms to learn the semantic relationship
between documents  one way is by measuring the semantic similarity among documents
or words  for instance  in vector space model  documents may be represented as a term
vector  and their similarity may be expressed in terms of cosine similarity  turney  pantel 
et al          other than word occurrences alone  there could also be additional signals
of semantic similarity  for instance  working with wikipedia corpus  the categories and
links are also took into account to determine the similarity among articles  gabrilovich
  markovitch        ponzetto   strube         our work diers from these in several
important respects  first  our objective is not in the similarity value per se  but rather in
determining lower dimensional embedding coordinates  which would allow visualization as
one application  second  our method is based on probabilistic modeling of latent variables 
akin to topic modeling  instead of operating on the vector space model representation of
documents 

   semantic visualization
we introduce the problem formulation for semantic visualization in section      our focus in
this paper is on the eects of the neighborhood graph structure on the semantic visualization
task  we gure that the clearest way to showcase these eects is to design a neighborhood
preservation framework over and above an existing generative process  such as plsv  iwata
et al          which we will review in section      in section      we describe an innovation
over the semantic visualization model  which is an abstraction of the mapping between the
topic space and the visualization space using radial basis function  rbf  kernels  this
allows the exploration of various kernels  of which we identify two for further exploration 
for ease of following the discussion  we include a table of notations in table   
    problem
for the task of semantic visualization  the input is a corpus of documents d    d            dn   
every dn is a bag of words  and wnm denotes the mth word in dn   the total number of
words in dn is mn   the objective is to learn  for each dn   a latent distribution over z
topics  p z dn   z
z     each topic z is associated with a parameter z   which is a probability
distribution  p w z   ww over words in the vocabulary w   the words with the highest
probabilities for a given topic capture the semantic of that topic 
    

file   lauw

notation
dn
xn
mn
z
z
z
w
n
z





description
a specic document
latent coordinate of dn in the visualization space
number of words in document dn
a specic topic
coordinate of topic z in the visualization space
word distribution of topic z
the vocabulary  the set of words in the lexicon 
total number of documents in the corpus
total number of topics  user dened 
the collection of xn s for all documents
the collection of z s for all topics
the collection of z s for all topics
the collective set of parameters       

table    notations 
in semantic visualization  there is an additional objective for semantic visualization 
which is to learn  for each document dn   its latent coordinate xn on a low dimensionality
visualization space  similarly  each topic z is associated with a latent coordinate z on
the visualization space  a document dn s topic distribution is then expressed in terms
of the euclidean distance between its coordinate xn and the dierent topic coordinates
    z  z
z     intuitively  the closer is xn to a topics z   the higher is p z dn   or the
probability of topic z for document dn  
in the following sections  we systematically describe the various components of our
solution  the generative process that links the latent variables  coordinates  and the words
in the documents is described in section      the specic relationship between documents
and topics coordinates constitutes a specic mapping function  which we model as an rbf
kernel in section      in the following section    we discuss how to incorporate neighborhood
structure into semantic visualization 
    generative process
we now describe the generative process of documents based on both topics and visualization
coordinates  below we review plsv whose graphical model is shown in figure    our
eventual complete model is a generalization of this model  involving enhancements through
kernelization  section      and neighborhood structure preservation  section    
the generative process is as follows 
   for each topic z              z 
 a  draw zs word distribution  z  dirichlet  
 b  draw zs coordinate  z  normal       i 
   for each document dn   where n              n  
 a  draw dn s coordinate  xn  normal       i 
    

fisemantic visualization with neighborhood graph regularization

n



mn w

x

z

z









figure    graphical model of plsv 
 b  for each word wnm  dn  
i  draw a topic  z  multi  p z xn     z
z    
ii  draw a word  wnm  multi z  
here   is a dirichlet prior  i is the identity matrix   and  control the variance of the
z
z
normal distributions  the parameters     xn  n
n         z  z         z  z     collectively
denoted as          are learned from documents d based on maximum a posteriori
estimation  the log likelihood function is shown in equation   
l  d   

mn
n 

n   m  

log

z


p z xn    p wnm  z  

   

z  

we reiterate that our focus here is on incorporating neighborhood graph structure into
semantic visualization  by building a neighborhood graph regularization framework into
an existing generative process  i e   plsv  we can clearly observe that any improvement
over plsv arises from the neighborhood graph regularization  in this sense  our work
is in the tradition of introducing neighborhood graph regularization to probabilistic topic
modeling  huh   fienberg        cai et al                where the contributions relate
to the neighborhood graph regularization  rather than the generative process  that said 
there is one signicant dierence to plsv  which is our exibility in allowing various kernel
functions  which we will discuss next 
    rbf kernels
in the step   b i of the above generative process  the topic z of a word is drawn from
the distribution  p z xn     z
z     this distribution relates the coordinates of topics in
the visualization space     z  z
z   and the coordinate xn of a document dn with the
documents topic distribution  p z dn   z
z    
this relationship can be formulated as a mapping problem where we want to nd a
function g which maps a point in visualization space to a point in the topic space  however 
the form of g cannot be known exactly because both visualization space and topic space
are latent spaces and g may be dierent across dierent domains  therefore  to compute
the topic distributions  we need a way to approximate g 
to build a function approximation of the unknown function g  we use the abstraction
of radial basis function  rbf  neural networks  bishop        because feedforward multilayered rbf neural networks with one hidden layer can serve as a universal approximator
    

file   lauw

k nz



t
z      
z










 xn

figure    topic distribution is expressed as a function of visualization coordinates using
radial basis function  rbf  network 

to arbitrary continuous functions  park   sandberg         this property provides the
condence that the model would have the ability to approximate any existing relationship
between visualization space and topic space with arbitrary precision  unlike plsv  iwata
et al         that dened a specic mapping function  our approach generalizes the semantic visualization model by dening the mapping problem in terms of kernelization  which
admits several mapping functions within the family of rbf kernels 
in our context  radial basis function  buhmann        will relate coordinate variables
based on distances which denes a kernel function    xn  z     in terms of how far a data
point  e g   xn   is from a center  e g   z    the kernel function  may take on various forms 
e g   gaussian  multi quadric  inverse quadratic  polyharmonic spline  to express p z dn  
as a function of xn   we consider the normalized architecture of rbf network  with three
layers as shown in figure    the input layer consists of one input node  xn    the hidden
layer consists of z number of normalized rbf activation functions  each is centered at
z and computes z    xn z       the linear output layer consists of z output nodes 
z    

   xn z    

each output node yz  xn   corresponds to p z dn    which is a linear combination of the rbf
functions  as shown in equation    here  wz z  is the weight of inuence of the rbf function

of z  on the p z dn    with the constraint z
z     wz z      
z
p z dn     yz  xn    

z     wz z      xn  z     
z
z        xn  z     

   

while equation   is the general form  to instantiate a specic mapping function  we
need to determine both the assignment of wz z  and the form of the function   for wz z   
we will experiment with a special case wz z      when z   z  and   otherwise 
for the kernel function   one variation we consider is gaussian  which yields the function in equation    where  refers to the collective set of z s  note that here we set variance
of gaussian to    however  its true value is not really important because a dierent variance
value just produces a re scaled visualization with the scaling factor equal to that variance 
    

fisemantic visualization with neighborhood graph regularization

exp       xn  z      
p z dn  gaussian   p z xn    gaussian   z
 
 
z     exp      xn  z      

   

another variation of  being considered is student t  this distribution is also used by
t sne  van der maaten   hinton        in the context of non semantic  direct embedding to mitigate the eects of crowding  due to mismatched dimensionalities  the points
are crunched together in the center of the visualization  which prevents gaps from forming
between the clusters  therefore  we hypothesize that using student t as radial basis function  which yields the function in equation    can help to improve the performance of our
model if crowding becomes an issue  note that the student t distribution with one degree
of freedom yields a radial basis function having the form similar to the inverse quadratic 
       xn  z       
p z dn  studentt   p z xn    studentt   z
   
z            xn  z      

   

the gaussian function  equation    was also used previously in the baseline plsv
 iwata et al         that we will compare to  its inclusion helps to establish parity for
comparative purposes  both to investigate the eectiveness of the alternative student t
kernel  described above   as well as that of the neighborhood regularization  described in
the next section  

   neighborhood graph regularization framework
there are recent works  cai et al               huh   fienberg        trying to preserve the
local neighborhood structure when learning low dimensional topic representations of documents  these works assume that documents are sampled from a nonlinear low dimensional
subspace that are embedded in a high dimensional space  therefore  the local neighborhood
structure is important for revealing the hidden topics of documents and should be preserved
when learning topic representations of documents  bai  guo  lan    cheng         in the
generative process for semantic visualization described in section    the document parameters are sampled independently  and may not necessarily reect the underlying local neighborhood structure  we therefore seek to realize this assumption for semantic visualization 
in particular  we assume that when two documents di and dj are close in the original space 
then their parameters i and j of the low rank representation are similar as well  coupled
with the kernelized semantic visualization model described in section    the neighborhood
preservation approach described in this section constitutes our proposed model  semafore 
which stands for semantic visualization with manifold regularization 
    neighborhood regularization
the neighborhood structure can be represented by a neighborhood graph  given a set of
data points in the euclidean space  a neighborhood graph is constructed with the input
data points as vertices  by denition  edges are symmetric  i e   ij   ji   and weighted 
the collection of edge weights are collectively denoted as     ij   
for the moment  we will assume that we have the neighborhood graph  and address
the issue of how this neighborhood graph may be incorporated into our semantic visualiza    

file   lauw

tion framework  in actuality  the neighborhood graph construction itself is an important
component  whose construction is described in detail in section     
one eective means to incorporate a neighborhood structure into a learning model is
through a regularization framework  belkin et al          this leads to a re design of the
log likelihood function in equation   into a new regularized function l  equation     where
 consists of the parameters  visualization coordinates and topic distributions   and d and
 are the documents and neighborhood structure 
l  d      l  d      r   

   

the rst component l is the log likelihood function in equation    which reects the
t between the latent parameters  and the observation d  the second component r is a
regularization function  which reects the consistency between the latent parameters  of
neighboring documents in the neighborhood structure    is the regularization parameter 
commonly found in neighborhood based algorithms  belkin et al         cai et al        
       which controls the extent of regularization  we will experiment with dierent s in
experiments  
      proposed regularization function
we now turn to the denition of the r function  the intuition is that the data points that
are close in the high dimensional space  should also be close in their low rank representations  i e   local consistency  also known as smoothness  one function that satises this is
r  in equation    here  f is a distance function that operates on the low rank space 
minimizing r  leads to minimizing the distance f i   j   between neighbors  ij      
r       

n


ij  f i   j  

   

i j   i j

the above level of local consistency is still insucient  because it does not regulate how
non neighbors  i e   ij      behave  for instance  it does not prevent non neighbors from
having similar low rank representations  another valid objective in visualization is to keep
non neighbors apart  which is satised by another objective function r in equation    r
is minimized when two non neighbors di and dj  i e   ij      are distant in their low rank
representations  the addition of   to f is to prevent division by zero error 
r      

n

i j   i j ij   

   ij
f i   j      

   

we hypothesize that neither objective is eective on its own  a more complete objective
would capture the spirits of both keeping neighbors close  and keeping non neighbors apart 
therefore  we put equation   and equation   together using summation and maximize the
objective function as shown in equation    note that the coecient    in equation   is for
simplifying the formula of the derivative of r     
 
r         r        r     
 
    

   

fisemantic visualization with neighborhood graph regularization

 

d 

 

d 

i 

i 

 
  

  

 
  

 

 

 

 

d 

  

figure    example of how the same topic distribution may have dierent visualization coordinates  any points on the red line have same topic distributions 

summation preserves the absolute magnitude of the distance  and helps to improve the
visualization task by keeping non neighbors separated on a visualizable euclidean space 
taking the product is unsuitable  because it constrains the ratio of distances between neighbors to distances between non neighbors  this may result in the crowding eect  where
many documents are clustered together  because the relative ratio may be maintained  but
the absolute distances on the visualization space could be too small 
other than the proposed regularization function above  it is also possible to consider
other regularization functions  for instance  we have also experimented with modifying the
regularization function adapted from discriminative topic model  dtm   huh   fienberg 
       which addressed topic modeling but not semantic visualization  note that while in
the original dtm formulation  the distance function f i   j   operates in the topic space 
we adapt it for semantic visualization by redening the distance function f i   j   so that
it will operate in the visualization space instead  this modied dtm formulation is shown
to underperform the proposed regularization function above  le   lauw      b  
      enforcing neighborhood structure  visualization vs  topic space
we now turn to the denition of f          in neighborhood based models  belkin et al  
      cai et al                there is only one low rank representative space  for semantic
visualization  there are two  topic and visualization spaces  we look into where and how
to enforce the neighborhood graph structure 
at rst glance  they seem equivalent  after all  they are representations of the same
documents  however  this is not necessarily the case  consider a simple example of two
topics z  and z  with visualization coordinates            and            respectively 
meanwhile  there are three documents  d    d    d    with coordinates x            x           
and x            if two documents have the same coordinates  they will also have the
same topic distributions  in this example  x  and x  are both equidistant from   and    
and therefore according to equation    they have the same topic distribution p z   d     
p z   d           and p z   d      p z   d           if two documents have the same topic
distributions  they may not necessarily have the same coordinates  d  also has the same
    

file   lauw

topic distribution as d  and d    but a dierent coordinate  in fact  any coordinate of the
form        will have the same topic distribution  this example is illustrated in figure   
this suggests that enforcing neighborhood structure on the topic space may not necessarily lead to having data points closer on the visualization space  we postulate that
regularizing the visualization space is more eective  there are also advantages in computational eciency to doing so  which we will describe further shortly  therefore  we
dene f i   j   as the squared euclidean distance   xi  xj     between the corresponding
visualization coordinates 
    neighborhood graph
we discuss how the neighborhood graph may be approximated  which concerns the two
issues of how the graph edges are dened  as well as how they are weighted  the neighborhood graph is constructed in the original data space where we represent each document as a
tf idf vector  manning  raghavan  schutze  et al          we also experiment with dierent
vector representations  including word counts and term frequencies  and nd tf idf to give
the best results  the distance between two document vectors is measured using euclidean
distance 
      graph construction
there have been research studies on the properties and methods for construction of neighborhood graphs  zemel   carreira perpinan        carey   mahadevan         since the
construction of neighborhood graph is a critical step that may aect the performance of
various graph based algorithms  this problem itself is a research issue of independent interest  our scope is in exploring how some well established graph construction techniques
may apply to the case of semantic visualization  we will investigate these various graph
construction methods empirically in section   
in the following  we briey review two categories of graph construction methods 
   neighborhood based graphs  in this formulation  edges are formed between data points
that are deemed to be suciently close to each other  this admits dierent denitions
of sucient closeness  the most common denitions found in the literature include
the two below 
 a   ball  the neighborhood graph contains an edge connecting two documents di
and dj   if di and dj have a distance less than a threshold  
 b  k nearest neighbors  k nn  graph  the neighborhood graph contains an edge
connecting two documents di and dj   if di is in the set nk  dj   of the knearest
neighbors of dj   or dj is in the set nk  di   
 ball and k nn both have strongly data dependent parameters  i e    and k  and it is
not straightforward to choose the best value for these parameters  neither guarantees
that the graph would be connected  they also need to be carefully selected or tuned 
as to some extent they also aect the balance between the contribution of neighbors
r  and non neighbors r to the neighborhood regularization r in equation    in
    

fisemantic visualization with neighborhood graph regularization

appendix a  we explore empirically how these graph parameters can help to maintain
this balance within the neighborhood regularization function 
 ball suers from another issue that it tends to produce many edges for the points
located at high density regions  and thus has little restriction on the maximum degree
of a vertex  k nn does not suer from that problem and is one of the most commonly
used types of graphs 
in our subsequent development and experiments  we will experiment with both  ball
and k nn graph as there may be some variance in the performance of dierent graph
construction techniques for dierent datasets  hein  audibert    luxburg        ting 
huang    jordan        coifman   lafon        
   minimum spanning tree based graphs  while  ball and k nn are quite sensitive
to noise and sparsity  graph construction based on combining multiple minimum
spanning trees can help to reduce sensitivity to noise of the output graph  zemel
  carreira perpinan         there are two variations based on this approach 
 a  perturbed minimum spanning trees  pmst   pmst builds a neighborhood
graph by generating t     perturbed copies of the whole dataset according
to the local noise model and tting an mst to each perturbed copy  a weight
eij         will be assigned to the edge between points xi and xj equal to the
average number of times that edge appears on the trees 
 b  disjoint minimum spanning trees  dmst   dmst produces a neighborhood
graph by nding a deterministic collection of r minimum spanning trees that
satises the property that no tree in the collection uses any edge of other trees 
the neighborhood graph is the union of all edges of trees and contains r n    
edges 
as the representative of this category  we use dmst  which is deterministic and easier
to construct than pmst while showing similar ecacies 
      graph weighting
the next issue is how to assign weights to the edges in the neighborhood graph  in this
respect  we consider two variations of edge weights 
   simple minded  

ij  

  
  

if only if di and dj are connected 
otherwise 

   

this is the simplest approach where we use binary weighting to assign the weights
to the edges  however  this approach to assign uniform weights to edges can be
sensitive to errors  because of the cli eect from   immediately to    moreover 
since the weights are not smoothed  it could result in some loss of information  we
hypothesize that among the connected nodes  there may still be some dierences in
terms of degrees of similarity  which are expressed by their mutual distances  this
motivates the second approach below 
    

file   lauw

   heat kernel  

ij  

exp 
  

  di dj    
  


if only if di and dj are connected 
otherwise 

    

an alternative approach is using the heat kernel function  belkin   niyogi       
jebara  wang    chang         heat kernel has the advantage over simple minded by
allowing smoother weights for the edges  which helps address the issues of sensitivity
and loss of information  however  while simple minded is not parameterized  heat
kernel has one parameter that needs to be determined  i e       note that for     
heat kernel degenerates into simple minded  i e   the former is the more general
formulation  the exact value of  is not important in our model because it would
eectively be absorbed by the regularization parameter  for simplicity  we set      

   model fitting
we now discuss how the parameters of the model described in sections   and   can be
learned  one well accepted framework to learn model parameters using maximum a posteriori  map  estimation is the expectation maximization or em algorithm  dempster 
laird    rubin        
for our model  the regularized conditional expectation of the complete data log likelihood in map estimation with priors is 
q     
 

mn 
n 
z




p z n  m    log p z xn    p wnm  z  

n   m   z  
n


z


n  

z  

log p xn     

log p z     

z


log p z   

z  

    r    
where  is the current estimate  p z n  m    is the class posterior probability of the nth
document and the mth word in the current estimate  p z   is a symmetric dirichlet prior
with parameter  for word probability z   p xn   and p z   are gaussian priors with a zero
mean and a spherical covariance for the document coordinates xn and topic coordinates z  
we set the hyper parameters to                n and       z following plsv  iwata
et al         
in the e step  p z n  m    is updated as follows 
p z n  m      z

p z xn    p wnm  z  

z     p z

  x

n    p wnm  z   

 

in the m step  by maximizing q    w r t zw   the next estimate of word probability
zw is as follows 
 n  mn
m   i wnm   w p z n  m      
n  
zw   w 
 
n  mn

m   i wnm   w  p z n  m      w
w   
n  
    

fisemantic visualization with neighborhood graph regularization

where i    is the indicator function  z and xn cannot be solved in a closed form  and are
estimated by maximizing q    using quasi newton  liu   nocedal        
the computation fo the gradients of q    w r t z and xn depend on the specic
kernel used  see section      
 for the gaussian kernel  we have the following gradients 
n


q     
 
p z xn      p z n  m     z  xn    z  
z

n

q   
 
xn

m

n   m  
mn 
z




m   z  


r   
 
p z xn      p z n  m     xn  z    xn    
xn

 for the student t kernel  we have the following gradients 

n mn 
  p z xn      p z n  m     z  xn  
q     
 
 z  
z
      xn  z    
n   m  


mn 
z

  p z xn      p z n  m     xn  z  
q   
r   
 
 xn    
 
 
xn
      xn  z   
xn
m   z  

the gradient of r    w r t  xn is computed depending on the form of the regularization function r     when we use the proposed regularization function r    
described in section        we have the following gradient 
r    
r   
 
xn
xn
 


 xn  xj  
   
 
 nj  xn  xj   
     nj  
 
 
 
 f n   j       
j   j n

j   j n

as mentioned earlier  there is an eciency advantage to regularizing on the visualization space  r    does not contain the variable z if we do regularization on visualization
is o n      in contrast  if we do regularizaspace  the complexity of computing all r   
xn
tion on topic space  we have to take the gradient of r    w r t to z   that contributes
  therefore  regularizatowards a greater complexity of o z    n     to compute all r   
z
tion on topic space would run much slower than on visualization space 

   experiments
the main objective of our experiments is to evaluate the eectiveness of neighborhood regularization for semantic visualization model  after describing the experimental setup  we
rst examine the dierent design choices of the model relating to kernel  graph construction  and regularization function  thereafter  we compare semafore against the baseline
methods that also aim to address both visualization and topic modeling  quantitatively and
qualitatively  rst in terms of visualization and then in terms of topic modeling 
    

file   lauw

    experimental setup
in this section  we give a description of benchmark datasets as well as suitable metrics that
are used for evaluation 
      datasets
we use three real life  publicly available datasets  cardoso cachopo        for evaluation 
   n ews contains newsgroup articles  in english  from    classes 
 reuters  contains newswire articles  in english  from   classes 
 cade   contains web pages  in brazilian portuguese  classied into    classes 
these are benchmark datasets used for document classication  while our task is fully
unsupervised  the ground truth class labels are useful for an objective evaluation  we
create balanced classes by sampling fty documents from each class  following the practice
in plsv  iwata et al          this results in  for one sample       documents for   n ews 
    for reuters   and     for cade    the vocabulary sizes are    k for   n ews     k for
reuters      k for cade    as the algorithms are probabilistic  we generate ve samples
for each dataset  for each sample  we conduct ve independent runs  therefore  the result
reported for each setting is the average over a total of    runs 
      metrics
for a suitable metric  we return to the fundamental principle that a good visualization
should preserve the relationship between documents  in high dimensional space  in the
lower dimensional visualization space  user studies  even when well designed  could be
overly subjective and may not be repeatable across dierent users reliably  therefore  for a
more objective evaluation  we rely on two types of quantitative analysis 
 classication  this evaluation relies on the ground truth class labels found in the
datasets  this is a well established practice in many clustering and visualization
works in machine learning  the basis for this evaluation is the reasonable assumption
that documents of the same class are more related than documents of dierent classes 
therefore a good visualization would place documents of the same class as neighbors
on the visualization 
for each document dn   we hide its true class cn   and generate a prediction for
its class ct  n  by taking the majority class among its t nearest neighbors  as determined by euclidean distance on the visualization space  classication accuracy
classif ication acc t  is dened as the fraction of documents whose predicted class
ct  n  matches the true class cn   more specically  we have 
n
   
 ct  n    cn   
classif ication acc t   
n
n  

    

fisemantic visualization with neighborhood graph regularization

where  is the delta function that equals   if the prediction matches and   otherwise 
the same metric is used in plsv  iwata et al          while accuracy is computed
based on documents coordinates  the same trends will be produced if computed based
on topic distributions  due to their coupling through the kernels described in section      
 neighborhood preservation  this evaluation does not rely on the ground truth class
labels but on the local neighborhood structure in the input data  the assumption is
that a good visualization would be able to preserve the local structure in the input
data as much as possible  if two documents are neighbors in the input data  they
should still be neighbors in the visualization space 
for every document dn   we compute sets of t nearest neighbors yt  n  and xt  n  of
document dn in the input data and the visualization respectively  the neighborhood
preservation accuracy p reservation acc t  is then dened as the average fraction of
the overlap size of yt  n  and xt  n  over the size of yt  n   i e  t   where n              n  
more specically  we have 

p reservation acc t   

n
    yt  n   xt  n  
 
n
t
n  

where  yt  n   xt  n   is the size of the overlap set yt  n   xt  n  
a similar measure can be found in the literature  akkucuk   carroll         where
it is called the rate of agreement in local structure or agreement rate and is used
to measure how well the local structure is preserved between the input data and the
low dimensional embedding  it is also used for tuning the parameters of a non linear
dimensionality reduction method  chen   buja        
in the subsequent experiments  we let t vary in the range         with the step size
  and report the accuracies  since dierent methods may behave dierently at dierent
ts  choosing a specic t for comparison may be unfair for some methods  moreover  a
method that consistently does well for dierent ts would also have a smoother local
structure  therefore  when comparing various methods  we present the preservation or
classication accuracies averaged across t           denoted p reservation acc avg  and
classif ication acc avg  respectively 
    parameter study
in this section  we study the eects of graph parameters on our model  specically  the
parameters concern the graph construction  including the number of neighbors k in k nn
graph  the distance threshold  in  ball graph  and the number of minimum spanning trees
r in dmst  for each type of graph  we use the simple minded weight  for the following
gures  the regularization function is r with       and the number of topics z      
we use neighborhood preservation accuracy p reservation acc t  to show the eects of
graph parameters because this metric does not need ground truth class labels  which are
not always available for tuning these graph parameters 
    

file   lauw




















































w 

w 



w 



e















z















figure    preservation accuracy of semafore when using k nn graph with dierent neighborhood size k for  a    n ews   b  reuters   and  c  cade   











































w 

w 

w 













e















z















figure    preservation accuracy of semafore when using dmst graph with dierent number of minimum spanning trees r for  a    n ews   b  reuters   and  c  cade   





































e











w 

w 



w 



















z





















figure    preservation accuracy of semafore when using  ball graph with dierent values
of distance threshold  for  a    n ews   b  reuters   and  c  cade   

    

fisemantic visualization with neighborhood graph regularization

in figure    we show the performance of our model with dierent neighborhood size k in
k nn graph for dierent datasets  for every k  we vary t and plot the p reservation acc t  
figure   shows that the optimum k for   n ews  reuters   and cade   is         and  
respectively  we compute the average accuracy p reservation acc avg  and it conrms
that the optima are indeed at those k values  from now on  we will use k    for   n ews
and reuters   and k   for cade   when k nn graph is used 
for dmst graph  we plot the p reservation acc t  for dierent number of minimum
spanning trees r with dierent datasets in figure    it is dicult to see which r is the best in
the gure because the dierences between them are not much  the p reservation acc avg 
is computed and it shows that for all three datasets  the optimum is about at r       
subsequently  we will use r   for dmst graphs for all three datasets 
for  ball graph  in figure   we plot the p reservation acc t  for dierent values of 
in the range               we choose that range because       and       roughly give an
average number of neighbors of   and     respectively  the p reservation acc avg  shows
that the optimum  for   n ews  reuters   and cade   is             and      respectively 
    model analysis
in this section  we study the various design choices involved in designing the semafore
model  before nally concluding on the eventual synthesis of design choices to be used for
comparison against the baselines  to keep the discussion focused and organized  in each of
the following sub section  we vary a single design choice  in order to isolate its eects  when
unvaried  the model has the following setup by default  the number of topics is z      
the graph construction method is k nn  the graph weighting method is simple minded  the
rbf kernel is gaussian  and the regularization function is r with       
      neighborhood graph construction
we investigate three graph construction methods  k nn   ball and dmst  which are representatives of neighborhood based and minimum spanning tree based methods respectively 
for each graph  its parameter is tuned as shown in section      for the regularization
parameter   we try dierent settings of  on each dataset  it so happens that      
performs the best for all the graph construction methods across the three datasets 
in figure    we run semafore with dierent types of graph on the three datasets and
report the p reservation acc avg  at dierent number of topics z  the results show that
dierent types of graph behave dierently with dierent datasets  in   n ews   ball and
dmst give our model highest performance  since the dierence between the two are not
statistically signicant  we choose to use dmst for subsequent experiments on   n ews 
for reuters   since  ball outperforms the others  signicant at      level   it is going to
be the default choice for subsequent experiments  for cade    the choice is dmst  which
is slightly better than k nn  statistically signicant for z               
      neighborhood graph weighting
we now compare two variations of graph weighting methods  namely  simple minded and
heat kernel methods  in this experiment  we use k nn graph with specic ks for dierent
    

file   lauw







d d





w 



w 

w 

ee











ed
e











ed



z





ed


figure    the eects of dierent graph construction methods on our models performance 







  t



w 



w 

w 

 dt











ed
e












ed
z





ed


figure    the eects of dierent graph weighting schemes on our models performance 
the graph used in this experiment is k nn graph with specic ks for dierent
datasets as studied in section     











ed
e

  



w 



w 

w 

  











ed
z












ed


figure    the eects of gaussian and student t rbf kernels on our models performance 
    

fisemantic visualization with neighborhood graph regularization

regularization function
graph construction
graph weighting
rbf kernel

  n ews
r
dmst
heat kernel
student t

reuters 
r
 ball
heat kernel
student t

cade  
r
dmst
simple minded
student t

table    synthesized model for each dataset 
datasets as studied in section      the regularization parameter  is set to    after trying
various settings and picking the best one 
in figure    we compare simple minded method and heat kernel method to see their
inuences on our model at dierent number of topics z  we observe that heat kernel
is signicantly and consistently better than simple minded method across all the cases
in   n ews and reuters   the dierence is statistically signicant at      level  one
explanation is that heat kernel assigns smoother weights to the graph edges  and thus is
more robust than simple minded  for cade    simple minded is slightly better  though
the dierences are statistically signicant at      level only for z       subsequently  we
will use heat kernel for   n ews and reuters   and simple minded for cade   as part of
the nal synthesis 
      rbf kernel
as described in section      we express topic distributions as a function of visualization
coordinates using rbf network as an abstraction  in this section  we show how dierent
rbf kernels aect our models performance  the two kernels we are exploring are gaussian
 equation    and student t  equation     we tune the regularization term  for each kernel
and see that the best one for the two kernels are       
figure   shows the results for dierent number of topics z  student t kernel has a slight
edge over gaussian kernel consistently across dierent number of topics  the dierence is
small  but is statistically signicant  at      level  in a majority of the cases  for   n ews
at z                   for reuters  at z       and for cade   at z                the slight
improvement could be a sign that crowding problem does exist in the model  student t
kernel would be even more useful when there is more extreme crowding issues  such as
when the number of documents to be visualized is even larger  subsequently  due to its
slight edge  we will use student t as part of the nal synthesis  as we will see shortly  using
student t within the synthesized model results in a signicant improvement overall 
      synthesised semafore model
based on the model analysis in the preceding paragraphs  we combine the design choices
into a nal synthesis model called semafore  the synthesized model is slightly dierent
for dierent datasets  as listed in table   
we now conduct another set of experiments to verify that those synthesized models
would produce a noticeable improvement over the earlier version  knn   simple minded
  gaussian kernel  that appeared in our earlier work  le   lauw      b   underlining
    

file   lauw

ee d  
    













ed










e





ed
z

ee d  
d d d  

w


w


w


ee d  
d d    













ed


figure     our synthesized models with dierent properties compared to the earlier version
 knn   simple minded   gaussian kernel  that appeared in our earlier work
 le   lauw      b  

the utility of the subsequent enhancements  figure    shows that this is indeed the case 
based on the standard deviations shown in the gures  the improvements are very clear
in   n ews and reuters  but not so clear in cade    paired samples t test indicate that
the improvement is signicant at      level or lower in all cases  except for the cases where
z          in cade    we will use these synthesized models in the comparisons against the
baseline methods in the following section 
    comparison of visualizations
we now compare our proposed model with several baselines  first  we outline the set of
comparative methods  thereafter  we discuss quantitative evaluation  in terms of accuracy  
as well as qualitative evaluation  in terms of example visualizations   finally  we will show
that the gains in visualization quality does not come at the expense of topic modeling 
as semantic visualization seeks to ensure consistency between topic model and visualization  the comparison focuses on methods producing both topics and visualization coordinates
which are listed in table   
 semafore is our proposed method that incorporates neighborhood structure into
semantic visualization 
 plsv  iwata et al         is the state of the art  representing the joint approach
without neighborhood structure preservation 
 pe  lda  represents the pipeline approach involving topic modeling with lda  blei
et al          followed by visualizing documents topic distributions with pe  iwata
et al          this pipeline is better than the lda mds that appeared in our earlier
work  le   lauw      b   there are other pipeline methods  shown inferior to plsv
 iwata et al          which are not reproduced here to avoid duplication 
    

fisemantic visualization with neighborhood graph regularization

visualization

topic model

joint model

neighborhood

semafore
plsv
pe  lda 
t sne  lda 
table    comparative methods 
 t sne  lda  is another pipeline approach that rst uses lda  blei et al         to
learn topic model and then use t sne  van der maaten   hinton        to visualize
documents topic distributions 
for completeness  we also conduct experiments for comparing our method with t sne
and laplacian eigenmaps  le   belkin   niyogi         direct visualization  without topic
modeling   to keep the discussion focused  we show them in appendix b  as we do not
consider t sne and le as comparative baselines because these two methods only model
visualization  but not topics 
      accuracy
in this section  we compare our model with several baselines in terms of classication
accuracy  figure     and neighborhood preservation accuracy  figure      in the two
gures  only the standard deviations for semafore are shown 
classcation accuracy  figure    a      c  and    e  show the classf ication acc t 
at dierent ts for z      for   n ews  reuters   and cade   respectively  at any t  the
comparison shows outperformance by semafore over the baselines consistently  all four
methods show the same behavior that their performances decrease when t increases  as t
increases  they may lose accuracy in predicting labels for documents near to the border of
each cluster 
now  we vary the number of topics z  in figure    b   we show the performance in
classf ication acc avg  on   n ews  figure    d  and    f  show the same for reuters 
and cade   respectively  from these gures  we draw the following observations about the
comparative methods 
 semafore performs the best on all datasets across various numbers of topics  z  
semafore beats plsv by     to     on   n ews  by      on reuters   and by
      on cade    these margins of performance with respect to plsv are statistically signicant at      signicant level or lower in all cases  this eectively showcases
the utility of neighborhood regularization in enhancing the quality of visualization 
by preserving local consistency  semafore achieves a good accuracy even at small
number of topics  e g       
 plsv performs better than pe  lda  and t sne  lda   which shows that there
is utility to having a joint  instead of separate  modeling of topics and visualization 
pe  lda  and t sne  lda  are worse than plsv because it embeds documents by
using two step reductions that optimize separately two dierent objective functions 
    

file   lauw

therefore  the errors from the previous step may propagate to the next  without an
opportunity for correction  this may cause distortions in the visualization 
 in some cases  plsv  pe  lda  and t sne  lda  tend to have decreasing accuracies when the number of topics increases  this may be because when number of topic
increases  the topic distributions and the word probabilities may overt the data and
thus the accuracy is reduced  in contrast  semafore shows a quite stable performance across dierent numbers of topics  this may be explained by the utility of
neighborhood regularization  which helps to prevent overtting when the number of
topics increases 
neighborhood preservation accuracy  while having better classication accuracy 
semafore also preserves well the local structure of the input data in the visualization space 
the p reservation acc t  results in figure    a      c  and    e  show that semafore is
consistently better than the other baselines in terms of neighborhood preservation across
dierent ts and dierent datasets  in figure    b      d  and    f   we vary the number
of topics z and report the p reservation acc avg  results  semafore beats plsv by
    to     on   n ews  by       on reuters   and by       on cade   in terms
of neighborhood preservation accuracy  the improvements of semafore over plsv are
statistically signicant at      signicant level or lower in all cases 
the above accuracy results are based on visualization coordinates  we have also computed accuracies based on topic distributions  which have similar trends 
      visualizations
to provide an intuitive appreciation  we briey describe a qualitative comparison of visualizations  for each method on each dataset  a visualization is shown as a scatterplot  best
seen in color   each document has a coordinate  and is assigned a shape and color based
on its class  each topic also has a coordinate  drawn as a black  hollow circle  a legend is
provided  mapping each symbol to the corresponding class label 
note that this is an illustrative  rather than a comparative discussion  as an objective
evaluation should not rely on eyeballing alone  however  as we have shown the quantitative
results in the preceding section  in this section  we focus on the qualitative study of the
output visualizations 
  news  figure    shows a visualization of   n ews dataset  semafores figure    a 
shows that the dierent classes are well separated  there are distinct clusters of blue squares
and purple diamonds at the top for hockey and baseball classes respectively  clusters of
orange triangles and pink asterisks at the bottom for cryptography and medicine  etc 
beyond individual classes  the visualization also places related classes nearby  computerrelated classes are found on the lower left  politics and religion are on the lower right 
comparatively  figure    b  by plsv shows crowding at the center  for instance 
motorcycle  green dashes  and autos  red dashes  are mixed at the center without a good
separation  figure    c  by pe  lda  is worse  pe  lda  does not give good separation
for not similar classes  it mixes autos  red dashes  and space  green circles  together at
the center  medicine  pink asterisks  is also mixed with other classes in pe  lda  while
semafore and plsv give a good separation for it  figure    d  is visualization by tsne  lda   although t sne  lda  can separate well hockey  blue squares  and baseball
    

fisemantic visualization with neighborhood graph regularization

w  s


















w 




 









es





w 











zs

zs







 





w  s




















 e 




ed



 

w  s






ed
es



w 




 

 e 


s 

 e 











ed
s

figure     classication accuracy comparison 

    





file   lauw

w  s


















w 
w

w

 









es

w

w 








zs

zs







 





w  s



















 e 




ed



w

w  s











ed
es



w 
w

w

 

 e 


s 

 e 










ed
s

figure     preservation accuracy comparison 

    





fisemantic visualization with neighborhood graph regularization

 purple diamonds  classes  it is not able to detect their semantic similarities  as baseball
and hockey are both about sports   in addition  it still mixes documents of dierent classes
together at the center and on the upper right 
reuters   figure    shows the visualization outputs for reuters  dataset  semafore
in figure    a  is better at separating the eight classes into distinct clusters  in an anticlockwise direction from the top  we have navy blue diamonds  money fx    red dashes
 interest   red squares  crude   light blue pluses  earn   green triangles  acq   purple crosses
 ship   blue asterisks  grain   and nally orange circles  trade  
in comparison  plsv in figure    b  shows that several classes are intermixed at the
center  including red dashes  interest   orange circles  trade   and navy blue diamonds
 money fx    pe  lda  in figure    c  is also worse when it mixes dierentiated classes
such as red dashes  interest  and navy blue diamonds  money fx   together  t sne  lda 
in figure    d  seems have better cluster separation but still mix documents with dierent
classes together such as red squares  crude  and green triangles  acq  on the upper right 
green triangles  acq  also mix with light blue pluses  earn  on the left in the visualization
by t sne  lda  
cade    figure    shows the visualization outputs for cade    this is the most
challenging dataset  even so  semafore in figure    a  still achieves a better separation
between the classes  as compared to plsv in figure    b   particularly  semafore gives
better separation for esportes  green triangles  as well as compras on line  orange circles 
than plsv and pe  lda   t sne  lda  shows quite good clusters for esportes  green
triangles  as well as compras on line  orange circles  but it also merges many dierent
classes together as in the clusters on the right and on the upper right 
    comparison of topic models
one question is whether semafores gain in visualization quality over the closest baseline
plsv is at the expense of the quality of its topic model  to investigate this  we will
compare the topic models of semafore and plsv  which share a core generative process 
for parity  in this comparison  we only include the joint models  whereby the visualization
coordinates aect the topic models as well 
the metric we use to measure the quality of topic models is pairwise mutual information
or pmi  it measures topic interpretability  based on coocurrence frequencies of the top words
in each topic in a large external corpus  although other metrics such as perplexity or heldout likelihood can show the generalization ability of a learned topic model on unseen test
data  these traditional metrics do not capture whether topics are coherent  chang  gerrish 
wang  boyd graber    blei         therefore  in this comparison  we rely on pmi  which
can measure the quality of topic words in terms of their interpretability to a human  to
human subjects  interpretability is closely related to coherence  newman  lau  grieser   
baldwin         i e   how much the top keywords in each topic are associated with each
other  after an extensive study of evaluation methods for coherence  newman et al        
identify pointwise mutual information  pmi  as the best measure  in terms of having the
greatest correlation with human judgments 
pmi is based on term cooccurrences  for a pair of words wi and wj   pmi is dened
p wi  wj  
as log p wi  p w
  for a topic  we average the pairwise pmis among the top    words of
j 
    

file   lauw

 

w  s

w 

 e 
 











































figure     visualization of documents in   n ews for number of topics z       each point
represents a document and the shape and color represent document class  each
topic is drawn as a black  hollow circle 
    

fisemantic visualization with neighborhood graph regularization

 

w  s

w 

 e 
 



















figure     visualization of documents in reuters  for number of topics z       each
point represents a document and the shape and color represent document class 
each topic is drawn as a black  hollow circle 

    

file   lauw

 

w  s

w 

 e 
 



























figure     visualization of documents in cade   for number of topics z       each point
represents a document and the shape and color represent document class  each
topic is drawn as a black  hollow circle 

    

fisemantic visualization with neighborhood graph regularization

w  s

 






wd  

wd  

 






w  s









ed








ed



z

e

figure     topic interpretability of semafore and plsv in terms of pmi score  higher
is better  

that topic  for a topic model  we average pmi across the topics  intuitively  pmi is higher
 better   if each topic features words that are highly correlated with one another 
key to pmi is the use of an external corpus to estimate p wi   wj   and p wi    following
newman et al          we use google web  t   gram version    brants   franz        
a huge corpus of n grams generated from   trillion word tokens  p wi   is estimated from
the frequencies of   grams  as recommended by newman et al   p wi   wj   is estimated from
the frequencies of   grams  we obtain pmi for the english based   n ews and reuters  
but not for cade   because we do not possess a large scale n gram corpus specically for
brazilian portuguese 
in figure     we plot the pmi score for various number of topics z  semafore performs
better than plsv across most of the topics settings  in figure    a  for   news  except for
the case at z       all cases of semafores outperformance are signicant at      level or
lower  in figure    b  for reuters   all cases of semafores outperformance are signicant
at      level or lower except for the z       these results show that semafore improves
visualization while not sacricing the topic interpretability of learned topics 
for a greater appreciation of the quality of the output topic models  in appendix c  we
show several examples of topic models for z       for both semafore and plsv  in terms
of the top keywords with the highest probabilities for each topic 

   conclusion
in this paper  we address the semantic visualization problem  which jointly conducts topic
modeling and visualization of documents  we propose a new framework to incorporate
neighborhood structure within a probabilistic semantic visualization model called semafore 
the model is carefully designed to reect the context of semantic visualization  leading to
a number of design choices related to the rbf kernel for mapping topic and visualiza    

file   lauw

tion spaces  the approximation of neighborhood graph through construction and weighting 
as well as the appropriate regularization functions and spaces  experiments on real life
datasets show that semafore signicantly outperforms the baselines in terms of visualization quality and accuracy  while having a similar  if not slightly better topic model  this
provides evidence that neighborhood structure  together with joint modeling of topics and
visualization  is important for semantic visualization 

appendix a  balancing contributions of neighbors and non neighbors
to regularization
as mentioned in section      the balance between the contribution of neighbors r  and
non neighbors r to the neighborhood regularization r in equation   may require careful
tuning of the graph parameters  i e    or k   for example  in the case of using k nn graph
and n total number of documents  we would have kn terms in the neighbor regularization
r    and  n k n terms in the non neighbor regularization r   supposing that n increases
signicantly  there might be imbalance if k were to remain unchanged  therefore  as n
changes  k should also be tuned accordingly to maintain this balance  for a simplistic
point  the ratio between kn and  n  k n would remain roughly the same if both n and
k grow by similar factors  in practice  we recommend tuning k carefully 
we run additional experiments to validate the above argument on the   n ews dataset 
our basic point is that as n changes  k can be tuned to still show signicant improvement
due to the neighborhood graph regularization  the closest baseline is plsv  both empirically in terms of classication accuracy  as well as conceptually as plsv shares a similar
generative process but with a dierent kernel and without neighborhood regularization 
hence  we compare the performance of our method semafore  with k nn graph  heat
kernel weighting  and student t kernel  to plsv on various data sizes at z      topics 
 figure    a  is for dataset of size n        and semafore runs with k      
 figure    b  is for dataset of size n         and semafore runs with k      
 figure    c  is for dataset of size n         and semafore runs with k      
we note that there is a   x dierence between the smallest and the largest datasets 
yet the relative outperformance of semafore over plsv by around     to     is evident
across the three datasets  this supports the case that k can be tuned to produce a positive
eect using neighborhood graph regularization 

appendix b  additional comparisons
as mentioned in section      for completeness  we include here additional comparisons to
visualization methods that do not also aim at topic modeling  in particular  we include two
methods  first  we include t sne  van der maaten   hinton         which is also used
in the composite t sne  lda   second  we include laplacian eigenmaps  le   belkin  
niyogi         which takes as input the neighborhood graph  figure    and figure   
show the classication accuracy and preservation accuracy of semafore   t sne and le
    

fisemantic visualization with neighborhood graph regularization










  



 



w  s

 


w  s




 















  












e

e

w  s





  





e

figure     classication accuracy comparison on   n ews with various data sizes  z       

 

 e







 


 





 













































e













z



figure     classication accuracy comparison 

 

 e







w


w

w



 
















e
























z

figure     preservation accuracy comparison 

    










file   lauw

when varying t  semafore outperforms le in all cases  for t sne  semafore outperforms t sne for reuters   however  for   n ews and cade    it is more dicult to tell
whether semafore or t sne is better  t sne tends to have decreasing accuracy as t increases  this is expected because t sne is known to focus on preserving the local structure
 van der maaten   hinton         when t is small  we basically consider only the local
structure of the visualization  when t increases  we consider the more global structure of the
visualization and semafore outperforms t sne signicantly  overall  semafore is more
stable than t sne as t changes  which indicates that semafore tries to balance preserving
the local and global structure better than t sne  we emphasize that this comparison is for
information purpose only  as we do not regard t sne and le as comparative baselines 

appendix c  topic model examples
we showcase the topic models derived by semafore and plsv  for   n ews  table   shows
the topics of semafore  and table   shows the topics of plsv  for reuters   table  
shows the topics of semafore  and table   shows the topics of plsv  for cade    table  
shows the topics of semafore  and table   shows the topics of plsv 
for each method  we show the list of twenty topics  for each topic  we produce the
top ten words with the highest probabilities  as shown by the top words  the topics do
correspond strongly to some of the classes  for example  topic s   in table   for   n ews is
about christianity  which corresponds to the soc religion christian class  topic s  is about
cars and motorcycles  corresponding to rec autos and rec motorcycles  topic s   is probably
concerning the categories of rec sport baseball and or rec sport hockey 
overall  we observe that the quality of topic words are comparable across the comparative methods  note that there is no direct correspondence between the topics of dierent
methods  e g   the rst topic of semafore may not correspond to the rst topic of plsv  
through manual inspection  we can see that there are some related topics  e g   s  and p   
or s   and p    however  the sets of topics and the set of keywords for each topic are not
identical  this is borne out in the slight dierence in terms of pmi scores 
this qualitative study helps to show that semafore improves the visualization quality 
while still maintaining at least the same quality of topic words  if not better  this supports
the conclusion reached by the quantitative comparisons in the main manuscript 

    

fisemantic visualization with neighborhood graph regularization

table    semafores topic model for   n ews  for    topics 
topic id
s 
s 
s 
s 
s 
s 
s 
s 
s 
s 
s  
s  
s  
s  
s  
s  
s  
s  
s  
s  

top    words
space  system   rcb   book  computer  university  list  post  price  science
article  year  good  write  guy  well  time  head  question  leave
gun  law  kuwait  people  death  fbus  article  control  weapon  child
window  le  program  widget  application  type  will  resource  call  function
car  bike  speed  engine  drive  lock  turn  mile  front  change
will  power  place  work  rate  write  sound  lead  good  interested
write  article  thing  time  people  better  start  problem  will  good
write  time  people  friend  pay  public  article  tax  opinion  money
people  claim  write  system  person  moral  evidence  objective  read  state
image  datum  graphic  send  le  format  package  software  mail  include
armenian  re  jew  child  kill  start  people  turkish  door  israel
system  board  will  datum  time  work  tape  test  copy  command
game  team  year  player  win  play  will  hit  season  hockey
will  post  space  good  time  include  cost  option  launch  people
drive  card  window  appear  disk  ram  driver  memory  work  color
mr   president  stephanopoulo  state  group  consider  party  question  issue  press
write  article  well  will  thing  work  point  include  time  help
key  article  chip  food  write  people  government  encryption  thing  algorithm
price  buy  apple  computer  dealer  t  model  problem  sell  monitor
god  jesus  will  christian  religion  faith  truth  bible  belief  church

table    plsvs topic model for   n ews  for    topics 
topic id
p 
p 
p 
p 
p 
p 
p 
p 
p 
p 
p  
p  
p  
p  
p  
p  
p  
p  
p  
p  

top    words
write  people  christian  belief  time  faith  god  religion  life  will
god  will  jesus  kuwait  atheist  church  christian  man  religion  sin
armenian  appear  art  turkish  tartar   st  village  armenia        genocide
will  key  write  time  article  government  system  thing  chip  hit
mr   stephanopoulo  president  will  party  state  door  time  meeting  open
write  re  article  gun  system   rcb   start  people  fbus  claim
car  will  bike  engine  drive  well  dealer  battery  change  front
game  win  year  will  team  play  season  good  goal  playo
player  team  write  hockey  game  fan  article  year  will  guy
space  system  datum  will  april  nasa  security  university  computer  list
graphic  image  le  ftp  send  format  package  system  datum  object
image  datum  program  window  version  le  software  tool  support  user
drive  jumper  master  ndet loop  slave  rate  gun  function  crime  set
window  le  card  will  program  color  driver  support  disk  bit
people  write  state  article  law  government  country  rights  jew  will
write  article  thing  people  good  will  time  lot  year  day
work  drive  tape  scsus  problem  simm  controller  write  memory  article
widget   rcb   window   lcb   application  resource  set  visual  type  le
price  will  write  system  computer  article  apple  chip  monitor  board
will  vote  comp  newsgroup  suit  problem  os   sco  post  mail

    

file   lauw

table    semafores topic model for reuters   for    topics 
topic id
s 
s 
s 
s 
s 
s 
s 
s 
s 
s 
s  
s  
s  
s  
s  
s  
s  
s  
s  
s  

top    words
company  pipeline  raise  crude  march  spokesman  renery  capacity  corp  post
pct  bank  day  stg  today  reuter  money  market  mln  bill
oer  share  company  board  group  acquire  stock  dlr  acquisition  receive
exchange  currency  dollar  west  nance  baker  monetary  germany  continue  interest
share  reuter  dlr  mln  buy  company  corp  pay  stock  group
price  opec  market  bpd  ocial  february  month  output  saudus  january
rate  bank  pct  cut  fund  prime  point  reserve  issue  lower
billion  foreign  import  increase  dlr  trade  economic  export  will  country
bank  billion  market  government  fall  stock  economy  rise  surplus  decit
will  company  sell  pct  vessel  operation  week  billion  shipping  unit
strike  port  union  spokesman  cargo  employer  worker  sector  redundancy  court
oil  export  dlr  industry  year  pct  future  company  report  price
reuter  pct  report  national  week  brazil  today  increase  pay  april
trade  japan  japanese  reagan  state  tari  unite  market  washington  ocial
grain  mln  soviet  crop  tonne  year  usda  production  fall  analyst
trade  talk  gulf  gatt  bill  yeutter  round  reuter  call  negotiation
certicate  reuter  cost  government  program  agreement  agriculture  will  study  loan
year  ocial  import  will  state  price  government  china  land  rise
mln  ct  loss  net  shr  dlr  prot  qtr  reuter  year
oil  mln  will  barrel  dlr  crude  source  level  petroleum  day

table    plsvs topic model for reuters   for    topics 
topic id
p 
p 
p 
p 
p 
p 
p 
p 
p 
p 
p  
p  
p  
p  
p  
p  
p  
p  
p  
p  

top    words
will  oil  company  reuter  industry  canada  price  shell  raise  sell
rate  currency  dollar  exchange  baker  west  will  bank  reuter  treasury
bank  pct  day  import  year  rate  export  february  expect  reuter
share  company  corp  oer  stock  board  will  reuter  dlr  buy
rate  bank  pct  prime  cut  point  interest  market  lower  savings
market  bank  stock  price  japan  ministry  rise  ocial  gulf  bond
reuter  pct  week  report  year  march  mark  american  commission  gure
mln  ct  loss  net  dlr  shr  year  prot  qtr  reuter
mln  pct  billion  stg  dlr  reuter  market  january  revise  rise
billion  dlr  rate  market  surplus  currency  reserve  trading  dollar  foreign
oil  opec  price  bpd  pipeline  mln  crude  ocial  dlr  output
crude  dlr  barrel  corp  capacity  renery  oil  company  oer  group
reuter  ocial  state  cut  gulf  government  today  action  force  tell
oil  government  indonesium  price  foreign  bank  billion  reserve  company  industry
certicate  company  mln  year  grain  cooperative  program  dlr  government  cost
year  trade  agriculture  reuter  grain  agreement  gatt  yeutter  nancial  agricultural
strike  port  union  spokesman  employer  brazil  cargo  worker  redundancy  sector
trade  japan  japanese  reagan  tari  unite  washington  state  nakasone  semiconductor
grain  mln  crop  tonne  soviet  year  ocial  china  pct  oer
trade  country  minister  talk  state  meeting  economic  exchange  issue  baldrige

    

fisemantic visualization with neighborhood graph regularization

table    semafores topic model for cade    for    topics 
topic id
s 
s 
s 
s 
s 
s 
s 
s 
s 
s 
s  
s  
s  
s  
s  
s  
s  
s  
s  
s  

top    words
sp  aulas  tecnologia  rj  sao  area  janeiro  particulares  areas  sica
terra  jun  gif  busca  virtual  brasil  forum  tempo  noticias  revistas
trabalho  seguranca  saude  medicina  ocupacional  prevencao  ppra  pcmso  imagem  imagens
peixes  cade  lazer  pesca  agua  rio  praia  hotel  sao  doce
agar  vida  personal  sica  base  tratamento  tem  pode  sistema  trainer
sao  br  rio  sul  criancas  www  escola  mail  http  atendimento
links  page  home  fotos  pagina  dicas  download  tenis  informacoes  jogos
internet  informatica  acesso  mg  br  servicos  provedor  mail  revista  horizonte
servicos  sao  paulo  entregas  entrega  sp  cesta  express  empresa  servico
pesca  sp  grupo  brasil  eventos  video  mg  informacoes  turismo  danca
astronomia  pagina  jose  foi  bem  espaco  tem  veja  losoa  correio
mp  banda  musicas  rock  musica  page  letras  bandas  pagina  site
historia  cultura  mundo  site  page  brasil  informacoes  rs  livro  arte
noticias  jornal  cidade  sp  sao  regiao  demolay  ordem  rio  capitulo
empresas  informacoes  informacao  dados  atraves  textos  mail  equipe  unicamp  centro
engenharia  servicos  projetos  empresa  consultoria  quimica  instituto  pesquisa  rio  manutencao
site  informacoes  brasil  associacao  educacao  pagina  organizacao  centro  brasileira  direitos
software  web  empresa  sistemas  sistema  br  marketing  desenvolvimento  windows  dados
virtual  online  venda  produtos  cade  shopping  internet  loja  compras  cursos
futebol  informacoes  fotos  clube  historia  paulo  sao  quake  pagina  cade

topic id
p 
p 
p 
p 
p 
p 
p 
p 
p 
p 
p  
p  
p  
p  
p  
p  
p  
p  
p  
p  

top    words
engenharia  projetos  servicos  trabalho  empresa  consultoria  seguranca  sp  medicina  sao
sao  ong  rio  instituto  personal  educacao  organizacao  sp  paulo  ns
sao  br  desenvolvimento  sistema  tratamento  mail  sistemas  clientes  informacoes  empresa
aulas  formula  quimica  particulares  informacoes  matematica  pilotos  fotos  sica  site
jornal  tenis  noticias  esportes  sp  informacoes  sao  esporte  fotos  links
musica  page  rock  bandas  links  home  pagina  musicas  music  fotos
pesca  demolay  sp  peixes  sao  fotos  ordem  capitulo  paulo  jitsu
mp  musicas  nacionais  agar  internacionais  rock  formato  site  page  pagina
pesquisa  tecnologia  informacoes  cade  ciencia  geograa  pesquisas  area  instituto  pagina
site  pagina  internet  mail  clique  veja  br  pode  foi  links
astronomia  informacoes  cultura  site  pagina  brasil  home  page  fotos  historia
banda  fotos  rock  letras  page  musicas  pagina  site  home  mp
internet  provedor  acesso  mg  informatica  software  servicos  belo  horizonte  manutencao
futebol  clube  sao  paulo  campeonato  historia  informacoes  pagina  turismo  tricolor
noticias  terra  internet  brasil  informatica  online  jornal  virtual  servicos  busca
links  page  quake  home  pagina  fotos  dicas  mp  download  informacoes
grupo  banda  karate  pagina  page  informacoes  fotos  home  rio  historia
produtos  virtual  shopping  cade  venda  online  sao  rio  loja  compras
br  sao  informacoes  marketing  mail  empresa  internet  www  fax  site
vida  dia  sao  foi  terra  panico  jose  tem  planetas  grande

table    plsvs topic model for cade    for    topics 

    

file   lauw

references
akkucuk  u     carroll  j  d          paramap vs  isomap  a comparison of two nonlinear
mapping algorithms  journal of classication                 
bai  l   guo  j   lan  y     cheng  x          local linear matrix factorization for
document modeling  in advances in information retrieval  pp          springer 
belkin  m     niyogi  p          laplacian eigenmaps and spectral techniques for embedding and clustering  in advances in neural information processing systems  nips  
vol      pp         
belkin  m     niyogi  p          laplacian eigenmaps for dimensionality reduction and data
representation  neural computation                   
belkin  m   niyogi  p     sindhwani  v          manifold regularization  a geometric framework for learning from labeled and unlabeled examples  journal of machine learning
research  jmlr               
bishop  c  m          neural networks for pattern recognition  oxford university press 
bishop  c  m   svensen  m     williams  c  k          gtm  the generative topographic
mapping  neural computation                 
blei  d  m   ng  a  y     jordan  m  i          latent dirichlet allocation  journal of
machine learning research  jmlr              
brants  t     franz  a          web  t   gram version    linguistic data consortium 
philadelphia 
buhmann  m  d          radial basis functions  acta numerica         
cai  d   mei  q   han  j     zhai  c          modeling hidden topics on document manifold 
in proceedings of the acm conference on information and knowledge management
 cikm  
cai  d   wang  x     he  x          probabilistic dyadic data analysis with local and global
consistency  in proceedings of the international conference on machine learning
 icml  
cardoso cachopo  a          improving methods for single label text categorization  phd
thesis  instituto superior tecnico  universidade tecnica de lisboa 
carey  c     mahadevan  s          manifold spanning graphs  in twenty eighth aaai
conference on articial intelligence 
chaney  a  j  b     blei  d  m          visualizing topic models  in proceedings of the
international aaai conference on web and social media  icwsm  
chang  j   gerrish  s   wang  c   boyd graber  j  l     blei  d  m          reading
tea leaves  how humans interpret topic models  in advances in neural information
processing systems  pp         
chen  l     buja  a          local multidimensional scaling for nonlinear dimension reduction  graph drawing  and proximity analysis  journal of the american statistical
association                    
    

fisemantic visualization with neighborhood graph regularization

chi  e  h  h          a taxonomy of visualization techniques using the data state reference model  in proceedings of the ieee symposium on information visualization
 infovis   pp       
choo  j   lee  c   reddy  c  k     park  h          utopian  user driven topic modeling based on interactive nonnegative matrix factorization  ieee transactions on
visualization and computer graphics                    
chuang  j   manning  c  d     heer  j          termite  visualization techniques for assessing textual topic models  in proceedings of the international working conference
on advanced visual interfaces  avi   pp       
coifman  r  r     lafon  s          diusion maps  applied and computational harmonic
analysis                
comon  p          independent component analysis  a new concept   signal processing 
               
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood from incomplete
data via the em algorithm  journal of the royal statistical society  series b         
    
dumais  s   furnas  g   landauer  t   deerwester  s   deerwester  s   et al          latent
semantic indexing  in proceedings of the text retrieval conference 
fisher  r  a          the use of multiple measurements in taxonomic problems  annals of
eugenics                
gabrilovich  e     markovitch  s          wikipedia based semantic interpretation for
natural language processing  journal of articial intelligence research  jair          
    
golub  g  h     van loan  c  f          matrix computations  vol     jhu press 
gretarsson  b   odonovan  j   bostandjiev  s   hollerer  t   asuncion  a   newman  d    
smyth  p          topicnets  visual analysis of large text corpora with topic modeling 
acm transactions on intelligent systems and technology  tist             
hein  m   audibert  j  y     luxburg  u  v          graph laplacians and their convergence
on random neighborhood graphs  in journal of machine learning research  pp 
         
hinton  g  e     roweis  s  t          stochastic neighbor embedding  in advances in
neural information processing systems  nips   pp         
hofmann  t          probabilistic latent semantic indexing  in proceedings of the international acm sigir conference on research and development in information
retrieval  sigir   pp       
hu  y   boyd graber  j   satino  b     smith  a          interactive topic modeling 
machine learning                 
huh  s     fienberg  s  e          discriminative topic modeling based on manifold learning 
acm transactions on knowledge discovery from data  tkdd             
    

file   lauw

iwata  t   saito  k   ueda  n   stromsten  s   griths  t  l     tenenbaum  j  b         
parametric embedding for class visualization  neural computation                   
iwata  t   yamada  t     ueda  n          probabilistic latent semantic visualization  topic
model for visualizing documents  in proceedings of the acm sigkdd international
conference on knowledge discovery and data mining  kdd   pp         
jebara  t   wang  j     chang  s  f          graph construction and b matching for semisupervised learning  in proceedings of the   th annual international conference on
machine learning  pp          acm 
jollie  i          principal component analysis  wiley online library 
kim  m     torre  f          local minima embedding  in proceedings of the international
conference on machine learning  icml   pp         
kohonen  t          the self organizing map  proceedings of the ieee                   
kruskal  j  b          multidimensional scaling by optimizing goodness of t to a nonmetric
hypothesis  psychometrika              
laerty  j  d     wasserman  l          statistical analysis of semi supervised regression 
in advances in neural information processing systems  nips   pp         
le  t     lauw  h  w       a   semantic visualization for spherical representation  in
proceedings of the acm sigkdd international conference on knowledge discovery
and data mining  pp            acm 
le  t  m     lauw  h  w       b   manifold learning for jointly modeling topic and
visualization  in proceedings of the aaai conference on articial intelligence 
liu  d  c     nocedal  j          on the limited memory bfgs method for large scale
optimization  mathematical programming             
manning  c  d   raghavan  p   schutze  h   et al          introduction to information
retrieval  vol     cambridge university press cambridge 
mccallum  a   wang  x     corrada emmanuel  a          topic and role discovery in
social networks with experiments on enron and academic email   journal of articial
intelligence research  jair              
millar  j  r   peterson  g  l     mendenhall  m  j          document clustering and
visualization with latent dirichlet allocation and self organizing maps  in flairs
conference  vol      pp       
newman  d   karimi  s     cavedon  l          external evaluation of topic models  in
australasian document computing symposium  adcs  
newman  d   lau  j  h   grieser  k     baldwin  t          automatic evaluation of topic
coherence  in human language technologies  the      annual conference of the
north american chapter of the association for computational linguistics  pp     
    
park  j     sandberg  i  w          universal approximation using radial basis function
networks  neural computation                
    

fisemantic visualization with neighborhood graph regularization

ponzetto  s  p     strube  m          knowledge derived from wikipedia for computing
semantic relatedness   journal of articial intelligence research  jair              
reisinger  j   waters  a   silverthorn  b     mooney  r  j          spherical topic models 
in proceedings of the international conference on machine learning  icml   pp 
       
roweis  s  t     saul  l  k          nonlinear dimensionality reduction by locally linear
embedding  science                       
shaw  b     jebara  t          minimum volume embedding  in proceedings of the international conference on articial intelligence and statistics  aistats   pp         
shaw  b     jebara  t          structure preserving embedding  in proceedings of the
international conference on machine learning  icml   pp          acm 
tenenbaum  j  b   de silva  v     langford  j  c          a global geometric framework
for nonlinear dimensionality reduction  science                       
ting  d   huang  l     jordan  m  i          an analysis of the convergence of graph laplacians  in proceedings of the international conference on machine learning  icml  
turney  p  d   pantel  p   et al          from frequency to meaning  vector space models
of semantics  journal of articial intelligence research  jair                  
van der maaten  l     hinton  g          visualizing data using t sne  journal of machine
learning research  jmlr                     
wei  f   liu  s   song  y   pan  s   zhou  m  x   qian  w   shi  l   tan  l     zhang 
q          tiara  a visual exploratory text analytic system  in proceedings of the
acm sigkdd international conference on knowledge discovery and data mining
 kdd   pp         
wu  h   bu  j   chen  c   zhu  j   zhang  l   liu  h   wang  c     cai  d          locally
discriminative topic modeling  pattern recognition                 
zemel  r  s     carreira perpinan  m  a          proximity graphs for clustering and
manifold learning  in advances in neural information processing systems  nips  
pp         
zhou  d   bousquet  o   lal  t  n   weston  j     scholkopf  b          learning with local
and global consistency  advances in neural information processing systems  nips  
        
zhu  x   ghahramani  z   laerty  j   et al          semi supervised learning using gaussian
elds and harmonic functions  in proceedings of the international conference on
machine learning  icml   vol     pp         

    

fi