journal artificial intelligence research                   

submitted        published      

distributed representation based framework
cross lingual transfer parsing
jiang guo
wanxiang che

jguo   ir   hit  edu   cn
car   ir   hit  edu   cn

research center social computing information retrieval
harbin institute technology
harbin  heilongjiang  china

david yarowsky

yarowsky   jhu   edu

center language speech processing
johns hopkins university
baltimore  md  usa

haifeng wang

wanghaifeng   baidu   com

baidu inc   beijing  china

ting liu

tliu   ir   hit  edu   cn

research center social computing information retrieval
harbin institute technology
harbin  heilongjiang  china

abstract
paper investigates problem cross lingual transfer parsing  aiming inducing dependency parsers low resource languages using training data resource rich
language  e g   english   existing model transfer approaches typically dont include lexical features  transferable across languages  paper  bridge lexical feature gap
using distributed feature representations composition  provide two algorithms
inducing cross lingual distributed representations words  map vocabularies two different languages common vector space  consequently  lexical features non lexical
features used model cross lingual transfer  furthermore  framework flexible
enough incorporate additional useful features cross lingual word clusters  combined
contributions achieve average relative error reduction       labeled attachment score
compared delexicalized parser  trained english universal treebank transferred
three languages  significantly outperforms state of the art delexicalized models augmented projected cluster features identical data  finally  demonstrate models
boosted minimal supervision  e g       annotated sentences  target languages  great significance practical usage 

   introduction
dependency parsing one long standing central problems natural language processing  nlp   goal dependency parsing induce implicit tree structures natural language
sentence following dependency grammar  highly beneficial various downstream
tasks  question answering  machine translation knowledge mining representation 
majority work dependency parsing dedicated resource rich languages  english chinese  languages  exists large scale annotated treebanks used
     ai access foundation  rights reserved 

fig uo   c   yarowsky  wang   l iu

supervised training dependency parsers  penn treebank  marcus  marcinkiewicz 
  santorini        xue  xia  chiou    palmer         however  languages
world  even labeled training data parsing  labor intensive
time consuming manually annotate treebanks languages  fact given rise
range research unsupervised methods  klein   manning         transfer methods  hwa  resnik  weinberg  cabezas    kolak        mcdonald  petrov    hall        linguistic
structure prediction 
considering unsupervised methods fall far behind transfer methods terms
accuracy  well difficulty evaluation  focus transfer methods study 
attempt build parsers low resource languages exploiting treebanks resource rich
languages  two approaches linguistic transfer general  namely data transfer
model transfer  data transfer methods emphasizes creation artificial training data
used supervised training target language side  appealing property
learn language specific linguistic structures effectively  major drawbacks
requirement parallel data noise automatically created training data introduced
word alignment based projection  hand  model transfer methods build models
source language side  used directly parsing target languages without need
creating annotated data target languages 
paper falls latter category  major obstacle transferring parsing system
one language another lexical features  e g   words  directly transferable
across languages  address challenge  mcdonald et al         built delexicalized parser parser non lexical features  delexicalized parser makes sense pos
tag features significantly predictive unlabeled dependency parsing  however  labeled
dependency parsing  especially semantic oriented dependencies stanford typed dependencies  de marneffe et al         de marneffe   manning         non lexical features
predictive enough  tackstrom  mcdonald  uszkoreit        proposed learn cross lingual
word clusters multilingual paralleled unlabeled data word alignments  apply
clusters features semi supervised delexicalized parsing  word clusters thought
kind coarse grained representations words  thus  approach partially fills gap lexical
features cross lingual learning dependency parsing 
paper proposes novel approach cross lingual dependency parsing based
pure distributed feature representations  contrast discrete feature representations used
traditional dependency parsers  distributed representations map symbolic features continuous
representation space  shared across languages  therefore  model ability
utilize lexical non lexical features naturally  specifically  framework contains two
primary components 
neural network based dependency parser  expect non linear model dependency
parsing study  distributed feature representations shown effective non linear architectures linear architectures  wang   manning         chen
manning        proposed transition based dependency parser using neural network
architecture  simple works well benchmark datasets  briefly  model simply replaces predictor transition based dependency parser well designed neural
network classifier  provide explanations merits model section   
well adapt cross lingual task 
   

fir epresentation l earning c ross  l ingual ransfer parsing

cross lingual word representation learning  key filling lexical feature gap
project representations features different languages common vector
space  preserving translational equivalence  study compare two approaches
learning cross lingual word representations section    first approach named
robust projection  second approach based canonical correlation analysis 
approaches simple implement scalable large data 
another drawback model transfer methods focus universal structures across various languages  thus lack ability recovering target language specific
structures  therefore  necessary conduct target language adaptation top transferred models  introduce practical straightforward solution incorporating minimal
supervision target languages  section    
evaluate models universal multilingual treebanks v     mcdonald et al         
case studies include transferring english  en  german  de   spanish  es  french
 fr   experiments show incorporating lexical features  performance cross lingual
dependency parsing improved significantly  embedding cross lingual cluster features  tackstrom et al          achieve average relative error reduction       labeled
attachment score  las   compared delexicalized parsers  significantly outperforms delexicalized models mcdonald et al  augmented cluster features identical
data  addition  show using small amount labeled training data  e g       sentences  target language side parameter adaptation  minimal supervision   performance
cross lingual transfer system boosted  recalls language specific dependency
structures improved dramatically  
original major contributions paper include 
propose novel flexible cross lingual learning framework dependency parsing
based distributed representations  effectively incorporate lexical nonlexical features 
present two novel effective approaches inducing cross lingual word representation
bridge lexical feature gap cross lingual dependency parsing transfer 
show cross lingual word cluster features effectively embedded model 
leading significant additive improvements 
show cross lingual transfer systems easily effectively adapted
target languages minimal supervision  demonstrating great potential practical usage 

   background
section describes necessary background crucial understanding transfer
parsing framework 
   article thoroughly revised extended version work guo  che  yarowsky  wang  liu        
provide detailed linguistic methodological background cross lingual parsing  additional extensions
primarily include experiments analysis target language adaptation minimal supervision  system
made publicly available at  https   github com jiangfeng     acl   clnndep 

   

fig uo   c   yarowsky  wang   l iu

punct
root

dobj
nsubj

root


pron

amod


verb

good
adj

control
noun

 
 

figure    example labeled dependency tree 

    dependency parsing
given input sentence x   w  w     wn wi ith word x  goal dependency
parsing build dependency tree  denoted     h  m  l    h n     
n  l l    h  m  l  indicates directed arc head word wh modifier wm
dependency label l  l label set  figure    
mainstream models proposed dependency parsing described
either graph based models transition based models  mcdonald   nivre         graph based
models  eisner        mcdonald  crammer    pereira        view parsing problem finding
highest scoring tree directed graph  score dependency tree typically factored
scores small independent structures  way factorization defines order
model complexity inference process  mcdonald   pereira        carreras 
      koo   collins         instance  first order models factored dependency arcs 
thus known arc factored models  higher order models would consider expressive
substructures sibling grandchild structures  transition based models instead aim
predict transition sequence initial parser state terminal states  conditioned
parsing history  yamada   matsumoto        nivre        nivre  hall    nilsson        
approach lot interest since fast  linear time projective parsing  incorporate
rich non local features  zhang   nivre        
considered past simple transition based parsing using greedy decoding
local training accurate graph based parsers globally trained use exact
inference algorithms  however  chen manning        showed greedy transition based
parsers significantly improved well designed neural network architecture  approach considered new paradigm parsing  based pure distributed
feature representations  recently  architecture improved different ways 
example  weiss  alberti  collins  petrov        combined neural network structured
perceptron  use beam search decoding  achieving new state of the art performance  dyer  ballesteros  ling  matthews  smith        instead explored novel techniques learning
better representations parser states utilizing long short term memory networks  lstm  
work includes zhou  zhang  huang  chen        applied structured learning
beam search decoding neural network model  study  choose original
chen   mannings architecture  without losing generality  build basic dependency parsing
models cross lingual transfer 
   

fir epresentation l earning c ross  l ingual ransfer parsing

    distributed representations nlp
recent years seen numerous attempts learning distributed representations different natural language objects  morphemes  words phrases  sentences documents  using
distributed representations  symbolic units embedded dense  continuous lowdimensional vector space  thus often referred embeddings  
distributed representation attractive nlp several reasons  first  provides straightforward way measuring similarities natural language objects  distributed
representations  easily tell two words phrases documents similar semantic
even aspects simply measuring cosine distance vectors 
second  learned large scale unannotated data general  thus highly beneficial various downstream applications source alleviate data sparsity 
straightforward way applying distributed representations nlp tasks fed distributed
feature representations existing supervised nlp systems augmented features  semisupervised fashion  turian  ratinov    bengio         despite simplicity effectiveness 
shown potential distributed representations cannot fully exploited generalized linear models adopted traditional nlp systems  wang   manning 
       one remedy discretize distributed feature representations  convert continuous  dense low dimensional vectors traditional discrete  sparse high dimensional
space  studied guo  che  wang  liu         however  believe non linear system
 e g   neural network  powerful promising solution  decent progress already
made paradigm nlp various tasks  neural sequence labeling  collobert
et al          dependency parsing  chen   manning         sentence classification  kim       
machine translation  sutskever  vinyals    le        
third  provides kind representation shared across languages  tasks
even diverse modalities data resources  property motivated lines research multilingual representation learning  klementiev et al         chandar p et al         hermann  
blunsom         multi task learning  collobert   weston        multi modal learning  srivastava   salakhutdinov         primary motivation work facilitates
cross lingual transfer parsing via multilingual distributed representation learning words 

   cross lingual dependency parsing
section  first describe primary transition based dependency parsing model utilizing
neural networks  details cross lingual transfer 
    neural network architecture transition based dependency parsing
section  first briefly describe transition based dependency parsing arc standard
parsing algorithm  revisit neural network architecture transition based dependency
parsing proposed chen manning        
discussed section      transition based parsing generates dependency tree predicting transition sequence initial parser state terminal state  several transition based
parsing algorithms presented literature  arc standard arc eager algorithms projective parsing  nivre               list based algorithm  nivre       
   paper  two terminologies used interchangeably 

   

fig uo   c   yarowsky  wang   l iu

swap based algorithm  nivre        non projective parsing  different algorithms different
transition actions  take arc standard algorithm example  parsing state  typically known
configuration  represented tuple consisting stack s  buffer b  partially derived forest  i e   set dependency arcs  a  given input word sequence x   w  w         wn  
initial configuration represented as   w   s    w  w         wn  b     terminal configuration  w   s     b   a  w  pseudo word indicating root whole dependency
tree  denoting si  i              ith element stack  bi  i              ith element buffer   arc standard system defines three types transition actions  l eft a rc r  
r ight a rc r   hift  r dependency relation 
r

l eft a rc r   extend new arc  s 
s     s  head s  modifier 
remove s  stack 
r

r ight a rc r   extend new arc  s 
s     s  head s  modifier 
pop s  stack 
hift  move b  buffer stack  precondition b empty 
typical approach greedy arc standard parsing build multi class classifier  e g  
support vector machines  maximum entropy models  predicting transition action given feature vector extracted specific configuration  conventional feature engineering suffers
problem sparsity  incompleteness expensive feature computation  chen   manning 
       neural network model provides effective solution 
architecture neural network based dependency parsing model illustrated figure    unlike high dimensional  sparse discrete features used traditional parsing models 
neural network model  apply distributed feature representations  primarily  three types
information extracted configuration chen   mannings model  word features  pos
features relation features respectively  study  add non local features including distance features indicating distance two items  valency features indicating
number children given item  zhang   nivre         distance valency features
discretized buckets  features projected embedding layer via corresponding lookup tables  i e   embedding matrices   estimated training
process  complete feature templates used system shown table   
then  feature compositions performed hidden layer via cube activation function 
h   g x     w   xw   xt   xr   xd   xv     b    
w  weight matrix input layer hidden layer  b  bias vector 
feature compositions important dependency parsing nlp general 
researchers used cost intensive manual feature engineering design large set feature
templates  however  approach cannot cover potentially useful features  lei  xin  zhang 
barzilay  jaakkola        showed full feature representation derived
kronecker product multiple views features  results tensor model  representing
tensor low rank form using c andecomp  parafac  cp  tensor decomposition  kolda  
bader         number parameters effectively reduced  thus suitable tasks
limited training data  cao   khudanpur        
   s   b  top head element stack buffer 

    

fir epresentation l earning c ross  l ingual ransfer parsing

softmax layer 
     



hidden layer 
          



transition actions


hidden representation


input layer 

               

feature extraction





words

clusters

lexical features
root

parsing configurations

stack
has verb

lookup tables
 


pos tags



 

relations

distance 
valency

non lexical features
good adj

buffer
control noun

   

nsubj
he pron

figure    neural network model dependency parsing  cluster features introduced
section         
type

feature templates
w
           
eswi   eb


word

w
w
w
w
elc  s
  erc  s
  elc  s
  erc  s
        
i 
i 
i 
i 
w
w
elc  lc  s
  erc  rc  s
        
  
  

           
est   eb


pos





elc  s
  erc  s
  elc  s
  erc  s
        
i 
i 
i 
i 


elc  lc  s
  erc  rc  s
        
  
  

relation

r
r
r
r
elc  s
  erc  s
  elc  s
  erc  s
        
i 
i 
i 
i 
r
r
elc  lc  s
  erc  rc  s
        
  
  

distance



es
  es
   s 
   b 

valency

eslv    eslv    esrv 

table    feature templates neural network model transition based dependency parsing 
 w c t r d lv rv 
ep
indicates various feature embeddings element position p  lc 
 rc   first child left  right  lc   rc   second child left  right  

indicates lexical features  indicates non lexical features 
suggest cube activation function g x    x  viewed special case
low rank tensor  verification  g x  expanded as 
g w  x          wm xm   b   
 wi wj wk  xi xj xk   b wi wj  xi xj      

i j k

i j

    

fig uo   c   yarowsky  wang   l iu

treat bias term b x  x       weight corresponding feature
combination xi xj xk wrote wi wj wk   exactly rank   component tensor low rank form using cp tensor decomposition  consequently  cube activation function
implicitly derives full feature combinations  fact  add many features possible
input layer improve parsing accuracy  show section     brown cluster
features readily incorporated model 
composed features propagated output layer  generating probabilistic distribution output labels  i e   transition actions  via softmax activation function   
sof tmax w  h   use following objective function train model 
j     

  n
 
crossent di   yi    
n i  
 

crossent p  q  cross entropy two distributions p q 
crossent p  q    pk ln qk
k

parameters trained using back propagation  model  typically consists
embedding matrices weights network  however  cases  may exclude
word embedding matrix e w   indicates word embeddings constrained fixed
 i e   without updating  training 
    cross lingual transfer
idea cross lingual transfer using parser examined straightforward  contrast
traditional approaches discard rich lexical features  delexicalizing  transferring
models one language another  model transferred using full model trained
source language side  i e   english  
since non lexical feature  pos  relation  distance  valency  embeddings directly transferable languages  key component framework cross lingual learning
lexical feature embeddings  i e   word embeddings   cross lingual word embeddings
induced  first learn dependency parser source language side  that  parser
directly used parsing target language data 
      u niversal ependencies
discussed previously  cross lingual model transfer assumes universal grammatical structures
identified multiple languages  therefore  evaluated test set target language
either unlabeled attachment score  uas  labeled attachment score  las   performance
transfer parsing rely heavily multilingual consistency annotation schemes  generally
syntactic annotation schemes differ head finding rules  e g   choice lexical versus functional head  dependency relation labels  i e   syntactic tagset   challenging task
construct multilingual treebanks consistent annotations  initial cross lingual parsing studies  conll shared task datasets  buchholz   marsi        broadly used  however 
inconsistencies occur head finding rules syntactic tagset across languages 
made difficult evaluate cross lingual parsers 
    

fir epresentation l earning c ross  l ingual ransfer parsing

order overcome difficulties  new collection multilingual treebanks homogeneous syntactic dependency annotation presented recently  namely universal dependency treebanks  udt   mcdonald et al          universal annotation scheme created
harmonizing available treebanks slightly different variants stanford typed dependencies  de marneffe et al          along universal part of speech tags  petrov  das    mcdonald         dataset greatly facilitates research multilingual syntactic analysis 
makes possible use las evaluation  fact  udt already used standard
dataset benchmarking research cross lingual transfer parsing  ma   xia        tiedemann 
      zhang   barzilay        duong  cohn  bird    cook      a      b  rasooli   collins 
       efforts towards universal dependencies include recent universal dependencies project  ud    hamledt  zeman et al          paper  conduct experiments
udt  v       dataset without losing generality 
      p rojective vs   n   projective parsing
non projectivity common phenomenon multilingual dependency parsing  term nonprojectivity indicates dependency tree crossing arcs  often appear morphologically rich languages  various algorithms proposed graph based transitionbased parsing algorithms produce non projective trees  example  arc standard algorithm
 section      readily extended adding swap action handle non projectivity 
gives expected linear worst case o n    complexity  nivre         strategies include
list based algorithm  nivre        adapted covington algorithm  covington         combination list based swap based algorithm  choi  
mccallum         unfortunately  systematically comparison different
algorithms literature far 
study  however  focus projective parsing non projective
trees source language  english  training data  consequently  non projectivities target languages handled moment  

   cross lingual word representation learning
prior introducing approaches cross lingual word representation learning  briefly review
basic model learning monolingual word embeddings  constitutes subprocedure
cross lingual approaches 
    continuous bag of words model
recent years  various approaches studied learning word embeddings largescale plain texts  approaches generally derived so called distributional hypothesis  firth         shall know word company keeps  study  consider
continuous bag of words  cbow  model  mikolov  chen  corrado    dean        imple   https   universaldependencies github io docs 
   https   github com ryanmcd uni dep tb
   note target languages address paper  non projectivity pervasive  specifically  proportion projective trees presented training corpus respectively     de      es     
fr 

    

fig uo   c   yarowsky  wang   l iu

mented open source toolkit word vec   basic principle cbow model predict
individual word sequence given bag context words within fixed window size
input  using log linear classifier  model avoids non linear transformation hidden
layers  hence trained high efficiency 
large window size  grouped words using resulting word embeddings topically similar  whereas small window size  grouped words syntactically similar  bansal  gimpel    livescu         set window size   parsing task 
next  introduce approach inducing bilingual word embeddings  general  expect
bilingual word embeddings preserve translational equivalences  example  cooking  english  close translation  kochen  german  embedding space 
    robust alignment based projection
first method inducing cross lingual word embeddings two stages  first  learn word
embeddings source language  s  corpora monolingual case  project
monolingual word embeddings target language  t   based word alignments 
given sentence aligned parallel corpus d  first conduct unsupervised bidirectional word
alignment  collect alignment dictionary  specifically  word aligned sentence pair
d  keep alignments conditional alignment probability exceeding threshold       
discard others  specifically  let     wit   wjs   ci j                 nt   j              ns  
alignment dictionary  ci j number times ith target word wit aligned
j th source word wjs   ns nt vocabulary sizes  use shorthand  i  j 
denote word pair   projection formalized weighted average
embeddings translation words 
ci j
v wit    
v wjs  
   

c
i 
 i j at
ci    j ci j   v w  embedding w 
obviously  simple projection method one drawback  assigns word embeddings
target language words occur word aligned data  typically smaller
monolingual datasets  therefore  order improve robustness projection  utilize
morphology inspired mechanism  propagate embeddings in vocabulary words out oft
vocabulary  oov  words  specifically  oov word woov
  extract list candidate
words similar terms edit distance  levenshtein distance   set averaged

vector embedding woov
  formally 

v woov
    avg  v w   
w c


c    ww editdist woov
  w   

   

reduce noise  choose small edit distance threshold     
process robust projection viewed two stage graph propagation algorithm 
illustrated figure    left panel   embeddings first propagated source language words
target language words appear bilingual lexicons  next  monolingual propagation
performed obtain oov word embeddings target language  using edit distance metric 
   http   code google com p word vec 

    

fir epresentation l earning c ross  l ingual ransfer parsing

 

source language



 
bilingual lexicon
 weighted 

target language

parallel data
wiktionary
panlex




 



 





 













 

 



cca
 



in vocabulary words
out of vocabulary words





figure    illustration robust projection  left  cca  right  inducing cross lingual word
embeddings 

    canonical correlation analysis
second approach consider similar faruqui dyer         uses cca
improve monolingual word embeddings multilingual correlation  cca way measuring
linear relationship multidimensional variables  two multidimensional variables 
cca aims find two projection matrices map original variables new basis  lowerdimensional   correlation two variables maximized 
refer readers work hardoon  szedmak  shawe taylor        theoretical
foundations algorithm specifics cca  lets treat cca black box  see cca
applied inducing bilingual word embeddings  suppose already two pre trained
monolingual word embeddings  e g   english german   rn  d  rn  d   
first step  extract one to one alignment dictionary alignment dictionary
ast    here    indicating every word translated one word   vice
versa 
process illustrated figure    right panel   denoting dimension resulting word
embeddings min d    d     first  derive two projection matrices v rd    w rd 
respectively using cca 
v  w   cca     

   

then  v w used project entire vocabulary  
  v 

  w

   

rn  rn  resulting word embeddings cross lingual task 
   worth trying  observed slight performance degradation experimental setting 

    

fig uo   c   yarowsky  wang   l iu

    pros cons
contrary robust projection approach  cca assigns embeddings every word monolingual vocabulary  however  one potential limitation cca assumes linear transformation
word embeddings  difficult satisfy  mean time  training source language
parser using cca cross lingual word embeddings  constrained e w fixed 
mentioned section      otherwise  translational equivalence broken  robust projection approach  however  doesnt limitation  discussion experiments
presented section       
note approaches generalized lower resource languages parallel bitexts
available  way  dictionary readily obtained either using bilingual lexicon
induction approaches  mann   yarowsky        koehn   knight        haghighi  liang  bergkirkpatrick    klein         online resources wiktionary  panlex   

   experiments
section describes experiments  first describe data settings used experiments  results 
    data settings
pre training word embeddings  use wmt      monolingual news corpora
english  german spanish    french  combined wmt      wmt      monolingual news corpora    got word alignment counts using fast align toolkit cdec  dyer
et al         parallel news commentary corpora  wmt          combined europarl corpus english german  spanish  french    
training neural network dependency parser  set number hidden units
     dimension embeddings different features shown table   

dim 

word
  

pos
  

label
  

distance
 

valency
 

cluster
 

table    dimensions various types feature embeddings 
mini batch adaptive stochastic gradient descent  adagrad   duchi  hazan    singer       
used optimization  cca approach  use implementation faruqui dyer
       
employ universal dependency treebanks  udt v     reliable evaluation
approach cross lingual dependency parsing  universal multilingual treebanks annotated
using universal pos tagset  petrov et al         contains    pos tags  well
universal dependencies defines    dependency relations  follow standard split
treebanks languages 
  
   
   
   
   

https   www wiktionary org 
http   panlex org 
http   www statmt org wmt   
http   www statmt org wmt   
http   www statmt org europarl 

    

fir epresentation l earning c ross  l ingual ransfer parsing

    baseline systems
compare approach following systems 
first baseline  evaluate delexicalized transfer neural network based parser
 d elex   use non lexical features  figure     investigate effect
non local features  distance  valency   delexicalized systems include
non local features referred  d elex  basic   
compare approach delexicalized parser presented mcdonald et al 
        m c d     used perceptron based transition based parser beam size   
along richer non local features  zhang   nivre         re implementation approach
framework zpar  zhang   clark        referred  m c d     
furthermore  consider strong baseline system proposed tackstrom et al         
utilized cross lingual word cluster features enhance perceptron based delexicalized
parser  m c d    cluster   use alignment dictionary described section    
induce cross lingual word clusters  re implement p rojected clustering approach described work tackstrom et al   assigns target word cluster
often aligned 
c wit     arg max ci j   c wjs     k 
k

 i j at

obviously  method drawback words occur alignment dictionary  oov  cannot assigned cluster  therefore  use strategy described
section     find likely clusters oov words  instead computing average
embeddings  solve argmax problem 

    arg max
c woov
k

  c w     k 

w c

   


  w   
c    weditdist woov

set   constantly  instead clustering model uszkoreit brants         use
brown clustering        induce hierarchical word clusters  word represented
bit string  use word cluster feature templates tackstrom et al          set
number brown clusters     
    experimental results
parsing models trained using development data english early stopping 
table   lists results cross lingual transfer experiments dependency parsing  table  
summarizes experimental gains detailed table   
first examine benefit brought non local distance valency features  observed
comparison elex  basic  elex  marginal improvements obtained de
fr  significant improvements es  therefore  adopted features
following experiments 
delexicalized system obtains slightly lower performance reported mcdonald
et al          m c d     used greedy decoding local training  re implementation
mcdonald et al s work attains comparable performance c d    languages consider study  using cross lingual word embeddings either alignment based projection
cca  obtain statistically significant improvements delexicalized system 
    

fig uo   c   yarowsky  wang   l iu

elex  basic 
elex
p roj
p roj cluster
cca
cca cluster

unlabeled attachment score  uas 
en
de
es
fr
avg
                             
                             
                             
                             
                             
                             

labeled attachment score  las 
en
de
es
fr
avg
                             
                             
                             
                             
                             
                             

c d  

     

     

     

     

     

     

     

     

     

     

c d  
c d    cluster

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

table    cross lingual transfer dependency parsing english test dataset   universal multilingual treebanks  results measured unlabeled attachment score  uas 
labeled attachment score  las   elex  basic  delexicalized model without nonlocal features  distance  valency   denotes re implementation c d    since
model varies different target languages cca based approach  indicates
averaged uas las 

experimental contribution
p roj
vs  elex
cca
vs  elex
p roj
vs  c d  
cca
vs  c d  
p roj cluster
vs  p roj
cca cluster
vs  cca
c d    cluster vs  c d  
p roj cluster
vs  elex
cca cluster
vs  elex
p roj cluster
vs  c d  
cca cluster
vs  c d  
p roj cluster
vs  c d    cluster
cca cluster
vs  c d    cluster

de es fr avg   relative 
            
            
            
            
            
            
            
             
            
            
            
            
            

table    summary experimental gains detailed table    absolute las gain
relative error reduction  gains statistically significant using malteval  nilsson
  nivre        p        

uas las  interestingly  notice p roj consistently outperforms cca significant
margin  comparable c d    cluster  analysis observation conducted section             
    

fir epresentation l earning c ross  l ingual ransfer parsing

type
cluster

feature templates
c
esc   eb
           

c
c
c
c
elc  si     erc  s
  elc  s
  erc  s
        
i 
i 
i 

c
c
elc  lc  s
  erc  rc  s
        
  
  

table    word cluster feature templates 

framework flexible incorporating richer features simply embedding
continuous vectors  thus embed cross lingual word cluster features model 
together proposed cross lingual word embeddings  cluster feature templates shown
table    similar pos tag feature templates  shown table    significant
additive improvements obtained p roj cca embedding cluster features 
compared delexicalized system  relative error reduced       uas 
      las  combined system outperforms c d    cluster significantly  
      e ffect robust p rojection
since p roj induction cross lingual word clusters  use edit distance measure
oov words  would see affects performance parsing 
intuitively  higher coverage projected words test dataset promote parsing
performance more  verify this  conduct experiments settings using
p roj cluster model  robust projection  examine effect edit distances ranging
     results shown table    improvements observed languages using
robust projection edit distance measure  especially fr  highest coverage gain
obtained robust projection  observe slightly improvements de es using
edit distance    performance starts degrade gets larger  reasonable  since
larger edit distance increases word coverage  introduces noise 

simple

de

es

fr

coverage
uas
las
coverage
uas
las
coverage
uas
las

     
     
     
     
     
     
     
     
     

  
     
     
     
     
     
     
     
     
     

robust
  
     
     
     
     
     
     
     
     
     

table    effect robust projection 

    

  
     
     
     
     
     
     
     
     
     

fig uo   c   yarowsky  wang   l iu

      e ffect f ine  t uning w ord e mbeddings
another reason effectiveness p roj cca lies fine tuning word embeddings
training parser 
cca viewed joint method inducing cross lingual word embeddings 
training source language dependency parser cross lingual word embeddings derived
cca  en word embeddings fixed  otherwise  translational equivalence
broken  however  p roj  limitation  word embeddings updated
non lexical feature embeddings  order obtain accurate dependency parser  refer
procedure fine tuning process word embeddings  verify benefits fine tuning 
conduct experiments see relative loss word embeddings fixed training  results
shown table    indicates fine tuning indeed offers considerable help 

de
es
fr

uas
las
uas
las
uas
las

fixed
     
     
     
     
     
     

fine tuning
     
     
     
     
     
     


     
     
     
     
     
     

table    effect fine tuning word embeddings 

    compare existing bilingual word embeddings
section  compare bilingual embeddings several previous approaches context dependency parsing  best knowledge  first work evaluation
bilingual word embeddings syntactic tasks 
approaches consider include multi task learning approach  klementiev et al        
 mtl   bilingual auto encoder approach  chandar p et al          b iae   bilingual compositional vector model  hermann   blunsom         b icvm   bilingual bag of words
approach  gouws et al          b ilbowa  
mtl b iae  adopt released word embeddings directly due inefficiency
training    b icvm b ilbowa  re run systems dataset previous
experiments    results summarized table   
cca p roj consistently outperforms approaches languages  p roj performs best  inferior performance mtl b iae partly due low word coverage 
example  cover     words universal de test treebank  whereas cca
p roj covers      moreover  b iae  b icvm b ilbowa introduce sentence level translational equivalence objectives regularizers learning bilingual word embeddings 
approaches advantageous dont assume require word alignment  however  word toword translational equivalence cannot well preserved way 
    mtl embeddings normalized training 
    b icvm uses bilingual parallel dataset 

    

fir epresentation l earning c ross  l ingual ransfer parsing

mtl  klementiev et al        
b iae  chandar p et al        
b icvm  hermann   blunsom       
b ilbowa  gouws et al        
cca
p roj

de
uas
las
           
           
           
           
           
           

es
uas
     
     
     
     
     
     

las
     
     
     
     
     
     

fr
uas
las
           
           
           
           
           
           

table    comparison existing bilingual word embeddings  mtl b iae  use
released bilingual word embeddings 

target word  es 

china
 china 

problemas
 problems 

septiembre
 september 

p roj
india
russia
taiwan
chinese
problem
difficulties
troubles
issues
october
august
january
december

cca
russia
indonesia
beijing
chinese
problems
woes
troubles
dilemmas
december
july
october
june

neighboring words  en 
mtl
b iae
china
korea
independent india
sumitomo
chinese
malaysian
brazil
events
problem
sanctions
greatly
conditions
highlighted
laws
scale
december
month
february
april
july
scheduled
march
november

b icvm
chinese
chinois
sino
     
problematic
problematical
difficulties
troubles
  th
     
  
eleventh

b ilbowa
helsinki
bulgarians
constituting
market
deficiencies
situations
omissions
attentively
a m
p m
twelve
         

table    target words spanish   similar words english  induced various
approaches 

verify assumption  taking en es case study  manually inspect  
similar words  by cosine similarity  english given set words spanish  table    
observe semantic syntactic shifting k nearest neighbors prediction b iae 
b icvm b ilbowa  whereas p roj cca give translational equivalent predictions 
example  b icvm yields adjective problematical target noun problemas  b ilbowa yields
semantic related word market china  general  p roj robust approach  behaving
consistently well sampled words 
worth noting dont assume require bilingual parallel data cca p roj 
need practice bilingual lexicon paired languages  especially important
generalizing approaches lower resource languages  parallel texts available 
    

fig uo   c   yarowsky  wang   l iu

   target language adaptation minimal supervision
important us distinguish linguistic structures learned via cross lingual transfer
versus learned basis monolingual information language
parsed  intuitively  cross lingual approaches learn common dependency structures
shared source target language  however  many languages 
specialized  language specific  syntactic characteristics learned data
target language 
take adjective noun order example  spanish french  adjectives often appears
nouns  thus forming right directed arc labeled amod  whereas english  amod
 adjectival modifier  arcs mostly left directed  illustrated figure    another example
subject verb object order  german  verbs often appear end sentence v  position 
causes much left directed dobj  direct object  arcs english  figure    
differences clearly observed universal treebanks  table    shows significant
distribution divergence left directed right directed arcs dobj amod relations
treebanks different languages 
relation  dobj  language  en vs  de
dobj
dobj
ratio
en
      
   
        
de
     
     
       
relation  amod  language  en vs  es  fr
amod amod
ratio
en
     
      
        
es
      
     
       
fr
      
     
       

table     distribution divergences left directed right directed arcs dobj relation en
de  top   amod relation en es fr  bottom  

amod

amod

noun

adj

noun

adj

spanish 

consejo

superior

conflictos

sociales

adj

noun

adj

noun

english 

superior

council

social

conflicts

amod

amod

figure    reverse direction amod relation spanish english  french adjectives following nouns 

    

fir epresentation l earning c ross  l ingual ransfer parsing

root
advmod

det

dobj

adv

det

noun

verb

de 

endlich

den

richtigen

gefunden

en 

finally

found



right man

adv

verb

det

noun

advmod

det
dobj
root

figure    reverse direction dobj relation german english 
therefore  section  investigate much cross lingual transfer model improved annotating small amount labeled training data target language side  even though
building large scale treebanks low resource languages supervised learning costly  annotating dependency structures small amount sentences  e g        difficult 
still conduct experiments universal dependency treebanks  provide labeled
training data multiple languages  language studied  de  es  fr   incrementally
augment amount labeled sentences           step      adapt parameters cross lingual transfer model specific target language  theoretically  since target
language treebanks contain non projective trees  would make sense apply non projective
algorithms  e g   swap based  target language adaptation  way  however  w 
re trained scratch  doesnt show good performance experiments since minimally supervised data small  consequently  still rely arc standard algorithm
adaption  process almost training source language parser described
section    except word embedding matrix e w fixed  rest parameters
 e  t l d v c    w    w    b    optimized using augmented labeled data target language 
taking equation     objective function  development data used process  thus
simply perform parameter updating       iterations 
addition  built another strong baseline system employs augmented labeled
training data supervised learning  system  utilize word embeddings brown
clusters features  derived separately language 
shown figure    results really promising  p roj cluster cca cluster
systems consistently outperform delexicalized system supervised system significant margin  p roj cluster cca cluster general achieve comparable performances 
cca cluster slightly better 
worthy noting performances p roj cluster cca cluster boosted
augmenting     sentences  take de example  uas increased               
las                nearly equal effect using       sentences
supervised learning  observation demonstrates great potential cross lingual transfer
system practical usage 
    



  

  

  

  

g uo   c   yarowsky  wang   l iu










  






















uas







  

uas

  

uas






  







  

  




  







  

proj cluster
cca cluster
delexicalized
supervised

 

   

   

   

   

    

 

   






   

   

   

    

   

   

   

    












  

  























  







  

  



las

  



  

las



  

  

las

   

  





proj cluster
cca cluster
delexicalized
supervised

labeled training data  fr 



  

 

labeled training data  es 

  

  

labeled training data  de 





  



  

proj cluster
cca cluster
delexicalized
supervised

  

  



  

  






 

   

   

   

   

labeled training data  de 

    



  

proj cluster
cca cluster
delexicalized
supervised

proj cluster
cca cluster
delexicalized
supervised

  

  



  

proj cluster
cca cluster
delexicalized
supervised

  

  



 

   

   

   

   

labeled training data  es 

    

 

   

   

   

   

    

labeled training data  fr 

figure    target language adaptation incrementally augmenting labeled training data  sentences  fine tune cross lingual transfer model  performances evaluated using
uas  top  las  bottom   note points whose x coordinates   represent
cross lingual transfer performance  labeled training data used 

analysis  primary hypothesis incorporating data target language  model
able learn special syntactic patterns consistent source language 
verify this  study influence target language adaptation two special relations 
dobj  de  amod  es  fr   measuring precision recall changes use
    target language sentences  results shown respectively table    table    
observe great improvements recall relations  indicates model indeed gains
ability learning target language specific dependency structures supervision
    sentences 

   related studies
cross lingual annotation projection method pioneered yarowsky  ngai  wicentowski        shallow nlp tasks  pos tagging  ner  etc    later applied dependency
parsing  hwa et al         smith   eisner        zhao et al         jiang et al         tiedemann 
       work along line dedicated improving robustness syntactic pro    

fir epresentation l earning c ross  l ingual ransfer parsing

relation  dobj  language  de
precision recall
proj cluster
     
     
    
     
     

          
cca cluster
     
     
    
     
     

          

table     effect minimal supervision      sentences  dobj 
relation  amod  language  es  fr
es
fr
precision recall precision recall
proj cluster
     
     
     
     
    
     
     
     
     

          
          
cca cluster
     
     
     
     
    
     
     
     
     

          
          

table     effect minimal supervision      sentences  amod 

jection alleviating noise errors introduced word alignment based projection  typical
approaches include soft projection  li  zhang    chen         treebank translation  tiedemann  agic    nivre         distribution transfer  ma   xia         recently proposed
density driven projection  rasooli   collins         worth mentioning remarkable results
achieved annotation projection methods  tiedemann        rasooli   collins 
       due large part parsers trained target language side 
cross lingual model transfer  learning cross lingual feature representations promising direction  typical approaches include cross lingual word clustering  tackstrom et al        
employed paper baseline system  projection features  durrett  pauls    klein         kozhevnikov titov        derived linear projection maps target instances
source side feature representations  extent similar cca approach  xiao
guo        learned cross lingual word embeddings applied mstparser linguistic
transfer  inspired work  sgaard et al         obtained multi source unified word embeddings via inverted indexing wikipedia  applied various nlp tasks  however 
results didnt show significant improvements parsing  nevertheless  idea utilizing multisource information learning cross lingual word embeddings makes great sense  recently 
duong et al       a      b  utilized neural network architecture parameter sharing
parsers different languages  however  approach requires annotated treebanks
target language side  makes distinct transfer parsing framework  addition
representation learning  attempts made integrate monolingual linguistic features parsing models  manually constructed universal dependency parsing rules  naseem 
    

fig uo   c   yarowsky  wang   l iu

chen  barzilay    johnson        manually specified typological features  naseem  barzilay 
  globerson        zhang   barzilay        
using neural networks dependency parsing new approach  best knowledge  mayberry miikkulainen        presented first work explored neural networks
shift reduce constituent based parsing  used one hot feature representations  henderson
       used simple synchrony network predict parse decisions constituency parser 
first use neural networks broad coverage penn treebank parser  titov henderson        applied incremental sigmoid belief networks constituent based parsing  garg
henderson        later extended work transition based dependency parsing using temporal restricted boltzman machine  parsers  however  much less scalable practice 
earlier progress made using deep learning parsing includes work collobert       
socher et al         constituent based parsing  stenetorp        built recursive neural
networks transition based dependency parsing 

   conclusion
paper proposes novel framework based distributed representations cross lingual dependency parsing  two algorithms proposed induction cross lingual word representations 
namely robust projection cca  bridge lexical feature gap 
experiments show using cross lingual word embeddings derived either approach 
transferred parsing performance improved significantly delexicalized system 
notable observation projection method performs significantly better cca  additionally  framework flexibly able incorporate cross lingual word cluster features 
significant gains use  combined system significantly outperforms delexicalized systems languages  average       error reduction las 
significantly outperforms models mcdonald et al         augmented projected word
cluster features 
furthermore  show performance cross lingual transfer system specific target language boosted minimal supervision language  great
significance practical usage 

acknowledgments
grateful manaal faruqui providing bilingual resources  thank ryan mcdonald
pointing evaluation issue experiment  thank sharon busching
proofreading anonymous reviewers insightful comments suggestions  work
supported national key basic research program china via grant     cb      
national natural science foundation china  nsfc  via grant                   
corresponding author  wanxiang che  e mail  car ir hit edu cn 

references
bansal  m   gimpel  k     livescu  k          tailoring continuous word representations dependency parsing  proceedings   nd annual meeting association computa    

fir epresentation l earning c ross  l ingual ransfer parsing

tional linguistics  volume    short papers   pp          baltimore  maryland  association
computational linguistics 
brown  p  f   desouza  p  v   mercer  r  l   pietra  v  j  d     lai  j  c          class based n gram
models natural language  computational linguistics                
buchholz  s     marsi  e          conll x shared task multilingual dependency parsing 
proceedings tenth conference computational natural language learning  conllx   pp          new york city  association computational linguistics 
cao  y     khudanpur  s          online learning tensor space  proceedings   nd
annual meeting association computational linguistics  volume    long papers  
pp          baltimore  maryland  association computational linguistics 
carreras  x          experiments higher order projective dependency parser  proceedings
conll shared task session emnlp conll       pp          prague  czech
republic  association computational linguistics 
chandar p  s   lauly  s   larochelle  h   khapra  m   ravindran  b   raykar  v  c     saha  a 
        autoencoder approach learning bilingual word representations  advances
neural information processing systems     pp            curran associates  inc 
chen  d     manning  c          fast accurate dependency parser using neural networks 
proceedings      conference empirical methods natural language processing
 emnlp   pp          doha  qatar  association computational linguistics 
choi  j  d     mccallum  a          transition based dependency parsing selectional branching  proceedings   st annual meeting association computational linguistics  volume    long papers   pp            sofia  bulgaria  association computational
linguistics 
collobert  r          deep learning efficient discriminative parsing  proceedings   th
international conference artificial intelligence statistics  aistats   pp         
fort lauderdale  fl  usa  jmlr org 
collobert  r     weston  j          unified architecture natural language processing  deep
neural networks multitask learning  proceedings   th international conference
machine learning  icml     pp          helsinki  finland  acm 
collobert  r   weston  j   bottou  l   karlen  m   kavukcuoglu  k     kuksa  p          natural
language processing  almost  scratch  journal machine learning research          
     
covington  m  a          fundamental algorithm dependency parsing  proceedings
  th annual acm southeast conference  pp        
de marneffe  m  c   maccartney  b   manning  c  d   et al          generating typed dependency
parses phrase structure parses  proceedings fifth international conference
language resources evaluation  lrec     pp          genoa  italy  european
language resources association  elra  
de marneffe  m  c     manning  c  d          stanford typed dependencies representation 
coling       proceedings workshop cross framework cross domain parser
evaluation  pp      manchester  uk  association computational linguistics 
    

fig uo   c   yarowsky  wang   l iu

duchi  j   hazan  e     singer  y          adaptive subgradient methods online learning
stochastic optimization  journal machine learning research               
duong  l   cohn  t   bird  s     cook  p       a   low resource dependency parsing  cross lingual
parameter sharing neural network parser  proceedings   rd annual meeting
association computational linguistics  th international joint conference
natural language processing  volume    short papers   pp          beijing  china 
association computational linguistics 
duong  l   cohn  t   bird  s     cook  p       b   neural network model low resource universal dependency parsing  proceedings      conference empirical methods
natural language processing  pp          lisbon  portugal  association computational
linguistics 
durrett  g   pauls  a     klein  d          syntactic transfer using bilingual lexicon  proceedings      joint conference empirical methods natural language processing
computational natural language learning  pp       jeju island  korea  association
computational linguistics 
dyer  c   ballesteros  m   ling  w   matthews  a     smith  n  a          transition based dependency parsing stack long short term memory  proceedings   rd annual meeting
association computational linguistics  th international joint conference
natural language processing  volume    long papers   pp          beijing  china  association computational linguistics 
dyer  c   lopez  a   ganitkevitch  j   weese  j   ture  f   blunsom  p   setiawan  h   eidelman  v  
  resnik  p          cdec  decoder  alignment  learning framework finite state
context free translation models  proceedings acl      system demonstrations  pp 
     uppsala  sweden  association computational linguistics 
eisner  j  m          three new probabilistic models dependency parsing  exploration 
proceedings   th conference computational linguistics volume    pp         
copenhagen  denmark  association computational linguistics 
faruqui  m     dyer  c          improving vector space word representations using multilingual
correlation  proceedings   th conference european chapter association computational linguistics  pp          gothenburg  sweden  association
computational linguistics 
firth  j  r          synopsis linguistic theory           studies linguistic analysis  pp 
     blackwell 
garg  n     henderson  j          temporal restricted boltzmann machines dependency parsing 
proceedings   th annual meeting association computational linguistics 
human language technologies  pp        portland  oregon  usa  association computational linguistics 
gouws  s   bengio  y     corrado  g          bilbowa  fast bilingual distributed representations
without word alignments  proceedings   nd international conference machine
learning  icml   pp          lille  france 
guo  j   che  w   wang  h     liu  t          revisiting embedding features simple semisupervised learning  proceedings      conference empirical methods natural
    

fir epresentation l earning c ross  l ingual ransfer parsing

language processing  emnlp   pp          doha  qatar  association computational
linguistics 
guo  j   che  w   yarowsky  d   wang  h     liu  t          cross lingual dependency parsing
based distributed representations  proceedings   rd annual meeting association computational linguistics  th international joint conference natural
language processing  volume    long papers   pp            beijing  china  association
computational linguistics 
haghighi  a   liang  p   berg kirkpatrick  t     klein  d          learning bilingual lexicons
monolingual corpora  proceedings acl     hlt  pp          columbus  ohio 
association computational linguistics 
hardoon  d  r   szedmak  s     shawe taylor  j          canonical correlation analysis 
overview application learning methods  neural computation                   
henderson  j          discriminative training neural network statistical parser  proceedings   nd meeting association computational linguistics  acl     main
volume  pp         barcelona  spain 
hermann  k  m     blunsom  p          multilingual models compositional distributed semantics  proceedings   nd annual meeting association computational
linguistics  volume    long papers   pp        baltimore  maryland  association computational linguistics 
hwa  r   resnik  p   weinberg  a   cabezas  c     kolak  o          bootstrapping parsers via
syntactic projection across parallel texts  natural language engineering                 
jiang  w   liu  q     lv  y          relaxed cross lingual projection constituent syntax 
proceedings      conference empirical methods natural language processing 
pp            edinburgh  scotland  uk  association computational linguistics 
kim  y          convolutional neural networks sentence classification  proceedings
     conference empirical methods natural language processing  emnlp   pp            doha  qatar  association computational linguistics 
klein  d     manning  c          corpus based induction syntactic structure  models dependency constituency  proceedings   nd meeting association computational linguistics  acl     main volume  pp          barcelona  spain 
klementiev  a   titov  i     bhattarai  b          inducing crosslingual distributed representations
words  proceedings coling       pp            mumbai  india  coling
     organizing committee 
koehn  p     knight  k          learning translation lexicon monolingual corpora  proceedings acl    workshop unsupervised lexical acquisition  pp       philadelphia  pennsylvania  usa  association computational linguistics 
kolda  t  g     bader  b  w          tensor decompositions applications  siam review        
       
koo  t     collins  m          efficient third order dependency parsers  proceedings
  th annual meeting association computational linguistics  pp       uppsala 
sweden  association computational linguistics 
    

fig uo   c   yarowsky  wang   l iu

kozhevnikov  m     titov  i          cross lingual model transfer using feature representation
projection  proceedings   nd annual meeting association computational
linguistics  volume    short papers   pp          baltimore  maryland  association
computational linguistics 
lei  t   xin  y   zhang  y   barzilay  r     jaakkola  t          low rank tensors scoring
dependency structures  proceedings   nd annual meeting association
computational linguistics  volume    long papers   pp            baltimore  maryland 
association computational linguistics 
li  z   zhang  m     chen  w          soft cross lingual syntax projection dependency parsing  proceedings coling         th international conference computational
linguistics  technical papers  pp          dublin  ireland  dublin city university association computational linguistics 
ma  x     xia  f          unsupervised dependency parsing transferring distribution via
parallel guidance entropy regularization  proceedings   nd annual meeting
association computational linguistics  volume    long papers   pp           
baltimore  maryland  association computational linguistics 
mann  g  s     yarowsky  d          multipath translation lexicon induction via bridge languages 
proceedings second meeting north american chapter association
computational linguistics language technologies  naacl     pp      pittsburgh 
pennsylvania  association computational linguistics 
marcus  m  p   marcinkiewicz  m  a     santorini  b          building large annotated corpus
english  penn treebank  computational linguistics                
mayberry  m  r     miikkulainen  r          sardsrn  neural network shift reduce parser 
proceedings sixteenth international joint conference artificial intelligence  pp 
        morgan kaufmann publishers inc 
mcdonald  r   crammer  k     pereira  f          online large margin training dependency
parsers  proceedings   rd annual meeting association computational
linguistics  acl     pp        ann arbor  michigan  association computational linguistics 
mcdonald  r     nivre  j          characterizing errors data driven dependency parsing
models  proceedings      joint conference empirical methods natural
language processing computational natural language learning  emnlp conll   pp 
        prague  czech republic  association computational linguistics 
mcdonald  r   nivre  j   quirmbach brundage  y   goldberg  y   das  d   ganchev  k   hall  k  
petrov  s   zhang  h   tackstrom  o   bedini  c   bertomeu castello  n     lee  j         
universal dependency annotation multilingual parsing  proceedings   st annual
meeting association computational linguistics  volume    short papers   pp    
    sofia  bulgaria  association computational linguistics 
mcdonald  r   petrov  s     hall  k          multi source transfer delexicalized dependency
parsers  proceedings      conference empirical methods natural language
processing  pp        edinburgh  scotland  uk  association computational linguistics 
    

fir epresentation l earning c ross  l ingual ransfer parsing

mcdonald  r  t     pereira  f  c          online learning approximate dependency parsing
algorithms  proceedings   st conference european chapter association computational linguistics  pp        trento  italy  association computer
linguistics 
mikolov  t   chen  k   corrado  g     dean  j          efficient estimation word representations
vector space  international conference learning representations  iclr  workshop 
naseem  t   barzilay  r     globerson  a          selective sharing multilingual dependency
parsing  proceedings   th annual meeting association computational
linguistics  volume    long papers   pp          jeju island  korea  association computational linguistics 
naseem  t   chen  h   barzilay  r     johnson  m          using universal linguistic knowledge
guide grammar induction  proceedings      conference empirical methods
natural language processing  pp            cambridge  ma  association computational linguistics 
nilsson  j     nivre  j          malteval  evaluation visualization tool dependency
parsing   proceedings sixth international language resources evaluation
 lrec     pp          marrakech  morocco  european language resources association
 elra  
nivre  j          efficient algorithm projective dependency parsing  proceedings
 th international workshop parsing technologies  iwpt   pp          nancy  france 
association computational linguistics 
nivre  j          incrementality deterministic dependency parsing  proceedings workshop incremental parsing  bringing engineering cognition together  pp       
barcelona  spain  association computational linguistics 
nivre  j          algorithms deterministic incremental dependency parsing  computational
linguistics                
nivre  j          non projective dependency parsing expected linear time  proceedings
joint conference   th annual meeting acl  th international joint
conference natural language processing afnlp  pp          suntec  singapore 
association computational linguistics 
nivre  j   hall  j     nilsson  j          memory based dependency parsing  hlt naacl     
workshop  eighth conference computational natural language learning  conll       
pp        boston  massachusetts  usa  association computational linguistics 
petrov  s   das  d     mcdonald  r          universal part of speech tagset  proceedings
eighth international conference language resources evaluation  lrec       
pp            istanbul  turkey  european language resources association  elra  
rasooli  m  s     collins  m          density driven cross lingual transfer dependency parsers 
proceedings      conference empirical methods natural language processing  pp          lisbon  portugal  association computational linguistics 
smith  d  a     eisner  j          parser adaptation projection quasi synchronous grammar
features  proceedings      conference empirical methods natural language
processing  pp          singapore  association computational linguistics 
    

fig uo   c   yarowsky  wang   l iu

socher  r   bauer  j   manning  c  d     andrew y   n          parsing compositional vector
grammars  proceedings   st annual meeting association computational
linguistics  volume    long papers   pp          sofia  bulgaria  association computational linguistics 
sgaard  a   agic  v   martnez alonso  h   plank  b   bohnet  b     johannsen  a          inverted
indexing cross lingual nlp  proceedings   rd annual meeting association
computational linguistics  th international joint conference natural language processing  volume    long papers   pp            beijing  china  association
computational linguistics 
srivastava  n     salakhutdinov  r  r          multimodal learning deep boltzmann machines  advances neural information processing systems     pp            curran
associates  inc 
stenetorp  p          transition based dependency parsing using recursive neural networks  deep
learning workshop nips  lake tahoe  nevada  usa 
sutskever  i   vinyals  o     le  q  v          sequence sequence learning neural networks  advances neural information processing systems     pp            curran
associates  inc 
tackstrom  o   mcdonald  r     uszkoreit  j          cross lingual word clusters direct transfer
linguistic structure  proceedings      conference north american chapter
association computational linguistics  human language technologies  pp     
     montreal  canada  association computational linguistics 
tiedemann  j          rediscovering annotation projection cross lingual parser induction 
proceedings coling         th international conference computational linguistics  technical papers  pp            dublin  ireland  dublin city university association computational linguistics 
tiedemann  j          cross lingual dependency parsing universal dependencies predicted
pos labels          
tiedemann  j   agic  v     nivre  j          treebank translation cross lingual parser induction  
       
titov  i     henderson  j          fast robust multilingual dependency parsing generative
latent variable model  proceedings conll shared task session emnlp conll
      pp          prague  czech republic  association computational linguistics 
turian  j   ratinov  l  a     bengio  y          word representations  simple general method
semi supervised learning  proceedings   th annual meeting association
computational linguistics  pp          uppsala  sweden  association computational linguistics 
uszkoreit  j     brants  t          distributed word clustering large scale class based language
modeling machine translation  proceedings acl     hlt  pp          columbus 
ohio  association computational linguistics 
wang  m     manning  c  d          effect non linear deep architecture sequence labeling 
proceedings sixth international joint conference natural language processing 
pp            nagoya  japan  asian federation natural language processing 
    

fir epresentation l earning c ross  l ingual ransfer parsing

weiss  d   alberti  c   collins  m     petrov  s          structured training neural network
transition based parsing  proceedings   rd annual meeting association
computational linguistics  th international joint conference natural language
processing  volume    long papers   pp          beijing  china  association computational linguistics 
xiao  m     guo  y          distributed word representation learning cross lingual dependency
parsing  proceedings eighteenth conference computational natural language
learning  pp          ann arbor  michigan  association computational linguistics 
xue  n   xia  f   chiou  f  d     palmer  m          penn chinese treebank  phrase structure
annotation large corpus  natural language engineering                 
yamada  h     matsumoto  y          statistical dependency analysis support vector machines 
proceedings  th international workshop parsing technologies  iwpt   pp     
     nancy  france  association computational linguistics 
yarowsky  d   ngai  g     wicentowski  r          inducing multilingual text analysis tools via
robust projection across aligned corpora  proceedings first international conference
human language technology research  pp      san diego  ca  usa  association
computational linguistics 
zeman  d   dusek  o   marecek  d   popel  m   ramasamy  l   stepanek  j   zabokrtsky  z    
hajic  j          hamledt  harmonized multi language dependency treebank  language
resources evaluation                
zhang  y     barzilay  r          hierarchical low rank tensors multilingual transfer parsing 
proceedings      conference empirical methods natural language processing 
pp            lisbon  portugal  association computational linguistics 
zhang  y     clark  s          syntactic processing using generalized perceptron beam
search  computational linguistics                
zhang  y     nivre  j          transition based dependency parsing rich non local features 
proceedings   th annual meeting association computational linguistics  human language technologies  pp          portland  oregon  usa  association
computational linguistics 
zhao  h   song  y   kit  c     zhou  g          cross language dependency parsing using bilingual
lexicon  proceedings joint conference   th annual meeting acl
 th international joint conference natural language processing afnlp  pp 
      suntec  singapore  association computational linguistics 
zhou  h   zhang  y   huang  s     chen  j          neural probabilistic structured prediction
model transition based dependency parsing  proceedings   rd annual meeting
association computational linguistics  th international joint conference
natural language processing  volume    long papers   pp            beijing  china 
association computational linguistics 

    


