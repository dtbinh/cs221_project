journal of artificial intelligence research                   

submitted        published      

a distributed representation based framework for
cross lingual transfer parsing
jiang guo
wanxiang che

jguo   ir   hit  edu   cn
car   ir   hit  edu   cn

research center for social computing and information retrieval
harbin institute of technology
harbin  heilongjiang  china

david yarowsky

yarowsky   jhu   edu

center for language and speech processing
johns hopkins university
baltimore  md  usa

haifeng wang

wanghaifeng   baidu   com

baidu inc   beijing  china

ting liu

tliu   ir   hit  edu   cn

research center for social computing and information retrieval
harbin institute of technology
harbin  heilongjiang  china

abstract
this paper investigates the problem of cross lingual transfer parsing  aiming at inducing dependency parsers for low resource languages while using only training data from a resource rich
language  e g   english   existing model transfer approaches typically dont include lexical features  which are not transferable across languages  in this paper  we bridge the lexical feature gap
by using distributed feature representations and their composition  we provide two algorithms for
inducing cross lingual distributed representations of words  which map vocabularies from two different languages into a common vector space  consequently  both lexical features and non lexical
features can be used in our model for cross lingual transfer  furthermore  our framework is flexible
enough to incorporate additional useful features such as cross lingual word clusters  our combined
contributions achieve an average relative error reduction of       in labeled attachment score as
compared with the delexicalized parser  trained on english universal treebank and transferred to
three other languages  it also significantly outperforms state of the art delexicalized models augmented with projected cluster features on identical data  finally  we demonstrate that our models
can be further boosted with minimal supervision  e g       annotated sentences  from target languages  which is of great significance for practical usage 

   introduction
dependency parsing has been one of the long standing central problems in natural language processing  nlp   the goal of dependency parsing is to induce implicit tree structures for natural language
sentence following the dependency grammar  which can be highly beneficial for various downstream
tasks  such as question answering  machine translation and knowledge mining representation  the
majority of work on dependency parsing has been dedicated to resource rich languages  such as english and chinese  for these languages  there exists large scale annotated treebanks that can be used
     ai access foundation  all rights reserved 

fig uo   c he   yarowsky  wang   l iu

for supervised training of dependency parsers  such as the penn treebank  marcus  marcinkiewicz 
  santorini        xue  xia  chiou    palmer         however  for most of the languages in the
world  there are very few or even no labeled training data for parsing  and it is both labor intensive
and time consuming to manually annotate treebanks for all languages  this fact has given rise to
a range of research on unsupervised methods  klein   manning         and transfer methods  hwa  resnik  weinberg  cabezas    kolak        mcdonald  petrov    hall        for linguistic
structure prediction 
considering that the unsupervised methods fall far behind the transfer methods in terms of
accuracy  as well as the difficulty in evaluation  we will focus on the transfer methods in this study 
we attempt to build parsers for low resource languages by exploiting treebanks from resource rich
languages  there are two approaches to linguistic transfer in general  namely data transfer and
model transfer  data transfer methods emphasizes the creation of artificial training data that can
be used for supervised training on the target language side  they have the appealing property
that they can learn language specific linguistic structures effectively  the major drawbacks are
the requirement of parallel data and the noise in the automatically created training data introduced
by word alignment based projection  on the other hand  model transfer methods build models on
the source language side  which are used directly for parsing target languages without the need of
creating annotated data in target languages 
this paper falls into the latter category  the major obstacle in transferring a parsing system
from one language to another is the lexical features  e g   words  that are not directly transferable
across languages  to address this challenge  mcdonald et al         built a delexicalized parser  a parser that only has non lexical features  a delexicalized parser makes sense in that pos
tag features are significantly predictive for unlabeled dependency parsing  however  for labeled
dependency parsing  especially for semantic oriented dependencies like stanford typed dependencies  de marneffe et al         de marneffe   manning         these non lexical features are not
predictive enough  tackstrom  mcdonald  and uszkoreit        proposed to learn cross lingual
word clusters from multilingual paralleled unlabeled data through word alignments  and apply these
clusters as features for semi supervised delexicalized parsing  word clusters can be thought of as a
kind of coarse grained representations of words  thus  this approach partially fills the gap of lexical
features in cross lingual learning of dependency parsing 
this paper proposes a novel approach for cross lingual dependency parsing that is based on
pure distributed feature representations  in contrast to the discrete feature representations used in
traditional dependency parsers  distributed representations map symbolic features into a continuous
representation space  that can be shared across languages  therefore  our model has the ability
to utilize both lexical and non lexical features naturally  specifically  our framework contains two
primary components 
 a neural network based dependency parser  we expect a non linear model for dependency
parsing in our study  because distributed feature representations are shown to be more effective in non linear architectures than in linear architectures  wang   manning         chen
and manning        proposed a transition based dependency parser using a neural network
architecture  which is simple but works well on benchmark datasets  briefly  this model simply replaces the predictor in transition based dependency parser with a well designed neural
network classifier  we will provide explanations for the merits of this model in section    as
well as how we adapt it to the cross lingual task 
   

fir epresentation l earning for c ross  l ingual t ransfer parsing

 cross lingual word representation learning  the key to filling the lexical feature gap is to
project the representations of these features from different languages into a common vector
space  preserving the translational equivalence  we will study and compare two approaches
of learning cross lingual word representations in section    the first approach is named
robust projection  and the second approach is based on canonical correlation analysis  both
approaches are simple to implement and are scalable to large data 
another drawback of the model transfer methods is that they focus only on the universal structures across various languages  and thus lack the ability of recovering the target language specific
structures  therefore  it is necessary to conduct target language adaptation on the top of the transferred models  we introduce a practical and straightforward solution by incorporating minimal
supervision from target languages  section    
we evaluate our models on the universal multilingual treebanks v     mcdonald et al         
case studies include transferring from english  en  to german  de   spanish  es  and french
 fr   experiments show that by incorporating lexical features  the performance of cross lingual
dependency parsing can be improved significantly  by further embedding cross lingual cluster features  tackstrom et al          we achieve an average relative error reduction of       in labeled
attachment score  las   as compared with the delexicalized parsers  it also significantly outperforms the delexicalized models of mcdonald et al  augmented with cluster features on identical
data  in addition  we show that by using a small amount of labeled training data  e g       sentences  at the target language side for parameter adaptation  minimal supervision   the performance
of our cross lingual transfer system can be boosted  and the recalls of language specific dependency
structures are improved dramatically  
the original major contributions of this paper include 
 we propose a novel and flexible cross lingual learning framework for dependency parsing
based on distributed representations  which can effectively incorporate both lexical and nonlexical features 
 we present two novel and effective approaches for inducing cross lingual word representation
that bridge the lexical feature gap in cross lingual dependency parsing transfer 
 we show that cross lingual word cluster features can be effectively embedded into our model 
leading to significant additive improvements 
 we show that the our cross lingual transfer systems can be easily and effectively adapted to
target languages with minimal supervision  demonstrating great potential in practical usage 

   background
this section describes the necessary background which is crucial for understanding our transfer
parsing framework 
   this article is a thoroughly revised and extended version of the work of guo  che  yarowsky  wang  and liu        
we provide a more detailed linguistic and methodological background of cross lingual parsing  additional extensions
primarily include experiments and analysis of target language adaptation with minimal supervision  our system is
made publicly available at  https   github com jiangfeng     acl   clnndep 

   

fig uo   c he   yarowsky  wang   l iu

punct
root

dobj
nsubj

root

he
pron

amod

has
verb

good
adj

control
noun

 
 

figure    an example labeled dependency tree 

    dependency parsing
given an input sentence x   w  w     wn where wi is the ith word of x  the goal of dependency
parsing is to build a dependency tree  which can be denoted by d     h  m  l      h  n      m 
n  l  l   where  h  m  l  indicates a directed arc from the head word wh to its modifier wm with a
dependency label l  and l is the label set  figure    
the mainstream models that have been proposed for dependency parsing can be described as
either graph based models or transition based models  mcdonald   nivre         graph based
models  eisner        mcdonald  crammer    pereira        view the parsing problem as finding
the highest scoring tree from a directed graph  the score of a dependency tree is typically factored
into scores of some small independent structures  the way of factorization defines the order of
a model and also the complexity in the inference process  mcdonald   pereira        carreras 
      koo   collins         for instance  first order models are factored into dependency arcs 
thus also known as arc factored models  higher order models would consider more expressive
substructures such as sibling and grandchild structures  transition based models instead aim to
predict a transition sequence from an initial parser state to some terminal states  conditioned on the
parsing history  yamada   matsumoto        nivre        nivre  hall    nilsson         this
approach has a lot of interest since it is fast  linear time for projective parsing  and can incorporate
rich non local features  zhang   nivre        
it has been considered in the past that simple transition based parsing using greedy decoding
and local training is not as accurate as graph based parsers that are globally trained and use exact
inference algorithms  however  chen and manning        showed that the greedy transition based
parsers can be significantly improved with a well designed neural network architecture  this approach can be considered as a new paradigm of parsing  in that it is based on pure distributed
feature representations  more recently  this architecture has been improved in different ways  for
example  weiss  alberti  collins  and petrov        combined the neural network with structured
perceptron  and use beam search for decoding  achieving the new state of the art performance  dyer  ballesteros  ling  matthews  and smith        instead explored novel techniques for learning
better representations of parser states by utilizing long short term memory networks  lstm   other
work also includes that of zhou  zhang  huang  and chen        who applied structured learning
with beam search decoding over the neural network model  in this study  we choose the original
chen   mannings architecture  without losing generality  to build our basic dependency parsing
models for cross lingual transfer 
   

fir epresentation l earning for c ross  l ingual t ransfer parsing

    distributed representations for nlp
recent years have seen numerous attempts of learning distributed representations for different natural language objects  from morphemes  words and phrases  to sentences and documents  using
distributed representations  these symbolic units are embedded into a dense  continuous and lowdimensional vector space  thus it is often referred to as embeddings  
distributed representation is attractive in nlp for several reasons  first  it provides a straightforward way of measuring the similarities between natural language objects  through distributed
representations  we can easily tell which two words phrases documents are similar in semantic or
even other aspects by simply measuring the cosine distance of vectors 
second  it can be learned from large scale unannotated data in general  and thus can be highly beneficial for various downstream applications as a source to alleviate data sparsity  the most
straightforward way of applying distributed representations to nlp tasks is to fed the distributed
feature representations into existing supervised nlp systems as augmented features  in a semisupervised fashion  turian  ratinov    bengio         despite the simplicity and effectiveness  it
has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the traditional nlp systems  wang   manning 
       one remedy is to discretize the distributed feature representations  that is to convert the continuous  dense and low dimensional vectors into traditional discrete  sparse and high dimensional
space  as studied by guo  che  wang  and liu         however  we believe that a non linear system
 e g   neural network  is a more powerful and promising solution  some decent progress has already
been made in this paradigm of nlp on various tasks  such as neural sequence labeling  collobert
et al          dependency parsing  chen   manning         sentence classification  kim        and
machine translation  sutskever  vinyals    le        
third  it provides such a kind of representation that can be shared across languages  tasks and
even diverse modalities of data resources  this property has motivated lines of research on multilingual representation learning  klementiev et al         chandar a p et al         hermann  
blunsom         multi task learning  collobert   weston        and multi modal learning  srivastava   salakhutdinov         this is also the primary motivation of this work that facilitates
cross lingual transfer parsing via multilingual distributed representation learning of words 

   cross lingual dependency parsing
in this section  we first describe the primary transition based dependency parsing model utilizing
neural networks  and then details for cross lingual transfer 
    a neural network architecture for transition based dependency parsing
in this section  we first briefly describe transition based dependency parsing and the arc standard
parsing algorithm  then we revisit the neural network architecture for transition based dependency
parsing proposed by chen and manning        
as discussed in section      transition based parsing generates a dependency tree by predicting a transition sequence from an initial parser state to the terminal state  several transition based
parsing algorithms have been presented in the literature  such as the arc standard and arc eager algorithms for projective parsing  nivre               the list based algorithm  nivre        and the
   in this paper  these two terminologies are used interchangeably 

   

fig uo   c he   yarowsky  wang   l iu

swap based algorithm  nivre        for non projective parsing  different algorithms have different
transition actions  take the arc standard algorithm for example  each parsing state  typically known
as configuration  can be represented as a tuple consisting of a stack s  a buffer b  and a partially derived forest  i e   a set of dependency arcs  a  given an input word sequence x   w  w         wn   the
initial configuration can be represented as   w   s    w  w         wn  b     and the terminal configuration is  w   s     b   a  where w  is a pseudo word indicating the root of the whole dependency
tree  denoting si  i              as the ith element in the stack  and bi  i              as the ith element in the buffer   the arc standard system defines three types of transition actions  l eft a rc r  
r ight a rc r   and s hift  r is the dependency relation 
r

 l eft a rc r   extend a with a new arc  s  
 s     s  the head and s  the modifier  and
remove s  from the stack 
r

 r ight a rc r   extend a with a new arc  s  
 s     s  the head and s  the modifier  and
pop s  from the stack 
 s hift  move b  from the buffer to the stack  precondition is that b is not empty 
the typical approach for greedy arc standard parsing is to build a multi class classifier  e g  
support vector machines  maximum entropy models  of predicting the transition action given a feature vector extracted from a specific configuration  while conventional feature engineering suffers
from the problem of sparsity  incompleteness and expensive feature computation  chen   manning 
       the neural network model provides an effective solution 
the architecture of the neural network based dependency parsing model is illustrated in figure    unlike the high dimensional  sparse and discrete features used by traditional parsing models 
in the neural network model  we apply distributed feature representations  primarily  three types of
information are extracted from a configuration in chen   mannings model  word features  pos
features and relation features respectively  in this study  we add non local features including distance features indicating the distance between two items  and the valency features indicating the
number of children for a given item  zhang   nivre         both distance and valency features
are discretized into buckets  all of these features are then projected to an embedding layer via corresponding lookup tables  i e   embedding matrices   which will be estimated through the training
process  the complete feature templates used in our system are shown in table   
then  feature compositions are performed at the hidden layer via the cube activation function 
h   g x     w    xw   xt   xr   xd   xv     b    
where w  is the weight matrix from the input layer to the hidden layer  and b  is the bias vector 
feature compositions are important not only in dependency parsing but in nlp in general 
researchers used to do cost intensive manual feature engineering to design a large set of feature
templates  however  this approach cannot cover all potentially useful features  lei  xin  zhang 
barzilay  and jaakkola        showed that a full feature representation can be derived from the
kronecker product of multiple views of features  which results in a tensor model  by representing
the tensor in a low rank form using c andecomp  parafac  cp  tensor decomposition  kolda  
bader         the number of parameters can be effectively reduced  and thus is suitable for tasks
with limited training data  cao   khudanpur        
   s   b  is the top head element of the stack buffer 

    

fir epresentation l earning for c ross  l ingual t ransfer parsing

softmax layer 
       



hidden layer 
               



transition actions


hidden representation


input layer 

                        

feature extraction





words

clusters

lexical features
root

parsing configurations

stack
has verb

lookup tables
 


pos tags



 

relations

distance 
valency

non lexical features
good adj

buffer
control noun

   

nsubj
he pron

figure    neural network model for dependency parsing  the cluster features are introduced in
section     and     
type

feature templates
w
  i          
eswi   eb
i

word

w
w
w
w
elc  s
  erc  s
  elc  s
  erc  s
  i       
i 
i 
i 
i 
w
w
elc  lc  s
  erc  rc  s
  i       
i   
i   
t
  i          
est i   eb
i

pos

t
t
t
t
elc  s
  erc  s
  elc  s
  erc  s
  i       
i 
i 
i 
i 
t
t
elc  lc  s
  erc  rc  s
  i       
i   
i   

relation

r
r
r
r
elc  s
  erc  s
  elc  s
  erc  s
  i       
i 
i 
i 
i 
r
r
elc  lc  s
  erc  rc  s
  i       
i   
i   

distance

d
d
es
  es
   s  
   b  

valency

eslv    eslv    esrv 

table    feature templates of the neural network model for transition based dependency parsing 
 w c t r d lv rv 
ep
indicates various feature embeddings of the element at position p  lc 
 rc   is the first child to the left  right  and lc   rc   is the second child to the left  right  

indicates the lexical features   indicates the non lexical features 
we suggest that the cube activation function g x    x  can be viewed as a special case of the
low rank tensor  for verification  g x  can be expanded as 
g w  x          wm xm   b   
  wi wj wk  xi xj xk    b wi wj  xi xj      

i j k

i j

    

fig uo   c he   yarowsky  wang   l iu

if we treat the bias term as b  x  where x       then the weight corresponding to each feature
combination xi xj xk can be wrote as wi wj wk   which is exactly the same as a rank   component tensor in the low rank form using cp tensor decomposition  consequently  the cube activation function
implicitly derives full feature combinations  in fact  we can add as many features as possible to the
input layer to improve the parsing accuracy  we will show in section     that the brown cluster
features can be readily incorporated into our model 
the composed features are then propagated to the output layer  generating a probabilistic distribution of the output labels  i e   transition actions  via the softmax activation function  y  
sof tmax w   h   we use the following objective function to train the model 
j     

  n
  
 crossent di   yi     
n i  
 

where crossent p  q  is the cross entropy between two distributions p and q 
crossent p  q     pk ln qk
k

all parameters in  are trained using back propagation  in this model   typically consists of all
the embedding matrices and weights in the network  however  in some cases   may exclude the
word embedding matrix e w   which indicates that the word embeddings are constrained to be fixed
 i e   without updating  while training 
    cross lingual transfer
the idea of cross lingual transfer using the parser we examined above is straightforward  in contrast
to traditional approaches that have to discard rich lexical features  delexicalizing  when transferring
models from one language to another  our model can be transferred using the full model trained on
the source language side  i e   english  
since the non lexical feature  pos  relation  distance  valency  embeddings are directly transferable between languages  the key component of this framework is the cross lingual learning of
lexical feature embeddings  i e   word embeddings   once the cross lingual word embeddings are
induced  we first learn a dependency parser at the source language side  after that  the parser will
be directly used for parsing target language data 
      u niversal d ependencies
as discussed previously  cross lingual model transfer assumes universal grammatical structures that
can be identified in multiple languages  therefore  when evaluated on the test set of target language
with either unlabeled attachment score  uas  or labeled attachment score  las   the performance
of transfer parsing rely heavily on the multilingual consistency of annotation schemes  generally
syntactic annotation schemes differ in the head finding rules  e g   the choice of lexical versus functional head  and the dependency relation labels  i e   the syntactic tagset   it is a challenging task to
construct multilingual treebanks with such consistent annotations  in the initial cross lingual parsing studies  the conll shared task datasets  buchholz   marsi        are broadly used  however 
inconsistencies occur both in the head finding rules and the syntactic tagset across languages  which
made it difficult to evaluate the cross lingual parsers 
    

fir epresentation l earning for c ross  l ingual t ransfer parsing

in order to overcome these difficulties  a new collection of multilingual treebanks with homogeneous syntactic dependency annotation has been presented recently  namely the universal dependency treebanks  udt   mcdonald et al          the universal annotation scheme was created
by harmonizing available treebanks in slightly different variants of the stanford typed dependencies  de marneffe et al          along with the universal part of speech tags  petrov  das    mcdonald         this dataset greatly facilitates research on multilingual syntactic analysis  and also
makes it possible to use las for evaluation  in fact  udt has already been used as a standard
dataset for benchmarking research on cross lingual transfer parsing  ma   xia        tiedemann 
      zhang   barzilay        duong  cohn  bird    cook      a      b  rasooli   collins 
       other efforts towards universal dependencies include the most recent universal dependencies project  ud    and hamledt  zeman et al          in this paper  we conduct experiments on
the udt  v       dataset without losing generality 
      p rojective vs   n on   projective parsing
non projectivity is a common phenomenon in multilingual dependency parsing  the term nonprojectivity indicates that a dependency tree has crossing arcs  which often appear in morphologically rich languages  various algorithms have been proposed for both graph based and transitionbased parsing algorithms to produce non projective trees  for example  the arc standard algorithm
 section      can be readily extended by adding a swap action to handle the non projectivity  which
gives an expected linear and worst case o n    complexity  nivre         other strategies include
the list based algorithm  nivre        which is adapted from the covington algorithm  covington         and a further combination of the list based and the swap based algorithm  choi  
mccallum         unfortunately  there has been no systematically comparison for these different
algorithms in the literature so far 
in this study  however  we focus only on projective parsing because there is no non projective
trees in our source language  english  training data  consequently  non projectivities in target languages will not be handled at this moment  

   cross lingual word representation learning
prior to introducing our approaches for cross lingual word representation learning  we briefly review
the basic model for learning monolingual word embeddings  which constitutes a subprocedure of
the cross lingual approaches 
    continuous bag of words model
in recent years  various approaches have been studied for learning word embeddings from largescale plain texts  all approaches are generally derived from the so called distributional hypothesis  firth         you shall know a word by the company it keeps  in this study  we consider
the continuous bag of words  cbow  model  mikolov  chen  corrado    dean        as imple   https   universaldependencies github io docs 
   https   github com ryanmcd uni dep tb
   note that for the target languages we address in this paper  non projectivity is not pervasive  specifically  the proportion of projective trees presented in their training corpus is respectively     for de      for es  and     for
fr 

    

fig uo   c he   yarowsky  wang   l iu

mented in the open source toolkit word vec   the basic principle of the cbow model is to predict
each individual word in a sequence given the bag of its context words within a fixed window size
as input  using a log linear classifier  this model avoids the non linear transformation in hidden
layers  and hence can be trained with high efficiency 
with large window size  grouped words using the resulting word embeddings are more topically similar  whereas with small window size  the grouped words will be more syntactically similar  bansal  gimpel    livescu         so we set the window size to   in our parsing task 
next  we introduce our approach for inducing bilingual word embeddings  in general  we expect
our bilingual word embeddings to preserve translational equivalences  for example  cooking  english  should be close to its translation  kochen  german  in the embedding space 
    robust alignment based projection
our first method for inducing cross lingual word embeddings has two stages  first  we learn word
embeddings from a source language  s  corpora as in the monolingual case  and then project the
monolingual word embeddings to a target language  t   based on word alignments 
given a sentence aligned parallel corpus d  we first conduct unsupervised bidirectional word
alignment  and then collect an alignment dictionary  specifically  in each word aligned sentence pair
of d  we keep all alignments with conditional alignment probability exceeding a threshold        
and discard the others  specifically  let at s     wit   wjs   ci j    i              nt   j              ns   be
the alignment dictionary  where ci j is the number of times when the ith target word wit is aligned
to the j th source word wjs   ns and nt are vocabulary sizes  we use the shorthand  i  j   at s
to denote a word pair in at s   the projection can be formalized as the weighted average of the
embeddings of translation words 
ci j
v wit    
 v wjs  
   

c
i 
 i j at s
where ci    j ci j   v w  is the embedding of w 
obviously  the simple projection method has one drawback  it only assigns word embeddings
for those target language words that occur in the word aligned data  which is typically smaller than
the monolingual datasets  therefore  in order to improve the robustness of projection  we utilize
a morphology inspired mechanism  to propagate embeddings from in vocabulary words to out oft
vocabulary  oov  words  specifically  for each oov word woov
  we extract a list of candidate
words that is similar to it in terms of edit distance  levenshtein distance   and then set the averaged
t
vector as the embedding of woov
  more formally 
t
v woov
    avg  v w   
w c

t
where c    ww  editdist woov
  w     

   

to reduce noise  we choose a small edit distance threshold      
the process of robust projection can be viewed as a two stage graph propagation algorithm  as
illustrated in figure    left panel   embeddings are first propagated from source language words
to target language words that appear in the bilingual lexicons  next  monolingual propagation is
performed to obtain oov word embeddings in the target language  using the edit distance metric 
   http   code google com p word vec 

    

fir epresentation l earning for c ross  l ingual t ransfer parsing

 

source language



 
bilingual lexicon
 weighted 

target language

parallel data
wiktionary
panlex




 



 





 













 

 



cca
 



in vocabulary words
out of vocabulary words





figure    illustration of robust projection  left  and cca  right  for inducing cross lingual word
embeddings 

    canonical correlation analysis
the second approach we consider is similar to that of faruqui and dyer         which uses cca to
improve monolingual word embeddings with multilingual correlation  cca is a way of measuring
the linear relationship between multidimensional variables  for two multidimensional variables 
cca aims to find two projection matrices to map the original variables to a new basis  lowerdimensional   such that the correlation between the two variables is maximized 
we refer the readers to the work of hardoon  szedmak  and shawe taylor        for theoretical
foundations and algorithm specifics of cca  here lets treat cca as a black box  and see how cca
can be applied for inducing bilingual word embeddings  suppose there are already two pre trained
monolingual word embeddings  e g   english and german     rn  d  and   rn  d    at the
first step  we extract a one to one alignment dictionary d     from the alignment dictionary
ast    here      indicating that every word in  is translated to one word in     and vice
versa 
the process is illustrated in figure    right panel   denoting the dimension of resulting word
embeddings by d  min d    d     first  we derive two projection matrices v  rd  d   w  rd  d
respectively for  and  using cca 
v  w   cca      

   

then  v and w are used to project the entire vocabulary  and  
   v 

   w

   

where   rn  d and   rn  d are the resulting word embeddings for our cross lingual task 
   at s is also worth trying  but we observed slight performance degradation in our experimental setting 

    

fig uo   c he   yarowsky  wang   l iu

    pros and cons
contrary to the robust projection approach  cca assigns embeddings for every word in the monolingual vocabulary  however  one potential limitation is that cca assumes linear transformation of
word embeddings  which is difficult to satisfy  at the mean time  when training the source language
parser using the cca cross lingual word embeddings  we have to constrained e w to be fixed  as
mentioned in section      otherwise  the translational equivalence will be broken  the robust projection approach  however  doesnt have such limitation  further discussion with experiments will
be presented in section       
note that both approaches can be generalized to lower resource languages where parallel bitexts
are not available  in that way  the dictionary a can be readily obtained either using bilingual lexicon
induction approaches  mann   yarowsky        koehn   knight        haghighi  liang  bergkirkpatrick    klein         or from online resources like wiktionary  and panlex   

   experiments
this section describes the experiments  we first describe the data and settings used in the experiments  and then the results 
    data and settings
for the pre training of word embeddings  we use the wmt      monolingual news corpora for
english  german and spanish    for french  we combined the wmt      and wmt      monolingual news corpora    we got the word alignment counts using the fast align toolkit in cdec  dyer
et al         from the parallel news commentary corpora  wmt          combined with the europarl corpus for english german  spanish  french    
for the training of the neural network dependency parser  we set the number of hidden units to
     the dimension of embeddings for different features are shown in table   

dim 

word
  

pos
  

label
  

distance
 

valency
 

cluster
 

table    dimensions of various types of feature embeddings 
mini batch adaptive stochastic gradient descent  adagrad   duchi  hazan    singer        is
used for optimization  for the cca approach  we use the implementation of faruqui and dyer
       
we employ the universal dependency treebanks  udt v     for a reliable evaluation of our
approach for cross lingual dependency parsing  the universal multilingual treebanks are annotated
using the universal pos tagset  petrov et al         which contains    pos tags  as well as the
universal dependencies which defines    dependency relations  we follow the standard split of the
treebanks for all languages 
  
   
   
   
   

https   www wiktionary org 
http   panlex org 
http   www statmt org wmt   
http   www statmt org wmt   
http   www statmt org europarl 

    

fir epresentation l earning for c ross  l ingual t ransfer parsing

    baseline systems
we compare our approach with the following systems 
for the first baseline  we evaluate the delexicalized transfer of our neural network based parser
 d elex   in which we only use the non lexical features  figure     here we investigate the effect
of the non local features  distance  valency   the delexicalized systems which do not include these
non local features is referred to as  d elex  basic   
we also compare our approach with the delexicalized parser presented by mcdonald et al 
        m c d     which used a perceptron based transition based parser with a beam of size   
along with richer non local features  zhang   nivre         our re implementation of this approach
under the framework of zpar  zhang   clark        is referred to as  m c d     
furthermore  we consider a strong baseline system as proposed by tackstrom et al         
which utilized cross lingual word cluster features to enhance the perceptron based delexicalized
parser  m c d    cluster   we use the same alignment dictionary as described in section     to
induce the cross lingual word clusters  we re implement the p rojected clustering approach described in the work of tackstrom et al   which assigns a target word to the cluster with which it is
most often aligned 
c wit     arg max  ci j    c wjs     k 
k

 i j at s

obviously  this method also has the drawback that words that do not occur in the alignment dictionary  oov  cannot be assigned a cluster  therefore  we use the same strategy as described in
section     to find the most likely clusters for the oov words  instead of computing the average of
embeddings  we solve an argmax problem 
t
    arg max 
c woov
k

  c w     k 

w c

   

t
  w     
where c    weditdist woov

 is set to   constantly  instead of the clustering model of uszkoreit and brants         we use
brown clustering        to induce hierarchical word clusters  where each word is represented as a
bit string  we use the same word cluster feature templates from tackstrom et al          and set the
number of brown clusters to     
    experimental results
all of the parsing models are trained using the development data from english for early stopping 
table   lists the results of the cross lingual transfer experiments for dependency parsing  table  
further summarizes each of the experimental gains detailed in table   
we first examine the benefit brought by the non local distance and valency features  as observed
in the comparison of d elex  basic  and d elex  marginal improvements are obtained for de and
fr  and more significant improvements for es  therefore  we adopted these features in all of the
following experiments 
our delexicalized system obtains slightly lower performance than those reported by mcdonald
et al          m c d     because we used greedy decoding and local training  our re implementation
of mcdonald et al s work attains comparable performance with m c d    for all languages we consider in this study  by using cross lingual word embeddings either from alignment based projection
or cca  we obtain statistically significant improvements against the delexicalized system  both in
    

fig uo   c he   yarowsky  wang   l iu

d elex  basic 
d elex
p roj
p roj cluster
cca
cca cluster

unlabeled attachment score  uas 
en
de
es
fr
avg
                             
                             
                             
                             
                             
                             

labeled attachment score  las 
en
de
es
fr
avg
                             
                             
                             
                             
                             
                             

m c d  

     

     

     

     

     

     

     

     

     

     

m c d  
m c d    cluster

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

     
     

table    cross lingual transfer dependency parsing from english on the test dataset of   universal multilingual treebanks  results measured by unlabeled attachment score  uas  and
labeled attachment score  las   d elex  basic  is the delexicalized model without nonlocal features  distance  valency    denotes our re implementation of m c d    since the
model varies for different target languages in the cca based approach   indicates the
averaged uas las 

experimental contribution
p roj
vs  d elex
cca
vs  d elex
p roj
vs  m c d  
cca
vs  m c d  
p roj cluster
vs  p roj
cca cluster
vs  cca
m c d    cluster vs  m c d  
p roj cluster
vs  d elex
cca cluster
vs  d elex
p roj cluster
vs  m c d  
cca cluster
vs  m c d  
p roj cluster
vs  m c d    cluster
cca cluster
vs  m c d    cluster

de es fr avg   relative 
            
            
            
            
            
            
            
             
            
            
            
            
            

table    summary of each of the experimental gains detailed in table    in both absolute las gain
and relative error reduction  all gains are statistically significant using malteval  nilsson
  nivre        at p        

uas and las  interestingly  we notice that p roj consistently outperforms cca by a significant
margin  and is comparable to m c d    cluster  further analysis to this observation will be conducted in section       and       
    

fir epresentation l earning for c ross  l ingual t ransfer parsing

type
cluster

feature templates
c
esc i   eb
  i          
i
c
c
c
c
elc  si     erc  s
  elc  s
  erc  s
  i       
i 
i 
i 

c
c
elc  lc  s
  erc  rc  s
  i       
i   
i   

table    word cluster feature templates 

our framework is flexible for incorporating richer features simply by embedding them into
continuous vectors  thus we further embed the cross lingual word cluster features into our model 
together with the proposed cross lingual word embeddings  the cluster feature templates are shown
in table    which is similar to the pos tag feature templates  as shown in table    significant
additive improvements are obtained for both p roj and cca by embedding the cluster features 
compared with our delexicalized system  the relative error is reduced by up to       in uas  and
up to       in las  the combined system further outperforms m c d    cluster significantly  
      e ffect of robust p rojection
since in both p roj and the induction of cross lingual word clusters  we use edit distance measure
for oov words  we would like to see how this affects the performance of parsing 
intuitively  higher coverage of projected words in the test dataset should promote the parsing
performance more  to verify this  we further conduct experiments under both settings using the
p roj cluster model  for robust projection  we examine the effect of edit distances ranging from
  to    results are shown in table    improvements are observed for all languages when using
robust projection with edit distance measure  especially for fr  where the highest coverage gain is
obtained by robust projection  we also observe slightly improvements for de and es when using
an edit distance of    but performance starts to degrade when it gets larger  this is reasonable  since
larger edit distance increases the word coverage  but also introduces more noise 

simple

de

es

fr

coverage
uas
las
coverage
uas
las
coverage
uas
las

     
     
     
     
     
     
     
     
     

   
     
     
     
     
     
     
     
     
     

robust
   
     
     
     
     
     
     
     
     
     

table    effect of robust projection 

    

   
     
     
     
     
     
     
     
     
     

fig uo   c he   yarowsky  wang   l iu

      e ffect of f ine  t uning w ord e mbeddings
another reason for the effectiveness of p roj over cca lies in the fine tuning of word embeddings
while training the parser 
cca can be viewed as a joint method for inducing cross lingual word embeddings  when
training the source language dependency parser with cross lingual word embeddings derived from
cca  the en word embeddings should be fixed  otherwise  the translational equivalence will be
broken  however  for p roj  there is no such limitation  word embeddings can be updated as other
non lexical feature embeddings  in order to obtain a more accurate dependency parser  we refer to
this procedure as a fine tuning process to the word embeddings  to verify the benefits of fine tuning 
we conduct experiments to see relative loss if word embeddings are fixed while training  results
are shown in table    which indicates that fine tuning indeed offers considerable help 

de
es
fr

uas
las
uas
las
uas
las

fixed
     
     
     
     
     
     

fine tuning
     
     
     
     
     
     


     
     
     
     
     
     

table    effect of fine tuning word embeddings 

    compare with existing bilingual word embeddings
in this section  we compare our bilingual embeddings with several previous approaches in the context of dependency parsing  to the best of our knowledge  this is the first work on evaluation of
bilingual word embeddings in syntactic tasks 
the approaches we consider include the multi task learning approach  klementiev et al        
 mtl   the bilingual auto encoder approach  chandar a p et al          b iae   the bilingual compositional vector model  hermann   blunsom         b icvm   and the bilingual bag of words
approach  gouws et al          b ilbowa  
for mtl and b iae  we adopt their released word embeddings directly due to the inefficiency of
training    for b icvm and b ilbowa  we re run their systems on the same dataset as our previous
experiments    results are summarized in table   
cca and p roj consistently outperforms all other approaches in all languages  and p roj performs the best  the inferior performance of mtl and b iae is partly due to the low word coverage 
for example  they cover only     of words in the universal de test treebank  whereas the cca
and p roj covers over      moreover  b iae  b icvm and b ilbowa introduce sentence level translational equivalence as objectives or regularizers for learning bilingual word embeddings  these
approaches are advantageous in that they dont assume require word alignment  however  word toword translational equivalence cannot be well preserved in this way 
    the mtl embeddings are normalized before training 
    b icvm only uses the bilingual parallel dataset 

    

fir epresentation l earning for c ross  l ingual t ransfer parsing

mtl  klementiev et al        
b iae  chandar a p et al        
b icvm  hermann   blunsom       
b ilbowa  gouws et al        
cca
p roj

de
uas
las
           
           
           
           
           
           

es
uas
     
     
     
     
     
     

las
     
     
     
     
     
     

fr
uas
las
           
           
           
           
           
           

table    comparison with existing bilingual word embeddings   for mtl and b iae  we use their
released bilingual word embeddings 

target word  es 

china
 china 

problemas
 problems 

septiembre
 september 

p roj
india
russia
taiwan
chinese
problem
difficulties
troubles
issues
october
august
january
december

cca
russia
indonesia
beijing
chinese
problems
woes
troubles
dilemmas
december
july
october
june

neighboring words  en 
mtl
b iae
china
korea
independent india
sumitomo
chinese
malaysian
brazil
events
problem
sanctions
greatly
conditions
highlighted
laws
scale
december
month
february
april
july
scheduled
march
november

b icvm
chinese
chinois
sino
     
problematic
problematical
difficulties
troubles
  th
     
  
eleventh

b ilbowa
helsinki
bulgarians
constituting
market
deficiencies
situations
omissions
attentively
a m
p m
twelve
         

table    target words in spanish and their   most similar words in english  as induced by various
approaches 

to verify this assumption  we taking en es as a case study  we manually inspect the   most
similar words  by cosine similarity  in english to a given set of words in spanish  table     we
can observe both semantic and syntactic shifting in the k nearest neighbors prediction of b iae 
b icvm and b ilbowa  whereas p roj and cca give more translational equivalent predictions  for
example  b icvm yields adjective like problematical for the target noun problemas  b ilbowa yields
semantic related word market for china  in general  p roj is the most robust approach  behaving
consistently well for most of the sampled words 
it is worth noting that we dont assume require bilingual parallel data in cca and p roj  what
we need in practice is a bilingual lexicon for each paired languages  this is especially important for
generalizing our approaches to lower resource languages  where parallel texts are not available 
    

fig uo   c he   yarowsky  wang   l iu

   target language adaptation with minimal supervision
it is important for us to distinguish what linguistic structures can be learned via cross lingual transfer
versus what can only be learned on the basis of monolingual information in the language to be
parsed  intuitively  cross lingual approaches can only learn the common dependency structures
shared between the source and target language  however  for many languages  there are some
specialized  language specific  syntactic characteristics that are can only be learned from data in
the target language 
take the adjective noun order for example  in spanish and french  adjectives often appears
after the nouns  thus forming a right directed arc labeled by amod  whereas in english  the amod
 adjectival modifier  arcs are mostly left directed  as illustrated in figure    another example is the
subject verb object order  in german  verbs often appear at the end of a sentence in v  position 
which causes much more left directed dobj  direct object  arcs than in english  figure     these
differences can be clearly observed from the universal treebanks  table    shows the significant
distribution divergence between left directed and right directed arcs of dobj and amod relations in
treebanks from different languages 
relation  dobj  language  en vs  de
dobj
dobj
ratio
en
      
   
        
de
     
     
       
relation  amod  language  en vs  es  fr
amod amod
ratio
en
     
      
        
es
      
     
       
fr
      
     
       

table     distribution divergences of left directed and right directed arcs with dobj relation in en
and de  top   and amod relation in en and es fr  bottom  

amod

amod

noun

adj

noun

adj

spanish 

consejo

superior

conflictos

sociales

adj

noun

adj

noun

english 

superior

council

social

conflicts

amod

amod

figure    reverse direction of the amod relation in spanish and english  french also has the adjectives following the nouns 

    

fir epresentation l earning for c ross  l ingual t ransfer parsing

root
advmod

det

dobj

adv

det

noun

verb

de 

endlich

den

richtigen

gefunden

en 

finally

found

the

right man

adv

verb

det

noun

advmod

det
dobj
root

figure    reverse direction of the dobj relation in german and english 
therefore  in this section  we investigate how much our cross lingual transfer model can be improved by annotating a small amount of labeled training data at target language side  even though
building large scale treebanks of low resource languages for supervised learning is costly  annotating dependency structures for a small amount of sentences  e g        is not that difficult 
we still conduct experiments on the universal dependency treebanks  which provide labeled
training data for multiple languages  for each language we studied  de  es  fr   we incrementally
augment the amount of labeled sentences from     to       with a step of      to adapt the parameters of the cross lingual transfer model to the specific target language  theoretically  since target
language treebanks contain non projective trees  it would make more sense to apply non projective
algorithms  e g   swap based  for target language adaptation  in this way  however  w  has to be
re trained from scratch  which doesnt show good performance in our experiments since the minimally supervised data is very small  consequently  we still rely on the arc standard algorithm
for adaption  the process is almost the same as training the source language parser as described
in section    except that the word embedding matrix e w is fixed  while the rest of parameters in 
 e  t l d v c    w    w    b    are optimized using the augmented labeled data from the target language 
taking equation     as objective function  no development data is used during this process  thus we
simply perform parameter updating for       iterations 
in addition  we built another strong baseline system which employs the same augmented labeled
training data for supervised learning  in this system  we utilize both word embeddings and brown
clusters as features  which are derived separately for each language 
as shown in figure    the results are really promising  the p roj cluster and cca cluster
systems consistently outperform the delexicalized system and the supervised system by a significant margin  p roj cluster and cca cluster in general achieve comparable performances  while
cca cluster is slightly better 
it is worthy noting that the performances of p roj cluster and cca cluster are boosted by
augmenting only     sentences  take de for example  uas is increased from        to        
and las from        to         which is nearly equal to the effect of using       sentences for
supervised learning  this observation demonstrates the great potential of our cross lingual transfer
system for practical usage 
    

fi

  

  

  

  

g uo   c he   yarowsky  wang   l iu










  






















uas







  

uas

  

uas






  







  

  




  







  

proj cluster
cca cluster
delexicalized
supervised

 

   

   

   

   

    

 

   






   

   

   

    

   

   

   

    












  

  























  







  

  



las

  



  

las



  

  

las

   

  





proj cluster
cca cluster
delexicalized
supervised

labeled training data  fr 



  

 

labeled training data  es 

  

  

labeled training data  de 





  



  

proj cluster
cca cluster
delexicalized
supervised

  

  



  

  






 

   

   

   

   

labeled training data  de 

    



  

proj cluster
cca cluster
delexicalized
supervised

proj cluster
cca cluster
delexicalized
supervised

  

  



  

proj cluster
cca cluster
delexicalized
supervised

  

  



 

   

   

   

   

labeled training data  es 

    

 

   

   

   

   

    

labeled training data  fr 

figure    target language adaptation by incrementally augmenting labeled training data  sentences  to fine tune the cross lingual transfer model  performances are evaluated using
uas  top  and las  bottom   note that the points whose x coordinates are   represent
our cross lingual transfer performance  where no labeled training data are used 

analysis  our primary hypothesis is that by incorporating data in the target language  our model
can be able to learn the special syntactic patterns that are not consistent with the source language  to
verify this  we further study the influence of target language adaptation on the two special relations 
dobj  de  and amod  es  fr   by measuring their precision and recall changes with the use of
only     target language sentences  results are shown respectively in table    and table     we
observe great improvements in recall for these relations  which indicates that the model indeed gains
the ability of learning target language specific dependency structures with the supervision of only
    sentences 

   related studies
the cross lingual annotation projection method was pioneered by yarowsky  ngai  and wicentowski        for shallow nlp tasks  pos tagging  ner  etc    and later applied to dependency
parsing  hwa et al         smith   eisner        zhao et al         jiang et al         tiedemann 
       most work along this line has been dedicated to improving the robustness of syntactic pro    

fir epresentation l earning for c ross  l ingual t ransfer parsing

relation  dobj  language  de
precision recall
proj cluster
     
     
    
     
     

            
cca cluster
     
     
    
     
     

            

table     effect of minimal supervision      sentences  on dobj 
relation  amod  language  es  fr
es
fr
precision recall precision recall
proj cluster
     
     
     
     
    
     
     
     
     

            
            
cca cluster
     
     
     
     
    
     
     
     
     

            
            

table     effect of minimal supervision      sentences  on amod 

jection and alleviating the noise and errors introduced by word alignment based projection  typical
approaches include soft projection  li  zhang    chen         treebank translation  tiedemann  agic    nivre         distribution transfer  ma   xia         and the most recently proposed
density driven projection  rasooli   collins         it is worth mentioning that remarkable results
have been achieved through annotation projection methods  tiedemann        rasooli   collins 
       due in large part to that parsers are trained at the target language side 
for cross lingual model transfer  learning cross lingual feature representations has been a promising direction  typical approaches include cross lingual word clustering  tackstrom et al        
which is employed in this paper as a baseline system  and projection features  durrett  pauls    klein         kozhevnikov and titov        derived a linear projection that maps target instances to
source side feature representations  which to some extent is similar to our cca approach  xiao and
guo        learned cross lingual word embeddings and applied them to mstparser for linguistic
transfer  which inspired our work  sgaard et al         obtained multi source unified word embeddings via inverted indexing in wikipedia  and applied them to various nlp tasks  however  their
results didnt show significant improvements in parsing  nevertheless  the idea of utilizing multisource information for learning cross lingual word embeddings makes great sense  more recently 
duong et al       a      b  also utilized the neural network architecture with parameter sharing
between parsers of different languages  however  their approach requires annotated treebanks from
the target language side  which makes it distinct from our transfer parsing framework  in addition
to representation learning  attempts were also made to integrate monolingual linguistic features into the parsing models  such as manually constructed universal dependency parsing rules  naseem 
    

fig uo   c he   yarowsky  wang   l iu

chen  barzilay    johnson        and manually specified typological features  naseem  barzilay 
  globerson        zhang   barzilay        
using neural networks for dependency parsing is not a new approach  to the best of our knowledge  mayberry and miikkulainen        presented the first work that explored neural networks
for shift reduce constituent based parsing  they used one hot feature representations  henderson
       used a simple synchrony network to predict parse decisions in a constituency parser  and
was the first to use neural networks in a broad coverage penn treebank parser  titov and henderson        applied incremental sigmoid belief networks to constituent based parsing  garg and
henderson        later extended this work to transition based dependency parsing using a temporal restricted boltzman machine  these parsers  however  are much less scalable in practice 
earlier progress made in using deep learning for parsing includes work by collobert        and
socher et al         for constituent based parsing  and stenetorp        who built recursive neural
networks for transition based dependency parsing 

   conclusion
this paper proposes a novel framework based on distributed representations for cross lingual dependency parsing  two algorithms are proposed for the induction of cross lingual word representations 
namely robust projection and cca  which bridge the lexical feature gap 
experiments show that by using cross lingual word embeddings derived from either approach 
the transferred parsing performance can be improved significantly against the delexicalized system 
a notable observation is that our projection method performs significantly better than cca  additionally  our framework is flexibly able to incorporate the cross lingual word cluster features  with
further significant gains in each use  the combined system significantly outperforms the delexicalized systems on all languages  by an average of       error reduction on las  and further
significantly outperforms the models of mcdonald et al         augmented with projected word
cluster features 
furthermore  we show that the performance of our cross lingual transfer system on a specific target language can be boosted by minimal supervision from that language  which is of great
significance for practical usage 

acknowledgments
we are grateful to manaal faruqui for providing the bilingual resources  we thank ryan mcdonald
for pointing out the evaluation issue in the experiment  we also thank sharon busching for the
proofreading and the anonymous reviewers for the insightful comments and suggestions  this work
was supported by the national key basic research program of china via grant     cb      
and the national natural science foundation of china  nsfc  via grant          and          
corresponding author  wanxiang che  e mail  car ir hit edu cn 

references
bansal  m   gimpel  k     livescu  k          tailoring continuous word representations for dependency parsing  in proceedings of the   nd annual meeting of the association for computa    

fir epresentation l earning for c ross  l ingual t ransfer parsing

tional linguistics  volume    short papers   pp          baltimore  maryland  association
for computational linguistics 
brown  p  f   desouza  p  v   mercer  r  l   pietra  v  j  d     lai  j  c          class based n gram
models of natural language  computational linguistics                
buchholz  s     marsi  e          conll x shared task on multilingual dependency parsing  in
proceedings of the tenth conference on computational natural language learning  conllx   pp          new york city  association for computational linguistics 
cao  y     khudanpur  s          online learning in tensor space  in proceedings of the   nd
annual meeting of the association for computational linguistics  volume    long papers  
pp          baltimore  maryland  association for computational linguistics 
carreras  x          experiments with a higher order projective dependency parser  in proceedings
of the conll shared task session of emnlp conll       pp          prague  czech
republic  association for computational linguistics 
chandar a p  s   lauly  s   larochelle  h   khapra  m   ravindran  b   raykar  v  c     saha  a 
        an autoencoder approach to learning bilingual word representations  in advances in
neural information processing systems     pp            curran associates  inc 
chen  d     manning  c          a fast and accurate dependency parser using neural networks  in
proceedings of the      conference on empirical methods in natural language processing
 emnlp   pp          doha  qatar  association for computational linguistics 
choi  j  d     mccallum  a          transition based dependency parsing with selectional branching  in proceedings of the   st annual meeting of the association for computational linguistics  volume    long papers   pp            sofia  bulgaria  association for computational
linguistics 
collobert  r          deep learning for efficient discriminative parsing  in proceedings of the   th
international conference on artificial intelligence and statistics  aistats   pp         
fort lauderdale  fl  usa  jmlr org 
collobert  r     weston  j          a unified architecture for natural language processing  deep
neural networks with multitask learning  in proceedings of the   th international conference
on machine learning  icml     pp          helsinki  finland  acm 
collobert  r   weston  j   bottou  l   karlen  m   kavukcuoglu  k     kuksa  p          natural
language processing  almost  from scratch  journal of machine learning research          
     
covington  m  a          a fundamental algorithm for dependency parsing  in proceedings of the
  th annual acm southeast conference  pp        
de marneffe  m  c   maccartney  b   manning  c  d   et al          generating typed dependency
parses from phrase structure parses  in proceedings of the fifth international conference
on language resources and evaluation  lrec     pp          genoa  italy  european
language resources association  elra  
de marneffe  m  c     manning  c  d          the stanford typed dependencies representation  in
coling       proceedings of the workshop on cross framework and cross domain parser
evaluation  pp      manchester  uk  association for computational linguistics 
    

fig uo   c he   yarowsky  wang   l iu

duchi  j   hazan  e     singer  y          adaptive subgradient methods for online learning and
stochastic optimization  journal of machine learning research               
duong  l   cohn  t   bird  s     cook  p       a   low resource dependency parsing  cross lingual
parameter sharing in a neural network parser  in proceedings of the   rd annual meeting
of the association for computational linguistics and the  th international joint conference
on natural language processing  volume    short papers   pp          beijing  china 
association for computational linguistics 
duong  l   cohn  t   bird  s     cook  p       b   a neural network model for low resource universal dependency parsing  in proceedings of the      conference on empirical methods in
natural language processing  pp          lisbon  portugal  association for computational
linguistics 
durrett  g   pauls  a     klein  d          syntactic transfer using a bilingual lexicon  in proceedings of the      joint conference on empirical methods in natural language processing
and computational natural language learning  pp       jeju island  korea  association
for computational linguistics 
dyer  c   ballesteros  m   ling  w   matthews  a     smith  n  a          transition based dependency parsing with stack long short term memory  in proceedings of the   rd annual meeting
of the association for computational linguistics and the  th international joint conference
on natural language processing  volume    long papers   pp          beijing  china  association for computational linguistics 
dyer  c   lopez  a   ganitkevitch  j   weese  j   ture  f   blunsom  p   setiawan  h   eidelman  v  
  resnik  p          cdec  a decoder  alignment  and learning framework for finite state and
context free translation models  in proceedings of the acl      system demonstrations  pp 
     uppsala  sweden  association for computational linguistics 
eisner  j  m          three new probabilistic models for dependency parsing  an exploration  in
proceedings of the   th conference on computational linguistics volume    pp         
copenhagen  denmark  association for computational linguistics 
faruqui  m     dyer  c          improving vector space word representations using multilingual
correlation  in proceedings of the   th conference of the european chapter of the association for computational linguistics  pp          gothenburg  sweden  association for
computational linguistics 
firth  j  r          a synopsis of linguistic theory           in studies in linguistic analysis  pp 
     blackwell 
garg  n     henderson  j          temporal restricted boltzmann machines for dependency parsing 
in proceedings of the   th annual meeting of the association for computational linguistics 
human language technologies  pp        portland  oregon  usa  association for computational linguistics 
gouws  s   bengio  y     corrado  g          bilbowa  fast bilingual distributed representations
without word alignments  in proceedings of the   nd international conference on machine
learning  icml   pp          lille  france 
guo  j   che  w   wang  h     liu  t          revisiting embedding features for simple semisupervised learning  in proceedings of the      conference on empirical methods in natural
    

fir epresentation l earning for c ross  l ingual t ransfer parsing

language processing  emnlp   pp          doha  qatar  association for computational
linguistics 
guo  j   che  w   yarowsky  d   wang  h     liu  t          cross lingual dependency parsing
based on distributed representations  in proceedings of the   rd annual meeting of the association for computational linguistics and the  th international joint conference on natural
language processing  volume    long papers   pp            beijing  china  association
for computational linguistics 
haghighi  a   liang  p   berg kirkpatrick  t     klein  d          learning bilingual lexicons
from monolingual corpora  in proceedings of acl     hlt  pp          columbus  ohio 
association for computational linguistics 
hardoon  d  r   szedmak  s     shawe taylor  j          canonical correlation analysis  an
overview with application to learning methods  neural computation                   
henderson  j          discriminative training of a neural network statistical parser  in proceedings of the   nd meeting of the association for computational linguistics  acl     main
volume  pp         barcelona  spain 
hermann  k  m     blunsom  p          multilingual models for compositional distributed semantics  in proceedings of the   nd annual meeting of the association for computational
linguistics  volume    long papers   pp        baltimore  maryland  association for computational linguistics 
hwa  r   resnik  p   weinberg  a   cabezas  c     kolak  o          bootstrapping parsers via
syntactic projection across parallel texts  natural language engineering                 
jiang  w   liu  q     lv  y          relaxed cross lingual projection of constituent syntax  in
proceedings of the      conference on empirical methods in natural language processing 
pp            edinburgh  scotland  uk  association for computational linguistics 
kim  y          convolutional neural networks for sentence classification  in proceedings of
the      conference on empirical methods in natural language processing  emnlp   pp            doha  qatar  association for computational linguistics 
klein  d     manning  c          corpus based induction of syntactic structure  models of dependency and constituency  in proceedings of the   nd meeting of the association for computational linguistics  acl     main volume  pp          barcelona  spain 
klementiev  a   titov  i     bhattarai  b          inducing crosslingual distributed representations
of words  in proceedings of coling       pp            mumbai  india  the coling
     organizing committee 
koehn  p     knight  k          learning a translation lexicon from monolingual corpora  in proceedings of the acl    workshop on unsupervised lexical acquisition  pp       philadelphia  pennsylvania  usa  association for computational linguistics 
kolda  t  g     bader  b  w          tensor decompositions and applications  siam review        
       
koo  t     collins  m          efficient third order dependency parsers  in proceedings of the
  th annual meeting of the association for computational linguistics  pp       uppsala 
sweden  association for computational linguistics 
    

fig uo   c he   yarowsky  wang   l iu

kozhevnikov  m     titov  i          cross lingual model transfer using feature representation
projection  in proceedings of the   nd annual meeting of the association for computational
linguistics  volume    short papers   pp          baltimore  maryland  association for
computational linguistics 
lei  t   xin  y   zhang  y   barzilay  r     jaakkola  t          low rank tensors for scoring
dependency structures  in proceedings of the   nd annual meeting of the association for
computational linguistics  volume    long papers   pp            baltimore  maryland 
association for computational linguistics 
li  z   zhang  m     chen  w          soft cross lingual syntax projection for dependency parsing  in proceedings of coling       the   th international conference on computational
linguistics  technical papers  pp          dublin  ireland  dublin city university and association for computational linguistics 
ma  x     xia  f          unsupervised dependency parsing with transferring distribution via
parallel guidance and entropy regularization  in proceedings of the   nd annual meeting
of the association for computational linguistics  volume    long papers   pp           
baltimore  maryland  association for computational linguistics 
mann  g  s     yarowsky  d          multipath translation lexicon induction via bridge languages 
in proceedings of the second meeting of the north american chapter of the association
for computational linguistics on language technologies  naacl     pp      pittsburgh 
pennsylvania  association for computational linguistics 
marcus  m  p   marcinkiewicz  m  a     santorini  b          building a large annotated corpus of
english  the penn treebank  computational linguistics                
mayberry  m  r     miikkulainen  r          sardsrn  a neural network shift reduce parser  in
proceedings of the sixteenth international joint conference on artificial intelligence  pp 
        morgan kaufmann publishers inc 
mcdonald  r   crammer  k     pereira  f          online large margin training of dependency
parsers  in proceedings of the   rd annual meeting of the association for computational
linguistics  acl     pp        ann arbor  michigan  association for computational linguistics 
mcdonald  r     nivre  j          characterizing the errors of data driven dependency parsing
models  in proceedings of the      joint conference on empirical methods in natural
language processing and computational natural language learning  emnlp conll   pp 
        prague  czech republic  association for computational linguistics 
mcdonald  r   nivre  j   quirmbach brundage  y   goldberg  y   das  d   ganchev  k   hall  k  
petrov  s   zhang  h   tackstrom  o   bedini  c   bertomeu castello  n     lee  j         
universal dependency annotation for multilingual parsing  in proceedings of the   st annual
meeting of the association for computational linguistics  volume    short papers   pp    
    sofia  bulgaria  association for computational linguistics 
mcdonald  r   petrov  s     hall  k          multi source transfer of delexicalized dependency
parsers  in proceedings of the      conference on empirical methods in natural language
processing  pp        edinburgh  scotland  uk  association for computational linguistics 
    

fir epresentation l earning for c ross  l ingual t ransfer parsing

mcdonald  r  t     pereira  f  c          online learning of approximate dependency parsing
algorithms  in proceedings of the   st conference of the european chapter of the association for computational linguistics  pp        trento  italy  the association for computer
linguistics 
mikolov  t   chen  k   corrado  g     dean  j          efficient estimation of word representations
in vector space  international conference on learning representations  iclr  workshop 
naseem  t   barzilay  r     globerson  a          selective sharing for multilingual dependency
parsing  in proceedings of the   th annual meeting of the association for computational
linguistics  volume    long papers   pp          jeju island  korea  association for computational linguistics 
naseem  t   chen  h   barzilay  r     johnson  m          using universal linguistic knowledge
to guide grammar induction  in proceedings of the      conference on empirical methods
in natural language processing  pp            cambridge  ma  association for computational linguistics 
nilsson  j     nivre  j          malteval  an evaluation and visualization tool for dependency
parsing   in proceedings of the sixth international language resources and evaluation
 lrec     pp          marrakech  morocco  european language resources association
 elra  
nivre  j          an efficient algorithm for projective dependency parsing  in proceedings of the
 th international workshop on parsing technologies  iwpt   pp          nancy  france 
association for computational linguistics 
nivre  j          incrementality in deterministic dependency parsing  in proceedings of the workshop on incremental parsing  bringing engineering and cognition together  pp       
barcelona  spain  association for computational linguistics 
nivre  j          algorithms for deterministic incremental dependency parsing  computational
linguistics                
nivre  j          non projective dependency parsing in expected linear time  in proceedings of
the joint conference of the   th annual meeting of the acl and the  th international joint
conference on natural language processing of the afnlp  pp          suntec  singapore 
association for computational linguistics 
nivre  j   hall  j     nilsson  j          memory based dependency parsing  in hlt naacl     
workshop  eighth conference on computational natural language learning  conll       
pp        boston  massachusetts  usa  association for computational linguistics 
petrov  s   das  d     mcdonald  r          a universal part of speech tagset  in proceedings of
the eighth international conference on language resources and evaluation  lrec       
pp            istanbul  turkey  european language resources association  elra  
rasooli  m  s     collins  m          density driven cross lingual transfer of dependency parsers 
in proceedings of the      conference on empirical methods in natural language processing  pp          lisbon  portugal  association for computational linguistics 
smith  d  a     eisner  j          parser adaptation and projection with quasi synchronous grammar
features  in proceedings of the      conference on empirical methods in natural language
processing  pp          singapore  association for computational linguistics 
    

fig uo   c he   yarowsky  wang   l iu

socher  r   bauer  j   manning  c  d     andrew y   n          parsing with compositional vector
grammars  in proceedings of the   st annual meeting of the association for computational
linguistics  volume    long papers   pp          sofia  bulgaria  association for computational linguistics 
sgaard  a   agic  v   martnez alonso  h   plank  b   bohnet  b     johannsen  a          inverted
indexing for cross lingual nlp  in proceedings of the   rd annual meeting of the association
for computational linguistics and the  th international joint conference on natural language processing  volume    long papers   pp            beijing  china  association for
computational linguistics 
srivastava  n     salakhutdinov  r  r          multimodal learning with deep boltzmann machines  in advances in neural information processing systems     pp            curran
associates  inc 
stenetorp  p          transition based dependency parsing using recursive neural networks  in deep
learning workshop at nips  lake tahoe  nevada  usa 
sutskever  i   vinyals  o     le  q  v          sequence to sequence learning with neural networks  in advances in neural information processing systems     pp            curran
associates  inc 
tackstrom  o   mcdonald  r     uszkoreit  j          cross lingual word clusters for direct transfer
of linguistic structure  in proceedings of the      conference of the north american chapter
of the association for computational linguistics  human language technologies  pp     
     montreal  canada  association for computational linguistics 
tiedemann  j          rediscovering annotation projection for cross lingual parser induction  in
proceedings of coling       the   th international conference on computational linguistics  technical papers  pp            dublin  ireland  dublin city university and association for computational linguistics 
tiedemann  j          cross lingual dependency parsing with universal dependencies and predicted
pos labels          
tiedemann  j   agic  v     nivre  j          treebank translation for cross lingual parser induction  
       
titov  i     henderson  j          fast and robust multilingual dependency parsing with a generative
latent variable model  in proceedings of the conll shared task session of emnlp conll
      pp          prague  czech republic  association for computational linguistics 
turian  j   ratinov  l  a     bengio  y          word representations  a simple and general method
for semi supervised learning  in proceedings of the   th annual meeting of the association
for computational linguistics  pp          uppsala  sweden  association for computational linguistics 
uszkoreit  j     brants  t          distributed word clustering for large scale class based language
modeling in machine translation  in proceedings of acl     hlt  pp          columbus 
ohio  association for computational linguistics 
wang  m     manning  c  d          effect of non linear deep architecture in sequence labeling 
in proceedings of the sixth international joint conference on natural language processing 
pp            nagoya  japan  asian federation of natural language processing 
    

fir epresentation l earning for c ross  l ingual t ransfer parsing

weiss  d   alberti  c   collins  m     petrov  s          structured training for neural network
transition based parsing  in proceedings of the   rd annual meeting of the association for
computational linguistics and the  th international joint conference on natural language
processing  volume    long papers   pp          beijing  china  association for computational linguistics 
xiao  m     guo  y          distributed word representation learning for cross lingual dependency
parsing  in proceedings of the eighteenth conference on computational natural language
learning  pp          ann arbor  michigan  association for computational linguistics 
xue  n   xia  f   chiou  f  d     palmer  m          the penn chinese treebank  phrase structure
annotation of a large corpus  natural language engineering                 
yamada  h     matsumoto  y          statistical dependency analysis with support vector machines 
in proceedings of the  th international workshop on parsing technologies  iwpt   pp     
     nancy  france  association for computational linguistics 
yarowsky  d   ngai  g     wicentowski  r          inducing multilingual text analysis tools via
robust projection across aligned corpora  in proceedings of the first international conference
on human language technology research  pp      san diego  ca  usa  association for
computational linguistics 
zeman  d   dusek  o   marecek  d   popel  m   ramasamy  l   stepanek  j   zabokrtsky  z    
hajic  j          hamledt  harmonized multi language dependency treebank  language
resources and evaluation                
zhang  y     barzilay  r          hierarchical low rank tensors for multilingual transfer parsing  in
proceedings of the      conference on empirical methods in natural language processing 
pp            lisbon  portugal  association for computational linguistics 
zhang  y     clark  s          syntactic processing using the generalized perceptron and beam
search  computational linguistics                
zhang  y     nivre  j          transition based dependency parsing with rich non local features 
in proceedings of the   th annual meeting of the association for computational linguistics  human language technologies  pp          portland  oregon  usa  association for
computational linguistics 
zhao  h   song  y   kit  c     zhou  g          cross language dependency parsing using a bilingual
lexicon  in proceedings of the joint conference of the   th annual meeting of the acl and
the  th international joint conference on natural language processing of the afnlp  pp 
      suntec  singapore  association for computational linguistics 
zhou  h   zhang  y   huang  s     chen  j          a neural probabilistic structured prediction
model for transition based dependency parsing  in proceedings of the   rd annual meeting
of the association for computational linguistics and the  th international joint conference
on natural language processing  volume    long papers   pp            beijing  china 
association for computational linguistics 

    

fi