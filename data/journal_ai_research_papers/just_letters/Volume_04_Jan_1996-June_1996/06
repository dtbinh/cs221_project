journal artificial intelligence research                 

submitted        published     

experimental evidence utility
occam s razor
geoffrey i  webb

webb deakin edu au

school computing mathematics
deakin university
geelong  vic        australia 

abstract

paper presents new experimental evidence utility occam s razor 
systematic procedure presented post processing decision trees produced c    
procedure derived rejecting occam s razor instead attending assumption similar objects likely belong class  increases decision
tree s complexity without altering performance tree training data
inferred  resulting complex decision trees demonstrated have 
average  variety common learning tasks  higher predictive accuracy less
complex original decision trees  result raises considerable doubt utility
occam s razor commonly applied modern machine learning 

   introduction

fourteenth century william occam stated  plurality assumed without necessity   principle since become known occam s razor  occam s razor
originally intended basis determining one s ontology  however  modern times
widely reinterpreted adopted epistemological principle a means
selecting alternative theories well ontologies  modern reinterpretations
occam s razor widely employed classification learning  however  utility
principle subject widespread theoretical experimental attack  paper
adds debate providing experimental evidence utility
modern interpretation occam s razor  evidence takes form systematic procedure adding non redundant complexity classifiers manner demonstrated
frequently improve predictive accuracy 
modern interpretation occam s razor characterized  of two hypotheses h h    explain e  simpler preferred   good        
however  specify aspect theory measured simplicity 
syntactic  semantic  epistemological pragmatic simplicity alternative criteria
employed bunge         practice  common use occam s razor
machine learning seeks minimize surface syntactic complexity  interpretation
paper addresses 
assumed occam s razor usually applied expectation
application will  general  lead particular form advantage  widely
accepted articulation precisely occam s razor applied advantages
expected application classification learning  however  literature
contain two statements seem capture least one widely adopted approach

c      ai access foundation morgan kaufmann publishers  rights reserved 

fiwebb

principle  blumer  ehrenfeucht  haussler  warmuth        suggest wield
occam s razor adopt goal discovering  the simplest hypothesis consistent
sample data  expectation simplest hypothesis  perform well
observations taken source   quinlan        states
 given choice two decision trees  correct
training set  seems sensible prefer simpler one grounds
likely capture structure inherent problem  simpler tree would
therefore expected classify correctly objects outside training set  
statements would necessarily accepted proponents occam s
razor  capture form occam s razor paper seeks address a learning
bias toward classifiers minimize surface syntactic complexity expectation
maximizing predictive accuracy 
statements occam s razor restrict classifiers
correctly classify objects training set  many modern machine learning systems
incorporate learning biases tolerate small levels misclassification training data
 clark   niblett        michalski        quinlan              example   context 
extending scope definition beyond decision trees classifiers general 
seems reasonable modify quinlan s        statement  above 
given choice two plausible classifiers perform identically
training set  simpler classifier expected classify correctly objects
outside training set 
referred occam thesis 
concept identical performance training set could defined many different
ways  might tempting opt definition requires identical error rates
two classifiers applied training set  less strict interpretation might allow two
classifiers differing error rates long difference within statistical
confidence limit  however  maximize applicability results  paper adopt
strict interpretation identical performance that every object training
set  classifiers provide classification o 
noted occam thesis claiming two classifiers
equal empirical support least complex always greater predictive accuracy
previously unseen objects  rather  claiming frequently less
complex higher predictive accuracy 
paper first examines arguments occam thesis 
presents new empirical evidence thesis  evidence acquired using
learning algorithm post processes decision trees learnt c     post processor
developed rejecting occam thesis instead attending assumption
similarity predictive class  post processor systematically adds complexity
decision trees without altering performance training data  demonstrated
lead increase predictive accuracy previously unseen objects range
 real world  learning tasks  evidence taken incompatible occam thesis 
   

fifurther experimental evidence utility occam s razor

   previous theoretical experimental work
provide context new evidence occam thesis  worth brie
examining previous relevant theoretical experimental work  relevant  outline
provided reasons contribution may failed persuade
side debate 

    law conservation generalization performance

conservation law generalization performance  schaffer        proves learning
bias outperform bias space possible learning tasks   follows
occam s razor valuable learning bias  subset
possible learning tasks  might argued set  real world  learning tasks
subset 
paper predicated accepting proposition set  real world  learning
tasks distinguished set possible learning tasks respects render
conservation law inapplicable  rao  gordon  spears        argue case
learning tasks universe uniformly distributed across space
possible learning tasks 
so  one argument support proposition follows 
 real world  learning tasks defined people use machine learning systems 
end  task constructors sought ensure independent variables
 class attributes  related dependent variables  other attributes  ways
captured within space classifiers made available learning system 
actual machine learning tasks drawn randomly space possible learning
tasks  human involvement formulation problems ensures this 
simple thought experiment support proposition  consider learning task
class attribute generated random number generator way
relates attributes  majority machine learning researchers would
slightest disconcerted systems failed perform well trained data 
example  consider learning task class attribute simple count
number missing attribute values object  assume learning task
submitted system  c     quinlan         develops classifiers
mechanism testing classification whether attribute value missing  again 
majority machine learning researchers would unconcerned systems failed
perform well circumstances  machine learning simply unsuited tasks 
knowledgeable user would apply machine learning data  least
expectation obtaining useful classifier therefrom 
paper explores applicability occam thesis  real world  learning tasks 

    theoretical objections occam thesis

machine learning systems explicitly implicitly employ occam s razor  addition
almost universal use machine learning  principle occam s razor widely
   law proved discrete valued learning tasks  reason believe
apply continuous valued tasks

   

fiwebb

accepted general scientific practice  persisted  despite occam s razor
subjected extensive philosophical  theoretical empirical attack  suggests
attacks found persuasive 
philosophical front  summarize bunge         complexity theory
 classifier  depends entirely upon language encoded  claim
acceptability theory depends upon language happens expressed
appears indefensible  further  obvious theoretical relationship syntactic complexity quality theory  possibility world
intrinsically simple use occam s razor enables discovery intrinsic
simplicity  however  even world intrinsically simple  reason
simplicity correspond syntactic simplicity arbitrary language 
merely state less complex explanation preferable specify
criterion preferable  implicit assumption underlying much machine learning research appears that  things equal  less complex classifiers be 
general  accurate  blumer et al         quinlan         occam thesis
paper seeks discredit 
straight forward interpretation  syntactic measure used predict
expected accuracy appears absurd  two classifiers identical meaning  such
  age   pos   age     age   pos 
possible accuracies differ  matter greatly complexities differ 
simple example highlights apparent dominance semantics syntax
determination predictive accuracy 

    previous experimental evidence occam thesis
empirical front  number recent experimental results appeared con ict
occam thesis  murphy pazzani        demonstrated number artificial classification learning tasks  simplest consistent decision trees lower predictive
accuracy slightly complex consistent trees  experimentation  however 
showed results dependent upon complexity target concept 
bias toward simplicity performed well target concept best described
simple classifier bias toward complexity performed well target concept
best described complex classifier  murphy         addition  simplest classifiers
obtained better average  over consistent classifiers  predictive accuracy
data augmented irrelevant attributes attributes strongly correlated target
concept  required classification 
webb        presented results suggest wide range learning tasks
uci repository learning tasks  murphy   aha         relative generality
classifiers better predictor classification performance relative surface
syntactic complexity  however  could argued results demonstrate
strategy selecting simplest pair theories lead maximization
predictive accuracy  demonstrate selecting simplest available
theories would fail maximize predictive accuracy 
schaffer              shown pruning techniques reduce complexity
decreasing resubstitution accuracy sometimes increase predictive accuracy sometimes
   

fifurther experimental evidence utility occam s razor

decrease predictive accuracy inferred decision trees  however  proponent occam
thesis could explain results terms positive effect application occam s
razor  the reduction complexity  counter balanced negative effect
reduction empirical support  resubstitution accuracy  
holte  acker  porter        shown specializing small disjuncts  rules
low empirical support  exclude areas instance space occupied training
objects frequently decreases error rate unseen objects covered disjuncts 
specialization involves increasing complexity  might viewed contrary
occam thesis  however  research shows total error rates classifiers
disjuncts embedded increases disjuncts specialized 
proponent occam thesis could thus dismiss relevance former results
arguing thesis applies complete classifiers elements
classifiers 

    theoretical experimental support occam thesis
theoretical experimental objections occam thesis exists
body apparent theoretical empirical support 
several attempts made provide theoretical support occam thesis
machine learning context  blumer et al         pearl        fayyad   irani        
however  proofs apply equally systematic learning bias favors small
subset hypothesis space  indeed  argued equally support
preference classifiers high complexity  schaffer        berkman   sandholm        
holte        compared learning simple classification rules use sophisticated learner complex decision trees  found that  number tasks uci
repository machine learning datasets  murphy   aha         simple rules achieved
accuracies within percentage points complex trees  could considered
supportive occam thesis  however  case simple rules outperform
complex decision trees  demonstrated exist yet another
learning bias consistently outperformed studied 
final argument might considered support occam thesis
majority machine learning systems employ form occam s razor appear perform well practice  however  demonstrated even better
performance would obtained occam s razor abandoned 

   new experimental evidence occam thesis
theoretical experimental objections occam thesis appear
greatly diminished machine learning community s use occam s razor  paper
seeks support objections occam thesis robust general experimental
counter evidence  end presents systematic procedure increasing complexity inferred decision trees without modifying performance training data 
procedure takes form post processor decision trees produced c     quinlan         application procedure range learning tasks uci
repository learning tasks  murphy   aha        demonstrated result  average 
   

fiwebb

increased predictive accuracy inferred decision trees applied previously
unseen data 

    theoretical basis decision tree post processor
similarity assumption common assumption machine learning that objects
similar high probability belonging class  rendell   seshu        
techniques described rely upon assumption theoretical justification
rather upon occam thesis 
starting similarity assumption  machine learning viewed inference
suitable similarity metric learning task  decision tree viewed
partitioning instance space  partition  represented leaf  contains
objects similar relevant respects thus expected belong
class 
raises issue similarity measured  instance based learning methods  aha  kibler    albert        tend map instance space onto ndimensional geometric space employ geometric distance measures within
space measure similarity  approach problematic number grounds 
first  assumes underlying metrics different attributes commensurable 
possible determine priori whether difference five years age signifies
greater lesser difference similarity difference one inch height  second 
assumes possible provide priori definitions similarity respect
single attribute  one really make universal prescription value    always
similar value   value     never case
relevant similarity metric based log  surface value  case    would
similar      
wish employ induction learn classifiers expressed particular language
would appear forced assume language question manner
captures relevant aspect similarity  potential leaf decision tree presents
plausible similarity metric  all objects fall within leaf similar respect  
empirical evaluation  the performance leaf training set  used
infer relevance similarity metric induction task hand  leaf l covers
large number objects class c classes  provides evidence
similarity respect tests define l predictive c 
figure   illustrates simple instance space partition c     quinlan       
imposes thereon  note c    forms nodes continuous attributes  b  
consist test cut value x  test takes form x  respect
figure   one cut  value   attribute a 
c    infers relevant similarity metric relates attribute only  partition
 shown dashed line  placed value   attribute a  however  one
accept occam thesis  accept similarity assumption  reason
believe area instance space b        lightly shaded
figure    belong class    as determined c     rather class   
c    uses occam thesis justify termination partitioning instance
space soon decision tree accounts adequately training set  consequence 
   

fifurther experimental evidence utility occam s razor

          
              
          
              
            
 
 
 
 
 
                    

figure    simple instance space
  
 
 
 
 
b 
 
 
 
 

large areas instance space occupied objects training set may
left within partitions similarity assumption provides little support 
example  respect figure    could argued relevant similarity metric
respect region   b     similarity respect b   within
entire instance space  objects values b     belong class    five
objects  contrast  three objects values   provide
evidence objects area instance space belong class   
tests represents plausible similarity metric basis available evidence  thus 
object within region similar plausible respect three positive five
negative objects  objects similar relevant respects high probability
belonging class  information available plausible
object similar three positive five negative objects  would appear
probable object negative positive 
disagreement c    similarity assumption case contrasts
with  example  area instance space   b      region 
similarity assumption suggests c    s partition appropriate plausible
similarity metrics indicate object region similar positive objects
only   
post processor developed research analyses decision trees produced c   
order identify regions those occupied objects training set
evidence  in terms similarity assumption  favoring relabeling
   provide example implausible similarity metric  consider similarity metric defined
root node  everything similar  plausible great level
dissimilarity classes respect metric  relevant similarity metric 
distribution training examples representative distribution objects domain
whole  similarity assumption would violated  similar objects would probability
     belonging class  probability calculated follows  probabilities
object             respectively  object   probability belonging
class another object similar      object   probability
belonging class another object similar      thus  probability
object belonging class another similar object                           numbers
involved simple example are  course  small reach conclusion high level
confidence the example intended illustrative only 

   

fiwebb

different class assigned c     regions identified  new branches
added decision tree  creating new partitionings instance space  trees
must provide identical performance respect training set regions
instance space occupied objects training set affected 
dicult see plausible metric complexity could interpret addition
branches increasing complexity tree 
end result post processor adds complexity decision tree without
altering tree applies training data  occam thesis predicts will 
general  lower predictive accuracy similarity assumption predicts will 
general  increase predictive accuracy  seen  latter prediction consistent
experimental evidence former not 

    post processor

process could applied continuous discrete attributes 
current implementation addresses continuous attributes 
post processor operates examining leaf l tree turn  l 
attribute considered turn  a  possible thresholds
region instance space occupied objects l explored  first  minimum
 min  maximum  max  determined values possible objects
reach l  l lies branch split threshold
split provides upper limit  max  values l  lies   branch 
threshold provides lower limit  min   node lie branch 
max      node lie   branch  min       objects
training set values within range min  max considered
following operations 
value observed training set attribute within allowable range
outside actual range values objects l  evidence evaluated
support reclassifying region threshold  level support
given threshold evaluated using laplacian accuracy estimate  niblett   bratko        
leaf relates binary classification  an object belongs class question
not   binary form laplace used  threshold attribute leaf l 
evidence support labeling partition class n maximum value
ancestor node x l formula
p   
  
number objects x min   t  p number
objects belong class n 
evidence support labeling partition threshold calculated identically
exception objects   max instead considered 
maximum evidence new labeling exceeds evidence current labeling
region  new branch added appropriate threshold creating new leaf node
labeled appropriate class 
addition evidence favor current labeling gathered above  evidence support current labeling region calculated using laplace accuracy
   

fifurther experimental evidence utility occam s razor

estimate considering objects leaf  number objects leaf
p number objects belong class node labeled 
approach ensures new partitions define true regions  is 
attribute value v possible partition v unless possible
objects domain values greater v objects values less
equal v reach node partitioned  even though objects
training set fall within new partition   particular  ensures new cuts
simple duplications existing cuts ancestors current node  thus  every
modification adds non redundant complexity tree 
algorithm presented figure    implemented modification
c    release    called c   x  source code modifications available
on line appendix paper 
c   x  multiple sets values equally satisfy specified constraints
maximize laplace function  values na nb deeper tree selected
closer root and  single node  preference values aa ab depends
upon order attributes definition data preference values va
vb dependent upon data order  selection strategies side effect
implementation system  reason believe experimental results
would differ general strategies used select competing constraints 
default  c    develops two decision trees time run  unpruned
pruned  simplified  decision tree  c   x produces post processed versions
trees 

    evaluation
evaluate post processor applied datasets containing continuous attributes
uci machine learning repository  murphy   aha        held  due
previous machine learning experimentation  local repository deakin university 
datasets believed broadly representative repository
whole  experimentation eleven data sets  two additional data sets  sick
euthyroid discordant results  retrieved uci repository added
study order investigate specific issues  discussed below 
resulting thirteen datasets described table    second column contains
number attributes object described  next proportion
continuous  fourth column indicates proportion attribute values
data missing  unknown   fifth column indicates number objects
data set contains  sixth column indicates proportion belong
class represented objects within data set  final column indicates
number classes data set describes  note glass type dataset uses
float not float other three class classification rather commonly used six
class classification 
data set divided training evaluation sets     times  training
set consisted     data  randomly selected  evaluation set consisted
remaining     data  c    c   x applied resulting     
    data sets     trials  training evaluation set pairs 
   

fiwebb

let cases n  denote set training examples reach node n 
let value a  x  denote value attribute training example x 
let pos x  c  denote number objects class c set training examples x 
let laplace x  c            x set training examples  jx j number training
examples c class 
let upperlim n  a  denote minimum value cut attribute ancestor node n
n lies branch  cut  upperlim n  a       determines
upper bound values may reach n 
let lowerlim n  a  denote maximum value cut attribute ancestor node n
n lies   branch  cut  lowerlim n  a        determines
lower bound values may reach n 
post process leaf l dominated class c
   find values
n   n ancestor l
  continuous attribute
v    x   x   cases n     v   value a   x    v min v    y     cases l    v  
value a   y     v   lowerlim l   
c   c class
maximize l   laplace fx   x   cases n     value a   x  v   value a   x   
lowerlim l   g  c   
   find values
n   n ancestor l
  continuous attribute
v    x   x   cases n     v   value a   x    v   max v    y     cases l    v  
value a   y     v upperlim l   
c   c class
maximize l   laplace fx   x   cases n     value a   x    v   value a   x 
upperlim l   g  c   
   l   laplace cases l   c    l l
 a  c    c
i  replace l node n test v  
ii  set branch n lead new leaf class c  
iii  set   branch n lead l 
else l   laplace cases l   c 
 b  c    c
i  replace l node n test v  
ii  set   branch n lead new leaf class c  
iii  set branch n lead l 
pos x c
jx j

































b

b

b

b



b

b

b

b







b

b

b



b

b

b

b

b

b

b

b



b

b



b









b

b

b

b

b

figure    c   x post processing algorithm
   

fifurther experimental evidence utility occam s razor

table    uci data sets used experimentation
 
 
no  contin 
no  common no 
name
attrs  uous missing objects class classes
breast cancer wisconsin
 
   
  
   
  
 
cleveland heart disease
  
  
  
   
  
 
credit rating
  
  
 
   
  
 
discordant results
  
  
 
    
  
 
echocardiogram
 
  
 
  
  
 
glass type
 
   
 
   
  
 
hepatitis
  
  
 
   
  
 
hungarian heart disease
  
  
  
   
  
 
hypothyroid
  
  
 
    
  
 
iris
 
   
 
   
  
 
new thyroid
 
   
 
   
  
 
pima indians diabetes
 
   
 
   
  
 
sick euthyroid
  
  
 
    
  
 
table   summarizes percentage predictive accuracy obtained unpruned decision trees generated c    c   x  presents mean  x  standard
deviation  s  set     trials respect data set c   
c   x along results two tailed matched pairs t test comparing means 
twelve thirteen data sets c   x obtained higher mean accuracy c    
remaining data set  hypothyroid  c    obtained higher mean predictive accuracy
c   cs  albeit small margin measured two decimal places respective mean accuracies              respectively   nine data sets advantage toward
c   x statistically significant      level  p        although advantage
respect discordant results data small apparent measured one
decimal place  measured two decimal places values             respectively  
advantage toward c    hypothyroid data statistically significant
     level  differences mean predictive accuracy hungarian heart disease 
new thyroid sick euthyroid data sets significant      level 
table   uses format table   summarize predictive accuracy obtained
pruned decision trees generated c    c   x  twelve data
sets c   x obtained higher mean predictive accuracy c     remaining data
set  hypothyroid  c    obtained higher mean predictive accuracy  although
magnitude difference small apparent level precision
displayed  measured two decimal places mean accuracies              
six data sets advantage toward c   x statistically significant     
level  although difference apparent precision two decimal places
discordant results data               respectively   advantage toward c   
hypothyroid data statistically significant      level  differences
   

fiwebb

table    percentage predictive accuracy unpruned decision trees 
name
breast cancer wisconsin
cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
hungarian heart disease
hypothyroid
iris
new thyroid
pima indians diabetes
sick euthyroid

c   

x

    
    
    
    
    
    
    
    
    
    
    
    
    



   
   
   
   
   
   
   
   
   
   
   
   
   

c   x

x





p

                   
                   
                   
                   
                    
                   
                   
                   
                  
                   
                   
                   
                   

table    percentage accuracy pruned decision trees 
name
breast cancer wisconsin
cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
hungarian heart disease
hypothyroid
iris
new thyroid
pima indians diabetes
sick euthyroid

c   

x

    
    
    
    
    
    
    
    
    
    
    
    
    



   
   
   
   
   
   
   
   
   
   
   
   
   

c   x

x

    
    
    
    
    
    
    
    
    
    
    
    
    



   
   
   
   
   
   
   
   
   
   
   
   
   



p

          
          
          
          
           
          
          
          
         
          
          
          
          

breast cancer wisconsin  echocardiogram  hungarian heart disease  iris  new thyroid
sick euthyroid statistically significant      level 
completing experimentation initial eleven data sets  results
hypothyroid data stood stark contrast ten  raised
possibility might distinguishing features hypothyroid data
   

fifurther experimental evidence utility occam s razor

accounted difference performance  table   indicates data set clearly
distinguishable ten initial data sets following six respects 

attributes 
containing greater proportion discrete attributes  which directly addressed
c   x  






containing objects 
greater proportion objects belong common class 
classes 
producing decision trees extremely high predictive accuracy without post processing 

explore issues discordant results sick euthyroid data sets retrieved
uci repository added study  data sets identical
hypothyroid data set exception different class attribute  three
data sets contain objects  described attributes  addition
discordant results sick euthyroid data little illuminate issue however 
three data sets changes accuracy small magnitude  hypothyroid
significant advantage c     sick euthyroid significant advantage
either system  discordant results data significant advantage c   x 
question whether distinguishing feature hypothyroid data
explains observed results remains unanswered  investigation issue lies
beyond scope current paper remains interesting direction future research 
results suggest c   x s post processing frequently increases predictive
accuracy type data found uci repository   of twenty six
comparisons  significant increase fifteen significant decrease
two  sign test reveals rate success significant      level 
p          
tables     summarize number nodes decision trees developed  table  
addresses unpruned decision trees table   addresses pruned decision trees  postprocessing modification replaces single leaf split two leaves  one
modification performed per leaf original tree  data sets postprocessed decision trees significantly complex original decision trees 
cases post processing increased mean number nodes decision trees
approximately      demonstrates post processing causing substantial
change 

   discussion

primary objective research discredit occam thesis 
end uses post processor disregards occam thesis instead theoretically
founded upon similarity assumption  experimentation post processor
   

fiwebb

table    number nodes unpruned decision trees 
c   
c   x
name
x

x


p
breast cancer wisconsin                               
cleveland heart disease                                
credit rating
                                 
discordant results
                               
echocardiogram
                             
glass type
                             
hepatitis
                             
hungarian heart disease                               
hypothyroid
                             
iris
                            
new thyroid
                             
pima indians diabetes                                   
sick euthyroid
                               
table    number nodes pruned decision trees 
c   

c   x

name
x

x
breast cancer wisconsin              
cleveland heart disease              
credit rating
              
discordant results
             
echocardiogram
             
glass type
             
hepatitis
             
hungarian heart disease               
hypothyroid
             
iris
            
new thyroid
             
pima indians diabetes                 
sick euthyroid
             



   
    
    
   
   
   
   
    
   
   
   
    
   



     
     
     
     
     
     
     
     
     
     
     
     
     

p

     
     
     
     
     
     
     
     
     
     
     
     
     

demonstrated possible develop systematic procedures that  range  realworld  learning tasks increase predictive accuracy inferred decision trees result
changes substantially increase complexity without altering performance
upon training data 
is  general  dicult attack occam thesis due absence widely
agreed formulation thereof  however  far apparent occam thesis might
   

fifurther experimental evidence utility occam s razor

          
                       
          
              
            
 
 
 
 
 
                    

figure    modified simple instance space
  
 
 
 
 
b 
 
 
 
 

recast accommodate experimental results provide practical learning
bias 

    directions future research
implications research reach beyond relevance occam s razor  postprocessor appears practical utility increasing quality inferred decision trees 
however  objective research improve predictive accuracy rather
discredit occam thesis  post processor would modified number ways 
first modification would enable addition multiple partitions single
leaf original tree  c   x selects single modification
maximum support  design decision originated desire minimize likelihood
performing modifications decrease accuracy  principle  however  would
appear desirable select modifications strong support 
could inserted tree order level supporting evidence 
even greater increases accuracy might expected one removed constraint
post processing alter performance decision tree respect
training set  case  new partitions may well found employ objects
regions instance space provide evidence support adding partitions
correct misclassifications small numbers objects leaf node original tree 
similarity assumption would provide strong evidence repartitioning 
situation would occur  example  respect learning problem illustrated
figure    additional object class   attribute values a   b   
illustrated figure    case c    would still create indicated partitions 
however  c   x would unable relabel area containing additional object due
constraint alter performance original decision tree respect
training set  thus addition object prevents c   x relabeling shaded
region even though  basis similarity assumption  improves evidence
support relabeling 
extended post processor would encourage following model inductive inference decision trees  role c     or similar system  would identify clusters
   

fiwebb

objects within instance space grouped single leaf node 
second stage would analyze regions instance space lie outside clusters
order allocate classes regions  current decision tree learners  motivated
occam thesis  ignore second stage  leaving regions outside identified clusters
associated whatever classes assigned by product cluster
identification process 

    related research
number researchers developed learning systems viewed considering
evidence neighboring regions instance space order derive classifications
within regions instance space occupied examples training
set  ting        explicitly  examining training set directly explore
neighborhood object classified  system uses instance based learning
classification within nodes decision tree low empirical support  small disjuncts  
number systems viewed considering evidence neighboring
regions classification  systems learn apply multiple classifiers  ali 
brunk    pazzani        nock   gascuel        oliver   hand         context 
point within region instance space occupied training objects
likely covered multiple leaves rules  these  leaf rule greatest
empirical support used classification 
c   x uses two distinct criteria evaluating potential splits  standard c    stage
tree induction employs information measure select splits  post processor uses
laplace accuracy estimate  similar uses dual criteria investigated elsewhere 
quinlan        employs laplace accuracy estimate considering neighboring regions
instance space estimate accuracy small disjuncts  lubinsky        brodley
       employ resubstitution accuracy select splits near leaves induction
decision trees 
adding split leaf  c   x specializing respect class leaf
 and generalizing respect class new leaf   holte et al         explored
number techniques specializing small disjuncts  c   x differs leaves
candidates specialization  low empirical support  differs
manner selects specialization perform considering evidence
support alternative splits rather strength evidence support
individual potential conditions current disjunct 

    bias versus variance
breiman  friedman  olshen  stone        provide analysis complexity induction terms trade off bias variance  classifier partitions instance
space regions  regions large  degree fit accurate partitioning instance space poor  increasing error rates  effect called bias 
regions small  probability individual regions labeled
wrong class increased  effect  called variance  increases error rates  according
analysis  due variance  fine partitioning instance space tends increase
   

fifurther experimental evidence utility occam s razor

error rate while  due bias  coarse partitioning tends increase error
rate 
increasing complexity decision tree creates finer partitionings instance
space  analysis used argue addition undue complexity
decision trees ground increase variance hence error rate 
however  success c   x decreasing error rate demonstrates
successfully managing bias variance trade off introduces complexity
decision tree  using evidence neighboring regions instance space  c   x
successful increasing error rate resulting variance lower rate
decreases error rate resulting bias  success c   x demonstrates
adding undue complexity c    s decision trees 

    minimum encoding length induction

minimum encoding length approaches perform induction seeking theory enables
compact encoding theory available data  two key approaches
developed  minimum message length  mml   wallace   boulton       
minimum description length  mdl   rissanen         approaches admit probabilistic interpretations  given prior probabilities theories data  minimization
mml encoding closely approximates maximization posterior probability  wallace   freeman         mdl code length defines upper bound  unconditional
likelihood   rissanen        
two approaches differ mdl employs universal prior  rissanen       
explicitly justifies terms occam s razor  mml allows specification distinct
appropriate priors induction task  however  practice  default prior usually
employed mml  one appears derive justification occam s razor 
neither mdl mml default prior would add complexity decision tree
justified solely basis evidence neighboring regions
instance space  evidence study presented herein appears support
potential desirability so  casts doubt upon utility universal
prior employed mdl default prior usually employed mml  least
respect use maximizing predictive accuracy 
noted  however  probabilistic interpretation minimum
encoding length techniques indicates encoding length minimization represents maximization posterior probability unconditional likelihood  maximization
factors necessarily directly linked maximizing predictive accuracy 

    appropriate application grafting pruning

important note although paper calls question value learning
biases penalize complexity  way provide support learning biases
encourage complexity sake  c   x grafts new nodes onto decision tree
empirical support so 
results way argue appropriate use decision tree pruning 
generate pruned trees  c    removes branches statistical estimates upper
bounds error rates indicate increase branch removed 
   

fiwebb

could argued c    reduces complexity empirical support
so  interesting note eight thirteen data sets examined  c   x s
post processing pruned trees resulted higher average predictive accuracy
post processing unpruned trees  results suggest pruning grafting
play valuable role applied appropriately 

   conclusion
paper presents systematic procedure adding complexity inferred decision trees
without altering performance training data  procedure demonstrated lead increases predictive accuracy range learning tasks applied
pruned unpruned trees inferred c     one thirteen learning
tasks examined procedure lead statistically significant loss accuracy
case magnitude difference mean accuracy extremely small 
face it  provides strong experimental evidence occam thesis 
post processing technique developed rejecting occam thesis instead attending similarity assumption that similar objects high probability
belonging class 
procedure developed constrained need ensure revised decision
tree performed identically original decision tree respect training data 
constraint arose desire obtain experimental evidence occam
thesis  possible constraint removed  basic techniques outlined
paper could result even greater improvements predictive accuracy reported
herein 
research considered one version occam s razor favors minimization
syntactic complexity expectation tend increase predictive accuracy 
interpretations occam s razor possible  one minimize
semantic complexity  others  bunge        provided philosophical objections
formulations occam s razor  paper sought investigate them 
version occam s razor examined research used widely machine
learning apparent success  objections principle substantiated research raise question  apparent success
awed  webb        suggests apparent success principle due
manner syntactic complexity usually associated relevant qualities
inferred classifiers generality prior probability  thesis accepted one
key challenges facing machine learning understand deeper qualities
employ understanding place machine learning sounder theoretical footing 
paper offers small contribution direction demonstrating minimization
surface syntactic complexity not  itself  general maximize predictive accuracy
inferred classifiers 
nonetheless important realize that  thrust paper notwithstanding 
occam s razor often useful learning bias employ  frequently good pragmatic reasons preferring simple hypothesis  simple hypothesis
general easier understand  communicate employ  preference simple
   

fifurther experimental evidence utility occam s razor

hypotheses cannot justified terms expected predictive accuracy may justified
pragmatic grounds 

acknowledgements
research supported australian research council  grateful
charlie clelland  david dowe  doug newlands  ross quinlan anonymous reviewers
extremely valuable comments paper benefited greatly 

references

aha  d  w   kibler  d     albert  m  k          instance based learning algorithms 
machine learning           
ali  k   brunk  c     pazzani  m          learning multiple descriptions concept 
proceedings tools artificial intelligence new orleans  la 
berkman  n  c     sandholm  t  w          minimized decision tree 
re examination  technical report        university massachusetts amherst 
computer science department  amherst  mass 
blumer  a   ehrenfeucht  a   haussler  d     warmuth  m  k          occam s razor 
information processing letters              
breiman  l   friedman  j  h   olshen  r  a     stone  c  j          classification
regression trees  wadsworth international  belmont  ca 
brodley  c  e          automatic selection split criterion tree growing based
node selection  proceedings twelth international conference machine
learning  pp        taho city  ca  morgan kaufmann 
bunge  m          myth simplicity  prentice hall  englewood cliffs  nj 
clark  p     niblett  t          cn  induction algorithm  machine learning    
        
fayyad  u  m     irani  k  b          minimized decision tree 
aaai     proceedings eighth national conference artificial intelligence  pp 
        boston  ma 
good  i  j          explicativity  mathematical theory explanation statistical
applications  proceedings royal society london series a               
holte  r  c          simple classification rules perform well commonly used
datasets  machine learning                
holte  r  c   acker  l  e     porter  b  w          concept learning problem
small disjuncts  proceedings eleventh international joint conference
artificial intelligence  pp          detroit  morgan kaufmann 
   

fiwebb

lubinsky  d  j          increasing performance consistency classification trees
using accuracy criterion leaves  proceedings twelth international
conference machine learning  pp          taho city  ca  morgan kaufmann 
michalski  r  s          theory methodology inductive learning  michalski 
r  s   carbonell  j  g     mitchell  t  m   eds    machine learning  artificial
intelligence approach  pp          springer verlag  berlin 
murphy  p  m          empirical analysis benefit decision tree size biases
function concept distribution  tech  rep         department information
computer science  university california  irvine 
murphy  p  m     aha  d  w          uci repository machine learning databases 
 machine readable data repository   university california  department information computer science  irvine  ca 
murphy  p  m     pazzani  m  j          exploring decision forest  empirical investigation occam s razor decision tree induction  journal artificial intelligence
research             
niblett  t     bratko  i          learning decision rules noisy domains  bramer 
m  a   ed    research development expert systems iii  pp         cambridge
university press  cambridge 
nock  r     gascuel  o          learning decision committees  proceedings
twelth international conference machine learning  pp          taho city  ca 
morgan kaufmann 
oliver  j  j     hand  d  j          pruning averaging decision trees  proceedings
twelth international conference machine learning  pp          taho city 
ca  morgan kaufmann 
pearl  j          connection complexity credibility inferred
models  international journal general systems             
quinlan  j  r          induction decision trees  machine learning            
quinlan  j  r          learning logical definitions relations  machine learning    
        
quinlan  j  r          improved estimates accuracy small disjuncts  machine
learning           
quinlan  j  r          c     programs machine learning  morgan kaufmann  los
altos 
rao  r  b   gordon  d     spears  w          every generalization action really
equal opposite reaction  analysis conservation law generalization performance  proceedings twelth international conference machine
learning  pp          taho city  ca  morgan kaufmann 
   

fifurther experimental evidence utility occam s razor

rendell  l     seshu  r          learning hard concepts constructive induction 
framework rationale  computational intelligence             
rissanen  j          universal prior integers estimation minimum description
length  annals statistics              
rissanen  j          stochastic complexity  journal royal statistical society series
b                  
schaffer  c          sparse data effect overfitting avoidance decision tree
induction  aaai     proceedings tenth national conference artificial
intelligence  pp          san jose  ca  aaai press 
schaffer  c          overfitting avoidance bias  machine learning              
schaffer  c          conservation law generalization performance  proceedings
     international conference machine learning san mateo  ca  morgan
kaufmann 
ting  k  m          problem small disjuncts  remedy decision trees 
proceedings tenth canadian conference artificial intelligence  pp        
morgan kaufmann  
wallace  c  s     boulton  d  m          information measure classification  computer journal              
wallace  c  s     freeman  p  r          estimation inference compact coding 
journal royal statistical society series b                  
webb  g  i          generality significant complexity  toward alternatives
occam s razor  zhang  c   debenham  j     lukose  d   eds    ai      proceedings seventh australian joint conference artificial intelligence  pp       
armidale  world scientific 

   


