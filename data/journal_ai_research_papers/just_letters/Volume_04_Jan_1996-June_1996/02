journal artificial intelligence research               

submitted       published     

logarithmic time updates queries
probabilistic networks
arthur l  delcher

computer science department  loyola college maryland
baltimore  md      

adam j  grove

delcher cs loyola edu

grove research nj nec com

nec research institute
princeton  nj      

simon kasif

kasif cs jhu edu

judea pearl

pearl lanai cs ucla edu

department computer science  johns hopkins university
baltimore  md      
department computer science  university california
los angeles  ca      

abstract

traditional databases commonly support ecient query update procedures
operate time sublinear size database  goal paper
take first step toward dynamic reasoning probabilistic databases comparable
eciency  propose dynamic data structure supports ecient algorithms
updating querying singly connected bayesian networks  conventional algorithm 
new evidence absorbed time o    queries processed time o n    n
size network  propose algorithm which  preprocessing phase 
allows us answer queries time o log n   expense o log n   time per evidence
absorption  usefulness sub linear processing time manifests applications
requiring  near  real time response large probabilistic databases  brie discuss
potential application dynamic probabilistic reasoning computational biology 

   introduction
probabilistic  bayesian  networks increasingly popular modeling technique
used successfully numerous applications intelligent systems real time planning navigation  model based diagnosis  information retrieval  classification  bayesian
forecasting  natural language processing  computer vision  medical informatics computational biology  probabilistic networks allow user describe environment using
 probabilistic database  consists large number random variables  corresponding important parameter environment  random variables could
fact hidden may correspond unknown parameters  causes  uence
observable variables  probabilistic networks quite general store information
probability failure particular component computer system  prob c      ai access foundation morgan kaufmann publishers  rights reserved 

fidelcher  grove  kasif   pearl

ability page computer cache requested near future  probability
document relevant particular query  probability amino acid
subsequence protein chain folding alpha helix conformation 
applications mind include networks dynamically maintained
keep track probabilistic model changing system  instance  consider task
automated detection power plant failures  might repeat cycle consists
following sequence operations  first perform sensing operations  operations
cause updates performed specific variables probabilistic database  based
evidence estimate  query  probability failure certain sites  precisely 
query probability distribution random variables measure probability
failure sites based evidence  since plant requires constant monitoring 
must repeat cycle sense evaluate frequent basis 
conventional  non probabilistic  database tracking plant s state would
appropriate here  possible directly observe whether failure
occur  hand  probabilistic  database  based bayesian network
useful operations update query can performed quickly 
real time near real time often necessary  question extremely
fast reasoning probabilistic networks important 
traditional  non probabilistic  databases support ecient query update procedures
often operate time sublinear size database  e g   using binary search   goal paper take step toward systems perform
dynamic probabilistic reasoning  such probability event given set
observations  time sublinear size probabilistic network  typically 
sublinear performance complex networks attained using parallelism  paper
relies preprocessing 
specifically  describe new algorithms performing queries updates belief
networks form trees  causal trees  polytrees join trees   define two natural
database operations probabilistic networks 
  

update node

  perform sensory input  modify evidence leaf node  single
variable  network absorb evidence network 

  

query node

  obtain marginal probability distribution values
arbitrary node  single variable  network 

standard algorithms introduced pearl        perform query node operation o    time although evidence absorption  i e   update node operation  takes
o n   time n size network  alternatively  one assume
update node operation takes o    time  by simply recording change  querynode operation takes o n   time  evaluating entire network  
paper describe approach perform queries updates o log n  
time  significant systems since improve ability system
respond change encountered o n   time o log n    approach
based preprocessing network using form node absorption carefully structured
way create hierarchy abstractions network  previous uses node absorption
techniques reported peot shachter        
  

fiqueries   updates probabilistic networks

note measuring complexity terms size network  n  
overlook important factors  suppose variable network domain
size k less  many purposes  k considered constant  nevertheless 
algorithms consider slowdown power k  become
significant practice unless n large  thus careful state slowdown
exists 
section   considers case causal trees  i e   singly connected networks
node one parent  standard algorithm  see pearl        must use o k  n  
time either updates retrieval  although one operations done
o    time  discuss brie section      straightforward variant
algorithm takes o k d  time queries updates  height
tree 
present algorithm takes o k  log n   time updates o k  log n  
time queries causal tree  course represent tremendous speedup 
especially large networks  algorithm begins polynomial time preprocessing
step  linear size network   constructing another data structure  which
probabilistic tree  supports fast queries updates  techniques use
motivated earlier algorithms dynamic arithmetic trees  involve  caching  sucient intermediate computations update phase querying relatively
easy  note  however  substantial interesting differences
algorithm probabilistic networks arithmetic trees  particular 
apparent later  computation probabilistic trees requires bottom up top down
processing  whereas arithmetic trees need former  perhaps even interesting relevant probabilistic operations different algebraic structure
arithmetic operations  for instance  lack distributivity  
bayesian trees many applications literature including classification 
instance  one popular methods classification bayes classifier
makes independence assumption features used perform classification
 duda   hart        rachlin  kasif  salzberg    aha         probabilistic trees
used computer vision  hel or   werman        chelberg         signal processing
 wilsky         game playing  delcher   kasif         statistical mechanics  berger
  ye         nevertheless  causal trees fairly limited modeling purposes  however
similar structures  called join trees  arise course one standard algorithms
computing arbitrary bayesian networks  see lauritzen spiegelhalter         thus
algorithm join trees potential relevance many networks trees 
join trees special structure  allow optimization basic
causal tree algorithm  elaborate section   
section   consider case arbitrary polytrees  give o log n   algorithm updates queries  involves transforming polytree join tree 
using results sections      join tree polytree particularly
simple form  giving algorithm updates take o kp   log n   time queries
o kp   log n    p maximum number parents node  although
constant appears large  must noted original polytree takes o kp   n   space
merely represent  conditional probability tables given explicit matrices 
  

fidelcher  grove  kasif   pearl


u

 
 

mv ju   
 

 


 
v


 



  x ju
 
 
r
 


x

 
 

jx  




 

 



  z jx
 
r
 


z


figure    segment causal tree 
finally  discuss specific modelling application computational biology probabilistic models used describe  analyze predict functional behavior biological sequences protein chains dna sequences  see delcher  kasif  goldberg 
hsu       references   much information computational biology databases
noisy  however  number successful attempts build probabilistic models
made  case  use probabilistic tree depth     consists     nodes
matrices conditional probabilities      tree used model dependence
protein s secondary structure chemical structure  detailed description
problem experimental results given delcher et al          problem
obtain effective speed up factor    perform update compared
standard algorithm  clearly  getting order magnitude improvement response
time probabilistic real time system could tremendous importance future use
systems 

   causal trees

probabilistic causal tree directed tree node represents discrete random
variable x   directed edge annotated matrix conditional probabilities
jx  associated edge x      is  x possible value x  y 
 x   th component jx pr y   jx   x   tree represents joint
probability distribution product space variables  detailed definitions
discussion see pearl         brie y  idea consider product  nodes 
conditional probability node given parents  example  figure  
implied distribution is 
pr u   u  v   v  x   x    y  z   z    
pr u   u  pr v   v ju   u  pr x   xju   u  pr y   jx   x  pr z   z jx   x  
given particular values u  v  x  y  z  conditional probabilities read
appropriate matrices   one advantage product representation
  

fiqueries   updates probabilistic networks

concise  example  need four matrices unconditional probability u  
size square largest variable s domain size  contrast 
general distribution n variables requires exponential  in n   representation 
course  every distribution represented causal tree  turns
product decomposition implied tree corresponds particular pattern
conditional independencies often hold  if perhaps approximately  real
applications  intuitively speaking  figure   implied independencies
conditional probability u given v   x   z depends values v
x   probability given u   v   x   z depends x   independencies
sort arise many reasons  instance causal modeling interactions
variables  refer reader pearl        details related modeling
independence assumptions using graphs 
following  make several assumptions significantly simplify presentation  sacrifice generality  first  assume variable ranges
same  constant  number values k   follows marginal probability distribution
variable viewed k dimensional vector  conditional probability
matrix jx square k k matrix  common case binary random
variables  k       distribution values  true  false   p      p 
probability p 
next assumption tree binary  complete  node  
  children  tree converted form  doubling number
nodes  instance  suppose node p children c    c   c  original tree 
create another  copy  p  p   rearrange tree two children p
c  p   two children p  c  c   constrain p  always
value p simply choosing identity matrix conditional probability table
p p    distribution represented new tree effectively
original  similarly  always add  dummy  leaf nodes necessary ensure
node two children  explained introduction  interested processes
certain variables  values observed  upon wish condition  final
assumption observed evidence nodes leaves tree  again 
possible  copy  nodes add dummy nodes  restrictive 
product distribution alluded corresponds distribution variables
prior observations  practice  interested conditional distribution 
simply result conditioning observed evidence  which  earlier
assumption  corresponds seeing values leaf nodes   thus  non leaf node
x interested conditional marginal probability x   i e   k dimensional
vector 
bel  x     pr x j evidence values  
main algorithmic problem compute bel  x    non evidence  node x
tree given current evidence  well known probability vector bel  x  
computed linear time  in size tree  popular algorithm based
   assumption nonrestrictive add  dummy  values variable s range 
given conditional probability    nevertheless  may computational advantage
allowing different variable domain sizes  changes required permit dicult  since
complicate presentation somewhat omit them 

  

fidelcher  grove  kasif   pearl

following equation 
bel  x     pr x j evidence     x    x  
normalizing constant   x   probability evidence subtree
node x given x    x   probability x given evidence rest
tree  interpret equation  note x    x   x          xk    y   y    y           yk  
two vectors define operation component wise product  pairwise
dyadic product vectors  
x    x y   x y          xkyk   
usefulness  x    x   derives fact computed recursively  follows 
   x root node   x   prior probability x  
   x leaf node   x   vector   ith position  where ith value
observed    elsewhere  value x observed   x  
vector consisting   s  
   otherwise  if  shown figure    children node x z   sibling
v parent u   have 
 x      my jx  y     mzjx  z   


 x     mx ju  u    mv ju  v   


presentation technique follows pearl         however  use
somewhat different notation don t describe messages sent parents successors  rather discuss direct relations among vectors terms simple
algebraic equations  take advantage algebraic properties equations
development 
easy see equations evaluated time proportional
size network  formal proof given pearl        
theorem    belief distribution every variable  that is  marginal probability
distribution variable  given evidence  causal tree evaluated
o k n   time n size tree   the factor k  due multiplication
matrix vector must performed node  
theorem shows possible perform evidence absorption o n   time 
queries constant time  i e   retrieving previously computed values lookup
table   next sections show perform queries updates
worst case o log n   time  intuitively  recompute marginal distributions
update  rather make small number changes  sucient  however 
compute value variable logarithmic delay 
   set   components corresponding possible values this especially useful
observed variable part joint tree clique  section     general   x   thought
likelihood vector x given observations x  

  

fiqueries   updates probabilistic networks

    simple preprocessing approach

obtain intuition new approach begin simple observation 
consider causal tree depth d  node x tree initially compute
 x   vector  vectors left uncomputed  given update node   calculate
revised  x   vectors nodes x ancestors tree  clearly
done time proportional depth tree  i e   o d   rest information
tree remains unchanged  consider query node operation node v
tree  obviously already accurate  v   vector every node tree
including v   however  order compute  v   vector need compute
 y   vectors nodes v tree multiply appropriate
vectors kept current  means compute accurate  v   vector
need perform o d  work well  thus  approach don t perform complete
update every  x    x   vector tree 
lemma    update node query node operations causal tree performed o k  d  time depth tree 
implies tree balanced  operations done o log n  
time  however  important applications trees balanced  e g   models
temporal sequences  delcher et al          obvious question therefore is  given causal
tree produce equivalent balanced tree    answer question
appears dicult  possible use sophisticated approach produce data
structure  which causal tree  process queries updates o log n   time 
approach described subsequent sections 

    dynamic data structure causal trees

data structure allow ecient incremental processing probabilistic tree  
t  sequence trees  t   t   t          ti         tlog n   ti   contracted
version ti  whose nodes subset ti   particular  ti   contain
half many leaves predecessor 
defer details contraction process next section  however  one key
idea maintain consistency  sense bel  x     x     x   given
values trees x appears  choose conditional probability
matrices contracted trees  i e   trees t    ensure this 
recall equations form

 x      my jx  y     mzjx  z   


 x     mx ju  u    mv ju  v   


z children x   x right child u   v x  s sibling  figure    
however  equations convenient form following notational
conventions helpful  first  let ai  x   resp   bi  x   denote conditional
probability matrix x x  s left  resp   right  child tree ti  note
identity children differ tree tree  x  s original children
might removed contraction process  one advantage new notation
  

fidelcher  grove  kasif   pearl

uj

ti

   
 
 
 
 

vj

e

xj

rake

   
 
 

p p zjp p

 e  x 

 

uj

ti  

   
 
 
 
 

vj

p p zjp p

figure    effect operation rake  e  x   e must leaf  z may may
leaf 
explicit dependence identity children suppressed  next  suppose x  s
parent ti u  let ci  x  denote either ai  u  bi  u   di x  denote either
bi  u  ai u    depending whether x right left child  respectively  u  
necessary keep careful track correspondences  simply note
equations become  
 x    ai x   y  bi  x   z 
 x    di x    u  ci x   v  
next section describe preprocessing step creates dynamic data
structure 


   

rake



operation

basic operation used contract tree rake removes leaf
parent tree  effect operation tree shown figure   
define algebraic effect operation equations associated tree 
recall want define conditional probability matrices raked tree
distribution remaining variables unchanged  achieve substituting
equations  x   x  equations  u    z     v    following 
important note  u    z    v   unaffected rake operation 
following  let diagff denote diagonal matrix whose diagonal entries
components vector ff  derive algebraic effect rake operation follows 
 u    ai  u   v  bi  u   x 
  ai  u   v   bi  u   ai  x   e  bi  x   z   
  ai  u   v   bi  u  diaga  x  e  bi  x   z  


  ai  u   v   bi  u  diaga  x  e  bi  x   z  
  ai    u   v   bi    u   z  
ai    u    ai  u  bi    u    bi  u  diaga  x  e  bi  x    of course  case
leaf raked right child generates analogous equations   thus  defining






   throughout  assume lower precedence matrix multiplication  indicated   

  

fiqueries   updates probabilistic networks

ai   u  bi    u  way  ensure values raked tree identical

corresponding values original tree  yet enough  must
check values similarly preserved  two values could possibly change
 z    v    check both  former  must

 z    di z    x  ci z   e  
  di    z      u  ci   z    v     
substituting  x  algebraic manipulation  see assured
ci   z    ci x  di   z    di z  diagc  z  e  di x   however recall that  definition  ci    z     ai    u  ci  x    ai  u   ci    z     ci x  follows  furthermore 
di   z    bi   u 
   bi  u  diaga  x  e  bi  x  
  bi  x  diaga  x  e  bi  u 
  di z   diagc  z  e  di  x 
















required 
 v   necessary verify

 v    di v    u  ci v   x  
  di    v      u  ci    v    z     
substituting  x   shown true di   v     di  v     ai  u   
ai   u  ci   v    ci v  diaga  x  e  bi x    bi   u   identities follow






definition  done 
beginning given tree   t   successive tree constructed performing
sequence rakes  rake away half remaining evidence nodes 
specifically  let contract operation apply rake operation every
leaf causal tree  left to right order  excluding leftmost rightmost
leaf  let ftig set causal trees constructed ti   causal tree generated
ti single application contract  following result proved using easy
inductive argument 

theorem    let t  causal tree size n   number leaves ti   equal

half leaves ti  not counting two extreme leaves  starting t  
o log n   applications contract  produce three node tree  root 
leftmost leaf rightmost leaf 
observations process 
   complexity contract linear size tree  additionally  log n applications contract reduce set tree equations single equation involving
root o n   total time 
   total space store sets equations associated fti g ilog n
twice space required store equations t  
  

fidelcher  grove  kasif   pearl

   equation ti   store equations describe relationship
conditional probability matrices ti   matrices ti   notice
that  even though ti   produced ti series rake operations  matrix
ti   depends directly matrices present ti  would case
attempted simultaneously rake adjacent children 
regard equations part ti    so  formally speaking ftig causal trees
augmented auxiliary equations  contracted trees describes
probability distribution subset first set variables consistent
original distribution 
note ideas behind rake operation originally developed miller
reif        context parallel computation bottom up arithmetic expression
trees  kosaraju   delcher        karp   ramachandran         contrast  using
context incremental update query operations sequential computing 
similar data structure independently proposed frederickson       
context dynamic arithmetic expression trees  different approach incremental
computing arithmetic trees developed cohen tamassia        
important interesting differences arithmetic expression tree case
own  arithmetic expressions computation done bottom up  however  probabilistic networks  messages must passed top down  furthermore  arithmetic expressions
two algebraic operations allowed  typically require distributivity one
operation other  analogous property hold us  respects approach substantial generalization previous work  remaining
conceptually simple practical 

   example  chain

obtain intuition algorithms  sketch generate utilize
ti    log n equations perform  value queries updates o log n  
time n    l     node chain length l  consider chain length   figure   
trees generated repeated application contract chain 
equations correspond contracted trees figure follows  ignoring trivial equations   recall ai  xj   matrix associated left edge
random variable xj ti 

 x  
 x  
 x  
 x  

 
 
 
 

a  x    e   b  x    x  
a  x    e   b  x    x  
a  x    e   b  x    x  
a  x    e   b  x    e  

 
 
 
 
 
 
 
 
 
 
 
b   x   diaga   x   e   b  x    
 
 
b   x   diaga   x   e   b  x    

 x     a  x    e   b  x    x  
 x     a  x    e   b  x    e  


b  x    
b  x    

 
 
 
 
 
 
 
 
 

  

t 

t 

fiqueries   updates probabilistic networks

t   

xm
 
em
 
 

t   

xm
 
em
 
 

t   

xm
 

xm   xm
    xm
    e m

   

em
 

em
 

 

em
 

 

 

xm   em
 

   

em
 
 

em

   

em
 
 

figure    simple chain example 

 x     a  x    e   b  x    e  

 
 
 
 
 
 
 


t 
b  x     b   x   diaga  x   e   b  x  
listed matrices because  example  constant 
consider query operation x    rather performing standard computation
find level x   raked   since occurred level    obtain
equation
 x     a  x    e   b  x    x  
thus must compute  x    find x   raked   happened
level    however  level equation associated x  is 
 x     a  x    e   b  x    e  
means need follow chain  general chain n nodes
answer query node chain evaluating log n equations instead n
equations 
consider update e    since e  raked immediately  first modify
equation
b  x     b  x   diaga  x   e   b  x  
first level e  occurs right hand side  since b   x   affected
change e    subsequently modify equation
b  x     b  x   diaga  x   e   b  x  
 

  

 

 

 

 

 

 

 

 

fidelcher  grove  kasif   pearl

second level  general  clearly need update log n equations  i e   one
per level  generalize example describe general algorithms queries
updates causal trees 

    performing queries updates eciently

section shall show utilize contracted trees ti    log n
perform queries updates o log n   time general causal trees  shall show
logarithmic amount work necessary sucient compute enough information
data structure update query value 

    queries

compute  x  node x following  first locate ind  x  
defined highest level x appears ti   equation  x 
form 
 x    ai x   y  bi x   z 
z left right children  respectively  x ti 
since x appear ti     raked level equations  implies
one child  we assume z   leaf  therefore need compute  y   
done recursively  instead raked leaf  would compute  z   recursively 
either case o    operations done addition one recursive call 
value higher level equations  since o log n   levels  operations
matrix vector multiplications  procedure takes o k  log n   time  function
 query  x  given figure   

    updates

describe update operations modify enough information data
structure allow us query vectors vectors eciently  importantly
reader note update operation try maintain correct
values  sucient ensure that  x  matrices ai x  bi  x   and
thus ci  x  di x   always date 
update value evidence node  simply changing value
leaf e  level equations  value  e  appear twice 
 equation e s parent  equation e s sibling ti  e
disappears  say level i  value incorporated one constant matrices ai    u 
bi    u  u grandparent e ti   constant matrix turn affects
exactly one constant matrix next higher level  on  since effect
level computed o k    time  due matrix multiplication  o log n  
levels equations  update accomplished o k  log n   time  constant k 
actually pessimistic  faster matrix multiplication algorithms exist 
update procedure given figure    update initially called update  e    
e  i  e leaf  level raked  e new evidence 
operation start sequence o log n   calls function  update  x   term  i 
change propagate log n equations 
  

fiqueries   updates probabilistic networks

function  query  x 
look equation associated  x  tind  x  
case    x leaf  equation form   x    e e known 
case return e 
case    equation associated  x  form

 x    ai x   y  bi  x   z 
z leaf therefore  z   known  case return
ai x    query  y  bi  x    z 
case leaf analogous 
figure    function compute value node 

    queries

relatively easy use similar recursive procedure perform  x  queries  unfortunately  approach yields o log n   time algorithm simply use recursion
calculate terms calculate terms using earlier procedure 
o log n   recursive calls calculate values  defined equation
involves term taking o log n   time compute 
achieve o log n   time  shall instead implement  x  queries defining procedure calc  x  i  returns triple vectors hp  l  ri p    x   l    y  
r    z   z left right children  respectively  x ti 
compute  x  node x following  let   ind  x   equation
 x  ti form 

 x    di x    u  ci x   v  
u parent x ti v sibling  call procedure calc  u      
return triple h  u    v    x i  immediately compute  x 
using equation 
procedure calc  x  i  implemented following fashion 
case    ti   node tree x root  children x leaves  hence
values known   x  given sequence prior probabilities x 
case    x appear ti     one x s children leaf  say e raked
level i  let z child  call calc  u        u parent
x ti  receive back h u    z    v i h u    v    z i according whether x
  

fidelcher  grove  kasif   pearl

function  update  term   value  i 
   find  at one  equation ti   defining ai bi   term
appears right hand side  let term  matrix defined equation
 i e   left hand side  
   update term   let value new value 
   call  update  term    value       recursively 
figure    update procedure 
left right child u ti  and v u s child   compute  x 
 u   v     e   z    return necessary triple 
specifically 

 x   

 

di x    u  ai    u   v  
di x    u  bi    u   v  

choice depends whether x right left child  respectively  u ti 
case    x appear ti    call calc  x        returns correct
value  x   child z x ti remains child x ti     returns
correct value  z    z child x occur ti     must
case z raked level one z  s children  say e  leaf let
child q   situation calc  x       returned value  q  
compute

 z    ai z   e  bi  z   q 
return value 
three cases  constant amount work done addition single recursive
call uses equations higher level  since o log n   levels equations 
requiring matrix vector multiplication  total work done o k  log n   

   extended example
section illustrate application algorithms specific example  consider
sequence contracted trees shown figure    corresponding trees
  

fiqueries   updates probabilistic networks

xl
 

t   







xl

 
 
 
 

 

xl
 

xl
 






e 

xl
 






e 

e 



e 











xl
 




xl
 

xl
 

xl
 






z
z

c
c
c
c

   
 
 

 
 

xl
 

t   

z
z
z

e 






xl
 



e 






xl
 

e 






e 

e 

e 

t  

xl
 






e 

e 

t   xl
 






xl
 

e 

e 

e 

figure    example tree contraction 

  






e 

e 

e 

e 

fidelcher  grove  kasif   pearl

equations following 
t   
 x     a  x     x    b   x     x   
  
 





 x     d   x     x   c  x    x    
  
 





t   
 x     a  x     x    b   x     e   
  
 











 x     d   x     x   c  x    e    
  
 





t   
 x     a  x     x    b   x     e   
  
 







 x     d   x     x   c  x    e    
  
 









t   
 x     a  x     e    b   x    e   






consider  instance  effect update e    since raked immediately 
new value  e   incorporated in 
b   x      b   x    diaga   x     e   b   x   
subsequent rake operations know a  x    depends b   x    a   x  
depends a   x    must update values follows 
a   x      a   x   diagb   x     e    a   x  
a   x      a   x   diagb   x     e    a   x  


















finally  consider query x    since x  raked together e  t    follow
steps outlined generate following calls  calc  x       calc  x      
calc  x       calc  x       provides us  x    case   x  
particularly easy compute since x   s children leaf nodes  simply
compute  x    x   normalize  giving us conditional marginal distribution
bel  x   required 

   join trees

perhaps best known technique computing arbitrary  i e   singly connected 
bayesian networks uses idea join trees  junction trees   lauritzen   spiegelhalter 
       many ways join tree thought causal tree  albeit one somewhat
special structure  thus algorithm previous section applied  however 
structure join tree permits optimization  describe section 
becomes especially relevant next section  use join tree technique
show o log n   updates queries done arbitrary polytrees  review
join trees utility extremely brief quite incomplete  clear expositions
see  instance  spiegelhalter et al         pearl        
given bayesian network  first step towards constructing join tree moralize
network  insert edges every pair parents common node  treat
  

fiqueries   updates probabilistic networks

edges graph undirected  spiegelhalter et al          resulting undirected
graph called moral graph  interested undirected graphs chordal  
every cycle length   contain chord  i e   edge two nodes
non adjacent cycle   moral graph chordal  necessary add
edges make so  various techniques triangulation stage known  for instance 
see spiegelhalter et al         
p probability distribution represented bayesian network g    v  e   
   v  f   result moralizing triangulating g  then 
   jv j cliques   say c          cjv j 
   cliques ordered     j  i   

ci   cj i    ci    c    c            ci    
tree formed treating cliques nodes  connecting node ci
 parent  cj  i   called join tree 
   p  





p cijcj i  

   p cijcj  i     p cijcj  i    ci  
     see direct edges away  parent  cliques 
resulting directed tree fact bayesian causal tree represent original
distribution p  true matter form original graph  course 
price cliques may large  domain size  the number possible values
clique node  exponential size  technique guaranteed
ecient 
use rake technique section   directed join tree without
modification  however  property   shows conditional probability matrices
join tree special structure  use gain eciency 
following  let k domain size variables g usual  let n maximum
size cliques join tree  without loss generality assume cliques
size  because add  dummy  variables   thus domain size
clique k   kn   finally  let c maximum intersection size clique parent
 i e   jcj  i    cij  l   kc  
standard algorithm  would represent p cijcj  i   k k matrix  mc jc  
however  p ci jcj  i    ci  represented smaller l k matrix  mc jc  c  
property   above  mc jc identical mc jc  c   except many rows repeated 
thus k l matrix j








j  i 



j  i 





j  i 

mc jc   j mc jc


j  i 

j  i 

j  i 

 ci  

 j actually simple matrix whose entries      exactly one   per row  however
use fact  
   clique maximal completely connected subgraph 

  

fidelcher  grove  kasif   pearl

claim that  case join trees  following true  first  matrices

ai bi used rake algorithm stored factored form  product
two matrices dimension k l l k respectively  so  instance  factor ai
ali ari   never need explicitly compute  store  full matrices 
seen  claim true     matrices factor way  proof
    uses inductive argument  illustrate below  second claim that 

matrices stored factored form  matrix multiplications used
rake algorithm one following types     l k matrix times k l matrix 
   l k matrix times k k diagonal matrix     l l matrix times l k
matrix     l k matrix times vector 
prove claims consider  instance  equation defining bi   terms lowerlevel matrices  section    bi    u    bi  u  diaga  x  e  bi  x   but  assumption 
is 
 bil  u  bir  u   diag a  x a  x   e   bil  x  bil  x   
which  using associativity  clearly equivalent
h

bil  u    bir  u  diaga  x  a  x  e    bil x   bil  x   
however  every multiplication expression one forms stated earlier  identifying
bil    u  bil u  bir    u  bracketed part expression proves case 
course case rake left child  so ai    u  updated  analogous 
thus  even using straightforward technique matrix multiplication  cost
updating bi   o kl     o kn  c    contrasts o k    factor
matrices  may represent worthwhile speedup c small  note overall time
update using scheme o kn  c log n    queries  involve matrix
vector multiplication  require o kn c log n   time 
many join trees difference n log n unimportant 
clique domain size k often enormous dominates complexity  indeed  k l
may large cannot represent required matrices explicitly  course 
cases technique little offer  cases benefits
worthwhile  important general class so  immediate
reason presenting technique join trees  case polytrees 


l


r


l


r


   polytrees

polytree singly connected bayesian network  drop assumption section  
node one parent  polytrees offer much exibility causal
trees  yet well known process update query o n   time 
causal trees  reason polytrees extremely popular class networks 
suspect possible present o log n   algorithm updates queries
polytrees  direct extension ideas section    instead propose different
technique  involves converting polytree join tree using ideas
preceding section  basis simple observation join tree
polytree already chordal  thus  as show detail below  little lost considering
join tree instead original polytree  specific property polytrees
require following  omit proof well known proposition 
  

fiqueries   updates probabilistic networks

proposition    moral graph polytree p    v  e   chordal 
set maximal cliques ffv g   parents  v     v   v g 
let p maximum number parents node  proposition  every
maximal clique join tree p    variables  domain size node
join tree k   kp     may large  recall conditional probability
matrix original polytree  variable p parents  k entries anyway since
must give conditional distribution every combination node s parents  thus k
really measure size polytree itself 
follows proposition perform query update
polytrees time o k   log n    simply using algorithm section   directed
join tree  but  noted section    better  recall savings depend
c  maximum size intersection node parent join tree 
however  join tree formed polytree  two cliques share
single node  follows immediately proposition    two cliques
one node common must either two nodes share one parent 
else node one parents share yet another parent  neither
consistent network polytree  thus complexity bounds section   
put c      follows process updates o kk c log n     o kp   log n  
time queries o kp   log n   

   application  towards automated site specific muta genesis

experiment commonly performed biology laboratories procedure
particular site protein changed  i e   single amino acid mutated 
tested see whether protein settles different conformation  many cases 
overwhelming probability protein change secondary structure outside
mutated region  process often called muta genesis  delcher et al         developed
probabilistic model protein structure basically long chain  length
chain varies         nodes  nodes network either protein structure
nodes  ps nodes  evidence nodes  e nodes   ps node network discrete
random variable xi assumes values corresponding descriptors secondary sequence
structure  helix  sheet coil  ps node model associates evidence node
corresponds occurrence particular subsequence amino acids particular
location protein 
model  protein structure nodes finite strings alphabet fh  e   c g 
example string hhhhhh string six residues ff helical conformation 
eecc string two residues  sheet conformation followed two residues folded
coil  evidence nodes nodes contain information particular region
protein  thus  main idea represent physical statistical rules form
probabilistic network 
first set experiments converged following model that  clearly
biologically naive  seems match prediction accuracy many existing approaches
neural networks  network looks set ps nodes connected chain 
node connect single evidence node  experiments ps nodes strings
length two three alphabet fh  e   c g evidence nodes strings
  

fidelcher  grove  kasif   pearl


cc

 
gs


ch
 
sa


hh
 







figure    example causal tree model using pairs  showing protein segment gsat
corresponding secondary structure cchh 
length set amino acids  following example clarifies representation 
assume string amino acids gsat  model string network comprised
three evidence nodes gs  sa  three ps nodes  network shown figure   
correct prediction assign values cc  ch  hh ps nodes shown
figure 
probabilistic model  test robustness protein
whether small changes protein affect structure certain critical sites
protein  experiments  probabilistic network performs  simulated evolution 
protein  namely simulator repeatedly mutates region chain tests
whether designated sites protein coiled helix predicted
remain conformation  main goal experiment test stable bonds far
away mutated location affected  previous results  delcher et al        
support current thesis biology community  namely local distant changes
rarely affect structure 
algorithms presented previous sections paper perfectly suited
type application predicted generate factor    improvement
eciency current brute force implementation presented delcher et al        
change propagated throughout network 

   summary
paper proposed several new algorithms yield substantial improvement
performance probabilistic networks form causal trees  updating procedures
absorb sucient information tree query procedure compute
correct probability distribution node given current evidence  addition 
procedures execute time o log n    n size network  algorithms
expected generate orders of magnitude speed ups causal trees contain long
paths  not necessarily chains  matrices conditional probabilities
relatively small  currently experimenting approach singly connected
networks  polytrees   likely dicult generalize techniques general
networks  since known general problem inference probabilistic networks
np  hard  cooper         obviously possible obtain polynomial time incremental
  

fiqueries   updates probabilistic networks

solutions type discussed paper general probabilistic networks 
natural open question extending approach developed paper dynamic
operations probabilistic networks addition deletion nodes modifying
matrices conditional probabilities  as result learning  
would interesting investigate practical logarithmic time parallel algorithms probabilistic networks realistic parallel models computation  one
main goals massively parallel ai research produce networks perform real time
inference large knowledge bases eciently  i e   time proportional depth
network rather size network  exploiting massive parallelism  jerry
feldman pioneered philosophy context neural architectures  see stanfill
waltz        shastri        feldman ballard         achieve type performance neural network framework  typically postulate parallel hardware
associates processor node network typically ignores communication requirements  careful mapping parallel architectures one indeed achieve ecient
parallel execution specific classes inference operations  see mani shastri       
kasif        kasif delcher         techniques outlined paper presented
alternative architecture supports fast  sub linear time  response capability
sequential machines based preprocessing  however  approach obviously limited
applications number updates queries time constant  one would
naturally hope develop parallel computers support real time probabilistic reasoning
general networks 

acknowledgements
simon kasif s research johns hopkins university sponsored part national
science foundation grants no  iri          iri         iri         

references

berger  t     ye  z          entropic aspects random fields trees  ieee trans 
information theory                    
chelberg  d  m          uncertainty interpretation range imagery  proc  intern 
conference computer vision  pp          
cohen  r  f     tamassia  r          dynamic trees applications  proceedings
 nd acm siam symposium discrete algorithms  pp        
cooper  g          computational complexity probabilistic inference using bayes
belief networks  artificial intelligence              
delcher  a     kasif  s          improved decision making game trees  recovering
pathology  proceedings      national conference artificial intelligence 
delcher  a  l   kasif  s   goldberg  h  r     hsu  b          probabilistic prediction protein secondary structure using causal networks  proceedings      international
conference intelligent systems computational biology  pp          
  

fidelcher  grove  kasif   pearl

duda  r     hart  p          pattern classification scene analysis  wiley  new york 
feldman  j  a     ballard  d          connectionist models properties  cognitive
science             
frederickson  g  n          data structure dynamically maintaining rooted trees 
proc   th annual symposium discrete algorithms  pp          
hel or  y     werman  m          absolute orientation uncertain data  unified
approach  proc  intern  conference computer vision pattern recognition 
pp        
karp  r  m     ramachandran  v          parallel algorithms shared memory machines 
van leeuwen  j   ed    handbook theoretical computer science  pp          
north holland 
kasif  s          parallel complexity discrete relaxation constraint networks 
artificial intelligence              
kasif  s     delcher  a          analysis local consistency parallel constraint networks 
artificial intelligence     
kosaraju  s  r     delcher  a  l          optimal parallel evaluation tree structured
computations raking  reif  j  h   ed    vlsi algorithms architectures 
proceedings      aegean workshop computing  pp           springer verlag 
lncs     
lauritzen  s     spiegelhalter  d          local computations probabilities graphical
structures applications expert systems  j  royal statistical soc  ser  b 
            
mani  d     shastri  l          massively parallel reasoning large knowledge
bases  tech  rep   intern  computer science institute 
miller  g  l     reif  j          parallel tree contraction application  proceedings
  th ieee symposium foundations computer science  pp          
pearl  j          probabilistic reasoning intelligent systems  morgan kaufmann 
peot  m  a     shachter  r  d          fusion propagation multiple observations
belief networks  artificial intelligence              
rachlin  j   kasif  s   salzberg  s     aha  d          towards better understanding
memory based bayesian classifiers  proceedings eleventh international
conference machine learning  pp          new brunswick  nj 
shastri  l          computational model tractable reasoning  taking inspiration
cognition  proceeding      intern  joint conference artificial intelligence 
aaai 
  

fiqueries   updates probabilistic networks

spiegelhalter  d   dawid  a   lauritzen  s     cowell  r          bayesian analysis expert
systems  statistical science                 
stanfill  c     waltz  d          toward memory based reasoning  communications
acm                     
wilsky  a          multiscale representation markov random fields  ieee trans  signal
processing                

  


