journal of artificial intelligence research               

submitted       published     

logarithmic time updates and queries
in probabilistic networks
arthur l  delcher

computer science department  loyola college in maryland
baltimore  md      

adam j  grove

delcher cs loyola edu

grove research nj nec com

nec research institute
princeton  nj      

simon kasif

kasif cs jhu edu

judea pearl

pearl lanai cs ucla edu

department of computer science  johns hopkins university
baltimore  md      
department of computer science  university of california
los angeles  ca      

abstract

traditional databases commonly support ecient query and update procedures that
operate in time which is sublinear in the size of the database  our goal in this paper is
to take a first step toward dynamic reasoning in probabilistic databases with comparable
eciency  we propose a dynamic data structure that supports ecient algorithms for
updating and querying singly connected bayesian networks  in the conventional algorithm 
new evidence is absorbed in time o    and queries are processed in time o n    where n
is the size of the network  we propose an algorithm which  after a preprocessing phase 
allows us to answer queries in time o log n   at the expense of o log n   time per evidence
absorption  the usefulness of sub linear processing time manifests itself in applications
requiring  near  real time response over large probabilistic databases  we briey discuss a
potential application of dynamic probabilistic reasoning in computational biology 

   introduction
probabilistic  bayesian  networks are an increasingly popular modeling technique that has
been used successfully in numerous applications of intelligent systems such as real time planning and navigation  model based diagnosis  information retrieval  classification  bayesian
forecasting  natural language processing  computer vision  medical informatics and computational biology  probabilistic networks allow the user to describe the environment using
a  probabilistic database  that consists of a large number of random variables  each corresponding to an important parameter in the environment  some random variables could in
fact be hidden and may correspond to some unknown parameters  causes  that inuence
the observable variables  probabilistic networks are quite general and can store information
such as the probability of failure of a particular component in a computer system  the probc      ai access foundation and morgan kaufmann publishers  all rights reserved 

fidelcher  grove  kasif   pearl

ability of page i in a computer cache being requested in the near future  the probability
of a document being relevant to a particular query  or the probability of an amino acid
subsequence in a protein chain folding into an alpha helix conformation 
the applications we have in mind include networks that are dynamically maintained to
keep track of a probabilistic model of a changing system  for instance  consider the task of
automated detection of power plant failures  we might repeat a cycle that consists of the
following sequence of operations  first we perform sensing operations  these operations
cause updates to be performed to specific variables in the probabilistic database  based on
this evidence we estimate  query  the probability of failure in certain sites  more precisely 
we query the probability distribution of the random variables that measure the probability
of failure in these sites based on the evidence  since the plant requires constant monitoring 
we must repeat the cycle of sense evaluate on a frequent basis 
a conventional  non probabilistic  database tracking the plant s state would not be
appropriate here  because it is not possible to directly observe whether a failure is about
to occur  on the other hand  a probabilistic  database  based on a bayesian network
will only be useful if the operations update and query can be performed very quickly 
because real time or near real time is so often necessary  the question of doing extremely
fast reasoning in probabilistic networks is important 
traditional  non probabilistic  databases support ecient query and update procedures
that often operate in time which is sublinear in the size of the database  e g   using binary search   our goal in this paper is to take a step toward systems that can perform
dynamic probabilistic reasoning  such as what is the probability of an event given a set of
observations  in time which is sublinear in the size of the probabilistic network  typically 
sublinear performance in complex networks is attained by using parallelism  this paper
relies on preprocessing 
specifically  we describe new algorithms for performing queries and updates in belief
networks in the form of trees  causal trees  polytrees and join trees   we define two natural
database operations on probabilistic networks 
  

update node

  perform sensory input  modify the evidence at a leaf node  single
variable  in the network and absorb this evidence into the network 

  

query node

  obtain the marginal probability distribution over the values of an
arbitrary node  single variable  in the network 

the standard algorithms introduced by pearl        can perform the query node operation in o    time although evidence absorption  i e   the update node operation  takes
o n   time where n is the size of the network  alternatively  one can assume that the
update node operation takes o    time  by simply recording the change  and the querynode operation takes o n   time  evaluating the entire network  
in this paper we describe an approach to perform both queries and updates in o log n  
time  this can be very significant in some systems since we improve the ability of a system to
respond after a change has been encountered from o n   time to o log n    our approach is
based on preprocessing the network using a form of node absorption in a carefully structured
way to create a hierarchy of abstractions of the network  previous uses of node absorption
techniques were reported by peot and shachter        
  

fiqueries   updates in probabilistic networks

we note that measuring complexity only in terms of the size of the network  n   can
overlook some important factors  suppose that each variable in the network has domain
size k or less  for many purposes  k can be considered constant  nevertheless  some of the
algorithms we consider have a slowdown which is some power of k  which can be become
significant in practice unless n is very large  thus we will be careful to state this slowdown
where it exists 
section   considers the case of causal trees  i e   singly connected networks in which each
node has at most one parent  the standard algorithm  see pearl        must use o k  n  
time for either updates or for retrieval  although one of these operations can be done in
o    time  as we discuss briey in section      there is also a straightforward variant on
this algorithm that takes o k d  time for both queries and updates  where d is the height
of the tree 
we then present an algorithm that takes o k  log n   time for updates and o k  log n  
time for queries in any causal tree  this can of course represent a tremendous speedup 
especially for large networks  our algorithm begins with a polynomial time preprocessing
step  linear in the size of the network   constructing another data structure  which is not
itself a probabilistic tree  that supports fast queries and updates  the techniques we use are
motivated by earlier algorithms for dynamic arithmetic trees  and involve  caching  sucient intermediate computations during the update phase so that querying is also relatively
easy  we note  however  that there are substantial and interesting differences between the
algorithm for probabilistic networks and those for arithmetic trees  in particular  as will be
apparent later  computation in probabilistic trees requires both bottom up and top down
processing  whereas arithmetic trees need only the former  perhaps even more interesting is that the relevant probabilistic operations have a different algebraic structure than
arithmetic operations  for instance  they lack distributivity  
bayesian trees have many applications in the literature including classification  for
instance  one of the most popular methods for classification is the bayes classifier that
makes independence assumption on the features that are used to perform classification
 duda   hart        rachlin  kasif  salzberg    aha         probabilistic trees have
been used in computer vision  hel or   werman        chelberg         signal processing
 wilsky         game playing  delcher   kasif         and statistical mechanics  berger
  ye         nevertheless  causal trees are fairly limited for modeling purposes  however
similar structures  called join trees  arise in the course of one of the standard algorithms for
computing with arbitrary bayesian networks  see lauritzen and spiegelhalter         thus
our algorithm for join trees has potential relevance to many networks that are not trees 
because join trees have some special structure  they allow some optimization of the basic
causal tree algorithm  we elaborate on this in section   
in section   we consider the case of arbitrary polytrees  we give an o log n   algorithm for updates and queries  which involves transforming the polytree to a join tree  and
then using the results of sections   and    the join tree of a polytree has a particularly
simple form  giving an algorithm in which updates take o kp   log n   time and queries
o kp   log n    where p is the maximum number of parents of any node  although the
constant appears large  it must be noted that the original polytree takes o kp   n   space
merely to represent  if conditional probability tables are given as explicit matrices 
  

fidelcher  grove  kasif   pearl


u

 
 

mv ju   
 

 

 
 
v


 

m

  x ju
 
 
r
 


x

 
 

my jx  


y

 
 
 

m

  z jx
 
r
 


z


figure    a segment of a causal tree 
finally  we discuss a specific modelling application in computational biology where probabilistic models are used to describe  analyze and predict the functional behavior of biological sequences such as protein chains or dna sequences  see delcher  kasif  goldberg  and
hsu       for references   much of the information in computational biology databases is
noisy  however  a number of successful attempts to build probabilistic models have been
made  in this case  we use a probabilistic tree of depth     that consists of     nodes and all
the matrices of conditional probabilities are       the tree is used to model the dependence
of a protein s secondary structure on its chemical structure  the detailed description of the
problem and experimental results are given by delcher et al          for this problem we
obtain an effective speed up of about a factor of    to perform an update as compared to the
standard algorithm  clearly  getting an order of magnitude improvement in the response
time of a probabilistic real time system could be of tremendous importance in future use of
such systems 

   causal trees

a probabilistic causal tree is a directed tree in which each node represents a discrete random
variable x   and each directed edge is annotated by a matrix of conditional probabilities
my jx  associated with edge x   y    that is  if x is a possible value of x  and y of y 
then the  x  y  th component of my jx is pr y   y jx   x   such a tree represents a joint
probability distribution over the product space of all variables  for detailed definitions and
discussion see pearl         briey  the idea is that we consider the product  over all nodes 
of the conditional probability of the node given its parents  for example  in figure   the
implied distribution is 
pr u   u  v   v  x   x  y   y  z   z    
pr u   u  pr v   v ju   u  pr x   xju   u  pr y   y jx   x  pr z   z jx   x  
given particular values of u  v  x  y  z  the conditional probabilities can be read from the
appropriate matrices m   one advantage of such a product representation is that it is very
  

fiqueries   updates in probabilistic networks

concise  in this example  we need four matrices and the unconditional probability over u  
but the size of each is at most the square of the largest variable s domain size  in contrast 
a general distribution over n variables requires an exponential  in n   representation 
of course  not every distribution can be represented as a causal tree  but it turns out
that the product decomposition implied by the tree corresponds to a particular pattern
of conditional independencies which often hold  if perhaps only approximately  in real
applications  intuitively speaking  in figure   some of these implied independencies are
that the conditional probability of u given v   x   y and z depends only on values of v and
x   and the probability of y given u   v   x   and z depends only on x   independencies of
this sort can arise for many reasons  for instance from a causal modeling of the interactions
between the variables  we refer the reader to pearl        for details related to the modeling
of independence assumptions using graphs 
in the following  we make several assumptions that significantly simplify the presentation  but do not sacrifice generality  first  we assume that each variable ranges over the
same  constant  number of values k   it follows that the marginal probability distribution
for each variable can be viewed as a k dimensional vector  and each conditional probability
matrix such as my jx is a square k  k matrix  a common case is that of binary random
variables  k       the distribution over the values  true  false  is then  p      p  for
some probability p 
the next assumption is that the tree is binary  and complete  so that each node has  
or   children  any tree can be converted into this form  by at most doubling the number
of nodes  for instance  suppose node p has children c    c   c  in the original tree  we can
create another  copy  of p  p   and rearrange the tree such that the two children of p are
c  and p   and the two children of p  are c  and c   we can constrain p  always to have the
same value as p simply by choosing the identity matrix for the conditional probability table
between p and p    then the distribution represented by the new tree is effectively the same
as the original  similarly  we can always add  dummy  leaf nodes if necessary to ensure a
node has two children  as explained in the introduction  we are interested in processes in
which certain variables  values are observed  upon which we wish to condition  our final
assumption is that these observed evidence nodes are all leaves of the tree  again  because
it is possible to  copy  nodes and to add dummy nodes  this is not restrictive 
the product distribution alluded to above corresponds to the distribution over variables
prior to any observations  in practice  we are more interested in the conditional distribution 
which is simply the result of conditioning on all the observed evidence  which  by the earlier
assumption  corresponds to seeing values for all the leaf nodes   thus  for each non leaf node
x we are interested in the conditional marginal probability over x   i e   the k dimensional
vector 
bel  x     pr x j all evidence values  
the main algorithmic problem is to compute bel  x   for each  non evidence  node x
in the tree given the current evidence  it is well known that the probability vector bel  x  
can be computed in linear time  in the size of the tree  by a popular algorithm based on
   this assumption is nonrestrictive because we can add  dummy  values to each variable s range  which
should be given conditional probability    nevertheless  there may some computational advantage in
allowing different variable domain sizes  the changes required to permit this are not dicult  but since
they complicate the presentation somewhat we omit them 

  

fidelcher  grove  kasif   pearl

the following equation 
bel  x     pr x j all evidence    ff   x      x  
here ff is a normalizing constant   x   is the probability of all the evidence in the subtree
below node x given x   and   x   is the probability of x given all evidence in the rest of the
tree  to interpret this equation  note that if x    x   x          xk   and  y   y    y           yk  
are two vectors we define  to be the operation of component wise product  pairwise or
dyadic product of vectors  
x  y    x y   x y          xkyk   
the usefulness of  x   and   x   derives from the fact that they can be computed recursively  as follows 
   if x is the root node    x   is the prior probability of x  
   if x is a leaf node   x   is a vector with   in the ith position  where the ith value
has been observed  and   elsewhere  if no value for x has been observed  then  x  
is a vector consisting of all   s  
   otherwise  if  as shown in figure    the children of node x are y and z   its sibling
is v and its parent is u   we have 
 x      my jx   y      mzjx   z   


 x     mx ju   u     mv ju   v   
t

our presentation of this technique follows that of pearl         however  we use a
somewhat different notation in that we don t describe messages sent to parents or successors  but rather discuss the direct relations among the  and  vectors in terms of simple
algebraic equations  we will take advantage of algebraic properties of these equations in
our development 
it is very easy to see that the equations above can be evaluated in time proportional to
the size of the network  the formal proof is given by pearl        
theorem    the belief distribution of every variable  that is  the marginal probability
distribution for each variable  given the evidence  in a causal tree can be evaluated in
o k n   time where n is the size of the tree   the factor k  is due to the multiplication
of a matrix by a vector that must be performed at each node  
this theorem shows that it is possible to perform evidence absorption in o n   time  and
queries in constant time  i e   by retrieving the previously computed values from a lookup
table   in the next sections we will show how to perform both queries and updates in
worst case o log n   time  intuitively  we will not recompute all the marginal distributions
after an update  but rather make only a small number of changes  sucient  however  to
compute the value of any variable with only a logarithmic delay 
   or we can set to   all components corresponding to possible values this is especially useful when the
observed variable is part of a joint tree clique  section     in general   x   should be thought of as the
likelihood vector over x given our observations about x  

  

fiqueries   updates in probabilistic networks

    a simple preprocessing approach

to obtain intuition about the new approach we begin with a very simple observation 
consider a causal tree t of depth d  for each node x in the tree we initially compute its
 x   vector   vectors are left uncomputed  given an update to a node y   we calculate the
revised  x   vectors for all nodes x that are ancestors of y in the tree  this clearly can be
done in time proportional to the depth of the tree  i e   o d   the rest of the information
in the tree remains unchanged  now consider a query node operation for some node v
in the tree  we obviously already have the accurate  v   vector for every node in the tree
including v   however  in order to compute its   v   vector we need to compute only the
 y   vectors for all the nodes above v in the tree and multiply these by the appropriate
 vectors that are kept current  this means that to compute the accurate  v   vector we
need to perform o d  work as well  thus  in this approach we don t perform the complete
update to every  x   and   x   vector in the tree 
lemma    update node and query node operations in a causal tree t can be performed in o k  d  time where d is the depth of the tree 
this implies that if the tree is balanced  both operations can be done in o log n  
time  however  in some important applications the trees are not balanced  e g   models of
temporal sequences  delcher et al          the obvious question therefore is  given a causal
tree t can we produce an equivalent balanced tree t    while the answer to this question
appears to be dicult  it is possible to use a more sophisticated approach to produce a data
structure  which is not a causal tree  to process queries and updates in o log n   time  this
approach is described in the subsequent sections 

    a dynamic data structure for causal trees

the data structure that will allow ecient incremental processing of a probabilistic tree t  
t  will be a sequence of trees  t   t   t          ti         tlog n   each ti   will be a contracted
version of ti  whose nodes are a subset of those in ti   in particular  ti   will contain about
half as many leaves as its predecessor 
we defer the details of this contraction process until the next section  however  one key
idea is that we maintain consistency  in the sense that bel  x     x    and   x   are given
the same values by all the trees in which x appears  we choose the conditional probability
matrices in the contracted trees  i e   all trees other than t    to ensure this 
recall that the  and  equations have the form

 x      my jx   y      mzjx   z   


 x     mx ju   u     mv ju   v   
t

if y and z are children of x   x is a right child of u   and v is x  s sibling  figure    
however  these equations are not in the most convenient form and the following notational
conventions will be very helpful  first  let ai  x   resp   bi  x   denote the conditional
probability matrix between x and x  s left  resp   right  child in the tree ti  note that the
identity of these children can differ from tree to tree  because some of x  s original children
might be removed by the contraction process  one advantage of the new notation is that
  

fidelcher  grove  kasif   pearl

uj

in ti

   
 
 
 
 

vj

e

xj

rake

   
 
 

p p zjp p

 e  x 

 

uj

in ti  

   
 
 
 
 

vj

p p zjp p

figure    the effect of the operation rake  e  x   e must be a leaf  but z may or may not
be a leaf 
the explicit dependence on the identity of the children is suppressed  next  suppose x  s
parent in ti is u  then we let ci  x  denote either ai  u  or bi  u   and di x  denote either
bi  u  or ai u    depending on whether x is the right or left child  respectively  of u   it
will not be necessary to keep careful track of these correspondences  but simply to note that
the above equations become  
 x    ai x    y   bi  x    z 
 x    di x     u   ci x    v  
in the next section we describe the preprocessing step that creates the dynamic data
structure 
t

   

rake

t

operation

the basic operation used to contract the tree is rake which removes both a leaf and its
parent from the tree  the effect of this operation on the tree is shown in figure    we
now define the algebraic effect of this operation on the equations associated with this tree 
recall that we want to define the conditional probability matrices in the raked tree so that
the distribution over the remaining variables is unchanged  we achieve this by substituting
the equations for  x  and   x  into the equations for  u     z    and   v    in the following 
it is important to note that   u    z   and  v   are unaffected by the rake operation 
in the following  let diagff denote the diagonal matrix whose diagonal entries are the
components of the vector ff  we derive the algebraic effect of the rake operation as follows 
 u    ai  u    v   bi  u    x 
  ai  u    v    bi  u    ai  x    e   bi  x    z    
  ai  u    v    bi  u   diaga  x  e   bi  x    z  


  ai  u    v    bi  u   diaga  x  e   bi  x    z  
  ai    u    v    bi    u    z  
where ai    u    ai  u  and bi    u    bi  u   diaga  x  e   bi  x    of course  the case
where the leaf being raked is a right child generates analogous equations   thus  by defining
i

i

i

   throughout  we assume that  has lower precedence than matrix multiplication  indicated by   

  

fiqueries   updates in probabilistic networks

ai   u  and bi    u  in this way  we ensure that all  values in the raked tree are identical

to the corresponding values in the original tree  this is not yet enough  because we must
check that  values are similarly preserved  the only two values that could possibly change
are   z   and   v    so we check them both  for the former  we must have

 z    di z     x   ci z    e  
  di    z       u   ci   z     v     
after substituting for   x  and some algebraic manipulation  we see that this is assured if
ci   z    ci x  and di   z    di z   diagc  z  e   di x   however recall that  by definition  ci    z     ai    u  and ci  x    ai  u   and so ci    z     ci x  follows  furthermore 
di   z    bi   u 
   bi  u   diaga  x  e   bi  x  
  bi  x   diaga  x  e   bi  u 
  di z    diagc  z  e   di  x 
i

t

t

i

t

t

i

i

as required 
for   v   it is necessary to verify that

 v    di v     u   ci v    x  
  di    v       u   ci    v     z     
by substituting for  x   this can be shown to be true if di   v     di  v     ai  u   
ai   u  and ci   v    ci v   diaga  x  e   bi x    bi   u   but these identities follow
t

t

i

by definition  so we are done 
beginning with the given tree t   t   each successive tree is constructed by performing
a sequence of rakes  so as to rake away about half of the remaining evidence nodes  more
specifically  let contract be the operation in which we apply the rake operation to every
other leaf of a causal tree  in left to right order  excluding the leftmost and the rightmost
leaf  let ftig be the set of causal trees constructed so that ti   is the causal tree generated
from ti by a single application of contract  the following result is proved using an easy
inductive argument 

theorem    let t  be a causal tree of size n   then the number of leaves in ti   is equal

to half the leaves in ti  not counting the two extreme leaves  so that starting with t  
after o log n   applications of contract  we produce a three node tree  the root  the
leftmost leaf and the rightmost leaf 
below are a few observations about this process 
   the complexity of contract is linear in the size of the tree  additionally  log n applications of contract reduce the set of tree equations to a single equation involving
the root in o n   total time 
   the total space to store all the sets of equations associated with fti g ilog n is about
twice the space required to store the equations for t  
  

fidelcher  grove  kasif   pearl

   with each equation in ti   we also store equations that describe the relationship
between the conditional probability matrices in ti   to the matrices in ti   notice
that  even though ti   is produced from ti by a series of rake operations  each matrix
in ti   depends directly on matrices present in ti  this would not be the case if we
attempted to simultaneously rake adjacent children 
we regard these equations as part of ti    so  formally speaking ftig are causal trees
augmented with some auxiliary equations  each of the contracted trees describes a
probability distribution on a subset of the first set of variables that is consistent with
the original distribution 
we note that the ideas behind the rake operation were originally developed by miller
and reif        in the context of parallel computation of bottom up arithmetic expression
trees  kosaraju   delcher        karp   ramachandran         in contrast  we are using
it in the context of incremental update and query operations in sequential computing  a
similar data structure to ours was independently proposed by frederickson        in the
context of dynamic arithmetic expression trees  and a different approach for incremental
computing on arithmetic trees was developed by cohen and tamassia         there are
important and interesting differences between the arithmetic expression tree case and our
own  for arithmetic expressions all computation is done bottom up  however  in probabilistic networks   messages must be passed top down  furthermore  in arithmetic expressions
when two algebraic operations are allowed  we typically require the distributivity of one
operation over the other  but the analogous property does not hold for us  in these respects our approach is a substantial generalization of the previous work  while remaining
conceptually simple and practical 

   example  a chain

to obtain an intuition about the algorithms  we sketch how to generate and utilize the
ti     i  log n and their equations to perform  value queries and updates in o log n  
time on an n    l     node chain of length l  consider the chain of length   in figure   
and the trees that are generated by repeated application of contract to the chain 
the equations that correspond to the contracted trees in the figure are as follows  ignoring trivial equations   recall that ai  xj   is the matrix associated with the left edge of
random variable xj in ti 

 x  
 x  
 x  
 x  

 
 
 
 

a  x     e    b  x     x  
a  x     e    b  x     x  
a  x     e    b  x     x  
a  x     e    b  x     e  

 
 
 
 
 
 
 
 
 
 
 
b   x    diaga   x   e    b  x    
 
 
b   x    diaga   x   e    b  x    

 x     a  x     e    b  x     x  
 x     a  x     e    b  x     e  

where
b  x    
b  x    

 
 
 
 
 
 
 
 
 

  

for t 

for t 

fiqueries   updates in probabilistic networks

t   

xm
 
em
 
 

t   

xm
 
em
 
 

t   

xm
 

xm   xm
    xm
    e m

   

em
 

em
 

 

em
 

 

 

xm   em
 

   

em
 
 

em

   

em
 
 

figure    a simple chain example 

 x     a  x     e    b  x     e  

 
 
 
 
 
 
 

where
for t 
b  x     b   x    diaga  x   e    b  x  
we have not listed the a matrices because  in this example  they are constant  now
consider a query operation on x    rather than performing the standard computation we
will find the level where x  was  raked   since this occurred on level    we obtain the
equation
 x     a  x     e    b  x     x  
thus we must compute  x    and to do this we find where x  is  raked   that happened
on level    however  on that level the equation associated with x  is 
 x     a  x     e    b  x     e  
that means that we need not follow down the chain  in general for a chain of n nodes we
can answer any query to a node on the chain by evaluating log n equations instead of n
equations 
now consider an update for e    since e  was raked immediately  we first modify the
equation
b  x     b  x    diaga  x   e    b  x  
on the first level where e  occurs on the right hand side  since b   x   is affected by the
change to e    we subsequently modify the equation
b  x     b  x    diaga  x   e    b  x  
 

  

 

 

 

 

 

 

 

 

fidelcher  grove  kasif   pearl

on the second level  in general  we clearly need to update at most log n equations  i e   one
per level  we now generalize this example and describe general algorithms for queries and
updates in causal trees 

    performing queries and updates eciently

in this section we shall show how to utilize the contracted trees ti     i  log n to
perform queries and updates in o log n   time in general causal trees  we shall show that a
logarithmic amount of work will be necessary and sucient to compute enough information
in our data structure to update and query any  or  value 

     queries

to compute  x  for some node x we can do the following  we first locate ind  x   which is
defined to be the highest level i such that x appears in ti   the equation for  x  is of the
form 
 x    ai x    y   bi x    z 
where y and z are the left and right children  respectively  of x in ti 
since x does not appear in ti     it was raked at this level of equations  which implies
that one child  we assume z   is a leaf  we therefore only need to compute  y    which can
be done recursively  if instead y was the raked leaf  we would compute  z   recursively 
in either case o    operations are done in addition to one recursive call  which is to a
value at a higher level of equations  since there are o log n   levels  and the only operations
are matrix by vector multiplications  the procedure takes o k  log n   time  the function
 query  x  is given in figure   

    updates

we now describe how the update operations can modify enough information in the data
structure to allow us to query the  vectors and  vectors eciently  most importantly the
reader should note that the update operation does not try to maintain the correct  and
 values  it is sucient to ensure that  for all i and x  the matrices ai x  and bi  x   and
thus also ci  x  and di x   are always up to date 
when we update the value of an evidence node  we are simply changing the  value of
some leaf e  at each level of equations  the value of  e  can appear at most twice  once
in the  equation of e s parent and once in the   equation of e s sibling in ti  when e
disappears  say at level i  its value is incorporated into one of the constant matrices ai    u 
or bi    u  where u is the grandparent of e in ti   this constant matrix in turn affects
exactly one constant matrix in the next higher level  and so on  since the effect at each
level can be computed in o k    time  due to matrix multiplication  and there are o log n  
levels of equations  the update can be accomplished in o k  log n   time  the constant k 
is actually pessimistic  because faster matrix multiplication algorithms exist 
the update procedure is given in figure    update is initially called as update  e    
e  i  where e is a leaf  i the level at which it was raked  and e is the new evidence  this
operation will start a sequence of o log n   calls to function  update  x   term  i  as
the change will propagate to log n equations 
  

fiqueries   updates in probabilistic networks

function  query  x 
we look up the equation associated with  x  in tind  x  
case    x is a leaf  then the equation is of the form   x    e where e is known  in
this case we return e 
case    the equation associated with  x  is of the form

 x    ai x    y   bi  x    z 
where z is a leaf and therefore  z   is known  in this case we return
ai x     query  y   bi  x     z 
the case where y is the leaf is analogous 
figure    function to compute the  value of a node 

     queries

it is relatively easy to use a similar recursive procedure to perform   x  queries  unfortunately  this approach yields an o log n   time algorithm if we simply use recursion to
calculate  terms and calculate  terms using our earlier procedure  this is because there
will be o log n   recursive calls to calculate  values  but each is defined by an equation
that also involves a  term taking o log n   time to compute 
to achieve o log n   time  we shall instead implement   x  queries by defining a procedure calc  x  i  which returns a triple of vectors hp  l  ri such that p     x   l    y  
and r    z   where y and z are the left and right children  respectively  of x in ti 
to compute   x  for some node x we can do the following  let i   ind  x   the equation
for   x  in ti is of the form 

 x    di x     u   ci x    v  
where u is the parent of x in ti and v its sibling  we then call procedure calc  u  i     
which will return the triple h  u    v    x i  from which we immediately can compute   x 
using the above equation 
procedure calc  x  i  can be implemented in the following fashion 
case    if ti is a   node tree with x as its root  then both children of x are leaves  hence
their  values are known  and   x  is a given sequence of prior probabilities for x 
case    if x does not appear in ti     then one of x s children is a leaf  say e which is raked
at level i  let z be the other child  we call calc  u  i       where u is the parent of
x in ti  and receive back h u    z    v i or h u    v    z i according to whether x
  

fidelcher  grove  kasif   pearl

function  update  term   value  i 
   find the  at most one  equation in ti   defining some ai or bi   in which term
appears on the right hand side  let term  be the matrix defined by this equation
 i e   its left hand side  
   update term   let value be the new value 
   call  update  term    value  i      recursively 
figure    the update procedure 
was a left or right child of u in ti  and v is u s other child   we can now compute   x 
from   u  and  v    and we have  e  and  z    so we can return the necessary triple 
specifically 

 x   

 

di x     u   ai    u    v  
di x     u   bi    u    v  

where the choice depends on whether x is the right or left child  respectively  of u in ti 
case    if x does appear in ti    then we call calc  x  i       this returns the correct
value of   x   for any child z of x in ti that remains a child of x in ti     it also returns
the correct value of  z    if z is a child of x that does not occur in ti     then it must be
the case that z was raked at level i so that one of z  s children  say e  is a leaf and let the
other child be q   in this situation calc  x  i      has returned the value of  q   and
we can compute

 z    ai z    e   bi  z    q 
and return this value 
in all three cases  there is a constant amount of work done in addition to a single recursive
call that uses equations at a higher level  since there are o log n   levels of equations  each
requiring only matrix by vector multiplication  the total work done is o k  log n   

   extended example
in this section we illustrate the application of our algorithms to a specific example  consider
the sequence of contracted trees shown in figure    corresponding to these trees we have
  

fiqueries   updates in probabilistic networks

xl
 

t   







xl

 
 
 
 

 

xl
 

xl
 

 a
 a

a

e 

xl
 

 a
 a

a

e 

e 



e 

 a
 a

a

 a
 a

a

xl
 

 a
 a

xl
 

xl
 

xl
 

 a
 a
a


z
z

c
c
c
c

   
 
 

 
 

xl
 

t   

z
z
z

e 

 a
 a
a


xl
 

a

e 

 a
 a
a


xl
 

e 

 a
 a

a

e 

e 

e 

t  

xl
 

 a
 a

a

e 

e 

t   xl
 

 a
 a
a


xl
 

e 

e 

e 

figure    example of tree contraction 

  

 a
 a

a

e 

e 

e 

e 

fidelcher  grove  kasif   pearl

such equations as the following 
for t   
 x     a  x     x    b   x     x   
  
 





 x     d   x     x   c  x    x    
  
 





for t   
 x     a  x     x    b   x     e   
  
 











 x     d   x     x   c  x    e    
  
 





for t   
 x     a  x     x    b   x     e   
  
 







 x     d   x     x   c  x    e    
  
 









for t   
 x     a  x     e    b   x    e   






now consider  for instance  the effect of an update for e    since it is raked immediately 
the new value of  e   is incorporated in 
b   x      b   x    diaga   x     e   b   x   
from subsequent rake operations we know that a  x    depends on b   x    and a   x  
depends on a   x    so we must also update these values as follows 
a   x      a   x   diagb   x     e    a   x  
a   x      a   x   diagb   x     e    a   x  


















finally  consider a query for x    since x  is raked together with e  in t    we follow
the steps outlined above and generate the following calls  calc  x       calc  x      
calc  x       and calc  x       this provides us with   x    in this case   x  
is particularly easy to compute since both x   s children are leaf nodes  then we simply
compute   x     x   and then normalize  giving us the conditional marginal distribution
bel  x   as required 

   join trees

perhaps the best known technique for computing with arbitrary  i e   not singly connected 
bayesian networks uses the idea of join trees  junction trees   lauritzen   spiegelhalter 
       in many ways a join tree can be thought of as a causal tree  albeit one with somewhat
special structure  thus the algorithm in the previous section can be applied  however  the
structure of a join tree permits some optimization  which we describe in this section  this
becomes especially relevant in the next section  where we use the join tree technique to
show how o log n   updates and queries can be done for arbitrary polytrees  our review
of join trees and their utility is extremely brief and quite incomplete  for clear expositions
see  for instance  spiegelhalter et al         and pearl        
given any bayesian network  the first step towards constructing a join tree is to moralize
the network  insert edges between every pair of parents of a common node  and then treat all
  

fiqueries   updates in probabilistic networks

edges in the graph as being undirected  spiegelhalter et al          the resulting undirected
graph is called the moral graph  we are interested in undirected graphs that are chordal  
every cycle of length   or more should contain a chord  i e   an edge between two nodes
that are non adjacent in the cycle   if the moral graph is not chordal  it is necessary to add
edges to make it so  various techniques for this triangulation stage are known  for instance 
see spiegelhalter et al         
if p is a probability distribution represented in a bayesian network g    v  e    and
m    v  f   is the result of moralizing and then triangulating g  then 
   m has at most jv j cliques   say c          cjv j 
   the cliques can be ordered so that for each i     there is some j  i    i such that

ci   cj i    ci    c    c            ci    
the tree t formed by treating the cliques as nodes  and connecting each node ci to
its  parent  cj  i   is called a join tree 
   p  

y

i

p cijcj i  

   p cijcj  i     p cijcj  i    ci  
from   and    we see that if we direct the edges in t away from the  parent  cliques 
the resulting directed tree is in fact a bayesian causal tree that can represent the original
distribution p  this is true no matter what the form of the original graph  of course  the
price is that the cliques may be large  and so the domain size  the number of possible values
of a clique node  can be of exponential size  this is why this technique is not guaranteed
to be ecient 
we can use the rake technique of section   on the directed join tree without any
modification  however  property   above shows that the conditional probability matrices
in the join tree have a special structure  we can use this to gain some eciency  in the
following  let k be the domain size of the variables in g as usual  let n be the maximum
size of cliques in the join tree  without loss of generality we can assume that all cliques are
of the same size  because we can add  dummy  variables   thus the domain size of each
clique is k   kn   finally  let c be the maximum intersection size of a clique and its parent
 i e   jcj  i    cij  and l   kc  
in the standard algorithm  we would represent p cijcj  i   as a k  k matrix  mc jc  
however  p ci jcj  i    ci  can be represented as a smaller l  k matrix  mc jc  c   by
property   above  mc jc is identical to mc jc  c   except that many rows are repeated 
thus there is a k  l matrix j such that
i

i

i

i

j  i 

i

j  i 

i

i

j  i 

mc jc   j  mc jc
i

j  i 

j  i 

j  i 

 ci  

 j is actually a simple matrix whose entries are   and    with exactly one   per row  however
we do not use this fact  
   a clique is a maximal completely connected subgraph 

  

fidelcher  grove  kasif   pearl

our claim is that  in the case of join trees  the following is true  first  the matrices

ai and bi used in the rake algorithm can be stored in factored form  as the product of
two matrices of dimension k  l and l  k respectively  so  for instance  we factor ai
as ali  ari   we never need to explicitly compute  or store  the full matrices  as we have
just seen  this claim is true when i     because the m matrices factor this way  the proof
for i     uses an inductive argument  which we illustrate below  the second claim is that 

when the matrices are stored in factored form  all the matrix multiplications used in the
rake algorithm are of one of the following types     an l  k matrix times a k  l matrix 
   an l  k matrix times a k  k diagonal matrix     an l  l matrix times an l  k
matrix  or    an l  k matrix times a vector 
to prove these claims consider  for instance  the equation defining bi   in terms of lowerlevel matrices  from section    bi    u    bi  u   diaga  x  e   bi  x   but  by assumption 
this is 
 bil  u   bir  u    diag a  x a  x   e    bil  x   bil  x   
which  using associativity  is clearly equivalent to
h
i
bil  u     bir  u   diaga  x  a  x  e     bil x    bil  x   
however  every multiplication in this expression is one of the forms stated earlier  identifying
bil    u  as bil u  and bir    u  as the bracketed part of the expression proves this case  and
of course the case where we rake a left child  so that ai    u  is updated  is analogous 
thus  even using the most straightforward technique for matrix multiplication  the cost of
updating bi   is o kl     o kn  c    this contrasts with o k    if we do not factor the
matrices  and may represent a worthwhile speedup if c is small  note that the overall time
for an update using this scheme is o kn  c log n    queries  which only involve matrix by
vector multiplication  require o kn c log n   time 
for many join trees the difference between n and log n is unimportant  because the
clique domain size k is often enormous and dominates the complexity  indeed  k and l
may be so large that we cannot represent the required matrices explicitly  of course  in such
cases our technique has little to offer  but there will be other cases in which the benefits
will be worthwhile  the most important general class in which this is so  and our immediate
reason for presenting the technique for join trees  is the case of polytrees 
i

l
i

r
i

l
i

r
i

   polytrees

a polytree is a singly connected bayesian network  we drop the assumption of section  
that each node has at most one parent  polytrees offer much more exibility than causal
trees  and yet there is a well known process that can update and query in o n   time  just
as for causal trees  for this reason polytrees are an extremely popular class of networks 
we suspect that it is possible to present an o log n   algorithm for updates and queries
in polytrees  as a direct extension of the ideas in section    instead we propose a different
technique  which involves converting a polytree to its join tree and then using the ideas of
the preceding section  the basis for this is the simple observation that the join tree of a
polytree is already chordal  thus  as we show in detail below  little is lost by considering
the join tree instead of the original polytree  the specific property of polytrees that we
require is the following  we omit the proof of this well known proposition 
  

fiqueries   updates in probabilistic networks

proposition    if t is the moral graph of a polytree p    v  e   then t is chordal  and
the set of maximal cliques in t is ffv g   parents  v     v   v g 
let p be the maximum number of parents of any node  from the proposition  every
maximal clique in the join tree has at most p    variables  and so the domain size of a node
in the join tree is k   kp     this may be large  but recall that the conditional probability
matrix in the original polytree  for a variable with p parents  has k entries anyway since we
must give the conditional distribution for every combination of the node s parents  thus k
is really a measure of the size of the polytree itself 
it now follows from the proposition above that we can perform query and update in
polytrees in time o k   log n    simply by using the algorithm of section   on the directed
join tree  but  as noted in section    we can do better  recall that the savings depend on
c  the maximum size of the intersection between any node and its parent in the join tree 
however  when the join tree is formed from a polytree  no two cliques can share more than a
single node  this follows immediately from proposition    for if two cliques have more than
one node in common then there must be either two nodes that share more than one parent 
or else a node and one of its parents that both share yet another parent  neither of these is
consistent with the network being a polytree  thus in the complexity bounds of section   
we can put c      it follows that we can process updates in o kk c log n     o kp   log n  
time and queries in o kp   log n   

   application  towards automated site specific muta genesis

an experiment which is commonly performed in biology laboratories is a procedure where
a particular site in a protein is changed  i e   a single amino acid is mutated  and then
tested to see whether the protein settles into a different conformation  in many cases  with
overwhelming probability the protein does not change its secondary structure outside the
mutated region  this process is often called muta genesis  delcher et al         developed a
probabilistic model of a protein structure which is basically a long chain  the length of the
chain varies between         nodes  the nodes in the network are either protein structure
nodes  ps nodes  or evidence nodes  e nodes   each ps node in the network is a discrete
random variable xi that assumes values corresponding to descriptors of secondary sequence
structure  helix  sheet or coil  with each ps node the model associates an evidence node
that corresponds to an occurrence of a particular subsequence of amino acids at a particular
location in the protein 
in our model  protein structure nodes are finite strings over the alphabet fh  e   c g  for
example the string hhhhhh is a string of six residues in an ff helical conformation  while
eecc is a string of two residues in a fi  sheet conformation followed by two residues folded as
a coil  evidence nodes are nodes that contain information about a particular region of the
protein  thus  the main idea is to represent physical and statistical rules in the form of a
probabilistic network 
in our first set of experiments we converged on the following model that  while clearly
biologically naive  seems to match in prediction accuracy many existing approaches such as
neural networks  the network looks like a set of ps nodes connected as a chain  to each
such node we connect a single evidence node  in our experiments the ps nodes are strings
of length two or three over the alphabet fh  e   c g and the evidence nodes are strings of the
  

fidelcher  grove  kasif   pearl


cc

 
gs


 ch
  
 sa


 hh
  
 at






figure    example of causal tree model using pairs  showing protein segment gsat with
corresponding secondary structure cchh 
same length over the set of amino acids  the following example clarifies our representation 
assume we have a string of amino acids gsat  we model the string as a network comprised
of three evidence nodes gs  sa  at and three ps nodes  the network is shown in figure   
a correct prediction will assign the values cc  ch  and hh to the ps nodes as shown in the
figure 
now that we have a probabilistic model  we can test the robustness of the protein or
whether small changes in the protein affect the structure of certain critical sites in the
protein  in our experiments  the probabilistic network performs a  simulated evolution  of
the protein  namely the simulator repeatedly mutates a region in the chain and then tests
whether some designated sites in the protein that are coiled into a helix are predicted to
remain in this conformation  the main goal of the experiment was to test if stable bonds far
away from the mutated location were affected  our previous results  delcher et al        
support the current thesis in the biology community  namely that local distant changes
rarely affect structure 
the algorithms we presented in the previous sections of the paper are perfectly suited
for this type of application and are predicted to generate a factor of    improvement in
eciency over the current brute force implementation presented by delcher et al        
where each change is propagated throughout the network 

   summary
this paper has proposed several new algorithms that yield a substantial improvement in the
performance of probabilistic networks in the form of causal trees  our updating procedures
absorb sucient information in the tree such that our query procedure can compute the
correct probability distribution of any node given the current evidence  in addition  all
procedures execute in time o log n    where n is the size of the network  our algorithms
are expected to generate orders of magnitude speed ups for causal trees that contain long
paths  not necessarily chains  and for which the matrices of conditional probabilities are
relatively small  we are currently experimenting with our approach with singly connected
networks  polytrees   it is likely to be more dicult to generalize the techniques to general
networks  since it is known that the general problem of inference in probabilistic networks is
np  hard  cooper         it obviously is not possible to obtain polynomial time incremental
  

fiqueries   updates in probabilistic networks

solutions of the type discussed in this paper for general probabilistic networks  the other
natural open question is extending the approach developed in this paper to other dynamic
operations on probabilistic networks such as addition and deletion of nodes and modifying
the matrices of conditional probabilities  as a result of learning  
it would also be interesting to investigate the practical logarithmic time parallel algorithms for probabilistic networks on realistic parallel models of computation  one of the
main goals of massively parallel ai research is to produce networks that perform real time
inference over large knowledge bases very eciently  i e   in time proportional to the depth
of the network rather than the size of the network  by exploiting massive parallelism  jerry
feldman pioneered this philosophy in the context of neural architectures  see stanfill and
waltz        shastri        and feldman and ballard         to achieve this type of performance in the neural network framework  we typically postulate a parallel hardware that
associates a processor with each node in a network and typically ignores communication requirements  with careful mapping to parallel architectures one can indeed achieve ecient
parallel execution of specific classes of inference operations  see mani and shastri       
kasif        and kasif and delcher         the techniques outlined in this paper presented
an alternative architecture that supports very fast  sub linear time  response capability on
sequential machines based on preprocessing  however  our approach is obviously limited to
applications where the number of updates and queries at any time is constant  one would
naturally hope to develop parallel computers that support real time probabilistic reasoning
for general networks 

acknowledgements
simon kasif s research at johns hopkins university was sponsored in part by national
science foundation under grants no  iri          iri         and iri         

references

berger  t     ye  z          entropic aspects of random fields on trees  ieee trans  on
information theory                    
chelberg  d  m          uncertainty in interpretation of range imagery  in proc  intern 
conference on computer vision  pp          
cohen  r  f     tamassia  r          dynamic trees and their applications  in proceedings
of the  nd acm siam symposium on discrete algorithms  pp        
cooper  g          the computational complexity of probabilistic inference using bayes
belief networks  artificial intelligence              
delcher  a     kasif  s          improved decision making in game trees  recovering from
pathology  in proceedings of the      national conference on artificial intelligence 
delcher  a  l   kasif  s   goldberg  h  r     hsu  b          probabilistic prediction of protein secondary structure using causal networks  in proceedings of      international
conference on intelligent systems for computational biology  pp          
  

fidelcher  grove  kasif   pearl

duda  r     hart  p          pattern classification and scene analysis  wiley  new york 
feldman  j  a     ballard  d          connectionist models and their properties  cognitive
science             
frederickson  g  n          a data structure for dynamically maintaining rooted trees  in
proc   th annual symposium on discrete algorithms  pp          
hel or  y     werman  m          absolute orientation from uncertain data  a unified
approach  in proc  intern  conference on computer vision and pattern recognition 
pp        
karp  r  m     ramachandran  v          parallel algorithms for shared memory machines 
in van leeuwen  j   ed    handbook of theoretical computer science  pp          
north holland 
kasif  s          on the parallel complexity of discrete relaxation in constraint networks 
artificial intelligence              
kasif  s     delcher  a          analysis of local consistency in parallel constraint networks 
artificial intelligence     
kosaraju  s  r     delcher  a  l          optimal parallel evaluation of tree structured
computations by raking  in reif  j  h   ed    vlsi algorithms and architectures 
proceedings of      aegean workshop on computing  pp           springer verlag 
lncs     
lauritzen  s     spiegelhalter  d          local computations with probabilities on graphical
structures and their applications to expert systems  j  royal statistical soc  ser  b 
            
mani  d     shastri  l          massively parallel reasoning with very large knowledge
bases  tech  rep   intern  computer science institute 
miller  g  l     reif  j          parallel tree contraction and its application  in proceedings
of the   th ieee symposium on foundations of computer science  pp          
pearl  j          probabilistic reasoning in intelligent systems  morgan kaufmann 
peot  m  a     shachter  r  d          fusion and propagation with multiple observations
in belief networks  artificial intelligence              
rachlin  j   kasif  s   salzberg  s     aha  d          towards a better understanding of
memory based and bayesian classifiers  in proceedings of the eleventh international
conference on machine learning  pp          new brunswick  nj 
shastri  l          a computational model of tractable reasoning  taking inspiration from
cognition  in proceeding of the      intern  joint conference on artificial intelligence 
aaai 
  

fiqueries   updates in probabilistic networks

spiegelhalter  d   dawid  a   lauritzen  s     cowell  r          bayesian analysis in expert
systems  statistical science                 
stanfill  c     waltz  d          toward memory based reasoning  communications of the
acm                     
wilsky  a          multiscale representation of markov random fields  ieee trans  signal
processing                

  

fi