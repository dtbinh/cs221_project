journal artificial intelligence research                 

submitted       published     

partially controlled multi agent systems
ronen i  brafman

brafman cs ubc ca

computer science department
university british columbia
vancouver  b c   canada v l  z 

moshe tennenholtz

moshet ie technion ac il

industrial engineering management
technion   israel institute technology
haifa        israel

abstract

motivated control theoretic distinction controllable uncontrollable
events  distinguish two types agents within multi agent system  controllable
agents   directly controlled system s designer  uncontrollable agents  
designer s direct control  refer systems partially
controlled multi agent systems  investigate one might uence behavior
uncontrolled agents appropriate design controlled agents  particular 
wish understand problems naturally described terms  methods
applied uence uncontrollable agents  effectiveness methods 
whether similar methods work across different domains  using game theoretic framework 
paper studies design partially controlled multi agent systems two contexts 
one context  uncontrollable agents expected utility maximizers 
reinforcement learners  suggest different techniques controlling agents 
behavior domain  assess success  examine relationship 

   introduction
control agents central research topic two engineering fields  artificial intelligence  ai  discrete events systems  des   ramadge   wonham         one
particular area fields concerned multi agent environments 
examples include work distributed ai  bond   gasser         work decentralized
supervisory control  lin   wonham         fields developed
techniques incorporated particular assumptions models  hence 
natural techniques assumptions used one field may adopted
may lead new insights field 
difference ai work multi agent systems  work decentralized discrete
event systems distinguishes controllable uncontrollable events  controllable
events events directly controlled system s designer  uncontrollable events directly controlled system s designer  translating terminology context multi agent systems  introduce distinction two
types agents  controllable agents   directly controlled system s designer 
uncontrollable agents   designer s direct control  leads
c      ai access foundation morgan kaufmann publishers  rights reserved 

fibrafman   tennenholtz

naturally concept partially controlled multi agent system  pcmas 
following design challenge  ensuring agents system behave appropriately
adequate design controllable agents  believe many problems
naturally formulated instances pcmas design  goal characterize important
instances design problem  examine tools used solve it 
assess effectiveness generality tools 
distinguishes partially controlled multi agent systems ai context similar models des structural assumptions make uncontrolled agents
involved  unlike typical des models concerned physical processes devices  ai particularly interested self motivated agents  two concrete examples
rational agents  i e   expected utility maximizers  learning agents  e g   reinforcement learners  indeed  examples constitute two central models self motivated
agents game theory decision theory  referred educative evolutive models
 e g   see gilboa   matsui         special nature uncontrollable agents
special structure uncontrollable events induce differentiates pcmas
corresponding models des literature  difference raises new questions
suggests new perspective design multi agent systems  particular  calls
techniques designing controllable agents that  exploiting structural assumptions 
uence behavior uncontrollable agents lead system desired
behavior 
order understand issues  study two problems stated solved
adopting perspective pcmas design  problems
interest large community  problems goal uence
behavior agents control  exert uence indirectly
choosing suitable behaviors agents direct control  one case 
attempt uence behavior rational agents  case  try
uence learning agents 
first study concerned enforcement social laws  number
agents designed different designers work within shared environment  beneficial
impose certain constraints behavior  that  overall  system function
better  example  shoham tennenholtz        show imposing certain  trac
laws   considerably simplify task motion planning robot  still
enabling ecient motions  indeed  see later  conventions heart
many coordination techniques multi agent systems  yet  without suitable mechanisms 
rational agents may incentive follow conventions  show how 
certain cases  use perspective partially controlled multi agent systems
structural assumption rationality enforce conventions 
second study involves two agent system consisting teacher student 
teacher knowledgeable agent  student agent learning
behave domain  goal utilize teacher  which control 
improve behavior student  which controlled us   hence 
instance partially controlled multi agent systems structural assumption
uncontrolled agent employs particular learning algorithm 
studies presented paper suggest techniques achieving satisfactory system
behavior design controllable agents  relevant  techniques
   

fion partially controlled multi agent systems

experimentally assessed  beyond formulation solution two interesting problems multi agent system design  paper suggests general perspective certain
design problems  although feel still premature draw general conclusion
potential general theory pcmas design  certain concepts  punishment
reward  suggest central area 
paper organized follows  section    describe problem enforcing
social behavior multi agent systems  section   describe standard game theoretic
model problem suggest mechanism threats punishments general
tool class problems  issues pertain design threats punishments
discussed section    section   introduces second case study pcmas design 
embedded teaching reinforcement learners  context  teacher learner
embedded shared environment teacher serving controller whose
aim direct learner desired behavior  formal model problem
introduced section    section    show derive optimal teaching policies  under
certain assumptions  viewing teaching markov decision process  effectiveness
different teaching policies studied experimentally section    finally  section   
examine relationship methods used two domains
possibility general methodology designing partially controlled multi agent systems 
conclude section     summary discussion related work 

   enforcement social behavior
section introduce problem enforcement social laws multi agent
context  proposed solution falls naturally pcmas design perspective
take  here  explain motivate particular problem social law enforcement
approach solution  sections     formalize investigate approach
framework general game theoretic model 
use following scenario illustrate problem 
hired design new working environment artificial
agents  part job involves designing number agents use
maintain warehouse  agents  designed different designers 
using warehouse obtain equipment  make sure different agents
designed different designers operate eciently environment 
choose introduce number social laws  is  constraints behavior
agents  help agents coordinate activities domain 
rules include number  trac laws   regulating motion domain 
well law specifies every tool used agent must
returned designated storage area  robots programmed follow
laws  expect others so  laws quite successful  allow ecient activity warehouse  new designer arrives 
pressed corporate bosses deliver better performance  decides
exploit rules  designs agent locally maximize performance 
regardless social laws  do 
   

fibrafman   tennenholtz

multi participant environments  one above  agent might
dynamic goals  interested finding ways agents coexist
achieving goals  several approaches coordination agent activity discussed
distributed systems dai literature  examples are  protocols reaching
consensus  dwork   moses         rational deals negotiations  zlotkin   rosenschein 
      kraus   wilkenfeld        rosenschein   genesereth         organizational structures  durfee  lesser    corkill        fox        malone         social laws  moses
  tennenholtz        shoham   tennenholtz        minsky        briggs   cook        
methods  behavior agent predetermined prescribed
certain stage  example  content deal reached  outcome
negotiation process completed  social law instituted  work
relies assumption agents follow prescribed behaviors  e g   obey
law stick agreement  assumption central success
methods  however  makes agents follow rules vulnerable rational agent
performs local maximization payoff  exploiting knowledge others follow
rules  example  new designer may program robot return tools 
saving time required so  thus causing agents fail tasks 
despite somewhat futuristic avor  although instances shared environments
beginning appear cyberspace   scenario useful illustrating vulnerability
popular coordination mechanism appearing multi agent literature
within ai  e g   see bond   gasser        assume agents involved
fully rational  aside  note that  case  actually need attribute much
intelligence agents themselves  sucient assume designers
design way maximizes utility  disregarding utility
agents 
order handle problem need modify existing design paradigms 
adopting perspective partially controlled multi agent systems  obtain one possible
handle problem  requires making following basic assumption 
original designer  scenario  controls number reliable agents  
basic idea reliable agents designed punish agents
deviate desirable social standard  punishment mechanism  hardwired   unchangeable  common knowledge  agents controlled
original designer aware punishment possibility  punishment
mechanism well designed  deviations social standard become irrational 
result  deviation actually occur punishment actually executed  hence 
making agents bit sophisticated  prevent temptation breaking
social laws 
suggested solution adopt perspective partially controlled multi agent systems  agents controllable  others uncontrollable assumed
adopt basic model expected utility maximization  punishment mechanism
 part of  control strategy used uence behavior uncontrolled
agents 
   ease exposition  assume reliable agents follow designer s instructions  assume
non malicious failures  crash failures  possible 

   

fion partially controlled multi agent systems

   dynamic game theoretic model

section introduce basic game theoretic model  use study
problem enforcement social behavior solution  later on  sections     
model used study embedded teaching  wish emphasize model
use common model representing emergent behavior population 
 e g   huberman   hogg        kandori  mailath    rob        altenberg   feldman 
      gilboa   matsui        weidlich   haag        kinderman   snell        

definition   k person game g defined k dimensional matrix size n 

nk   nm number possible actions  or strategies  m th agent  entries
vectors length k real numbers  called payoff vectors  joint strategy
tuple  i   i          ik      j k  case   ij nj  
intuitively  dimension matrix represents possible actions one k
players game  following convention used game theory  often use term
strategy place action   since dimensions matrix n  nk   i th
agent ni possible strategies choose from  j  th component vector residing
 i   i          ik   cell  i e   mi   i       ik   represents feedback player j receives
players  joint strategy  i   i          ik    is  agent m s strategy im
  k  here  use term joint strategy refer combined choice
strategies agents 

definition   n k g iterative game consists set n agents given k person
game g   game g played repetitively unbounded number times  iteration 
random k tuple agents play instance game  members k tuple
selected uniform distribution set agents 

every iteration n k g game represents local interaction k agents  agents
play particular iteration game must choose strategy use
interaction  agent use different strategies different interactions  outcome
iteration represented payoff vector corresponding agents  joint strategy 
intuitively  payoff tells us good outcome joint behavior point
view agent  many situations represented n k g game  example 
 trac  aspect multi agent system represented n k g game 
time number agents meet intersection  encounter instance
game agents choose number strategies  e g   move ahead  yield 
payoff function gives utility set strategies  example  time
two agents meet agents choose move ahead  collision occurs payoffs
low 
definition   joint strategy game g called ecient sum players  payoffs
maximal 
   paper use term emergent behavior classical mathematical economics interpretation 
evolution behavior based repetitive local interactions  usually pairs of  agents 
agent may change strategy following interactions based feedback received previous
interactions 

   

fibrafman   tennenholtz

hence  eciency one global criterion judging  goodness  outcomes
system s perspective  unlike single payoffs describe single agent s perspective  

definition   let fixed joint strategy given game g  payoff pi s  player
i  instance g joint strategy s  played  pi  s  pi  s    say
i s punishment w r t  pi  s    pi  s     otherwise say benefit w r t 
pi s     pi  s  
hence  punishment benefit w r t  joint strategy measure gain  benefit 
loss  punishment  agent somehow change joint behavior agents
s   
current discussion punishment benefit always respect chosen
ecient solution 
designers multi agent system  would prefer ecient possible 
cases entails behavior sense unstable  is  individual agents
may locally prefer behave differently  thus  agents may need constrained behave
way locally sub optimal  refer constraints exclude
possible behaviors social laws  
due symmetry system assumption agents
rational utility additive  i e   utility two outcomes sum
utilities   clear agent s expected payoff higher one obtained
using strategies giving ecient solution  thus  clear case ecient
solution fair  sense agents get least could law
existed  solution provide better expected payoff 
however  good intentions designer creating environment beneficial
participating agents  may backfire  social law provides information behavior
agents conforming it  information agents  or respective designers 
use increase expected payoff 
example   assume playing n   g game g prisoner s dilemma 
represented strategic form following matrix 
agent  
agent  
 
 
 
              
 
                
ecient solution game obtained players play strategy    assume
solution chosen original designer  followed agents
control 
designer new agent function environment social law
obeyed may tempted program agent conform chosen law  instead 
program agent play strategy maximizes expected outcome  strategy
   addition payoffs utilities across agents dangerous practice  however  particular model 
shown system joint strategies always ecient maximizes agent s expected
cumulative rewards 

   

fion partially controlled multi agent systems

    new agent obtain payoff    playing one  good  agents 
thus  even though social law accepted order guarantee payoff  
agent   good  agents obtain payoff     playing non conforming
agents  note new designer exploits information strategies  good  players 
dictated social law  agents controlled new designer uncontcolable
agents  behavior dictated original designer 
agents conforming social law referred malicious agents   order
prevent temptation exploit social law  introduce number punishing
agents   designed initial designer  play  irrationally  detect behavior
conforming social law  attempting minimize payoff malicious agents 
knowledge future participants punishment policy would deter deviations eliminate need carrying out  hence  punishing behavior used
threat aimed deterring agents violating social law  threat  part of 
control strategy adopted controllble agents order uence behavior
unconrollable agents  notice control strategy relies structural assumption
unconrollable agents expected utility maximizers 
define minimized malicious payoff minimal expected payoff malicious players guaranteed punishing agents  punishment exists  
minimized malicious payoff lower expected payoff obtained playing according
social law  strategy guarantees malicious agents expected payoff lower
one obtained playing according social law called punishing strategy  
throughout section following section make natural assumption
expected payoff malicious agents playing greater
one obtained ecient solution   
example    continued  example    punishment would simply play strategy
  on  may cause payoff punishing agent decrease  would
guarantee malicious agent obtains payoff better    playing punishing
agent  many non malicious agents punishing  malicious agents  expected payoff
would decrease become smaller payoff guaranteed social law  strategy
  would punishing strategy 

   design punishments

previous section described general model multi agent interaction showed
perspecive partially controlled multi agent systems leads one possible solution
problem enforcing social behavior setting  via idea threats
punishments  proceed examine issue punishment design 
assume p agents designer controls either ability
observe instances game occur  informed outcome
games  c additional agents conform law  that is  play strategies
entailed chosen ecient solution   malicious agents  bound
law 
   assumptions may treated similarly 

   

fibrafman   tennenholtz

would answer questions as  game offer ability punish 
minimized malicious payoff  optimal ratio p  c  m 
difference different social laws 
example    continued  consider example   again  observed
cause expected maximal loss malicious agents                 occurs
punishing agents play strategy    gain malicious agent makes
playing agent following social law               order punishing
strategy effective  must case expected payoff malicious agent
greater expected payoff obtained following social law  order
achieve this  must ensure ratio punishing conforming agents
malicious agent sucient encounters punishing agents  case  assuming
  deviators meet expected benefit   recalling agent equally
likely meet agent  need pc      make incentive deviate negative 
implementing punishment approach requires complex behavior  agents
must able detect deviations well switch new punishing strategy 
whole behavior viewed new  complex  social law  calls
complex agents carry out  makes programming task harder 
clearly  would minimize number complex agents  keeping
benefit malicious behavior negative  here  major question ratio
benefit deviation prospective punishment 
seen example  larger punishment  smaller number
sophisticated punishing agents needed  therefore  would find
strategies minimize malicious agent s payoff  order require
additional definitions 

definition   two person game g zero sum game every joint strategy
players  sum players  payoffs   

hence  zero sum game  win win situations  larger payoff one
agent  smaller payoff agent  convention  payoff matrix two
person zero sum game mention payoffs player   

definition   let g two person game  let pig  s t  payoff player g  where
  f    g  strategies played player     respectively  projected

game  gp   following two person zero sum game  strategies players
g   payoff matrix p gp  s  t     p g  s t   define transposed game g   g  
game g roles players change 

projected game  first agent s payoff equals negated value second agent s
payoff original game  thus  game ects desire lower payoffs
second player original game 
give general result two person game  g  with number strategies  
make use following standard game theoretic definition 
   

fion partially controlled multi agent systems

definition   given game g  joint strategy players nash equilibrium

g whenever player takes action different action   payoff given
players play higher payoff given everybody plays  
is  strategy nash equilibrium game agent obtain better payoff
unilaterally changing behavior agents play according  
nash equilibrium central notion theory non cooperative games  luce  
raiffa        owen        fudenberg   tirole         result  notion well studied
understood  reducing new concepts basic concept may quite useful
design perspective  particular  nash equilibrium always exists finite games 
payoffs prescribed nash equilibria given zero sum game uniquely defined 
show 

theorem   given n   g iterative game  minimized malicious payoff achieved

playing strategy player   prescribed nash equilibrium projected game gp 
playing player    in g    strategy player   prescribed nash equilibrium
projected game  g  p  playing player    in g    

proof  assume punishing agent plays role player    player   adopts

strategy prescribed nash equilibrium player   get better payoff
one guaranteed since deviation player   improve situation  by
definition nash equilibrium   hand  player   cause harm
harm obtained playing strategy   see this  assume player   uses
arbitrary strategy s  player   adopts strategy prescribed   outcome
player   higher one guaranteed playing nash equilibrium
 by definition nash equilibrium   addition  due fact
zero sum game implies outcome player   lower one
guaranteed player   would play according   case punishing agent
player   treated similarly 

example    continued  continuing prisoner s dilemma example  gp would
agent  
agent      
 
      
 
    
nash equilibrium attained playing strategies yielding    example 
 g  p   gp  therefore  punishing strategies strategy     case 

corollary   let n   g iterative game  p punishing agents  let v v 

payoffs nash equilibria gp gpt respectively  which  case  uniquely
defined   let b b  maximal payoffs player   obtain g g respectively 
   notice that  cases  strategies prescribed original game determined strategies
player   nash equilibria projected games 

   

fibrafman   tennenholtz

assuming player   obeying social law  let e e  payoffs player     
respectively  g   players play according ecient solution prescribed
social law  finally  assume expected benefit two malicious agents meet
   necessary sucient condition existence punishing strategy
 n   p   b   b     p  v   v        e   e    
n  
n  

proof  expected payoff obtained malicious agent encountering law 

abiding agent b  b   expected payoff encountering punishing agent   v  v    
order test conditions existence punishing strategy would need
consider best case scenario point view malicious agent  case
non punishing agents law abiding agents  order obtain expected utility
malicious agent make average quantities taking account
proportion law abiding punishing agents population  gives us
expected utility malicious agent    n n     p   b   b       np     v   v     definition 
punishing strategy exists expected utility lower expected
utility guaranteed social law  since expected utility guaranteed
social law e  e   get desired result 
value punishment   v  v   above  independent ecient solution
chosen  e   e  identical ecient solutions  definition  however  b   b  depends
choice ecient solution  number solutions exist  minimizing
b   b  important consideration design social law  affects incentive
 cheat  
 

 

 

 

example   let s look slightly different version prisoner s dilemma  game
matrix

agent  
agent  
 
 
 
              
 
                
  ecient solutions  given joint strategies                     
case       b b      gained playing strategy   instead     case
            b b    
clearly  incentive deviate social law prescribing strategies      
social law prescribing             
summarize  preceding discussion suggests designing number punishing agents 
whose behavior punishment mode prescribed theorem   case n   g games 
ensuring sucient number agents take away incentive deviate
social laws  hence  given malicious agents rational  follow social
norm  consequently  need utilize punishment mechanism 
observed different social laws leading solutions equally ecient different
properties comes punishment design  consequently  assumption
would minimize number punishing agents guaranteeing ecient
   

fion partially controlled multi agent systems

solution participants  choose ecient solution minimizes value
b   b  

   embedded teaching
section move second study pcmas design problem  now 
uncontrollable agent reinforcement learner  choice arbitrary  rational
agents reinforcement learners two major types agents studied mathematical
economics  decision theory  game theory  types agents discussed
work dai concerned self motivated agents  e g   zlotkin   rosenschein 
      kraus   wilkenfeld        yanco   stein        sen  sekaran    hale        
agent s ability function environment greatly affected knowledge
environment  special cases  design agents sucient knowledge
performing task  gold         but  general  agents must acquire information on line
order optimize performance  i e   must learn  one possible approach
improving performance learning algorithms employing teacher  example 
lin        uses teaching example improve performance agents  supplying
examples show task achieved  tan s work       
viewed form teaching agents share experiences  methods nontrivial form communication perception required  strive model broad notion
teaching encompasses behavior improve learning agent s performance 
is  wish conduct general study partially controlled multi agent systems
uncontrollable agent runs learning algorithm  time  want
model clearly delineate limits teacher s  i e   controlling agent s  ability
uence student 
here  propose teaching approach maintains situated  spirit  much
reinforcement learning  sutton        watkins        kaelbling         call
embedded teaching   embedded teacher simply  knowledgeable  controlled agent
situated student shared environment  her  goal lead student
adopt specific behavior  however  teacher s ability teach restricted
nature environment share  repertoire actions limited 
may lack full control outcome actions  example  consider
two mobile robots without means direct communication  robot   familiar
surroundings  robot   not  situation  robot   help robot   reach
goal certain actions  blocking robot   headed wrong
direction  however  robot   may limited control outcome
interaction uncertainty behavior robot   control uncertainty 
nevertheless  robot   specific structure  learner obeying learning scheme 
attempt control indirectly choice actions robot    
   differentiate teacher student  use female pronouns former male pronouns
latter 
   general  fact agent controllable imply perfectly control outcome
actions  choice  hence  robot may controllable sense  running program
supplied us  yet move forward command may always desired outcome 

   

fibrafman   tennenholtz

follows  goal understand embedded teacher help student
adopt particular behavior  address number theoretical questions relating
problem  experimentally explore techniques teaching two types
reinforcement learners 

   basic teaching setting
consider teacher student repeatedly engage joint activity 
student prior knowledge pertaining activity  teacher understands
dynamics  model  teacher s goal lead student adopt particular
behavior interactions  example  teacher student meet occasionally
road teacher wants teach student drive right side  perhaps 
teacher student share resource  cpu time  goal teach
judicious use resource  model encounters     g iterative games 
capture idea teacher knowledgeable student  assume
knows structure game  i e   knows payoff function 
recognizes actions taken play  hand  student know
payoff function  although perceive payoff receives  paper  make
simplifying assumptions teacher student two actions
choose outcome depends choice actions  furthermore 
excluding study section      ignore cost teaching  hence  omit
teacher payoff description   provides basic setting take
first step towards understanding teaching problem  
teaching model concisely modeled     matrix  teacher s actions
designated ii   student s actions designated numbers  
   entry corresponds joint action represents student s payoff
joint action played  suppose matrix figure   
wish teach student use action    stage  assume student
always receives better payoff following action   learn play it 
see situations teaching trivial  assume first row dominates second row  i e     c b   d  case  student naturally prefer
take action    teaching challenging  although might useful speeding
learning process  example    c   b   d  matrix b figure    teacher
make advantage action   noticeable student always playing action
i 
suppose one   c b   holds  case  teaching still easy 
use basic teaching strategy  call preemption   preemption teacher
chooses action makes action   look better action    example 
situation described matrix c figure    teacher always choose action  
   case could made inherent value teaching  may appropriate forum
airing views 
   fact  idea consider basic embedded teaching setting already challenging  later see  basic setting closely related fundamental issue non cooperative
games 

   

fion partially controlled multi agent systems

ii

ii

  b

     

  c
 a 

ii

     
     
 c 

     
 b 

ii

      
     
 d 



ii

       
       
 e 

figure    game matrices a  b  c  d  e  teacher s possible actions ii  
student s possible actions     
next  assume c greater b  matrix figure   
regardless action teacher chooses  student receives higher payoff
playing action    since minf    g   maxf     g   therefore  matter teacher
does  student learn prefer action    teaching hopeless situation 
types interactions isomorphic case c       b 
matrix e figure    still challenging situation teacher action  
dominates action    because                   therefore  preemption cannot work 
teaching strategy exists  complex always choosing action 
since seems challenging teaching situation  devote attention
teaching reinforcement learner choose action   class games 
turns situation quite important game theory multi agent
interaction  projection famous game  prisoner s dilemma  discussed
previous sections  general  represent prisoner s dilemma using
following game matrix 
teacher
student coop defect
student coop
coop  a a   b c  commonly coop  a a 
defect  c b   d d 
defect  c  c 

teacher
defect
  c c 
 d d 

c       b  actions prisoner s dilemma called cooperate  coop 
defect  identify coop actions     defect actions   ii  
prisoner s dilemma captures essence many important social economic situations 
particular  encapsulates notion cooperation  thus motivated enormous discussion among game theorists mathematical economists  for overview  see eatwell 
milgate    newman         prisoner s dilemma  whatever choice one player 
second player maximize payoff playing defect  thus seems  rational 
player defect  however  players defect  payoffs much worse
cooperate 
   

fibrafman   tennenholtz

example  suppose two agents given     moving object 
agent perform task alone  take amount time energy
value      however  together  effort make valued     get
following instance prisoner s dilemma 
agent  
agent   move
rest
move
              
rest               
experimental part study  teacher s task teach student
cooperate prisoner s dilemma  measure success teaching strategy
looking cooperation rate induces students period time  is 
percentage student s actions coop  experimental results presented
paper involving prisoner s dilemma respect following matrix 
teacher
student coop defect
coop                 
defect                 
observed qualitatively similar results instantiations prisoner s
dilemma  although precise cooperation rate varies 

   optimal teaching policies

previous section concentrated modeling teaching context instance
partially controlled multi agent system  determining particular problems
interesting  section start exploring question teacher
teach  first  define optimal policy is  then  define markov decision
processes  mdp   bellman         show certain assumptions teaching
viewed mdp  allow us tap vast knowledge accumulated
solving problems  particular  use well known methods  value
iteration  bellman         find optimal teaching policy 
start defining optimal teaching policy  teaching policy function
returns action iteration  possibly  may depend complete history
past joint actions   right  definition optimal policy  teacher s
motivation may vary  however  paper  teacher s objective maximize
number iterations student s action  good   coop prisoner s
dilemma  teacher know precise number iterations playing 
slightly prefers earlier success later success 
formalized follows  let u a  value teacher places student s
action  a  let teacher s policy  assume induces probability distribution
pr k set possible student actions time k  define value strategy

 
x
val      kek  u 
k  

   

fion partially controlled multi agent systems

ek  u  expected value u 

ek  u   

x pr

a as

 k  a  u a 

here  student s set actions  teacher s goal find strategy
maximizes val    discounted expected value student s actions  example 
case prisoner s dilemma  could
  fcoop defectg u coop      u defect      
next  define mdps  mdp  decision maker continually moving
different states  point time observes current state  receives payoff
 which depends state   chooses action  action current state
determine  perhaps stochastically  next state  goal maximize function
payoffs  formally  mdp four tuple hs  a  p  ri  state space 
decision maker s set possible actions  p            probability
transition states given decision maker s action  r       reward
function  notice given initial state     policy decision maker   p
induces probability distribution ps  k   ps  k  s    probability
kth state obtained s  current state s 
  optimal policy mdp policy maximizes state
discounted sum expected values payoffs received future states  starting
s  i e  
 
x
x
 k  ps  k  s    r s   
k  

 s
 

although may immediately obvious  single policy maximizing discounted sums
starting state exists  well known ways finding policy 
experiments use method based value iteration  bellman        
suppose student set possible states  set actions
  teacher s set actions   moreover  suppose following
properties satisfied 
    student s new state function old state current joint action 
denoted      
    student s action stochastic function current state  probability
choosing state  s  a  
    teacher knows student s state   the natural way happen
teacher knows student s initial state  function   outcome game 
uses simulate agent  
notice assumptions teaching policy function  
know student s next action function next state  know
student s next state function current state  current action  teacher s
current action  hence  next action function current state action 
well teacher s current action  however  know student s current action
function current state  hence  student s next action function
current state teacher s current action  implies knowledge
teacher needs optimally choose current action student s current state 
   

fibrafman   tennenholtz

additional information redundant cannot improve success  generally 
repeat line reasoning indefinitely future  see teacher s
policy function student s state  function  
possible see makings following mdp 
given observation three assumptions  see that  indeed  teacher s
policy induces probability distribution set possible student actions time k 
implies definition val makes sense here 
define teacher s mdp tmdp  h  at  p  u i 

x

p  s  s   at  def
 

 as

 s  as     s as at 
 

 i j defined     j     otherwise   is  probability transition
s  sum probabilities student s actions induce
transition  reward function expected value u 

x

u  s  def
 

as as

 s  as  u as 

theorem   optimal teaching policy given   optimal policy tmdp 
proof  definition    optimal policy tmdp policy  
maximizes
is 

 
x
x
k  p
 

k  

 

s  k  s    u  s    

 

 
x
x
k  p

k  

however  equal

  

 

 
s  k  s    

 
 

x
as as

 s    as  u as   

 
x
x x  s     p
k

k  

 



as as  
 

 
s  k  s   u as  

know ps  k  s    probability s  state student time
k  given teacher uses current state s  hence 

x  s     p

 
 



 
s  k  s  

probability action taken student time k given initial
 current  state s  upon examination  see     identical val    
optimal policy used teaching  teacher possess sucient information determine current state student  even case 
allows us calculate upper bound success val    teaching policy  
number property learning algorithm  measures degree uence
agent given student 
   

fion partially controlled multi agent systems

   experimental study

section describe experimental study embedded teaching  first  define
learning schemes considered  then  describe set results obtained using computer
simulations 

    learning schemes

experiment two types students  one uses reinforcement learning algorithm
viewed q learning one state  uses q learning  choosing
parameters students tried emulate choices made reinforcement learning
literature 
first student  call blind q learner  bql   perceive rewards 
cannot see teacher acted remember past actions  keeps
one value action  example  q  coop  q  defect  case prisoner s
dilemma  update rule following  performed action received reward
r
qnew  a         ff  qold a    r
parameter ff  learning rate  fixed  unless stated otherwise      experiments  wish emphasize although bql bit less sophisticated  real 
reinforcement learners discussed ai literature  which defined below   popular powerful type learning rule  much discussed used literature
 narendra   thathachar        
second student q learner  ql   observe teacher s actions
number possible states  ql maintains q value state action pair 
states encode recent experiences  i e   past joint actions  update rule is 
qnew  s  a         ff  qold  s  a     r   v  s   
r reward received upon performing state s  s  state student
following performance s  called discount factor       unless
otherwise noted  v  s   current estimate value best policy s   
defined maxa as q  s    a   q values initially set zero 
student s update rule tells us q values change result new experiences  must specify q values determine behavior  ql
bql students choose actions based boltzmann distribution  distribution
associates probability ps  a  performance action state  p  a 
bql  
exp q  a  t  
q s  a  t  
def
p
 
ql
 
p
 

 
 
 bql 
ps a  def
  p exp exp 
 
q  s    t  
 a
 a exp q  a   t  
called temperature   usually  one starts high value  
makes action choice random  inducing exploration part student 
slowly reduced  making q values play greater role student s choice action 
use following schedule            n        n            schedule
characteristic properties fast initial decay slow later decay  experiment
fixed temperature 
 

 

   

fibrafman   tennenholtz

approximately optimal policy
 

      iterations
     iterations
     iterations
    iterations

   

fraction coops

fraction coops

 

two q learners

   
   
   

      iterations
     iterations
     iterations
    iterations

   
   
   
   

 

 
 

 

 

 

       
temperature

 

    

 

 

 

 

       
temperature

 

    

figure    fraction coops function temperature approximately optimal
policy  left   teaching  using identical q learner  right   curve
corresponds coop rate fixed number iterations  approx 
optimal policy curves                  iterations nearly identical 

    blind q learners

motivated discussion section   concentrate section
following section teaching context prisoner s dilemma  section    
discuss another type teaching setting  section describes experimental results
bql  examined policy approximates optimal policy  two teaching
methods rely student model 
      optimal policy

first show bqls fit student model section    state space  use
set possible assignment q values  continuous subspace    
discretize  in order able compute policy   obtaining state space
approximately        states  next  notice transitions stochastic function
current state  current q values  teacher s action  see notice q value
updates function current q value payoff  payoff function
teacher s student s actions  student s actions stochastic function
current q value  left side figure   see success teaching using policy
generated using dynamic programming solve optimization problem  curve
represents fraction coops function temperature fixed number
iterations  values means     experiments 
      two q learners

ran experiments two identical bqls  viewed  teaching  using
another q learner  results shown right side figure    temperatures
optimal strategy performs better q learning  teaching  strategy  fact
temperatures     less success rate approaches   beneficial later
add temperature decay  however  see inherent limit ability
   

fion partially controlled multi agent systems

tit for tat
 

      iterations
     iterations
     iterations
    iterations

   

fraction coops

fraction coops

 

  tit for tat

   
   
   

      iterations
     iterations
     iterations
    iterations

   
   
   
   

 

 
 

 

 

 

       
temperature

 

    

 

 

 

 

       
temperature

 

    

figure    fraction coops function temperature teaching strategy based
tft  left   tft  right  
affect behavior higher temperatures  interesting phenomenon phase
transition observed around        qualitative explanation phenomenon
high temperature adds randomness student s choice action  makes
probabilities p  a  less extreme  consequently  ability predict student s behavior
lessens  probability choosing good action  however  randomness
serves lower success rate initially  guarantees level effective cooperation 
approach     temperature increases  finally  notice although
 coop coop  seems best joint action pair agents  two interacting q learners
never learn play joint strategy consistently  although approach     coops
low temperatures 
      teaching without model

teacher precise model student  cannot use techniques
section   derive optimal policy  models  assume teacher
 observe  student s current state  i e  knows student s q values  
therefore explore two teaching methods exploit knowledge game
fact student bql 
methods motivated basic strategy countering student s move 
basic idea try counter good actions student action lead
high payoff  counter bad actions action give low payoff 
ideally  would play coop student plays coop  defect
student plays defect  course  don t know action student choose 
try predict past actions 
assume q values change little one iteration other 
student s likely action next game action took
recent game  therefore  saw student play coop previous turn  play
coop   similarly  teacher follow defect student defect
   

fibrafman   tennenholtz

fraction coops time

fraction coops

 
   
   
   
approximately optimal
q learning
tit for tat
  tit for tat

   
   
    

         
iterations

    

     

figure    fraction coops function time bql using temperature decay
scheme section      teaching strategies shown  approximately optimal strategy  q learning  tft   tft 
part  strategy  called tit for tat  tft short   well known  eatwell et al         
experiments show successful teaching bql  see figure    
experimented variant tft  call  tft  strategy
teacher plays defect observing two consecutive defects part student 
motivated observation certain situations better let student
enjoy free lunch  that is  match defect coop  make coop look bad
him  may cause q value coop low unlikely
try again  two consecutive defects indicate probability student playing
defect next quite high  results  shown figure    indicate strategy worked
better tft  ranges temperature  better q learning  however 
general  tft  tft gave disappointing results   
finally  figure   shows performance four teaching strategies discussed
far incorporate temperature decay  see optimal policy
successful  explained before  teaching easier student predictable 
case temperature lower  temperature decay student spends
time relatively low temperature behaves similarly case fixed 
low temperature  initial high temperature phase could altered behavior 
observe effects 

    teaching q learners

unlike bql  q learners  ql  number possible states encode joint actions
previous games played  ql memory one four possible states  corresponding
four possible joint actions prisoner s dilemma  ql memory
states  encoding sequence joint actions 
complex learning architectures structure  brings certain
problems  one possible problem may structure  teaching resistant  

    sense use identical q learner implies model student  tft
 tft make use model 

   

fion partially controlled multi agent systems

tit for tat

two q learners
 
fraction coops

fraction coops

 
   
   
   

      iterations
     iterations
     iterations
    iterations

   

      iterations
     iterations
     iterations
    iterations

   
   
   
   

 

 
 

 

 

 

       
temperature

 

    

 

 

 

 

       
temperature

 

    

figure    curve shows fraction coops ql function temperature
fixed number iterations tft used teach  left  identical q learner used teach  right   values means     experiments 
real threat added computational complexity  mentioned  approximate
optimal teaching policy bql compute space approximately       
discretized states  representing state bql requires two numbers  one
q value  representing state ql states requires  m     numbers 
one q value state action pair  one encoding current state  size
corresponding discretized state space teacher s markov decision process grows
exponentially m  simplest case memory one  a student four states 
would      states  since solving problem        states took    hours
sun sparcstation     able approximate optimal teaching policies
even simplest ql 
lost  structure may mean complexity  means
properties exploit  reach surprisingly good results exploiting structure
q learners  moreover  using teaching method introduced previous
section  however  ql method takes new meaning suggests familiar
notions reward punishment  interestingly  one may recall punishment
major tool approach enforcement social behavior 
choosing actions  qls  care  immediate rewards 
current action s effect future rewards  makes suitable reward
punishment scheme  idea following  suppose ql something  bad   defect
case   although cannot reliably counter move move lower
reward  punish later choosing action always gives negative
payoff  matter student plays  achieve following student s defect
defect teacher  immediate reward obtained ql playing defect
may high  learn associate subsequent punishment defect action 
thus  may locally beneficial perform defect  may able make
long term rewards defect less desirable  similarly  follow student s coop
reward form coop teacher  since guarantees positive payoff
student 
   

fibrafman   tennenholtz

fraction coops time

fraction coops

 

tit for tat
q learning

   
   
   
   
   
    

         
iterations

    

     

figure    fraction coops ql function time temperature decay tft
q learning teaching strategies 
suggests using tit for tat again  notice bqls  tft cannot understood
reward punishment strategy bqls care immediate outcome
action  value associate action weighted average immediate
payoffs generated playing action 
figure   see success rates tft function temperature  well
rates q learning teaching strategy  latter case  teacher identical
student  apparent tft extremely successful  especially higher temperatures 
interestingly  behavior quite different two qls  indeed  examine
behavior two qls  see that  lesser extent  phase change noticed
bqls still exists  obtain completely different behavior tft used  coop levels
increase temperature  reaching almost           hence  see tft works
better student q learner exhibits certain level experimentation  indeed 
examine success teaching strategies low temperature  see
q learning performs better tft  explains behavior tft ql
temperature decay introduced  described figure    figure  ql seems
effective tft  probably result fact experiment
student s temperature quite low time 
experiments ql remembers last joint action  experimented
ql memory performance worse  explained follows 
ql memory one more  problem fully observable markov decision process
teacher plays tft  tft deterministic function previous joint action 
know q learning converges optimal policy conditions  watkins  
dayan         adding memory effectively adds irrelevant attributes  which  turn 
causes slower learning rate  examined whether  tft would successful
agents memory two  results shown here  success rate
considerably lower tft  although better two qls 
tft performed well teaching strategy  explained motivation using it 
want produce quantitative explanation  one used predict
success vary various parameters  payoff matrix 
   

fion partially controlled multi agent systems

coop rates function dif

fraction coops

 
   
   
   
   
 
       

 

  

     
dif

  

  

  

figure    coop rates function dif     b    a   c     c      b   d    means
    experiments        iterations each  student s memory   
let student s payoff matrix matrix figure    let p probability
student plays coop  let q       p probability student plays
defect  probabilities function student s q values  see description
section       let us assume probabilities p q change considerably
one iteration next  seems especially justified learning rate  ff  small 
given information  student s expected reward playing coop 
tft  teacher s current action student s previous action  assume
teacher play coop probability p  thus  student s expected payoff
playing coop  p   q b   since q learners care discounted future reward
 not current reward   happens next important  since assumed
student cooperated  teacher cooperate next iteration  still
assume p probability student cooperate next  student s expected
payoff next step  p   q c   ignore higher order terms expected reward
playing coop becomes  p   q b    p   q c   expected reward defect thus 
p c   q    p b   q d   therefore  tft succeed teaching strategy when 

p   q b    p   q c    p c   q    p b   q d  
since initially p   q        behavior stage p q approximately equal determine whether tft succeeds  attempt predict
success tft based whether 
dif     b    a   c      c      b   d     
test hypothesis ran tft number matrices using q learners different
discount factors  results figure   show fraction coops       iterations
function dif teacher using tft  temperature decay  see
dif reasonable predictor success     almost rates
       rates      however      successful 
   

fibrafman   tennenholtz

    teaching design tool

section   identified class games challenging teach  previous
sections mostly devoted exploring teaching strategies games
student q learner  one assumptions made teacher trying
optimize function student s behavior care
order achieve optimal behavior  however  often teacher would maximize
function depends behavior student s behavior 
case  even simple games discussed section   pose challenge 
section  examine basic coordination problem  block pushing 
objective teaching  teaching essential obtaining good results 
aim section demonstrate point  hence value understanding
embedded teaching  results show teaching strategy achieves much
better performance naive teaching strategy leads behavior much better
two reinforcement learners 
consider two agents must push block far possible along given path
course        time units  time unit agent push block along
path  either gently  saving energy  hard  spending much energy   block
move iteration c x h        x  h units desired direction  h  c    
constants x number agents push hard  iteration  agents
paid according distance block pushed  naturally  agents wish work
little possible paid much possible  payoff iteration
function cost pushing payment received  assume agent prefers
block pushed hard least one agents  guaranteeing reasonable
payment   agent prefers agent one pushing hard 
denote two actions gentle hard  get related game described
follows 
hard gentle
hard            
gentle            

notice game falls category games teaching easy 
teacher cares student learn push hard  simply push
gently  however  teacher actually trying maximize distance
block moved  teaching strategy may optimal  notice
      instances hard push  naive teaching strategy mentioned yield
      instances hard push  order increase number  need
complex teaching strategy 
results use bql          consider following strategy
teacher  push gently k iterations  start push hard  see 
right selection k   obtain desired behavior  student push hard
time  total number hard push instances improve dramatically 
figure    x coordinate corresponds parameter k   coordinate
corresponds number hard push instances occur       iterations 
results obtained average results    trials 
   

fion partially controlled multi agent systems

     
     
     
     
     
     
     
    

    

    

    

figure    teaching push hard  number hard push instances student      
iterations function number iterations teacher
push hard  avg     trials  
see figure    eciency system non monotonic
threshold k   behavior obtain appropriate selection k much better
would obtained naive teaching strategy  interesting
note existence sharp phase transition performance neighborhood
optimal k   finally  mention agents reinforcement learners 
get      instances  push hard   much worse obtained
knowledgeable agent utilizes knowledge uence behavior
agent 

   towards general theory
two case studies presented paper raise natural question whether general 
domain independent techniques pcmas design exist  whether learned
tools case studies  believe still premature say whether
general theory pcmas design emerge  requires much additional work  indeed 
given considerable differences exist two domains explored
paper  given large range multi agent systems agents envisioned 
doubt existence common low level techniques pcmas design  even within
class rational agents investigated  agents differ considerably
physical  computational  memory capabilities  approach decision making
 e g   expected utility maximization  maximization worst case outcomes  minimization
regret   similarly  problem social law enforcement take different forms 
example  malicious agents could cooperate among other  however 
abstract view taken  certain important unifying concepts appear  namely  punishment
reward 
punishment reward abstract descriptions two types high level feedbacks
controllable agents provide uncontrollable agents  although punishment
reward take different form meaning two domains  cases  uncon   

fibrafman   tennenholtz

trollable agents seem  care  controllable agent s reaction action 
see cases  controllable agents uence uncontrollable agents 
perception worthiness actions  precise manner controllable
agents affect perception differs  cases utilizes inherent aspect
uncertainty uncontrollable agent s world model  case rational agents  despite
perfect knowledge dynamics world  uncertainty remains regarding
outcome non malicious agents  actions  fixing certain course action
controllable agents  uence malicious agents  perception outcome
actions  case learning agent  one affect perception student s
action affecting basic world model  hence  seems high level approach pcmas design two stages  first  analyze factors uence uncontrollable
agent s perception actions  next  analyze ability control factors 
retrospect  implicit approach  study social law enforcement 
used projected game find agent s perception action
changed used indirect mechanism threats enforce perception desired 
study embedded teaching  started analysis different games
possibility affecting agent s perception action games  next  tried
provide perception  case bql students  controllable teacher
complete control elements determine student s perception
random nature student s action  yet  try somehow affect them 
case q learners  direct control available factors determining
student s perception  yet  teacher could control aspects perception 
found sucient 
one might ask representative studies general pcmas domains 
therefore  relevant insight may provide  chosen two domains
belief represent key aspects types agents studied ai 
ai  study dynamic agents act improve state  agents likely
use information revise assessment state world  much learning
agents  need make decisions based current information  much
expected utility maximizers studied  hence  typical multi agent systems studied
ai include agents exhibit one properties 
punishment rewards provide conceptual basis designing controllable agents  mdps supply natural model many domains  particular  mdps
suitable uncertainty exists  stemming either agents  choices
nature  showed section    least principle  use established techniques
obtain strategies controllable agents problem phrased markov
decision process  using mdp perspective cases would require sophisticates tools number important challenges must met first      assumptions
agent s state fully observable environment s state fully observable
unrealistic many domains  assumptions invalid  obtain partially
observable markov decision process  pomdp   sondik         unfortunately  although
pomdps used principle obtain ideal policy agents  current techniques solving pomdps limited small problems  hence  practice one
resort heuristic punishment reward strategies      section  
   

fion partially controlled multi agent systems

one controlling agent  poses natural challenge generalizing tools techniques
mdps distributed decision making processes 

    summary related work
paper introduces distinction controllable uncontrollable agents
concept partially controlled multi agent systems  provides two problems multi agent
system design naturally fall framework pcmas design suggests concrete
techniques uencing behavior uncontrollable agents domains 
work contributes ai research introducing exploring promising perspective
system design contributes des research considering two types structural
assumptions agents  corresponding rational learning agents 
application approach enforcement social behavior introduces new
tool design multi agent systems  punishment threats  used notion
investigated part explicit design paradigm  punishment  deterrence  threats
studied political science  dixit   nalebuff        schelling         yet 
difference line work  and related game theoretic models   consider case
dynamic multi agent system concentrate punishment design issues 
question minimizing number reliable agents needed control system  unlike
much work multi agent systems  assume agents rational agents
law abiding  rather  assumed designer control agents
deviations social laws uncontrolled agents need rational 
notice behavior controllable agents may considered irrational cases 
however  eventually lead desired behavior agents  approaches
negotiations viewed incorporating threats  particular  rosenschein
genesereth        consider mechanism making deals among rational agents  agents
asked offer joint strategy followed agents declare move
would take agreement joint strategy  latter move viewed
threat describing implications refusing agent s suggested joint strategy 
example  prisoner s dilemma setting agent may propose joint cooperation
threaten defecting otherwise  work first part paper could viewed
examining threat could credible effective particular context
iterative multi agent interactions 
part study  proposed embedded teaching situated teaching paradigm
suitable modeling wide range teaching instances  modeled teacher
student players iterated two player game  concentrated particular
iterative game  showed challenging game type  model 
dynamics teacher student interaction made explicit  clearly delineated
limits placed teacher s ability uence student  showed
detailed model student  optimal teaching policies theoretically generated
viewing teaching problem markov decision process  performance
optimal teaching policy serves bound agent s ability uence student 
examined ability teach two types reinforcement learners  particular 
showed optimal policy cannot used  use tft teaching method 
case q learners policy successful  consequently  proposed model
   

fibrafman   tennenholtz

explains success  finally  showed even games teaching
challenging  nevertheless quite useful  moreover  objective
simply teaching student  even simpler domains present non trivial
choices  future hope examine learning architectures see whether
lessons learned domain generalized  whether use methods
accelerate learning domains 
number authors discussed reinforcement learning multi agent systems 
yanco stein        examine evolution communication among cooperative reinforcement learners  sen et al         use q learning induce cooperation two
block pushing robots  matraic        parker        consider use reinforcement
learning physical robots  consider features real robots  discussed
paper  shoham tennenholtz        examine evolution conventions
society reinforcement learners  kittock        investigates effects societal structure multi agent learning  littman        develops reinforcement learning techniques
agents whose goals opposed  tan        examines benefit sharing information
among reinforcement learners  finally  whitehead        shown n reinforcement
learners observe everything decrease learning time factor
n  however  work concerned teaching  question
much uence one agent another  lin        explicitly concerned
teaching way accelerating learning enhanced q learners  uses experience replay supplies students examples task achieved  remarked
earlier  teaching approach different ours  since teachers embedded
student s domain  within game theory extensive body work tries
understand evolution cooperation iterated prisoner s dilemma find
good playing strategies  eatwell et al          work players
knowledge  teaching issue 
last least  work important links work conditioning especially
operant conditioning psychology  mackintosh         conditioning experiments
experimenter tries induce changes subjects arranging certain relationships
hold environment  explicitly  in operant conditioning  reinforcing
subjects  actions  framework controlled agent plays similar role
experimenter  work uses control theoretic approach related problem 
applying two basic ai contexts 
main drawback case studies simple domains conducted  typical initial exploration new problems  future work try
remove limiting assumptions models incorporate  example 
embedded teaching context  assumed uncertainty outcome joint action  similarly  model multi agent interaction section  
symmetric  assuming agents play k roles game  equally
likely play role  etc  another assumption made malicious agents
 loners  acting own  opposed team agents  perhaps importantly 
future work identify additional domains naturally described terms
pcmas formalize general methodology solving pcmas design problems 
   

fion partially controlled multi agent systems

acknowledgements
grateful yoav shoham members nobotics group stanford
input  anonymous referees productive comments suggestions 
especially grateful james kittock comments help improving
presentation paper  research supported fund promotion
research technion  nsf grant iri          afosr grant af f               

references

altenberg  l     feldman  m  w          selection  generalized transmission 
evolution modifier genes  i  reduction principle  genetics          
bellman  r          dynamic programming  princeton university press 
bond  a  h     gasser  l          readings distributed artificial intelligence  ablex
publishing corporation 
briggs  w     cook  d          flexible social laws  proc    th international joint
conference artificial intelligence  pp          
dixit  a  k     nalebuff  b  j          thinking strategically   competitive edge
business  politics  everyday life  norton  new york 
durfee  e  h   lesser  v  r     corkill  d  d          coherent cooperation among communicating problem solvers  ieee transactions computers                
dwork  c     moses  y          knowledge common knowledge byzantine environment  crash failures  information computation                  
eatwell  j   milgate  m     newman  p   eds            new palgrave  game theory 
w w norton   company  inc 
fox  m  s          organizational view distributed systems  ieee trans  sys   man  
cyber             
fudenberg  d     tirole  j          game theory  mit press 
gilboa  i     matsui  a          social stability equilibrium  econometrica         
        
gold  m          complexity automaton identificaion given data  information
control              
huberman  b  a     hogg  t          behavior computational ecologies 
huberman  b  a   ed    ecology computation  elsevier science 
kaelbling  l          learning embedded systems  ph d  thesis  stanford university 
   

fibrafman   tennenholtz

kandori  m   mailath  g     rob  r          learning  mutation long equilibria
games  mimeo  university pennsylvania       
kinderman  r     snell  s  l          markov random fields applications 
american mathematical society 
kittock  j  e          impact locality authority emergent conventions 
proceedings twelfth national conference artificial intelligence  aaai      
pp          
kraus  s     wilkenfeld  j          function time cooperative negotiations 
proc  aaai     pp          
lin  f     wonham  w          decentralized control coordination discrete event
systems  proceedings   th ieee conf  decision control  pp            
lin  l          self improving reactive agents based reinforcement learning  planning 
teaching  machine learning          
littman  m          markov games framework multi agent reinforcement learning 
proc    th int  conf  mach  learn 
luce  r  d     raiffa  h          games decisions  introduction critical survey 
john wiley sons 
mackintosh  n          conditioning associative learning  oxford university press 
malone  t  w          modeling coordination organizations markets  management
science                     
mataric  m  j          reward functions accelerating learning  proceedings
  th international conference machine learning  pp          
minsky  n          imposition protocols open distributed systems  ieee
transactions software engineering                  
moses  y     tennenholtz  m          artificial social systems  computers artificial
intelligence                  
narendra  k     thathachar  m  a  l          learning automata  introduction 
prentice hall 
owen  g          game theory   nd ed    academic press 
parker  l  e          learning cooperative robot teams  proceedings ijcai   
workshop dynamically interacting robots 
ramadge  p     wonham  w          control discrete event systems  proceedings
ieee                
rosenschein  j  s     genesereth  m  r          deals among rational agents  proc 
 th international joint conference artificial intelligence  pp        
   

fion partially controlled multi agent systems

schelling  t          strategy con ict  harvard university press 
sen  s   sekaran  m     hale  j          learning coordinate without sharing information 
proc  aaai     pp          
shoham  y     tennenholtz  m          emergent conventions multi agent systems 
initial experimental results observations  proc   rd international conference principles knowledge representation reasoning  pp          
shoham  y     tennenholtz  m          social laws artificial agent societies  off line
design  artificial inteligence     
sondik  e  j          optimal control partially observable markov processes
infinite horizon  discounted costs  operations research         
sutton  r          learning predict method temporal differences  mach  lear  
            
tan  m          multi agent reinforcement learning  independent vs  cooperative agents 
proceedings   th international conference machine learning 
watkins  c          learning delayed rewards  ph d  thesis  cambridge university 
watkins  c     dayan  p          q learning  machine learning                   
weidlich  w     haag  g          concepts models quantitative sociology 
dynamics interacting populations  springer verlag 
whitehead  s          complexity analysis cooperative mechanisms reinforcement
learning  proceedings aaai     pp          
yanco  h     stein  l          adaptive communication protocol cooperating
mobile robots  animal animats  proceedings second international
conference simulation adaptive behavior  pp          
zlotkin  g     rosenschein  j  s          domain theory task oriented negotiation 
proc    th international joint conference artificial intelligence  pp          

   


