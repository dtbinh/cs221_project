journal of artificial intelligence research              

submitted       published     

the design and experimental analysis of algorithms for
temporal reasoning
peter van beek
dennis w  manchak

department of computing science  university of alberta
edmonton  alberta  canada t g  h 

vanbeek cs ualberta ca
dmanchak vnet ibm com

abstract

many applications from planning and scheduling to problems in molecular biology 
rely heavily on a temporal reasoning component  in this paper  we discuss the design and
empirical analysis of algorithms for a temporal reasoning system based on allen s inuential
interval based framework for representing temporal information  at the core of the system
are algorithms for determining whether the temporal information is consistent  and  if so 
finding one or more scenarios that are consistent with the temporal information  two
important algorithms for these tasks are a path consistency algorithm and a backtracking
algorithm  for the path consistency algorithm  we develop techniques that can result
in up to a ten fold speedup over an already highly optimized implementation  for the
backtracking algorithm  we develop variable and value ordering heuristics that are shown
empirically to dramatically improve the performance of the algorithm  as well  we show
that a previously suggested reformulation of the backtracking search problem can reduce
the time and space requirements of the backtracking search  taken together  the techniques
we develop allow a temporal reasoning component to solve problems that are of practical
size 

   introduction
temporal reasoning is an essential part of many artificial intelligence tasks  it is desirable 
therefore  to develop a temporal reasoning component that is useful across applications 
some applications  such as planning and scheduling  can rely heavily on a temporal reasoning component and the success of the application can depend on the eciency of the
underlying temporal reasoning component  in this paper  we discuss the design and empirical analysis of two algorithms for a temporal reasoning system based on allen s       
inuential interval based framework for representing temporal information  the two algorithms  a path consistency algorithm and a backtracking algorithm  are important for two
fundamental tasks  determining whether the temporal information is consistent  and  if so 
finding one or more scenarios that are consistent with the temporal information 
our stress is on designing algorithms that are robust and ecient in practice  for
the path consistency algorithm  we develop techniques that can result in up to a ten fold
speedup over an already highly optimized implementation  for the backtracking algorithm 
we develop variable and value ordering heuristics that are shown empirically to dramatically
improve the performance of the algorithm  as well  we show that a previously suggested
reformulation of the backtracking search problem  van beek        can reduce the time and
space requirements of the backtracking search  taken together  the techniques we develop
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fivan beek   manchak

relation
symbol inverse
x before y
b
bi
x meets y
m
mi
x overlaps y
o
oi
x starts y

s

si

x during y

d

di

x finishes y

f

fi

x equal y

eq

eq

meaning
x
y
x
y
x
y
x
y
x
y
x
y
x
y

figure    basic relations between intervals
allow a temporal reasoning component to solve problems that are of realistic size  as part of
the evidence to support this claim  we evaluate the techniques for improving the algorithms
on a large problem that arises in molecular biology 

   representing temporal information
in this section  we review allen s        framework for representing relations between intervals  we then discuss the set of problems that was chosen to test the algorithms 

    allen s framework

there are thirteen basic relations that can hold between two intervals  see figure    allen 
      bruce         in order to represent indefinite information  the relation between two
intervals is allowed to be a disjunction of the basic relations  sets are used to list the
disjunctions  for example  the relation fm o sg between events a and b represents the
disjunction   a meets b     a overlaps b     a starts b   let i be the set of all basic
relations  fb bi m mi o oi s si d di f fi eqg  allen allows the relation between two events to
be any subset of i  
we use a graphical notation where vertices represent events and directed edges are
labeled with sets of basic relations  as a graphical convention  we never show the edges
 i  i   and if we show the edge  i  j    we do not show the edge  j  i   any edge for which
we have no explicit knowledge of the relation is labeled with i   by convention such edges
are also not shown  we call networks with labels that are arbitrary subsets of i   interval
algebra or ia networks 
example    allen and koomen        show how ia networks can be used in non linear
planning with concurrent actions  as an example of representing temporal information using
ia networks  consider the following blocks world planning problem  there are three blocks 
a  b  and c  in the initial state  the three blocks are all on the table  the goal state
 

fialgorithms for temporal reasoning

is simply a tower of the blocks with a on b and b on c  we associate states  actions 
and properties with the intervals they hold over  and we can immediately write down the
following temporal information 
initial conditions
goal conditions
initial fdg clear a 
goal fdg on a b 
initial fdg clear b 
goal fdg on b c 
initial fdg clear c 
there is an action called  stack   the effect of the stack action is on x  y    block x is
on top of block y   for the action to be successfully executed  the conditions clear x  and
clear y   must hold  neither block x or block y have a block on them  planning introduces
two stacking actions and the following temporal constraints 
stacking action
stacking action
stack a b  fbi mig initial
stack b c  fbi mig initial
stack a b  fdg clear a 
stack b c  fdg clear b 
stack a b  ffg clear b 
stack b c  ffg clear c 
stack a b  fmg on a b 
stack b c  fmg on b c 
a graphical representation of the ia network for this planning problem is shown in
figure  a  two fundamental tasks are determining whether the temporal information is
consistent  and  if so  finding one or more scenarios that are consistent with the temporal
information  an ia network is consistent if and only if there exists a mapping m of a
real interval m  u  for each event or vertex u in the network such that the relations between events are satisfied  i e   one of the disjuncts is satisfied   for example  consider
the small subnetwork in figure  a consisting of the events on a b   on b c   and goal 
this subnetwork is consistent as demonstrated by the assignment  m  on a b            
m  on b c             and m  goal            if we were to change the subnetwork and insist
that on a b  must be before on b c   no such mapping would exist and the subnetwork
would be inconsistent  a consistent scenario of an ia network is a non disjunctive subnetwork  i e   every edge is labeled with a single basic relation  that is consistent  in our
planning example  finding a consistent scenario of the network corresponds to finding an
ordering of the actions that will accomplish the goal of stacking the three blocks  one such
consistent scenario can be reconstructed from the qualitative mapping shown in figure  b 
example    golumbic and shamir        discuss how ia networks can be used in a
problem in molecular biology  examining the structure of the dna of an organism  benzer         the intervals in the ia network represent segments of dna  experiments
can be performed to determine whether a pair of segments is either disjoint or intersects 
thus  the ia networks that result contain edges labeled with disjoint  fb big   intersects
 fm mi o oi s si d di f fi eqg   or i   the set of all basic relations which indicates no experiment was performed  if the ia network is consistent  this is evidence for the hypothesis
that dna is linear in structure  if it is inconsistent  dna is nonlinear  it forms loops 
for example   golumbic and shamir        show that determining consistency in this restricted version of ia networks is np complete  we will show that problems that arise in
this application can often be solved quickly in practice 
 

fivan beek   manchak










 






   

 


 


 r

 







 pp   
 
 hyhhphpppppq j 
jj  
hh 
hhh
jj   
z 
zzj   
hhh z
hh
 

 a  ia network for block stacking example 

fbi mig
fdg

fdg

 
initial

fdg

fbi mig

fdg

 
clear a 
 
clear b 
 
clear c 

 
stack a b 
ffg

fdg

ffg

fmg

 
on a b 
 
on b c 

fmg

pip  
 pp 

fdg
fdg

 
goal

 
stack b c 

 b  consistent scenario 
initial

stack b c 
goal
stack a b 
clear c 
on b c 
clear b 
on a b 
clear a 

figure    representing qualitative relations between intervals

    test problems
we tested how well the heuristics we developed for improving path consistency and backtracking algorithms perform on a test suite of problems 
the purpose of empirically testing the algorithms is to determine the performance of
the algorithms and the proposed improvements on  typical  problems  there are two
approaches   i  collect a set of  benchmark  problems that are representative of problems
that arise in practice  and  ii  randomly generate problems and  investigate how algorithmic
performance depends on problem characteristics     and learn to predict how an algorithm
will perform on a given problem class   hooker        
for ia networks  there is no existing collection of large benchmark problems that actually
arise in practice as opposed to  for example  planning in a toy domain such as the blocks
world  as a start to a collection  we propose an ia network with     intervals that arose
from a problem in molecular biology  benzer        pp           see example    above  
the proposed benchmark problem is not strictly speaking a temporal reasoning problem
 

fialgorithms for temporal reasoning

as the intervals represent segments of dna  not intervals of time  nevertheless  it can be
formulated as a temporal reasoning problem  the value is that the benchmark problem
arose in a real application  we will refer to this problem as benzer s matrix 
in addition to the benchmark problem  in this paper we use two models of a random ia
network  denoted b n  and s n  p   to evaluate the performance of the algorithms  where n
is the number of intervals  and p is the probability of a  non trivial  constraint between two
intervals  model b n  is intended to model the problems that arise in molecular biology  as
estimated from the problem discussed in benzer         model s n  p  allows us to study
how algorithm performance depends on the important problem characteristic of sparseness
of the underlying constraint graph  both models  of course  allow us to study how algorithm
performance depends on the size of the problem 
for b n   the random instances are generated as follows 
step    generate a  solution  of size n as follows  generate n real intervals by randomly
generating values for the end points of the intervals  determine the ia network by determining  for each pair of intervals  whether the two intervals either intersect or are disjoint 
step    change some of the constraints on edges to be the trivial constraint by setting the
label to be i   the set of all    basic relations  this represents the case where no experiment
was performed to determine whether a pair of dna segments intersect or are disjoint 
constraints are changed so that the percentage of non trivial constraints  approximately
   are intersects and     are disjoint  and their distribution in the graph are similar to
those in benzer s matrix 
for s n  p   the random instances are generated as follows 
step    generate the underlying constraint graph by indicating which of the possible  n  
edges is present  let each edge be present with probability p  independently of the presence
or absence of other edges 
step    if an edge occurs in the underlying constraint graph  randomly chose a label for
the edge from the set of all possible labels  excluding the empty label  where each label is
chosen with equal probability  if an edge does not occur  label the edge with i   the set of
all    basic relations 
step    generate a  solution  of size n as follows  generate n real intervals by randomly
generating values for the end points of the intervals  determine the consistent scenario by
determining the basic relations which are satisfied by the intervals  finally  add the solution
to the ia network generated in steps     
hence  only consistent ia networks are generated from s n  p   if we omit step    it
can be shown both analytically and empirically that almost all of the different possible
ia networks generated by this distribution are inconsistent and that the inconsistency is
easily detected by a path consistency algorithm  to avoid this potential pitfall  we test
our algorithms on consistent instances of the problem  this method appears to generate a
reasonable test set for temporal reasoning algorithms with problems that range from easy
to hard  it was found  for example  that instances drawn from s n       were hard problems
for the backtracking algorithms to solve  whereas for values of p on either side  s n      
and s n        the problems were easier 
 

fivan beek   manchak

   path consistency algorithm
path consistency or transitive closure algorithms  aho  hopcroft    ullman        mackworth        montanari        are important for temporal reasoning  allen        shows
that a path consistency algorithm can be used as a heuristic test for whether an ia network
is consistent  sometimes the algorithm will report that the information is consistent when
really it is not   a path consistency algorithm is useful also in a backtracking search for a
consistent scenario where it can be used as a preprocessing algorithm  mackworth       
ladkin   reinefeld        and as an algorithm that can be interleaved with the backtracking search  see the next section  nadel        ladkin   reinefeld         in this section 
we examine methods for speeding up a path consistency algorithm 
the idea behind the path consistency algorithm is the following  choose any three
vertices i  j   and k in the network  the labels on the edges  i  j   and  j  k  potentially
constrain the label on the edge  i  k  that completes the triangle  for example  consider
the three vertices stack a b   on a b   and goal in figure  a  from stack a b  fmg
on a b  and on a b  fdig goal we can deduce that stack a b  fbg goal and therefore
can change the label on that edge from i   the set of all basic relations  to the singleton
set fbg  to perform this deduction  the algorithm uses the operations of set intersection
    and composition    of labels and checks whether c   c   c  c   where c is
the label on edge  i  k   if c is updated  it may further constrain other labels  so  i  k  is
added to a list to be processed in turn  provided that the edge is not already on the list 
the algorithm iterates until no more such changes are possible  a unary operation  inverse 
is also used in the algorithm  the inverse of a label is the inverse of each of its elements
 see figure   for the inverses of the basic relations  
we designed and experimentally evaluated techniques for improving the eciency of a
path consistency algorithm  our starting point was the variation on allen s        algorithm
shown in figure    for an implementation of the algorithm to be ecient  the intersection
and composition operations on labels must be ecient  steps          intersection was
made ecient by implementing the labels as bit vectors  the intersection of two labels
is then simply the logical and of two integers  composition is harder to make ecient 
unfortunately  it is impractical to implement the composition of two labels using table
lookup as the table would need to be of size           there being     possible labels 
we experimentally compared two practical methods for composition that have been
proposed in the literature  allen        gives a method for composition which uses a table
of size         the table gives the composition of the basic relations  see allen       
for the table   the composition of two labels is computed by a nested loop that forms the
union of the pairwise composition of the basic relations in the labels  hogge        gives a
method for composition which uses four tables of size                         and        
the composition of two labels is computed by taking the union of the results of four array
references  h  kautz independently devised a similar scheme   in our experiments  the
implementations of the two methods differed only in how composition was computed  in
both  the list  l  of edges to be processed was implemented using a first in  first out policy
 i e   a stack  
we also experimentally evaluated methods for reducing the number of composition operations that need to be performed  one idea we examined for improving the eciency is
ik

ik

 

ik

ij

jk

ik

fialgorithms for temporal reasoning

 c  n 

path consistency

   l f i  j   j    i   j  ng
   while  l is not empty 
   do select and delete an  i  j   from l
  
for k   to n  k    i and k    j
  
do t c   c  c
  
if  t    c  
  
then c t
  
c
inverse t 
  
l
l   f i  k  g
   
t
c
 c
c
   
if  t    c  
   
then c t
   
c
inverse t 
   
l
l   f k  j  g
ik

ij

jk

ki

ij

ik

ik

ki

kj

kj

kj

jk

figure    path consistency algorithm for ia networks
to avoid the computation when it can be predicted that the result will not constrain the
label on the edge that completes the triangle  three such cases we identified are shown in
figure    another idea we examined  as first suggested by mackworth        p        is
that the order that the edges are processed can affect the eciency of the algorithm  the
reason is the following  the same edge can appear on the list  l  of edges to be processed
many times as it progressively gets constrained  the number of times a particular edge
appears on the list can be reduced by a good ordering  for example  consider the edges
       and        in figure  a  if we process edge        first  edge        will be updated to
fo oi s si d di f fi eqg and will be added to l  k     in steps       now if we process edge
        edge        will be updated to fo s dg and will be added to l a second time  however 
if we process edge        first         will be immediately updated to fo s dg and will only be
added to l once 
three heuristics we devised for ordering the edges are shown in figure    the edges
are assigned a heuristic value and are processed in ascending order  when a new edge is
added to the list  steps          the edge is inserted at the appropriate spot according to its
new heuristic value  there has been little work on ordering heuristics for path consistency
algorithms  wallace and freuder        discuss ordering heuristics for arc consistency
algorithms  which are closely related to path consistency algorithms  two of their heuristics
cannot be applied in our context as the heuristics assume a constraint satisfaction problem
with finite domains  whereas ia networks are examples of constraint satisfaction problems
with infinite domains  a third heuristic  due to b  nudel        closely corresponds to our
cardinality heuristic 
all experiments were performed on a sun      with    megabytes of memory  we
report timings rather than some other measure such as number of iterations as we believe
this gives a more accurate picture of whether the results are of practical interest  care was
 

fivan beek   manchak

the computation  c   c  c   can be skipped when it is known that the result of the
composition will not constrain the label on the edge  i  k  
a  if either c or c is equal to i   the result of the composition will be i and therefore
will not constrain the label on the edge  i  k   thus  in step   of figure    edges that
are labeled with i are not added to the list of edges to process 
b  if the condition 
ik

ij

ij

jk

jk

 b   c

ij  

bi   c      bi   c

ij  

jk

b   c      d   c
jk

ij  

di   c   
jk

is true  the result of composing c and c will be i   the condition is quickly tested
using bit operations  thus  if the above condition is true just before step    steps    
can be skipped  a similar condition can be formulated and tested before step    
c  if at some point in the computation of c  c it is determined that the result
accumulated so far would not constrain the label c   the rest of the computation can
be skipped 
ij

jk

ij

jk

ik

figure    skipping techniques
taken to always start with the same base implementation of the algorithm and only add
enough code to implement the composition method  new technique  or heuristic that we
were evaluating  as well  every attempt was made to implement each method or heuristic
as eciently as we could 
given our implementations  hogge s method for composition was found to be more
ecient than allen s method for both the benchmark problem and the random instances
 see figures       this much was not surprising  however  with the addition of the skipping
techniques  the two methods became close in eciency  the skipping techniques sometimes
dramatically improved the eciency of both methods  the ordering heuristics can improve
the eciency  although here the results were less dramatic  the cardinality heuristic and
the constraintedness heuristic were also tried for ordering the edges  it was found that the
cardinality heuristic was just as costly to compute as the weight heuristic but did not out
perform it  the constraintedness heuristic reduced the number of iterations but proved too
costly to compute  this illustrates the balance that must be struck between the effectiveness
of a heuristic and the additional overhead the heuristic introduces 
for s n  p   the skipping techniques and the weight ordering heuristic together can result
in up to a ten fold speedup over an already highly optimized implementation using hogge s
method for composition  the largest improvements in eciency occur when the ia networks
are sparse  p is smaller   this is encouraging for it appears that the problems that arise in
planning and molecular biology are also sparse  for b n  and benzer s matrix  the speedup
is approximately four fold  perhaps most importantly  the execution times reported indicate
that the path consistency algorithm  even though it is an o n    algorithm  can be used on
practical sized problems  in figure    we show how well the algorithms scale up  it can be
 

fialgorithms for temporal reasoning

allen

     

hogge

    

allen skip

   

hogge skip

   

hogge skip weight

   

figure    effect of heuristics on time  sec   of path consistency algorithms applied to
benzer s matrix

time  sec  

   

  

 

allen
hogge
allen skip
hogge skip
hogge skip weight

   
  

  

   
n

   

   

figure    effect of heuristics on average time  sec   of path consistency algorithms  each
data point is the average of     tests on random instances of ia networks drawn
from b n   the coecient of variation  standard deviation   average  for each set
of     tests is bounded by     
seen that the algorithm that includes the weight ordering heuristic out performs all others 
however  this algorithm requires much space and the largest problem we were able to solve
was with     intervals  the algorithms that included only the skipping techniques were
able to solve much larger problems before running out of space  up to      intervals  and
here the constraint was the time it took to solve the problems 

 

fivan beek   manchak

   

time  sec  

  

 

allen
hogge
allen skip
hogge skip
hogge skip weight

   
   

   

   
p

   

 

figure    effect of heuristics on average time  sec   of path consistency algorithms  each
data point is the average of     tests on random instances of ia networks drawn
from s      p   the coecient of variation  standard deviation   average  for each
set of     tests is bounded by     
    
    
s n      

allen skip
hogge skip
hogge skip weight

    

time  sec  

    

b n  

allen skip
hogge skip
hogge skip weight

    
    
    
    
    
 
   

   

   

   

   

   
n

   

   

   

    

figure    effect of heuristics on average time  sec   of path consistency algorithms  each
data point is the average of    tests on random instances of ia networks drawn
from s n       and b n   the coecient of variation  standard deviation   average  for each set of    tests is bounded by     
  

fialgorithms for temporal reasoning

   backtracking algorithm

allen        was the first to propose that a backtracking algorithm  golomb   baumert 
      could be used to find a consistent scenario of an ia network  in the worst case  a
backtracking algorithm can take an exponential amount of time to complete  this worst
case also applies here as vilain and kautz              show that finding a consistent
scenario is np complete for ia networks  in spite of the worst case estimate  backtracking
algorithms can work well in practice  in this section  we examine methods for speeding up a
backtracking algorithm for finding a consistent scenario and present results on how well the
algorithm performs on different classes of problems  in particular  we compare the eciency
of the algorithm on two alternative formulations of the problem  one that has previously
been proposed by others and one that we have proposed  van beek         we also improve
the eciency of the algorithm by designing heuristics for ordering the instantiation of the
variables and for ordering the values in the domains of the variables 
as our starting point  we modeled our backtracking algorithm after that of ladkin and
reinefeld        as the results of their experimentation suggests that it is very successful at
finding consistent scenarios quickly  following ladkin and reinefeld our algorithm has the
following characteristics  preprocessing using a path consistency algorithm  static order of
instantiation of the variables  chronological backtracking  and forward checking or pruning
using a path consistency algorithm  in chronological backtracking  when the search reaches
a dead end  the search simply backs up to the next most recently instantiated variable and
tries a different instantiation  forward checking  haralick   elliott        is a technique
where it is determined and recorded how the instantiation of the current variable restricts
the possible instantiations of future variables  this technique can be viewed as a hybrid of
tree search and consistency algorithms  see nadel        nudel          see dechter       
for a general survey on backtracking  

    alternative formulations

let c be the matrix representation of an ia network  where c is the label on edge  i  j   
the traditional method for finding a consistent scenario of an ia network is to search for a
subnetwork s of a network c such that 
 a  s  c  
 b  js j      for all i  j   and
 c  s is consistent 
to find a consistent scenario we simply search through the different possible s  s that satisfy
conditions  a  and  b  it is a simple matter to enumerate them until we find one that
also satisfies condition  c   allen        was the first to propose using backtracking search
to search through the potential s  s 
our alternative formulation is based on results for two restricted classes of ia networks 
denoted here as sa networks and nb networks  in ia networks  the relation between two
intervals can be any subset of i   the set of all thirteen basic relations  in sa networks
 vilain   kautz         the allowed relations between two intervals are only those subsets
of i that can be translated  using the relations f               g  into conjunctions of
ij

ij

ij

ij

  

fivan beek   manchak

relations between the endpoints of the intervals  for example  the ia network in figure  a
is also an sa network  as a specific example  the interval relation  a fbi mig b  can be
expressed as the conjunction of point relations   b    b       a    a       a   b    
where a  and a  represent the start and end points of interval a  respectively   see ladkin
  maddux        van beek   cohen        for an enumeration of the allowed relations for
sa networks   in nb networks  nebel   burckert         the allowed relations between
two intervals are only those subsets of i that can be translated  using the relations f  
            g  into conjunctions of horn clauses that express the relations between the
endpoints of the intervals  the set of nb relations is a strict superset of the sa relations 
our alternative formulation is as follows  we describe the method in terms of sa
networks  but the same method applies to nb networks  the idea is that  rather than
search directly for a consistent scenario of an ia network as in previous work  we first
search for something more general  a consistent sa subnetwork of the ia network  that
is  we use backtrack search to find a subnetwork s of a network c such that 
 a 

sij  cij

 b 

sij

 c 

s

 

is an allowed relation for sa networks  for all i  j   and

is consistent 

in previous work  the search is through the alternative singleton labelings of an edge  i e  
js j      the key idea in our proposal is that we decompose the labels into the largest
possible sets of basic relations that are allowed for sa networks and search through these
decompositions  this can considerably reduce the size of the search space  for example 
suppose the label on an edge is fb bi m o oi sig  there are six possible ways to label the
edge with a singleton label  fbg  fbig  fmg  fog  foig  fsig  but only two possible ways to
label the edge if we decompose the labels into the largest possible sets of basic relations
that are allowed for sa networks  fb m og and fbi oi sig  as another example  consider the
network shown in figure  a  when searching through alternative singleton labelings  the
worst case size of the search space is c    c        c          the edges labeled with
i must be included in the calculation   but when decomposing the labels into the largest
possible sets of basic relations that are allowed for sa networks and searching through the
decompositions  the size of the search space is    so no backtracking is necessary  in general 
the search is  of course  not always backtrack free  
to test whether an instantiation of a variable is consistent with instantiations of past
variables and with possible instantiations of future variables  we use an incremental path
consistency algorithm  in step   of figure   instead of initializing l to be all edges  it is
initialized to the single edge that has changed   the result of the backtracking algorithm is a
consistent sa subnetwork of the ia network  or a report that the ia network is inconsistent 
after backtracking completes  a solution of the sa network can be found using a fast
algorithm given by van beek        
ij

    ordering heuristics

backtracking proceeds by progressively instantiating variables  if no consistent instantiation
exists for the current variable  the search backs up  the order in which the variables
  

fialgorithms for temporal reasoning

weight  the weight heuristic is an estimate of how much the label on an edge will restrict
the labels on other edges  restrictiveness was measured for each basic relation by successively composing the basic relation with every possible label and summing the cardinalities
of the results  the results were then suitably scaled to give the table shown below 
relation b bi m mi o oi s si d di f fi eq
weight                          
the weight of a label is then the sum of the weights of its elements  for example  the weight
of the relation fm o sg is               
cardinality  the cardinality heuristic is a variation on the weight heuristic  here  the
weight of every basic relation is set to one 
constraint  the constraintedness heuristic is an estimate of how much a change in a label
on an edge will restrict the labels on other edges  it is determined as follows  suppose
the edge we are interested in is  i  j    the constraintedness of the label on edge  i  j   is
the sum of the weights of the labels on the edges  k  i  and  j  k   k           n  k    i  k    j  
the intuition comes from examining the path consistency algorithm  figure    which would
propagate a change in the label c   we see that c will be composed with c  step   
and c  step      k           n  k    i  k    j  
ij

ij

ki

jk

figure    ordering heuristics
are instantiated and the order in which the values in the domains are tried as possible
instantiations can greatly affect the performance of a backtracking algorithm and various
methods for ordering the variables  e g  bitner   reingold        freuder        nudel 
      and ordering the values  e g  dechter   pearl        ginsberg et al         haralick
  elliott        have been proposed 
the idea behind variable ordering heuristics is to instantiate variables first that will
constrain the instantiation of the other variables the most  that is  the backtracking search
attempts to solve the most highly constrained part of the network first  three heuristics
we devised for ordering the variables  edges in the ia network  are shown in figure   
for our alternative formulation  cardinality is redefined to count the decompositions rather
than the elements of a label  the variables are put in ascending order  in our experiments
the ordering is static it is determined before the backtracking search starts and does not
change as the search progresses  in this context  the cardinality heuristic is similar to a
heuristic proposed by bitner and reingold        and further studied by purdom        
the idea behind value ordering heuristics is to order the values in the domains of the
variables so that the values most likely to lead to a solution are tried first  generally  this
is done by putting values first that constrain the choices for other variables the least  here
we propose a novel technique for value ordering that is based on knowledge of the structure
of solutions  the idea is to first choose a small set of problems from a class of problems 
and then find a consistent scenario for each instance without using value ordering  once we
have a set of solutions  we examine the solutions and determine which values in the domains
  

fivan beek   manchak

   

   

si

sa

time  sec  

  

  

  

  

 
  

   

   
n

   

   

figure     effect of decomposition method on average time  sec   of backtracking algorithm  each data point is the average of     tests on random instances of ia
networks drawn from b n   the coecient of variation  standard deviation  
average  for each set of     tests is bounded by     
are most likely to appear in a solution and which values are least likely  this information
is then used to order the values in subsequent searches for solutions to problems from this
class of problems  for example  five problems were generated using the model s          
and consistent scenarios were found using backtracking search and the variable ordering
heuristic constraintedness weight cardinality  after rounding to two significant digits  the
relations occurred in the solutions with the following frequency 
relation
b  bi d  di o  oi
value                  

eq
  

m  mi f  fi s  si
  
     

as an example of using this information to order the values in a domain  suppose that the
label on an edge is fb bi m o oi sig  if we are decomposing the labels into singleton labels 
we would order the values in the domain as follows  most preferred first   fbg  fbig  fog 
foig  fmg  fsig  if we are decomposing the labels into the largest possible sets of basic
relations that are allowed for sa networks  we would order the values in the domain as
follows  fb m og  fbi oi sig  since                                    this technique can be
used whenever something is known about the structure of solutions 

    experiments

all experiments were performed on a sun      with   megabytes of memory 
the first set of experiments  summarized in figure     examined the effect of problem
formulation on the execution time of the backtracking algorithm  we implemented three
  

fialgorithms for temporal reasoning

     
random value ordering  random
heuristic value ordering  random
random value ordering  best heuristic
heuristic value ordering  best heuristic

variable
variable
variable
variable

ordering
ordering
ordering
ordering

time

    

   

  
 

  

  

  

  

  
test

  

  

  

  

   

figure     effect of variable and value ordering heuristics on time  sec   of backtracking
algorithm  each curve represents     tests on random instances of ia networks
drawn from s           where the tests are ordered by time taken to solve the
instance  the backtracking algorithm used the sa decomposition method 
versions of the algorithm that were identical except that one searched through singleton
labelings  denoted hereafter and in figure    as the si method  and the other two searched
through decompositions of the labels into the largest possible allowed relations for sa networks and nb networks  respectively  all of the methods solved the same set of random
problems drawn from b n  and were also applied to benzer s matrix  denoted   and 
in figure      for each problem  the amount of time required to solve the given ia network was recorded  as mentioned earlier  each ia network was preprocessed with a path
consistency algorithm before backtracking search  the timings include this preprocessing
time  the experiments indicate that the speedup by using the sa decomposition method
can be up to three fold over the si method  as well  the sa decomposition method was
able to solve larger problems before running out of space  n       versus n         the
nb decomposition method gives exactly the same result as for the sa method on these
problems because of the structure of the constraints  we also tested all three methods on
a set of random problems drawn from s      p   where p                and      in these
experiments  the sa and nb methods were consistently twice as fast as the si method  as
well  the nb method showed no advantage over the sa method on these problems  this is
surprising as the branching factor  and hence the size of the search space  is smaller for the
nb method than for the sa method 
the second set of experiments  summarized in figure     examined the effect on the
execution time of the backtracking algorithm of heuristically ordering the variables and
the values in the domains of the variables before backtracking search begins  for variable
ordering  all six permutations of the cardinality  constraint  and weight heuristics were tried
  

fivan beek   manchak

as the primary  secondary  and tertiary sorting keys  respectively  as a basis of comparison 
the experiments included the case of no heuristics  figure    shows approximate cumulative
frequency curves for some of the experimental results  thus  for example  we can read from
the curve representing heuristic value ordering and best heuristic variable ordering that
approximately     of the tests completed within    seconds  whereas with random value
and variable ordering only approximately    of the tests completed within    seconds  we
can also read from the curves the                    percentiles of the data sets  where the
value of the median is the   th percentile or the value of the   th test   the curves are
truncated at time             hour   as the backtracking search was aborted when this
time limit was exceeded 
in our experiments we found that s           represents a particularly dicult class
of problems and it was here that the different heuristics resulted in dramatically different performance  both over the no heuristic case and also between the different heuristics 
with no value ordering  the best heuristic for variable ordering was the combination constraintedness weight cardinality where constraintedness is the primary sorting key and the
remaining keys are used to break subsequent ties  somewhat surprisingly  the best heuristic
for variable ordering changes when heuristic value ordering is incorporated  here the combination weight constraintedness cardinality works much better  this heuristic together
with value ordering is particularly effective at  attening out  the distribution and so allowing a much greater number of problems to be solved in a reasonable amount of time  for
s      p   where p                and      the problems were much easier and all but three
of the hundreds of tests completed within    seconds  in these problems  the heuristic used
did not result in significantly different performance 
in summary  the experiments indicate that by changing the decomposition method we
are able to solve larger problems before running out of space  n       vs n       on a
machine with   megabytes  see figure      the experiments also indicate that good heuristic
orderings can be essential to being able to find a consistent scenario of an ia network in
reasonable time  with a good heuristic ordering we were able to solve much larger problems
before running out of time  see figure      the experiments also provide additional evidence
for the ecacy of ladkin and reinefeld s              algorithm  nevertheless  even with
all of our improvements  some problems still took a considerable amount of time to solve 
on consideration  this is not surprising  after all  the problem is known to be np complete 

   conclusions
temporal reasoning is an essential part of tasks such as planning and scheduling  in this paper  we discussed the design and an empirical analysis of two key algorithms for a temporal
reasoning system  the algorithms are a path consistency algorithm and a backtracking algorithm  the temporal reasoning system is based on allen s        interval based framework
for representing temporal information  our emphasis was on how to make the algorithms
robust and ecient in practice on problems that vary from easy to hard  for the path consistency algorithm  the bottleneck is in performing the composition operation  we developed
methods for reducing the number of composition operations that need to be performed 
these methods can result in almost an order of magnitude speedup over an already highly
optimized implementation of the algorithm  for the backtracking algorithm  we developed
  

fialgorithms for temporal reasoning

variable and value ordering heuristics and showed that an alternative formulation of the
problem can considerably reduce the time taken to find a solution  the techniques allow an
interval based temporal reasoning system to be applied to larger problems and to perform
more eciently in existing applications 

references

aho  a  v   hopcroft  j  e     ullman  j  d          the design and analysis of computer
algorithms  addison wesley 
allen  j  f          maintaining knowledge about temporal intervals  comm  acm     
        
allen  j  f     koomen  j  a          planning using a temporal world model  in proceedings
of the eighth international joint conference on artificial intelligence  pp         
karlsruhe  west germany 
benzer  s          on the topology of the genetic fine structure  proc  nat  acad  sci  usa 
              
bitner  j  r     reingold  e  m          backtrack programming techniques  comm  acm 
            
bruce  b  c          a model for temporal references and its application in a question
answering program  artificial intelligence          
dechter  r          from local to global consistency  artificial intelligence             
dechter  r     pearl  j          network based heuristics for constraint satisfaction problems  artificial intelligence           
freuder  e  c          a sucient condition for backtrack free search  j  acm            
ginsberg  m  l   frank  m   halpin  m  p     torrance  m  c          search lessons learned
from crossword puzzles  in proceedings of the eighth national conference on artificial
intelligence  pp          boston  mass 
golomb  s     baumert  l          backtrack programming  j  acm              
golumbic  m  c     shamir  r          complexity and algorithms for reasoning about
time  a graph theoretic approach  j  acm                
haralick  r  m     elliott  g  l          increasing tree search eciency for constraint
satisfaction problems  artificial intelligence              
hogge  j  c          tplan  a temporal interval based planner with novel extensions  department of computer science technical report uiucdcs r     university of illinois 
hooker  j  n          needed  an empirical science of algorithms  operations research 
            
  

fivan beek   manchak

ladkin  p     reinefeld  a          effective solution of qualitative interval constraint
problems  artificial intelligence              
ladkin  p     reinefeld  a          a symbolic approach to interval constraint problems  in
calmet  j     campbell  j   eds    artificial intelligence and symbolic mathematical
computing  springer lecture notes in computer science      springer verlag 
ladkin  p  b     maddux  r  d          on binary constraint networks  technical report 
kestrel institute  palo alto  calif 
mackworth  a  k          consistency in networks of relations  artificial intelligence    
       
montanari  u          networks of constraints  fundamental properties and applications
to picture processing  inform  sci             
nadel  b  a          constraint satisfaction algorithms  computational intelligence    
        
nebel  b     burckert  h  j          reasoning about temporal relations  a maximal
tractable subclass of allen s interval algebra  j  acm            
nudel  b          consistent labeling problems and their algorithms  expected complexities
and theory based heuristics  artificial intelligence              
purdom  jr   p  w          search rearrangement backtracking and polynomial average
time  artificial intelligence              
van beek  p          reasoning about qualitative temporal information  artificial intelligence              
van beek  p     cohen  r          exact and approximate reasoning about temporal
relations  computational intelligence             
vilain  m     kautz  h          constraint propagation algorithms for temporal reasoning 
in proceedings of the fifth national conference on artificial intelligence  pp         
philadelphia  pa 
vilain  m   kautz  h     van beek  p          constraint propagation algorithms for
temporal reasoning  a revised report  in weld  d  s     de kleer  j   eds    readings
in qualitative reasoning about physical systems  pp           morgan kaufmann 
wallace  r  j     freuder  e  c          ordering heuristics for arc consistency algorithms 
in proceedings of the ninth canadian conference on artificial intelligence  pp      
    vancouver  b c 

  

fi