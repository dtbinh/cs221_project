journal of artificial intelligence research                 

submitted       published     

on partially controlled multi agent systems
ronen i  brafman

brafman cs ubc ca

computer science department
university of british columbia
vancouver  b c   canada v l  z 

moshe tennenholtz

moshet ie technion ac il

industrial engineering and management
technion   israel institute of technology
haifa        israel

abstract

motivated by the control theoretic distinction between controllable and uncontrollable
events  we distinguish between two types of agents within a multi agent system  controllable
agents   which are directly controlled by the system s designer  and uncontrollable agents  
which are not under the designer s direct control  we refer to such systems as partially
controlled multi agent systems  and we investigate how one might inuence the behavior of
the uncontrolled agents through appropriate design of the controlled agents  in particular 
we wish to understand which problems are naturally described in these terms  what methods
can be applied to inuence the uncontrollable agents  the effectiveness of such methods  and
whether similar methods work across different domains  using a game theoretic framework 
this paper studies the design of partially controlled multi agent systems in two contexts  in
one context  the uncontrollable agents are expected utility maximizers  while in the other
they are reinforcement learners  we suggest different techniques for controlling agents 
behavior in each domain  assess their success  and examine their relationship 

   introduction
the control of agents is a central research topic in two engineering fields  artificial intelligence  ai  and discrete events systems  des   ramadge   wonham         one
particular area both of these fields have been concerned with is multi agent environments 
examples include work in distributed ai  bond   gasser         and work on decentralized
supervisory control  lin   wonham         each of these fields has developed its own
techniques and has incorporated particular assumptions into its models  hence  it is only
natural that techniques and assumptions used by one field may be adopted by the other or
may lead to new insights for the other field 
in difference to most ai work on multi agent systems  work on decentralized discrete
event systems distinguishes between controllable and uncontrollable events  controllable
events are events that can be directly controlled by the system s designer  while uncontrollable events are not directly controlled by the system s designer  translating this terminology into the context of multi agent systems  we introduce the distinction between two
types of agents  controllable agents   which are directly controlled by the system s designer 
and uncontrollable agents   which are not under the designer s direct control  this leads
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fibrafman   tennenholtz

naturally to the concept of partially controlled multi agent system  pcmas  and to the
following design challenge  ensuring that all agents in the system behave appropriately
through adequate design of the controllable agents  we believe that many problems are
naturally formulated as instances of pcmas design  our goal is to characterize important
instances of this design problem  to examine the tools that can be used to solve it  and to
assess the effectiveness and generality of these tools 
what distinguishes partially controlled multi agent systems in the ai context from similar models in des are the structural assumptions we make about the uncontrolled agents
involved  unlike typical des models which are concerned with physical processes or devices  ai is particularly interested in self motivated agents  two concrete examples of which
are rational agents  i e   expected utility maximizers  and learning agents  e g   reinforcement learners  indeed  these examples constitute the two central models of self motivated
agents in game theory and decision theory  referred to as the educative and evolutive models
 e g   see gilboa   matsui         the special nature of the uncontrollable agents and the
special structure of the uncontrollable events they induce is what differentiates pcmas
from corresponding models in the des literature  this difference raises new questions and
suggests a new perspective on the design of multi agent systems  in particular  it calls for
techniques for designing controllable agents that  by exploiting the structural assumptions 
can inuence the behavior of the uncontrollable agents and lead the system to a desired
behavior 
in order to understand these issues  we study two problems that can be stated and solved
by adopting the perspective of pcmas design  problems which by themselves should be
of interest to a large community  in both of these problems our goal is to inuence the
behavior of agents that are not under our control  we exert this inuence indirectly by
choosing suitable behaviors for those agents that are under our direct control  in one case 
we attempt to inuence the behavior of rational agents  while in the other case  we try to
inuence learning agents 
our first study is concerned with the enforcement of social laws  when a number of
agents designed by different designers work within a shared environment  it can be beneficial
to impose certain constraints on their behavior  so that  overall  the system will function
better  for example  shoham and tennenholtz        show that by imposing certain  trac
laws   they can considerably simplify the task of motion planning for each robot  while still
enabling ecient motions  indeed  as we see later  such conventions are at the heart of
many coordination techniques in multi agent systems  yet  without suitable mechanisms 
rational agents may have an incentive not to follow these conventions  we show how  in
certain cases  we can use the perspective of partially controlled multi agent systems and the
structural assumption of rationality to enforce these conventions 
our second study involves a two agent system consisting of a teacher and a student 
the teacher is a knowledgeable agent  while the student is an agent that is learning how
to behave in its domain  our goal is to utilize the teacher  which is under our control 
to improve the behavior of the student  which is not controlled by us   hence  this is an
instance of partially controlled multi agent systems in which the structural assumption is
that the uncontrolled agent employs a particular learning algorithm 
both studies presented in this paper suggest techniques for achieving satisfactory system
behavior through the design of the controllable agents  and where relevant  these techniques
   

fion partially controlled multi agent systems

are experimentally assessed  beyond the formulation and solution of two interesting problems in multi agent system design  this paper suggests a more general perspective on certain
design problems  although we feel that it is still premature to draw general conclusion about
the potential for a general theory of pcmas design  certain concepts  those of punishment
and reward  suggest themselves as central to this area 
the paper is organized as follows  in section    we describe the problem of enforcing
social behavior in multi agent systems  in section   we describe a standard game theoretic
model for this problem and suggest the mechanism of threats and punishments as a general
tool for this class of problems  issues that pertain to the design of threats and punishments
are discussed in section    section   introduces our second case study in pcmas design 
embedded teaching of reinforcement learners  in this context  a teacher and a learner
are embedded in a shared environment with the teacher serving as the controller whose
aim is to direct the learner to a desired behavior  a formal model of this problem is
introduced in section    in section    we show how to derive optimal teaching policies  under
certain assumptions  by viewing teaching as a markov decision process  the effectiveness
of different teaching policies is studied experimentally in section    finally  in section   
we examine the relationship between the methods used in each of the two domains and the
possibility of a general methodology for designing partially controlled multi agent systems 
we conclude in section     with a summary and discussion of related work 

   the enforcement of social behavior
in this section we introduce the problem of the enforcement of social laws in a multi agent
context  our proposed solution falls naturally out of the pcmas design perspective we
take  here  we explain and motivate the particular problem of social law enforcement and
our approach to its solution  in sections   and   we formalize and investigate this approach
in the framework of a general game theoretic model 
we use the following scenario to illustrate the problem 
you have been hired to design a new working environment for artificial
agents  part of your job involves designing a number of agents that will use
and maintain a warehouse  other agents  designed by different designers  will
be using the warehouse to obtain equipment  to make sure that different agents
designed by different designers can operate eciently in this environment  you
choose to introduce a number of social laws  that is  constraints on the behavior
of agents  that will help the agents coordinate their activities in this domain 
these rules include a number of  trac laws   regulating motion in the domain 
as well as a law that specifies that every tool that is used by an agent must be
returned to its designated storage area  your robots are programmed to follow
these laws  and you expect the others to do so  your laws are quite successful  and allow ecient activity in the warehouse  until a new designer arrives 
pressed by his corporate bosses to deliver better performance  he decides to
exploit all your rules  he designs his agent to locally maximize its performance 
regardless of the social laws  what can you do 
   

fibrafman   tennenholtz

in multi participant environments  as the one above  each agent might have its own
dynamic goals  and we are interested in finding ways in which agents can coexist while
achieving their goals  several approaches for coordination of agent activity are discussed in
the distributed systems and the dai literature  some examples are  protocols for reaching
consensus  dwork   moses         rational deals and negotiations  zlotkin   rosenschein 
      kraus   wilkenfeld        rosenschein   genesereth         organizational structures  durfee  lesser    corkill        fox        malone         and social laws  moses
  tennenholtz        shoham   tennenholtz        minsky        briggs   cook        
in some of these methods  the behavior of an agent is predetermined or prescribed from a
certain stage  for example  the content of the deal after it is reached  the outcome of the
negotiation process after it is completed  or the social law after it is instituted  this work
relies on the assumption that the agents follow these prescribed behaviors  e g   they obey
the law or stick to the agreement  this assumption is central to the success of any of these
methods  however  it makes agents that follow the rules vulnerable to any rational agent
that performs local maximization of payoff  exploiting the knowledge that others follow the
rules  in our example  the new designer may program his robot not to return the tools 
saving the time required to do so  thus causing other agents to fail in their tasks 
despite its somewhat futuristic avor  although instances of such shared environments
are beginning to appear in cyberspace   this scenario is useful in illustrating the vulnerability
of some of the most popular coordination mechanism appearing in the multi agent literature
within ai  e g   see bond   gasser        when we assume that the agents involved are
fully rational  as an aside  note that  in this case  we actually need not attribute much
intelligence to the agents themselves  and it is sucient to assume that their designers
design them in a way that maximizes their own utility  disregarding the utility of the other
agents 
in order to handle this problem we need to modify existing design paradigms  by
adopting the perspective of partially controlled multi agent systems  we obtain one possible
handle on this problem  which requires making the following basic assumption  that the
original designer  as in the above scenario  controls a number of reliable agents   our
basic idea is that some of these reliable agents will be designed to punish agents that
deviate from the desirable social standard  the punishment mechanism will be  hardwired   unchangeable  and will be common knowledge  the agents that are not controlled
by the original designer will be aware of this punishment possibility  if the punishment
mechanism is well designed  deviations from the social standard become irrational  as a
result  no deviation will actually occur and no punishment will actually be executed  hence 
by making our agents a bit more sophisticated  we can prevent the temptation of breaking
social laws 
in the suggested solution we adopt the perspective of partially controlled multi agent systems  some of the agents are controllable  while others are uncontrollable but are assumed
to adopt the basic model of expected utility maximization  the punishment mechanism
is  part of  the control strategy that is used to inuence the behavior of the uncontrolled
agents 
   for ease of exposition  we assume that reliable agents follow the designer s instructions  we assume that
no non malicious failures  such as crash failures  are possible 

   

fion partially controlled multi agent systems

   dynamic game theoretic model

in this section we introduce a basic game theoretic model  which we use to study the
problem of the enforcement of social behavior and its solution  later on  in sections     
this model will be used to study embedded teaching  we wish to emphasize that the model
we use is the most common model for representing emergent behavior in a population 
 e g   huberman   hogg        kandori  mailath    rob        altenberg   feldman 
      gilboa   matsui        weidlich   haag        kinderman   snell        

definition   a k person game g is defined by a k dimensional matrix m of size n    

nk   where nm is the number of possible actions  or strategies  of the m th agent  the entries
of m are vectors of length k of real numbers  called payoff vectors  a joint strategy in m
is a tuple  i   i          ik    where for each    j  k  it is the case that    ij  nj  
intuitively  each dimension of the matrix represents the possible actions of one of the k
players of the game  following the convention used in game theory  we often use the term
strategy in place of action   since the dimensions of the matrix are n       nk   the i th
agent has ni possible strategies to choose from  the j  th component of the vector residing
in the  i   i          ik   cell of m  i e   mi   i       ik   represents the feedback player j receives
when the players  joint strategy is  i   i          ik    that is  if agent m s strategy is im for
all    m  k  here  we use the term joint strategy to refer to the combined choice of
strategies of all the agents 

definition   a n k g iterative game consists of a set of n agents and a given k person
game g   the game g is played repetitively an unbounded number of times  at each iteration 
a random k tuple of agents play an instance of the game  where the members of this k tuple
are selected with uniform distribution from the set of agents 

every iteration of an n k g game represents some local interaction of k agents  those agents
that play in a particular iteration of the game must choose the strategy they will use in this
interaction  an agent can use different strategies in different interactions  the outcome of
each iteration is represented by the payoff vector corresponding to the agents  joint strategy 
intuitively  this payoff tells us how good the outcome of this joint behavior is from the point
of view of each agent  many situations can be represented as an n k g game  for example 
the  trac  aspect of a multi agent system can be represented by an n k g game  where
each time a number of agents meet at an intersection  each such encounter is an instance
of a game in which agents can choose from a number of strategies  e g   move ahead  yield 
the payoff function gives the utility to each set of strategies  for example  if each time only
two agents meet and both agents choose to move ahead  a collision occurs and their payoffs
are very low 
definition   a joint strategy of a game g is called ecient if the sum of the players  payoffs
is maximal 
   in this paper we use the term emergent behavior in its classical mathematical economics interpretation 
an evolution of a behavior based on repetitive local interactions of  usually pairs of  agents  where each
agent may change its strategy for the following interactions based on the feedback it received in previous
interactions 

   

fibrafman   tennenholtz

hence  eciency is one global criterion for judging the  goodness  of outcomes from the
system s perspective  unlike single payoffs which describe a single agent s perspective  

definition   let s be a fixed joint strategy for a given game g  with payoff pi s  for player
i  in an instance of g in which a joint strategy s  was played  if pi  s   pi  s    we say that
i s punishment w r t  s is pi  s    pi  s     and otherwise we say that its benefit w r t  s is
pi s     pi  s  
hence  punishment and benefit w r t  some joint strategy s measure the gain  benefit  or
loss  punishment  of an agent if we can somehow change the joint behavior of the agents
from s to s   
in our current discussion punishment and benefit will always be with respect to a chosen
ecient solution 
as designers of the multi agent system  we would prefer it to be as ecient as possible 
in some cases this entails behavior that is in some sense unstable  that is  individual agents
may locally prefer to behave differently  thus  agents may need to be constrained to behave
in a way that is locally sub optimal  we refer to such constraints that exclude some of the
possible behaviors as social laws  
due to the symmetry of the system and under the assumption that the agents are
rational and their utility is additive  i e   that the utility of two outcomes is the sum of their
utilities   it is clear that no agent s expected payoff can be higher than the one obtained
using the strategies giving the ecient solution  thus  it is clear that in this case an ecient
solution is fair  in the sense that all agents can get at least what they could if no such law
existed  and no other solution can provide a better expected payoff 
however  the good intentions of the designer of creating an environment beneficial to
the participating agents  may backfire  a social law provides information on the behavior
of agents conforming to it  information that other agents  or their respective designers  can
use to increase their expected payoff 
example   assume that we are playing an n   g game where g is the prisoner s dilemma 
represented in strategic form by the following matrix 
agent  
agent  
 
 
 
              
 
                
the ecient solution of this game is obtained when both players play strategy    assume
that this solution is chosen by the original designer  and is followed by all agents under its
control 
a designer of a new agent that will function in an environment in which the social law is
obeyed may be tempted to program her agent not to conform to the chosen law  instead  he
will program the agent to play the strategy the maximizes its expected outcome  strategy
   addition of payoffs or utilities across agents is a dangerous practice  however  in our particular model  it
can be shown that a system in which joint strategies are always ecient maximizes each agent s expected
cumulative rewards 

   

fion partially controlled multi agent systems

    this new agent will obtain a payoff of    when playing against one of the  good  agents 
thus  even though the social law was accepted in order to guarantee a payoff of   to any
agent   good  agents will obtain a payoff of     when playing against such non conforming
agents  note that the new designer exploits information on the strategies of  good  players 
as dictated by the social law  the agents controlled by the new designer are uncontcolable
agents  their behavior can not be dictated by the original designer 
agents not conforming to the social law will be referred to as malicious agents   in order
to prevent the temptation to exploit the social law  we introduce a number of punishing
agents   designed by the initial designer  that will play  irrationally  if they detect behavior
not conforming to the social law  attempting to minimize the payoff of the malicious agents 
the knowledge that future participants have of the punishment policy would deter deviations and eliminate the need for carrying it out  hence  the punishing behavior is used as a
threat aimed at deterring other agents from violating the social law  this threat is  part of 
the control strategy adopted the controllble agents in order to inuence the behavior of the
unconrollable agents  notice that this control strategy relies on the structural assumption
that the unconrollable agents are expected utility maximizers 
we define the minimized malicious payoff as the minimal expected payoff of the malicious players that can be guaranteed by the punishing agents  a punishment exists   if the
minimized malicious payoff is lower than the expected payoff obtained by playing according
to the social law  a strategy that guarantees the malicious agents an expected payoff lower
than the one obtained by playing according to the social law is called a punishing strategy  
throughout this section and the following section we make the natural assumption that the
expected payoff of malicious agents when playing against each other is no greater than the
one obtained in the ecient solution   
example    continued  in example    the punishment would simply be to play strategy
  from now on  this may cause the payoff of a punishing agent to decrease  but would
guarantee that no malicious agent obtains a payoff better than    playing against a punishing
agent  if many non malicious agents are punishing  the malicious agents  expected payoff
would decrease and become smaller then the payoff guaranteed by the social law  strategy
  would be the punishing strategy 

   the design of punishments

in the previous section we described a general model of multi agent interaction and showed
how the perspecive of partially controlled multi agent systems leads to one possible solution
to the problem of enforcing social behavior in this setting  via the idea of threats and
punishments  we now proceed to examine the issue of punishment design 
we assume that there are p agents which the designer controls that either have an ability
to observe instances of the game that occur  or that can be informed as to the outcome of
games  there are c additional agents that conform with the law  that is  play the strategies
entailed by the chosen ecient solution   and m malicious agents  that are not bound by
the law 
   other assumptions may be treated similarly 

   

fibrafman   tennenholtz

we would like to answer questions such as  does a game offer the ability to punish 
what is the minimized malicious payoff  what is the optimal ratio between p  c  and m 
is there a difference between different social laws 
example    continued  consider example   again  we have observed above that we can
cause an expected maximal loss for the malicious agents of                 this occurs
when the punishing agents play strategy    the gain that a malicious agent makes when
playing against an agent following the social law is               in order for a punishing
strategy to be effective  it must be the case that the expected payoff of a malicious agent
will be no greater than the expected payoff obtained when following the social law  in order
to achieve this  we must ensure that the ratio of punishing conforming agents is such that a
malicious agent will have sucient encounters with punishing agents  in our case  assuming
that when   deviators meet their expected benefit is   and recalling that an agent is equally
likely to meet any other agent  we need pc      to make the incentive to deviate negative 
implementing the punishment approach requires more complex behavior  our agents
must be able to detect deviations as well as to switch to a new punishing strategy  this
whole behavior can be viewed as a new  more complex  social law  this calls for more
complex agents to carry it out  and makes the programming task harder 
clearly  we would like to minimize the number of such complex agents  keeping the
benefit of malicious behavior negative  here  the major question is the ratio between the
benefit of deviation and the prospective punishment 
as can be seen from the example  the larger the punishment  the smaller the number
of the more sophisticated punishing agents that is needed  therefore  we would like to find
out which strategies minimize the malicious agent s payoff  in order to do this we require a
few additional definitions 

definition   a two person game g is a zero sum game if for every joint strategy of the
players  the sum of the players  payoffs is   

hence  in a zero sum game  there are no win win situations  the larger the payoff of one
agent  the smaller the payoff of the other agent  by convention  the payoff matrix of a two
person zero sum game will mention only the payoffs of player   

definition   let g be a two person game  let pig  s t  be the payoff of player i in g  where
i   f    g  when strategies s and t are played by player   and   respectively  the projected

game  gp   is the following two person zero sum game  the strategies of both players are as
in g   and the payoff matrix is p gp  s  t     p g  s t   define the transposed game of g   g t  
to be the game g where the roles of the players change 

in the projected game  the first agent s payoff equals the negated value of the second agent s
payoff in the original game  thus  this game reects the desire to lower the payoffs of the
second player in the original game 
we give a general result for a two person game  g  with any number of strategies   we
make use of the following standard game theoretic definition 
   

fion partially controlled multi agent systems

definition   given a game g  a joint strategy  for the players is a nash equilibrium of

g if whenever a player takes an action that is different than its action at    its payoff given
that the other players play as in  is no higher than its payoff given that everybody plays   
that is  a strategy  is a nash equilibrium of a game if no agent can obtain a better payoff
by unilaterally changing its behavior when all the other agents play according to   
nash equilibrium is the central notion in the theory of non cooperative games  luce  
raiffa        owen        fudenberg   tirole         as a result  this notion is well studied
and understood  and reducing new concepts to this basic concept may be quite useful from
a design perspective  in particular  nash equilibrium always exists for finite games  and the
payoffs prescribed by any nash equilibria of a given zero sum game are uniquely defined 
we can show 

theorem   given an n   g iterative game  the minimized malicious payoff is achieved by

playing the strategy of player   prescribed by the nash equilibrium of the projected game gp 
when playing player    in g    and the strategy of player   prescribed by the nash equilibrium
of the projected game  g t  p  when playing player    in g    

proof  assume that the punishing agent plays the role of player    if player   adopts the

strategy prescribed by a nash equilibrium  then player   can not get a better payoff than
the one guaranteed by  since each deviation by player   will not improve its situation  by
the definition of nash equilibrium   on the other hand  player   can not cause more harm
than the harm obtained by playing its strategy in    to see this  assume that player   uses
an arbitrary strategy s  and that player   adopts the strategy prescribed by    the outcome
for player   will be not higher than the one guaranteed by playing the nash equilibrium
 by the definition of nash equilibrium   in addition  due to the fact that we have here a
zero sum game this implies that the outcome for player   will be no lower than the one
guaranteed if player   would play according to    the case where the punishing agent is
player   is treated similarly 

example    continued  continuing our prisoner s dilemma example  gp would be
agent  
agent      
 
      
 
    
with the nash equilibrium attained by playing the strategies yielding    in this example 
 g t  p   gp  therefore  the punishing strategies will be strategy     for each case 

corollary   let n   g be an iterative game  with p punishing agents  let v and v  be the

payoffs of the nash equilibria of gp and gpt respectively  which  in this case  are uniquely
defined   let b b  be the maximal payoffs player   can obtain in g and g t respectively 
   notice that  in both cases  the strategies prescribed for the original game are determined by the strategies
of player   in the nash equilibria of the projected games 

   

fibrafman   tennenholtz

assuming player   is obeying the social law  let e and e  be the payoffs of player   and   
respectively  in g   when the players play according to the ecient solution prescribed by the
social law  finally  assume that expected benefit of two malicious agents when they meet
is    a necessary and sucient condition for the existence of a punishing strategy is that
 n   p    b   b     p   v   v        e   e    
n  
n  

proof  the expected payoff obtained by a malicious agent when encountering a law 

abiding agent is b  b   and its expected payoff when encountering a punishing agent is   v  v    
in order to test the conditions for the existence of a punishing strategy we would need to
consider the best case scenario from the point of view of a malicious agent  in such a case all
non punishing agents are law abiding agents  in order to obtain the expected utility for a
malicious agent we have to make an average of the above quantities taking into account the
proportion of law abiding and punishing agents in the population  this gives us that the
expected utility for a malicious agent is    n n     p    b   b       np      v   v     by definition 
a punishing strategy exists if and only if this expected utility is lower than the expected
utility guaranteed by the social law  since the expected utility which can be guaranteed by
a social law is e  e   we get the desired result 
the value of the punishment   v  v   in the above  is independent of the ecient solution
chosen  and e   e  is identical for all ecient solutions  by definition  however  b   b  depends
on our choice of an ecient solution  when a number of such solutions exist  minimizing
b   b  is an important consideration in the design of the social law  as it affects the incentive
to  cheat  
 

 

 

 

example   let s look at a slightly different version of the prisoner s dilemma  the game
matrix is

agent  
agent  
 
 
 
              
 
                
here there are   ecient solutions  given by the joint strategies                      in
the case of       we have b b      gained by playing strategy   instead of     in the case
of       and       b b    
clearly  there is more incentive to deviate from a social law prescribing strategies      
than from a social law prescribing       or       
to summarize  the preceding discussion suggests designing a number of punishing agents 
whose behavior in punishment mode is prescribed by theorem   in the case of n   g games 
by ensuring a sucient number of such agents we take away any incentive to deviate from
the social laws  hence  given that the malicious agents are rational  they will follow the social
norm  and consequently  there will be no need to utilize the punishment mechanism  we
observed that different social laws leading to solutions that are equally ecient have different
properties when it comes to punishment design  consequently  under the assumption that
we would like to minimize the number of punishing agents while guaranteeing an ecient
   

fion partially controlled multi agent systems

solution to the participants  we should choose an ecient solution that minimizes the value
of b   b  

   embedded teaching
in this section we move on to our second study of a pcmas design problem  only now 
the uncontrollable agent is a reinforcement learner  this choice is not arbitrary  rational
agents and reinforcement learners are the two major types of agents studied in mathematical
economics  decision theory  and game theory  they are also the types of agents discussed
in work in dai which is concerned with self motivated agents  e g   zlotkin   rosenschein 
      kraus   wilkenfeld        yanco   stein        sen  sekaran    hale        
an agent s ability to function in an environment is greatly affected by its knowledge of
the environment  in some special cases  we can design agents with sucient knowledge for
performing a task  gold         but  in general  agents must acquire information on line
in order to optimize their performance  i e   they must learn  one possible approach to
improving the performance of learning algorithms is employing a teacher  for example 
lin        uses teaching by example to improve the performance of agents  supplying them
with examples that show how the task can be achieved  tan s work        can also be
viewed as a form of teaching in which agents share experiences  in both methods some nontrivial form of communication or perception is required  we strive to model a broad notion
of teaching that encompasses any behavior that can improve a learning agent s performance 
that is  we wish to conduct a general study of partially controlled multi agent systems in
which the uncontrollable agent runs a learning algorithm  at the same time  we want our
model to clearly delineate the limits of the teacher s  i e   the controlling agent s  ability to
inuence the student 
here  we propose a teaching approach that maintains a situated  spirit  much like that
of reinforcement learning  sutton        watkins        kaelbling         which we call
embedded teaching   an embedded teacher is simply a  knowledgeable  controlled agent
situated with the student in a shared environment  her  goal is to lead the student to
adopt some specific behavior  however  the teacher s ability to teach is restricted by the
nature of the environment they share  not only is her repertoire of actions limited  but
she may also lack full control over the outcome of these actions  as an example  consider
two mobile robots without any means of direct communication  robot   is familiar with
the surroundings  while robot   is not  in this situation  robot   can help robot   reach
its goal through certain actions  such as blocking robot   when it is headed in the wrong
direction  however  robot   may have only limited control over the outcome of such an
interaction because of uncertainty about the behavior of robot   and its control uncertainty 
nevertheless  robot   has a specific structure  it is a learner obeying some learning scheme 
and we can attempt to control it indirectly through our choice of actions for robot    
   to differentiate between teacher and student  we use female pronouns for the former and male pronouns
for the latter 
   in general  the fact that an agent is controllable does not imply that we can perfectly control the outcome
of its actions  only their choice  hence  a robot may be controllable in our sense  running a program
supplied by us  yet its move forward command may not always have the desired outcome 

   

fibrafman   tennenholtz

in what follows  our goal is to understand how an embedded teacher can help a student
adopt a particular behavior  we address a number of theoretical questions relating to
this problem  and then we experimentally explore techniques for teaching two types of
reinforcement learners 

   a basic teaching setting
we consider a teacher and a student that repeatedly engage in some joint activity  while
the student has no prior knowledge pertaining to this activity  the teacher understands its
dynamics  in our model  the teacher s goal is to lead the student to adopt a particular
behavior in such interactions  for example  teacher and student meet occasionally at the
road and the teacher wants to teach the student to drive on the right side  or perhaps  the
teacher and the student share some resource  such as cpu time  and the goal is to teach
him judicious use of this resource  we model such encounters as     g iterative games 
to capture the idea that the teacher is more knowledgeable than the student  we assume
that she knows the structure of the game  i e   she knows the payoff function  and that she
recognizes the actions taken at each play  on the other hand  the student does not know the
payoff function  although he can perceive the payoff he receives  in this paper  we make the
simplifying assumptions that both teacher and student have only two actions from which
to choose and that the outcome depends only on their choice of actions  furthermore 
excluding our study in section      we ignore the cost of teaching  and hence  we omit the
teacher payoff from our description   this provides a basic setting in which to take this
first step towards understanding the teaching problem  
the teaching model can be concisely modeled by a      matrix  the teacher s actions
are designated by i and ii   while the student s actions are designated by the numbers  
and    each entry corresponds to a joint action and represents the student s payoff when
this joint action is played  we will suppose that we have matrix a of figure    and that we
wish to teach the student to use action    at this stage  all we assume about the student is
that if he always receives a better payoff following action   he will learn to play it 
we can see that in some situations teaching is trivial  assume that the first row dominates the second row  i e   a   c and b   d  in that case  the student will naturally prefer to
take action    and teaching is not very challenging  although it might be useful in speeding
the learning process  for example  if a   c   b   d  as in matrix b in figure    the teacher
can make the advantage of action   more noticeable to the student by always playing action
i 
now suppose that only one of a   c or b   d holds  in this case  teaching is still easy 
we use a basic teaching strategy  which we call preemption   in preemption the teacher
chooses an action that makes action   look better than action    for example  when the
situation is described by matrix c in figure    the teacher will always choose action i  
   a case could be made for the inherent value of teaching  but this may not be the appropriate forum for
airing these views 
   in fact  our idea has been to consider the most basic embedded teaching setting which is already challenging  as we later see  this basic setting is closely related to a fundamental issue in non cooperative
games 

   

fion partially controlled multi agent systems

i ii

i ii

  a b

     

  c d
 a 

i ii

     
     
 c 

     
 b 

i ii

      
     
 d 

i

ii

       
       
 e 

figure    game matrices a  b  c  d  and e  the teacher s possible actions are i and ii  
and the student s possible actions are   and   
next  assume that both c and d are greater than both a or b  as in matrix d in figure   
regardless of which action the teacher chooses  the student receives a higher payoff by
playing action    since minf    g   maxf     g   therefore  no matter what the teacher
does  the student will learn to prefer action    teaching is hopeless in this situation 
all other types of interactions are isomorphic to the case where c   a   d   b  as in
matrix e in figure    this is still a challenging situation for the teacher because action  
dominates action    because        and            therefore  preemption cannot work 
if a teaching strategy exists  it will be more complex than always choosing the same action 
since this seems to the most challenging teaching situation  we devote our attention to
teaching a reinforcement learner to choose action   in this class of games 
it turns out that the above situation is quite important in game theory and multi agent
interaction  it is a projection of a very famous game  the prisoner s dilemma  discussed
in the previous sections  in general  we can represent the prisoner s dilemma using the
following game matrix 
teacher
student coop defect
student coop
coop  a a   b c  or more commonly coop  a a 
defect  c b   d d 
defect  c  c 

teacher
defect
  c c 
 d d 

where c   a   d   b  the actions in the prisoner s dilemma are called cooperate  coop 
and defect  we identify coop with actions   and i   and defect with actions   and ii   the
prisoner s dilemma captures the essence of many important social and economic situations 
in particular  it encapsulates the notion of cooperation  it has thus motivated enormous discussion among game theorists and mathematical economists  for an overview  see eatwell 
milgate    newman         in the prisoner s dilemma  whatever the choice of one player 
the second player can maximize its payoff by playing defect  it thus seems  rational  for
each player to defect  however  when both players defect  their payoffs are much worse than
if they both cooperate 
   

fibrafman   tennenholtz

as an example  suppose two agents will be given     each for moving some object  each
agent can perform the task alone  but it will take an amount of time and energy which they
value at      however  together  the effort each will make is valued at     we get the
following instance of the prisoner s dilemma 
agent  
agent   move
rest
move
              
rest               
in the experimental part of our study  the teacher s task will be to teach the student
to cooperate in the prisoner s dilemma  we measure the success of a teaching strategy by
looking at the cooperation rate it induces in students over some period of time  that is  the
percentage of the student s actions which are coop  the experimental results presented in
this paper involving the prisoner s dilemma are with respect to the following matrix 
teacher
student coop defect
coop                 
defect                 
we have observed qualitatively similar results in other instantiations of the prisoner s
dilemma  although the precise cooperation rate varies 

   optimal teaching policies

in the previous section we concentrated on modeling the teaching context as an instance
of a partially controlled multi agent system  and determining which particular problems
are interesting  in this section we start exploring the question of how a teacher should
teach  first  we define what an optimal policy is  then  we will define markov decision
processes  mdp   bellman         and show that under certain assumptions teaching can be
viewed as an mdp  this will allow us to tap into the vast knowledge that has accumulated
on solving these problems  in particular  we can use well known methods  such as value
iteration  bellman         to find the optimal teaching policy 
we start by defining an optimal teaching policy  a teaching policy is a function that
returns an action at each iteration  possibly  it may depend on a complete history of the
past joint actions  there is no  right  definition for an optimal policy  as the teacher s
motivation may vary  however  in this paper  the teacher s objective is to maximize the
number of iterations in which the student s action are  good   such as coop in the prisoner s
dilemma  the teacher does not know the precise number of iterations she will be playing 
so she slightly prefers earlier success to later success 
this is formalized as follows  let u a  be the value the teacher places on a student s
action  a  let  be the teacher s policy  and assume that it induces a probability distribution
pr k over the set of possible student actions at time k  we define the value of the strategy
 as
 
x
val       kek  u 
k  

   

fion partially controlled multi agent systems

where ek  u  is the expected value of u 

ek  u   

x pr

a as

 k  a   u a 

here  as is the student s set of actions  the teacher s goal is to find a strategy  that
maximizes val    the discounted expected value of the student s actions  for example  in
the case of the prisoner s dilemma  we could have
as   fcoop defectg and u coop      and u defect      
next  we define mdps  in an mdp  a decision maker is continually moving between
different states  at each point in time she observes her current state  receives some payoff
 which depends on this state   and chooses an action  her action and her current state
determine  perhaps stochastically  her next state  the goal is to maximize some function
of the payoffs  formally  an mdp is a four tuple hs  a  p  ri  where s is the state space  a
is the decision maker s set of possible actions  p   s  s  a          is the probability of
a transition between states given the decision maker s action  and r   s     is the reward
function  notice that given an initial state s   s   and a policy of the decision maker    p
induces a probability distribution ps  k over s   where ps  k  s    is the probability that the
kth state obtained will be s  if the current state is s 
the   optimal policy in an mdp is the policy that maximizes at each state s the
discounted sum of the expected values of the payoffs received at all future states  starting
at s  i e  
 
x
x
 k  ps  k  s     r s   
k  

s  s
 

although it may not be immediately obvious  a single policy maximizing discounted sums
for any starting state exists  and there are well known ways of finding this policy  in the
experiments below we use a method based on value iteration  bellman        
now suppose that the student can be in a set  of possible states  that his set of actions
is as   and that the teacher s set of actions is at   moreover  suppose that the following
properties are satisfied 
    the student s new state is a function of his old state and the current joint action 
denoted by      as  at    
    the student s action is a stochastic function of his current state  where the probability
of choosing a at state s is  s  a  
    the teacher knows the student s state   the most natural way for this to happen is that
the teacher knows the student s initial state  the function    and the outcome of each game 
and she uses them to simulate the agent  
notice that under these assumptions a teaching policy should be a function of   we
know that the student s next action is a function of his next state  we know that the
student s next state is a function of his current state  his current action  and the teacher s
current action  hence  his next action is a function of his current state and action  as
well as the teacher s current action  however  we know that the student s current action
is a function of his current state  hence  the student s next action is a function of his
current state and the teacher s current action  this implies that the only knowledge the
teacher needs to optimally choose her current action is the student s current state  and any
   

fibrafman   tennenholtz

additional information will be redundant and cannot improve her success  more generally 
when we repeat this line of reasoning indefinitely into the future  we see that the teacher s
policy should be a function of the student s state  a function from  to at   it is now
possible to see that we have the makings of the following mdp 
given this observation and our three assumptions  we see that  indeed  the teacher s
policy induces a probability distribution over the set of possible student actions at time k 
this implies that our definition of val makes sense here 
define the teacher s mdp to be tmdp  h  at  p  u i  where

x

p  s  s   at  def
 

as  as

 s  as   s    s as at 
 

 i j is defined as   when i   j   and   otherwise   that is  the probability of a transition
from s to s  under at is the sum of probabilities of the student s actions that will induce
this transition  the reward function is the expected value of u 

x

u  s  def
 

as as

 s  as   u as 

theorem   the optimal teaching policy is given by the   optimal policy in tmdp 
proof  by definition  the   optimal policy in tmdp is the policy  that for each s   
maximizes
that is 

 
x
x
 k  p
 

k  

s  

s  k  s     u  s    

 

 
x
x
 k  p

k  

however  this is equal to

  

 

 
s  k  s     

s  
 

x
as as

 s    as   u as   

 
x
x x  s   a    p
k

k  

 

s

as as s  
 

 
s  k  s    u as  

we know that ps  k  s    is the probability that s  will be the state of the student in time
k  given that the teacher uses  and that her current state is s  hence 

x  s   a    p

s  
 

s

 
s  k  s  

is the probability that as will be the action taken by the student at time k given the initial
 current  state is s  upon examination  we see now that     is identical to val    
the optimal policy can be used for teaching  when the teacher possess sucient information to determine the current state of the student  but even when that is not the case  it
allows us to calculate an upper bound on the success val    of any teaching policy    this
number is a property of the learning algorithm  and measures the degree of inuence any
agent can have over the given student 
   

fion partially controlled multi agent systems

   an experimental study

in this section we describe an experimental study of embedded teaching  first  we define the
learning schemes considered  and then  we describe a set of results obtained using computer
simulations 

    the learning schemes

we experiment with two types of students  one uses a reinforcement learning algorithm
which can be viewed as q learning with one state  and the other uses q learning  in choosing
parameters for these students we tried to emulate choices made in the reinforcement learning
literature 
the first student  which we call a blind q learner  bql   can perceive rewards  but
cannot see how the teacher has acted or remember his own past actions  he only keeps
one value for each action  for example  q  coop  and q  defect  in the case of the prisoner s
dilemma  his update rule is the following  if he performed action a and received a reward
of r then
qnew  a         ff   qold a    ff  r
the parameter ff  the learning rate  is fixed  unless stated otherwise  to     in our experiments  we wish to emphasize that although bql is a bit less sophisticated than  real 
reinforcement learners discussed in the ai literature  which is defined below   it is a popular and powerful type of learning rule  which is much discussed and used in the literature
 narendra   thathachar        
the second student is a q learner  ql   he can observe the teacher s actions and has
a number of possible states  the ql maintains a q value for each state action pair  his
states encode his recent experiences  i e   the past joint actions  the update rule is 
qnew  s  a         ff   qold  s  a    ff   r   v  s   
here r is the reward received upon performing a at state s  s  is the state of the student
following the performance of a at s   is called the discount factor  and will be      unless
otherwise noted  and v  s   is the current estimate of the value of the best policy on s   
defined as maxa as q  s    a   all q values are initially set to zero 
the student s update rule tells us how his q values change as a result of new experiences  we must also specify how these q values determine his behavior  both ql and
bql students choose their actions based on the boltzmann distribution  this distribution
associates a probability ps  a  with the performance of an action a at a state s  p  a  for
the bql  
exp q  a  t  
q s  a  t  
def
p
 
ql
 
p
 
a
 
 
 bql 
ps a  def
  p exp exp 
 
q  s  a   t  
a  a
a  a exp q  a   t  
here t is called the temperature   usually  one starts with a high value for t   which
makes the action choice more random  inducing more exploration on the part of the student 
t is slowly reduced  making the q values play a greater role in the student s choice of action 
we use the following schedule  t          and t  n       t  n             this schedule has
the characteristic properties of fast initial decay and slow later decay  we also experiment
with fixed temperature 
 

 

   

fibrafman   tennenholtz

approximately optimal policy
 

      iterations
     iterations
     iterations
    iterations

   

fraction of coops

fraction of coops

 

two q learners

   
   
   

      iterations
     iterations
     iterations
    iterations

   
   
   
   

 

 
 

 

 

 

       
temperature

 

    

 

 

 

 

       
temperature

 

    

figure    fraction of coops as a function of temperature for the approximately optimal
policy  left  and for  teaching  using an identical q learner  right   each curve
corresponds to coop rate over some fixed number of iterations  in the approx 
optimal policy the curves for            and       iterations are nearly identical 

    blind q learners

motivated by our discussion in section   we will concentrate in this section and in the
following section on teaching in the context of the prisoner s dilemma  in section     we
discuss another type of teaching setting  this section describes our experimental results
with bql  we examined a policy that approximates the optimal policy  and two teaching
methods that do not rely on a student model 
      optimal policy

first we show that bqls fit the student model of section    for their state space  we use
the set of all possible assignment for their q values  this is a continuous subspace of    
and we discretize it  in order to be able to compute a policy   obtaining a state space with
approximately        states  next  notice that transitions are a stochastic function of the
current state  current q values  and the teacher s action  to see this notice that q value
updates are a function of the current q value and the payoff  the payoff is a function of the
teacher s and student s actions  and the student s actions are a stochastic function of the
current q value  in the left side of figure   we see the success of teaching using the policy
generated by using dynamic programming to solve this optimization problem  each curve
represents the fraction of coops as a function of the temperature for some fixed number of
iterations  the values are means over     experiments 
      two q learners

we also ran experiments with two identical bqls  this can be viewed as  teaching  using
another q learner  the results are shown in the right side of figure    at all temperatures
the optimal strategy performs better than q learning as a  teaching  strategy  the fact that
at temperatures of     or less the success rate approaches   will be beneficial when we later
add temperature decay  however  we also see that there is an inherent limit to our ability
   

fion partially controlled multi agent systems

tit for tat
 

      iterations
     iterations
     iterations
    iterations

   

fraction of coops

fraction of coops

 

  tit for tat

   
   
   

      iterations
     iterations
     iterations
    iterations

   
   
   
   

 

 
 

 

 

 

       
temperature

 

    

 

 

 

 

       
temperature

 

    

figure    fraction of coops as a function of temperature for the teaching strategy based
on tft  left  and  tft  right  
to affect the behavior at higher temperatures  an interesting phenomenon is the phase
transition observed around t        a qualitative explanation of this phenomenon is that
high temperature adds randomness to the student s choice of action  because it makes the
probabilities p  a  less extreme  consequently  the ability to predict the student s behavior
lessens  and with it the probability of choosing a good action  however  while randomness
serves to lower the success rate initially  it also guarantees a level of effective cooperation 
which should approach     as the temperature increases  finally  notice that although
 coop coop  seems like the best joint action for a pair of agents  two interacting q learners
never learn to play this joint strategy consistently  although they approach     coops at
low temperatures 
      teaching without a model

when the teacher does not have a precise model of the student  we cannot use the techniques
of section   to derive an optimal policy  in these models  we assume that the teacher
can  observe  the student s current state  i e  that it knows the student s q values   we
therefore explore two teaching methods that only exploit knowledge of the game and the
fact that the student is a bql 
both methods are motivated by a basic strategy of countering the student s move  the
basic idea is to try and counter good actions by the student with an action that will lead
to a high payoff  and to counter bad actions with an action that will give him a low payoff 
ideally  we would like to play coop when the student plays coop  and defect when the
student plays defect  of course  we don t know what action the student will choose  so we
try to predict from his past actions 
if we assume that the q values change very little from one iteration to the other  the
student s most likely action in the next game is the same action that he took in the most
recent game  therefore  if we saw the student play coop in the previous turn  we will play
coop now   similarly  the teacher will follow a defect by the student with a defect on her
   

fibrafman   tennenholtz

fraction of coops over time

fraction of coops

 
   
   
   
approximately optimal
q learning
tit for tat
  tit for tat

   
   
    

         
iterations

    

     

figure    fraction of coops as a function of time for bql using the temperature decay
scheme of section      teaching strategies shown  approximately optimal strategy  q learning  tft  and  tft 
part  this strategy  called tit for tat  tft for short   is well known  eatwell et al         
our experiments show that it is not very successful in teaching a bql  see figure    
we also experimented with a variant of tft  which we call  tft  in this strategy the
teacher plays defect only after observing two consecutive defects on the part of the student 
it is motivated by our observation that in certain situations it is better to let the student
enjoy a free lunch  that is  match his defect with a coop  than to make coop look bad
to him  because that may cause his q value for coop to be so low that he is unlikely to
try it again  two consecutive defects indicate that the probability of the student playing
defect next is quite high  the results  shown in figure    indicate that this strategy worked
better than tft  and in some ranges of temperature  better than q learning  however  in
general  both tft and  tft gave disappointing results   
finally  figure   shows the performance of all four teaching strategies discussed so
far when we incorporate temperature decay  we can see that the optimal policy is very
successful  as we explained before  teaching is easier when the student is more predictable 
which is the case when temperature is lower  with temperature decay the student spends
most of his time in relatively low temperature and behaves similarly to the case of fixed 
low temperature  while an initial high temperature phase could have altered this behavior 
we did not observe such effects 

    teaching q learners

unlike bql  q learners  ql  have a number of possible states which encode the joint actions
of previous games played  a ql with memory one has four possible states  corresponding
to the four possible joint actions in the prisoner s dilemma  a ql with more memory will
have more states  encoding a sequence of joint actions 
more complex learning architectures have more structure  which brings with it certain
problems  one possible problem may be that this structure is more  teaching resistant   a

    in some sense the use of an identical q learner implies having a model of the student  while tft and
 tft do not make use of such a model 

   

fion partially controlled multi agent systems

tit for tat

two q learners
 
fraction of coops

fraction of coops

 
   
   
   

      iterations
     iterations
     iterations
    iterations

   

      iterations
     iterations
     iterations
    iterations

   
   
   
   

 

 
 

 

 

 

       
temperature

 

    

 

 

 

 

       
temperature

 

    

figure    each curve shows the fraction of coops of ql as a function of temperature for a
fixed number of iterations when tft was used to teach  left  and when an identical q learner was used to teach  right   values are means over     experiments 
more real threat is added computational complexity  as we mentioned  to approximate the
optimal teaching policy for bql we had to compute over a space of approximately       
discretized states  while representing the state of a bql requires only two numbers  one
for each q value  representing the state of a ql with m states requires  m     numbers 
one for the q value of each state action pair  and one encoding the current state  the size
of the corresponding discretized state space for the teacher s markov decision process grows
exponentially in m  for the simplest case of memory one  a student with four states  this
would be about      states  since solving the problem with        states took    hours
on a sun sparcstation     we were not able to approximate optimal teaching policies for
even the simplest ql 
but all is not lost  more structure may mean more complexity  but it also means more
properties to exploit  we can reach surprisingly good results by exploiting the structure of
q learners  moreover  we can do this using a teaching method introduced in the previous
section  however  in ql this method takes on a new meaning that suggests the familiar
notions of reward and punishment  interestingly  one may recall that punishment has been
our major tool in our approach to the enforcement of social behavior 
in choosing their actions  qls  care  not only about immediate rewards  but also about
the current action s effect on future rewards  this makes them suitable for a reward and
punishment scheme  the idea is the following  suppose the ql did something  bad   defect
in our case   although we cannot reliably counter such a move with a move that will lower
his reward  we can punish him later by choosing an action that always gives a negative
payoff  no matter what the student plays  we achieve this by following a student s defect
with a defect by the teacher  while the immediate reward obtained by a ql playing defect
may be high  he will also learn to associate a subsequent punishment with the defect action 
thus  while it may be locally beneficial to perform defect  we may be able to make the
long term rewards of defect less desirable  similarly  we can follow a student s coop with
a reward in the form of a coop by the teacher  since it guarantees a positive payoff to the
student 
   

fibrafman   tennenholtz

fraction of coops over time

fraction of coops

 

tit for tat
q learning

   
   
   
   
   
    

         
iterations

    

     

figure    fraction of coops of ql as a function of time with temperature decay with tft
and with q learning as teaching strategies 
this suggests using tit for tat again  notice that for bqls  tft cannot be understood
as a reward punishment strategy because bqls care only about the immediate outcome of
an action  the value they associate with each action is a weighted average of the immediate
payoffs generated by playing this action 
in figure   we see the success rates of tft as a function of temperature  as well as the
rates for q learning as a teaching strategy  in this latter case  the teacher is identical to the
student  it is apparent that tft is extremely successful  especially in higher temperatures 
interestingly  the behavior is quite different than that of two qls  indeed  when we examine
the behavior of two qls  we see that  to a lesser extent  the phase change noticed in
bqls still exists  we obtain completely different behavior when tft is used  coop levels
increase with temperature  reaching almost      above      hence  we see that tft works
better when the student q learner exhibits a certain level of experimentation  indeed  if
we examine the success of these teaching strategies at a very low temperature  we see that
q learning performs better than tft  this explains the behavior of tft and ql when
temperature decay is introduced  as described in figure    in this figure  ql seems to be
more effective than tft  this is probably a result of the fact that in this experiment the
student s temperature is quite low most of the time 
in these experiments the ql remembers only the last joint action  we experimented with
ql with more memory and performance was worse  this can be explained as follows  for a
ql with memory one or more  the problem is a fully observable markov decision process once
the teacher plays tft  because tft is a deterministic function of the previous joint action 
we know that q learning converges to the optimal policy under such conditions  watkins  
dayan         adding more memory effectively adds irrelevant attributes  which  in turn 
causes a slower learning rate  we have also examined whether  tft would be successful
when agents have a memory of two  the results are not shown here  but the success rate
was considerably lower than for tft  although better than for two qls 
tft performed well as a teaching strategy  and we explained the motivation for using it 
we now want to produce a more quantitative explanation  one that can be used to predict
its success when we vary various parameters  such as the payoff matrix 
   

fion partially controlled multi agent systems

coop rates as a function of dif

fraction of coops

 
   
   
   
   
 
       

 

  

     
dif

  

  

  

figure    coop rates as a function of dif   a   b     a   c     c   d     b   d    the means
are for     experiments        iterations each  student s memory is   
let the student s payoff matrix be as in matrix a of figure    let p be the probability
that the student plays coop  and let q       p be the probability that the student plays
defect  these probabilities are a function of the student s q values  see the description in
section       let us assume that the probabilities p and q do not change considerably from
one iteration to the next  this seems especially justified when the learning rate  ff  is small 
given this information  what is the student s expected reward for playing coop  in
tft  the teacher s current action is the student s previous action  so we can also assume
that the teacher will play coop with probability p  thus  the student s expected payoff for
playing coop is  p  a   q  b   since q learners care about their discounted future reward
 not just their current reward   what happens next is also important  since we assumed
that the student cooperated  the teacher will cooperate in the next iteration  and if we still
assume p to be the probability that the student will cooperate next  the student s expected
payoff in the next step is  p  a   q  c   if we ignore higher order  terms the expected reward
of playing coop becomes  p  a   q  b     p  a   q  c   the expected reward of defect is thus 
p  c   q  d     p  b   q  d   therefore  tft should succeed as a teaching strategy when 

p  a   q  b     p  a   q  c    p  c   q  d     p  b   q  d  
since initially p   q        and it is the behavior at the stage where p and q are approximately equal that will determine whether tft succeeds  we can attempt to predict the
success of tft based on whether 
dif   a   b     a   c      c   d     b   d      
to test this hypothesis we ran tft on a number of matrices using q learners with different
discount factors  the results in figure   show the fraction of coops over       iterations
as a function of dif for a teacher using tft  and with temperature decay  we see that
dif is a reasonable predictor of success  when it is below    almost all rates are below
     and above   most rates are above      however  between   and   it is not successful 
   

fibrafman   tennenholtz

    teaching as a design tool

in section   we identified a class of games that are challenging to teach  and the previous
sections were mostly devoted to exploring teaching strategies in these games when the
student is a q learner  one of the assumptions we made was that the teacher is trying to
optimize some function of the student s behavior and does not care what she has to do in
order to achieve this optimal behavior  however  often the teacher would like to maximize
some function that depends both on her behavior and on the student s behavior  when
this is the case  even the more simple games discussed in section   pose a challenge 
in this section  we examine a basic coordination problem  block pushing  in which our
objective is not teaching  but where teaching is essential for obtaining good results  our
aim in this section is to demonstrate this point  and hence the value of understanding
embedded teaching  our results show that there is a teaching strategy that achieves much
better performance than a naive teaching strategy and leads to behavior that is much better
than that of two reinforcement learners 
consider two agents that must push a block as far as possible along a given path in
the course of        time units  at each time unit each agent can push the block along
the path  either gently  saving its energy  or hard  spending much energy   the block will
move in each iteration c  x  h        x   h units in the desired direction  where h  c     are
constants and x is the number of agents which push hard  at each iteration  the agents are
paid according to the distance the block was pushed  naturally  the agents wish to work as
little as possible while being paid as much as possible  and the payoff in each iteration is a
function of the cost of pushing and the payment received  we assume that each agent prefers
that the block will be pushed hard by at least one of the agents  guaranteeing reasonable
payment   but each agent also prefers that the other agent will be the one pushing hard  if
we denote the two actions by gentle and hard  we get that the related game can be described
as follows 
hard gentle
hard            
gentle            

notice that the above game falls into the category of games where teaching is easy  if
all the teacher cares about is that the student will learn to push hard  she will simply push
gently  however  when the teacher is actually trying to maximize the distance in which the
block moved  this teaching strategy may not be optimal  notice that there can be at most
      instances of hard push  the naive teaching strategy mentioned above will yield no
more than       instances of hard push  in order to increase the number  we need a more
complex teaching strategy 
in the results below we use bql with ff          consider the following strategy for
the teacher  push gently for k iterations  and then start to push hard  as we will see  by
a right selection of k   we obtain the desired behavior  not only will the student push hard
most of the time  but the total number of hard push instances will improve dramatically 
in figure    the x coordinate corresponds to the parameter k   while the y coordinate
corresponds to the number of hard push instances which occur in       iterations  the
results obtained are average results for    trials 
   

fion partially controlled multi agent systems

     
     
     
     
     
     
     
    

    

    

    

figure    teaching to push hard  number of hard push instances by the student in      
iterations as a function if the number of iterations in which the teacher does not
push hard  avg  over    trials  
as we can see from figure    the eciency of the system is non monotonic in the
threshold k   the behavior we obtain with an appropriate selection of k is much better
than what we would have obtained with the naive teaching strategy  it is interesting to
note the existence of a sharp phase transition in the performance at the neighborhood of
the optimal k   finally  we mention that when both agents are reinforcement learners  we
get only      instances of  push hard   which is much worse than what is obtained when
we have a knowledgeable agent that utilizes its knowledge to inuence the behavior of the
other agent 

   towards a general theory
the two case studies presented in this paper raise the natural question of whether general 
domain independent techniques for pcmas design exist  and whether we have learned
about such tools from our case studies  we believe that it is still premature to say whether
a general theory of pcmas design will emerge  this requires much additional work  indeed 
given the considerable differences that exist between the two domains explored in this
paper  and given the large range of multi agent systems and agents that can be envisioned 
we doubt the existence of common low level techniques for pcmas design  even within
the class of rational agents which we investigated  agents can differ considerably in their
physical  computational  and memory capabilities  and in their approach to decision making
 e g   expected utility maximization  maximization of worst case outcomes  minimization of
regret   similarly  the problem of social law enforcement can take on different forms  for
example  the malicious agents could cooperate among each other  however  once a more
abstract view is taken  certain important unifying concepts appear  namely  punishment
and reward 
punishment and reward are abstract descriptions of two types of high level feedbacks
that the controllable agents can provide to the uncontrollable agents  although punishment
and reward take different form and meaning in the two domains  in both cases  the uncon   

fibrafman   tennenholtz

trollable agents seem to  care  about the controllable agent s reaction to their action  what
we see is that in both cases  the controllable agents can inuence the uncontrollable agents 
perception of the worthiness of their actions  the precise manner in which the controllable
agents affect this perception differs  but in both cases it utilizes some inherent aspect of
uncertainty in the uncontrollable agent s world model  in the case of rational agents  despite
their perfect knowledge of the dynamics of the world  uncertainty remains regarding the
outcome of the non malicious agents  actions  by fixing a certain course of action for the
controllable agents  we inuence the malicious agents  perception of the outcome of their
own actions  in the case of the learning agent  one can affect the perception of the student s
action by affecting its basic world model  hence  it seems that a high level approach for pcmas design has two stages  first  we analyze the factors that inuence the uncontrollable
agent s perception of their actions  next  we analyze our ability to control these factors  in
retrospect  this has been implicit in our approach  in our study of social law enforcement 
we used the projected game to find out how an agent s perception of an action can be
changed and used the indirect mechanism of threats to enforce the perception we desired 
in our study of embedded teaching  we started with an analysis of different games and the
possibility of affecting an agent s perception of an action in these games  next  we tried
to provide this perception  in the case of bql students  our controllable teacher did not
have complete control over the elements that determine the student s perception because
of the random nature of the student s action  yet  she did try to somehow affect them  in
the case of the q learners  direct control was not available over all factors determining the
student s perception  yet  the teacher could control some aspects of this perception  which
were found to be sucient 
one might ask how representative our studies are of general pcmas domains  and
therefore  how relevant is the insight they may provide  we have chosen these two domains
with the belief that they represent key aspects of the types of agents studied in ai  in
ai  we study dynamic agents that act to improve their state  these agents are likely to
use information to revise their assessment of the state of the world  much like the learning
agents  and will need to make decisions based on their current information  much like the
expected utility maximizers we have studied  hence  typical multi agent systems studied in
ai include agents that exhibit one or both of these properties 
while punishment and rewards provide the conceptual basis for designing the controllable agents  mdps supply a natural model for many domains  in particular  mdps are
suitable when uncertainty exists  stemming either from the other agents  choices or from
nature  as we showed in section    at least in principle  we can use established techniques to
obtain strategies for the controllable agents when the problem can be phrased as a markov
decision process  using the mdp perspective in other cases would require more sophisticates tools and a number of important challenges must be met first      the assumptions
that the agent s state is fully observable and that the environment s state is fully observable
is unrealistic in many domains  when these assumptions are invalid  we obtain a partially
observable markov decision process  pomdp   sondik         unfortunately  although
pomdps can be used in principle to obtain the ideal policy for our agents  current techniques for solving pomdps are limited to very small problems  hence  in practice one will
have to resort to heuristic punishment and reward strategies      in section   we had only
   

fion partially controlled multi agent systems

one controlling agent  this poses a natural challenge of generalizing tools and techniques
from mdps to distributed decision making processes 

    summary and related work
this paper introduces the distinction between controllable and uncontrollable agents and the
concept of partially controlled multi agent systems  it provides two problems in multi agent
system design that naturally fall into the framework of pcmas design and suggests concrete
techniques for inuencing the behavior of the uncontrollable agents in these domains  this
work contributes to ai research by introducing and exploring a promising perspective on
system design and it contributes to des research by considering two types of structural
assumptions on agents  corresponding to rational and learning agents 
the application of our approach to the enforcement of social behavior introduces a new
tool in the design of multi agent systems  punishment and threats  we used this notion and
investigated it as part of an explicit design paradigm  punishment  deterrence  and threats
have been studied in political science  dixit   nalebuff        schelling         yet  in
difference to that line of work  and its related game theoretic models   we consider the case
of a dynamic multi agent system and concentrate on punishment design issues  such as the
question of minimizing the number of reliable agents needed to control the system  unlike
much work in multi agent systems  we did not assume all agents to be rational or all agents
to be law abiding  rather  we only assumed that the designer can control some of the agents
and that deviations from the social laws by the uncontrolled agents need to be rational 
notice that the behavior of controllable agents may be considered irrational in some cases 
however  it will eventually lead to desired behavior for all the agents  some approaches
to negotiations can be viewed as incorporating threats  in particular  rosenschein and
genesereth        consider a mechanism making deals among rational agents  where agents
are asked to offer a joint strategy to be followed by all agents and declare the move they
would take if there will be no agreement on the joint strategy  this latter move can be viewed
as a threat describing the implications of refusing the agent s suggested joint strategy  for
example  in the prisoner s dilemma setting an agent may propose joint cooperation and
threaten defecting otherwise  the work in the first part of this paper could be viewed
as examining how such a threat could be credible and effective in a particular context of
iterative multi agent interactions 
as part of our study  we proposed embedded teaching as a situated teaching paradigm
suitable for modeling a wide range of teaching instances  we modeled the teacher and
the student as players in an iterated two player game  we concentrated on a particular
iterative game  which we showed to be the most challenging game of its type  in our model 
the dynamics of the teacher student interaction is made explicit  and it clearly delineated
the limits placed on the teacher s ability to inuence the student  we showed that with
a detailed model of the student  optimal teaching policies can be theoretically generated
by viewing the teaching problem as a markov decision process  the performance of the
optimal teaching policy serves as a bound on any agent s ability to inuence this student 
we examined our ability to teach two types of reinforcement learners  in particular  we
showed that when an optimal policy cannot be used  we can use tft as a teaching method 
in the case of q learners this policy was very successful  consequently  we proposed a model
   

fibrafman   tennenholtz

that explains this success  finally  we showed that even in those games in which teaching
is not challenging  it is nevertheless quite useful  moreover  when our objective is more
than simply teaching the student  even those simpler domains present some non trivial
choices  in the future we hope to examine other learning architectures and see whether the
lessons learned in this domain can be generalized  and whether we can use these methods
to accelerate learning in other domains 
a number of authors have discussed reinforcement learning in multi agent systems 
yanco and stein        examine the evolution of communication among cooperative reinforcement learners  sen et al         use q learning to induce cooperation between two
block pushing robots  matraic        and parker        consider the use of reinforcement
learning in physical robots  they consider features of real robots  which are not discussed
in this paper  shoham and tennenholtz        examine the evolution of conventions in a
society of reinforcement learners  kittock        investigates the effects of societal structure on multi agent learning  littman        develops reinforcement learning techniques for
agents whose goals are opposed  and tan        examines the benefit of sharing information
among reinforcement learners  finally  whitehead        has shown that n reinforcement
learners that can observe everything about each other can decrease learning time by a factor
of n  however  the above work is not concerned with teaching  or with the question of how
much inuence one agent can have over another  lin        is explicitly concerned with
teaching as a way of accelerating learning of enhanced q learners  he uses experience replay and supplies students with examples of how the task can be achieved  as we remarked
earlier  this teaching approach is different from ours  since the teachers are not embedded
in the student s domain  within game theory there is an extensive body of work that tries
to understand the evolution of cooperation in the iterated prisoner s dilemma and to find
good playing strategies for it  eatwell et al          in that work both players have the
same knowledge  and teaching is not an issue 
last but not least  our work has important links to work on conditioning and especially
operant conditioning in psychology  mackintosh         in conditioning experiments an
experimenter tries to induce changes in its subjects by arranging that certain relationships
will hold in their environment  or by explicitly  in operant conditioning  reinforcing the
subjects  actions  in our framework the controlled agent plays a similar role to that of the
experimenter  our work uses a control theoretic approach to the related problem  while
applying it to two basic ai contexts 
the main drawback of our case studies is the simple domains in which they were conducted  while this is typical of initial exploration of new problems  future work should try
to remove some of the limiting assumptions that our models incorporate  for example  in
the embedded teaching context  we assumed that there is no uncertainty about the outcome of a joint action  similarly  our model of multi agent interaction in section   is very
symmetric  assuming all agents can play all of the k roles in the game  that they are equally
likely to play each role  etc  another assumption made was that malicious agents were
 loners  acting on their own  as opposed to a team of agents  perhaps more importantly 
future work should identify additional domains that are naturally described in terms of
pcmas and formalize a general methodology for solving pcmas design problems 
   

fion partially controlled multi agent systems

acknowledgements
we are grateful to yoav shoham and other members of the nobotics group at stanford for
their input  and to the anonymous referees for their productive comments and suggestions 
we are especially grateful to james kittock for his comments and his help in improving the
presentation of this paper  this research was supported by the fund for the promotion of
research at the technion  by nsf grant iri          and by afosr grant af f               

references

altenberg  l     feldman  m  w          selection  generalized transmission  and the
evolution of modifier genes  i  the reduction principle  genetics          
bellman  r          dynamic programming  princeton university press 
bond  a  h     gasser  l          readings in distributed artificial intelligence  ablex
publishing corporation 
briggs  w     cook  d          flexible social laws  in proc    th international joint
conference on artificial intelligence  pp          
dixit  a  k     nalebuff  b  j          thinking strategically   the competitive edge in
business  politics  and everyday life  norton  new york 
durfee  e  h   lesser  v  r     corkill  d  d          coherent cooperation among communicating problem solvers  ieee transactions on computers                
dwork  c     moses  y          knowledge and common knowledge in a byzantine environment  crash failures  information and computation                  
eatwell  j   milgate  m     newman  p   eds            the new palgrave  game theory 
w w norton   company  inc 
fox  m  s          an organizational view of distributed systems  ieee trans  sys   man  
cyber             
fudenberg  d     tirole  j          game theory  mit press 
gilboa  i     matsui  a          social stability and equilibrium  econometrica         
        
gold  m          complexity of automaton identificaion from given data  information
and control              
huberman  b  a     hogg  t          the behavior of computational ecologies  in
huberman  b  a   ed    the ecology of computation  elsevier science 
kaelbling  l          learning in embedded systems  ph d  thesis  stanford university 
   

fibrafman   tennenholtz

kandori  m   mailath  g     rob  r          learning  mutation and long equilibria in
games  mimeo  university of pennsylvania       
kinderman  r     snell  s  l          markov random fields and their applications 
american mathematical society 
kittock  j  e          the impact of locality and authority on emergent conventions  in
proceedings of the twelfth national conference on artificial intelligence  aaai      
pp          
kraus  s     wilkenfeld  j          the function of time in cooperative negotiations  in
proc  of aaai     pp          
lin  f     wonham  w          decentralized control and coordination of discrete event
systems  in proceedings of the   th ieee conf  decision and control  pp            
lin  l          self improving reactive agents based on reinforcement learning  planning 
and teaching  machine learning          
littman  m          markov games as a framework for multi agent reinforcement learning 
in proc  of the   th int  conf  on mach  learn 
luce  r  d     raiffa  h          games and decisions  introduction and critical survey 
john wiley and sons 
mackintosh  n          conditioning and associative learning  oxford university press 
malone  t  w          modeling coordination in organizations and markets  management
science                     
mataric  m  j          reward functions for accelerating learning  in proceedings of the
  th international conference on machine learning  pp          
minsky  n          the imposition of protocols over open distributed systems  ieee
transactions on software engineering                  
moses  y     tennenholtz  m          artificial social systems  computers and artificial
intelligence                  
narendra  k     thathachar  m  a  l          learning automata  an introduction 
prentice hall 
owen  g          game theory   nd ed    academic press 
parker  l  e          learning in cooperative robot teams  in proceedings of ijcai   
workshop on dynamically interacting robots 
ramadge  p     wonham  w          the control of discrete event systems  proceedings
of the ieee                
rosenschein  j  s     genesereth  m  r          deals among rational agents  in proc 
 th international joint conference on artificial intelligence  pp        
   

fion partially controlled multi agent systems

schelling  t          the strategy of conict  harvard university press 
sen  s   sekaran  m     hale  j          learning to coordinate without sharing information 
in proc  of aaai     pp          
shoham  y     tennenholtz  m          emergent conventions in multi agent systems 
initial experimental results and observations  in proc  of the  rd international conference on principles of knowledge representation and reasoning  pp          
shoham  y     tennenholtz  m          social laws for artificial agent societies  off line
design  artificial inteligence     
sondik  e  j          the optimal control of partially observable markov processes over the
infinite horizon  discounted costs  operations research         
sutton  r          learning to predict by method of temporal differences  mach  lear  
            
tan  m          multi agent reinforcement learning  independent vs  cooperative agents 
in proceedings of the   th international conference on machine learning 
watkins  c          learning with delayed rewards  ph d  thesis  cambridge university 
watkins  c     dayan  p          q learning  machine learning                   
weidlich  w     haag  g          concepts and models of a quantitative sociology  the
dynamics of interacting populations  springer verlag 
whitehead  s          a complexity analysis of cooperative mechanisms in reinforcement
learning  in proceedings of aaai     pp          
yanco  h     stein  l          an adaptive communication protocol for cooperating
mobile robots  in from animal to animats  proceedings of the second international
conference on the simulation of adaptive behavior  pp          
zlotkin  g     rosenschein  j  s          a domain theory for task oriented negotiation 
in proc    th international joint conference on artificial intelligence  pp          

   

fi