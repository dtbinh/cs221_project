journal of artificial intelligence research               

submitted       published     

cached sucient statistics for ecient machine learning
with large datasets
andrew moore
mary soon lee

school of computer science and robotics institute
carnegie mellon university  pittsburgh pa      

awm cs cmu edu
mslee cs cmu edu

abstract

this paper introduces new algorithms and data structures for quick counting for machine
learning datasets  we focus on the counting task of constructing contingency tables  but
our approach is also applicable to counting the number of records in a dataset that match
conjunctive queries  subject to certain assumptions  the costs of these operations can be
shown to be independent of the number of records in the dataset and loglinear in the
number of non zero entries in the contingency table 
we provide a very sparse data structure  the adtree  to minimize memory use  we
provide analytical worst case bounds for this structure for several models of data distribution  we empirically demonstrate that tractably sized data structures can be produced for
large real world datasets by  a  using a sparse tree structure that never allocates memory
for counts of zero   b  never allocating memory for counts that can be deduced from other
counts  and  c  not bothering to expand the tree fully near its leaves 
we show how the adtree can be used to accelerate bayes net structure finding algorithms  rule learning algorithms  and feature selection algorithms  and we provide a
number of empirical results comparing adtree methods against traditional direct counting
approaches  we also discuss the possible uses of adtrees in other machine learning methods  and discuss the merits of adtrees in comparison with alternative representations such
as kd trees  r trees and frequent sets 

   caching sucient statistics

computational eciency is an important concern for machine learning algorithms  especially
when applied to large datasets  fayyad  mannila    piatetsky shapiro        fayyad  
uthurusamy        or in real time scenarios  in earlier work we showed how kd trees with
multiresolution cached regression matrix statistics can enable very fast locally weighted
and instance based regression  moore  schneider    deng         in this paper  we attempt
to accelerate predictions for symbolic attributes using a kind of kd tree that splits on all
dimensions at all nodes 
many machine learning algorithms operating on datasets of symbolic attributes need to
do frequent counting  this work is also applicable to online analytical processing  olap 
applications in data mining  where operations on large datasets such as multidimensional
database access  datacube operations  harinarayan  rajaraman    ullman         and
association rule learning  agrawal  mannila  srikant  toivonen    verkamo        could be
accelerated by fast counting 
let us begin by establishing some notation  we are given a data set with r records
and m attributes  the attributes are called a   a         am   the value of attribute ai in the
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fimoore   lee

th record is a small integer lying in the range f            ni g where ni is called the arity of
attribute i  figure   gives an example 
k

attributes

a 

a 

a 

n     

n     

n     

record  

 

 

 

m  

record  

 

 

 

r  

record  

 

 

 

record  

 

 

 

record  

 

 

 

record  

 

 

 

arity

figure    a simple dataset used as an example  it has r     records and
m     attributes 

    queries

a query is a set of  attribute   value  pairs in which the left hand sides of the pairs form
a subset of fa        am g arranged in increasing order of index  four examples of queries for
our dataset are
 a         a       a             a       a       a      

   

notice that the total number of possible queries is m
i    ni       this is because each
attribute can either appear in the query with one of the ni values it may take  or it may be
omitted  which is equivalent to giving it a ai      don t care  value  

    counts

the count of a query  denoted by c  query  is simply the number of records in the dataset
matching all the  attribute   value  pairs in query  for our example dataset we find 
        
c  a       a      
c   
c  a       a       a      
c a

    contingency tables

 
 
 
 

 
 
 
 

each subset of attributes  ai          ai n    has an associated contingency table denoted by
ct ai          ai n    this is a table with a row for each of the possible sets of values for
ai          ai n    the row corresponding to ai      v        ai n    vn records the count c  ai     
 
v        ai n    vn    our example dataset has   attributes and so       contingency tables
exist  depicted in figure   
  

ficached sufficient statistics for efficient machine learning

ct  

ct a   

ct a   

 

a  

a  

a a a  
 

 

 

 

   
   

   
   

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

ct a   

ct a  a  a   

 

a  
 

 
 
 
 

 
 
 
 

ct a  a   

ct a   a   
a a  
 

 

 
 
 
 

 
 
 
 

 
 
 
 

ct a  a   

a a  
 

 

a a  
 

 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

figure    the eight possible contingency tables for the dataset of figure   
a conditional contingency table  written
ct ai          ai n  j aj      u         aj p    up 

   

is the contingency table for the subset of records in the dataset that match the query to
the right of the j symbol  for example 
a 

 
ct a    a  j a          
 
 

a 

 
 
 
 

 
 
 
 
 

contingency tables are used in a variety of machine learning applications  including
building the probability tables for bayes nets and evaluating candidate conjunctive rules in
rule learning algorithms  quinlan        clark   niblett         it would thus be desirable
to be able to perform such counting eciently 
if we are prepared to pay a one time cost for building a caching data structure  then it is
easy to suggest a mechanism for doing counting in constant time  for each possible query 
we precompute the contingency table  the total amount of numbers stored in memory for
such a data structure would be m
i    ni       which even for our humble dataset of figure  
is     as revealed by figure    for a real dataset with more than ten attributes of medium
arity  or fifteen binary attributes  this is far too large to fit in main memory 
we would like to retain the speed of precomputed contingency tables without incurring
an intractable memory demand  that is the subject of this paper 

   cache reduction    the dense adtree for caching sucient statistics

first we will describe the adtree  the data structure we will use to represent the set of
all possible counts  our initial simplified description is an obvious tree representation that
does not yield any immediate memory savings   but will later provide several opportunities
   the se tree  rymon        is a similar data structure 

  

fimoore   lee

for cutting off zero counts and redundant counts  this structure is shown in figure    an
adtree node  shown as a rectangle  has child nodes called  vary nodes   shown as ovals  
each adnode represents a query and stores the number of records that match the query  in
the c     field   the vary aj child of an adnode has one child for each of the nj values
of attribute aj   the kth such child represents the same query as vary aj  s parent  with the
additional constraint that aj   k 
a   
 

 
 
 

am    
c  

vary a  
a   

a  n

a   

a   

a   

a   

  
 

  
 

 
 

  
 

 

   

 

   

vary a  

 

a   

 
 

 

   

a  n
 

  
 

 

am    

am    

am    

am    

c  

c  

c  

c  

   

   

   
vary a  

vary a m

vary a  

vary a m

  
 

  
 

   
vary a m

figure    the top adnodes of an adtree  described in the text 
notes regarding this structure 

 although drawn on the diagram  the description of the query  e g  

     a   
     am     on the leftmost adnode of the second level  is not explicitly recorded in
the adnode  the contents of an adnode are simply a count and a set of pointers to
the vary aj children 
the contents of a vary aj node are a set of pointers to adnodes 
 

a

 the cost of looking up a count is proportional to the number of instantiated variables

in the query  for example  to look up c  a       a        a        we would follow the
following path in the tree  vary a    a        vary a     a         vary a    
a        then the count is obtained from the resulting node 

 notice that if a node adn has vary
vary ai   vary

ai

  

ai

as its parent  then adn s children are

    vary am  

it is not necessary to store vary nodes with indices below i   because that information
can be obtained from another path in the tree 
  

ficached sufficient statistics for efficient machine learning

    cutting off nodes with counts of zero

as described  the tree is not sparse and will contain exactly m
i    ni      nodes  sparseness
is easily achieved by storing a null instead of a node for any query that matches zero
records  all of the specializations of such a query will also have a count of zero and they
will not appear anywhere in the tree  for some datasets this can reduce the number of
numbers that need to be stored  for example  the dataset in figure    which previously
needed    numbers to represent all contingency tables  will now only need    numbers 

   cache reduction ii  the sparse adtree

it is easy to devise datasets for which there is no benefit in failing to store counts of zero 
suppose we have m binary attributes and  m records in which the kth record is the bits of
the binary representation of k  then no query has a count of zero and the tree contains  m
nodes  to reduce the tree size despite this  we will take advantage of the observation that
very many of the counts stored in the above tree are redundant 
each vary aj node in the above adtree stores nj subtrees one subtree for each value
of aj   instead  we will find the most common of the values of aj  call it mcv  and store a
null in place of the mcvth subtree  the remaining nj     subtrees will be represented
as before  an example for a simple dataset is given in figure    each vary aj node now
records which of its values is most common in a mcv field  appendix b describes the
straightforward algorithm for building such an adtree 
as we will see in section    it is still possible to build full exact contingency tables  or
give counts for specific queries  in time that is only slightly longer than for the full adtree
of section    but first let us examine the memory consequences of this representation 
appendix a shows that for binary attributes  given m attributes and r records  the
number of nodes needed to store the tree is bounded above by  m in the worst case  and
much less if r    m    in contrast  the amount of memory needed by the dense tree of
section   is  m in the worst case 
notice in figure   that the mcv value is context dependent  depending on constraints
on parent nodes  a  s mcv is sometimes   and sometimes    this context dependency
can provide dramatic savings if  as is frequently the case  there are correlations among the
attributes  this is discussed further in appendix a 

   computing contingency tables from the sparse adtree

given an adtree  we wish to be able to quickly construct contingency tables for any arbitrary set of attributes fai          ai n  g 
notice that a conditional contingency table ct ai          ai n  j query  can be built recursively  we first build

ct 
ct 

          ai n  j ai         query 
ai          ai n  j ai         query 

ai

ct 

ai

          ai n 

j

  
 

      ni     query 

ai

  

fimoore   lee

a   
 
a   
 

c   
vary a  
mcv    

vary a  
mcv    

a   

a   

null

a   

a   

 mcv 

c   

c   

 
 

vary a  
mcv    

 
 

a   
 
a   

null
 mcv 

 

c   

a

vary a  
mcv    

null

null

null

a   

count  

 mcv 

 mcv 

a   

 

 
 
 
 
 
 
 
 

 
 

c   

a

 

 
 
 
 
 
 
 
 

figure    a sparse adtree built for the dataset shown in the bottom
right  the most common value for a  is    and so the a      subtree
of the vary a  child of the root node is null  at each of the vary a 
nodes the most common child is also set to null  which child is most
common depends on the context  
for example  to build ct a   a   using the dataset in figure    we can build ct a  j a      
and ct a  j a       and combine them as in figure   
ct a    a       

ct a    a   

a   
 
 

 
 

ct a    a       
a   
 
 

a 

a   

 
 

 
 

 
 

 
 

 
 

 
 

 
 

figure    an example  using numbers from figure    of how contingency
tables can be combined recursively to form larger contingency tables 
when building a conditional contingency table from an adtree  we will not need to
explicitly specify the query condition  instead  we will supply an adnode of the adtree 
which implicitly is equivalent information  the algorithm is 

  

ficached sufficient statistics for efficient machine learning

makecontab  f

ai          ai n  g   adn 
let vn    the vary ai    subnode of adn 
let mcv    vn mcv 
for k                  ni   
if k    mcv
let adnk    the ai      k subnode of vn 
ctk    makecontab fai          ai n  g  adnk   

ctmcv

  

 calculated as explained below 

return the concatenation of ct        ctn  
i   

the base case of this recursion occurs when the first argument is empty  in which case we
return a one element contingency table containing the count associated with the current
adnode  adn 
there is an omission in the algorithm  in the iteration over k   f            ni   g we are
unable to compute the conditional contingency table for ctmcv because the ai      mcv
subtree is deliberately missing as per section    what can we do instead 
we can take advantage of the following property of contingency tables 

ct 

ai

          ai n  j query   

x

ni   

ct 

          ai n 

ai

k  

j

      k  query 

ai

   

the value ct ai          ai n  j query  can be computed from within our algorithm by calling

makecontab f

          ai n  g  adn 

   

ai

and so the missing conditional contingency table in the algorithm can be computed by the
following row wise subtraction 
x
ctmcv    makecontab fai          ai n  g  adn   
ctk
   
k  m cv

frequent sets  agrawal et al          which are traditionally used for learning association
rules  can also be used for computing counts  a recent paper  mannila   toivonen        
which also employs a similar subtraction trick  calculates counts from frequent sets  in
section   we will discuss the strengths and weaknesses of frequent sets in comparison with
adtrees 

    complexity of building a contingency table

what is the cost of computing a contingency table  let us consider the theoretical worstcase cost of computing a contingency table for n attributes each of arity k note that this
cost is unrealistically pessimistic  except when k       because most contingency tables are
sparse  as discussed later  the assumption that all attributes have the same arity  k  is
made to simplify the calculation of the worst case cost  but is not needed by the code 
  

fimoore   lee

a contingency table for n attributes has kn entries  write c  n    the cost of computing
such a contingency table  in the top level call of makecontab there are k calls to build
contingency tables from n     attributes  k     of these calls are to build ct ai          ai n  j
ai      j  query  for every j in f            k g except the mcv  and the final call is to build
ct ai          ai n  j query   then there will be k     subtractions of contingency tables 
which will each require kn   numeric subtractions  so we have
c    
   
   
n  
c  n 
  kc  n         k     k
if n    
   
the solution to this recurrence relation is c  n         n k      kn     this cost is loglinear
in the size of the contingency table  by comparison  if we used no cached data structure 
but simply counted through the dataset in order to build a contingency table we would
need o nr   kn   operations where r is the number of records in the dataset  we are thus
cheaper than the standard counting method if kn  r  we are interested in large datasets
in which r may be more than           in such a case our method will present a several
order of magnitude speedup for  say  a contingency table of eight binary attributes  notice
that this cost is independent of m  the total number of attributes in the dataset  and
only depends upon the  almost always much smaller  number of attributes n requested for
the contingency table 

    sparse representation of contingency tables

in practice  we do not represent contingency tables as multidimensional arrays  but rather
as tree structures  this gives both the slow counting approach and the adtree approach
a substantial computational advantage in cases where the contingency table is sparse  i e 
has many zero entries  figure   shows such a sparse contingency table representation  this
can mean average case behavior is much faster than worst case for contingency tables with
large numbers of attributes or high arity attributes 
indeed  our experiments in section   show costs rising much more slowly than o nkn    
as n increases  note too that when using a sparse representation  the worst case for
makecontab is now o min nr  nkn     because r is the maximum possible number of
non zero contingency table entries 

   cache reduction iii  leaf lists

we now introduce a scheme for further reducing memory use  it is not worth building
the adtree data structure for a small number of records  for example  suppose we have
   records and    binary attributes  then the analysis in appendix a shows us that in
the worst case the adtree might require       nodes  but computing contingency tables
using the resulting adtree would  with so few records  be no faster than the conventional
counting approach  which would merely require us to retain the dataset in memory 
aside from concluding that adtrees are not useful for very small datasets  this also
leads to a final method for saving memory in large adtrees  any adtree node with fewer
than rmin records does not expand its subtree  instead it maintains a list of pointers into
the original dataset  explicitly listing those records that match the current adnode  such
a list of pointers is called a leaf list  figure   gives an example 
  

ficached sufficient statistics for efficient machine learning

v  

ct a  a   a   

v  

 

a a a  
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

v  
v  

 

v  

v  

a 

v  

v  

a

a

v  

null

null

null

v  

a

 

v  

 

null

 

v  

null

null

 

v  

v  

v  

a

 

a

v  
v  

a

 
null
null

 

v  

 

figure    the right hand figure is the sparse representation of the contingency table on the left 
the use of leaf lists has one minor and two major consequences  the minor consequence is the need to include a straightforward change in the contingency table generating
algorithm to handle leaf list nodes  this minor alteration is not described here  the first
major consequence is that now the dataset itself must be retained in main memory so that
algorithms that inspect leaf lists can access the rows of data pointed to in those leaf lists 
the second major consequence is that the adtree may require much less memory  this is
documented in section   and worst case bounds are provided in appendix a 

   using adtrees for machine learning
as we will see in section    the adtree structure can substantially speed up the computation
of contingency tables for large real datasets  how can machine learning and statistical
algorithms take advantage of this  here we provide three examples  feature selection 
bayes net scoring and rule learning  but it seems likely that many other algorithms can
also benefit  for example stepwise logistic regression  gmdh  madala   ivakhnenko        
and text classification  even decision tree  quinlan        breiman  friedman  olshen   
stone        learning may benefit    in future work we will also examine ways to speed up
nearest neighbor and other memory based queries using adtrees 
   this depends on whether the cost of initially building the adtree can be amortized over many runs of
the decision tree algorithm  repeated runs of decision tree building can occur if one is using the wrapper
model of feature selection  john  kohavi    peger         or if one is using a more intensive search over
tree structures than the traditional greedy search  quinlan        breiman et al        

  

fimoore   lee

a     
a     
a     
c   
row a   a   a  

a     
a     
a     
c   
see rows      

vary a  

vary a  

vary a  

 

 

 

 

mcv    

mcv    

mcv    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

vary a  

 

 

 

 

mcv    

 

 

 

 

null
 mcv 

a     
a     
a     

null
 mcv 

a     
a     
a     

c   

null
 mcv 

a     
a     
a     

c   

c   

see rows    

null
 mcv 

a     
a     
a     
c   
see rows    

figure    an adtree built using leaf lists with rmin      any node
matching   or fewer records is not expanded  but simply records a set of
pointers into the dataset  shown on the right  

    datasets

the experiments used the datasets in table    each dataset was supplied to us with all
continuous attributes already discretized into ranges 

    using adtrees for feature selection

given m attributes  of which one is an output that we wish to predict  it is often interesting
to ask  which subset of n attributes   n   m    is the best predictor of the output on the
same distribution of datapoints that are reected in this dataset    kohavi         there
are many ways of scoring a set of features  but a particularly simple one is information
gain  cover   thomas        
let aout be the attribute we wish to predict and let ai          ai n  be the set of attributes
used as inputs  let x be the set of possible assignments of values to ai          ai n  and write
assignk   x as the kth such assignment  then
 x
jx j c  assign   nx  c  a   v  assign   
nx 
c  aout   v  
out
k
k
infogain   f
 
f
   
r
r
c  assignk  
v  
v  
k  
out

out

where r is the number of records in the entire dataset and
       x log  x
   
the counts needed in the above computation can be read directly from ct aout   ai          ai n    
searching for the best subset of attributes is simply a question of search among all
attribute sets of size n  n specified by the user   this is a simple example designed to test
f x

  

ficached sufficient statistics for efficient machine learning

name

  num  m   num 
records attributes
adult 
         
the small  adult income  dataset placed in the uci
repository by ron kohavi  kohavi         contains
census data related to job  wealth  and nationality  attribute arities range from   to     in the uci repository this is called the test set  rows with missing
values were removed 
adult 
         
the same kinds of records as above but with different
data  the training set 
adult 
         
adult  and adult  concatenated 
census 
          
a larger dataset based on a different census  also provided by ron kohavi 
census 
          
the same data as census   but with the addition of
two extra  high arity attributes 
birth
        
records concerning a very wide number of readings
and factors recorded at various stages during pregnancy  most attributes are binary  and    of the attributes are very sparse  with over     of the values
being false 
synth
  k    k   
synthetic datasets of entirely binary attributes generated using the bayes net in figure   
r

table    datasets used in experiments 

our counting methods  any practical feature selector would need to penalize the number of
rows in the contingency table  else high arity attributes would tend to win  

    using adtrees for bayes net structure discovery

there are many possible bayes net learning tasks  all of which entail counting  and hence
might be speeded up by adtrees  in this paper we present experimental results for the
particular example of scoring the structure of a bayes net to decide how well it matches the
data 
we will use maximum likelihood scoring with a penalty for the number of parameters 
we first compute the probability table associated with each node  write parents j  for the
parent attributes of node j and write xj as the set of possible assignments of values to
parents j   the maximum likelihood estimate for
    v j xj  

p aj

is estimated as

    

    v  xj  
    
c  xj  
and all such estimates for node j  s probability tables can be read from ct aj   parents j   
the next step in scoring a structure is to decide the likelihood of the data given the
probability tables we computed and to penalize the number of parameters in our network
 without the penalty the likelihood would increase every time a link was added to the
c aj

  

fimoore   lee

figure    a bayes net that generated our synth datasets  there are
three kinds of nodes  the nodes marked with triangles are generated with
p  ai             p  ai             the square nodes are deterministic 
a square node takes value   if the sum of its four parents is even  else it
takes value    the circle nodes are probabilistic functions of their single
parent  defined by p  ai     j parent          and p  ai     j parent  
          this provides a dataset with fairly sparse values and with many
interdependencies 
network   the penalized log likelihood score  friedman   yakhini        is

 

n

params log r      r

n
m
x
x x
j

j   

asgn

 xj v  

    v   asgn  log p  aj   v j asgn 

p aj

    

where nparams is the total number of probability table entries in the network 
we search among structures to find the best score  in these experiments we use randomrestart stochastic hill climbing in which the operations are random addition or removal of
a network link or randomly swapping a pair of nodes  the latter operation is necessary to
allow the search algorithm to choose the best ordering of nodes in the bayes net  stochastic
searches such as this are a popular method for finding bayes net structures  friedman  
yakhini         only the probability tables of the affected nodes are recomputed on each
step 
figure   shows the bayes net structure returned by our bayes net structure finder after
       iterations of hill climbing 

    using adtrees for rule finding
given an output attribute aout and a distinguished value
conjunctive queries of the form

out 

v

assign    ai      v        ai n    vn  
  

rule finders search among
    

ficached sufficient statistics for efficient machine learning

a 
 
t 
t 
r 
i 
b 
u 
t 
e
relationship
class
sex
capital gain
hours per week
marital status
education num
capital loss
age
race
education
workclass
native country
fnlwgt
occupation

 
s 
c 
o 
r 
e
 
n 
p
       
 
        
  
        
  
            
        
  
        
  
      
  
       
  
        
  
        
  
       
  
       
   
        
  
            
       
   

pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 no parents 
relationship
relationship class
class
relationship class sex
relationship sex
class
class
marital status
relationship education num
relationship education num
relationship hours per week education num
education num race
 no parents 
class sex education workclass

score is       
the search took     seconds 

figure    output from the bayes structure finder running on the
adult  dataset  score is the contribution to the sum in equation    due
to the specified attribute  np is the number of entries in the probability
table for the specified attribute 
to find the query that maximizes the estimated value
 

p aout

vout   assign 
  vout j assign    c  aoutc  assign
 

    

to avoid rules without significant support  we also insist that c  assign   the number of
records matching the query  must be above some threshold smin 
in these experiments we implement a brute force search that looks through all possible
queries that involve a user specified number of attributes  n  we build each ct aout   ai          ai n   
in turn  there are m choose n such tables   and then look through the rows of each table
for all queries using the ai          ai n  that have greater than minimum support smin  we
return a priority queue of the highest scoring rules  for instance on the adult  dataset 
the best rule for predicting  class  from   attributes was 
score                    workclass   private  education num   above    maritalstatus   married civ spouse  capital loss   above       class    k

   experimental results
let us first examine the memory required by an adtree on our datasets  table   shows us 
for example  that the adult  dataset produced an adtree with        nodes  the tree
required almost    megabytes of memory  among the three adult datasets  the size of
the tree varied approximately linearly with the number of records 
unless otherwise specified  in all the experiments in this section  the adult datasets
used no leaf lists  the birth and synthetic datasets used leaf lists of size rmin      by
default  the birth dataset  with its large number of sparse attributes  required a modest
  megabytes to store the tree many magnitudes below the worst case bounds  among the
synthetic datasets  the tree size increased sublinearly with the dataset size  this indicates
  

fimoore   lee

dataset
census 
census 
adult 
adult 
adult 
birth
syn  k
syn  k
syn   k
syn   k
syn   k

m
  
  
  
  
  
  
  
  
  
  
  

r
      
      
     
     
     
    
     
     
      
      
      

nodes megabytes build time
     
   
  
      
    
  
     
   
 
     
    
  
      
    
  
     
   
  
     
   
 
     
   
  
     
   
  
      
    
  
      
    
   

table    the size of adtrees for various datasets  m is the number of
attributes  r is the number of records  nodes is the number of nodes
in the adtree  megabytes is the amount of memory needed to store the
tree  build time is the number of seconds needed to build the tree  to
the nearest second  

that as the dataset gets larger  novel records  which may cause new nodes to appear in the
tree  become less frequent 
table   shows the costs of performing        iterations of bayes net structure searching 
all experiments were performed on a    mhz pentium pro machine with     megabytes
of main memory  recall that each bayes net iteration involves one random change to the
network and so requires recomputation of one contingency table  the exception is the first
iteration  in which all nodes must be computed   this means that the time to run       
iterations is essentially the time to compute        contingency tables  among the adult
datasets  the advantage of the adtree over conventional counting ranges between a factor
of    to     unsurprisingly  the computational costs for adult increase sublinearly with
dataset size for the adtree but linearly for the conventional counting  the computational
advantages and the sublinear behavior are much more pronounced for the synthetic data 
next  table   examines the effect of leaf lists on the adult  and birth datasets  for
the adult dataset  the byte size of the tree decreases by a factor of   when leaf lists are
increased from   to     but the computational cost of running the bayes search increases
by only      indicating a worth while tradeoff if memory is scarce 
the bayes net scoring results involved the average cost of computing contingency tables
of many different sizes  the following results in tables   and   make the savings for fixed
size attribute sets easier to discern  these tables give results for the feature selection and
rule finding algorithms  respectively  the biggest savings come from small attribute sets 
computational savings for sets of size one or two are  however  not particularly interesting
since all such counts could be cached by straightforward methods without needing any tricks 
in all cases  however  we do see large savings  especially for the birth data  datasets with
larger numbers of rows would  of course  reveal larger savings 
  

ficached sufficient statistics for efficient machine learning

dataset
census 
census 
adult 
adult 
adult 
birth
syn  k
syn  k
syn   k
syn   k
syn   k

m
  
  
  
  
  
  
  
  
  
  
  

r adtree time regular time speedup factor
      
   
     
     
      
   
     
    
     
   
    
  
     
   
    
    
     
   
    
    
    
  
    
     
     
  
     
     
     
  
     
     
      
  
     
      
      
  
     
      
      
  
      
      

table    the time  in seconds  to perform        hill climbing iterations
searching for the best bayes net structure  adtree time is the time
when using the adtree and regular time is the time taken when using
the conventional probability table scoring method of counting through
the dataset  speedup factor is the number of times by which the adtree
method is faster than the conventional method  the adtree times do
not include the time for building the adtree in the first place  given in
table     a typical use of adtrees will build the tree only once and then
be able to use it for many data analysis operations  and so its building
cost can be amortized  in any case  even including tree building cost
would have only a minor impact on the results 

min

r

 
 
 
 
  
  
  
   
   
   
    
    
    

adult 
birth
 mb  nodes build search  mb  nodes build search
secs
secs
secs
secs
            
  
   
           
  
   
           
 
                 
  
  
           
 
                 
  
  
           
 
               
  
  
           
 
               
  
  
           
 
               
 
  
           
 
               
 
  
           
 
        
     
 
  
    
     
 
        
     
 
   
    
     
 
        
   
 
   
    
     
 
         
   
 
   
    
   
 
         
   
 
   

table    investigating the effect of the rmin parameter on the adult 
dataset and the birth dataset   mb is the memory used by the adtree 
 nodes is the number of nodes in the adtree  build secs is the time to
build the adtree  search secs is the time needed to perform       
iterations of the bayes net structure search 
  

fimoore   lee

number
number
attributes attribute
sets
 
  
 
  
 
   
 
     
 
     

adult 

adtree regular speedup

time

time

factor

       
      
     
     
    

    
    
    
   
   

     
     
    
    
   

number
attribute
sets
  
     
       
         
          

birth

adtree regular speedup
time

time

            
            
            
      
      

factor
   
   
   

table    the time taken to search among all attribute sets of a given size
 number attributes  for the set that gives the best information gain in
predicting the output attribute  the times  in seconds  are the average
evaluation times per attribute set 

number number
attributes
rules
 
   
 
     
 
      
 
       
           

adult 

adtree regular speedup

time
       
       
       
       
       

time
     
     
      
      
      

factor
     
    
    
   
   

number
rules
   
      
       
          
             

birth

adtree regular speedup

time
       
       
       
       
       

time
     
     
     
     

table    the time taken to search among all rules of a given size  number attributes  for the highest scoring rules for predicting the output
attribute  the times  in seconds  are the average evaluation time per
rule 

  

factor
   
   
   
   

ficached sufficient statistics for efficient machine learning

   alternative data structures
    why not use a kd tree 
kd trees can be used for accelerating learning algorithms  omohundro        moore et al  
       the primary difference is that a kd tree node splits on only one attribute instead
of all attributes  this results in much less memory  linear in the number of records   but
counting can be expensive  suppose  for example  that level one of the tree splits on a    level
two splits on a   etc  then  in the case of binary variables  if we have a query involving only
attributes a   and higher  we have to explore all paths in the tree down to level     with
datasets of fewer than     records this may be no cheaper than performing a linear search
through the records  another possibility  r trees  guttman        roussopoulos   leifker 
       store databases of m  dimensional geometric objects  however  in this context  they
offer no advantages over kd trees 

    why not use a frequent set finder 
frequent set finders  agrawal et al         are typically used with very large databases
of millions of records containing very sparse binary attributes  ecient algorithms exist
for finding all subsets of attributes that co occur with value true in more than a fixed
number  chosen by the user  and called the support  of records  recent research mannila  
toivonen        suggests that such frequent sets can be used to perform ecient counting 
in the case where support      all such frequent sets are gathered and  if counts of each
frequent set are retained  this is equivalent to producing an adtree in which instead of
performing a node cutoff for the most common value  the cutoff always occurs for value
false 
the use of frequent sets in this way would thus be very similar to the use of adtrees 
with one advantage and one disadvantage  the advantage is that ecient algorithms have
been developed for building frequent sets from a small number of sequential passes through
data  the adtree requires random access to the dataset while it is being built  and for its
leaf lists  this is impractical if the dataset is too large to reside in main memory and is
accessed through database queries 
the disadvantage of frequent sets in comparison with adtrees is that  under some
circumstances  the former may require much more memory  assume the value   is rarer
than   throughout all attributes in the dataset and assume reasonably that we thus choose
to find all frequent sets of  s  unnecessarily many sets will be produced if there are
correlations  in the extreme case  imagine a dataset in which     of the values are       
are   and attributes are perfectly correlated all values in each record are identical  then 
with m attributes there would be  m frequent sets of  s  in contrast  the adtree would
only contain m     nodes  this is an extreme example  but datasets with much weaker
inter attribute correlations can similarly benefit from using an adtree 
leaf lists are another technique to reduce the size of adtrees further  they could also
be used for the frequent set representation 
  

fimoore   lee

    why not use hash tables 

if we knew that only a small set of contingency tables would ever be requested  instead of
all possible contingency tables  then an adtree would be unnecessary  it would be better to
remember this small set of contingency tables explicitly  then  some kind of tree structure
could be used to index the contingency tables  but a hash table would be equally time
ecient and require less space  a hash table coding of individual counts in the contingency
tables would similarly allow us to use space proportional only to the number of non zero
entries in the stored tables  but for representing sucient statistics to permit fast solution
to any contingency table request  the adtree structure remains more memory ecient than
the hash table approach  or any method that stores all non zero counts  because of the
memory reductions when we exploit the ignoring of most common values 

   discussion

    what about numeric attributes 

the adtree representation is designed entirely for symbolic attributes  when faced with
numeric attributes  the simplest solution is to discretize them into a fixed finite set of values
which are then treated as symbols  but this is of little help if the user requests counts for
queries involving inequalities on numeric attributes  in future work we will evaluate the use
of structures combining elements from multiresolution kd trees of real attributes  moore
et al         with adtrees 

    algorithm specific counting tricks

many algorithms that count using the conventional  linear  method have algorithm specific
ways of accelerating their performance  for example  a bayes net structure finder may try
to remember all the contingency tables it has tried previously in case it needs to re evaluate
them  when it deletes a link  it can deduce the new contingency table from the old one
without needing a linear count 
in such cases  the most appropriate use of the adtree may be as a lazy caching mechanism  at birth  the adtree consists only of the root node  whenever the structure finder
needs a contingency table that cannot be deduced from the current adtree structure  the
appropriate nodes of the adtree are expanded  the adtree then takes on the role of the
algorithm specific caching methods  while  in general  using up much less memory than if
all contingency tables were remembered 

    hard to update incrementally

although the tree can be built cheaply  see the experimental results in section     and
although it can be built lazily  the adtree cannot be updated cheaply with a new record 
this is because one new record may match up to  m nodes in the tree in the worst case 

    scaling up

the adtree representation can be useful for datasets of the rough size and shape used in this
paper  on the first datasets we have looked at the ones described in this paper we have
  

ficached sufficient statistics for efficient machine learning

shown empirically that the sizes of the adtrees are tractable given real noisy data  this
included one dataset with    attributes  it is the extent to which the attributes are skewed
in their values and correlated with each other that enables the adtree to avoid approaching
its worse case bounds  the main technical contribution of this paper is the trick that allows
us to prune off most common values  without it  skewedness and correlation would hardly
help at all    the empirical contribution of this paper has been to show that the actual
sizes of the adtrees produced from real data are vastly smaller than the sizes we would get
from the worst case bounds in appendix a 
but despite these savings  adtrees cannot yet represent all the sucient statistics for
huge datasets with many hundreds of non sparse and poorly correlated attributes  what
should we do if our dataset or our adtree cannot fit into main memory  in the latter case 
we could simply increase the size of leaf lists  trading off decreased memory against increased
time to build contingency tables  but if that is inadequate at least three possibilities remain 
first  we could build approximate adtrees that do not store any information for nodes
that match fewer than a threshold number of records  then approximate contingency
tables  complete with error bounds  can be produced  mannila   toivonen         a
second possibility is to exploit secondary storage and store deep  rarely visited nodes of the
adtree on disk  this would doubtless best be achieved by integrating the machine learning
algorithms with current database management tools a topic of considerable interest in the
data mining community  fayyad et al          a third possibility  which restricts the size
of contingency tables we may ask for  is to refuse to store counts for queries with more than
some threshold number of attributes 

    what about the cost of building the tree 
in practice  adtrees could be used in two ways 

 one off  when a traditional algorithm is required we build the adtree  run the fast
version of the algorithm  discard the adtree  and return the results 

 amortized  when a new dataset becomes available  a new adtree is built for it 

the tree is then shipped and re used by anyone who wishes to do real time counting
queries  multivariate graphs and charts  or any machine learning algorithms on any
subset of the attributes  the cost of the initial tree building is then amortized over
all the times it is used  in database terminology  the process is known as materializing  harinarayan et al         and has been suggested as desirable for datamining by
several researchers  john   lent        mannila   toivonen        

the one off option is only useful if the cost of building the adtree plus the cost of running
the adtree based algorithm is less than the cost of the original counting based algorithm 
for the intensive machine learning methods studied here  this condition is safely satisfied 
but what if we decided to use a less intensive  greedier bayes net structure finder  table  
   without pruning  on all of our datasets we ran out of memory on a     megabyte machine before we
had built even    of the tree  and it is easy to show that the birth dataset would have needed to store
more than      nodes 

  

fimoore   lee

dataset

speedup ignoring
speedup allowing for speedup allowing for
build time        
build time        
build time     
iterations
iterations
iterations
census 
      
      
     
census 
     
     
    
adult 
     
     
    
adult 
     
     
    
adult 
     
     
    
birth
      
     
    
syn  k
      
      
     
syn  k
      
      
     
syn   k
       
      
     
syn   k
       
      
     
syn   k
       
      
     

table    computational economics of building adtrees and using them
to search for bayes net structures using the experiments of section   
shows that if we only run for     iterations instead of         and if we account for a one off
adtree building cost  then the relative speedup of using adtrees declines greatly 
to conclude  if the data analysis is intense then there is benefit to using adtrees even
if they are used in a one off fashion  if the adtree is used for multiple purposes then its
build time is amortized and the resulting relative eciency gains over traditional counting
are the same for both exhaustive searches and non exhaustive searches  algorithms that
use non exhaustive searches include hill climbing bayes net learners  greedy rule learners
such as cn   clark   niblett        and decision tree learners  quinlan        breiman
et al         

acknowledgements
this work was sponsored by a national science foundation career award to andrew moore 
the authors thank justin boyan  scott davies  nir friedman  and jeff schneider for their
suggestions  and ron kohavi for providing the census datasets 

appendix a  memory costs

in this appendix we examine the size of the tree  for simplicity  we restrict attention to the
case of binary attributes 

the worst case number of nodes in an adtree
given a dataset with m attributes and r records  the worst case for the adtree will occur if
all  m possible records exist in the dataset  then  for every subset of attributes there exists
exactly one node in the adtree  for example consider the attribute set fai          ai n  g 
where i      i            i n   suppose there is a node in the tree corresponding to the
   unsurprisingly  the resulting bayes nets have a highly inferior structure 

  

ficached sufficient statistics for efficient machine learning

query fai      v        ai n    vn g for some values v        vn   from the definition of an adtree 
and remembering we are only considering the case of binary attributes  we can state 
 v  is the least common value of ai    
 v  is the least common value of ai    among those records that match  ai      v   
  
 



is the least common value of ai k    among those records that match  ai     
 
 k    vk   
so there is at most one such node  moreover  since our worst case assumption is that
all possible records exist in the database  we see that the adtree will indeed contain this
node  thus  the worst case number of nodes is the same as the number of possible subsets
of attributes   m  
vk

  

v           ai

the worst case number of nodes in an adtree with a reasonable number of rows
it is frequently the case that a dataset has   m   with fewer records  there is a much
r

lower worst case bound on the adtree size  a node at the kth level of the tree corresponds
to a query involving k attributes  counting the root node as level     such a node can match
at most r  k records because each of the node s ancestors up the tree has pruned off at
least half the records by choosing to expand only the least common value of the attribute
introduced by that ancestor  thus  there can be no tree nodes at level blog  rc     of the
tree  because such nodes would have to match fewer than r  blog rc       records  they
would thus match no records  making them null 
the nodes in an adtree must all exist at level blog  rc or higher  the number of nodes
at level k is at most      because every node at level k involves an attribute set of size
k and because  given binary attributes  for every attribute set there is at most one node
in the adtree  thus the total number of nodes in the tree  summing over the levels is less
than
 
blog
xrc m
bounded above by o m blog rc   blog  rc       
    
k
 

m
k

 

 

k  

the number of nodes if we assume skewed independent attribute values

imagine that all values of all attributes in the dataset are independent random binary
variables  taking value   with probability p and taking value   with probability     p  then
the further p is from      the smaller we can expect the adtree to be  this is because 
on average  the less common value of a vary node will match fraction min p      p  of its
parent s records  and  on average  the number of records matched at the kth level of the
tree will be r min p      p  k   thus  the maximum level in the tree at which we may
find a node matching one or more records is approximately b log  r     log  q  c  where
q   min p      p   and so the total number of nodes in the tree is approximately
 
b log r x
    log q c
m
bounded above by o m b log r     log q c   b log  r     log  q  c     
k
 

 

 

k  

 

    

  

fimoore   lee

since the exponent is reduced by a factor of log     q    skewedness among the attributes
thus brings enormous savings in memory 

the number of nodes if we assume correlated attribute values

the adtree benefits from correlations among attributes in much the same way that it
benefits from skewedness  for example  suppose that each record was generated by the
simple bayes net in figure     where the random variable b is hidden  not included in the
record   then for i    j   p  ai    aj      p     p   if adn is any node in the resulting adtree
then the number of records matching any other node two levels below adn in the tree will
be fraction  p     p  of the number of records matching adn  from this we can see that
the number of nodes in the tree is approximately
 
b log r x
    log q c
m
bounded above by o m b log r     log q c   b log  r     log  q  c     
k
 

 

 

 

k  

    
p
where q    p     p   correlation among the attributes can thus also bring enormous
savings in memory even if  as is the case in our example  the marginal distribution of
individual attributes is uniform 
p b       

b

p a i   b        p
a 

a 

   

am

p a i    b    p

figure     a bayes net that generates correlated boolean attributes
a    a        am  

the number of nodes for the dense adtree of section  

the dense adtrees do not cut off the tree for the most common value of a vary node  the
worst case adtree will occur if all  m possible records exist in the dataset  then the dense
adtree will require  m nodes because every possible query  with each attribute taking
values      or    will have a count in the tree  the number of nodes at the kth level of the
dense adtree can be  k     in the worst case 
m
k

the number of nodes when using leaf lists

leaf lists were described in section    if a tree is built using maximum leaf list size of rmin 
then any node in the adtree matching fewer than rmin records is a leaf node  this means
that formulae        and    can be re used  replacing r with r rmin  it is important to
remember  however  that the leaf nodes must now contain room for rmin numbers instead
of a single count 
  

ficached sufficient statistics for efficient machine learning

appendix b  building the adtree
we define the function makeadtree  i   recordnums  where recordnums is a subset of
f   
g   is the total number of records in the dataset  and where       this
a

 

         r

r

i

m

makes an adtree from the rows specified in recordnums in which all adnodes represent
queries in which only attributes ai and higher are used 

makeadtree  i   recordnums 
a

make a new adnode called adn 
adn count    j recordnums j 
for j    i  i              m
j th vary node of adn    makevarynode aj   recordnums  

makeadtree uses the function makevarynode  which we now define 
makevarynode  i   recordnums 
a

make a new vary node called vn 
for k                ni
let childnumsk    fg 
for each j   recordnums
let vij   value of attribute ai in record j
add j to the set childnumsv
let vn mcv    argmaxk j childnumsk j 
for k                ni
if j childnumsk j    or if k   mcv
set the ai   k subtree of vn to null 
else
set the ai   k subtree of vn to makeadtree ai     childnumsk  
ij

to build the entire tree  we must call makeadtree a   f        rg   assuming binary attributes  the cost of building a tree from r records and m attributes is bounded above
by
blog
xrc r m  
    
 k k
k  
 

references

agrawal  r   mannila  h   srikant  r   toivonen  h     verkamo  a  i          fast discovery of association rules  in fayyad  u  m   piatetsky shapiro  g   smyth  p    
uthurusamy  r   eds    advances in knowledge discovery and data mining  aaai
press 
  

fimoore   lee

breiman  l   friedman  j  h   olshen  r  a     stone  c  j          classification and
regression trees  wadsworth 
clark  p     niblett  r          the cn  induction algorithm  machine learning    
        
cover  t  m     thomas  j  a          elements of information theory  john wiley  
sons 
fayyad  u   mannila  h     piatetsky shapiro  g          data mining and knowledge
discovery  kluwer academic publishers  a new journal 
fayyad  u     uthurusamy  r          special issue on data mining  communications of
the acm          
friedman  n     yakhini  z          on the sample complexity of learning bayesian networks  in proceedings of the   th conference on uncertainty in artificial intelligence 
morgan kaufmann 
guttman  a          r trees  a dynamic index structure for spatial searching  in proceedings of the third acm sigact sigmod symposium on principles of database
systems  assn for computing machinery 
harinarayan  v   rajaraman  a     ullman  j  d          implementing data cubes eciently  in proceedings of the fifteenth acm sigact sigmod sigart symposium
on principles of database systems   pods       pp           assn for computing
machinery 
john  g  h   kohavi  r     peger  k          irrelevant features and the subset selection
problem  in cohen  w  w     hirsh  h   eds    machine learning  proceedings of
the eleventh international conference  morgan kaufmann 
john  g  h     lent  b          sipping from the data firehose  in proceedings of the third
international conference on knowledge discovery and data mining  aaai press 
kohavi  r          the power of decision tables  in lavrae  n     wrobel  s   eds   
machine learning   ecml       th european conference on machine learning 
heraclion  crete  greece  springer verlag 
kohavi  r          scaling up the accuracy of naive bayes classifiers  a decision tree hybrid  in e  simoudis and j  han and u  fayyad  ed    proceedings of the second
international conference on knowledge discovery and data mining  aaai press 
madala  h  r     ivakhnenko  a  g          inductive learning algorithms for complex
systems modeling  crc press inc   boca raton 
mannila  h     toivonen  h          multiple uses of frequent sets and condensed representations  in e  simoudis and j  han and u  fayyad  ed    proceedings of the second
international conference on knowledge discovery and data mining  aaai press 
  

ficached sufficient statistics for efficient machine learning

moore  a  w   schneider  j     deng  k          ecient locally weighted polynomial
regression predictions  in d  fisher  ed    proceedings of the      international
machine learning conference  morgan kaufmann 
omohundro  s  m          ecient algorithms with neural network behaviour  journal
of complex systems                 
quinlan  j  r          learning ecient classification procedures and their application to
chess end games  in michalski  r  s   carbonell  j  g     mitchell  t  m   eds    machine learning an artificial intelligence approach  i   tioga publishing company 
palo alto 
quinlan  j  r          learning logical definitions from relations  machine learning    
        
roussopoulos  n     leifker  d          direct spatial search on pictorial databases using
packed r trees  in navathe  s   ed    proceedings of acm sigmod      international conference on management of data  assn for computing machinery 
rymon  r          an se tree based characterization of the induction problem  in p 
utgoff  ed    proceedings of the   th international conference on machine learning 
morgan kaufmann 

  

fi