journal artificial intelligence research               

submitted       published     

cached sucient statistics ecient machine learning
large datasets
andrew moore
mary soon lee

school computer science robotics institute
carnegie mellon university  pittsburgh pa      

awm cs cmu edu
mslee cs cmu edu

abstract

paper introduces new algorithms data structures quick counting machine
learning datasets  focus counting task constructing contingency tables 
approach applicable counting number records dataset match
conjunctive queries  subject certain assumptions  costs operations
shown independent number records dataset loglinear
number non zero entries contingency table 
provide sparse data structure  adtree  minimize memory use 
provide analytical worst case bounds structure several models data distribution  empirically demonstrate tractably sized data structures produced
large real world datasets  a  using sparse tree structure never allocates memory
counts zero   b  never allocating memory counts deduced
counts   c  bothering expand tree fully near leaves 
show adtree used accelerate bayes net structure finding algorithms  rule learning algorithms  feature selection algorithms  provide
number empirical results comparing adtree methods traditional direct counting
approaches  discuss possible uses adtrees machine learning methods  discuss merits adtrees comparison alternative representations
kd trees  r trees frequent sets 

   caching sucient statistics

computational eciency important concern machine learning algorithms  especially
applied large datasets  fayyad  mannila    piatetsky shapiro        fayyad  
uthurusamy        real time scenarios  earlier work showed kd trees
multiresolution cached regression matrix statistics enable fast locally weighted
instance based regression  moore  schneider    deng         paper  attempt
accelerate predictions symbolic attributes using kind kd tree splits
dimensions nodes 
many machine learning algorithms operating datasets symbolic attributes need
frequent counting  work applicable online analytical processing  olap 
applications data mining  operations large datasets multidimensional
database access  datacube operations  harinarayan  rajaraman    ullman        
association rule learning  agrawal  mannila  srikant  toivonen    verkamo        could
accelerated fast counting 
let us begin establishing notation  given data set r records
attributes  attributes called a   a           value attribute ai
c      ai access foundation morgan kaufmann publishers  rights reserved 

fimoore   lee

th record small integer lying range f            ni g ni called arity
attribute i  figure   gives example 
k

attributes

a 

a 

a 

n     

n     

n     

record  

 

 

 

m  

record  

 

 

 

r  

record  

 

 

 

record  

 

 

 

record  

 

 

 

record  

 

 

 

arity

figure    simple dataset used example  r     records
    attributes 

    queries

query set  attribute   value  pairs left hand sides pairs form
subset fa        g arranged increasing order index  four examples queries
dataset
 a         a       a             a       a       a      

   

notice total number possible queries
i    ni      
attribute either appear query one ni values may take  may
omitted  which equivalent giving ai      don t care  value  

    counts

count query  denoted c  query  simply number records dataset
matching  attribute   value  pairs query  example dataset find 
        
c  a       a      
c   
c  a       a       a      
c

    contingency tables

 
 
 
 

 
 
 
 

subset attributes  ai          ai n    associated contingency table denoted
ct ai          ai n    table row possible sets values
ai          ai n    row corresponding ai      v        ai n    vn records count c  ai     
 
v        ai n    vn    example dataset   attributes       contingency tables
exist  depicted figure   
  

ficached sufficient statistics efficient machine learning

ct  

ct a   

ct a   

 

 

 

 
 

 

 

 

   
   

   
   

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 

ct a   

ct a  a  a   

 

 
 

 
 
 
 

 
 
 
 

ct a  a   

ct a   a   
 
 

 

 
 
 
 

 
 
 
 

 
 
 
 

ct a  a   

 
 

 

 
 

 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

figure    eight possible contingency tables dataset figure   
conditional contingency table  written
ct ai          ai n  j aj      u         aj p    up 

   

contingency table subset records dataset match query
right j symbol  example 
a 

 
ct a    a  j a          
 
 

a 

 
 
 
 

 
 
 
 
 

contingency tables used variety machine learning applications  including
building probability tables bayes nets evaluating candidate conjunctive rules
rule learning algorithms  quinlan        clark   niblett         would thus desirable
able perform counting eciently 
prepared pay one time cost building caching data structure 
easy suggest mechanism counting constant time  possible query 
precompute contingency table  total amount numbers stored memory
data structure would
i    ni       even humble dataset figure  
    revealed figure    real dataset ten attributes medium
arity  fifteen binary attributes  far large fit main memory 
would retain speed precomputed contingency tables without incurring
intractable memory demand  subject paper 

   cache reduction    dense adtree caching sucient statistics

first describe adtree  data structure use represent set
possible counts  initial simplified description obvious tree representation
yield immediate memory savings   later provide several opportunities
   se tree  rymon        similar data structure 

  

fimoore   lee

cutting zero counts redundant counts  structure shown figure   
adtree node  shown rectangle  child nodes called  vary nodes   shown ovals  
adnode represents query stores number records match query  in
c     field   vary aj child adnode one child nj values
attribute aj   kth child represents query vary aj  s parent 
additional constraint aj   k 
  
 

 
 
 

   
c  

vary  
  

 n

  

  

  

  

  
 

  
 

 
 

  
 

 

   

 

   

vary  

 

  

 
 

 

   

 n
 

  
 

 

   

   

   

   

c  

c  

c  

c  

   

   

   
vary  

vary

vary  

vary

  
 

  
 

   
vary

figure    top adnodes adtree  described text 
notes regarding structure 

although drawn diagram  description query  e g  

     a   
     am     leftmost adnode second level  explicitly recorded
adnode  contents adnode simply count set pointers
vary aj children 
contents vary aj node set pointers adnodes 
 



cost looking count proportional number instantiated variables

query  example  look c  a       a        a        would follow
following path tree  vary a    a        vary a     a         vary a    
a        count obtained resulting node 

notice node adn vary
vary ai   vary

ai

  

ai

parent  adn s children

    vary  

necessary store vary nodes indices i   information
obtained another path tree 
  

ficached sufficient statistics efficient machine learning

    cutting nodes counts zero

described  tree sparse contain exactly
i    ni      nodes  sparseness
easily achieved storing null instead node query matches zero
records  specializations query count zero
appear anywhere tree  datasets reduce number
numbers need stored  example  dataset figure    previously
needed    numbers represent contingency tables  need    numbers 

   cache reduction ii  sparse adtree

easy devise datasets benefit failing store counts zero 
suppose binary attributes  m records kth record bits
binary representation k  query count zero tree contains  m
nodes  reduce tree size despite this  take advantage observation
many counts stored tree redundant 
vary aj node adtree stores nj subtrees one subtree value
aj   instead  find common values aj  call mcv  store
null place mcvth subtree  remaining nj     subtrees represented
before  example simple dataset given figure    vary aj node
records values common mcv field  appendix b describes
straightforward algorithm building adtree 
see section    still possible build full exact contingency tables  or
give counts specific queries  time slightly longer full adtree
section    first let us examine memory consequences representation 
appendix shows binary attributes  given attributes r records 
number nodes needed store tree bounded  m worst case  and
much less r    m    contrast  amount memory needed dense tree
section    m worst case 
notice figure   mcv value context dependent  depending constraints
parent nodes  a  s mcv sometimes   sometimes    context dependency
provide dramatic savings  as frequently case  correlations among
attributes  discussed appendix a 

   computing contingency tables sparse adtree

given adtree  wish able quickly construct contingency tables arbitrary set attributes fai          ai n  g 
notice conditional contingency table ct ai          ai n  j query  built recursively  first build

ct 
ct 

          ai n  j ai         query 
ai          ai n  j ai         query 

ai

ct 

ai

          ai n 

j

  
 

      ni     query 

ai

  

fimoore   lee

  
 
  
 

c   
vary  
mcv    

vary  
mcv    

  

  

null

  

  

 mcv 

c   

c   

 
 

vary  
mcv    

 
 

  
 
  

null
 mcv 

 

c   



vary  
mcv    

null

null

null

  

count  

 mcv 

 mcv 

  

 

 
 
 
 
 
 
 
 

 
 

c   



 

 
 
 
 
 
 
 
 

figure    sparse adtree built dataset shown bottom
right  common value a     a      subtree
vary a  child root node null  vary a 
nodes common child set null  which child
common depends context  
example  build ct a   a   using dataset figure    build ct a  j a      
ct a  j a       combine figure   
ct a          

ct a    a   

a   
 
 

 
 

ct a          
a   
 
 

a 

a   

 
 

 
 

 
 

 
 

 
 

 
 

 
 

figure    example  using numbers figure    contingency
tables combined recursively form larger contingency tables 
building conditional contingency table adtree  need
explicitly specify query condition  instead  supply adnode adtree 
implicitly equivalent information  algorithm is 

  

ficached sufficient statistics efficient machine learning

makecontab  f

ai          ai n  g   adn 
let vn    vary ai    subnode adn 
let mcv    vn mcv 
k                  ni   
k    mcv
let adnk    ai      k subnode vn 
ctk    makecontab fai          ai n  g  adnk   

ctmcv

  

 calculated explained below 

return concatenation ct        ctn  
i   

base case recursion occurs first argument empty  case
return one element contingency table containing count associated current
adnode  adn 
omission algorithm  iteration k   f            ni   g
unable compute conditional contingency table ctmcv ai      mcv
subtree deliberately missing per section    instead 
take advantage following property contingency tables 

ct 

ai

          ai n  j query   

x

ni   

ct 

          ai n 

ai

k  

j

      k  query 

ai

   

value ct ai          ai n  j query  computed within algorithm calling

makecontab f

          ai n  g  adn 

   

ai

missing conditional contingency table algorithm computed
following row wise subtraction 
x
ctmcv    makecontab fai          ai n  g  adn   
ctk
   
k  m cv

frequent sets  agrawal et al          traditionally used learning association
rules  used computing counts  recent paper  mannila   toivonen        
employs similar subtraction trick  calculates counts frequent sets 
section   discuss strengths weaknesses frequent sets comparison
adtrees 

    complexity building contingency table

cost computing contingency table  let us consider theoretical worstcase cost computing contingency table n attributes arity k note
cost unrealistically pessimistic  except k       contingency tables
sparse  discussed later  assumption attributes arity  k 
made simplify calculation worst case cost  needed code 
  

fimoore   lee

contingency table n attributes kn entries  write c  n    cost computing
contingency table  top level call makecontab k calls build
contingency tables n     attributes  k     calls build ct ai          ai n  j
ai      j  query  every j f            k g except mcv  final call build
ct ai          ai n  j query   k     subtractions contingency tables 
require kn   numeric subtractions 
c    
   
   
n  
c  n 
  kc  n         k     k
n    
   
solution recurrence relation c  n         n k      kn     cost loglinear
size contingency table  comparison  used cached data structure 
simply counted dataset order build contingency table would
need o nr   kn   operations r number records dataset  thus
cheaper standard counting method kn r  interested large datasets
r may           case method present several
order magnitude speedup for  say  contingency table eight binary attributes  notice
cost independent m  total number attributes dataset 
depends upon  almost always much smaller  number attributes n requested
contingency table 

    sparse representation contingency tables

practice  represent contingency tables multidimensional arrays  rather
tree structures  gives slow counting approach adtree approach
substantial computational advantage cases contingency table sparse  i e 
many zero entries  figure   shows sparse contingency table representation 
mean average case behavior much faster worst case contingency tables
large numbers attributes high arity attributes 
indeed  experiments section   show costs rising much slowly o nkn    
n increases  note using sparse representation  worst case
makecontab o min nr  nkn     r maximum possible number
non zero contingency table entries 

   cache reduction iii  leaf lists

introduce scheme reducing memory use  worth building
adtree data structure small number records  example  suppose
   records    binary attributes  analysis appendix shows us
worst case adtree might require       nodes  computing contingency tables
using resulting adtree would  records  faster conventional
counting approach  would merely require us retain dataset memory 
aside concluding adtrees useful small datasets 
leads final method saving memory large adtrees  adtree node fewer
rmin records expand subtree  instead maintains list pointers
original dataset  explicitly listing records match current adnode 
list pointers called leaf list  figure   gives example 
  

ficached sufficient statistics efficient machine learning

v  

ct a  a   a   

v  

 

 
 

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

v  
v  

 

v  

v  

a 

v  

v  





v  

null

null

null

v  



 

v  

 

null

 

v  

null

null

 

v  

v  

v  



 



v  
v  



 
null
null

 

v  

 

figure    right hand figure sparse representation contingency table left 
use leaf lists one minor two major consequences  minor consequence need include straightforward change contingency table generating
algorithm handle leaf list nodes  minor alteration described here  first
major consequence dataset must retained main memory
algorithms inspect leaf lists access rows data pointed leaf lists 
second major consequence adtree may require much less memory 
documented section   worst case bounds provided appendix a 

   using adtrees machine learning
see section    adtree structure substantially speed computation
contingency tables large real datasets  machine learning statistical
algorithms take advantage this  provide three examples  feature selection 
bayes net scoring rule learning  seems likely many algorithms
benefit  example stepwise logistic regression  gmdh  madala   ivakhnenko        
text classification  even decision tree  quinlan        breiman  friedman  olshen   
stone        learning may benefit    future work examine ways speed
nearest neighbor memory based queries using adtrees 
   depends whether cost initially building adtree amortized many runs
decision tree algorithm  repeated runs decision tree building occur one using wrapper
model feature selection  john  kohavi    p eger         one using intensive search
tree structures traditional greedy search  quinlan        breiman et al        

  

fimoore   lee

a     
a     
a     
c   
row      

a     
a     
a     
c   
see rows      

vary  

vary  

vary  

 

 

 

 

mcv    

mcv    

mcv    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

vary  

 

 

 

 

mcv    

 

 

 

 

null
 mcv 

a     
a     
a     

null
 mcv 

a     
a     
a     

c   

null
 mcv 

a     
a     
a     

c   

c   

see rows    

null
 mcv 

a     
a     
a     
c   
see rows    

figure    adtree built using leaf lists rmin      node
matching   fewer records expanded  simply records set
pointers dataset  shown right  

    datasets

experiments used datasets table    dataset supplied us
continuous attributes already discretized ranges 

    using adtrees feature selection

given attributes  one output wish predict  often interesting
ask  which subset n attributes   n      best predictor output
distribution datapoints ected dataset    kohavi        
many ways scoring set features  particularly simple one information
gain  cover   thomas        
let aout attribute wish predict let ai          ai n  set attributes
used inputs  let x set possible assignments values ai          ai n  write
assignk   x kth assignment 
x
jx j c  assign   nx c  a   v  assign  
nx
c  aout   v  

k
k
infogain   f
 
f
   
r
r
c  assignk  
v  
v  
k  




r number records entire dataset
       x log  x
   
counts needed computation read directly ct aout   ai          ai n    
searching best subset attributes simply question search among
attribute sets size n  n specified user   simple example designed test
f x

  

ficached sufficient statistics efficient machine learning

name

  num    num 
records attributes
adult 
         
small  adult income  dataset placed uci
repository ron kohavi  kohavi         contains
census data related job  wealth  nationality  attribute arities range       uci repository called test set  rows missing
values removed 
adult 
         
kinds records different
data  training set 
adult 
         
adult  adult  concatenated 
census 
          
larger dataset based different census  provided ron kohavi 
census 
          
data census   addition
two extra  high arity attributes 
birth
        
records concerning wide number readings
factors recorded various stages pregnancy  attributes binary     attributes sparse      values
false 
synth
  k    k   
synthetic datasets entirely binary attributes generated using bayes net figure   
r

table    datasets used experiments 

counting methods  practical feature selector would need penalize number
rows contingency table  else high arity attributes would tend win  

    using adtrees bayes net structure discovery

many possible bayes net learning tasks  entail counting  hence
might speeded adtrees  paper present experimental results
particular example scoring structure bayes net decide well matches
data 
use maximum likelihood scoring penalty number parameters 
first compute probability table associated node  write parents j 
parent attributes node j write xj set possible assignments values
parents j   maximum likelihood estimate
    v j xj  

p aj

estimated

    

    v  xj  
    
c  xj  
estimates node j  s probability tables read ct aj   parents j   
next step scoring structure decide likelihood data given
probability tables computed penalize number parameters network
 without penalty likelihood would increase every time link added
c aj

  

fimoore   lee

figure    bayes net generated synth datasets 
three kinds nodes  nodes marked triangles generated
p  ai             p  ai             square nodes deterministic 
square node takes value   sum four parents even  else
takes value    circle nodes probabilistic functions single
parent  defined p  ai     j parent          p  ai     j parent  
          provides dataset fairly sparse values many
interdependencies 
network   penalized log likelihood score  friedman   yakhini       

 

n

params log r      r

n

x
x x
j

j   

asgn

 xj v  

    v   asgn  log p  aj   v j asgn 

p aj

    

nparams total number probability table entries network 
search among structures find best score  experiments use randomrestart stochastic hill climbing operations random addition removal
network link randomly swapping pair nodes  latter operation necessary
allow search algorithm choose best ordering nodes bayes net  stochastic
searches popular method finding bayes net structures  friedman  
yakhini         probability tables affected nodes recomputed
step 
figure   shows bayes net structure returned bayes net structure finder
       iterations hill climbing 

    using adtrees rule finding
given output attribute aout distinguished value
conjunctive queries form

out 

v

assign    ai      v        ai n    vn  
  

rule finders search among
    

ficached sufficient statistics efficient machine learning

a 
 
t 
t 
r 
i 
b 
u 
t 
e
relationship
class
sex
capital gain
hours per week
marital status
education num
capital loss
age
race
education
workclass
native country
fnlwgt
occupation

 
s 
c 
o 
r 
e
 
n 
p
       
 
        
  
        
  
            
        
  
        
  
      
  
       
  
        
  
        
  
       
  
       
   
        
  
            
       
   

pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 no parents 
relationship
relationship class
class
relationship class sex
relationship sex
class
class
marital status
relationship education num
relationship education num
relationship hours per week education num
education num race
 no parents 
class sex education workclass

score       
search took     seconds 

figure    output bayes structure finder running
adult  dataset  score contribution sum equation    due
specified attribute  np number entries probability
table specified attribute 
find query maximizes estimated value
 

p aout

vout   assign 
  vout j assign    c  aoutc  assign
 

    

avoid rules without significant support  insist c  assign   the number
records matching query  must threshold smin 
experiments implement brute force search looks possible
queries involve user specified number attributes  n  build ct aout   ai          ai n   
turn  there choose n tables   look rows table
queries using ai          ai n  greater minimum support smin 
return priority queue highest scoring rules  instance adult  dataset 
best rule predicting  class    attributes was 
score                    workclass   private  education num   above    maritalstatus   married civ spouse  capital loss   above       class   k

   experimental results
let us first examine memory required adtree datasets  table   shows us 
example  adult  dataset produced adtree        nodes  tree
required almost    megabytes memory  among three adult datasets  size
tree varied approximately linearly number records 
unless otherwise specified  experiments section  adult datasets
used leaf lists  birth synthetic datasets used leaf lists size rmin     
default  birth dataset  large number sparse attributes  required modest
  megabytes store tree many magnitudes worst case bounds  among
synthetic datasets  tree size increased sublinearly dataset size  indicates
  

fimoore   lee

dataset
census 
census 
adult 
adult 
adult 
birth
syn  k
syn  k
syn   k
syn   k
syn   k


  
  
  
  
  
  
  
  
  
  
  

r
      
      
     
     
     
    
     
     
      
      
      

nodes megabytes build time
     
   
  
      
    
  
     
   
 
     
    
  
      
    
  
     
   
  
     
   
 
     
   
  
     
   
  
      
    
  
      
    
   

table    size adtrees various datasets  number
attributes  r number records  nodes number nodes
adtree  megabytes amount memory needed store
tree  build time number seconds needed build tree  to
nearest second  

dataset gets larger  novel records  which may cause new nodes appear
tree  become less frequent 
table   shows costs performing        iterations bayes net structure searching 
experiments performed    mhz pentium pro machine     megabytes
main memory  recall bayes net iteration involves one random change
network requires recomputation one contingency table  the exception first
iteration  nodes must computed   means time run       
iterations essentially time compute        contingency tables  among adult
datasets  advantage adtree conventional counting ranges factor
       unsurprisingly  computational costs adult increase sublinearly
dataset size adtree linearly conventional counting  computational
advantages sublinear behavior much pronounced synthetic data 
next  table   examines effect leaf lists adult  birth datasets 
adult dataset  byte size tree decreases factor   leaf lists
increased       computational cost running bayes search increases
     indicating worth while tradeoff memory scarce 
bayes net scoring results involved average cost computing contingency tables
many different sizes  following results tables     make savings fixed
size attribute sets easier discern  tables give results feature selection
rule finding algorithms  respectively  biggest savings come small attribute sets 
computational savings sets size one two are  however  particularly interesting
since counts could cached straightforward methods without needing tricks 
cases  however  see large savings  especially birth data  datasets
larger numbers rows would  course  reveal larger savings 
  

ficached sufficient statistics efficient machine learning

dataset
census 
census 
adult 
adult 
adult 
birth
syn  k
syn  k
syn   k
syn   k
syn   k


  
  
  
  
  
  
  
  
  
  
  

r adtree time regular time speedup factor
      
   
     
     
      
   
     
    
     
   
    
  
     
   
    
    
     
   
    
    
    
  
    
     
     
  
     
     
     
  
     
     
      
  
     
      
      
  
     
      
      
  
      
      

table    time  in seconds  perform        hill climbing iterations
searching best bayes net structure  adtree time time
using adtree regular time time taken using
conventional probability table scoring method counting
dataset  speedup factor number times adtree
method faster conventional method  adtree times
include time building adtree first place  given
table     typical use adtrees build tree
able use many data analysis operations  building
cost amortized  case  even including tree building cost
would minor impact results 

min

r

 
 
 
 
  
  
  
   
   
   
    
    
    

adult 
birth
 mb  nodes build search  mb  nodes build search
secs
secs
secs
secs
            
  
   
           
  
   
           
 
                 
  
  
           
 
                 
  
  
           
 
               
  
  
           
 
               
  
  
           
 
               
 
  
           
 
               
 
  
           
 
        
     
 
  
    
     
 
        
     
 
   
    
     
 
        
   
 
   
    
     
 
         
   
 
   
    
   
 
         
   
 
   

table    investigating effect rmin parameter adult 
dataset birth dataset   mb memory used adtree 
 nodes number nodes adtree  build secs time
build adtree  search secs time needed perform       
iterations bayes net structure search 
  

fimoore   lee

number
number
attributes attribute
sets
 
  
 
  
 
   
 
     
 
     

adult 

adtree regular speedup

time

time

factor

       
      
     
     
    

    
    
    
   
   

     
     
    
    
   

number
attribute
sets
  
     
       
         
          

birth

adtree regular speedup
time

time

            
            
            
      
      

factor
   
   
   

table    time taken search among attribute sets given size
 number attributes  set gives best information gain
predicting output attribute  times  seconds  average
evaluation times per attribute set 

number number
attributes
rules
 
   
 
     
 
      
 
       
           

adult 

adtree regular speedup

time
       
       
       
       
       

time
     
     
      
      
      

factor
     
    
    
   
   

number
rules
   
      
       
          
             

birth

adtree regular speedup

time
       
       
       
       
       

time
     
     
     
     

table    time taken search among rules given size  number attributes  highest scoring rules predicting output
attribute  times  seconds  average evaluation time per
rule 

  

factor
   
   
   
   

ficached sufficient statistics efficient machine learning

   alternative data structures
    use kd tree 
kd trees used accelerating learning algorithms  omohundro        moore et al  
       primary difference kd tree node splits one attribute instead
attributes  results much less memory  linear number records  
counting expensive  suppose  example  level one tree splits a    level
two splits a   etc  then  case binary variables  query involving
attributes a   higher  explore paths tree level    
datasets fewer     records may cheaper performing linear search
records  another possibility  r trees  guttman        roussopoulos   leifker 
       store databases  dimensional geometric objects  however  context 
offer advantages kd trees 

    use frequent set finder 
frequent set finders  agrawal et al         typically used large databases
millions records containing sparse binary attributes  ecient algorithms exist
finding subsets attributes co occur value true fixed
number  chosen user  called support  records  recent research mannila  
toivonen        suggests frequent sets used perform ecient counting 
case support      frequent sets gathered and  counts
frequent set retained  equivalent producing adtree instead
performing node cutoff common value  cutoff always occurs value
false 
use frequent sets way would thus similar use adtrees 
one advantage one disadvantage  advantage ecient algorithms
developed building frequent sets small number sequential passes
data  adtree requires random access dataset built 
leaf lists  impractical dataset large reside main memory
accessed database queries 
disadvantage frequent sets comparison adtrees that 
circumstances  former may require much memory  assume value   rarer
  throughout attributes dataset assume reasonably thus choose
find frequent sets  s  unnecessarily many sets produced
correlations  extreme case  imagine dataset     values       
  attributes perfectly correlated all values record identical  then 
attributes would  m frequent sets  s  contrast  adtree would
contain     nodes  extreme example  datasets much weaker
inter attribute correlations similarly benefit using adtree 
leaf lists another technique reduce size adtrees further  could
used frequent set representation 
  

fimoore   lee

    use hash tables 

knew small set contingency tables would ever requested  instead
possible contingency tables  adtree would unnecessary  would better
remember small set contingency tables explicitly  then  kind tree structure
could used index contingency tables  hash table would equally time
ecient require less space  hash table coding individual counts contingency
tables would similarly allow us use space proportional number non zero
entries stored tables  representing sucient statistics permit fast solution
contingency table request  adtree structure remains memory ecient
hash table approach  or method stores non zero counts 
memory reductions exploit ignoring common values 

   discussion

    numeric attributes 

adtree representation designed entirely symbolic attributes  faced
numeric attributes  simplest solution discretize fixed finite set values
treated symbols  little help user requests counts
queries involving inequalities numeric attributes  future work evaluate use
structures combining elements multiresolution kd trees real attributes  moore
et al         adtrees 

    algorithm specific counting tricks

many algorithms count using conventional  linear  method algorithm specific
ways accelerating performance  example  bayes net structure finder may try
remember contingency tables tried previously case needs re evaluate
them  deletes link  deduce new contingency table old one
without needing linear count 
cases  appropriate use adtree may lazy caching mechanism  birth  adtree consists root node  whenever structure finder
needs contingency table cannot deduced current adtree structure 
appropriate nodes adtree expanded  adtree takes role
algorithm specific caching methods   in general  using much less memory
contingency tables remembered 

    hard update incrementally

although tree built cheaply  see experimental results section    
although built lazily  adtree cannot updated cheaply new record 
one new record may match  m nodes tree worst case 

    scaling

adtree representation useful datasets rough size shape used
paper  first datasets looked at the ones described paper we
  

ficached sufficient statistics efficient machine learning

shown empirically sizes adtrees tractable given real noisy data 
included one dataset    attributes  extent attributes skewed
values correlated enables adtree avoid approaching
worse case bounds  main technical contribution paper trick allows
us prune most common values  without it  skewedness correlation would hardly
help all    empirical contribution paper show actual
sizes adtrees produced real data vastly smaller sizes would get
worst case bounds appendix a 
despite savings  adtrees cannot yet represent sucient statistics
huge datasets many hundreds non sparse poorly correlated attributes 
dataset adtree cannot fit main memory  latter case 
could simply increase size leaf lists  trading decreased memory increased
time build contingency tables  inadequate least three possibilities remain 
first  could build approximate adtrees store information nodes
match fewer threshold number records  approximate contingency
tables  complete error bounds  produced  mannila   toivonen        
second possibility exploit secondary storage store deep  rarely visited nodes
adtree disk  would doubtless best achieved integrating machine learning
algorithms current database management tools a topic considerable interest
data mining community  fayyad et al          third possibility  restricts size
contingency tables may ask for  refuse store counts queries
threshold number attributes 

    cost building tree 
practice  adtrees could used two ways 

one off  traditional algorithm required build adtree  run fast
version algorithm  discard adtree  return results 

amortized  new dataset becomes available  new adtree built it 

tree shipped re used anyone wishes real time counting
queries  multivariate graphs charts  machine learning algorithms
subset attributes  cost initial tree building amortized
times used  database terminology  process known materializing  harinarayan et al         suggested desirable datamining
several researchers  john   lent        mannila   toivonen        

one off option useful cost building adtree plus cost running
adtree based algorithm less cost original counting based algorithm 
intensive machine learning methods studied here  condition safely satisfied 
decided use less intensive  greedier bayes net structure finder  table  
   without pruning  datasets ran memory     megabyte machine
built even    tree  easy show birth dataset would needed store
     nodes 

  

fimoore   lee

dataset

speedup ignoring
speedup allowing speedup allowing
build time        
build time        
build time     
iterations
iterations
iterations
census 
      
      
     
census 
     
     
    
adult 
     
     
    
adult 
     
     
    
adult 
     
     
    
birth
      
     
    
syn  k
      
      
     
syn  k
      
      
     
syn   k
       
      
     
syn   k
       
      
     
syn   k
       
      
     

table    computational economics building adtrees using
search bayes net structures using experiments section   
shows run     iterations instead         account one off
adtree building cost  relative speedup using adtrees declines greatly 
conclude  data analysis intense benefit using adtrees even
used one off fashion  adtree used multiple purposes
build time amortized resulting relative eciency gains traditional counting
exhaustive searches non exhaustive searches  algorithms
use non exhaustive searches include hill climbing bayes net learners  greedy rule learners
cn   clark   niblett        decision tree learners  quinlan        breiman
et al         

acknowledgements
work sponsored national science foundation career award andrew moore 
authors thank justin boyan  scott davies  nir friedman  jeff schneider
suggestions  ron kohavi providing census datasets 

appendix a  memory costs

appendix examine size tree  simplicity  restrict attention
case binary attributes 

worst case number nodes adtree
given dataset attributes r records  worst case adtree occur
 m possible records exist dataset  then  every subset attributes exists
exactly one node adtree  example consider attribute set fai          ai n  g 
i      i            i n   suppose node tree corresponding
   unsurprisingly  resulting bayes nets highly inferior structure 

  

ficached sufficient statistics efficient machine learning

query fai      v        ai n    vn g values v        vn   definition adtree 
remembering considering case binary attributes  state 
v  least common value ai    
v  least common value ai    among records match  ai      v   
  
 



least common value ai k    among records match  ai     
 
 k    vk   
one node  moreover  since worst case assumption
possible records exist database  see adtree indeed contain
node  thus  worst case number nodes number possible subsets
attributes   m  
vk

  

v           ai

worst case number nodes adtree reasonable number rows
frequently case dataset  m   fewer records  much
r

lower worst case bound adtree size  node kth level tree corresponds
query involving k attributes  counting root node level     node match
r  k records node s ancestors tree pruned
least half records choosing expand least common value attribute
introduced ancestor  thus  tree nodes level blog  rc    
tree  nodes would match fewer r  blog rc       records 
would thus match records  making null 
nodes adtree must exist level blog  rc higher  number nodes
level k      every node level k involves attribute set size
k  given binary attributes  every attribute set one node
adtree  thus total number nodes tree  summing levels less

 
blog
xrc
bounded o m blog rc   blog  rc       
    
k
 


k

 

 

k  

number nodes assume skewed independent attribute values

imagine values attributes dataset independent random binary
variables  taking value   probability p taking value   probability     p 
p      smaller expect adtree be  because 
average  less common value vary node match fraction min p      p 
parent s records  and  average  number records matched kth level
tree r min p      p  k   thus  maximum level tree may
find node matching one records approximately b log  r     log  q  c 
q   min p      p   total number nodes tree approximately
 
b log r x
    log q c

bounded o m b log r     log q c   b log  r     log  q  c     
k
 

 

 

k  

 

    

  

fimoore   lee

since exponent reduced factor log     q    skewedness among attributes
thus brings enormous savings memory 

number nodes assume correlated attribute values

adtree benefits correlations among attributes much way
benefits skewedness  example  suppose record generated
simple bayes net figure     random variable b hidden  not included
record      j   p  ai    aj      p     p   adn node resulting adtree
number records matching node two levels adn tree
fraction  p     p  number records matching adn  see
number nodes tree approximately
 
b log r x
    log q c

bounded o m b log r     log q c   b log  r     log  q  c     
k
 

 

 

 

k  

    
p
q    p     p   correlation among attributes thus bring enormous
savings memory even  as case example  marginal distribution
individual attributes uniform 
p b       

b

p a   b        p
a 

a 

   



p a    b    p

figure     bayes net generates correlated boolean attributes
a    a         

number nodes dense adtree section  

dense adtrees cut tree common value vary node 
worst case adtree occur  m possible records exist dataset  dense
adtree require  m nodes every possible query  with attribute taking
values         count tree  number nodes kth level
dense adtree  k     worst case 

k

number nodes using leaf lists

leaf lists described section    tree built using maximum leaf list size rmin 
node adtree matching fewer rmin records leaf node  means
formulae           re used  replacing r r rmin  important
remember  however  leaf nodes must contain room rmin numbers instead
single count 
  

ficached sufficient statistics efficient machine learning

appendix b  building adtree
define function makeadtree    recordnums  recordnums subset
f   
g   total number records dataset     


 

         r

r





makes adtree rows specified recordnums adnodes represent
queries attributes ai higher used 

makeadtree    recordnums 


make new adnode called adn 
adn count    j recordnums j 
j    i              
j th vary node adn    makevarynode aj   recordnums  

makeadtree uses function makevarynode  define 
makevarynode    recordnums 


make new vary node called vn 
k                ni
let childnumsk    fg 
j   recordnums
let vij   value attribute ai record j
add j set childnumsv
let vn mcv    argmaxk j childnumsk j 
k                ni
j childnumsk j    k   mcv
set ai   k subtree vn null 
else
set ai   k subtree vn makeadtree ai     childnumsk  
ij

build entire tree  must call makeadtree a   f        rg   assuming binary attributes  cost building tree r records attributes bounded

blog
xrc r  
    
 k k
k  
 

references

agrawal  r   mannila  h   srikant  r   toivonen  h     verkamo  a  i          fast discovery association rules  fayyad  u  m   piatetsky shapiro  g   smyth  p    
uthurusamy  r   eds    advances knowledge discovery data mining  aaai
press 
  

fimoore   lee

breiman  l   friedman  j  h   olshen  r  a     stone  c  j          classification
regression trees  wadsworth 
clark  p     niblett  r          cn  induction algorithm  machine learning    
        
cover  t  m     thomas  j  a          elements information theory  john wiley  
sons 
fayyad  u   mannila  h     piatetsky shapiro  g          data mining knowledge
discovery  kluwer academic publishers  new journal 
fayyad  u     uthurusamy  r          special issue data mining  communications
acm          
friedman  n     yakhini  z          sample complexity learning bayesian networks  proceedings   th conference uncertainty artificial intelligence 
morgan kaufmann 
guttman  a          r trees  dynamic index structure spatial searching  proceedings third acm sigact sigmod symposium principles database
systems  assn computing machinery 
harinarayan  v   rajaraman  a     ullman  j  d          implementing data cubes eciently  proceedings fifteenth acm sigact sigmod sigart symposium
principles database systems   pods       pp           assn computing
machinery 
john  g  h   kohavi  r     p eger  k          irrelevant features subset selection
problem  cohen  w  w     hirsh  h   eds    machine learning  proceedings
eleventh international conference  morgan kaufmann 
john  g  h     lent  b          sipping data firehose  proceedings third
international conference knowledge discovery data mining  aaai press 
kohavi  r          power decision tables  lavrae  n     wrobel  s   eds   
machine learning   ecml       th european conference machine learning 
heraclion  crete  greece  springer verlag 
kohavi  r          scaling accuracy naive bayes classifiers  decision tree hybrid  e  simoudis j  han u  fayyad  ed    proceedings second
international conference knowledge discovery data mining  aaai press 
madala  h  r     ivakhnenko  a  g          inductive learning algorithms complex
systems modeling  crc press inc   boca raton 
mannila  h     toivonen  h          multiple uses frequent sets condensed representations  e  simoudis j  han u  fayyad  ed    proceedings second
international conference knowledge discovery data mining  aaai press 
  

ficached sufficient statistics efficient machine learning

moore  a  w   schneider  j     deng  k          ecient locally weighted polynomial
regression predictions  d  fisher  ed    proceedings      international
machine learning conference  morgan kaufmann 
omohundro  s  m          ecient algorithms neural network behaviour  journal
complex systems                 
quinlan  j  r          learning ecient classification procedures application
chess end games  michalski  r  s   carbonell  j  g     mitchell  t  m   eds    machine learning an artificial intelligence approach  i   tioga publishing company 
palo alto 
quinlan  j  r          learning logical definitions relations  machine learning    
        
roussopoulos  n     leifker  d          direct spatial search pictorial databases using
packed r trees  navathe  s   ed    proceedings acm sigmod      international conference management data  assn computing machinery 
rymon  r          se tree based characterization induction problem  p 
utgoff  ed    proceedings   th international conference machine learning 
morgan kaufmann 

  


