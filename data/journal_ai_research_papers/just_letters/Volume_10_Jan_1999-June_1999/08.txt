journal of artificial intelligence research                  

submitted        published     

issues in stacked generalization
kai ming ting

kmting deakin edu au

ian h  witten

ihw cs waikato ac nz

school of computing and mathematics
deakin university  australia 
department of computer science
university of waikato  new zealand 

abstract

stacked generalization is a general method of using a high level model to combine lowerlevel models to achieve greater predictive accuracy  in this paper we address two crucial
issues which have been considered to be a  black art  in classification tasks ever since the
introduction of stacked generalization in      by wolpert  the type of generalizer that is
suitable to derive the higher level model  and the kind of attributes that should be used as
its input  we find that best results are obtained when the higher level model combines the
confidence  and not just the predictions  of the lower level ones 
we demonstrate the effectiveness of stacked generalization for combining three different
types of learning algorithms for classification tasks  we also compare the performance of
stacked generalization with majority vote and published results of arcing and bagging 

   introduction
stacked generalization is a way of combining multiple models that have been learned for a
classification task  wolpert         which has also been used for regression  breiman      a 
and even unsupervised learning  smyth   wolpert         typically  different learning
algorithms learn different models for the task at hand  and in the most common form of
stacking the first step is to collect the output of each model into a new set of data  for each
instance in the original training set  this data set represents every model s prediction of that
instance s class  along with its true classification  during this step  care is taken to ensure
that the models are formed from a batch of training data that does not include the instance
in question  in just the same way as ordinary cross validation  the new data are treated
as the data for another learning problem  and in the second step a learning algorithm is
employed to solve this problem  in wolpert s terminology  the original data and the models
constructed for them in the first step are referred to as level   data and level   models 
respectively  while the set of cross validated data and the second stage learning algorithm
are referred to as level   data and the level   generalizer 
in this paper  we show how to make stacked generalization work for classification tasks
by addressing two crucial issues which wolpert        originally described as  black art 
and have not been resolved since  the two issues are  i  the type of attributes that should
be used to form level   data  and  ii  the type of level   generalizer in order to get improved
accuracy using the stacked generalization method 
breiman      a  demonstrated the success of stacked generalization in the setting of
ordinary regression  the level   models are regression trees of different sizes or linear
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fiting   witten

regressions using different number of variables  but instead of selecting the single model
that works best as judged by  for example  cross validation  breiman used the different level  regressors  output values for each member of the training set to form level   data  then
he used least squares linear regression  under the constraint that all regression coecients
be non negative  as the level   generalizer  the non negativity constraint turned out to be
crucial to guarantee that the predictive accuracy would be better than that achieved by
selecting the single best predictor 
here we show how stacked generalization can be made to work reliably in classification
tasks  we do this by using the output class probabilities generated by level   models to
form level   data  then for the level   generalizer we use a version of least squares linear
regression adapted for classification tasks  we find the use of class probabilities to be crucial
for the successful application of stacked generalization in classification tasks  however 
the non negativity constraints found necessary by breiman in regression are found to be
irrelevant to improved predictive accuracy in our classification situation 
in section    we formally introduce the technique of stacked generalization and describe
pertinent details of each learning algorithm used in our experiments  section   describes
the results of stacking three different types of learning algorithms  section   compares
stacked generalization with arcing and bagging  two recent methods that employ sampling
techniques to modify the data distribution in order to produce multiple models from a single
learning algorithm  the following section describes related work  and the paper ends with
a summary of our conclusions 

   stacked generalization

given a data set l   f y   x    n              n g  where y is the class value and x is a vector
representing the attribute values of the nth instance  randomly split the data into j almost
equal parts l            l   define l and l       l   l to be the test and training sets for
the j th fold of a j  fold cross validation  given k learning algorithms  which we call level  
generalizers  invoke the kth algorithm on the data in the training set l     to induce a
model m       for k              k   these are called level   models 
for each instance x in l   the test set for the j th cross validation fold  let z denote
the prediction of the model m     on x   at the end of the entire cross validation process 
the data set assembled from the outputs of the k models is
n

n

n

j

j

j

n

j

j

j

k

n

j

kn

j

n

k

l   f y   z            z    n              n g 
cv

n

n

kn

these are the level   data  use some learning algorithm that we call the level   generalizer
  for y as a function of  z            z    this is the level  
to derive from these data a model m
model  figure   illustrates the cross validation process  to complete the training process 
the final level   models m   k              k   are derived using all the data in l 
now let us consider the classification process  which uses the models m   k              k  
    given a new instance  models m produce a vector  z            z   
in conjunction with m
    whose output is the final classification result for
this vector is input to the level   model m
that instance  this completes the stacked generalization method as proposed by wolpert
        and also used by breiman      a  and leblanc   tibshirani        
k

k

k

k

   

k

fiissues in stacked generalization

 
m

l cv
level  
level  

  j 

  j 

m 

  j 

mk

mk

  j 

l

figure    this figure illustrates the j  fold cross validation process in level    and the level  
  
data set l at the end of this process is used to produce level   model m
cv

    the
as well as the situation described above  which results in the level   model m
present paper also considers a further situation where the output from the level   models is
a set of class probabilities rather than a single class prediction  if model m     is used to
classify an instance x in l   let p  x  denote the probability of the ith output class  and
the vector
p    p    x            p  x            p  x   
gives the model s class probabilities for the nth instance  assuming that there are i classes 
as the level   data  assemble together the class probability vector from all k models  along
with the actual class 
l    f y   p            p           p    n              n g 
    to contrast it with m
  
denote the level   model derived from this as m
the following two subsections describe the algorithms used as level   and level   generalizers in the experiments reported in section   
j

k

j

ki

kn

cv

n

k

n

n

ki

kn

n

ki

n

kn

    level   generalizers

three learning algorithms are used as the level   generalizers  c     a decision tree learning
algorithm  quinlan         nb  a re implementation of a naive bayesian classifier  cestnik 
       and ib   a variant of a lazy learning algorithm  aha  kibler   albert        which
employs the p nearest neighbor method using a modified value difference metric for nominal
and binary attributes  cost   salzberg         for each of these learning algorithms we
now show the formula that we use
p for the estimated output class probabilities p  x  for an
instance x  where  in all cases  p  x       
c     consider the leaf of the decision tree at which the instance x falls  let m be the
number of  training  instances with class i at this leaf  and suppose the majority class
i

i

i

i

   

fiting   witten

p
at the leaf is i   let e        m   then  using a laplace estimator 
i

i

i

p   x        pem        
i

i

i

p  x         p   x    me   for i    i  
i

i

i

note that only pruned trees and default settings of c    are used in our experiments 
nb  let p  ijx  be the posterior probability of class i  given instance x  then
p  x    pp p ij xij x   
i

i

note that nb uses a laplacian estimate for estimating the conditional probabilities
for each nominal attribute to compute p  ijx   for each continuous valued attribute 
a normal distribution is assumed in which case the conditional probabilities can be
conveniently represented entirely in terms of the mean and variance of the observed
values for each class 
ib   suppose p nearest neighbors are used  denote them by f y   x    s              pg for
instance x   we use p     in the experiments   then
s

s

p f  y   d x  x  
p  x    p     d x  x    
p

s

s

i

s

p

  

s

s

where f  y       if i   y and   otherwise  and d is the euclidean distance function 
s

s

in all three learning algorithms  the predicted class of the level   model  given an instance
x  is that i  for which
p   x    p  x  for all i    i  
i

i

    level   generalizers

we compare the effect of four different learning algorithms as the level   generalizer  c    
ib  using p      nearest neighbors    nb  and a multi response linear regression algorithm 
mlr  only the last needs further explanation 
mlr is an adaptation of a least squares linear regression algorithm that breiman      a 
used in regression settings  any classification problem with real valued attributes can be
transformed into a multi response regression problem  if the original classification problem
has i classes  it is converted into i separate regression problems  where the problem for
class   has instances with responses equal to one when they have class   and zero otherwise 
the input to mlr is level   data  and we need to consider the situation for the model
 
 
m   where the attributes are probabilities  separately from that for the model m    where
   a large value is used following wolpert s        advice that   it is reasonable that  relatively global 
smooth   level   generalizers should perform well  
p

   

   

   

fiissues in stacked generalization

they are classes  in the former case  where the attributes are already real valued  the linear
regression for class   is simply

x
lr  x    ff
k

 

k 

p  x  
k 

k

in the latter case  the classes are unordered nominal attributes  we map them into binary
values in the obvious way  setting p  x  to   if the class of instance x is   and zero otherwise 
and then use the above linear regression 
choose the linear regression coecients fff g to minimize
k 

x x
j

 

yn  xn

  lj

k 

 y  
n

xff

p      x      
j

k 

k 

n

k

the coecients fff g are constrained to be non negative  following breiman s      a  discovery that this is necessary for the successful application of stacked generalization to regression problems  the non negative coecient least squares algorithm described by lawson
  hanson        is employed here to derive the linear regression for each class  we show
later that  in fact  the non negative constraint is unnecessary in classification tasks 
with this in place  we can now describe the working of mlr  to classify a new instance
x  compute lr  x  for all i classes and assign the instance to that class   which has the
greatest value  
lr  x    lr  x  for all         
in the next section we investigate the stacking of c     nb and ib  
k 

 

 

  

   stacking c     nb and ib 

    when does stacked generalization work 

the experiments in this section show that
 for successful stacked generalization it is necessary to use output class prob    rather than m
  
abilities rather than class predictions that is  m
 only the mlr algorithm is suitable for the level   generalizer  among the four
algorithms used 
we use two artificial datasets and eight real world datasets from the uci repository of
machine learning databases  blake  keogh   merz         details of these are given in
table   
for the artificial datasets led   and waveform each training dataset l of size    
and      respectively  is generated using a different seed  the algorithms used for the
experiments are then tested on a separate dataset of      instances  results are expressed
as the average error rate of ten repetitions of this entire procedure 
for the real world datasets  w  fold cross validation is performed  in each fold of this
cross validation  the training dataset is used as l  and the models derived are evaluated
   the pattern recognition community calls this type of classifier a linear machine  duda   hart        

   

fiting   witten

datasets   samples   classes   attr   type
led  
        
  
  n
waveform         
 
  c
horse
   
 
 b   n  c
credit
   
 
 b  n  c
vowel
   
  
  c
euthyroid
    
 
  b  c
splice
    
 
  n
abalone
    
 
 n  c
nettalk s 
    
 
 n
coding
     
 
  n

n nominal  b binary  c continuous 

table    details of the datasets used in the experiment 
on the test dataset  the result is expressed as the average error rate of the w  fold crossvalidation  note that this cross validation is used for evaluation of the entire procedure 
whereas the j  fold cross validation mentioned in section   is the internal operation of
stacked generalization  however  both w and j are set to    in the experiments 
  and
in this section  we present results of model combination using level   models m
 
 
m   as well as a model selection method  employing the same j  fold cross validation procedure  note that the only difference between model combination and model selection here
is whether the level   learning is employed or not 
table   shows the average error rates  obtained using w  fold cross validation  of c    
nb and ib   and bestcv  which is the best of the three  selected using j  fold crossvalidation  as expected  bestcv is almost always the classifier with the lowest error rate  
    for which
table   shows the result of stacked generalization using the level   model m
      for
the level   data comprise the classifications generated by the level   models  and m
which the level   data comprise the probabilities generated by the level   models  results
are shown for all four level   generalizers in each case  along with bestcv  the lowest error
rate for each dataset is given in bold 
table   summarizes the results in table   in terms of a comparison of each level  
    derived
model with bestcv totaled over all datasets  clearly  the best level   model is m
using mlr  it performs better than bestcv in nine datasets and equally well in the tenth 
  is derived from nb  which performs better than bestcv in seven
the best performing m
datasets but significantly worse in two  waveform and vowel   we regard a difference of
more than two standard errors as significant      confidence   the standard error figures
are omitted in this table to increase readability 
the datasets are shown in the order of increasing size  mlr performs significantly
better than bestcv in the four largest datasets  this indicates that stacked generalization
is more likely to give significant improvements in predictive accuracy if the volume of data
is large a direct consequence of more accurate estimation using cross validation 
   note that bestcv does not always select the same classifier in all folds  that is why its error rate is
not always equal to the lowest error rate among the three classifiers 
w

   

fiissues in stacked generalization

datasets

level   generalizers
c    nb
ib 
led  
         
    
waveform          
    
horse
         
    
credit
         
    
vowel
         
   
euthyroid        
   
splice
       
   
abalone
         
    
nettalk s           
    
coding
         
    

bestcv
        
        
        
        
       
       
       
        
        
        

table    average error rates of c     nb and ib   and bestcv the best among them
selected using j  fold cross validation  the standard errors are shown in the last
column 
datasets

 
level   model  m
c    nb ib  mlr
                   
                   
                   
                   
           
   

bestcv
led  
    
waveform
    
horse
    
credit
    
vowel
   
euthyroid
               
splice
               
abalone
                   
nettalk s 
                   
coding
                   

   
   

    
    
    

  
level   model  m
c    nb ib  mlr
                   
                   
                   
                   
               
               
               
                   
                   
                   

table    average error rates for stacking c     nb and ib  
 
  
level   model  m
level   model  m
c    nb ib  mlr c    nb ib  mlr
 win vs   loss                                
  and m
    
table    summary of table   comparison of bestcv with m

   

fiting   witten

when one of the level   models performs significantly much better than the rest  like in
the euthyroid and vowel datasets  mlr performs either as good as bestcv by selecting
the best performing level   model  or better than bestcv 
mlr has an advantage over the other three level   generalizers in that its model can
easily be interpreted  examples of the combination weights it derives  for the probability      appear in table   for the horse  credit  splice  abalone  waveform  led  
based model m
and vowel datasets  the weights indicate the relative importance of the level   generalizers
for each prediction class  for example  in the splice dataset  in table   b    nb is the
dominant generalizer for predicting class    nb and ib  are both good at predicting class
   and all three generalizers make a worthwhile contribution to the prediction of class   
in contrast  in the abalone dataset all three generalizers contribute substantially to the
prediction of all three classes  note that the weights for each class do not sum to one
because no such constraint is imposed on mlr 

    are non negativity constraints necessary 
both breiman      a  and leblanc   tibshirani        use the stacked generalization
method in a regression setting and report that it is necessary to constrain the regression
coecients to be non negative in order to guarantee that stacked regression improves predictive accuracy  here we investigate this finding in the domain of classification tasks 
to assess the effect of the non negativity constraint on performance  three versions of
    
mlr are employed to derive the level   model m
i  each linear regression in mlr is calculated with an intercept constant  that is 
i     weights for the i classes  but without any constraints 
ii  each linear regression is derived with neither an intercept constant  i weights
for i classes  nor constraints 
iii  each linear regression is derived without an intercept constant  but with nonnegativity constraints  i non negative weights for i classes  
the third version is the one used for the results presented earlier  table   shows the
results of all three versions  they all have almost indistinguishable error rates  we conclude
that in classification tasks  non negativity constraints are not necessary to guarantee that
stacked generalization improves predictive accuracy 
however  there is another reason why it is a good idea to employ non negativity constraints  table   shows an example of the weights derived by these three versions of mlr on
the led   dataset  the third version  shown in column  iii   supports a more perspicuous
interpretation of each level   generalizer s contribution to the class predictions than do the
other two  in this dataset  ib  is the dominant generalizer in predicting classes      and   
and both nb and ib  make a worthwhile contribution in predicting class    as evidenced
by their high weights  however  the negative weights used in predicting these classes render
the interpretation of the other two versions much less clear 
   

fiissues in stacked generalization

horse
credit
class c    nb ib  c    nb ib 
 
                             
 
                             
c    for ff    nb for ff    ib  for ff   
      for the horse and credit datasets 
table     a  weights generated by mlr  model m
class
 
 
 

c   
    
    
    

splice
nb
    
    
    

ib 
    
    
    

abalone
c    nb ib 
              
              
              

waveform
c    nb ib 
              
              
              

      for the splice  abalone and waveform
table     b  weights generated by mlr  model m
datasets 
vowel
c    nb ib 
              
              
              
              
              
              
              
              
              
              
              
      for the led   and vowel datasets 
table     c  weights generated by mlr  model m
class
 
 
 
 
 
 
 
 
 
  
  

c   
    
    
    
    
    
    
    
    
    
    
 

led  
nb
    
    
    
    
    
    
    
    
    
    
 

ib 
    
    
    
    
    
    
    
    
    
    
 

   

fiting   witten

datasets

mlr with
no constraints no intercept non negativity
led  
    
    
    
waveform
    
    
    
horse
    
    
    
credit
    
    
    
vowel
   
   
   
euthyroid
   
   
   
splice
   
   
   
abalone
    
    
    
nettalk s 
    
    
    
coding
    
    
    
table    average error rates of three versions of mlr 
class
 
 
 
 
 
 
 
 
 
  

ff 

    
    
    
    
    
    
    
    
    
    

ff 

 i 

ff 

ff 

ff 

 ii 

ff 

ff 

ff 

                                  
                                    
                                    
                                    
                                    
                                  
                                  
                                      
                                    
                                    

 iii 

ff 

    
    
    
    
    
    
    
    
    
    

ff 

    
    
    
    
    
    
    
    
    
    

table    weights generated by three versions of mlr   i  no constraints   ii  no intercept 
and  iii  non negativity constraints  for the led   dataset 

   

fiissues in stacked generalization

dataset
 se bestcv majority mlr
horse
   
    
         
splice
   
   
       
abalone
   
    
         
led  
   
    
         
credit
   
    
         
nettalk s      
    
         
coding
    
    
         
waveform     
    
         
euthyroid     
   
       
vowel
     
   
        
       along with
table    average error rates of bestcv  majority vote and mlr  model m
the number of standard error   se  between bestcv and the worst performing
level   generalizers 

    how does stacked generalization compare to majority vote 
      derived from mlr  to that of majority vote 
let us now compare the error rate of m

a simple decision combination method which requires neither cross validation nor level  learning  table   shows the average error rates of bestcv  majority vote and mlr 
in order to see whether the relative performances of level   generalizers have any effect
on these methods  the number of standard errors   se  between the error rates of the
worst performing level   generalizer and bestcv is given  and the datasets are re ordered
according to this measure  since bestcv almost always selects the best performing level  
generalizer  small values of  se indicate that the level   generalizers perform comparably
to one another  and vice versa 
mlr compares favorably to majority vote  with eight wins versus two losses  out of
the eight wins  six have significant differences  the two exceptions are for the splice and
led   datasets   whereas both losses  for the horse and credit datasets  have insignificant
differences  thus the extra computation for cross validation and level   learning seems to
have paid off 
it is interesting to note that the performance of majority vote is related to the size of
 se  majority vote compares favorably to bestcv in the first seven datasets  where the
values of  se are small  in the last three  where  se is large  majority vote performs
worse  this indicates that if the level   generalizers perform comparably  it is not worth
using cross validation to determine the best one  because the result of majority vote which
is far cheaper is not significantly different  although small values of  se are a necessary
condition for majority vote to rival bestcv  they are not a sucient condition see matan
       for an example  the same applies when majority vote is compared with mlr  mlr
performs significantly better in the five datasets that have large  se values  but in only
one of the other cases 
   

fiting   witten

m  versus m   

c    nb ib  mlr
 win vs   loss                
  versus m
    for each generalizer summarized results from table   
table    m
it is worth mentioning a method that averages p  x  for each i over all level   models 
yielding p  x   and then predicts class i  for which p  x    p  x  for all i    i   according to
breiman      b   this method produces an error rate almost identical to that of majority
vote 
i

i

i

i

    why does stacked generalization work best with m    generated from
mlr 
we have shown that stacked generalization works best when output class probabilities
 rather than class predictions  are used with the mlr algorithm  rather than c     ib  
nb   in retrospect  this is not surprising  and can be explained intuitively as follows  the
level   model should provide a simple way of combining all the evidence available  this
evidence includes not just the predictions  but the confidence of each level   model in
its predictions  a linear combination is the simplest way of pooling the level   models 
confidence  and mlr provides just that 
the alternative methods of nb  c     and ib  each have shortcomings  a bayesian approach could form the basis for a suitable alternative way of pooling the level   models  confidence  but the independence assumption central to naive bayes hampers its performance in
some datasets because the evidence provided by the individual level   models is certainly not
independent  c    builds trees that can model interaction amongst attributes particularly
when the tree is large but this is not desirable for combining confidences  nearest neighbor methods do not really give a way of combining confidences  also  the similarity metric
employed could misleadingly assume that two different sets of confidence levels are similar 
  with m
    for each level  
table   summarizes the results in table   by comparing m
generalizer  across all datasets  c    is clearly better off with a label based representation 
because discretizing continuous valued attributes creates intra attribute interaction in addition to interactions between different attributes  the evidence from table   is that nb
is indifferent to the use of labels or confidences  the normal distribution assumption that
it embodies in the latter case could be another reason why it is unsuitable for combining
confidence measures  both mlr and ib  handle continuous valued attributes better than
label based ones  since this is the domain in which they are designed to work 
summary

we summarize our findings in this section as follows 

 none of the four learning algorithms used to obtain model m  perform satisfactorily 
   

fiissues in stacked generalization

 mlr is the best of the four learning algorithms to use as the level   generalizer for
    
obtaining the model m
 when obtained using mlr  m    has lower predictive error rate than the best model
selected by j  fold cross validation  for almost all datasets used in the experiments 

 another advantage of mlr over the other three level   generalizers is its interpretability 
the weights ff indicate the different contributions that each level   model k makes
to the prediction classes   
k 

 model m    can be derived by mlr with or without non negativity constraints  such
constraints make little difference to the model s predictive accuracy 

 the use of non negativity constraints in mlr has the advantage of interpretability  non 

negative weights ff support easier interpretation of the extent to which each model
contributes to each prediction class 
k 

 when derived using mlr  model m    compares favorably with majority vote 
 mlr provides a method of combining the confidence generated by the level   models into
a final decision  for various reasons  nb  c     and ib  are not suitable for this task 

   comparison with arcing and bagging
this section compares the results of stacking c     nb and ib  with the results of arcing
 called boosting by its originator  schapire        and bagging that are reported by breiman
     b      c   both arcing and bagging employ sampling techniques to modify the data
distribution in order to produce multiple models from a single learning algorithm  to
combine the decisions of the individual models  arcing uses a weighted majority vote and
bagging uses an unweighted majority vote  breiman reports that both arcing and bagging
can substantially improve the predictive accuracy of a single model derived using a base
learning algorithm 

    experimental results

first we describe the differences between the experimental procedures  our results for
stacking are averaged over ten fold cross validation for all datasets except waveform  which
is averaged over ten repeated trials  standard errors are also shown  results for arcing and
bagging are those obtained by breiman      b      c   which are averaged over     trials 
in breiman s experiments  each trial uses a random     split to form the training and test
sets for all datasets except waveform  also note that the waveform dataset we used has   
irrelevant attributes  but breiman used a version without irrelevant attributes  which would
be expected to degrade the performance of level   generalizers in our experiments   in both
cases     training instances were used for this dataset  but we used      test instances
whereas breiman used       arcing and bagging are done with    decision tree models
derived from cart  breiman et al         in each trial 
   

fiting   witten

dataset
 samples stacking arcing bagging
waveform
   
             
    
glass
   
             
    
ionosphere
   
       
   
   
soybean
   
       
   
   
breast cancer
   
       
   
   
diabetes
   
             
    
table     comparing stacking with arcing and bagging classifiers 
the results on six datasets are given in table     and indicate that the three methods
are very competitive   stacking performs better than both arcing and bagging in three
datasets  waveform  soybean and breast cancer   and is better than arcing but worse than
bagging in the diabetes dataset  note that stacking performs very poorly on glass and
ionosphere  two small real world datasets  this is not surprising  because cross validation
inevitably produces poor estimates for small datasets 

    discussion

like bagging  stacking is ideal for parallel computation  the construction of each level  
model proceeds independently  no communication with the other modeling processes being
necessary 
arcing and bagging require a considerable number of member models because they
rely on varying the data distribution to get a diverse set of models from a single learning
algorithm  using a level   generalizer  stacking can work with only two or three level  
models 
suppose the computation time required for a learning algorithm is c   and arcing or
bagging needs h models  the learning time required is t   hc   suppose stacking requires
g models and each model employs j  fold cross validation  assuming that time c is needed
to derive each of the g level   models and the level   model  the learning time for stacking
is t    g j          c   for the results given in table     h       j       and g      thus
t     c and t     c   however  in practice the learning time required for the level  
and level   generalizers may be different 
users of stacking have a free choice of level   models  they may either be derived from a
single learning algorithm  or from a variety of different algorithms  the example in section
  uses different types of learning algorithms  while bag stacking stacking bagged models
 ting   witten        uses data variation to obtain a diverse set of models from a single
learning algorithm  in the former case  performance may vary substantially between the
level   models for example nb performs very poorly in the vowel and euthyroid datasets
compared to the other two models  see table     stacking copes well with this situation 
the performance variation among the member models in bagging is rather small because
they are derived from the same learning algorithm using bootstrap samples  section    
a

s

a

s

   the heart dataset used by breiman      b      c  is omitted because it was very much modified from
the original one 

   

fiissues in stacked generalization

shows that a small performance variation among member models is a necessary condition
for majority vote  as employed by bagging  to work well 
it is worth noting that arcing and bagging can be incorporated into the framework of
stacked generalization by using arced or bagged models as level   models  ting   witten
       show one possible way of incorporating bagged models with level   learning  employing mlr instead of voting  in this implementation  l is used as a test set for each
of the bagged models to derive level   data rather than the cross validated data  this is
viable because each bootstrap sample leaves out about     of the examples  ting   witten
       show that bag stacking almost always has higher predictive accuracy than bagging
models derived from either c    or nb  note that the only difference here is whether an
adaptive level   model or a simple majority vote is employed
according to breiman      b      c   arcing and bagging can only improve the predictive accuracy of learning algorithms that are  unstable    an unstable learning algorithm
is one for which small perturbations in the training set can produce large changes in the
derived model  decision trees and neural networks are unstable  nb and ib  are stable 
stacking works with both 
while mlr is the most successful candidate for level   learning that we have found 
other algorithms might work equally well  one candidate is neural networks  however 
we have experimented with back propagation neural networks for this purpose and found
that they have a much slower learning rate than mlr  for example  mlr only took    
seconds as compare to      seconds for the neural network in the nettalk dataset  while
both have the same error rate  other possible candidates are the multinomial logit model
 jordan   jacobs         which is a special case of generalized linear models  mccullagh
  nelder         and the supra bayesian procedure  jacobs        which treats the level  
models  confidence as data that may be combined with prior distribution of level   models
via bayes  rule 

   related work

our analysis of stacked generalization was motivated by that of breiman      a   discussed
earlier  and leblanc   tibshirani         leblanc   tibshirani        examine the stacking
of a linear discriminant and a nearest neighbor classifier and show that  for one artificial
dataset  a method similar to mlr performs better with non negativity constraints than
without  our results in section     show that these constraints are irrelevant to mlr s
predictive accuracy in the classification situation 
leblanc   tibshirani        and ting   witten        use a version of mlr that
employs all class probabilities from each level   model to induce each linear regression  in
this case  the linear regression for class   is

lr  x   
 

xxff
k

i

k

i

ki 

p  x  
ki

this implementation requires the fitting of ki parameters  as compared to k parameters
for the version used in this paper  see the corresponding formula in section       both
   schapire  r e   y  freund  p  bartlett    w s  lee        provide an alternative explanation for the
effectiveness of arcing and bagging 

   

fiting   witten

versions give comparable results in terms of predictive accuracy  but the version used in
this paper runs considerably faster because it needs to fit fewer parameters 
the limitations of mlr are well known  duda   hart         for a i  class problem  it
divides the description space into i convex decision regions  every region must be singly
connected  and the decision boundaries are linear hyperplanes  this means that mlr is
most suitable for problems with unimodal probability densities  despite these limitations 
mlr still performs better as a level   generalizer than ib   its nearest competitor in deriving
m     these limitations may hold the key to a fuller understanding of the behavior of stacked
generalization  jacobs        reviews linear combination methods like that used in mlr 
previous work on stacked generalization  especially as applied to classification tasks 
has been limited in several ways  some only applies to a particular dataset  e g   zhang 
mesirov   waltz         others report results that are less than convincing  merz        
still others have a different focus and evaluate the results on just a few datasets  leblanc
  tibshirani        chan   stolfo        kim   bartlett        fan et al         
one might consider a degenerate form of stacked generalization that does not use crossvalidation to produce data for level   learning  then  level   learning can be done  on the
y  during the training process  jacobs et al          in another approach  level   learning
takes place in batch mode  after all level   models are derived  ho et al         
several researchers have worked on a still more degenerate form of stacked generalization
without any cross validation or learning at level    examples are neural network ensembles
 hansen   salamon        perrone   cooper        krogh   vedelsby         multiple
decision tree combination  kwok   carter        buntine        oliver   hand         and
multiple rule combination  kononenko   kovacic         the methods used at level   are
majority voting  weighted averaging and bayesian combination  other possible methods are
distribution summation and likelihood combination  there are various forms of re ordering
class rank  and ali   pazzani        study some of these methods for a rule learner  ting
       uses the confidence of each prediction to combine a nearest neighbor classifier and a
naive bayesian classifier 

   conclusions
we have addressed two crucial issues for the successful implementation of stacked generalization in classification tasks  first  class probabilities should be used instead of the single
predicted class as input attributes for higher level learning  the class probabilities serve as
the confidence measure for the prediction made  second  the multi response least squares
linear regression technique should be employed as the high level generalizer  this technique
provides a method of combining level   models  confidence  the other three learning algorithms have either algorithmic limitations or are not suitable for aggregating confidences 
when combining three different types of learning algorithms  this implementation of
stacked generalization was found to achieve better predictive accuracy than both model
selection based on cross validation and majority vote  it was also found to be competitive with arcing and bagging  unlike stacked regression  non negativity constraints in the
least squares regression are not necessary to guarantee improved predictive accuracy in
classification tasks  however  these constraints are still preferred because they increase the
interpretability of the level   model 
   

fiissues in stacked generalization

the implication of our successful implementation of stacked generalization is that earlier
model combination methods employing  weighted  majority vote  averaging  or other computations that do not make use of level   learning  can now apply this learning to improve
their predictive accuracy 

acknowledgment

the authors are grateful to the new zealand marsden fund for financial support for this
research  this work was conducted when the first author was in department of computer
science  university of waikato  the authors are grateful to j  ross quinlan for providing
c    and david w  aha for providing ib   the anonymous reviewers and the editor have
provided many helpful comments 

references

aha  d w   d  kibler   m k  albert         instance based learning algorithms  machine learning     pp        
ali  k m    m j  pazzani         error reduction through learning multiple descriptions  machine learning  vol      no     pp          
blake  c   e  keogh   c j  merz         uci repository of machine learning databases
 http    www ics uci edu  mlearn mlrepository html   irvine  ca  university of california  department of information and computer science 
breiman  l       a   stacked regressions  machine learning  vol      pp        
breiman  l       b   bagging predictors  machine learning  vol      no     pp          
breiman  l       c   bias  variance  and arcing classifiers  technical report      department of statistics  university of california  berkeley  ca 
breiman  l   j h  friedman  r a  olshen   c j  stone         classification and regression trees  belmont  ca  wadsworth 
cestnik  b          estimating probabilities  a crucial task in machine learning  in
proceedings of the european conference on artificial intelligence  pp          
chan  p k    s j  stolfo         a comparative evaluation of voting and meta learning
on partitioned data  in proceedings of the twelfth international conference on machine learning  pp         morgan kaufmann 
cost  s   s  salzberg         a weighted nearest neighbor algorithm for learning with
symbolic features  machine learning      pp        
fan  d w   p k  chan  s j  stolfo         a comparative evaluation of combiner and
stacked generalization  in proceedings of aaai    workshop on integrating multiple
learned models  pp        
hansen  l k    p  salamon         neural network ensembles  ieee transactions of
pattern analysis and machine intelligence      pp           
   

fiting   witten

ho  t k   j j  hull   s n  srihari         decision combination in multiple classifier
systems  ieee transactions on pattern analysis and machine intelligence  vol     
no     pp        
jacobs  r a          methods of combining experts  probability assessments  neural
computation    pp           mit press 
jacobs  r a   m i  jordan  s j  nowlan   g e  hinton         adaptive mixtures of local
experts  neural computation    pp        
jacobs  r a    m i  jordan         hierachical mixtures of experts and the em algorithms  neural computation    pp          
kim  k    e b  bartlett         error estimation by series association for neural network
systems  neural computation    pp           mit press 
kononenko  i    m  kovacic         learning as optimization  stochastic generation
of multiple knowledge  in proceedings of the ninth international conference on
machine learning  pp           morgan kaufmann 
krogh  a    j  vedelsby         neural network ensembles  cross validation  and active
learning  advances in neural information processing systems    g  tesauro  d s 
touretsky   t k  leen  editors   pp           mit press 
kwok  s    c  carter         multiple decision trees  uncertainty in artificial intelligence    r  shachter  t  levitt  l  kanal and j  lemmer  editors   pp          
north holland 
lawson c l    r j  hanson         solving least squares problems  siam publications 
leblanc  m    r  tibshirani         combining estimates in regression and classification  technical report       department of statistics  university of toronto 
matan  o          on voting ensembles of classifiers  extended abstract   in proceedings
of aaai    workshop on integrating multiple learned models  pp        
mccullagh  p    j a  nelder         generalized linear models  london  chapman and
hall 
merz  c j          dynamic learning bias selection  in proceedings of the fifth international workshop on artificial intelligence and statistics  ft  lauderdale  fl 
unpublished  pp          
oliver  j j    d j  hand         on pruning and averaging decision trees  in proceedings
of the twelfth international conference on machine learning  pp           morgan
kaufmann 
perrone  m p    l n  cooper         when networks disagree  ensemble methods for
hybrid neural networks  artificial neural networks for speech and vision  r j 
mammone  editor   chapman hall 
quinlan  j r          c     program for machine learning  morgan kaufmann 
   

fiissues in stacked generalization

schapire  r e          the strength of weak learnability  machine learning     pp 
         kluwer academic publishers 
schapire  r e   y  freund  p  bartlett    w s  lee         boosting the margin  a new
explanation for the effectiveness of voting methods  in proceedings of the fourteenth
international conference on machine learning  pages          morgan kaufmann 
smyth  p    d  wolpert         stacked density estimation  advances in neural information processing systems 
ting  k m          the characterisation of predictive accuracy and decision combination  in proceedings of the thirteenth international conference on machine learning 
pp           morgan kaufmann 
ting  k m    i h  witten         stacking bagged and dagged models  in proceedings of
the fourteenth international conference on machine learning  pp           morgan
kaufmann 
weiss s  m    c  a  kulikowski         computer systems that learns  morgan kaufmann 
wolpert  d h          stacked generalization  neural networks  vol     pp          
pergamon press 
zhang  x   j p  mesirov   d l  waltz         hybrid system for protein secondary
structure prediction  journal of molecular biology       pp            

   

fi