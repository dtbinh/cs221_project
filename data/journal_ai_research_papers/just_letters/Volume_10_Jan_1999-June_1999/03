journal artificial intelligence research                  

submitted       published     

ecient heuristic hypothesis ranking
steve chien jpl nasa gov
andre stechert jpl nasa gov
darren mutz jpl nasa gov

steve chien
andre stechert
darren mutz
jet propulsion laboratory
california institute technology
     oak grove drive  m s        
pasadena  ca           

abstract

paper considers problem learning ranking set stochastic alternatives based upon incomplete information  i e   limited number samples   describe
system that  decision cycle  outputs either complete ordering hypotheses
decides gather additional information  i e   observations  cost  ranking
problem generalization previously studied hypothesis selection problem in selection  algorithm must select single best hypothesis  ranking  algorithm
must order hypotheses 
central problem address achieving desired ranking quality minimizing cost acquiring additional samples  describe two algorithms hypothesis
ranking application probably approximately correct  pac  expected
loss  el  learning criteria  empirical results provided demonstrate effectiveness
ranking procedures synthetic real world datasets 
   introduction

many applications  cost information quite high  imposing requirement
learning algorithms glean much usable information possible minimum
data  example 



data may scarce  making learning possible limited training data
key 



speedup learning  minimizing processing time critical  here  reducing number
necessary training examples key since expense processing example
significant  tadepalli        



decision tree learning  cost using available training examples evaluating potential attributes partitioning computationally expensive  musick 
catlett    russell        



evaluating medical treatment policies  acquiring additional training examples might
imply human subjects exposed experimental treatment longer
period necessary 

one wishes sort guarantee quality solution  statistical decision
theoretic framework useful  framework answers questions  much information

c      ai access foundation morgan kaufmann publishers  rights reserved 

fichien  stechert    mutz
enough  point adequate information rank alternatives
requested confidence 
paper focuses parametric ranking problems  general class statistical machine learning problems goal rank set alternative hypotheses
goodness hypothesis function set parameters whose values unknown
 e g   chien  stechert    mutz        gratch        greiner   jurisica        kaelbling 
      moore   lee        musick et al          learning system determines refines estimates parameters using training examples  secondary goal
minimizing learning cost 
principal contributions paper are 



define two families hypothesis ranking algorithms  based recursive selection
adjacency  respectively  provide specific details apply using
probably approximately correct  pac  expected loss  el  decision criteria 



provide empirical results demonstrating effectiveness algorithms
achieving requested decision criteria synthetic data 



provide empirical results showing algorithms significantly outperform
existing statistical methods real world data spacecraft design optimization
image compression applications 

remainder paper structured follows  first  describe hypothesis ranking problem formally  including definitions probably approximately
correct  pac  expected loss  el  decision criteria  define two algorithms
establishing criteria hypothesis ranking problem a recursive hypothesis selection algorithm adjacent comparison algorithm  next  describe empirical tests
demonstrating effectiveness algorithms well documenting improved
performance standard algorithm statistical ranking literature  finally 
describe related work future extensions algorithms 
   hypothesis ranking problems

hypothesis ranking problems abstract class learning problems algorithm
given set hypotheses rank  ranking desired orders hypotheses
expected utility  determined hypothesis  underlying probability
distribution  expected utilities unknown algorithm must estimated
training data 
hypothesis ranking problems extension hypothesis selection problems  chien 
gratch    burl         learning system attempts select best alternative
set hypotheses  distinction hypothesis ranking hypothesis selection selection learning algorithm interested single best hypothesis 
ranking learning algorithm must determine relative order hypotheses   
hypothesis selection ranking important aspect many machine learning
problems  example  utility problem speedup learning viewed selection
   algorithms results described paper extend straightforward fashion hybrid rankingselection problems system must select rank top n hypotheses 

   

fiefficient heuristic hypothesis ranking
problem single problem solving heuristic strategy chosen larger set
candidates  case  expected utility typically defined average time solve
problem  gratch        greiner   jurisica        minton         attribute selection
problem machine learning viewed hypothesis selection problem
one must select best attribute split set possible attribute splits utility
often measured information gain  musick et al          reinforcement learning 
system must learn appropriate action context  utility interpreted
expected reward  kaelbling         
key observation regarding problems  and learning problems  general  could viewed optimization problem  utility
function optimized  then  application traditional  or non traditional 
optimization methods yield good results within guarantees provided algorithm depending features landscape optimized  however 
addition model sampling cost  new degree freedom added problem 
cost samples high  traditional optimization algorithms fare poorly 
additionally  many mentioned applications system chooses single
alternative never revisits decision  many cases system
want investigate several prioritized options  either serially parallel   hence
ranking useful  motivation provided following scenarios 



upper lower bounds  span  minimax search algorithms use metaknowledge

 such upper lower bounds node  pruning parts tree  also 
times knowing span expected utilities candidate set
useful  e g   checking convergence conditions adaptive algorithm
ga  



augmenting external knowledge  another area hypothesis ranking may



entire ranking  cases  entire ranking significant  instance 

important applications hypothesis selection human supervision 
stochastic objective function  i e   hypothesis  represents part problem  ranking used augment external knowledge problem 
example  engineering simulations usually capture physical properties candidate designs  usually choose forego details manufacturing  logistics 
economics 
evolutionary algorithms  individuals propagated future generations
often selected likelihood proportionate rank current
generation  goldberg         another example arises case search algorithms
take advantage node ordering heuristics  beam search iterative
broadening  ginsberg   harvey        

hypothesis evaluation problem  always achieving correct ranking impossible
practice  exact underlying probability distributions unknown  thus 
always  perhaps vanishingly  small chance algorithms unlucky
   note analogous reinforcement learning problem one learning appropriate action immediate feedback rather delayed feedback 

   

fichien  stechert    mutz
finite number samples taken  consequently  rather always
requiring algorithm output correct ranking  impose probabilistic criteria
rankings produced  several families requirements exist  paper
examine two criteria  probably approximately correct  pac  model selecting
hypothesis function approximates well target function  valiant       
expected loss  el  requirement frequently used decision theory gaming problems
 russell   wefald         informally  satisfy pac requirement  algorithm must
produce result high probability close correct  e g   incorrect orderings
likely occur hypotheses similar expected utilities   satisfy
el requirement  hand  bound must established expected loss
result  loss difference utilities two incorrectly ordered hypothese
incorrect ranking 
expected utility hypothesis estimated observing values
finite set training examples  however  satisfy decision criteria  algorithm must
able reason potential difference estimated true utilities
hypotheses  let ui denote true expected utility hypothesis let u i
estimated expected utility hypothesis i  without loss generality  let us presume
proposed ranking hypotheses u    u            uk     uk  
pac requirement states that  user specified   probability    
k  

  ui       max  ui          uk   

   

i  

context pac criterion  number called indifference interval

overall ranking error total error rate   

issue allocate overall ranking error among many possible pairwise
comparisons hypotheses discussed next section 
correspondingly  selecting hypothesis h  best set k hypotheses h         hk   let selection loss l follows 
l h    fh         hk g    max     max  u         uk  

u   

   

then  ranking loss rl ranking h         hk would be 
rl h         hk    

k  
x

l hi   fhi          hk g 

   

i  

   distinction betwen true means estimated means  for use sample means 
confusing one  assessing validity ranking produced algorithm  one would use
true means distributions  if available  test distributions  accurate estimation
possible  such edxtremely large sampling distribution   however  ranking algorithm
uses estimated parameters  including sample mean  estimate error  estimation single
mean estimate mean normally distributed around true mean usage
justified  however  proven  and indeed unsure  whether using estimate
complex ranking selection contexts guaranteed correct  see later section heuristic nture
algorithms  

   

fiefficient heuristic hypothesis ranking
hypothesis ranking algorithm obeys expected loss requirement must produce
rankings average less ranking loss requested expected loss bound 
policy loss allocation discussed next section 
example  consider ranking hypotheses expected utilities  u         u   
      u          ranking u    u    u  valid pac ranking indifference
interval               observed ranking loss                 
however  confidence pairwise comparison two hypotheses well
understood complement probability comparison s result
error  less clear define ensure desired confidence met set
comparisons required selection even complex set comparisons required
ranking  equation   defines confidence ui     uj   utilities
normally distributed unknown unequal variances 

pn



   u i

j    
 

si

   

j

represents cumulative standard normal distribution function  n  u i j  
s i j size  sample mean  sample standard deviation blocked differential
distribution   respectively 
likewise  computation expected loss asserting ordering pair
hypotheses well understood  estimation expected loss entire ranking
less clear  equation   defines expected loss drawing conclusion ui   uj  
assumption normality  see chien et al         details  
el ui   uj    

s i

je

u i j  
 
j

   n   
si

p

 n

 

u i

p

j

 

z

 

u i j pn
s i j

e

   z  

dz

   

next two subsections  describe two interpretations estimating likelihood
overall ranking satisfies pac el requirements estimating combining
pairwise pac errors el estimates  interpretations lends directly
algorithmic implementation described below 
    ranking recursive selection

one obvious way determine ranking h         hk view ranking recursive selection
set remaining candidate hypotheses  view  overall ranking error 
specified desired confidence pac algorithms loss threshold el
algorithms  first distributed among k   selection errors subdivided
pairwise comparison errors  figure     data sampled estimates
pairwise comparison error  as dictated equation      satisfy bounds set
algorithm 
   note approach block  match  examples reduce sampling complexity  blocking
makes estimates using difference utility competing hypotheses observed example  blocking significantly reduce variance data hypotheses independent 
differential distribution formed taking differences blocked individual samples form
new distribution  trivial modify formulas address cases possible
block data  see moore   lee        chien et al         details  

   

fichien  stechert    mutz

h 

h 

h 

h 

h 

h 

h 

h 

h 

h 

h 

h 

h 

h 


 

figure    computing overall error recursive ranking  per comparison errors
summed level recursion  overall sum  across levels 
compared specified total error   

thus  another degree freedom design recursive ranking algorithms
method overall ranking error ultimately distributed among individual pairwise comparisons hypotheses  two factors uence way compute
error distribution  first  model error combination determines error allocated
individual comparisons selections combines overall ranking error therefore
many candidates available distribution error 
using bonferroni s inequality  asserts probability union events
greater sum probabilities individual events    one would inclined
combine errors additively  however  following conservative approach  one
assert predicted  best  hypothesis may change sampling
worst case  conclusion might dependon possible pairwise comparisons
error distributed among n  pairs hypotheses  
second  policy respect allocation error among candidate comparisons
selections determines samples distributed  example  contexts 
consequences early selections far outweigh later selections  scenarios 
implemented ranking algorithms divide overall ranking error unequally
   note simplest bonferonni inequalities  fall clean correspondence
terms expansion probability union events according principle
inclusion exclusion natural way 
   discussion issue  see pp         gratch        

   

fiefficient heuristic hypothesis ranking
favor earlier selections   also  possible divide selection error pairwise error
unequally based estimates hypothesis parameters order reduce sampling cost
 for example  gratch  chien    dejong        allocates error rationally  
within scope paper  consider algorithms that   i  combine pairwise
error selection error additively   ii  combine selection error overall ranking error
additively   iii  allocate error equally level 
one disadvantage recursive selection hypothesis selected 
removed pool candidate hypotheses  issue rare cases when 
sampling increase confidence later selection  estimate hypothesis 
mean changes enough previously selected hypothesis longer dominates it 
however  remains original hypotheses shown dominate others
specified level certainty   
assumptions result following formulations  where  u  fu         uk g 
used denote error due action selecting hypothesis   equation  
set fh         hk g  u  fu         uk g  denotes error due selection loss
situations equation   applies  
rec  u    u          uk    

rec  u    u          uk  
   u  fu         uk g 

   

rec  uk        the base case recursion  selection error defined
 chien et al         
 u  fu         uk g   

k
x

  i

   

i  

using equation   compute pairwise confidence 

algorithmically  implement following pseudo code 

ensure n  samples per hypothesis
distribute error individual selections
 stopping criteria met 
take samples
 means ordered differently ranking 
restart algorithm
analogous recursive selection algorithm based expected loss defined follows
elrec  u    u          uk    

elrec  u    u          uk  
 el u  fu         uk g 

   

elrec uk       selection el defined  chien et al         
el u  fu         uk g   

k
x
i  

   space constraints preclude description here 

   

el u    ui  

   

fichien  stechert    mutz

 



   

h 

   

h 

k   k

h 

hk  



hk

figure    computing overall error adjacent ranking  per comparison errors neighboring hypotheses proposed ranking summed compared
required total error   

    ranking adjacency comparison

another interpretation ranking confidence  or loss  adjacent elements
ranking need compared  case  overall ranking error divided directly
k   pairwise comparison errors  figure     leads following confidence equation
pac criteria 
adj  u    u          uk    

k  
x

i i  

    

i  

following equation el criteria 
eladj  u    u          uk    

k  
x

el ui   ui    

    

i  

ranking comparison adjacent hypotheses establish dominance
loss bounds non adjacent hypotheses  where hypotheses ordered
observed mean utility   advantage requiring fewer comparisons recursive
selection  and thus may require fewer samples recursive selection   however 
reason  adjacency algorithms may less likely recursive selection algorithms
bound probability correct ranking  or average loss  correctly  case
pac algorithms   dominance necessarily transitive  case
el algorithms  expected loss necessarily additive considering two
hypothesis comparisons sharing common hypothesis  
   example ranking loss non adjacent hypotheses exceeds desired loss bound
ranking  even though sum adjacent losses not  occurs blocked differential
distribution induced two non adjacent hypotheses high variance relative hypothesis adjacent

   

fiefficient heuristic hypothesis ranking
    heuristic nature algorithms

recusrsive selection adjacency algorithms heuristic sense
proven statistically meet specified decision criteria  i e   pac criteria
select ranking satisfies equation     probability   similarly el
criteria average ranking loss specified equation     less requested bound 
indeed  several aspects algorithms make extremely dicult prove
would  probabilistically  achieve corresponding decision criteria  aspects include 



sharing samples  order n  samples differential distribution  i e 



heuristic error combination  recursive selection adjacency error com 



ignorance lead switches multiple comparison paths  sampling pro 



blocking  h  h    takes n  samples h  n  samples
problems h    algorithms reduce sampling cost reusing
samples differential distributions comparing h  hypotheses h 
hypotheses  makes errors derived samples independent 
hence traded accuracy ease analysis algorithms heuristic
eciency  particularly recursive selection approach  samples lowest
ranking hypothesis would used k   differential comparisons 
bination models heuristic means combining pairwise errors 
pairwise errors independent  see above   empirically observed
pairwise errors tend overestimated error combination function
tends under combine  overall empirically combined error estimates tend
reasonably accurate  remaining sections show 
cess  ordering hypotheses may change  e g   ordering sample means
may change   means implicitly  decision depended additional
pairwise comparison may ected final set comparisons contributing pairwise error  complexity could avoided fixing order
hypotheses n  samples  however  would require samples would
involve showing  dominance hypothesis higher sample mean hypothesis
 indeed  may never converge   choose ignore complexity base
combined error used stopping condition final ordering 

use non normal distributions  many applications described re 

mainder article  real world data distributed manner simlar
normal distributions  we investigate issue later article  
algorithms describe heuristic presume data normally
distributed even though case 

 i e   currently ranked them   variance differential distribution makes
maximum contribution sample set small  so  e g                     n        
                  n         exists configuration                  
expected losses el h    h            el h    h            el h    h                  

   

fichien  stechert    mutz
    relevant approaches

standard statistical ranking selection approaches make strong assumptions
form problem  e g   variances associated underlying utility distribution
hypotheses might assumed known equal   among these  method turnbull
weiss  turnbull   weiss        comparable pac based approach  
turnbull weiss  algorithm sequential interval based procedure selecting
member population largest mean  treat hypotheses normally
distributed random variables unknown mean unknown possibly unequal
variance  algorithm carries additional stipulation hypotheses
independent  procedure consists taking initial sample n  observations
hypotheses taking samples sequentially according stopping criteria 
stopping criteria satisfied  hypothesis highest sample mean
 
chosen  stopping criteria inequality snii n  satisfied  si
ni sample mean number samples ith hypothesis n chosen
 
according indifference
interval confidence level   particular  n   d 
r 
chosen satisfy    f  y   d  k  f  y dy   f  y  f  y  cumulative
distribution function probability density function standard normal distribution 
still reasonable use approach candidate hypotheses
independent  excessive statistical error unnecessarily large training set sizes may result 
case hypotheses truly independent  turnbull weiss  technique
able exploit knowledge outperform methods adopt
assumption 
   empirical performance evaluation

turn empirical evaluation hypothesis ranking techniques synthetic
real world datasets  evaluation serves three purposes  first  demonstrates
techniques perform predicted  in terms bounding probability incorrect selection expected loss   second  validates performance techniques compared
standard algorithms statistical literature  third  evaluation demonstrates
robustness new approaches real world hypothesis ranking problems 
experimental trial consists solving hypothesis ranking problem given
technique given set problem control parameters  measure performance
    well algorithms satisfy respective criteria      number
samples taken or  alternatively  cost  in seconds  executing algorithm  since
performance statistical algorithms single trial provides little information
overall behavior  trial repeated multiple times results averaged
across trials  synthetic experimental trials repeated     times  trials
real world data repeated     times  pac expected loss criteria
directly comparable  approaches analyzed separately 
   pac based approaches investigated extensively statistical ranking selection literature topic confidence interval based algorithms  see haseeb        review recent
literature  

   

fiefficient heuristic hypothesis ranking
hk

h 

h 

h 

h 

  k   

  

  

 



utility

figure    stepped means hypothesis configuration 
    evaluation synthetic datasets

evaluation synthetic data used show that      techniques correctly bound probability incorrect ranking expected loss predicted underlying assumptions
valid even underlying utility distributions inherently hard rank     
    pac techniques compare favorably algorithm turnbull weiss
wide variety circumstances 
synthetic datasets  utility distributions hypotheses modeled
random variables defined underlying parameterized distribution  thus  characterizing ranking problem consists choosing number hypotheses rank
assigning values parameters representing utility distributions hypotheses  case  model utilities independent normal random variables
mean standard deviation  thus  let k number hypotheses  hypothesis ranking problem described  k parameters specifying expected utility
utility standard deviation hypothesis  general  several parameters may required characterize ranking problem fully    number hypotheses
choices parameters utility distributions underlying hypotheses
characterize overall diculty ranking problem 
statistical ranking selection community uses standard family selection
problems known diculty analyze performance hypothesis selection strategies 
method  called least favorable configuration  lfc  population means
assignment parameters distributions likely cause technique
choose wrong hypothesis thus provides severe test technique s abilities 
configuration  utilities independent normally distributed variables equal
variance  k   hypotheses utilities equal expectation    remaining
hypothesis expected utility    
interested hypothesis ranking problems rather selection problems 
use generalization lfc call stepped means  configuration  one
hypotheses assigned expected utility successive hypotheses assigned
expected utility         k    figure    
general  problems based least favorable configuration become dicult
 i e   require samples  number hypotheses k increases  common utility
variance   increases  difference means utility distributions decreases 
standard methodology  technique evaluated ability achieve confidence
    configurations contain hypotheses high variance relative separation means
dicult rank 
    instance  samples allocated rationally  chien et al          becomes necessary assign
parameters cost distribution well  candidate hypotheses ranked 
number hypotheses rank would another problem parameter 

   

fichien  stechert    mutz
correct selection using several settings k   last ratio combines
single quantity which  increases  makes problem dicult  methodology
extends stepped means directly 
hypothesis ranking strategies algorithm control parameters
govern attack problem  pac techniques three control parameters 
initial sample size n    desired confidence correct ranking indifference setting
     expected loss techniques two control parameters  initial sample size n 
loss threshold h  
observed number samples required achieved accuracy pac techniques
stepped means configuration shown table      results indicate
systems roughly comparable number examples required choose hypotheses 
expected  number examples increases k      p acadj algorithm
required least number samples inconsistent meeting desired accuracy
bound  as indicated failure meet prescribed error bound several cases  
interesting turnbull weiss method significantly outperform pac
techniques despite fact algorithm assumes hypotheses independent
 as case stepped means configuration   pac approaches make
assumption  comparison  principal performance metric number
samples required achieve requested ranking  methods effective achieving
requested accuracy 
expected loss experiments  ran expected loss hypothesis ranking algorithms
stepped means configurations described range expected loss
bounds  table     shows results experiment  displaying number samples
required produce ranking average observed loss configuration 
results show elrec algorithm correctly bounded loss eladj algorithm required less samples elrec algorithm  correctly bound
expected loss  since observed loss greater loss bound h    
    evaluation real datasets

test real world applicability based data drawn several datasets relating
spacecraft design processing science data gathered context planetary
exploration  first two datasets investigate relate spacecraft design optimization
problems hypotheses wish rank candidate solutions design
problem  third last dataset examine involves ranking various lossless image
compression approaches based performance large set terrestrial images collected spacecraft galileo  cost evaluation given seconds empirical data
    note formulation stepped means test pac approaches  difference
expected mean successive hypotheses indifference interval algorithm  thus 
plays roles problem parameter control parameter here 
    one confusing point identical hypothesis ranking algorithm settings  one observe
lower loss ranking larger number hypotheses  algorithm first divides
loss number pirwise comparisons  thus  overall error  or expected loss bound  
hypotheses  pairwise expected error  or loss  smaller hypotheses 
ranking loss defined previously  thus  possible observed loss increase decrease
compared settings fewer hypotheses 

   

fiefficient heuristic hypothesis ranking

k
 
 
 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  



    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    




 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

turnbull
         
          
         
          
          
          
          
          
          
          
          
          
          
            
          
            
          
            

p acrec

         
          
         
          
          
          
          
          
          
          
          
          
          
            
          
            
          
            

p acadj

         
         
         
         
         
          
          
          
          
          
          
          
          
          
          
          
          
            

table    estimated expected total number observations pac algorithms
stepped means configuration  achieved probability correct ranking shown
parenthesis 

parameters
k h
       
        
       
        
       
        
       
        
        
         
        
         

elrec

samples
  
   
   
   
   
   
   
   
     
     
     
     

loss
   
   
   
   
   
   
   
   
   
   
   
   

eladj

samples
  
  
  
   
   
   
   
   
   
   
   
     

loss
   
   
   
   
   
   
   
   
   
   
   
   

table    estimated expected total number observations el algorithms stepped
means configuration  observed average loss produced rankings 

   

fichien  stechert    mutz
because  unlike synthetic problems  cost sampling hypothesis constant
domains  table   gives summary three ranking problems considered 
dataset
ds   penetrator

fixed parameters
penetrator diameter
penetrator length

ds   aeroshell

fore body overlap
nose cone angle
bluntness ratio
fillet radius
outer diameter
tail geometry
compression method

lossless image comp 

random variables
impact orientation
impact velocity
soil density
stagnation pressure coef 

optimization criteria
maximize penetration probability
maximize penetration depth

randomly selected test image

maximize compression ratio

minimize weight
achieve target entry velocity

table    description datasets used algorithm evaluation 

      ds   penetrator

goal new millennium deep space two  ds    mission deliver pair
microprobes planet mars scientific study martian soil  probes
released orbit  travel martian atmosphere  embed
soil near southern polar ice cap  primary science objectives mission
 balacuit         





determine ice present surface mars 
measure local atmospheric pressure 
characterize thermal properties martian subsurface soil 

goal spacecraft design problem determine good set physical dimensions penetrator a small  robust probe designed impact surface extremely
high velocity operate extreme cold  specifically  use design simulation
data ds   mission penetrator design 
casting design problem  hold shape penetrator constant
generate design candidates based different values variables penetrator diameter
length  specific design sample taken acquiring impact orientation  impact
velocity  soil density parameterized multivariate distribution calling
complex physical simulation determine depth penetrator bored
martian surface  goal penetrator design problem determine physical
dimensions penetrator maximize probability penetration  cases
penetration  maximize penetration depth 
tables     show results applying pac based  turnbull  expected loss
algorithms ranking problem system requested rank    penetrator
designs    problem utility function depth penetration penetrator 
     true  expected utility values computed performing        samples using sample mean
large sample ground truth  expected utilities used compute pac  validity
rankings observed loss using provided definitions 

   

fiefficient heuristic hypothesis ranking
cases penetrator penetrate assigned zero utility 
shown table    pac algorithms significantly outperformed turnbull algorithm 
expected hypotheses somewhat correlated  via impact orientations soil densities   table   shows elrec expected loss algorithm effectively
bounded actual loss eladj algorithm inconsistent 
k
  
  
  



    
    
    




 
 
 

turnbull
          
          
          

p acrec

          
          
          

p acadj

         
         
          

table    estimated expected total number observations rank ds   spacecraft designs 
achieved probability correct ranking shown parenthesis 

parameters
k
h
  
    
  
    
  
    

elrec

samples
   
   
   

loss
    
    
    

eladj

samples
  
  
   

loss
    
    
    

table    estimated expected total number observations expected loss incorrect
ranking ds   penetrator designs 

      ds   aeroshell design ranking

objective problem design aeroshell soil penetrator described
previous section gives appropriate entry velocity minimum weight  design
candidates defined six continuous variables represent various geometric quantities  extent fore body overlaps aftbody  nose cone angle  bluntness
ratio  fillet radius  outer diameter  tail geometry  candidate designs  hypotheses 
evaluated running simple physical simulation aeroshell s behavior 
sample taken running simulation fixed design variables hypothesis
value stagnation pressure coecient taken normal distribution 
simulation computes values achieved entry velocity mass aeroshell 
weighted sum reciprocals values maximized 
give results ranking three  five  ten hypotheses using turnbull  pac 
expected loss algorithms tables       
previous experiment  pac based algorithms outperformed turnbull
algorithm cases  p acadj algorithm represents significant increase
    again  deep sampling      samples  performed obtain  correct  ranking 
algorithms compared 

   

fichien  stechert    mutz
performance here  note achieve desired level confidence cases 
turnbull p acrec algorithms achieve required confidence 

k
 
 
 
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  



    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    




 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

turnbull
          
           
           
           
           
           
           
           
           
           
           
            
           
            
            
            
            
            

p acrec

          
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
            

p acadj

          
          
          
          
          
          
           
           
           
           
           
           
           
           
           
           
           
           

table    estimated expected cost  in seconds  rank aeroshell designs  achieved probability correct ranking shown parenthesis 

parameters
k
h
 
  
 
  
 
  
 
  
 
  
 
  
  
  
  
  
  
  

elrec

execution cost
   
   
   
    
    
    
    
    
    

loss
   
   
   
   
    
   
   
   
    

eladj

execution cost
   
   
   
   
   
    
    
    
    

loss
   
   
   
   
    
   
   
   
   

table    estimated expected cost  in seconds  expected loss incorrect ranking
ds   aeroshell designs 

   

fiefficient heuristic hypothesis ranking
      lossless image compression galileo image data

problem utilizes large set raw image data acquired galileo spacecraft 
images         size made greyscale pixels ranging      
intensity  goal select lossless compression method   performs best
class images  performance image compression algorithm particular image
could measured number ways  example  execution time  compression ratio 
image quality  in case lossy compression methods considered  could
define algorithm performance  tests chose consider compression ratio
achieved given compression method utility function  sample method
 hypothesis   image randomly selected  method applied image 
achieved compression ratio recorded 
given  tables      results ranking three  five  seven hypotheses
using turnbull  pac  expected loss algorithms  ranking correctness determined
comparison  correct  ranking established sampling compression method
set      distinct images 
note substantial performance improvement pac based algorithms
turnbull algorithm  although turnbull algorithm pac
algorithms  table    achieved desired confidence level  adjacent version el
algorithm  table    failed bound loss specified level half cases 
interesting consider results presented section light fact
statistical techniques used makes form normality assumption 
fact  three problem domains investigate number hypotheses whose
utility functions normally distributed  past experience known utility
functions ds   penetrator domain  section        highly non normal  figure  
illustrates difference data normally distributed data not 
    

    

    
   
    
    
    

    

    

    
    
    
    
    

 
   

 
   

   

   

   

   

   

 

  

   

   

   

   

   

   

   

   

   

figure    comparison  a  data normally distributed high likelihood  b 
data likely normally distributed  case  histogram
experimental data shown solid boxes  data drawn normal distribution
mean standard deviation shown dashed lines 
determine extent utilities hypotheses remaining two domains normally distributed applied kolmogorov smirnov test  see appendix
    seven compression methods considered were  calic  lossless jpeg  gif  tiff  pack  gzip 
compress 

   

fichien  stechert    mutz

k
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 



    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    




 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

turnbull
           
            
           
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            

p acrec

           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

p acadj

           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

table    estimated expected cost  in seconds  rank lossless image compression approaches galileo image data  achieved probability correct ranking shown
parenthesis 

parameters
k
h
 
  
 
 
 
 
 
  
 
 
 
 
 
  
 
 
 
 

elrec

execution cost
    
    
    
    
    
     
    
     
     

loss
   
   
   
   
   
   
   
   
   

eladj

execution cost
    
    
    
    
    
    
    
    
    

loss
   
   
   
    
    
    
    
    
    

table    estimated expected cost  in seconds  expected loss incorrect ranking
ds   penetrator designs 

   

fiefficient heuristic hypothesis ranking
details   test determined none ten hypotheses ds   aeroshell
domain  section        normally distributed utility  additionally  two seven
hypotheses image compression domain  section        shown greater
    likelihood normally distributed utility functions    reasons 
evaluating ranking strategies datasets provides particularly strong test
applicability techniques 
draw reader s attention particularly large disparity performance
turnbull algorithm pac based algorithms image compression domain 
especially apparent number hypotheses  confidence level  high 
additionally  problem domain two hypotheses normally distributed utility
five non normal  observations suggest pac based algorithms
perform better  in relative terms  faced domain violates assumption
normality 
   discussion conclusions

number areas related work  first  considerable analysis
hypothesis selection problems  selection problems formalized using bayesian
framework  moore   lee        rivest   sloan        require initial
sample  uses rigorous encoding prior knowledge  howard  howard       
details bayesian framework analyzing learning cost selection problems  one
uses hypothesis selection framework ranking  allocation pairwise errors
performed rationally  gratch et al          reinforcement learning work  kaelbling       
immediate feedback viewed hypothesis selection problem 
framework presented invites future work number directions  currently 
stopping criteria used relaxations ranking requirement  another approach
could used bound resources available ranking  limiting number
samples sample cost high limiting time computation  so
anytime algorithm  two straightforward application areas 
another area future work discovery composite strategies hypotheses  thus
far examined ranking  and articles  selection  hypothesis highest expected value entire distribution  example  learning scheduling control
strategy well distribution problems  however  likely
distributions problems  exists composite strategy would outperform
single strategy  example  single strategy might apply method solve
problem  composite strategy would be  test problem feature x  x true apply method a  else apply method b  composite strategies correspond algorithm
portfolios named operations research  indeed results applying methods could
viewed strategies  one might composite strategy trying method
   cpu seconds  fails trying method b  course  composition portfolio approaches  diculty iseciently proposing evaluating plausible
compositions  even small set base strategies number copositions enormous 
    reference  data figure    a  normally distributed       likelihood  according
kolmogorov smirnov test 

   

fichien  stechert    mutz
summary  paper described hypothesis ranking problem  extension
hypothesis selection problem  defined application two decision criteria  probably approximately correct expected loss  problem  defined two families
algorithms  recursive selection adjacency  solution hypothesis ranking problems 
finally  demonstrated effectiveness algorithms synthetic realworld datasets  documenting improved performance existing statistical approaches 
acknowledgments

work performed jet propulsion laboratory  california institute technology  contract national aeronautics space administration 
appendix a  applying k s test real datasets

kolmogorov smirnov test statistical means accepting  certain level
confidence  hypothesis sampleset fits parametric distribution given
set parameters  method compares cdf generated empirical distribution
corresponding parametric distribution  i e   estimated parameters  
k s test gives confidence based maximum  d  discrepancies
two cdfs 
  maxjf   x 

f   x j

purposes wish determine  hypothesis given domain  whether
values utility function normally distributed not  case  half
utility samples taken used compute mean standard deviation normal 
remaining half used compute cdf 
a   ds   penetrator

      samples taken 
design number
 
 
 
 
 
 
 
 
 
  

maxjf   x 

f   x j
      
      
      
      
      
      
      
      
      
      

   

normally distributed 
    likely
    likely
    likely
    likely
    likely
    likely
    likely
    likely
    likely
    likely

fiefficient heuristic hypothesis ranking
a   ds   aeroshell design ranking

    samples taken 
design number
 
 
 
 
 
 
 
 
 
  

maxjf   x 

f   x j

    
    
    
    
    
    
    
    
    
    

normally distributed 
      likely
      likely
      likely
      likely
      likely
      likely
      likely
      likely
      likely
      likely

a   lossless image compression galileo image data

    samples taken 
compression method
gif
compress
calic
gzip
jpegls
pack
tiff

maxjf   x 

f   x j

    
    
    
    
    
    
    

normally distributed 
    likely
      likely
    likely
      likely
    likely
      likely
      likely

references

balacuit   c  p          deep space     mars microprobe home page  mission objectives
statement   tech  rep  http   nmp jpl nasa gov ds   nasa jpl 
chien  s  a   gratch  j  m     burl  m  c          ecient allocation resources
hypothesis evaluation  statistical approach  ieee trans  pattern analysis
machine intelligence                  
chien  s  a   stechert  a  d     mutz  d  h          ecient heuristic ranking hypotheses  advances neural information processing systems     jordan  kearns 
solla eds    pp          denver  colorado  nips 
ginsberg  m     harvey  w          iterative broadening  artificial intelligence journal 
            
   

fichien  stechert    mutz
goldberg  d          genetic algorithms search  optimization  machine learning 
addison wesley 
gratch  j          composer  probabilistic solution utility problem speed up
learning  proceedings tenth national conference artificial intelligence 
pp          san jose  ca  aaai 
gratch  j          composer  decision theoretic approach adaptive problem solving  tech  rep  uiucdcs r          department computer science  university
illinois 
gratch  j   chien  s     dejong  g          improving learning performance
rational resource allocation  proceedings twelfth national conference
artificial intelligence  pp          seattle  wa  aaai 
greiner  r     jurisica  i          statistical approach solving ebl utility
problem  proceedings tenth national conference artificial intelligence 
pp          san jose  ca  aaai 
haseeb  r  m          modern statistical selection  american sciences press  columbus 
oh 
howard  r  a          decision analysis  perspectives inference  decision  experimentation  proceedings ieee                  
kaelbling  l  p          learning embedded systems  mit press  cambridge  ma 
minton  s          learning search control knowledge  explanation based approach 
kluwer academic publishers  norwell  ma 
moore  a  w     lee  m  s          ecient algorithms minimizing cross validation
error  proceedings international conference machine learning new
brunswick  ma 
musick  r   catlett  j     russell  s          decision theoretic subsampling induction large databases  proceedings international conference machine
learning  pp          amherst  ma 
rivest  r  l     sloan  r          new model inductive inference  proceedings
second conference theoretical aspects reasoning knowledge 
russell  s     wefald  e          right thing  studies limited rationality  mit
press  cambridge  ma 
tadepalli  p          theory unsupervised speedup learning  proc  tenth
national conference artificial intelligence  pp          san jose  ca  aaai 
turnbull  b  w     weiss  l  i          class sequential procedures k sample
problems concerning normal means unknown equal variances  santner 
t  j     tamhane  a  c   eds    design experiments  ranking selection  pp 
         marcel dekker 
   

fiefficient heuristic hypothesis ranking
valiant  l  g          theory learnable  communications acm     
          

   


