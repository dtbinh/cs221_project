journal of artificial intelligence research                  

submitted        published     

variational probabilistic inference
and the qmr dt network

tommi s  jaakkola

tommi ai mit edu

artificial intelligence laboratory 
massachusetts institute of technology 
cambridge  ma       usa

michael i  jordan

computer science division and department of statistics 
university of california 
berkeley  ca            usa

jordan cs berkeley edu

abstract

we describe a variational approximation method for ecient inference in large scale
probabilistic models  variational methods are deterministic procedures that provide approximations to marginal and conditional probabilities of interest  they provide alternatives to approximate inference methods based on stochastic sampling or search  we describe
a variational approach to the problem of diagnostic inference in the  quick medical reference   qmr  network  the qmr network is a large scale probabilistic graphical model
built on statistical and expert knowledge  exact probabilistic inference is infeasible in this
model for all but a small set of cases  we evaluate our variational inference algorithm on a
large set of diagnostic test cases  comparing the algorithm to a state of the art stochastic
sampling method 

   introduction
probabilistic models have become increasingly prevalent in ai in recent years  beyond
the significant representational advantages of probability theory  including guarantees of
consistency and a naturalness at combining diverse sources of knowledge  pearl        
the discovery of general exact inference algorithms has been principally responsible for the
rapid growth in probabilistic ai  see  e g   lauritzen   spiegelhalter        pearl       
shenoy         these exact inference methods greatly expand the range of models that can
be treated within the probabilistic framework and provide a unifying perspective on the
general problem of probabilistic computation in graphical models 
probability theory can be viewed as a combinatorial calculus that instructs us in how
to merge the probabilities of sets of events into probabilities of composites  the key operation is that of marginalization  which involves summing  or integrating  over the values
of variables  exact inference algorithms essentially find ways to perform as few sums as
possible during marginalization operations  in terms of the graphical representation of
probability distributions in which random variables correspond to nodes and conditional
independencies are expressed as missing edges between nodes exact inference algorithms
define a notion of  locality   for example as cliques in an appropriately defined graph   and
attempt to restrict summation operators to locally defined sets of nodes 
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fijaakkola   jordan

while this approach manages to stave off the exponential explosion of exact probabilistic
computation  such an exponential explosion is inevitable for any calculus that explicitly
performs summations over sets of nodes  that is  there are models of interest in which
 local  is overly large  see jordan  et al   in press   from this point of view  it is perhaps
not surprising that exact inference is np hard  cooper        
in this paper we discuss the inference problem for a particular large scale graphical
model  the quick medical reference  qmr  model   the qmr model consists of a combination of statistical and expert knowledge for approximately     significant diseases and
approximately      findings  in the probabilistic formulation of the model  the qmr dt  
the diseases and the findings are arranged in a bi partite graph  and the diagnosis problem
is to infer a probability distribution for the diseases given a subset of findings  given that
each finding is generally relevant to a wide variety of diseases  the graph underlying the
qmr dt is dense  reecting high order stochastic dependencies  the computational complexity of treating these dependencies exactly can be characterized in terms of the size of
the maximal clique of the  moralized  graph  see  e g   dechter        lauritzen   spiegelhalter         in particular  the running time is exponential in this measure of size  for the
qmr dt  considering the standardized  clinocopathologic conference   cpc  cases that
we discuss below  we find that the median size of the maximal clique of the moralized graph
is       nodes  this rules out the use of general exact algorithms for the qmr dt 
the general algorithms do not take advantage of the particular parametric form of the
probability distributions at the nodes of the graph  and it is conceivable that additional
factorizations might be found that take advantage of the particular choice made by the
qmr dt  such a factorization was in fact found by heckerman         his  quickscore
algorithm  provides an exact inference algorithm that is tailored to the qmr dt  unfortunately  however  the run time of the algorithm is still exponential in the number of positive
findings  for the cpc cases  we estimate that the algorithm would require an average of
   years to solve the inference problem on current computers 
faced with the apparent infeasibility of exact inference for large scale models such as
the qmr dt  many researchers have investigated approximation methods  one general
approach to developing approximate algorithms is to perform exact inference  but to do so
partially  one can consider partial sets of node instantiations  partial sets of hypotheses 
and partial sets of nodes  this point of view has led to the development of algorithms for
approximate inference based on heuristic search  another approach to developing approximation algorithms is to exploit averaging phenomena in dense graphs  in particular  laws
of large numbers tell us that sums of random variables can behave simply  converging to
predictable numerical results  thus  there may be no need to perform sums explicitly  either
exactly or partially  this point of view leads to the variational approach to approximate
inference  finally  yet another approach to approximate inference is based on stochastic
sampling  one can sample from simplified distributions and in so doing obtain information
about a more complex distribution of interest  we discuss each of these methods in turn 
horvitz  suermondt and cooper        have developed a partial evaluation algorithm
known as  bounded conditioning  that works by considering partial sets of node instan   the acronym  qmr dt  that we use in this paper refers to the  decision theoretic  reformulation of
the qmr by shwe  et al          shwe  et al  replaced the heuristic representation employed in the
original qmr model  miller  fasarie    myers        by a probabilistic representation 

   

fivariational probabilistic inference and qmr dt

tiations  the algorithm is based on the notion of a  cutset   a subset of nodes whose
removal renders the remaining graph singly connected  ecient exact algorithms exist for
singly connected graphs  pearl         summing over all instantiations of the cutset  one
can calculate posterior probabilities for general graphs using the ecient algorithm as a
subroutine  unfortunately  however  there are exponentially many such cutset instantiations  the bounded conditioning algorithm aims at forestalling this exponential growth by
considering partial sets of instantiations  although this algorithm has promise for graphs
that are  nearly singly connected   it seems unlikely to provide a solution for dense graphs
such as the qmr dt  in particular  the median cutset size for the qmr dt across the
cpc cases is        yielding an unmanageably large number of        cutset instantiations 
another approach to approximate inference is provided by  search based  methods 
which consider node instantiations across the entire graph  cooper        henrion       
peng   reggia         the general hope in these methods is that a relatively small fraction
of the  exponentially many  node instantiations contains a majority of the probability mass 
and that by exploring the high probability instantiations  and bounding the unexplored
probability mass  one can obtain reasonable bounds on posterior probabilities  the qmrdt search space is huge  containing approximately      disease hypotheses  if  however 
one only considers cases with a small number of diseases  and if the hypotheses involving
a small number of diseases contain most of the high probability posteriors  then it may
be possible to search a significant fraction of the relevant portions of the hypothesis space 
henrion        was in fact able to run a search based algorithm on the qmr dt inference
problem  for a set of cases characterized by a small number of diseases  these were cases 
however  for which the exact quickscore algorithm is ecient  the more general corpus of
cpc cases that we discuss in the current paper is not characterized by a small number of
diseases per case  in general  even if we impose the assumption that patients have a limited
number n of diseases  we cannot assume a priori that the model will show a sharp cutoff
in posterior probability after disease n   finally  in high dimensional search problems it is
often necessary to allow paths that are not limited to the target hypothesis subspace  in
particular  one would like to be able to arrive at a hypothesis containing few diseases by
pruning hypotheses containing additional diseases  peng   reggia         imposing such a
limitation can lead to failure of the search 
more recent partial evaluation methods include the  localized partial evaluation  method
of draper and hanks         the  incremental spi  algorithm of d ambrosio         the
 probabilistic partial evaluation  method of poole         and the  mini buckets  algorithm
of dechter         the former algorithm considers partial sets of nodes  and the latter three
consider partial evaluations of the sums that emerge during an exact inference run  these
are all promising methods  but like the other partial evaluation methods it is yet not clear if
they restrict the exponential growth in complexity in ways that yield realistic accuracy time
tradeoffs in large scale models such as the qmr dt  
variational methods provide an alternative approach to approximate inference  they
are similar in spirit to partial evaluation methods  in particular the incremental spi and
mini buckets algorithms   in that they aim to avoid performing sums over exponentially
   d ambrosio        reports  mixed  results using incremental spi on the qmr dt  for a somewhat
more dicult set of cases than heckerman        and henrion         but still with a restricted number
of positive findings 

   

fijaakkola   jordan

many summands  but they come at the problem from a different point of view  from the
variational point of view  a sum can be avoided if it contains a sucient number of terms
such that a law of large numbers can be invoked  a variational approach to inference
replaces quantities that can be expected to be the beneficiary of such an averaging process
with surrogates known as  variational parameters   the inference algorithm manipulates
these parameters directly in order to find a good approximation to a marginal probability of
interest  the qmr dt model turns out to be a particularly appealing architecture for the
development of variational methods  as we will show  variational methods have a simple
graphical interpretation in the case of the qmr dt 
a final class of methods for performing approximate inference are the stochastic sampling methods  stochastic sampling is a large family  including techniques such as rejection
sampling  importance sampling  and markov chain monte carlo methods  mackay        
many of these methods have been applied to the problem of approximate probabilistic inference for graphical models and analytic results are available  dagum   horvitz        
in particular  shwe and cooper        proposed a stochastic sampling method known as
 likelihood weighted sampling  for the qmr dt model  their results are the most promising results to date for inference for the qmr dt they were able to produce reasonably
accurate approximations in reasonable time for two of the dicult cpc cases  we consider
the shwe and cooper algorithm later in this paper  in particular we compare the algorithm
empirically to our variational algorithm across the entire corpus of cpc cases 
although it is important to compare approximation methods  it should be emphasized
at the outset that we do not think that the goal should be to identify a single champion
approximate inference technique  rather  different methods exploit different structural
features of large scale probability models  and we expect that optimal solutions will involve
a combination of methods  we return to this point in the discussion section  where we
consider various promising hybrids of approximate and exact inference algorithms 
the general problem of approximate inference is np hard  dagum   luby        and
this provides additional reason to doubt the existence of a single champion approximate
inference technique  we think it important to stress  however  that this hardness result 
together with cooper s        hardness result for exact inference cited above  should not
be taken to suggest that exact inference and approximate inference are  equally hard   to
take an example from a related field  there exist large domains of solid and uid mechanics
in which exact solutions are infeasible but in which approximate techniques  finite element
methods  work well  similarly  in statistical physics  very few models are exactly solvable 
but there exist approximate methods  mean field methods  renormalization group methods 
that work well in many cases  we feel that the goal of research in probabilistic inference
should similarly be that of identifying effective approximate techniques that work well in
large classes of problems 

   the qmr dt network
the qmr dt network  shwe et al         is a two level or bi partite graphical model  see
figure     the top level of the graph contains nodes for the diseases   and the bottom level
contains nodes for the findings  
   

fivariational probabilistic inference and qmr dt

there are a number of conditional independence assumptions reected in the bi partite
graphical structure  in particular  the diseases are assumed to be marginally independent 
 i e   they are independent in the absence of findings  note that diseases are not assumed
to be mutually exclusive  a patient can have multiple diseases   also  given the states
of the disease nodes  the findings are assumed to be conditionally independent   for a
discussion regarding the medical validity and the diagnostic consequences of these and
other assumptions embedded into the qmr dt belief network  see shwe et al         
diseases

d 

f 

dn

fm
findings

figure    the qmr belief network is a two level graph where the dependencies between
the diseases and their associated findings have been modeled via noisy or gates 
to state more precisely the probability model implied by the qmr dt model  we write
the joint probability of diseases and findings as 

p  f  d    p  f jd p  d   

 

y

i

 
  
y
p  fi jd    p  dj   

j

   

where d and f are binary       vectors referring to presence absence states of the diseases
and the positive negative states or outcomes of the findings  respectively  the conditional
probabilities p  fi jd  are represented by the  noisy or model   pearl        
y
p  fi    jd    p  fi    jl  p  fi    jdj  
   
       qi   



y

j  i

     qij  dj

j  
p i
 
i    j i ij dj
e
 

   

   
where i is the set of diseases that are parents of the finding fi in the qmr graph  qij  
p  fi    jdj      is the probability that the disease j   if present  could alone cause the
finding to have a positive outcome  and qi    p  fi    jl  is the  leak  probability  i e  
the probability that the finding is caused by means other than the diseases included in
the qmr model  in the final line  we reparameterize the noisy or probability model
using an exponentiated notation  in this notation  the model parameters are given by
ij     log     qij   
   

fijaakkola   jordan

   inference
carrying out diagnostic inference in the qmr model involves computing the posterior
marginal probabilities of the diseases given a set of observed positive  fi      and negative
 fi       findings  note that the set of observed findings is considerably smaller than the set
of possible findings  note moreover  from the bi partite structure of the qmr dt graph 
that unobserved findings have no effect on the posterior probabilities for the diseases  for
brevity we adopt a notation in which fi  corresponds to the event fi      and fi  refers
to fi      positive and negative findings respectively   thus the posterior probabilities of
interest are p  dj jf     f      where f   and f   are the vectors of positive and negative findings 
the negative findings f   are benign with respect to the inference problem they can be
incorporated into the posterior probability in linear time in the number of associated diseases
and in the number of negative findings  as we discuss below  this can be seen from the
fact that the probability of a negative finding in eq      is the exponential of an expression
that is linear in the dj   the positive findings  on the other hand  are more problematic  in
the worst case the exact calculation of posterior probabilities is exponentially costly in the
number of positive findings  heckerman        d ambrosio         moreover  in practical
diagnostic situations the number of positive findings often exceeds the feasible limit for
exact calculations 
let us consider the inference calculations in more detail  to find the posterior probability
p  djf     f     we first absorb the evidence from negative findings  i e   we compute p  djf    
this is just p  f   jd p  d  with normalization  since both p  f   jd  and p  d  factorize over
the diseases  see eq      and eq      above   the posterior p  djf     must factorize as well 
the normalization of p  f   jd p  d  therefore reduces to independent normalizations over
each disease and can be carried out in time linear in the number of diseases  or negative
findings   in the remainder of the paper  we concentrate solely on the positive findings as
they pose the real computational challenge  unless otherwise stated  we assume that the
prior distribution over the diseases already contains the evidence from the negative findings 
in other words  we presume that the updates p  dj   p  dj jf     have already been made 
we now turn to the question of computing p  dj jf      the posterior marginal probability
based on the positive findings  formally  obtaining such a posterior involves marginalizing
p  f   jd p  d  across the remaining diseases 

p  dj jf      

x

dndj

p  f   jd p  d 

   

where the summation is over all the possible configurations of the disease variables other
than dj  we use the shorthand summation index d n dj for this   in the qmr model
p  f   jd p  d  has the form 

p  f   jd p  d   
 

 
 

y

i

 
  
y
p  fi  jd    p  dj   

j

 
 
  y
    e  i    j ij dj   p  dj   

y

i

   

p

j

   

   

fivariational probabilistic inference and qmr dt

which follows from eq      and the fact that p  fi  jd        p  f   jd   to perform the
summation in eq      over the diseases  we would have to multiply out the terms     efg
corresponding to the conditional probabilities for each positive finding  the number of
such terms is exponential in the number of positive findings  while algorithms exist that
attempt to find and exploit factorizations in this expression  based on the particular pattern
of observed evidence  cf  heckerman        d ambrosio         these algorithms are limited
to roughly    positive findings on current computers  it seems unlikely that there is sucient
latent factorization in the qmr dt model to be able to handle the full cpc corpus  which
has a median number of    positive findings per case and a maximum number of    positive
findings 

   variational methods
exact inference algorithms perform many millions of arithmetic operations when applied to
complex graphical models such as the qmr dt  while this proliferation of terms expresses
the symbolic structure of the model  it does not necessarily express the numeric structure
of the model  in particular  many of the sums in the qmr dt inference problem are sums
over large numbers of random variables  laws of large numbers suggest that these sums
may yield predictable numerical results over the ensemble of their summands  and this fact
might enable us to avoid performing the sums explicitly 
to exploit the possibility of numerical regularity in dense graphical models we develop
a variational approach to approximate probabilistic inference  variational methods are a
general class of approximation techniques with wide application throughout applied mathematics  variational methods are particularly useful when applied to highly coupled systems  by introducing additional parameters  known as  variational parameters  which
essentially serve as low dimensional surrogates for the high dimensional couplings of the
system these methods achieve a decoupling of the system  the mathematical machinery
of the variational approach provides algorithms for finding values of the variational parameters such that the decoupled system is a good approximation to the original coupled
system 
in the case of probabilistic graphical models variational methods allow us to simplify a
complicated joint distribution such as the one in eq       this is achieved via parameterized transformations of the individual node probabilities  as we will see later  these node
transformations can be interpreted graphically as delinking the nodes from the graph 
how do we find appropriate transformations  the variational methods that we consider
here come from convex analysis  see appendix     let us begin by considering methods for
obtaining upper bounds on probabilities  a well known fact from convex analysis is that
any concave function can be represented as the solution to a minimization problem 

f  x    min
f  t x   f      g


   

where f      is the conjugate function of f  x   the function f      is itself obtained as the
solution to a minimization problem 
t
f        min
x f  x   f  x  g 

   

   

fijaakkola   jordan

the formal identity of this pair of minimization problems expresses the  duality  of f and
its conjugate f   
the representation of f in eq      is known as a variational transformation  the parameter  is known as a variational parameter  if we relax the minimization and fix the the
variational parameter to an arbitrary value  we obtain an upper bound 

f  x   t x   f      

    

the bound is better for some values of the variational parameter than for others  and for a
particular value of  the bound is exact 
we also want to obtain lower bounds on conditional probabilities  a straightforward
way to obtain lower bounds is to again appeal to conjugate duality and to express functions in terms of a maximization principle  this representation  however  applies to convex
functions in the current paper we require lower bounds for concave functions  our concave functions  however  have a special form that allows us to exploit conjugatepduality in a
different way  in particular  we require bounds for functions of the form f  a   j zj    where
f is a concave function  where zj for i   f             ng are non negative variables  and where
a is a constant  the variables zj in this expression are effectively coupled the impact of
changing one variable is contingent on the settings of the remaining variables  we can use
jensen s inequality  however  to obtain a lower bound in which the variables are decoupled  
in particular 

f  a  

x

j

qj zqj  
j
j
x

qj f   a   zqj  
j
j

zj     f   a  

x

    
    

where the qj can be viewed as defining a probability distribution over the variables zj   the
variational parameter in this case is thep probability distribution q   the optimal setting
of this parameter is given by qj   zj   k zk   this is easily verified by substitution into
eq        and demonstrates that the lower bound is tight 

    variational upper and lower bounds for noisy or

let us now return to the problem of computing the posterior probabilities in the qmr
model  recall that it is the conditional probabilities corresponding to the positive findings
that need to be simplified  to this end  we write
p

p  fi  jd        e  i   

j ij dj

  e log   e x  

    

p

where x   i    j ij dj   consider the exponent f  x    log     e x    for noisy or  as
well as for many other conditional models involving compact representations  e g   logistic
regression   the exponent f  x  is a concave function of x  based on the discussion in the
p

p

   jensen s inequality  which states that f  a   j qj xj    j qj f  a   xj    for concave
f   where
p
and    qj     is a simple consequence of eq       where x is taken to be a   j qj xj  

   

p

qj

    

fivariational probabilistic inference and qmr dt

previous section  we know that there must exist a variational upper bound for this function
that is linear in x 

f  x   x   f    

    

using eq      to evaluate the conjugate function f      for noisy or  we obtain 

f         log           log      

    

the desired
bound is obtained by substituting into eq        and recalling the definition
x   i    pj ij dj   

p  fi  jd 

 




p

e f  i    pj ij dj  

e i  i   j ij dj   f  i 
p  fi  jd  i  

    
    
    

note that the  variational evidence  p  fi  jd  i  is the exponential of a term that is linear
in the disease vector d  just as with the negative findings  this implies that the variational
evidence can be incorporated into the posterior in time linear in the number of diseases
associated with the finding 
there is also a graphical way to understand the effect of the transformation  we rewrite
the variational evidence as follows 
p



p  fi jd  i    e i i    j ij dj   f  i 
id
yh
  e i i   f   i  e iij j  
j

    
    

note that the first term is a constant  and note moreover that the product is factorized
across the diseases  each of the latter factors can be multiplied with the pre existing
prior on the corresponding disease  possibly itself modulated by factors from the negative
evidence   the constant term can be viewed as associated with a delinked finding node fi  
indeed  the effect of the variational transformation is to delink the finding node fi from the
graph  altering the priors of the disease nodes that are connected to that finding node  this
graphical perspective will be important for the presentation of our variational algorithm 
we will be able to view variational transformations as simplifying the graph until a point
at which exact methods can be run 
we now turn
to the lower bounds on the conditional probabilities p  fi  jd   the expop
nent f  i    j ij dj   in the exponential representation is of the form to which we applied
jensen s inequality in the previous section  indeed  since f is concave we need only identify
the non negative variables zj   which in this case are ij dj   and the constant a  which is now
i   applying the bound in eq       we have 

p  fi  jd 

 

p

e f   i    j ij dj  

 e

ij dj 
j qjji f io   qjji

p



   

    
    

fijaakkola   jordan

  e
  e

h

p





ij
j qjji dj f io   qjji     dj   f   io  

i

i
ij 
j qjji dj f io   qjji  f   io    f   io  

p

h 

    

    

    
where we have allowed a different variational distribution qji for each finding  note that
once again the bound is linear in the exponent  as in the case of the upper bound  this
implies that the variational evidence can be incorporated into the posterior distribution in
time linear in the number of diseases  moreover  we can once again view the variational
transformation in terms of delinking the finding node fi from the graph 

p  fi  jd  qji 

    approximate inference for qmr

in the previous section we described how variational transformations are derived for individual findings in the qmr model  we now discuss how to utilize these transformations in
the context of an overall inference algorithm 
conceptually the overall approach is straightforward  each transformation involves
replacing an exact conditional probability of a finding with a lower bound and an upper
bound 
p  fi  jd  qji   p  fi  jd   p  fi  jd  i  
    
given that such transformations can be viewed as delinking the ith finding node from
the graph  we see that the transformations not only yield bounds  but also yield a simplified graphical structure  we can imagine introducing transformations sequentially until
the graph is sparse enough that exact methods become feasible  at that point we stop
introducing transformations and run an exact algorithm 
there is a problem with this approach  however  we need to decide at each step which
node to transform  and this requires an assessment of the effect on overall accuracy of
transforming the node  we might imagine calculating the change in a probability of interest
both before and after a given transformation  and choosing to transform that node that
yields the least change to our target probability  unfortunately we are unable to calculate
probabilities in the original untransformed graph  and thus we are unable to assess the effect
of transforming any one node  we are unable to get the algorithm started 
suppose instead that we work backwards  that is  we introduce transformations for
all of the findings  reducing the graph to an entirely decoupled set of nodes  we optimize
the variational parameters for this fully transformed graph  more on optimization of the
variational parameters below   for this graph inference is trivial  moreover  it is also easy
to calculate the effect of reinstating a single exact conditional at one node  we choose to
reinstate that node which yields the most change 
consider in particular the case of the upper bounds  lower bounds are analogous   each
transformation introduces an upper bound on a conditional probability p  fi  jd   thus the
likelihood of observing the  positive  findings p  f     is also upper bounded by its variational
counterpart p  f   j   
x
x
p  f       p  f   jd p  d   p  f   jd    p  d   p  f   j  
    
d

d

   

fivariational probabilistic inference and qmr dt

we can assess the accuracy of each variational transformation after introducing and optimizing the variational transformations for all the positive findings  separately for each
positive finding we replace the variationally transformed conditional probability p  fi  jd  i 
with the corresponding exact conditional p  fi  jd  and compute the difference between the
resulting bounds on the likelihood of the observations 

i   p  f   j     p  f   j n i  

    

where p  f   j n i   is computed without transforming the ith positive finding  the larger
the difference i is  the worse the ith variational transformation is  we should therefore
introduce the transformations in the ascending order of i s  put another way  we should
treat exactly  not transform  those conditional probabilities whose i measure is large 
in practice  an intelligent method for ordering the transformations is critical  figure  
compares the calculation of likelihoods based on the i measure as opposed to a method
that chooses the ordering of transformations at random  the plot corresponds to a representative diagnostic case  and shows the upper bounds on the log likelihoods of the observed
findings as a function of the number of conditional probabilities that were left intact  i e 
not transformed   note that the upper bound must improve  decrease  with fewer transformations  the results are striking the choice of ordering has a large effect on accuracy
 note that the plot is on a log scale  
  

loglikelihood

  
  
  
  
  
  
 

 

 
 
 
  
  of exactly treated findings

  

figure    the upper bound on the log likelihood for the delta method of removing transformations  solid line  and a method that bases the choice on a random ordering
 dashed line  
note also that the curve for the proposed ranking is convex  thus the bound improves
less the fewer transformations there are left  this is because we first remove the worst
transformations  replacing them with the exact conditionals  the remaining transformations are better as indicated by the delta measure and thus the bound improves less with
further replacements 
we make no claims for optimality of the delta method  it is simply a useful heuristic
that allows us to choose an ordering for variational transformations in a computationally
ecient way  note also that our implementation of the method optimizes the variational
parameters only once at the outset and chooses the ordering of further transformations
based on these fixed parameters  these parameters are suboptimal for graphs in which
   

fijaakkola   jordan

substantial numbers of nodes have been reinstated  but we have found in practice that this
simplified algorithm still produces reasonable orderings 
once we have decided which nodes to reinstate  the approximate inference algorithm
can be run  we introduce transformations at those nodes that were left transformed by the
ordering algorithm  the product of all of the exact conditional probabilities in the graph
with the transformed conditional probabilities yields an upper or lower bound on the overall
joint probability associated with the graph  the product of bounds is a bound   sums of
bounds are still bounds  and thus the likelihood  the marginal probability of the findings 
is bounded by summing across the bounds on the joint probability  in particular  an upper
bound on the likelihood is obtained via 
x
x
p  f       p  f   jd p  d   p  f   jd    p  d   p  f   j  
    
d

d

d

d

dndj

dndj

and the corresponding lower bound on the likelihood is obtained similarly 
x
x
p  f       p  f  jd p  d   p  f   jd  q  p  d   p  f   jq  

    

in both cases we assume that the graph has been suciently simplified by the variational
transformations so that the sums can be performed eciently 
the expressions in eq       and eq       yield upper and lower bounds for arbitrary
values of the variational parameters  and q   we wish to obtain the tightest possible bounds 
thus we optimize these expressions with respect to  and q   we minimize with respect to
 and maximize with respect to q  appendix   discusses these optimization problems in
detail  it turns out that the upper bound is convex in the  and thus the adjustment of the
variational parameters for the upper bound reduces to a convex optimization problem that
can be carried out eciently and reliably  there are no local minima   for the lower bound
it turns out that the maximization can be carried out via the em algorithm 
finally  although bounds on the likelihood are useful  our ultimate goal is to approximate
the marginal posterior probabilities p  dj jf      there are two basic approaches to utilizing
the variational bounds in eq       and eq       for this purpose  the first method  which will
be our emphasis in the current paper  involves using the transformed probability model  the
model based either on upper or lower bounds  as a computationally ecient surrogate for the
original probability model  that is  we tune the variational parameters of the transformed
model by requiring that the model give the tightest possible bound on the likelihood  we
then use the tuned transformed model as an inference engine to provide approximations to
other probabilities of interest  in particular the marginal posterior probabilities p  dj jf     
the approximations found in this manner are not bounds  but are computationally ecient
approximations  we provide empirical data in the following section that show that this
approach indeed yields good approximations to the marginal posteriors for the qmr dt
network 
a more ambitious goal is to obtain interval bounds for the marginal posterior probabilities themselves  to this end  let p  f     dj j   denote the combined event that the qmr dt
model generates the observed findings f   and that the j th disease takes the value dj   these
bounds follow directly from 
x
x
p  f     dj     p  f   jd p  d   p  f   jd    p  d   p  f     dj j 
    
   

fivariational probabilistic inference and qmr dt

where p  f   jd     is a product of upper bound transformed conditional probabilities and
exact  untransformed  conditionals  analogously we can compute a lower bound p  f     dj jq  
by applying the lower bound transformations 

p  f     dj    

x

dndj

p  f  jd p  d  

x

dndj

p  f   jd  q  p  d   p  f     dj jq  

    

combining these bounds we can obtain interval bounds on the posterior marginal probabilities for the diseases  cf  draper   hanks       

p  f    dj j 
p  f     dj jq 
   

p
 
d
j
f
j
p  f    dj j    p  f     dj jq  
p  f     dj j     p  f     dj jq   
where dj is the binary complement of dj  

    

   experimental evaluation

the diagnostic cases that we used in evaluating the performance of the variational techniques were cases abstracted from clinocopathologic conference   cpc   cases  these cases
generally involve multiple diseases and are considered to be clinically dicult cases  they
are the cases in which middleton et al         did not find their importance sampling method
to work satisfactorily 
our evaluation of the variational methodology consists of three parts  in the first part
we exploit the fact that for a subset of the cpc cases    of the    cases  there are a
suciently small number of positive findings that we can calculate exact values of the
posterior marginals using the quickscore algorithm  that is  for these four cases we were
able to obtain a  gold standard  for comparison  we provide an assessment of the accuracy
and eciency of variational methods on those four cpc cases  we present variational
upper and lower bounds on the likelihood as well as scatterplots that compare variational
approximations of the posterior marginals to the exact values  we also present comparisons
with the likelihood weighted sampler of shwe and cooper        
in the second section we present results for the remaining  intractable cpc cases  we
use lengthy runs of the shwe and cooper sampling algorithm to provide a surrogate for the
gold standard in these cases 
finally  in the third section we consider the problem of obtaining interval bounds on
the posterior marginals 

    comparison to exact marginals

four of the cpc cases have    or fewer positive findings  see table     and for these cases
it is possible to calculate the exact values of the likelihood and the posterior marginals
in a reasonable amount of time  we used heckerman s  quickscore  algorithm  heckerman       an algorithm tailored to the qmr dt architecture to perform these exact
calculations 
figure   shows the log likelihood for the four tractable cpc cases  the figure also shows
the variational lower and upper bounds  we calculated the variational bounds twice  with
differing numbers of positive findings treated exactly in the two cases   treated exactly 
   

fijaakkola   jordan

case   of pos  findings   of neg  findings
 
  
  
 
  
  
 
  
  
 
  
  

  

  

  

  

  

loglikelihood

loglikelihood

table    description of the cases for which we evaluated the exact posterior marginals 

  

  

  

  
  
  
  

 a 

 

 

 
 
sorted cases

 

 b 

 

 

 

 
 
sorted cases

 

 

figure    exact values and variational upper and lower bounds on the log likelihood
log p  f   j   for the four tractable cpc cases  in  a    positive findings were
treated exactly  and in  b     positive findings were treated exactly 
simply means that the finding is not transformed variationally   in panel  a  there were  
positive findings treated exactly  and in  b     positive findings were treated exactly  as
expected  the bounds were tighter when more positive findings were treated exactly  
the average running time across the four tractable cpc cases was      seconds for
the exact method       seconds for the variational method with   positive findings treated
exactly  and      seconds for the variational method with    positive findings treated exactly 
 these results were obtained on a     mhz dec alpha computer  
although the likelihood is an important quantity to approximate  particularly in applications in which parameters need to be estimated   of more interest in the qmr dt setting
are the posterior marginal probabilities for the individual diseases  as we discussed in the
previous section  the simplest approach to obtaining variational estimates of these quantities is to define an approximate variational distribution based either on the distribution
p  f   j    which upper bounds the likelihood  or the distribution p  f   jq    which lowerbounds the likelihood  for fixed values of the variational parameters  chosen to provide
a tight bound to the likelihood   both distributions provide partially factorized approximations to the joint probability distribution  these factorized forms can be exploited as
   given that a significant fraction of the positive findings are being treated exactly in these simulations  one
may wonder what if any additional accuracy is due to the variational transformations  we address this
concern later in this section and demonstrate that the variational transformations are in fact responsible
for a significant portion of the accuracy in these cases 

   

fivariational probabilistic inference and qmr dt

 

 

   

   
variational estimates

variational estimates

ecient approximate inference engines for general posterior probabilities  and in particular
we can use them to provide approximations to the posterior marginals of individual diseases 
in practice we found that the distribution p  f   j   yielded more accurate posterior
marginals than the distribution p  f   jq    and we restrict our presentation to p  f   j    figure   displays a scatterplot of these approximate posterior marginals  with panel  a  corre 

   

   

   

 a 

 
 

   

   

   

   

   
   
exact marginals

   

 b 

 

 
 

   

   
   
exact marginals

   

 

figure    scatterplot of the variational posterior estimates and the exact marginals  in
 a    positive findings were treated exactly and in  b     positive findings were
treated exactly 
sponding to the case in which   positive findings were treated exactly and panel  b  the case
in which    positive findings treated exactly  the plots were obtained by first extracting
the    highest posterior marginals from each case using exact methods and then computing
the approximate posterior marginals for the corresponding diseases  if the approximate
marginals are in fact correct then the points in the figures should align along the diagonals
as shown by the dotted lines  we see a reasonably good correspondence the variational
algorithm appears to provide a good approximation to the largest posterior marginals   we
quantify this correspondence with a ranking measure later in this section  
a current state of the art algorithm for the qmr dt is the enhanced version of likelihoodweighted sampling proposed by shwe and cooper         likelihood weighted sampling is
a stochastic sampling method proposed by fung and chang        and shachter and peot
        likelihood weighted sampling is basically a simple forward sampling method that
weights samples by their likelihoods  it can be enhanced and improved by utilizing  selfimportance sampling   see shachter   peot         a version of importance sampling in
which the importance sampling distribution is continually updated to reect the current
estimated posterior distribution  middleton et al         utilized likelihood weighted sampling with self importance sampling  as well as a heuristic initialization scheme known as
 iterative tabular bayes   for the qmr dt model and found that it did not work satisfactorily  subsequent work by shwe and cooper         however  used an additional
enhancement to the algorithm known as  markov blanket scoring   see shachter   peot 
       which distributes fractions of samples to the positive and negative values of a node
in proportion to the probability of these values conditioned on the markov blanket of the
node  the combination of markov blanket scoring and self importance sampling yielded
   

fijaakkola   jordan

an effective algorithm   in particular  with these modifications in place  shwe and cooper
reported reasonable accuracy for two of the dicult cpc cases 
we re implemented the likelihood weighted sampling algorithm of shwe and cooper 
incorporating the markov blanket scoring heuristic and self importance sampling   we did
not utilize  iterative tabular bayes  but instead utilized a related initialization scheme 
 heuristic tabular bayes  also discussed by shwe and cooper   in this section we discuss
the results of running this algorithm on the four tractable cpc cases  comparing to the
results of variational inference   in the following section we present a fuller comparative
analysis of the two algorithms for all of the cpc cases 
likelihood weighting sampling  and indeed any sampling algorithm  realizes a timeaccuracy tradeoff taking additional samples requires more time but improves accuracy 
in comparing the sampling algorithm to the variational algorithm  we ran the sampling
algorithm for several different total time periods  so that the accuracy achieved by the
sampling algorithm roughly covered the range achieved by the variational algorithm  the
results are shown in figure    with the right hand curve corresponding to the sampling runs 
the figure displays the mean correlations between the approximate and exact posterior
marginals across ten independent runs of the algorithm  for the four tractable cpc cases  
 

mean correlation

    
    
    
    
   
    
      
  

 

 

  
  
execution time in seconds

 

  

figure    the mean correlation between the approximate and exact posterior marginals as
a function of the execution time  in seconds   solid line  variational estimates 
dashed line  likelihood weighting sampling  the lines above and below the sampling result represent standard errors of the mean based on the ten independent
runs of the sampler 
variational algorithms are also characterized by a time accuracy tradeoff  in particular 
the accuracy of the method generally improves as more findings are treated exactly  at
the cost of additional computation  figure   also shows the results from the variational
algorithm  the left hand curve   the three points on the curve correspond to up to       and
   the initialization method proved to have little effect on the inference results 
   we also investigated gibbs sampling  pearl         the results from gibbs sampling were not as good
as the results from likelihood weighted sampling  and we report only the latter results in the remainder
of the paper 

   

fivariational probabilistic inference and qmr dt

   positive findings treated exactly  note that the variational estimates are deterministic
and thus only a single run was made 
the figure shows that to achieve roughly equivalent levels of accuracy  the sampling
algorithm requires significantly more computation time than the variational method 
although scatterplots and correlation measures provide a rough indication of the accuracy of an approximation algorithm  they are deficient in several respects  in particular  in
diagnostic practice the interest is in the ability of an algorithm to rank diseases correctly 
and to avoid both false positives  diseases that are not in fact significant but are included
in the set of highly ranked diseases  and false negatives  significant diseases that are omitted from the set of highly ranked diseases   we defined a ranking measure as follows  see
also middleton et al          consider a set of the n highest ranking disease hypotheses 
where the ranking is based on the correct posterior marginals  corresponding to this set
of diseases we can find the smallest set of n   approximately ranked diseases that includes
the n significant ones  in other words  for any n  true positives  an approximate method
produces n     n  false positives   plotting false positives as a function of true positives
provides a meaningful and useful measure of the accuracy of an approximation scheme 
to the extent that a method provides a nearly correct ranking of true positives the plot
increases slowly and the area under the curve is small  when a significant disease appears
late in the approximate ordering the plot increases rapidly near the true rank of the missed
disease and the area under the curve is large 
we also plot the number of  false negatives  in a set of top n highly ranked diseases 
false negatives refer to the number of diseases  out of the n highest ranking diseases 
that do not appear in the set of n approximately ranked diseases  note that unlike the
previous measure  this measure does not reveal the severity of the misplacements  only their
frequency 
with this improved diagnostic measure in hand  let us return to the evaluation of the
inference algorithms  beginning with the variational algorithm  figure   provides plots of
  

 

  

 

false negatives

false positives

 
  
  
  

 
 
 

  

 a 

 
 

 

  

  
  
true positives

  

 b 

  

 
 

  

  
  
approximate ranking

  

  

figure     a  average number of false positives as a function of true positives for the variational method  solid lines  and the partially exact method  dashed line    b  false
negatives in the set of top n approximately ranked diseases  in both figures  
positive findings were treated exactly 
the false positives  panel a  and false negatives  panel b  against the true positives for the
   

fijaakkola   jordan

  

 

  

   

 a 

 
false negatives

false positives

  
  
  
  

   
 
   

  

 

 

   

 
 

  

  
  
true positives

  

 b 

  

 
 

  

  
  
approximate ranking

  

  

figure     a  average number of false positives as a function of true positives for the variational method  solid line  and the partially exact method  dashed line    b  false
negatives in the set of top n approximately ranked diseases  in both figures   
positive findings were treated exactly 
tractable cpc cases  eight positive findings were treated exactly in the simulation shown
in this figure  figure   displays the results when    positive finding were treated exactly 
as we noted earlier    and    positive findings comprise a significant fraction of the
total positive findings for the tractable cpc cases  and thus it is important to verify that
the variational transformations are in fact contributing to the accuracy of the posterior
approximations above and beyond the exact calculations  we did this by comparing the
variational method to a method which we call the  partially exact  method in which the
posterior probabilities were obtained using only those findings that were treated exactly in
the variational calculations  i e   using only those findings that were not transformed   if
the variational transformations did not contribute to the accuracy of the approximation 
then the performance of the partially exact method should be comparable to that of the
variational method   figure   and figure   clearly indicate that this is not the case  the
difference in accuracy between these methods is substantial while their computational load
is comparable  about     seconds on a    mhz dec alpha  
we believe that the accuracy portrayed in the false positive plots provides a good indication of the potential of the variational algorithm for providing a practical solution to
the approximate inference problem for the qmr dt  as the figures show  the number of
false positives grows slowly with the number of true positives  for example  as shown in
figure   where eight positive findings are treated exactly  to find the    most likely diseases
we would only need to entertain the top    diseases in the list of approximately ranked
diseases  compared to more than    for the partially exact method  
the ranking plot for the likelihood weighted sampler is shown in figure    with the
curve for the variational method from figure   included for comparison  to make these
plots  we ran the likelihood weighted sampler for an amount of time       seconds  that was
   it should be noted that this is a conservative comparison  because the partially exact method in fact
benefits from the variational transformation the set of exactly treated positive findings is selected on
the basis of the accuracy of the variational transformations  and these accuracies correlate with the
diagnostic relevance of the findings 

   

fivariational probabilistic inference and qmr dt

  
  

false positives

  
  
  
  
  
 
 
 

  

  
  
true positives

  

  

figure    average number of false positives as a function of true positives for the likelihoodweighted sampler  dashed line  and the variational method  solid line  with   
positive findings treated exactly 
comparable to the time allocated to our slowest variational method       seconds  this was
the case in which    positive findings were treated exactly  recall that the time required
for the variational algorithm with    positive findings treated exactly was      seconds   as
the plots show  for these tractable cpc cases  the variational method is significantly more
accurate than the sampling algorithm for comparable computational loads 

    the full cpc corpus

we now consider the full cpc corpus  the majority of these cases     of    cases   have
more than    positive findings and thus appear to be beyond the reach of exact methods 
an important attraction of sampling methods is the mathematical guarantee of accurate
estimates in the limit of a suciently large sample size  gelfand   smith         thus
sampling methods have the promise of providing a general methodology for approximate
inference  with two caveats      the number of samples that is needed can be dicult to
diagnosis  and     very many samples may be required to obtain accurate estimates  for
real time applications  the latter issue can rule out sampling solutions  however  long term
runs of a sampler can still provide a useful baseline for the evaluation of the accuracy of faster
approximation algorithms  we begin by considering this latter possibility in the context of
likelihood weighted sampling for the qmr dt  we then turn to a comparative evaluation
of likelihood weighted sampling and variational methods in the time limited setting 
to explore the viability of the likelihood weighted sampler for providing a surrogate for
the gold standard  we carried out two independent runs each consisting of         samples 
figure   a  shows the estimates of the log likelihood from the first sampling run for all
of the cpc cases  we also show the variational upper and lower bounds for these cases
 the cases have been sorted according to the lower bound   note that these bounds are
rigorous bounds on the true log likelihood  and thus they provide a direct indication of the
accuracy of the sampling estimates  although we see that many of the estimates lie between
the bounds  we also see in many cases that the sampling estimates deviate substantially
from the bounds  this suggests that the posterior marginal estimates obtained from these
samples are likely to be unreliable as well  indeed  figure   b  presents a scatterplot of
   

fijaakkola   jordan

  
 

  
   
sampling estimates  

loglikelihood

  
  
   
   
   
   

   

   

   

   

 a 

   
 

  

  
  
sorted cases

  

  

 b 

 
 

   

   
   
sampling estimates  

   

 

figure     a  upper and lower bounds  solid lines  and the corresponding sampling estimates  dashed line  of the log likelihood of observed findings for the cpc cases 
 b  a correlation plot between the posterior marginal estimates from two independent sampling runs 
estimated posterior marginals for the two independent runs of the sampler  although we
see many cases in which the results lie on the diagonal  indicating agreement between the
two runs  we also see many pairs of posterior estimates that are far from the diagonal 
these results cast some doubt on the viability of the likelihood weighted sampler as a
general approximator for the full set of cpc cases  even more problematically we appear
to be without a reliable surrogate for the gold standard for these cases  making it dicult
to evaluate the accuracy of real time approximations such as the variational method  note 
however  that the estimates in figure   a  seem to fall into two classes estimates that
lie within the variational bounds and estimates that are rather far from the bounds  this
suggests the possibility that the distribution being sampled from is multi modal  with some
estimates falling within the correct mode and providing good approximations and with
others falling in spurious modes and providing seriously inaccurate approximations  if the
situation holds  then an accurate surrogate for the gold standard might be obtained by using
the variational bounds to filter the sampling results and retaining only those estimates that
lie between the bounds given by the variational approach 
figure    provides some evidence of the viability of this approach  in    out of the   
cpc cases both of the independent runs of the sampler resulted in estimates of the loglikelihood lying approximately within the variational bounds  we recomputed the posterior
marginal estimates for these selected cases and plotted them against each other in the figure 
the scatterplot shows a high degree of correspondence of the posterior estimates in these
cases  we thus tentatively assume that these estimates are accurate enough to serve as a
surrogate gold standard and proceed to evaluate the real time approximations 
figure    plots the false positives against the true positives on the    selected cpc
cases for the variational method  twelve positive findings were treated exactly in this
simulation  obtaining the variational estimates took      seconds of computer time per
case  although the curve increases more rapidly than with the tractable cpc cases  the
variational algorithm still appears to provide a reasonably accurate ranking of the posterior
marginals  within a reasonable time frame 
   

fivariational probabilistic inference and qmr dt

 

sampling estimates  

   

   

   

   

 
 

   

   
   
sampling estimates  

   

 

figure     a correlation plot between the selected posterior marginal estimates from two
independent sampling runs  where the selection was based on the variational
upper and lower bounds 
  
  

false positives

  
  
  
  
  
 
 

  

  
  
true positives

  

  

figure     average number of false positives as a function of true positives for the variational method  solid line  and the likelihood weighted sampler  dashed line  
for the variational method    positive findings were treated exactly  and for the
sampler the results are averages across ten runs 
to compare the variational algorithm to a time limited version of the likelihood weighted
sampler we ran the latter algorithm for a period of time       seconds per case  roughly comparable to the running time of the variational algorithm       seconds per case   figure   
shows the corresponding plot of false positives against true positives  where we have averaged over ten independent runs  we see that the curve increases significantly more steeply
than the variational curve  to find the    most likely diseases with the variational method
we would only need to entertain the top    diseases in the list of approximately ranked
diseases  for the sampling method we would need to entertain the top    approximately
ranked diseases 

    interval bounds on the marginal probabilities

thus far we have utilized the variational approach to produce approximations to the posterior marginals  the approximations that we have discussed originate from upper and lower
   

fijaakkola   jordan

bounds on the likelihood  but they are not themselves bounds  that is  they are not guaranteed to lie above or below the true posteriors  as we see in figure    as we discussed in
section      however  it is also possible to induce upper and lower bounds on the posterior
marginals from upper and lower bounds on the likelihood  cf  eq       in this section we
evaluate these interval bounds for the qmr dt posterior marginals 
figure    displays histogram of the interval bounds for the four tractable cpc cases  the
   selected cpc cases from the previous section  and all of the cpc cases  these histograms
include all of the diseases in the qmr dt network  in the case of the tractable cases the
   

   

   

   

   

   

 
 

   

   

   

 a 

frequency

 

frequency

 

frequency

 

   

   

   

   

   

interval size

   

 

 b 

 
 

   

   

   

   

interval size

   

 

 c 

 
 

   

   

   

   

interval size

figure     histograms of the size of the interval bounds on all of the diseases in the qmrdt network for  a  the four tractable cpc cases   b  the    selected cpc cases
from the previous section  and  c  all of the cpc cases 
variational method was run with    positive findings treated exactly  for the remaining
cpc cases the variational method was run with    positive findings treated exactly  the
running time of the algorithm was less than    seconds of computer time per cpc case 
for the tractable cpc cases  the interval bounds are tight for nearly all of the diseases
in the network  however      few of the positive findings are treated variationally in these
cases  and     there is no need in practice to compute variational bounds for these cases 
we get a somewhat better picture of the viability of the variational interval bounds in
figure    b  and figure    c   and the picture is decidedly mixed  for the    selected
cases  tight bounds are provided for approximately half of the diseases  the bounds are
vacuous for approximately a quarter of the diseases  and there are a range of diseases in
between  when we consider all of the cpc cases  approximately a third of the bounds are
tight and nearly half are vacuous 
although these results may indicate limitations in our variational approximation  there
is another more immediate problem that appears to be responsible for the looseness of
the bounds in many cases  in particular  recall that we use the quickscore algorithm
 heckerman        to handle the exact calculations within the framework of our variational
algorithm  unfortunately quickscore suffers from vanishing numerical precision for large
numbers of positive findings  and in general we begin to run into numerical problems 
resulting in vacuous bounds  when    positive findings are incorporated exactly into the
variational approximation  thus  although it is clearly of interest to run the variational
algorithm for longer durations  and thereby improve the bounds  we are unable to do so
within our current implementation of the exact subroutine 
   

 

fivariational probabilistic inference and qmr dt

while it is clearly worth studying methods other than quickscore for treating the exact findings within the variational algorithm  it is also of interest to consider combining
variational methods with other methods  such as search based or other partial evaluation
methods  that are based on intervals  these methods may help in simplifying the posterior
and obviating the need for improving the exact calculations 
it is also worth emphasizing the positive aspect of these results and their potential
practical utility  the previous section showed that the variational method can provide accurate approximations to the posterior marginals  combined with the interval bounds in
this section which are calculated eciently the user can obtain guarantees on approximately a third of these approximations  given the relatively benign rate of increase in false
positives as a function of true positives  figure      such guarantees may suce  finally 
for diseases in which the bounds are loose there are also perturbation methods available
 jaakkola        that can help to validate the approximations for these diseases 

   discussion
let us summarize the variational inference method and evaluate the results that we have
obtained 
the variational method begins with parameterized upper and lower bounds on the individual conditional probabilities at the nodes of the model  for the qmr dt  these bounds
are exponentials of linear functions  and introducing them into the model corresponds to
delinking nodes from the graph  sums of products of these bounds yield bounds  and thus
we readily obtain parameterized bounds on marginal probabilities  in particular upper and
lower bounds on the likelihood 
we exploited the likelihood bounds in evaluating the output of the likelihood weighted
sampling algorithm  although the sampling algorithm did not yield reliable results across
the corpus of cpc cases  when we utilized the variational upper and lower bounds to select
among the samples we were able to obtain sampling results that were consistent between
runs  this suggests a general procedure in which variational bounds are used to assess the
convergence of a sampling algorithm   one can also imagine a more intimate relationship
between these algorithms in which the variational bounds are used to adjust the on line
course of the sampler  
the fact that we have bounds on the likelihood  or other marginal probabilities  is
critical the bounding property allows us to find optimizing values of the variational parameters by minimizing the upper bounding variational distribution and maximizing the
lower bounding variational distribution  in the case of the qmr dt network  a bipartite noisy or graph   the minimization problem is a convex optimization problem and the
maximization problem is solved via the em algorithm 
once the variational parameters are optimized  the resulting variational distribution can
be exploited as an inference engine for calculating approximations to posterior probabilities 
this technique has been our focus in the paper  graphically  the variationally transformed
model can be viewed as a sub graph of the original model in which some of the finding
nodes have been delinked  if a sucient number of findings are delinked variationally
then it is possible to run an exact algorithm on the resulting graph  this approach yields
approximations to the posterior marginals of the disease nodes 
   

fijaakkola   jordan

we found empirically that these approximations appeared to provide good approximations to the true posterior marginals  this was the case for the tractable set of cpc cases
 cf  figure    and subject to our assumption that we have obtained a good surrogate for
the gold standard via the selected output of the sampler also the case for the full cpc
corpus  cf  figure     
we also compared the variational algorithm to a state of the art algorithm for the qmrdt  the likelihood weighted sampler of shwe and cooper         we found that the variational algorithm outperformed the likelihood weighted sampler both for the tractable cases
and for the full corpus  in particular  for a fixed accuracy requirement the variational algorithm was significantly faster  cf  figure     and for a fixed time allotment the variational
algorithm was significantly more accurate  cf  figure   and figure     
our results were less satisfactory for the interval bounds on the posterior marginals 
across the full cpc corpus we found that for approximately one third of the disease the
bounds were tight but for half of the diseases the bounds were vacuous  a major impediment
to obtaining tighter bounds appears to lie not in the variational approximation per se but
rather in the exact subroutine  and we are investigating exact methods with improved
numerical properties 
although we have focused in detail on the qmr dt model in this paper  it is worth
noting that the variational probabilistic inference methodology is considerably more general 
specifically  the methods that we have described here are not limited to the bi partite
graphical structure of the qmr dt model  nor is it necessary to employ noisy or nodes
 jaakkola   jordan         it is also the case that the type of transformations that we
have exploited in the qmr dt setting extend to a larger class of dependence relations
based on generalized linear models  jaakkola         finally  for a review of applications of
variational methods to a variety of other graphical model architectures  see jordan  et al 
       
a promising direction for future research appears to be in the integration of various
kinds of approximate and exact methods  see  e g   dagum   horvitz        jensen  kong 
  kjrulff         in particular  search based methods  cooper        peng   reggia 
      henrion        and variational methods both yield bounds on probabilities  and  as
we have indicated in the introduction  they seem to exploit different aspects of the structure of complex probability distributions  it may be possible to combine the bounds from
these algorithm the variational bounds might be used to guide the search  or the searchbased bounds might be used to aid the variational approximation  similar comments can
be made with respect to localized partial evaluation methods and bounded conditioning
methods  draper   hanks        horvitz  et al          also  we have seen that variational
bounds can be used for assessing whether estimates from monte carlo sampling algorithms
have converged  a further interesting hybrid would be a scheme in which variational approximations are refined by treating them as initial conditions for a sampler 
even without extensions our results in this paper appear quite promising  we have
presented an algorithm which runs in real time on a large scale graphical model for which
exact algorithms are in general infeasible  the results that we have obtained appear to
be reasonably accurate across a corpus of dicult diagnostic cases  while further work
is needed  we believe that our results indicate a promising role for variational inference in
developing  critiquing and exploiting large scale probabilistic models such as the qmr dt 
   

fivariational probabilistic inference and qmr dt

acknowledgements
we would like to thank the university of pittsburgh and randy miller for the use of the
qmr dt database  we also want to thank david heckerman for suggesting that we attack
qmr dt with variational methods  and for providing helpful counsel along the way 

appendix a  duality
the upper and lower bounds for individual conditional probability distributions that form
the basis of our variational method are based on the  dual  or  conjugate  representations
of convex functions  we present a brief description of convex duality in this appendix  and
refer the reader to rockafellar        for a more extensive treatment 
let f  x  be a real valued  convex function defined on a convex set x  for example 
x   rn    for simplicity of exposition  we assume that f is a well behaved  differentiable 
function  consider the graph of f   i e   the points  x  f  x   in an n     dimensional space 
the fact that the function f is convex translates into convexity of the set f x  y     y  f  x g
called the epigraph of f and denoted by epi f    figure      it is an elementary property
f x 
epi f 

x    y   f      

x    y   f      

x y  

figure     half spaces containing the convex set epi f    the conjugate function f     
defines the critical half spaces whose intersection is epi f    or  equivalently  it
defines the tangent planes of f  x  
of convex sets that they can be represented as the intersection of all the half spaces that
contain them  see figure      through parameterizing these half spaces we obtain the dual
representations of convex functions  to this end  we define a half space by the condition 
all  x  y   such that xt    y      

    

where  and  parameterize all  non vertical  half spaces  we are interested in characterizing the half spaces that contain the epigraph of f   we require therefore that the points
in the epigraph must satisfy the half space condition  for  x  y     epi f    we must have
xt    y        this holds whenever xt    f  x        as the points in the epigraph have
the property that y  f  x   since the condition must be satisfied by all x   x   it follows
   

fijaakkola   jordan

that
max
f xt    f  x     g    
x x

    

as well  equivalently 

  max
f xt    f  x  g
x x

    

where the right hand side of this equation defines a function of    which is known as the
 dual  or  conjugate  function f      this function  which is also a convex function  defines
the critical half spaces which are needed for the representation of epi f   as an intersection
of half spaces  figure     
to clarify the duality between f  x  and f   x   let us drop the maximum and rewrite
the inequality as 

xt   f  x    f     

    

in this equation  the roles of the two functions are interchangeable and we may suspect that
also f  x  can obtained from the dual function f   x  by an optimization procedure  this is
in fact the case and we have 

f  x    max
f xt    f    g
 

    

this equality states that the dual of the dual gives back the original function  it provides
the computational tool for calculating dual functions 
for concave  convex down  functions the results are analogous  we replace max with
min  and lower bounds with upper bounds 

appendix b  optimization of the variational parameters

the variational method that we have described involves replacing selected local conditional
probabilities with either upper bounding or lower bounding variational transformations 
because the product of bounds is a bound  the variationally transformed joint probability
distribution is a bound  upper or lower  on the true joint probability distribution  moreover  because sums of bounds is a bound on the sum  we can obtain bounds on marginal
probabilities by marginalizing the variationally transformed joint probability distribution 
in particular  this provides a method for obtaining bounds on the likelihood  the marginal
probability of the evidence  
note that the variationally transformed distributions are bounds for arbitrary values of
the variational parameters  because each individually transformed node conditional probability is a bound for arbitrary values of its variational parameter   to obtain optimizing
values of the variational parameters  we take advantage of the fact that our transformed
distribution is a bound  and either minimize  in the case of upper bounds  or maximize
 in the case of lower bounds  the transformed distribution with respect to the variational
parameters  it is this optimization process which provides a tight bound on the marginal
probability of interest  e g   the likelihood  and thereby picks out a particular variational
distribution that can subsequently be used for approximate inference 
   

fivariational probabilistic inference and qmr dt

in this appendix we discuss the optimization problems that we must solve in the case
of noisy or networks  we consider the upper and lower bounds separately  beginning with
the upper bound 

upper bound transformations

our goal ispto compute a tight upper bound on the likelihood of the observed findings 
p  f       d p  f   jd p  d   as discussed in section      we obtain an upper bound on
p  f   jd  by introducing upper bounds for individual node conditional probabilities  we
represent this upper bound as p  f   jd      which is a product across the individual variational transformations and may contain contributions due to findings that are being treated
exactly  i e   are not transformed   marginalizing across d we obtain a bound 

p  f     

x

d

p  f   jd    p  d   p  f   j  

    

it is this latter quantity that we wish to minimize with respect to the variational parameters
 
to simplify the notation we assume that the first m positive findings have been transformed  and therefore need to be optimized  while the remaining conditional probabilities
will be treated exactly  in this notation p  f   j   is given by

p  f   j  

 

 

 
x y
 

  

 
y
y
 
 
p  fi jd  i  
p  fi jd  p  dj  
i m
j
d im
 
 
 y
 
e   p  fi  jd  i    
im

    
    

where the expectation is taken with respect to the posterior distribution for the diseases
given those positive findings that we plan to treat exactly  note that the proportionality
constant does not depend on the variational parameters  it is the likelihood of the exactly
treated positive findings   we now insert the explicit forms of the transformed conditional
probabilities  see eq        into eq       and find 

p  f   j   
 

 
 
  y     p  d   f       
i
e   e i i  j ij j
 
im
 p

p

e im  i i   f  i   e e j  im i ij dj

    
    

where we have simply converted the products over i into sums in the exponent and pulled
out the terms that are constants with respect to the expectation  on a log scale  the
proportionality becomes an equivalence up to a constant 
 p

x


d
i
ij
j
 

j i

m
log p  f j     c    i i    f  i     log e e
im

   

    

fijaakkola   jordan

several observations are in order  recall that f  i   is the conjugate of the concave function
f  the exponent   and is therefore also concave  for this reason  f  i   is convex  in
appendix c we prove that the remaining term 
 p

log e e

j im iij dj



    

is also a convex function of the variational parameters  now  since any sum of convex
functions is convex  we conclude that log p  f   j   is a convex function of the variational
parameters  this means that there are no local minima in our optimization problem  we
may safely employ the standard newton raphson procedure to solve r log p  f   j       
alternatively we can utilize fixed point iterations  in particular  we calculate the derivatives
of the variational form and iteratively solve for the individual variational parameters k such
that the derivatives are zero  the derivatives are given as follows 
 

 

  log p  f   j       log k   e  x  d  
k 
kj j  
 
 k
    k
j

    

    log p  f   j            var  x  d    
  j kj j  
    k
k     k

    

 

 

where the expectation and the variance are with respect to the posterior approximation
p  djf         and both derivatives can be computed in time linear in the number of associated diseases for the finding  the benign scaling of the variance calculations comes from
exploiting the special properties of the noisy or dependence and the marginal independence
of the diseases 
calculating the expectations in eq      is exponentially costly in the number of exactly
treated positive findings  when there are a large number of positive findings  we can have
recourse to a simplified procedure in which we optimize variational parameters after having
transformed all or most of the positive findings  while the resulting variational parameters
are suboptimal  we have found in practice that the incurred loss in accuracy is typically quite
small  in the simulations reported in the paper  we optimized the variational parameters
after approximately half of the exactly treated findings had been introduced   to be precise 
in the case of       and    total findings treated exactly  we optimized the parameters after
      and   findings  respectively  were introduced  

lower bound transformations

mimicking the case of upper bounds  we replace individual conditional probabilities of
the findings with lower bounding transformations  resulting in a lower bounding expression
p  f   jd  q   taking the product with p  d  and marginalizing over d yields a lower bound
on the likelihood 
x
p  f      p  f   jd  q  p  d   p  f   jq  
    
d

we wish to maximize p  f   jq   with respect to the variational parameters q to obtain the
tightest possible bound 
   

fivariational probabilistic inference and qmr dt

our problem can be mapped onto a standard optimization problem in statistics  in
particular  treating d as a latent variable  f as an observed variable  and q as a parameter
vector  the optimization of p  f   jq    or its logarithm  can be viewed as a standard maximum
likelihood estimation problem for a latent variable model  it can be solved using the em
algorithm  dempster  laird    rubin         the algorithm yields a sequence of variational
parameters that monotonically increase the objective function log p  f   jq    within the em
framework  we obtain an update of the variational parameters by maximizing the expected
complete log likelihood 


 

e log p  f   jd  q  p  d   

x

i

n

o

e log p  fi  jd  qji    constant 

    

where q old denotes the vector of variational parameters before the update  where the constant term is independent of the variational parameters q and where the expectation is with
respect to the posterior distribution p  djf     q old     p  f   jd  q old p  d   since the variational
parameters associated with the conditional probabilities p  fi  jd  qji  are independent of one
another  we can maximize each term in the above sum separately  recalling the form of the
variational transformation  see eq         we have 
 

 

 

e
 
qjji e fdjg f io   qij   f   io  
j ji
j
 f   io  
    
which we are to maximize with respect to qj ji while keeping the expectations e fdj g fixed 
n

log p  fi  jd  qji 

o

x

this optimization problem can be solved iteratively and monotonically by performing the
following synchronous updates with normalization 

qj ji

 

 

 

e fdj g qjji f io   qij   ij f   io   qij   qjji f   io  
j ji
j ji

 

    

where f   denotes the derivative of f    the update is guaranteed to be non negative  
this algorithm can be easily extended to handle the case where not all the positive
findings have been transformed  the only new feature is that some of the conditional
probabilities in the products p  f   jd  q old  and p  f   jd  q   have been left intact  i e   not
transformed  the optimization with respect to the variational parameters corresponding to
the transformed conditionals proceeds as before 

appendix c  convexity

the purpose of this appendix is to demonstrate that the function 
 p

log e e

j im iij dj



    

is a convex function of the variational parameters i   we note first that
ane transformap
tions do not change convexity properties  thus convexity in x   j im i ij dj implies
   

fijaakkola   jordan

convexity in the variational parameters    it remains to show that
n

o

log e e x   log

x

i

pi e xi   f  x   

    

is a convex function of the vector x    fx       xn gt   here we have indicated the discrete
values in the range of the random variable x by xi and denoted the probability measure
on such values by pi   taking the gradient of f with respect to xk gives 

  f  x      ppk e xk  q
k
 xk
i pi e xi

    

 
hkl    x   x f  x      kl qk   qk ql

    

x
x
x
z  t hz    qk zk      qk zk    ql zl    varfz g   

    

where qk defines a probability distribution  the convexity is revealed by a positive semidefinite hessian h  whose components in this case are
k

l

to see that h is positive semi definite  consider
k

k

l

where varfz g is the variance of a discrete random variable z which takes the values zi
with probability qi  

references

d ambrosio  b          incremental probabilistic inference  in proceedings of the ninth
conference on uncertainty in artificial intelligence  san mateo  ca  morgan kaufmann 
d ambrosio  b          symbolic probabilistic inference in large bn   networks  in proceedings of the tenth conference on uncertainty in artificial intelligence  san mateo 
ca  morgan kaufmann 
cooper  g          nestor  a computer based medical diagnostic aid that integrates
causal and probabilistic knowledge  ph d  dissertation  medical informatics sciences 
stanford university  stanford  ca   available from umi at
http   wwwlib umi com dissertations main  
cooper  g          the computational complexity of probabilistic inference using bayesian
belief networks  artificial intelligence              
dagum  p     horvitz  e          reformulating inference problems through selective
conditioning  in proceedings of the eighth annual conference on uncertainty in
artificial intelligence 
dagum  p     horvitz  e          a bayesian analysis of simulation algorithms for inference
in belief networks  networks              
   

fivariational probabilistic inference and qmr dt

dagum  p     luby  m          approximate probabilistic reasoning in bayesian belief
networks is np hard  artificial intelligence              
dechter  r          mini buckets  a general scheme of generating approximations in automated reasoning  in proceedings of the fifteenth international joint conference on
artificial intelligence 
dechter  r          bucket elimination  a unifying framework for probabilistic inference 
in m  i  jordan  ed    learning in graphical models  cambridge  ma  mit press 
dempster  a   laird  n     rubin  d          maximum likelihood from incomplete data
via the em algorithm  journal of the royal statistical society b           
draper  d     hanks  s          localized partial evaluation of belief networks  in proceedings of the tenth annual conference on uncertainty in artificial intelligence 
fung  r     chang  k  c          weighting and integrating evidence for stochastic simulation in bayesian networks  in proceedings of fifth conference on uncertainty in
artificial intelligence  amsterdam  elsevier science 
gelfand  a     smith  a          sampling based approaches to calculating marginal densities  journal of the american statistical association              
heckerman  d          a tractable inference algorithm for diagnosing multiple diseases  in
proceedings of the fifth conference on uncertainty in artificial intelligence 
henrion  m          search based methods to bound diagnostic probabilities in very large
belief nets  in proceedings of seventh conference on uncertainty in artificial intelligence 
horvitz  e  suermondt  h     cooper  g          bounded conditioning  flexible inference
for decisions under scarce resources  in proceedings of fifth conference on uncertainty in artificial intelligence 
jaakkola  t          variational methods for inference and learning in graphical models 
phd thesis  department of brain and cognitive sciences  massachusetts institute of
technology 
jaakkola  t     jordan  m          recursive algorithms for approximating probabilities
in graphical models  in advances of neural information processing systems    cambridge  ma  mit press 
jensen  c  s   kong  a     kjrulff  u          blocking gibbs sampling in very large
probabilistic expert systems  international journal of human computer studies     
        
jensen  f          introduction to bayesian networks  new york  springer 
   

fijaakkola   jordan

jordan  m   ghaharamani  z  jaakkola  t     saul  l   in press   an introduction to
variational methods for graphical models  machine learning 
lauritzen  s     spiegelhalter  d          local computations with probabilities on graphical structures and their application to expert systems  with discussion   journal of
the royal statistical society b              
mackay  d  j  c          introduction to monte carlo methods  in m  i  jordan  ed   
learning in graphical models  cambridge  ma  mit press 
middleton  b   shwe  m   heckerman  d   henrion  m   horvitz  e   lehmann  h     cooper 
g          probabilistic diagnosis using a reformulation of the internist   qmr
knowledge base ii  evaluation of diagnostic performance  section on medical informatics technical report smi          stanford university 
miller  r  a   fasarie  f  e     myers  j  d          quick medical reference  qmr  for
diagnostic assistance  medical computing           
pearl  j          probabilistic reasoning in intelligent systems  san mateo  ca  morgan
kaufmann 
peng  y     reggia  j          a probabilistic causal model for diagnostic problem solving  
part    diagnostic strategy  ieee trans  on systems  man  and cybernetics  special
issue for diagnosis              
poole  d          probabilistic partial evaluation  exploiting rule structure in probabilistic
inference  in proceedings of the fifteenth international joint conference on artificial
intelligence 
rockafellar  r          convex analysis  princeton university press 
shachter  r  d     peot  m          simulation approaches to general probabilistic inference
on belief networks  in proceedings of fifth conference on uncertainty in artificial
intelligence  elsevier science  amsterdam 
shenoy  p  p          valuation based systems for bayesian decision analysis  operations
research              
shwe  m     cooper  g          an empirical analysis of likelihood   weighting simulation
on a large  multiply connected medical belief network  computers and biomedical
research              
shwe  m   middleton  b   heckerman  d   henrion  m   horvitz  e   lehmann  h     g 
cooper         probabilistic diagnosis using a reformulation of the internist  qmr knowledge base i  the probabilistic model and inference algorithms  methods
of information in medicine              

   

fi