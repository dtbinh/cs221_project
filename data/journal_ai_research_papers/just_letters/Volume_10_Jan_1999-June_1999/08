journal artificial intelligence research                  

submitted        published     

issues stacked generalization
kai ming ting

kmting deakin edu au

ian h  witten

ihw cs waikato ac nz

school computing mathematics
deakin university  australia 
department computer science
university waikato  new zealand 

abstract

stacked generalization general method using high level model combine lowerlevel models achieve greater predictive accuracy  paper address two crucial
issues considered  black art  classification tasks ever since
introduction stacked generalization      wolpert  type generalizer
suitable derive higher level model  kind attributes used
input  find best results obtained higher level model combines
confidence  and predictions  lower level ones 
demonstrate effectiveness stacked generalization combining three different
types learning algorithms classification tasks  compare performance
stacked generalization majority vote published results arcing bagging 

   introduction
stacked generalization way combining multiple models learned
classification task  wolpert         used regression  breiman      a 
even unsupervised learning  smyth   wolpert         typically  different learning
algorithms learn different models task hand  common form
stacking first step collect output model new set data 
instance original training set  data set represents every model s prediction
instance s class  along true classification  step  care taken ensure
models formed batch training data include instance
question  way ordinary cross validation  new data treated
data another learning problem  second step learning algorithm
employed solve problem  wolpert s terminology  original data models
constructed first step referred level   data level   models 
respectively  set cross validated data second stage learning algorithm
referred level   data level   generalizer 
paper  show make stacked generalization work classification tasks
addressing two crucial issues wolpert        originally described  black art 
resolved since  two issues  i  type attributes
used form level   data   ii  type level   generalizer order get improved
accuracy using stacked generalization method 
breiman      a  demonstrated success stacked generalization setting
ordinary regression  level   models regression trees different sizes linear
c      ai access foundation morgan kaufmann publishers  rights reserved 

fiting   witten

regressions using different number variables  instead selecting single model
works best judged  for example  cross validation  breiman used different level  regressors  output values member training set form level   data 
used least squares linear regression  constraint regression coecients
non negative  level   generalizer  non negativity constraint turned
crucial guarantee predictive accuracy would better achieved
selecting single best predictor 
show stacked generalization made work reliably classification
tasks  using output class probabilities generated level   models
form level   data  level   generalizer use version least squares linear
regression adapted classification tasks  find use class probabilities crucial
successful application stacked generalization classification tasks  however 
non negativity constraints found necessary breiman regression found
irrelevant improved predictive accuracy classification situation 
section    formally introduce technique stacked generalization describe
pertinent details learning algorithm used experiments  section   describes
results stacking three different types learning algorithms  section   compares
stacked generalization arcing bagging  two recent methods employ sampling
techniques modify data distribution order produce multiple models single
learning algorithm  following section describes related work  paper ends
summary conclusions 

   stacked generalization

given data set l   f y   x    n              n g  class value x vector
representing attribute values nth instance  randomly split data j almost
equal parts l            l   define l l       l   l test training sets
j th fold j  fold cross validation  given k learning algorithms  call level  
generalizers  invoke kth algorithm data training set l     induce
model m       k              k   called level   models 
instance x l   test set j th cross validation fold  let z denote
prediction model m     x   end entire cross validation process 
data set assembled outputs k models
n

n

n

j

j

j

n

j

j

j

k

n

j

kn

j

n

k

l   f y   z            z    n              n g 
cv

n

n

kn

level   data  use learning algorithm call level   generalizer
  function  z            z    level  
derive data model
model  figure   illustrates cross validation process  complete training process 
final level   models   k              k   derived using data l 
let us consider classification process  uses models   k              k  
    given new instance  models produce vector  z            z   
conjunction
    whose output final classification result
vector input level   model
instance  completes stacked generalization method proposed wolpert
        used breiman      a  leblanc   tibshirani        
k

k

k

k

   

k

fiissues stacked generalization

 


l cv
level  
level  

  j 

  j 

m 

  j 

mk

mk

  j 

l

figure    figure illustrates j  fold cross validation process level    level  
  
data set l end process used produce level   model
cv

   
well situation described above  results level   model
present paper considers situation output level   models
set class probabilities rather single class prediction  model m     used
classify instance x l   let p  x  denote probability ith output class 
vector
p    p    x            p  x            p  x   
gives model s class probabilities nth instance  assuming classes 
level   data  assemble together class probability vector k models  along
actual class 
l    f y   p            p           p    n              n g 
    contrast
  
denote level   model derived
following two subsections describe algorithms used level   level   generalizers experiments reported section   
j

k

j

ki

kn

cv

n

k

n

n

ki

kn

n

ki

n

kn

    level   generalizers

three learning algorithms used level   generalizers  c     decision tree learning
algorithm  quinlan         nb  re implementation naive bayesian classifier  cestnik 
       ib   variant lazy learning algorithm  aha  kibler   albert       
employs p nearest neighbor method using modified value difference metric nominal
binary attributes  cost   salzberg         learning algorithms
show formula use
p estimated output class probabilities p  x 
instance x  where  cases  p  x       
c     consider leaf decision tree instance x falls  let
number  training  instances class leaf  suppose majority class








   

fiting   witten

p
leaf i   let e          then  using laplace estimator 






p   x        pem        






p  x         p   x        i  






note pruned trees default settings c    used experiments 
nb  let p  ijx  posterior probability class i  given instance x 
p  x    pp p ij xij x   




note nb uses laplacian estimate estimating conditional probabilities
nominal attribute compute p  ijx   continuous valued attribute 
normal distribution assumed case conditional probabilities
conveniently represented entirely terms mean variance observed
values class 
ib   suppose p nearest neighbors used  denote f y   x                 pg
instance x   we use p     experiments  




p f  y   d x  x  
p  x    p     d x  x    
p









p

  





f  y           otherwise  euclidean distance function 




three learning algorithms  predicted class level   model  given instance
x  i 
p   x    p  x     i  




    level   generalizers

compare effect four different learning algorithms level   generalizer  c    
ib  using p      nearest neighbors    nb  multi response linear regression algorithm 
mlr  last needs explanation 
mlr adaptation least squares linear regression algorithm breiman      a 
used regression settings  classification problem real valued attributes
transformed multi response regression problem  original classification problem
classes  converted separate regression problems  problem
class   instances responses equal one class   zero otherwise 
input mlr level   data  need consider situation model
 
 
  attributes probabilities  separately model m   
   large value used following wolpert s        advice   reasonable  relatively global 
smooth   level   generalizers perform well  
p

   

   

   

fiissues stacked generalization

classes  former case  attributes already real valued  linear
regression class   simply

x
lr  x   
k

 

k 

p  x  
k 

k

latter case  classes unordered nominal attributes  map binary
values obvious way  setting p  x    class instance x   zero otherwise 
use linear regression 
choose linear regression coecients fff g minimize
k 

x x
j

 

yn  xn

  lj

k 

 y  
n

xff

p      x      
j

k 

k 

n

k

coecients fff g constrained non negative  following breiman s      a  discovery necessary successful application stacked generalization regression problems  non negative coecient least squares algorithm described lawson
  hanson        employed derive linear regression class  show
later that  fact  non negative constraint unnecessary classification tasks 
place  describe working mlr  classify new instance
x  compute lr  x  classes assign instance class  
greatest value  
lr  x    lr  x          
next section investigate stacking c     nb ib  
k 

 

 

  

   stacking c     nb ib 

    stacked generalization work 

experiments section show
successful stacked generalization necessary use output class prob    rather
  
abilities rather class predictions that is 
mlr algorithm suitable level   generalizer  among four
algorithms used 
use two artificial datasets eight real world datasets uci repository
machine learning databases  blake  keogh   merz         details given
table   
artificial datasets led   waveform each training dataset l size    
     respectively  generated using different seed  algorithms used
experiments tested separate dataset      instances  results expressed
average error rate ten repetitions entire procedure 
real world datasets  w  fold cross validation performed  fold
cross validation  training dataset used l  models derived evaluated
   pattern recognition community calls type classifier linear machine  duda   hart        

   

fiting   witten

datasets   samples   classes   attr   type
led  
        
  
  n
waveform         
 
  c
horse
   
 
 b   n  c
credit
   
 
 b  n  c
vowel
   
  
  c
euthyroid
    
 
  b  c
splice
    
 
  n
abalone
    
 
 n  c
nettalk s 
    
 
 n
coding
     
 
  n

n nominal  b binary  c continuous 

table    details datasets used experiment 
test dataset  result expressed average error rate w  fold crossvalidation  note cross validation used evaluation entire procedure 
whereas j  fold cross validation mentioned section   internal operation
stacked generalization  however  w j set    experiments 
 
section  present results model combination using level   models
 
 
  well model selection method  employing j  fold cross validation procedure  note difference model combination model selection
whether level   learning employed not 
table   shows average error rates  obtained using w  fold cross validation  c    
nb ib   bestcv  best three  selected using j  fold crossvalidation  expected  bestcv almost always classifier lowest error rate  
   
table   shows result stacked generalization using level   model
     
level   data comprise classifications generated level   models 
level   data comprise probabilities generated level   models  results
shown four level   generalizers case  along bestcv  lowest error
rate dataset given bold 
table   summarizes results table   terms comparison level  
    derived
model bestcv totaled datasets  clearly  best level   model
using mlr  performs better bestcv nine datasets equally well tenth 
  derived nb  performs better bestcv seven
best performing
datasets significantly worse two  waveform vowel   regard difference
two standard errors significant      confidence   standard error figures
omitted table increase readability 
datasets shown order increasing size  mlr performs significantly
better bestcv four largest datasets  indicates stacked generalization
likely give significant improvements predictive accuracy volume data
large a direct consequence accurate estimation using cross validation 
   note bestcv always select classifier folds  error rate
always equal lowest error rate among three classifiers 
w

   

fiissues stacked generalization

datasets

level   generalizers
c    nb
ib 
led  
         
    
waveform          
    
horse
         
    
credit
         
    
vowel
         
   
euthyroid        
   
splice
       
   
abalone
         
    
nettalk s           
    
coding
         
    

bestcv
        
        
        
        
       
       
       
        
        
        

table    average error rates c     nb ib   bestcv the best among
selected using j  fold cross validation  standard errors shown last
column 
datasets

 
level   model 
c    nb ib  mlr
                   
                   
                   
                   
           
   

bestcv
led  
    
waveform
    
horse
    
credit
    
vowel
   
euthyroid
               
splice
               
abalone
                   
nettalk s 
                   
coding
                   

   
   

    
    
    

  
level   model 
c    nb ib  mlr
                   
                   
                   
                   
               
               
               
                   
                   
                   

table    average error rates stacking c     nb ib  
 
  
level   model 
level   model 
c    nb ib  mlr c    nb ib  mlr
 win vs   loss                                
 
    
table    summary table   comparison bestcv

   

fiting   witten

one level   models performs significantly much better rest 
euthyroid vowel datasets  mlr performs either good bestcv selecting
best performing level   model  better bestcv 
mlr advantage three level   generalizers model
easily interpreted  examples combination weights derives  for probability      appear table   horse  credit  splice  abalone  waveform  led  
based model
vowel datasets  weights indicate relative importance level   generalizers
prediction class  example  splice dataset  in table   b    nb
dominant generalizer predicting class    nb ib  good predicting class
   three generalizers make worthwhile contribution prediction class   
contrast  abalone dataset three generalizers contribute substantially
prediction three classes  note weights class sum one
constraint imposed mlr 

    non negativity constraints necessary 
breiman      a  leblanc   tibshirani        use stacked generalization
method regression setting report necessary constrain regression
coecients non negative order guarantee stacked regression improves predictive accuracy  investigate finding domain classification tasks 
assess effect non negativity constraint performance  three versions
    
mlr employed derive level   model
i  linear regression mlr calculated intercept constant  that is 
    weights classes  without constraints 
ii  linear regression derived neither intercept constant  i weights
classes  constraints 
iii  linear regression derived without intercept constant  nonnegativity constraints  i non negative weights classes  
third version one used results presented earlier  table   shows
results three versions  almost indistinguishable error rates  conclude
classification tasks  non negativity constraints necessary guarantee
stacked generalization improves predictive accuracy 
however  another reason good idea employ non negativity constraints  table   shows example weights derived three versions mlr
led   dataset  third version  shown column  iii   supports perspicuous
interpretation level   generalizer s contribution class predictions
two  dataset  ib  dominant generalizer predicting classes        
nb ib  make worthwhile contribution predicting class    evidenced
high weights  however  negative weights used predicting classes render
interpretation two versions much less clear 
   

fiissues stacked generalization

horse
credit
class c    nb ib  c    nb ib 
 
                             
 
                             
c    ff    nb ff    ib  ff   
      horse credit datasets 
table     a  weights generated mlr  model
class
 
 
 

c   
    
    
    

splice
nb
    
    
    

ib 
    
    
    

abalone
c    nb ib 
              
              
              

waveform
c    nb ib 
              
              
              

      splice  abalone waveform
table     b  weights generated mlr  model
datasets 
vowel
c    nb ib 
              
              
              
              
              
              
              
              
              
              
              
      led   vowel datasets 
table     c  weights generated mlr  model
class
 
 
 
 
 
 
 
 
 
  
  

c   
    
    
    
    
    
    
    
    
    
    
 

led  
nb
    
    
    
    
    
    
    
    
    
    
 

ib 
    
    
    
    
    
    
    
    
    
    
 

   

fiting   witten

datasets

mlr
constraints intercept non negativity
led  
    
    
    
waveform
    
    
    
horse
    
    
    
credit
    
    
    
vowel
   
   
   
euthyroid
   
   
   
splice
   
   
   
abalone
    
    
    
nettalk s 
    
    
    
coding
    
    
    
table    average error rates three versions mlr 
class
 
 
 
 
 
 
 
 
 
  

ff 

    
    
    
    
    
    
    
    
    
    

ff 

 i 

ff 

ff 

ff 

 ii 

ff 

ff 

ff 

                                  
                                    
                                    
                                    
                                    
                                  
                                  
                                      
                                    
                                    

 iii 

ff 

    
    
    
    
    
    
    
    
    
    

ff 

    
    
    
    
    
    
    
    
    
    

table    weights generated three versions mlr   i  constraints   ii  intercept 
 iii  non negativity constraints  led   dataset 

   

fiissues stacked generalization

dataset
 se bestcv majority mlr
horse
   
    
         
splice
   
   
       
abalone
   
    
         
led  
   
    
         
credit
   
    
         
nettalk s      
    
         
coding
    
    
         
waveform     
    
         
euthyroid     
   
       
vowel
     
   
        
       along
table    average error rates bestcv  majority vote mlr  model
number standard error   se  bestcv worst performing
level   generalizers 

    stacked generalization compare majority vote 
      derived mlr  majority vote 
let us compare error rate

simple decision combination method requires neither cross validation level  learning  table   shows average error rates bestcv  majority vote mlr 
order see whether relative performances level   generalizers effect
methods  number standard errors   se  error rates
worst performing level   generalizer bestcv given  datasets re ordered
according measure  since bestcv almost always selects best performing level  
generalizer  small values  se indicate level   generalizers perform comparably
one another  vice versa 
mlr compares favorably majority vote  eight wins versus two losses 
eight wins  six significant differences  the two exceptions splice
led   datasets   whereas losses  for horse credit datasets  insignificant
differences  thus extra computation cross validation level   learning seems
paid off 
interesting note performance majority vote related size
 se  majority vote compares favorably bestcv first seven datasets 
values  se small  last three   se large  majority vote performs
worse  indicates level   generalizers perform comparably  worth
using cross validation determine best one  result majority vote which
far cheaper is significantly different  although small values  se necessary
condition majority vote rival bestcv  sucient condition see matan
       example  applies majority vote compared mlr  mlr
performs significantly better five datasets large  se values 
one cases 
   

fiting   witten

m  versus m   

c    nb ib  mlr
 win vs   loss                
  versus
    generalizer summarized results table   
table   
worth mentioning method averages p  x  level   models 
yielding p  x   predicts class i  p  x    p  x     i   according
breiman      b   method produces error rate almost identical majority
vote 








    stacked generalization work best m    generated
mlr 
shown stacked generalization works best output class probabilities
 rather class predictions  used mlr algorithm  rather c     ib  
nb   retrospect  surprising  explained intuitively follows 
level   model provide simple way combining evidence available 
evidence includes predictions  confidence level   model
predictions  linear combination simplest way pooling level   models 
confidence  mlr provides that 
alternative methods nb  c     ib  shortcomings  bayesian approach could form basis suitable alternative way pooling level   models  confidence  independence assumption central naive bayes hampers performance
datasets evidence provided individual level   models certainly
independent  c    builds trees model interaction amongst attributes particularly
tree large but desirable combining confidences  nearest neighbor methods really give way combining confidences  also  similarity metric
employed could misleadingly assume two different sets confidence levels similar 
 
    level  
table   summarizes results table   comparing
generalizer  across datasets  c    clearly better label based representation 
discretizing continuous valued attributes creates intra attribute interaction addition interactions different attributes  evidence table   nb
indifferent use labels confidences  normal distribution assumption
embodies latter case could another reason unsuitable combining
confidence measures  mlr ib  handle continuous valued attributes better
label based ones  since domain designed work 
summary

summarize findings section follows 

none four learning algorithms used obtain model m  perform satisfactorily 
   

fiissues stacked generalization

mlr best four learning algorithms use level   generalizer
    
obtaining model
obtained using mlr  m    lower predictive error rate best model
selected j  fold cross validation  almost datasets used experiments 

another advantage mlr three level   generalizers interpretability 
weights indicate different contributions level   model k makes
prediction classes   
k 

model m    derived mlr without non negativity constraints 
constraints make little difference model s predictive accuracy 

use non negativity constraints mlr advantage interpretability  non 

negative weights support easier interpretation extent model
contributes prediction class 
k 

derived using mlr  model m    compares favorably majority vote 
mlr provides method combining confidence generated level   models
final decision  various reasons  nb  c     ib  suitable task 

   comparison arcing bagging
section compares results stacking c     nb ib  results arcing
 called boosting originator  schapire        bagging reported breiman
     b      c   arcing bagging employ sampling techniques modify data
distribution order produce multiple models single learning algorithm 
combine decisions individual models  arcing uses weighted majority vote
bagging uses unweighted majority vote  breiman reports arcing bagging
substantially improve predictive accuracy single model derived using base
learning algorithm 

    experimental results

first describe differences experimental procedures  results
stacking averaged ten fold cross validation datasets except waveform 
averaged ten repeated trials  standard errors shown  results arcing
bagging obtained breiman      b      c   averaged     trials 
breiman s experiments  trial uses random     split form training test
sets datasets except waveform  note waveform dataset used   
irrelevant attributes  breiman used version without irrelevant attributes  which would
expected degrade performance level   generalizers experiments  
cases     training instances used dataset  used      test instances
whereas breiman used       arcing bagging done    decision tree models
derived cart  breiman et al         trial 
   

fiting   witten

dataset
 samples stacking arcing bagging
waveform
   
             
    
glass
   
             
    
ionosphere
   
       
   
   
soybean
   
       
   
   
breast cancer
   
       
   
   
diabetes
   
             
    
table     comparing stacking arcing bagging classifiers 
results six datasets given table     indicate three methods
competitive   stacking performs better arcing bagging three
datasets  waveform  soybean breast cancer   better arcing worse
bagging diabetes dataset  note stacking performs poorly glass
ionosphere  two small real world datasets  surprising  cross validation
inevitably produces poor estimates small datasets 

    discussion

bagging  stacking ideal parallel computation  construction level  
model proceeds independently  communication modeling processes
necessary 
arcing bagging require considerable number member models
rely varying data distribution get diverse set models single learning
algorithm  using level   generalizer  stacking work two three level  
models 
suppose computation time required learning algorithm c   arcing
bagging needs h models  learning time required   hc   suppose stacking requires
g models model employs j  fold cross validation  assuming time c needed
derive g level   models level   model  learning time stacking
   g j          c   results given table     h       j       g      thus
    c     c   however  practice learning time required level  
level   generalizers may different 
users stacking free choice level   models  may either derived
single learning algorithm  variety different algorithms  example section
  uses different types learning algorithms  bag stacking stacking bagged models
 ting   witten        uses data variation obtain diverse set models single
learning algorithm  former case  performance may vary substantially
level   models for example nb performs poorly vowel euthyroid datasets
compared two models  see table     stacking copes well situation 
performance variation among member models bagging rather small
derived learning algorithm using bootstrap samples  section    








   heart dataset used breiman      b      c  omitted much modified
original one 

   

fiissues stacked generalization

shows small performance variation among member models necessary condition
majority vote  as employed bagging  work well 
worth noting arcing bagging incorporated framework
stacked generalization using arced bagged models level   models  ting   witten
       show one possible way incorporating bagged models level   learning  employing mlr instead voting  implementation  l used test set
bagged models derive level   data rather cross validated data 
viable bootstrap sample leaves     examples  ting   witten
       show bag stacking almost always higher predictive accuracy bagging
models derived either c    nb  note difference whether
adaptive level   model simple majority vote employed
according breiman      b      c   arcing bagging improve predictive accuracy learning algorithms  unstable    unstable learning algorithm
one small perturbations training set produce large changes
derived model  decision trees neural networks unstable  nb ib  stable 
stacking works both 
mlr successful candidate level   learning found 
algorithms might work equally well  one candidate neural networks  however 
experimented back propagation neural networks purpose found
much slower learning rate mlr  example  mlr took    
seconds compare      seconds neural network nettalk dataset 
error rate  possible candidates multinomial logit model
 jordan   jacobs         special case generalized linear models  mccullagh
  nelder         supra bayesian procedure  jacobs        treats level  
models  confidence data may combined prior distribution level   models
via bayes  rule 

   related work

analysis stacked generalization motivated breiman      a   discussed
earlier  leblanc   tibshirani         leblanc   tibshirani        examine stacking
linear discriminant nearest neighbor classifier show that  one artificial
dataset  method similar mlr performs better non negativity constraints
without  results section     show constraints irrelevant mlr s
predictive accuracy classification situation 
leblanc   tibshirani        ting   witten        use version mlr
employs class probabilities level   model induce linear regression 
case  linear regression class  

lr  x   
 

xxff
k



k



ki 

p  x  
ki

implementation requires fitting ki parameters  compared k parameters
version used paper  see corresponding formula section      
   schapire  r e   y  freund  p  bartlett    w s  lee        provide alternative explanation
effectiveness arcing bagging 

   

fiting   witten

versions give comparable results terms predictive accuracy  version used
paper runs considerably faster needs fit fewer parameters 
limitations mlr well known  duda   hart          class problem 
divides description space convex decision regions  every region must singly
connected  decision boundaries linear hyperplanes  means mlr
suitable problems unimodal probability densities  despite limitations 
mlr still performs better level   generalizer ib   nearest competitor deriving
m     limitations may hold key fuller understanding behavior stacked
generalization  jacobs        reviews linear combination methods used mlr 
previous work stacked generalization  especially applied classification tasks 
limited several ways  applies particular dataset  e g   zhang 
mesirov   waltz         others report results less convincing  merz        
still others different focus evaluate results datasets  leblanc
  tibshirani        chan   stolfo        kim   bartlett        fan et al         
one might consider degenerate form stacked generalization use crossvalidation produce data level   learning  then  level   learning done  on
y  training process  jacobs et al          another approach  level   learning
takes place batch mode  level   models derived  ho et al         
several researchers worked still degenerate form stacked generalization
without cross validation learning level    examples neural network ensembles
 hansen   salamon        perrone   cooper        krogh   vedelsby         multiple
decision tree combination  kwok   carter        buntine        oliver   hand        
multiple rule combination  kononenko   kovacic         methods used level  
majority voting  weighted averaging bayesian combination  possible methods
distribution summation likelihood combination  various forms re ordering
class rank  ali   pazzani        study methods rule learner  ting
       uses confidence prediction combine nearest neighbor classifier
naive bayesian classifier 

   conclusions
addressed two crucial issues successful implementation stacked generalization classification tasks  first  class probabilities used instead single
predicted class input attributes higher level learning  class probabilities serve
confidence measure prediction made  second  multi response least squares
linear regression technique employed high level generalizer  technique
provides method combining level   models  confidence  three learning algorithms either algorithmic limitations suitable aggregating confidences 
combining three different types learning algorithms  implementation
stacked generalization found achieve better predictive accuracy model
selection based cross validation majority vote  found competitive arcing bagging  unlike stacked regression  non negativity constraints
least squares regression necessary guarantee improved predictive accuracy
classification tasks  however  constraints still preferred increase
interpretability level   model 
   

fiissues stacked generalization

implication successful implementation stacked generalization earlier
model combination methods employing  weighted  majority vote  averaging  computations make use level   learning  apply learning improve
predictive accuracy 

acknowledgment

authors grateful new zealand marsden fund financial support
research  work conducted first author department computer
science  university waikato  authors grateful j  ross quinlan providing
c    david w  aha providing ib   anonymous reviewers editor
provided many helpful comments 

references

aha  d w   d  kibler   m k  albert         instance based learning algorithms  machine learning     pp        
ali  k m    m j  pazzani         error reduction learning multiple descriptions  machine learning  vol      no     pp          
blake  c   e  keogh   c j  merz         uci repository machine learning databases
 http    www ics uci edu  mlearn mlrepository html   irvine  ca  university california  department information computer science 
breiman  l       a   stacked regressions  machine learning  vol      pp        
breiman  l       b   bagging predictors  machine learning  vol      no     pp          
breiman  l       c   bias  variance  arcing classifiers  technical report      department statistics  university california  berkeley  ca 
breiman  l   j h  friedman  r a  olshen   c j  stone         classification regression trees  belmont  ca  wadsworth 
cestnik  b          estimating probabilities  crucial task machine learning 
proceedings european conference artificial intelligence  pp          
chan  p k    s j  stolfo         comparative evaluation voting meta learning
partitioned data  proceedings twelfth international conference machine learning  pp         morgan kaufmann 
cost    s  salzberg         weighted nearest neighbor algorithm learning
symbolic features  machine learning      pp        
fan  d w   p k  chan  s j  stolfo         comparative evaluation combiner
stacked generalization  proceedings aaai    workshop integrating multiple
learned models  pp        
hansen  l k    p  salamon         neural network ensembles  ieee transactions
pattern analysis machine intelligence      pp           
   

fiting   witten

ho  t k   j j  hull   s n  srihari         decision combination multiple classifier
systems  ieee transactions pattern analysis machine intelligence  vol     
no     pp        
jacobs  r a          methods combining experts  probability assessments  neural
computation    pp           mit press 
jacobs  r a   m i  jordan  s j  nowlan   g e  hinton         adaptive mixtures local
experts  neural computation    pp        
jacobs  r a    m i  jordan         hierachical mixtures experts em algorithms  neural computation    pp          
kim  k    e b  bartlett         error estimation series association neural network
systems  neural computation    pp           mit press 
kononenko  i    m  kovacic         learning optimization  stochastic generation
multiple knowledge  proceedings ninth international conference
machine learning  pp           morgan kaufmann 
krogh  a    j  vedelsby         neural network ensembles  cross validation  active
learning  advances neural information processing systems    g  tesauro  d s 
touretsky   t k  leen  editors   pp           mit press 
kwok  s    c  carter         multiple decision trees  uncertainty artificial intelligence    r  shachter  t  levitt  l  kanal j  lemmer  editors   pp          
north holland 
lawson c l    r j  hanson         solving least squares problems  siam publications 
leblanc  m    r  tibshirani         combining estimates regression classification  technical report       department statistics  university toronto 
matan  o          voting ensembles classifiers  extended abstract   proceedings
aaai    workshop integrating multiple learned models  pp        
mccullagh  p    j a  nelder         generalized linear models  london  chapman
hall 
merz  c j          dynamic learning bias selection  proceedings fifth international workshop artificial intelligence statistics  ft  lauderdale  fl 
unpublished  pp          
oliver  j j    d j  hand         pruning averaging decision trees  proceedings
twelfth international conference machine learning  pp           morgan
kaufmann 
perrone  m p    l n  cooper         networks disagree  ensemble methods
hybrid neural networks  artificial neural networks speech vision  r j 
mammone  editor   chapman hall 
quinlan  j r          c     program machine learning  morgan kaufmann 
   

fiissues stacked generalization

schapire  r e          strength weak learnability  machine learning     pp 
         kluwer academic publishers 
schapire  r e   y  freund  p  bartlett    w s  lee         boosting margin  new
explanation effectiveness voting methods  proceedings fourteenth
international conference machine learning  pages          morgan kaufmann 
smyth  p    d  wolpert         stacked density estimation  advances neural information processing systems 
ting  k m          characterisation predictive accuracy decision combination  proceedings thirteenth international conference machine learning 
pp           morgan kaufmann 
ting  k m    i h  witten         stacking bagged dagged models  proceedings
fourteenth international conference machine learning  pp           morgan
kaufmann 
weiss s  m    c  a  kulikowski         computer systems learns  morgan kaufmann 
wolpert  d h          stacked generalization  neural networks  vol     pp          
pergamon press 
zhang  x   j p  mesirov   d l  waltz         hybrid system protein secondary
structure prediction  journal molecular biology       pp            

   


