journal artificial intelligence research                  

submitted        published     

variational probabilistic inference
qmr dt network

tommi s  jaakkola

tommi ai mit edu

artificial intelligence laboratory 
massachusetts institute technology 
cambridge        usa

michael i  jordan

computer science division department statistics 
university california 
berkeley  ca            usa

jordan cs berkeley edu

abstract

describe variational approximation method ecient inference large scale
probabilistic models  variational methods deterministic procedures provide approximations marginal conditional probabilities interest  provide alternatives approximate inference methods based stochastic sampling search  describe
variational approach problem diagnostic inference  quick medical reference   qmr  network  qmr network large scale probabilistic graphical model
built statistical expert knowledge  exact probabilistic inference infeasible
model small set cases  evaluate variational inference algorithm
large set diagnostic test cases  comparing algorithm state of the art stochastic
sampling method 

   introduction
probabilistic models become increasingly prevalent ai recent years  beyond
significant representational advantages probability theory  including guarantees
consistency naturalness combining diverse sources knowledge  pearl        
discovery general exact inference algorithms principally responsible
rapid growth probabilistic ai  see  e g   lauritzen   spiegelhalter        pearl       
shenoy         exact inference methods greatly expand range models
treated within probabilistic framework provide unifying perspective
general problem probabilistic computation graphical models 
probability theory viewed combinatorial calculus instructs us
merge probabilities sets events probabilities composites  key operation marginalization  involves summing  or integrating  values
variables  exact inference algorithms essentially find ways perform sums
possible marginalization operations  terms graphical representation
probability distributions in random variables correspond nodes conditional
independencies expressed missing edges nodes exact inference algorithms
define notion  locality   for example cliques appropriately defined graph  
attempt restrict summation operators locally defined sets nodes 
c      ai access foundation morgan kaufmann publishers  rights reserved 

fijaakkola   jordan

approach manages stave exponential explosion exact probabilistic
computation  exponential explosion inevitable calculus explicitly
performs summations sets nodes  is  models interest
 local  overly large  see jordan  et al   press   point view  perhaps
surprising exact inference np hard  cooper        
paper discuss inference problem particular large scale graphical
model  quick medical reference  qmr  model   qmr model consists combination statistical expert knowledge approximately     significant diseases
approximately      findings  probabilistic formulation model  the qmr dt  
diseases findings arranged bi partite graph  diagnosis problem
infer probability distribution diseases given subset findings  given
finding generally relevant wide variety diseases  graph underlying
qmr dt dense  ecting high order stochastic dependencies  computational complexity treating dependencies exactly characterized terms size
maximal clique  moralized  graph  see  e g   dechter        lauritzen   spiegelhalter         particular  running time exponential measure size 
qmr dt  considering standardized  clinocopathologic conference   cpc  cases
discuss below  find median size maximal clique moralized graph
      nodes  rules use general exact algorithms qmr dt 
general algorithms take advantage particular parametric form
probability distributions nodes graph  conceivable additional
factorizations might found take advantage particular choice made
qmr dt  factorization fact found heckerman          quickscore
algorithm  provides exact inference algorithm tailored qmr dt  unfortunately  however  run time algorithm still exponential number positive
findings  cpc cases  estimate algorithm would require average
   years solve inference problem current computers 
faced apparent infeasibility exact inference large scale models
qmr dt  many researchers investigated approximation methods  one general
approach developing approximate algorithms perform exact inference 
partially  one consider partial sets node instantiations  partial sets hypotheses 
partial sets nodes  point view led development algorithms
approximate inference based heuristic search  another approach developing approximation algorithms exploit averaging phenomena dense graphs  particular  laws
large numbers tell us sums random variables behave simply  converging
predictable numerical results  thus  may need perform sums explicitly  either
exactly partially  point view leads variational approach approximate
inference  finally  yet another approach approximate inference based stochastic
sampling  one sample simplified distributions obtain information
complex distribution interest  discuss methods turn 
horvitz  suermondt cooper        developed partial evaluation algorithm
known  bounded conditioning  works considering partial sets node instan   acronym  qmr dt  use paper refers  decision theoretic  reformulation
qmr shwe  et al          shwe  et al  replaced heuristic representation employed
original qmr model  miller  fasarie    myers        probabilistic representation 

   

fivariational probabilistic inference qmr dt

tiations  algorithm based notion  cutset   subset nodes whose
removal renders remaining graph singly connected  ecient exact algorithms exist
singly connected graphs  pearl         summing instantiations cutset  one
calculate posterior probabilities general graphs using ecient algorithm
subroutine  unfortunately  however  exponentially many cutset instantiations  bounded conditioning algorithm aims forestalling exponential growth
considering partial sets instantiations  although algorithm promise graphs
 nearly singly connected   seems unlikely provide solution dense graphs
qmr dt  particular  median cutset size qmr dt across
cpc cases        yielding unmanageably large number        cutset instantiations 
another approach approximate inference provided  search based  methods 
consider node instantiations across entire graph  cooper        henrion       
peng   reggia         general hope methods relatively small fraction
 exponentially many  node instantiations contains majority probability mass 
exploring high probability instantiations  and bounding unexplored
probability mass  one obtain reasonable bounds posterior probabilities  qmrdt search space huge  containing approximately      disease hypotheses  if  however 
one considers cases small number diseases  hypotheses involving
small number diseases contain high probability posteriors  may
possible search significant fraction relevant portions hypothesis space 
henrion        fact able run search based algorithm qmr dt inference
problem  set cases characterized small number diseases  cases 
however  exact quickscore algorithm ecient  general corpus
cpc cases discuss current paper characterized small number
diseases per case  general  even impose assumption patients limited
number n diseases  cannot assume priori model show sharp cutoff
posterior probability disease n   finally  high dimensional search problems
often necessary allow paths limited target hypothesis subspace 
particular  one would able arrive hypothesis containing diseases
pruning hypotheses containing additional diseases  peng   reggia         imposing
limitation lead failure search 
recent partial evaluation methods include  localized partial evaluation  method
draper hanks          incremental spi  algorithm d ambrosio        
 probabilistic partial evaluation  method poole          mini buckets  algorithm
dechter         former algorithm considers partial sets nodes  latter three
consider partial evaluations sums emerge exact inference run 
promising methods  partial evaluation methods yet clear
restrict exponential growth complexity ways yield realistic accuracy time
tradeoffs large scale models qmr dt  
variational methods provide alternative approach approximate inference 
similar spirit partial evaluation methods  in particular incremental spi
mini buckets algorithms   aim avoid performing sums exponentially
   d ambrosio        reports  mixed  results using incremental spi qmr dt  somewhat
dicult set cases heckerman        henrion         still restricted number
positive findings 

   

fijaakkola   jordan

many summands  come problem different point view 
variational point view  sum avoided contains sucient number terms
law large numbers invoked  variational approach inference
replaces quantities expected beneficiary averaging process
surrogates known  variational parameters   inference algorithm manipulates
parameters directly order find good approximation marginal probability
interest  qmr dt model turns particularly appealing architecture
development variational methods  show  variational methods simple
graphical interpretation case qmr dt 
final class methods performing approximate inference stochastic sampling methods  stochastic sampling large family  including techniques rejection
sampling  importance sampling  markov chain monte carlo methods  mackay        
many methods applied problem approximate probabilistic inference graphical models analytic results available  dagum   horvitz        
particular  shwe cooper        proposed stochastic sampling method known
 likelihood weighted sampling  qmr dt model  results promising results date inference qmr dt they able produce reasonably
accurate approximations reasonable time two dicult cpc cases  consider
shwe cooper algorithm later paper  particular compare algorithm
empirically variational algorithm across entire corpus cpc cases 
although important compare approximation methods  emphasized
outset think goal identify single champion
approximate inference technique  rather  different methods exploit different structural
features large scale probability models  expect optimal solutions involve
combination methods  return point discussion section 
consider various promising hybrids approximate exact inference algorithms 
general problem approximate inference np hard  dagum   luby       
provides additional reason doubt existence single champion approximate
inference technique  think important stress  however  hardness result 
together cooper s        hardness result exact inference cited above 
taken suggest exact inference approximate inference  equally hard  
take example related field  exist large domains solid uid mechanics
exact solutions infeasible approximate techniques  finite element
methods  work well  similarly  statistical physics  models exactly solvable 
exist approximate methods  mean field methods  renormalization group methods 
work well many cases  feel goal research probabilistic inference
similarly identifying effective approximate techniques work well
large classes problems 

   qmr dt network
qmr dt network  shwe et al         two level bi partite graphical model  see
figure     top level graph contains nodes diseases   bottom level
contains nodes findings  
   

fivariational probabilistic inference qmr dt

number conditional independence assumptions ected bi partite
graphical structure  particular  diseases assumed marginally independent 
 i e   independent absence findings  note diseases assumed
mutually exclusive  patient multiple diseases   also  given states
disease nodes  findings assumed conditionally independent   for
discussion regarding medical validity diagnostic consequences
assumptions embedded qmr dt belief network  see shwe et al         
diseases

d 

f 

dn

fm
findings

figure    qmr belief network two level graph dependencies
diseases associated findings modeled via noisy or gates 
state precisely probability model implied qmr dt model  write
joint probability diseases findings as 

p  f  d    p  f jd p  d   

 





 
  

p  fi jd    p  dj   

j

   

f binary       vectors referring presence absence states diseases
positive negative states outcomes findings  respectively  conditional
probabilities p  fi jd  represented  noisy or model   pearl        

p  fi    jd    p  fi    jl  p  fi    jdj  
   
       qi   





j  i

     qij  dj

j  
p
 
i    j i ij dj
e
 

   

   
set diseases parents finding qmr graph  qij  
p  fi    jdj      probability disease j   present  could alone cause
finding positive outcome  qi    p  fi    jl   leak  probability  i e  
probability finding caused means diseases included
qmr model  final line  reparameterize noisy or probability model
using exponentiated notation  notation  model parameters given
ij     log     qij   
   

fijaakkola   jordan

   inference
carrying diagnostic inference qmr model involves computing posterior
marginal probabilities diseases given set observed positive  fi      negative
 fi       findings  note set observed findings considerably smaller set
possible findings  note moreover  from bi partite structure qmr dt graph 
unobserved findings effect posterior probabilities diseases 
brevity adopt notation fi  corresponds event      fi  refers
     positive negative findings respectively   thus posterior probabilities
interest p  dj jf     f      f   f   vectors positive negative findings 
negative findings f   benign respect inference problem they
incorporated posterior probability linear time number associated diseases
number negative findings  discuss below  seen
fact probability negative finding eq      exponential expression
linear dj   positive findings  hand  problematic 
worst case exact calculation posterior probabilities exponentially costly
number positive findings  heckerman        d ambrosio         moreover  practical
diagnostic situations number positive findings often exceeds feasible limit
exact calculations 
let us consider inference calculations detail  find posterior probability
p  djf     f     first absorb evidence negative findings  i e   compute p  djf    
p  f   jd p  d  normalization  since p  f   jd  p  d  factorize
diseases  see eq      eq      above   posterior p  djf     must factorize well 
normalization p  f   jd p  d  therefore reduces independent normalizations
disease carried time linear number diseases  or negative
findings   remainder paper  concentrate solely positive findings
pose real computational challenge  unless otherwise stated  assume
prior distribution diseases already contains evidence negative findings 
words  presume updates p  dj   p  dj jf     already made 
turn question computing p  dj jf      posterior marginal probability
based positive findings  formally  obtaining posterior involves marginalizing
p  f   jd p  d  across remaining diseases 

p  dj jf      

x

dndj

p  f   jd p  d 

   

summation possible configurations disease variables
dj  we use shorthand summation index n dj this   qmr model
p  f   jd p  d  form 

p  f   jd p  d   
 

 
 





 
  

p  fi  jd    p  dj   

j

 
 
 
    e  i    j ij dj   p  dj   





   

p

j

   

   

fivariational probabilistic inference qmr dt

follows eq      fact p  fi  jd        p  f   jd   perform
summation eq      diseases  would multiply terms     efg
corresponding conditional probabilities positive finding  number
terms exponential number positive findings  algorithms exist
attempt find exploit factorizations expression  based particular pattern
observed evidence  cf  heckerman        d ambrosio         algorithms limited
roughly    positive findings current computers  seems unlikely sucient
latent factorization qmr dt model able handle full cpc corpus 
median number    positive findings per case maximum number    positive
findings 

   variational methods
exact inference algorithms perform many millions arithmetic operations applied
complex graphical models qmr dt  proliferation terms expresses
symbolic structure model  necessarily express numeric structure
model  particular  many sums qmr dt inference problem sums
large numbers random variables  laws large numbers suggest sums
may yield predictable numerical results ensemble summands  fact
might enable us avoid performing sums explicitly 
exploit possibility numerical regularity dense graphical models develop
variational approach approximate probabilistic inference  variational methods
general class approximation techniques wide application throughout applied mathematics  variational methods particularly useful applied highly coupled systems  introducing additional parameters  known  variational parameters  which
essentially serve low dimensional surrogates high dimensional couplings
system these methods achieve decoupling system  mathematical machinery
variational approach provides algorithms finding values variational parameters decoupled system good approximation original coupled
system 
case probabilistic graphical models variational methods allow us simplify
complicated joint distribution one eq       achieved via parameterized transformations individual node probabilities  see later  node
transformations interpreted graphically delinking nodes graph 
find appropriate transformations  variational methods consider
come convex analysis  see appendix     let us begin considering methods
obtaining upper bounds probabilities  well known fact convex analysis
concave function represented solution minimization problem 

f  x    min
f x   f     g


   

f     conjugate function f  x   function f     obtained
solution minimization problem 

f       min
x f x   f  x  g 

   

   

fijaakkola   jordan

formal identity pair minimization problems expresses  duality  f
conjugate f  
representation f eq      known variational transformation  parameter known variational parameter  relax minimization fix
variational parameter arbitrary value  obtain upper bound 

f  x  x   f     

    

bound better values variational parameter others 
particular value bound exact 
want obtain lower bounds conditional probabilities  straightforward
way obtain lower bounds appeal conjugate duality express functions terms maximization principle  representation  however  applies convex
functions in current paper require lower bounds concave functions  concave functions  however  special form allows us exploit conjugatepduality
different way  particular  require bounds functions form f  a   j zj   
f concave function  zj   f             ng non negative variables 
constant  variables zj expression effectively coupled the impact
changing one variable contingent settings remaining variables  use
jensen s inequality  however  obtain lower bound variables decoupled  
particular 

f   

x

j

qj zqj  
j
j
x

qj f     zqj  
j
j

zj     f    

x

    
    

qj viewed defining probability distribution variables zj  
variational parameter case thep probability distribution q   optimal setting
parameter given qj   zj   k zk   easily verified substitution
eq        demonstrates lower bound tight 

    variational upper lower bounds noisy or

let us return problem computing posterior probabilities qmr
model  recall conditional probabilities corresponding positive findings
need simplified  end  write
p

p  fi  jd        e  i   

j ij dj

  e log   e x  

    

p

x   i    j ij dj   consider exponent f  x    log     e x    noisy or 
well many conditional models involving compact representations  e g   logistic
regression   exponent f  x  concave function x  based discussion
p

p

   jensen s inequality  states f  a   j qj xj   j qj f  a   xj    concave
f  
p
  qj    simple consequence eq       x taken   j qj xj  

   

p

qj

    

fivariational probabilistic inference qmr dt

previous section  know must exist variational upper bound function
linear x 

f  x  x   f   

    

using eq      evaluate conjugate function f     noisy or  obtain 

f        log          log      

    

desired
bound obtained substituting eq        and recalling definition
x   i    pj ij dj   

p  fi  jd 

 




p

e f  i    pj ij dj  

e  i   j ij dj   f  i 
p  fi  jd  i  

    
    
    

note  variational evidence  p  fi  jd  i  exponential term linear
disease vector d  negative findings  implies variational
evidence incorporated posterior time linear number diseases
associated finding 
graphical way understand effect transformation  rewrite
variational evidence follows 
p



p  fi jd  i    e i i    j ij dj   f  i 
id
yh
  e i   f  i  e iij j  
j

    
    

note first term constant  note moreover product factorized
across diseases  latter factors multiplied pre existing
prior corresponding disease  possibly modulated factors negative
evidence   constant term viewed associated delinked finding node  
indeed  effect variational transformation delink finding node
graph  altering priors disease nodes connected finding node 
graphical perspective important presentation variational algorithm 
able view variational transformations simplifying graph point
exact methods run 
turn
lower bounds conditional probabilities p  fi  jd   expop
nent f  i    j ij dj   exponential representation form applied
jensen s inequality previous section  indeed  since f concave need identify
non negative variables zj   case ij dj   constant a 
i   applying bound eq       have 

p  fi  jd 

 

p

e f   i    j ij dj  

e

ij dj
j qjji f io   qjji

p



   

    
    

fijaakkola   jordan

  e
  e

h

p





ij
j qjji dj f io   qjji     dj   f   io  




ij
j qjji dj f io   qjji  f   io    f   io  

p

h

    

    

    
allowed different variational distribution qji finding  note
bound linear exponent  case upper bound 
implies variational evidence incorporated posterior distribution
time linear number diseases  moreover  view variational
transformation terms delinking finding node graph 

p  fi  jd  qji 

    approximate inference qmr

previous section described variational transformations derived individual findings qmr model  discuss utilize transformations
context overall inference algorithm 
conceptually overall approach straightforward  transformation involves
replacing exact conditional probability finding lower bound upper
bound 
p  fi  jd  qji  p  fi  jd  p  fi  jd  i  
    
given transformations viewed delinking ith finding node
graph  see transformations yield bounds  yield simplified graphical structure  imagine introducing transformations sequentially
graph sparse enough exact methods become feasible  point stop
introducing transformations run exact algorithm 
problem approach  however  need decide step
node transform  requires assessment effect overall accuracy
transforming node  might imagine calculating change probability interest
given transformation  choosing transform node
yields least change target probability  unfortunately unable calculate
probabilities original untransformed graph  thus unable assess effect
transforming one node  unable get algorithm started 
suppose instead work backwards  is  introduce transformations
findings  reducing graph entirely decoupled set nodes  optimize
variational parameters fully transformed graph  more optimization
variational parameters below   graph inference trivial  moreover  easy
calculate effect reinstating single exact conditional one node  choose
reinstate node yields change 
consider particular case upper bounds  lower bounds analogous  
transformation introduces upper bound conditional probability p  fi  jd   thus
likelihood observing  positive  findings p  f     upper bounded variational
counterpart p  f   j   
x
x
p  f       p  f   jd p  d  p  f   jd   p  d  p  f   j  
    




   

fivariational probabilistic inference qmr dt

assess accuracy variational transformation introducing optimizing variational transformations positive findings  separately
positive finding replace variationally transformed conditional probability p  fi  jd  i 
corresponding exact conditional p  fi  jd  compute difference
resulting bounds likelihood observations 

  p  f   j     p  f   j n  

    

p  f   j n   computed without transforming ith positive finding  larger
difference is  worse ith variational transformation is  therefore
introduce transformations ascending order s  put another way 
treat exactly  not transform  conditional probabilities whose measure large 
practice  intelligent method ordering transformations critical  figure  
compares calculation likelihoods based measure opposed method
chooses ordering transformations random  plot corresponds representative diagnostic case  shows upper bounds log likelihoods observed
findings function number conditional probabilities left intact  i e 
transformed   note upper bound must improve  decrease  fewer transformations  results striking the choice ordering large effect accuracy
 note plot log scale  
  

loglikelihood

  
  
  
  
  
  
 

 

 
 
 
  
  exactly treated findings

  

figure    upper bound log likelihood delta method removing transformations  solid line  method bases choice random ordering
 dashed line  
note curve proposed ranking convex  thus bound improves
less fewer transformations left  first remove worst
transformations  replacing exact conditionals  remaining transformations better indicated delta measure thus bound improves less
replacements 
make claims optimality delta method  simply useful heuristic
allows us choose ordering variational transformations computationally
ecient way  note implementation method optimizes variational
parameters outset chooses ordering transformations
based fixed parameters  parameters suboptimal graphs
   

fijaakkola   jordan

substantial numbers nodes reinstated  found practice
simplified algorithm still produces reasonable orderings 
decided nodes reinstate  approximate inference algorithm
run  introduce transformations nodes left transformed
ordering algorithm  product exact conditional probabilities graph
transformed conditional probabilities yields upper lower bound overall
joint probability associated graph  the product bounds bound   sums
bounds still bounds  thus likelihood  the marginal probability findings 
bounded summing across bounds joint probability  particular  upper
bound likelihood obtained via 
x
x
p  f       p  f   jd p  d  p  f   jd   p  d  p  f   j  
    








dndj

dndj

corresponding lower bound likelihood obtained similarly 
x
x
p  f       p  f  jd p  d  p  f   jd  q  p  d  p  f   jq  

    

cases assume graph suciently simplified variational
transformations sums performed eciently 
expressions eq       eq       yield upper lower bounds arbitrary
values variational parameters q   wish obtain tightest possible bounds 
thus optimize expressions respect q   minimize respect
maximize respect q  appendix   discusses optimization problems
detail  turns upper bound convex thus adjustment
variational parameters upper bound reduces convex optimization problem
carried eciently reliably  there local minima   lower bound
turns maximization carried via em algorithm 
finally  although bounds likelihood useful  ultimate goal approximate
marginal posterior probabilities p  dj jf      two basic approaches utilizing
variational bounds eq       eq       purpose  first method 
emphasis current paper  involves using transformed probability model  the
model based either upper lower bounds  computationally ecient surrogate
original probability model  is  tune variational parameters transformed
model requiring model give tightest possible bound likelihood 
use tuned transformed model inference engine provide approximations
probabilities interest  particular marginal posterior probabilities p  dj jf     
approximations found manner bounds  computationally ecient
approximations  provide empirical data following section show
approach indeed yields good approximations marginal posteriors qmr dt
network 
ambitious goal obtain interval bounds marginal posterior probabilities themselves  end  let p  f     dj j   denote combined event qmr dt
model generates observed findings f   j th disease takes value dj  
bounds follow directly from 
x
x
p  f     dj     p  f   jd p  d  p  f   jd   p  d  p  f     dj j 
    
   

fivariational probabilistic inference qmr dt

p  f   jd    product upper bound transformed conditional probabilities
exact  untransformed  conditionals  analogously compute lower bound p  f     dj jq  
applying lower bound transformations 

p  f     dj    

x

dndj

p  f  jd p  d 

x

dndj

p  f   jd  q  p  d  p  f     dj jq  

    

combining bounds obtain interval bounds posterior marginal probabilities diseases  cf  draper   hanks       

p  f    dj j 
p  f     dj jq 
  

p
 

j
f
j
p  f    dj j    p  f     dj jq  
p  f     dj j     p  f     dj jq   
dj binary complement dj  

    

   experimental evaluation

diagnostic cases used evaluating performance variational techniques cases abstracted clinocopathologic conference   cpc   cases  cases
generally involve multiple diseases considered clinically dicult cases 
cases middleton et al         find importance sampling method
work satisfactorily 
evaluation variational methodology consists three parts  first part
exploit fact subset cpc cases       cases 
suciently small number positive findings calculate exact values
posterior marginals using quickscore algorithm  is  four cases
able obtain  gold standard  comparison  provide assessment accuracy
eciency variational methods four cpc cases  present variational
upper lower bounds likelihood well scatterplots compare variational
approximations posterior marginals exact values  present comparisons
likelihood weighted sampler shwe cooper        
second section present results remaining  intractable cpc cases 
use lengthy runs shwe cooper sampling algorithm provide surrogate
gold standard cases 
finally  third section consider problem obtaining interval bounds
posterior marginals 

    comparison exact marginals

four cpc cases    fewer positive findings  see table     cases
possible calculate exact values likelihood posterior marginals
reasonable amount time  used heckerman s  quickscore  algorithm  heckerman       an algorithm tailored qmr dt architecture to perform exact
calculations 
figure   shows log likelihood four tractable cpc cases  figure shows
variational lower upper bounds  calculated variational bounds twice 
differing numbers positive findings treated exactly two cases   treated exactly 
   

fijaakkola   jordan

case   pos  findings   neg  findings
 
  
  
 
  
  
 
  
  
 
  
  

  

  

  

  

  

loglikelihood

loglikelihood

table    description cases evaluated exact posterior marginals 

  

  

  

  
  
  
  

 a 

 

 

 
 
sorted cases

 

 b 

 

 

 

 
 
sorted cases

 

 

figure    exact values variational upper lower bounds log likelihood
log p  f   j   four tractable cpc cases   a    positive findings
treated exactly   b     positive findings treated exactly 
simply means finding transformed variationally   panel  a   
positive findings treated exactly   b     positive findings treated exactly 
expected  bounds tighter positive findings treated exactly  
average running time across four tractable cpc cases      seconds
exact method       seconds variational method   positive findings treated
exactly       seconds variational method    positive findings treated exactly 
 these results obtained     mhz dec alpha computer  
although likelihood important quantity approximate  particularly applications parameters need estimated   interest qmr dt setting
posterior marginal probabilities individual diseases  discussed
previous section  simplest approach obtaining variational estimates quantities define approximate variational distribution based either distribution
p  f   j    upper bounds likelihood  distribution p  f   jq    lowerbounds likelihood  fixed values variational parameters  chosen provide
tight bound likelihood   distributions provide partially factorized approximations joint probability distribution  factorized forms exploited
   given significant fraction positive findings treated exactly simulations  one
may wonder additional accuracy due variational transformations  address
concern later section demonstrate variational transformations fact responsible
significant portion accuracy cases 

   

fivariational probabilistic inference qmr dt

 

 

   

   
variational estimates

variational estimates

ecient approximate inference engines general posterior probabilities  particular
use provide approximations posterior marginals individual diseases 
practice found distribution p  f   j   yielded accurate posterior
marginals distribution p  f   jq    restrict presentation p  f   j    figure   displays scatterplot approximate posterior marginals  panel  a  corre 

   

   

   

 a 

 
 

   

   

   

   

   
   
exact marginals

   

 b 

 

 
 

   

   
   
exact marginals

   

 

figure    scatterplot variational posterior estimates exact marginals 
 a    positive findings treated exactly  b     positive findings
treated exactly 
sponding case   positive findings treated exactly panel  b  case
   positive findings treated exactly  plots obtained first extracting
   highest posterior marginals case using exact methods computing
approximate posterior marginals corresponding diseases  approximate
marginals fact correct points figures align along diagonals
shown dotted lines  see reasonably good correspondence the variational
algorithm appears provide good approximation largest posterior marginals   we
quantify correspondence ranking measure later section  
current state of the art algorithm qmr dt enhanced version likelihoodweighted sampling proposed shwe cooper         likelihood weighted sampling
stochastic sampling method proposed fung chang        shachter peot
        likelihood weighted sampling basically simple forward sampling method
weights samples likelihoods  enhanced improved utilizing  selfimportance sampling   see shachter   peot         version importance sampling
importance sampling distribution continually updated ect current
estimated posterior distribution  middleton et al         utilized likelihood weighted sampling self importance sampling  as well heuristic initialization scheme known
 iterative tabular bayes   qmr dt model found work satisfactorily  subsequent work shwe cooper         however  used additional
enhancement algorithm known  markov blanket scoring   see shachter   peot 
       distributes fractions samples positive negative values node
proportion probability values conditioned markov blanket
node  combination markov blanket scoring self importance sampling yielded
   

fijaakkola   jordan

effective algorithm   particular  modifications place  shwe cooper
reported reasonable accuracy two dicult cpc cases 
re implemented likelihood weighted sampling algorithm shwe cooper 
incorporating markov blanket scoring heuristic self importance sampling   we
utilize  iterative tabular bayes  instead utilized related initialization scheme 
 heuristic tabular bayes  also discussed shwe cooper   section discuss
results running algorithm four tractable cpc cases  comparing
results variational inference   following section present fuller comparative
analysis two algorithms cpc cases 
likelihood weighting sampling  indeed sampling algorithm  realizes timeaccuracy tradeoff taking additional samples requires time improves accuracy 
comparing sampling algorithm variational algorithm  ran sampling
algorithm several different total time periods  accuracy achieved
sampling algorithm roughly covered range achieved variational algorithm 
results shown figure    right hand curve corresponding sampling runs 
figure displays mean correlations approximate exact posterior
marginals across ten independent runs algorithm  for four tractable cpc cases  
 

mean correlation

    
    
    
    
   
    
      
  

 

 

  
  
execution time seconds

 

  

figure    mean correlation approximate exact posterior marginals
function execution time  in seconds   solid line  variational estimates 
dashed line  likelihood weighting sampling  lines sampling result represent standard errors mean based ten independent
runs sampler 
variational algorithms characterized time accuracy tradeoff  particular 
accuracy method generally improves findings treated exactly 
cost additional computation  figure   shows results variational
algorithm  the left hand curve   three points curve correspond      
   initialization method proved little effect inference results 
   investigated gibbs sampling  pearl         results gibbs sampling good
results likelihood weighted sampling  report latter results remainder
paper 

   

fivariational probabilistic inference qmr dt

   positive findings treated exactly  note variational estimates deterministic
thus single run made 
figure shows achieve roughly equivalent levels accuracy  sampling
algorithm requires significantly computation time variational method 
although scatterplots correlation measures provide rough indication accuracy approximation algorithm  deficient several respects  particular 
diagnostic practice interest ability algorithm rank diseases correctly 
avoid false positives  diseases fact significant included
set highly ranked diseases  false negatives  significant diseases omitted set highly ranked diseases   defined ranking measure follows  see
middleton et al          consider set n highest ranking disease hypotheses 
ranking based correct posterior marginals  corresponding set
diseases find smallest set n   approximately ranked diseases includes
n significant ones  words  n  true positives  approximate method
produces n     n  false positives   plotting false positives function true positives
provides meaningful useful measure accuracy approximation scheme 
extent method provides nearly correct ranking true positives plot
increases slowly area curve small  significant disease appears
late approximate ordering plot increases rapidly near true rank missed
disease area curve large 
plot number  false negatives  set top n highly ranked diseases 
false negatives refer number diseases  n highest ranking diseases 
appear set n approximately ranked diseases  note unlike
previous measure  measure reveal severity misplacements 
frequency 
improved diagnostic measure hand  let us return evaluation
inference algorithms  beginning variational algorithm  figure   provides plots
  

 

  

 

false negatives

false positives

 
  
  
  

 
 
 

  

 a 

 
 

 

  

  
  
true positives

  

 b 

  

 
 

  

  
  
approximate ranking

  

  

figure     a  average number false positives function true positives variational method  solid lines  partially exact method  dashed line    b  false
negatives set top n approximately ranked diseases  figures  
positive findings treated exactly 
false positives  panel a  false negatives  panel b  true positives
   

fijaakkola   jordan

  

 

  

   

 a 

 
false negatives

false positives

  
  
  
  

   
 
   

  

 

 

   

 
 

  

  
  
true positives

  

 b 

  

 
 

  

  
  
approximate ranking

  

  

figure     a  average number false positives function true positives variational method  solid line  partially exact method  dashed line    b  false
negatives set top n approximately ranked diseases  figures   
positive findings treated exactly 
tractable cpc cases  eight positive findings treated exactly simulation shown
figure  figure   displays results    positive finding treated exactly 
noted earlier       positive findings comprise significant fraction
total positive findings tractable cpc cases  thus important verify
variational transformations fact contributing accuracy posterior
approximations beyond exact calculations  comparing
variational method method call  partially exact  method
posterior probabilities obtained using findings treated exactly
variational calculations  i e   using findings transformed  
variational transformations contribute accuracy approximation 
performance partially exact method comparable
variational method   figure   figure   clearly indicate case 
difference accuracy methods substantial computational load
comparable  about     seconds    mhz dec alpha  
believe accuracy portrayed false positive plots provides good indication potential variational algorithm providing practical solution
approximate inference problem qmr dt  figures show  number
false positives grows slowly number true positives  example  shown
figure   eight positive findings treated exactly  find    likely diseases
would need entertain top    diseases list approximately ranked
diseases  compared    partially exact method  
ranking plot likelihood weighted sampler shown figure   
curve variational method figure   included comparison  make
plots  ran likelihood weighted sampler amount time       seconds 
   noted conservative comparison  partially exact method fact
benefits variational transformation the set exactly treated positive findings selected
basis accuracy variational transformations  accuracies correlate
diagnostic relevance findings 

   

fivariational probabilistic inference qmr dt

  
  

false positives

  
  
  
  
  
 
 
 

  

  
  
true positives

  

  

figure    average number false positives function true positives likelihoodweighted sampler  dashed line  variational method  solid line    
positive findings treated exactly 
comparable time allocated slowest variational method       seconds 
case    positive findings treated exactly  recall time required
variational algorithm    positive findings treated exactly      seconds  
plots show  tractable cpc cases  variational method significantly
accurate sampling algorithm comparable computational loads 

    full cpc corpus

consider full cpc corpus  majority cases        cases  
   positive findings thus appear beyond reach exact methods 
important attraction sampling methods mathematical guarantee accurate
estimates limit suciently large sample size  gelfand   smith         thus
sampling methods promise providing general methodology approximate
inference  two caveats      number samples needed dicult
diagnosis      many samples may required obtain accurate estimates 
real time applications  latter issue rule sampling solutions  however  long term
runs sampler still provide useful baseline evaluation accuracy faster
approximation algorithms  begin considering latter possibility context
likelihood weighted sampling qmr dt  turn comparative evaluation
likelihood weighted sampling variational methods time limited setting 
explore viability likelihood weighted sampler providing surrogate
gold standard  carried two independent runs consisting         samples 
figure   a  shows estimates log likelihood first sampling run
cpc cases  show variational upper lower bounds cases
 the cases sorted according lower bound   note bounds
rigorous bounds true log likelihood  thus provide direct indication
accuracy sampling estimates  although see many estimates lie
bounds  see many cases sampling estimates deviate substantially
bounds  suggests posterior marginal estimates obtained
samples likely unreliable well  indeed  figure   b  presents scatterplot
   

fijaakkola   jordan

  
 

  
   
sampling estimates  

loglikelihood

  
  
   
   
   
   

   

   

   

   

 a 

   
 

  

  
  
sorted cases

  

  

 b 

 
 

   

   
   
sampling estimates  

   

 

figure     a  upper lower bounds  solid lines  corresponding sampling estimates  dashed line  log likelihood observed findings cpc cases 
 b  correlation plot posterior marginal estimates two independent sampling runs 
estimated posterior marginals two independent runs sampler  although
see many cases results lie diagonal  indicating agreement
two runs  see many pairs posterior estimates far diagonal 
results cast doubt viability likelihood weighted sampler
general approximator full set cpc cases  even problematically appear
without reliable surrogate gold standard cases  making dicult
evaluate accuracy real time approximations variational method  note 
however  estimates figure   a  seem fall two classes estimates
lie within variational bounds estimates rather far bounds 
suggests possibility distribution sampled multi modal 
estimates falling within correct mode providing good approximations
others falling spurious modes providing seriously inaccurate approximations 
situation holds  accurate surrogate gold standard might obtained using
variational bounds filter sampling results retaining estimates
lie bounds given variational approach 
figure    provides evidence viability approach       
cpc cases independent runs sampler resulted estimates loglikelihood lying approximately within variational bounds  recomputed posterior
marginal estimates selected cases plotted figure 
scatterplot shows high degree correspondence posterior estimates
cases  thus tentatively assume estimates accurate enough serve
surrogate gold standard proceed evaluate real time approximations 
figure    plots false positives true positives    selected cpc
cases variational method  twelve positive findings treated exactly
simulation  obtaining variational estimates took      seconds computer time per
case  although curve increases rapidly tractable cpc cases 
variational algorithm still appears provide reasonably accurate ranking posterior
marginals  within reasonable time frame 
   

fivariational probabilistic inference qmr dt

 

sampling estimates  

   

   

   

   

 
 

   

   
   
sampling estimates  

   

 

figure     correlation plot selected posterior marginal estimates two
independent sampling runs  selection based variational
upper lower bounds 
  
  

false positives

  
  
  
  
  
 
 

  

  
  
true positives

  

  

figure     average number false positives function true positives variational method  solid line  likelihood weighted sampler  dashed line  
variational method    positive findings treated exactly 
sampler results averages across ten runs 
compare variational algorithm time limited version likelihood weighted
sampler ran latter algorithm period time       seconds per case  roughly comparable running time variational algorithm       seconds per case   figure   
shows corresponding plot false positives true positives  averaged ten independent runs  see curve increases significantly steeply
variational curve  find    likely diseases variational method
would need entertain top    diseases list approximately ranked
diseases  sampling method would need entertain top    approximately
ranked diseases 

    interval bounds marginal probabilities

thus far utilized variational approach produce approximations posterior marginals  approximations discussed originate upper lower
   

fijaakkola   jordan

bounds likelihood  bounds  is  guaranteed lie true posteriors  see figure    discussed
section      however  possible induce upper lower bounds posterior
marginals upper lower bounds likelihood  cf  eq       section
evaluate interval bounds qmr dt posterior marginals 
figure    displays histogram interval bounds four tractable cpc cases 
   selected cpc cases previous section  cpc cases  histograms
include diseases qmr dt network  case tractable cases
   

   

   

   

   

   

 
 

   

   

   

 a 

frequency

 

frequency

 

frequency

 

   

   

   

   

   

interval size

   

 

 b 

 
 

   

   

   

   

interval size

   

 

 c 

 
 

   

   

   

   

interval size

figure     histograms size interval bounds diseases qmrdt network  a  four tractable cpc cases   b     selected cpc cases
previous section   c  cpc cases 
variational method run    positive findings treated exactly  remaining
cpc cases variational method run    positive findings treated exactly 
running time algorithm less    seconds computer time per cpc case 
tractable cpc cases  interval bounds tight nearly diseases
network  however      positive findings treated variationally
cases      need practice compute variational bounds cases 
get somewhat better picture viability variational interval bounds
figure    b  figure    c   picture decidedly mixed     selected
cases  tight bounds provided approximately half diseases  bounds
vacuous approximately quarter diseases  range diseases
between  consider cpc cases  approximately third bounds
tight nearly half vacuous 
although results may indicate limitations variational approximation 
another immediate problem appears responsible looseness
bounds many cases  particular  recall use quickscore algorithm
 heckerman        handle exact calculations within framework variational
algorithm  unfortunately quickscore suffers vanishing numerical precision large
numbers positive findings  general begin run numerical problems 
resulting vacuous bounds     positive findings incorporated exactly
variational approximation  thus  although clearly interest run variational
algorithm longer durations  thereby improve bounds  unable
within current implementation exact subroutine 
   

 

fivariational probabilistic inference qmr dt

clearly worth studying methods quickscore treating exact findings within variational algorithm  interest consider combining
variational methods methods  search based partial evaluation
methods  based intervals  methods may help simplifying posterior
obviating need improving exact calculations 
worth emphasizing positive aspect results potential
practical utility  previous section showed variational method provide accurate approximations posterior marginals  combined interval bounds
section which calculated eciently the user obtain guarantees approximately third approximations  given relatively benign rate increase false
positives function true positives  figure      guarantees may suce  finally 
diseases bounds loose perturbation methods available
 jaakkola        help validate approximations diseases 

   discussion
let us summarize variational inference method evaluate results
obtained 
variational method begins parameterized upper lower bounds individual conditional probabilities nodes model  qmr dt  bounds
exponentials linear functions  introducing model corresponds
delinking nodes graph  sums products bounds yield bounds  thus
readily obtain parameterized bounds marginal probabilities  particular upper
lower bounds likelihood 
exploited likelihood bounds evaluating output likelihood weighted
sampling algorithm  although sampling algorithm yield reliable results across
corpus cpc cases  utilized variational upper lower bounds select
among samples able obtain sampling results consistent
runs  suggests general procedure variational bounds used assess
convergence sampling algorithm   one imagine intimate relationship
algorithms variational bounds used adjust on line
course sampler  
fact bounds likelihood  or marginal probabilities 
critical the bounding property allows us find optimizing values variational parameters minimizing upper bounding variational distribution maximizing
lower bounding variational distribution  case qmr dt network  a bipartite noisy or graph   minimization problem convex optimization problem
maximization problem solved via em algorithm 
variational parameters optimized  resulting variational distribution
exploited inference engine calculating approximations posterior probabilities 
technique focus paper  graphically  variationally transformed
model viewed sub graph original model finding
nodes delinked  sucient number findings delinked variationally
possible run exact algorithm resulting graph  approach yields
approximations posterior marginals disease nodes 
   

fijaakkola   jordan

found empirically approximations appeared provide good approximations true posterior marginals  case tractable set cpc cases
 cf  figure    and subject assumption obtained good surrogate
gold standard via selected output sampler also case full cpc
corpus  cf  figure     
compared variational algorithm state of the art algorithm qmrdt  likelihood weighted sampler shwe cooper         found variational algorithm outperformed likelihood weighted sampler tractable cases
full corpus  particular  fixed accuracy requirement variational algorithm significantly faster  cf  figure     fixed time allotment variational
algorithm significantly accurate  cf  figure   figure     
results less satisfactory interval bounds posterior marginals 
across full cpc corpus found approximately one third disease
bounds tight half diseases bounds vacuous  major impediment
obtaining tighter bounds appears lie variational approximation per se
rather exact subroutine  investigating exact methods improved
numerical properties 
although focused detail qmr dt model paper  worth
noting variational probabilistic inference methodology considerably general 
specifically  methods described limited bi partite
graphical structure qmr dt model  necessary employ noisy or nodes
 jaakkola   jordan         case type transformations
exploited qmr dt setting extend larger class dependence relations
based generalized linear models  jaakkola         finally  review applications
variational methods variety graphical model architectures  see jordan  et al 
       
promising direction future research appears integration various
kinds approximate exact methods  see  e g   dagum   horvitz        jensen  kong 
  kjrulff         particular  search based methods  cooper        peng   reggia 
      henrion        variational methods yield bounds probabilities  and 
indicated introduction  seem exploit different aspects structure complex probability distributions  may possible combine bounds
algorithm the variational bounds might used guide search  searchbased bounds might used aid variational approximation  similar comments
made respect localized partial evaluation methods bounded conditioning
methods  draper   hanks        horvitz  et al          also  seen variational
bounds used assessing whether estimates monte carlo sampling algorithms
converged  interesting hybrid would scheme variational approximations refined treating initial conditions sampler 
even without extensions results paper appear quite promising 
presented algorithm runs real time large scale graphical model
exact algorithms general infeasible  results obtained appear
reasonably accurate across corpus dicult diagnostic cases  work
needed  believe results indicate promising role variational inference
developing  critiquing exploiting large scale probabilistic models qmr dt 
   

fivariational probabilistic inference qmr dt

acknowledgements
would thank university pittsburgh randy miller use
qmr dt database  want thank david heckerman suggesting attack
qmr dt variational methods  providing helpful counsel along way 

appendix a  duality
upper lower bounds individual conditional probability distributions form
basis variational method based  dual   conjugate  representations
convex functions  present brief description convex duality appendix 
refer reader rockafellar        extensive treatment 
let f  x  real valued  convex function defined convex set x  for example 
x   rn    simplicity exposition  assume f well behaved  differentiable 
function  consider graph f   i e   points  x  f  x   n     dimensional space 
fact function f convex translates convexity set f x      f  x g
called epigraph f denoted epi f    figure      elementary property
f x 
epi f 

x     f     

x     f     

x y  

figure     half spaces containing convex set epi f    conjugate function f    
defines critical half spaces whose intersection epi f    or  equivalently 
defines tangent planes f  x  
convex sets represented intersection half spaces
contain  see figure      parameterizing half spaces obtain dual
representations convex functions  end  define half space condition 
 x    xt      

    

parameterize  non vertical  half spaces  interested characterizing half spaces contain epigraph f   require therefore points
epigraph must satisfy half space condition   x      epi f    must
xt        holds whenever xt   f  x      points epigraph
property f  x   since condition must satisfied x   x   follows
   

fijaakkola   jordan


max
f xt   f  x    g   
x x

    

well  equivalently 

max
f xt   f  x  g
x x

    

right hand side equation defines function   known
 dual   conjugate  function f      function  convex function  defines
critical half spaces needed representation epi f   intersection
half spaces  figure     
clarify duality f  x  f  x   let us drop maximum rewrite
inequality as 

xt f  x    f    

    

equation  roles two functions interchangeable may suspect
f  x  obtained dual function f  x  optimization procedure 
fact case have 

f  x    max
f xt   f    g
 

    

equality states dual dual gives back original function  provides
computational tool calculating dual functions 
concave  convex down  functions results analogous  replace max
min  lower bounds upper bounds 

appendix b  optimization variational parameters

variational method described involves replacing selected local conditional
probabilities either upper bounding lower bounding variational transformations 
product bounds bound  variationally transformed joint probability
distribution bound  upper lower  true joint probability distribution  moreover  sums bounds bound sum  obtain bounds marginal
probabilities marginalizing variationally transformed joint probability distribution 
particular  provides method obtaining bounds likelihood  the marginal
probability evidence  
note variationally transformed distributions bounds arbitrary values
variational parameters  because individually transformed node conditional probability bound arbitrary values variational parameter   obtain optimizing
values variational parameters  take advantage fact transformed
distribution bound  either minimize  in case upper bounds  maximize
 in case lower bounds  transformed distribution respect variational
parameters  optimization process provides tight bound marginal
probability interest  e g   likelihood  thereby picks particular variational
distribution subsequently used approximate inference 
   

fivariational probabilistic inference qmr dt

appendix discuss optimization problems must solve case
noisy or networks  consider upper lower bounds separately  beginning
upper bound 

upper bound transformations

goal ispto compute tight upper bound likelihood observed findings 
p  f       p  f   jd p  d   discussed section      obtain upper bound
p  f   jd  introducing upper bounds individual node conditional probabilities 
represent upper bound p  f   jd     product across individual variational transformations may contain contributions due findings treated
exactly  i e   transformed   marginalizing across obtain bound 

p  f    

x



p  f   jd   p  d  p  f   j  

    

latter quantity wish minimize respect variational parameters
 
simplify notation assume first positive findings transformed  and therefore need optimized  remaining conditional probabilities
treated exactly  notation p  f   j   given

p  f   j  

 

 

 
x
 

  

 


 
 
p  fi jd  i  
p  fi jd  p  dj  
i m
j
im
 
 
 y
 
e   p  fi  jd  i    
im

    
    

expectation taken respect posterior distribution diseases
given positive findings plan treat exactly  note proportionality
constant depend variational parameters  it likelihood exactly
treated positive findings   insert explicit forms transformed conditional
probabilities  see eq        eq       find 

p  f   j   
 

 
 
     p   f      

e   e i  j ij j
 
im
p

p

e im  i i   f  i   e e j  im ij dj

    
    

simply converted products sums exponent pulled
terms constants respect expectation  log scale 
proportionality becomes equivalence constant 
p

x




ij
j
 

j i


log p  f j     c    i i    f  i     log e e
im

   

    

fijaakkola   jordan

several observations order  recall f  i   conjugate concave function
f  the exponent   therefore concave  reason  f  i   convex 
appendix c prove remaining term 
p

log e e

j im iij dj



    

convex function variational parameters  now  since sum convex
functions convex  conclude log p  f   j   convex function variational
parameters  means local minima optimization problem 
may safely employ standard newton raphson procedure solve r log p  f   j       
alternatively utilize fixed point iterations  particular  calculate derivatives
variational form iteratively solve individual variational parameters k
derivatives zero  derivatives given follows 
 

 

  log p  f   j      log k   e  x  
k 
kj j  
 
 k
    k
j

    

    log p  f   j            var  x    
  j kj j  
    k
k     k

    

 

 

expectation variance respect posterior approximation
p  djf        derivatives computed time linear number associated diseases finding  benign scaling variance calculations comes
exploiting special properties noisy or dependence marginal independence
diseases 
calculating expectations eq      exponentially costly number exactly
treated positive findings  large number positive findings 
recourse simplified procedure optimize variational parameters
transformed positive findings  resulting variational parameters
suboptimal  found practice incurred loss accuracy typically quite
small  simulations reported paper  optimized variational parameters
approximately half exactly treated findings introduced   to precise 
case          total findings treated exactly  optimized parameters
        findings  respectively  introduced  

lower bound transformations

mimicking case upper bounds  replace individual conditional probabilities
findings lower bounding transformations  resulting lower bounding expression
p  f   jd  q   taking product p  d  marginalizing yields lower bound
likelihood 
x
p  f     p  f   jd  q  p  d  p  f   jq  
    


wish maximize p  f   jq   respect variational parameters q obtain
tightest possible bound 
   

fivariational probabilistic inference qmr dt

problem mapped onto standard optimization problem statistics 
particular  treating latent variable  f observed variable  q parameter
vector  optimization p  f   jq    or logarithm  viewed standard maximum
likelihood estimation problem latent variable model  solved using em
algorithm  dempster  laird    rubin         algorithm yields sequence variational
parameters monotonically increase objective function log p  f   jq    within em
framework  obtain update variational parameters maximizing expected
complete log likelihood 




e log p  f   jd  q  p  d   

x



n



e log p  fi  jd  qji    constant 

    

q old denotes vector variational parameters update  constant term independent variational parameters q expectation
respect posterior distribution p  djf     q old     p  f   jd  q old p  d   since variational
parameters associated conditional probabilities p  fi  jd  qji  independent one
another  maximize term sum separately  recalling form
variational transformation  see eq         have 
 

 

 

e
 
qjji e fdjg f io   qij   f   io  
j ji
j
 f   io  
    
maximize respect qj ji keeping expectations e fdj g fixed 
n

log p  fi  jd  qji 



x

optimization problem solved iteratively monotonically performing
following synchronous updates normalization 

qj ji

 

 

 

e fdj g qjji f io   qij   ij f   io   qij   qjji f   io  
j ji
j ji

 

    

f   denotes derivative f    the update guaranteed non negative  
algorithm easily extended handle case positive
findings transformed  new feature conditional
probabilities products p  f   jd  q old  p  f   jd  q   left intact  i e  
transformed  optimization respect variational parameters corresponding
transformed conditionals proceeds before 

appendix c  convexity

purpose appendix demonstrate function 
p

log e e

j im iij dj



    

convex function variational parameters   note first
ane transformap
tions change convexity properties  thus convexity x   j im ij dj implies
   

fijaakkola   jordan

convexity variational parameters   remains show
n



log e e x   log

x



pi e xi   f  x   

    

convex function vector x    fx       xn gt   indicated discrete
values range random variable x xi denoted probability measure
values pi   taking gradient f respect xk gives 

  f  x      ppk e xk q
k
 xk
pi e xi

    

 
hkl    x   x f  x      kl qk   qk ql

    

x
x
x
z  hz    qk zk      qk zk    ql zl    varfz g  

    

qk defines probability distribution  convexity revealed positive semidefinite hessian h  whose components case
k

l

see h positive semi definite  consider
k

k

l

varfz g variance discrete random variable z takes values zi
probability qi  

references

d ambrosio  b          incremental probabilistic inference  proceedings ninth
conference uncertainty artificial intelligence  san mateo  ca  morgan kaufmann 
d ambrosio  b          symbolic probabilistic inference large bn   networks  proceedings tenth conference uncertainty artificial intelligence  san mateo 
ca  morgan kaufmann 
cooper  g          nestor  computer based medical diagnostic aid integrates
causal probabilistic knowledge  ph d  dissertation  medical informatics sciences 
stanford university  stanford  ca   available umi
http   wwwlib umi com dissertations main  
cooper  g          computational complexity probabilistic inference using bayesian
belief networks  artificial intelligence              
dagum  p     horvitz  e          reformulating inference problems selective
conditioning  proceedings eighth annual conference uncertainty
artificial intelligence 
dagum  p     horvitz  e          bayesian analysis simulation algorithms inference
belief networks  networks              
   

fivariational probabilistic inference qmr dt

dagum  p     luby  m          approximate probabilistic reasoning bayesian belief
networks np hard  artificial intelligence              
dechter  r          mini buckets  general scheme generating approximations automated reasoning  proceedings fifteenth international joint conference
artificial intelligence 
dechter  r          bucket elimination  unifying framework probabilistic inference 
m  i  jordan  ed    learning graphical models  cambridge  ma  mit press 
dempster  a   laird  n     rubin  d          maximum likelihood incomplete data
via em algorithm  journal royal statistical society b           
draper  d     hanks  s          localized partial evaluation belief networks  proceedings tenth annual conference uncertainty artificial intelligence 
fung  r     chang  k  c          weighting integrating evidence stochastic simulation bayesian networks  proceedings fifth conference uncertainty
artificial intelligence  amsterdam  elsevier science 
gelfand  a     smith  a          sampling based approaches calculating marginal densities  journal american statistical association              
heckerman  d          tractable inference algorithm diagnosing multiple diseases 
proceedings fifth conference uncertainty artificial intelligence 
henrion  m          search based methods bound diagnostic probabilities large
belief nets  proceedings seventh conference uncertainty artificial intelligence 
horvitz  e  suermondt  h     cooper  g          bounded conditioning  flexible inference
decisions scarce resources  proceedings fifth conference uncertainty artificial intelligence 
jaakkola  t          variational methods inference learning graphical models 
phd thesis  department brain cognitive sciences  massachusetts institute
technology 
jaakkola  t     jordan  m          recursive algorithms approximating probabilities
graphical models  advances neural information processing systems    cambridge  ma  mit press 
jensen  c  s   kong  a     kjrulff  u          blocking gibbs sampling large
probabilistic expert systems  international journal human computer studies     
        
jensen  f          introduction bayesian networks  new york  springer 
   

fijaakkola   jordan

jordan  m   ghaharamani  z  jaakkola  t     saul  l   in press   introduction
variational methods graphical models  machine learning 
lauritzen  s     spiegelhalter  d          local computations probabilities graphical structures application expert systems  with discussion   journal
royal statistical society b              
mackay  d  j  c          introduction monte carlo methods  m  i  jordan  ed   
learning graphical models  cambridge  ma  mit press 
middleton  b   shwe  m   heckerman  d   henrion  m   horvitz  e   lehmann  h     cooper 
g          probabilistic diagnosis using reformulation internist   qmr
knowledge base ii  evaluation diagnostic performance  section medical informatics technical report smi          stanford university 
miller  r  a   fasarie  f  e     myers  j  d          quick medical reference  qmr 
diagnostic assistance  medical computing           
pearl  j          probabilistic reasoning intelligent systems  san mateo  ca  morgan
kaufmann 
peng  y     reggia  j          probabilistic causal model diagnostic problem solving  
part    diagnostic strategy  ieee trans  systems  man  cybernetics  special
issue diagnosis              
poole  d          probabilistic partial evaluation  exploiting rule structure probabilistic
inference  proceedings fifteenth international joint conference artificial
intelligence 
rockafellar  r          convex analysis  princeton university press 
shachter  r  d     peot  m          simulation approaches general probabilistic inference
belief networks  proceedings fifth conference uncertainty artificial
intelligence  elsevier science  amsterdam 
shenoy  p  p          valuation based systems bayesian decision analysis  operations
research              
shwe  m     cooper  g          empirical analysis likelihood   weighting simulation
large  multiply connected medical belief network  computers biomedical
research              
shwe  m   middleton  b   heckerman  d   henrion  m   horvitz  e   lehmann  h     g 
cooper         probabilistic diagnosis using reformulation internist  qmr knowledge base i  probabilistic model inference algorithms  methods
information medicine              

   


