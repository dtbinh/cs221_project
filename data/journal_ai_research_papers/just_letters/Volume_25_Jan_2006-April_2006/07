journal artificial intelligence research                 

submitted        published     

logical hidden markov models
kristian kersting
luc de raedt

kersting informatik uni freiburg de
deraedt informatik uni freiburg de

institute computer science
albert ludwigs universitat freiburg
georges koehler allee    
d       freiburg  germany

tapani raiko

tapani raiko hut fi

laboratory computer information science
helsinki university technology
p o  box     
fin       hut  finland

abstract
logical hidden markov models  lohmms  upgrade traditional hidden markov models
deal sequences structured symbols form logical atoms  rather flat
characters 
note formally introduces lohmms presents solutions three central inference problems lohmms  evaluation  likely hidden state sequence parameter estimation  resulting representation algorithms experimentally evaluated
problems domain bioinformatics 

   introduction
hidden markov models  hmms   rabiner   juang        extremely popular analyzing sequential data  application areas include computational biology  user modelling 
speech recognition  empirical natural language processing  robotics  despite successes  hmms major weakness  handle sequences flat  i e   unstructured symbols  yet  many applications symbols occurring sequences structured  consider  e g   sequences unix commands  may parameters
emacs lohmms tex  ls  latex lohmms tex       thus  commands essentially structured 
tasks considered unix command sequences include prediction
next command sequence  davison   hirsh         classification command
sequence user category  korvemaker   greiner        jacobs   blockeel        
anomaly detection  lane         traditional hmms cannot easily deal type
structured sequences  indeed  applying hmms requires either    ignoring structure
commands  i e   parameters      taking possible parameters explicitly
account  former approach results serious information loss  latter leads
combinatorial explosion number symbols parameters hmm
consequence inhibits generalization 
sketched problem hmms akin problem dealing structured examples traditional machine learning algorithms studied fields inductive logic programming  muggleton   de raedt        multi relational learnc
    
ai access foundation  rights reserved 

fikersting  de raedt    raiko

ing  dzeroski   lavrac         paper  propose  inductive  logic programming
framework  logical hmms  lohmms   upgrades hmms deal structure 
key idea underlying lohmms employ logical atoms structured  output state 
symbols  using logical atoms  unix command sequence represented
emacs lohmms tex   ls  latex lohmms tex         two important motivations
using logical atoms symbol level  first  variables atoms allow one make
abstraction specific symbols  e g   logical atom emacs x  tex  represents files x
latex user tex could edit using emacs  second  unification allows one share information among states  e g   sequence emacs x  tex   latex x  tex  denotes
file used argument emacs latex 
paper organized follows  reviewing logical preliminaries  introduce
lohmms define semantics section    section    upgrade basic
hmm inference algorithms use lohmms  investigate benefits lohmms
section    show lohmms strictly expressive hmms 
design order magnitude smaller corresponding propositional
instantiations  unification yield models  better fit data  section   
empirically investigate benefits lohmms real world data  concluding 
discuss related work section    proofs theorems found appendix 

   logical preliminaries
first order alphabet set relation symbols r arity    written r m 
set functor symbols f arity n    written f n  n     f called constant 
    p called propositional variable   we assume least one constant
given   atom r t            tn   relation symbol r followed bracketed n tuple
terms ti   term variable v functor symbol f t            tk   immediately followed
bracketed k tuple terms ti   variables written upper case  constant  functor predicate symbols lower case  symbol denote anonymous variables
read treated distinct  new variables time encountered  iterative
clause formula form h b h  called head  b  called body  logical
atoms  substitution    v   t            vn  tn    e g   x tex   assignment terms ti
variables vi   applying substitution term  atom clause e yields instantiated term  atom  clause e occurrences variables v simultaneously
replaced term ti   e g  ls x  emacs f  x  x tex  yields ls tex  emacs f  tex  
substitution called unifier finite set atoms singleton  unifier
called general unifier  mgu  if  unifier s  exists
substitution     term  atom clause e called ground contains
variables  i e   vars e      herbrand base   denoted hb   set
ground atoms constructed predicate functor symbols   set g  a 
atom consists ground atoms belong hb  

   logical hidden markov models
logical component traditional hmm corresponds mealy machine  hopcroft
  ullman         i e   finite state machine output symbols associated
   

filogical hidden markov models

transitions  essentially propositional representation symbols used
represent states output symbols flat  i e  structured  key idea underlying
lohmms replace flat symbols abstract symbols  abstract symbol
definition logical atom  abstract represents set ground  i e  
variable free atoms alphabet   denoted g  a   ground atoms play
role traditional symbols used hmms 
example   consider alphabet   constant symbols tex  dvi  hmm  
lohmm   relation symbols emacs    ls    xdvi    latex    atom
emacs file  tex  represents set  emacs hmm   tex   emacs lohmm   tex    assume
alphabet typed avoid useless instantiations emacs tex  tex   
use atoms instead flat symbols allows us analyze logical structured sequences
emacs hmm   tex   latex hmm   tex   xdvi hmm   dvi  


definition   abstract transition expressions form p   h
b p        
h  b atoms  variables implicitly assumed universally quantified 
i e   scope variables single abstract transition 
atoms h b represent abstract states represents abstract output symbol 

semantics abstract transition p   h
b one one states
g  b   say bb   one go probability p one states g  hb    say hb h  
emitting symbol g  ob h    say ob h  
latex file 

example   consider c       xdvi file  dvi  latex file  tex   general
h  b share predicate  due nature running example  assume state latex hmm   tex   i e 
b    file hmm    c specifies probability     next state
g   xdvi hmm   dvi      xdvi hmm   dvi     i e   probability    
next state xdvi hmm   dvi    one symbols g    latex hmm     
 latex hmm      i e   latex hmm    emitted  abstract states might
complex latex file filestem  fileextension   user 
example simple h empty  situation becomes complicated substitutions empty  then  resulting
state output symbol sets necessarily singletons  indeed  transilatex file 

tion       emacs file    dvi  latex file  tex  resulting state set would
g   emacs file    dvi      emacs hmm   tex   emacs lohmm   tex    thus transition
non deterministic two possible resulting states  therefore need
mechanism assign probabilities possible alternatives 
definition   selection distribution specifies abstract state observation
symbol alphabet distribution     a  g  a  
continue example  let  emacs hmm   tex    emacs file    tex        
 emacs lohmm   tex    emacs file    tex          would probability                next state emacs hmm   tex      
emacs lohmm   tex  
   

fikersting  de raedt    raiko



taking account  meaning abstract transition p   h
b summarized follows  let bb g  b   hb h g  hb   ob h g  ob h   
model makes transition state bb hb h emits symbol ob h probability
p  hb h   hb    ob h   ob h   

   

represent   probabilistic representation   principle   used  e g  bayesian
network markov chain  throughout remainder present paper  however 
use nave bayes approach  precisely  associate argument
r m
r m
relation r m finite domain di
constants probability distribution pi

r m
di   let vars a     v            vl   variables occurring atom r m 
let    v   s          vl  sl   substitution grounding a  vj considered
r m
random variable domain darg vj   argument arg vj   appears first in  then 
q
r m
 a   a    lj   parg vj    sj    e g   emacs hmm   tex    emacs f  e    computed
emacs  

emacs  

product p 
 hmm   p 
 tex  
thus far semantics single abstract transition defined  lohmm
usually consists multiple abstract transitions creates complication 
example   consider

emacs file 

      latex file  tex  emacs file  tex 



emacs file 

      dvi file  emacs file  user  
two abstract transitions make
conflicting statements state resulting emacs hmm   tex   indeed  according
first transition  probability     resulting state latex hmm   tex 
according second one assigns     xdvi hmm   
essentially two ways deal situation  one hand  one might want
combine normalize two transitions assign probability    respectively     
hand  one might want one rule firing  paper  chose
latter option allows us consider transitions independently  simplifies
learning  yields locally interpretable models  employ subsumption  or generality  relation among b parts two abstract transitions  indeed  b part
first transition b    emacs file  tex  specific second transition b    emacs file  user  exists substitution    user tex 
b    b    i e   b  subsumes b    therefore g   b    g   b    first transition
regarded informative second one  therefore preferred
second one starting emacs hmm   tex   say first transition specific second one  remark generality relation imposes
partial order set transitions  considerations lead strategy
considering maximally specific transitions apply state order determine
successor states  implements kind exception handling default reasoning
akin katzs        back off n gram models  back off n gram models 
detailed model deemed provide sufficiently reliable information current
context used  is  one encounters n gram sufficiently reliable 
back off use  n    gram  reliable either back off level n    etc 
conflict resolution strategy work properly provided bodies maximally specific transitions  matching given state  represent abstract state 
   

filogical hidden markov models

start

ls      

    

    
emacs f       
ls u   

emacs f  u 
emacs f       

ls      
emacs f       

emacs f    u 

latex f       
emacs f       

latex f       
latex f  tex 

emacs f  tex 
emacs f       

latex f       

figure    logical hidden markov model 

enforced requiring generality relation b parts closed
greatest lower bound  glb  predicate  i e   pair b    b  bodies 
  mgu b    b    exists  another body b  called lower bound  subsumes b 
 therefore b    subsumed b    b    lower bound
subsumed b  e g   body second abstract transition example
emacs hmm   user  set abstract transitions would closed glb 
finally  order specify prior distribution states  assume finite set
clauses form p   h start using distinguished start symbol p
probability lohmm start state g  h  
able formally define logical hidden markov models 
definition   logical hidden markov model  lohmm  tuple         
logical alphabet  selection probability   set abstract transitions 
set abstract transitions encoding prior distribution  let b set atoms
occur body parts transitions   assume b closed glb require
x
b b  
p      
   

p h
b

probabilities p clauses sum      

hmms special cases lohmms contains relation symbols arity
zero selection probability irrelevant  thus  lohmms directly generalize hmms 
lohmms represented graphically  figure   contains example  underlying language   consists   together constant symbol denotes
user employ latex  graphical notation  nodes represent abstract states
black tipped arrows denote abstract transitions  white tipped arrows used represent meta knowledge  precisely  white tipped  dashed arrows represent generality
subsumption ordering abstract states  follow transition abstract state
outgoing white tipped  dotted arrow dotted arrow always followed 
dotted arrows needed abstract state occur different cirlatex file 
cumstances  consider transition p   latex file    user    latex file  user  
   

fikersting  de raedt    raiko

   

start

    

em f  u 



em f  t 

state

abstract state

abstract state
   

ls


ls t 

em f   

em f    t 

   

la f  t 

la f    t 

abstract state

state

ls u   

em f   

state abstract state

   

la f   

   

em f    o 
state



em f  u 
abstract state

em f    u 
abstract state

figure    generating

observation
sequence
emacs hmm    latex hmm   
emacs lohmm    ls lohmm figure    command emacs
abbreviated em  f  denotes filename hmm   f  represents lohmm   denotes
tex user  user  white tipped solid arrows indicate selections 

even though atoms head body transition syntactically different
represent abstract state  accurately represent meaning transition
cannot use black tipped arrow latex file  user  itself  would actulatex file 

ally represent abstract transition p   latex file  user  latex file  user  
furthermore  graphical representation clarifies lohmms generative models  let us explain model figure   would generate observation sequence
emacs hmm    latex hmm    emacs lohmm    ls  cf  figure     chooses initial abstract state  say emacs f  u   since variables f u uninstantiated  model
samples state emacs hmm   tex  g  using   indicated dashed arrow  emacs f  tex  specific emacs f  u   moreover  emacs hmm   tex  matches
emacs f  tex   thus  model enters emacs f  tex   since value f already
instantiated previous abstract state  emacs hmm   tex  sampled probability
     now  model goes latex f  tex   emitting emacs hmm   abstract
observation emacs f  already fully instantiated  again  since f already instantiated 
latex hmm   tex  sampled probability      next  move emacs f     u   emitting latex hmm    variables f  u emacs f    u  yet bound  so  values  say
lohmm  others  sampled   dotted arrow brings us back emacs f  u  
variables implicitly universally quantified abstract transitions  scope
variables restricted single abstract transitions  turn  f treated distinct 
new variable  automatically unified f    bound lohmm   contrast 
variable u already instantiated  emitting emacs lohmm    model makes transition
ls u     assume samples tex u    then  remains ls u    probability
      considering possible samples  allows one prove following theorem 
theorem    semantics  logical hidden markov model language defines
discrete time stochastic process  i e   sequence random variables hx it          
domain
xt hb   hb    induced probability measure cartesian product
n
hb   hb   exists unique     limit  
concluding section  let us address design choices underlying lohmms 
first  lohmms introduced mealy machines  i e   output symbols
associated transitions  mealy machines fit logical setting quite intuitively
directly encode conditional probability p  o  s   s  making transition s 
   

filogical hidden markov models

emitting observation o  logical hidden markov models define distribution
x
p  o  s   s   
p  s    hb    o   o  b h  
o 
p hb
o 

sum runs abstract transitions h b b specific s 
observations correspond  partially  observed proof steps and  hence  provide information
shared among heads bodies abstract transitions  contrast  hmms usually
introduced moore machines  here  output symbols associated states implicitly
assuming s  independent  thus  p  o  s    s  factorizes p  o   s  p  s    s  
makes difficult observe information shared among heads bodies 
turn  moore lohmms less intuitive harder understand  detailed
discussion issue  refer appendix b essentially show
propositional case mealy  moore lohmms equivalent 
second  nave bayes approach selection distribution reduces model complexity expense lower expressivity  functors neglected variables
treated independently  adapting expressive approaches interesting future line
research  instance  bayesian networks allow one represent factorial hmms  ghahramani   jordan         factorial hmms viewed lohmms  hidden
states summarized   k ary abstract state  first k arguments encode k
state variables  last k arguments serve memory previous joint state 
i th argument conditioned   k th argument  markov chains allow one
sample compound terms finite depth s s s      model e g  misspelled
filenames  akin generalized hmms  kulp  haussler  reese    eeckman        
node may output finite sequence symbols rather single symbol 
finally  lohmms introduced present paper specify probability distribution sequences given length  reconsider lohmm figure    already probabilities observation sequences length    i e   ls  emacs hmm   

p
emacs lohmm    sum    precisely      holds x       xt p  x   
x       p
    xt p
  xt           order model distribution sequences variable length 
i e   t   x       xt p  x    x            xt   xt         may add distinguished end state 
end state absorbing whenever model makes transition state 
terminates observation sequence generated 

   three inference problems lohmms
hmms  three inference problems interest  let lohmm let
  o    o            ot        finite sequence ground observations 
    evaluation  determine probability p  o     sequence generated
model  
    likely state sequence  determine hidden state sequence
likely produced observation sequence o  i e    arg maxs p  s   o     
    parameter estimation  given set    o            ok   observation sequences  determine likely parameters abstract transitions selection
     
distribution   i e    arg max p  o
   

fipsfrag replacements
kersting  de raedt    raiko

sc   
abstract selection abstract
transition
transition

selection

abstract selection
sc   
transition

sc y 
ls o 
ls o 

ls o 

ls u 
ls t 

hc   

ls o 
ls t 

ls t 

hc   

ls t 
ls u 

start

hc x 

   

ls u 
em f  o 

sc z 

em f  o 

em f u 

em f  t 

em f o 

em f  t 

latex f  t 

latex f  t  latex f  t 

em f u 

o 

em f u 

o 
o 

abstract state

s 

s 

s 

em f o 

states

figure    trellis induced lohmm figure    sets reachable states time
            denoted s    s          contrast hmms  additional
layer states sampled abstract states 

address problems turn upgrading existing solutions
hmms  realized computing grounded trellis figure    possible
ground successor states given state computed first selecting applicable
abstract transitions applying selection probabilities  while taking account
substitutions  ground resulting states  two step factorization coalesced
one step hmms 
evaluate o  consider probability partial observation sequence     o            ot
 ground  state time t        given model           
 s     p  o    o            ot   qt      
qt   denotes system state time t  hmms   s  computed using dynamic programming approach       set    s    p  q         
i e      s  probability starting state and       compute  s  based
t   s    
   s      start 
                  
  
st  
  
foreach st 
  
  
  
  
  

   initialize set reachable states  
   initialize set reachable states clock t  


foreach maximally specific p   h
b s t  b   mgu s  b  exists
foreach s    hb h g  hb   s t  ot  unifies ob h
s    st
st    st  s   
 s          

 s        s      t   s  p
p
    return p  o       sst  s 

   

   

 s    hb    ot    ob h  

filogical hidden markov models

assume sake simplicity start abstract transition p   h
start   furthermore  boxed parts specify differences hmm formula 
unification taken account 
p
clearly  hmms p  o       sst  s  holds  computational complexity
forward procedure o t   b    g     o t s      maxt         t  st    
maximal number outgoing abstract transitions regard abstract state 
g maximal number ground instances abstract state  completely
analogous manner  one devise backward procedure compute
 s    p  ot     ot             ot   qt   s     
useful solving problem     
forward procedure  straightforward adapt viterbi algorithm
solution problem      i e   computing likely state sequence  let  s 
denote highest probability along single path time accounts first
observations ends state s  i e  
 s   

max

s   s       st 

p  s    s            st    st   s  o            ot   m    

procedure finding likely state sequence basically follows forward procedure  instead summing ground transition probabilities line     maximize
them  precisely  proceed follows 
  s      start 
   initialize set reachable states  
                  
  
st  
   initialize set reachable states clock t  
foreach st 
  

  
foreach maximally specific p   h
b s t  b   mgu s  b  exists
foreach s    hb h g  hb   s t  ot  unifies ob h
  
s    st
  
  
st    st  s   
 s  s          
  
   
 s  s        s  s      t   s  p  s    hb    ot    ob h  
   
foreach s  st
   
 s      maxsst   s  s   
   
 s      arg maxsst   s  s   
here   s  s    stores probability making transition s   s     with
   s    start states s  keeps track state maximizing probability along
single path time accounts first observations ends state    
likely hidden state sequence computed
st      arg max     s 


st

 

sst   
 st    

  t                

one consider problem     abstract level  instead considering
contributions different abstract transitions single ground transition state
   

fikersting  de raedt    raiko

state s  line     one might consider likely abstract transition only 
realized replacing line    forward procedure
 s       max t  s     t   s  p  s    hb    ot    ob h     
solves problem finding       likely state abstract transition
sequence 
determine sequence states abstract transitions gt  
s    t    s    t    s            st   tt   st   exists substitutions si  
si ti likely produced observation sequence o  i e 
gt   arg maxgt p  gt   o     
thus  logical hidden markov models pose new types inference problems 
parameter estimation  estimate maximum likelihood transition
probabilities selection distributions  estimate former  upgrade well known
baum welch algorithm  baum        estimating maximum likelihood parameters
hmms probabilistic context free grammars 
hmms  baum welch algorithm computes improved estimate p trano
sition probability  ground  transition p   h
b taking ratio
p  p

 t 
h 

o 

b

 t   

   

expected number  t  times making transitions time given
model observation sequence o  total number times transitions
made b time given o 
basically applies abstract transition  however 
little bit careful direct access  t   let  gcl  t 
go
probability following abstract transition via ground instance gcl p   gh gb
time t  i e  
 gcl  t   

 gb  p t    gh 
 gh   hb    ot    ob h    
p  o    

   

b   h forward procedure  see above  p  o     probability
model generated sequence o  again  boxed terms constitute main
difference corresponding hmm formula  order apply equation     compute
improved estimates probabilities associated abstract transitions  set
 t   


x
t  

 t   

x
x

 gcl  t 

t   gcl

inner sum runs ground instances t 
leads following re estimation method  assume sets
reachable states reused computations    values 
   

filogical hidden markov models

  
  
  
  
  
  
  
  
  

   initialization expected counts   
foreach
 t          using pseudocounts   
   compute expected counts   
               
foreach st


foreach max  specific p   h
b s t  b   mgu s  b  exists
foreach s    hb h g  hb   s t  s  st   mgu ot   ob h   exists

 t      t     s  p t    s    p  o      s    hb    ot    ob h  

here  equation     found line    line    set pseudocounts small samplesize regularizers  methods avoid biased underestimate probabilities even
zero probabilities m estimates  see e g   mitchell        easily adapted 
estimate selection probabilities  recall follows nave bayes scheme  therefore  estimated probability domain element domain ratio
number times selected number times d  selected 
procedure computing  values thus reused 
altogether  baum welch algorithm works follows  converged      estimate abstract transition probabilities      selection probabilities  since
instance em algorithm  increases likelihood data every update 
according mclachlan krishnan         guaranteed reach stationary
point  standard techniques overcome limitations em algorithms applicable 
computational complexity  per iteration  o k     d     o k s    k d 
k number sequences  complexity computing  values  see above  
sum sizes domains associated predicates  recently  kersting
raiko        combined baum welch algorithm structure search model
selection logical hidden markov models using inductive logic programming  muggleton
  de raedt        refinement operators  refinement operators account different
abstraction levels explored 

   advantages lohmms
section  investigate benefits lohmms      lohmms strictly
expressive hmms       using abstraction  logical variables unification
beneficial  specifically       show
 b   lohmms design smaller propositional instantiations 
 b   unification yield better log likelihood estimates 
    expressivity lohmms
whereas hmms specify probability distributions regular languages  lohmms specify
probability distributions expressive languages 

   

fikersting  de raedt    raiko

theorem    consistent  probabilistic context free grammar  pcfg  g
language l exists lohmm s t  pg  w    pm  w  w l 
proof  see appendix c  makes use abstract states unbounded depth 
precisely  functors used implement stack  without functors  lohmms cannot
encode pcfgs and  herbrand base finite  proven always
exists equivalent hmm 
furthermore  functors allowed  lohmms strictly expressive pcfgs 
specify probability distributions languages context sensitive 
     
stack s     s    

     
stack s x   s x  


      unstack s x   s x  

b
     
unstack x  y 

c
     
unstack s     y 

end
     
end

start
stack x  x 
stack x  x 
unstack s x   y 
unstack s     s y  
unstack s     s    

lohmm defines distribution  an bn cn   n      
finally  use logical variables enables one deal identifiers  identifiers
special types constants denote objects  indeed  recall unix command
sequence emacs lohmms tex  ls  latex lohmms tex        introduction  filename
lohmms tex identifier  usually  specific identifiers matter rather
fact object occurs multiple times sequence  lohmms easily deal
identifiers setting selection probability constant arguments
identifiers occur  unification takes care necessary variable bindings 
    benefits abstraction variables unification
reconsider domain unix command sequences  unix users oftenly reuse newly created directory subsequent commands mkdir vt   x   cd vt   x   ls vt   x   
unification allow us elegantly employ information allows us specify that  observing created directory  model makes transition state
newly created directory used 
p    cd dir  mkdir  mkdir dir  com 



p    cd    mkdir  mkdir dir  com 

first transition followed  cd command move newly created directory 
second transition followed  specified directory cd move to  thus 
lohmm captures reuse created directories argument future commands 
moreover  lohmm encodes simplest possible case show benefits unification  time  observation sequence uniquely determines state sequence 
functors used  therefore  left abstract output symbols associated
abstract transitions  total  lohmm u   modelling reuse directories  consists
    parameters still covers         ground  states  see appendix
complete model  compression number parameters supports  b   
empirically investigate benefits unification  compare u variant n
u variables shared  i e   unification used instance
   

filogical hidden markov models

first transition allowed  see appendix d  n     parameters less u  
computed following zero one win function
 


  log pu  o  log pn  o     
f  o   
  otherwise
leave one out cross validated unix shell logs collected greenberg         overall 
data consists     users four groups  computer scientists  nonprogrammers  novices
others         commands logged average     sessions
per user  present results subset data  considered computer
scientist sessions least single mkdir command appears  yield     logical
sequences total      ground atoms  loo win         loo statistics
favor u  

u
n

training
o 
o  log ppu  o
log p  o
o 
n  o
       
      
       

test
log p  o  log ppnu  o 
 o 
    
    
    

thus  although u     parameters n   shows better generalization performance  result supports  b    pattern often found u  
       cd dir  mkdir  mkdir dir  com 



       cd    mkdir  mkdir dir  com 

favoring changing directory made  knowledge cannot captured n
       cd    mkdir  mkdir dir  com  
results clearly show abstraction variables unification beneficial
applications  i e    b    b   hold 

   real world applications
intentions investigate whether lohmms applied real world
domains  precisely  investigate whether benefits  b    b  
exploited real world application domains  additionally  investigate whether
 b   lohmms competitive ilp algorithms utilize unification
abstraction variables 
 b   lohmms handle tree structured data similar pcfgs 
aim  conducted experiments two bioinformatics application domains  protein
fold recognition  kersting  raiko  kramer    de raedt        mrna signal structure
detection  horvath  wrobel    bohnebeck         application domains multiclass
problems five different classes each 
   sum probabilities                              use pseudo counts
subliminal non determinism  w r t  abstract states  u   i e   case first
transition fires  second one fires 

   

fikersting  de raedt    raiko

    methodology
order tackle multiclass problem lohmms  followed plug in estimate
approach  let  c    c            ck   set possible classes  given finite set training
examples   xi   yi   ni   x  c    c            cn    one tries find f   x  c    c            ck  
f  x    arg

max

c c   c       ck  

p  x   m  c   p  c   

   

low approximation error training data well unseen examples 
equation      denotes model structure classes  c denotes
maximum likelihood parameters class c estimated training examples
yi   c only  p  c  prior class distribution 
implemented baum welch algorithm  with pseudocounts m  see line    maximum likelihood parameter estimation using prolog system yap        experiments 
set     let baum welch algorithm stop change log likelihood
less     one iteration next  experiments ran pentium iv
    ghz linux machine 
    protein fold recognition
protein fold recognition concerned proteins fold nature  i e   threedimensional structures  important problem biological functions proteins
depend way fold  common approach use database searches find proteins  of known fold  similar newly discovered protein  of unknown fold   facilitate
protein fold recognition  several expert based classification schemes proteins
developed group current set known protein structures according similarity
folds  instance  structural classification proteins  hubbard  murzin  brenner    chotia         scop  database hierarchically organizes proteins according
structures evolutionary origin  machine learning perspective  scop induces
classification problem  given protein unknown fold  assign best matching group
classification scheme  protein fold classification problem investigated
turcotte  muggleton  sternberg        based inductive logic programming
 ilp  system progol kersting et al         based lohmms 
secondary structure protein domains  elegantly represented logical sequences  example  secondary structure ribosomal protein l  represented
st null      he right  alpha      st plus      he right  alpha      st plus     
he right  alpha      st plus      he right  alpha      st plus      he hright  alpha    
helices certain type  orientation length he helixtype  helixorientation  length  
strands certain orientation length st strandorientation  length  atoms
logical predicates  application traditional hmms sequences requires one
either ignore structure helices strands  results loss information 
take possible combinations  of arguments orientation length  account 
leads combinatorial explosion number parameters
   domain viewed sub section protein appears number distantly related
proteins fold independently rest protein 

   

filogical hidden markov models

end
block b length  

block s b  length  

dynamics within block

dynamics within block

block b  s p  

block s b   s p  

block b  p 

block s b   p 
transition next block

transition next block

block s b   s    

block b  s s s      

block b    

block s b     

figure    scheme left to right lohmm block model 
results reported kersting et al         indicate lohmms well suited
protein fold classification  number parameters lohmm order
magnitude smaller number corresponding hmm      versus approximately
       generalization performance      accuracy  comparable turcotte
et al s        result based ilp system progol      accuracy  kersting et al 
        however  cross validate results investigate common
bioinformatics impact primary sequence similarity classification accuracy 
instance  two commonly requested astral subsets subset sequences
less     identity     cut  less     identity
    cut   motivated this  conducted following new experiments 
data consists logical sequences secondary structure protein domains 
work kersting et al          task predict one five populated
scop folds alpha beta proteins  a b   tim beta alpha barrel  fold     nad p binding rossmann fold domains  fold     ribosomal protein l   fold      cysteine hydrolase
 fold      phosphotyrosine protein phosphatases i like  fold      class a b
proteins consists proteins mainly parallel beta sheets  beta alpha beta units  
data extracted automatically astral dataset version       chandonia 
hon  walker  lo conte  p koehl    brenner           cut    cut 
work kersting et al          consider strands helices only  i e   coils
isolated strands discarded     cut  yields     logical sequences consisting
total       ground atoms  number sequences classes listed     
                     cut  yields     logical sequences consisting total
      ground atoms  number sequences classes listed                   
   
lohmm structure  used lohmm structure follows left to right block topology 
see figure    model blocks consecutive helices  resp  strands   block
size s  say    model remain block     time steps  similar
idea used model haplotypes  koivisto  perola  varilo  hennah  ekelund  lukk 
peltonen  ukkonen    mannila        koivisto  kivioja  mannila  rastas    ukkonen 
       contrast common hmm block models  won  prugel bennett    krogh        
   

fikersting  de raedt    raiko

transition parameters shared within block one ensure model
makes transition next state s block   end block  example
exactly   intra block transitions  furthermore  specific abstract transitions
helix types strand orientations model priori distribution  intra 
inter block transitions  number blocks sizes chosen according
empirical distribution sequence lengths data beginning
ending protein domains likely captured detail  yield following block
structure
   

   

     

     

   

     

     

     

     

numbers denote positions within protein domains  furthermore  note
last block gathers remaining transitions  blocks modelled using
hidden abstract states
hc helixtype  helixorientation  length  block   sc strandorientation  length  block    
here  length denotes number consecutive bases structure element consists of 
length discretized    bins original lengths uniformally
distributed  total  lohmm     parameters  corresponding hmm without
parameter sharing       parameters  clearly confirms  b   
results  performed    fold cross validation     cut dataset  accuracy
    took approx     minutes per cross validation iteration     cut  accuracy
    took approx     minutes per cross validation iteration  results validate
kersting et al s        results and  turn  clearly show  b   holds  moreover 
novel results    cut dataset indicate similarities detected lohmms
protein domain structures accompanied high sequence similarity 
    mrna signal structure detection
mrna sequences consist bases  guanine  adenine  uracil  cytosine  fold intramolecularly form number short base paired stems  durbin  eddy  krogh    mitchison 
       base paired structure called secondary structure  cf  figures     
secondary structure contains special subsequences called signal structures responsible special biological functions  rna protein interactions cellular transport 
function signal structure class based common characteristic binding
site class elements  elements necessarily identical similar 
vary topology  tree structure   size  number constituting bases   base
sequence 
goal experiments recognize instances signal structures classes
mrna molecules  first application relational learning recognize signal structure class mrna molecules described works bohnebeck  horvath 
wrobel        horvath et al          relational instance based learner
ribl applied  dataset   used similar one described horvath
   dataset described work horvath et al         could obtain
original dataset  compare smaller data set used horvath et al   consisted

   

filogical hidden markov models

et al          consisted    mrna secondary structure sequences  precisely 
composed      secis  selenocysteine insertion sequence      ire  iron responsive
element      tar  trans activating region     histone stem loops constituting five
classes 
secondary structure composed different building blocks stacking region 
hairpin loops  interior loops etc  contrast secondary structure proteins forms
chains  secondary structure mrna forms tree  trees easily handled
using hmms  mrna secondary structure data challenging proteins 
moreover  horvath et al         report making tree structure available ribl
background knowledge influence classification accuracy  precisely 
using simple chain representation ribl achieved       leave one out cross validation
 loo  accuracy whereas using tree structure background knowledge ribl achieved
      loo accuracy 
followed horvath et al s experimental setup  is  adapted data representations lohmms compared chain model tree model 

chain representation  chain representation  see figure    
signal
structures

described

single typesingle  position  acid  

helical typehelical   position  acid   acid   
depending type  structure element represented either single   helical   
first argument
typesingle  resp 
typehelical   specifies type structure element  i e  
single  bulge   bulge   hairpin  resp  stem   argument position position sequence element within corresponding structure element counted down 
i e      n        n                n        maximal position set   
maximal position observed data  last argument encodes observed nucleotide
 pair  
used lohmm structure follows left to right block structure shown
figure    underlying idea model blocks consecutive helical structure elements  hidden states modelled using single typesingle  position  acid   block  
helical typehelical   position  acid   acid   block    block consecutive helical  resp  single  structure elements  model remain block transition
single element  transition single  resp  helical  element occurs position
n     positions n position   transitions helical  resp  single 
structure elements helical  resp  single  structure elements position capturing dynamics nucleotide pairs  resp  nucleotides  within structure elements  instance 

   signal structures close data set  larger data set  with     structures  horvath
et al  report error rate       
   nm     shorthand recursive application functor n   times  i e   position m 

   

fikersting  de raedt    raiko

helical stem  n     c  g  
helical stem  n n      c  g  
helical stem  n n n       c  g  
single bulge   n     a  
single bulge   n n      a  
single bulge   n n n       g  
helical stem  n     c  g  
helical stem  n n      c  g  
single bulge   n     a  
helical stem  n     a  a  
helical stem  n n      u  a  
helical stem  n n n       u  g  
helical stem  n n n n        u  a  
helical stem  n n n n n         c  a  
helical stem  n n n n n n          u  a  
helical stem  n n n n n n n           a  u  

u


u
c
c
c

g
g
g



g


c
c

single hairpin  n n n       a  
single hairpin  n n      u  
single hairpin  n     u  

single bulge   n     a  

g
g



u
u
u
c
u




g



u

figure    chain representation secis signal structure  ground atoms
ordered clockwise starting helical stem  n n n n n n n           a  u 
lower left hand side corner 

transitions block n    position n n    
pa  he stem n    x y 

  he stem  n     x  y  n     he stem  n n      x  y  n     
pb  he stem n    x y 

b 

he stem  n     y  x  n     he stem  n n      x  y  n     

c 

he stem  n     x    n     he stem  n n      x  y  n     

d 

he stem  n       y  n     he stem  n n      x  y  n     

e 

he stem  n         n     he stem  n n      x  y  n     

pc  he stem n    x y 

pd  he stem n    x y 
pe  he stem n    x y 

total    possible blocks maximal number blocks consecutive
helical structure elements observed data  overall  lohmm     parameters 
contrast  corresponding hmm       transitions validating  b   
results  loo test log likelihood       em iteration took average
   seconds 
without unification based transitions b d  i e   using abstract transitions
pa  he stem n    x y 

  he stem  n     x  y  n     he stem  n n      x  y  n     
e 

pe  he stem n    x y 

he stem  n         n     he stem  n n      x  y  n      

model     parameters  loo test log likelihood        em iteration took average    seconds  difference loo test log likelihood statistically
significant  paired t test  p         
omitting even transition a  loo test log likelihood dropped       
average time per em iteration    seconds  model     parameters 
difference average loo log likelihood statistically significant  paired t test  p          
results clearly show unification yield better loo test log likelihoods  i e  
 b   holds 
   

filogical hidden markov models

nucleotide pair  c  g   
nucleotide pair  c  g   
nucleotide pair  c  g   
helical s s s s s         s s s        c   stem  n n n       
nucleotide a  
nucleotide a  
nucleotide g  
single s s s s        s s s           bulge   n n n       
nucleotide pair  c  g   
nucleotide pair  c  g   
helical s s s       s      c  c  c   stem  n n      
nucleotide a  
single s s      s         bulge   n     
nucleotide pair  a  a   
nucleotide pair  u  a   
nucleotide pair  u  g   
nucleotide pair  u  a   
nucleotide pair  c  a   
nucleotide pair  u  a   
nucleotide pair  a  u   
helical s         c  c   stem  n n n n n n n           

u


u
c
c
c

g
g
g



g


c
c

g
g

single s s s s s s          s s s s s        
    hairpin  n n n       
nucleotide a  
nucleotide u  
nucleotide u  

single s s s s s s s           s s s      
    bulge   n     
nucleotide a  



u
u
u
c
u




g



u

 
s   
s s    s s s     
s s s s      
s s s s s s s         
s s s s s      
s s s s s s        

root    root   c   

figure    tree representation secis signal structure   a  logical sequence 
i e   sequence ground atoms representing secis signal structure 
ground atoms ordered clockwise starting root    root   c   lower
left hand side corner   b  tree formed secondary structure elements 

tree representation  tree representation  see figure    a    idea capture
tree structure formed secondary structure elements  see figure    b  
training instance described sequence ground facts
root    root   children  
helical id  parentid   children  type  size  
nucleotide pair basepair   
single id  parentid   children  type  size  
nucleotide base   
here  id parentid natural numbers    s     s s            encoding childparent relation   children denotes number  children      c    c  c          type
type structure element stem  hairpin         size natural number
   n     n n            atoms root    root   children  used root topology 
maximal  children   maximal size    maximal value
observed data 
trees easily handled using hmms  used lohmm basically
encodes pcfg  due theorem    possible  used lohmm structure
found appendix e  processes mrna trees in order  unification used
parsing tree  chain representation  used position argument hidden
states encode dynamics nucleotides  nucleotide pairs  within secondary structure
   here  use prolog short hand notation    lists  list either constant    representing
empty list  compound term functor     two arguments  respectively head
tail list  thus  a  b  c  compound term   a    b    c        

   

fikersting  de raedt    raiko

elements  maximal position     contrast chain representation 
nucleotide pairs  a  u  treated constants  thus  argument basepair
consists    elements 
results  loo test log likelihood        thus  exploiting tree structure
yields better probabilistic models  average  em iteration took    seconds  overall 
result shows  b   holds 
although baum welch algorithm attempts maximize different objective function  namely likelihood data  interesting compare lohmms ribl
terms classification accuracy 
classification accuracy  chain representation  loo accuracies
lohmms              considerable improvement ribls              
loo accuracy representation  tree representation  lohmm
achieved loo accuracy              comparable ribls loo accuracy
            kind representation 
thus  already chain lohmms show marked increases loo accuracy compared ribl  horvath et al          order achieve similar loo accuracies  horvath
et al         make tree structure available ribl background knowledge 
lohmms  significant influence loo test log likelihood 
loo accuracies  clearly supports  b    moreover  according horvath et al  
mrna application considered success terms application domain 
although primary goal experiments  exist alternative
parameter estimation techniques models  covariance models  eddy  
durbin        pair hidden markov models  sakakibara         might
used well basis comparison  however  lohmms employ  inductive  logic programming principles  appropriate compare systems within paradigm
ribl 

   related work
lohmms combine two different research directions  one hand  related
several extensions hmms probabilistic grammars  hand 
related recent interest combining inductive logic programming principles
probability theory  de raedt   kersting              
first type approaches  underlying idea upgrade hmms probabilistic
grammars represent structured state spaces 
hierarchical hmms  fine  singer    tishby         factorial hmms  ghahramani  
jordan         hmms based tree automata  frasconi  soda    vullo        decompose state variables smaller units  hierarchical hmms states
hmms  factorial hmms factored k state variables depend one
another observation  tree based hmms represented probability
distributions defined tree structures  key difference lohmms
approaches employ logical concept unification  unification essential
   

filogical hidden markov models

allows us introduce abstract transitions  consist detailed
states  experimental evidence shows  sharing information among abstract states
means unification lead accurate model estimation  holds relational markov models  rmms   anderson  domingos    weld        lohmms
closely related  rmms  states different types  type described
different set variables  domain variable hierarchically structured 
main differences lohmms rmms rmms either support
variable binding unification hidden states 
equivalent hmms context free languages probabilistic context free grammars  pcfgs   hmms  consider sequences logical atoms
employ unification  nevertheless  formal resemblance baum welch
algorithms lohmms pcfgs  case lohmm encodes pcfg
algorithms identical theoretical point view  re estimate parameters
ratio expected number times transition  resp  production  used
expected number times transition  resp  production  might used  proof
theorem   assumes pcfg given greibach normal form   gnf  uses
pushdown automaton parse sentences  grammars gnf  pushdown automata
common parsing  contrast  actual computations baum welch algorithm
pcfgs  called inside outside algorithm  baker        lari   young        
usually formulated grammars chomsky normal form    inside outside algorithm
make use efficient cyk algorithm  hopcroft   ullman        parsing strings 
alternative learning pcfgs strings learn structured data
skeletons  derivation trees nonterminal nodes removed  levy  
joshi         skeletons exactly set trees accepted skeletal tree automata  sta  
informally  sta  given tree input  processes tree bottom up  assigning
state node based states nodes children  sta accepts tree iff
assigns final state root tree  due automata based characterization
skeletons derivation trees  learning problem  p cfgs reduced
problem sta  particular  sta techniques adapted learning tree
grammars  p cfgs  sakakibara        sakakibara et al         efficiently 
pcfgs extended several ways  closely related lohmms
unification based grammars extensively studied computational linguistics  examples  stochastic  attribute value grammars  abney         probabilistic feature grammars  goodman         head driven phrase structure grammars  pollard   sag 
       lexical functional grammars  bresnan         learning within frameworks  methods undirected graphical models used  see work johnson       
description recent work  key difference lohmms nonterminals replaced structured  complex entities  thus  observation sequences
flat symbols atoms modelled  goodmans probabilistic feature grammars
exception  treat terminals nonterminals vectors features  abstraction
made  i e   feature vectors ground instances  unification employed 
   grammar gnf iff productions form av variable  exactly one
terminal v string none variables 
   grammar cnf iff every production form b  c a  b c variables 
terminal 

   

fikersting  de raedt    raiko

con

mkdir
con
mkdir

mv

ls

cd

mv
con

vt   x

vt   x

ls
new

vt   x

vt   x

vt   x

new

 a 

cd

vt   x

 b 

vt   x

vt   x

figure     a  atom logical sequence mkdir vt   x   mv new  vt   x  
ls vt   x   cd vt   x  forms tree  shaded nodes denote shared labels
among trees   b  sequence represented single tree  predicate con   represents concatenation operator 

therefore  number parameters needs estimated becomes easily large 
data sparsity serious problem  goodman applied smoothing overcome problem 
lohmms generally related  stochastic  tree automata  see e g   carrasco  oncina  calera rubio         reconsider unix command sequence
mkdir vt   x   mv new  vt   x   ls vt   x   cd vt   x    atom forms tree  see
figure    a   and  indeed  whole sequence atoms forms  degenerated  tree 
see figure    b   tree automata process single trees vertically  e g   bottom up  state
automaton assigned every node tree  state depends node label
states associated siblings node  focus sequential
domains  contrast  lohmms intended learning sequential domains 
process sequences trees horizontally  i e   left right  furthermore  unification
used share information consecutive sequence elements  figure    b 
illustrates  tree automata employ information allowing higher order
transitions  i e   states depend node labels states associated
predecessors             levels tree 
second type approaches  attention devoted developing highly
expressive formalisms  e g  pcup  eisele         pclp  riezler         slps  muggleton         plps  ngo   haddawy         rbns  jaeger         prms  friedman 
getoor  koller    pfeffer         prism  sato   kameya         blps  kersting   de
raedt      b      a   dprms  sanghai  domingos    weld         lohmms
seen attempt towards downgrading highly expressive frameworks  indeed  applying main idea underlying lohmms non regular probabilistic grammar  i e   replacing
flat symbols atoms  yields principle stochastic logic programs  muggleton        
consequence  lohmms represent interesting position expressiveness scale 
whereas retain essential logical features expressive formalisms 
seem easier understand  adapt learn  akin many contemporary consid   

filogical hidden markov models

erations inductive logic programming  muggleton   de raedt        multi relational
data mining  dzeroski   lavrac        

   conclusions
logical hidden markov models  new formalism representing probability distributions
sequences logical atoms  introduced solutions three central
inference problems  evaluation  likely state sequence parameter estimation 
provided  experiments demonstrated unification improve generalization
accuracy  number parameters lohmm order magnitude smaller
number parameters corresponding hmm  solutions presented
perform well practice lohmms possess several advantages traditional
hmms applications involving structured sequences 
acknowledgments authors thank andreas karwath johannes horstmann
interesting collaborations protein data  ingo thon interesting collaboration
analyzing unix command sequences  saul greenberg providing unix command sequence data  authors would thank anonymous reviewers comments considerably improved paper  research partly supported
european union ist programme contract numbers ist            fp        
 application probabilistic inductive logic programming  april  ii   tapani raiko
supported marie curie fellowship daisy  hpmt ct            

appendix a  proof theorem  
let            lohmm  show specifies time discrete stochastic
process  i e   sequence random variables hxt it           domains random
variable xt hb    herbrand base   define immediate state operator
tm  operator current emission operator em  operator 
definition    tm  operator  em  operator   operators tm    hb  hb em  
 hb  hb


tm  i     hb h    p   h
b    bb i  hb h g  h  


em  i     ob h    p   h
b    bb i  hb g g  h 
ob h g  o  
i  
  start   
                  set tm
  start      tm  tm
 
tm   start      tm   start   specifies state set clock forms random varii   start   specifies possible symbols emitted transitioning
able yi   set um
     forms variable ui   yi  resp  ui   extended random
variable zi  resp  ui   hb  

p  zi   z   



  start  
      z   tm
p  yi   z    otherwise

   

fikersting  de raedt    raiko

psfrag replacements

z 

z 

z 

u 

u 

   

u 

figure    discrete time stochastic process induced lohmm  nodes z ui
represent random variables hb  

figure   depicts influence relation among zi ui   using standard arguments
probability theory noting
p  ui   ui   zi     zi     zi   zi    
p  zi     zi    

x

p  zi     ui   zi  

p  zi     zi     ui   ui   zi  
p
ui p  zi     ui   zi  

ui

probability distributions due equation      easy show kolmogorovs extension theorem  see bauer        fristedt
gray        holds  thus 
nt
specifies unique probability distribution
 z
ui      

i  
limit  


appendix b  moore representations lohmms
hmms  moore representations  i e   output symbols associated states mealy
representations  i e   output symbols associated transitions  equivalent 
appendix  investigate extend holds lohmms 
let l mealy lohmm according definition    following  derive
notation equivalent lohmm l  moore representation abstract
transitions abstract emissions  see below   predicate b n l extended b n 
  l    domains first n arguments b n  last argument
store observation emitted  precisely  abstract transition
o v       vk  

p   h w            wl   b u            un  
l  abstract transition
p   h w            wl   o v             v k    b u            un    
l    primes o v             v k   denote replaced free   variables o v            vk  
distinguished constant symbol  say    due this  holds
 h w            wl       h w            wl   o v             v k      
   variable x vars o v            vk    free iff x   vars h w            wl    vars b u            un    

   

   

filogical hidden markov models

l  output distribution specified using abstract emissions expressions
form
      o v            vk   h w            wl   o v             v k     
   
semantics abstract transition l 
state s t g   b u            un      system make transition
s t   g   h w            wl   o v             v k     probability
p  s t     h w            wl   o v             v k      s t  


state

   

s t   mgu s t   b u            un       due equation      equation     rewritten

p  s t     h w            wl     s t    
due equation      system emit output symbol ot   g   o v            vk   
state s t   probability
 ot     o v            vk  s t   s t  
s t     mgu h w            wl   o v             v k     s t      due construction l   
exists triple  st   st     ot     l triple  s t   s t     ot           l   and vise
versa   hence both lohmms assign overall transition probability 
l l  differ way initialize sequences h s t   s t     ot   it        t  resp 
h st   st     ot   it        t    whereas l starts state s  makes transition s 
emitting o    moore lohmm l  supposed emit symbol o  s   making
transition s     compensate using prior distribution  existence
correct prior distribution l  seen follows  l  finitely many
states reachable time      i e  pl  q    s      holds finite set ground
states s  probability pl  q    s  computed similar    s   set     line
   neglecting condition ot  line     dropping  ot    ob h   line    
completely listing states s  together pl  q    s   i e   pl  q    s    start  
constitutes prior distribution l   
argumentation basically followed approach transform mealy machine
moore machine  see e g   hopcroft ullman         furthermore  mapping
moore lohmm introduced present section mealy lohmm straightforward 

appendix c  proof theorem  
let terminal alphabet n nonterminal alphabet  probabilistic context free
grammar  pcfg  g consists distinguished start symbol n plus finite set

productions
p form p   x   x n    n   p        
x n    x p      pcfg defines stochastic process sentential forms states 
leftmost rewriting steps transitions  denote single rewriting operation
grammar single arrow   result one ore rewriting operations
able rewrite  n   sequence  n   nonterminals terminals 
write   probability rewriting product probability
   

fikersting  de raedt    raiko

values associated productions used derivation  assume g consistent  i e  
sum probabilities derivations sum     
assume pcfg g greibach normal form  follows abney
et al s        theorem   g consistent  thus  every production p g
form p   x ay        yn n    order encode g lohmm  
introduce     non terminal symbol x g constant symbol nx    
terminal symbol g constant symbol t  production p g  include

abstract transition form p   stack  ny            nyn  s  
stack  nx s    n     

p   stack s 
stack  nx s    n      furthermore  include       stack  s   start
end
      end stack      straightforward prove induction g
equivalent 


appendix d  logical hidden markov model unix command
sequences
lohmms described model unix command sequences triggered mkdir 
aim  transformed original greenberg data sequence logical atoms
com  mkdir dir  lastcom   ls dir  lastcom   cd dir  dir  lastcom   cp dir  dir  lastcom 
mv dir  dir  lastcom   domain lastcom  start  com  mkdir  ls  cd  cp  mv  
domain dir consisted argument entries mkdir  ls  cd  cp  mv original
dataset  switches  pipes  etc  neglected  paths made absolute  yields
    constants domain dir  original commands  mkdir  ls  cd 
cp  mv  represented com  mkdir appear within    time steps
command c  ls  cd  cp mv   c represented com  overall  yields
       ground states covered markov model 
unification lohmm u basically implements second order markov model  i e  
probability making transition depends upon current state previous
state      parameters following structure 
com start 
mkdir dir  start  start 

com com 
mkdir dir  com  com 
end com 

furthermore  c  start  com 
mkdir dir  com 
mkdir    com 
com
end
ls dir  mkdir 
ls    mkdir 
cd dir  mkdir 









mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  

cd    mkdir 
cp    dir  mkdir 
cp dir    mkdir 
cp      mkdir 
mv    dir  mkdir 
mv dir    mkdir 
mv      mkdir 

   









mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  

filogical hidden markov models

together c  mkdir  ls  cd  cp  mv  c    cd  ls   resp 
c   cp  mv  
mkdir dir  com 
mkdir    com 
com
end
ls dir c   
ls   c   
cd dir c   
cd   c   
cp    dir c   
cp dir   c   
cp     c   
mv    dir c   
mv dir   c   
mv     c   
















c   dir c   mkdir    com 
com
c   dir c  
c   dir c  
end
ls from c   
c   dir c  
ls to c   
c   dir c  
c   dir c  
ls   c   
c   dir c  
cd from c   
c   dir c  
cd to c   
c   dir c  
cd   c   
c   dir c   cp from   c   
c   dir c  
cp    to c   
c   dir c  
cp     c   
c   dir c   mv from   c   
mv    to c   
c   dir c  
mv     c   

c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  

states fully observable  omitted output symbols associated
clauses  and  sake simplicity  omitted associated probability values 
unification lohmm n variant u variables shared

mkdir    com  cp from  to c  
ls 
com cp from  to c  
cd 
end cp from  to c   cp   
mv   

  cp 
  cp 
  cp 
  cp 






cp from  to c  
cp from  to c  
cp from  to c  
cp from  to c  

transitions affected  n     parameters less u   i e       

appendix e  tree based lohmm mrna sequences
lohmm processes nodes mrna trees in order  structure lohmm
shown end section  copies shaded parts  terms
abbreviated using starting alphanumerical  tr stands tree  helical  si
single  nuc nucleotide  nuc p nucleotide pair 
domain  children covers maximal branching factor found data  i e  
  c    c  c            c  c  c  c  c  c  c  c  c    domain type consists types occurring
data  i e    stem  single  bulge   bulge   hairpin   size  domain covers
maximal length secondary structure element data  i e   longest sequence
consecutive bases respectively base pairs constituting secondary structure element 
length encoded  n       n               n        nm     denotes recursive
application functor n times  base basepair   domains   bases
respectively    base pairs  total      parameters 
   

fimy start
      root    root  x 

copies tr id   c    pa  c  r    tr id   c  c    pa  c  r   
tr id   c  c  c    pa  c  r  
       he s id   pa      t  l 
tr id     pa  c  r  

tr    x     x  

tr id   c  c  c    pa  c   c  cs  r  

       he s id   pa  b  t  l 

       he s id   pa      t  l 
       he s id   pa  b  t  l 
tr id     pa  c   c  cs  r  
       si s id   pa  b  t  l 
       si s id   pa      t  l 

       si s id   pa  b  t  l 
       si s id   pa      t  l 
se t  l  s id   b   s id  b r  

se t  l  s id       r 

copies tr id   c    pa  c   c  cs  r    tr id   c  c    pa  c   c  cs  r   

tree
model

se t  l  s id        pa  c  cs  r   se t  l  s id   b   s id  b  pa  c  cs  r  

copies type single  bulge   bulge 
copies n n     n n n     

copies length sequence n n      n n n       n n n n      
se stem  n a   id  b  s 

se hairpin  n a   id  b  s 

copies nuc p a  g           nuc p u  u 

copies nuc g   nuc c   nuc u 
       nuc a 

         nuc p a  a 

se hairpin  a  id  b  s 

se hairpin  n     id  b  s 

se stem  a  id  b  s 

se hairpin  n     s          

       nuc a 

se stem  n     s          

         nuc p a  a 

       nuc a 

se stem  n     id  b  s 

         nuc p a  a 

copies nuc p a  g           nuc p u  u 

copies nuc g   nuc c   nuc u 
end

tr id  b  s 

sequence
model

kersting  de raedt    raiko

figure    mrna lohmm structure  symbol denotes anonymous variables
read treated distinct  new variables time encountered 
copies shaded part  terms abbreviated using starting
alphanumerical  tr stands tree  se structure element  helical 
si single  nuc nucleotide  nuc p nucleotide pair 

references

   

abney  s          stochastic attribute value grammars  computational linguistics         
       

   

start

filogical hidden markov models

abney  s   mcallester  d     pereira  f          relating probabilistic grammars automata  proceedings   th annual meeting association computational
linguistics  acl        pp          morgan kaufmann 
anderson  c   domingos  p     weld  d          relational markov models application adaptive web navigation  proceedings eighth international
conference knowledge discovery data mining  kdd        pp         edmonton  canada  acm press 
baker  j          trainable grammars speech recognition  speech communication
paper presented th   th meeting acoustical society america  pp        
boston  ma 
bauer  h          wahrscheinlichkeitstheorie     edition   walter de gruyter  berlin  new
york 
baum  l          inequality associated maximization technique statistical estimation probabilistic functions markov processes  inequalities        
bohnebeck  u   horvath  t     wrobel  s          term comparison first order similarity
measures  proceedings eigth international conference inductive logic
programming  ilp      vol       lncs  pp        springer 
bresnan  j          lexical functional syntax  blackwell  malden  ma 
carrasco  r   oncina  j     calera rubio  j          stochastic inference regular tree
languages  machine learning                   
chandonia  j   hon  g   walker  n   lo conte  l   p koehl    brenner  s         
astral compendium       nucleic acids research      d   d    
davison  b     hirsh  h          predicting sequences user actions  predicting
future  ai approaches time series analysis  pp       aaai press 
de raedt  l     kersting  k          probabilistic logic learning  acm sigkdd explorations  special issue multi relational data mining              
de raedt  l     kersting  k          probabilistic inductive logic programming 
ben david  s   case  j     maruoka  a   eds    proceedings   th international
conference algorithmic learning theory  alt        vol       lncs  pp 
     padova  italy  springer 
durbin  r   eddy  s   krogh  a     mitchison  g          biological sequence analysis 
probabilistic models proteins nucleic acids  cambridge university press 
dzeroski  s     lavrac  n   eds            relational data mining  springer verlag  berlin 
eddy  s     durbin  r          rna sequence analysis using covariance models  nucleic
acids res                     
   

fikersting  de raedt    raiko

eisele  a          towards probabilistic extensions contraint based grammars 
dorne  j   ed    computational aspects constraint based linguistics decription ii 
dyna   deliverable r    b 
fine  s   singer  y     tishby  n          hierarchical hidden markov model  analysis
applications  machine learning           
frasconi  p   soda  g     vullo  a          hidden markov models text categorization
multi page documents  journal intelligent information systems             
friedman  n   getoor  l   koller  d     pfeffer  a          learning probabilistic relational
models  proceedings sixteenth international joint conference artificial intelligence  ijcai        pp            morgan kaufmann 
fristedt  b     gray  l          modern approach probability theory  probability
applications  birkhauser boston 
ghahramani  z     jordan  m          factorial hidden markov models  machine learning 
           
goodman  j          probabilistic feature grammars  proceedings fifth international workshop parsing technologies  iwpt     boston  ma  usa 
greenberg  s          using unix  collected traces     users  tech  rep   dept 
computer science  university calgary  alberta 
hopcroft  j     ullman  j          introduction automata theory  languages 
computation  addison wesley publishing company 
horvath  t   wrobel  s     bohnebeck  u          relational instance based learning
lists terms  machine learning                 
hubbard  t   murzin  a   brenner  s     chotia  c          scop   structural classification
proteins database  nar                 
jacobs  n     blockeel  h          learning shell  automated macro construction 
user modeling       pp       
jaeger  m          relational bayesian networks  proceedings thirteenth conference uncertainty artificial intelligence  uai   pp          morgan kaufmann 
katz  s          estimation probabilities sparse data hte language model component speech recognizer  ieee transactions acoustics  speech  signal
processing  assp              
kersting  k     de raedt  l       a   adaptive bayesian logic programs  rouveirol 
c     sebag  m   eds    proceedings   th international conference inductive
logic programming  ilp      vol       lnai  pp          springer 
   

filogical hidden markov models

kersting  k     de raedt  l       b   towards combining inductive logic programming
bayesian networks  rouveirol  c     sebag  m   eds    proceedings
  th international conference inductive logic programming  ilp      vol      
lnai  pp          springer 
kersting  k     raiko  t          say em selecting probabilistic models logical
sequences  bacchus  f     jaakkola  t   eds    proceedings   st conference
uncertainty artificial intelligence  uai       pp         edinburgh  scotland 
kersting  k   raiko  t   kramer  s     de raedt  l          towards discovering structural signatures protein folds based logical hidden markov models  altman 
r   dunker  a   hunter  l   jung  t     klein  t   eds    proceedings pacific symposium biocomputing  psb      pp         kauai  hawaii  usa  world
scientific 
koivisto  m   kivioja  t   mannila  h   rastas  p     ukkonen  e          hidden markov
modelling techniques haplotype analysis  ben david  s   case  j     maruoka 
a   eds    proceedings   th international conference algorithmic learning theory  alt      vol       lncs  pp        springer 
koivisto  m   perola  m   varilo  t   hennah  w   ekelund  j   lukk  m   peltonen  l  
ukkonen  e     mannila  h          mdl method finding haplotype blocks
estimating strength haplotype block boundaries  altman  r   dunker 
a   hunter  l   jung  t     klein  t   eds    proceedings pacific symposium
biocomputing  psb      pp          world scientific 
korvemaker  b     greiner  r          predicting unix command files  adjusting user
patterns  adaptive user interfaces  papers      aaai spring symposium 
pp       
kulp  d   haussler  d   reese  m     eeckman  f          generalized hidden markov
model recognition human genes dna  states  d   agarwal  p  
gaasterland  t   hunter  l     smith  r   eds    proceedings fourth international conference intelligent systems molecular biology  ismb      pp     
    st  louis  mo  usa  aaai 
lane  t          hidden markov models human computer interface modeling 
rudstrom  a   ed    proceedings ijcai    workshop learning users 
pp       stockholm  sweden 
lari  k     young  s          estimation stochastic context free grammars using
inside outside algorithm  computer speech language          
levy  l     joshi  a          skeletal structural descriptions  information control 
              
mclachlan  g     krishnan  t          em algorithm extensions  wiley  new
york 
   

fikersting  de raedt    raiko

mitchell  t  m          machine learning  mcgraw hill companies  inc 
muggleton  s          stochastic logic programs  de raedt  l   ed    advances
inductive logic programming  pp          ios press 
muggleton  s     de raedt  l          inductive logic programming  theory methods 
journal logic programming                  
ngo  l     haddawy  p          answering queries context sensitive probabilistic
knowledge bases  theoretical computer science              
pollard  c     sag  i          head driven phrase structure grammar  university
chicago press  chicago 
rabiner  l     juang  b          introduction hidden markov models  ieee assp
magazine             
riezler  s          statistical inference probabilistic modelling constraint based
nlp  schrder  b   lenders  w     und t  portele  w  h   eds    proceedings
 th conference natural language processing  konvens      corr
cs cl         
sakakibara  y          efficient learning context free grammars positive structural
examples  information computation               
sakakibara  y          pair hidden markov models tree structures  bioinformatics 
    suppl     i   i    
sakakibara  y   brown  m   hughey  r   mian  i   sjolander  k     underwood  r         
stochastic context free grammars trna modelling  nucleic acids research 
                  
sanghai  s   domingos  p     weld  d          dynamic probabilistic relational models 
gottlob  g     walsh  t   eds    proceedings eighteenth international joint
conference artificial intelligence  ijcai      pp         acapulco  mexico  morgan kaufmann 
sato  t     kameya  y          parameter learning logic programs symbolic statistical
modeling  journal artificial intelligence research  jair              
scholkopf  b     warmuth  m   eds            learning parsing stochastic unificationbased grammars  vol       lncs  springer 
turcotte  m   muggleton  s     sternberg  m          effect relational background
knowledge learning protein three dimensional fold signatures  machine learning 
               
won  k   prugel bennett  a     krogh  a          block hidden markov model biological sequence analysis  negoita  m   howlett  r     jain  l   eds    proceedings
eighth international conference knowledge based intelligent information
engineering systems  kes      vol       lncs  pp        springer 

   


