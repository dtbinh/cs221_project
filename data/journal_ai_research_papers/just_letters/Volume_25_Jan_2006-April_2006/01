journal artificial intelligence research                

submitted        published      

decision theoretic planning non markovian rewards
sylvie thiebaux
charles gretton
john slaney
david price

sylvie thiebaux anu edu au
charles gretton anu edu au
john slaney anu edu au
david price anu edu au

national ict australia  
australian national university
canberra  act       australia

froduald kabanza

kabanza usherbrooke ca

departement dinformatique
universite de sherbrooke
sherbrooke  quebec j k  r   canada

abstract
decision process rewards depend history rather merely current state called decision process non markovian rewards  nmrdp   decisiontheoretic planning  many desirable behaviours naturally expressed properties execution sequences rather properties states  nmrdps form
natural model commonly adopted fully markovian decision process  mdp  model 
tractable solution methods developed mdps directly apply
presence non markovian rewards  number solution methods nmrdps
proposed literature  exploit compact specification non markovian
reward function temporal logic  automatically translate nmrdp equivalent mdp solved using efficient mdp solution methods  paper presents
nmrdpp non markovian reward decision process planner   software platform
development experimentation methods decision theoretic planning nonmarkovian rewards  current version nmrdpp implements  single interface 
family methods based existing well new approaches describe detail  include dynamic programming  heuristic search  structured methods  using
nmrdpp  compare methods identify certain problem features affect
performance  nmrdpps treatment non markovian rewards inspired treatment
domain specific search control knowledge tlplan planner  incorporates
special case  first international probabilistic planning competition  nmrdpp
able compete perform well domain independent hand coded
tracks  using search control knowledge latter 

c
    
ai access foundation  rights reserved 

fithiebaux  gretton  slaney  price   kabanza

   introduction
    problem
markov decision processes  mdps  widely accepted preferred model
decision theoretic planning problems  boutilier  dean    hanks         fundamental
assumption behind mdp formulation system dynamics
reward function markovian  therefore  information needed determine reward
given state must encoded state itself 
requirement always easy meet planning problems  many desirable
behaviours naturally expressed properties execution sequences  see e g   drummond        haddawy   hanks        bacchus   kabanza        pistore   traverso 
       typical cases include rewards maintenance property  periodic
achievement goal  achievement goal within given number steps
request made  even simply first achievement goal
becomes irrelevant afterwards 
instance  consider health care robot assists ederly disabled people
achieving simple goals reminding important tasks  e g  taking pill  
entertaining them  checking transporting objects  e g  checking stoves
temperature bringing coffee   escorting them  searching  e g  glasses
nurse   cesta et al          domain  might want reward robot making
sure given patient takes pill exactly every   hours  and penalise fails
prevent patient within time frame    may
reward repeatedly visiting rooms ward given order reporting
problem detects  may receive reward patients request answered
within appropriate time frame  etc  another example elevator control domain
 koehler   schuster         elevator must get passengers origin
destination efficiently possible  attempting satisfying range
conditions providing priority services critical customers  domain 
trajectories elevator desirable others  makes natural encode
problem assigning rewards trajectories 
decision process rewards depend sequence states passed
rather merely current state called decision process non markovian
rewards  nmrdp   bacchus  boutilier    grove         difficulty nmrdps
efficient mdp solution methods directly apply them  traditional way
circumvent problem formulate nmrdp equivalent mdp  whose states
result augmenting original nmrdp extra information capturing
enough history make reward markovian  hand crafting mdp however
difficult general  exacerbated fact size mdp
impacts effectiveness many solution methods  therefore  interest
automating translation mdp  starting natural specification nonmarkovian rewards systems dynamics  bacchus et al         bacchus  boutilier 
  grove         problem focus on 

  

fidecision theoretic planning non markovian rewards

    existing approaches
solving nmrdps setting  central issue define non markovian reward
specification language translation mdp adapted class mdp solution
methods representations would use type problems hand 
precisely  tradeoff effort spent translation  e g  producing
small equivalent mdp without many irrelevant history distinctions  effort required
solve it  appropriate resolution tradeoff depends type representations
solution methods envisioned mdp  instance  structured representations
solution methods ability ignore irrelevant information may cope
crude translation  state based  flat  representations methods require
sophisticated translation producing mdp small feasible 
two previous proposals within line research rely past linear temporal
logic  pltl  formulae specify behaviours rewarded  bacchus et al               
nice feature pltl yields straightforward semantics non markovian
rewards  lends range translations crudest finest  two
proposals adopt different translations adapted two different types solution
methods representations  first  bacchus et al         targets classical state based
solution methods policy iteration  howard        generate complete policies
cost enumerating states entire mdp  consequently  adopts expensive
translation attempts produce minimal mdp  contrast  second translation
 bacchus et al         efficient crude  targets structured solution methods
representations  see e g   hoey  st aubin  hu    boutilier        boutilier  dearden   
goldszmidt        feng   hansen         require explicit state enumeration 
    new approach
first contribution paper provide language translation adapted
another class solution methods proven quite effective dealing large
mdps  namely anytime state based heuristic search methods lao   hansen  
zilberstein         lrtdp  bonet   geffner         ancestors  barto  bardtke   
singh        dean  kaelbling  kirman    nicholson        thiebaux  hertzberg  shoaff 
  schneider         methods typically start compact representation
mdp based probabilistic planning operators  search forward initial state 
constructing new states expanding envelope policy time permits  may
produce approximate even incomplete policy  explicitly construct explore
fraction mdp  neither two previous proposals well suited
solution methods  first cost translation  most performed
prior solution phase  annihilates benefits anytime algorithms  second
size mdp obtained obstacle applicability state based
methods  since cost translation size mdp results
severely impact quality policy obtainable deadline  need
appropriate resolution tradeoff two 
approach following main features  translation entirely embedded
anytime solution method  full control given parts mdp
explicitly constructed explored  mdp obtained minimal 
  

fithiebaux  gretton  slaney  price   kabanza

minimal size achievable without stepping outside anytime framework  i e  
without enumerating parts state space solution method would necessarily
explore  formalise relaxed notion minimality  call blind minimality
reference fact require lookahead  beyond fringe  
appropriate context anytime state based solution methods  want
minimal mdp achievable without expensive pre processing 
rewarding behaviours specified pltl  appear
way achieving relaxed notion minimality powerful blind minimality without
prohibitive translation  therefore instead pltl  adopt variant future linear
temporal logic  fltl  specification language  extend handle rewards 
language complex semantics pltl  enables natural translation blind minimal mdp simple progression reward formulae  moreover 
search control knowledge expressed fltl  bacchus   kabanza        fits particularly
nicely framework  used dramatically reduce fraction search
space explored solution method 
    new system
second contribution nmrdpp  first reported implementation nmrdp solution
methods  nmrdpp designed software platform development experimentation common interface  given description actions domain  nmrdpp
lets user play compare various encoding styles non markovian rewards
search control knowledge  various translations resulting nmrdp mdp 
various mdp solution methods  solving problem  made record
range statistics space time behaviour algorithms  supports
graphical display mdps policies generated 
nmrdpps primary interest treatment non markovian rewards 
competitive platform decision theoretic planning purely markovian rewards 
first international probabilistic planning competition  nmrdpp able enrol
domain independent hand coded tracks  attempting problems featuring
contest  thanks use search control knowledge  scored second place
hand coded track featured probabilistic variants blocks world logistics
problems  surprisingly  scored second domain independent subtrack consisting problems taken blocks world logistic domains 
latter problems released participants prior competition 
    new experimental analysis
third contribution experimental analysis factors affect performance
nmrdp solution methods  using nmrdpp  compared behaviours
influence parameters structure degree uncertainty dynamics 
type rewards syntax used described them  reachability conditions
tracked  relevance rewards optimal policy  able identify number
general trends behaviours methods provide advice concerning
best suited certain circumstances  experiments lead us rule one

  

fidecision theoretic planning non markovian rewards

methods systematically underperforming  identify issues claim
minimality made one pltl approaches 
    organisation paper
paper organised follows  section   begins background material mdps 
nmrdps  existing approaches  section   describes new approach section  
presents nmrdpp  sections     report experimental analysis various approaches  section   explains used nmrdpp competition  section   concludes
remarks related future work  appendix b gives proofs theorems 
material presented compiled series recent conference workshop
papers  thiebaux  kabanza    slaney      a      b  gretton  price    thiebaux      a 
    b   details logic use represent rewards may found      paper
 slaney        

   background
    mdps  nmrdps  equivalence
start notation definitions  given finite set states  write
set finite sequences states s  set possibly infinite
state sequences  stands possibly infinite state sequence
natural number  mean state index    i  mean prefix
h                  denotes concatenation    
      mdps
markov decision process type consider   tuple hs  s    a  pr  ri 
finite set fully observable states  s  initial state  finite set actions
 a s  denotes subset actions applicable s    pr s  a      s  a s  
family probability distributions s  pr s  a  s    probability
state s  performing action state s  r     ir reward function
r s  immediate reward state s  well known mdp
compactly represented using dynamic bayesian networks  dean   kanazawa       
boutilier et al         probabilistic extensions traditional planning languages  see e g  
kushmerick  hanks    weld        thiebaux et al         younes   littman        
stationary policy mdp function     a   s  a s 
action executed state s  value v policy s    seek
maximise  sum expected future rewards infinite horizon  discounted
far future occur 
v  s      lim e
n

x
n





r i           s 

i  

      discount factor controlling contribution distant rewards 

  

fithiebaux  gretton  slaney  price   kabanza

                             
        
   
    
   
    
 

                     
   
           
 
    
                   

 
r



                                  
    
       
   
   
  

initial state s    p false two actions
possible  causes transition s  probability
     change probability      b
transition probabilities      state s    p true 
actions c  stay go  lead s  s 
respectively probability   
reward received first time p true 
subsequently  is  rewarded state sequences are 
hs    s 
hs    s    s 
hs    s    s    s 
hs    s    s    s    s 
etc 



  
  
   
   
   
  
    
   
 
 
 
 
 
       
 
 
 
 
 
 
 
 
 
 
                                  
                                
   
   
   
   
   
   
    
 
  
  
  
  
   
  
   
 
  
   
 
    
  
                               
           
           
     
 
           
 
    
                   



   

b
  

   

   

   

   

 s  
i     
 
   
  
c

     
   
    
            

   
  

                      

figure    simple nmrdp
      nmrdps
decision process non markovian rewards identical mdp except
domain reward function   idea process passed
state sequence  i  stage i  reward r  i   received stage i  figure  
gives example  reward function  policy nmrdp depends history 
mapping a  before  value policy expectation
discounted cumulative reward infinite horizon 
x

n

v  s      lim e
r  i           s 
n

i  

e
decision process   hs  s    a  pr  ri state s  let d s 
stand
set state sequences rooted feasible actions d  is 
e
d s 
          a i   pr i   a  i           note definition
e
d s  depend r therefore applies mdps nmrdps 
      equivalence
clever algorithms developed solve mdps cannot directly applied nmrdps 
one way dealing problem translate nmrdp equivalent mdp
expanded state space  bacchus et al          expanded states mdp
 e states  short  augment states nmrdp encoding additional information
sufficient make reward history independent  instance  want reward
first achievement goal g nmrdp  states equivalent mdp would
carry one extra bit information recording whether g already true  e state
seen labelled state nmrdp  via function definition   below 
history information  dynamics nmrdps markovian  actions
probabilistic effects mdp exactly nmrdp  following definition 
adapted given bacchus et al          makes concept equivalent mdp
precise  figure   gives example 

  

fidecision theoretic planning non markovian rewards


                              
       
  
    
   
    
 



                                 
    
       
   
   
  

                   
    
 
            
           
  
  
                
   
  
   
   
    
   
 
 
 
 
 
       
 
 
 
 
 
 
 
 
 
 
 
 
 
                              
                                
 
   
   
   
   
   
  
    
 
  
  
  
  
   
 
  
   
 
 
   
 
 
    
  
                              
 
           
                      
   
 
 
 
          
               

 
r  




   

   

b
  

   

 s  

c    

 
    
 
 
     
   
    
           

   

   

                      
           
      
   
   
   
    
 
 
  
                                   
 
 
 
 
 
  
 
 
 
 
 
 
          
  
     
 
 
 
 
 
 
 
 
 
 
 
 
   
 
   
 
 
 
 
   
                    
                    
   
   
 
 
            
            
           
           
                
               
   
    
      
 
    
      
        
   
                                          
   
          
 
  
   
   
  
  
   
 
 
 
    
 
 
 
 
       
                                 

   

   

              



b

 

c

 







   
  
  
    
            

   





 


   

figure    mdp equivalent nmrdp figure     s        s       s     s      
 s       s    initial state s     state s   rewarded  states not 

definition   mdp d  hs     s     a    pr   r  equivalent nmrdp   hs  s    a  pr  ri
exists mapping       that  
    s       s   
   s      a   s      a   s     
   s    s  s  a s    pr s    a  s         s    
 s       s    exists unique s        s       s   
a  a   s      pr   s     a    s       pr s    a    s    
e      
e    s          
   feasible state sequence d s
 

i  have  r    i     r  i   i 
items    ensure bijection feasible state sequences nmrdp
feasible e state sequences mdp  therefore  stationary policy mdp
reinterpreted non stationary policy nmrdp  furthermore  item   ensures
two policies identical values  consequently  solving nmrdp optimally
reduces producing equivalent mdp solving optimally  bacchus et al         
proposition   let nmrdp  d  equivalent mdp it    policy
e       i            
d    let function defined sequence prefixes  i  d s

 
j  j     j   policy v  s      v   s     
   technically  definition allows sets actions a  different  action
differ must inapplicable reachable states nmrdp e states equivalent
mdp  practical purposes  a  seen identical 

  

fithiebaux  gretton  slaney  price   kabanza

    existing approaches
existing approaches nmrdps  bacchus et al               use temporal logic
past  pltl  compactly represent non markovian rewards exploit compact
representation translate nmrdp mdp amenable off the shelf solution
methods  however  target different classes mdp representations solution methods  consequently  adopt different styles translations 
bacchus et al         target state based mdp representations  equivalent mdp
first generated entirely involves enumeration e states transitions
them  then  solved using traditional dynamic programming methods
value policy iteration  methods extremely sensitive number
states  attention paid producing minimal equivalent mdp  with least number
states   first simple translation call pltlsim produces large mdp
post processed minimisation solved  another  call pltlmin 
directly results minimal mdp  relies expensive pre processing phase 
second approach  bacchus et al          call pltlstr  targets structured
mdp representations  transition model  policies  reward value functions represented compact form  e g  trees algebraic decision diagrams  adds   hoey et al  
      boutilier et al          instance  probability given proposition  state
variable  true execution action specified tree whose internal
nodes labelled state variables whose previous values given variable depends  whose arcs labelled possible previous values      variables 
whose leaves labelled probabilities  translation amounts augmenting
structured mdp new temporal variables tracking relevant properties state
sequences  together compact representation     dynamics  e g  trees
previous values relevant variables      non markovian reward function
terms variables current values  then  structured solution methods structured
policy iteration spudd algorithm run resulting structured mdp  neither
translation solution methods explicitly enumerates states 
review approaches detail  reader referred respective
papers additional information 
      representing rewards pltl
syntax pltl  language chosen represent rewarding behaviours 
propositional logic  augmented operators  previously   since   see emerson         whereas classical propositional logic formula denotes set states  a subset
s   pltl formula denotes set finite sequences states  a subset    formula
without temporal modality expresses property must true current state  i e  
last state finite sequence  f specifies f holds previous state  the
state one last   f  f    requires f  true point sequence  and  unless point present  f  held ever since  formally 
modelling relation    stating whether formula f holds finite sequence  i  defined
recursively follows 
 i     p iff p   p p  set atomic propositions

  

fidecision theoretic planning non markovian rewards

 i     f iff  i      f
 i     f  f  iff  i     f   i     f 
 i     f iff      i       f
 i     f  f  iff j i   j     f  k  j   k i   k     f 
  f   f meaning f true
s  one define useful operators
  f meaning f always true  e g  g
  g denotes
point  fif
set finite sequences ending state g true first time sequence 
  k f
useful abbreviation k  k times ago   k iterations modality 
ki   f  f true k last steps   fik f ki   f  f true
k last steps  
non markovian reward functions described set pairs  fi   ri  
pltl reward formula ri real  semantics reward assigned
sequence sum ri sequence model   below  let
f denote set reward formulae description reward function  bacchus
et al         give list behaviours might useful reward  together
expression pltl  instance  f atemporal formula   f   r  rewards
r units achievement f whenever happens  markovian reward 
  f   r  rewards every state following  and including  achievement f  
contrast  
  f   r  rewards first occurrence f    f fik f   r  rewards occurrence
 f
f every k steps   n   r  rewards nth state  independently
properties     f  f  f    r  rewards occurrence f  immediately followed
f  f    reactive planning  so called response formulae describe
achievement f triggered condition  or command  c particularly useful 
  c   r  every state f true following first issue
written  f
command rewarded  alternatively  written  f  f c    r 
first occurrence f rewarded command  common
  k c   r 
reward achievement f within k steps trigger  write example  f
reward states f holds 
theoretical point view  known  lichtenstein  pnueli    zuck       
behaviours representable pltl exactly corresponding star free regular
languages  non star free behaviours  pp   reward even number states
containing p  therefore representable  nor  course  non regular behaviours
pn q n  e g  reward taking equal numbers steps left right   shall
speculate severe restriction purposes planning 
      principles behind translations
three translations mdp  pltlsim  pltlmin  pltlstr  rely equivalence f  f  f   f   f  f      decompose temporal modalities
requirement last state sequence  i   requirement
prefix  i    sequence  precisely  given state formula f   one com 

  

fithiebaux  gretton  slaney  price   kabanza

pute in  o   f     new formula reg f  s  called regression f s  regression
property that       f true finite sequence  i  ending   iff
reg f  s  true prefix  i     is  reg f  s  represents must
true previously f true now  reg defined follows 
reg p  s      iff p otherwise  p p
reg f  s    reg f  s 
reg f  f    s    reg f    s  reg f    s 
reg f  s    f
reg f  f    s    reg f    s   reg f    s   f  f    
instance  take state p holds q not  take f    q   p q  
meaning q must false   step ago  must held point
past p must held since q last did  reg f  s    q  p q   is 
f hold now  previous stage  q false p q requirement
still hold  p q false s  reg f  s      indicating f
cannot satisfied  regardless came earlier sequence 
notational convenience  x set formulae write x x x   x x  
translations exploit pltl representation rewards follows  expanded
state  e state  generated mdp seen labelled set sub f  
subformulae reward formulae f  and negations   subformulae must
    true paths leading e state      sufficient determine current
truth reward formulae f   needed compute current reward  ideally
    small enough enable that  i e  contain
subformulae draw history distinctions irrelevant determining reward
one point another  note however worst case  number distinctions
needed  even minimal equivalent mdp  may exponential   f     happens
instance formula k f   requires k additional bits information memorising
truth f last k steps 
      pltlsim
choice s  bacchus et al         consider two cases  simple case 
call pltlsim  mdp obeying properties         produced simply labelling
e state set subformulae sub f   true sequence leading
e state  mdp generated forward  starting initial e state labelled
s  set   sub f   subformulae true sequence
hs  i  successors e state labelled nmrdp state subformula set
generated follows  labelled successor s  nmrdp
set subformulae     sub f        reg      s     
instance  consider nmrdp shown figure    set f    qp  consists
single reward formula  set sub f   consists subformulae reward formula 
   size   f    reward formula measured length size   f    set reward formulae
f measured sum lengths formulae f  

  

fidecision theoretic planning non markovian rewards

start state
a      
p

a    b   

a       b     
a      

a      

b     

q

a      b   

a     
p  q

a    b   

initial state  p q false 
p false  action independently sets
p q true probability     
p q false  action b sets q true
probability      actions
effect otherwise  reward obtained
whenever q p  optimal policy
apply b q gets produced  making
sure avoid state left hand side 
apply p gets produced 
apply b indifferently forever 

figure    another simple nmrdp
negations  sub f      p  q  p  p  q p  p  q  p  p   q
p    equivalent mdp produced pltlsim shown figure   
      pltlmin
unfortunately  mdps produced pltlsim far minimal  although could
postprocessed minimisation invoking mdp solution method 
expansion may still constitute serious bottleneck  therefore  bacchus et al         consider
complex two phase translation  call pltlmin  capable producing
mdp satisfying property      here  preprocessing phase iterates states
s  computes  state s  set l s  subformulae  function l
solution fixpoint equation l s    f  reg      s        l s     s  successor s  
subformulae l s  candidates inclusion sets labelling respective
e states labelled s  is  subsequent expansion phase above  taking
  l s      l s    instead   sub f     sub f    subformulae
l s  exactly relevant way feasible execution sequences starting
e states labelled rewarded  leads expansion phase produce minimal
equivalent mdp 
figure   shows equivalent mdp produced pltlmin nmrdp example
figure    together function l labels built  observe
mdp smaller pltlsim mdp  reach state left hand side
p true q false  point tracking values subformulae 
q cannot become true reward formula cannot either  reflected fact
l  p   contains reward formula 
worst case  computing l requires space  number iterations s 
exponential   f     hence question arises whether gain expansion
phase worth extra complexity preprocessing phase  one questions
experimental analysis section   try answer 
      pltlstr
pltlstr translation seen symbolic version pltlsim  set
added temporal variables contains purely temporal subformulae ptsub f   reward
formulae f   modality prepended  unless already there        

  

fithiebaux  gretton  slaney  price   kabanza

start state
f  f  f  f  f  
reward  
a      

a       b     

a      

p
f  f  f  f  f  
reward  

a      

q
f  f  f  f  f  
reward  

a    b   

p
f  f  f  f  f  
reward  

following subformulae sub f   label
e states 
f    p
f    q
f    p
f    p
f    q p
f    p
f    q
f    p
f    p
f      q p 

a      b   

a     

p
f  f  f  f  f  
reward  
a   

b     

p  q
f  f  f  f  f  
reward  
b   

a   

a    b   

b   

p  q
f  f  f  f  f  
reward  
a    b   
p  q
f  f  f  f  f 
reward  

a    b   

figure    equivalent mdp produced pltlsim

start state
f  f  f 
reward  
a      
p
f 
reward  

a    b   

a       b     

a      

b     

q
f  f  f 
reward  

a      

function l given by 
l        q p  p  p 
l  p      q p 
l  q      q p  p  p 
l  p  q      q p  p  p 

a      b   

a     

following formulae label e states 
f    q p
f    p
f    p
f     q p 
f    p
f    p

p  q
f  f  f 
reward  
a   

b   

p  q
f  f  f 
reward  
a   

b   

p  q
f  f  f 
reward  

a    b   

figure    equivalent mdp produced pltlmin

  

fidecision theoretic planning non markovian rewards

q

p

    

prv prv p

prv p

    

   dynamics p

    

    

   dynamics p

    

    

   reward

figure    adds produced pltlstr  prv  previously  stands
ptsub f               ptsub f     repeatedly applying equivalence
f  f  f   f   f  f     subformula ptsub f    express current
value  hence reward formulae  function current values formulae
state variables  required compact representation transition
reward models 
nmrdp example figure    set purely temporal variables ptsub f    
 p  p   identical ptsub f    figure   shows adds forming
part symbolic mdp produced pltlstr  adds describing dynamics
temporal variables  i e   adds describing effects actions b
respective values  add describing reward 
complex illustration  consider example  bacchus et al        
   p  q r        p  q r   
f    

ptsub f         p  q r    p  q r   r 
set temporal variables used
   t        p  q r     t     p  q r    t    r 
using equivalences  reward decomposed expressed means
propositions p  q temporal variables t    t    t  follows 
   p  q r  

 p  q r       p  q r   
 q r   p  p  q r    t 

 q t     p t    t 
pltlsim  underlying mdp produced pltlstr far minimal
encoded history features even vary one state next  however  size
problematic state based approaches  structured solution methods
enumerate states able dynamically ignore variables become
irrelevant policy construction  instance  solving mdp  may
  

fithiebaux  gretton  slaney  price   kabanza

able determine temporal variables become irrelevant situation
track  although possible principle  costly realised good policy 
dynamic analysis rewards contrast pltlmins static analysis  bacchus et al  
      must encode enough history determine reward reachable future
states policy 
one question arises circumstances analysis irrelevance structured solution methods  especially dynamic aspects  really effective 
another question experimental analysis try address 

   fltl  forward looking approach
noted section   above  two key issues facing approaches nmrdps
specify reward functions compactly exploit compact representation
automatically translate nmrdp equivalent mdp amenable chosen
solution method  accordingly  goals provide reward function specification
language translation adapted anytime state based solution methods 
brief reminder relevant features methods  consider two goals
turn  describe syntax semantics language  notion formula
progression language form basis translation  translation
itself  properties  embedding solution method  call approach
fltl  finish section discussion features distinguish fltl
existing approaches 
    anytime state based solution methods
main drawback traditional dynamic programming algorithms policy iteration
 howard        explicitly enumerate states reachable s 
entire mdp  interest state based solution methods  may
produce incomplete policies  enumerate fraction states policy iteration
requires 
let e   denote envelope policy   set states reachable
 with non zero probability  initial state s  policy  defined
e    say policy complete  incomplete otherwise 
set states e   undefined called fringe policy 
fringe states taken absorbing  value heuristic  common feature
anytime state based algorithms perform forward search  starting s 
repeatedly expanding envelope current policy one step forward adding one
fringe states  provided admissible heuristic values fringe states 
eventually converge optimal policy without necessarily needing explore
entire state space  fact  since planning operators used compactly represent
state space  may even need construct small subset mdp
returning optimal policy  interrupted convergence  return
possibly incomplete often useful policy 
methods include envelope expansion algorithm  dean et al         
deploys policy iteration judiciously chosen larger larger envelopes  using successive policy seed calculation next  recent lao algorithm  hansen
  

fidecision theoretic planning non markovian rewards

  zilberstein        combines dynamic programming heuristic search
viewed clever implementation particular case envelope expansion algorithm 
fringe states given admissible heuristic values  policy iteration run
convergence envelope expansions  clever implementation runs
policy iteration states whose optimal value actually affected new fringe
state added envelope  another example backtracking forward search
space  possibly incomplete  policies rooted s   thiebaux et al          performed interrupted  point best policy found far returned  real time
dynamic programming  rtdp   barto et al         another popular anytime algorithm
mdps learning real time  korf        deterministic domains 
asymptotic convergence guarantees  rtdp envelope made sample
paths visited frequency determined current greedy policy
transition probabilities domain  rtdp run on line  off line given number
steps interrupted  variant called lrtdp  bonet   geffner        incorporates
mechanisms focus search states whose value yet converged  resulting
convergence speed finite time convergence guarantees 
fltl translation present targets anytime algorithms  although
could used traditional methods value policy iteration 
    language semantics
compactly representing non markovian reward functions reduces compactly representing
behaviours interest  behaviour mean set finite sequences states
 a subset    e g   hs    s  i  hs    s    s  i  hs    s    s    s         figure    recall
reward issued end prefix  i  set  behaviours compactly
represented  straightforward represent non markovian reward functions mappings
behaviours real numbers shall defer looking section     
represent behaviours compactly  adopt version future linear temporal logic
 fltl   see emerson         augmented propositional constant    intended
read behaviour want reward happened reward received now 
language  fltl begins set basic propositions p giving rise literals 
l     p   p          
  stand true false  respectively  connectives classical
  temporal modalities  next  u  weak until   giving formulae 
f     l   f f   f f   f   f u f
weak  f  u f  means f  true f  is  ever  unlike
commonly used strong until  imply f  eventually true 
allows us define useful operator  always   f f u  f always true
k f k iterations modality  f
on   adopt notations
wk
true exactly k steps   k f i   f  f true within next k steps  
v
k f ki   f  f true throughout next k steps  
although negation officially occurs literals  i e   formulae negation
normal form  nnf   allow write formulae involving usual way 
  

fithiebaux  gretton  slaney  price   kabanza

provided equivalent nnf  every formula equivalent 
literal   eventualities  f true time 
expressible  restrictions deliberate  use notation
logic theorise allocation rewards  would indeed need means say
rewards received express features liveness  always 
reward eventually   fact using mechanism ensuring
rewards given be  restricted purpose eventualities
negated dollar needed  fact  including would create technical difficulties
relating formulae behaviours represent 
semantics language similar fltl  important difference 
interpretation constant   depends behaviour b want reward
 whatever is   modelling relation    must indexed b  therefore write
   i    b f mean formula f holds i th stage arbitrary sequence  
relative behaviour b  defining   b first step description semantics 
   i    b   iff  i  b
   i    b  
   i     b
   i    b p  p p  iff p
   i    b p  p p  iff p  
   i    b f  f  iff    i    b f     i    b f 
   i    b f  f  iff    i    b f     i    b f 
   i    b

f

iff           b f

   i    b f  u f  iff k  j  j k    j     b f       k    b f 
note except subscript b first rule  standard fltl
semantics  therefore   free formulae keep fltl meaning  fltl 
say   b f iff         b f     b f iff   b f  
modelling relation   b seen specifying formula holds 
reading takes b input  next final step use   b relation define 
formula f   behaviour bf represents  must rather assume
f holds  solve b  instance  let f  p     i e   get rewarded
every time p true  would bf set finite sequences ending
state containing p  arbitrary f   take bf set prefixes
rewarded f hold sequences 

definition   bf  b     b f  
understand definition    recall b contains prefixes end get
reward   evaluates true  since f supposed describe way rewards
received arbitrary sequence  interested behaviours b make  
true way make f hold without imposing constraints evolution
world  however  may many behaviours property  take

  

fidecision theoretic planning non markovian rewards

intersection   ensuring bf reward prefix prefix
every behaviour satisfying f   pathological cases  see section       makes bf
coincide  set inclusion  minimal behaviour b   b f   reason
stingy semantics  making rewards minimal  f actually say rewards
allocated prefixes required truth  instance   p    says
reward given every time p true  even though generous distribution
rewards would consistent it 
    examples
intuitively clear many behaviours specified means  fltl formulae 
simple way general translate past future tense expressions   examples used illustrate pltl section     expressible
naturally  fltl  follows 
classical goal formula g saying goal p rewarded whenever happens
easily expressed   p     already noted  bg set finite sequences states
p holds last state  care p achieved get rewarded
state on  write  p     behaviour formula represents
set finite state sequences least one state p holds  contrast 
formula p u  p    stipulates first occurrence p rewarded  i e 
specifies behaviour figure     reward occurrence p every k
steps  write    k   p k p  k      
response formulae  achievement p triggered command c 
write  c  p     reward every state p true following first
issue command  reward first occurrence p command  write
 c  p u  p      bounded variants reward goal achievement
within k steps trigger command  write example  c k  p     reward
states p holds 
worth noting express simple behaviours involving past tense operators 
stipulate reward p always true  write   u p  say rewarded
p true since q was  write  q    u p   
finally  often find useful reward holding p occurrence q 
neatest expression q u   p q   q     
    reward normality
 fltl therefore quite expressive  unfortunately  rather expressive 
contains formulae describe unnatural allocations rewards  instance 
may make rewards depend future behaviours rather past  may
  
b   b f   case   free f logical theorem 
bf i e  following normal set theoretic conventions  limiting case harm  since
  free formulae describe attribution rewards 
   open question whether set representable behaviours  fltl pltl 
star free regular languages  even behaviours same  little hope
practical translation one exists 

  

fithiebaux  gretton  slaney  price   kabanza

leave open choice several behaviours rewarded   example
dependence future p    stipulates reward p going hold
next  call formula reward unstable  reward stable f amounts
whether particular prefix needs rewarded order make f true depend
future sequence  example open choice behavior reward
 p     p    says either reward achievements goal p
reward achievements p determine which  call formula rewardindeterminate  reward determinate f amounts set behaviours
modelling f   i e   b     b f    unique minimum  not  bf insufficient  too
small  make f true 
investigating  fltl  slaney         examine notions reward stability
reward determinacy depth  motivate claim formulae rewardstable reward determinate call reward normal precisely
capture notion funny business  intuition ask reader
note  needed rest paper  reference then  define 
definition   f reward normal iff every every b     b f iff
every i   i  bf  i  b 
property reward normality decidable  slaney         appendix give
simple syntactic constructions guaranteed result reward normal formulae 
reward abnormal formulae may interesting  present purposes restrict attention
reward normal ones  indeed  stipulate part method reward normal
formulae used represent behaviours  naturally  formulae section    
normal 
     fltl formula progression
defined language represent behaviours rewarded  turn
problem computing  given reward formula  minimum allocation rewards states
actually encountered execution sequence  way satisfy formula 
ultimately wish use anytime solution methods generate state sequences
incrementally via forward search  computation best done fly  sequence
generated  therefore devise incremental algorithm based model checking
technique normally used check whether state sequence model fltl formula
 bacchus   kabanza         technique known formula progression
progresses pushes formula sequence 
progression technique shown algorithm    essence  computes modelling relation   b given section      however unlike definition   b   designed
useful states sequence become available one time  defers
evaluation part formula refers future point next
state becomes available  let state  say   last state sequence prefix  i 
   difficulties inherent use linear time formalisms contexts principle
directionality must enforced  shared instance formalisms developed reasoning
actions event calculus ltl action theories  see e g   calvanese  de giacomo   
vardi        

  

fidecision theoretic planning non markovian rewards

generated far  let b boolean true iff  i  behaviour b
rewarded  let  fltl formula f describe allocation rewards possible
futures  progression f given b  written prog b  s  f    new formula
describe allocation rewards possible futures next state  given
passed s  crucially  function prog markovian  depending
current state single boolean value b  note prog computable
linear time length f     free formulae  collapses fltl formula
progression  bacchus   kabanza         regardless value b  assume prog
incorporates usual simplification sentential constants    f simplifies
  f   simplifies f   etc 
algorithm    fltl progression
prog true  s    
   
prog false  s    
 
prog b  s    
   
prog b  s   
 
prog b  s  p 
    iff p otherwise
prog b  s  p 
    iff p   otherwise
prog b  s  f  f      prog b  s  f    prog b  s  f   
prog b  s  f  f      prog b  s  f    prog b  s  f   
prog b  s  f  
  f
prog b  s  f  u f      prog b  s  f     prog b  s  f    f  u f   
rew s  f  
 prog s  f  

  true iff prog false  s  f    
  prog rew s  f    s  f  

fundamental property prog following  b   i  b  
property      i    b f iff           b prog b    f  
proof 

see appendix b 



  b   function prog seems require b  or least b  input  course
progression applied practice f one new state time  
really want compute appropriate b  namely represented
f   so  similarly section      turn second step  use prog
decide fly whether newly generated sequence prefix  i  bf
allocated reward  purpose functions  prog rew  given
algorithm    given f   function  prog algorithm   defines infinite sequence
formulae hf    f         i obvious way 
f    f
fi      prog i    
decide whether prefix  i  rewarded  rew first tries progressing
formula boolean flag set false  gives consistent result 
need reward prefix continue without rewarding  i   result
  

fithiebaux  gretton  slaney  price   kabanza

know  i  must rewarded order satisfy f   case 
obtain fi   must progress again  time boolean flag set
value true  sum up  behaviour corresponding f   i  rew i      
illustrate behaviour  fltl progression  consider formula f   p u  p   
stating reward received first time p true  let state p
holds  prog false  s  f       p u  p       therefore  since formula
progressed   rew s  f   true reward received   prog s  f     prog true  s  f    
    p u  p        reward formula fades away affect subsequent
progression steps  if  hand  p false s  prog false  s  f       
p u  p    p u  p     therefore  since formula progressed   rew s  f  
false reward received   prog s  f     prog false  s  f     p u  p    reward
formula persists subsequent progression steps 
following theorem states weak assumptions  rewards correctly allocated progression 
theorem   let f reward normal  let hf    f         i result progressing
successive states sequence using function  prog  then  provided
  rew i     iff  i  bf  
proof  see appendix b



premise theorem f never progresses   indeed  
i  means even rewarding  i  suffice make f true  something must
gone wrong  earlier stage  boolean rew made false
made true  usual explanation original f reward normal 
instance p    reward unstable  progresses next state p
true there  regardless     f    p     p    rew     f      false  f    p 
p   f      however   admittedly bizarre  possibilities exist 
example  although p   reward unstable  substitution instance     
progresses steps  logically equivalent   reward normal 
progression method deliver correct minimal behaviour cases
 even reward normal cases  would backtrack choice values
boolean flags  interest efficiency  choose allow backtracking  instead 
algorithm raises exception whenever reward formula progresses   informs
user sequence caused problem  onus thus placed domain
modeller select sensible reward formulae avoid possible progression  
noted worst case  detecting reward normality cannot easier
decision problem  fltl expected simple
syntactic criterion reward normality  practice  however  commonsense precautions
avoiding making rewards depend explicitly future tense expressions suffice
keep things normal routine cases  generous class syntactically recognisable
reward normal formulae  see appendix a 
    reward functions
language defined far  able compactly represent behaviours 
extension non markovian reward function straightforward  represent
  

fidecision theoretic planning non markovian rewards

function set   fltl ir formulae associated real valued rewards 
call reward function specification  formula f associated reward r  
write  f   r    rewards assumed independent additive 
reward function r represented given by 
x
 r    i  bf  
definition   r   i    
 f  r 

e g   p u  p            q            get reward     first time p
holds  reward     first time q holds onwards  reward     
conditions met    otherwise 
again  progress reward function specification compute reward
stages   before  progression defines sequence h             i reward function
specifications  i     rprog i      rprog function applies prog
formulae reward function specification 
rprog s        prog s  f     r     f   r   
then  total reward received stage simply sum real valued rewards
granted progression function behaviours represented formulae  
x
 r   rew i   f   
 f  r i

proceeding way  get expected analog theorem    states progression
correctly computes non markovian reward functions 
theorem   let reward normal  reward function specification  let h           i
result progressing successive states
x sequence using function
rprog  then  provided     r    i 
 r   rew i   f      r   i   
 f  r i

proof 

immediate theorem   



    translation mdp
exploit compact representation non markovian reward function reward
function specification translate nmrdp equivalent mdp amenable statebased anytime solution methods  recall section   e state mdp
labelled state nmrdp history information sufficient determine
immediate reward  case compact representation reward function specification
    additional information summarised progression  
sequence states passed through  e state form hs  i 
   strictly speaking  multiset  convenience represent set  rewards multiple
occurrences formula multiset summed 
   extend definition reward normality reward specification functions obvious way 
requiring reward formulae involved reward normal 

  

fithiebaux  gretton  slaney  price   kabanza

state   fltl ir reward function specification  obtained progression  
two e states hs  ht  equal   t  immediate rewards same 
results progressing semantically equivalent  
definition   let   hs  s    a  pr  ri nmrdp    reward function specification representing r  i e   r    r  see definition     translate mdp
d    hs     s     a    pr    r  defined follows 
       fltl ir
   s     hs     
   a   hs  i    a s 
pr s  a  s        rprog s   
 
otherwise
 
 
   hs  i   pr  hs  i  a    undefined
x
   r   hs  i   
 r   rew s  f   
   a   hs  i   pr   hs  i  a  hs      i   



 f  r 

   s      s  reachable a  s    
item   says e states labelled state reward function specification  item
  says initial e state labelled initial state original reward
function specification  item   says action applicable e state applicable
state labelling it  item   explains successor e states probabilities
computed  given action applicable e state hs  i  successor e state
labelled successor state s  via nmrdp progression
s  probability e state pr s  a  s    nmrdp  note
cost computing pr  linear computing pr sum lengths
formulae   item   motivated  see section       finally  since items   
leave open choice many mdps differing unreachable states contain 
item   excludes irrelevant extensions  easy show translation leads
equivalent mdp  defined definition    obviously  function required
definition  given  hs  i    s  proof matter checking conditions 
practical implementation  labelling one step ahead definition 
label initial e state rprog s        compute current reward current reward specification label progression predecessor reward specifications
current state rather predecessor states  apparent below 
potential reduce number states generated mdp 
figure   shows equivalent mdp produced  fltl version nmrdp
example figure    recall example  pltl reward formula q p 
 fltl  allocation rewards described   p q      figure
   care needed notion semantic equivalence  rewards additive  determining
equivalence may involve arithmetic well theorem proving  example  reward function specification   p          q         equivalent    p q            p q            p q         
although one one correspondence formulae two sets 

  

fidecision theoretic planning non markovian rewards

start state
f 
reward  
a      
p
f  f 
reward  
a   
p
f  f  f 
reward  

a       b     

a      

a      

q
f 
reward  

b   

a    b   

b     

a      b   

a     

following formulae label e states 
f      p q    
f    q  
f    q  

p  q
f  f 
reward  
a    b   
p  q
f  f  f 
reward  
a    b   
p  q
f  f  f 
reward  

a    b   

figure    equivalent mdp produced fltl
shows relevant formulae labelling e states  obtained progression reward
formula  note without progressing one step ahead  would   e states state
 p  left hand side  labelled  f      f    f      f    f    f     respectively 
    blind minimality
size mdp obtained  i e  number e states contains key issue
us  amenable state based solution methods  ideally  would
mdp minimal size  however  know method building minimal
equivalent mdp incrementally  adding parts required solution method  since
worst case even minimal equivalent mdp larger nmrdp
factor exponential length reward formulae  bacchus et al          constructing
entirely would nullify interest anytime solution methods 
however  explain  definition   leads equivalent mdp exhibiting relaxed
notion minimality  amenable incremental construction  inspection 
may observe wherever e state hs  successor hs      via action a 
means order succeed rewarding behaviours described means
execution sequences start going s  via a  necessary future
starting s  succeeds rewarding behaviours described     hs 
minimal equivalent mdp  really execution sequences succeeding
rewarding behaviours described   hs      must minimal mdp 
is  construction progression introduce e states priori needed 
note e state priori needed may really needed  may fact
execution sequence using available actions exhibits given behaviour 

  

fithiebaux  gretton  slaney  price   kabanza

instance  consider response formula  p   k q k      i e   every time trigger p
true  rewarded k steps later provided q true then  obviously  whether p
true stage affects way future states rewarded  however 
transition relation happens property k steps state satisfying p 
state satisfying q reached  posteriori p irrelevant  need
label e states differently according whether p true observe occurrence
example figure    leads fltl produce extra state
bottom left figure  detect cases  would look perhaps quite deep
feasible futures  cannot constructing e states fly  hence
relaxed notion call blind minimality always coincide absolute
minimality 
formalise difference true blind minimality  purpose 
convenient define functions mapping e states e functions
ir intuitively assigning rewards sequences nmrdp starting  e   recall
definition   maps e state mdp underlying nmrdp state 
definition   let nmrdp  let   set e states equivalent mdp d 
f   s   
d  let e reachable e state     let    i  sequence e states
 
e     obtained
   i    e  let  i  corresponding sequence d s
sense that  j i   j      j      define

 e     



 e     

r  i          
 
otherwise

e i 
r  i       d 
 
otherwise

unreachable e state e  define  e     e       
note carefully difference   former describes rewards assigned
continuations given state sequence  latter confines rewards feasible
continuations  note well defined despite indeterminacy
choice    i   since clause   definition    choices lead values
r 
theorem   let   set e states equivalent mdp d    hs  s    a  pr  ri 
d  minimal iff every e state   reachable   contains two distinct e states s  
s    s        s      s        s     
proof 

see appendix b 



blind minimality similar  except that  since looking ahead  distinction
drawn feasible trajectories others future s 
definition   let   set e states equivalent mdp d    hs  s    a  pr  ri 
d  blind minimal iff every e state   reachable   contains two distinct estates s   s    s        s      s        s     
  

fidecision theoretic planning non markovian rewards

theorem   let d  translation definition    d  blind minimal
equivalent mdp d 
proof 

see appendix b 



size difference blind minimal minimal mdps depend
precise interaction rewards dynamics problem hand  making theoretical analyses difficult experimental results rather anecdotal  however  experiments
section     show computation time point view  often preferable work blind minimal mdp invest overhead computing
truly minimal one 
finally  recall syntactically different semantically equivalent reward function
specifications define e state  therefore  neither minimality blind minimality
achieved general without equivalence check least complex theorem
proving ltl  pratical implementations  avoid theorem proving favour embedding  fast  formula simplification progression regression algorithms 
means principle approximate minimality blind minimality 
appears enough practical purposes 
    embedded solution construction
blind minimality essentially best achievable anytime state based solution methods typically extend envelope one step forward without looking deeper
future  translation blind minimal mdp trivially embedded
solution methods  results on line construction mdp  method entirely
drives construction parts mdp feels need explore 
leave others implicit  time short  suboptimal even incomplete policy may
returned  fraction state expanded state spaces might constructed 
note solution method raise exception soon one reward formulae progresses   i e   soon expanded state hs  built     r   
since acts detector unsuitable reward function specifications 
extent enabled blind minimality  approach allows dynamic analysis
reward formulae  much pltlstr  bacchus et al          indeed  execution
sequences feasible particular policy actually explored solution method contribute analysis rewards policy  specifically  reward formulae generated
progression given policy determined prefixes execution sequences
feasible policy  dynamic analysis particularly useful  since relevance
reward formulae particular policies  e g  optimal policy  cannot detected priori 
forward chaining planner tlplan  bacchus   kabanza        introduced idea
using fltl specify domain specific search control knowledge formula progression
prune unpromising sequential plans  plans violating knowledge  deterministic
search spaces  shown provide enormous time gains  leading tlplan
win      planning competition hand tailored track  approach based
progression  provides elegant way exploit search control knowledge  yet
context decision theoretic planning  results dramatic reduction

  

fithiebaux  gretton  slaney  price   kabanza

fraction mdp constructed explored  therefore substantially better
policies deadline 
achieve follows  specify  via   free formula c    properties know
must verified paths feasible promising policies  simply progress c 
alongside reward function specification  making e states triples hs    ci c   free
formula obtained progression  prevent solution method applying action
leads control knowledge violated  action applicability condition  item
  definition    becomes  a   hs    ci  iff a s  c     the changes
straightforward   instance  effect control knowledge formula  p q 
remove consideration feasible path p followed q  detected
soon violation occurs  formula progresses   although paper focuses
non markovian rewards rather dynamics  noted   free formulae
used express non markovian constraints systems dynamics 
incorporated approach exactly control knowledge 
     discussion
existing approaches  bacchus et al               advocate use pltl finite
past specify non markovian rewards  pltl style specification  describe
past conditions get rewarded now   fltl describe
conditions present future future states rewarded 
behaviours rewards may scheme  naturalness thinking one
style depends case  letting kids strawberry dessert
good day fits naturally past oriented account rewards  whereas
promising may watch movie tidy room  indeed  making sense
whole notion promising  goes naturally  fltl  one advantage pltl
formulation trivially enforces principle present rewards depend
future states   fltl  responsibility placed domain modeller  best
offer exception mechanism recognise mistakes effects appear 
syntactic restrictions  hand  greater expressive power  fltl opens
possibility considering richer class decision processes  e g  uncertainty
rewards received  the dessert movie   some time next week 
rains  
rate  believe  fltl better suited pltl solving nmrdps
using anytime state based solution methods  pltlsim translation could easily embedded solution method  loses structure original formulae
considering subformulae individually  consequently  expanded state space easily
becomes exponentially bigger blind minimal one  problematic
solution methods consider  size severely affects performance solution
quality  pre processing phase pltlmin uses pltl formula regression find sets
subformulae potential labels possible predecessor states  subsequent
generation phase builds mdp representing histories make difference way actually feasible execution sequences rewarded 
recover structure original formula  best case  mdp produced
exponentially smaller blind minimal one  however  prohibitive cost

  

fidecision theoretic planning non markovian rewards

pre processing phase makes unsuitable anytime solution methods  consider method based pltl regression achieve meaningful relaxed
notion minimality without costly pre processing phase  fltl approach based
 fltl progression precisely that  letting solution method resolve
tradeoff quality cost principled way intermediate two extreme
suggestions above 
structured representation solution methods targeted bacchus et al        
differ anytime state based solution methods fltl primarily aims at  particular
require explicit state enumeration all  here  non minimality
problematic state based approaches  virtue size mdp produced 
pltlstr translation is  pltlsim  clearly unsuitable anytime state based methods  
another sense  too  fltl represents middle way  combining advantages conferred
state based structured approaches  e g  pltlmin one side  pltlstr
other  former fltl inherits meaningful notion minimality  latter 
approximate solution methods used perform restricted dynamic analysis
reward formulae  particular  formula progression enables even state based methods
exploit structure  fltl space  however  gap blind
true minimality indicates progression alone insufficient always fully exploit
structure  hope pltlstr able take advantage full structure
reward function  possibility fail exploit even much structure
fltl  efficiently  empirical comparison three approaches needed answer
question identify domain features favoring one other 

   nmrdpp
first step towards decent comparison different approaches framework
includes all  non markovian reward decision process planner  nmrdpp 
platform development experimentation approaches nmrdps 
provides implementation approaches described common framework 
within single system  common input language  nmrdpp available on line 
see http   rsise anu edu au  charlesg nmrdpp  worth noting bacchus et al 
             report implementation approaches 
    input language
input language enables specification actions  initial states  rewards  search
control knowledge  format action specification essentially
spudd system  hoey et al          reward specification one formulae 
associated name real number  formulae either pltl  fltl 
control knowledge given language chosen reward  control
knowledge formulae verified sequence states feasible
generated policies  initial states simply specified part control knowledge
explicit assignments propositions 
   would interesting  hand  use pltlstr conjunction symbolic versions
methods  e g  symbolic lao   feng   hansen        symbolic rtdp  feng  hansen    zilberstein 
      

  

fithiebaux  gretton  slaney  price   kabanza

action flip
heads      
endaction
action tilt
heads  heads             
endaction
heads  
 first        heads  prv  pdi heads 
 seq         prv   heads   prv heads   heads
figure    input coin example  prv  previously  stands
  
pdi  past diamond  stands
instance  consider simple example consisting coin showing either heads
tails  heads   two actions performed  flip action changes
coin show heads tails     probability  tilt action changes    
probability  otherwise leaving is  initial state tails  get reward    
  heads pltl  reward    
first head  this written heads
time achieve sequence heads  heads  tails    heads heads heads pltl  
input language  nmrdp described shown figure   
    common framework
common framework underlying nmrdpp takes advantage fact nmrdp
solution methods can  general  divided distinct phases preprocessing 
expansion  solving  first two optional 
pltlsim  preprocessing simply computes set sub f   subformulae reward
formulae  pltlmin  includes computing labels l s  state s 
pltlstr  preprocessing involves computing set temporal variables well
adds dynamics rewards  fltl require preprocessing 
expansion optional generation entire equivalent mdp prior solving 
whether off line expansion sensible depends mdp solution method used 
state based value policy iteration used  mdp needs expanded anyway 
if  hand  anytime search algorithm structured method used 
definitely bad idea  experiments  often used expansion solely purpose
measuring size generated mdp 
solving mdp done using number methods  currently  nmrdpp provides
implementations classical dynamic programming methods  namely state based value
policy iteration  howard         heuristic search methods  state based lao   hansen  
zilberstein        using either value policy iteration subroutine  one structured
method  namely spudd  hoey et al          prime candidates future developments
 l rtdp  bonet   geffner         symbolic lao   feng   hansen         symbolic
rtdp  feng et al         
  

fidecision theoretic planning non markovian rewards

load coin nmrdp
pltlstr preprocessing

  loadworld coin 
  preprocess spltl 
  startcputimer
  spudd              
  stopcputimer
  readcputimer
       
  iterationcount
    
  displaydot valuetodot 
expected value

     

     

report number iterations
display add value function

 prv heads 

 prv  prv pdi heads  

     

report solving time

heads

 prv heads 

 prv  prv pdi heads  

solve mdp spudd    

     

 prv   heads 

 prv pdi heads 

     

     

 prv pdi heads 

     

     

display policy

  displaydot policytodot 
optimal policy

heads

 prv heads 

flip

 
 
 
 
 

tilt

pltlmin preprocessing
completely expand mdp
report mdp size

preprocess mpltl 
expand
domainstatesize
printdomain        show domain rb
reward  
flip     

flip     

display postcript rendering mdp

tilt     

tilt     

heads
reward  
flip     
heads
reward  
tilt     

flip     

tilt     

tilt     

flip     

tilt     

flip     

reward  
tilt     

flip     

tilt     

flip     

tilt     

tilt     

flip     
reward  
flip     

flip     

flip     

tilt     

tilt     

heads
reward  

solve mdp vi    
report number iterations

  valit              
  iterationcount
    
  getpolicy
   

output policy  textual 

figure    sample session
  

fithiebaux  gretton  slaney  price   kabanza

    approaches covered
altogether  various types preprocessing  choice whether expand 
mdp solution methods  give rise quite number nmrdp approaches  including 
limited previously mentioned  see e g  pltlstr a  below   combinations possible  e g   state based processing variants incompatible structured
solution methods  the converse possible principle  however   also  present
structured form preprocessing  fltl formulae 
pltlstr a  example interesting variant pltlstr  obtain
considering additional preprocessing  whereby state space explored  without explicitly
enumerating it  produce bdd representation e states reachable start
state  done starting bdd representing start e state  repeatedly
applying action  non zero probabilities converted ones result or ed
last result  action adds reachable e states bdd 
sure represents reachable e state space  used additional control
knowledge restrict search  noted without phase pltlstr makes
assumptions start state  thus left possible disadvantage  similar
structured reachability analysis techniques used symbolic implementation
lao   feng   hansen         however  important aspect
temporal variables included bdd 
    nmrdpp system
nmrdpp controlled command language  read either file interactively  command language provides commands different phases  preprocessing 
expansion  solution  methods  commands inspect resulting policy value
functions  e g  rendering via dot  at t labs research         well supporting
commands timing memory usage  sample session  coin nmrdp
successively solved pltlstr pltlmin shown figure   
nmrdpp implemented c    makes use number supporting libraries 
particular  relies heavily cudd package manipulating adds  somenzi 
       action specification trees converted stored adds system 
moreover structured algorithms rely heavily cudd add computations 
state based algorithms make use mtl matrix template library matrix
operations  mtl takes advantage modern processor features mmx sse
provides efficient sparse matrix operations  believe implementations
mdp solution methods comparable state art  instance  found
implementation spudd comparable performance  within factor   
reference implementation  hoey et al          hand  believe data
structures used regression progression temporal formulae could optimised 

   experimental analysis
faced three substantially different approaches easy compare 
performance depend domain features varied structure
transition model  type  syntax  length temporal reward formula  presence

  

fidecision theoretic planning non markovian rewards

rewards unreachable irrelevant optimal policy  availability good heuristics
control knowledge  etc  interactions factors  section 
report experimental investigation influence factors try
answer questions raised previously   
   dynamics domain predominant factor affecting performance 
   type reward major factor 
   syntax used describe rewards major factor 
   overall best method 
   overall worst method 
   preprocessing phase pltlmin pay  compared pltlsim 
   simplicity fltl translation compensate blind minimality 
benefit true minimality outweigh cost pltlmin preprocessing 
   dynamic analyses rewards pltlstr fltl effective 
   one analyses powerful  rather complementary 
cases all  able identify systematic patterns  results
section obtained using pentium     ghz gnu linux        machine    mb
ram 
    preliminary remarks
clearly  fltl pltlstr a  great potential exploiting domain specific heuristics control knowledge  pltlmin less so  avoid obscuring results  therefore
refrained incorporating features experiments  running lao  
heuristic value state crudest possible  the sum reward values
problem   performance results interpreted light necessarily
reflect practical abilities methods able exploit features 
begin general observations  one question raised whether
gain pltl expansion phase worth expensive preprocessing performed
pltlmin  i e  whether pltlmin typically outperforms pltlsim  definitively answer
question  pathological exceptions  preprocessing pays  found expansion
bottleneck  post hoc minimisation mdp produced pltlsim
help much  pltlsim therefore little practical interest  decided
report results performance  often order magnitude worse
pltlmin  unsurprisingly  found pltlstr would typically scale larger state
spaces  inevitably leading outperform state based methods  however  effect
uniform  structured solution methods sometimes impose excessive memory requirements
makes uncompetitive certain cases  example n f   large n 
features reward formula 
    executive summary answers executive reader     no     yes     yes     pltlstr
fltl     pltlsim     yes     yes no  respectively     yes     yes  respectively 

  

fithiebaux  gretton  slaney  price   kabanza

    domains
experiments performed four hand coded domains  propositions   dynamics 
random domains  hand coded domain n propositions pi   dynamics
makes every state possible eventually reachable initial state
propositions false  first two domains  spudd linear spudd expon
discussed hoey et al          two others own 
intention spudd linear take advantage best case behaviour
spudd  proposition pi   action ai sets pi true propositions
pj     j   false  spudd expon  used hoey et al         demonstrate
worst case behaviour spudd  proposition pi   action ai sets pi
true propositions pj     j   true  and sets pi false otherwise  
sets latter propositions false  third domain  called on off  one turn on
one turn off action per proposition  turn on pi action probabilistically
succeeds setting pi true pi false  turn off action similar  fourth
domain  called complete  fully connected reflexive domain  proposition pi
action ai sets pi true probability i  n       and false otherwise 
pj   j    true false probability      note ai cause transition
 n states 
random domains size n involve n propositions  method generating
dynamics detailed appendix c  let us summarise saying able
generate random dynamics exhibiting given degree structure given degree
uncertainty  lack structure essentially measures bushiness internal part
adds representing actions  uncertainty measures bushiness leaves 
    influence dynamics
interaction dynamics reward certainly affects performance
different approaches  though strikingly factors reward type  see
below   found reward scheme  varying degree structure
uncertainty generally change relative success different approaches 
instance  figures       show average run time methods function
degree structure  resp  degree uncertainty  random problems size n    
reward n    the state encountered stage n rewarded  regardless properties     
run time increases slightly degrees  significant change relative
performance  typical graphs obtain rewards 
clearly  counterexamples observation exist  notable cases
extreme dynamics  instance spudd expon domain  although small values
n  n      pltlstr approaches faster others handling reward
n   virtually type dynamics encountered  perform poorly
reward spudd expon  explained fact small fraction
spudd expon states reachable first n steps  n steps  fltl immediately
recognises reward consequence  formula progressed   
pltlmin discovers fact expensive preprocessing  pltlstr 
hand  remains concerned prospect reward  pltlsim would 
   

n  

 fltl

  

fiaverage cpu time  sec 

decision theoretic planning non markovian rewards

  
  
  
  
  
 

   

   

   

   

   

   

structure    structured        unstructured 
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     changing degree structure

average cpu time  sec 

  
  
  
  
  
 

 

   

   

   

   

 

   

uncertainty    certain        uncertain 
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     changing degree uncertainty
    influence reward types
type reward appears stronger influence performance dynamics 
unsurprising  reward type significantly affects size generated mdp 
certain rewards make size minimal equivalent mdp increase constant
number states constant factor  others make increase factor exponential
length formula  table   illustrates this  third column reports size
minimal equivalent mdp induced formulae left hand side   
legitimate question whether direct correlation size increase
 in appropriateness different methods  instance  might expect state based
methods particularly well conjunction reward types inducing small mdp
    figures necessarily valid non completely connected nmrdps  unfortunately  even
completely connected domains  appear much cheaper way determine mdp
size generate count states 

  

fithiebaux  gretton  slaney  price   kabanza

type
first time pi
pi sequence start state
two consecutive pi
pi n times ago

formula
  ni   pi  
 ni   pi    
 ni   pi   n  
n 
i  
 pi pi    
n ni   pi

size
o     s  
o n   s  
o nk    s  
o  n    s  

fastest
pltlstr a 
fltl
pltlstr
pltlstr

slowest
pltlmin
pltlstr
fltl
pltlmin

table    influence reward type mdp size method performance

average cpu time  sec 

    
   
   
   

 

   

 

   

 

   

 

   

n
approaches prvin
approaches prvout

figure     changing syntax
otherwise badly comparison structured methods  interestingly  always
case  instance  table   whose last two columns report fastest slowest
methods range hand coded domains   n     first row contradicts
expectation  moreover  although pltlstr fastest last row  larger values
n  not represented table   aborts lack memory  unlike
methods 
obvious observations arising experiments pltlstr nearly
always fastest runs memory  perhaps interesting results
second row  expose inability methods based pltl deal
rewards specified long sequences events  converting reward formula
set subformulae  lose information order events 
recovered laboriously reasoning   fltl progression contrast takes events one
time  preserving relevant structure step  experimentation led us
observe pltl based algorithms perform poorly reward specified using
  k f   fik f  f true k steps ago  within last k
formulae form k f  
steps  last k steps  
    influence syntax
unsurprisingly  find syntax used express rewards  affects length
formula  major influence run time  typical example effect
captured figure     graph demonstrates re expressing prvout n  ni   pi  
  

fidecision theoretic planning non markovian rewards

state count    n 

  
 
 
 
 
 
 

 

 

 

 

  

  

  

n
pltlmin
fltl

figure     effect multiple rewards mdp size

total cpu time  sec 

    
    
   

 

 

 

 

 

  

  

  

n
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     effect multiple rewards run time
prvin ni   n pi   thereby creating n times temporal subformulae  alters
running time pltl methods  fltl affected  fltl progression requires two
iterations reward formula  graph represents averages running
times methods  complete domain 
serious concern relation pltl approaches handling reward
specifications containing multiple reward elements  notably found pltlmin
necessarily produce minimal equivalent mdp situation  demonstrate  consider set reward formulae  f    f            fn    associated
real value r  given this  pltl approaches distinguish unnecessarily past
behaviours lead identical future rewards  may occur reward
e state determined truth value f  f    formula necessarily require
e states distinguish cases  f     f     f    f    
hold  however  given specification  pltlmin makes distinction  example 

  

fithiebaux  gretton  slaney  price   kabanza

taking   pi   figure    shows fltl leads mdp whose size   times
nmrdp  contrast  relative size mdp produced pltlmin
linear n  number rewards propositions  results obtained
hand coded domains except spudd expon  figure    shows run times function
n complete  fltl dominates overtaken pltlstr a  large values
n  mdp becomes large explicit exploration practical  obtain
minimal equivalent mdp using pltlmin  bloated reward specification form
   ni    pi nj   j  i pj     r             ni   pi   n r   necessary  which  virtue
exponential length  adequate solution 
    influence reachability
approaches claim ability ignore variables irrelevant
condition track unreachable    pltlmin detects preprocessing 
pltlstr exploits ability structured solution methods ignore them  fltl ignores progression never exposes them  however  given mechanisms
avoiding irrelevance different  expect corresponding differences effects 
experimental investigation  found differences performance best illustrated looking response formulae  assert trigger condition c reached
reward received upon achievement goal g in  resp  within  k steps 
  k c   fltl   c k  g      resp 
pltl  written g k c  resp  g
 c k  g    
goal unreachable  pltl approaches perform well  always false 
goal g lead behavioural distinctions  hand  constructing
mdp  fltl considers successive progressions k g without able detect
unreachable actually fails happen  exactly blindness blind
minimality amounts to  figure    illustrates difference performance function
number n propositions involved spudd linear domain  reward
form g n c  g unreachable 
fltl shines trigger unreachable  since c never happens  formula
always progress itself  goal  however complicated  never tracked generated mdp  situation pltl approaches still consider k c subformulae 
discover  expensive preprocessing pltlmin  reachability analysis pltlstr a   never pltlstr  irrelevant  illustrated figure    
spudd linear reward form g n c  c unreachable 
    dynamic irrelevance
earlier claimed one advantage pltlstr fltl pltlmin pltlsim
former perform dynamic analysis rewards capable detecting irrelevance
variables particular policies  e g  optimal policy  experiments confirm
claim  however  reachability  whether goal triggering condition
response formula becomes irrelevant plays important role determining whether
    sometimes speak conditions goals reachable achievable rather feasible 
although may temporally extended  keep line conventional vocabulary
phrase reachability analysis 

  

fidecision theoretic planning non markovian rewards

total cpu time  sec 

   
   
   
   
  

 

 

 

 

  

  

  

n
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     response formula unachievable goal

total cpu time  sec 

   
   
   
   
  

 

 

 

 

 

  

n
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     response formula unachievable trigger
pltlstr fltl approach taken  pltlstr able dynamically ignore goal 
fltl able dynamically ignore trigger 
illustrated figures        figures  domain considered
on off n     propositions  response formula g n c before 
g c achievable  response formula assigned fixed reward  study effect
dynamic irrelevance goal  figure     achievement g rewarded value
r  i e   g   r  pltl   figure     hand  study effect
dynamic irrelevance trigger achievement c rewarded value r 
figures show runtime methods r increases 
achieving goal  resp  trigger  made less attractive r increases
point response formula becomes irrelevant optimal policy 
happens  run time pltlstr resp  fltl  exhibits abrupt durable improvement 
figures show fltl able pick irrelevance trigger  pltlstr able
exploit irrelevance goal  expected  pltlmin whose analysis static pick
  

fithiebaux  gretton  slaney  price   kabanza

total cpu time  sec 

   
   
   
  

 

  

   

   

   

   

   

   

r
pltlmin

pltlstruct

fltl

pltlstruct  a 

average cpu time  sec 

figure     response formula unrewarding goal
   
   
   
  

 

  

   

   

   

   

   

   

r
pltlmin

fltl

pltlstruct

pltlstruct a 

figure     response formula unrewarding trigger
either performs consistently badly  note figures  pltlstr progressively
takes longer compute r increases value iteration requires additional iterations
converge 
    summary
experiments artificial domains  found pltlstr fltl preferable statebased pltl approaches cases  one insists using latter  strongly
recommend preprocessing  fltl technique choice reward requires tracking
long sequence events desired behaviour composed many elements
identical rewards  response formulae  advise use pltlstr probability
reaching goal low achieving goal costly  conversely  advise
use fltl probability reaching triggering condition low reaching
costly  cases  attention paid syntax reward formulae
  

fidecision theoretic planning non markovian rewards

particular minimising length  indeed  could expected  found syntax
formulae type non markovian reward encode predominant
factor determining difficulty problem  much features
markovian dynamics domain 

   concrete example
experiments far focused artificial problems aimed characterising
strengths weaknesses various approaches  look concrete example
order give sense size interesting problems techniques
solve  example derived miconic elevator classical planning benchmark
 koehler   schuster         elevator must get number passengers origin
floor destination  initially  elevator arbitrary floor passenger
served boarded elevator  version problem  one single
action causes elevator service given floor  effect unserved
passengers whose origin serviced floor board elevator  boarded passengers
whose destination serviced floor unboard become served  task plan
elevator movement passengers eventually served   
two variants miconic  simple variant  reward received
time passenger becomes served  hard variant  elevator attempts
provide range priority services passengers special requirements  many passengers
prefer travelling single direction  either down  destination  certain
passengers might offered non stop travel destination  finally  passengers
disabilities young children supervised inside elevator
passenger  the supervisor  assigned them  omit vip conflicting group
services present original hard miconic problem  reward formulae
create additional difficulties 
formulation problem makes use propositions pddl description miconic used      international planning competition  dynamic propositions
record floor elevator currently whether passengers served boarded 
static propositions record origin destination floors passengers  well
categories  non stop  direct travel  supervisor  supervised  passengers fall in  however 
formulation differs pddl description two interesting ways  firstly  since
use rewards instead goals  able find preferred solution even
goals cannot simultaneously satisfied  secondly  priority services naturally
described terms non markovian rewards  able use action description simple hard versions  whereas pddl description hard miconic
requires additional actions  up  down  complex preconditions monitor satisfaction priority service constraints  reward schemes miconic encapsulated
four different types reward formula 
   simple variant  reward received first time passenger pi served 
    experimented stochastic variants miconic passengers small probability
desembarking wrong floor  however  find useful present results deterministic
version since closer miconic deterministic planning benchmark since  shown
before  rewards far crucial impact dynamics relative performance methods 

  

fithiebaux  gretton  slaney  price   kabanza

pltl 

servedpi servedpi

 fltl 

servedpi u  servedpi   

   next  reward received time non stop passenger pi served one step
boarding elevator 
pltl 

n onstoppi boardedpi servedpi servedpi

 fltl 

  n onstoppi boardedpi servedpi servedpi     

   then  reward received time supervised passenger pi served
accompanied times inside elevator supervisor   pj  
pltl 
 fltl 

supervisedpi supervisorpj pi servedpi
servedpi fi boardedpi boardedpj  
servedpi u   boardedpi supervisedpi  boardedpj supervisorpj pi  
servedpi    servededpi    

   finally  reward received time direct travel passenger pi served
travelled one direction since boarding  e g   case going up 
directp
w w servedpi servedpi
   j k j  atf loork atf loorj     boardedpi boardedpi   
w w
 fltl    directpi boardedpi    servedpi u     j k i atf loorj atf loork  
servedpi    servedpi      

pltl 

similarly case going down 
experiments section run dual pentium     ghz gnu linux       
machine  gb ram  first experimented simple variant  giving reward
   time passenger first served  figure    shows cpu time taken
various approaches solve random problems increasing number n floors
passengers  figure    shows number states expanded so  data
point corresponds one random problem  fair structured approach 
ran pltlstr a  able exploit reachability start state  first observation
although pltlstr a  best small values n  quickly runs memory 
pltlstr a  pltlsim need track formulae form servedpi
pltlsim not  conjecture run memory earlier 
second observation attempts pltl minimisation pay much here 
pltlmin reduced memory tracks fewer subformulae  size
mdp produces identical size pltlsim mdp larger
fltl mdp  size increase due fact pltl approaches label differently
e states passengers served  depending become served
 for passengers  reward formula true e state   contrast  fltl
implementation progression one step ahead labels e states reward
    understand  fltl formula  observe get reward iff  boardedpi supervisedpi  
 boardedpj supervisorpj pi   holds servedpi becomes true  recall formula q u   p
q   q     rewards holding p occurrence q 

  

fidecision theoretic planning non markovian rewards

total cpu time  sec 

    
    
    
    

 

 

 

 

  

  

  

n
fltl
pltlsim
pltlmin
pltlstr a 

figure     simple miconic   run time

  

state count    n 

  
  
  
  
  
  
  
 
 
 

 

 

 

  

  

  

n
fltl
pltlsim  pltlmin

figure     simple miconic   number expanded states
formulae relevant passengers still need served  formulae
progressed    gain number expanded states materialises run time gains 
resulting fltl eventually taking lead 
second experiment illustrates benefits using even extremely simple admissible heuristic conjunction fltl  heuristic applicable discounted stochastic
shortest path problems  discounts rewards shortest time future
possible  simply amounts assigning fringe state value    times
number still unserved passengers  discounted once   results avoiding floors
passenger waiting destination boarded passenger 
figures       compare run time number states expanded fltl used
conjunction value iteration  valit  used conjunction lao 
  

fithiebaux  gretton  slaney  price   kabanza

total cpu time  sec 

     
     
     
    

 

 

 

 

  

  

  

n
fltllao h 
fltllao u 
fltlvalit

figure     effect simple heuristic run time

state count    n 

  
  
  
  
  
 
 

 

 

 

  

  

  

n
fltllao h 
fltlvalit fltllao u 

figure     effect simple heuristic number expanded states
search informed heuristic  lao h    uninformed lao   lao  u   i e  lao 
heuristic    n node  included reference point show
overhead induced heuristic search  seen graphs  heuristic search
generates significantly fewer states eventually pays terms run time 
final experiment  considered hard variant  giving reward   
service      reward   non stop travel      reward   appropriate supervision
     reward    direct travel      regardless number n floors
passengers  problems feature single non stop traveller  third passengers require
supervision  half passengers care traveling direct  cpu time number
states expanded shown figures        respectively  simple case 
pltlsim pltlstr quickly run memory  formulae type         create
many additional variables track approaches  problem seem
  

fidecision theoretic planning non markovian rewards

total cpu time  sec 

     
    
    
    

 

 

 

 

 

 

n
fltl
pltlsim
pltlmin
pltlstruct a 

figure     hard miconic   run time

state count    n 

   
  
  
  
  
 
 

 

 

 

 

 

n
fltl
pltlsim
pltlmin

figure     hard miconic   number expanded states
exhibit enough structure help pltlstr  fltl remains fastest  here 
seem much due size generated mdp slightly
pltlmin mdp  rather overhead incurred minimisation  another
observation arising experiment small instances handled
comparison classical planning version problem solved state art
optimal classical planners  example       international planning competition 
propplan planner  fourman        optimally solved instances hard miconic
   passengers    floors      seconds much less powerful machine 

  

fithiebaux  gretton  slaney  price   kabanza

   nmrdpp probabilistic planning competition
report behaviour nmrdpp probabilistic track  th international planning competition  ipc     since competition feature non markovian
rewards  original motivation taking part compare solution methods
implemented nmrdpp markovian setting  objective largely underestimated
challenges raised merely getting planner ready competition  especially
competition first kind  end  decided successfully preparing nmrdpp attempt problems competition using one solution method  and possibly
search control knowledge   would honorable result 
crucial problem encountered translation ppddl  younes  
littman         probabilistic variant pddl used input language competition  nmrdpps add based input language  translating ppddl adds
possible theory  devising translation practical enough need
competition  small number variables  small  quickly generated  easily manipulable
adds  another matter  mtbdd  translator kindly made available participants
competition organisers  always able achieve required efficiency 
times  translation quick nmrdpp unable use generated adds efficiently  consequently  implemented state based translator top pddl parser
backup  opted state based solution method since rely adds
could operate translators 
version nmrdpp entered competition following 
   attempt get translation adds using mtbdd  proves infeasible 
abort rely state based translator instead 
   run fltl expansion state space  taking search control knowledge account
available  break   mn complete 
   run value iteration convergence  failing achieve useful result  e g 
expansion complete enough even reach goal state   go back step   
   run many    trials possible remaining time    following generated policy defined  falling back non deterministic search control
policy available 
step   trying maximise instances original add based
nmrdpp version could run intact  step    decided use lao 
run good heuristic  often incurs significant overhead compared value
iteration 
problems featured competition classified goal based rewardbased problems  goal based problems   positive  reward received goal
state reached  reward based problems  action performance may incur  usually
negative  reward  another orthogonal distinction made problems
    given problem  planners   mn run whatever computation saw appropriate  including parsing  pre processing  policy generation any   execute    trial runs generated
policy initial state goal state 

  

fidecision theoretic planning non markovian rewards

domains communicated advance participants domains
were  latter consisted variants blocks world logistics  or box world 
problems  gave participating planners opportunity exploit knowledge
domain  much hand coded deterministic planning track 
decided enroll nmrdpp control knowledge mode domain independent
mode  difference two modes first uses fltl search
control knowledge written known domains additional input  main concern
writing control knowledge achieve reasonable compromise size
effectiveness formulae  blocks world domain  two actions
pickup from putdown to     chance dropping block onto table 
control knowledge used encoded variant well known gn  near optimal strategy
deterministic blocks world planning  slaney   thiebaux         whenever possible 
try putting clear block goal position  otherwise put arbitrary clear block
table  blocks get dropped table whenever action fails 
success probabilities rewards identical across actions  optimal policies
problem essentially made optimal sequences actions deterministic blocks
world little need sophisticated strategy    colored blocks
world domain  several blocks share color goal refers
color blocks  control knowledge selected arbitrary goal state non colored
blocks world consistent colored goal specification  used strategy
non colored blocks world  performance strategy depends entirely
goal state selected therefore arbitrarily bad 
logistics problems ipc   distinguish airports locations within
city  trucks drive two locations city planes fly
two airports  contrast  box world features cities 
airport  accessible truck  priori  map truck
plane connections arbitrary  goal get packages city origin
city destination  moving truck     chance resulting reaching one
three cities closest departure city rather intended one  size box
world search space turned quite challenging nmrdpp  therefore  writing
search control knowledge  gave optimality consideration favored maximal
pruning  helped fact box world generator produces problems
following structure  cities divided clusters  composed
least one airport city  furthermore cluster least one hamiltonian circuit
trucks follow  control knowledge used forced planes one  trucks
one cluster idle  cluster  truck allowed move could
attempt driving along chosen hamiltonian circuit  picking dropping parcels
went 
planners participating competition shown table    planners e  g  
j   j  domain specific  either tuned blocks box worlds  use
domain specific search control knowledge  learn examples  participating
planners domain independent 
    sophisticated near optimal strategies deterministic blocks world exist  see slaney   thiebaux 
       much complex encode might caused time performance problems 

  

fithiebaux  gretton  slaney  price   kabanza

part 
c
e 
g 
g  
j  
j  
j 
p
q
r

description
symbolic lao 
first order heuristic search fluent calculus
nmrdpp without control knowledge
nmrdpp control knowledge
interpreter hand written classy policies
learns classy policies random walks
version replanning upon failure
mgpt  lrtdp automatically extracted heuristics
probaprop  conformant probabilistic planner
structured reachability analysis structured pi

reference
 feng   hansen       
 karabaev   skvortsova       
paper
paper
 fern et al        
 fern et al        
 hoffmann   nebel       
 bonet   geffner       
 onder et al        
 teichteil konigsbuch   fabiani       

table    competition participants  domain specific planners starred
dom
prob
g  
j  
j  
e 
j 
g 
r
p
c
q

 
   
   
   
   
   

bw c nr
 
  
       
       
       
       
       

bw nc nr
 
   
   
   
   
   

bx nr
          
       
   
   
   
  
   

expl bw
  

hanoise
   

zeno
       

tire nr
    

 



  
  


   
  
   
   
 

  
  
  
  
 
  

   

 

total
   
   
   
   
   
   
   
   
   
  

table    results goal based problems  domain specific planners starred  entries
percentage runs goal reached  blank indicates
planner unable attempt problem  indicates planner
attempted problem never able achieve goal    indicates
result unavailable  due bug evaluation software  couple
results initially announced found invalid  
dom
prob
j  
g  
e 
j  
j 
p
c
g 
r
q

 
   
   
   
   
   

bw c r
 
  
       
       
       
       
       

 
   
   
   
   
   
   
   
   
   
   

 
   
   
   
   
   
   

bw nc r
        
  
               
               
       
   
       

   
   

bx r
                
   
   
   
       

   
   
   


   



   

file
    

tire r
    

  
  



 




  

total
    
    
    
    
    
    
   
   
   
   

table    results reward based problems  domain specific planners starred  entries
average reward achieved    runs  blank indicates
planner unable attempt problem  indicates planner
attempted problem achieve strictly positive reward    indicates
result unavailable 
  

fidecision theoretic planning non markovian rewards

tables     show results competition  extracted competition overview paper  younes  littman  weissmann    asmuth       
competition web site http   www cs rutgers edu  mlittman topics ipc   pt  
first tables concerns goal based problems second reward based problems  entries tables represent goal achievement percentage average reward achieved various planner versions  left column  various problems  top
two rows   planners top part tables domain specific  problems
known domains lie left hand side tables  colored blocks world problems
bw c nr  goal based version  bw c r  reward version           blocks 
non colored blocks world problems bw nc nr  goal based version    blocks  bwnc r  reward based version                       blocks  box world problems
bx nr  goal based  bx r  reward based        cities       boxes  problems unknown domains lie right hand side tables  comprise 
expl bw  exploding version    block blocks world problem putting
block may destroy object put on  zeno  probabilistic variant zeno travel
domain problem ipc     plane    persons    cities   fuel levels  hanoise 
probabilistic variant tower hanoi problem   disks   rods  file  problem
putting    files   randomly chosen folders  tire  variant tire world problem
   cities spare tires   them  tire may go flat driving 
planner nmrdpp g  g  version  able attempt problems  achieving strictly positive reward   them  even  j    competition overall
winner  able successfully attempt many problems  nmrdpp performed particularly well goal based problems  achieving goal      runs except expl bw 
hanoise  tire nr  note three problems  goal achievement probability
optimal policy exceed       planner outperformed nmrdpp
scale  pointed before  behaves well probabilistic version blocks box
world optimal policies close deterministic problem
hoffmann        analyses reasons heuristic works well traditional planning benchmarks blocks world logistics  hand  unable
solve unknown problems different structure require substantial
probabilistic reasoning  although problems easily solved number participating planners  expected  large discrepancy version nmrdpp
allowed use search control  g   domain independent version  g   
latter performs okay unknown goal based domains  able solve
known ones  fact  except ff  none participating domain independent
planners able solve problems 
reward based case  nmrdpp control knoweldge behaves well known
problems  human encoded policies  j   performed better  without control knowledge nmrdpp unable scale problems  participants
mgpt are  furthermore nmrdpp appears perform poorly two unknown problems 
cases  might due fact fails generate optimal policy  suboptimal policies easily high negative score domains  see younes et al         
r tire  know nmrdpp indeed generate suboptimal policy  additionally 
could nmrdpp unlucky sampling based policy evaluation process 

  

fithiebaux  gretton  slaney  price   kabanza

tire r particular  high variance costs various trajectories
optimal policy 
alltogether  competition results suggest control knowledge likely essential solving larger problems  markovian not  nmrdpp  that 
observed deterministic planners  approaches making use control knowledge
quite powerful 

   conclusion  related  future work
paper  examined problem solving decision processes nonmarkovian rewards  described existing approaches exploit compact representation reward function automatically translate nmrdp equivalent
process amenable mdp solution methods  computational model underlying
framework traced back work relationship linear temporal logic
automata areas automated verification model checking  vardi       
wolper         remaining framework  proposed new representation
non markovian reward functions translation mdps aimed making best
possible use state based anytime heuristic search solution method  representation extends future linear temporal logic express rewards  translation
effect embedding model checking solution method  results mdp
minimal size achievable without stepping outside anytime framework  consequently
better policies deadline  described nmrdpp  software platform
implements approaches common interface  proved useful tool
experimental analysis  system analysis first kind 
able identify number general trends behaviours methods
provide advice best suited certain circumstances  obvious
reasons  analysis focused artificial domains  additional work examine
wider range domains practical interest  see form results take
context  ultimately  would analysis help nmrdpp automatically select
appropriate method  unfortunately  difficulty translating
pltl  fltl  likely nmrdpp would still maintain pltl
 fltl version reward formulae 
detailed comparison approach solving nmrdps existing methods  bacchus et al               found sections         two important aspects future
work would help take comparison further  one settle question appropriateness translation structured solution methods  symbolic implementations
solution methods consider  e g  symbolic lao   feng   hansen         well
formula progression context symbolic state representations  pistore   traverso 
      could investigated purpose  take advantage greater
expressive power  fltl consider richer class decision processes  instance
uncertainty rewards received when  many extensions language
possible  adding eventualities  unrestricted negation  first class reward propositions 
quantitative time  etc  course  dealing via progression without backtracking
another matter 

  

fidecision theoretic planning non markovian rewards

investigate precise relationship line work recent work
planning temporally extended goals non deterministic domains  particular
interest weak temporally extended goals expressible eagle language
 dal lago et al          temporally extended goals expressible  ctl   baral  
zhao         eagle enables expression attempted reachability maintenance goals
form try reach p try maintain p  add goals do reach p
do maintain p already expressible ctl  idea generated policy
make every attempt satisfying proposition p  furthermore  eagle includes recovery goals
form g  fail g    meaning goal g  must achieved whenever goal g  fails 
cyclic goals form repeat g  meaning g achieved cyclically
fails  semantics goals given terms variants buchi tree automata
preferred transitions  dal lago et al         present planning algorithm based
symbolic model checking generates policies achieving goals  baral zhao
       describe  ctl   alternative framework expressing subset eagle goals
variety others   ctl  variant ctl  allows formulae involving
two types path quantifiers  quantifiers tied paths feasible generated
policy  usual  quantifiers generally tied paths feasible
domain actions  baral zhao        present planning algorithm 
would interesting know whether eagle  ctl  goals encoded nonmarkovian rewards framework  immediate consequence would nmrdpp
could used plan them  generally  would examine respective
merits non deterministic planning temporally extended goals decision theoretic
planning non markovian rewards 
pure probabilistic setting  no rewards   recent related research includes work
planning controller synthesis probabilistic temporally extended goals expressible
probabilistic temporal logics csl pctl  younes   simmons        baier et al  
       logics enable expressing statements probability policy satisfying given temporal goal exceeding given threshold  instance  younes simmons
       describe general probabilistic planning framework  involving concurrency  continuous time  temporally extended goals  rich enough model generalised semi markov
processes  solution algorithms directly comparable presented here 
another exciting future work area investigation temporal logic formalisms
specifying heuristic functions nmrdps generally search problems
temporally extended goals  good heuristics important solution methods
targeting  surely value ought depend history  methods
described could applicable description processing heuristics  related
problem extending search control knowledge fully operate
presence temporally extended goals  rewards  stochastic actions  first issue
branching probabilistic logics ctl pctl variants preferred
fltl describing search control knowledge  stochastic actions
involved  search control often needs refer possible futures even
probabilities    another major problem goalp modality 
key specification reusable search control knowledge interpreted respect
    would argue  hand  ctl necessary representing non markovian rewards 

  

fithiebaux  gretton  slaney  price   kabanza

fixed reachability goal    bacchus   kabanza         such  applicable
domains temporally extended goals  let alone rewards  kabanza thiebaux       
present first approach search control presence temporally extended goals
deterministic domains  much remains done system nmrdpp able
support meaningful extension goalp 
finally  let us mention related work area databases uses similar approach
pltlstr extend database auxiliary relations containing sufficient information
check temporal integrity constraints  chomicki         issues somewhat different
raised nmrdps  ever one sequence databases  matters
size auxiliary relations avoiding making redundant distinctions 

acknowledgements
many thanks fahiem bacchus  rajeev gore  marco pistore  ron van der meyden  moshe
vardi  lenore zuck useful discussions comments  well anonymous
reviewers david smith thorough reading paper excellent
suggestions  sylvie thiebaux  charles gretton  john slaney  david price thank national ict australia support  nicta funded australian governments
backing australias ability initiative  part australian research council  froduald kabanza supported canadian natural sciences engineering research
council  nserc  

appendix a  class reward normal formulae
existing decision procedure  slaney        determining whether formula rewardnormal guaranteed terminate finitely  involves construction comparison
automata rather intricate practice  therefore useful give simple syntactic
characterisation set constructors obtaining reward normal formulae even though
formulae constructible 
say formula material iff contains   temporal operators
is  material formulae boolean combinations atoms 
consider four operations behaviours representable formulae  fltl  firstly 
behaviour may delayed specified number timesteps  secondly  may made
conditional material trigger  thirdly  may started repeatedly material
termination condition met  fourthly  two behaviours may combined form
union  operations easily realised syntactically corresponding operations
formulae  material formula 
delay f    

f

cond m  f     f
loop m  f     f u
union f    f      f  f 
    f atemporal formula  goalp f   true iff f true goal states 

  

fidecision theoretic planning non markovian rewards

shown  slaney        set reward normal formulae closed delay 
cond  for material m   loop  for material m  union  closure
    operations represents class behaviours closed intersection
concatenation well union 
many familiar reward normal formulae obtainable   applying four operations  example   p    loop   cond p       sometimes paraphrase necessary 
example    p q     required form antecedent
conditional  equivalent  p  q     loop   cond p  delay cond q        
cases easy  example formula p u  p   stipulates reward
first time p happens form suggested  capture
behaviour using operations requires formula  p       p    u p  

appendix b  proofs theorems
property   b   i  b      i    b f iff           b prog b    f   
proof 
induction structure f   several base cases  fairly trivial 
f     f   nothing prove  progress hold
everywhere nowhere respectively  f   p f holds progresses  
holds i   f hold progresses
hold i     case f   p similar  last base case  f      following
equivalent 
   i    b f
 i  b
b
prog b    f      
          b prog b    f  
induction case    f   g h  following equivalent 
   i    b f
   i    b g    i    b h
          b prog b    g            b prog b    h   by induction hypothesis 
          b prog b    g  prog b    h 
          b prog b    f  
induction case    f   g h  analogous case   
induction case    f   g  trivial inspection definitions 
induction case    f   g u h  f logically equivalent h  g  g u h 
cases        holds stage behaviour b iff prog b    f   holds stage i   

theorem   let f reward normal  let hf    f         i result progressing
successive states sequence   then  provided   rew i    
iff  i  bf  

  

fithiebaux  gretton  slaney  price   kabanza

proof  first  definition reward normality  f reward normal   b f iff
i   i  bf  i  b  next    b f progressing f according
b  that is  letting bi true iff  i  b  cannot lead contradiction
property    progression truth preserving 
remains  then  show    b f progressing f according b
must lead eventually   proof induction structure f
usual base case f literal  an atom  negated atom       trivial 
case f   g h  suppose    b f   either    b g    b h  induction
hypothesis either g h progresses eventually   hence conjunction 
case f   g h  suppose    b f      b g    b h  induction
hypothesis g h progresses eventually   suppose without loss generality
g progress h does  point g progressed
formula g   f progressed g   simplifies g     since g   progresses
eventually  f  
case f   g  suppose    b f   let       let b           b  
   b   g  induction hypothesis g progressed according b   eventually
reaches   progression f according b exactly
first step  leads  
case f   g u h  suppose    b f   j    j     b g
j     i     b h  proceed induction j  base case j         b g
   b h whence main induction hypothesis g h eventually progress
  thus h  g f     progresses eventually f     particular f     f  
establishing base case  induction case  suppose   b g  and course    b h  
since f equivalent h  g f      b f      b h   b g  clearly    b f  
b   previous case  therefore     b   f failure occurs stage j  
  therefore hypothesis induction j applies  f progressed
according b   goes eventually   f progressed according b goes
similarly  

theorem   let   set e states equivalent mdp d    hs  s    a  pr  ri 
d  minimal iff every e state   reachable   contains two distinct e states s  
s    s        s      s        s     
proof  proof construction canonical equivalent mdp dc   let set
e     partitioned equivalence classes 
finite prefixes state sequences d s
e      r   i      
  i    j  iff  i    j   i   d s
r   j      let   i   denote equivalence class  i   let e set
equivalence classes  let function takes   i   e a i   
 i   j  a   i     let    i    a    j    pr i   a  s    j    
  i   hsi   otherwise let    i    a    j         let r   i    r  i    note
following four facts 
   functions a  r well defined 
   dc   he   hs  i   a    ri equivalent mdp    i       

  

fidecision theoretic planning non markovian rewards

   equivalent mdp d   mapping subset states
d   onto e 
   d  satisfies condition every e state   reachable   contains two
distinct e states s   s    s        s      s        s     iff dc isomorphic
d   
fact   amounts   i    j  matter
two sequences used define a  r equivalence class  cases
simply  i    j   case r  special case   h i
equality rewards extensions 
fact   matter checking four conditions definition   hold  these 
conditions       s       s       a   i      a i    hold trivially construction 
e      r   i      r  i  
condition   says feasible state sequence d s
i  given construction  condition   states 
s    s  s  a s    pr s    a  s        
e       s    exists unique   j   e  j   s   
 i  d s
a   i        i    a    j      pr s    a  s    
e       s    required  j   i   hs  i 
suppose pr s      s          i  d s
course a   i      a i    required condition reads 
  i   hs  i  unique element x e  x    s 
a i       i    a  x    pr s    a  s    
establish existence  need a i      i    a    i   hs  i     pr i   a  s    
immediate definition above  establish uniqueness  suppose
 x    s     i    a  x    pr s    a  s    actions a i    since pr s      s     
   transition probability   i   x nonzero action  definition
  x   i   hs  i  
fact   readily observed  let equivalent mdp d  states s 
s  d  state x  x    s  one state
 y     s  action a s    gives nonzero probability
transition x   follows uniqueness part condition   definition  
together fact transition function probability distribution  sums    
therefore given finite state sequence  i  one state reached
start state following  i   therefore induces equivalence relation
   i   j  iff lead state  the sequences
feasible may regarded equivalent    reachable state
associated nonempty equivalence class finite sequences states d  working
definitions  may observe sub relation  if  i   j 
 i   j    hence function takes equivalence class
feasible sequence  i    i   induces mapping h  an epimorphism fact 
reachable subset states onto e 
establish fact    must shown case d  mapping
reversed  equivalence class   i   dc corresponds exactly one element
  

fithiebaux  gretton  slaney  price   kabanza

e   
d    suppose  for contradiction   exist sequences   i    j  d s
 
  i    j  following two sequences s  arrive two different
elements s   s   d   s        i    j    s      s         s      therefore
e
exists sequence  k  d s 
r   i      k      r   j      k   
contradicts condition   i    j  

theorem   follows immediately facts    
theorem   let d  translation definition    d  blind minimal
equivalent mdp d 
proof  reachability e states obvious  constructed
reached  e state pair hs  state reward function
specification  fact     hs  i  determines distribution rewards
continuations sequences
reach hs  i  is      s 
p
reward  f  r   r   bf    d  blind minimal  exist
distinct e states hs  hs    sum   makes
  semantically equivalent  contradicting supposition distinct 


appendix c  random problem domains
random problem domains produced first creating random action specification
defining domain dynamics  experiments conducted   involved
producing  second step  random reward specification desired properties
relation generated dynamics 
random generation domain dynamics takes parameters number n
propositions domain number actions produced  starts
assigning effects action proposition affected exactly one
action  example    actions    propositions  first   actions may affect
  propositions each   th one    affected propositions different 
action initial effects  continue add effects one time 
sufficient proportion state space reachable see proportion reachable parameter
below  additional effect generated picking random action random
proposition  producing random decision diagram according uncertainty
structure parameters below 
uncertainty parameter probability non zero one value leaf node 
uncertainty   result leaf nodes random values uniform
distribution  uncertainty   result leaf nodes values    
equal probability 
structure  or influence  parameter probability decision diagram containing
particular proposition  influence   result decision diagrams
    none included paper  however 

  

fidecision theoretic planning non markovian rewards

including propositions  and unlikely significant structure    
result decision diagrams depend values propositions 
proportion reachable parameter lower bound proportion entire  n
state space reachable start state  algorithm adds behaviour
lower bound reached  value   result algorithm running
actions sufficient allow entire state space reachable 
reward specification produced regard generated dynamics
specified number rewards reachable specified number unreachable 
first  decision diagram produced represent states reachable
not  given domain dynamics  next  random path taken root
decision diagram true terminal generating attainable reward  false
terminal producing unattainable reward  propositions encountered
path  negated not  form conjunction reward formula  process
repeated desired number reachable unreachable rewards obtained 

references
at t labs research         graphviz  available http   www research att com 
sw tools graphviz  
bacchus  f   boutilier  c     grove  a          rewarding behaviors  proc  american
national conference artificial intelligence  aaai   pp           
bacchus  f   boutilier  c     grove  a          structured solution methods nonmarkovian decision processes  proc  american national conference artificial
intelligence  aaai   pp         
bacchus  f     kabanza  f          planning temporally extended goals  annals
mathematics artificial intelligence          
bacchus  f     kabanza  f          using temporal logic express search control knowledge
planning  artificial intelligence            
baier  c   groer  m   leucker  m   bollig  b     ciesinski  f          controller synthesis
probabilistic systems  extended abstract   proc  ifip international conference
theoretical computer science  ifip tcs  
baral  c     zhao  j          goal specification presence nondeterministic actions 
proc  european conference artificial intelligence  ecai   pp         
barto  a   bardtke  s     singh  s          learning act using real time dynamic programming  artificial intelligence            
bonet  b     geffner  h          labeled rtdp  improving convergence real time
dynamic programming  proc  international conference automated planning
scheduling  icaps   pp       

  

fithiebaux  gretton  slaney  price   kabanza

bonet  b     geffner  h          mgpt  probabilistic planner based heuristic search 
journal artificial intelligence research             
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions computational leverage  journal artificial intelligence research 
vol      pp      
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
factored representations  artificial intelligence                   
calvanese  d   de giacomo  g     vardi  m          reasoning actions planning ltl action theories  proc  international conference principles
knowledge representation reasoning  kr   pp         
cesta  a   bahadori  s   g  c   grisetti  g   giuliani  m   loochi  l   leone  g   nardi  d  
oddi  a   pecora  f   rasconi  r   saggase  a     scopelliti  m          robocare
project  cognitive systems care elderly  proc  international conference
aging  disability independence  icadi  
chomicki  j          efficient checking temporal integrity constraints using bounded
history encoding  acm transactions database systems                 
dal lago  u   pistore  m     traverso  p          planning language extended
goals  proc  american national conference artificial intelligence  aaai   pp 
       
dean  t   kaelbling  l   kirman  j     nicholson  a          planning time constraints stochastic domains  artificial intelligence           
dean  t     kanazawa  k          model reasoning persistance causation 
computational intelligence            
drummond  m          situated control rules  proc  international conference
principles knowledge representation reasoning  kr   pp         
emerson  e  a          temporal modal logic  handbook theoretical computer
science  vol  b  pp           elsevier mit press 
feng  z     hansen  e          symbolic lao search factored markov decision processes  proc  american national conference artificial intelligence  aaai   pp 
       
feng  z   hansen  e     zilberstein  s          symbolic generalization on line planning 
proc  conference uncertainty artificial intelligence  uai   pp         
fern  a   yoon  s     givan  r          learning domain specific knowledge random
walks  proc  international conference automated planning scheduling
 icaps   pp         
fourman  m          propositional planning  proc  aips workshop model theoretic
approaches planning  pp       
  

fidecision theoretic planning non markovian rewards

gretton  c   price  d     thiebaux  s       a   implementation comparison solution
methods decision processes non markovian rewards  proc  conference
uncertainty artificial intelligence  uai   pp         
gretton  c   price  d     thiebaux  s       b   nmrdpp  system decision theoretic
planning non markovian rewards  proc  icaps workshop planning
uncertainty incomplete information  pp       
haddawy  p     hanks  s          representations decision theoretic planning  utility
functions deadline goals  proc  international conference principles
knowledge representation reasoning  kr   pp       
hansen  e     zilberstein  s          lao   heuristic search algorithm finds solutions
loops  artificial intelligence            
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning using
decision diagrams  proc  conference uncertainty artificial intelligence  uai  
pp         
hoffmann  j          local search topology planning benchmarks  theoretical analysis 
proc  international conference ai planning scheduling  aips   pp        
hoffmann  j     nebel  b          planning system  fast plan generation
heuristic search  journal artificial intelligence research             
howard  r          dynamic programming markov processes  mit press  cambridge 
ma 
kabanza  f     thiebaux  s          search control planning temporally extended
goals  proc  international conference automated planning scheduling
 icaps   pp         
karabaev  e     skvortsova  o          heuristic search algorithm solving firstorder mdps  proc  conference uncertainty artificial intelligence  uai  
pp         
koehler  j     schuster  k          elevator control planning problem  proc 
international conference ai planning scheduling  aips   pp         
korf  r          real time heuristic search  artificial intelligence             
kushmerick  n   hanks  s     weld  d          algorithm probabilistic planning 
artificial intelligence             
lichtenstein  o   pnueli  a     zuck  l          glory past  proc  conference
logics programs  pp          lncs  volume     
onder  n   whelan  g  c     li  l          engineering conformant probabilistic planner 
journal artificial intelligence research          

  

fithiebaux  gretton  slaney  price   kabanza

pistore  m     traverso  p          planning model checking extended goals
non deterministic domains  proc  international joint conference artificial intelligence  ijcai      pp         
slaney  j          semi positive ltl uninterpreted past operator  logic journal
igpl             
slaney  j     thiebaux  s          blocks world revisited  artificial intelligence      
       
somenzi  f         
cudd  cu decision diagram package 
ftp   vlsi colorado edu pub  

available

teichteil konigsbuch  f     fabiani  p          symbolic heuristic policy iteration algorithms structured decision theoretic exploration problems  proc  icaps workshop planning uncertainty autonomous systems 
thiebaux  s   hertzberg  j   shoaff  w     schneider  m          stochastic model
actions plans anytime planning uncertainty  international journal
intelligent systems                 
thiebaux  s   kabanza  f     slaney  j       a   anytime state based solution methods
decision processes non markovian rewards  proc  conference uncertainty
artificial intelligence  uai   pp         
thiebaux  s   kabanza  f     slaney  j       b   model checking approach decisiontheoretic planning non markovian rewards  proc  ecai workshop modelchecking artificial intelligence  mochart      pp         
vardi  m          automated verification   graph  logic  automata  proc  international joint conference artificial intelligence  ijcai   pp          invited
paper 
wolper  p          relation programs computations models temporal
logic  proc  temporal logic specification  lncs      pp        
younes  h  l  s     littman  m          ppddl     extension pddl expressing
planning domains probabilistic effects  tech  rep  cmu cs         school
computer science  carnegie mellon university  pittsburgh  pennsylvania 
younes  h  l  s   littman  m   weissmann  d     asmuth  j          first probabilistic
track international planning competition  journal artificial intelligence
research  vol      pp         
younes  h     simmons  r  g          policy generation continuous time stochastic
domains concurrency  proc  international conference automated planning
scheduling  icaps   pp         

  


