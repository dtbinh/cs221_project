journal artificial intelligence research                 

submitted        published     

approximate policy iteration policy language bias 
solving relational markov decision processes
alan fern

afern cs orst edu
school electrical engineering computer science  oregon state university
sungwook yoon
sy purdue edu
robert givan
givan purdue edu
school electrical computer engineering  purdue university

abstract
study approach policy selection large relational markov decision processes
 mdps   consider variant approximate policy iteration  api  replaces
usual value function learning step learning step policy space  advantageous
domains good policies easier represent learn corresponding
value functions  often case relational mdps interested in 
order apply api problems  introduce relational policy language
corresponding learner  addition  introduce new bootstrapping routine goalbased planning domains  based random walks  bootstrapping necessary
many large relational mdps  reward extremely sparse  api ineffective
domains initialized uninformed policy  experiments show
resulting system able find good policies number classical planning domains
stochastic variants solving extremely large relational mdps 
experiments point limitations approach  suggesting future work 

   introduction
many planning domains naturally represented terms objects relations
among them  accordingly  ai researchers long studied algorithms planning
learning to plan relational state action spaces  include  example  classical
strips domains blocks world logistics 
common criticism domains algorithms assumption idealized 
deterministic world model  this  part  led ai researchers study planning
learning within decision theoretic framework  explicitly handles stochastic environments generalized reward based objectives  however  work based
explicit propositional state space models  far demonstrated scalability
large relational domains commonly addressed classical planning 
intelligent agents must able simultaneously deal complexity arising
relational structure complexity arising uncertainty  primary goal
research move toward agents bridging gap classical
decision theoretic techniques 
paper  describe straightforward practical method solving large 
relational mdps  work viewed form relational reinforcement learning
 rrl  assume strong simulation model environment  is  assume
access black box simulator  provide  relationally represented 
c
    
ai access foundation  rights reserved 

fifern  yoon    givan

state action pair receive sample appropriate next state reward distributions  goal interact simulator order learn policy achieving high
expected reward  separate challenge  considered here  combine work
methods learning environment simulator avoid dependence provided
simulator 
dynamic programming approaches finding optimal control policies mdps  bellman        howard         using explicit  flat  state space representations  break
state space becomes extremely large  recent work extends algorithms
use propositional  boutilier   dearden        dean   givan        dean  givan   
leach        boutilier  dearden    goldszmidt        givan  dean    greig        guestrin 
koller  parr    venkataraman      b  well relational  boutilier  reiter    price       
guestrin  koller  gearhart    kanodia      a  state space representations  extensions significantly expanded set approachable problems  yet shown
capacity solve large classical planning problems benchmark problems
used planning competitions  bacchus         let alone stochastic variants  one possible reason methods based calculating representing value
functions  familiar strips planning domains  among others   useful value functions
difficult represent compactly  manipulation becomes bottle neck 
techniques purely deductivethat is  value function guaranteed certain level accuracy  rather  work  focus inductive
techniques make guarantees practice  existing inductive forms approximate policy iteration  api  utilize machine learning select compactly represented
approximate value functions iteration dynamic programming  bertsekas   tsitsiklis         machine learning algorithm  selection hypothesis space 
space value functions  critical performance  example space used frequently
space linear combinations human selected feature set 
knowledge  previous work applies form api
benchmark problems classical planning  stochastic variants   again  one
reason high complexity typical value functions large relational
domains  making difficult specify good value function spaces facilitate learning 
comparably  often much easier compactly specify good policies  accordingly
good policy spaces learning  observation basis recent work inductive policy selection relational planning domains  deterministic  khardon      a 
martin   geffner         probabilistic  yoon  fern    givan         techniques
show useful policies learned using policy space bias described generic
 relational  knowledge representation language  incorporate ideas variant api  achieves significant success without representing learning approximate
value functions  course  natural direction future work combine policy space
techniques value function techniques  leverage advantages both 
given initial policy  approach uses simulation technique policy rollout
 tesauro   galperin        generate trajectories improved policy  trajectories given classification learner  searches classifier  policy 
matches trajectory data  resulting approximately improved policy  two
   recent work relational reinforcement learning applied strips problems much simpler
goals typical benchmark planning domains  discussed section   

  

fiapi policy language bias

steps iterated improvement observed  resulting algorithm
viewed form api iteration carried without inducing approximate
value functions 
avoiding value function learning  algorithm helps address representational
challenge applying api relational planning domains  however  another fundamental
challenge that  non trivial relational domains  api requires form bootstrapping  particular  strips planning domains reward  corresponds
achieving goal condition  sparsely distributed unlikely reached random exploration  thus  initializing api random uninformed policy  likely result
reward signal hence guidance policy improvement  one approach bootstrapping rely user provide good initial policy heuristic gives guidance
toward achieving reward  rather  work develop new automatic bootstrapping
approach goal based planning domains  require user intervention 
bootstrapping approach based idea random walk problem distributions 
given planning domain  blocks world  distribution randomly generates
problem  i e   initial state goal  selecting random initial state
executing sequence n random actions  taking goal condition subset
properties resulting state  problem difficulty typically increases n 
small n  short random walks  even random policies uncover reward  intuitively 
good policy problems walk length n used bootstrap api problems
slightly longer walk lengths  bootstrapping approach iterates idea  starting
random policy small n  gradually increasing walk length
learn policy long random walks  long random walk policies clearly capture
much domain knowledge  used various ways  here  show empirically
policies often perform well problem distributions relational domains used
recent deterministic probabilistic planning competitions 
implementation bootstrapped api approach took second place   competitors hand tailored track      international probabilistic planning competition   knowledge first machine learning based system entered
planning competition  either deterministic probabilistic 
here  give evaluation system number probabilistic deterministic
relational planning domains  including aips      competition benchmarks  benchmarks hand tailored track      probabilistic planning competition 
results show system often able learn policies domains perform
well long random walk problems  addition  policies often perform well
planning competition problem distributions  comparing favorably state of theart planner deterministic domains  experiments highlight number
limitations current system  point interesting directions future work 
remainder paper proceeds follows  section    introduce problem
setup then  section    present new variant api  section    provide
   note  however  approach hand tailored  rather  given domain definition  system
learns policy offline  automatically  applied problem domain 
entered hand tailored track track facilitated use offline learning 
providing domains problem generators competition  entrants humanwritten domain 

  

fifern  yoon    givan

technical analysis algorithm  giving performance bounds policy improvement
step  sections      describe implemented instantiation api approach
relational planning domains  includes description generic policy language
relational domains  classification learner language  novel bootstrapping
technique goal based domains  section   presents empirical results  finally
sections     discuss related work future directions 

   problem setup
formulate work framework markov decision processes  mdps  
primary motivation develop algorithms relational planning domains  first
describe problem setup approach general  action simulatorbased mdp representation  later  section    describe particular representation planning domains
relational mdps corresponding relational instantiation approach 
following adapting kearns  mansour  ng        bertsekas tsitsiklis
        represent mdp using generative model hs  a  t  r  ii  finite
set states  finite  ordered set actions  randomized action simulation
algorithm that  given state action a  returns next state s  according unknown
probability distribution pt  s   s  a   component r reward function maps
real numbers  r s  a  representing reward taking action state s 
randomized initial state algorithm inputs returns state according
unknown distribution p   s   sometimes treat  s  a  random variables
distributions p     pt   s  a  respectively 
mdp   hs  a  t  r  ii  policy  possibly stochastic  mapping
a  value function   denoted v  s   represents expected  cumulative  discounted
reward following policy starting state s  unique solution
v  s    e r s   s     v  t  s   s    

   

      discount factor  q value function q  s  a  represents
expected  cumulative  discounted reward taking action state following  
given
q  s  a    r s  a    e v  t  s  a   

   

measure quality policy objective function v      e v  i    giving
expected value obtained policy starting randomly drawn initial
state  common objective mdp planning reinforcement learning find
optimal policy   argmax v     however  automated technique  including one
present here  date able guarantee finding optimal policy relational
planning domains consider  reasonable running time 
well known fact given current policy   define new improved
policy
pi  s    argmaxaa q  s  a 

   

value function pi guaranteed    worse
state s     strictly improve state optimal  policy iteration
  

fiapi policy language bias

algorithm computing optimal policies iterating policy improvement  pi 
initial policy reach fixed point  guaranteed optimal policy 
iteration policy improvement involves two steps     policy evaluation compute
value function v current policy      policy selection  where  given v
step    select action maximizes q  s  a  state  defining new improved
policy 
finite horizons  since api variant based simulation  must bound
simulation trajectories horizon h  technical analysis section   use notion
finite horizon discounted reward  h horizon value function vh recursively defined

v   s      

vh  s    e r s   s     vh   t  s   s    

   

giving expected discounted reward obtained following h steps s 
 t  s  a     h horizon
define h horizon q function qh  s  a    r s  a    e vh 

objective function v h      e vh  i    well known  effect using finite
horizon made arbitrarily small  particular  states
actions a  approximation error decreases exponentially h 
 v  s  vh  s   h vmax  
 q  s  a  qh  s  a   h vmax  
rmax
vmax  
 
 
rmax maximum absolute value reward action state 
get  v h    v     h vmax  

   approximate policy iteration policy language bias
exact solution techniques  policy iteration  typically intractable large statespace mdps  arising relational planning domains  section 
introduce new variant approximate policy iteration  api  intended domains 
first  review generic form api used prior work  based learning approximate
value functions  next  motivated fact value functions often difficult learn
relational domains  describe api variant  avoids learning value functions
instead learns policies directly state action mappings 
    api approximate value functions
api  described bertsekas tsitsiklis         uses combination monte carlo
simulation inductive machine learning heuristically approximate policy iteration
large state space mdps  given current policy   iteration api approximates
policy evaluation policy selection  resulting approximately improved policy  
first  policy evaluation step constructs training set samples v small
representative set states  sample computed using simulation  estimating v  s 
policy state drawing number sample trajectories starting
  

fifern  yoon    givan

averaging cumulative  discounted reward along trajectories  next 
policy selection step uses function approximator  e g   neural network  learn
approximation v v based training data  v serves representation
  selects actions using sampled one step lookahead based v  
 s    arg max r s  a    e v  t  s  a    
aa

common variant procedure learns approximation q rather v  
api exploits function approximators generalization ability avoid evaluating
state state space  instead directly evaluating small number training states 
thus  use api assumes states perhaps actions represented factored
form  typically  feature vector  facilitates generalizing properties training data
entire state action spaces  note case perfect generalization  i e  
v  s    v  s  states s   equal exact policy improvement
pi   thus api simulates exact policy iteration  however  practice  generalization
perfect  typically guarantees policy improvement  nevertheless 
api often converges usefully  tesauro        tsitsiklis   van roy        
success api procedure depends critically ability represent
learn good value function approximations  mdps  arising
relational planning domains  often difficult specify space value functions
learning mechanism facilitate good generalization  example  work relational
reinforcement learning  dzeroski  deraedt    driessens        shown learning
approximate value functions classical domains  blocks world  problematic   spite this  often relatively easy compactly specify good policies using
language  relational  state action mappings  suggests languages may
provide useful policy space biases learning api  however  prior api methods
based approximating value functions hence leverage biases 
motivation  consider form api directly learns policies without directly
representing approximating value functions 
    using policy language bias
policy simply classifier  possibly stochastic  maps states actions  api
approach based view  motived recent work casts policy selection
standard classification learning problem  particular  given ability observe
trajectories target policy  use machine learning select policy  classifier 
mimics target closely possible  khardon      b  studied learning setting
provided pac like learnability results  showing certain assumptions  small
number trajectories sufficient learn policy whose value close
target  addition  recent empirical work  relational planning domains  khardon      a 
martin   geffner        yoon et al          shown using expressive languages
   strong assumptions  api shown converge infinite limit near optimal
value function  see proposition     bertsekas tsitsiklis        
   particular  rrl work considered variety value function representation including relational
regression trees  instance based methods  graph kernels  none generalized well
varying numbers objects 

  

fiapi policy language bias

specifying state action mappings  good policies learned sample trajectories
good policies 
results suggest that  given policy   somehow generate trajectories
improved policy  learn approximately improved policy based
trajectories  idea basis approach  figure   gives pseudo code api
variant  starts initial policy   produces sequence approximately
improved policies  iteration involves two primary steps  first  given current
policy   procedure improved trajectories  approximately  generates trajectories
improved policy     pi   second  trajectories used training data
procedure learn policy  returns approximation     describe
step detail 
step    generating improved trajectories  given base policy   simulation technique policy rollout  tesauro   galperin        bertsekas   tsitsiklis       
computes approximation improved policy     pi     result
applying one step policy iteration   furthermore  given state s  policy rollout
computes  s  without need solve   states  thus provides
tractable way approximately simulate improved policy   large state space mdps 
often   significantly better   hence   lead substantially
improved performance small cost  policy rollout provided significant benefits
number application domains  including example backgammon  tesauro   galperin 
       instruction scheduling  mcgovern  moss    barto         network congestion control
 wu  chong    givan         solitaire  yan  diaconis  rusmevichientong    van roy 
      
policy rollout computes  s   estimate    s   estimating q  s  a 
action taking maximizing action  s  suggested equation   
q  s  a  estimated drawing w trajectories length h  trajectory
result starting s  taking action a  following actions selected
h   steps  estimate q  s  a  taken average cumulative
discounted reward along trajectory  sampling width w horizon h specified
user  control trade increased computation time large values 
reduced accuracy small values  note rollout applies stochastic
deterministic policies due variance q value estimates  rollout policy
stochastic even deterministic base policies 
procedure improved trajectories uses rollout generate n length h trajectories
  beginning randomly drawn initial state  rather recording
states actions along trajectory  store additional information used
policy learning algorithm  particular  ith element trajectory form
hsi    si    q si   a             q si    i  giving ith state si along trajectory  action
selected current  unimproved  policy si   q value estimates q si   a 
action  note given q value information si learning algorithm
determine approximately improved action  s   maximizing actions  desired 
step    learn policy  intuitively  want learn policy select new policy
closely matches training trajectories  experiments  use relatively simple
learning algorithms based greedy search within space policies specified policylanguage bias  sections         detail policy language learning bias used
  

fifern  yoon    givan

technique  associated learning algorithm  section   provide
technical analysis idealized version algorithm  providing guidance regarding
required number training trajectories  note labeling training state
trajectories associated q values action  rather simply
best action  enable learner make informed trade offs  focusing accuracy
states wrong decisions high costs  empirically useful  also 
inclusion  s  training data enables learner adjust data relative  
desirede g   learner uses bias focuses states large improvement
appears possible 
finally  note api effective  important initial policy
  provide guidance toward improvement  i e     must bootstrap api process 
example  goal based planning domains   reach goal sampled
states  section   discuss important issue bootstrapping introduce
new bootstrapping technique 

   technical analysis
section  consider variant policy improvement step main api loop 
learns improved policy given base policy   show select sampling
width w  horizon h  training set size n that  certain assumptions  quality
learned policy close quality   policy iteration improvement  similar
results shown previous forms api based approximate value functions
 bertsekas   tsitsiklis         however  assumptions much different nature  
analysis divided two parts  first  following khardon      b   consider
sample complexity policy learning  is  consider many trajectories
target policy must observed learner guarantee good approximation
target  second  show apply result  deterministic policies 
problem learning rollout policies  stochastic  throughout
assume context mdp   hs  a  t  r  ii 
    learning deterministic policies
trajectory length h sequence  s    a    s    a            ah    sh   alternating states si
actions ai   say deterministic policy consistent trajectory  s    a            sh  
    h   si     ai   define dh distribution set
length h trajectories  dh  t  probability generates trajectory
   s    a    s    a            ah    sh   according following process  first draw s  according
initial state distribution i  draw si    si    si        h  note
dh  t  non zero consistent t 
policy improvement step first generates trajectories rollout policy  see section       via procedure improved trajectories  learns approximation
   particular  bertsekas tsitsiklis        assumes bound l norm value function
approximation  i e   state approximation almost perfect  rather assume
improved policy   comes finite class policies consistent learner 
cases policy improvement guaranteed given additional assumption minimum
q advantage mdp  see below  

  

fiapi policy language bias

api  n  w  h  m       
   training set size n  sampling width w  horizon h 
   mdp   hs   a               t  r  ii  initial policy     discount factor  
   
loop
improved trajectories n  w  h  m    
learn policy t   
satisfied  
   e g   change small
return  
improved trajectories n  w  h  m   
   training set size n  sampling width w 
   horizon h  mdp   current policy
 
repeat n times    generate n trajectories improved policy
nil 
state drawn i     draw random initial state
    h
hq s  a             q s   i policy rollout   s  w  h        q  s  a  estimates
hs   s   q s  a             q s    i     concatenate new sample onto trajectory
action maximizing q s  a      action improved policy state
state sampled  s  a      simulate action improved policy
t 
return  
policy rollout    s  w  h   

   compute q  s  a  estimates hq s  a             q s   i

   policy   state s  sampling width w  horizon h  mdp
action ai
q s  ai     
repeat w times    q s  ai   average w trajectories
r r s  ai    s  state sampled  s  ai       take action ai
    h      take h   steps accumulating discounted reward r
r r   r s     s     
s  state sampled  s     s    
q s  ai   q s  ai     r     include trajectory average
q s  ai  

q s ai  
 
w

return hq s  a             q s   i

figure    pseudo code api algorithm  see section     instantiation learnpolicy called learn decision list 
  note rollout policy serves stochastic approximation     pi
policy iteration improvement   thus  improved trajectories viewed at 
tempting draw trajectories dh   learning step viewed learning
  

fifern  yoon    givan

 

approximation     imagining moment draw trajectories dh  
fundamental question many trajectories sufficient ensure learned
policy good     khardon      b  studied question case
deterministic policies undiscounted goal based planning domains  i e   mdps
reward received goal states   give straightforward adaptation
main result problem setting general reward functions measure
quality policy v    
learning problem formulation similar spirit standard framework probably approximately correct  pac  learning  particular  assume target
policy comes finite class deterministic policies h  example  h may correspond
set policies described bounded length decision lists  addition 
assume learner consistenti e   returns policy h consistent
training trajectories  assumptions  relatively small number
trajectories  logarithmic  h   sufficient ensure high probability
learned policy good target 
proposition    let h finite class deterministic policies  h 

set n     ln  h 
trajectories drawn independently dh     probability
every h consistent trajectories satisfies v    v     vmax     h   
proof proposition appendix  computational complexity
finding consistent policy depends policy class h  polynomial time algorithms
given interesting classes bounded length decision listshowever 
algorithms typically expensive policy classes consider practice  rather 
described section      use learner based greedy heuristic search  often
works well practice 
assumption target policy comes fixed size class h often
violated  however  pointed khardon      b   straightforward give
extension proposition   setting learner considers increasingly complex
policies consistent one found  case  sample complexity related
encoding size target policy rather size h  thus allowing use
large expressive policy classes without necessarily paying full sample complexity
price proposition   
    learning rollout policies
proof proposition   relies critically fact policy class h contains
deterministic policies  however  main api loop  target policies computed via
rollout hence stochastic due uncertainty introduced finite sampling  thus 
cannot directly use proposition   context learning trajectories produced
rollout  deal problem describe variant improved trajectories
reliably generate training trajectories deterministic policy     pi  see
equation     guaranteed improve improvement possible 
given base policy   first define  s  set actions maximize
q  s  a   note    s    min  s   minimum taken respect action ordering provided mdp  importantly policy deterministic thus
  

fiapi policy language bias

generate trajectories it  apply result learn close approximation  order generate trajectories   slightly modify improved trajectories 
modification introduced analysis only  experiments based procedures given figure    modification replace action maximization step
improved trajectories  second last statement loop   chooses next
action execute  following two steps
a   s   a    maxa q s  a  q s  a     
min a   s 
q s  a  estimate qh  s  a  computed policy rollout using sampling width
w  newly introduced parameter 
note a   s     s   selected action equal    s   condition true every state encountered modified improved trajectories
effectively generate trajectories     thus  would bound probability
a   s      s  small value appropriately choosing sampling width w 
horizon h    unfortunately  choice parameters depends mdp 
is  given particular parameter values  mdp event
a   s      s  non negligible probability state  reason first
define q advantage mdp show select appropriate parameter values
given lower bound  
given mdp policy   let   set states   iff
two actions a  q  s  a     q  s  a     i e   actions distinct
q values  state   define a   s  a   s  best action second
best action respectively measured q  s  a   q advantage defined  
minss   a   s  a   s   measures minimum q value gap optimal
sub optimal action state space  given lower bound q advantage
mdp following proposition indicates select parameter values ensure
a   s     s  high probability 
proposition   


mdp q advantage least             

h   log


 vmax

 vmax



 



w  
 

 

ln

 a 
 

state s  a   s     s  probability least      
proof given appendix  thus  parameter values satisfying conditions  mdp q advantage least guaranteed probability
least     a   s    a   s   means improved trajectories correctly select action    s  probability least       note proposition
  

fifern  yoon    givan

agrees intuition h w increase decreasing q advantage
increasing vmax w increase decreasing     
order generate n length h trajectories     modified improved trajectories
routine must compute set a     n h states  yielding n h opportunities make
error  ensure error made  modified procedure sets sampling width w

  guarantees error free training set created probability
using      nh

least      
combining observation assumption   h apply proposition
 
  follows  first  generate n     ln   h 
trajectories using modified improved
trajectories routine  with      nh
   next  learn policy trajectories using
consistent learner  know probability generating imperfect training set
bounded     chosen value n  failure probability learner
bounded     thus  get probability least     learned policy
satisfies v    v        vmax     h    giving approximation guarantee relative
improved policy     summarized following proposition 
proposition    let h finite class deterministic policies                   
mdp q advantage
least   policy pi h  set

 
 
n   ln   h 
trajectories produced modified improved trajectories using
parameters satisfying 
 


 

h   log


 vmax

 vmax    nh a 
w  
ln


least   probability every h consistent trajectories satisfies
v    v  pi    vmax     h   




one notable aspect result logarithmic dependence
number actions  a      however  practical utility hindered dependence
typically known practice  exponentially small
planning horizon  unfortunately  dependence appears unavoidable type
approach try learn trajectories pi produced rollout 
particular setting parameters  always mdp
small enough q advantage  value rollout policy arbitrarily worse
pi  

   api relational planning
work motivated goal solving relational mdps  particular  interested finding policies relational mdps represent classical planning domains
   first glance appears lower bound h decreases increasing vmax decreasing  
however  opposite true since base logarithm discount factor  strictly less
one  note since upper bounded  vmax bound h always positive 

  

fiapi policy language bias

stochastic variants  policies applied problem instance
planning domain  hence viewed form domain specific control knowledge 
section  first describe straightforward way view classical planning domains
 not single problem instances  relationally factored mdps  next  describe
relational policy space policies compactly represented taxonomic decision
lists  finally  present heuristic learning algorithm policy space 
    planning domains mdps 
say mdp hs  a  t  r  ii relational defined giving finite
set objects o  finite set predicates p   finite set action types   fact
predicate applied appropriate number objects  e g   on a  b  blocks world
fact  state set facts  interpreted representing true facts state 
state space contains possible sets facts  action action type applied
appropriate number objects  e g   putdown a  blocks world action  action
space set actions 
classical planning domain describes set problem instances related structure 
problem instance gives initial world state goal  example  blocks
world classical planning domain  problem instance specifies initial block
configuration set goal conditions  classical planners attempt find solutions
specific problem instances domain  rather  goal solve entire planning domains
finding policy applied problem instances  described below 
straightforward view classical planning domain relational mdp mdp
state corresponds problem instance 
state action spaces  classical planning domain specifies set action
types   world predicates w   possible world objects o  together define
mdp action space  state mdp corresponds single problem instance  i e  
world state goal  planning domain specifying current world
goal  achieve letting set relational mdp predicates p   w g 
g set goal predicates  set goal predicates contains predicate
world predicate w   named prepending g onto corresponding
world predicate name  e g   goal predicate gclear corresponds world predicate
clear   definition p see mdp states sets goal world
facts  indicating true world facts problem instance goal conditions 
important note  described below  mdp actions change world
facts goal facts  thus  large relational mdp viewed collection
disconnected sub mdps  sub mdp corresponds distinct goal condition 
reward function  given mdp state objective reach another mdp state
goal facts subset corresponding world factsi e   reach world state
satisfies goal  call states goal states mdp  example 
mdp state
 on table a   on a  b   clear b   gclear b  
goal state blocks world mdp  would goal state without world fact
clear b   represent objective reaching goal state quickly defining r assign
reward zero actions taken goal states negative rewards actions
  

fifern  yoon    givan

states  representing cost taking actions  typically  classical planning
domains  action costs uniformly     however  framework allows cost vary
across actions 
transition function  classical planning domain provides action simulator
 e g   defined strips rules  that  given world state action  returns new world
state  define mdp transition function simulator modified treat goal
states terminal preserve without change goal predicates mdp state  since
classical planning domains typically large number actions  action definitions
usually accompanied preconditions indicate legal actions given state 
usually legal actions small subset possible actions  assume
treats actions legal no ops  simplicity  relational mdp definition
explicitly represent action preconditions  however  assume algorithms
access preconditions thus need consider legal actions  example 
restrict rollout legal actions given state 
initial state distribution  finally  initial state distribution program
generates legal problem instances  mdp states  planning domain  example  problem domains planning competitions commonly distributed problem
generators 
definitions  good policy one reach goal states via low cost
action sequences initial states drawn i  note policies mappings
problem instances actions thus sensitive goal conditions 
way  learned policies able generalize across different goals  next describe
language representing generalized policies 
    taxonomic decision list policies 
single argument action types  many useful rules planning domains take form
apply action type object class c  martin   geffner         example 
blocks world  pick clear block belongs table table 
logistics world  unload object destination  using concept
language describing object classes  martin geffner        introduced use
decision lists rules useful learning bias  showing promising experiments
deterministic blocks world  motivation  consider policy space similar
one used originally martin geffner  generalized handle multiple action
arguments  also  historical reasons  concept language based upon taxonomic
syntax  mcallester        mcallester   givan         rather description logic
used martin geffner 
comparison predicates  relational mdps world goal predicates 
corresponding classical planning domains  often useful polices compare
current state goal  end  introduce new set predicates  called
comparison predicates  derived world goal predicates 
world predicate p corresponding goal predicate gp  introduce new comparison
predicate cp defined conjunction p gp  is  comparison predicate
fact true corresponding world goal predicates facts true 
  

fiapi policy language bias

example  blocks world  comparison predicate fact con a  b  indicates
b current state goali e   on a  b  gon a  b  true 
taxonomic syntax  taxonomic syntax provides language writing class expressions represent sets objects properties interest serve fundamental
pieces build policies  class expressions built mdp predicates
 including comparison predicates applicable  variables  policy representation 
variables used denote action arguments  runtime instantiated
objects  simplicity consider predicates arity one two  call
primitive classes relations  respectively  domain contains predicates arity
three more  automatically convert multiple auxiliary binary predicates  given
list variables x    x            xk    class expressions given by 
c x      c    xi   a thing   c x     r c x      min r 
r     r    r     r
c x  class expression  r relation expression  c  primitive class  r 
primitive relation  xi variable x  note that  classical planning domains 
primitive classes relations world  goal  comparison predicates  define depth d c x   class expression c x  one c x  either primitive
class  a thing  variable   min r   otherwise define d c x   d r c x  
d c x        r relation expression c x  class expression  given
relational mdp denote cd  x  set class expressions c x  depth
less 
intuitively class expression  r c x   denotes set objects related
relation r object set c x   expression  r c x   denotes
set objects related r chain object c x this
constructor important representing recursive concepts  e g   blocks a  
expression  min r  denotes set objects minimal relation r 
formally  let mdp state    o            ok   variable assignment 
assigns object oi variable xi   interpretation c x  relative
set objects denoted c x s o   primitive class c  interpreted set
objects predicate symbol c  true s  likewise  primitive relation r 
interpreted set object tuples relation r  holds s  class
expression a thing denotes set objects s  class expression xi   xi
variable  interpreted singleton set  oi    interpretation compound
expressions given by 
 c x  s o    o     c x s o  
 r c x  s o    o   o  c x s o s t   o    o  rs o  
 min r s o    o   o  s t   o  o    rs o     o  s t   o    o  rs o  
 r  s o   id   o    ov     o            ov  s t   oi   oi     rs o     v 
 r   s o     o  o       o    o  rs o  
c x  class expression  r relation expression  id identity relation 
examples useful blocks world concepts  given primitive classes clear  gclear 
holding  con table  along primitive relations on  gon  con  are 
  

fifern  yoon    givan

 gon  holding  depth two  denotes block want block
held 
 on  on gclear   depth three  denotes blocks currently blocks
want make clear 
 con con table  depth two  denotes set blocks well constructed
towers  see note block bv class exists
sequence blocks b            bv b  table goal
current state  i e  con table b     bi   bi goal current state
 i e  con bi   bi          v 
 gon  con con table   depth three  denotes blocks belong top
currently well constructed tower 
decision list policies represent policies decision lists action selection rules 
rule form a x            xk     l    l          lm   k argument action type 
li literals  xi action argument variables  denote list
action argument variables x    x            xk    literal form x c x  
c x  taxonomic syntax class expression x action argument variable 
given mdp state list action argument objects    o            ok    say
literal xi c x  true given iff oi c x s o   say rule
r   a x            xk     l    l          lm allows action a o          ok   iff literal rule
true given o  note literals rule action type a 
possible actions type allowed rule  rule viewed placing mutual
constraints tuples objects action type applied to  note
single rule may allow actions many actions one type  given decision list
rules say action allowed list allowed rule list 
previous rule allows actions  again  decision list may allow actions
multiple actions one type  decision list l mdp defines deterministic policy
 l  mdp  l allows actions state s   l  s  least  legal action
s  otherwise   l  s  least legal action allowed l  important
note since  l  considers legal actions  specified action preconditions 
rules need encode preconditions  allows simpler rules learning 
words  think rule implicitly containing preconditions
action type 
example taxonomic decision list policy consider simple blocks world domain
goal condition always clear red blocks  primitive classes
domain red  clear  holding  single relation on  following
policy solve problem domain 
putdown x      x  holding
pickup x      x  clear  x   on  on red  
   action ordering relational mdp defined lexicographically terms orderings action
types objects 

  

fiapi policy language bias

first rule cause agent putdown block held  otherwise 
block held  find block x  clear red block  expressed
 on  on red    pick up  appendix b gives examples complex policies
learned system experiments 
    learning taxonomic decision lists
given relational mdp  define rd l set action selection rules
length l literals whose class expression depth d  also  let
hd l denote policy space defined decision lists whose rules rd l   since
number depth bounded class expressions finite finite number rules 
hence hd l finite  though exponentially large  implementation learn policy 
used main api loop  learns policy hd l user specified values l 
use rivest style decision list learning approach  rivest       an approach
taken martin geffner        learning class based policies  primary difference
martin geffner        technique method selecting individual
rules decision list  use greedy  heuristic search  previous work used
exhaustive enumeration approach  difference allows us find rules
complex  potential cost failing find good simple rules enumeration
might discover 
recall section    training set given learn policy contains trajectories
rollout policy  learning algorithm  however  sensitive trajectory
structure  i e   order trajectory elements  thus  simplify discussion 
take input learner training set contains union
trajectory elements  means trajectory set contains n length h
trajectories  contain total n h training examples  described section   
training example form hs   s   q s  a             q s   i  state 
 s  action selected previous policy  q s  ai   q value estimate
q  s  ai    note experiments training examples contain values
legal actions state 
given training set d  natural learning goal find decision list policy
training example selects action maximum estimated q value  learning
goal  however  problematic practice often several best  or close
best  actions measured true q function  case  due random sampling 
particular action looks best according q value estimates training set
arbitrary  attempting learn concise policy matches arbitrary actions
difficult best likely impossible 
one approach  lagoudakis   parr        avoiding problem use statistical
tests determine actions clearly best  positive examples  ones
clearly best  negative examples   learner asked find
policy consistent positive negative examples  approach
shown empirical success  potential shortcoming throwing away
q value information  particular  may always possible find policy
exactly matches training data  cases  would learner make informed
trade offs regarding sub optimal actionsi e   prefer sub optimal actions larger
  

fifern  yoon    givan

learn decision list  d  d  l  b 
   training set d  concept depth d  rule length l  beam width b
l nil 
 d empty 
r learn rule d  d  l  b  
 d   r covers d  
l extend list l  r      add r end list
return l 
learn rule d  d  l  b 
   training set d  concept depth d  rule length l  beam width b
action type

   compute rule action type

ra beam search d  d  l  b  a  
return argmaxa hvalue ra   d  
beam search  d  d  l  b  a 
   training set d  concept depth d  rule length l  beam width b  action type
k arity a  x  x            xk   

  

l   x c    x x  c cd  x      

x sequence action argument variables
construct set depth bounded candidate literals

b    a x    nil          initialize beam single rule literals
loop
g   bi   r rd l   r   add literal r    l   r  bi    l l  
bi beam select g  b  d     

select best b heuristic values

    
bi    bi     

loop improvement heuristic

return argmaxrbi hvalue r  d    

return best rule final beam

figure    pseudo code learning decision list hd l given training data d 
procedure add literal r  l  simply returns rule literal l added end
rule r  procedure beam select g  w  d  selects best b rules g different
heuristic values  procedure hvalue r  d  returns heuristic value rule r relative
training data described text 

q values  motivation  describe cost sensitive decision list learner
sensitive full set q values d  learning goal roughly find decision
list selects actions large cumulative q value training set 
learning list rules  say decision list l covers training example
hs   s   q s  a             q s   i l suggests action state s  given set training
examples d  search decision list selects actions high q value via
iterative set covering approach carried learn decision list  decision list rules
  

fiapi policy language bias

constructed one time order list covers training examples 
pseudo code algorithm given figure    initially  decision list null list
cover training examples  iteration  search high quality
rule r quality measured relative set currently uncovered training examples 
selected rule appended current decision list  training examples newly
covered selected rule removed training set  process repeats
list covers training examples  success approach depends heavily
function learn rule  selects good rule relative uncovered training
examplestypically good rule one selects actions best  or close best 
q value covers significant number examples 
learning individual rules  input rule learner learn rule set
training examples  along depth length parameters l  beam width b 
action type a  rule learner calls routine beam search find good rule
ra rd l action type a  learn rule returns rule ra highest value
measured heuristic  described later section 
given action type a  procedure beam search generates beam b    b        
bi set rules rd l action type a  sets evolve specializing
rules previous sets adding literals them  guided heuristic function  search
begins general rule a x    nil  allows action type state 
search iteration produces set bi contains b rules highest different heuristic
values among following set 
g   bi   r rd l   r   add literal r    l   r  bi    l l 
l set possible literals depth less  set includes
current best rules  those bi    rule rd l formed adding
new literal rule bi    search ends improvement heuristic value
occurs  bi   bi    beam search returns best rule bi according
heuristic 
heuristic function  training instance hs   s   q s  a             q s   i  define q advantage taking action ai instead  s  state  s  ai     q s  ai  
q s   s    likewise  q advantage rule r sum q advantages actions
allowed r s  given rule r set training examples d  heuristic function
hvalue r  d  equal number training examples rule covers plus
cumulative q advantage rule training examples   using q advantage rather
q value focuses learner toward instances large improvement previous policy possible  naturally  one could consider using different weights coverage
q advantage terms  possibly tuning weight automatically using validation data 
   since many rules rd l equivalent  must prevent beam filling semantically
equivalent rules  rather deal problem via expensive equivalence testing take ad hoc 
practically effective approach  assume rules coincidentally heuristic
value  ones must equivalent  thus  construct beams whose members
different heuristic values  choose rules value preferring shorter rules 
arbitrarily 
   coverage term included  covering zero q advantage example
covering it  zero q advantage good  e g   previous policy optimal state  

  

fifern  yoon    givan

   random walk bootstrapping
two issues critical success api technique  first  api
fundamentally limited expressiveness policy language strength
learner  dictates ability capture improved policy described training
data iteration  second  api yield improvement improved trajectories
successfully generates training data describes improved policy  large classical
planning domains  initializing api uninformed random policy typically result
essentially random training data  helpful policy improvement 
example  consider mdp corresponding    block blocks world initial
problem distribution generates random initial goal states  case  random
policy unlikely reach goal state within practical horizon time  hence 
rollout trajectories unlikely reach goal  providing guidance toward learning
improved policy  i e   policy reliably reach goal  
interested solving large domains this  providing guiding inputs
api critical  fern  yoon  givan         showed bootstrapping api
domain independent heuristic planner  hoffmann   nebel         api
able uncover good policies blocks world  simplified logistics world  no planes  
stochastic variants  approach  however  limited heuristics ability
provide useful guidance  vary widely across domains 
describe new bootstrapping procedure goal based planning domains  based
random walks  guiding api toward good policies  planning system 
evaluated section    based integrating procedure api order find
policies goal based planning domains  non goal based mdps  bootstrapping
procedure directly applied  bootstrapping mechanisms must used
necessary  might include providing initial non trivial policy  providing heuristic
function  form reward shaping  mataric         below  first describe
idea random walk distributions  next  describe use distributions
context bootstrapping api  giving new algorithm lrw api 
    random walk distributions
throughout consider mdp   hs  a  t  r  ii correspond goal based planning domains  described section      recall state corresponds
planning problem  specifying world state  via world facts  set goal conditions  via
goal facts   use terms mdp state planning problem interchangeably 
note that  context  distribution planning problems  convenience
denote mdp states tuples    w  g   w g sets world facts
goal facts respectively 
given mdp state    w  g  set goal predicates g  define s g
mdp state  w  g     g   contains goal facts g applications predicate
g  given set goal predicates g  define n step random walk problem
distribution rw n  m  g  following stochastic algorithm 
   draw random state s     w    g    initial state distribution i 
  

fiapi policy language bias

   starting s  take n uniformly random actions     giving state sequence  s            sn   
sn    wn   g     recall actions change goal facts   uniformly
random action selection  assume extra no op action  that change
state  selected fixed probability  reasons explained below 
   let g set goal facts corresponding world facts wn   e g  
wn    on a  b   clear a    g    gon a  b   gclear a    return planning
problem  mdp state   s    g  g output 
sometimes abbreviate rw n  m  g  rw n g clear context 
intuitively  perform well distribution policy must able achieve facts
involving goal predicates typically result n step random walk
initial state  restricting set goal predicates g specify types facts
interested achievinge g   blocks world may interested
achieving facts involving predicate 
random walk distributions provide natural way span range problem difficulties  since longer random walks tend take us initial state  small
n typically expect planning problems generated rw n become
difficult n grows  however  n becomes large  problems generated require far
fewer n steps solvei e   direct paths initial state
end state long random walk  eventually  since finite  problem difficulty
stop increasing n 
question raised idea whether  large n  good performance rw n
ensures good performance problem distributions interest domain 
domains  simple blocks world     good random walk performance
seem yield good performance distributions interest  domains 
grid world  with keys locked doors   intuitively  random walk unlikely
uncover problem requires unlocking sequence doors  indeed  since rw n
insensitive goal distribution underlying planning domain  random walk
distribution may quite different 
believe good performance long random walks often useful 
addressing one component difficulty many planning benchmarks  successfully
address problems components difficulty  planner need deploy orthogonal technology landmark extraction setting subgoals  hoffman  porteous   
sebastia         example  grid world  could automatically set subgoal
possessing key first door  long random walk policy could provide useful
macro getting key 
purpose developing bootstrapping technique api  limit focus
finding good policies long random walks  experiments  define long
specifying large walk length n   theoretically  inclusion no op action
definition rw ensures induced random walk markov chain   aperiodic 
    practice  select random actions set applicable actions state si   provided
simulator makes possible identify set 
    blocks world large n  rw n generates various pairs random block configurations  typically
pairing states far apartclearly  policy performs well distribution captured
significant information blocks world 
    dont formalize chain here  various formalizations work well 

  

fifern  yoon    givan

thus distribution states reached increasingly long random walks converges
stationary distribution     thus rw   limn rw n well defined  take
good performance rw goal 
    random walk bootstrapping
mdp   define  i     mdp identical initial state
distribution replaced     define success ratio sr    i    i 
probability solves problem drawn i  treating random variable 
average length al    i    i  conditional expectation solution
length problems drawn given solves i  typically solution length
problem taken number actions  however  action costs uniform 
length taken sum action costs  note mdp formulation
classical planning domains  given section      policy achieves high v   
high success ratio low average cost 
given mdp set goal predicates g  system attempts find good
policy  rw n    n selected large enough adequately approximate
rw   still allowing tractable completion learning  naively  given initial
random policy     could try apply api directly  however  already discussed 
work general  since interested planning domains rw produces
extremely large difficult problems random policies provide ineffective starting
point 
however  small n  e g   n       rw n typically generates easy problems 
likely api  starting even random initial policy  reliably find good
policy rw n   furthermore  expect policy n performs well rw n  
provide reasonably good  perhaps perfect  guidance problems drawn
rw moderately larger n  thus  expect able find
good policy rw bootstrapping api initial policy n   suggests natural
iterative bootstrapping technique find good policy large n  in particular  n   n   
figure   gives pseudo code procedure lrw api integrates api
random walk bootstrapping find policy long random walk problem distribution 
intuitively  algorithm viewed iterating two stages  first  finding
hard enough distribution current policy  by increasing n   and  then  finding good
policy hard distribution using api  algorithm maintains current policy
current walk length n  initially n       long success ratio rwn
success threshold   constant close one  simply iterate steps
approximate policy improvement  achieve success ratio policy  
if statement increases n success ratio rw n falls   is 
performs well enough current n step distribution move distribution
slightly harder  constant determines much harder set small enough
likely used bootstrap policy improvement harder distribution 
 the simpler method increasing n   whenever success ratio achieved
    markov chain may irreducible  stationary distribution may reached
initial states  however  considering one initial state  described i 

  

fiapi policy language bias

lrw api  n  g  n  w  h  m       
   max random walk length n   goal predicates g
   training set size n  sampling width w  horizon h 
   mdp   initial policy     discount factor  
    n   
loop
c  n   
sr

   find harder n step distribution  
c  i      n none 
n least  n  n   s t  sr
     rw n  m  g   
improved trajectories n  w  h        
learn policy t   
satisfied
return  

c  n  estimates success ratio planning
figure    pseudo code lrw api  sr
domain problems drawn rw n  m  g  drawing set problems returning
fraction solved   constants described text 

find good policies whenever method does  take much longer  may run
api repeatedly training set already good policy  
n becomes equal maximum walk length n   n   n future
iterations  important note even find policy good success ratio
rw n may still possible improve average length policy  thus 
continue api distribution satisfied success ratio
average length current policy 

   relational planning experiments
section  evaluate lrw api technique relational mdps corresponding
deterministic stochastic classical planning domains  first give results number
deterministic benchmark domains  showing promising results comparison stateof the art planner  hoffmann   nebel         highlighting limitations
approach  next  give results several stochastic planning domains including
domain specific track      international probabilistic planning competition
 ippc   domain definitions problem generators used experiments
available upon request 
experiments  use policy learner described section     learn
taxonomic decision list policies  cases  number training trajectories     
policies restricted rules depth bound length bound l  discount
  

fifern  yoon    givan

factor always one  lrw api always initialized policy selects
random actions  utilize maximum walk length parameter n           set
equal         respectively 
    deterministic planning experiments
perform experiments seven familiar strips planning domains including used
aips      planning competition  used evaluate tl plan bacchus
kabanza         gripper domain  domain standard problem generator
accepts parameters  control size difficulty randomly generated
problems  list domain parameters associated them  detailed
description domains found hoffmann nebel        
blocks world  n    standard blocks worlds n blocks 
freecell  s  c  f  l    version solitaire suits  c cards per suit  f freecells 
l columns 
logistics  a c l p    logistics transportation domain airplanes  c cities  l
locations  p packages 
schedule  p    job shop scheduling domain p parts 
elevator  f  p    elevator scheduling f floors p people 
gripper  b    robotic gripper domain b balls 
briefcase  i    transportation domain items 
lrw experiments  first set experiments evaluates ability lrw api
find good policies rw   utilize sampling width one rollout  since
deterministic domains  recall iteration lrw api compute
 approximately  improved policy may increase walk length n find harder
problem distribution  continued iterating lrw api observed
improvement  training time per iteration approximately five hours    though
initial training period significant  policy learned used solve new
problems quickly  terminating seconds solution one found  even
large problems 

figure   provides data iteration lrw api seven domains
indicated parameter settings  first column  domain  indicates
iteration number  e g   blocks world run   iterations   second column
records walk length n used learning corresponding iteration  third
fourth columns record sr al policy learned corresponding iteration
    timing information relatively unoptimized scheme implementation  reimplementation
c would likely result      fold speed up 

  

fin

rw n
sr
al

iter   

iter   

api policy language bias

rw
sr
al

n

blocks world     
 
 
 
 
 
 
 
 

 
  
  
  
  
  
   
   

    
    
    
    
    
    
    
    


   
   
    
    
    
    
    
    

 
    
    
    
    
    
    
 
    

 
 
  
  
  
  
  
  
  

    
    
    
    
    
    
    
    
    


   
   
   
   
   
   
   
   
   

    
    
    
    
    
    
    
    
    
 

rw
sr
al

logistics          
 
    
    
    
    
    
    
    
    

 
 
 
 
 
 
 
 
 
  

  
  
  

freecell          
 
 
 
 
 
 
 
 
 

rw n
sr
al

   
   
   
   
   
   
   
   
   
   

 
  
  
  
  
  
  
  
  
  

  
  
  

    
    
    
    
    
    
    
    
    
    

    
    
    


   
   
   
   
   
   
   
   
   
   

   
   
   

    
    
    
    
    
    
    
    
    
    

    
    
    
 

    
   
   
   
   
   
   
    
   
   

   
   
   
  

    
 
 

  
  
  

 
   
 
 

 
  
  
  

schedule     
 
 

 
 

    
 


 
    

briefcase     
elevator        
 

  

 

   



 
 

  
  

 
 

  
  

 
 
 

 
  
  

    
    
 


   
   
   

gripper     
 

  

 


   

figure    results iteration lrw api seven deterministic planning domains 
iteration  show walk length n used learning  along success ratio
 sr  average length  al  learned policy rw n rw   final
policy shown domain performs       sr walks length n          
 with exception logistics   iteration improve performance 
benchmark show sr al planner problems drawn
rw  

measured     problems drawn rw n corresponding value n  i e  
distribution used learning   sr exceeds   next iteration seeks
increased walk length n  fifth sixth columns record sr al
  

fifern  yoon    givan

policy  measured     problems drawn lrw target distribution rw  
experiments approximated rw n n           
so  example  see blocks world total   iterations 
learn first one iteration n      one iteration n       four iterations
n       two iterations n        point see resulting
policy performs well rw   iterations n   n   shown  showed
improvement policy found iteration eight  domains  observed
improvement iterating n   n   thus show iterations 
note domains except logistics  see below  achieve policies good performance
rw n learning much shorter rw n distributions  indicating indeed
selected large enough value n capture rw   desired 
general observations  several domains  learner bootstraps quickly
short random walk problems  finding policy works well even much longer
random walk problems  include schedule  briefcase  gripper  elevator  typically  large problems domains many somewhat independent subproblems
short solutions  short random walks generate instances different typical
subproblems  domains  best lrw policy found small number
iterations performs comparably rw   note considered
good domain independent planner domains  consider successful
result 
two domains  logistics   freecell  planner unable find policy
success ratio one rw   believe result limited knowledge representation allowed policies following reasons  first  cannot write good
policies domains within current policy language  example  logistics  one
important concept set containing packages trucks truck
packages goal city  however  domain defined way concept
cannot expressed within language used experiments  second  final learned
decision lists logistics freecell  appendix b  contain much larger
number specific rules lists learned domains  indicates
learner difficulty finding general rules  within language restrictions 
applicable large portions training data  resulting poor generalization  third 
success ratio  not shown  sampling based rollout policy  i e   improved policy
simulated improved trajectories  substantially higher resulting
learned policy becomes policy next iteration  indicates learndecision list learning much weaker policy sampling based policy generating
training data  indicating weakness either policy language learning algorithm  example  logistics domain  iteration eight  training data learning
iteration nine policy generated sampling rollout policy achieves success ratio
         training problems drawn rw    distribution  learned
iteration nine policy achieves success ratio       shown figure iteration
nine  extending policy language incorporate expressiveness appears
required domains require sophisticated learning algorithm 
point future work 
    logistics  planner generates long sequence policies similar  oscillating success ratio
elided table ellipsis space reasons 

   

fiapi policy language bias


domain
blocks


sr al
    
  
        

size
    
    

sr
 
 

al
  
   

freecell

         
          

    
 

  


 
    

  
   

logistics

         
           

    
 

 


 
 

 
   

elevator

       

 

   

 

  

schedule

    

 

   

 

   

briefcase

    
    

 
 

  
   

 
 

  


gripper

    

 

   

 

   

figure    results standard problem distributions seven benchmarks  success ratio
 sr  average length  al  provided policy learned lrw
problem distribution  given domain  learned lrw policy used
problem size shown 

remaining domain  blocks world  bootstrapping provided increasingly
long random walks appears particularly useful  policies learned walk
lengths                increasingly effective target lrw distribution rw  
walks length         takes multiple iterations master provided level
difficulty beyond previous walk length  finally  upon mastering walk length     
resulting policy appears perform well walk length  learned policy modestly
superior rw success ratio average length 
evaluation original problem distributions  domain denote
best learned lrw policyi e   policy  domain  highest
performance rw   shown figure    taxonomic decision lists corresponding
domain given appendix b  figure   shows performance  
comparison ff  original intended problem distributions domains 
measured success ratio systems giving time limit     seconds solve
problem  attempted select largest problem sizes previously used
evaluation domain specific planners  either aips      bacchus kabanza
        well show smaller problem size cases one planners
show performed poorly large size  case  use problem generators
provided domains  evaluate     problems size 
overall  results indicate learned  reactive policies competitive
domain independent planner ff  important remember policies
learned domain independent fashion  thus lrw api viewed general
approach generating domain specific reactive planners  two domains  blocks world
   

fifern  yoon    givan

briefcase  learned policies substantially outperform success ratio  especially
large domain sizes  three domains  elevator  schedule  gripper  two approaches perform quite similarly success ratio  approach superior average
length schedule superior average length elevator 
two domains  logistics freecell  substantially outperforms learned policies success ratio  believe partly due inadequate policy language 
discussed above  believe  however  another reason poor performance
long random walk distribution rw correspond well standard
problem distributions  seems particularly true freecell  policy learned
freecell           achieved success ratio    percent rw   however  standard distribution achieved    percent  suggests rw generates problems
significantly easier standard distribution  supported fact
solutions produced standard distribution average twice long
produced rw   one likely reason easy random walks
end dead states freecell  actions applicable  thus random walk
distribution typically produce many problems goals correspond dead
states  standard distribution hand treat dead states goals 
    probabilistic planning experiments
present experiments three probabilistic domains described probabilistic planning domain language ppddl  younes        
ground logistics  c  p    probabilistic version logistics airplanes  c
cities p packages  driving action probability failure domain 
colored blocks world  n    probabilistic blocks world n colored blocks 
goals involve constructing towers certain color patterns  probability
moved blocks fall floor 
boxworld  c  p    probabilistic version full logistics c cities p packages 
transportation actions probability going wrong direction 
ground logistics domain originally boutilier et al          used
evaluation yoon et al          colored blocks world boxworld domains
domains used hand tailored track ippc lrw api technique
entered  hand tailored track  participants provided problem generators
domain competition allowed incorporate domain knowledge
planner use competition time  provided problem generators lrw api
learned policies domains  entered competition 
conducted experiments probabilistic domains yoon et al 
        including variants blocks world variant ground logistics 
appeared fern et al          however  show results since
qualitatively identical deterministic blocks world results described
ground logistics results show below 
three probabilistic domains  conducted lrw experiments using
procedure above  parameters given lrw api except
   

fin

sr

rw n
al

iter   

iter   

api policy language bias

rw
sr
al

boxworld       
 
       
   
 
       
   
 
       
   
 
       
   
          
    
    
          
          
    
    
          
          
    
standard distribution        

n

sr

rw n
al

sr

rw
al

ground logistics          
    
    
    
    
    
    
    
    
    
 

    
    
    
    
    
    
  
    
    


 
      
 
       
     
 
standard distribution

    
    
    
          

    
    
 
 

     
    
   
  

colored blocks world     
 
 
 
 
 

      
   
      
   
       
    
        
    
        
    
standard distribution     

    
    
    
    
    
    

    
    
    
    
    
   

figure    results iteration lrw api three probabilistic planning domains 
iteration  show walk length n used learning  along success ratio
 sr  average length  al  learned policy rw n rw  
benchmark  show performance standard problem distribution policy whose
performance best rw  
sampling width used rollout set w       set      order
account stochasticity domains  results experiments shown
figure    tables form figure   last row given
domain gives performance standard distribution  i e   problems drawn
domains problem generator  colored blocks world problem generator
produces problems whose goals specified using existential quantifiers  example 
simple goal may exists blocks x x red  blue x y 
since policy language cannot directly handle existentially quantified goals preprocess
planning problems produced problem generator remove them  done
assigning particular block names existential variables  ensuring static
properties block  in case color  satisfied static properties variable
assigned to  domain  finding assignment trivial  resulting
assignment taken goal  giving planning problem learned policy
applied  since blocks world states fully connected  resulting goal always
guaranteed achievable 

boxworld  lrw api able find good policy rw standard
distribution  again  deterministic logistics freecell  believe
primarily restricted policy languages currently used learner 
here  domains  see decision list learned boxworld contains many
specific rules  indicating learner able generalize well beyond
   

fifern  yoon    givan

training trajectories  ground logistics  see lrw api quickly finds good
policy rw standard distribution 
colored blocks world  see lrw api able quickly find good
policy rw standard distribution  however  unlike deterministic
 uncolored  blocks world  success ratio observed less one  solving   
percent problems  unclear  lrw api able find perfect policy 
relatively easy hand code policy colored blocks world using language
learner  hence inadequate knowledge representation answer  predicates
action types domain deterministic counterpart
stochastic variants previously considered  difference apparently
interacts badly learners search bias  causing fail find perfect policy 
nevertheless  two results  along probabilistic planning results shown
here  indicate good policy expressible language  lrw api
find good policies complex relational mdps  makes lrw api one
techniques simultaneously cope complexity resulting stochasticity
relational structure domains these 

   related work
boutilier et al         presented first exact solution technique relational mdps
based structured dynamic programming  however  practical implementation
approach provided  primarily due need simplification first order
logic formulas  ideas  however  served basis logic programming based
system  kersting  van otterlo    deraedt        successfully applied blocksworld problems involving simple goals simplified logistics world  style approach
inherently limited domains exact value functions and or policies
compactly represented chosen knowledge representation  unfortunately 
generally case types domains consider here  particularly planning
horizon grows  nevertheless  providing techniques directly reason
mdp model important direction  note api approach essentially ignores
underlying mdp model  simply interacts mdp simulator black box 
interesting research direction consider principled approximations techniques discover good policies difficult domains  considered
guestrin et al       a   class based mdp value function representation
used compute approximate value function could generalize across different sets
objects  promising empirical results shown multi agent tactical battle domain 
presently class based representation support representation features commonly found classical planning domains  e g   relational facts
on a  b  change time   thus directly applicable contexts  however  extending work richer representations interesting direction  ability
reason globally domain may give advantages compared api 
approach closely related work relational reinforcement learning  rrl   dzeroski et al          form online api learns relational value function approximations  q value functions learned form relational decision trees  q trees 
used learn corresponding policies  p  trees   rrl results clearly demonstrate
   

fiapi policy language bias

difficulty learning value function approximations relational domains  compared p trees  q trees tend generalize poorly much larger  rrl yet demonstrated
scalability problems complex considered hereprevious rrl blocks world
experiments include relatively simple goals     lead value functions much
less complex ones here  reason  suspect rrl would difficulty
domains consider  precisely value function approximation step
avoid  however  needs experimentally tested 
note  however  api approach advantage using unconstrained
simulator  whereas rrl learns irreversible world experience  pure rl   using
simulator  able estimate q values actions training state 
providing us rich training data  without simulator  rrl able directly
estimate q value action training statethus  rrl learns q tree
provide estimates q value information needed learn p  tree  way  valuefunction learning serves critical role simulator unavailable  believe 
many relational planning problems  possible learn model simulator
world experiencein case  api approach incorporated planning
component rrl  otherwise  finding ways either avoid learning effectively
learn relational value functions rrl interesting research direction 
researchers classical planning long studied techniques learning improve
planning performance  collection survey work learning planning domains see minton        zimmerman kambhampati         two primary approaches learn domain specific control rules guiding search based planners e g  
minton  carbonell  knoblock  kuokka  etzioni  gil         veloso  carbonell  perez 
borrajo  fink  blythe         estlin mooney         huang  selman  kautz
        ambite  knoblock  minton         aler  borrajo  isasi         and 
closely related  learn domain specific reactive control policies  khardon      a  martin
  geffner        yoon et al         
regarding latter  work novel using api iteratively improve stand alone
control policies  regarding former  theory  search based planners iteratively
improved continually adding newly learned control knowledgehowever  difficult avoid utility problem  minton         i e   swamped low utility rules 
critically  policy language bias confronts issue preferring simpler policies 
learning approach tied base planner  let alone tied single particular base planner   unlike previous work  rather  require domain simulator 
ultimate goal systems allow planning large  difficult problems
beyond reach domain independent planning technology  clearly  learning
achieve goal requires form bootstrapping almost previous systems
relied human purpose  far  common human bootstrapping
approach learning small problems  here  human provides small problem
distribution learner  limiting number objects  e g   using     blocks
blocks world   control knowledge learned small problems  approach
work  human must ensure small distribution good control knowledge
small problems good large target distribution  contrast  long    complex blocks world goal rrl achieve on a  b  n block environment 
consider blocks world goals involve n blocks 

   

fifern  yoon    givan

random walk bootstrapping approach applied without human assistance directly
large planning domains  however  already pointed out  goal performing well
lrw distribution may always correspond well particular target problem
distribution 
bootstrapping approach similar spirit bootstrapping framework learning exercises natarajan        reddy   tadepalli         here  learner provided planning problems  exercises  order increasing difficulty  learning
easier problems  learner able use new knowledge  skills  order bootstrap learning harder problems  work  however  previously relied
human provide exercises  typically requires insight planning domain
underlying form control knowledge planner  work viewed
automatic instantiation learning exercises  specifically designed learning lrw
policies 
random walk bootstrapping similar approach used micro hillary
 finkelstein   markovitch         macro learning system problem solving 
work  instead generating problems via random walks starting initial state  random
walks generated backward goal states  approach assumes actions
invertible given set backward actions  assumptions hold 
backward random walk approach may preferable provided goal
distribution match well goals generated forward random walks 
course  cases forward random walks may preferable  micro hillary
empirically tested n n sliding puzzle domain  however  discussed work 
remain challenges applying system complex domains parameterized actions recursive structure  familiar strips domains  best
knowledge  idea learning random walks previously explored
context strips planning domains 
idea searching good policy directly policy space rather value function
space primary motivation policy gradient rl algorithms  however  algorithms
largely explored context parametric policy spaces  approach
demonstrated impressive success number domains  appears difficult define
policy spaces types planning problem considered here 
api approach viewed type reduction planning reinforcement
learning classification learning  is  solve mdp generating solving
series cost sensitive classification problems  recently  several
proposals reducing reinforcement learning classification  dietterich wang       
proposed reinforcement learning approach based batch value function approximation 
one proposed approximations enforced learned approximation assign
best action highest value  type classifier learning  lagoudakis parr
       proposed classification based api approach closely related ours  primary difference form classification problem produced iteration 
generate standard multi class classification problems  whereas generate cost sensitive
problems  bagnell  kakade  ng  schneider        introduced closely related algorithm learning non stationary policies reinforcement learning  specified horizon
time h  approach learns sequence h policies  iteration  policies
held fixed except one  optimized forming classification problem via policy
   

fiapi policy language bias

rollout     finally  langford zadrozny        provide formal reduction reinforcement learning classification  showing  accurate classification learning implies
near optimal reinforcement learning  approach uses optimistic variant sparse
sampling generate h classification problems  one horizon time step 

   summary future work
introduced new variant api learns policies directly  without representing
approximate value functions  allowed us utilize relational policy language
learning compact policy representations  introduced new api bootstrapping
technique goal based planning domains  experiments show lrw api
algorithm  combines techniques  able find good policies variety
relational mdps corresponding classical planning domains stochastic variants 
know previous mdp technique successfully applied problems
these 
experiments pointed number weaknesses current approach  first 
bootstrapping technique  based long random walks  always correspond
well problem distribution interest  investigating automatic bootstrapping
techniques interesting direction  related general problems exploration
reward shaping reinforcement learning  second  seen limitations
current policy language learner partly responsible failures
system  cases  must either     depend human provide useful features
system     extend policy language develop advanced learning techniques  policy language extensions considering include various extensions
knowledge representation used represent sets objects domain  in particular 
route finding maps grids   well non reactive policies incorporate search
decision making 
consider ever complex planning domains  inevitable brute force
enumeration approach learning policies trajectories scale  presently
policy learner  well entire api technique  makes attempt use definition
domain one available  believe developing learner exploit
information bias search good policies important direction future work 
recently  gretton thiebaux        taken step direction using logical
regression  based domain model  generate candidate rules learner  developing tractable variations approach promising research direction  addition 
exploring ways incorporating domain model approach modelblind approaches critical  ultimately  scalable ai planning systems need combine
experience stronger forms explicit reasoning 

    initial state distribution dictated policies previous time steps  held fixed 
likewise actions selected along rollout trajectories dictated policies future time steps 
held fixed 

   

fifern  yoon    givan

acknowledgments
would thank lin zhu originally suggesting idea using random walks
bootstrapping  would thank reviewers editors helping
vastly improve paper  work supported part nsf grants         iis
        iis 

appendix a  omitted proofs
proposition    let h finite class deterministic policies  h 

set n     ln  h 
trajectories drawn independently dh     probability
every h consistent trajectories satisfies v    v     vmax     h   
proof  first introduce basic properties notation used below 
deterministic policy   consistent trajectory t  dh  t  entirely
determined underlying mdp transition dynamics  implies two deterministic policies   consistent trajectory dh  t    dh  t  
denote v t  cumulative discounted reward accumulated executing trajectory
p
t  policy   v h      dh  t  v t  summation taken
length h trajectories  or simply consistent    finally set
p
trajectories let dh      t  dh  t  giving cumulative probability
generating trajectories  
consider particular h h consistent n trajectories
  let denote set length h trajectories consistent
denote set trajectories consistent   following khardon      b 
first give standard argument showing high probability dh          see
consider probability consistent n     ln  h 
trajectories


given dh        probability occurs     n   en    h 
 

thus probability choosing  h   h 
    thus  probability

least   know dh          note dh      dh    
given condition dh        show v h    v h     vmax
considering difference two value functions 
v h    v h     

x

dh  t  v t 



 

x

 

v t   

x

 dh  t  dh  t   v t 



dh  t 





dh  t  v t 



dh  t 



x

x

v t     



dh  t 



    dh     

dh        dh    

vmax  dh  

  vmax   

x

 vmax
   

x

v t 

dh  t  v t 

fiapi policy language bias

third lines follows since dh  t    dh  t  consistent t 
last line follows substituting assumption dh      dh        previous
line  combining result approximation due using finite horizon 
v    v    v h    v h        h vmax
get probability least     v    v     vmax     h    completes
proof   
proposition   


mdp q advantage least             

h   log


 vmax

 vmax



 



w  
 

 

ln

 a 
 

state s  a   s     s  probability least      
proof  given real valued random variable x bounded absolute value xmax
average x w independently drawn samples x 
q additive chernoff bound states
probability least      e x  x  xmax wln  
note qh  s  a  expectation random variable x s  a    r s  a   

vh   t  s  a   q s  a  simply average w independent samples x s  a  
 
chernoff bound tells us probability least    a 
   qh  s  a  q s  a  
q

 


vmax ln  a ln
   a  number actions  substituting choice w
w

get probability least        qh  s  a  q s  a       satisfied actions
simultaneously  know  q  s  a  qh  s  a   h vmax   choice

h gives   q  s  a  qh  s  a         combining relationships get

probability least        q  s  a  q s  a       holds actions simultaneously 
use bound show high probability q value estimates

actions  s  within   range other  actions outside
range  particular  consider action  s  action a   
a   s  q  s  a    q  s  a     bound get

 q s  a  q s  a           otherwise a     s  assumption mdp
q advantage get q  s  a  q  s  a      using bound implies

q s  a  q s  a          relationships definition a   s  imply
probability least     a   s     s    

appendix b  learned policies
give final taxonomic decision list policies learned domain
experiments  rather write rules form a x            xk     l  l  lm
   

fifern  yoon    givan

drop variables head simply write    l  l  lm   addition
use notation r short hand  r    r relation  interpreting policies  important remember rule action type a 
preconditions action type implicitly included constraints  thus  rules
often allow actions legal  actions never considered
system 
gripper
   move   x   not  gat  carry  gripper      x   not  gat  at  at robby      x   gat  not
 cat  room      x   cat ball  
   drop   x   gat  at robby  
   pick   x   gat   gat  carry  gripper      x   gat   not at robby   
   pick   x   at  not  gat  room      x   gat   not at robby   
   pick   x   gat   not at robby   
briefcase
   put in   x   gat   not is at   
   move   x   at  not  cat  location      x   not  at  gat  cis at    
   move   x   gat in    x   not  cat in   
   take out   x   cat  is at  
   move   x  gis at 
   move   x   at  gat  cis at   
   put in   x  universal 
schedule
   do immersion paint   x   not  painted  x       x   gpainted  x    
   do drill press   x   ghas holeo  x      x   ghas holew  x    
   do lathe   x   not  shape  cylindrical     x   gshape  cylindrical  
   do drill press   x   ghas holew  x    
   do drill press   x   ghas holeo  x    
   do grind   x   not  surface condition  smooth     x   gsurface condition  smooth  
   do polish   x   not  surface condition  polished     x   gsurface condition  polished  
   do time step 
elevator
   depart   x  gserved 
   down   x   destin boarded    x   destin gserved  
   up   x   destin boarded    x   destin gserved    x   above  origin boarded     x   not
 destin boarded   
   board   x   not cserved    x  gserved 
   up   x   origin gserved    x   not  destin boarded     x   not  destin gserved     x 
 origin  not cserved     x   above  destin passenger     x   not  destin boarded   
   down   x   origin gserved    x   origin  not cserved     x   not  destin boarded   

   

fiapi policy language bias

   up   x   not  origin boarded     x   not  destin boarded   
freecell
   sendtohome   x   canstack   canstack  suit   suit incell       x   not ghome  
   move b   x   not  canstack  on ghome      x   canstack ghome    x   value   not
colspace     x   canstack   suit   suit bottomcol    
   move   x   canstack   on  canstack   on  ghome       x   canstack  on  suit   suit bottomcol       x   on  bottomcol    x   canstack   on ghome     x   on   canstack 
 on   not  canstack  value  cellspace         x   not  canstack   suit   suit incell     
 x   canstack bottomcol    x   suit   suit  on   not  canstack  value  cellspace       
 x   value   not colspace        on   not  canstack   suit   suit incell        x   not
 canstack  chome   
   sendtohome b   x   not ghome  
   sendtohome   x   on   canstack  canstack   suit   suit incell        x   not ghome  
   sendtohome   x   on   on  ghome     x   canstack   not ghome     x   canstack   not
 on  ghome      x   not ghome  
   move b   x   not  canstack  ghome     x   value   not colspace     x   canstack 
 suit   suit bottomcol    
   sendtofree   x   on   on  ghome     x   not ghome  
   sendtohome   x   canstack   canstack  on ghome      x   not ghome  
    sendtohome     ghome   x   value   not colspace     x   not  canstack   on   not
ghome       x   on   not  on  ghome      x   not ghome  
    newcolfromfreecell   x  ghome 
    sendtohome   x   canstack   on ghome     x  ghome   x   not ghome  
    move b   x   value   value home     x   value   not colspace     x   canstack   suit 
 suit bottomcol    
    sendtohome   x   canstack   on   canstack   suit   suit incell        x   not ghome  
    sendtohome   x   on   on   canstack   on   not ghome        x   not ghome  
    sendtofree   x   canstack   on  on  ghome      x   suit   suit bottomcol     x   on 
bottomcol  
    move   x   on   canstack  clear     x   on   canstack  on   not  canstack  value 
cellspace         x   not ghome    x  ghome   x   canstack bottomcol    x   on 
 canstack   on   not  canstack  value  cellspace         x   not  canstack   suit 
 suit incell       x   on  bottomcol    x   suit   suit  on   not  canstack  value 
cellspace         x   value   not colspace     x   on   not  canstack   suit   suit
incell        x   not  canstack  chome   
    move   x   suit   suit chome     x   not ghome    x   not  on  ghome     x   on 
 canstack  bottomcol   
    sendtohome   x   canstack  on  canstack  on ghome       x  ghome   x   not ghome  
    sendtohome   x   canstack   on  canstack   on  ghome       x   not  suit   suit bottomcol      x   not ghome  
    sendtofree   x   canstack  on  canstack  value  cellspace       x   canstack chome  
    sendtohome   x   canstack   suit   suit incell      x   on   not  canstack  value 
cellspace       x   not ghome  
    sendtonewcol   x   canstack  canstack   on  ghome    
    sendtofree   x   canstack  on   canstack   on  ghome       x   not  canstack ghome   
 x   not  on  ghome     x   on   not  canstack   suit   suit incell      

   

fifern  yoon    givan

    sendtofree   x   on   canstack  canstack   on  ghome       x   not  canstack bottomcol     x   not  canstack   canstack  on ghome     
    sendtofree   x   canstack  on   canstack   on   not ghome        x   not  canstack
ghome     x   canstack  not  suit   suit bottomcol     
    sendtohome   x   canstack   canstack  on  ghome      x   on   canstack   on   not
ghome       x   not ghome    x   not ghome  
    sendtofree   x   canstack  on   canstack   on   not ghome        x   canstack  canstack 
 on  ghome      x   not ghome    x   on   canstack   on   not  canstack  value 
cellspace       
    sendtofree   x   canstack chome    x   suit   suit  canstack   on  ghome     
    sendtohome   x  ghome   x   suit   suit bottomcol     x   canstack   not  on 
ghome      x   not ghome  
    sendtofree   x   canstack   on  ghome     x   canstack   on   not ghome    
    sendtofree   x   canstack  on  ghome     x   not ghome    x   on   canstack   on 
 not ghome     
    sendtohome   x   on   canstack  bottomcol     x   canstack   not ghome     x 
 not ghome  
    sendtofree   x   canstack  on  canstack   on   not ghome        x   not  suit   suit
bottomcol      x   not ghome  
    sendtohome   x   not  canstack  ghome     x   not  suit   suit bottomcol      x 
 not ghome  
    sendtofree   x   not  on  ghome     x   canstack  canstack   on   not ghome     
    sendtofree b   x   not ghome  
    sendtofree   x  universal 
logistics
   fly airplane   x   in  gat  airport     x   not  in  gat   at airplane       x   not  gat
 in  truck      x   not  in  gat   not airport     
   load truck   x   in  not  gat   not airport       x   gat   gat  in  truck      x   not
 cat  location   
   drive truck   x   at  at   gat  in  truck       x   in city   in city  at airplane      x 
 at   not  gat  in  truck     
   unload truck   x   gat   at  in obj      x   gat   at obj     x   not  gat   at airplane      x   at   gat  in  truck      x   gat   at truck   
   fly airplane   x   gat  in  airplane     x   in  not  gat   at truck       x   at   not
 gat  in  truck     
   unload airplane   x   not  in  gat   not airport       x   gat   at airplane   
   load truck   x   in  not  gat  location      x   not  gat   at truck      x   gat 
location  
   unload truck   x   gat   at truck     x   at  airport    x   not  in  gat   not airport       x   gat   at airplane   
   fly airplane   x   at  at   gat  in  truck       x   at   gat  gat  location      x 
 not  at   cat obj    
    drive truck   x   in  gat  location     x   at   not  gat  in  truck       x   at   not
 at airplane    
    unload truck   x   at   gat  gat   not airport       x   not  gat  airport   
    fly airplane   x   not  gat  gat  location      x   at   gat  at   cat obj       x   at
 not  gat   at airplane       x   at obj    x   not  in  gat  airport      x   not  at
 in obj    

   

fiapi policy language bias

    unload truck   x   gat  airport  
    load truck   x   at   cat  gat   at airplane       x   not  gat  location   
    load truck   x   gat   cat  gat   at airplane       x   not  gat   at truck      x 
 gat   at  gat   at airplane     
    load truck   x   gat   not airport     x   not  gat   at truck    
    fly airplane   x   at  gat   at airplane      x   at   cat obj   
    fly airplane   x   not  gat  at   cat obj       x   at   gat  at   cat obj       x   at 
 gat  gat   at truck     
    load truck   x   gat   at airplane     x   not  gat   at truck      x   at   cat obj   
    load airplane   x   gat  airport    x   not  cat  location     x   gat   not  at
airplane      x   not  in  gat   not airport     
    fly airplane   x   at  gat   at airplane      x   not  at truck   
    load truck   x   at   cat  gat   not airport       x   gat  airport  
    drive truck   x   not  at obj     x   not  at   cat obj      x   at   gat  gat  location    
    load truck   x   gat   cat  cat  airport      x   not  cat  location   
    fly airplane   x   at  gat   at airplane      x   at   at obj   
    drive truck   x   in obj  
    drive truck   x   at   gat  gat  airport      x   at  gat  airport     x   at   not
 at airplane    
    fly airplane   x   cat  gat   at truck      x   at   gat  gat  location    
    load truck   x   gat   at obj     x   not  cat  location   
    drive truck   x   at  gat   at airplane      x   not  at   cat obj    
    drive truck   x   at airplane    x   at  gat   at truck    
    unload airplane   x   not  at   cat obj      x   gat   not airport   
    drive truck   x   at  gat   at truck    
    load truck   x   at   not airport     x   gat  airport  
    fly airplane   x   at  gat  location   
    fly airplane   x   in obj    x   not  gat  gat  location      x   not  in  gat  airport    
 x   not  at  in obj      x   at   gat  at   cat obj      
    drive truck   x   at   at airplane   
    load airplane   x   gat   not airport   
blocks world
   stack   x   gon holding    x   con  min gon     x   gon on table  
   putdown 
   unstack   x   on  on  min gon      x   con  on  min gon    
   unstack   x   on   gon clear     x   gon  on  min gon      x   on  gon on table   
 x   gon  not clear   
   pickup   x   gon   con  min gon      x   gon  clear    x   gon   con on table   
   unstack   x   con  gon  clear     x   gon   on  min gon      x   gon   con
clear   

   

fifern  yoon    givan

   unstack   x   not  gon  min gon    
   unstack   x   gon on table    x   gon   con  min gon      x   gon  clear  
   unstack   x   not  con  min gon      x   on  gon  on table     x   gon  not ontable     x   gon  gon on table     x   gon  not clear   
    unstack   x   not  con clear     x   gon   con on table   
    unstack   x   gon  clear    x   on  on  min gon   
ground logistics
   load   x   not  in  gin  city      x   not  cin  city     x   gin  city  
   unload   x   gin  x    
   drive   x   in  gin  x     
   drive   x   not  gin block     x   in  gin  city     x  car   x  clear 
   drive   x   in  gin  rain     x  truck 
colored blocks world
   pick up block from   x   not  con top of table     x   gon top of   on top of block   
   put down block on   x   con top of   con top of  block     x   gon top of holding  
 x   con top of table  
   pick up block from   x   not  con top of block     x   on top of  gon top of  table   
 x   gon top of  gon top of  block     x   not  con top of  block     x   on topof   gon top of block     x   gon top of  gon top of  block   
   pick up block from   x   not  con top of table     x   gon top of   con top of table     x   gon top of  on top of  block   
   put down block on   x   con top of   on top of  table     x   gon top of holding    x 
 con top of table  
   put down block on   x   con top of  on top of block     x   gon top of   gon top of 
block   
   put down block on   x   gon top of holding    x   con top of table  
   put down block on   x  table 
   pick up block from   x   not  con top of table     x   gon top of   con top of table   
    pick up block from   x   gon top of   con top of  table     x  table   x   gon top of
 gon top of block     x   gon top of  on top of  table   
    pick up block from   x   on top of  con top of block     x   gon top of   con top of 
table   
    pick up block from   x   on top of  block    x   not  con top of table     x   gontop of  on top of  block     x   gon top of  on top of  block   
    pick up block from   x   gon top of   gon top of  table   
boxworld
   drive truck   x   gbox at city  box at city  x       x   not  can fly  truck at city  not
previous       x   can drive  previous    x   not  can fly  truck at city  not previous       x   not  can fly  box at city box      x   can drive  can drive  box at city box    
 x   not  can fly  truck at city  box on truck  gbox at city  city      
   unload box from truck in city   x   gbox at city   truck at city previous     x   gboxat city box    x   not  box at city previous     x   gbox at city   can drive   candrive   can fly city       x   box on truck  gbox at city  previous   
   drive truck   x   box on truck  gbox at city  x       x   not  can drive  truck at city
 box on truck  gbox at city  city      

   

fiapi policy language bias

   drive truck   x   can drive  box at city previous     x   can fly  can drive   box at city
box      x   can drive  can fly  truck at city truck      x   not  can drive  truck at city
 box on truck  gbox at city  city        x  previous   x   can drive  can drive x       x 
 not  truck at city  box on truck  gbox at city  city       x   not  can fly previous   
 x   can drive  not  box at city box      x   can drive  can drive  x       x   can drive
 not  truck at city truck    
   load box on truck in city   x   gbox at city   can drive  truck at city truck      x   not
 plane at city previous     x   can drive  can drive   can fly city      x   can drive 
 not  truck at city  not previous     
   unload box from truck in city   x   gbox at city  box on truck  truck     x   not  canfly  truck at city  box on truck  gbox at city  city        x   gbox at city  city  
   drive truck   x   box on truck  gbox at city  previous     x   can drive  gbox at city
 gbox at city  previous      x   not  plane at city plane     x   not  can fly  gbox atcity  gbox at city  previous     
   fly plane   x   box on plane  gbox at city  x     
   unload box from plane in city   x   gbox at city  previous  
    fly plane   x   not  can drive  truck at city  box on truck  gbox at city  city        x 
 gbox at city box    x   not  plane at city previous     x   not previous  
    load box on plane in city   x   gbox at city   can fly previous     x   not  truck at city
 not previous      x   not  can drive  truck at city  box on truck  gbox at city  city      
    drive truck   x   box on truck  gbox at city  x       x   not  can drive  can fly previous      x   can drive   can fly city   
    load box on truck in city   x   gbox at city  previous  

references
aler  r   borrajo  d     isasi  p          using genetic programming learn improve
control knowledge  artificial intelligence                  
ambite  j  l   knoblock  c  a     minton  s          learning plan rewriting rules 
artificial intelligence planning systems  pp      
bacchus  f          aips    planning competition  ai magazine                 
bacchus  f     kabanza  f          using temporal logics express search control knowledge planning  artificial intelligence             
bagnell  j   kakade  s   ng  a     schneider  j          policy search dynamic programming  proceedings   th conference advances neural information
processing 
bellman  r          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
boutilier  c     dearden  r          approximating value trees structured dynamic
programming  saitta  l   ed    international conference machine learning 
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
factored representations  artificial intelligence                   
boutilier  c   reiter  r     price  b          symbolic dynamic programming first order
mdps  international joint conference artificial intelligence 
   

fifern  yoon    givan

dean  t     givan  r          model minimization markov decision processes  national
conference artificial intelligence  pp         
dean  t   givan  r     leach  s          model reduction techniques computing approximately optimal solutions markov decision processes  conference uncertainty
artificial intelligence  pp         
dietterich  t     wang  x          batch value function approximation via support vectors 
proceedings conference advances neural information processing 
dzeroski  s   deraedt  l     driessens  k          relational reinforcement learning  machine learning          
estlin  t  a     mooney  r  j          multi strategy learning search control partialorder planning  national conference artificial intelligence 
fern  a   yoon  s     givan  r          approximate policy iteration policy language bias  proceedings   th conference advances neural information
processing 
finkelstein  l     markovitch  s          selective macro learning algorithm
application nxn sliding tile puzzle  journal artificial intelligence research 
          
givan  r   dean  t     greig  m          equivalence notions model minimization
markov decision processes  artificial intelligence                    
gretton  c     thiebaux  s          exploiting first order regression inductive policy
selection  conference uncertainty artificial intelligence 
guestrin  c   koller  d   gearhart  c     kanodia  n       a   generalizing plans new
environments relational mdps  international joint conference artificial intelligence 
guestrin  c   koller  d   parr  r     venkataraman  s       b   efficient solution algorithms
factored mdps  journal artificial intelligence research             
hoffman  j   porteous  j     sebastia  l          ordered landmarks planning  journal
artificial intelligence research             
hoffmann  j     nebel  b          planning system  fast plan generation
heuristic search  journal artificial intelligence research             
howard  r          dynamic programming markov decision processes  mit press 
huang  y  c   selman  b     kautz  h          learning declarative control rules
constraint based planning  international conference machine learning  pp 
       
kearns  m  j   mansour  y     ng  a  y          sparse sampling algorithm nearoptimal planning large markov decision processes  machine learning          
       
kersting  k   van otterlo  m     deraedt  l          bellman goes relational  proceedings
twenty first international conference machine learning 
   

fiapi policy language bias

khardon  r       a   learning action strategies planning domains  artificial intelligence                    
khardon  r       b   learning take actions  machine learning               
lagoudakis  m     parr  r          reinforcement learning classification  leveraging
modern classifiers  international conference machine learning 
langford  j     zadrozny  b          reducing t step reinforcement learning classification 
http   hunch net jl projects reductions rl class colt submission ps 
martin  m     geffner  h          learning generalized policies planning domains using
concept languages  international conference principles knowledge representation reasoning 
mataric  m          reward functions accelarated learning  proceedings international conference machine learning 
mcallester  d     givan  r          taxonomic syntax first order inference  journal
acm                 
mcallester  d          observations cognitive judgements  national conference
artificial intelligence 
mcgovern  a   moss  e     barto  a          building basic block instruction scheduler
using reinforcement learning rollouts  machine learning                   
minton  s          quantitative results concerning utility explanation based learning 
national conference artificial intelligence 
minton  s   ed            machine learning methods planning  morgan kaufmann 
minton  s   carbonell  j   knoblock  c  a   kuokka  d  r   etzioni  o     gil  y         
explanation based learning  problem solving perspective  artificial intelligence     
      
natarajan  b  k          learning exercises  annual workshop computational
learning theory 
reddy  c     tadepalli  p          learning goal decomposition rules using exercises 
international conference machine learning  pp          morgan kaufmann 
rivest  r          learning decision lists  machine learning                
tesauro  g          practical issues temporal difference learning  machine learning    
       
tesauro  g     galperin  g          on line policy improvement using monte carlo search 
conference advances neural information processing 
tsitsiklis  j     van roy  b          feature based methods large scale dp  machine
learning           
veloso  m   carbonell  j   perez  a   borrajo  d   fink  e     blythe  j          integrating
planning learning  prodigy architecture  journal experimental
theoretical ai        
wu  g   chong  e     givan  r          congestion control via online sampling  infocom 
   

fifern  yoon    givan

yan  x   diaconis  p   rusmevichientong  p     van roy  b          solitaire  man versus
machine  conference advances neural information processing 
yoon  s   fern  a     givan  r          inductive policy selection first order mdps 
conference uncertainty artificial intelligence 
younes  h          extending pddl model stochastic decision processes  proceedings
international conference automated planning scheduling workshop
pddl 
zimmerman  t     kambhampati  s          learning assisted automated planning  looking back  taking stock  going forward  ai magazine                 

   


