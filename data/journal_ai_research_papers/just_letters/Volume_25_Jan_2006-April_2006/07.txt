journal of artificial intelligence research                 

submitted        published     

logical hidden markov models
kristian kersting
luc de raedt

kersting informatik uni freiburg de
deraedt informatik uni freiburg de

institute for computer science
albert ludwigs universitat freiburg
georges koehler allee    
d       freiburg  germany

tapani raiko

tapani raiko hut fi

laboratory of computer and information science
helsinki university of technology
p o  box     
fin       hut  finland

abstract
logical hidden markov models  lohmms  upgrade traditional hidden markov models
to deal with sequences of structured symbols in the form of logical atoms  rather than flat
characters 
this note formally introduces lohmms and presents solutions to the three central inference problems for lohmms  evaluation  most likely hidden state sequence and parameter estimation  the resulting representation and algorithms are experimentally evaluated
on problems from the domain of bioinformatics 

   introduction
hidden markov models  hmms   rabiner   juang        are extremely popular for analyzing sequential data  application areas include computational biology  user modelling 
speech recognition  empirical natural language processing  and robotics  despite their successes  hmms have a major weakness  they handle only sequences of flat  i e   unstructured symbols  yet  in many applications the symbols occurring in sequences are structured  consider  e g   sequences of unix commands  which may have parameters such
as emacs lohmms tex  ls  latex lohmms tex       thus  commands are essentially structured 
tasks that have been considered for unix command sequences include the prediction of
the next command in the sequence  davison   hirsh         the classification of a command
sequence in a user category  korvemaker   greiner        jacobs   blockeel         and
anomaly detection  lane         traditional hmms cannot easily deal with this type of
structured sequences  indeed  applying hmms requires either    ignoring the structure of
the commands  i e   the parameters   or    taking all possible parameters explicitly into
account  the former approach results in a serious information loss  the latter leads to a
combinatorial explosion in the number of symbols and parameters of the hmm and as a
consequence inhibits generalization 
the above sketched problem with hmms is akin to the problem of dealing with structured examples in traditional machine learning algorithms as studied in the fields of inductive logic programming  muggleton   de raedt        and multi relational learnc
    
ai access foundation  all rights reserved 

fikersting  de raedt    raiko

ing  dzeroski   lavrac         in this paper  we propose an  inductive  logic programming
framework  logical hmms  lohmms   that upgrades hmms to deal with structure  the
key idea underlying lohmms is to employ logical atoms as structured  output and state 
symbols  using logical atoms  the above unix command sequence can be represented
as emacs lohmms tex   ls  latex lohmms tex         there are two important motivations for
using logical atoms at the symbol level  first  variables in the atoms allow one to make
abstraction of specific symbols  e g   the logical atom emacs x  tex  represents all files x
that a latex user tex could edit using emacs  second  unification allows one to share information among states  e g   the sequence emacs x  tex   latex x  tex  denotes that the
same file is used as an argument for both emacs and latex 
the paper is organized as follows  after reviewing the logical preliminaries  we introduce
lohmms and define their semantics in section    in section    we upgrade the basic
hmm inference algorithms for use in lohmms  we investigate the benefits of lohmms in
section    we show that lohmms are strictly more expressive than hmms  that they can
be  by design  an order of magnitude smaller than their corresponding propositional
instantiations  and that unification can yield models  which better fit the data  in section   
we empirically investigate the benefits of lohmms on real world data  before concluding 
we discuss related work in section    proofs of all theorems can be found in the appendix 

   logical preliminaries
a first order alphabet  is a set of relation symbols r with arity m     written r m  and a
set of functor symbols f with arity n     written f n  if n     then f is called a constant 
if m     then p is called a propositional variable   we assume that at least one constant
is given   an atom r t            tn   is a relation symbol r followed by a bracketed n tuple of
terms ti   a term t is a variable v or a functor symbol f t            tk   immediately followed by
a bracketed k tuple of terms ti   variables will be written in upper case  and constant  functor and predicate symbols lower case  the symbol will denote anonymous variables which
are read and treated as distinct  new variables each time they are encountered  an iterative
clause is a formula of the form h  b where h  called head  and b  called body  are logical
atoms  a substitution     v   t            vn  tn    e g   x tex   is an assignment of terms ti
to variables vi   applying a substitution  to a term  atom or clause e yields the instantiated term  atom  or clause e where all occurrences of the variables v i are simultaneously
replaced by the term ti   e g  ls x   emacs f  x  x tex  yields ls tex   emacs f  tex  
a substitution  is called a unifier for a finite set s of atoms if s is singleton  a unifier 
for s is called a most general unifier  mgu  for s if  for each unifier  of s  there exists a
substitution  such that      a term  atom or clause e is called ground when it contains
no variables  i e   vars e      the herbrand base of   denoted as hb   is the set of all
ground atoms constructed with the predicate and functor symbols in   the set g   a  of
an atom a consists of all ground atoms a that belong to hb  

   logical hidden markov models
the logical component of a traditional hmm corresponds to a mealy machine  hopcroft
  ullman         i e   a finite state machine where the output symbols are associated with
   

filogical hidden markov models

transitions  this is essentially a propositional representation because the symbols used to
represent states and output symbols are flat  i e  not structured  the key idea underlying
lohmms is to replace these flat symbols by abstract symbols  an abstract symbol a is 
by definition  a logical atom  it is abstract in that it represents the set of all ground  i e  
variable free atoms of a over the alphabet   denoted by g  a   ground atoms then play
the role of the traditional symbols used in a hmms 
example   consider the alphabet   which has as constant symbols tex  dvi  hmm  
and lohmm   and as relation symbols emacs    ls    xdvi    latex    then the atom
emacs file  tex  represents the set  emacs hmm   tex   emacs lohmm   tex    we assume
that the alphabet is typed to avoid useless instantiations such as emacs tex  tex   
the use of atoms instead of flat symbols allows us to analyze logical and structured sequences
such as emacs hmm   tex   latex hmm   tex   xdvi hmm   dvi  
o

definition   abstract transition are expressions of the form p   h 
 b where p         
and h  b and o are atoms  all variables are implicitly assumed to be universally quantified 
i e   the scope of variables is a single abstract transition 
the atoms h and b represent abstract states and o represents an abstract output symbol 
o
the semantics of an abstract transition p   h 
 b is that if one is in one of the states in
g  b   say bb   one will go with probability p to one of the states in g  hb    say hb h  
while emitting a symbol in g  ob h    say ob h o  
latex file 

example   consider c        xdvi file  dvi   latex file  tex   in general
h  b and o do not have to share the same predicate  this is only due to the nature of our running example  assume now that we are in state latex hmm   tex   i e 
b    file hmm    then c specifies that there is a probability of     that the next state
will be in g   xdvi hmm   dvi      xdvi hmm   dvi     i e   the probability is     that the
next state will be xdvi hmm   dvi    and that one of the symbols in g    latex hmm     
 latex hmm      i e   latex hmm    will be emitted  abstract states might also be more
complex such as latex file filestem  fileextension   user 
the above example was simple because h and o were both empty  the situation becomes more complicated when these substitutions are not empty  then  the resulting
state and output symbol sets are not necessarily singletons  indeed  for the transilatex file 

tion       emacs file    dvi   latex file  tex  the resulting state set would be
g   emacs file    dvi      emacs hmm   tex   emacs lohmm   tex    thus the transition
is non deterministic because there are two possible resulting states  we therefore need a
mechanism to assign probabilities to these possible alternatives 
definition   the selection distribution  specifies for each abstract state and observation
symbol a over the alphabet  a distribution     a  over g  a  
to continue our example  let  emacs hmm   tex    emacs file    tex         and
 emacs lohmm   tex    emacs file    tex          then there would be a probability of                 that the next state is emacs hmm   tex  and of      that it is
emacs lohmm   tex  
   

fikersting  de raedt    raiko

o

taking  into account  the meaning of an abstract transition p   h 
 b can be summarized as follows  let bb  g  b   hb h  g  hb   and ob h o  g  ob h    then the
model makes a transition from state bb to hb h and emits symbol ob h o with probability
p   hb h   hb     ob h o   ob h   

   

to represent   any probabilistic representation can   in principle   be used  e g  a bayesian
network or a markov chain  throughout the remainder of the present paper  however 
we will use a nave bayes approach  more precisely  we associate to each argument of a
r m
r m
relation r m a finite domain di
of constants and a probability distribution pi
over
r m
di   let vars a     v            vl   be the variables occurring in an atom a over r m  and
let     v   s          vl  sl   be a substitution grounding a  each vj is then considered a
r m
random variable over the domain darg vj   of the argument arg vj   it appears first in  then 
q
r m
 a   a    lj   parg vj    sj    e g   emacs hmm   tex    emacs f  e    is computed as the
emacs  

emacs  

product of p 
 hmm   and p 
 tex  
thus far the semantics of a single abstract transition has been defined  a lohmm
usually consists of multiple abstract transitions and this creates a further complication 
example   consider

emacs file 

      latex file  tex   emacs file  tex 

and

emacs file 

      dvi file   emacs file  user  
these two abstract transitions make
conflicting statements about the state resulting from emacs hmm   tex   indeed  according
to the first transition  the probability is     that the resulting state is latex hmm   tex  and
according to the second one it assigns     to xdvi hmm   
there are essentially two ways to deal with this situation  on the one hand  one might want
to combine and normalize the two transitions and assign a probability of    respectively     
on the other hand  one might want to have only one rule firing  in this paper  we chose the
latter option because it allows us to consider transitions more independently  it simplifies
learning  and it yields locally interpretable models  we employ the subsumption  or generality  relation among the b parts of the two abstract transitions  indeed  the b part of
the first transition b    emacs file  tex  is more specific than that of the second transition b    emacs file  user  because there exists a substitution     user tex  such that
b     b    i e   b  subsumes b    therefore g   b     g   b    and the first transition can
be regarded as more informative than the second one  it should therefore be preferred over
the second one when starting from emacs hmm   tex   we will also say that the first transition is more specific than the second one  remark that this generality relation imposes a
partial order on the set of all transitions  these considerations lead to the strategy of only
considering the maximally specific transitions that apply to a state in order to determine
the successor states  this implements a kind of exception handling or default reasoning
and is akin to katzs        back off n gram models  in back off n gram models  the most
detailed model that is deemed to provide sufficiently reliable information about the current
context is used  that is  if one encounters an n gram that is not sufficiently reliable  then
back off to use an  n     gram  if that is not reliable either then back off to level n     etc 
the conflict resolution strategy will work properly provided that the bodies of all maximally specific transitions  matching a given state  represent the same abstract state  this
   

filogical hidden markov models

start

ls      

    

    
emacs f       
ls u   

emacs f  u 
emacs f       

ls      
emacs f       

emacs f    u 

latex f       
emacs f       

latex f       
latex f  tex 

emacs f  tex 
emacs f       

latex f       

figure    a logical hidden markov model 

can be enforced by requiring the generality relation over the b parts to be closed under the
greatest lower bound  glb  for each predicate  i e   for each pair b    b  of bodies  such that
   mgu b    b    exists  there is another body b  called lower bound  which subsumes b  
 therefore also b    and is subsumed by b    b    and if there is any other lower bound then
it is subsumed by b  e g   if the body of the second abstract transition in our example is
emacs hmm   user  then the set of abstract transitions would not be closed under glb 
finally  in order to specify a prior distribution over states  we assume a finite set  of
clauses of the form p   h  start using a distinguished start symbol such that p is the
probability of the lohmm to start in a state of g  h  
by now we are able to formally define logical hidden markov models 
definition   a logical hidden markov model  lohmm  is a tuple          where  is
a logical alphabet   a selection probability over    is a set of abstract transitions  and 
is a set of abstract transitions encoding a prior distribution  let b be the set of all atoms
that occur as body parts of transitions in   we assume b to be closed under glb and require
x
b  b  
p      
   
o
p h
b

and that the probabilities p of clauses in  sum up to      

hmms are a special cases of lohmms in which  contains only relation symbols of arity
zero and the selection probability is irrelevant  thus  lohmms directly generalize hmms 
lohmms can also be represented graphically  figure   contains an example  the underlying language   consists of   together with the constant symbol other which denotes a
user that does not employ latex  in this graphical notation  nodes represent abstract states
and black tipped arrows denote abstract transitions  white tipped arrows are used to represent meta knowledge  more precisely  white tipped  dashed arrows represent the generality or
subsumption ordering between abstract states  if we follow a transition to an abstract state
with an outgoing white tipped  dotted arrow then this dotted arrow will always be followed 
dotted arrows are needed because the same abstract state can occur under different cirlatex file 
cumstances  consider the transition p   latex file    user     latex file  user  
   

fikersting  de raedt    raiko

   

start

    

em f  u 



em f  t 

state

abstract state

abstract state
   

ls


ls t 

em f   

em f    t 

   

la f  t 

la f    t 

abstract state

state

ls u   

em f   

state abstract state

   

la f   

   

em f    o 
state



em f  u 
abstract state

em f    u 
abstract state

figure    generating
the
observation
sequence
emacs hmm    latex hmm   
emacs lohmm    ls by the lohmm in figure    the command emacs is
abbreviated by em  f  denotes the filename hmm   f  represents lohmm   t denotes
a tex user  and o some other user  white tipped solid arrows indicate selections 

even though the atoms in the head and body of the transition are syntactically different they
represent the same abstract state  to accurately represent the meaning of this transition we
cannot use a black tipped arrow from latex file  user  to itself  because this would actulatex file 

ally represent the abstract transition p   latex file  user   latex file  user  
furthermore  the graphical representation clarifies that lohmms are generative models  let us explain how the model in figure   would generate the observation sequence
emacs hmm    latex hmm    emacs lohmm    ls  cf  figure     it chooses an initial abstract state  say emacs f  u   since both variables f and u are uninstantiated  the model
samples the state emacs hmm   tex  from g  using   as indicated by the dashed arrow  emacs f  tex  is more specific than emacs f  u   moreover  emacs hmm   tex  matches
emacs f  tex   thus  the model enters emacs f  tex   since the value of f was already
instantiated in the previous abstract state  emacs hmm   tex  is sampled with probability
     now  the model goes over to latex f  tex   emitting emacs hmm   because the abstract
observation emacs f  is already fully instantiated  again  since f was already instantiated 
latex hmm   tex  is sampled with probability      next  we move on to emacs f     u   emitting latex hmm    variables f  and u in emacs f    u  were not yet bound  so  values  say
lohmm  and others  are sampled from   the dotted arrow brings us back to emacs f  u  
because variables are implicitly universally quantified in abstract transitions  the scope of
variables is restricted to single abstract transitions  in turn  f is treated as a distinct 
new variable  and is automatically unified with f    which is bound to lohmm   in contrast 
variable u is already instantiated  emitting emacs lohmm    the model makes a transition
to ls u     assume that it samples tex for u    then  it remains in ls u    with probability
      considering all possible samples  allows one to prove the following theorem 
theorem    semantics  a logical hidden markov model over a language  defines a
discrete time stochastic process  i e   a sequence of random variables hx t it           where the
domain
of xt is hb    hb    the induced probability measure over the cartesian product
n
t hb    hb   exists and is unique for each t     and in the limit t   
before concluding this section  let us address some design choices underlying lohmms 
first  lohmms have been introduced as mealy machines  i e   output symbols are
associated with transitions  mealy machines fit our logical setting quite intuitively as they
directly encode the conditional probability p  o  s   s  of making a transition from s to s 
   

filogical hidden markov models

emitting an observation o  logical hidden markov models define this distribution as
x
p  o  s   s   
p   s    hb     o   o  b h  
o 
p hb
o 

where the sum runs over all abstract transitions h  b such that b is most specific for s 
observations correspond to  partially  observed proof steps and  hence  provide information
shared among heads and bodies of abstract transitions  in contrast  hmms are usually
introduced as moore machines  here  output symbols are associated with states implicitly
assuming o and s  to be independent  thus  p  o  s    s  factorizes into p  o   s   p  s    s  
this makes it more difficult to observe information shared among heads and bodies  in
turn  moore lohmms are less intuitive and harder to understand  for a more detailed
discussion of the issue  we refer to appendix b where we essentially show that  as in the
propositional case  mealy  and moore lohmms are equivalent 
second  the nave bayes approach for the selection distribution reduces the model complexity at the expense of a lower expressivity  functors are neglected and variables are
treated independently  adapting more expressive approaches is an interesting future line of
research  for instance  bayesian networks allow one to represent factorial hmms  ghahramani   jordan         factorial hmms can be viewed as lohmms  where the hidden
states are summarized by a    k ary abstract state  the first k arguments encode the k
state variables  and the last k arguments serve as a memory of the previous joint state  
of the i th argument is conditioned on the i   k th argument  markov chains allow one to
sample compound terms of finite depth such as s s s      and to model e g  misspelled
filenames  this is akin to generalized hmms  kulp  haussler  reese    eeckman         in
which each node may output a finite sequence of symbols rather than a single symbol 
finally  lohmms  as introduced in the present paper  specify a probability distribution over all sequences of a given length  reconsider the lohmm in figure    already the probabilities of all observation sequences of length    i e   ls  emacs hmm   
and
p
emacs lohmm    sum up to    more precisely  for each t     it holds that x       xt p  x   
x       p
    xt p
  xt           in order to model a distribution over sequences of variable length 
i e   t   x       xt p  x    x            xt   xt         we may add a distinguished end state 
the end state is absorbing in that whenever the model makes a transition into this state 
it terminates the observation sequence generated 

   three inference problems for lohmms
as for hmms  three inference problems are of interest  let m be a lohmm and let
o   o    o            ot   t      be a finite sequence of ground observations 
    evaluation  determine the probability p  o   m   that sequence o was generated by
the model m  
    most likely state sequence  determine the hidden state sequence s that has most
likely produced the observation sequence o  i e  s   arg maxs p  s   o  m    
    parameter estimation  given a set o    o            ok   of observation sequences  determine the most likely parameters  for the abstract transitions and the selection
o      
distribution of m   i e     arg max p  o
   

fipsfrag replacements
kersting  de raedt    raiko

sc   
abstract selection abstract
transition
transition

selection

abstract selection
sc   
transition

sc y 
ls o 
ls o 

ls o 

ls u 
ls t 

hc   

ls o 
ls t 

ls t 

hc   

ls t 
ls u 

start

hc x 

   

ls u 
em f  o 

sc z 

em f  o 

em f u 

em f  t 

em f o 

em f  t 

latex f  t 

latex f  t  latex f  t 

em f u 

o 

em f u 

o 
o 

abstract state

s 

s 

s 

em f o 

states

figure    trellis induced by the lohmm in figure    the sets of reachable states at time
            are denoted by s    s          in contrast with hmms  there is an additional
layer where the states are sampled from abstract states 

we will now address each of these problems in turn by upgrading the existing solutions for
hmms  this will be realized by computing a grounded trellis as in figure    the possible
ground successor states of any given state are computed by first selecting the applicable
abstract transitions and then applying the selection probabilities  while taking into account
the substitutions  to ground the resulting states  this two step factorization is coalesced
into one step for hmms 
to evaluate o  consider the probability of the partial observation sequence o     o            ot
and  ground  state s at time t      t  t   given the model m           
t  s     p  o    o            ot   qt   s   m  
where qt   s denotes that the system is in state s at time t  as for hmms  t  s  can be computed using a dynamic programming approach  for t      we set    s    p  q    s   m    
i e      s  is the probability of starting in state s and  for t      we compute t  s  based
on t   s    
   s      start 
   for t                 t do
  
st   
  
foreach s  st  do
  
  
  
  
  

   initialize the set of reachable states  
   initialize the set of reachable states at clock t  
o

foreach maximally specific p   h 
 b     s t  b   mgu s  b  exists do
foreach s    hb h  g  hb   s t  ot  unifies with ob h do
if s    st then
st    st   s   
t  s          

t  s       t  s      t   s   p 
p
    return p  o   m     sst t  s 

   

   

 s    hb     ot    ob h  

filogical hidden markov models

where we assume for the sake of simplicity o  start for each abstract transition p   h 
start    furthermore  the boxed parts specify all the differences to the hmm formula 
unification and  are taken into account 
p
clearly  as for hmms p  o   m     sst t  s  holds  the computational complexity
of this forward procedure is o t  s    b    o  g     o t  s    where s   maxt         t  st    
o is the maximal number of outgoing abstract transitions with regard to an abstract state 
and g is the maximal number of ground instances of an abstract state  in a completely
analogous manner  one can devise a backward procedure to compute
t  s    p  ot     ot             ot   qt   s  m    
this will be useful for solving problem     
having a forward procedure  it is straightforward to adapt the viterbi algorithm as a
solution to problem      i e   for computing the most likely state sequence  let t  s 
denote the highest probability along a single path at time t which accounts for the first t
observations and ends in state s  i e  
t  s   

max

s   s       st 

p  s    s            st    st   s  o            ot   m    

the procedure for finding the most likely state sequence basically follows the forward procedure  instead of summing over all ground transition probabilities in line     we maximize
over them  more precisely  we proceed as follows 
  s      start 
   initialize the set of reachable states  
   for t                 t do
  
st   
   initialize the set of reachable states at clock t  
foreach s  st  do
  
o
  
foreach maximally specific p   h 
 b     s t  b   mgu s  b  exists do
foreach s    hb h  g  hb   s t  ot  unifies with ob h do
  
if s    st then
  
  
st    st   s   
t  s  s          
  
   
t  s  s       t  s  s      t   s   p   s    hb     ot    ob h  
   
foreach s   st do
   
t  s      maxsst  t  s  s   
   
t  s      arg maxsst  t  s  s   
here  t  s  s    stores the probability of making a transition from s to s  and t  s     with
   s    start for all states s  keeps track of the state maximizing the probability along
a single path at time t which accounts for the first t observations and ends in state s     the
most likely hidden state sequence s can now be computed as
st      arg max t     s 
and

st

 

sst   
t  st     for

t   t  t                

one can also consider problem     on a more abstract level  instead of considering all
contributions of different abstract transitions t to a single ground transition from state s
   

fikersting  de raedt    raiko

to state s  in line     one might also consider the most likely abstract transition only  this
is realized by replacing line    in the forward procedure with
t  s       max t  s     t   s   p   s    hb     ot    ob h     
this solves the problem of finding the       most likely state and abstract transition
sequence 
determine the sequence of states and abstract transitions gt  
s    t    s    t    s            st   tt   st   where there exists substitutions i with si   
si  ti i that has most likely produced the observation sequence o  i e 
gt   arg maxgt p  gt   o  m    
thus  logical hidden markov models also pose new types of inference problems 
for parameter estimation  we have to estimate the maximum likelihood transition
probabilities and selection distributions  to estimate the former  we upgrade the well known
baum welch algorithm  baum        for estimating the maximum likelihood parameters
of hmms and probabilistic context free grammars 
for hmms  the baum welch algorithm computes the improved estimate p of the trano
sition probability of some  ground  transition t  p   h 
 b by taking the ratio
p  p

 t 
h 

o 

b

 t   

   

between the expected number  t  of times of making the transitions t at any time given
the model m and an observation sequence o  and the total number of times a transitions
is made from b at any time given m and o 
basically the same applies when t is an abstract transition  however  we have to be
a little bit more careful because we have no direct access to  t   let  t  gcl  t  be the
go
probability of following the abstract transition t via its ground instance gcl  p   gh  gb
at time t  i e  
t  gcl  t   

t  gb   p  t    gh 
  gh   hb     ot    ob h    
p  o   m  

   

where b   h are as in the forward procedure  see above  and p  o   m   is the probability
that the model generated the sequence o  again  the boxed terms constitute the main
difference to the corresponding hmm formula  in order to apply equation     to compute
improved estimates of probabilities associated with abstract transitions  we set
 t   

t
x
t  

t  t   

t x
x

t  gcl  t 

t   gcl

where the inner sum runs over all ground instances of t 
this leads to the following re estimation method  where we assume that the sets s i of
reachable states are reused from the computations of the   and  values 
   

filogical hidden markov models

  
  
  
  
  
  
  
  
  

   initialization of expected counts   
foreach t     do
 t     m    or   if not using pseudocounts   
   compute expected counts   
for t                 t do
foreach s  st do
o

foreach max  specific t  p   h 
 b     s t  b   mgu s  b  exists do
foreach s    hb h  g  hb   s t  s   st    mgu ot   ob h   exists do

 t      t    t  s   p  t    s    p  o   m    s    hb     ot    ob h  

here  equation     can be found in line    in line    we set pseudocounts as small samplesize regularizers  other methods to avoid a biased underestimate of probabilities and even
zero probabilities such as m estimates  see e g   mitchell        can be easily adapted 
to estimate the selection probabilities  recall that  follows a nave bayes scheme  therefore  the estimated probability for a domain element d  d for some domain d is the ratio
between the number of times d is selected and the number of times any d   d is selected 
the procedure for computing the  values can thus be reused 
altogether  the baum welch algorithm works as follows  while not converged      estimate the abstract transition probabilities  and     the selection probabilities  since it is
an instance of the em algorithm  it increases the likelihood of the data with every update 
and according to mclachlan and krishnan         it is guaranteed to reach a stationary
point  all standard techniques to overcome limitations of em algorithms are applicable 
the computational complexity  per iteration  is o k      d     o k  t  s    k  d  where
k is the number of sequences   is the complexity of computing the  values  see above  
and d is the sum over the sizes of domains associated to predicates  recently  kersting
and raiko        combined the baum welch algorithm with structure search for model
selection of logical hidden markov models using inductive logic programming  muggleton
  de raedt        refinement operators  the refinement operators account for different
abstraction levels which have to be explored 

   advantages of lohmms
in this section  we will investigate the benefits of lohmms      lohmms are strictly
more expressive than hmms  and      using abstraction  logical variables and unification
can be beneficial  more specifically  with      we will show that
 b   lohmms can be  by design  smaller than their propositional instantiations  and
 b   unification can yield better log likelihood estimates 
    on the expressivity of lohmms
whereas hmms specify probability distributions over regular languages  lohmms specify
probability distributions over more expressive languages 

   

fikersting  de raedt    raiko

theorem   for any  consistent  probabilistic context free grammar  pcfg  g for some
language l there exists a lohmm m s t  pg  w    pm  w  for all w  l 
the proof  see appendix c  makes use of abstract states of unbounded depth  more
precisely  functors are used to implement a stack  without functors  lohmms cannot
encode pcfgs and  because the herbrand base is finite  it can be proven that there always
exists an equivalent hmm 
furthermore  if functors are allowed  lohmms are strictly more expressive than pcfgs 
they can specify probability distributions over some languages that are context sensitive 
     
stack s     s     
a
     
stack s x   s x   

a
      unstack s x   s x   

b
     
unstack x  y  

c
     
unstack s     y  

end
     
end 

start
stack x  x 
stack x  x 
unstack s x   y 
unstack s     s y  
unstack s     s    

the lohmm defines a distribution over  an bn cn   n      
finally  the use of logical variables also enables one to deal with identifiers  identifiers
are special types of constants that denote objects  indeed  recall the unix command
sequence emacs lohmms tex  ls  latex lohmms tex        from the introduction  the filename
lohmms tex is an identifier  usually  the specific identifiers do not matter but rather the
fact that the same object occurs multiple times in the sequence  lohmms can easily deal
with identifiers by setting the selection probability  to a constant for the arguments in
which identifiers can occur  unification then takes care of the necessary variable bindings 
    benefits of abstraction through variables and unification
reconsider the domain of unix command sequences  unix users oftenly reuse a newly created directory in subsequent commands such as in mkdir vt   x   cd vt   x   ls vt   x   
unification should allow us to elegantly employ this information because it allows us to specify that  after observing the created directory  the model makes a transition into a state
where the newly created directory is used 
p    cd dir  mkdir   mkdir dir  com 

and

p    cd    mkdir   mkdir dir  com 

if the first transition is followed  the cd command will move to the newly created directory 
if the second transition is followed  it is not specified which directory cd will move to  thus 
the lohmm captures the reuse of created directories as an argument of future commands 
moreover  the lohmm encodes the simplest possible case to show the benefits of unification  at any time  the observation sequence uniquely determines the state sequence  and
functors are not used  therefore  we left out the abstract output symbols associated with
abstract transitions  in total  the lohmm u   modelling the reuse of directories  consists
of     parameters only but still covers more than         ground  states  see appendix d
for the complete model  the compression in the number of parameters supports  b   
to empirically investigate the benefits of unification  we compare u with the variant n
of u where no variables are shared  i e   no unification is used such that for instance the
   

filogical hidden markov models

first transition above is not allowed  see appendix d  n has     parameters less than u  
we computed the following zero one win function
 


  if log pu  o   log pn  o     
f  o   
  otherwise
leave one out cross validated on unix shell logs collected by greenberg         overall 
the data consists of     users of four groups  computer scientists  nonprogrammers  novices
and others  about        commands have been logged with an average of     sessions
per user  we present here results for a subset of the data  we considered all computer
scientist sessions in which at least a single mkdir command appears  these yield     logical
sequences over in total      ground atoms  the loo win was         other loo statistics
are also in favor of u  

u
n

training
o 
o  log ppu  o
log p  o
o 
n  o
       
      
       

test
log p  o  log ppnu  o 
 o 
    
    
    

thus  although u has     parameters more than n   it shows a better generalization performance  this result supports  b    a pattern often found in u was  
       cd dir  mkdir   mkdir dir  com 

and

       cd    mkdir   mkdir dir  com 

favoring changing to the directory just made  this knowledge cannot be captured in n
       cd    mkdir   mkdir dir  com  
the results clearly show that abstraction through variables and unification can be beneficial
for some applications  i e    b   and  b   hold 

   real world applications
our intentions here are to investigate whether lohmms can be applied to real world
domains  more precisely  we will investigate whether benefits  b   and  b   can also be
exploited in real world application domains  additionally  we will investigate whether
 b   lohmms are competitive with ilp algorithms that can also utilize unification and
abstraction through variables  and
 b   lohmms can handle tree structured data similar to pcfgs 
to this aim  we conducted experiments on two bioinformatics application domains  protein
fold recognition  kersting  raiko  kramer    de raedt        and mrna signal structure
detection  horvath  wrobel    bohnebeck         both application domains are multiclass
problems with five different classes each 
   the sum of probabilities is not the same                              because of the use of pseudo counts
and because of the subliminal non determinism  w r t  abstract states  in u   i e   in case that the first
transition fires  the second one also fires 

   

fikersting  de raedt    raiko

    methodology
in order to tackle the multiclass problem with lohmms  we followed a plug in estimate
approach  let  c    c            ck   be the set of possible classes  given a finite set of training
examples   xi   yi   ni    x   c    c            cn    one tries to find f   x   c    c            ck  
f  x    arg

max

c c   c       ck  

p  x   m  c    p  c   

   

with low approximation error on the training data as well as on unseen examples  in
equation      m denotes the model structure which is the same for all classes   c denotes
the maximum likelihood parameters of m for class c estimated on the training examples
with yi   c only  and p  c  is the prior class distribution 
we implemented the baum welch algorithm  with pseudocounts m  see line    for maximum likelihood parameter estimation using the prolog system yap        in all experiments 
we set m     and let the baum welch algorithm stop if the change in log likelihood was
less than     from one iteration to the next  the experiments were ran on a pentium iv
    ghz linux machine 
    protein fold recognition
protein fold recognition is concerned with how proteins fold in nature  i e   their threedimensional structures  this is an important problem as the biological functions of proteins
depend on the way they fold  a common approach is to use database searches to find proteins  of known fold  similar to a newly discovered protein  of unknown fold   to facilitate
protein fold recognition  several expert based classification schemes of proteins have been
developed that group the current set of known protein structures according to the similarity
of their folds  for instance  the structural classification of proteins  hubbard  murzin  brenner    chotia         scop  database hierarchically organizes proteins according to their
structures and evolutionary origin  from a machine learning perspective  scop induces a
classification problem  given a protein of unknown fold  assign it to the best matching group
of the classification scheme  this protein fold classification problem has been investigated
by turcotte  muggleton  and sternberg        based on the inductive logic programming
 ilp  system progol and by kersting et al         based on lohmms 
the secondary structure of protein domains  can elegantly be represented as logical sequences  for example  the secondary structure of the ribosomal protein l  is represented as
st null      he right  alpha      st plus      he right  alpha      st plus     
he right  alpha      st plus      he right  alpha      st plus      he hright  alpha    
helices of a certain type  orientation and length he helixtype  helixorientation  length  
and strands of a certain orientation and length st strandorientation  length  are atoms over
logical predicates  the application of traditional hmms to such sequences requires one to
either ignore the structure of helices and strands  which results in a loss of information  or to
take all possible combinations  of arguments such as orientation and length  into account 
which leads to a combinatorial explosion in the number of parameters
   a domain can be viewed as a sub section of a protein which appears in a number of distantly related
proteins and which can fold independently of the rest of the protein 

   

filogical hidden markov models

end
block b of length  

block s b  of length  

dynamics within block

dynamics within block

block b  s p  

block s b   s p  

block b  p 

block s b   p 
transition to next block

transition to next block

block s b   s    

block b  s s s      

block b    

block s b     

figure    scheme of a left to right lohmm block model 
the results reported by kersting et al         indicate that lohmms are well suited
for protein fold classification  the number of parameters of a lohmm can by an order of
magnitude be smaller than the number of a corresponding hmm      versus approximately
       and the generalization performance  a     accuracy  is comparable to turcotte
et al s        result based on the ilp system progol  a     accuracy  kersting et al 
        however  do not cross validate their results nor investigate  as it is common in
bioinformatics  the impact of primary sequence similarity on the classification accuracy  for
instance  the two most commonly requested astral subsets are the subset of sequences
with less than     identity to each other     cut  and with less than     identity to each
other     cut   motivated by this  we conducted the following new experiments 
the data consists of logical sequences of the secondary structure of protein domains  as
in the work of kersting et al          the task is to predict one of the five most populated
scop folds of alpha and beta proteins  a b   tim beta alpha barrel  fold     nad p binding rossmann fold domains  fold     ribosomal protein l   fold      cysteine hydrolase
 fold      and phosphotyrosine protein phosphatases i like  fold      the class of a b
proteins consists of proteins with mainly parallel beta sheets  beta alpha beta units   the
data have been extracted automatically from the astral dataset version       chandonia 
hon  walker  lo conte  p koehl    brenner        for the    cut and for the    cut  as
in the work of kersting et al          we consider strands and helices only  i e   coils and
isolated strands are discarded  for the    cut  this yields     logical sequences consisting
of in total       ground atoms  the number of sequences in the classes are listed as     
              and     for the    cut  this yields     logical sequences consisting of in total
      ground atoms  the number of sequences in the classes are listed as                   
and    
lohmm structure  the used lohmm structure follows a left to right block topology 
see figure    to model blocks of consecutive helices  resp  strands   being in a block of
some size s  say    the model will remain in the same block for s     time steps  a similar
idea has been used to model haplotypes  koivisto  perola  varilo  hennah  ekelund  lukk 
peltonen  ukkonen    mannila        koivisto  kivioja  mannila  rastas    ukkonen 
       in contrast to common hmm block models  won  prugel bennett    krogh        
   

fikersting  de raedt    raiko

the transition parameters are shared within each block and one can ensure that the model
makes a transition to the next state s block   only at the end of a block  in our example
after exactly   intra block transitions  furthermore  there are specific abstract transitions
for all helix types and strand orientations to model the priori distribution  the intra  and
the inter block transitions  the number of blocks and their sizes were chosen according
to the empirical distribution over sequence lengths in the data so that the beginning and
the ending of protein domains was likely captured in detail  this yield the following block
structure
   

   

     

     

   

     

     

     

     

where the numbers denote the positions within protein domains  furthermore  note that
the last block gathers all remaining transitions  the blocks themselves are modelled using
hidden abstract states over
hc helixtype  helixorientation  length  block   and sc strandorientation  length  block    
here  length denotes the number of consecutive bases the structure element consists of 
the length was discretized into    bins such that the original lengths were uniformally
distributed  in total  the lohmm has     parameters  the corresponding hmm without
parameter sharing has more than       parameters  this clearly confirms  b   
results  we performed a    fold cross validation  on the    cut dataset  the accuracy was
    and took approx     minutes per cross validation iteration  on the    cut  the accuracy
was     and took approx     minutes per cross validation iteration  the results validate
kersting et al s        results and  in turn  clearly show that  b   holds  moreover  the
novel results on the    cut dataset indicate that the similarities detected by the lohmms
between the protein domain structures were not accompanied by high sequence similarity 
    mrna signal structure detection
mrna sequences consist of bases  guanine  adenine  uracil  cytosine  and fold intramolecularly to form a number of short base paired stems  durbin  eddy  krogh    mitchison 
       this base paired structure is called the secondary structure  cf  figures   and    the
secondary structure contains special subsequences called signal structures that are responsible for special biological functions  such as rna protein interactions and cellular transport 
the function of each signal structure class is based on the common characteristic binding
site of all class elements  the elements are not necessarily identical but very similar  they
can vary in topology  tree structure   in size  number of constituting bases   and in base
sequence 
the goal of our experiments was to recognize instances of signal structures classes in
mrna molecules  the first application of relational learning to recognize the signal structure class of mrna molecules was described in the works of bohnebeck  horvath  and
wrobel        and of horvath et al          where the relational instance based learner
ribl was applied  the dataset   we used was similar to the one described by horvath
   the dataset is not the same as described in the work by horvath et al         because we could not obtain
the original dataset  we will compare to the smaller data set used by horvath et al   which consisted of

   

filogical hidden markov models

et al          it consisted of    mrna secondary structure sequences  more precisely  it was
composed of    and   secis  selenocysteine insertion sequence      ire  iron responsive
element      tar  trans activating region  and    histone stem loops constituting five
classes 
the secondary structure is composed of different building blocks such as stacking region 
hairpin loops  interior loops etc  in contrast to the secondary structure of proteins that forms
chains  the secondary structure of mrna forms a tree  as trees can not easily be handled
using hmms  mrna secondary structure data is more challenging than that of proteins 
moreover  horvath et al         report that making the tree structure available to ribl
as background knowledge had an influence on the classification accuracy  more precisely 
using a simple chain representation ribl achieved a       leave one out cross validation
 loo  accuracy whereas using the tree structure as background knowledge ribl achieved
a       loo accuracy 
we followed horvath et al s experimental setup  that is  we adapted their data representations to lohmms and compared a chain model with a tree model 

chain representation  in the chain representation  see also figure    
signal
structures
are
described
by
single typesingle  position  acid  
or
helical typehelical   position  acid   acid   
depending on its type  a structure element is represented by either single   or helical   
their first argument
typesingle  resp 
typehelical   specifies the type of the structure element  i e  
single  bulge   bulge   hairpin  resp  stem   the argument position is the position of the sequence element within the corresponding structure element counted down 
i e      n        n                n        the maximal position was set to    as this was the
maximal position observed in the data  the last argument encodes the observed nucleotide
 pair  
the used lohmm structure follows again the left to right block structure shown in
figure    its underlying idea is to model blocks of consecutive helical structure elements  the hidden states are modelled using single typesingle  position  acid   block  
and helical typehelical   position  acid   acid   block    being in a block of consecutive helical  resp  single  structure elements  the model will remain in the block or transition to a
single element  the transition to a single  resp  helical  element only occurs at position
n     at all other positions n position   there were transitions from helical  resp  single 
structure elements to helical  resp  single  structure elements at position capturing the dynamics of the nucleotide pairs  resp  nucleotides  within structure elements  for instance 

   signal structures and is very close to our data set  on a larger data set  with     structures  horvath
et al  report an error rate of       
   nm     is shorthand for the recursive application of the functor n on   m times  i e   for position m 

   

fikersting  de raedt    raiko

helical stem  n     c  g  
helical stem  n n      c  g  
helical stem  n n n       c  g  
single bulge   n     a  
single bulge   n n      a  
single bulge   n n n       g  
helical stem  n     c  g  
helical stem  n n      c  g  
single bulge   n     a  
helical stem  n     a  a  
helical stem  n n      u  a  
helical stem  n n n       u  g  
helical stem  n n n n        u  a  
helical stem  n n n n n         c  a  
helical stem  n n n n n n          u  a  
helical stem  n n n n n n n           a  u  

u
a

u
c
c
c

g
g
g

a
a
g

a
c
c

single hairpin  n n n       a  
single hairpin  n n      u  
single hairpin  n     u  

single bulge   n     a  

g
g

a
a
u
u
u
c
u
a

a
a
g
a
a
a
u

figure    the chain representation of a secis signal structure  the ground atoms are
ordered clockwise starting with helical stem  n n n n n n n           a  u  at the
lower left hand side corner 

the transitions for block n    at position n n     were
pa  he stem n    x y 

a   he stem  n     x  y  n      he stem  n n      x  y  n     
pb  he stem n    x y 

b 

he stem  n     y  x  n      he stem  n n      x  y  n     

c 

he stem  n     x    n      he stem  n n      x  y  n     

d 

he stem  n       y  n      he stem  n n      x  y  n     

e 

he stem  n         n      he stem  n n      x  y  n     

pc  he stem n    x y 

pd  he stem n    x y 
pe  he stem n    x y 

in total  there were   possible blocks as this was the maximal number of blocks of consecutive
helical structure elements observed in the data  overall  the lohmm has     parameters 
in contrast  the corresponding hmm has more than       transitions validating  b   
results  the loo test log likelihood was       and an em iteration took on average
   seconds 
without the unification based transitions b d  i e   using only the abstract transitions
pa  he stem n    x y 

a   he stem  n     x  y  n      he stem  n n      x  y  n     
e 

pe  he stem n    x y 

he stem  n         n      he stem  n n      x  y  n      

the model has     parameters  the loo test log likelihood was        and an em iteration took on average    seconds  the difference in loo test log likelihood is statistically
significant  paired t test  p         
omitting even transition a  the loo test log likelihood dropped to        and the
average time per em iteration was    seconds  the model has     parameters  the
difference in average loo log likelihood is statistically significant  paired t test  p          
the results clearly show that unification can yield better loo test log likelihoods  i e  
 b   holds 
   

filogical hidden markov models

nucleotide pair  c  g   
nucleotide pair  c  g   
nucleotide pair  c  g   
helical s s s s s         s s s        c   stem  n n n       
nucleotide a  
nucleotide a  
nucleotide g  
single s s s s        s s s           bulge   n n n       
nucleotide pair  c  g   
nucleotide pair  c  g   
helical s s s       s      c  c  c   stem  n n      
nucleotide a  
single s s      s         bulge   n     
nucleotide pair  a  a   
nucleotide pair  u  a   
nucleotide pair  u  g   
nucleotide pair  u  a   
nucleotide pair  c  a   
nucleotide pair  u  a   
nucleotide pair  a  u   
helical s         c  c   stem  n n n n n n n           

u
a

u
c
c
c

g
g
g

a
a
g

a
c
c

g
g

single s s s s s s          s s s s s        
    hairpin  n n n       
nucleotide a  
nucleotide u  
nucleotide u  

single s s s s s s s           s s s      
    bulge   n     
nucleotide a  

a
a
u
u
u
c
u
a

a
a
g
a
a
a
u

 
s   
s s    s s s     
s s s s      
s s s s s s s         
s s s s s      
s s s s s s        

root    root   c   

figure    the tree representation of a secis signal structure   a  the logical sequence 
i e   the sequence of ground atoms representing the secis signal structure  the
ground atoms are ordered clockwise starting with root    root   c   in the lower
left hand side corner   b  the tree formed by the secondary structure elements 

tree representation  in the tree representation  see figure    a    the idea is to capture
the tree structure formed by the secondary structure elements  see figure    b   each
training instance is described as a sequence of ground facts over
root    root   children  
helical id  parentid   children  type  size  
nucleotide pair basepair   
single id  parentid   children  type  size  
nucleotide base   
here  id and parentid are natural numbers    s     s s            encoding the childparent relation   children denotes the number  of children      c    c  c          type is the
type of the structure element such as stem  hairpin         and size is a natural number
   n     n n            atoms root    root   children  are used to root the topology  the
maximal  children was   and the maximal size was    as this was the maximal value
observed in the data 
as trees can not easily be handled using hmms  we used a lohmm which basically
encodes a pcfg  due to theorem    this is possible  the used lohmm structure can be
found in appendix e  it processes the mrna trees in in order  unification is only used for
parsing the tree  as for the chain representation  we used a position argument in the hidden
states to encode the dynamics of nucleotides  nucleotide pairs  within secondary structure
   here  we use the prolog short hand notation    for lists  a list either is the constant    representing the
empty list  or is a compound term with functor     and two arguments  which are respectively the head
and tail of the list  thus  a  b  c  is the compound term   a    b    c        

   

fikersting  de raedt    raiko

elements  the maximal position was again     in contrast to the chain representation 
nucleotide pairs such as  a  u  are treated as constants  thus  the argument basepair
consists of    elements 
results  the loo test log likelihood was        thus  exploiting the tree structure
yields better probabilistic models  on average  an em iteration took    seconds  overall 
the result shows that  b   holds 
although the baum welch algorithm attempts to maximize a different objective function  namely the likelihood of the data  it is interesting to compare lohmms and ribl in
terms of classification accuracy 
classification accuracy  on the chain representation  the loo accuracies of all
lohmms were              this is a considerable improvement on ribls              
loo accuracy for this representation  on the tree representation  the lohmm also
achieved a loo accuracy of              this is comparable to ribls loo accuracy of
            on this kind of representation 
thus  already the chain lohmms show marked increases in loo accuracy when compared to ribl  horvath et al          in order to achieve similar loo accuracies  horvath
et al         had to make the tree structure available to ribl as background knowledge 
for lohmms  this had a significant influence on the loo test log likelihood  but not on
the loo accuracies  this clearly supports  b    moreover  according to horvath et al  
the mrna application can also be considered a success in terms of the application domain 
although this was not the primary goal of our experiments  there exist also alternative
parameter estimation techniques and other models  such as covariance models  eddy  
durbin        or pair hidden markov models  sakakibara         that might have been
used as well as a basis for comparison  however  as lohmms employ  inductive  logic programming principles  it is appropriate to compare with other systems within this paradigm
such as ribl 

   related work
lohmms combine two different research directions  on the one hand  they are related to
several extensions of hmms and probabilistic grammars  on the other hand  they are also
related to the recent interest in combining inductive logic programming principles with
probability theory  de raedt   kersting              
in the first type of approaches  the underlying idea is to upgrade hmms and probabilistic
grammars to represent more structured state spaces 
hierarchical hmms  fine  singer    tishby         factorial hmms  ghahramani  
jordan         and hmms based on tree automata  frasconi  soda    vullo        decompose the state variables into smaller units  in hierarchical hmms states themselves can be
hmms  in factorial hmms they can be factored into k state variables which depend on one
another only through the observation  and in tree based hmms the represented probability
distributions are defined over tree structures  the key difference with lohmms is that
these approaches do not employ the logical concept of unification  unification is essential
   

filogical hidden markov models

because it allows us to introduce abstract transitions  which do not consist of more detailed
states  as our experimental evidence shows  sharing information among abstract states by
means of unification can lead to more accurate model estimation  the same holds for relational markov models  rmms   anderson  domingos    weld        to which lohmms
are most closely related  in rmms  states can be of different types  with each type described
by a different set of variables  the domain of each variable can be hierarchically structured 
the main differences between lohmms and rmms are that rmms do not either support
variable binding nor unification nor hidden states 
the equivalent of hmms for context free languages are probabilistic context free grammars  pcfgs   like hmms  they do not consider sequences of logical atoms and do not
employ unification  nevertheless  there is a formal resemblance between the baum welch
algorithms for lohmms and for pcfgs  in case that a lohmm encodes a pcfg both
algorithms are identical from a theoretical point of view  they re estimate the parameters
as the ratio of the expected number of times a transition  resp  production  is used and the
expected number of times a transition  resp  production  might have been used  the proof
of theorem   assumes that the pcfg is given in greibach normal form   gnf  and uses a
pushdown automaton to parse sentences  for grammars in gnf  pushdown automata are
common for parsing  in contrast  the actual computations of the baum welch algorithm
for pcfgs  the so called inside outside algorithm  baker        lari   young         is
usually formulated for grammars in chomsky normal form    the inside outside algorithm
can make use of the efficient cyk algorithm  hopcroft   ullman        for parsing strings 
an alternative to learning pcfgs from strings only is to learn from more structured data
such as skeletons  which are derivation trees with the nonterminal nodes removed  levy  
joshi         skeletons are exactly the set of trees accepted by skeletal tree automata  sta  
informally  an sta  when given a tree as input  processes the tree bottom up  assigning a
state to each node based on the states of that nodes children  the sta accepts a tree iff
it assigns a final state to the root of the tree  due to this automata based characterization
of the skeletons of derivation trees  the learning problem of  p cfgs can be reduced to
the problem of an sta  in particular  sta techniques have been adapted to learning tree
grammars and  p cfgs  sakakibara        sakakibara et al         efficiently 
pcfgs have been extended in several ways  most closely related to lohmms are
unification based grammars which have been extensively studied in computational linguistics  examples are  stochastic  attribute value grammars  abney         probabilistic feature grammars  goodman         head driven phrase structure grammars  pollard   sag 
       and lexical functional grammars  bresnan         for learning within such frameworks  methods from undirected graphical models are used  see the work of johnson       
for a description of some recent work  the key difference to lohmms is that only nonterminals are replaced with structured  more complex entities  thus  observation sequences of
flat symbols and not of atoms are modelled  goodmans probabilistic feature grammars are
an exception  they treat terminals and nonterminals as vectors of features  no abstraction
is made  i e   the feature vectors are ground instances  and no unification can be employed 
   a grammar is in gnf iff all productions are of the form a  av where a is a variable  a is exactly one
terminal and v is a string of none or more variables 
   a grammar is in cnf iff every production is of the form a  b  c or a  a where a  b and c are variables 
and a is a terminal 

   

fikersting  de raedt    raiko

con

mkdir
con
mkdir

mv

ls

cd

mv
con

vt   x

vt   x

ls
new

vt   x

vt   x

vt   x

new

 a 

cd

vt   x

 b 

vt   x

vt   x

figure     a  each atom in the logical sequence mkdir vt   x   mv new  vt   x  
ls vt   x   cd vt   x  forms a tree  the shaded nodes denote shared labels
among the trees   b  the same sequence represented as a single tree  the predicate con   represents the concatenation operator 

therefore  the number of parameters that needs to be estimated becomes easily very large 
data sparsity is a serious problem  goodman applied smoothing to overcome the problem 
lohmms are generally related to  stochastic  tree automata  see e g   carrasco  oncina  and calera rubio         reconsider the unix command sequence
mkdir vt   x   mv new  vt   x   ls vt   x   cd vt   x    each atom forms a tree  see
figure    a   and  indeed  the whole sequence of atoms also forms a  degenerated  tree 
see figure    b   tree automata process single trees vertically  e g   bottom up  a state in
the automaton is assigned to every node in the tree  the state depends on the node label
and on the states associated to the siblings of the node  they do not focus on sequential
domains  in contrast  lohmms are intended for learning in sequential domains  they
process sequences of trees horizontally  i e   from left to right  furthermore  unification
is used to share information between consecutive sequence elements  as figure    b 
illustrates  tree automata can only employ this information when allowing higher order
transitions  i e   states depend on their node labels and on the states associated to
predecessors             levels down the tree 
in the second type of approaches  most attention has been devoted to developing highly
expressive formalisms  such as e g  pcup  eisele         pclp  riezler         slps  muggleton         plps  ngo   haddawy         rbns  jaeger         prms  friedman 
getoor  koller    pfeffer         prism  sato   kameya         blps  kersting   de
raedt      b      a   and dprms  sanghai  domingos    weld         lohmms can be
seen as an attempt towards downgrading such highly expressive frameworks  indeed  applying the main idea underlying lohmms to non regular probabilistic grammar  i e   replacing
flat symbols with atoms  yields  in principle  stochastic logic programs  muggleton        
as a consequence  lohmms represent an interesting position on the expressiveness scale 
whereas they retain most of the essential logical features of the more expressive formalisms 
they seem easier to understand  adapt and learn  this is akin to many contemporary consid   

filogical hidden markov models

erations in inductive logic programming  muggleton   de raedt        and multi relational
data mining  dzeroski   lavrac        

   conclusions
logical hidden markov models  a new formalism for representing probability distributions
over sequences of logical atoms  have been introduced and solutions to the three central
inference problems  evaluation  most likely state sequence and parameter estimation  have
been provided  experiments have demonstrated that unification can improve generalization
accuracy  that the number of parameters of a lohmm can be an order of magnitude smaller
than the number of parameters of the corresponding hmm  that the solutions presented
perform well in practice and also that lohmms possess several advantages over traditional
hmms for applications involving structured sequences 
acknowledgments the authors thank andreas karwath and johannes horstmann for
interesting collaborations on the protein data  ingo thon for interesting collaboration on
analyzing the unix command sequences  and saul greenberg for providing the unix command sequence data  the authors would also like to thank the anonymous reviewers for comments which considerably improved the paper  this research was partly supported by the
european union ist programme under contract numbers ist            and fp        
 application of probabilistic inductive logic programming  april  i and ii   tapani raiko
was supported by a marie curie fellowship at daisy  hpmt ct            

appendix a  proof of theorem  
let m            be a lohmm  to show that m specifies a time discrete stochastic
process  i e   a sequence of random variables hxt it           where the domains of the random
variable xt is hb    the herbrand base over   we define the immediate state operator
tm  operator and the current emission operator em  operator 
definition    tm  operator  em  operator   the operators tm    hb   hb and em  
 hb   hb are
o

tm  i     hb h    p   h 
 b   m   bb  i  hb h  g  h  
o

em  i     ob h o    p   h 
 b   m   bb  i  hb g  g  h 
and ob h o  g  o  
i  
i   start    with
for each i                   the set tm
  start      tm  tm
 
tm   start      tm   start   specifies the state set at clock i which forms a random varii   start   specifies the possible symbols emitted when transitioning
able yi   the set um
from i to i      it forms the variable ui   each yi  resp  ui   can be extended to a random
variable zi  resp  ui   over hb  

p  zi   z   



i   start  
      z   tm
p  yi   z    otherwise

   

fikersting  de raedt    raiko

psfrag replacements

z 

z 

z 

u 

u 

   

u 

figure    discrete time stochastic process induced by a lohmm  the nodes z i and ui
represent random variables over hb  

figure   depicts the influence relation among zi and ui   using standard arguments from
probability theory and noting that
p  ui   ui   zi     zi     zi   zi    
and p  zi     zi    

x

p  zi     ui   zi  

p  zi     zi     ui   ui   zi  
p
ui p  zi     ui   zi  

ui

where the probability distributions are due to equation      it is easy to show that kolmogorovs extension theorem  see bauer        fristedt
and gray        holds  thus  m
nt
specifies a unique probability distribution over
 z
 ui   for each t     and in the
i
i  
limit t   


appendix b  moore representations of lohmms
for hmms  moore representations  i e   output symbols are associated with states and mealy
representations  i e   output symbols are associated with transitions  are equivalent  in this
appendix  we will investigate to which extend this also holds for lohmms 
let l be a mealy lohmm according to definition    in the following  we will derive
the notation of an equivalent lohmm l  in moore representation where there are abstract
transitions and abstract emissions  see below   each predicate b n in l is extended to b n 
  in l    the domains of the first n arguments are the same as for b n  the last argument
will store the observation to be emitted  more precisely  for each abstract transition
o v       vk  

p   h w            wl    b u            un  
in l  there is an abstract transition
p   h w            wl   o v             v k     b u            un    
in l    the primes in o v             v k   denote that we replaced each free   variables o v            vk  
by some distinguished constant symbol  say    due to this  it holds that
 h w            wl       h w            wl   o v             v k      
   a variable x  vars o v            vk    is free iff x   vars h w            wl     vars b u            un    

   

   

filogical hidden markov models

and l  s output distribution can be specified using abstract emissions which are expressions
of the form
      o v            vk    h w            wl   o v             v k     
   
the semantics of an abstract transition in l  is that being in
state s t  g   b u            un      the system will make a transition into
s t    g   h w            wl   o v             v k     with probability
p   s t     h w            wl   o v             v k      s t  

some
state

   

where s t   mgu s t   b u            un       due to equation      equation     can be rewritten
as
p   s t     h w            wl     s t    
due to equation      the system will emit the output symbol ot    g   o v            vk    in
state s t   with probability
 ot     o v            vk  s t   s t  
where s t     mgu h w            wl   o v             v k     s t      due to the construction of l    there
exists a triple  st   st     ot     in l for each triple  s t   s t     ot      t      in l   and vise
versa   hence both lohmms assign the same overall transition probability 
l and l  differ only in the way the initialize sequences h s t   s t     ot   it        t  resp 
h st   st     ot   it        t    whereas l starts in some state s  and makes a transition to s 
emitting o    the moore lohmm l  is supposed to emit a symbol o  in s   before making a
transition to s     we compensate for this using the prior distribution  the existence of the
correct prior distribution for l  can be seen as follows  in l  there are only finitely many
states reachable at time t      i e  pl  q    s      holds for only a finite set of ground
states s  the probability pl  q    s  can be computed similar to    s   we set t     in line
   neglecting the condition on ot  in line     and dropping  ot    ob h   from line    
completely listing all states s  s  together with pl  q    s   i e   pl  q    s    s  start  
constitutes the prior distribution of l   
the argumentation basically followed the approach to transform a mealy machine into
a moore machine  see e g   hopcroft and ullman         furthermore  the mapping of a
moore lohmm  as introduced in the present section  into a mealy lohmm is straightforward 

appendix c  proof of theorem  
let t be a terminal alphabet and n a nonterminal alphabet  a probabilistic context free
grammar  pcfg  g consists of a distinguished start symbol s  n plus a finite set of

productions
p of the form p   x    where x  n      n  t   and p          for all
x  n    x p      a pcfg defines a stochastic process with sentential forms as states 
and leftmost rewriting steps as transitions  we denote a single rewriting operation of the
grammar by a single arrow   if as a result of one ore more rewriting operations we are
able to rewrite    n  t   as a sequence    n  t   of nonterminals and terminals 
then we write     the probability of this rewriting is the product of all probability
   

fikersting  de raedt    raiko

values associated to productions used in the derivation  we assume g to be consistent  i e  
that the sum of all probabilities of derivations s   such that   t  sum to     
we can assume that the pcfg g is in greibach normal form  this follows from abney
et al s        theorem   because g is consistent  thus  every production p  g is of
the form p   x  ay        yn for some n     in order to encode g as a lohmm m   we
introduce     for each non terminal symbol x in g a constant symbol nx and     for each
terminal symbol t in g a constant symbol t  for each production p  g  we include an
a
abstract transition of the form p   stack  ny            nyn  s   
 stack  nx s    if n      and
a
p   stack s  
 stack  nx s    if n      furthermore  we include       stack  s    start
end
and       end  stack      it is now straightforward to prove by induction that m and g
are equivalent 


appendix d  logical hidden markov model for unix command
sequences
the lohmms described below model unix command sequences triggered by mkdir  to
this aim  we transformed the original greenberg data into a sequence of logical atoms over
com  mkdir dir  lastcom   ls dir  lastcom   cd dir  dir  lastcom   cp dir  dir  lastcom 
and mv dir  dir  lastcom   the domain of lastcom was  start  com  mkdir  ls  cd  cp  mv  
the domain of dir consisted of all argument entries for mkdir  ls  cd  cp  mv in the original
dataset  switches  pipes  etc  were neglected  and paths were made absolute  this yields
    constants in the domain of dir  all original commands  which were not mkdir  ls  cd 
cp  or mv  were represented as com  if mkdir did not appear within    time steps before a
command c   ls  cd  cp mv   c was represented as com  overall  this yields more than
       ground states that have to be covered by a markov model 
the unification lohmm u basically implements a second order markov model  i e  
the probability of making a transition depends upon the current state and the previous
state  it has     parameters and the following structure 
com  start 
mkdir dir  start   start 

com  com 
mkdir dir  com   com 
end  com 

furthermore  for each c   start  com  there are
mkdir dir  com 
mkdir    com 
com
end
ls dir  mkdir 
ls    mkdir 
cd dir  mkdir 









mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  

cd    mkdir 
cp    dir  mkdir 
cp dir    mkdir 
cp      mkdir 
mv    dir  mkdir 
mv dir    mkdir 
mv      mkdir 

   









mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  
mkdir dir c  

filogical hidden markov models

together with for each c   mkdir  ls  cd  cp  mv  and for each c     cd  ls   resp 
c    cp  mv  
mkdir dir  com 
mkdir    com 
com
end
ls dir c   
ls   c   
cd dir c   
cd   c   
cp    dir c   
cp dir   c   
cp     c   
mv    dir c   
mv dir   c   
mv     c   
















c   dir c   mkdir    com  
com 
c   dir c  
c   dir c  
end 
ls from c    
c   dir c  
ls to c    
c   dir c  
c   dir c  
ls   c    
c   dir c  
cd from c    
c   dir c  
cd to c    
c   dir c  
cd   c    
c   dir c   cp from   c    
c   dir c  
cp    to c    
c   dir c  
cp     c    
c   dir c   mv from   c    
mv    to c    
c   dir c  
mv     c    

c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  
c   from  to c  

because all states are fully observable  we omitted the output symbols associated with
clauses  and  for the sake of simplicity  we omitted associated probability values 
the no unification lohmm n is the variant of u where no variables were shared
such as
mkdir    com   cp from  to c  
ls 
com  cp from  to c  
cd 
end  cp from  to c   cp   
mv   

  cp 
  cp 
  cp 
  cp 






cp from  to c  
cp from  to c  
cp from  to c  
cp from  to c  

because only transitions are affected  n has     parameters less than u   i e       

appendix e  tree based lohmm for mrna sequences
the lohmm processes the nodes of mrna trees in in order  the structure of the lohmm
is shown at the end of the section  there are copies of the shaded parts  terms are
abbreviated using their starting alphanumerical  tr stands for tree  he for helical  si for
single  nuc for nucleotide  and nuc p for nucleotide pair 
the domain of  children covers the maximal branching factor found in the data  i e  
  c    c  c            c  c  c  c  c  c  c  c  c    the domain of type consists of all types occurring in
the data  i e    stem  single  bulge   bulge   hairpin   and for size  the domain covers
the maximal length of a secondary structure element in the data  i e   the longest sequence
of consecutive bases respectively base pairs constituting a secondary structure element 
the length was encoded as  n       n               n        where nm     denotes the recursive
application of the functor n m times  for base and basepair   the domains were the   bases
respectively the    base pairs  in total  there are     parameters 
   

fimy start
      root    root  x 

copies for tr id   c    pa   c  r    tr id   c  c    pa   c  r   
and tr id   c  c  c    pa   c  r  
       he s id   pa      t  l 
tr id     pa   c  r  

tr    x      x  

and tr id   c  c  c    pa   c   c  cs  r  

       he s id   pa  b  t  l 

       he s id   pa      t  l 
       he s id   pa  b  t  l 
tr id     pa   c   c  cs  r  
       si s id   pa  b  t  l 
       si s id   pa      t  l 

       si s id   pa  b  t  l 
       si s id   pa      t  l 
se t  l  s id   b   s id   b r  

se t  l  s id       r 

copies for tr id   c    pa   c   c  cs  r    tr id   c  c    pa   c   c  cs  r   

tree
model

se t  l  s id        pa   c  cs  r   se t  l  s id   b   s id   b  pa   c  cs  r  

copies for each type single  bulge   bulge 
copies for n n     and n n n     

copies for each length of sequence n n      n n n       n n n n      
se stem  n a   id  b  s 

se hairpin  n a   id  b  s 

copies for nuc p a  g           nuc p u  u 

copies for nuc g   nuc c   and nuc u 
       nuc a 

         nuc p a  a 

se hairpin  a  id  b  s 

se hairpin  n     id  b  s 

se stem  a  id  b  s 

se hairpin  n     s          

       nuc a 

se stem  n     s          

         nuc p a  a 

       nuc a 

se stem  n     id  b  s 

         nuc p a  a 

copies for nuc p a  g           nuc p u  u 

copies for nuc g   nuc c   and nuc u 
end

tr id  b  s 

sequence
model

kersting  de raedt    raiko

figure    the mrna lohmm structure  the symbol denotes anonymous variables which
are read and treated as distinct  new variables each time they are encountered 
there are copies of the shaded part  terms are abbreviated using their starting
alphanumerical  tr stands for tree  se for structure element  he for helical 
si for single  nuc for nucleotide  and nuc p for nucleotide pair 

references

   

abney  s          stochastic attribute value grammars  computational linguistics         
       

   

start

filogical hidden markov models

abney  s   mcallester  d     pereira  f          relating probabilistic grammars and automata  in proceedings of   th annual meeting of the association for computational
linguistics  acl        pp          morgan kaufmann 
anderson  c   domingos  p     weld  d          relational markov models and their application to adaptive web navigation  in proceedings of the eighth international
conference on knowledge discovery and data mining  kdd        pp         edmonton  canada  acm press 
baker  j          trainable grammars for speech recognition  in speech communication
paper presented at th   th meeting of the acoustical society of america  pp        
boston  ma 
bauer  h          wahrscheinlichkeitstheorie     edition   walter de gruyter  berlin  new
york 
baum  l          an inequality and associated maximization technique in statistical estimation for probabilistic functions of markov processes  inequalities        
bohnebeck  u   horvath  t     wrobel  s          term comparison in first order similarity
measures  in proceedings of the eigth international conference on inductive logic
programming  ilp      vol       of lncs  pp        springer 
bresnan  j          lexical functional syntax  blackwell  malden  ma 
carrasco  r   oncina  j     calera rubio  j          stochastic inference of regular tree
languages  machine learning                   
chandonia  j   hon  g   walker  n   lo conte  l   p koehl    brenner  s          the
astral compendium in       nucleic acids research      d   d    
davison  b     hirsh  h          predicting sequences of user actions  in predicting the
future  ai approaches to time series analysis  pp       aaai press 
de raedt  l     kersting  k          probabilistic logic learning  acm sigkdd explorations  special issue on multi relational data mining              
de raedt  l     kersting  k          probabilistic inductive logic programming  in
ben david  s   case  j     maruoka  a   eds    proceedings of the   th international
conference on algorithmic learning theory  alt        vol       of lncs  pp 
     padova  italy  springer 
durbin  r   eddy  s   krogh  a     mitchison  g          biological sequence analysis 
probabilistic models of proteins and nucleic acids  cambridge university press 
dzeroski  s     lavrac  n   eds            relational data mining  springer verlag  berlin 
eddy  s     durbin  r          rna sequence analysis using covariance models  nucleic
acids res                     
   

fikersting  de raedt    raiko

eisele  a          towards probabilistic extensions of contraint based grammars  in
dorne  j   ed    computational aspects of constraint based linguistics decription ii 
dyna   deliverable r    b 
fine  s   singer  y     tishby  n          the hierarchical hidden markov model  analysis
and applications  machine learning           
frasconi  p   soda  g     vullo  a          hidden markov models for text categorization
in multi page documents  journal of intelligent information systems             
friedman  n   getoor  l   koller  d     pfeffer  a          learning probabilistic relational
models  in proceedings of sixteenth international joint conference on artificial intelligence  ijcai        pp            morgan kaufmann 
fristedt  b     gray  l          a modern approach to probability theory  probability and
its applications  birkhauser boston 
ghahramani  z     jordan  m          factorial hidden markov models  machine learning 
           
goodman  j          probabilistic feature grammars  in proceedings of the fifth international workshop on parsing technologies  iwpt     boston  ma  usa 
greenberg  s          using unix  collected traces of     users  tech  rep   dept  of
computer science  university of calgary  alberta 
hopcroft  j     ullman  j          introduction to automata theory  languages  and
computation  addison wesley publishing company 
horvath  t   wrobel  s     bohnebeck  u          relational instance based learning with
lists and terms  machine learning                 
hubbard  t   murzin  a   brenner  s     chotia  c          scop   a structural classification
of proteins database  nar                 
jacobs  n     blockeel  h          the learning shell  automated macro construction  in
user modeling       pp       
jaeger  m          relational bayesian networks  in proceedings of the thirteenth conference on uncertainty in artificial intelligence  uai   pp          morgan kaufmann 
katz  s          estimation of probabilities from sparse data for hte language model component of a speech recognizer  ieee transactions on acoustics  speech  and signal
processing  assp              
kersting  k     de raedt  l       a   adaptive bayesian logic programs  in rouveirol 
c     sebag  m   eds    proceedings of the   th international conference on inductive
logic programming  ilp      vol       of lnai  pp          springer 
   

filogical hidden markov models

kersting  k     de raedt  l       b   towards combining inductive logic programming
with bayesian networks  in rouveirol  c     sebag  m   eds    proceedings of the
  th international conference on inductive logic programming  ilp      vol      
of lnai  pp          springer 
kersting  k     raiko  t          say em for selecting probabilistic models for logical
sequences  in bacchus  f     jaakkola  t   eds    proceedings of the   st conference
on uncertainty in artificial intelligence  uai       pp         edinburgh  scotland 
kersting  k   raiko  t   kramer  s     de raedt  l          towards discovering structural signatures of protein folds based on logical hidden markov models  in altman 
r   dunker  a   hunter  l   jung  t     klein  t   eds    proceedings of the pacific symposium on biocomputing  psb      pp         kauai  hawaii  usa  world
scientific 
koivisto  m   kivioja  t   mannila  h   rastas  p     ukkonen  e          hidden markov
modelling techniques for haplotype analysis  in ben david  s   case  j     maruoka 
a   eds    proceedings of   th international conference on algorithmic learning theory  alt      vol       of lncs  pp        springer 
koivisto  m   perola  m   varilo  t   hennah  w   ekelund  j   lukk  m   peltonen  l  
ukkonen  e     mannila  h          an mdl method for finding haplotype blocks
and for estimating the strength of haplotype block boundaries  in altman  r   dunker 
a   hunter  l   jung  t     klein  t   eds    proceedings of the pacific symposium
on biocomputing  psb      pp          world scientific 
korvemaker  b     greiner  r          predicting unix command files  adjusting to user
patterns  in adaptive user interfaces  papers from the      aaai spring symposium 
pp       
kulp  d   haussler  d   reese  m     eeckman  f          a generalized hidden markov
model for the recognition of human genes in dna  in states  d   agarwal  p  
gaasterland  t   hunter  l     smith  r   eds    proceedings of the fourth international conference on intelligent systems for molecular biology  ismb      pp     
    st  louis  mo  usa  aaai 
lane  t          hidden markov models for human computer interface modeling  in
rudstrom  a   ed    proceedings of the ijcai    workshop on learning about users 
pp       stockholm  sweden 
lari  k     young  s          the estimation of stochastic context free grammars using the
inside outside algorithm  computer speech and language          
levy  l     joshi  a          skeletal structural descriptions  information and control 
              
mclachlan  g     krishnan  t          the em algorithm and extensions  wiley  new
york 
   

fikersting  de raedt    raiko

mitchell  t  m          machine learning  the mcgraw hill companies  inc 
muggleton  s          stochastic logic programs  in de raedt  l   ed    advances in
inductive logic programming  pp          ios press 
muggleton  s     de raedt  l          inductive logic programming  theory and methods 
journal of logic programming                  
ngo  l     haddawy  p          answering queries from context sensitive probabilistic
knowledge bases  theoretical computer science              
pollard  c     sag  i          head driven phrase structure grammar  the university of
chicago press  chicago 
rabiner  l     juang  b          an introduction to hidden markov models  ieee assp
magazine             
riezler  s          statistical inference and probabilistic modelling for constraint based
nlp  in schrder  b   lenders  w     und t  portele  w  h   eds    proceedings of
the  th conference on natural language processing  konvens      also as corr
cs cl         
sakakibara  y          efficient learning of context free grammars from positive structural
examples  information and computation               
sakakibara  y          pair hidden markov models on tree structures  bioinformatics 
    suppl     i   i    
sakakibara  y   brown  m   hughey  r   mian  i   sjolander  k     underwood  r         
stochastic context free grammars for trna modelling  nucleic acids research 
                  
sanghai  s   domingos  p     weld  d          dynamic probabilistic relational models 
in gottlob  g     walsh  t   eds    proceedings of the eighteenth international joint
conference on artificial intelligence  ijcai      pp         acapulco  mexico  morgan kaufmann 
sato  t     kameya  y          parameter learning of logic programs for symbolic statistical
modeling  journal of artificial intelligence research  jair              
scholkopf  b     warmuth  m   eds            learning and parsing stochastic unificationbased grammars  vol       of lncs  springer 
turcotte  m   muggleton  s     sternberg  m          the effect of relational background
knowledge on learning of protein three dimensional fold signatures  machine learning 
               
won  k   prugel bennett  a     krogh  a          the block hidden markov model for biological sequence analysis  in negoita  m   howlett  r     jain  l   eds    proceedings
of the eighth international conference on knowledge based intelligent information
and engineering systems  kes      vol       of lncs  pp        springer 

   

fi