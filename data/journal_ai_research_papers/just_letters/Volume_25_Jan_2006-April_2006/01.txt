journal of artificial intelligence research                

submitted        published      

decision theoretic planning with non markovian rewards
sylvie thiebaux
charles gretton
john slaney
david price

sylvie thiebaux anu edu au
charles gretton anu edu au
john slaney anu edu au
david price anu edu au

national ict australia  
the australian national university
canberra  act       australia

froduald kabanza

kabanza usherbrooke ca

departement dinformatique
universite de sherbrooke
sherbrooke  quebec j k  r   canada

abstract
a decision process in which rewards depend on history rather than merely on the current state is called a decision process with non markovian rewards  nmrdp   in decisiontheoretic planning  where many desirable behaviours are more naturally expressed as properties of execution sequences rather than as properties of states  nmrdps form a more
natural model than the commonly adopted fully markovian decision process  mdp  model 
while the more tractable solution methods developed for mdps do not directly apply in the
presence of non markovian rewards  a number of solution methods for nmrdps have been
proposed in the literature  these all exploit a compact specification of the non markovian
reward function in temporal logic  to automatically translate the nmrdp into an equivalent mdp which is solved using efficient mdp solution methods  this paper presents
nmrdpp non markovian reward decision process planner   a software platform for the
development and experimentation of methods for decision theoretic planning with nonmarkovian rewards  the current version of nmrdpp implements  under a single interface 
a family of methods based on existing as well as new approaches which we describe in detail  these include dynamic programming  heuristic search  and structured methods  using
nmrdpp  we compare the methods and identify certain problem features that affect their
performance  nmrdpps treatment of non markovian rewards is inspired by the treatment
of domain specific search control knowledge in the tlplan planner  which it incorporates
as a special case  in the first international probabilistic planning competition  nmrdpp
was able to compete and perform well in both the domain independent and hand coded
tracks  using search control knowledge in the latter 

c
    
ai access foundation  all rights reserved 

fithiebaux  gretton  slaney  price   kabanza

   introduction
    the problem
markov decision processes  mdps  are now widely accepted as the preferred model for
decision theoretic planning problems  boutilier  dean    hanks         the fundamental
assumption behind the mdp formulation is that not only the system dynamics but also the
reward function are markovian  therefore  all information needed to determine the reward
at a given state must be encoded in the state itself 
this requirement is not always easy to meet for planning problems  as many desirable
behaviours are naturally expressed as properties of execution sequences  see e g   drummond        haddawy   hanks        bacchus   kabanza        pistore   traverso 
       typical cases include rewards for the maintenance of some property  for the periodic
achievement of some goal  for the achievement of a goal within a given number of steps
of the request being made  or even simply for the very first achievement of a goal which
becomes irrelevant afterwards 
for instance  consider a health care robot which assists ederly or disabled people by
achieving simple goals such as reminding them to do important tasks  e g  taking a pill  
entertaining them  checking or transporting objects for them  e g  checking the stoves
temperature or bringing coffee   escorting them  or searching  e g  for glasses or for the
nurse   cesta et al          in this domain  we might want to reward the robot for making
sure a given patient takes his pill exactly once every   hours  and penalise it if it fails
to prevent the patient from doing this more than once within this time frame    we may
reward it for repeatedly visiting all rooms in the ward in a given order and reporting any
problem it detects  it may also receive a reward once for each patients request answered
within the appropriate time frame  etc  another example is the elevator control domain
 koehler   schuster         in which an elevator must get passengers from their origin to
their destination as efficiently as possible  while attempting to satisfying a range of other
conditions such as providing priority services to critical customers  in this domain  some
trajectories of the elevator are more desirable than others  which makes it natural to encode
the problem by assigning rewards to those trajectories 
a decision process in which rewards depend on the sequence of states passed through
rather than merely on the current state is called a decision process with non markovian
rewards  nmrdp   bacchus  boutilier    grove         a difficulty with nmrdps is that
the most efficient mdp solution methods do not directly apply to them  the traditional way
to circumvent this problem is to formulate the nmrdp as an equivalent mdp  whose states
result from augmenting those of the original nmrdp with extra information capturing
enough history to make the reward markovian  hand crafting such an mdp can however
be very difficult in general  this is exacerbated by the fact that the size of the mdp
impacts the effectiveness of many solution methods  therefore  there has been interest
in automating the translation into an mdp  starting from a natural specification of nonmarkovian rewards and of the systems dynamics  bacchus et al         bacchus  boutilier 
  grove         this is the problem we focus on 

  

fidecision theoretic planning with non markovian rewards

    existing approaches
when solving nmrdps in this setting  the central issue is to define a non markovian reward
specification language and a translation into an mdp adapted to the class of mdp solution
methods and representations we would like to use for the type of problems at hand  more
precisely  there is a tradeoff between the effort spent in the translation  e g  in producing a
small equivalent mdp without many irrelevant history distinctions  and the effort required
to solve it  appropriate resolution of this tradeoff depends on the type of representations
and solution methods envisioned for the mdp  for instance  structured representations and
solution methods which have some ability to ignore irrelevant information may cope with a
crude translation  while state based  flat  representations and methods will require a more
sophisticated translation producing an mdp as small as feasible 
both the two previous proposals within this line of research rely on past linear temporal
logic  pltl  formulae to specify the behaviours to be rewarded  bacchus et al               
a nice feature of pltl is that it yields a straightforward semantics of non markovian
rewards  and lends itself to a range of translations from the crudest to the finest  the two
proposals adopt very different translations adapted to two very different types of solution
methods and representations  the first  bacchus et al         targets classical state based
solution methods such as policy iteration  howard        which generate complete policies
at the cost of enumerating all states in the entire mdp  consequently  it adopts an expensive
translation which attempts to produce a minimal mdp  by contrast  the second translation
 bacchus et al         is very efficient but crude  and targets structured solution methods
and representations  see e g   hoey  st aubin  hu    boutilier        boutilier  dearden   
goldszmidt        feng   hansen         which do not require explicit state enumeration 
    a new approach
the first contribution of this paper is to provide a language and a translation adapted to
another class of solution methods which have proven quite effective in dealing with large
mdps  namely anytime state based heuristic search methods such as lao   hansen  
zilberstein         lrtdp  bonet   geffner         and ancestors  barto  bardtke   
singh        dean  kaelbling  kirman    nicholson        thiebaux  hertzberg  shoaff 
  schneider         these methods typically start with a compact representation of the
mdp based on probabilistic planning operators  and search forward from an initial state 
constructing new states by expanding the envelope of the policy as time permits  they may
produce an approximate and even incomplete policy  but explicitly construct and explore
only a fraction of the mdp  neither of the two previous proposals is well suited to such
solution methods  the first because the cost of the translation  most of which is performed
prior to the solution phase  annihilates the benefits of anytime algorithms  and the second
because the size of the mdp obtained is an obstacle to the applicability of state based
methods  since here both the cost of the translation and the size of the mdp it results
in will severely impact on the quality of the policy obtainable by the deadline  we need an
appropriate resolution of the tradeoff between the two 
our approach has the following main features  the translation is entirely embedded in
the anytime solution method  to which full control is given as to which parts of the mdp
will be explicitly constructed and explored  while the mdp obtained is not minimal  it
  

fithiebaux  gretton  slaney  price   kabanza

is of the minimal size achievable without stepping outside of the anytime framework  i e  
without enumerating parts of the state space that the solution method would not necessarily
explore  we formalise this relaxed notion of minimality  which we call blind minimality in
reference to the fact that it does not require any lookahead  beyond the fringe   this is
appropriate in the context of anytime state based solution methods  where we want the
minimal mdp achievable without expensive pre processing 
when the rewarding behaviours are specified in pltl  there does not appear to be a
way of achieving a relaxed notion of minimality as powerful as blind minimality without
a prohibitive translation  therefore instead of pltl  we adopt a variant of future linear
temporal logic  fltl  as our specification language  which we extend to handle rewards 
while the language has a more complex semantics than pltl  it enables a natural translation into a blind minimal mdp by simple progression of the reward formulae  moreover 
search control knowledge expressed in fltl  bacchus   kabanza        fits particularly
nicely in this framework  and can be used to dramatically reduce the fraction of the search
space explored by the solution method 
    a new system
our second contribution is nmrdpp  the first reported implementation of nmrdp solution
methods  nmrdpp is designed as a software platform for their development and experimentation under a common interface  given a description of the actions in a domain  nmrdpp
lets the user play with and compare various encoding styles for non markovian rewards
and search control knowledge  various translations of the resulting nmrdp into mdp  and
various mdp solution methods  while solving the problem  it can be made to record a
range of statistics about the space and time behaviour of the algorithms  it also supports
the graphical display of the mdps and policies generated 
while nmrdpps primary interest is in the treatment of non markovian rewards  it is
also a competitive platform for decision theoretic planning with purely markovian rewards 
in the first international probabilistic planning competition  nmrdpp was able to enrol
in both the domain independent and hand coded tracks  attempting all problems featuring
in the contest  thanks to its use of search control knowledge  it scored a second place
in the hand coded track which featured probabilistic variants of blocks world and logistics
problems  more surprisingly  it also scored second in the domain independent subtrack consisting of all problems that were not taken from the blocks world and logistic domains  most
of these latter problems had not been released to the participants prior to the competition 
    a new experimental analysis
our third contribution is an experimental analysis of the factors that affect the performance
of nmrdp solution methods  using nmrdpp  we compared their behaviours under the
influence of parameters such as the structure and degree of uncertainty in the dynamics 
the type of rewards and the syntax used to described them  reachability of the conditions
tracked  and relevance of rewards to the optimal policy  we were able to identify a number
of general trends in the behaviours of the methods and provide advice concerning which
are best suited in certain circumstances  our experiments also lead us to rule out one of

  

fidecision theoretic planning with non markovian rewards

the methods as systematically underperforming  and to identify issues with the claim of
minimality made by one of the pltl approaches 
    organisation of the paper
the paper is organised as follows  section   begins with background material on mdps 
nmrdps  and existing approaches  section   describes our new approach and section  
presents nmrdpp  sections   and   report our experimental analysis of the various approaches  section   explains how we used nmrdpp in the competition  section   concludes
with remarks about related and future work  appendix b gives the proofs of the theorems 
most of the material presented is compiled from a series of recent conference and workshop
papers  thiebaux  kabanza    slaney      a      b  gretton  price    thiebaux      a 
    b   details of the logic we use to represent rewards may be found in our      paper
 slaney        

   background
    mdps  nmrdps  equivalence
we start with some notation and definitions  given a finite set s of states  we write
s  for the set of finite sequences of states over s  and s  for the set of possibly infinite
state sequences  where  stands for a possibly infinite state sequence in s  and i is a
natural number  by i  we mean the state of index i in   by  i  we mean the prefix
h            i i  s  of       denotes the concatenation of   s  and    s   
      mdps
a markov decision process of the type we consider is a   tuple hs  s    a  pr  ri  where s is
a finite set of fully observable states  s   s is the initial state  a is a finite set of actions
 a s  denotes the subset of actions applicable in s  s    pr s  a      s  s  a  a s   is a
family of probability distributions over s  such that pr s  a  s    is the probability of being
in state s  after performing action a in state s  and r   s   ir is a reward function such
that r s  is the immediate reward for being in state s  it is well known that such an mdp
can be compactly represented using dynamic bayesian networks  dean   kanazawa       
boutilier et al         or probabilistic extensions of traditional planning languages  see e g  
kushmerick  hanks    weld        thiebaux et al         younes   littman        
a stationary policy for an mdp is a function    s   a  such that  s   a s  is
the action to be executed in state s  the value v of the policy at s    which we seek to
maximise  is the sum of the expected future rewards over an infinite horizon  discounted by
how far into the future they occur 
v  s      lim e
n

x
n

i



 r i           s 

i  

where         is the discount factor controlling the contribution of distant rewards 

  

fithiebaux  gretton  slaney  price   kabanza

                             
        
   
    
   
    
 

                     
   
           
 
    
                   

 
r

s

                                  
    
       
   
   
  

in the initial state s    p is false and two actions are
possible  a causes a transition to s  with probability
     and no change with probability      while for b the
transition probabilities are      in state s    p is true 
and actions c and d  stay and go  lead to s  and s 
respectively with probability   
a reward is received the first time p is true  but not
subsequently  that is  the rewarded state sequences are 
hs    s  i
hs    s    s  i
hs    s    s    s  i
hs    s    s    s    s  i
etc 

 

  
  
   
   
   
  
    
   
 
 
 
 
 
       
 
 
 
 
 
 
 
 
 
 
                                  
                                
   
   
   
   
   
   
    
 
  
  
  
  
   
  
   
 
  
   
 
    
  
                               
           
           
     
 
           
 
    
                   

a

   

b
  

d    

   

   

   

 s  
i     
 
   
  
c

     
   
    
            

   
  

                      

figure    a simple nmrdp
      nmrdps
a decision process with non markovian rewards is identical to an mdp except that the
domain of the reward function is s    the idea is that if the process has passed through
state sequence  i  up to stage i  then the reward r  i   is received at stage i  figure  
gives an example  like the reward function  a policy for an nmrdp depends on history 
and is a mapping from s  to a  as before  the value of policy  is the expectation of the
discounted cumulative reward over an infinite horizon 
x

n
i
v  s      lim e
 r  i           s 
n

i  

e
for a decision process d   hs  s    a  pr  ri and a state s  s  we let d s 
stand for
the set of state sequences rooted at s that are feasible under the actions in d  that is 
e
d s 
     s        s and i a  a i   pr i   a  i           note that the definition
e
of d s  does not depend on r and therefore applies to both mdps and nmrdps 
      equivalence
the clever algorithms developed to solve mdps cannot be directly applied to nmrdps 
one way of dealing with this problem is to translate the nmrdp into an equivalent mdp
with an expanded state space  bacchus et al          the expanded states in this mdp
 e states  for short  augment the states of the nmrdp by encoding additional information
sufficient to make the reward history independent  for instance  if we only want to reward
the very first achievement of goal g in an nmrdp  the states of an equivalent mdp would
carry one extra bit of information recording whether g has already been true  an e state can
be seen as labelled by a state of the nmrdp  via the function  in definition   below  and
by history information  the dynamics of nmrdps being markovian  the actions and their
probabilistic effects in the mdp are exactly those of the nmrdp  the following definition 
adapted from that given by bacchus et al          makes this concept of equivalent mdp
precise  figure   gives an example 

  

fidecision theoretic planning with non markovian rewards


                              
       
  
    
   
    
 



                                 
    
       
   
   
  

                   
    
 
            
           
  
  
                
   
  
   
   
    
   
 
 
 
 
 
       
 
 
 
 
 
 
 
 
 
 
 
 
 
                              
                                
 
   
   
   
   
   
  
    
 
  
  
  
  
   
 
  
   
 
 
   
 
 
    
  
                              
 
           
                      
   
 
 
 
          
               

 
r   
s

a

   

   

b
  

d    

 s  

c    
i
 
    
 
 
     
   
    
           

   

   

                      
           
      
   
   
   
    
 
 
  
                                   
 
 
 
 
 
  
 
 
 
 
 
 
          
  
     
 
 
 
 
 
 
 
 
 
 
 
 
   
 
   
 
 
 
 
   
                    
                    
   
   
 
 
            
            
           
           
                
               
   
    
      
 
    
      
        
   
                                          
   
          
 
  
   
   
  
  
   
 
 
 
    
 
 
 
 
       
                                 

   

   

              

d

b

 

c

 

s

a

o

   
  
  
    
            

   

s

 

 
i

   

figure    an mdp equivalent to the nmrdp in figure      s         s       s      s      
  s       s    the initial state is s     state s   is rewarded  the other states are not 

definition   mdp d  hs     s     a    pr   r  i is equivalent to nmrdp d   hs  s    a  pr  ri if
there exists a mapping    s     s such that  
     s       s   
   for all s   s     a   s      a   s     
   for all s    s   s  if there is a  a s    such that pr s    a  s         then for all s    s  
such that   s       s    there exists a unique s    s       s       s    such that for all
a   a   s      pr   s     a    s       pr s    a    s    
e     and    d
e    s    such that         i for
   for any feasible state sequence   d s
 
i
all i  we have  r    i     r  i   for all i 
items    ensure that there is a bijection between feasible state sequences in the nmrdp
and feasible e state sequences in the mdp  therefore  a stationary policy for the mdp can be
reinterpreted as a non stationary policy for the nmrdp  furthermore  item   ensures that
the two policies have identical values  and that consequently  solving an nmrdp optimally
reduces to producing an equivalent mdp and solving it optimally  bacchus et al         
proposition   let d be an nmrdp  d  an equivalent mdp for it  and    a policy for
e     by   i             
d    let  be the function defined on the sequence prefixes  i   d s
i
 
where for all j  i   j     j   then  is a policy for d such that v  s      v   s     
   technically  the definition allows the sets of actions a and a  to be different  but any action in which
they differ must be inapplicable in reachable states in the nmrdp and in all e states in the equivalent
mdp  for all practical purposes  a and a  can be seen as identical 

  

fithiebaux  gretton  slaney  price   kabanza

    existing approaches
both existing approaches to nmrdps  bacchus et al               use a temporal logic of
the past  pltl  to compactly represent non markovian rewards and exploit this compact
representation to translate the nmrdp into an mdp amenable to off the shelf solution
methods  however  they target different classes of mdp representations and solution methods  and consequently  adopt different styles of translations 
bacchus et al         target state based mdp representations  the equivalent mdp
is first generated entirely  this involves the enumeration of all e states and all transitions
between them  then  it is solved using traditional dynamic programming methods such as
value or policy iteration  because these methods are extremely sensitive to the number of
states  attention is paid to producing a minimal equivalent mdp  with the least number of
states   a first simple translation which we call pltlsim produces a large mdp which can
be post processed for minimisation before being solved  another  which we call pltlmin 
directly results in a minimal mdp  but relies on an expensive pre processing phase 
the second approach  bacchus et al          which we call pltlstr  targets structured
mdp representations  the transition model  policies  reward and value functions are represented in a compact form  e g  as trees or algebraic decision diagrams  adds   hoey et al  
      boutilier et al          for instance  the probability of a given proposition  state
variable  being true after the execution of an action is specified by a tree whose internal
nodes are labelled with the state variables on whose previous values the given variable depends  whose arcs are labelled by the possible previous values    or   of these variables 
and whose leaves are labelled with probabilities  the translation amounts to augmenting
the structured mdp with new temporal variables tracking the relevant properties of state
sequences  together with the compact representation of     their dynamics  e g  as trees over
the previous values of relevant variables  and     of the non markovian reward function in
terms of the variables current values  then  structured solution methods such as structured
policy iteration or the spudd algorithm are run on the resulting structured mdp  neither
the translation nor the solution methods explicitly enumerates the states 
we now review these approaches in some detail  the reader is referred to the respective
papers for additional information 
      representing rewards with pltl
the syntax of pltl  the language chosen to represent rewarding behaviours  is that of
propositional logic  augmented with the operators   previously  and s  since   see emerson         whereas a classical propositional logic formula denotes a set of states  a subset
of s   a pltl formula denotes a set of finite sequences of states  a subset of s     a formula
without temporal modality expresses a property that must be true of the current state  i e  
the last state of the finite sequence  f specifies that f holds in the previous state  the
state one before the last   f  s f    requires f  to have been true at some point in the sequence  and  unless that point is the present  f  to have held ever since  more formally  the
modelling relation    stating whether a formula f holds of a finite sequence  i  is defined
recursively as follows 
  i     p iff p  i   for p  p  the set of atomic propositions

  

fidecision theoretic planning with non markovian rewards

  i     f iff  i      f
  i     f   f  iff  i     f  and  i     f 
  i     f iff i     and  i        f
  i     f  s f  iff j  i   j     f  and k  j   k  i   k     f 
  f    s f meaning that f has been true at
from s  one can define the useful operators 
  f meaning that f has always been true  e g  g    
  g denotes
some point  and fif  
the set of finite sequences ending in a state where g is true for the first time in the sequence 
  k f for
other useful abbreviation are k  k times ago   for k iterations of the  modality  
ki   i f  f was true at some of the k last steps   and fik f for ki   i f  f was true at all
the k last steps  
non markovian reward functions are described with a set of pairs  fi   ri   where fi is
a pltl reward formula and ri is a real  with the semantics that the reward assigned to a
sequence in s  is the sum of the ri s for which that sequence is a model of fi   below  we let
f denote the set of reward formulae fi in the description of the reward function  bacchus
et al         give a list of behaviours which it might be useful to reward  together with
their expression in pltl  for instance  where f is an atemporal formula   f   r  rewards
with r units the achievement of f whenever it happens  this is a markovian reward  in
  f   r  rewards every state following  and including  the achievement of f   while
contrast  
  f   r  only rewards the first occurrence of f    f fik f   r  rewards the occurrence
 f  
of f at most once every k steps   n      r  rewards the nth state  independently of
its properties     f   f   f    r  rewards the occurrence of f  immediately followed by
f  and then f    in reactive planning  so called response formulae which describe that the
achievement of f is triggered by a condition  or command  c are particularly useful  these
  c   r  if every state in which f is true following the first issue of
can be written as  f  
the command is to be rewarded  alternatively  they can be written as  f   f s c    r  if
only the first occurrence of f is to be rewarded after each command  it is common to only
  k c   r 
reward the achievement f within k steps of the trigger  we write for example  f  
to reward all such states in which f holds 
from a theoretical point of view  it is known  lichtenstein  pnueli    zuck        that
the behaviours representable in pltl are exactly those corresponding to star free regular
languages  non star free behaviours such as  pp   reward an even number of states all
containing p  are therefore not representable  nor  of course  are non regular behaviours
such as pn q n  e g  reward taking equal numbers of steps to the left and right   we shall not
speculate here on how severe a restriction this is for the purposes of planning 
      principles behind the translations
all three translations into an mdp  pltlsim  pltlmin  and pltlstr  rely on the equivalence f  s f   f    f    f  s f      with which we can decompose temporal modalities
into a requirement about the last state i of a sequence  i   and a requirement about the
prefix  i     of the sequence  more precisely  given state s and a formula f   one can com 

  

fithiebaux  gretton  slaney  price   kabanza

pute in  o   f     a new formula reg f  s  called the regression of f through s  regression
has the property that  for i      f is true of a finite sequence  i  ending with i   s iff
reg f  s  is true of the prefix  i      that is  reg f  s  represents what must have been
true previously for f to be true now  reg is defined as follows 
 reg p  s      iff p  s and  otherwise  for p  p
 reg f  s    reg f  s 
 reg f   f    s    reg f    s   reg f    s 
 reg f  s    f
 reg f  s f    s    reg f    s    reg f    s    f  s f    
for instance  take a state s in which p holds and q does not  and take f    q    p s q  
meaning that q must have been false   step ago  but that it must have held at some point
in the past and that p must have held since q last did  reg f  s    q   p s q   that is 
for f to hold now  then at the previous stage  q had to be false and the p s q requirement
still had to hold  when p and q are both false in s  then reg f  s      indicating that f
cannot be satisfied  regardless of what came earlier in the sequence 
for notational convenience  where x is a set of formulae we write x for x x   x  x  
now the translations exploit the pltl representation of rewards as follows  each expanded
state  e state  in the generated mdp can be seen as labelled with a set   sub f   of
subformulae of the reward formulae in f  and their negations   the subformulae in  must
be     true of the paths leading to the e state  and     sufficient to determine the current
truth of all reward formulae in f   as this is needed to compute the current reward  ideally
the s should also be     small enough to enable just that  i e  they should not contain
subformulae which draw history distinctions that are irrelevant to determining the reward
at one point or another  note however that in the worst case  the number of distinctions
needed  even in the minimal equivalent mdp  may be exponential in   f     this happens for
instance with the formula k f   which requires k additional bits of information memorising
the truth of f over the last k steps 
      pltlsim
for the choice of the s  bacchus et al         consider two cases  in the simple case  which
we call pltlsim  an mdp obeying properties     and     is produced by simply labelling
each e state with the set of all subformulae in sub f   which are true of the sequence leading
to that e state  this mdp is generated forward  starting from the initial e state labelled
with s  and with the set    sub f   of all subformulae which are true of the sequence
hs  i  the successors of any e state labelled by nmrdp state s and subformula set  are
generated as follows  each of them is labelled by a successor s  of s in the nmrdp and by
the set of subformulae      sub f         reg      s     
for instance  consider the nmrdp shown in figure    the set f    qp  consists of
a single reward formula  the set sub f   consists of all subformulae of this reward formula 
   the size   f    of a reward formula is measured as its length and the size   f    of a set of reward formulae
f is measured as the sum of the lengths of the formulae in f  

  

fidecision theoretic planning with non markovian rewards

start state
a      
p

a    b   

a       b     
a      

a      

b     

q

a      b   

a     
p  q

a    b   

in the initial state  both p and q are false 
when p is false  action a independently sets
p and q to true with probability      when
both p and q are false  action b sets q to true
with probability      both actions have
no effect otherwise  a reward is obtained
whenever q    p  the optimal policy
is to apply b until q gets produced  making
sure to avoid the state on the left hand side 
then to apply a until p gets produced  and
then to apply a or b indifferently forever 

figure    another simple nmrdp
and their negations  that is sub f      p  q  p    p  q    p  p  q    p    p   q 
  p    the equivalent mdp produced by pltlsim is shown in figure   
      pltlmin
unfortunately  the mdps produced by pltlsim are far from minimal  although they could
be postprocessed for minimisation before invoking the mdp solution method  the above
expansion may still constitute a serious bottleneck  therefore  bacchus et al         consider
a more complex two phase translation  which we call pltlmin  capable of producing an
mdp also satisfying property      here  a preprocessing phase iterates over all states in
s  and computes  for each state s  a set l s  of subformulae  where the function l is the
solution of the fixpoint equation l s    f   reg      s          l s     s  is a successor of s  
only subformulae in l s  will be candidates for inclusion in the sets labelling the respective
e states labelled with s  that is  the subsequent expansion phase will be as above  but taking
   l s    and     l s    instead of    sub f   and     sub f    as the subformulae in
l s  are exactly those that are relevant to the way feasible execution sequences starting from
e states labelled with s are rewarded  this leads the expansion phase to produce a minimal
equivalent mdp 
figure   shows the equivalent mdp produced by pltlmin for the nmrdp example in
figure    together with the function l from which the labels are built  observe how this
mdp is smaller than the pltlsim mdp  once we reach the state on the left hand side in
which p is true and q is false  there is no point in tracking the values of subformulae  because
q cannot become true and so the reward formula cannot either  this is reflected by the fact
that l  p   only contains the reward formula 
in the worst case  computing l requires a space  and a number of iterations through s 
exponential in   f     hence the question arises of whether the gain during the expansion
phase is worth the extra complexity of the preprocessing phase  this is one of the questions
our experimental analysis in section   will try to answer 
      pltlstr
the pltlstr translation can be seen as a symbolic version of pltlsim  the set t of
added temporal variables contains the purely temporal subformulae ptsub f   of the reward
formulae in f   to which the  modality is prepended  unless already there   t        

  

fithiebaux  gretton  slaney  price   kabanza

start state
f  f  f  f  f  
reward  
a      

a       b     

a      

p
f  f  f  f  f  
reward  

a      

q
f  f  f  f  f  
reward  

a    b   

p
f  f  f  f  f  
reward  

the following subformulae in sub f   label
the e states 
f    p
f    q
f    p
f      p
f    q    p
f    p
f    q
f      p
f      p
f      q    p 

a      b   

a     

p
f  f  f  f  f  
reward  
a   

b     

p  q
f  f  f  f  f  
reward  
b   

a   

a    b   

b   

p  q
f  f  f  f  f  
reward  
a    b   
p  q
f  f  f  f  f 
reward  

a    b   

figure    equivalent mdp produced by pltlsim

start state
f  f  f 
reward  
a      
p
f 
reward  

a    b   

a       b     

a      

b     

q
f  f  f 
reward  

a      

the function l is given by 
l        q    p  p  p 
l  p      q    p 
l  q      q    p  p  p 
l  p  q      q    p  p  p 

a      b   

a     

the following formulae label the e states 
f    q    p
f    p
f    p
f     q    p 
f      p
f    p

p  q
f  f  f 
reward  
a   

b   

p  q
f  f  f 
reward  
a   

b   

p  q
f  f  f 
reward  

a    b   

figure    equivalent mdp produced by pltlmin

  

fidecision theoretic planning with non markovian rewards

q

p

    

prv prv p

prv p

    

   dynamics of p

    

    

   dynamics of   p

    

    

   reward

figure    adds produced by pltlstr  prv  previously  stands for 
ptsub f                    ptsub f     by repeatedly applying the equivalence
f  s f   f    f    f  s f     to any subformula in ptsub f    we can express its current
value  and hence that of reward formulae  as a function of the current values of formulae
in t and state variables  as required by the compact representation of the transition and
reward models 
for our nmrdp example in figure    the set of purely temporal variables is ptsub f    
 p    p   and t is identical to ptsub f    figure   shows some of the adds forming
part of the symbolic mdp produced by pltlstr  the adds describing the dynamics of
the temporal variables  i e   the adds describing the effects of the actions a and b on their
respective values  and the add describing the reward 
as a more complex illustration  consider this example  bacchus et al         in which
   p s  q  r        s  p s  q  r   
f    
we have that
ptsub f        s  p s  q  r    p s  q  r   r 
and so the set of temporal variables used is
t    t       s  p s  q  r     t     p s  q  r    t    r 
using the equivalences  the reward can be decomposed and expressed by means of the
propositions p  q and the temporal variables t    t    t  as follows 
  s  p s  q  r  

 p s  q  r       s  p s  q  r    
 q  r    p   p s  q  r     t 

 q  t      p  t     t 
as with pltlsim  the underlying mdp produced by pltlstr is far from minimal  the
encoded history features do not even vary from one state to the next  however  size is
not as problematic as with state based approaches  because structured solution methods do
not enumerate states and are able to dynamically ignore some of the variables that become
irrelevant during policy construction  for instance  when solving the mdp  they may be
  

fithiebaux  gretton  slaney  price   kabanza

able to determine that some temporal variables have become irrelevant because the situation
they track  although possible in principle  is too costly to be realised under a good policy 
this dynamic analysis of rewards contrast with pltlmins static analysis  bacchus et al  
      which must encode enough history to determine the reward at all reachable future
states under any policy 
one question that arises is that of the circumstances under which this analysis of irrelevance by structured solution methods  especially the dynamic aspects  is really effective 
this is another question our experimental analysis will try to address 

   fltl  a forward looking approach
as noted in section   above  the two key issues facing approaches to nmrdps are how
to specify the reward functions compactly and how to exploit this compact representation
to automatically translate an nmrdp into an equivalent mdp amenable to the chosen
solution method  accordingly  our goals are to provide a reward function specification
language and a translation that are adapted to anytime state based solution methods  after
a brief reminder of the relevant features of these methods  we consider these two goals
in turn  we describe the syntax and semantics of the language  the notion of formula
progression for the language which will form the basis of our translation  the translation
itself  its properties  and its embedding into the solution method  we call our approach
fltl  we finish the section with a discussion of the features that distinguish fltl from
existing approaches 
    anytime state based solution methods
the main drawback of traditional dynamic programming algorithms such as policy iteration
 howard        is that they explicitly enumerate all states that are reachable from s  in
the entire mdp  there has been interest in other state based solution methods  which may
produce incomplete policies  but only enumerate a fraction of the states that policy iteration
requires 
let e   denote the envelope of policy   that is the set of states that are reachable
 with a non zero probability  from the initial state s  under the policy  if  is defined
at all s  e    we say that the policy is complete  and that it is incomplete otherwise 
the set of states in e   at which  is undefined is called the fringe of the policy  the
fringe states are taken to be absorbing  and their value is heuristic  a common feature of
anytime state based algorithms is that they perform a forward search  starting from s  and
repeatedly expanding the envelope of the current policy one step forward by adding one or
more fringe states  when provided with admissible heuristic values for the fringe states 
they eventually converge to the optimal policy without necessarily needing to explore the
entire state space  in fact  since planning operators are used to compactly represent the
state space  they may not even need to construct more than a small subset of the mdp
before returning the optimal policy  when interrupted before convergence  they return a
possibly incomplete but often useful policy 
these methods include the envelope expansion algorithm  dean et al          which
deploys policy iteration on judiciously chosen larger and larger envelopes  using each successive policy to seed the calculation of the next  the more recent lao algorithm  hansen
  

fidecision theoretic planning with non markovian rewards

  zilberstein        which combines dynamic programming with heuristic search can be
viewed as a clever implementation of a particular case of the envelope expansion algorithm 
where fringe states are given admissible heuristic values  where policy iteration is run up to
convergence between envelope expansions  and where the clever implementation only runs
policy iteration on the states whose optimal value can actually be affected when a new fringe
state is added to the envelope  another example is a backtracking forward search in the
space of  possibly incomplete  policies rooted at s   thiebaux et al          which is performed until interrupted  at which point the best policy found so far is returned  real time
dynamic programming  rtdp   barto et al         is another popular anytime algorithm
which is to mdps what learning real time a  korf        is to deterministic domains  and
which has asymptotic convergence guarantees  the rtdp envelope is made up of sample
paths which are visited with a frequency determined by the current greedy policy and the
transition probabilities in the domain  rtdp can be run on line  off line for a given number
of steps or until interrupted  a variant called lrtdp  bonet   geffner        incorporates
mechanisms that focus the search on states whose value has not yet converged  resulting in
convergence speed up and finite time convergence guarantees 
the fltl translation we are about to present targets these anytime algorithms  although
it could also be used with more traditional methods such as value and policy iteration 
    language and semantics
compactly representing non markovian reward functions reduces to compactly representing
the behaviours of interest  where by behaviour we mean a set of finite sequences of states
 a subset of s     e g  the  hs    s  i  hs    s    s  i  hs    s    s    s  i        in figure    recall that the
reward is issued at the end of any prefix  i  in that set  once behaviours are compactly
represented  it is straightforward to represent non markovian reward functions as mappings
from behaviours to real numbers  we shall defer looking at this until section     
to represent behaviours compactly  we adopt a version of future linear temporal logic
 fltl   see emerson         augmented with a propositional constant    intended to be
read the behaviour we want to reward has just happened or the reward is received now 
the language  fltl begins with a set of basic propositions p giving rise to literals 
l     p   p           
where   and  stand for true and false  respectively  the connectives are classical  and
  and the temporal modalities   next  and u  weak until   giving formulae 
f     l   f  f   f  f   f   f u f
our until is weak  f  u f  means f  will be true from now on until f  is  if ever  unlike
the more commonly used strong until  this does not imply that f  will eventually be true 
it allows us to define the useful operator   always   f  f u   f will always be true
k f for k iterations of the  modality  f will
from now on   we also adopt the notations
wk
be true in exactly k steps   k f for i   i f  f will be true within the next k steps   and
v
k f for ki   i f  f will be true throughout the next k steps  
although negation officially occurs only in literals  i e   the formulae are in negation
normal form  nnf   we allow ourselves to write formulae involving it in the usual way 
  

fithiebaux  gretton  slaney  price   kabanza

provided that they have an equivalent in nnf  not every formula has such an equivalent 
because there is no such literal as   and because eventualities  f will be true some time 
are not expressible  these restrictions are deliberate  if we were to use our notation and
logic to theorise about the allocation of rewards  we would indeed need the means to say
when rewards are not received or to express features such as liveness  always  there will be
a reward eventually   but in fact we are using them only as a mechanism for ensuring that
rewards are given where they should be  and for this restricted purpose eventualities and
the negated dollar are not needed  in fact  including them would create technical difficulties
in relating formulae to the behaviours they represent 
the semantics of this language is similar to that of fltl  with an important difference 
because the interpretation of the constant   depends on the behaviour b we want to reward
 whatever that is   the modelling relation    must be indexed by b  we therefore write
   i    b f to mean that formula f holds at the i th stage of an arbitrary sequence   s   
relative to behaviour b  defining   b is the first step in our description of the semantics 
   i    b   iff  i   b
   i    b  
   i     b 
   i    b p  for p  p  iff p  i
   i    b p  for p  p  iff p   i
   i    b f   f  iff    i    b f  and    i    b f 
   i    b f   f  iff    i    b f  or    i    b f 
   i    b

f

iff    i        b f

   i    b f  u f  iff k  i if  j  i  j  k    j     b f    then    k    b f 
note that except for subscript b and for the first rule  this is just the standard fltl
semantics  and that therefore   free formulae keep their fltl meaning  as with fltl  we
say    b f iff         b f   and   b f iff    b f for all   s   
the modelling relation   b can be seen as specifying when a formula holds  on which
reading it takes b as input  our next and final step is to use the   b relation to define 
for a formula f   the behaviour bf that it represents  and for this we must rather assume
that f holds  and then solve for b  for instance  let f be  p      i e   we get rewarded
every time p is true  we would like bf to be the set of all finite sequences ending with a
state containing p  for an arbitrary f   we take bf to be the set of prefixes that have to be
rewarded if f is to hold in all sequences 
t
definition   bf   b     b f  
to understand definition    recall that b contains prefixes at the end of which we get
a reward and   evaluates to true  since f is supposed to describe the way rewards will
be received in an arbitrary sequence  we are interested in behaviours b which make  
true in such a way as to make f hold without imposing constraints on the evolution of
the world  however  there may be many behaviours with this property  so we take their

  

fidecision theoretic planning with non markovian rewards

intersection   ensuring that bf will only reward a prefix if it has to because that prefix is in
every behaviour satisfying f   in all but pathological cases  see section       this makes bf
coincide with the  set inclusion  minimal behaviour b such that   b f   the reason for this
stingy semantics  making rewards minimal  is that f does not actually say that rewards
are allocated to more prefixes than are required for its truth  for instance   p     says
only that a reward is given every time p is true  even though a more generous distribution
of rewards would be consistent with it 
    examples
it is intuitively clear that many behaviours can be specified by means of  fltl formulae 
while there is no simple way in general to translate between past and future tense expressions   all of the examples used to illustrate pltl in section     above are expressible
naturally in  fltl  as follows 
the classical goal formula g saying that a goal p is rewarded whenever it happens is
easily expressed   p      as already noted  bg is the set of finite sequences of states such
that p holds in the last state  if we only care that p is achieved once and get rewarded at
each state from then on  we write  p      the behaviour that this formula represents
is the set of finite state sequences having at least one state in which p holds  by contrast 
the formula p u  p     stipulates only that the first occurrence of p is rewarded  i e  it
specifies the behaviour in figure     to reward the occurrence of p at most once every k
steps  we write   k   p  k p   k      
for response formulae  where the achievement of p is triggered by the command c 
we write  c   p      to reward every state in which p is true following the first
issue of the command  to reward only the first occurrence p after each command  we write
 c   p u  p      as for bounded variants for which we only reward goal achievement
within k steps of the trigger command  we write for example  c  k  p      to reward
all such states in which p holds 
it is also worth noting how to express simple behaviours involving past tense operators 
to stipulate a reward if p has always been true  we write   u p  to say that we are rewarded
if p has been true since q was  we write  q     u p   
finally  we often find it useful to reward the holding of p until the occurrence of q  the
neatest expression for this is q u   p  q    q      
    reward normality
 fltl is therefore quite expressive  unfortunately  it is rather too expressive  in that it
contains formulae which describe unnatural allocations of rewards  for instance  they
may make rewards depend on future behaviours rather than on the past  or they may
   if there
t is no b such that   b f   which is the case for any   free f which is not a logical theorem  then
bf is   i e  s  following normal set theoretic conventions  this limiting case does no harm  since
  free formulae do not describe the attribution of rewards 
   it is an open question whether the set of representable behaviours is the same for  fltl as for pltl 
that is star free regular languages  even if the behaviours were the same  there is little hope that a
practical translation from one to the other exists 

  

fithiebaux  gretton  slaney  price   kabanza

leave open a choice as to which of several behaviours is to be rewarded   an example of
dependence on the future is p     which stipulates a reward now if p is going to hold
next  we call such formula reward unstable  what a reward stable f amounts to is that
whether a particular prefix needs to be rewarded in order to make f true does not depend
on the future of the sequence  an example of an open choice of which behavior to reward is
 p       p     which says we should either reward all achievements of the goal p
or reward achievements of p but does not determine which  we call such formula rewardindeterminate  what a reward determinate f amounts to is that the set of behaviours
modelling f   i e   b     b f    has a unique minimum  if it does not  bf is insufficient  too
small  to make f true 
in investigating  fltl  slaney         we examine the notions of reward stability and
reward determinacy in depth  and motivate the claim that formulae that are both rewardstable and reward determinate  we call them reward normal  are precisely those that
capture the notion of no funny business  this is the intuition that we ask the reader to
note  as it will be needed in the rest of the paper  just for reference then  we define 
definition   f is reward normal iff for every   s  and every b  s       b f iff for
every i  if  i   bf then  i   b 
the property of reward normality is decidable  slaney         in appendix a we give
some simple syntactic constructions guaranteed to result in reward normal formulae  while
reward abnormal formulae may be interesting  for present purposes we restrict attention to
reward normal ones  indeed  we stipulate as part of our method that only reward normal
formulae should be used to represent behaviours  naturally  all formulae in section     are
normal 
     fltl formula progression
having defined a language to represent behaviours to be rewarded  we now turn to the
problem of computing  given a reward formula  a minimum allocation of rewards to states
actually encountered in an execution sequence  in such a way as to satisfy the formula 
because we ultimately wish to use anytime solution methods which generate state sequences
incrementally via forward search  this computation is best done on the fly  while the sequence
is being generated  we therefore devise an incremental algorithm based on a model checking
technique normally used to check whether a state sequence is a model of an fltl formula
 bacchus   kabanza         this technique is known as formula progression because it
progresses or pushes the formula through the sequence 
our progression technique is shown in algorithm    in essence  it computes the modelling relation   b given in section      however unlike the definition of   b   it is designed
to be useful when states in the sequence become available one at a time  in that it defers the
evaluation of the part of the formula that refers to the future to the point where the next
state becomes available  let s be a state  say i   the last state of the sequence prefix  i 
   these difficulties are inherent in the use of linear time formalisms in contexts where the principle of
directionality must be enforced  they are shared for instance by formalisms developed for reasoning
about actions such as the event calculus and ltl action theories  see e g   calvanese  de giacomo   
vardi        

  

fidecision theoretic planning with non markovian rewards

that has been generated so far  and let b be a boolean true iff  i  is in the behaviour b to
be rewarded  let the  fltl formula f describe the allocation of rewards over all possible
futures  then the progression of f through s given b  written prog b  s  f    is a new formula
which will describe the allocation of rewards over all possible futures of the next state  given
that we have just passed through s  crucially  the function prog is markovian  depending
only on the current state and the single boolean value b  note that prog is computable in
linear time in the length of f   and that for   free formulae  it collapses to fltl formula
progression  bacchus   kabanza         regardless of the value of b  we assume that prog
incorporates the usual simplification for sentential constants  and    f   simplifies to
  f    simplifies to f   etc 
algorithm    fltl progression
prog true  s    
   
prog false  s    
  
prog b  s    
   
prog b  s   
  
prog b  s  p 
    iff p  s and  otherwise
prog b  s  p 
    iff p   s and  otherwise
prog b  s  f   f      prog b  s  f     prog b  s  f   
prog b  s  f   f      prog b  s  f     prog b  s  f   
prog b  s  f  
  f
prog b  s  f  u f      prog b  s  f     prog b  s  f     f  u f   
rew s  f  
 prog s  f  

  true iff prog false  s  f     
  prog rew s  f    s  f  

the fundamental property of prog is the following  where b    i   b  
property      i    b f iff    i        b prog b  i   f  
proof 

see appendix b 



like   b   the function prog seems to require b  or at least b  as input  but of course
when progression is applied in practice we only have f and one new state at a time of  
and what we really want to do is compute the appropriate b  namely that represented by
f   so  similarly as in section      we now turn to the second step  which is to use prog to
decide on the fly whether a newly generated sequence prefix  i  is in bf and so should
be allocated a reward  this is the purpose of the functions  prog and rew  also given in
algorithm    given  and f   the function  prog in algorithm   defines an infinite sequence
of formulae hf    f         i in the obvious way 
f    f
fi      prog i   fi  
to decide whether a prefix  i  of  is to be rewarded  rew first tries progressing the
formula fi through i with the boolean flag set to false  if that gives a consistent result 
we need not reward the prefix and we continue without rewarding  i   but if the result is
  

fithiebaux  gretton  slaney  price   kabanza

 then we know that  i  must be rewarded in order for  to satisfy f   in that case  to
obtain fi   we must progress fi through i again  this time with the boolean flag set to the
value true  to sum up  the behaviour corresponding to f is   i  rew i   fi    
to illustrate the behaviour of  fltl progression  consider the formula f   p u  p    
stating that a reward will be received the first time p is true  let s be a state in which p
holds  then prog false  s  f          p u  p         therefore  since the formula has
progressed to   rew s  f   is true and a reward is received   prog s  f     prog true  s  f    
      p u  p          so the reward formula fades away and will not affect subsequent
progression steps  if  on the other hand  p is false in s  then prog false  s  f          
p u  p     p u  p     therefore  since the formula has not progressed to   rew s  f  
is false and no reward is received   prog s  f     prog false  s  f     p u  p    so the reward
formula persists as is for subsequent progression steps 
the following theorem states that under weak assumptions  rewards are correctly allocated by progression 
theorem   let f be reward normal  and let hf    f         i be the result of progressing it
through the successive states of a sequence  using the function  prog  then  provided no
fi is   for all i rew i   fi   iff  i   bf  
proof  see appendix b



the premise of the theorem is that f never progresses to   indeed if fi    for some
i  it means that even rewarding  i  does not suffice to make f true  so something must
have gone wrong  at some earlier stage  the boolean rew was made false where it should
have been made true  the usual explanation is that the original f was not reward normal 
for instance p     which is reward unstable  progresses to  in the next state if p is
true there  regardless of     f    p      p     rew     f      false  and f    p 
so if p    then f      however  other  admittedly bizarre  possibilities exist  for
example  although p    is reward unstable  its substitution instance       which
also progresses to  in a few steps  is logically equivalent to   and is reward normal 
if the progression method were to deliver the correct minimal behaviour in all cases
 even in all reward normal cases  it would have to backtrack on the choice of values for the
boolean flags  in the interest of efficiency  we choose not to allow backtracking  instead 
our algorithm raises an exception whenever a reward formula progresses to   and informs
the user of the sequence which caused the problem  the onus is thus placed on the domain
modeller to select sensible reward formulae so as to avoid possible progression to   it
should be noted that in the worst case  detecting reward normality cannot be easier than
the decision problem for  fltl so it is not to be expected that there will be a simple
syntactic criterion for reward normality  in practice  however  commonsense precautions
such as avoiding making rewards depend explicitly on future tense expressions suffice to
keep things normal in all routine cases  for a generous class of syntactically recognisable
reward normal formulae  see appendix a 
    reward functions
with the language defined so far  we are able to compactly represent behaviours  the
extension to a non markovian reward function is straightforward  we represent such a
  

fidecision theoretic planning with non markovian rewards

function by a set     fltl  ir of formulae associated with real valued rewards  we
call  a reward function specification  where formula f is associated with reward r in  
we write  f   r     the rewards are assumed to be independent and additive  so that
the reward function r represented by  is given by 
x
 r    i   bf  
definition   r   i    
 f  r 

e g  if  is  p u  p             q             we get a reward of     the first time that p
holds  a reward of     from the first time that q holds onwards  a reward of      when both
conditions are met  and   otherwise 
again  we can progress a reward function specification  to compute the reward at
all stages i of   as before  progression defines a sequence h             i of reward function
specifications  with i     rprog i   i    where rprog is the function that applies prog to
all formulae in a reward function specification 
rprog s        prog s  f     r     f   r    
then  the total reward received at stage i is simply the sum of the real valued rewards
granted by the progression function to the behaviours represented by the formulae in i  
x
 r   rew i   f   
 f  r i

by proceeding that way  we get the expected analog of theorem    which states progression
correctly computes non markovian reward functions 
theorem   let  be a reward normal  reward function specification  and let h           i be
the result of progressing it through the successive states
x of a sequence  using the function
rprog  then  provided     r    i for any i  then
 r   rew i   f      r   i   
 f  r i

proof 

immediate from theorem   



    translation into mdp
we now exploit the compact representation of a non markovian reward function as a reward
function specification to translate an nmrdp into an equivalent mdp amenable to statebased anytime solution methods  recall from section   that each e state in the mdp is
labelled by a state of the nmrdp and by history information sufficient to determine the
immediate reward  in the case of a compact representation as a reward function specification
    this additional information can be summarised by the progression of   through the
sequence of states passed through  so an e state will be of the form hs  i  where s  s is
   strictly speaking  a multiset  but for convenience we represent it as a set  with the rewards for multiple
occurrences of the same formula in the multiset summed 
   we extend the definition of reward normality to reward specification functions in the obvious way  by
requiring that all reward formulae involved be reward normal 

  

fithiebaux  gretton  slaney  price   kabanza

a state  and    fltl  ir is a reward function specification  obtained by progression  
two e states hs  i and ht  i are equal if s   t  the immediate rewards are the same  and
the results of progressing  and  through s are semantically equivalent  
definition   let d   hs  s    a  pr  ri be an nmrdp  and   be a reward function specification representing r  i e   r    r  see definition     we translate d into the mdp
d    hs     s     a    pr    r  i defined as follows 
   s    s    fltl ir
   s     hs      i
   a   hs  i    a s 
pr s  a  s    if     rprog s   
 
otherwise
 
 
if a   a  hs  i   then pr  hs  i  a    is undefined
x
   r   hs  i   
 r   rew s  f   
   if a  a   hs  i   then pr   hs  i  a  hs      i   



 f  r 

   for all s   s     s  is reachable under a  from s    
item   says that the e states are labelled by a state and a reward function specification  item
  says that the initial e state is labelled with the initial state and with the original reward
function specification  item   says that an action is applicable in an e state if it is applicable
in the state labelling it  item   explains how successor e states and their probabilities are
computed  given an action a applicable in an e state hs  i  each successor e state will
be labelled by a successor state s  of s via a in the nmrdp and by the progression of 
through s  the probability of that e state is pr s  a  s    as in the nmrdp  note that the
cost of computing pr  is linear in that of computing pr and in the sum of the lengths of the
formulae in   item   has been motivated before  see section       finally  since items   
leave open the choice of many mdps differing only in the unreachable states they contain 
item   excludes all such irrelevant extensions  it is easy to show that this translation leads
to an equivalent mdp  as defined in definition    obviously  the function  required for
definition  is given by   hs  i    s  and then the proof is a matter of checking conditions 
in our practical implementation  the labelling is one step ahead of that in the definition 
we label the initial e state with rprog s        and compute the current reward and the current reward specification label by progression of predecessor reward specifications through
the current state rather than through the predecessor states  as will be apparent below 
this has the potential to reduce the number of states in the generated mdp 
figure   shows the equivalent mdp produced for the  fltl version of our nmrdp
example in figure    recall that for this example  the pltl reward formula was q    p 
in  fltl  the allocation of rewards is described by   p  q       the figure also
   care is needed over the notion of semantic equivalence  because rewards are additive  determining
equivalence may involve arithmetic as well as theorem proving  for example  the reward function specification   p           q          is equivalent to    p  q             p  q             p  q          
although there is no one one correspondence between the formulae in the two sets 

  

fidecision theoretic planning with non markovian rewards

start state
f 
reward  
a      
p
f  f 
reward  
a   
p
f  f  f 
reward  

a       b     

a      

a      

q
f 
reward  

b   

a    b   

b     

a      b   

a     

the following formulae label the e states 
f      p  q     
f    q   
f    q   

p  q
f  f 
reward  
a    b   
p  q
f  f  f 
reward  
a    b   
p  q
f  f  f 
reward  

a    b   

figure    equivalent mdp produced by fltl
shows the relevant formulae labelling the e states  obtained by progression of this reward
formula  note that without progressing one step ahead  there would be   e states with state
 p  on the left hand side  labelled with  f      f    f     and  f    f    f     respectively 
    blind minimality
the size of the mdp obtained  i e  the number of e states it contains is a key issue for
us  as it has to be amenable to state based solution methods  ideally  we would like the
mdp to be of minimal size  however  we do not know of a method building the minimal
equivalent mdp incrementally  adding parts as required by the solution method  and since
in the worst case even the minimal equivalent mdp can be larger than the nmrdp by a
factor exponential in the length of the reward formulae  bacchus et al          constructing
it entirely would nullify the interest of anytime solution methods 
however  as we now explain  definition   leads to an equivalent mdp exhibiting a relaxed
notion of minimality  and which is amenable to incremental construction  by inspection 
we may observe that wherever an e state hs  i has a successor hs      i via action a  this
means that in order to succeed in rewarding the behaviours described in  by means of
execution sequences that start by going from s to s  via a  it is necessary that the future
starting with s  succeeds in rewarding the behaviours described in     if hs  i is in the
minimal equivalent mdp  and if there really are such execution sequences succeeding in
rewarding the behaviours described in   then hs      i must also be in the minimal mdp 
that is  construction by progression can only introduce e states which are a priori needed 
note that an e state that is a priori needed may not really be needed  there may in fact
be no execution sequence using the available actions that exhibits a given behaviour  for

  

fithiebaux  gretton  slaney  price   kabanza

instance  consider the response formula  p   k q  k      i e   every time trigger p
is true  we will be rewarded k steps later provided q is true then  obviously  whether p
is true at some stage affects the way future states should be rewarded  however  if the
transition relation happens to have the property that k steps from a state satisfying p  no
state satisfying q can be reached  then a posteriori p is irrelevant  and there was no need to
label e states differently according to whether p was true or not  observe an occurrence of
this in the example in figure    and how this leads fltl to produce an extra state at the
bottom left of the figure  to detect such cases  we would have to look perhaps quite deep
into feasible futures  which we cannot do while constructing the e states on the fly  hence
the relaxed notion which we call blind minimality does not always coincide with absolute
minimality 
we now formalise the difference between true and blind minimality  for this purpose 
it is convenient to define some functions  and  mapping e states e to functions from s 
to ir intuitively assigning rewards to sequences in the nmrdp starting from   e   recall
from definition   that  maps each e state of the mdp to the underlying nmrdp state 
definition   let d be an nmrdp  let s   be the set of e states in an equivalent mdp d 
f   s   
for d  let e be any reachable e state in s     let    i  be a sequence of e states in d
 
e     obtained under  in
such that    i    e  let  i  be the corresponding sequence in d s
the sense that  for each j  i   j       j    then for any   s    we define

 e      
and


 e      

r  i        if     i
 
otherwise

e i 
r  i        if   d 
 
otherwise

for any unreachable e state e  we define both  e    and  e    to be   for all  
note carefully the difference between  and   the former describes the rewards assigned
to all continuations of a given state sequence  while the latter confines rewards to feasible
continuations  note also that  and  are well defined despite the indeterminacy in the
choice of    i   since by clause   of definition    all such choices lead to the same values for
r 
theorem   let s   be the set of e states in an equivalent mdp d  for d   hs  s    a  pr  ri 
d  is minimal iff every e state in s   is reachable and s   contains no two distinct e states s  
and s   with   s         s     and  s        s     
proof 

see appendix b 



blind minimality is similar  except that  since there is no looking ahead  no distinction can
be drawn between feasible trajectories and others in the future of s 
definition   let s   be the set of e states in an equivalent mdp d  for d   hs  s    a  pr  ri 
d  is blind minimal iff every e state in s   is reachable and s   contains no two distinct estates s   and s   with   s         s     and  s        s     
  

fidecision theoretic planning with non markovian rewards

theorem   let d  be the translation of d as in definition    d  is a blind minimal
equivalent mdp for d 
proof 

see appendix b 



the size difference between the blind minimal and minimal mdps will depend on the
precise interaction between rewards and dynamics for the problem at hand  making theoretical analyses difficult and experimental results rather anecdotal  however  our experiments
in section   and   will show that from a computation time point of view  it is often preferable to work with the blind minimal mdp than to invest in the overhead of computing the
truly minimal one 
finally  recall that syntactically different but semantically equivalent reward function
specifications define the same e state  therefore  neither minimality nor blind minimality
can be achieved in general without an equivalence check at least as complex as theorem
proving for ltl  in pratical implementations  we avoid theorem proving in favour of embedding  fast  formula simplification in our progression and regression algorithms  this
means that in principle we only approximate minimality and blind minimality  but this
appears to be enough for practical purposes 
    embedded solution construction
blind minimality is essentially the best achievable with anytime state based solution methods which typically extend their envelope one step forward without looking deeper into the
future  our translation into a blind minimal mdp can be trivially embedded in any of these
solution methods  this results in an on line construction of the mdp  the method entirely
drives the construction of those parts of the mdp which it feels the need to explore  and
leave the others implicit  if time is short  a suboptimal or even incomplete policy may be
returned  but only a fraction of the state and expanded state spaces might be constructed 
note that the solution method should raise an exception as soon as one of the reward formulae progresses to   i e   as soon as an expanded state hs  i is built such that     r    
since this acts as a detector of unsuitable reward function specifications 
to the extent enabled by blind minimality  our approach allows for a dynamic analysis of
the reward formulae  much as in pltlstr  bacchus et al          indeed  only the execution
sequences feasible under a particular policy actually explored by the solution method contribute to the analysis of rewards for that policy  specifically  the reward formulae generated
by progression for a given policy are determined by the prefixes of the execution sequences
feasible under this policy  this dynamic analysis is particularly useful  since relevance of
reward formulae to particular policies  e g  the optimal policy  cannot be detected a priori 
the forward chaining planner tlplan  bacchus   kabanza        introduced the idea
of using fltl to specify domain specific search control knowledge and formula progression
to prune unpromising sequential plans  plans violating this knowledge  from deterministic
search spaces  this has been shown to provide enormous time gains  leading tlplan to
win the      planning competition hand tailored track  because our approach is based
on progression  it provides an elegant way to exploit search control knowledge  yet in the
context of decision theoretic planning  here this results in a dramatic reduction of the

  

fithiebaux  gretton  slaney  price   kabanza

fraction of the mdp to be constructed and explored  and therefore in substantially better
policies by the deadline 
we achieve this as follows  we specify  via a   free formula c    properties which we know
must be verified by paths feasible under promising policies  then we simply progress c 
alongside the reward function specification  making e states triples hs    ci where c is a   free
formula obtained by progression  to prevent the solution method from applying an action
that leads to the control knowledge being violated  the action applicability condition  item
  in definition    becomes  a  a   hs    ci  iff a  a s  and c      the other changes are
straightforward   for instance  the effect of the control knowledge formula  p  q  is to
remove from consideration any feasible path in which p is not followed by q  this is detected
as soon as violation occurs  when the formula progresses to   although this paper focuses
on non markovian rewards rather than dynamics  it should be noted that   free formulae
can also be used to express non markovian constraints on the systems dynamics  which
can be incorporated in our approach exactly as we do for the control knowledge 
     discussion
existing approaches  bacchus et al               advocate the use of pltl over a finite
past to specify non markovian rewards  in the pltl style of specification  we describe
the past conditions under which we get rewarded now  while with  fltl we describe the
conditions on the present and future under which future states will be rewarded  while the
behaviours and rewards may be the same in each scheme  the naturalness of thinking in one
style or the other depends on the case  letting the kids have a strawberry dessert because
they have been good all day fits naturally into a past oriented account of rewards  whereas
promising that they may watch a movie if they tidy their room  indeed  making sense of the
whole notion of promising  goes more naturally with  fltl  one advantage of the pltl
formulation is that it trivially enforces the principle that present rewards do not depend
on future states  in  fltl  this responsibility is placed on the domain modeller  the best
we can offer is an exception mechanism to recognise mistakes when their effects appear 
or syntactic restrictions  on the other hand  the greater expressive power of  fltl opens
the possibility of considering a richer class of decision processes  e g  with uncertainty as
to which rewards are received  the dessert or the movie  and when  some time next week 
before it rains  
at any rate  we believe that  fltl is better suited than pltl to solving nmrdps
using anytime state based solution methods  while the pltlsim translation could be easily embedded in such a solution method  it loses the structure of the original formulae
when considering subformulae individually  consequently  the expanded state space easily
becomes exponentially bigger than the blind minimal one  this is problematic with the
solution methods we consider  because size severely affects their performance in solution
quality  the pre processing phase of pltlmin uses pltl formula regression to find sets
of subformulae as potential labels for possible predecessor states  so that the subsequent
generation phase builds an mdp representing all and only the histories which make a difference to the way actually feasible execution sequences should be rewarded  not only does
this recover the structure of the original formula  but in the best case  the mdp produced
is exponentially smaller than the blind minimal one  however  the prohibitive cost of the

  

fidecision theoretic planning with non markovian rewards

pre processing phase makes it unsuitable for anytime solution methods  we do not consider that any method based on pltl and regression will achieve a meaningful relaxed
notion of minimality without a costly pre processing phase  fltl is an approach based on
 fltl and progression which does precisely that  letting the solution method resolve the
tradeoff between quality and cost in a principled way intermediate between the two extreme
suggestions above 
the structured representation and solution methods targeted by bacchus et al        
differ from the anytime state based solution methods fltl primarily aims at  in particular
in that they do not require explicit state enumeration at all  here  non minimality is not as
problematic as with the state based approaches  in virtue of the size of the mdp produced 
the pltlstr translation is  as pltlsim  clearly unsuitable to anytime state based methods  
in another sense  too  fltl represents a middle way  combining the advantages conferred by
state based and structured approaches  e g  by pltlmin on one side  and pltlstr on the
other  from the former fltl inherits a meaningful notion of minimality  as with the latter 
approximate solution methods can be used and can perform a restricted dynamic analysis of
the reward formulae  in particular  formula progression enables even state based methods
to exploit some of the structure in  fltl space  however  the gap between blind and
true minimality indicates that progression alone is insufficient to always fully exploit that
structure  there is a hope that pltlstr is able to take advantage of the full structure of
the reward function  but also a possibility that it will fail to exploit even as much structure
as fltl  as efficiently  an empirical comparison of the three approaches is needed to answer
this question and identify the domain features favoring one over the other 

   nmrdpp
the first step towards a decent comparison of the different approaches is to have a framework
that includes them all  the non markovian reward decision process planner  nmrdpp 
is a platform for the development and experimentation of approaches to nmrdps  it
provides an implementation of the approaches we have described in a common framework 
within a single system  and with a common input language  nmrdpp is available on line 
see http   rsise anu edu au  charlesg nmrdpp  it is worth noting that bacchus et al 
             do not report any implementation of their approaches 
    input language
the input language enables the specification of actions  initial states  rewards  and search
control knowledge  the format for the action specification is essentially the same as in the
spudd system  hoey et al          the reward specification is one or more formulae  each
associated with a name and a real number  these formulae are in either pltl or  fltl 
control knowledge is given in the same language as that chosen for the reward  control
knowledge formulae will have to be verified by any sequence of states feasible under the
generated policies  initial states are simply specified as part of the control knowledge or as
explicit assignments to propositions 
   it would be interesting  on the other hand  to use pltlstr in conjunction with symbolic versions of such
methods  e g  symbolic lao   feng   hansen        or symbolic rtdp  feng  hansen    zilberstein 
      

  

fithiebaux  gretton  slaney  price   kabanza

action flip
heads      
endaction
action tilt
heads  heads             
endaction
heads   ff
 first        heads and  prv  pdi heads 
 seq         prv   heads  and  prv heads  and  heads
figure    input for the coin example  prv  previously  stands for  and
  
pdi  past diamond  stands for 
for instance  consider a simple example consisting of a coin showing either heads or
tails  heads   there are two actions that can be performed  the flip action changes the
coin to show heads or tails with a     probability  the tilt action changes it with    
probability  otherwise leaving it as it is  the initial state is tails  we get a reward of     for
  heads in pltl  and a reward of     each
the very first head  this is written heads    
time we achieve the sequence heads  heads  tails    heads  heads  heads in pltl   in
our input language  this nmrdp is described as shown in figure   
    common framework
the common framework underlying nmrdpp takes advantage of the fact that nmrdp
solution methods can  in general  be divided into the distinct phases of preprocessing 
expansion  and solving  the first two are optional 
for pltlsim  preprocessing simply computes the set sub f   of subformulae of the reward
formulae  for pltlmin  it also includes computing the labels l s  for each state s  for
pltlstr  preprocessing involves computing the set t of temporal variables as well as the
adds for their dynamics and for the rewards  fltl does not require any preprocessing 
expansion is the optional generation of the entire equivalent mdp prior to solving 
whether or not off line expansion is sensible depends on the mdp solution method used  if
state based value or policy iteration is used  then the mdp needs to be expanded anyway 
if  on the other hand  an anytime search algorithm or structured method is used  it is
definitely a bad idea  in our experiments  we often used expansion solely for the purpose of
measuring the size of the generated mdp 
solving the mdp can be done using a number of methods  currently  nmrdpp provides
implementations of classical dynamic programming methods  namely state based value and
policy iteration  howard         of heuristic search methods  state based lao   hansen  
zilberstein        using either value or policy iteration as a subroutine  and of one structured
method  namely spudd  hoey et al          prime candidates for future developments are
 l rtdp  bonet   geffner         symbolic lao   feng   hansen         and symbolic
rtdp  feng et al         
  

fidecision theoretic planning with non markovian rewards

load coin nmrdp
pltlstr preprocessing

  loadworld coin 
  preprocess spltl 
  startcputimer
  spudd              
  stopcputimer
  readcputimer
       
  iterationcount
    
  displaydot valuetodot 
expected value

     

     

report number of iterations
display add of value function

 prv heads 

 prv  prv pdi heads  

     

report solving time

heads

 prv heads 

 prv  prv pdi heads  

solve mdp with spudd    

     

 prv   heads 

 prv pdi heads 

     

     

 prv pdi heads 

     

     

display policy

  displaydot policytodot 
optimal policy

heads

 prv heads 

flip

 
 
 
 
 

tilt

pltlmin preprocessing
completely expand mdp
report mdp size

preprocess mpltl 
expand
domainstatesize
printdomain        show domain rb
reward  
flip     

flip     

display postcript rendering of mdp

tilt     

tilt     

heads
reward  
flip     
heads
reward  
tilt     

flip     

tilt     

tilt     

flip     

tilt     

flip     

reward  
tilt     

flip     

tilt     

flip     

tilt     

tilt     

flip     
reward  
flip     

flip     

flip     

tilt     

tilt     

heads
reward  

solve mdp with vi    
report number of iterations

  valit              
  iterationcount
    
  getpolicy
   

output policy  textual 

figure    sample session
  

fithiebaux  gretton  slaney  price   kabanza

    approaches covered
altogether  the various types of preprocessing  the choice of whether to expand  and the
mdp solution methods  give rise to quite a number of nmrdp approaches  including  but
not limited to those previously mentioned  see e g  pltlstr a  below   not all combinations are possible  e g   state based processing variants are incompatible with structured
solution methods  the converse is possible in principle  however   also  there is at present
no structured form of preprocessing for  fltl formulae 
pltlstr a  is an example of an interesting variant of pltlstr  which we obtain by
considering additional preprocessing  whereby the state space is explored  without explicitly
enumerating it  to produce a bdd representation of the e states reachable from the start
state  this is done by starting with a bdd representing the start e state  and repeatedly
applying each action  non zero probabilities are converted to ones and the result or ed
with the last result  when no action adds any reachable e states to this bdd  we can
be sure it represents the reachable e state space  this is then used as additional control
knowledge to restrict the search  it should be noted that without this phase pltlstr makes
no assumptions about the start state  and thus is left at a possible disadvantage  similar
structured reachability analysis techniques have been used in the symbolic implementation
of lao   feng   hansen         however  an important aspect of what we do here is that
temporal variables are also included in the bdd 
    the nmrdpp system
nmrdpp is controlled by a command language  which is read either from a file or interactively  the command language provides commands for the different phases  preprocessing 
expansion  solution  of the methods  commands to inspect the resulting policy and value
functions  e g  with rendering via dot  at t labs research         as well as supporting
commands for timing and memory usage  a sample session  where the coin nmrdp is
successively solved with pltlstr and pltlmin is shown in figure   
nmrdpp is implemented in c    and makes use of a number of supporting libraries 
in particular  it relies heavily on the cudd package for manipulating adds  somenzi 
       action specification trees are converted into and stored as adds by the system 
and moreover the structured algorithms rely heavily on cudd for add computations 
the state based algorithms make use of the mtl  matrix template library for matrix
operations  mtl takes advantage of modern processor features such as mmx and sse
and provides efficient sparse matrix operations  we believe that our implementations of
mdp solution methods are comparable with the state of the art  for instance  we found
that our implementation of spudd is comparable in performance  within a factor of    to
the reference implementation  hoey et al          on the other hand  we believe that data
structures used for regression and progression of temporal formulae could be optimised 

   experimental analysis
we are faced with three substantially different approaches that are not easy to compare 
as their performance will depend on domain features as varied as the structure in the
transition model  the type  syntax  and length of the temporal reward formula  the presence

  

fidecision theoretic planning with non markovian rewards

of rewards unreachable or irrelevant to the optimal policy  the availability of good heuristics
and control knowledge  etc  and on the interactions between these factors  in this section 
we report an experimental investigation into the influence of some of these factors and try
to answer the questions raised previously   
   is the dynamics of the domain the predominant factor affecting performance 
   is the type of reward a major factor 
   is the syntax used to describe rewards a major factor 
   is there an overall best method 
   is there an overall worst method 
   does the preprocessing phase of pltlmin pay  compared to pltlsim 
   does the simplicity of the fltl translation compensate for blind minimality  or does
the benefit of true minimality outweigh the cost of pltlmin preprocessing 
   are the dynamic analyses of rewards in pltlstr and fltl effective 
   is one of these analyses more powerful  or are they rather complementary 
in some cases but not all  we were able to identify systematic patterns  the results in this
section were obtained using a pentium     ghz gnu linux        machine with    mb
of ram 
    preliminary remarks
clearly  fltl and pltlstr a  have great potential for exploiting domain specific heuristics and control knowledge  pltlmin less so  to avoid obscuring the results  we therefore
refrained from incorporating these features in the experiments  when running lao   the
heuristic value of a state was the crudest possible  the sum of all reward values in the
problem   performance results should be interpreted in this light  they do not necessarily
reflect the practical abilities of the methods that are able to exploit these features 
we begin with some general observations  one question raised above was whether the
gain during the pltl expansion phase is worth the expensive preprocessing performed by
pltlmin  i e  whether pltlmin typically outperforms pltlsim  we can definitively answer
this question  up to pathological exceptions  preprocessing pays  we found that expansion
was the bottleneck  and that post hoc minimisation of the mdp produced by pltlsim did
not help much  pltlsim is therefore of little or no practical interest  and we decided not to
report results on its performance  as it is often an order of magnitude worse than that of
pltlmin  unsurprisingly  we also found that pltlstr would typically scale to larger state
spaces  inevitably leading it to outperform state based methods  however  this effect is not
uniform  structured solution methods sometimes impose excessive memory requirements
which makes them uncompetitive in certain cases  for example where n f   for large n 
features as a reward formula 
    here is an executive summary of the answers for the executive reader     no     yes     yes     pltlstr
and fltl     pltlsim     yes     yes and no  respectively     yes     no and yes  respectively 

  

fithiebaux  gretton  slaney  price   kabanza

    domains
experiments were performed on four hand coded domains  propositions   dynamics  and
on random domains  each hand coded domain has n propositions pi   and a dynamics
which makes every state possible and eventually reachable from the initial state in which
all propositions are false  the first two such domains  spudd linear and spudd expon
were discussed by hoey et al          the two others are our own 
the intention of spudd linear was to take advantage of the best case behaviour of
spudd  for each proposition pi   it has an action ai which sets pi to true and all propositions
pj      j   i to false  spudd expon  was used by hoey et al         to demonstrate the
worst case behaviour of spudd  for each proposition pi   it has an action ai which sets pi
to true only when all propositions pj      j   i are true  and sets pi to false otherwise   and
sets the latter propositions to false  the third domain  called on off  has one turn on
and one turn off action per proposition  the turn on pi  action only probabilistically
succeeds in setting pi to true when pi was false  the turn off action is similar  the fourth
domain  called complete  is a fully connected reflexive domain  for each proposition pi
there is an action ai which sets pi to true with probability i  n       and to false otherwise 
and pj   j    i to true or false with probability      note that ai can cause a transition to
any of the  n states 
random domains of size n also involve n propositions  the method for generating their
dynamics is detailed in appendix c  let us just summarise by saying that we are able to
generate random dynamics exhibiting a given degree of structure and a given degree of
uncertainty  lack of structure essentially measures the bushiness of the internal part of the
adds representing the actions  and uncertainty measures the bushiness of their leaves 
    influence of dynamics
the interaction between dynamics and reward certainly affects the performance of the
different approaches  though not so strikingly as other factors such as the reward type  see
below   we found that under the same reward scheme  varying the degree of structure or
uncertainty did not generally change the relative success of the different approaches  for
instance  figures    and    show the average run time of the methods as a function of
the degree of structure  resp  degree of uncertainty  for random problems of size n     and
reward n    the state encountered at stage n is rewarded  regardless of its properties     
run time increases slightly with both degrees  but there is no significant change in relative
performance  these are typical of the graphs we obtain for other rewards 
clearly  counterexamples to this observation exist  these are most notable in cases of
extreme dynamics  for instance with the spudd expon domain  although for small values
of n  such as n      pltlstr approaches are faster than the others in handling the reward
n     for virtually any type of dynamics we encountered  they perform very poorly with
that reward on spudd expon  this is explained by the fact that only a small fraction of
spudd expon states are reachable in the first n steps  after n steps  fltl immediately
recognises that reward is of no consequence  because the formula has progressed to   
pltlmin discovers this fact only after expensive preprocessing  pltlstr  on the other
hand  remains concerned by the prospect of reward  just as pltlsim would 
   

n  

in  fltl

  

fiaverage cpu time  sec 

decision theoretic planning with non markovian rewards

  
  
  
  
  
 

   

   

   

   

   

   

structure    structured        unstructured 
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     changing the degree of structure

average cpu time  sec 

  
  
  
  
  
 

 

   

   

   

   

 

   

uncertainty    certain        uncertain 
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     changing the degree of uncertainty
    influence of reward types
the type of reward appears to have a stronger influence on performance than dynamics 
this is unsurprising  as the reward type significantly affects the size of the generated mdp 
certain rewards only make the size of the minimal equivalent mdp increase by a constant
number of states or a constant factor  while others make it increase by a factor exponential
in the length of the formula  table   illustrates this  the third column reports the size of
the minimal equivalent mdp induced by the formulae on the left hand side   
a legitimate question is whether there is a direct correlation between size increase and
 in appropriateness of the different methods  for instance  we might expect the state based
methods to do particularly well in conjunction with reward types inducing a small mdp and
    the figures are not necessarily valid for non completely connected nmrdps  unfortunately  even for
completely connected domains  there does not appear to be a much cheaper way to determine the mdp
size than to generate it and count states 

  

fithiebaux  gretton  slaney  price   kabanza

type
first time all pi s
pi s in sequence from start state
two consecutive pi s
all pi s n times ago

formula
  ni   pi  
 ni   pi       
 ni   i pi    n    
n 
i  
 pi  pi    
n ni   pi

size
o     s  
o n   s  
o nk    s  
o  n    s  

fastest
pltlstr a 
fltl
pltlstr
pltlstr

slowest
pltlmin
pltlstr
fltl
pltlmin

table    influence of reward type on mdp size and method performance

average cpu time  sec 

    
   
   
   

 

   

 

   

 

   

 

   

n
all approaches prvin
all approaches prvout

figure     changing the syntax
otherwise badly in comparison with structured methods  interestingly  this is not always
the case  for instance  in table   whose last two columns report the fastest and slowest
methods over the range of hand coded domains where    n      the first row contradicts
that expectation  moreover  although pltlstr is fastest in the last row  for larger values
of n  not represented in the table   it aborts through lack of memory  unlike the other
methods 
the most obvious observations arising out of these experiments is that pltlstr is nearly
always the fastest  until it runs out of memory  perhaps the most interesting results are
those in the second row  which expose the inability of methods based on pltl to deal
with rewards specified as long sequences of events  in converting the reward formula to
a set of subformulae  they lose information about the order of events  which then has to
be recovered laboriously by reasoning   fltl progression in contrast takes the events one
at a time  preserving the relevant structure at each step  further experimentation led us
to observe that all pltl based algorithms perform poorly where reward is specified using
  k f   and fik f  f has been true k steps ago  within the last k
formulae of the form k f   
steps  or at all of the last k steps  
    influence of syntax
unsurprisingly  we find that the syntax used to express rewards  which affects the length
of the formula  has a major influence on the run time  a typical example of this effect is
captured in figure     this graph demonstrates how re expressing prvout  n  ni   pi  
  

fidecision theoretic planning with non markovian rewards

state count    n 

  
 
 
 
 
 
 

 

 

 

 

  

  

  

n
pltlmin
fltl

figure     effect of multiple rewards on mdp size

total cpu time  sec 

    
    
   

 

 

 

 

 

  

  

  

n
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     effect of multiple rewards on run time
as prvin  ni   n pi   thereby creating n times more temporal subformulae  alters the
running time of all pltl methods  fltl is affected too as  fltl progression requires two
iterations through the reward formula  the graph represents the averages of the running
times over all the methods  for the complete domain 
our most serious concern in relation to the pltl approaches is their handling of reward
specifications containing multiple reward elements  most notably we found that pltlmin
does not necessarily produce the minimal equivalent mdp in this situation  to demonstrate  we consider the set of reward formulae  f    f            fn    each associated with the
same real value r  given this  pltl approaches will distinguish unnecessarily between past
behaviours which lead to identical future rewards  this may occur when the reward at an
e state is determined by the truth value of f   f    this formula does not necessarily require
e states that distinguish between the cases in which  f      f     and  f     f     
hold  however  given the above specification  pltlmin makes this distinction  for example 

  

fithiebaux  gretton  slaney  price   kabanza

taking fi   pi   figure    shows that fltl leads to an mdp whose size is at most   times
that of the nmrdp  in contrast  the relative size of the mdp produced by pltlmin is
linear in n  the number of rewards and propositions  these results are obtained with all
hand coded domains except spudd expon  figure    shows the run times as a function
of n for complete  fltl dominates and is only overtaken by pltlstr a  for large values
of n  when the mdp becomes too large for explicit exploration to be practical  to obtain
the minimal equivalent mdp using pltlmin  a bloated reward specification of the form
   ni    pi nj   j  i pj     r             ni   pi   n  r   is necessary  which  by virtue of its
exponential length  is not an adequate solution 
    influence of reachability
all approaches claim to have some ability to ignore variables which are irrelevant because
the condition they track is unreachable    pltlmin detects them through preprocessing 
pltlstr exploits the ability of structured solution methods to ignore them  and fltl ignores them when progression never exposes them  however  given that the mechanisms for
avoiding irrelevance are so different  we expect corresponding differences in their effects 
on experimental investigation  we found that the differences in performance are best illustrated by looking at response formulae  which assert that if a trigger condition c is reached
then a reward will be received upon achievement of the goal g in  resp  within  k steps 
  k c  and in  fltl   c  k  g       resp 
in pltl  this is written g  k c  resp  g  
 c  k  g     
when the goal is unreachable  pltl approaches perform well  as it is always false  the
goal g does not lead to behavioural distinctions  on the other hand  while constructing the
mdp  fltl considers the successive progressions of k g without being able to detect that it
is unreachable until it actually fails to happen  this is exactly what the blindness of blind
minimality amounts to  figure    illustrates the difference in performance as a function of
the number n of propositions involved in the spudd linear domain  when the reward is of
the form g  n c  with g unreachable 
fltl shines when the trigger is unreachable  since c never happens  the formula will
always progress to itself  and the goal  however complicated  is never tracked in the generated mdp  in this situation pltl approaches still consider k c and its subformulae  only
to discover  after expensive preprocessing for pltlmin  after reachability analysis for pltlstr a   and never for pltlstr  that these are irrelevant  this is illustrated in figure    
again with spudd linear and a reward of the form g  n c  with c unreachable 
    dynamic irrelevance
earlier we claimed that one advantage of pltlstr and fltl over pltlmin and pltlsim
is that the former perform a dynamic analysis of rewards capable of detecting irrelevance
of variables to particular policies  e g  to the optimal policy  our experiments confirm
this claim  however  as for reachability  whether the goal or the triggering condition in
a response formula becomes irrelevant plays an important role in determining whether a
    here we sometimes speak of conditions and goals being reachable or achievable rather than feasible 
although they may be temporally extended  this is to keep in line with conventional vocabulary as in
the phrase reachability analysis 

  

fidecision theoretic planning with non markovian rewards

total cpu time  sec 

   
   
   
   
  

 

 

 

 

  

  

  

n
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     response formula with unachievable goal

total cpu time  sec 

   
   
   
   
  

 

 

 

 

 

  

n
fltl

pltlmin

pltlstruct

pltlstruct a 

figure     response formula with unachievable trigger
pltlstr or fltl approach should be taken  pltlstr is able to dynamically ignore the goal 
while fltl is able to dynamically ignore the trigger 
this is illustrated in figures    and     in both figures  the domain considered is
on off with n     propositions  the response formula is g  n c as before  here with both
g and c achievable  this response formula is assigned a fixed reward  to study the effect of
dynamic irrelevance of the goal  in figure     achievement of g is rewarded by the value
r  i e  we have  g   r  in pltl   in figure     on the other hand  we study the effect of
dynamic irrelevance of the trigger and achievement of c is rewarded by the value r  both
figures show the runtime of the methods as r increases 
achieving the goal  resp  the trigger  is made less attractive as r increases up to the
point where the response formula becomes irrelevant under the optimal policy  when this
happens  the run time of pltlstr resp  fltl  exhibits an abrupt but durable improvement 
the figures show that fltl is able to pick up irrelevance of the trigger  while pltlstr is able
to exploit irrelevance of the goal  as expected  pltlmin whose analysis is static does not pick
  

fithiebaux  gretton  slaney  price   kabanza

total cpu time  sec 

   
   
   
  

 

  

   

   

   

   

   

   

r
pltlmin

pltlstruct

fltl

pltlstruct  a 

average cpu time  sec 

figure     response formula with unrewarding goal
   
   
   
  

 

  

   

   

   

   

   

   

r
pltlmin

fltl

pltlstruct

pltlstruct a 

figure     response formula with unrewarding trigger
up either and performs consistently badly  note that in both figures  pltlstr progressively
takes longer to compute as r increases because value iteration requires additional iterations
to converge 
    summary
in our experiments with artificial domains  we found pltlstr and fltl preferable to statebased pltl approaches in most cases  if one insists on using the latter  we strongly
recommend preprocessing  fltl is the technique of choice when the reward requires tracking
a long sequence of events or when the desired behaviour is composed of many elements with
identical rewards  for response formulae  we advise the use of pltlstr if the probability of
reaching the goal is low or achieving the goal is very costly  and conversely  we advise the
use of fltl if the probability of reaching the triggering condition is low or if reaching it is
very costly  in all cases  attention should be paid to the syntax of the reward formulae and
  

fidecision theoretic planning with non markovian rewards

in particular to minimising its length  indeed  as could be expected  we found the syntax
of the formulae and the type of non markovian reward they encode to be a predominant
factor in determining the difficulty of the problem  much more so than the features of the
markovian dynamics of the domain 

   a concrete example
our experiments have so far focused on artificial problems and have aimed at characterising
the strengths and weaknesses of the various approaches  we now look at a concrete example
in order to give a sense of the size of more interesting problems that these techniques can
solve  our example is derived from the miconic elevator classical planning benchmark
 koehler   schuster         an elevator must get a number of passengers from their origin
floor to their destination  initially  the elevator is at some arbitrary floor and no passenger
is served nor has boarded the elevator  in our version of the problem  there is one single
action which causes the elevator to service a given floor  with the effect that the unserved
passengers whose origin is the serviced floor board the elevator  while the boarded passengers
whose destination is the serviced floor unboard and become served  the task is to plan the
elevator movement so that all passengers are eventually served   
there are two variants of miconic  in the simple variant  a reward is received each
time a passenger becomes served  in the hard variant  the elevator also attempts to
provide a range of priority services to passengers with special requirements  many passengers
will prefer travelling in a single direction  either up or down  to their destination  certain
passengers might be offered non stop travel to their destination  and finally  passengers
with disabilities or young children should be supervised inside the elevator by some other
passenger  the supervisor  assigned to them  here we omit the vip and conflicting group
services present in the original hard miconic problem  as the reward formulae for those do
not create additional difficulties 
our formulation of the problem makes use of the same propositions as the pddl description of miconic used in the      international planning competition  dynamic propositions
record the floor the elevator is currently at and whether passengers are served or boarded 
and static propositions record the origin and destination floors of passengers  as well as the
categories  non stop  direct travel  supervisor  supervised  the passengers fall in  however 
our formulation differs from the pddl description in two interesting ways  firstly  since
we use rewards instead of goals  we are able to find a preferred solution even when all
goals cannot simultaneously be satisfied  secondly  because priority services are naturally
described in terms of non markovian rewards  we are able to use the same action description for both the simple and hard versions  whereas the pddl description of hard miconic
requires additional actions  up  down  and complex preconditions to monitor the satisfaction of priority service constraints  the reward schemes for miconic can be encapsulated
through four different types of reward formula 
   in the simple variant  a reward is received the first time each passenger pi is served 
    we have experimented with stochastic variants of miconic where passengers have some small probability
of desembarking at the wrong floor  however  we find it more useful to present results for the deterministic
version since it is closer to the miconic deterministic planning benchmark and since  as we have shown
before  rewards have a far more crucial impact than dynamics on the relative performance of the methods 

  

fithiebaux  gretton  slaney  price   kabanza

pltl 

servedpi   fi servedpi

 fltl 

servedpi u  servedpi    

   next  a reward is received each time a non stop passenger pi is served in one step
after boarding the elevator 
pltl 

n onstoppi    boardedpi    servedpi  servedpi

 fltl 

  n onstoppi  boardedpi  servedpi  servedpi      

   then  a reward is received each time a supervised passenger pi is served while having
been accompanied at all times inside the elevator by his supervisor   pj  
pltl 
 fltl 

supervisedpi  supervisorpj pi  servedpi 
 fi servedpi  fi boardedpi  boardedpj  
servedpi u   boardedpi  supervisedpi   boardedpj  supervisorpj pi  
servedpi     servededpi     

   finally  reward is received each time a direct travel passenger pi is served while having
travelled only in one direction since boarding  e g   in the case of going up 
directp
w w i  servedpi  servedpi 
   j k j  atf loork  atf loorj    s  boardedpi  boardedpi   
w w
 fltl    directpi  boardedpi     servedpi u     j k i atf loorj  atf loork  
servedpi     servedpi       

pltl 

and similarly in the case of going down 
experiments in this section were run on a dual pentium     ghz gnu linux       
machine with  gb of ram  we first experimented with the simple variant  giving a reward
of    each time a passenger is first served  figure    shows the cpu time taken by the
various approaches to solve random problems with an increasing number n of floors and
passengers  and figure    shows the number of states expanded when doing so  each data
point corresponds to just one random problem  to be fair with the structured approach  we
ran pltlstr a  which is able to exploit reachability from the start state  a first observation
is that although pltlstr a  does best for small values of n  it quickly runs out of memory 
pltlstr a  and pltlsim both need to track formulae of the form  fi servedpi while
pltlsim does not  and we conjecture that this is why they run out of memory earlier  a
second observation is that attempts at pltl minimisation do not pay very much here 
while pltlmin has reduced memory because it tracks fewer subformulae  the size of the
mdp it produces is identical to the size of the pltlsim mdp and larger than that of the
fltl mdp  this size increase is due to the fact that pltl approaches label differently
e states in which the same passengers are served  depending on who has just become served
 for those passengers  the reward formula is true at the e state   in contrast  our fltl
implementation with progression one step ahead labels all these e states with the reward
    to understand the  fltl formula  observe that we get a reward iff  boardedpi  supervisedpi   
 boardedpj supervisorpj pi   holds until servedpi becomes true  and recall that the formula q u   p
q    q      rewards the holding of p until the occurrence of q 

  

fidecision theoretic planning with non markovian rewards

total cpu time  sec 

    
    
    
    

 

 

 

 

  

  

  

n
fltl
pltlsim
pltlmin
pltlstr a 

figure     simple miconic   run time

  

state count    n 

  
  
  
  
  
  
  
 
 
 

 

 

 

  

  

  

n
fltl
pltlsim  pltlmin

figure     simple miconic   number of expanded states
formulae relevant to the passengers that still need to be served  the other formulae having
progressed to    the gain in number of expanded states materialises into run time gains 
resulting in fltl eventually taking the lead 
our second experiment illustrates the benefits of using an even extremely simple admissible heuristic in conjunction with fltl  our heuristic is applicable to discounted stochastic
shortest path problems  and discounts rewards by the shortest time in the future in which
they are possible  here it simply amounts to assigning a fringe state to a value of    times
the number of still unserved passengers  discounted once   and results in avoiding floors at
which no passenger is waiting and which are not the destination of a boarded passenger 
figures    and    compare the run time and number of states expanded by fltl when used
in conjunction with value iteration  valit  to when it is used in conjunction with an lao 
  

fithiebaux  gretton  slaney  price   kabanza

total cpu time  sec 

     
     
     
    

 

 

 

 

  

  

  

n
fltllao h 
fltllao u 
fltlvalit

figure     effect of a simple heuristic on run time

state count    n 

  
  
  
  
  
 
 

 

 

 

  

  

  

n
fltllao h 
fltlvalit fltllao u 

figure     effect of a simple heuristic on the number of expanded states
search informed by the above heuristic  lao h    uninformed lao   lao  u   i e  lao 
with a heuristic of     n at each node  is also included as a reference point to show the
overhead induced by heuristic search  as can be seen from the graphs  the heuristic search
generates significantly fewer states and this eventually pays in terms of run time 
in our final experiment  we considered the hard variant  giving a reward of    as before
for service      a reward of   for non stop travel      a reward of   for appropriate supervision
     and a reward of    for direct travel      regardless of the number n of floors and
passengers  problems only feature a single non stop traveller  a third of passengers require
supervision  and only half the passengers care about traveling direct  cpu time and number
of states expanded are shown in figures    and     respectively  as in the simple case 
pltlsim and pltlstr quickly run out of memory  formulae of type     and     create too
many additional variables to track for these approaches  and the problem does not seem
  

fidecision theoretic planning with non markovian rewards

total cpu time  sec 

     
    
    
    

 

 

 

 

 

 

n
fltl
pltlsim
pltlmin
pltlstruct a 

figure     hard miconic   run time

state count    n 

   
  
  
  
  
 
 

 

 

 

 

 

n
fltl
pltlsim
pltlmin

figure     hard miconic   number of expanded states
to exhibit enough structure to help pltlstr  fltl remains the fastest  here  this does
not seem to be so much due to the size of the generated mdp which is just slightly below
that of the pltlmin mdp  but rather to the overhead incurred by minimisation  another
observation arising from this experiment is that only very small instances can be handled
in comparison to the classical planning version of the problem solved by state of the art
optimal classical planners  for example  at the      international planning competition 
the propplan planner  fourman        optimally solved instances of hard miconic with
   passengers and    floors in about      seconds on a much less powerful machine 

  

fithiebaux  gretton  slaney  price   kabanza

   nmrdpp in the probabilistic planning competition
we now report on the behaviour of nmrdpp in the probabilistic track of the  th international planning competition  ipc     since the competition did not feature non markovian
rewards  our original motivation in taking part was to further compare the solution methods
implemented in nmrdpp in a markovian setting  this objective largely underestimated the
challenges raised by merely getting a planner ready for a competition  especially when that
competition is the first of its kind  in the end  we decided that successfully preparing nmrdpp to attempt all problems in the competition using one solution method  and possibly
search control knowledge   would be an honorable result 
the most crucial problem we encountered was the translation of ppddl  younes  
littman         the probabilistic variant of pddl used as input language for the competition  into nmrdpps add based input language  while translating ppddl into adds
is possible in theory  devising a translation which is practical enough for the need of the
competition  small number of variables  small  quickly generated  and easily manipulable
adds  is another matter  mtbdd  the translator kindly made available to participants by
the competition organisers  was not always able to achieve the required efficiency  at other
times  the translation was quick but nmrdpp was unable to use the generated adds efficiently  consequently  we implemented a state based translator on top of the pddl parser
as a backup  and opted for a state based solution method since it did not rely on adds
and could operate with both translators 
the version of nmrdpp entered in the competition did the following 
   attempt to get a translation into adds using mtbdd  and if that proves infeasible 
abort it and rely on the state based translator instead 
   run fltl expansion of the state space  taking search control knowledge into account
when available  break after   mn if not complete 
   run value iteration to convergence  failing to achieve any useful result  e g  because
expansion was not complete enough to even reach a goal state   go back to step   
   run as many of the    trials as possible in the remaining time    following the generated policy where defined  and falling back on the non deterministic search control
policy when available 
with step   we were trying to maximise the instances in which the original add based
nmrdpp version could be run intact  in step    it was decided not to use lao  because
when run with no good heuristic  it often incurs a significant overhead compared to value
iteration 
the problems featured in the competition can be classified into goal based or rewardbased problems  in goal based problems  a  positive  reward is only received when a goal
state is reached  in reward based problems  action performance may also incur a  usually
negative  reward  another orthogonal distinction can be made between problems from
    on each given problem  planners had   mn to run whatever computation they saw as appropriate  including parsing  pre processing  and policy generation if any   and execute    trial runs of the generated
policy from an initial state to a goal state 

  

fidecision theoretic planning with non markovian rewards

domains that were not communicated in advance to the participants and those from domains
that were  the latter consisted of variants of blocks world and logistics  or box world 
problems  and gave the participating planners an opportunity to exploit knowledge of the
domain  much as in the hand coded deterministic planning track 
we decided to enroll nmrdpp in a control knowledge mode and in a domain independent
mode  the only difference between the two modes is that the first uses fltl search
control knowledge written for the known domains as additional input  our main concern
in writing the control knowledge was to achieve a reasonable compromise between the size
and effectiveness of the formulae  for the blocks world domain  in which the two actions
pickup from and putdown to had a     chance of dropping the block onto the table  the
control knowledge we used encoded a variant of the well known gn  near optimal strategy
for deterministic blocks world planning  slaney   thiebaux         whenever possible 
try putting a clear block in its goal position  otherwise put an arbitrary clear block on
the table  because blocks get dropped on the table whenever an action fails  and because
the success probabilities and rewards are identical across actions  optimal policies for the
problem are essentially made up of optimal sequences of actions for the deterministic blocks
world and there was little need for a more sophisticated strategy    in the colored blocks
world domain  where several blocks can share the same color and the goal only refers to the
color of the blocks  the control knowledge selected an arbitrary goal state of the non colored
blocks world consistent with the colored goal specification  and then used the same strategy
as for the non colored blocks world  the performance of this strategy depends entirely on
the goal state selected and can therefore be arbitrarily bad 
logistics problems from ipc   distinguish between airports and other locations within
a city  trucks can drive between any two locations in a city and planes can fly between
any two airports  in contrast  the box world only features cities  some of which have an
airport  some of which are only accessible by truck  a priori  the map of the truck and
plane connections is arbitrary  the goal is to get packages from their city of origin to their
city of destination  moving by truck has a     chance of resulting in reaching one of the
three cities closest to the departure city rather than the intended one  the size of the box
world search space turned out to be quite challenging for nmrdpp  therefore  when writing
search control knowledge  we gave up any optimality consideration and favored maximal
pruning  we were helped by the fact that the box world generator produces problems with
the following structure  cities are divided into clusters  all of which are composed of at
least one airport city  furthermore each cluster has at least one hamiltonian circuit which
trucks can follow  the control knowledge we used forced all planes but one  and all trucks
but one in each cluster to be idle  in each cluster  the truck allowed to move could only
attempt driving along the chosen hamiltonian circuit  picking up and dropping parcels as
it went 
the planners participating in the competition are shown in table    planners e  g  
j   and j  are domain specific  either they are tuned for blocks and box worlds  or they use
domain specific search control knowledge  or learn from examples  the other participating
planners are domain independent 
    more sophisticated near optimal strategies for deterministic blocks world exist  see slaney   thiebaux 
       but are much more complex to encode and might have caused time performance problems 

  

fithiebaux  gretton  slaney  price   kabanza

part 
c
e 
g 
g  
j  
j  
j 
p
q
r

description
symbolic lao 
first order heuristic search in the fluent calculus
nmrdpp without control knowledge
nmrdpp with control knowledge
interpreter of hand written classy policies
learns classy policies from random walks
version of ff replanning upon failure
mgpt  lrtdp with automatically extracted heuristics
probaprop  conformant probabilistic planner
structured reachability analysis and structured pi

reference
 feng   hansen       
 karabaev   skvortsova       
this paper
this paper
 fern et al        
 fern et al        
 hoffmann   nebel       
 bonet   geffner       
 onder et al        
 teichteil konigsbuch   fabiani       

table    competition participants  domain specific planners are starred
dom
prob
g  
j  
j  
e 
j 
g 
r
p
c
q

 
   
   
   
   
   

bw c nr
 
  
       
       
       
       
       

bw nc nr
 
   
   
   
   
   

bx nr
          
       
   
   
   
  
   

expl bw
  

hanoise
   

zeno
       

tire nr
    

 



  
  


   
  
   
   
 

  
  
  
  
 
  

   

 

total
   
   
   
   
   
   
   
   
    
  

table    results for goal based problems  domain specific planners are starred  entries
are the percentage of runs in which the goal was reached  a blank indicates that
the planner was unable to attempt the problem  a  indicates that the planner
attempted the problem but was never able to achieve the goal  a   indicates that
the result is unavailable  due to a bug in the evaluation software  a couple of the
results initially announced were found to be invalid  
dom
prob
j  
g  
e 
j  
j 
p
c
g 
r
q

 
   
   
   
   
   

bw c r
 
  
       
       
       
       
       

 
   
   
   
   
   
   
   
   
   
   

 
   
   
   
   
   
   

bw nc r
        
  
               
               
       
    
       

    
    

bx r
                
   
   
   
       

   
   
   


   



   

file
    

tire r
    

  
  



 




  

total
    
    
    
    
    
    
    
   
   
   

table    results for reward based problems  domain specific planners are starred  entries
are the average reward achieved over the    runs  a blank indicates that the
planner was unable to attempt the problem  a  indicates that the planner
attempted the problem but did not achieve a strictly positive reward  a   indicates
that the result is unavailable 
  

fidecision theoretic planning with non markovian rewards

tables   and   show the results of the competition  which we extracted from the competition overview paper  younes  littman  weissmann    asmuth        and from the
competition web site http   www cs rutgers edu  mlittman topics ipc   pt   the
first of those tables concerns goal based problems and the second the reward based problems  the entries in the tables represent the goal achievement percentage or average reward achieved by the various planner versions  left column  on the various problems  top
two rows   planners in the top part of the tables are domain specific  problems from the
known domains lie on the left hand side of the tables  the colored blocks world problems
are bw c nr  goal based version  and bw c r  reward version  with       and    blocks  the
non colored blocks world problems are bw nc nr  goal based version  with   blocks  and bwnc r  reward based version  with                   and    blocks  the box world problems
are bx nr  goal based  and bx r  reward based   with   or    cities and    or    boxes  problems from the unknown domains lie on the right hand side of the tables  they comprise 
expl bw  an exploding version of the    block blocks world problem in which putting down
a block may destroy the object it is put on  zeno  a probabilistic variant of a zeno travel
domain problem from the ipc   with   plane    persons    cities and   fuel levels  hanoise 
a probabilistic variant of the tower of hanoi problem with   disks and   rods  file  a problem
of putting    files in   randomly chosen folders  and tire  a variant a the tire world problem
with    cities and spare tires at   of them  where the tire may go flat while driving 
our planner nmrdpp in its g  or g  version  was able to attempt all problems  achieving a strictly positive reward in all but   of them  not even ff  j    the competition overall
winner  was able to successfully attempt that many problems  nmrdpp performed particularly well on goal based problems  achieving the goal in      of the runs except in expl bw 
hanoise  and tire nr  note that for these three problems  the goal achievement probability of
the optimal policy does not exceed       no other planner outperformed nmrdpp on that
scale  as pointed out before  ff behaves well on the probabilistic version of blocks and box
world because the optimal policies are very close to those for the deterministic problem 
hoffmann        analyses the reasons why the ff heuristic works well for traditional planning benchmarks such as blocks world and logistics  on the other hand  ff is unable to
solve the unknown problems which have a different structure and require more substantial
probabilistic reasoning  although these problems are easily solved by a number of participating planners  as expected  there is a large discrepancy between the version of nmrdpp
allowed to use search control  g   and the domain independent version  g    while the
latter performs okay with the unknown goal based domains  it is not able to solve any of
the known ones  in fact  to except for ff  none of the participating domain independent
planners were able to solve these problems 
in the reward based case  nmrdpp with control knoweldge behaves well on the known
problems  only the human encoded policies  j   performed better  without control knowledge nmrdpp is unable to scale on those problems  while other participants such as ff and
mgpt are  furthermore nmrdpp appears to perform poorly on the two unknown problems 
in both cases  this might be due to the fact that it fails to generate an optimal policy  suboptimal policies easily have a high negative score in these domains  see younes et al         
for r tire  we know that nmrdpp did indeed generate a suboptimal policy  additionally  it
could be that nmrdpp was unlucky with the sampling based policy evaluation process  in

  

fithiebaux  gretton  slaney  price   kabanza

tire r in particular  there was a high variance between the costs of various trajectories in
the optimal policy 
alltogether  the competition results suggest that control knowledge is likely to be essential when solving larger problems  markovian or not  with nmrdpp  and that  as has
been observed with deterministic planners  approaches making use of control knowledge are
quite powerful 

   conclusion  related  and future work
in this paper  we have examined the problem of solving decision processes with nonmarkovian rewards  we have described existing approaches which exploit a compact representation of the reward function to automatically translate the nmrdp into an equivalent
process amenable to mdp solution methods  the computational model underlying this
framework can be traced back to work on the relationship between linear temporal logic
and automata in the areas of automated verification and model checking  vardi       
wolper         while remaining in this framework  we have proposed a new representation
of non markovian reward functions and a translation into mdps aimed at making the best
possible use of state based anytime heuristic search as the solution method  our representation extends future linear temporal logic to express rewards  our translation has the
effect of embedding model checking in the solution method  it results in an mdp of the
minimal size achievable without stepping outside the anytime framework  and consequently
in better policies by the deadline  we have described nmrdpp  a software platform that
implements such approaches under a common interface  and which proved a useful tool in
their experimental analysis  both the system and the analysis are the first of their kind 
we were able to identify a number of general trends in the behaviours of the methods and
to provide advice as to which are the best suited to certain circumstances  for obvious
reasons  our analysis has focused on artificial domains  additional work should examine a
wider range of domains of more practical interest  to see what form these results take in that
context  ultimately  we would like our analysis to help nmrdpp automatically select the
most appropriate method  unfortunately  because of the difficulty of translating between
pltl and  fltl  it is likely that nmrdpp would still have to maintain both a pltl and
a  fltl version of the reward formulae 
a detailed comparison of our approach to solving nmrdps with existing methods  bacchus et al               can be found in sections      and    two important aspects of future
work would help take the comparison further  one is to settle the question of the appropriateness of our translation to structured solution methods  symbolic implementations of
the solution methods we consider  e g  symbolic lao   feng   hansen         as well as
formula progression in the context of symbolic state representations  pistore   traverso 
      could be investigated for that purpose  the other is to take advantage of the greater
expressive power of  fltl to consider a richer class of decision processes  for instance with
uncertainty as to which rewards are received and when  many extensions of the language
are possible  adding eventualities  unrestricted negation  first class reward propositions 
quantitative time  etc  of course  dealing with them via progression without backtracking
is another matter 

  

fidecision theoretic planning with non markovian rewards

we should investigate the precise relationship between our line of work and recent work
on planning for temporally extended goals in non deterministic domains  of particular
interest are weak temporally extended goals such as those expressible in the eagle language
 dal lago et al          and temporally extended goals expressible in  ctl   baral  
zhao         eagle enables the expression of attempted reachability and maintenance goals
of the form try reach p and try maintain p  which add to the goals do reach p and
do maintain p already expressible in ctl  the idea is that the generated policy should
make every attempt at satisfying proposition p  furthermore  eagle includes recovery goals
of the form g  fail g    meaning that goal g  must be achieved whenever goal g  fails  and
cyclic goals of the form repeat g  meaning that g should be achieved cyclically until it
fails  the semantics of these goals is given in terms of variants of buchi tree automata
with preferred transitions  dal lago et al         present a planning algorithm based on
symbolic model checking which generates policies achieving those goals  baral and zhao
       describe  ctl   an alternative framework for expressing a subset of eagle goals
and a variety of others   ctl  is a variant of ctl  which allows for formulae involving
two types of path quantifiers  quantifiers tied to the paths feasible under the generated
policy  as is usual  but also quantifiers more generally tied to the paths feasible under any
of the domain actions  baral and zhao        do not present any planning algorithm  it
would be very interesting to know whether eagle and  ctl  goals can be encoded as nonmarkovian rewards in our framework  an immediate consequence would be that nmrdpp
could be used to plan for them  more generally  we would like to examine the respective
merits of non deterministic planning for temporally extended goals and decision theoretic
planning with non markovian rewards 
in the pure probabilistic setting  no rewards   recent related research includes work on
planning and controller synthesis for probabilistic temporally extended goals expressible in
probabilistic temporal logics such as csl or pctl  younes   simmons        baier et al  
       these logics enable expressing statements about the probability of the policy satisfying a given temporal goal exceeding a given threshold  for instance  younes and simmons
       describe a very general probabilistic planning framework  involving concurrency  continuous time  and temporally extended goals  rich enough to model generalised semi markov
processes  the solution algorithms are not directly comparable to those presented here 
another exciting future work area is the investigation of temporal logic formalisms for
specifying heuristic functions for nmrdps or more generally for search problems with
temporally extended goals  good heuristics are important to some of the solution methods
we are targeting  and surely their value ought to depend on history  the methods we have
described could be applicable to the description and processing of such heuristics  related
to this is the problem of extending search control knowledge to fully operate under the
presence of temporally extended goals  rewards  and stochastic actions  a first issue is
that branching or probabilistic logics such as ctl or pctl variants should be preferred
to fltl when describing search control knowledge  because when stochastic actions are
involved  search control often needs to refer to some of the possible futures and even to
their probabilities    another major problem is that the goalp modality  which is the
key to the specification of reusable search control knowledge is interpreted with respect to
    we would not argue  on the other hand  that ctl is necessary for representing non markovian rewards 

  

fithiebaux  gretton  slaney  price   kabanza

a fixed reachability goal    bacchus   kabanza         and as such  is not applicable to
domains with temporally extended goals  let alone rewards  kabanza and thiebaux       
present a first approach to search control in the presence of temporally extended goals in
deterministic domains  but much remains to be done for a system like nmrdpp to be able
to support a meaningful extension of goalp 
finally  let us mention that related work in the area of databases uses a similar approach
to pltlstr to extend a database with auxiliary relations containing sufficient information
to check temporal integrity constraints  chomicki         the issues are somewhat different
from those raised by nmrdps  as there is only ever one sequence of databases  what matters
is more the size of these auxiliary relations than avoiding making redundant distinctions 

acknowledgements
many thanks to fahiem bacchus  rajeev gore  marco pistore  ron van der meyden  moshe
vardi  and lenore zuck for useful discussions and comments  as well as to the anonymous
reviewers and to david smith for their thorough reading of the paper and their excellent
suggestions  sylvie thiebaux  charles gretton  john slaney  and david price thank national ict australia for its support  nicta is funded through the australian governments
backing australias ability initiative  in part through the australian research council  froduald kabanza is supported by the canadian natural sciences and engineering research
council  nserc  

appendix a  a class of reward normal formulae
the existing decision procedure  slaney        for determining whether a formula is rewardnormal is guaranteed to terminate finitely  but involves the construction and comparison of
automata and is rather intricate in practice  it is therefore useful to give a simple syntactic
characterisation of a set of constructors for obtaining reward normal formulae even though
not all such formulae are so constructible 
we say that a formula is material iff it contains no   and no temporal operators  that
is  the material formulae are the boolean combinations of atoms 
we consider four operations on behaviours representable by formulae of  fltl  firstly 
a behaviour may be delayed for a specified number of timesteps  secondly  it may be made
conditional on a material trigger  thirdly  it may be started repeatedly until a material
termination condition is met  fourthly  two behaviours may be combined to form their
union  these operations are easily realised syntactically by corresponding operations on
formulae  where m is any material formula 
delay f    

f

cond m  f     m  f
loop m  f     f u m
union f    f      f   f 
    where f is an atemporal formula  goalp f   is true iff f is true of all goal states 

  

fidecision theoretic planning with non markovian rewards

we have shown  slaney        that the set of reward normal formulae is closed under delay 
cond  for any material m   loop  for any material m  and union  and also that the closure
of     under these operations represents a class of behaviours closed under intersection and
concatenation as well as union 
many familiar reward normal formulae are obtainable from   by applying the four operations  for example   p     is loop   cond p       sometimes a paraphrase is necessary 
for example    pq      is not of the required form because of the  in the antecedent
of the conditional  but the equivalent  p   q      is loop   cond p  delay cond q        
other cases are not so easy  an example is the formula p u  p   which stipulates a reward
the first time p happens and which is not at all of the form suggested  to capture the same
behaviour using the above operations requires a formula like  p        p     u p  

appendix b  proofs of theorems
property   where b    i   b      i    b f iff    i        b prog b  i   f   
proof 
induction on the structure of f   there are several base cases  all fairly trivial 
if f     or f    there is nothing to prove  as these progress to themselves and hold
everywhere and nowhere respectively  if f   p then if f holds in i then it progresses to  
which holds in i   while if f does not hold in i then it progresses to  which does not
hold in i     the case f   p is similar  in the last base case  f      then the following
are equivalent 
   i    b f
 i   b
b
prog b  i   f      
   i        b prog b  i   f  
induction case    f   g  h  the following are equivalent 
   i    b f
   i    b g and    i    b h
   i        b prog b  i   g  and    i        b prog b  i   h   by induction hypothesis 
   i        b prog b  i   g   prog b  i   h 
   i        b prog b  i   f  
induction case    f   g  h  analogous to case   
induction case    f   g  trivial by inspection of the definitions 
induction case    f   g u h  then f is logically equivalent to h   g   g u h  which by
cases      and   holds at stage i of  for behaviour b iff prog b  i   f   holds at stage i   

theorem   let f be reward normal  and let hf    f         i be the result of progressing it
through the successive states of a sequence   then  provided no fi is   for all i rew i   fi  
iff  i   bf  

  

fithiebaux  gretton  slaney  price   kabanza

proof  first  by the definition of reward normality  if f is reward normal then    b f iff
for all i  if  i   bf then  i   b  next  if    b f then progressing f through  according
to b  that is  letting each bi be true iff  i   b  cannot lead to a contradiction because
by property    progression is truth preserving 
it remains  then  to show that if     b f then progressing f through  according to b
must lead eventually to   the proof of this is by induction on the structure of f and as
usual the base case in which f is a literal  an atom  a negated atom or     or    is trivial 
case f   g  h  suppose     b f   then either     b g or     b h  so by the induction
hypothesis either g or h progresses eventually to   and hence so does their conjunction 
case f   g  h  suppose     b f   then both     b g and     b h  so by the induction
hypothesis each of g and h progresses eventually to   suppose without loss of generality
that g does not progress to  before h does  then at some point g has progressed to some
formula g   and f has progressed to g     which simplifies to g     since g   also progresses to
 eventually  so does f  
case f   g  suppose     b f   let         and let b             b   then
    b   g  so by the induction hypothesis g progressed through  according to b   eventually
reaches   but the progression of f through  according to b is exactly the same after
the first step  so that too leads to  
case f   g u h  suppose     b f   then there is some j such that    j     b g and for all
i  j     i     b h  we proceed by induction on j  in the base case j      and both     b g
and     b h whence by the main induction hypothesis both g and h will eventually progress
to   thus h   g  f     progresses eventually to  for any f     and in particular for f     f  
establishing the base case  for the induction case  suppose    b g  and of course     b h  
since f is equivalent to h   g  f   and     b f       b h and    b g  clearly     b f   where
 and b   are as in the previous case  therefore      b   f and the failure occurs at stage j   
of   therefore the hypothesis of the induction on j applies  and f progressed through 
according to b   goes eventually to   and so f progressed through  according to b goes
similarly to  

theorem   let s   be the set of e states in an equivalent mdp d  for d   hs  s    a  pr  ri 
d  is minimal iff every e state in s   is reachable and s   contains no two distinct e states s  
and s   with   s         s     and  s        s     
proof  proof is by construction of the canonical equivalent mdp dc   let the set of
e     be partitioned into equivalence classes  where
finite prefixes of state sequences in d s
e      r   i      
  i     j  iff  i    j and for all   s  such that   i     d s
r   j      let   i   denote the equivalence class of  i   let e be the set of these
equivalence classes  let a be the function that takes each   i   in e to a i    for each
 i  and  j  and for each a  a   i     let t    i    a    j    be pr i   a  s  if   j    
  i   hsi   otherwise let t    i    a    j         let r   i    be r  i    then note the
following four facts 
   each of the functions a  t and r is well defined 
   dc   he   hs  i   a  t   ri is an equivalent mdp for d with     i      i  

  

fidecision theoretic planning with non markovian rewards

   for any equivalent mdp d   of d there is a mapping from a subset of the states of
d   onto e 
   d  satisfies the condition that every e state in s   is reachable and s   contains no two
distinct e states s   and s   with   s         s     and  s        s     iff dc is isomorphic
to d   
what fact   above amounts to is that if   i     j  then it does not matter which of
the two sequences is used to define a  t and r of their equivalence class  in the cases of
a and t this is simply that  i    j   in the case of r  it is the special case    h i i of
the equality of rewards over extensions 
fact   is a matter of checking that the four conditions of definition   hold  of these 
conditions       s       s    and    a   i      a i    hold trivially by the construction 
e      we have r   i      r  i  
condition   says that for any feasible state sequence   d s
for all i  this also is given in the construction  condition   states 
for all s    s   s  if there is a  a s    such that pr s    a  s         then for all
e     such that i   s    there exists a unique   j    e  j   s    such
 i   d s
that for all a  a   i     t    i    a    j      pr s    a  s    
e     and i   s    then the required  j  is  i   hs  i 
suppose pr s      s          i   d s
and of course a   i      a i    so the required condition reads 
  i   hs  i  is the unique element x of e with   x    s  such that for all a 
a i    t    i    a  x    pr s    a  s    
to establish existence  we need that if a  a i   then t    i    a    i   hs  i     pr i   a  s    
which is immediate from the definition of t above  to establish uniqueness  suppose that
  x    s  and t    i    a  x    pr s    a  s    for all actions a  a i    since pr s      s     
   the transition probability from   i   to x is nonzero for some action  so by the definition
of t   x can only be   i   hs  i  
fact   is readily observed  let m be any equivalent mdp for d  for any states s 
and s  of d  and any state x of m such that   x    s  there is at most one state y
of m with   y     s  such that some action a  a s    gives a nonzero probability of
transition from x to y   this follows from the uniqueness part of condition   of definition  
together with the fact that the transition function is a probability distribution  sums to    
therefore for any given finite state sequence  i  there is at most one state of m reached
from the start state of m by following  i   therefore m induces an equivalence relation
m on s     i  m  j  iff they lead to the same state of m  the sequences which are not
feasible in m may all be regarded as equivalent under m    each reachable state of m has
associated with it a nonempty equivalence class of finite sequences of states of d  working
through the definitions  we may observe that m is a sub relation of   if  i  m  j 
then  i    j    hence the function that takes the equivalence class under m of each
feasible sequence  i  to   i   induces a mapping h  an epimorphism in fact  from the
reachable subset of states of m onto e 
to establish fact    it must be shown that in the case of d  the mapping can be
reversed  or that each equivalence class   i   in dc corresponds to exactly one element of
  

fithiebaux  gretton  slaney  price   kabanza

e   
d    suppose not  for contradiction   then there exist sequences   i  and   j  in d s
 
such that   i     j  but on following the two sequences from s  we arrive at two different
elements s   and s   of d  with   s        i    j     s     but with  s         s      therefore
e
there exists a sequence  k   d s 
such that r   i       k      r   j       k   
but this contradicts the condition for   i     j  

theorem   follows immediately from facts    
theorem   let d  be the translation of d as in definition    d  is a blind minimal
equivalent mdp for d 
proof  reachability of all the e states is obvious  as they are constructed only when
reached  each e state is a pair hs  i where s is a state of d and  is a reward function
specification  in fact  s     hs  i  and  determines a distribution of rewards over all
continuations of the sequences
that reach hs  i  that is  for all  in s  such that     s 
p
the reward for  is  f  r   r     bf    if d  is not blind minimal  then there exist
distinct e states hs  i and hs    i for which this sum is the same for all   but this makes
 and   semantically equivalent  contradicting the supposition that they are distinct 


appendix c  random problem domains
random problem domains are produced by first creating a random action specification
defining the domain dynamics  some of the experiments we conducted   also involved
producing  in a second step  a random reward specification that had desired properties in
relation to the generated dynamics 
the random generation of the domain dynamics takes as parameters the number n
of propositions in the domain and the number of actions to be produced  and starts by
assigning some effects to each action such that each proposition is affected by exactly one
action  for example  if we have   actions and    propositions  the first   actions may affect
  propositions each  the  th one only    and the affected propositions are all different  once
each action has some initial effects  we continue to add more effects one at a time  until a
sufficient proportion of the state space is reachable  see proportion reachable parameter
below  each additional effect is generated by picking up a random action and a random
proposition  and producing a random decision diagram according to the uncertainty and
structure parameters below 
the uncertainty parameter is the probability of a non zero one value as a leaf node  an
uncertainty of   will result in all leaf nodes having random values from a uniform
distribution  an uncertainty of   will result in all leaf nodes having values   or   with
an equal probability 
the structure  or influence  parameter is the probability of a decision diagram containing
a particular proposition  so an influence of   will result in all decision diagrams
    none of those are included in this paper  however 

  

fidecision theoretic planning with non markovian rewards

including all propositions  and very unlikely to have significant structure   while  
will result in decision diagrams that do not depend on the values of propositions 
the proportion reachable parameter is a lower bound on the proportion of the entire  n
state space that is reachable from the start state  the algorithm adds behaviour until
this lower bound is reached  a value of   will result in the algorithm running until
the actions are sufficient to allow the entire state space to be reachable 
a reward specification can be produced with regard to the generated dynamics such that
a specified number of the rewards are reachable and a specified number are unreachable 
first  a decision diagram is produced to represent which states are reachable and which
are not  given the domain dynamics  next  a random path is taken from the root of this
decision diagram to a true terminal if we are generating an attainable reward  or a false
terminal if we are producing an unattainable reward  the propositions encountered on this
path  both negated and not  form a conjunction that is the reward formula  this process
is repeated until the desired number of reachable and unreachable rewards are obtained 

references
at t labs research         graphviz  available from http   www research att com 
sw tools graphviz  
bacchus  f   boutilier  c     grove  a          rewarding behaviors  in proc  american
national conference on artificial intelligence  aaai   pp           
bacchus  f   boutilier  c     grove  a          structured solution methods for nonmarkovian decision processes  in proc  american national conference on artificial
intelligence  aaai   pp         
bacchus  f     kabanza  f          planning for temporally extended goals  annals of
mathematics and artificial intelligence          
bacchus  f     kabanza  f          using temporal logic to express search control knowledge
for planning  artificial intelligence            
baier  c   groer  m   leucker  m   bollig  b     ciesinski  f          controller synthesis
for probabilistic systems  extended abstract   in proc  ifip international conference
on theoretical computer science  ifip tcs  
baral  c     zhao  j          goal specification in presence of nondeterministic actions  in
proc  european conference on artificial intelligence  ecai   pp         
barto  a   bardtke  s     singh  s          learning to act using real time dynamic programming  artificial intelligence            
bonet  b     geffner  h          labeled rtdp  improving the convergence of real time
dynamic programming  in proc  international conference on automated planning
and scheduling  icaps   pp       

  

fithiebaux  gretton  slaney  price   kabanza

bonet  b     geffner  h          mgpt  a probabilistic planner based on heuristic search 
journal of artificial intelligence research             
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  in journal of artificial intelligence research 
vol      pp      
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
with factored representations  artificial intelligence                   
calvanese  d   de giacomo  g     vardi  m          reasoning about actions and planning in ltl action theories  in proc  international conference on the principles of
knowledge representation and reasoning  kr   pp         
cesta  a   bahadori  s   g  c   grisetti  g   giuliani  m   loochi  l   leone  g   nardi  d  
oddi  a   pecora  f   rasconi  r   saggase  a     scopelliti  m          the robocare
project  cognitive systems for the care of the elderly  in proc  international conference
on aging  disability and independence  icadi  
chomicki  j          efficient checking of temporal integrity constraints using bounded
history encoding  acm transactions on database systems                 
dal lago  u   pistore  m     traverso  p          planning with a language for extended
goals  in proc  american national conference on artificial intelligence  aaai   pp 
       
dean  t   kaelbling  l   kirman  j     nicholson  a          planning under time constraints in stochastic domains  artificial intelligence           
dean  t     kanazawa  k          a model for reasoning about persistance and causation 
computational intelligence            
drummond  m          situated control rules  in proc  international conference on the
principles of knowledge representation and reasoning  kr   pp         
emerson  e  a          temporal and modal logic  in handbook of theoretical computer
science  vol  b  pp           elsevier and mit press 
feng  z     hansen  e          symbolic lao search for factored markov decision processes  in proc  american national conference on artificial intelligence  aaai   pp 
       
feng  z   hansen  e     zilberstein  s          symbolic generalization for on line planning 
in proc  conference on uncertainty in artificial intelligence  uai   pp         
fern  a   yoon  s     givan  r          learning domain specific knowledge from random
walks  in proc  international conference on automated planning and scheduling
 icaps   pp         
fourman  m          propositional planning  in proc  aips workshop on model theoretic
approaches to planning  pp       
  

fidecision theoretic planning with non markovian rewards

gretton  c   price  d     thiebaux  s       a   implementation and comparison of solution
methods for decision processes with non markovian rewards  in proc  conference on
uncertainty in artificial intelligence  uai   pp         
gretton  c   price  d     thiebaux  s       b   nmrdpp  a system for decision theoretic
planning with non markovian rewards  in proc  icaps workshop on planning under
uncertainty and incomplete information  pp       
haddawy  p     hanks  s          representations for decision theoretic planning  utility
functions and deadline goals  in proc  international conference on the principles of
knowledge representation and reasoning  kr   pp       
hansen  e     zilberstein  s          lao   a heuristic search algorithm that finds solutions
with loops  artificial intelligence            
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning using
decision diagrams  in proc  conference on uncertainty in artificial intelligence  uai  
pp         
hoffmann  j          local search topology in planning benchmarks  a theoretical analysis 
in proc  international conference on ai planning and scheduling  aips   pp        
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research             
howard  r          dynamic programming and markov processes  mit press  cambridge 
ma 
kabanza  f     thiebaux  s          search control in planning for temporally extended
goals  in proc  international conference on automated planning and scheduling
 icaps   pp         
karabaev  e     skvortsova  o          a heuristic search algorithm for solving firstorder mdps  in proc  conference on uncertainty in artificial intelligence  uai  
pp         
koehler  j     schuster  k          elevator control as a planning problem  in proc 
international conference on ai planning and scheduling  aips   pp         
korf  r          real time heuristic search  artificial intelligence             
kushmerick  n   hanks  s     weld  d          an algorithm for probabilistic planning 
artificial intelligence             
lichtenstein  o   pnueli  a     zuck  l          the glory of the past  in proc  conference
on logics of programs  pp          lncs  volume     
onder  n   whelan  g  c     li  l          engineering a conformant probabilistic planner 
journal of artificial intelligence research          

  

fithiebaux  gretton  slaney  price   kabanza

pistore  m     traverso  p          planning as model checking for extended goals in
non deterministic domains  in proc  international joint conference on artificial intelligence  ijcai      pp         
slaney  j          semi positive ltl with an uninterpreted past operator  logic journal of
the igpl             
slaney  j     thiebaux  s          blocks world revisited  artificial intelligence      
       
somenzi  f         
cudd  cu decision diagram package 
ftp   vlsi colorado edu pub  

available from

teichteil konigsbuch  f     fabiani  p          symbolic heuristic policy iteration algorithms for structured decision theoretic exploration problems  in proc  icaps workshop on planning under uncertainty for autonomous systems 
thiebaux  s   hertzberg  j   shoaff  w     schneider  m          a stochastic model of
actions and plans for anytime planning under uncertainty  international journal of
intelligent systems                 
thiebaux  s   kabanza  f     slaney  j       a   anytime state based solution methods for
decision processes with non markovian rewards  in proc  conference on uncertainty
in artificial intelligence  uai   pp         
thiebaux  s   kabanza  f     slaney  j       b   a model checking approach to decisiontheoretic planning with non markovian rewards  in proc  ecai workshop on modelchecking in artificial intelligence  mochart      pp         
vardi  m          automated verification   graph  logic  and automata  in proc  international joint conference on artificial intelligence  ijcai   pp          invited
paper 
wolper  p          on the relation of programs and computations to models of temporal
logic  in proc  temporal logic in specification  lncs      pp        
younes  h  l  s     littman  m          ppddl     an extension to pddl for expressing
planning domains with probabilistic effects  tech  rep  cmu cs         school of
computer science  carnegie mellon university  pittsburgh  pennsylvania 
younes  h  l  s   littman  m   weissmann  d     asmuth  j          the first probabilistic
track of the international planning competition  in journal of artificial intelligence
research  vol      pp         
younes  h     simmons  r  g          policy generation for continuous time stochastic
domains with concurrency  in proc  international conference on automated planning
and scheduling  icaps   pp         

  

fi