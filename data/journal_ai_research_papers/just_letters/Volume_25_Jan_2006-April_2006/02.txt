journal of artificial intelligence research                 

submitted        published     

approximate policy iteration with a policy language bias 
solving relational markov decision processes
alan fern

afern cs orst edu
school of electrical engineering and computer science  oregon state university
sungwook yoon
sy purdue edu
robert givan
givan purdue edu
school of electrical and computer engineering  purdue university

abstract
we study an approach to policy selection for large relational markov decision processes
 mdps   we consider a variant of approximate policy iteration  api  that replaces the
usual value function learning step with a learning step in policy space  this is advantageous
in domains where good policies are easier to represent and learn than the corresponding
value functions  which is often the case for the relational mdps we are interested in 
in order to apply api to such problems  we introduce a relational policy language and
corresponding learner  in addition  we introduce a new bootstrapping routine for goalbased planning domains  based on random walks  such bootstrapping is necessary for
many large relational mdps  where reward is extremely sparse  as api is ineffective in
such domains when initialized with an uninformed policy  our experiments show that the
resulting system is able to find good policies for a number of classical planning domains
and their stochastic variants by solving them as extremely large relational mdps  the
experiments also point to some limitations of our approach  suggesting future work 

   introduction
many planning domains are most naturally represented in terms of objects and relations
among them  accordingly  ai researchers have long studied algorithms for planning and
learning to plan in relational state and action spaces  these include  for example  classical
strips domains such as the blocks world and logistics 
a common criticism of such domains and algorithms is the assumption of an idealized 
deterministic world model  this  in part  has led ai researchers to study planning and
learning within a decision theoretic framework  which explicitly handles stochastic environments and generalized reward based objectives  however  most of this work is based on
explicit or propositional state space models  and so far has not demonstrated scalability to
the large relational domains that are commonly addressed in classical planning 
intelligent agents must be able to simultaneously deal with both the complexity arising
from relational structure and the complexity arising from uncertainty  the primary goal
of this research is to move toward such agents by bridging the gap between classical and
decision theoretic techniques 
in this paper  we describe a straightforward and practical method for solving very large 
relational mdps  our work can be viewed as a form of relational reinforcement learning
 rrl  where we assume a strong simulation model of the environment  that is  we assume
access to a black box simulator  for which we can provide any  relationally represented 
c
    
ai access foundation  all rights reserved 

fifern  yoon    givan

state action pair and receive a sample from the appropriate next state and reward distributions  the goal is to interact with the simulator in order to learn a policy for achieving high
expected reward  it is a separate challenge  not considered here  to combine our work with
methods for learning the environment simulator to avoid dependence on being provided
such a simulator 
dynamic programming approaches to finding optimal control policies in mdps  bellman        howard         using explicit  flat  state space representations  break down
when the state space becomes extremely large  more recent work extends these algorithms
to use propositional  boutilier   dearden        dean   givan        dean  givan   
leach        boutilier  dearden    goldszmidt        givan  dean    greig        guestrin 
koller  parr    venkataraman      b  as well as relational  boutilier  reiter    price       
guestrin  koller  gearhart    kanodia      a  state space representations  these extensions have significantly expanded the set of approachable problems  but have not yet shown
the capacity to solve large classical planning problems such as the benchmark problems
used in planning competitions  bacchus         let alone their stochastic variants  one possible reason for this is that these methods are based on calculating and representing value
functions  for familiar strips planning domains  among others   useful value functions
can be difficult to represent compactly  and their manipulation becomes a bottle neck 
most of the above techniques are purely deductivethat is  each value function is guaranteed to have a certain level of accuracy  rather  in this work  we will focus on inductive
techniques that make no such guarantees in practice  most existing inductive forms of approximate policy iteration  api  utilize machine learning to select compactly represented
approximate value functions at each iteration of dynamic programming  bertsekas   tsitsiklis         as with any machine learning algorithm  the selection of the hypothesis space 
here a space of value functions  is critical to performance  an example space used frequently
is the space of linear combinations of a human selected feature set 
to our knowledge  there has been no previous work that applies any form of api to
benchmark problems from classical planning  or their stochastic variants   again  one
reason for this is the high complexity of typical value functions for these large relational
domains  making it difficult to specify good value function spaces that facilitate learning 
comparably  it is often much easier to compactly specify good policies  and accordingly
good policy spaces for learning  this observation is the basis for recent work on inductive policy selection in relational planning domains  both deterministic  khardon      a 
martin   geffner         and probabilistic  yoon  fern    givan         these techniques
show that useful policies can be learned using a policy space bias described by a generic
 relational  knowledge representation language  here we incorporate those ideas into a variant of api  that achieves significant success without representing or learning approximate
value functions  of course  a natural direction for future work is to combine policy space
techniques with value function techniques  to leverage the advantages of both 
given an initial policy  our approach uses the simulation technique of policy rollout
 tesauro   galperin        to generate trajectories of an improved policy  these trajectories are then given to a classification learner  which searches for a classifier  or policy  that
matches the trajectory data  resulting in an approximately improved policy  these two
   recent work in relational reinforcement learning has been applied to strips problems with much simpler
goals than typical benchmark planning domains  and is discussed in section   

  

fiapi with a policy language bias

steps are iterated until no further improvement is observed  the resulting algorithm can be
viewed as a form of api where the iteration is carried out without inducing approximate
value functions 
by avoiding value function learning  this algorithm helps address the representational
challenge of applying api to relational planning domains  however  another fundamental
challenge is that  for non trivial relational domains  api requires some form of bootstrapping  in particular  for most strips planning domains the reward  which corresponds to
achieving a goal condition  is sparsely distributed and unlikely to be reached by random exploration  thus  initializing api with a random or uninformed policy  will likely result in no
reward signal and hence no guidance for policy improvement  one approach to bootstrapping is to rely on the user to provide a good initial policy or heuristic that gives guidance
toward achieving reward  rather  in this work we develop a new automatic bootstrapping
approach for goal based planning domains  which does not require user intervention 
our bootstrapping approach is based on the idea of random walk problem distributions 
for a given planning domain  such as the blocks world  this distribution randomly generates
a problem  i e   an initial state and a goal  by selecting a random initial state and then
executing a sequence of n random actions  taking the goal condition to be a subset of
properties from the resulting state  the problem difficulty typically increases with n  and
for small n  short random walks  even random policies can uncover reward  intuitively  a
good policy for problems with walk length n can be used to bootstrap api for problems with
slightly longer walk lengths  our bootstrapping approach iterates this idea  by starting with
a random policy and very small n  and then gradually increasing the walk length until we
learn a policy for very long random walks  such long random walk policies clearly capture
much domain knowledge  and can be used in various ways  here  we show that empirically
such policies often perform well on problem distributions from relational domains used in
recent deterministic and probabilistic planning competitions 
an implementation of this bootstrapped api approach took second place of   competitors in the hand tailored track of the      international probabilistic planning competition   to our knowledge this is the first machine learning based system to be entered in
any planning competition  either deterministic or probabilistic 
here  we give an evaluation of our system on a number of probabilistic and deterministic
relational planning domains  including the aips      competition benchmarks  and benchmarks from the hand tailored track of the      probabilistic planning competition  the
results show that the system is often able to learn policies in these domains that perform
well for long random walk problems  in addition  these same policies often perform well on
the planning competition problem distributions  comparing favorably with the state of theart planner ff in the deterministic domains  our experiments also highlight a number of
limitations of the current system  which point to interesting directions for future work 
the remainder of paper proceeds as follows  in section    we introduce our problem
setup and then  in section    present our new variant of api  in section    we provide some
   note  however  that this approach is not hand tailored  rather  given a domain definition  our system
learns a policy offline  automatically  which can then be applied to any problem from the domain  we
entered the hand tailored track because it was the only track that facilitated the use of offline learning 
by providing domains and problem generators before the competition  the other entrants were humanwritten for each domain 

  

fifern  yoon    givan

technical analysis of the algorithm  giving performance bounds on the policy improvement
step  in sections   and    we describe an implemented instantiation of our api approach
for relational planning domains  this includes a description of a generic policy language
for relational domains  a classification learner for that language  and a novel bootstrapping
technique for goal based domains  section   presents our empirical results  and finally
sections   and   discuss related work and future directions 

   problem setup
we formulate our work in the framework of markov decision processes  mdps   while
our primary motivation is to develop algorithms for relational planning domains  we first
describe our problem setup and approach for a general  action simulatorbased mdp representation  later  in section    we describe a particular representation of planning domains
as relational mdps and the corresponding relational instantiation of our approach 
following and adapting kearns  mansour  and ng        and bertsekas and tsitsiklis
        we represent an mdp using a generative model hs  a  t  r  ii  where s is a finite
set of states  a is a finite  ordered set of actions  and t is a randomized action simulation
algorithm that  given state s and action a  returns a next state s  according to some unknown
probability distribution pt  s   s  a   the component r is a reward function that maps s  a
to real numbers  with r s  a  representing the reward for taking action a in state s  and i
is a randomized initial state algorithm with no inputs that returns a state s according to
some unknown distribution p   s   we sometimes treat i and t  s  a  as random variables
with distributions p     and pt   s  a  respectively 
for an mdp m   hs  a  t  r  ii  a policy  is a  possibly stochastic  mapping from s to
a  the value function of   denoted v   s   represents the expected  cumulative  discounted
reward of following policy  in m starting from state s  and is the unique solution to
v   s    e r s   s     v   t  s   s    

   

where         is the discount factor  the q value function q  s  a  represents the
expected  cumulative  discounted reward of taking action a in state s and then following  
and is given by
q  s  a    r s  a    e v   t  s  a   

   

we will measure the quality of a policy by the objective function v      e v   i    giving
the expected value obtained by that policy when starting from a randomly drawn initial
state  a common objective in mdp planning and reinforcement learning is to find an
optimal policy     argmax v     however  no automated technique  including the one
we present here  has to date been able to guarantee finding an optimal policy in the relational
planning domains we consider  in reasonable running time 
it is a well known fact that given a current policy   we can define a new improved
policy
pi   s    argmaxaa q  s  a 

   

such that the value function of pi  is guaranteed to    be no worse than that of  at each
state s  and    strictly improve at some state when  is not optimal  policy iteration is an
  

fiapi with a policy language bias

algorithm for computing optimal policies by iterating policy improvement  pi  from any
initial policy to reach a fixed point  which is guaranteed to be an optimal policy  each
iteration of policy improvement involves two steps     policy evaluation where we compute
the value function v  of the current policy   and    policy selection  where  given v  from
step    we select the action that maximizes q  s  a  at each state  defining a new improved
policy 
finite horizons  since our api variant is based on simulation  and must bound the
simulation trajectories by a horizon h  our technical analysis in section   will use the notion
of finite horizon discounted reward  the h horizon value function vh is recursively defined
as
v   s      

vh  s    e r s   s     vh   t  s   s    

   

giving the expected discounted reward obtained by following  for h steps from s  we also
  t  s  a     and the h horizon
define the h horizon q function qh  s  a    r s  a    e vh 

objective function v h      e vh  i    it is well known  that the effect of using a finite
horizon can be made arbitrarily small  in particular  we have that for all states s and
actions a  the approximation error decreases exponentially with h 
 v   s   vh  s     h vmax  
 q  s  a   qh  s  a     h vmax   and
rmax
vmax  
 
 
where rmax is the maximum of the absolute value of the reward for any action at any state 
from this we also get that  v h     v       h vmax  

   approximate policy iteration with a policy language bias
exact solution techniques  such as policy iteration  are typically intractable for large statespace mdps  such as those arising from relational planning domains  in this section  we
introduce a new variant of approximate policy iteration  api  intended for such domains 
first  we review a generic form of api used in prior work  based on learning approximate
value functions  next  motivated by the fact that value functions are often difficult to learn
in relational domains  we describe our api variant  which avoids learning value functions
and instead learns policies directly as state action mappings 
    api with approximate value functions
api  as described in bertsekas and tsitsiklis         uses a combination of monte carlo
simulation and inductive machine learning to heuristically approximate policy iteration in
large state space mdps  given a current policy   each iteration of api approximates
policy evaluation and policy selection  resulting in an approximately improved policy  
first  the policy evaluation step constructs a training set of samples of v  from a small but
representative set of states  each sample is computed using simulation  estimating v   s 
for the policy  at each state s by drawing some number of sample trajectories of  starting
  

fifern  yoon    givan

at s and then averaging the cumulative  discounted reward along those trajectories  next 
the policy selection step uses a function approximator  e g   a neural network  to learn an
approximation v  to v  based on the training data  v  then serves as a representation
for   which selects actions using sampled one step lookahead based on v    that is
 s    arg max r s  a    e v   t  s  a    
aa

a common variant of this procedure learns an approximation of q rather than v   
api exploits the function approximators generalization ability to avoid evaluating each
state in the state space  instead only directly evaluating a small number of training states 
thus  the use of api assumes that states and perhaps actions are represented in a factored
form  typically  a feature vector  that facilitates generalizing properties of the training data
to the entire state and action spaces  note that in the case of perfect generalization  i e  
v   s    v   s  for all states s   we have that  is equal to the exact policy improvement
pi    and thus api simulates exact policy iteration  however  in practice  generalization is
not perfect  and there are typically no guarantees for policy improvement  nevertheless 
api often converges usefully  tesauro        tsitsiklis   van roy        
the success of the above api procedure depends critically on the ability to represent
and learn good value function approximations  for some mdps  such as those arising from
relational planning domains  it is often difficult to specify a space of value functions and
learning mechanism that facilitate good generalization  for example  work in relational
reinforcement learning  dzeroski  deraedt    driessens        has shown that learning
approximate value functions for classical domains  such as the blocks world  can be problematic   in spite of this  it is often relatively easy to compactly specify good policies using
a language for  relational  state action mappings  this suggests that such languages may
provide useful policy space biases for learning in api  however  all prior api methods are
based on approximating value functions and hence can not leverage these biases  with
this motivation  we consider a form of api that directly learns policies without directly
representing or approximating value functions 
    using a policy language bias
a policy is simply a classifier  possibly stochastic  that maps states to actions  our api
approach is based on this view  and is motived by recent work that casts policy selection
as a standard classification learning problem  in particular  given the ability to observe
trajectories of a target policy  we can use machine learning to select a policy  or classifier 
that mimics the target as closely as possible  khardon      b  studied this learning setting
and provided pac like learnability results  showing that under certain assumptions  a small
number of trajectories is sufficient to learn a policy whose value is close to that of the
target  in addition  recent empirical work  in relational planning domains  khardon      a 
martin   geffner        yoon et al          has shown that by using expressive languages
   under very strong assumptions  api can be shown to converge in the infinite limit to a near optimal
value function  see proposition     of bertsekas and tsitsiklis        
   in particular  the rrl work has considered a variety of value function representation including relational
regression trees  instance based methods  and graph kernels  but none of them have generalized well over
varying numbers of objects 

  

fiapi with a policy language bias

for specifying state action mappings  good policies can be learned from sample trajectories
of good policies 
these results suggest that  given a policy   if we can somehow generate trajectories of
an improved policy  then we can learn an approximately improved policy based on those
trajectories  this idea is the basis of our approach  figure   gives pseudo code for our api
variant  which starts with an initial policy   and produces a sequence of approximately
improved policies  each iteration involves two primary steps  first  given the current
policy   the procedure improved trajectories  approximately  generates trajectories of
the improved policy      pi    second  these trajectories are used as training data for the
procedure learn policy  which returns an approximation of      we now describe each
step in more detail 
step    generating improved trajectories  given a base policy   the simulation technique of policy rollout  tesauro   galperin        bertsekas   tsitsiklis       
computes an approximation  to the improved policy      pi    where    is the result of
applying one step of policy iteration to   furthermore  for a given state s  policy rollout
computes  s  without the need to solve for    at all other states  and thus provides a
tractable way to approximately simulate the improved policy    in large state space mdps 
often    is significantly better than   and hence so is   which can lead to substantially
improved performance at a small cost  policy rollout has provided significant benefits in a
number of application domains  including for example backgammon  tesauro   galperin 
       instruction scheduling  mcgovern  moss    barto         network congestion control
 wu  chong    givan         and solitaire  yan  diaconis  rusmevichientong    van roy 
      
policy rollout computes  s   the estimate of     s   by estimating q  s  a  for each
action a and then taking the maximizing action to be  s  as suggested by equation   
each q  s  a  is estimated by drawing w trajectories of length h  where each trajectory is
the result of starting at s  taking action a  and then following the actions selected by  for
h    steps  the estimate of q  s  a  is then taken to be the average of the cumulative
discounted reward along each trajectory  the sampling width w and horizon h are specified
by the user  and control the trade off between increased computation time for large values 
and reduced accuracy for small values  note that rollout applies to both stochastic and
deterministic policies and that due to variance in the q value estimates  the rollout policy
can be stochastic even for deterministic base policies 
the procedure improved trajectories uses rollout to generate n length h trajectories
of   each beginning at a randomly drawn initial state  rather than just recording the
states and actions along each trajectory  we store additional information that is used by
our policy learning algorithm  in particular  the ith element of a trajectory has the form
hsi    si    q si   a             q si   am  i  giving the ith state si along the trajectory  the action
selected by the current  unimproved  policy at si   and the q value estimates q si   a  for
each action  note that given the q value information for si the learning algorithm can
determine the approximately improved action  s   by maximizing over actions  if desired 
step    learn policy  intuitively  we want learn policy to select a new policy
that closely matches the training trajectories  in our experiments  we use relatively simple
learning algorithms based on greedy search within a space of policies specified by a policylanguage bias  in sections     and     we detail the policy language learning bias used
  

fifern  yoon    givan

by our technique  and the associated learning algorithm  in section   we provide some
technical analysis of an idealized version of this algorithm  providing guidance regarding
the required number of training trajectories  we note that by labeling each training state
in the trajectories with the associated q values for each action  rather than simply with the
best action  we enable the learner to make more informed trade offs  focusing on accuracy
at states where wrong decisions have high costs  which was empirically useful  also  the
inclusion of  s  in the training data enables the learner to adjust the data relative to  
if desirede g   our learner uses a bias that focuses on states where large improvement
appears possible 
finally  we note that for api to be effective  it is important that the initial policy
  provide guidance toward improvement  i e     must bootstrap the api process  for
example  in goal based planning domains   should reach a goal from some of the sampled
states  in section   we will discuss this important issue of bootstrapping and introduce a
new bootstrapping technique 

   technical analysis
in this section  we consider a variant of the policy improvement step of our main api loop 
which learns an improved policy given a base policy   we show how to select a sampling
width w  horizon h  and training set size n such that  under certain assumptions  the quality
of the learned policy is close to the quality of    the policy iteration improvement  similar
results have been shown for previous forms of api based on approximate value functions
 bertsekas   tsitsiklis         however  our assumptions are of a much different nature  
the analysis is divided into two parts  first  following khardon      b   we consider
the sample complexity of policy learning  that is  we consider how many trajectories of a
target policy must be observed by a learner before we can guarantee a good approximation
to the target  second  we show how to apply this result  which is for deterministic policies 
to the problem of learning from rollout policies  which can be stochastic  throughout we
assume the context of an mdp m   hs  a  t  r  ii 
    learning deterministic policies
a trajectory of length h is a sequence  s    a    s    a            ah    sh   of alternating states si and
actions ai   we say that a deterministic policy  is consistent with a trajectory  s    a            sh  
if and only if for each    i   h   si     ai   we define dh to be a distribution over the set
of all length h trajectories  such that dh  t  is the probability that  generates trajectory
t    s    a    s    a            ah    sh   according to the following process  first draw s  according
to the initial state distribution i  and then draw si   from t  si    si    for    i   h  note
that dh  t  is non zero only if  is consistent with t 
our policy improvement step first generates trajectories of the rollout policy   see section       via the procedure improved trajectories  and then learns an approximation
   in particular  bertsekas and tsitsiklis        assumes a bound on the l norm of the value function
approximation  i e   that at each state the approximation is almost perfect  rather we assume that
the improved policy    comes from a finite class of policies for which we have a consistent learner 
in both cases policy improvement can be guaranteed given an additional assumption on the minimum
q advantage of the mdp  see below  

  

fiapi with a policy language bias

api  n  w  h  m       
   training set size n  sampling width w  horizon h 
   mdp m   hs   a            am    t  r  ii  initial policy     discount factor  
     
loop
t  improved trajectories n  w  h  m    
  learn policy t   
until satisfied with  
   e g   until change is small
return  
improved trajectories n  w  h  m   
   training set size n  sampling width w 
   horizon h  mdp m   current policy 
t   
repeat n times    generate n trajectories of improved policy
t  nil 
s  state drawn from i     draw random initial state
for i     to h
hq s  a             q s  am  i  policy rollout   s  w  h  m       q  s  a  estimates
t  t  hs   s   q s  a             q s  am   i     concatenate new sample onto trajectory
a  action maximizing q s  a      action of the improved policy at state s
s  state sampled from t  s  a      simulate action of improved policy
t  t  t 
return t  
policy rollout    s  w  h  m  

   compute q  s  a  estimates hq s  a             q s  am  i

   policy   state s  sampling width w  horizon h  mdp m
for each action ai in a
q s  ai      
repeat w times    q s  ai   is an average over w trajectories
r  r s  ai    s   a state sampled from t  s  ai       take action ai in s
for i     to h       take h    steps of  accumulating discounted reward in r
r  r    i r s     s     
s   a state sampled from t  s     s    
q s  ai    q s  ai     r     include trajectory in average
q s  ai   

q s ai  
 
w

return hq s  a             q s  am  i

figure    pseudo code for our api algorithm  see section     for an instantiation of learnpolicy called learn decision list 
of   note that the rollout policy serves as a stochastic approximation of      pi  the
policy iteration improvement of   thus  improved trajectories can be viewed as at 
tempting to draw trajectories from dh   and the learning step can be viewed as learning an
  

fifern  yoon    givan

 

approximation of      imagining for the moment that we can draw trajectories from dh  
a fundamental question is how many trajectories are sufficient to ensure that the learned
policy will be about as good as      khardon      b  studied this question for the case
of deterministic policies in undiscounted goal based planning domains  i e   mdps where
reward is only received at goal states   here we give a straightforward adaptation of his
main result to our problem setting where we have general reward functions and measure
the quality of a policy by v    
the learning problem formulation is similar in spirit to the standard framework of probably approximately correct  pac  learning  in particular  we will assume that the target
policy comes from a finite class of deterministic policies h  for example  h may correspond
to the set of policies that can be described by bounded length decision lists  in addition 
we assume that the learner is consistenti e   it returns a policy from h that is consistent
with all of the training trajectories  under these assumptions  a relatively small number
of trajectories  logarithmic in  h   are sufficient to ensure that with high probability the
learned policy is about as good as the target 
proposition    let h be a finite class of deterministic policies  for any   h  and any

set of n     ln  h 
 trajectories drawn independently from dh   there is a     probability
that every   h consistent with the trajectories satisfies v     v      vmax      h   
the proof of this proposition is in the appendix  the computational complexity of
finding a consistent policy depends on the policy class h  polynomial time algorithms
can be given for interesting classes such as bounded length decision listshowever  these
algorithms are typically too expensive for the policy classes we consider in practice  rather 
as described in section      we use a learner based on greedy heuristic search  which often
works well in practice 
the assumption that the target policy comes from a fixed size class h will often be
violated  however  as pointed out by khardon      b   it is straightforward to give an
extension of proposition   for the setting where the learner considers increasingly complex
policies until a consistent one is found  in this case  the sample complexity is related to the
encoding size of the target policy rather than the size of h  thus allowing the use of very
large and expressive policy classes without necessarily paying the full sample complexity
price of proposition   
    learning from rollout policies
the proof of proposition   relies critically on the fact that the policy class h contains only
deterministic policies  however  in our main api loop  the target policies are computed via
rollout and hence are stochastic due to the uncertainty introduced by finite sampling  thus 
we cannot directly use proposition   in the context of learning from trajectories produced
by rollout  to deal with this problem we describe a variant of improved trajectories
that can reliably generate training trajectories from the deterministic policy      pi   see
equation     which is guaranteed to improve on  if improvement is possible 
given a base policy   we first define a  s  to be the set of actions that maximize
q  s  a   note that     s    min a  s   where the minimum is taken with respect to the action ordering provided by the mdp  importantly this policy is deterministic and thus if we
  

fiapi with a policy language bias

can generate trajectories of it  then we can apply the above result to learn a close approximation  in order to generate trajectories of    we slightly modify improved trajectories 
the modification is introduced for analysis only  and our experiments are based on the procedures given in figure    our modification is to replace the action maximization step of
improved trajectories  second to last statement of the for loop   which chooses the next
action a to execute  with the following two steps
a   s    a    maxa q s  a   q s  a      
a  min a   s 
where q s  a  is the estimate of qh  s  a  computed by policy rollout using a sampling width
w  and  is a newly introduced parameter 
note that if a   s    a  s   then the selected action a will equal     s   if this condition is true for every state encountered then the modified improved trajectories will
effectively generate trajectories of      thus  we would like to bound the probability that
a   s     a  s  to a small value by appropriately choosing the sampling width w  the
horizon h  and   unfortunately  the choice of these parameters depends on the mdp 
that is  given any particular parameter values  there is an mdp such that the event
a   s     a  s  has a non negligible probability at some state  for this reason we first
define the q advantage  of an mdp and show how to select appropriate parameter values
given a lower bound on   
given an mdp and policy   let s   be the set of states such that s  s   iff there are
two actions a and a  such that q  s  a     q  s  a     i e   there are actions with distinct
q values  also for each state in s   define a   s  and a   s  be a best action and a second
best action respectively as measured by q  s  a   the q advantage is defined as   
minss   a   s   a   s   which measures the minimum q value gap between an optimal and
sub optimal action over the state space  given a lower bound on the q advantage of an
mdp the following proposition indicates how to select parameter values to ensure that
a   s    a  s  with high probability 
proposition   
have

for any mdp with q advantage at least    and any             if we

h   log


 vmax

 vmax



 



w  
  

 

ln

 a 
 

then for any state s  a   s    a  s  with probability at least        
the proof is given in the appendix  thus  for parameter values satisfying the above conditions  if our mdp has q advantage at least  then we are guaranteed that with probability
at least       that a   s    a   s   this means that improved trajectories will correctly select the action     s  with probability at least         note that this proposition
  

fifern  yoon    givan

agrees with the intuition that both h and w should increase with decreasing q advantage
and increasing vmax and also that w should increase for decreasing      
in order to generate n length h trajectories of      the modified improved trajectories
routine must compute the set a     at n  h states  yielding n  h opportunities to make an
error  to ensure that no error is made  the modified procedure sets the sampling width w

  this guarantees that an error free training set is created with probability
using       nh

at least       
combining this observation with the assumption that     h we can apply proposition
 
  as follows  first  generate n     ln   h 
 trajectories of  using the modified improved
trajectories routine  with       nh
   next  learn a policy  from these trajectories using
a consistent learner  we know that the probability of generating an imperfect training set
is bounded by     and for the chosen value of n  the failure probability of the learner is
also bounded by     thus  we get that with probability at least      the learned policy 
satisfies v     v         vmax      h    giving an approximation guarantee relative to the
improved policy      this is summarized by the following proposition 
proposition    let h be a finite class of deterministic policies            and          
for any mdp with q advantage
at least    any policy  such that pi   h  and any set

 
 
of n    ln   h 
trajectories produced by modified improved trajectories using
parameters satisfying 
  


 

h   log


 vmax

 vmax    nh a 
w  
ln


there is at least a     probability that every   h consistent with the trajectories satisfies
v     v  pi      vmax      h   




one notable aspect of this result is that there is only a logarithmic dependence on the
number of actions  a  and      however  the practical utility is hindered by its dependence
on  which is typically not known in practice  and can be exponentially small in the
planning horizon  unfortunately  this dependence appears to be unavoidable for our type
of approach where we try to learn from trajectories of pi  produced by rollout  this is
because for any particular setting of the above parameters  there is always an mdp with
a small enough q advantage  such that the value of the rollout policy is arbitrarily worse
than that of pi   

   api for relational planning
our work is motivated by the goal of solving relational mdps  in particular  we are interested in finding policies for relational mdps that represent classical planning domains and
   at first glance it appears that the lower bound on h decreases with increasing vmax and decreasing   
however  the opposite is true since the base of the logarithm is the discount factor  which is strictly less
than one  also note that since  is upper bounded by  vmax the bound on h will always be positive 

  

fiapi with a policy language bias

their stochastic variants  such policies can then be applied to any problem instance from a
planning domain  and hence can be viewed as a form of domain specific control knowledge 
in this section  we first describe a straightforward way to view classical planning domains
 not just single problem instances  as relationally factored mdps  next  we describe our
relational policy space in which policies are compactly represented as taxonomic decision
lists  finally  we present a heuristic learning algorithm for this policy space 
    planning domains as mdps 
we say that an mdp hs  a  t  r  ii is relational when s and a are defined by giving a finite
set of objects o  a finite set of predicates p   and a finite set of action types y   a fact is
a predicate applied to the appropriate number of objects  e g   on a  b  is a blocks world
fact  a state is a set of facts  interpreted as representing the true facts in the state  the
state space s contains all possible sets of facts  an action is an action type applied to the
appropriate number of objects  e g   putdown a  is a blocks world action  and the action
space a is the set of all such actions 
a classical planning domain describes a set of problem instances with related structure 
where a problem instance gives an initial world state and goal  for example  the blocks
world is a classical planning domain  where each problem instance specifies an initial block
configuration and a set of goal conditions  classical planners attempt to find solutions to
specific problem instances of a domain  rather  our goal is to solve entire planning domains
by finding a policy that can be applied to all problem instances  as described below  it is
straightforward to view a classical planning domain as a relational mdp where each mdp
state corresponds to a problem instance 
state and action spaces  each classical planning domain specifies a set of action
types y   world predicates w   and possible world objects o  together y and o define the
mdp action space  each state of the mdp corresponds to a single problem instance  i e   a
world state and a goal  from the planning domain by specifying both the current world and
the goal  we achieve this by letting the set of relational mdp predicates be p   w  g 
where g is a set of goal predicates  the set of goal predicates contains a predicate for
each world predicate in w   which is named by prepending a g onto the corresponding
world predicate name  e g   the goal predicate gclear corresponds to the world predicate
clear   with this definition of p we see that the mdp states are sets of goal and world
facts  indicating the true world facts of a problem instance and the goal conditions  it
is important to note  as described below  that the mdp actions will only change world
facts and not goal facts  thus  this large relational mdp can be viewed as a collection of
disconnected sub mdps  where each sub mdp corresponds to a distinct goal condition 
reward function  given an mdp state the objective is to reach another mdp state
where the goal facts are a subset of the corresponding world factsi e   reach a world state
that satisfies the goal  we will call such states goal states of the mdp  for example  the
mdp state
 on table a   on a  b   clear b   gclear b  
is a goal state in a blocks world mdp  but would not be a goal state without the world fact
clear b   we represent the objective of reaching a goal state quickly by defining r to assign
a reward of zero for actions taken in goal states and negative rewards for actions in all
  

fifern  yoon    givan

other states  representing the cost of taking those actions  typically  for classical planning
domains  the action costs are uniformly     however  our framework allows the cost to vary
across actions 
transition function  each classical planning domain provides an action simulator
 e g   as defined by strips rules  that  given a world state and action  returns a new world
state  we define the mdp transition function t to be this simulator modified to treat goal
states as terminal and to preserve without change all goal predicates in an mdp state  since
classical planning domains typically have a large number of actions  the action definitions
are usually accompanied by preconditions that indicate the legal actions in a given state 
where usually the legal actions are a small subset of all possible actions  we assume that
t treats actions that are not legal as no ops  for simplicity  our relational mdp definition
does not explicitly represent action preconditions  however  we assume that our algorithms
do have access to preconditions and thus only need to consider legal actions  for example 
we can restrict rollout to only the legal actions in a given state 
initial state distribution  finally  the initial state distribution i can be any program
that generates legal problem instances  mdp states  of the planning domain  for example  problem domains from planning competitions are commonly distributed with problem
generators 
with these definitions  a good policy is one that can reach goal states via low cost
action sequences from initial states drawn from i  note that here policies are mappings
from problem instances to actions and thus can be sensitive to goal conditions  in this
way  our learned policies are able to generalize across different goals  we next describe a
language for representing such generalized policies 
    taxonomic decision list policies 
for single argument action types  many useful rules for planning domains take the form of
apply action type a to any object in class c  martin   geffner         for example  in the
blocks world  pick up any clear block that belongs on the table but is not on the table 
or in a logistics world  unload any object that is at its destination  using a concept
language for describing object classes  martin and geffner        introduced the use of
decision lists of such rules as a useful learning bias  showing promising experiments in the
deterministic blocks world  with that motivation  we consider a policy space that is similar
to the one used originally by martin and geffner  but generalized to handle multiple action
arguments  also  for historical reasons  our concept language is based upon taxonomic
syntax  mcallester        mcallester   givan         rather than on description logic as
used by martin and geffner 
comparison predicates  for relational mdps with world and goal predicates  such
as those corresponding to classical planning domains  it is often useful for polices to compare
the current state with the goal  to this end  we introduce a new set of predicates  called
comparison predicates  which are derived from the world and goal predicates  for each
world predicate p and corresponding goal predicate gp  we introduce a new comparison
predicate cp that is defined as the conjunction of p and gp  that is  a comparison predicate
fact is true if and only if both the corresponding world and goal predicates facts are true 
  

fiapi with a policy language bias

for example  in the blocks world  the comparison predicate fact con a  b  indicates that a
is on b in both the current state and the goali e   on a  b  and gon a  b  are true 
taxonomic syntax  taxonomic syntax provides a language for writing class expressions that represent sets of objects with properties of interest and serve as the fundamental
pieces with which we build policies  class expressions are built from the mdp predicates
 including comparison predicates if applicable  and variables  in our policy representation 
the variables will be used to denote action arguments  and at runtime will be instantiated
by objects  for simplicity we only consider predicates of arity one and two  which we call
primitive classes and relations  respectively  when a domain contains predicates of arity
three or more  we automatically convert them to multiple auxiliary binary predicates  given
a list of variables x    x            xk    class expressions are given by 
c x      c    xi   a thing   c x     r c x      min r 
r     r    r     r
where c x  is a class expression  r is a relation expression  c  is a primitive class  r  is
a primitive relation  and xi is a variable in x  note that  for classical planning domains 
the primitive classes and relations can be world  goal  or comparison predicates  we define the depth d c x   of a class expression c x  to be one if c x  is either a primitive
class  a thing  a variable  or  min r   otherwise we define d c x   and d r c x   to be
d c x        where r is a relation expression and c x  is a class expression  for a given
relational mdp we denote by cd  x  the set of all class expressions c x  that have a depth
of d or less 
intuitively the class expression  r c x   denotes the set of objects that are related
through relation r to some object in the set c x   the expression  r c x   denotes
the set of objects that are related through some r chain to an object in c x this
constructor is important for representing recursive concepts  e g   the blocks above a   the
expression  min r  denotes the set of objects that are minimal under the relation r 
more formally  let s be an mdp state and o    o            ok   be a variable assignment 
which assigns object oi to variable xi   the interpretation of c x  relative to s and o is a
set of objects and is denoted by c x s o   a primitive class c  is interpreted as the set of
objects for which the predicate symbol c  is true in s  likewise  a primitive relation r  is
interpreted as the set of all object tuples for which the relation r  holds in s  the class
expression a thing denotes the set of all objects in s  the class expression xi   where xi
is a variable  is interpreted to be the singleton set  oi    the interpretation of compound
expressions is given by 
 c x  s o    o   o   c x s o  
 r c x  s o    o   o   c x s o s t   o    o   rs o  
 min r s o    o   o  s t   o  o     rs o     o  s t   o    o   rs o  
 r  s o   id    o    ov     o            ov  s t   oi   oi      rs o for    i   v 
 r   s o     o  o       o    o   rs o  
where c x  is a class expression  r is a relation expression  and id is the identity relation 
some examples of useful blocks world concepts  given the primitive classes clear  gclear 
holding  and con table  along with the primitive relations on  gon  and con  are 
  

fifern  yoon    givan

  gon  holding  has depth two  and denotes the block that we want under the block
being held 
  on  on gclear   has depth three  and denotes the blocks currently above blocks
that we want to make clear 
  con con table  has depth two  and denotes the set of blocks in well constructed
towers  to see this note that a block bv is in this class if and only if there exists a
sequence of blocks b            bv such that b  is on the table in both the goal and the
current state  i e  con table b     and bi   is on bi in both the goal and current state
 i e  con bi   bi      for    i   v 
  gon  con con table   has depth three  and denotes the blocks that belong on top
of a currently well constructed tower 
decision list policies we represent policies as decision lists of action selection rules 
each rule has the form a x            xk     l    l          lm   where a is a k argument action type 
the li are literals  and the xi are action argument variables  we will denote the list of
action argument variables as x    x            xk    each literal has the form x  c x   where
c x  is a taxonomic syntax class expression and x is an action argument variable 
given an mdp state s and a list of action argument objects o    o            ok    we say
that a literal xi  c x  is true given s and o iff oi  c x s o   we say that a rule
r   a x            xk     l    l          lm allows action a o          ok   in s iff each literal in the rule
is true given s and o  note that if there are no literals in a rule for action type a  then all
possible actions of type a are allowed by the rule  a rule can be viewed as placing mutual
constraints on the tuples of objects that an action type can be applied to  note that a
single rule may allow no actions or many actions of one type  given a decision list of such
rules we say that an action is allowed by the list if it is allowed by some rule in the list 
and no previous rule allows any actions  again  a decision list may allow no actions or
multiple actions of one type  a decision list l for an mdp defines a deterministic policy
 l  for that mdp  if l allows no actions in state s  then  l  s  is the least  legal action
in s  otherwise   l  s  is the least legal action that is allowed by l  it is important to
note that since  l  only considers legal actions  as specified by action preconditions  the
rules do not need to encode the preconditions  which allows for simpler rules and learning 
in other words  we can think of each rule as implicitly containing the preconditions of its
action type 
as an example of a taxonomic decision list policy consider a simple blocks world domain
where the goal condition is always to clear off all of the red blocks  the primitive classes
in this domain are red  clear  and holding  and the single relation is on  the following
policy will solve any problem in the domain 
putdown x      x   holding
pickup x      x   clear  x    on  on red  
   the action ordering in a relational mdp is defined lexicographically in terms of orderings on the action
types and objects 

  

fiapi with a policy language bias

the first rule will cause the agent to putdown any block that is being held  otherwise  if
no block is being held  then find a block x  that is clear and is above a red block  expressed
by  on  on red    and pick it up  appendix b gives examples of more complex policies
that are learned by our system in the experiments 
    learning taxonomic decision lists
for a given relational mdp  define rd l to be the set of action selection rules that have
a length of at most l literals and whose class expression have depth at most d  also  let
hd l denote the policy space defined by decision lists whose rules are from rd l   since the
number of depth bounded class expressions is finite there are a finite number of rules  and
hence hd l is finite  though exponentially large  our implementation of learn policy  as
used in the main api loop  learns a policy in hd l for user specified values of d and l 
we use a rivest style decision list learning approach  rivest       an approach also
taken by martin and geffner        for learning class based policies  the primary difference
between martin and geffner        and our technique is the method for selecting individual
rules in the decision list  we use a greedy  heuristic search  while previous work used an
exhaustive enumeration approach  this difference allows us to find rules that are more
complex  at the potential cost of failing to find some good simple rules that enumeration
might discover 
recall from section    that the training set given to learn policy contains trajectories
of the rollout policy  our learning algorithm  however  is not sensitive to the trajectory
structure  i e   the order of trajectory elements  and thus  to simplify our discussion  we
will take the input to our learner to be a training set d that contains the union of all
the trajectory elements  this means that for a trajectory set that contains n length h
trajectories  d will contain a total of n  h training examples  as described in section   
each training example in d has the form hs   s   q s  a             q s  am  i  where s is a state 
 s  is the action selected in s by the previous policy  and q s  ai   is the q value estimate
of q  s  ai    note that in our experiments the training examples only contain values for
the legal actions in a state 
given a training set d  a natural learning goal is to find a decision list policy that for
each training example selects an action with the maximum estimated q value  this learning
goal  however  can be problematic in practice as often there are several best  or close to
best  actions as measured by the true q function  in such case  due to random sampling 
the particular action that looks best according to the q value estimates in the training set
is arbitrary  attempting to learn a concise policy that matches these arbitrary actions will
be difficult at best and likely impossible 
one approach  lagoudakis   parr        to avoiding this problem is to use statistical
tests to determine the actions that are clearly the best  positive examples  and the ones
that are clearly not the best  negative examples   the learner is then asked to find a
policy that is consistent with the positive and negative examples  while this approach has
shown some empirical success  it has the potential shortcoming of throwing away most of
the q value information  in particular  it may not always be possible to find a policy that
exactly matches the training data  in such cases  we would like the learner to make informed
trade offs regarding sub optimal actionsi e   prefer sub optimal actions that have larger
  

fifern  yoon    givan

learn decision list  d  d  l  b 
   training set d  concept depth d  rule length l  beam width b
l  nil 
while  d is not empty 
r  learn rule d  d  l  b  
d  d   d  d   r covers d  
l  extend list l  r      add r to end of list
return l 
learn rule d  d  l  b 
   training set d  concept depth d  rule length l  beam width b
for each action type a

   compute rule for each action type a

ra  beam search d  d  l  b  a  
return argmaxa hvalue ra   d  
beam search  d  d  l  b  a 
   training set d  concept depth d  rule length l  beam width b  action type a
k  arity of a  x   x            xk   

  

l    x  c    x  x  c  cd  x      

x is a sequence of action argument variables
construct the set of depth bounded candidate literals

b     a x    nil    i        initialize beam to a single rule with no literals
loop
g   bi    r  rd l   r   add literal r    l   r   bi    l  l  
bi  beam select g  b  d     

select best b heuristic values

i  i     
until bi    bi     

loop until there is no more improvement in heuristic

return argmaxrbi hvalue r  d    

return best rule in final beam

figure    pseudo code for learning a decision list in hd l given training data d  the
procedure add literal r  l  simply returns a rule where literal l is added to the end of
rule r  the procedure beam select g  w  d  selects the best b rules in g with different
heuristic values  the procedure hvalue r  d  returns the heuristic value of rule r relative
to training data d and is described in the text 

q values  with this motivation  below we describe a cost sensitive decision list learner that
is sensitive to the full set of q values in d  the learning goal is roughly to find a decision
list that selects actions with large cumulative q value over the training set 
learning list of rules  we say that a decision list l covers a training example
hs   s   q s  a             q s  am  i if l suggests an action in state s  given a set of training
examples d  we search for a decision list that selects actions with high q value via an
iterative set covering approach carried out by learn decision list  decision list rules
  

fiapi with a policy language bias

are constructed one at a time and in order until the list covers all of the training examples 
pseudo code for our algorithm is given in figure    initially  the decision list is the null list
and does not cover any training examples  during each iteration  we search for a high quality
rule r with quality measured relative to the set of currently uncovered training examples 
the selected rule is appended to the current decision list  and the training examples newly
covered by the selected rule are removed from the training set  this process repeats until
the list covers all of the training examples  the success of this approach depends heavily
on the function learn rule  which selects a good rule relative to the uncovered training
examplestypically a good rule is one that selects actions with the best  or close to best 
q value and also covers a significant number of examples 
learning individual rules  the input to the rule learner learn rule is a set of
training examples  along with depth and length parameters d and l  and a beam width b 
for each action type a  the rule learner calls the routine beam search to find a good rule
ra in rd l for action type a  learn rule then returns the rule ra with the highest value
as measured by our heuristic  which is described later in this section 
for a given action type a  the procedure beam search generates a beam b    b        
where each bi is a set of rules in rd l for action type a  the sets evolve by specializing
rules in previous sets by adding literals to them  guided by our heuristic function  search
begins with the most general rule a x    nil  which allows any action of type a in any state 
search iteration i produces a set bi that contains b rules with the highest different heuristic
values among those in the following set 
g   bi    r  rd l   r   add literal r    l   r   bi    l  l 
where l is the set of all possible literals with a depth of d or less  this set includes the
current best rules  those in bi    and also any rule in rd l that can be formed by adding
a new literal to a rule in bi    the search ends when no improvement in heuristic value
occurs  that is when bi   bi    beam search then returns the best rule in bi according
to the heuristic 
heuristic function  for a training instance hs   s   q s  a             q s  am  i  we define the q advantage of taking action ai instead of  s  in state s by  s  ai     q s  ai   
q s   s    likewise  the q advantage of a rule r is the sum of the q advantages of actions
allowed by r in s  given a rule r and a set of training examples d  our heuristic function
hvalue r  d  is equal to the number of training examples that the rule covers plus the
cumulative q advantage of the rule over the training examples   using q advantage rather
than q value focuses the learner toward instances where large improvement over the previous policy is possible  naturally  one could consider using different weights for the coverage
and q advantage terms  possibly tuning the weight automatically using validation data 
   since many rules in rd l are equivalent  we must prevent the beam from filling up with semantically
equivalent rules  rather than deal with this problem via expensive equivalence testing we take an ad hoc 
but practically effective approach  we assume that rules do not coincidentally have the same heuristic
value  so that ones that do must be equivalent  thus  we construct beams whose members all have
different heuristic values  we choose between rules with the same value by preferring shorter rules  then
arbitrarily 
   if the coverage term is not included  then covering a zero q advantage example is the same as not
covering it  but zero q advantage can be good  e g   the previous policy is optimal in that state  

  

fifern  yoon    givan

   random walk bootstrapping
there are two issues that are critical to the success of our api technique  first  api is
fundamentally limited by the expressiveness of the policy language and the strength of the
learner  which dictates its ability to capture the improved policy described by the training
data at each iteration  second  api can only yield improvement if improved trajectories
successfully generates training data that describes an improved policy  for large classical
planning domains  initializing api with an uninformed random policy will typically result
in essentially random training data  which is not helpful for policy improvement  for
example  consider the mdp corresponding to the    block blocks world with an initial
problem distribution that generates random initial and goal states  in this case  a random
policy is unlikely to reach a goal state within any practical horizon time  hence  the
rollout trajectories are unlikely to reach the goal  providing no guidance toward learning an
improved policy  i e   a policy that can more reliably reach the goal  
because we are interested in solving large domains such as this  providing guiding inputs
to api is critical  in fern  yoon  and givan         we showed that by bootstrapping api
with the domain independent heuristic of the planner ff  hoffmann   nebel         api
was able to uncover good policies for the blocks world  simplified logistics world  no planes  
and stochastic variants  this approach  however  is limited by the heuristics ability to
provide useful guidance  which can vary widely across domains 
here we describe a new bootstrapping procedure for goal based planning domains  based
on random walks  for guiding api toward good policies  our planning system  which is
evaluated in section    is based on integrating this procedure with api in order to find
policies for goal based planning domains  for non goal based mdps  this bootstrapping
procedure can not be directly applied  and other bootstrapping mechanisms must be used
if necessary  this might include providing an initial non trivial policy  providing a heuristic
function  or some form of reward shaping  mataric         below  we first describe the
idea of random walk distributions  next  we describe how to use these distributions in the
context of bootstrapping api  giving a new algorithm lrw api 
    random walk distributions
throughout we consider an mdp m   hs  a  t  r  ii that correspond to goal based planning domains  as described in section      recall that each state s  s corresponds to a
planning problem  specifying a world state  via world facts  and a set of goal conditions  via
goal facts   we will use the terms mdp state and planning problem interchangeably 
note that  in this context  i is a distribution over planning problems  for convenience we
will denote mdp states as tuples s    w  g   where w and g are the sets of world facts and
goal facts in s respectively 
given an mdp state s    w  g  and set of goal predicates g  we define s g to be the
mdp state  w  g     where g   contains those goal facts in g that are applications of a predicate
in g  given m and a set of goal predicates g  we define the n step random walk problem
distribution rw n  m  g  by the following stochastic algorithm 
   draw a random state s     w    g    from the initial state distribution i 
  

fiapi with a policy language bias

   starting at s  take n uniformly random actions     giving a state sequence  s            sn   
where sn    wn   g     recall that actions do not change goal facts   at each uniformly
random action selection  we assume that an extra no op action  that does not change
the state  is selected with some fixed probability  for reasons explained below 
   let g be the set of goal facts corresponding to the world facts in wn   so e g   if
wn    on a  b   clear a    then g    gon a  b   gclear a    return the planning
problem  mdp state   s    g  g as the output 
we will sometimes abbreviate rw n  m  g  by rw n when m and g are clear in context 
intuitively  to perform well on this distribution a policy must be able to achieve facts
involving the goal predicates that typically result after an n step random walk from an
initial state  by restricting the set of goal predicates g we can specify the types of facts
that we are interested in achievinge g   in the blocks world we may only be interested in
achieving facts involving the on predicate 
the random walk distributions provide a natural way to span a range of problem difficulties  since longer random walks tend to take us further from an initial state  for small
n we typically expect that the planning problems generated by rw n will become more
difficult as n grows  however  as n becomes large  the problems generated will require far
fewer than n steps to solvei e   there will be more direct paths from an initial state to the
end state of a long random walk  eventually  since s is finite  the problem difficulty will
stop increasing with n 
a question raised by this idea is whether  for large n  good performance on rw n
ensures good performance on other problem distributions of interest in the domain  in
some domains  such as the simple blocks world     good random walk performance does
seem to yield good performance on other distributions of interest  in other domains  such
as the grid world  with keys and locked doors   intuitively  a random walk is very unlikely
to uncover a problem that requires unlocking a sequence of doors  indeed  since rw n is
insensitive to the goal distribution of the underlying planning domain  the random walk
distribution may be quite different 
we believe that good performance on long random walks is often useful  but is only
addressing one component of the difficulty of many planning benchmarks  to successfully
address problems with other components of difficulty  a planner will need to deploy orthogonal technology such as landmark extraction for setting subgoals  hoffman  porteous   
sebastia         for example  in the grid world  if we could automatically set the subgoal
of possessing a key for the first door  a long random walk policy could provide a useful
macro for getting that key 
for the purpose of developing a bootstrapping technique for api  we limit our focus
to finding good policies for long random walks  in our experiments  we define long by
specifying a large walk length n   theoretically  the inclusion of the no op action in the
definition of rw ensures that the induced random walk markov chain   is aperiodic  and
    in practice  we only select random actions from the set of applicable actions in a state si   provided our
simulator makes it possible to identify this set 
    in the blocks world with large n  rw n generates various pairs of random block configurations  typically
pairing states that are far apartclearly  a policy that performs well on this distribution has captured
significant information about the blocks world 
    we dont formalize this chain here  but various formalizations work well 

  

fifern  yoon    givan

thus that the distribution over states reached by increasingly long random walks converges
to a stationary distribution     thus rw    limn rw n is well defined  and we take
good performance on rw  to be our goal 
    random walk bootstrapping
for an mdp m   we define m  i     to be an mdp identical to m only with the initial state
distribution replaced by i     we also define the success ratio sr   m  i   of  on m  i  as
the probability that  solves a problem drawn from i  also treating i as a random variable 
the average length al   m  i   of  on m  i  is the conditional expectation of the solution
length of  on problems drawn from i given that  solves i  typically the solution length of
a problem is taken to be the number of actions  however  when action costs are not uniform 
the length is taken to be the sum of the action costs  note that for the mdp formulation
of classical planning domains  given in section      if a policy  achieves a high v    then
it will also have a high success ratio and low average cost 
given an mdp m and set of goal predicates g  our system attempts to find a good
policy for m  rw n    where n is selected to be large enough to adequately approximate
rw    while still allowing tractable completion of the learning  naively  given an initial
random policy     we could try to apply api directly  however  as already discussed  this
will not work in general  since we are interested in planning domains where rw  produces
extremely large and difficult problems where random policies provide an ineffective starting
point 
however  for very small n  e g   n       rw n typically generates easy problems  and
it is likely that api  starting with even a random initial policy  can reliably find a good
policy for rw n   furthermore  we expect that if a policy n performs well on rw n   then
it will also provide reasonably good  but perhaps not perfect  guidance on problems drawn
from rw m when m is only moderately larger than n  thus  we expect to be able to find a
good policy for rw m by bootstrapping api with initial policy n   this suggests a natural
iterative bootstrapping technique to find a good policy for large n  in particular  for n   n   
figure   gives pseudo code for the procedure lrw api which integrates api and
random walk bootstrapping to find a policy for the long random walk problem distribution 
intuitively  this algorithm can be viewed as iterating through two stages  first  finding a
hard enough distribution for the current policy  by increasing n   and  then  finding a good
policy for the hard distribution using api  the algorithm maintains a current policy 
and current walk length n  initially n       as long as the success ratio of  on rwn is
below the success threshold    which is a constant close to one  we simply iterate steps of
approximate policy improvement  once we achieve a success ratio of  with some policy  
the if statement increases n until the success ratio of  on rw n falls below     that is 
when  performs well enough on the current n step distribution we move on to a distribution
that is slightly harder  the constant  determines how much harder and is set small enough
so that  can likely be used to bootstrap policy improvement on the harder distribution 
 the simpler method of just increasing n by   whenever success ratio  is achieved will also
    the markov chain may not be irreducible  so the same stationary distribution may not be reached from
all initial states  however  we are only considering one initial state  described by i 

  

fiapi with a policy language bias

lrw api  n  g  n  w  h  m       
   max random walk length n   goal predicates g
   training set size n  sampling width w  horizon h 
   mdp m   initial policy     discount factor  
      n    
loop
c   n    
if sr

   find harder n step distribution for  
c   i        or n if none 
n  least i   n  n   s t  sr
m     m  rw n  m  g   
t  improved trajectories n  w  h  m       
  learn policy t   
until satisfied with 
return  

c   n  estimates the success ratio of  in planning
figure    pseudo code for lrw api  sr
domain d on problems drawn from rw n  m  g  by drawing a set of problems and returning
the fraction solved by   constants  and  are described in the text 

find good policies whenever this method does  this can take much longer  as it may run
api repeatedly on a training set for which we already have a good policy  
once n becomes equal to the maximum walk length n   we will have n   n for all future
iterations  it is important to note that even after we find a policy with a good success ratio
on rw n it may still be possible to improve on the average length of the policy  thus 
we continue api on this distribution until we are satisfied with both the success ratio and
average length of the current policy 

   relational planning experiments
in this section  we evaluate the lrw api technique on relational mdps corresponding to
deterministic and stochastic classical planning domains  we first give results for a number of
deterministic benchmark domains  showing promising results in comparison with the stateof the art planner ff  hoffmann   nebel         while also highlighting limitations of our
approach  next  we give results for several stochastic planning domains including those
in the domain specific track of the      international probabilistic planning competition
 ippc   all of the domain definitions and problem generators used in our experiments are
available upon request 
in all of our experiments  we use the policy learner described in section     to learn
taxonomic decision list policies  in all cases  the number of training trajectories is      and
policies are restricted to rules with a depth bound d and length bound l  the discount
  

fifern  yoon    givan

factor  was always one  and lrw api was always initialized with a policy that selects
random actions  we utilize a maximum walk length parameter n           and set  and
 equal to     and     respectively 
    deterministic planning experiments
we perform experiments in seven familiar strips planning domains including those used
in the aips      planning competition  those used to evaluate tl plan in bacchus and
kabanza         and the gripper domain  each domain has a standard problem generator
that accepts parameters  which control the size and difficulty of the randomly generated
problems  below we list each domain and the parameters associated with them  a detailed
description of these domains can be found in hoffmann and nebel        
 blocks world  n    the standard blocks worlds with n blocks 
 freecell  s  c  f  l    a version of solitaire with s suits  c cards per suit  f freecells  and
l columns 
 logistics  a c l p    the logistics transportation domain with a airplanes  c cities  l
locations  and p packages 
 schedule  p    a job shop scheduling domain with p parts 
 elevator  f  p    elevator scheduling with f floors and p people 
 gripper  b    a robotic gripper domain with b balls 
 briefcase  i    a transportation domain with i items 
lrw experiments  our first set of experiments evaluates the ability of lrw api
to find good policies for rw    here we utilize a sampling width of one for rollout  since
these are deterministic domains  recall that in each iteration of lrw api we compute an
 approximately  improved policy and may also increase the walk length n to find a harder
problem distribution  we continued iterating lrw api until we observed no further
improvement  the training time per iteration is approximately five hours    though the
initial training period is significant  once a policy is learned it can be used to solve new
problems very quickly  terminating in seconds with a solution when one is found  even for
very large problems 

figure   provides data for each iteration of lrw api in each of the seven domains
with the indicated parameter settings  the first column  for each domain  indicates the
iteration number  e g   the blocks world was run for   iterations   the second column
records the walk length n used for learning in the corresponding iteration  the third and
fourth columns record the sr and al of the policy learned at the corresponding iteration
    this timing information is for a relatively unoptimized scheme implementation  a reimplementation in
c would likely result in a      fold speed up 

  

fin

rw n
sr
al

iter   

iter   

api with a policy language bias

rw 
sr
al

n

blocks world     
 
 
 
 
 
 
 
 

 
  
  
  
  
  
   
   

    
    
    
    
    
    
    
    
ff

   
   
    
    
    
    
    
    

 
    
    
    
    
    
    
 
    

 
 
  
  
  
  
  
  
  

    
    
    
    
    
    
    
    
    
ff

   
   
   
   
   
   
   
   
   

    
    
    
    
    
    
    
    
    
 

rw 
sr
al

logistics          
 
    
    
    
    
    
    
    
    

 
 
 
 
 
 
 
 
 
  

  
  
  

freecell          
 
 
 
 
 
 
 
 
 

rw n
sr
al

   
   
   
   
   
   
   
   
   
   

 
  
  
  
  
  
  
  
  
  

  
  
  

    
    
    
    
    
    
    
    
    
    

    
    
    
ff

   
   
   
   
   
   
   
   
   
   

   
   
   

    
    
    
    
    
    
    
    
    
    

    
    
    
 

    
   
   
   
   
   
   
    
   
   

   
   
   
  

    
 
 

  
  
  

 
   
 
 

 
  
  
  

schedule     
 
 

 
 

    
 
ff

 
    

briefcase     
elevator        
 

  

 

   

ff

 
 

  
  

 
 

  
  

 
 
 

 
  
  

    
    
 
ff

   
   
   

gripper     
 

  

 
ff

   

figure    results for each iteration of lrw api in seven deterministic planning domains 
for each iteration  we show the walk length n used for learning  along with the success ratio
 sr  and average length  al  of the learned policy on both rw n and rw    the final
policy shown in each domain performs above        sr on walks of length n          
 with the exception of logistics   and further iteration does not improve the performance 
for each benchmark we also show the sr and al of the planner ff on problems drawn
from rw   

as measured on     problems drawn from rw n for the corresponding value of n  i e  
the distribution used for learning   when this sr exceeds    the next iteration seeks an
increased walk length n  the fifth and sixth columns record the sr and al of the same
  

fifern  yoon    givan

policy  but measured on     problems drawn from the lrw target distribution rw    which
in these experiments is approximated by rw n for n           
so  for example  we see that in the blocks world there are a total of   iterations  where
we learn at first for one iteration with n      one more iteration with n       four iterations
with n       and then two iterations with n        at this point we see that the resulting
policy performs well on rw    further iterations with n   n   not shown  showed no
improvement over the policy found after iteration eight  in other domains  we also observed
no improvement after iterating with n   n   and thus do not show those iterations  we
note that all domains except logistics  see below  achieve policies with good performance
on rw n by learning on much shorter rw n distributions  indicating that we have indeed
selected a large enough value of n to capture rw    as desired 
general observations  for several domains  our learner bootstraps very quickly
from short random walk problems  finding a policy that works well even for much longer
random walk problems  these include schedule  briefcase  gripper  and elevator  typically  large problems in these domains have many somewhat independent subproblems with
short solutions  so that short random walks can generate instances of all the different typical
subproblems  in each of these domains  our best lrw policy is found in a small number
of iterations and performs comparably to ff on rw    we note that ff is considered a
very good domain independent planner for these domains  so we consider this a successful
result 
for two domains  logistics   and freecell  our planner is unable to find a policy with
success ratio one on rw    we believe that this is a result of the limited knowledge representation we allowed for policies for the following reasons  first  we ourselves cannot write good
policies for these domains within our current policy language  for example  in logistics  one
of the important concept is the set containing all packages on trucks such that the truck is
in the packages goal city  however  the domain is defined in such a way that this concept
cannot be expressed within the language used in our experiments  second  the final learned
decision lists for logistics and freecell  which are in appendix b  contain a much larger
number of more specific rules than the lists learned in the other domains  this indicates
that the learner has difficulty finding general rules  within the language restrictions  that
are applicable to large portions of training data  resulting in poor generalization  third 
the success ratio  not shown  for the sampling based rollout policy  i e   the improved policy
simulated by improved trajectories  is substantially higher than that for the resulting
learned policy that becomes the policy of the next iteration  this indicates that learndecision list is learning a much weaker policy than the sampling based policy generating
its training data  indicating a weakness in either the policy language or the learning algorithm  for example  in the logistics domain  at iteration eight  the training data for learning
the iteration nine policy is generated by a sampling rollout policy that achieves success ratio
     on     training problems drawn from the same rw    distribution  but the learned
iteration nine policy only achieves success ratio       as shown in the figure at iteration
nine  extending our policy language to incorporate the expressiveness that appears to be
required in these domains will require a more sophisticated learning algorithm  which is a
point of future work 
    in logistics  the planner generates a long sequence of policies with similar  oscillating success ratio that
are elided from the table with an ellipsis for space reasons 

   

fiapi with a policy language bias


domain
blocks

ff
sr al
    
  
        

size
    
    

sr
 
 

al
  
   

freecell

         
          

    
 

  


 
    

  
   

logistics

         
           

    
 

 


 
 

 
   

elevator

       

 

   

 

  

schedule

    

 

   

 

   

briefcase

    
    

 
 

  
   

 
 

  


gripper

    

 

   

 

   

figure    results on standard problem distributions for seven benchmarks  success ratio
 sr  and average length  al  are provided for both ff and our policy learned for the lrw
problem distribution  for a given domain  the same learned lrw policy is used for each
problem size shown 

in the remaining domain  the blocks world  the bootstrapping provided by increasingly
long random walks appears particularly useful  the policies learned at each of the walk
lengths            and     are increasingly effective on the target lrw distribution rw   
for walks of length    and      it takes multiple iterations to master the provided level of
difficulty beyond the previous walk length  finally  upon mastering walk length      the
resulting policy appears to perform well for any walk length  the learned policy is modestly
superior to ff on rw  in success ratio and average length 
evaluation on the original problem distributions  in each domain we denote
by  the best learned lrw policyi e   the policy  from each domain  with the highest
performance on rw    as shown in figure    the taxonomic decision lists corresponding
to  for each domain is given in appendix b  figure   shows the performance of    in
comparison to ff  on the original intended problem distributions for each of our domains 
we measured the success ratio of both systems by giving a time limit of     seconds to solve
a problem  here we have attempted to select the largest problem sizes previously used in
evaluation of domain specific planners  either in aips      or in bacchus and kabanza
        as well as show a smaller problem size for those cases where one of the planners
we show performed poorly on the large size  in each case  we use the problem generators
provided with the domains  and evaluate on     problems of each size 
overall  these results indicate that our learned  reactive policies are competitive with
the domain independent planner ff  it is important to remember that these policies are
learned in a domain independent fashion  and thus lrw api can be viewed as a general
approach to generating domain specific reactive planners  on two domains  blocks world
   

fifern  yoon    givan

and briefcase  our learned policies substantially outperform ff on success ratio  especially
on large domain sizes  on three domains  elevator  schedule  and gripper  the two approaches perform quite similarly on success ratio  with our approach superior in average
length on schedule but ff superior in average length on elevator 
on two domains  logistics and freecell  ff substantially outperforms our learned policies on success ratio  we believe that this is partly due to an inadequate policy language 
as discussed above  we also believe  however  that another reason for the poor performance
is that the long random walk distribution rw  does not correspond well to the standard
problem distributions  this seems to be particularly true for freecell  the policy learned
for freecell           achieved a success ratio of    percent on rw    however  for the standard distribution it only achieved    percent  this suggests that rw  generates problems
that are significantly easier than the standard distribution  this is supported by the fact
that the solutions produced by ff on the standard distribution are on average twice as long
as those produced on rw    one likely reason for this is that it is easy for random walks to
end up in dead states in freecell  where no actions are applicable  thus the random walk
distribution will typically produce many problems where the goals correspond to such dead
states  the standard distribution on the other hand will not treat such dead states as goals 
    probabilistic planning experiments
here we present experiments in three probabilistic domains that are described in the probabilistic planning domain language ppddl  younes        
 ground logistics  c  p    a probabilistic version of logistics with no airplanes  with c
cities and p packages  the driving action has a probability of failure in this domain 
 colored blocks world  n    a probabilistic blocks world with n colored blocks  where
goals involve constructing towers with certain color patterns  there is a probability
that moved blocks fall to the floor 
 boxworld  c  p    a probabilistic version of full logistics with c cities and p packages 
transportation actions have a probability of going in the wrong direction 
the ground logistics domain is originally from boutilier et al          and was also used
for evaluation in yoon et al          the colored blocks world and boxworld domains are
the domains used in the hand tailored track of ippc in which our lrw api technique was
entered  in the hand tailored track  participants were provided with problem generators for
each domain before the competition and were allowed to incorporate domain knowledge into
the planner for use at competition time  we provided the problem generators to lrw api
and learned policies for these domains  which were then entered into the competition 
we have also conducted experiments in the other probabilistic domains from yoon et al 
        including variants of the blocks world and a variant of ground logistics  some of
which appeared in fern et al          however  we do not show those results here since they
are qualitatively identical to the deterministic blocks world results described above and the
ground logistics results we show below 
for our three probabilistic domains  we conducted lrw experiments using the same
procedure as above  all parameters given to lrw api were the same as above except
   

fin

sr

rw n
al

iter   

iter   

api with a policy language bias

rw 
sr
al

boxworld       
 
       
   
 
       
   
 
       
   
 
       
   
          
    
    
          
          
    
    
          
          
    
standard distribution        

n

sr

rw n
al

sr

rw 
al

ground logistics          
    
    
    
    
    
    
    
    
    
 

    
    
    
    
    
    
  
    
    


 
      
 
       
     
 
standard distribution

    
    
    
          

    
    
 
 

     
    
   
  

colored blocks world     
 
 
 
 
 

      
   
      
   
       
    
        
    
        
    
standard distribution     

    
    
    
    
    
    

    
    
    
    
    
   

figure    results for each iteration of lrw api in three probabilistic planning domains 
for each iteration  we show the walk length n used for learning  along with the success ratio
 sr  and average length  al  of the learned policy on both rw n and rw    for each
benchmark  we show performance on the standard problem distribution of the policy whose
performance is best on rw   
that the sampling width used for rollout was set to w       and  was set to      in order to
account for the stochasticity in these domains  the results of these experiments are shown
in figure    these tables have the same form as figure   only the last row given for each
domain now gives the performance of  on the standard distribution  i e   problems drawn
from the domains problem generator  for colored blocks world the problem generator
produces problems whose goals are specified using existential quantifiers  for example  a
simple goal may be there exists blocks x and y such that x is red  y is blue and x is on y 
since our policy language cannot directly handle existentially quantified goals we preprocess
the planning problems produced by the problem generator to remove them  this was done
by assigning particular block names to the existential variables  ensuring that the static
properties of a block  in this case color  satisfied the static properties of the variable is
was assigned to  in this domain  finding such an assignment was trivial  and the resulting
assignment was taken to be the goal  giving a planning problem to which our learned policy
was applied  since the blocks world states are fully connected  the resulting goal is always
guaranteed to be achievable 

for boxworld  lrw api is not able to find a good policy for rw  or the standard
distribution  again  as for deterministic logistics and freecell  we believe that this is
primarily because of the restricted policy languages that is currently used by our learner 
here  as for those domains  we see that the decision list learned for boxworld contains many
very specific rules  indicating that the learner was not able to generalize well beyond the
   

fifern  yoon    givan

training trajectories  for ground logistics  we see that lrw api quickly finds a good
policy for both rw  and the standard distribution 
for colored blocks world  we also see that lrw api is able to quickly find a good
policy for both rw  and the standard distribution  however  unlike the deterministic
 uncolored  blocks world  here the success ratio is observed to be less than one  solving   
percent of the problems  it is unclear  why lrw api is not able to find a perfect policy 
it is relatively easy to hand code a policy for colored blocks world using the language of the
learner  hence inadequate knowledge representation is not the answer  the predicates and
action types for this domain are not the same as those in its deterministic counterpart and
other stochastic variants that we have previously considered  this difference apparently
interacts badly with our learners search bias  causing it to fail to find a perfect policy 
nevertheless  these two results  along with the probabilistic planning results not shown
here  indicate that when a good policy is expressible in our language  lrw api can
find good policies in complex relational mdps  this makes lrw api one of the few
techniques that can simultaneously cope with the complexity resulting from stochasticity
and from relational structure in domains such as these 

   related work
boutilier et al         presented the first exact solution technique for relational mdps
based on structured dynamic programming  however  a practical implementation of the
approach was not provided  primarily due to the need for the simplification of first order
logic formulas  these ideas  however  served as the basis for a logic programming based
system  kersting  van otterlo    deraedt        that was successfully applied to blocksworld problems involving simple goals and a simplified logistics world  this style of approach
is inherently limited to domains where the exact value functions and or policies can be
compactly represented in the chosen knowledge representation  unfortunately  this is not
generally the case for the types of domains that we consider here  particularly as the planning
horizon grows  nevertheless  providing techniques such as these that directly reason about
the mdp model is an important direction  note that our api approach essentially ignores
the underlying mdp model  and simply interacts with the mdp simulator as a black box 
an interesting research direction is to consider principled approximations of these techniques that can discover good policies in more difficult domains  this has been considered
by guestrin et al       a   where a class based mdp and value function representation was
used to compute an approximate value function that could generalize across different sets
of objects  promising empirical results were shown in a multi agent tactical battle domain 
presently the class based representation does not support some of the representation features that are commonly found in classical planning domains  e g   relational facts such as
on a  b  that change over time   and thus is not directly applicable in these contexts  however  extending this work to richer representations is an interesting direction  its ability to
reason globally about a domain may give it some advantages compared to api 
our approach is closely related to work in relational reinforcement learning  rrl   dzeroski et al          a form of online api that learns relational value function approximations  q value functions are learned in the form of relational decision trees  q trees  and
are used to learn corresponding policies  p  trees   the rrl results clearly demonstrate the
   

fiapi with a policy language bias

difficulty of learning value function approximations in relational domains  compared to p trees  q trees tend to generalize poorly and be much larger  rrl has not yet demonstrated
scalability to problems as complex as those considered hereprevious rrl blocks world
experiments include relatively simple goals     which lead to value functions that are much
less complex than the ones here  for this reason  we suspect that rrl would have difficulty
in the domains we consider  precisely because of the value function approximation step that
we avoid  however  this needs to be experimentally tested 
we note  however  that our api approach has the advantage of using an unconstrained
simulator  whereas rrl learns from irreversible world experience  pure rl   by using
a simulator  we are able to estimate the q values for all actions at each training state 
providing us with rich training data  without such a simulator  rrl is not able to directly
estimate the q value for each action in each training statethus  rrl learns a q tree to
provide estimates of the q value information needed to learn the p  tree  in this way  valuefunction learning serves a more critical role when a simulator is unavailable  we believe 
that in many relational planning problems  it is possible to learn a model or simulator
from world experiencein this case  our api approach can be incorporated as the planning
component of rrl  otherwise  finding ways to either avoid learning or to more effectively
learn relational value functions in rrl is an interesting research direction 
researchers in classical planning have long studied techniques for learning to improve
planning performance  for a collection and survey of work on learning for planning domains see minton        and zimmerman and kambhampati         two primary approaches are to learn domain specific control rules for guiding search based planners e g  
minton  carbonell  knoblock  kuokka  etzioni  and gil         veloso  carbonell  perez 
borrajo  fink  and blythe         estlin and mooney         huang  selman  and kautz
        ambite  knoblock  and minton         aler  borrajo  and isasi         and  more
closely related  to learn domain specific reactive control policies  khardon      a  martin
  geffner        yoon et al         
regarding the latter  our work is novel in using api to iteratively improve stand alone
control policies  regarding the former  in theory  search based planners can be iteratively
improved by continually adding newly learned control knowledgehowever  it can be difficult to avoid the utility problem  minton         i e   being swamped by low utility rules 
critically  our policy language bias confronts this issue by preferring simpler policies  our
learning approach is also not tied to having a base planner  let alone tied to a single particular base planner   unlike most previous work  rather  we only require a domain simulator 
the ultimate goal of such systems is to allow for planning in large  difficult problems
that are beyond the reach of domain independent planning technology  clearly  learning
to achieve this goal requires some form of bootstrapping and almost all previous systems
have relied on the human for this purpose  by far  the most common human bootstrapping
approach is learning from small problems  here  the human provides a small problem
distribution to the learner  by limiting the number of objects  e g   using     blocks in the
blocks world   and control knowledge is learned for the small problems  for this approach to
work  the human must ensure that the small distribution is such that good control knowledge
for the small problems is also good for the large target distribution  in contrast  our long    the most complex blocks world goal for rrl was to achieve on a  b  in an n block environment  we
consider blocks world goals that involve all n blocks 

   

fifern  yoon    givan

random walk bootstrapping approach can be applied without human assistance directly to
large planning domains  however  as already pointed out  our goal of performing well on
the lrw distribution may not always correspond well with a particular target problem
distribution 
our bootstrapping approach is similar in spirit to the bootstrapping framework of learning from exercises natarajan        reddy   tadepalli         here  the learner is provided with planning problems  or exercises  in order of increasing difficulty  after learning
on easier problems  the learner is able to use its new knowledge  or skills  in order to bootstrap learning on the harder problems  this work  however  has previously relied on a
human to provide the exercises  which typically requires insight into the planning domain
and the underlying form of control knowledge and planner  our work can be viewed as an
automatic instantiation of learning from exercises  specifically designed for learning lrw
policies 
our random walk bootstrapping is most similar to the approach used in micro hillary
 finkelstein   markovitch         a macro learning system for problem solving  in that
work  instead of generating problems via random walks starting at an initial state  random
walks were generated backward from goal states  this approach assumes that actions are
invertible or that we are given a set of backward actions  when such assumptions hold 
the backward random walk approach may be preferable when we are provided with a goal
distribution that does not match well with the goals generated by forward random walks 
of course  in other cases forward random walks may be preferable  micro hillary was
empirically tested in the n  n sliding puzzle domain  however  as discussed in that work 
there remain challenges for applying the system to more complex domains with parameterized actions and recursive structure  such as familiar strips domains  to the best of our
knowledge  the idea of learning from random walks has not been previously explored in the
context of strips planning domains 
the idea of searching for a good policy directly in policy space rather than value function
space is a primary motivation for policy gradient rl algorithms  however  these algorithms
have been largely explored in the context of parametric policy spaces  while this approach
has demonstrated impressive success in a number of domains  it appears difficult to define
such policy spaces for the types of planning problem considered here 
our api approach can be viewed as a type of reduction from planning or reinforcement
learning to classification learning  that is  we solve an mdp by generating and solving
a series of cost sensitive classification problems  recently  there have been several other
proposals for reducing reinforcement learning to classification  dietterich and wang       
proposed a reinforcement learning approach based on batch value function approximation 
one of the proposed approximations enforced only that the learned approximation assign
the best action the highest value  which is a type of classifier learning  lagoudakis and parr
       proposed a classification based api approach that is closely related to ours  the primary difference is the form of the classification problem produced on each iteration  they
generate standard multi class classification problems  whereas we generate cost sensitive
problems  bagnell  kakade  ng  and schneider        introduced a closely related algorithm for learning non stationary policies in reinforcement learning  for a specified horizon
time h  their approach learns a sequence of h policies  at each iteration  all policies are
held fixed except for one  which is optimized by forming a classification problem via policy
   

fiapi with a policy language bias

rollout     finally  langford and zadrozny        provide a formal reduction from reinforcement learning to classification  showing that  accurate classification learning implies
near optimal reinforcement learning  this approach uses an optimistic variant of sparse
sampling to generate h classification problems  one for each horizon time step 

   summary and future work
we introduced a new variant of api that learns policies directly  without representing
approximate value functions  this allowed us to utilize a relational policy language for
learning compact policy representations  we also introduced a new api bootstrapping
technique for goal based planning domains  our experiments show that the lrw api
algorithm  which combines these techniques  is able to find good policies for a variety of
relational mdps corresponding to classical planning domains and their stochastic variants 
we know of no previous mdp technique that has been successfully applied to problems
such as these 
our experiments also pointed to a number of weaknesses of our current approach  first 
our bootstrapping technique  based on long random walks  does not always correspond
well to the problem distribution of interest  investigating other automatic bootstrapping
techniques is an interesting direction  related to the general problems of exploration and
reward shaping in reinforcement learning  second  we have seen that limitations of our
current policy language and learner are partly responsible for some of the failures of our
system  in such cases  we must either     depend on the human to provide useful features
to the system  or    extend the policy language and develop more advanced learning techniques  policy language extensions that we are considering include various extensions to the
knowledge representation used to represent sets of objects in the domain  in particular  for
route finding in maps grids   as well as non reactive policies that incorporate search into
decision making 
as we consider ever more complex planning domains  it is inevitable that our brute force
enumeration approach to learning policies from trajectories will not scale  presently our
policy learner  as well as the entire api technique  makes no attempt to use the definition
of a domain when one is available  we believe that developing a learner that can exploit
this information to bias its search for good policies is an important direction of future work 
recently  gretton and thiebaux        have taken a step in this direction by using logical
regression  based on a domain model  to generate candidate rules for the learner  developing tractable variations of this approach is a promising research direction  in addition 
exploring other ways of incorporating a domain model into our approach and other modelblind approaches is critical  ultimately  scalable ai planning systems will need to combine
experience with stronger forms of explicit reasoning 

    here the initial state distribution is dictated by the policies at previous time steps  which are held fixed 
likewise the actions selected along the rollout trajectories are dictated by policies at future time steps 
which are also held fixed 

   

fifern  yoon    givan

acknowledgments
we would like to thank lin zhu for originally suggesting the idea of using random walks
for bootstrapping  we would also like to thank the reviewers and editors for helping to
vastly improve this paper  this work was supported in part by nsf grants         iis
and         iis 

appendix a  omitted proofs
proposition    let h be a finite class of deterministic policies  for any   h  and any

set of n     ln  h 
 trajectories drawn independently from dh   there is a     probability
that every   h consistent with the trajectories satisfies v     v      vmax      h   
proof  we first introduce some basic properties and notation that will be used below  for
any deterministic policy   if  is consistent with a trajectory t  then dh  t  is entirely
determined by the underlying mdp transition dynamics  this implies that if two deterministic policies  and    are both consistent with a trajectory t then dh  t    dh  t   we
will denote by v t  the cumulative discounted reward accumulated by executing trajectory
p
t  for any policy   we have that v h      t dh  t   v t  where the summation is taken
over all length h trajectories  or simply those that are consistent with    finally for a set
p
of trajectories  we will let dh      t  dh  t  giving the cumulative probability of 
generating the trajectories in  
consider a particular   h and any   h that is consistent with the n trajectories of
  we will let  denote the set of all length h trajectories that are consistent with  and
 denote the set of trajectories that are consistent with   following khardon      b  we
first give a standard argument showing that with high probability dh           to see
this consider the probability that  is consistent with all n     ln  h 
 trajectories of 


given that dh          the probability that this occurs is at most      n   en    h 
 

thus the probability of choosing such a  is at most  h   h 
    thus  with probability at

least     we know that dh           note that dh      dh    
now given the condition that dh          we show that v h     v h      vmax by
considering the difference of the two value functions 
v h     v h     

x

dh  t   v t  

t

 

x

 

 v t   

x

 dh  t   dh  t    v t  

t

dh  t 

t



dh  t   v t 

t

dh  t 

t

x

x

 v t      

t

dh  t 

t

     dh      

dh         dh    

vmax  dh  

  vmax    

x

  vmax
   

x

 v t 

dh  t   v t 

fiapi with a policy language bias

the third lines follows since dh  t    dh  t  when  and  are both consistent with t  the
last line follows by substituting our assumption of dh      dh        into the previous
line  combining this result with the approximation due to using a finite horizon 
v     v     v h     v h        h vmax
we get that with probability at least      v     v      vmax      h    which completes
the proof   
proposition   
have

for any mdp with q advantage at least    and any             if we

h   log


 vmax

 vmax



 



w  
  

 

ln

 a 
 

then for any state s  a   s    a  s  with probability at least        
proof  given a real valued random variable x bounded in absolute value by xmax and
an average x of w independently drawn samples of x  the
q additive chernoff bound states
that with probability at least       e x   x   xmax  wln   
note that qh  s  a  is the expectation of the random variable x s  a    r s  a   

vh   t  s  a   and q s  a  is simply an average of w independent samples of x s  a  
 
the chernoff bound tells us that with probability at least     a 
   qh  s  a   q s  a   
q

 


vmax ln  a ln
  where  a  is the number of actions  substituting in our choice of w we
w

get that with probability at least          qh  s  a   q s  a       is satisfied by all actions
simultaneously  we also know that  q  s  a   qh  s  a     h vmax   which by our choice

of h gives   q  s  a   qh  s  a         combining these relationships we get that with

probability at least          q  s  a   q s  a       holds for all actions simultaneously 
we can use this bound to show that with high probability the q value estimates for

actions in a  s  will be within a   range of each other  and other actions will be outside
of that range  in particular  consider any action a  a  s  and some other action a    if
a   a  s  then we have that q  s  a    q  s  a     from the above bound we get that

 q s  a   q s  a           otherwise a    a  s  and by our assumption about the mdp
q advantage we get that q  s  a   q  s  a        using the above bound this implies

that q s  a   q s  a          these relationships and the definition of a   s  imply that
with probability at least       we have that a   s    a  s    

appendix b  learned policies
below we give the final taxonomic decision list policies that were learned for each domain
in our experiments  rather than write rules in the form a x            xk     l   l       lm
   

fifern  yoon    givan

we drop the variables from the head and simply write  a   l   l       lm   in addition
below we use the notation r as short hand for  r    where r is a relation  when interpreting the policies  it is important to remember that for each rule of action type a  the
preconditions for action type a are implicitly included in the constraints  thus  the rules
will often allow actions that are not legal  but those actions will never be considered by the
system 
gripper
   move   x    not  gat  carry  gripper       x    not  gat  at  at robby       x    gat  not
 cat  room       x    cat ball  
   drop   x    gat  at robby  
   pick   x    gat   gat  carry  gripper       x    gat   not at robby   
   pick   x    at  not  gat  room       x    gat   not at robby   
   pick   x    gat   not at robby   
briefcase
   put in   x    gat   not is at   
   move   x    at  not  cat  location       x    not  at  gat  cis at    
   move   x    gat in     x    not  cat in   
   take out   x    cat  is at  
   move   x   gis at 
   move   x    at  gat  cis at   
   put in   x   universal 
schedule
   do immersion paint   x    not  painted  x        x    gpainted  x    
   do drill press   x    ghas holeo  x       x    ghas holew  x    
   do lathe   x    not  shape  cylindrical      x    gshape  cylindrical  
   do drill press   x    ghas holew  x    
   do drill press   x    ghas holeo  x    
   do grind   x    not  surface condition  smooth      x    gsurface condition  smooth  
   do polish   x    not  surface condition  polished      x    gsurface condition  polished  
   do time step 
elevator
   depart   x   gserved 
   down   x    destin boarded     x    destin gserved  
   up   x    destin boarded     x    destin gserved     x    above  origin boarded      x   not
 destin boarded   
   board   x    not cserved     x   gserved 
   up   x    origin gserved     x    not  destin boarded      x    not  destin gserved      x  
 origin  not cserved      x    above  destin passenger      x    not  destin boarded   
   down   x    origin gserved     x    origin  not cserved      x    not  destin boarded   

   

fiapi with a policy language bias

   up   x    not  origin boarded      x    not  destin boarded   
freecell
   sendtohome   x   canstack   canstack  suit   suit incell        x    not ghome  
   move b   x    not  canstack  on ghome       x    canstack ghome     x    value   not
colspace      x    canstack   suit   suit bottomcol    
   move   x    canstack   on  canstack   on  ghome        x    canstack  on  suit   suit bottomcol        x    on  bottomcol     x    canstack   on ghome      x    on   canstack 
 on   not  canstack  value  cellspace          x    not  canstack   suit   suit incell      
 x    canstack bottomcol     x    suit   suit  on   not  canstack  value  cellspace       
  x    value   not colspace        on   not  canstack   suit   suit incell         x    not
 canstack  chome   
   sendtohome b   x    not ghome  
   sendtohome   x    on   canstack  canstack   suit   suit incell         x    not ghome  
   sendtohome   x   on   on  ghome      x    canstack   not ghome      x    canstack   not
 on  ghome      x    not ghome  
   move b   x    not  canstack  ghome      x    value   not colspace      x    canstack 
 suit   suit bottomcol    
   sendtofree   x    on   on  ghome      x    not ghome  
   sendtohome   x    canstack   canstack  on ghome       x    not ghome  
    sendtohome     ghome   x    value   not colspace      x    not  canstack   on   not
ghome        x    on   not  on  ghome       x    not ghome  
    newcolfromfreecell   x   ghome 
    sendtohome   x    canstack   on ghome      x   ghome    x    not ghome  
    move b   x    value   value home      x    value   not colspace      x    canstack   suit 
 suit bottomcol    
    sendtohome   x    canstack   on   canstack   suit   suit incell         x    not ghome  
    sendtohome   x    on   on   canstack   on   not ghome        x    not ghome  
    sendtofree   x    canstack   on  on  ghome       x    suit   suit bottomcol      x    on 
bottomcol  
    move   x    on   canstack  clear      x    on   canstack  on   not  canstack  value 
cellspace          x    not ghome     x   ghome    x    canstack bottomcol     x    on 
 canstack   on   not  canstack  value  cellspace          x    not  canstack   suit 
 suit incell        x    on  bottomcol     x    suit   suit  on   not  canstack  value 
cellspace          x    value   not colspace      x    on   not  canstack   suit   suit
incell         x    not  canstack  chome   
    move   x    suit   suit chome      x    not ghome     x    not  on  ghome      x    on 
 canstack  bottomcol   
    sendtohome   x    canstack  on  canstack  on ghome        x   ghome   x    not ghome  
    sendtohome   x    canstack   on  canstack   on  ghome        x    not  suit   suit bottomcol       x    not ghome  
    sendtofree   x    canstack  on  canstack  value  cellspace        x    canstack chome  
    sendtohome   x    canstack   suit   suit incell       x    on   not  canstack  value 
cellspace        x    not ghome  
    sendtonewcol   x    canstack  canstack   on  ghome    
    sendtofree   x    canstack  on   canstack   on  ghome        x    not  canstack ghome   
  x    not  on  ghome      x    on   not  canstack   suit   suit incell      

   

fifern  yoon    givan

    sendtofree   x    on   canstack  canstack   on  ghome        x    not  canstack bottomcol      x    not  canstack   canstack  on ghome     
    sendtofree   x    canstack  on   canstack   on   not ghome         x    not  canstack
ghome      x    canstack  not  suit   suit bottomcol     
    sendtohome   x    canstack   canstack  on  ghome       x    on   canstack   on   not
ghome        x    not ghome     x    not ghome  
    sendtofree   x    canstack  on   canstack   on   not ghome         x    canstack  canstack 
 on  ghome       x    not ghome     x    on   canstack   on   not  canstack  value 
cellspace       
    sendtofree   x    canstack chome     x    suit   suit  canstack   on  ghome     
    sendtohome   x   ghome    x    suit   suit bottomcol      x    canstack   not  on 
ghome       x    not ghome  
    sendtofree   x    canstack   on  ghome      x    canstack   on   not ghome    
    sendtofree   x    canstack  on  ghome      x    not ghome     x    on   canstack   on 
 not ghome     
    sendtohome   x    on   canstack  bottomcol      x    canstack   not ghome      x  
 not ghome  
    sendtofree   x    canstack  on  canstack   on   not ghome         x    not  suit   suit
bottomcol       x    not ghome  
    sendtohome   x    not  canstack  ghome      x    not  suit   suit bottomcol       x  
 not ghome  
    sendtofree   x    not  on  ghome      x    canstack  canstack   on   not ghome     
    sendtofree b   x    not ghome  
    sendtofree   x   universal 
logistics
   fly airplane   x    in  gat  airport      x    not  in  gat   at airplane        x    not  gat
 in  truck       x    not  in  gat   not airport     
   load truck   x    in  not  gat   not airport        x    gat   gat  in  truck       x    not
 cat  location   
   drive truck   x    at  at   gat  in  truck        x    in city   in city  at airplane       x 
  at   not  gat  in  truck     
   unload truck   x    gat   at  in obj       x    gat   at obj      x    not  gat   at airplane       x    at   gat  in  truck       x    gat   at truck   
   fly airplane   x    gat  in  airplane      x    in  not  gat   at truck        x    at   not
 gat  in  truck     
   unload airplane   x    not  in  gat   not airport        x    gat   at airplane   
   load truck   x    in  not  gat  location       x    not  gat   at truck       x    gat 
location  
   unload truck   x    gat   at truck      x    at  airport     x    not  in  gat   not airport        x    gat   at airplane   
   fly airplane   x    at  at   gat  in  truck        x    at   gat  gat  location       x  
 not  at   cat obj    
    drive truck   x    in  gat  location      x    at   not  gat  in  truck        x    at   not
 at airplane    
    unload truck   x    at   gat  gat   not airport        x    not  gat  airport   
    fly airplane   x    not  gat  gat  location       x    at   gat  at   cat obj        x    at
 not  gat   at airplane        x    at obj     x    not  in  gat  airport       x    not  at
 in obj    

   

fiapi with a policy language bias

    unload truck   x    gat  airport  
    load truck   x    at   cat  gat   at airplane        x    not  gat  location   
    load truck   x    gat   cat  gat   at airplane        x    not  gat   at truck       x  
 gat   at  gat   at airplane     
    load truck   x    gat   not airport      x    not  gat   at truck    
    fly airplane   x    at  gat   at airplane       x    at   cat obj   
    fly airplane   x    not  gat  at   cat obj        x    at   gat  at   cat obj        x    at 
 gat  gat   at truck     
    load truck   x    gat   at airplane      x    not  gat   at truck       x    at   cat obj   
    load airplane   x    gat  airport     x    not  cat  location      x    gat   not  at
airplane       x    not  in  gat   not airport     
    fly airplane   x    at  gat   at airplane       x    not  at truck   
    load truck   x    at   cat  gat   not airport        x    gat  airport  
    drive truck   x    not  at obj      x    not  at   cat obj       x    at   gat  gat  location    
    load truck   x    gat   cat  cat  airport       x    not  cat  location   
    fly airplane   x    at  gat   at airplane       x    at   at obj   
    drive truck   x    in obj  
    drive truck   x    at   gat  gat  airport       x    at  gat  airport      x    at   not
 at airplane    
    fly airplane   x    cat  gat   at truck       x    at   gat  gat  location    
    load truck   x    gat   at obj      x    not  cat  location   
    drive truck   x    at  gat   at airplane       x    not  at   cat obj    
    drive truck   x    at airplane     x    at  gat   at truck    
    unload airplane   x    not  at   cat obj       x    gat   not airport   
    drive truck   x    at  gat   at truck    
    load truck   x    at   not airport      x    gat  airport  
    fly airplane   x    at  gat  location   
    fly airplane   x    in obj     x    not  gat  gat  location       x    not  in  gat  airport    
  x    not  at  in obj       x    at   gat  at   cat obj      
    drive truck   x    at   at airplane   
    load airplane   x    gat   not airport   
blocks world
   stack   x    gon holding     x    con  min gon      x    gon on table  
   putdown 
   unstack   x    on  on  min gon       x    con  on  min gon    
   unstack   x    on   gon clear      x    gon  on  min gon       x    on  gon on table   
 x    gon  not clear   
   pickup   x    gon   con  min gon       x    gon  clear    x    gon   con on table   
   unstack   x    con  gon  clear      x    gon   on  min gon       x    gon   con
clear   

   

fifern  yoon    givan

   unstack   x    not  gon  min gon    
   unstack   x    gon on table     x    gon   con  min gon       x    gon  clear  
   unstack   x    not  con  min gon       x    on  gon  on table      x    gon  not ontable      x    gon  gon on table     x    gon  not clear   
    unstack   x    not  con clear      x    gon   con on table   
    unstack   x    gon  clear     x    on  on  min gon   
ground logistics
   load   x    not  in  gin  city       x    not  cin  city      x    gin  city  
   unload   x    gin  x    
   drive   x    in  gin  x     
   drive   x    not  gin block      x    in  gin  city      x   car   x   clear 
   drive   x    in  gin  rain      x   truck 
colored blocks world
   pick up block from   x    not  con top of table      x    gon top of   on top of block   
   put down block on   x    con top of   con top of  block      x    gon top of holding   
 x    con top of table  
   pick up block from   x    not  con top of block      x    on top of  gon top of  table   
  x    gon top of  gon top of  block      x    not  con top of  block      x    on topof   gon top of block      x    gon top of  gon top of  block   
   pick up block from   x    not  con top of table      x    gon top of   con top of table      x    gon top of  on top of  block   
   put down block on   x    con top of   on top of  table      x    gon top of holding     x 
  con top of table  
   put down block on   x    con top of  on top of block      x    gon top of   gon top of 
block   
   put down block on   x    gon top of holding     x    con top of table  
   put down block on   x   table 
   pick up block from   x    not  con top of table      x    gon top of   con top of table   
    pick up block from   x    gon top of   con top of  table      x   table    x    gon top of
 gon top of block      x    gon top of  on top of  table   
    pick up block from   x    on top of  con top of block      x    gon top of   con top of 
table   
    pick up block from   x    on top of  block     x    not  con top of table      x    gontop of  on top of  block      x    gon top of  on top of  block   
    pick up block from   x    gon top of   gon top of  table   
boxworld
   drive truck   x    gbox at city  box at city  x        x    not  can fly  truck at city  not
previous        x    can drive  previous     x    not  can fly  truck at city  not previous        x    not  can fly  box at city box       x    can drive  can drive  box at city box    
  x    not  can fly  truck at city  box on truck  gbox at city  city      
   unload box from truck in city   x    gbox at city   truck at city previous      x    gboxat city box     x    not  box at city previous      x    gbox at city   can drive   candrive   can fly city        x    box on truck  gbox at city  previous   
   drive truck   x    box on truck  gbox at city  x        x    not  can drive  truck at city
 box on truck  gbox at city  city      

   

fiapi with a policy language bias

   drive truck   x    can drive  box at city previous      x    can fly  can drive   box at city
box       x    can drive  can fly  truck at city truck       x    not  can drive  truck at city
 box on truck  gbox at city  city         x   previous    x    can drive  can drive x        x 
  not  truck at city  box on truck  gbox at city  city        x    not  can fly previous    
 x    can drive  not  box at city box       x    can drive  can drive  x        x    can drive
 not  truck at city truck    
   load box on truck in city   x    gbox at city   can drive  truck at city truck       x    not
 plane at city previous      x    can drive  can drive   can fly city       x    can drive 
 not  truck at city  not previous     
   unload box from truck in city   x    gbox at city  box on truck  truck      x    not  canfly  truck at city  box on truck  gbox at city  city         x    gbox at city  city  
   drive truck   x    box on truck  gbox at city  previous      x    can drive  gbox at city
 gbox at city  previous       x    not  plane at city plane      x    not  can fly  gbox atcity  gbox at city  previous     
   fly plane   x    box on plane  gbox at city  x     
   unload box from plane in city   x    gbox at city  previous  
    fly plane   x    not  can drive  truck at city  box on truck  gbox at city  city         x  
 gbox at city box     x    not  plane at city previous      x    not previous  
    load box on plane in city   x    gbox at city   can fly previous      x    not  truck at city
 not previous       x    not  can drive  truck at city  box on truck  gbox at city  city      
    drive truck   x    box on truck  gbox at city  x        x    not  can drive  can fly previous       x    can drive   can fly city   
    load box on truck in city   x    gbox at city  previous  

references
aler  r   borrajo  d     isasi  p          using genetic programming to learn and improve
control knowledge  artificial intelligence                  
ambite  j  l   knoblock  c  a     minton  s          learning plan rewriting rules  in
artificial intelligence planning systems  pp      
bacchus  f          the aips    planning competition  ai magazine                 
bacchus  f     kabanza  f          using temporal logics to express search control knowledge for planning  artificial intelligence             
bagnell  j   kakade  s   ng  a     schneider  j          policy search by dynamic programming  in proceedings of the   th conference on advances in neural information
processing 
bellman  r          dynamic programming  princeton university press 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
boutilier  c     dearden  r          approximating value trees in structured dynamic
programming  in saitta  l   ed    international conference on machine learning 
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
with factored representations  artificial intelligence                   
boutilier  c   reiter  r     price  b          symbolic dynamic programming for first order
mdps  in international joint conference on artificial intelligence 
   

fifern  yoon    givan

dean  t     givan  r          model minimization in markov decision processes  in national
conference on artificial intelligence  pp         
dean  t   givan  r     leach  s          model reduction techniques for computing approximately optimal solutions for markov decision processes  in conference on uncertainty
in artificial intelligence  pp         
dietterich  t     wang  x          batch value function approximation via support vectors 
in proceedings of the conference on advances in neural information processing 
dzeroski  s   deraedt  l     driessens  k          relational reinforcement learning  machine learning          
estlin  t  a     mooney  r  j          multi strategy learning of search control for partialorder planning  in national conference on artificial intelligence 
fern  a   yoon  s     givan  r          approximate policy iteration with a policy language bias  in proceedings of the   th conference on advances in neural information
processing 
finkelstein  l     markovitch  s          a selective macro learning algorithm and its
application to the nxn sliding tile puzzle  journal of artificial intelligence research 
          
givan  r   dean  t     greig  m          equivalence notions and model minimization in
markov decision processes  artificial intelligence                    
gretton  c     thiebaux  s          exploiting first order regression in inductive policy
selection  in conference on uncertainty in artificial intelligence 
guestrin  c   koller  d   gearhart  c     kanodia  n       a   generalizing plans to new
environments in relational mdps  in international joint conference on artificial intelligence 
guestrin  c   koller  d   parr  r     venkataraman  s       b   efficient solution algorithms
for factored mdps  journal of artificial intelligence research             
hoffman  j   porteous  j     sebastia  l          ordered landmarks in planning  journal
of artificial intelligence research             
hoffmann  j     nebel  b          the ff planning system  fast plan generation through
heuristic search  journal of artificial intelligence research             
howard  r          dynamic programming and markov decision processes  mit press 
huang  y  c   selman  b     kautz  h          learning declarative control rules for
constraint based planning  in international conference on machine learning  pp 
       
kearns  m  j   mansour  y     ng  a  y          a sparse sampling algorithm for nearoptimal planning in large markov decision processes  machine learning          
       
kersting  k   van otterlo  m     deraedt  l          bellman goes relational  in proceedings
of the twenty first international conference on machine learning 
   

fiapi with a policy language bias

khardon  r       a   learning action strategies for planning domains  artificial intelligence                    
khardon  r       b   learning to take actions  machine learning               
lagoudakis  m     parr  r          reinforcement learning as classification  leveraging
modern classifiers  in international conference on machine learning 
langford  j     zadrozny  b          reducing t step reinforcement learning to classification 
http   hunch net jl projects reductions rl to class colt submission ps 
martin  m     geffner  h          learning generalized policies in planning domains using
concept languages  in international conference on principles of knowledge representation and reasoning 
mataric  m          reward functions for accelarated learning  in proceedings of the international conference on machine learning 
mcallester  d     givan  r          taxonomic syntax for first order inference  journal of
the acm                 
mcallester  d          observations on cognitive judgements  in national conference on
artificial intelligence 
mcgovern  a   moss  e     barto  a          building a basic block instruction scheduler
using reinforcement learning and rollouts  machine learning                   
minton  s          quantitative results concerning the utility of explanation based learning 
in national conference on artificial intelligence 
minton  s   ed            machine learning methods for planning  morgan kaufmann 
minton  s   carbonell  j   knoblock  c  a   kuokka  d  r   etzioni  o     gil  y         
explanation based learning  a problem solving perspective  artificial intelligence     
      
natarajan  b  k          on learning from exercises  in annual workshop on computational
learning theory 
reddy  c     tadepalli  p          learning goal decomposition rules using exercises  in
international conference on machine learning  pp          morgan kaufmann 
rivest  r          learning decision lists  machine learning                
tesauro  g          practical issues in temporal difference learning  machine learning    
       
tesauro  g     galperin  g          on line policy improvement using monte carlo search 
in conference on advances in neural information processing 
tsitsiklis  j     van roy  b          feature based methods for large scale dp  machine
learning           
veloso  m   carbonell  j   perez  a   borrajo  d   fink  e     blythe  j          integrating
planning and learning  the prodigy architecture  journal of experimental and
theoretical ai        
wu  g   chong  e     givan  r          congestion control via online sampling  in infocom 
   

fifern  yoon    givan

yan  x   diaconis  p   rusmevichientong  p     van roy  b          solitaire  man versus
machine  in conference on advances in neural information processing 
yoon  s   fern  a     givan  r          inductive policy selection for first order mdps  in
conference on uncertainty in artificial intelligence 
younes  h          extending pddl to model stochastic decision processes  in proceedings
of the international conference on automated planning and scheduling workshop on
pddl 
zimmerman  t     kambhampati  s          learning assisted automated planning  looking back  taking stock  going forward  ai magazine                 

   

fi