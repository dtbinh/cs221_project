journal of artificial intelligence research                  

submitted        published      

the latent relation mapping engine 
algorithm and experiments
peter d  turney

peter turney nrc cnrc gc ca

institute for information technology
national research council canada
ottawa  ontario  canada  k a  r 

abstract
many ai researchers and cognitive scientists have argued that analogy is the core
of cognition  the most influential work on computational modeling of analogy making is
structure mapping theory  smt  and its implementation in the structure mapping engine
 sme   a limitation of sme is the requirement for complex hand coded representations 
we introduce the latent relation mapping engine  lrme   which combines ideas from
sme and latent relational analysis  lra  in order to remove the requirement for handcoded representations  lrme builds analogical mappings between lists of words  using a
large corpus of raw text to automatically discover the semantic relations among the words 
we evaluate lrme on a set of twenty analogical mapping problems  ten based on scientific
analogies and ten based on common metaphors  lrme achieves human level performance
on the twenty problems  we compare lrme with a variety of alternative approaches and
find that they are not able to reach the same level of performance 

   introduction
when we are faced with a problem  we try to recall similar problems that we have faced
in the past  so that we can transfer our knowledge from past experience to the current
problem  we make an analogy between the past situation and the current situation  and we
use the analogy to transfer knowledge  gentner        minsky        holyoak   thagard 
      hofstadter        hawkins   blakeslee        
in his survey of the computational modeling of analogy making  french        cites
structure mapping theory  smt   gentner        and its implementation in the structure
mapping engine  sme   falkenhainer  forbus    gentner        as the most influential
work on modeling of analogy making  in sme  an analogical mapping m   a  b is from
a source a to a target b  the source is more familiar  more known  or more concrete 
whereas the target is relatively unfamiliar  unknown  or abstract  the analogical mapping
is used to transfer knowledge from the source to the target 
gentner        argues that there are two kinds of similarity  attributional similarity
and relational similarity  the distinction between attributes and relations may be understood in terms of predicate logic  an attribute is a predicate with one argument  such as
large x   meaning x is large  a relation is a predicate with two or more arguments  such
as collides with x  y    meaning x collides with y  
the structure mapping engine prefers mappings based on relational similarity over
mappings based on attributional similarity  falkenhainer et al          for example  sme
is able to build a mapping from a representation of the solar system  the source  to a
c
    
national research council canada  reprinted with permission 

fiturney

representation of the rutherford bohr model of the atom  the target   the sun is mapped
to the nucleus  planets are mapped to electrons  and mass is mapped to charge  note that
this mapping emphasizes relational similarity  the sun and the nucleus are very different
in terms of their attributes  the sun is very large and the nucleus is very small  likewise 
planets and electrons have little attributional similarity  on the other hand  planets revolve
around the sun like electrons revolve around the nucleus  the mass of the sun attracts the
mass of the planets like the charge of the nucleus attracts the charge of the electrons 
gentner        provides evidence that children rely primarily on attributional similarity
for mapping  gradually switching over to relational similarity as they mature  she uses the
terms mere appearance to refer to mapping based mostly on attributional similarity  analogy
to refer to mapping based mostly on relational similarity  and literal similarity to refer to a
mixture of attributional and relational similarity  since we use analogical mappings to solve
problems and make predictions  we should focus on structure  especially causal relations 
and look beyond the surface attributes of things  gentner         the analogy between
the solar system and the rutherford bohr model of the atom illustrates the importance of
going beyond mere appearance  to the underlying structures 
figures   and   show the lisp representations used by sme as input for the analogy
between the solar system and the atom  falkenhainer et al          chalmers  french 
and hofstadter        criticize smes requirement for complex hand coded representations 
they argue that most of the hard work is done by the human who creates these high level
hand coded representations  rather than by sme 
 defentity sun  type inanimate 
 defentity planet  type inanimate 
 defdescription solar system
entities  sun planet 
expressions    mass sun   name mass sun 
  mass planet   name mass planet 
  greater mass sun mass planet   name  mass 
  attracts sun planet   name attracts form 
  revolve around planet sun   name revolve 
  and  mass attracts form   name and  
  cause and  revolve   name cause revolve 
  temperature sun   name temp sun 
  temperature planet   name temp planet 
  greater temp sun temp planet   name  temp 
  gravity mass sun mass planet   name force gravity 
  cause force gravity attracts form   name why attracts   

figure    the representation of the solar system in sme  falkenhainer et al         
gentner  forbus  and their colleagues have attempted to avoid hand coding in their
recent work with sme   the cogsketch system can generate lisp representations from
simple sketches  forbus  usher  lovett  lockwood    wetzel         the gizmo system
can generate lisp representations from qualitative physics models  yan   forbus        
the learning reader system can generate lisp representations from natural language text
 forbus et al          these systems do not require lisp input 
   dedre gentner  personal communication  october          

   

fithe latent relation mapping engine

 defentity nucleus  type inanimate 
 defentity electron  type inanimate 
 defdescription rutherford atom
entities  nucleus electron 
expressions    mass nucleus   name mass n 
  mass electron   name mass e 
  greater mass n mass e   name  mass 
  attracts nucleus electron   name attracts form 
  revolve around electron nucleus   name revolve 
  charge electron   name q electron 
  charge nucleus   name q nucleus 
  opposite sign q nucleus q electron   name  charge 
  cause  charge attracts form   name why attracts   

figure    the rutherford bohr model of the atom in sme  falkenhainer et al         

however  the cogsketch user interface requires the person who draws the sketch to identify the basic components in the sketch and hand label them with terms from a knowledge
base derived from opencyc  forbus et al         note that opencyc contains more than
       hand coded concepts  and they have added further hand coded concepts to opencyc 
in order to support cogsketch  the gizmo system requires the user to hand code a physical
model  using the methods of qualitative physics  yan   forbus         learning reader
uses more than        phrasal patterns  which were derived from researchcyc  forbus
et al          it is evident that sme still requires substantial hand coded knowledge 
the work we present in this paper is an effort to avoid complex hand coded representations  our approach is to combine ideas from sme  falkenhainer et al         and latent
relational analysis  lra   turney         we call the resulting algorithm the latent relation mapping engine  lrme   we represent the semantic relation between two terms
using a vector  in which the elements are derived from pattern frequencies in a large corpus
of raw text  because the semantic relations are automatically derived from a corpus  lrme
does not require hand coded representations of relations  it only needs a list of terms from
the source and a list of terms from the target  given these two lists  lrme uses the corpus
to build representations of the relations among the terms  and then it constructs a mapping
between the two lists 
tables   and   show the input and output of lrme for the analogy between the solar
system and the rutherford bohr model of the atom  although some human effort is involved
in constructing the input lists  it is considerably less effort than sme requires for its input
 contrast figures   and   with table    
scientific analogies  such as the analogy between the solar system and the rutherfordbohr model of the atom  may seem esoteric  but we believe analogy making is ubiquitous
in our daily lives  a potential practical application for this work is the task of identifying
semantic roles  gildea   jurafsky         since roles are relations  not attributes  it is
appropriate to treat semantic role labeling as an analogical mapping problem 
for example  the judgement semantic frame contains semantic roles such as judge 
evaluee  and reason  and the statement frame contains roles such as speaker  addressee  message  topic  and medium  gildea   jurafsky         the task of identifying
   

fiturney

source a
planet
attracts
revolves
sun
gravity
solar system
mass

target b
revolves
atom
attracts
electromagnetism
nucleus
charge
electron

table    the representation of the input in lrme 
source a
solar system
sun
planet
mass
attracts
revolves
gravity

mapping m








target b
atom
nucleus
electron
charge
attracts
revolves
electromagnetism

table    the representation of the output in lrme 
semantic roles is to automatically label sentences with their roles  as in the following examples  gildea   jurafsky        
  judge she  blames  evaluee the government   reason for failing to do enough to
help  
  speaker we  talked  topic about the proposal   medium over the phone  
if we have a training set of labeled sentences and a testing set of unlabeled sentences  then
we may view the task of labeling the testing sentences as a problem of creating analogical
mappings between the training sentences  sources  and the testing sentences  targets   table   shows how she blames the government for failing to do enough to help  might be
mapped to they blame the company for polluting the environment  once a mapping has
been found  we can transfer knowledge  in the form of semantic role labels  from the source
to the target 
source a
she
blames
government
failing
help

mapping m






target b
they
blame
company
polluting
environment

table    semantic role labeling as analogical mapping 
in section    we briefly discuss the hypotheses behind the design of lrme  we then
precisely define the task that is performed by lrme  a specific form of analogical mapping 
   

fithe latent relation mapping engine

in section    lrme builds on latent relational analysis  lra   hence we summarize lra
in section    we discuss potential applications of lrme in section   
to evaluate lrme  we created twenty analogical mapping problems  ten science analogy problems  holyoak   thagard        and ten common metaphor problems  lakoff  
johnson         table   is one of the science analogy problems  our intended solution is
given in table    to validate our intended solutions  we gave our colleagues the lists of
terms  as in table    and asked them to generate mappings between the lists  section  
presents the results of this experiment  across the twenty problems  the average agreement
with our intended solutions  as in table    was       
the lrme algorithm is outlined in section    along with its evaluation on the twenty
mapping problems  lrme achieves an accuracy of        the difference between this
performance and the human average of       is not statistically significant 
section   examines a variety of alternative approaches to the analogy mapping task  the
best approach achieves an accuracy of        but this approach requires hand coded partof speech tags  this performance is significantly below lrme and human performance 
in section    we discuss some questions that are raised by the results in the preceding
sections  related work is described in section     future work and limitations are considered
in section     and we conclude in section    

   guiding hypotheses
in this section  we list some of the assumptions that have guided the design of lrme  the
results we present in this paper do not necessarily require these assumptions  but it might
be helpful to the reader  to understand the reasoning behind our approach 
   analogies and semantic relations  analogies are based on semantic relations
 gentner         for example  the analogy between the solar system and the rutherford bohr model of the atom is based on the similarity of the semantic relations
among the concepts involved in our understanding of the solar system to the semantic
relations among the concepts involved in the rutherford bohr model of the atom 
   co occurrences and semantic relations  two terms have an interesting  significant semantic relation if and only if they they tend to co occur within a relatively
small window  e g   five words  in a relatively large corpus  e g        words   having
an interesting semantic relation causes co occurrence and co occurrence is a reliable
indicator of an interesting semantic relation  firth        
   meanings and semantic relations  meaning has more to do with relations among
words than individual words  individual words tend to be ambiguous and polysemous 
by putting two words into a pair  we constrain their possible meanings  by putting
words into a sentence  with multiple relations among the words in the sentence  we
constrain the possible meanings further  if we focus on word pairs  or tuples   instead
of individual words  word sense disambiguation is less problematic  perhaps a word
has no sense apart from its relations with other words  kilgarriff        
   pattern distributions and semantic relations  there is a many to many mapping between semantic relations and the patterns in which two terms co occur  for
example  the relation causeeffect x  y   may be expressed as x causes y   y
   

fiturney

from x  y due to x  y because of x  and so on  likewise  the pattern
y from x may be an expression of causeeffect x  y    sick from bacteria  or
originentity x  y    oranges from spain   however  for a given x and y   the statistical distribution of patterns in which x and y co occur is a reliable signature of
the semantic relations between x and y  turney        
to the extent that lrme works  we believe its success lends some support to these hypotheses 

   the task
in this paper  we examine algorithms that generate analogical mappings  for simplicity  we
restrict the task to generating bijective mappings  that is  mappings that are both injective
 one to one  there is no instance in which two terms in the source map to the same term
in the target  and surjective  onto  the source terms cover all of the target terms  there is
no target term that is left out of the mapping   we assume that the entities that are to be
mapped are given as input  formally  the input i for the algorithms is two sets of terms  a
and b 
i    ha  bi 

   

since the mappings are bijective  a and b must contain the same number of terms  m 
a    a    a            am  

   

b    b    b            bm  

   

a term  ai or bj   may consist of a single word  planet  or a compound of two or more words
 solar system   the words may be any part of speech  nouns  verbs  adjectives  or adverbs  
the output o is a bijective mapping m from a to b 
o    m   a  b 

   

m  ai    b

   

m  a     m  a     m  a             m  am      b

   

the algorithms that we consider here can accept a batch of multiple independent mapping
problems as input and generate a mapping for each one as output 
i    ha    b  i   ha    b  i           han   bn i 

   

o    m    a   b    m    a   b            mn   an  bn  

   

suppose the terms in a are in some arbitrary order a 
a   ha    a            am i
the mapping function m   a  b  given a  determines a unique ordering b of b 
   

   

fithe latent relation mapping engine

b   hm  a     m  a             m  am  i

    

likewise  an ordering b of b  given a  defines a unique mapping function m   since there
are m  possible orderings of b  there are also m  possible mappings from a to b  the task
is to search through the m  mappings and find the best one   section   shows that there is
a relatively high degree of consensus about which mappings are best  
let p  a  b  be the set of all m  bijective mappings from a to b   p stands for permutation  since each mapping corresponds to a permutation  
p  a  b     m    m            mm   

    

m    a     b 

    

m     p  a  b  

    

in the following experiments  m is   on average and   at most  so m  is usually around
            and at most                it is feasible for us to exhaustively search p  a  b  
we explore two basic kinds of algorithms for generating analogical mappings  algorithms
based on attributional similarity and algorithms based on relational similarity  turney 
       the attributional similarity between two words  sima  a  b      depends on the
degree of correspondence between the properties of a and b  the more correspondence
there is  the greater their attributional similarity  the relational similarity between two
pairs of words  simr  a   b  c   d      depends on the degree of correspondence between the
relations of a   b and c   d  the more correspondence there is  the greater their relational
similarity  for example  dog and wolf have a relatively high degree of attributional similarity 
whereas dog   bark and cat   meow have a relatively high degree of relational similarity 
attributional mapping algorithms seek the mapping  or mappings  ma that maximizes
the sum of the attributional similarities between the terms in a and the corresponding
terms in b   when there are multiple mappings that maximize the sum  we break the tie
by randomly choosing one of them  
ma   arg max

m
x

sima  ai   m  ai   

    

m p  a b  i  

relational mapping algorithms seek the mapping  or mappings  mr that maximizes the
sum of the relational similarities 
mr   arg max

m x
m
x

simr  ai   aj   m  ai     m  aj   

    

m p  a b  i   j i  

in       we assume that simr is symmetrical  for example  the degree of relational similarity
between dog   bark and cat   meow is the same as the degree of relational similarity between
bark   dog and meow   cat 
simr  a   b  c   d    simr  b   a  d   c 

    

we also assume that simr  a   a  b   b  is not interesting  for example  it may be some constant
value for all a and b  therefore      is designed so that i is always less than j 
   

fiturney

let scorer  m   and scorea  m   be defined as follows 

scorer  m    
scorea  m    

m x
m
x

simr  ai   aj   m  ai     m  aj   

i   j i  
m
x

sima  ai   m  ai   

    

    

i  

now mr and ma may be defined in terms of scorer  m   and scorea  m   
mr   arg max scorer  m  

    

m p  a b 

ma   arg max scorea  m  

    

m p  a b 

mr is the best mapping according to simr and ma is the best mapping according to sima  
recall gentners        terms  discussed in section    mere appearance  mostly attributional similarity   analogy  mostly relational similarity   and literal similarity  a mixture of
attributional and relational similarity   we take it that mr is an abstract model of mapping based on analogy and ma is a model of mere appearance  for literal similarity  we can
combine mr and ma   but we should take care to normalize scorer  m   and scorea  m   before
we combine them   we experiment with combining them in section      

   latent relational analysis
lrme uses a simplified form of latent relational analysis  lra   turney             
to calculate the relational similarity between pairs of words  we will briefly describe past
work with lra before we present lrme 
lra takes as input i a set of word pairs and generates as output o the relational
similarity simr  ai   bi   aj   bj   between any two pairs in the input 
i    a    b    a    b            an   bn  

    

o    simr   i  i    

    

lra was designed to evaluate proportional analogies  proportional analogies have the form
a   b    c   d  which means a is to b as c is to d  for example  mason   stone    carpenter   wood
means mason is to stone as carpenter is to wood  a mason is an artisan who works with
stone and a carpenter is an artisan who works with wood 
we consider proportional analogies to be a special case of bijective analogical mapping 
as defined in section    in which  a     b    m      for example  a    a     b    b  is equivalent
to m  in      
a    a    a      b    b    b      m   a      b    m   a      b   
from the definition of scorer  m   in       we have the following result for m   
   

    

fithe latent relation mapping engine

scorer  m      simr  a    a    m   a      m   a       simr  a    a    b    b   

    

that is  the quality of the proportional analogy mason   stone    carpenter   wood is given by
simr  mason   stone  carpenter   wood  
proportional analogies may also be evaluated using attributional similarity  from the
definition of scorea  m   in       we have the following result for m   
scorea  m      sima  a    m   a       sima  a    m   a       sima  a    b      sima  a    b   

    

for attributional similarity  the quality of the proportional analogy mason   stone    carpenter  
wood is given by sima  mason  carpenter    sima  stone  wood  
lra only handles proportional analogies  the main contribution of lrme is to extend
lra beyond proportional analogies to bijective analogies for which m     
turney        describes ten potential applications of lra  recognizing proportional
analogies  structure mapping theory  modeling metaphor  classifying semantic relations 
word sense disambiguation  information extraction  question answering  automatic thesaurus generation  information retrieval  and identifying semantic roles  two of these
applications  evaluating proportional analogies and classifying semantic relations  are experimentally evaluated  with state of the art results 
turney        compares the performance of relational similarity      and attributional
similarity      on the task of solving     multiple choice proportional analogy questions from
the sat college entrance test  lra is used to measure relational similarity and a variety
of lexicon based and corpus based algorithms are used to measure attributional similarity 
lra achieves an accuracy of     on the     sat questions  which is not significantly
different from the average human score of      on the other hand  the best performance
by attributional similarity is      the results show that attributional similarity is better
than random guessing  but not as good as relational similarity  this result is consistent
with gentners        theory of the maturation of human similarity judgments 
turney        also applies lra to the task of classifying semantic relations in nounmodifier expressions  a noun modifier expression is a phrase  such as laser printer  in which
the head noun  printer  is preceded by a modifier  laser   the task is to identify the semantic
relation between the noun and the modifier  in this case  the relation is instrument  the
laser is an instrument used by the printer  on a set of     hand labeled noun modifier pairs
with five different classes of semantic relations  lra attains     accuracy 
turney        employs a variation of lra for solving four different language tests 
achieving     accuracy on sat analogy questions      accuracy on toefl synonym
questions      accuracy on the task of distinguishing synonyms from antonyms  and    
accuracy on the task of distinguishing words that are similar  words that are associated 
and words that are both similar and associated  the same core algorithm is used for all
four tests  with no tuning of the parameters to the particular test 

   applications for lrme
since lrme is an extension of lra  every potential application of lra is also a potential
application of lrme  the advantage of lrme over lra is the ability to handle bijective
   

fiturney

analogies when m      where m    a     b    in this section  we consider the kinds of
applications that might benefit from this ability 
in section      we evaluate lrme on science analogies and common metaphors  which
supports the claim that these two applications benefit from the ability to handle larger sets
of terms  in section    we saw that identifying semantic roles  gildea   jurafsky       
also involves more than two terms  and we believe that lrme will be superior to lra for
semantic role labeling 
semantic relation classification usually assumes that the relations are binary  that is 
a semantic relation is a connection between two terms  rosario   hearst        nastase
  szpakowicz        turney        girju et al          yuret observed that binary relations may be linked by underlying n ary relations   for example  nastase and szpakowicz
       defined a taxonomy of    binary semantic relations  table   shows how six binary relations from nastase and szpakowicz        can be covered by one   ary relation 
agent tool action affected theme  an agent uses a tool to perform an action  somebody
or something is affected by the action  the whole event can be summarized by its theme 
nastase and szpakowicz       
relation
example
agent
student protest
purpose
concert hall
beneficiary
student discount
instrument
laser printer
object
metal separator
object property sunken ship

agent tool action affected theme
agent action
theme tool
affected action
tool agent
affected tool
action affected

table    how six binary semantic relations from nastase and szpakowicz        can be
viewed as different fragments of one   ary semantic relation 

in semeval task    we found it easier to manually tag the datasets when we expanded
binary relations to their underlying n ary relations  girju et al          we believe that this
expansion would also facilitate automatic classification of semantic relations  the results
in section     suggest that all of the applications for lra that we discussed in section  
might benefit from being able to handle bijective analogies when m     

   the mapping problems
to evaluate our algorithms for analogical mapping  we created twenty mapping problems 
given in appendix a  the twenty problems consist of ten science analogy problems  based
on examples of analogy in science from chapter   of holyoak and thagard         and ten
common metaphor problems  derived from lakoff and johnson        
the tables in appendix a show our intended mappings for each of the twenty problems  to validate these mappings  we invited our colleagues in the institute for information
technology to participate in an experiment  the experiment was hosted on a web server
   deniz yuret  personal communication  february           this observation was in the context of our
work on building the datasets for semeval      task    girju et al         

   

fithe latent relation mapping engine

 only accessible inside our institute  and people participated anonymously  using their web
browsers in their offices  there were    volunteers who began the experiment and    who
went all the way to the end  in our analysis  we use only the data from the    participants
who completed all of the mapping problems 
the instructions for the participants are in appendix a  the sequence of the problems
and the order of the terms within a problem were randomized separately for each participant 
to remove any effects due to order  table   shows the agreement between our intended
mapping and the mappings generated by the participants  across the twenty problems 
the average agreement was        which is higher than the agreement figures for many
linguistic annotation tasks  this agreement is impressive  given that the participants had
minimal instructions and no training 
type

science
analogies

common
metaphors

mapping
a 
a 
a 
a 
a 
a 
a 
a 
a 
a  
m 
m 
m 
m 
m 
m 
m 
m 
m 
m  

source  target
solar system  atom
water flow  heat transfer
waves  sounds
combustion  respiration
sound  light
projectile  planet
artificial selection  natural selection
billiard balls  gas molecules
computer  mind
slot machine  bacterial mutation
war  argument
buying an item  accepting a belief
grounds for a building  reasons for a theory
impediments to travel  difficulties
money  time
seeds  ideas
machine  mind
object  idea
following  understanding
seeing  understanding

average

agreement
    
    
    
    
    
    
    
    
    
    
    
    
    
     
    
    
    
    
    
    
    

m
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   

table    the average agreement between our intended mappings and the mappings of the
   participants  see appendix a for the details 

the column labeled m gives the number of terms in the set of source terms for each
mapping problem  which is equal to the number of terms in the set of target terms   for the
average problem  m      the third column in table   gives a mnemonic that summarizes
the mapping  e g   solar system  atom   note that the mnemonic is not used as input for
any of the algorithms  nor was the mnemonic shown to the participants in the experiment 
the agreement figures in table   for each individual mapping problem are averages over
the m mappings for each problem  appendix a gives a more detailed view  showing the
agreement for each individual mapping in the m mappings  the twenty problems contain
a total of     individual mappings          appendix a shows that every one of these    
   

fiturney

mappings has an agreement of     or higher  that is  in every case  the majority of the
participants agreed with our intended mapping   there are two cases where the agreement
is exactly      see problems a  in table    and m  in table    in appendix a  
if we select the mapping that is chosen by the majority of the    participants  then we
will get a perfect score on all twenty problems  more precisely  if we try all m  mappings for
each problem  and select the mapping that maximizes the sum of the number of participants
who agree with each individual mapping in the m mappings  then we will have a score of
     on all twenty problems  this is strong support for the intended mappings that are
given in appendix a 
in section    we applied genters        categories  mere appearance  mostly attributional similarity   analogy  mostly relational similarity   and literal similarity  a mixture
of attributional and relational similarity   to the mappings mr and ma   where mr is the
best mapping according to simr and ma is the best mapping according to sima   the twenty
mapping problems were chosen as analogy problems  that is  the intended mappings in
appendix a are meant to be relational mappings  mr   mappings that maximize relational
similarity  simr   we have tried to avoid mere appearance and literal similarity 
in section   we use the twenty mapping problems to evaluate a relational mapping
algorithm  lrme   and in section   we use them to evaluate several different attributional
mapping algorithms  our hypothesis is that lrme will perform significantly better than
any of the attributional mapping algorithms on the twenty mapping problems  because they
are analogy problems  not mere appearance problems and not literal similarity problems  
we expect relational and attributional mapping algorithms would perform approximately
equally well on literal similarity problems  and we expect that mere appearance problems
would favour attributional algorithms over relational algorithms  but we do not test these
latter two hypotheses  because our primary interest in this paper is analogy making 
our goal is to test the hypothesis that there is a real  practical  effective  measurable
difference between the output of lrme and the output of the various attributional mapping algorithms  a skeptic might claim that relational similarity simr  a   b  c   d  can be
reduced to attributional similarity sima  a  c    sima  b  d   therefore our relational mapping
algorithm is a complicated solution to an illusory problem  a slightly less skeptical claim
is that relational similarity versus attributional similarity is a valid distinction in cognitive
psychology  but our relational mapping algorithm does not capture this distinction  to test
our hypothesis and refute these skeptical claims  we have created twenty analogical mapping
problems  and we will show that lrme handles these problems significantly better than
the various attributional mapping algorithms 

   the latent relation mapping engine
the latent relation mapping engine  lrme  seeks the mapping mr that maximizes the
sum of the relational similarities 
mr   arg max

m x
m
x

simr  ai   aj   m  ai     m  aj   

    

m p  a b  i   j i  

we search for mr by exhaustively evaluating all of the possibilities  ties are broken randomly  we use a simplified form of lra  turney        to calculate simr  
   

fithe latent relation mapping engine

    algorithm
briefly  the idea of lrme is to build a pair pattern matrix x  in which the rows correspond
to pairs of terms and the columns correspond to patterns  for example  the row xi  might
correspond to the pair of terms sun   solar system and the column x j might correspond to
the pattern  x centered y   in these patterns   is a wild card  which can match
any single word  the value of an element xij in x is based on the frequency of the pattern
for x j   when x and y are instantiated by the terms in the pair for xi    for example  if we
take the pattern  x centered y  and instantiate x   y with the pair sun   solar system 
then we have the pattern  sun centered solar system   and thus the value of the element
xij is based on the frequency of  sun centered solar system  in the corpus  the matrix
x is smoothed with a truncated singular value decomposition  svd   golub   van loan 
      and the relational similarity simr between two pairs of terms is given by the cosine of
the angle between the two corresponding row vectors in x 
in more detail  lrme takes as input i a set of mapping problems and generates as
output o a corresponding set of mappings 

i    ha    b  i   ha    b  i           han   bn i 

    

o    m    a   b    m    a   b            mn   an  bn  

    

in the following experiments  all twenty mapping problems  appendix a  are processed in
one batch  n       
the first step is to make a list r that contains all pairs of terms in the input i  for
each mapping problem ha  bi in i  we add to r all pairs ai   aj   such that ai and aj are
members of a  i    j  and all pairs bi   bj   such that bi and bj are members of b  i    j 
if  a     b    m  then there are m m     pairs from a and m m     pairs from b   a
typical pair in r would be sun   solar system  we do not allow duplicates in r  r is a list
of pair types  not pair tokens  for our twenty mapping problems  r is a list of       pairs 
for each pair r in r  we make a list s r  of the phrases in the corpus that contain the
pair r  let ai   aj be the terms in the pair r  we search in the corpus for all phrases of the
following form 
   to   words  ai    to   words  aj    to   words 

    

if ai   aj is in r  then aj   ai is also in r  so we find phrases with the members of the pairs
in both orders  s ai   aj   and s aj   ai    the search template      is the same as used by
turney        
in the following experiments  we search in a corpus of         english words  about    
gb of plain text   consisting of web pages gathered by a web crawler   to retrieve phrases
   we have m m     here  not m m        because we need the pairs in both orders  we only want
to calculate simr for one order of the pairs  because i is always less than j in       however  to ensure
that simr is symmetrical  as in       we need to make the matrix x symmetrical  by having rows in the
matrix for both orders of every pair 
   the corpus was collected by charles clarke at the university of waterloo  we can provide copies of the
corpus on request 

   

fiturney

from the corpus  we use wumpus  buttcher   clarke         an efficient search engine for
passage retrieval from large corpora  
with the       pairs in r  we find a total of           phrases in the corpus  an average
of about       phrases per pair  for the pair r   sun   solar system  a typical phrase s in
s r  would be a sun centered solar system illustrates 
next we make a list c of patterns  based on the phrases we have found  for each pair
r in r  where r   ai   aj   if we found a phrase s in s r   then we replace ai in s with x
and we replace aj with y   the remaining words may be either left as they are or replaced
with a wild card symbol   we then replace ai in s with y and aj with x  and replace
the remaining words with wild cards or leave them as they are  if there are n remaining
words in s  after ai and aj are replaced  then we generate  n   patterns from s  and we add
these patterns to c  we only add new patterns to c  that is  c is a list of pattern types 
not pattern tokens  there are no duplicates in c 
for example  for the pair sun   solar system  we found the phrase a sun centered solar
system illustrates  when we replace ai   aj with x   y   we have a x centered y
illustrates  there are three remaining words  so we can generate eight patterns  such as
a x  y illustrates  a x centered y    x  y illustrates  and so on  each of these
patterns is added to c  then we replace ai   aj with y   x  yielding a y centered x
illustrates  this gives us another eight patterns  such as a y centered x   thus the
phrase a sun centered solar system illustrates generates a total of sixteen patterns  which
we add to c 
now we revise r  to make a list of pairs that will correspond to rows in the frequency
matrix f  we remove any pairs from r for which no phrases were found in the corpus 
when the terms were in either order  let ai   aj be the terms in the pair r  we remove
r from r if both s ai   aj   and s aj   ai   are empty  we remove such rows because they
would correspond to zero vectors in the matrix f  this reduces r from       pairs to      
pairs  let nr be the number of pairs in r 
next we revise c  to make a list of patterns that will correspond to columns in the
frequency matrix f  in the following experiments  at this stage  c contains millions of
patterns  too many for efficient processing with a standard desktop computer  we need to
reduce c to a more manageable size  we select the patterns that are shared by the most
pairs  let c be a pattern in c  let r be a pair in r  if there is a phrase s in s r   such
that there is a pattern generated from s that is identical to c  then we say that r is one of
the pairs that generated c  we sort the patterns in c in descending order of the number
of pairs in r that generated each pattern  and we select the top tnr patterns from this
sorted list  following turney         we set the parameter t to     hence c is reduced to
the top        patterns  tnr                        let nc be the number of patterns in
c  nc   tnr   
now that the rows r and columns c are defined  we can build the frequency matrix
f  let ri be the i th pair of terms in r  e g   let ri be sun   solar system  and let cj be
the j th pattern in c  e g   let cj be  x centered y    we instantiate x and y in the
pattern cj with the terms in ri   sun centered solar system    the element fij in f is
the frequency of this instantiated pattern in the corpus 
   wumpus was developed by stefan buttcher and it is available at http   www wumpus search org  

   

fithe latent relation mapping engine

note that we do not need to search again in the corpus for the instantiated pattern for
fij   in order to find its frequency  in the process of creating each pattern  we can keep track
of how many phrases generated the pattern  for each pair  we can get the frequency for fij
by checking our record of the patterns that were generated by ri  
the next step is to transform the matrix f of raw frequencies into a form x that
enhances the similarity measurement  turney        used the log entropy transformation 
as suggested by landauer and dumais         this is a kind of tf idf  term frequency
times inverse document frequency  transformation  which gives more weight to elements in
the matrix that are statistically surprising  however  bullinaria and levy        recently
achieved good results with a new transformation  called ppmic  positive pointwise mutual
information with cosine   therefore lrme uses ppmic  the raw frequencies in f are used
to calculate probabilities  from which we can calculate the pointwise mutual information
 pmi  of each element in the matrix  any element with a negative pmi is then set to zero 

fij
pij   pnr pnc

j   fij

i  

    

pnc

j   fij
pi   pnr pnc

    

pnr
f
pncij
  pnr i  

    

i  

pj

i  



j   fij
j   fij

pij
pi pj



pmiij   log

pmiij if pmiij    
xij  
  otherwise

    
    

let ri be the i th pair of terms in r  e g   let ri be sun   solar system  and let cj be the
j th pattern in c  e g   let cj be  x centered y    in       pij is the estimated probability
of the of the pattern cj instantiated with the pair ri   sun centered solar system    pi
is the estimated probability of ri   and pj is the estimated probability of cj   if ri and cj are
statistically independent  then pi pj   pij  by the definition of independence   and thus
pmiij is zero  since log          if there is an interesting semantic relation between the
terms in ri   and the pattern cj captures an aspect of that semantic relation  then we should
expect pij to be larger than it would be if ri and cj were indepedent  hence we should find
that pij   pi pj   and thus pmiij is positive   see hypothesis   in section     on the other
hand  terms from completely different domains may avoid each other  in which case we
should find that pmiij is negative  ppmic is designed to give a high value to xij when the
pattern cj captures an aspect of the semantic relation between the terms in ri   otherwise 
xij should have a value of zero  indicating that the pattern cj tells us nothing about the
semantic relation between the terms in ri  
in our experiments  f has a density of       the percentage of nonzero elements  and
x has a density of       the lower density of x is due to elements with a negative pmi 
which are transformed to zero by ppmic 
   

fiturney

now we smooth x by applying a truncated singular value decomposition  svd   golub
  van loan         we use svdlibc to calculate the svd of x   svdlibc is designed
for sparse  low density  matrices  svd decomposes x into the product of three matrices
uvt   where u and v are in column orthonormal form  i e   the columns are orthogonal
and have unit length  ut u   vt v   i  and  is a diagonal matrix of singular values
 golub   van loan         if x is of rank r  then  is also of rank r  let k   where
k   r  be the diagonal matrix formed from the top k singular values  and let uk and vk be
the matrices produced by selecting the corresponding columns from u and v  the matrix
uk k vkt is the matrix of rank k that best approximates the original matrix x  in the sense
that it minimizes the approximation errors  that is  x   uk k vkt minimizes kx  xkf
over all matrices x of rank k  where k       kf denotes the frobenius norm  golub   van
loan         we may think of this matrix uk k vkt as a smoothed or compressed version
of the original matrix x  following turney         we set the parameter k to     
the relational similarity simr between two pairs in r is the inner product of the two
corresponding rows in uk k vkt   after the rows have been normalized to unit length  we can
simplify calculations by dropping vk  deerwester  dumais  landauer  furnas    harshman 
       we take the matrix uk k and normalize each row to unit length  let w be the
resulting matrix  now let z be wwt   a square matrix of size nr nr   this matrix contains
the cosines of all combinations of two pairs in r 
for a mapping problem ha  bi in i  let a   a  be a pair of terms from a and let b   b  be
a pair of terms from b  suppose that ri   a   a  and rj   b   b    where ri and rj are the
i th and j th pairs in r  then simr  a   a    b   b      zij   where zij is the element in the i th
row and j th column of z  if either a   a  or b   b  is not in r  because s a   a     s a    a  
s b   b     or s b    b  is empty  then we set the similarity to zero  finally  for each mapping
problem in i  we output the map mr that maximizes the sum of the relational similarities 

mr   arg max

m x
m
x

simr  ai   aj   m  ai     m  aj   

    

m p  a b  i   j i  

the simplified form of lra used here to calculate simr differs from lra used by turney
       in several ways  in lrme  there is no use of synonyms to generate alternate forms of
the pairs of terms  in lrme  there is no morphological processing of the terms  lrme uses
ppmic  bullinaria   levy        to process the raw frequencies  instead of log entropy 
following turney         lrme uses a slightly different search template      and lrme
sets the number of columns nc to tnr   instead of using a constant  in section      we
evaluate the impact of two of these changes  ppmic and nc    but we have not tested
the other changes  which were mainly motivated by a desire for increased efficiency and
simplicity 
    experiments
we implemented lrme in perl  making external calls to wumpus for searching the corpus
and to svdlibc for calculating svd  we used the perl net  telnet package for interprocess
   svdlibc is the work of doug rohde and it is available at http   tedlab mit edu dr svdlibc  

   

fithe latent relation mapping engine

communication with wumpus  the pdl  perl data language  package for matrix manipulations  e g   calculating cosines   and the list  permutor package to generate permutations
 i e   to loop through p  a  b   
we ran the following experiments on a dual core amd opteron    computer  running
   bit linux  most of the running time is spent searching the corpus for phrases  it took
   hours and    minutes for wumpus to fetch the           phrases  the remaining steps
took    minutes  of which svd took    minutes  the running time could be cut in half by
using raid   to speed up disk access 
table   shows the performance of lrme in its baseline configuration  for comparison 
the agreement of the    volunteers with our intended mapping has been copied from table   
the difference between the performance of lrme         and the human participants
        is not statistically significant  paired t test      confidence level  

mapping
a 
a 
a 
a 
a 
a 
a 
a 
a 
a  
m 
m 
m 
m 
m 
m 
m 
m 
m 
m  
average

source  target
solar system  atom
water flow  heat transfer
waves  sounds
combustion  respiration
sound  light
projectile  planet
artificial selection  natural selection
billiard balls  gas molecules
computer  mind
slot machine  bacterial mutation
war  argument
buying an item  accepting a belief
grounds for a building  reasons for a theory
impediments to travel  difficulties
money  time
seeds  ideas
machine  mind
object  idea
following  understanding
seeing  understanding

accuracy
lrme humans
     
    
     
    
     
    
     
    
    
    
     
    
    
    
     
    
    
    
     
    
    
    
     
    
     
    
     
     
     
    
     
    
     
    
    
    
     
    
     
    
    
    

table    lrme in its baseline configuration  compared with human performance 
in table    the column labeled humans is the average of    people  whereas the lrme
column is only one algorithm  it is not an average   comparing an average of several scores
to an individual score  whether the individual is a human or an algorithm  may give a
misleading impression  in the results for any individual person  there are typically several
     scores and a few scores in the        range  the average mapping problem has seven
terms  it is not possible to have exactly one term mapped incorrectly  if there are any
incorrect mappings  then there must be two or more incorrect mappings  this follows from
the nature of bijections  therefore a score of             is not uncommon 
   

fiturney

table   looks at the results from another perspective  the column labeled lrme wrong
gives the number of incorrect mappings made by lrme for each of the twenty problems 
the five columns labeled number of people with n wrong show  for various values of n  
how may of the    people made n incorrect mappings  for the average mapping problem 
   out of    participants had a perfect score  n       of the remaining   participants   
made only two mistakes  n       table   shows more clearly than table   that lrmes
performance is not significantly different from  individual  human performance   for yet
another perspective  see section      

mapping
a 
a 
a 
a 
a 
a 
a 
a 
a 
a  
m 
m 
m 
m 
m 
m 
m 
m 
m 
m  
average

lrme
wrong
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

number of people with n wrong
n    n    n    n    n  
  
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
  
 
 
 
 
 
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
 
 
  
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 
  
 
 
 
 

m
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

table    another way of viewing lrme versus human performance 
in table    we examine the sensitivity of lrme to the parameter settings  the first row
shows the accuracy of the baseline configuration  as in table    the next eight rows show
the impact of varying k  the dimensionality of the truncated singular value decomposition 
from    to      the eight rows after that show the effect of varying t  the column factor 
from   to     the number of columns in the matrix  nc   is given by the number of rows  nr
         multiplied by t  the second last row shows the effect of eliminating the singular
value decomposition from lrme  this is equivalent to setting k to        the number
of rows in the matrix  the final row gives the result when ppmic  bullinaria   levy 
      is replaced with log entropy  turney         lrme is not sensitive to any of these
manipulations  none of the variations in table   perform significantly differently from the
baseline configuration  paired t test      confidence level    this does not necessarily mean
that the manipulations have no effect  rather  it suggests that a larger sample of problems
would be needed to show a significant effect  
   

fithe latent relation mapping engine

experiment
baseline configuration

varying k

varying t

dropping svd
log entropy

k
   
  
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
   

t
  
  
  
  
  
  
  
  
  
 
  
  
  
  
  
  
  
  
  

nc
      
      
      
      
      
      
      
      
      
     
      
      
      
      
      
      
      
      
      

accuracy
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    exploring the sensitivity of lrme to various parameter settings and modifications 

   attribute mapping approaches
in this section  we explore a variety of attribute mapping approaches for the twenty mapping
problems  all of these approaches seek the mapping ma that maximizes the sum of the
attributional similarities 
ma   arg max

m
x

sima  ai   m  ai   

    

m p  a b  i  

we search for ma by exhaustively evaluating all of the possibilities  ties are broken randomly  we use a variety of different algorithms to calculate sima  
    algorithms
in the following experiments  we test five lexicon based attributional similarity measures
that use wordnet   hso  hirst   st onge         jc  jiang   conrath         lc  leacock   chodrow         lin  lin         and res  resnik         all five are implemented
in the perl package wordnet  similarity   which builds on the wordnet  querydata  package  the core idea behind them is to treat wordnet as a graph and measure the semantic
distance between two terms by the length of the shortest path between them in the graph 
similarity increases as distance decreases 
   wordnet was developed by a team at princeton and it is available at http   wordnet princeton edu  
   ted pedersens wordnet  similarity package is at http   www d umn edu tpederse similarity html 
   jason rennies wordnet  querydata package is at http   people csail mit edu jrennie wordnet  

   

fiturney

hso works with nouns  verbs  adjectives  and adverbs  but jc  lc  lin  and res only
work with nouns and verbs  we used wordnet  similarity to try all possible parts of speech
and all possible senses for each input word  many adjectives  such as true and valuable 
also have noun and verb senses in wordnet  so jc  lc  lin  and res are still able to
calculate similarity for them  when the raw form of a word is not found in wordnet 
wordnet  similarity searches for morphological variations of the word  when there are
multiple similarity scores  for multiple parts of speech and multiple senses  we select the
highest similarity score  when there is no similarity score  because a word is not in wordnet 
or because jc  lc  lin  or res could not find an alternative noun or verb form for an
adjective or adverb  we set the score to zero 
we also evaluate two corpus based attributional similarity measures  pmi ir  turney 
      and lsa  landauer   dumais         the core idea behind them is that a word
is characterized by the company it keeps  firth         the similarity of two terms is
measured by the similarity of their statistical distributions in a corpus  we used the corpus
of section   along with wumpus to implement pmi ir  pointwise mutual information
with information retrieval   for lsa  latent semantic analysis   we used the online
demonstration    we selected the matrix comparison option with the general reading up
to  st year college      factors  topic space and the term to term comparison type  pmi ir
and lsa work with all parts of speech 
our eighth similarity measure is based on the observation that our intended mappings
map terms that have the same part of speech  see appendix a   let pos a  be the partof speech tag assigned to the term a  we use part of speech tags to define a measure of
attributional similarity  simpos  a  b   as follows 

     if a   b
   if pos a    pos b 
    
simpos  a  b   

  otherwise
we hand labeled the terms in the mapping problems with part of speech tags  santorini 
       automatic taggers assume that the words that are to be tagged are embedded in
a sentence  but the terms in our mapping problems are not in sentences  so their tags are
ambiguous  we used our knowledge of the intended mappings to manually disambiguate
the part of speech tags for the terms  thus guaranteeing that corresponding terms in the
intended mapping always have the same tags 
for each of the first seven attributional similarity measures above  we created seven more
similarity measures by combining them with simpos  a  b   for example  let simhso  a  b  be
the hirst and st onge        similarity measure  we combine simpos  a  b  and simhso  a  b 
by simply adding them 
simhso pos  a  b    simhso  a  b    simpos  a  b 

    

the values returned by simpos  a  b  range from   to      whereas the values returned by
simhso  a  b  are much smaller  we chose large values in      so that getting pos tags to
match up has more weight than any of the other similarity measures  the manual pos tags
    the online demonstration of lsa is the work of a team at the university of colorado at boulder  it is
available at http   lsa colorado edu  

   

fithe latent relation mapping engine

and the high weight of simpos  a  b  give an unfair advantage to the attributional mapping
approach  but the relational mapping approach can afford to be generous 
    experiments
table   presents the accuracy of the various measures of attributional similarity  the
best result without pos labels is        hso   the best result with pos labels is      
 lin pos   the       accuracy of lrme  see table    is significantly higher than the
      accuracy of lin pos  and thus  of course  significantly higher than everything else
in table    paired t test      confidence level   the average human performance of      
 see table    is also significantly higher than the       accuracy of lin pos  paired t test 
    confidence level   in summary  humans and lrme perform significantly better than
all of the variations of attributional mapping approaches that were tested 
algorithm
hso
jc
lc
lin
res
pmi ir
lsa
pos  hand labeled 
hso pos
jc pos
lc pos
lin pos
res pos
pmi ir pos
lsa pos

reference
hirst and st onge       
jiang and conrath       
leacock and chodrow       
lin       
resnik       
turney       
landauer and dumais       
santorini       
hirst and st onge       
jiang and conrath       
leacock and chodrow       
lin       
resnik       
turney       
landauer and dumais       

accuracy
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    the accuracy of attribute mapping approaches for a wide variety of measures of
attributional similarity 

   discussion
in this section  we examine three questions that are suggested by the preceding results 
is there a difference between the science analogy problems and the common metaphor
problems  is there an advantage to combining the relational and attributional mapping approaches  what is the advantage of the relational mapping approach over the attributional
mapping approach 
    science analogies versus common metaphors
table   suggests that science analogies may be more difficult than common metaphors  this
is supported by table     which shows how the agreement of the    participants with our
intended mapping  see section    varies between the science problems and the metaphor
   

fiturney

problems  the science problems have a lower average performance and greater variation in
performance  the difference between the science problems and the metaphor problems is
statistically significant  paired t test      confidence level  
participant
 
 
 
 
 
 
 
 
 
  
  
  
  
  
  
  
  
  
  
  
  
  
average
standard deviation

all   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   

average accuracy
   science    metaphor
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   

table     a comparison of the difficulty of the science problems versus the metaphor problems for the    participants  the numbers in bold font are the scores that are
above the scores of lrme 
the average science problem has more terms       than the average metaphor problem
       which might contribute to the difficulty of the science problems  however  table   
shows that there is no clear relation between the number of terms in a problem  m in
table    and the level of agreement  we believe that people find the metaphor problems
easier than the science problems because these common metaphors are entrenched in our
language  whereas the science analogies are more peripheral 
table    shows that the    algorithms studied here perform slightly worse on the science
problems than on the metaphor problems  but the difference is not statistically significant
 paired t test      confidence level   we hypothesize that the attributional mapping approaches are not performing well enough to be sensitive to subtle differences between science
analogies and common metaphors 
incidentally  these tables give us another view of the performance of lrme in comparison to human performance  the first row in table    shows the performance of lrme on
   

fithe latent relation mapping engine

num terms
 
 
 
 
 

agreement
    
    
    
    
    

table     the average agreement among the    participants as a function of the number of
terms in the problems 

algorithm
lrme
hso
jc
lc
lin
res
pmi ir
lsa
pos
hso pos
jc pos
lc pos
lin pos
res pos
pmi ir pos
lsa pos
average
standard deviation

all   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

average accuracy
   science    metaphor
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table     a comparison of the difficulty of the science problems versus the metaphor problems for the    algorithms 

the science and metaphor problems  in table     we have marked in bold font the cases
where human scores are greater than lrmes scores  for all    problems  there are  
such cases  for the    science problems  there are   such cases  for the    metaphor problems  there are    such cases  this is further evidence that lrmes performance is not
significantly different from human performance  lrme is near the middle of the range of
performance of the    human participants 
    hybrid relational attributional approaches
recall the definitions of scorer  m   and scorea  m   given in section   
   

fiturney

scorer  m    
scorea  m    

m x
m
x

simr  ai   aj   m  ai     m  aj   

i   j i  
m
x

sima  ai   m  ai   

    

    

i  

we can combine the scores by simply adding them or multiplying them  but scorer  m   and
scorea  m   may be quite different in the scales and distributions of their values  therefore
we first normalize them to probabilities 
scorer  m  
mi p  a b  scorer  mi  

    

scorea  m  
mi p  a b  scorea  mi  

    

probr  m     p
proba  m     p

for these probability estimates  we assume that scorer  m      and scorea  m       if
necessary  a constant value may be added to the scores  to ensure that they are not negative 
now we can combine the scores by adding or multiplying the probabilities 


mr a   arg max probr  m     proba  m  

    

m p  a b 



mra   arg max probr  m    proba  m  

    

m p  a b 

table    shows the accuracy when lrme is combined with lin pos  the best attributional mapping algorithm in table    with an accuracy of        or with hso  the best
attributional mapping algorithm that does not use the manual pos tags  with an accuracy
of         we try both adding and multiplying probabilities  on its own  lrme has an
accuracy of        combining lrme with lin pos increases the accuracy to        but
this improvement is not statistically significant  paired t test      confidence level   combining lrme with hso results in a decrease in accuracy  the decrease is not significant
when the probabilities are multiplied          but it is significant when the probabilities
are added         
in summary  the experiments show no significant advantage to combining lrme with
attributional mapping  however  it is possible that a larger sample of problems would
show a significant advantage  also  the combination methods we explored  addition and
multiplication of probabilities  are elementary  a more sophisticated approach  such as a
weighted combination  may perform better 
    coherent relations
we hypothesize that lrme benefits from a kind of coherence among the relations  on the
other hand  attributional mapping approaches do not involve this kind of coherence 
   

fithe latent relation mapping engine

components
relational attributional
lrme
lin pos
lrme
lin pos
lrme
hso
lrme
hso

combination
add probabilities
multiply probabilities
add probabilities
multiply probabilities

accuracy
    
    
    
    

table     the performance of four different hybrids of relational and attributional mapping
approaches 

suppose we swap two of the terms in a mapping  let m be the original mapping and
let m   be the new mapping  where m    a      m  a     m    a      m  a     and m    ai     m  ai  
for i      with attributional similarity  the impact of this swap on the score of the mapping
is limited  part of the score is not affected 

scorea  m     sima  a    m  a       sima  a    m  a      

m
x

sima  ai   m  ai   

    

sima  ai   m  ai   

    

i  

scorea  m       sima  a    m  a       sima  a    m  a      

m
x
i  

on the other hand  with relational similarity  the impact of a swap is not limited in this
way  a change to any part of the mapping affects the whole score  there is a kind of global
coherence to relational similarity that is lacking in attributional similarity 
testing the hypothesis that lrme benefits from coherence is somewhat complicated 
because we need to design the experiment so that the coherence effect is isolated from any
other effects  to do this  we move some of the terms outside of the accuracy calculation 
let m   a  b be one of our twenty mapping problems  where m is our intended
mapping and m    a     b   let a  be a randomly selected subset of a of size m    let b  
be m  a     the subset of b to which m maps a   

a   a

    

 

b b
 

    
 

b   m  a  
   
m    a     b   
 

m  m

    
    
    

there are two ways that we might use lrme to generate a mapping m     a   b   for this
new reduced mapping problem  internal coherence and total coherence 
   internal coherence  we can select m   based on ha    b   i alone 
   

fiturney

a     a         am   

    

 

b    b         bm   

    
m 

m     arg max

m 

x x

m p  a   b     i   j i  

simr  ai   aj   m  ai     m  aj   

    

in this case  m   is chosen based only on the relations that are internal to ha    b   i 
   total coherence  we can select m   based on ha  bi and the knowledge that m  
must satisfy the constraint that m    a      b      this knowledge is also embedded in
internal coherence  

a    a         am  

    

b    b         bm  


p  a  b    m   m  p  a  b  and m  a      b  
m x
m
x
 
m   arg max
simr  ai   aj   m  ai     m  aj   
 

    
    
    

m p    a b  i   j i  

in this case  m   is chosen using both the relations that are internal to ha    b   i and
other relations in ha  bi that are external to ha    b   i 

suppose that we calculate the accuracy of these two methods based only on the subproblem ha    b   i  at first it might seem that there is no advantage to total coherence 
because it must explore a larger space of possible mappings than internal coherence  since
 p    a  b   is larger than  p  a    b        but the additional terms that it explores are not
involved in calculating the accuracy  however  we hypothesize that total coherence will
have a higher accuracy than internal coherence  because the additional external relations
help to select the correct mapping 
to test this hypothesis  we set m  to   and we randomly generated ten new reduced
mapping problems for each of the twenty problems  i e   a total of     new problems of size
    the average accuracy of internal coherence was        whereas the average accuracy
of total coherence was        the difference is statistically significant  paired t test     
confidence level  
on the other hand  the attributional mapping approaches cannot benefit from total
coherence  because there is no connection between the attributes that are in ha    b   i and
the attributes that are outside  we can decompose scorea  m   into two independent parts 
   

fithe latent relation mapping engine

a     a   a 
 

    
  

a a a


p  a  b    m   m  p  a  b  and m  a      b  
x
m     arg max
sima  ai   m  ai   

    

 

    
    

m p    a b  a a
i


  arg max 
m p    a b 


x

ai

sima  ai   m  ai     

a 

x
ai

sima  ai   m  ai   

    

a  

these two parts can be optimized independently  thus the terms that are external to
ha    b   i have no influence on the part of m   that covers ha    b   i 
relational mapping cannot be decomposed into independent parts in this way  because
the relations connect the parts  this gives relational mapping approaches an inherent
advantage over attributional mapping approaches 
to confirm this analysis  we compared internal and total coherence using lin pos
on the same     new problems of size    the average accuracy of internal coherence was
       whereas the average accuracy of total coherence was        the difference is not
statistically significant  paired t test      confidence level    the only reason that there is
any difference is that  when two mappings have the same score  we break the ties randomly 
this causes random variation in the accuracy  
the benefit from coherence suggests that we can make analogy mapping problems easier
for lrme by adding more terms  the difficulty is that the new terms cannot be randomly
chosen  they must fit with the logic of the analogy and not overlap with the existing terms 
of course  this is not the only important difference between the relational and attributional mapping approaches  we believe that the most important difference is that relations
are more reliable and more general than attributes  when using past experiences to make
predictions about the future  hofstadter        gentner         unfortunately  this hypothesis is more difficult to evaluate experimentally than our hypothesis about coherence 

    related work
french        gives a good survey of computational approaches to analogy making  from the
perspective of cognitive science  where the emphasis is on how well computational systems
model human performance  rather than how well the systems perform   we will sample a
few systems from his survey and add a few more that were not mentioned 
french        categorizes analogy making systems as symbolic  connectionist  or symbolicconnectionist hybrids  gardenfors        proposes another category of representational
systems for ai and cognitive science  which he calls conceptual spaces  these spatial or geometric systems are common in information retrieval and machine learning  widdows       
van rijsbergen         an influential example is latent semantic analysis  landauer  
dumais         the first spatial approaches to analogy making began to appear around the
same time as frenchs        survey  lrme takes a spatial approach to analogy making 
   

fiturney

     symbolic approaches
computational approaches to analogy making date back to analogy  evans        and
argus  reitman         both of these systems were designed to solve proportional analogies
 analogies in which  a     b       see section     analogy could solve proportional
analogies with simple geometric figures and argus could solve simple word analogies  these
systems used hand coded rules and were only able to solve the limited range of problems
that their designers had anticipated and coded in the rules 
french        cites structure mapping theory  smt   gentner        and the structure
mapping engine  sme   falkenhainer et al         as the prime examples of symbolic
approaches 
smt is unquestionably the most influential work to date on the modeling of
analogy making and has been applied in a wide range of contexts ranging from
child development to folk physics  smt explicitly shifts the emphasis in analogymaking to the structural similarity between the source and target domains  two
major principles underlie smt 
 the relation matching principle  good analogies are determined by mappings of relations and not attributes  originally only identical predicates
were mapped  and
 the systematicity principle  mappings of coherent systems of relations are
preferred over mappings of individual relations 
this structural approach was intended to produce a domain independent mapping process 
lrme follows both of these principles  lrme uses only relational similarity  no attributional similarity is involved  see section       coherent systems of relations are preferred
over mappings of individual relations  see section       however  the spatial  statistical 
corpus based  approach of lrme is quite different from the symbolic  logical  hand coded 
approach of sme 
martin        uses a symbolic approach to handle conventional metaphors  gentner 
bowdle  wolff  and boronat        argue that novel metaphors are processed as analogies 
but conventional metaphors are recalled from memory without special processing  however 
the line between conventional and novel metaphor can be unclear 
dolan        describes an algorithm that can extract conventional metaphors from a
dictionary  a semantic parser is used to extract semantic relations from the longman
dictionary of contemporary english  ldoce   a symbolic algorithm finds metaphorical
relations between words  using the extracted relations 
veale              has developed a symbolic approach to analogy making  using wordnet as a lexical resource  using a spreading activation algorithm  he achieved a score of
      on a set of     multiple choice lexical proportional analogy questions from the sat
college entrance test  veale        
lepage        has demonstrated that a symbolic approach to proportional analogies can
be used for morphology processing  lepage and denoual        apply a similar approach
to machine translation 
   

fithe latent relation mapping engine

     connectionist approaches
connectionist approaches to analogy making include acme  holyoak   thagard       
and lisa  hummel   holyoak         like symbolic approaches  these systems use handcoded knowledge representations  but the search for mappings takes a connectionist approach  in which there are nodes with weights that are incrementally updated over time 
until the system reaches a stable state 
     symbolic connectionist hybrid approaches
the third family examined by french        is hybrid approaches  containing elements
of both the symbolic and connectionist approaches  examples include copycat  mitchell 
      and tabletop  french         much of the work in the fluid analogies research
group  farg  concerns symbolic connectionist hybrids  hofstadter   farg        
     spatial approaches
marx  dagan  buhmann  and shamir        present the coupled clustering algorithm  which
uses a feature vector representation to find analogies in collections of text  for example 
given documents on buddhism and christianity  it finds related terms  such as  school 
mahayana  zen  for buddhism and  tradition  catholic  protestant  for christianity 
mason        describes the cormet system for extracting conventional metaphors from
text  cormet is based on clustering feature vectors that represent the selectional preferences
of verbs  given keywords for the source domain laboratory and the target domain finance 
it is able to discover mappings such as liquid  income and container  institution 
turney  littman  bigham  and shnayder        present a system for solving lexical
proportional analogy questions from the sat college entrance test  which combines thirteen
different modules  twelve of the modules use either attributional similarity or a symbolic
approach to relational similarity  but one module uses a spatial  feature vector  approach
to measuring relational similarity  this module worked much better than any of the other
modules  therefore  it was studied in more detail by turney and littman         the
relation between a pair of words is represented by a vector  in which the elements are pattern
frequencies  this is similar to lrme  but one important difference is that turney and
littman        used a fixed  hand coded set of     patterns  whereas lrme automatically
generates a variable number of patterns from the given corpus         patterns in our
experiments here  
turney        introduced latent relational analysis  lra   which was examined more
thoroughly by turney         lra achieves human level performance on a set of    
multiple choice proportional analogy questions from the sat college entrance exam  lrme
uses a simplified form of lra  a similar simplification of lra is used by turney         in
a system for processing analogies  synonyms  antonyms  and associations  the contribution
of lrme is to go beyond proportional analogies  to larger systems of analogical mappings 
     general theories of analogy and metaphor
many theories of analogy making and metaphor either do not involve computation or they
suggest general principles and concepts that are not specific to any particular computational
   

fiturney

approach  the design of lrme has been influenced by several theories of this type  gentner 
      hofstadter   farg        holyoak   thagard        hofstadter        gentner 
      
lakoff and johnson        provide extensive evidence that metaphor is ubiquitous in
language and thought  we believe that a system for analogy making should be able to
handle metaphorical language  which is why ten of our analogy problems are derived from
lakoff and johnson         we agree with their claim that a metaphor does not merely
involve a superficial relation between a couple of words  rather  it involves a systematic set
of mappings between two domains  thus our analogy problems involve larger sets of words 
beyond proportional analogies 
holyoak and thagard        argue that analogy making is central in our daily thought 
and especially in finding creative solutions to new problems  our ten scientific analogies
were derived from their examples of analogy making in scientific creativity 

    limitations and future work
in section    we mentioned ten applications for lra  and in section   we claimed that the
results of the experiments in section     suggest that lrme may perform better than lra
on all ten of these applications  due to its ability to handle bijective analogies when m     
our focus in future work will be testing this hypothesis  in particular  the task of semantic
role labeling  discussed in section    seems to be a good candidate application for lrme 
the input to lrme is simpler than the input to sme  compare figures   and   in
section   with table     but there is still some human effort involved in creating the input 
lrme is not immune to the criticism of chalmers  french  and hofstadter         that
the human who generates the input is doing more work than the computer that makes the
mappings  although it is not a trivial matter to find the right mapping out of           
choices 
in future work  we would like to relax the requirement that ha  bi must be a bijection
 see section     by adding irrelevant words  distractors  and synonyms  the mapping
algorithm will be forced to decide what terms to include in the mapping and what terms
to leave out 
we would also like to develop an algorithm that can take a proportional analogy  m     
as input  e g   sun planet  nucleus electron  and automatically expand it to a larger analogy
 m      e g   table     that is  it would automatically search the corpus for new terms to
add to the analogy 
the next step would be to give the computer only the topic of the source domain  e g  
solar system  and the topic of the target domain  e g   atomic structure   and let it work
out the rest on its own  this might be possible by combining ideas from lrme with ideas
from coupled clustering  marx et al         and cormet  mason        
it seems that analogy making is triggered in people when we encounter a problem
 holyoak   thagard         the problem defines the target for us  and we immediately
start searching for a source  analogical mapping enables us to transfer our knowledge of
the source to the target  hopefully leading to a solution to the problem  this suggests that
the input to the ideal analogical mapping algorithm would be simply a statement that there
   

fithe latent relation mapping engine

is a problem  e g   what is the structure of the atom    ultimately  the computer might
find the problems on its own as well  the only input would be a large corpus 
the algorithms we have considered here all perform exhaustive search of the set of
possible mappings p  a  b   this is acceptable when the sets are small  as they are here 
but it will be problematic for larger problems  in future work  it will be necessary to use
heuristic search algorithms instead of exhaustive search 
it takes almost    hours for lrme to process the twenty mapping problems  section    
with better hardware and some changes to the software  this time could be significantly
reduced  for even greater speed  the algorithm could run continuously  building a large
database of vector representations of term pairs  so that it is ready to create mappings as
soon as a user requests them  this is similar to the vision of banko and etzioni        
lrme  like lra and lsa  landauer   dumais         uses a truncated singular value
decomposition  svd  to smooth the matrix  many other algorithms have been proposed
for smoothing matrices  in our past work with lra  turney         we experimented with
nonnegative matrix factorization  nmf   lee   seung         probabilistic latent semantic analysis  plsa   hofmann         iterative scaling  is   ando         and kernel
principal components analysis  kpca   scholkopf  smola    muller         we had some
interesting results with small matrices  around              but none of the algorithms
seemed substantially better than truncated svd  and none of them scaled up to the matrix
sizes that we have here                  however  we believe that svd is not unique  and
future work is likely to discover a smoothing algorithm that is more efficient and effective
than svd  the results in section     do not show a significant benefit from svd  table  
hints that ppmic  bullinaria   levy        is more important than svd 
lrme extracts knowledge from many fragments of text  in section      we noted
that we found an average of       phrases per pair  the information from these      
phrases is combined in a vector  to represent the semantic relation for a pair  this is
quite different from relation extraction in  for example  the automatic content extraction
 ace  evaluation    the task in ace is to identify and label a semantic relation in a single
sentence  semantic role labeling also involves labeling a single sentence  gildea   jurafsky 
      
the contrast between lrme and ace is analogous to the distinction in cognitive
psychology between semantic and episodic memory  episodic memory is memory of a
specific event in ones personal past  whereas semantic memory is memory of basic facts and
concepts  unrelated to any specific event in the past  lrme extracts relational information
that is independent of any specific sentence  like semantic memory  ace is concerned with
extracting the relation in a specific sentence  like episodic memory  in cognition  episodic
memory and semantic memory work together synergistically  when we experience an event 
we use our semantic memory to interpret the event and form a new episodic memory 
but semantic memory is itself constructed from our past experiences  our accumulated
episodic memories  this suggests that there should be a synergy from combining lrme like
semantic information extraction algorithms with ace like episodic information extraction
algorithms 
    ace is an annual event that began in       relation detection and characterization  rdc  was
introduced to ace in       for more information  see http   www nist gov speech tests ace  

   

fiturney

    conclusion
analogy is the core of cognition  we understand the present by analogy to the past  we
predict the future by analogy to the past and the present  we solve problems by searching
for analogous situations  holyoak   thagard         our daily language is saturated with
metaphor  lakoff   johnson         and metaphor is based on analogy  gentner et al  
       to understand human language  to solve human problems  to work with humans 
computers must be able to make analogical mappings 
our best theory of analogy making is structure mapping theory  gentner         but
the structure mapping engine  falkenhainer et al         puts too much of the burden
of analogy making on its human users  chalmers et al          lrme is an attempt to
shift some of that burden onto the computer  while remaining consistent with the general
principles of smt 
we have shown that lrme is able to solve bijective analogical mapping problems with
human level performance  attributional mapping algorithms  at least  those we have tried
so far  are not able to reach this level  this supports smt  which claims that relations are
more important than attributes when making analogical mappings 
there is still much research to be done  lrme takes some of the load off the human
user  but formulating the input to lrme is not easy  this paper is an incremental step
towards a future in which computers can make surprising and useful analogies with minimal
human assistance 

acknowledgments
thanks to my colleagues at the institute for information technology for participating in
the experiment in section    thanks to charles clarke and egidio terra for their corpus 
thanks to stefan buttcher for making wumpus available and giving me advice on its use 
thanks to doug rohde for making svdlibc available  thanks to the wordnet team at
princeton university for wordnet  ted pedersen for the wordnet  similarity perl package 
and jason rennie for the wordnet  querydata perl package  thanks to the lsa team
at the university of colorado at boulder for the use of their online demonstration of lsa 
thanks to deniz yuret  andre vellino  dedre gentner  vivi nastase  yves lepage  diarmuid
o seaghdha  roxana girju  chris drummond  howard johnson  stan szpakowicz  and the
anonymous reviewers of jair for their helpful comments and suggestions 

appendix a  details of the mapping problems
in this appendix  we provide detailed information about the twenty mapping problems 
figure   shows the instructions that were given to the participants in the experiment in
section    these instructions were displayed in their web browsers  tables            
and    show the twenty mapping problems  the first column gives the problem number
 e g   a   and a mnemonic that summarizes the mapping  e g   solar system  atom   the
second column gives the source terms and the third column gives the target terms 
the mappings shown in these tables are our intended mappings  the fourth column
shows the percentage of participants who agreed with our intended mappings  for example 
   

fithe latent relation mapping engine

systematic analogies and metaphors
instructions
you will be presented with twenty analogical mapping problems  ten based on scientific
analogies and ten based on common metaphors  a typical problem will look like this 
horse
legs
hay
brain
dung













 
 
 
 
 

you may click on the drop down menus above  to see what options are available 
your task is to construct an analogical mapping  that is  a one to one mapping between the
items on the left and the items on the right  for example 
horse
legs
hay

 car
 wheels
 gasoline





brain
dung

 driver
 exhaust




this mapping expresses an analogy between a horse and a car  the horses legs are like the
cars wheels  the horse eats hay and the car consumes gasoline  the horses brain controls
the movement of the horse like the cars driver controls the movement of the car  the horse
generates dung as a waste product like the car generates exhaust as a waste product 
you should have no duplicate items in your answers on the right hand side  if there are
any duplicates or missing items  question marks   you will get an error message when you
submit your answer 
you are welcome to use a dictionary as you work on the problems  if you would find it
helpful 
if you find the above instructions unclear  then please do not continue with this exercise 
your answers to the twenty problems will be used as a standard for evaluating the output
of a computer algorithm  therefore  you should only proceed if you are confident that you
understand this task 
figure    the instructions for the participants in the experiment in section   

   

fiturney

mapping
a 
solar system
 atom

a 
water flow
 heat transfer

a 
waves
 sounds

a 
combustion
 respiration

a 
sound
 light

source
solar system
sun
planet
mass
attracts
revolves
gravity
average agreement 
water
flows
pressure
water tower
bucket
filling
emptying
hydrodynamics
average agreement 
waves
shore
reflects
water
breakwater
rough
calm
crashing
average agreement 
combustion
fire
fuel
burning
hot
intense
oxygen
carbon dioxide
average agreement 
sound
low
high
echoes
loud
quiet
horn
average agreement 










target
atom
nucleus
electron
charge
attracts
revolves
electromagnetism










heat
transfers
temperature
burner
kettle
heating
cooling
thermodynamics










sounds
wall
echoes
air
insulation
loud
quiet
vibrating










respiration
animal
food
breathing
living
vigorous
oxygen
carbon dioxide









light
red
violet
reflects
bright
dim
lens

agreement
    
     
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     
    
    
    
    
    
    
    
    
    
    
    
    
    
    
     
    
    
    
    

pos
nn
nn
nn
nn
vbz
vbz
nn
nn
vbz
nn
nn
nn
vbg
vbg
nn
nns
nn
vbz
nn
nn
jj
jj
vbg
nn
nn
nn
vbg
jj
jj
nn
nn
nn
jj
jj
vbz
jj
jj
nn

table     science analogy problems a  to a   derived from chapter   of holyoak and
thagard        

   

fithe latent relation mapping engine

mapping
a 
projectile
 planet

a 
artificial selection
 natural selection

a 
billiard balls
 gas molecules

a 
computer
 mind

a  
slot machine
 bacterial mutation

source
projectile
trajectory
earth
parabolic
air
gravity
attracts
average agreement 
breeds
selection
conformance
artificial
popularity
breeding
domesticated
average agreement 
balls
billiards
speed
table
bouncing
moving
slow
fast
average agreement 
computer
processing
erasing
write
read
memory
outputs
inputs
bug
average agreement 
slot machines
reels
spinning
winning
losing
average agreement 










target
planet
orbit
sun
elliptical
space
gravity
attracts









species
competition
adaptation
natural
fitness
mating
wild










molecules
gas
temperature
container
pressing
moving
cold
hot











mind
thinking
forgetting
memorize
remember
memory
muscles
senses
mistake







bacteria
genes
mutating
reproducing
dying

agreement
     
     
     
     
     
    
    
    
     
    
    
    
    
    
    
    
    
    
    
    
    
    
     
     
    
    
    
     
    
    
    
    
    
     
    
    
    
    
    
     
    

pos
nn
nn
nn
jj
nn
nn
vbz
nns
nn
nn
jj
nn
vbg
jj
nns
nn
nn
nn
vbg
vbg
jj
jj
nn
vbg
vbg
vb
vb
nn
nns
nns
nn
nns
nns
vbg
vbg
vbg

table     science analogy problems a  to a    derived from chapter   of holyoak and
thagard        

   

fiturney

mapping
m 
war
 argument

m 
buying an item
 accepting a belief

m 
grounds for a building
 reasons for a theory

m 
impediments to travel
 difficulties

m 
money
 time

source
war
soldier
destroy
fighting
defeat
attacks
weapon
average agreement 
buyer
merchandise
buying
selling
returning
valuable
worthless
average agreement 
foundations
buildings
supporting
solid
weak
crack
average agreement 
obstructions
destination
route
traveller
travelling
companion
arriving
average agreement 
money
allocate
budget
effective
cheap
expensive
average agreement 










target
argument
debater
refute
arguing
acceptance
criticizes
logic









believer
belief
accepting
advocating
rejecting
true
false








reasons
theories
confirming
rational
dubious
flaw









difficulties
goal
plan
person
problem solving
partner
succeeding








time
invest
schedule
efficient
quick
slow

agreement
    
     
    
    
    
    
    
    
     
    
    
     
    
    
    
    
    
    
    
    
    
    
    
     
     
     
     
     
     
     
     
    
    
    
    
    
    
    

pos
nn
nn
vb
vbg
nn
vbz
nn
nn
nn
vbg
vbg
vbg
jj
jj
nns
nns
vbg
jj
jj
nn
nns
nn
nn
nn
vbg
nn
vbg
nn
vb
nn
jj
jj
jj

table     common metaphor problems m  to m   derived from lakoff and johnson        

   

fithe latent relation mapping engine

mapping
m 
seeds
 ideas

m 
machine
 mind

m 
object
 idea

m 
following
 understanding

m  
seeing
 understanding

source
seeds
planted
fruitful
fruit
grow
wither
blossom
average agreement 
machine
working
turned on
turned off
broken
power
repair
average agreement 
object
hold
weigh
heavy
light
average agreement 
follow
leader
path
follower
lost
wanders
twisted
straight
average agreement 
seeing
light
illuminating
darkness
view
hidden
average agreement 










target
ideas
inspired
productive
product
develop
fail
succeed









mind
thinking
awake
asleep
confused
intelligence
therapy







idea
understand
analyze
important
trivial










understand
speaker
argument
listener
misunderstood
digresses
complicated
simple








understanding
knowledge
explaining
confusion
interpretation
secret

agreement
    
    
    
    
    
     
    
    
    
     
     
     
     
    
     
    
    
    
    
    
    
    
     
     
     
     
    
    
    
     
    
    
    
    
    
    
    
    

pos
nns
vbd
jj
nn
vb
vb
vb
nn
vbg
jj
jj
jj
nn
nn
nn
vb
vb
jj
jj
vb
nn
nn
nn
jj
vbz
jj
jj
vbg
nn
vbg
nn
nn
jj

table     common metaphor problems m  to m    derived from lakoff and johnson
       

   

fiturney

in problem a         of the participants     out of     mapped gravity to electromagnetism 
the final column gives the part of speech  pos  tags for the source and target terms  we
used the penn treebank tags  santorini         we assigned these tags manually  our
intended mappings and our tags were chosen so that mapped terms have the same tags 
for example  in a   sun maps to nucleus  and both sun and nucleus are tagged nn  the
pos tags are used in the experiments in section    the pos tags are not used by lrme
and they were not shown to the participants in the experiment in section   

references
ando  r  k          latent semantic space  iterative scaling improves precision of interdocument similarity measurement  in proceedings of the   rd annual acm sigir
conference on research and development in information retrieval  sigir        pp 
       
banko  m     etzioni  o          strategies for lifelong knowledge extraction from the web 
in proceedings of the  th international conference on knowledge capture  k cap
       pp        
bullinaria  j     levy  j          extracting semantic representations from word cooccurrence statistics  a computational study  behavior research methods         
       
buttcher  s     clarke  c          efficiency vs  effectiveness in terabyte scale information retrieval  in proceedings of the   th text retrieval conference  trec       
gaithersburg  md 
chalmers  d  j   french  r  m     hofstadter  d  r          high level perception  representation  and analogy  a critique of artificial intelligence methodology  journal of
experimental   theoretical artificial intelligence                
deerwester  s  c   dumais  s  t   landauer  t  k   furnas  g  w     harshman  r  a 
        indexing by latent semantic analysis  journal of the american society for
information science  jasis                  
dolan  w  b          metaphor as an emergent property of machine readable dictionaries  in proceedings of the aaai      spring symposium series  representation and
acquisition of lexical knowledge  polysemy  ambiguity and generativity  pp       
evans  t          a heuristic program to solve geometric analogy problems  in proceedings
of the spring joint computer conference  pp         
falkenhainer  b   forbus  k  d     gentner  d          the structure mapping engine 
algorithm and examples  artificial intelligence              
firth  j  r          a synopsis of linguistic theory           in studies in linguistic
analysis  pp       blackwell  oxford 
forbus  k   usher  j   lovett  a   lockwood  k     wetzel  j          cogsketch  opendomain sketch understanding for cognitive science research and for education  in
proceedings of the fifth eurographics workshop on sketch based interfaces and modeling  annecy  france 
   

fithe latent relation mapping engine

forbus  k  d   riesbeck  c   birnbaum  l   livingston  k   sharma  a     ureel  l          a
prototype system that learns by reading simplified texts  in aaai spring symposium
on machine reading  stanford university  california 
french  r          the subtlety of sameness  a theory and computer model of analogymaking  mit press  cambridge  ma 
french  r  m          the computational modeling of analogy making  trends in cognitive
sciences                
gardenfors  p          conceptual spaces  the geometry of thought  mit press 
gentner  d          structure mapping  a theoretical framework for analogy  cognitive
science                
gentner  d          language and the career of similarity  in gelman  s     byrnes  j 
 eds    perspectives on thought and language  interrelations in development  pp 
        cambridge university press 
gentner  d          why were so smart  in gentner  d     goldin meadow  s   eds   
language in mind  advances in the study of language and thought  pp         
mit press 
gentner  d   bowdle  b  f   wolff  p     boronat  c          metaphor is like analogy  in
gentner  d   holyoak  k  j     kokinov  b  n   eds    the analogical mind  perspectives from cognitive science  pp          mit press  cambridge  ma 
gildea  d     jurafsky  d          automatic labeling of semantic roles  computational
linguistics                 
girju  r   nakov  p   nastase  v   szpakowicz  s   turney  p     yuret  d          semeval     task     classification of semantic relations between nominals  in proceedings
of the fourth international workshop on semantic evaluations  semeval        pp 
      prague  czech republic 
golub  g  h     van loan  c  f          matrix computations  third edition   johns
hopkins university press  baltimore  md 
hawkins  j     blakeslee  s          on intelligence  henry holt 
hirst  g     st onge  d          lexical chains as representations of context for the detection
and correction of malapropisms  in fellbaum  c   ed    wordnet  an electronic
lexical database  pp          mit press 
hofmann  t          probabilistic latent semantic indexing  in proceedings of the   nd
annual acm conference on research and development in information retrieval  sigir      pp        berkeley  california 
hofstadter  d          epilogue  analogy as the core of cognition  in gentner  d   holyoak 
k  j     kokinov  b  n   eds    the analogical mind  perspectives from cognitive
science  pp          mit press 
hofstadter  d     farg         fluid concepts and creative analogies  computer models
of the fundamental mechanisms of thought  basic books  new york  ny 
   

fiturney

holyoak  k     thagard  p          analogical mapping by constraint satisfaction  cognitive
science             
holyoak  k     thagard  p          mental leaps  mit press 
hummel  j     holyoak  k          distributed representations of structure  a theory of
analogical access and mapping  psychological review              
jiang  j  j     conrath  d  w          semantic similarity based on corpus statistics
and lexical taxonomy  in proceedings of the international conference on research in
computational linguistics  rocling x   pp        tapei  taiwan 
kilgarriff  a          i dont believe in word senses  computers and the humanities     
      
lakoff  g     johnson  m          metaphors we live by  university of chicago press 
landauer  t  k     dumais  s  t          a solution to platos problem  the latent semantic analysis theory of the acquisition  induction  and representation of knowledge 
psychological review                  
leacock  c     chodrow  m          combining local context and wordnet similarity for
word sense identification  in fellbaum  c   ed    wordnet  an electronic lexical
database  mit press 
lee  d  d     seung  h  s          learning the parts of objects by nonnegative matrix
factorization  nature              
lepage  y          solving analogies on words  an algorithm  in proceedings of the   th
annual conference of the association for computational linguistics  pp         
lepage  y     denoual  e          purest ever example based machine translation  detailed
presentation and assessment  machine translation                 
lin  d          an information theoretic definition of similarity  in proceedings of the   th
international conference on machine learning  icml     
martin  j  h          computer understanding of conventional metaphoric language  cognitive science                 
marx  z   dagan  i   buhmann  j     shamir  e          coupled clustering  a method
for detecting structural correspondence  journal of machine learning research    
       
mason  z          cormet  a computational  corpus based conventional metaphor extraction system  computational linguistics               
minsky  m          the society of mind  simon   schuster  new york  ny 
mitchell  m          analogy making as perception  a computer model  mit press  cambridge  ma 
nastase  v     szpakowicz  s          exploring noun modifier semantic relations  in
fifth international workshop on computational semantics  iwcs     pp         
tilburg  the netherlands 
reitman  w  r          cognition and thought  an information processing approach  john
wiley and sons  new york  ny 
   

fithe latent relation mapping engine

resnik  p          using information content to evaluate semantic similarity in a taxonomy 
in proceedings of the   th international joint conference on artificial intelligence
 ijcai      pp          san mateo  ca  morgan kaufmann 
rosario  b     hearst  m          classifying the semantic relations in noun compounds
via a domain specific lexical hierarchy  in proceedings of the      conference on
empirical methods in natural language processing  emnlp      pp       
santorini  b          part of speech tagging guidelines for the penn treebank project  tech 
rep   department of computer and information science  university of pennsylvania 
  rd revision   nd printing  
scholkopf  b   smola  a  j     muller  k  r          kernel principal component analysis  in
proceedings of the international conference on artificial neural networks  icann       pp          berlin 
turney  p  d          mining the web for synonyms  pmi ir versus lsa on toefl  in
proceedings of the twelfth european conference on machine learning  ecml     
pp          freiburg  germany 
turney  p  d          measuring semantic similarity by latent relational analysis  in proceedings of the nineteenth international joint conference on artificial intelligence
 ijcai      pp            edinburgh  scotland 
turney  p  d          similarity of semantic relations  computational linguistics         
       
turney  p  d          a uniform approach to analogies  synonyms  antonyms  and associations  in proceedings of the   nd international conference on computational
linguistics  coling        pp          manchester  uk 
turney  p  d     littman  m  l          corpus based learning of analogies and semantic
relations  machine learning                  
turney  p  d   littman  m  l   bigham  j     shnayder  v          combining independent
modules to solve multiple choice synonym and analogy problems  in proceedings of
the international conference on recent advances in natural language processing
 ranlp      pp          borovets  bulgaria 
van rijsbergen  c  j          the geometry of information retrieval  cambridge university
press  cambridge  uk 
veale  t          the analogical thesaurus  in proceedings of the   th innovative applications of artificial intelligence conference  iaai        pp          acapulco 
mexico 
veale  t          wordnet sits the sat  a knowledge based approach to lexical analogy  in
proceedings of the   th european conference on artificial intelligence  ecai       
pp          valencia  spain 
widdows  d          geometry and meaning  center for the study of language and
information  stanford  ca 
yan  j     forbus  k  d          similarity based qualitative simulation  in proceedings of
the   th annual meeting of the cognitive science society  stresa  italy 

   

fi