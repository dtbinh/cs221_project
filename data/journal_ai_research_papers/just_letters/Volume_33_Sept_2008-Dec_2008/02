journal artificial intelligence research                 

submitted        published      

use automatically acquired examples
all nouns word sense disambiguation
david martinez

davidm csse unimelb edu au

university melbourne
      melbourne  australia

oier lopez de lacalle

oier lopezdelacalle ehu es

university basque country
       donostia  basque country

eneko agirre

e agirre ehu es

university basque country
       donostia  basque country

abstract
article focuses word sense disambiguation  wsd   natural language processing task thought important many language technology
applications  information retrieval  information extraction  machine translation  one main issues preventing deployment wsd technology lack
training examples machine learning systems  known knowledge acquisition bottleneck  method shown work small samples words
automatic acquisition examples  previously shown one
promising example acquisition methods scales produces freely available database
    million examples web snippets polysemous nouns wordnet 
paper focuses issues arise using examples  alone addition
manually tagged examples  train supervised wsd system nouns  extensive
evaluation lexical sample all words senseval benchmarks shows
able improve commonly used baselines achieve top rank performance 
good use prior distributions senses proved crucial factor 

   introduction
paper devoted word sense disambiguation  wsd  task natural language
processing  nlp   goal task determine senses words
appear context  instance  given sentence took money bank  
focus word bank  goal would identify intended sense 
context would financial sense  instead possibilities edge
river sense  senses defined dictionary  knowledge base ontology 
task defined intermediate step towards natural language understanding 
construction efficient algorithms wsd would benefit many nlp applications
machine translation  mt   information retrieval  ir  systems  resnik        
instance  mt system translate previous example french  would
need choose among possible translations word bank  word
translated banque used financial sense  as example  
rive used edge river sense  see work vickrey  biewald 
c
    
ai access foundation  rights reserved 

fimartinez  lopez de lacalle   agirre

teyssier  koller        recent evaluation cross lingual wsd mt  ir
engines  would useful determine sense word query
order retrieve relevant documents  specially working multilingual documents
cross language information retrieval  clir   ir scenarios recall key
performance factor  retrieving images captions  evidence favor
using wsd ir gathered lately  kim  seo    rim        liu  liu  yu    meng 
      stevenson   clough        vossen  rigau  alegra  agirre  farwell    fuentes        
wsd techniques fill important role context semantic web 
web grown focusing human communication  rather automatic processing 
semantic web vision automatic agents working information
web semantic level  achieving interoperability use common terminologies
ontologies  daconta  obrst    smith         unfortunately information
web unstructured textual form  task linking terms texts
concepts reference ontology paramount semantic web 
narrower domains biomedicine calling wsd techniques  unified
medical language system  umls   humphreys  lindberg  schoolman    barnett       
one extensive ontologies field  studies mapping terms medical
documents resource reported high levels ambiguity  calls wsd
technology  weeber  mork    aronson        
wsd received attention many groups researchers  general nlp books
dedicating separate chapters wsd  manning   schutze        jurafsky   martin       
dale  moisl    somers         special issues wsd nlp journals  ide   veronis 
      edmonds   kilgarriff         books devoted specifically issue  ravin  
leacock        stevenson        agirre   edmonds         interested reader start
dedicated chapter manning schutze        wsd book  agirre
  edmonds         widespread interest motivated senseval initiative   
joined different research groups common wsd evaluation framework since      
goal follow example successful competitive evaluations  duc
 document understanding conference  trec  text retrieval conference  
wsd systems classified according knowledge use build models  derived different resources corpora  dictionaries  ontologies 
another distinction drawn corpus based systems  distinguishing
rely hand tagged corpora  supervised systems   require resource  unsupervised systems   distinction important effort required
hand tag senses high  would costly obtain tagged examples word
senses languages  estimations show  mihalcea   chklovski         spite
drawback  referred knowledge acquisition bottleneck   recent
efforts devoted improvement supervised systems  ones
obtain highest performance  even current low amounts training data 
systems rely sophisticated machine learning  ml  algorithms construct
models based features extracted training examples 
alternatively  senseval defines two kinds wsd tasks  lexical sample all words 
lexical sample task systems need disambiguate specific occurrences handful
   http   www senseval org

  

fion use automatically acquired examples all nouns wsd

words relatively large numbers training examples provided  more
    examples cases   all words task  training data provided  testing
done whole documents  systems need tag content words occurring texts 
even small amounts external training data available 
analysis results english lexical sample exercise third edition
senseval  mihalcea   edmonds        suggested plateau performance
reached ml methods  task  systems relatively large amounts
training data  many systems top  performing close other 
systems able significantly improve baselines attained accuracies
     mihalcea  chklovski    killgariff        
case different all words task  snyder   palmer         supervised
systems performed best  used training examples semcor  miller  leacock 
tengi    bunker         sizable all words sense tagged corpus time
writing paper  scarcity examples use test documents corpora
unrelated semcor heavily affected performance  systems scored
baseline method assigning frequent sense semcor  order useful
nlp applications  wsd systems address knowledge acquisition bottleneck
 or least significant part  word types  evaluated all words tasks 
lexical sample tasks useful evaluating wsd systems ideal conditions  i e 
regarding availability training data   show systems scalable
words vocabulary  work use lexical sample task order
adjust parameters system  main evaluation all words task 
experiments designed accordingly  lexical sample tests show empirical evidence
specific parameters  all words evaluation compares systems state
art 
article  explore method alleviate knowledge acquisition bottleneck
large scale  use wordnet  fellbaum        automatically acquire examples
web  seminal work leacock  chodorow  miller        showed approach
promising  good results small sample nouns  works field
automatic acquisition examples focused exploring different approaches
acquisition process  agirre  ansa  martinez    hovy        mihalcea        cuadros  padro 
  rigau         straightforward application wsd  explorations typically
required costly querying web  thus tried limited number variations
handful words  approach different spirit  want go whole
process nouns  acquisition examples use wsd
thorough evaluation senseval   lexical sample senseval   all words datasets 
comes cost exploring different possibilities step 
advantage showing results extensive  limited small set
nouns 
reasons  given prior work acquisition techniques  use
efficient effective example acquisition method according independent experiments
performed agirre et al         cuadros et al          focus paper thus
issues arise using examples training data supervised ml
system  paper show automatically acquired examples effectively
  

fimartinez  lopez de lacalle   agirre

used without pre existing data  deciding amount examples use
sense  the prior distribution  key issue 
objectives paper show existing methods acquire examples
web scale up nouns  study issues arise examples
used training data all nouns wsd system  goal build stateof the art wsd system nouns using automatically retrieved examples 
given cost large scale example acquisition  decided limit scope
work nouns  think noun disambiguation useful tool
many applications  specially ir tasks mentioned above  method easily
adapted verbs adjectives  cuadros et al          plan pursue line
future 
work reported partially published two previous conference papers 
method automatic acquisition examples described agirre lopez
de lacalle         first try application examples word sense disambiguation presented agirre martinez      b   paper present global
view whole system  together thorough evaluation  shows
automatically acquired examples used build state of the art wsd systems
variety settings 
article structured follows  introduction  related work knowledge acquisition bottleneck wsd described section    focus automatic
example acquisition  section   introduces method automatically build sensecorpus 
automatically acquired examples wordnet senses  section   describes experimental setting  section   explores factors use sensecorpus evaluates
lexical sample task  final systems thoroughly evaluated all nouns
task section    finally  section   provides discussion  conclusions
work outlined section   

   related work
construction wsd systems applicable words goal many research initiatives  section describe related work looks ways
alleviate knowledge acquisition bottleneck using following techniques  bootstrapping  active learning  parallel corpora  automatic acquisition examples acquisition
topic signatures  sections      evaluate proposed system public datasets 
review best performing systems literature 
bootstrapping techniques consist algorithms learn instances labeled
data  seeds  big set unlabeled examples  among approaches  highlight
co training  blum   mitchell        derivatives  collins   singer        abney 
       techniques appropriate wsd nlp tasks
wide availability untagged data scarcity tagged data  however 
systems shown perform well fine grained wsd  well known work 
yarowsky        applied iterative bootstrapping process induce classifier based
decision lists  minimum set seed examples  disambiguation results comparable
supervised methods obtained limited set binary sense distinctions 
success extended fine grained senses 
  

fion use automatically acquired examples all nouns wsd

recent work bootstrapping applied wsd reported mihalcea       
pham  ng  lee         former  use unlabeled data significantly
increases performance lexical sample system  latter  pham et al  apply
wsd classifier all words task senseval    targeting words threshold
frequency semcor wsj corpora  observe slight increase accuracy
relying unlabeled data 
active learning used choose informative examples hand tagging  order
reduce manual cost  one works directly applied wsd  fujii  inui  tokunaga  tanaka        used selective sampling acquisition examples
disambiguation verb senses  iterative process human taggers  informative
examples chosen following two criteria  maximum number neighbors unsupervised
data  minimum similarity supervised example set  another active learning
approach open mind word expert  mihalcea   chklovski         project
collect sense tagged examples web users  system selects examples
tagged applying selective sampling method based two different classifiers  choosing
unlabeled examples disagreement  collected data used
senseval   english lexical sample task 
parallel corpora another alternative avoid need hand tagged data  recently
chan ng        built classifier english chinese parallel corpora  grouped
senses share chinese translation  occurrences word
english side parallel corpora considered disambiguated sense
tagged appropriate chinese translations  system successfully evaluated
all words task senseval    however  parallel corpora expensive resource
obtain target words  related approach use monolingual corpora second
language use bilingual dictionaries translate training data  wang   carroll 
       instead using bilingual dictionaries  wang martinez        applied machine
translation text snippets foreign languages back english achieved good results
english lexical sample wsd 
automatic acquisition training examples  external lexical resource  wordnet 
instance  sense tagged corpus used obtain new examples large
untagged corpus  e g  web   leacock et al         present method obtain sensetagged examples using monosemous relatives wordnet  approach based
early work  cf  section     algorithm  leacock et al         retrieve number
examples per sense  give preference monosemous relatives consist
multiword containing target word  experiment evaluated    nouns
coarse sense granularity senses  results showed monosemous
corpus provided precision close hand tagged data 
another automatic acquisition approach  mihalcea   moldovan        used information
wordnet  e g  monosemous synonyms glosses  construct queries  later
fed altavista  search engine  four procedures used sequentially  decreasing
order precision  increasing levels coverage  results evaluated hand 
showing     examples correctly retrieved among set       instances
    word senses  however  corpus resulting experiment used
   http   www altavista com

  

fimartinez  lopez de lacalle   agirre

train real wsd system  agirre martinez         early precursor work
presented here  tried apply technique train wsd system unsatisfactory
results  authors concluded examples correct 
somehow mislead ml classifier  providing biased features 
related work  mihalcea        generated sense tagged corpus  gencor  using
set seeds consisting sense tagged examples four sources   i  semcor   ii  wordnet 
 iii  examples created using method above   iv  hand tagged examples
sources  e g  senseval   corpus   means iterative process  system obtained
new seeds retrieved examples  total  corpus         examples
gathered  however  evaluation carried lexical sample task  showing
method useful subset senseval   testing words  results   words
provided   without analysing sources performance gain  even
work presented uses techniques  work seen extension
limited study  sense evaluate all words tasks 
previous works focused use two different kinds techniques
automatic acquisition examples  namely  use monosemous relatives alone  leacock
et al         use combination monosemous relatives glosses  mihalcea
  moldovan        mihalcea         cases examples directly used feed
supervised ml wsd system  limited evaluation indication
methods scale up  unfortunately  direct comparison alternative methods
parameters automatically acquire examples wsd exists  see preference
use web  existing corpora would contain occurrences monosemous
terms gloss fragments 
closely related area automatic acquisition examples wsd
enriching knowledge bases topic signatures  instance  agirre et al        
agirre  ansa  martinez  hovy        used combined monosemous relatives plus
glosses strategy query altavista  retrieve original documents build lists related
words word sense  so called topic signatures   topic signatures difficult
evaluate hand  applied context vectors wsd straightforward way 
note authors train ml algorithm  rather combined examples
one vector per sense  showed using web compared favorably using
fixed corpus  computationally costly  system first needs query search
engine retrieve original document order get example sense 
alternative  agirre lopez de lacalle        showed possible scale
gather examples nouns wordnet query limited using monosemous
relatives snippets returned google used instead whole document 
point  cuadros et al         set systematic framework evaluation
different parameters affect construction topic signatures  including
methods automatically acquire examples  study explores wide range querying
strategies  monosemous synonyms  monosemous relatives different distances  glosses 
combined using either operators  particular corpus  the british national corpus  web  best results obtained using infomap  british
national corpus monosemous relatives method web  agirre   lopez de
   http   infomap nlp sourceforge net

  

fion use automatically acquired examples all nouns wsd

lacalle         contrary method  infomap returns lists related words 
thus used retrieve training examples  results confirmed
experiments reported cuadros rigau        
all  literature shows using monosemous relatives snippets
web  agirre   lopez de lacalle        provides method automatically acquire examples
scales nouns wordnet  provides topic signatures better quality
alternative methods  explain examples acquired 

   building sense tagged corpus nouns automatically
order build corpus  which refer sensecorpus  acquired      
google snippets monosemous noun wordnet      including multiwords  e g 
church building   then  word sense ambiguous noun  gathered examples monosemous relatives  e g  sense    church  gather examples
relative church building   way collect examples simply querying corpus
word string words  e g  church building   method inspired
work leacock et al         and  already mentioned section    shown
efficient effective experiments topic signature acquisition 
basic assumption method given word sense target word 
monosemous synonym word sense  examples synonym
similar target word sense  could therefore used train
classifier target word sense  idea   lesser extent  applied
monosemous relatives  direct hyponyms  direct hypernyms  siblings  indirect
hyponyms  etc  expected reliability decreases distance hierarchy
monosemous relative target word sense 
actual method build sensecorpus following  collected examples
web monosemous relatives  relatives associated number
 type   correlates roughly distance target word  indicates
relevance  higher type  less reliable relative  synonyms type    direct
hyponyms get type    distant hyponyms receive type number equal distance
target sense  direct hypernyms get type    general
target sense  thus introduce noise direct hyponyms  decided
include less reliable siblings  type    sophisticated schemes could tried 
using wordnet similarity weight distance target relative
word  however  chose approach capture notion distance simplicity 
avoid testing many parameters  sample monosemous relatives different
senses church  together sense inventory wordnet     shown figure   
following subsections describe step step method construct
corpus  first explain acquisition highest possible amount examples per
sense  explain different ways limit number examples per sense
better performance 
    collecting examples
method collect examples previously published  agirre   lopez de
lacalle         comprises following steps 
  

fimartinez  lopez de lacalle   agirre

sense inventory  church 
sense    group christians  group professing christian doctrine belief 
sense    place public  especially christian  worship 
sense    service conducted church 
monosemous relatives different senses  of church 
synonyms  type     church building  sense     church service  sense       
direct hyponyms  type     protestant church  sense     coptic church  sense       
direct hypernyms  type     house prayer  sense     religious service  sense       
distant hyponyms  type           
 sense      

greek church  sense     western church

siblings  type     hebraism  sense     synagogue  sense       

figure    sense inventory sample monosemous relatives wordnet     church 

   query google  monosemous relatives sense  extract
snippets returned search engine  snippets used  up        
dropped next step 
   try detect full meaningful sentences snippets contain target
word  first detect sentence boundaries snippet extract sentence
encloses target word  sentences filtered out  according following
criteria  length shorter   words  non alphanumeric characters words
divided two  words uppercase lowercase 
   automatically acquired examples contain monosemous relative target
word  order use examples train classifiers  monosemous relative  which
multiword term  substituted target word  case monosemous
relative multiword contains target word  e g  protestant church church 
choose substitute  protestant  instance  useful feature
first sense church  tried alternatives  section   show
obtain slightly better results substitution applied multiwords 
   given word sense  collect desired number examples  see following
section  order type  first retrieve examples type    type    etc 
type   necessary examples obtained  collect examples type  
upwards  make distinctions relatives type  contrary
leacock et al         give preference multiword relatives containing
target word 
all  acquired around     million examples nouns wordnet using
technique  publicly available   
   use off line xml interface kindly provided google research 
   http   ixa si ehu es ixa resources sensecorpus 

  

fion use automatically acquired examples all nouns wsd

    number examples per sense  prior 
previous work  agirre   martinez        reported distribution number
examples per word sense  prior short  strong influence quality
results  is  results degrade significantly whenever training testing samples
different distributions senses  shown type based approach
predicts majority sense word domain provide good performance
 mccarthy  koeling  weeds    carroll        
extracting examples automatically  decide many examples
use sense  order test impact prior  different settings
tried 
prior  take equal amount examples sense 
web prior  take examples gathered web 
automatic ranking  number examples given ranking obtained following
method mccarthy et al         
sense tagged prior  take number examples proportional relative frequency word senses hand tagged corpus 
first method assumes uniform priors  second assumes number
monosemous relatives occurrences correlated sense importance  is 
frequent senses would occurrences monosemous relatives  fourth
method uses information hand tagged corpus  typically semcor  note
last kind prior requires hand tagged data  rest  including third method
below  completely unsupervised 
third method sophisticated deserves clarification  mccarthy et al         present method acquire sense priors automatically domain
corpus  two step process  first step corpus based method  given
target word builds list contextually similar words  lin        weights 
case  co occurrence data gathered british national corpus  instance 
given target word authority  list topmost contextually similar words include government  police  official agency     second step ranks senses
target word  depending scores wordnet based similarity metric  patwardhan
  pedersen        relative list contextually similar words  following
example  pairwise wordnet similarity authority government greater
sense   authority  evidence sense prominence corpus 
pairwise similarity scores added  yielding ranking   senses authority 
table   shows column named auto mr normalized scores assigned
senses authority according technique 
table   shows number examples per type           acquired church
following semcor prior  last column gives number examples semcor  note
number examples sometimes smaller        maximum number snippets
returned google one query   due rare monosemous relatives 
   actual list words taken demo http   www cs ualberta ca  lindek demos depsim htm 

  

fimartinez  lopez de lacalle   agirre

sense
church  
church  
church  
overall

 
 
   
   
   

 
   
   
 
   

 
   
   
  
     

 
 
 
 
 

total
     
   
   
     

semcor
  
  
  
   

table    examples per type           acquired web three senses
church following semcor prior  total number examples semcor 

sense
authority  
authority  
authority  
authority  
authority  
authority  
authority  
overall

semcor
 ex
  
 
 
 
 
 
 
  

 
    
    
    
   
   
   
   
     

web pr
 ex
 
   
   
     
    
     
    
   
   
    
   
  
   
    
   
     
     

auto 
 ex
   
  
  
  
   
  
  
   

sensecorpus
mr
semcor pr
 
 ex
 
    
   
    
    
   
    
    
   
    
   
   
    
    
  
   
   
  
   
   
 
   
     
    
     

semcor
 ex
   
  
  
  
  
  
 
   

mr
 
    
    
    
   
   
   
   
     

senseval
test
 ex
 
  
    
  
    
 
   
 
   
  
    
  
    
 
   
  
     

table    distribution examples senses authority different corpora  pr
 proportional  mr  minimum ratio  columns correspond different ways
apply semcor prior 

usually caused sentence extraction filtering process  discards around    
snippets 
way apply prior straightforward  illustration  focus
semcor prior  first approach semcor prior  assigned       examples
major sense semcor  gave senses proportion examples  call
method proportional  pr   cases number examples extracted
less expected distribution senses semcor  result  actual number
examples available would follow desired distribution 
alternative  computed  word  minimum ratio  mr  examples
available given target distribution given number examples extracted
web  observed last approach would reflect better original prior 
cost less examples 
table   presents different distributions examples authority  see
senseval testing semcor distributions  together total number examples
web  web pr   semcor proportional distribution  semcor pr  minimum
ratio  semcor mr   automatic distribution minimum ratio  auto mr  
getting maximum one thousand examples per monosemous relative allows get
       examples second sense  web pr column      sixth sense 
  

fion use automatically acquired examples all nouns wsd

semcor
word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
average
total

web
prior
      
      
      
      
      
      
      
     
      
      
     
     
      
     
     
      
       

automatic
prior
     
   
     
     
     
      
   
     
     
     
   
   
     
     
   
     
       

semcor
prior
      
   
      
     
     
     
     
     
     
     
     
     
     
     
     
     
       

semcor
word
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew

web
prior
      
     
      
      
       
   
   
      
      
      
      
     
      
      

automatic
prior
   
     
     
   
     
   
   
     
     
     
     
     
     
     

semcor
prior
     
     
     
     
     
   
   
      
     
     
     
     
     
     

table    number examples following different sense distributions senseval  
nouns  minimum ratio applied semcor automatic priors 

sixth sense single monosemous relative  rare word hits
google  second sense many frequent monosemous relatives 
regarding use minimum ratio  table illustrates mr allows better
approximate distribution senses semcor  first sense      semcor 
gets       sensecorpus proportional semcor prior
    examples sensecorpus first sense  contrast sensecorpus
minimum ratio using semcor assign       examples first sense 
better approximation comes cost getting     examples authority  contrast
      pr  note authority occurs    times semcor 
table shows word distributions senses semcor
senseval test important differences  sense   gets            respectively   although frequent sense same  web automatic distributions 
salient sense different semcor  web prior  web pr column 
assigning      first sense  note automatic method able detect
sense   salient test corpus  semcor ranks  th  general  distribution
discrepancies similar table observed words test
set 
conclude section  table   shows number examples acquired automatically
word senseval   lexical sample following three approaches  web prior 
semcor prior minimum ratio  automatic prior minimum ratio 
see retrieving examples  web prior  get        examples average per
word  respectively             apply semcor prior automatic prior 
   senses wordnet numbered according frequency semcor  first sense
wordnet paramount frequent sense semcor 

  

fimartinez  lopez de lacalle   agirre

    decision lists
supervised learning method used measure quality corpus decision lists
 dl   simple method performs reasonably well comparison supervised
methods senseval words  as illustrate table       preliminary experiments showed perform better automatically retrieved examples
sophisticated methods support vector machines vector space model  well
known learning methods perform differently according several conditions  showed
instance yarowsky florian         analyzed depth performance
various learning methods  including dl  wsd tasks 
think main reason dl perform better preliminary experiments
sensecorpus noisy corpus conflicting features  decision lists use
single powerful feature test context make predictions  contrast
ml techniques  could make perform better corpus  specially
all words task  hand tagged examples per word cases  even
sophisticate ml algorithms cannot deal problem themselves 
best systems senseval   lexical sample rely complex kernel based methods 
all words task top systems find external ways deal sparseness
data apply well known methods  memory based learning decision
trees  mihalcea   edmonds        
dl algorithm described yarowsky         method  sense sk
highest weighted feature selected  according log likelihood  see formula    
implementation  applied simple smoothing method  cases
denominator zero  use     denominator  roughly equivalent assigning
    probability mass rest senses  shown effective enough
compared complex methods  yarowsky        agirre   martinez      a  
p r sk  fi  
 
j  k p r sj  fi  

weight sk       log  p

   

    feature types
feature types extracted context grouped three main sets 
local collocations  bigrams trigrams formed words around target 
features constituted lemmas  word forms  pos tags    local features
formed previous posterior lemma word form context 
syntactic dependencies  syntactic dependencies extracted using heuristic patterns 
regular expressions defined pos tags around target    following relations used  object  subject  noun modifier  preposition  sibling 
topical features  extract lemmas content words whole sentence
  word window around target  obtain salient bigrams context 
methods software described pedersen        
   pos tagging performed fntbl toolkit  ngai   florian        
   software kindly provided david yarowskys group  johns hopkins university 

  

fion use automatically acquired examples all nouns wsd

complete feature set applied main experiments all words senseval  corpus  however  initial experiments lexical sample task local features
topical features  without salient bigrams  applied 

   experimental setting
already noted introduction lexical sample evaluations defined senseval
realistic  relatively large amounts training examples available  drawn
corpus test examples  train test examples tagged
team  besides  developing system handful words necessarily
show scalable  contrast  all words evaluations provide training data 
supervised wsd systems typically use semcor  miller et al         training 
corpus offers tagged examples open class words occurring         word subset
balanced brown corpus  tagged wordnet     senses  contrast lexicalsample  polysemous words authority get handful examples    
case  cf  table     note test examples  from senseval  semcor come
different corpora thus might related different domains  topics genres 
added difficulty posed fact tagged different teams
annotators distinct institutions 
mind  designed two sets experiments  first set performed
sample nouns  lexical sample   used develop fine tune method
basic aspects effect kinds features importance prior 
use training examples  except measure impact priors  provide
comparison state of the art systems 
second set experiment used show method scalable  useful
noun  performs state of the art wsd realistic setting  thus
selected apply wsd nouns running text  all nouns   setting apply
best configurations obtained first set experiments  explore use
sensecorpus alone  combined priors semcor  training data
semcor  provide comparison results state of the art systems 
lexical sample evaluation  test part senseval   english lexical sample
task chosen  consisted instances    nouns  tagged wordnet     senses 
advantage corpus could focus word set enough examples
testing  besides  different corpus  therefore evaluation realistic
made using cross validation semcor  order factor pre processing
focus wsd  test examples whose senses multiwords phrasal verbs
removed  note problematic since efficiently detected
methods preprocess 
important note training part senseval   lexical sample used
construction systems  goal test performance could achieve
minimal resources  i e  available word   relied senseval  
training prior preliminary experiments local topical features  upperbound
compare performance types priors 
all words evaluation relied senseval   all words corpus  snyder  
palmer         test data task consisted       words text  data
  

fimartinez  lopez de lacalle   agirre

extracted two wall street journal articles one excerpt brown corpus 
texts represent three different domains  editorial  news story  fiction  overall       
words tagged wordnet        senses        include multiwords  
these      occurrences correspond polysemous nouns part multiwords 
comprise testing set 
rest senseval participants  added difficulty wordnet versions
coincide  therefore used one freely available mappings wordnet
versions  daude  padro    rigau        convert training material semcor
 tagged wordnet     senses  wordnet     wordnet       versions  depending
target corpus   preferred use mapping rather relying
available mappings converted semcors  knowledge  comparative evaluation
among mappings performed  daude et al  show mapping obtained
high scores extensive manual evaluation  note versions semcor
available web  other original one  tagged wordnet     
obtained using automatic mapping 
lexical sample all nouns settings  provide set baselines 
based frequent heuristic  heuristic known hard beat wsd 
specially unsupervised systems access priors  even
supervised systems all nouns setting 

   lexical sample evaluation
performed four sets experiments order study different factors  compare
performance state of the art unsupervised systems senseval   lexical sample
task  first analyzed results systems using different sets local
topical features  well substituting multiwords  next experiments
devoted measure effect prior performance  that  compared
approach unsupervised systems participated senseval    mentioned
introduction  results obtained lexical sample evaluations realistic 
cannot expect hand tagged data words target corpus 
reason report results supervised systems  which use training data  
next section all nouns evaluation  realistic  compare supervised
systems
    local vs  topical features  substitution
previous work automatic acquisition examples  leacock et al         reported
lower performance using local collocations formed pos tags closed class words 
contrast  kohomban lee         related approach  used local features
wsd discriminated better senses  given fact sensecorpus
constructed automatically  contradictory results previous
works  performed initial experiment comparing results using local features  topical
features  combination both  case used sensecorpus senseval training
prior  distributed according mr approach  always substituting target word 
results  per word overall  given table   
  

fion use automatically acquired examples all nouns wsd

local feats 
word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
overall

coverage
    
    
    
     
     
    
     
     
    
    
     
     
    
     
     
     
     
     
     
     
     
     
     
    
    
    
     
     
     
    

precision
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

recall
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

topical
feats 
recall
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

combined
subst 
recall
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

combined
subst 
recall
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    results per feature type  local  topical  combination   using sensecorpus
senseval   training prior  mr   coverage precision given local
features  topical combination full coverage   combination shown
substitution substitution options  best recall per word given
bold 

  

fimartinez  lopez de lacalle   agirre

experiment  observed local collocations achieved best precision overall  combination features obtained best recall  local features achieve
      precision       coverage overall     topical combined features
full coverage  table shows clear differences results per word  fact
known algorithms using real training data  yarowsky   florian        
variability another important factor focus all words settings  large numbers
different words involved 
show results substituting monosemous relative target
word monosemous relative multiword  see results mixed 
slight overall improvement choose substitute cases 
following experiments  chose work combination features
substitution  achieved best overall recall 
    impact prior
order evaluate acquired corpus  first task analyze impact
prior  mentioned section      training decision lists examples
sensecorpus  need decide amount examples sense  what seen
estimation prior probabilities senses  
table   shows recall   attained dl four proposed methods
estimate priors target word  plus use training part senseval   lexical
sample estimate prior  note last estimation method realistic  one
cannot expect hand tagged data words given target corpus 
thus taken upperbound  fact presented section completeness 
used comparison systems 
results show constant improvement less informative priors
informed ones  among three unsupervised prior estimation methods  best results
obtained automatic ranking  worst uniform distribution  no prior
column   distribution examples returned sensecorpus  web prior 
middle  estimating priors hand tagged data improves results considerably 
even target corpus estimation corpus different  semcor   best
results overall obtained priors estimated training part senseval  lexical sample dataset  results word word show word behaves differently 
well known behavior wsd  note priors except informed
one number words performances      might indicate dl trained
sensecorpus sensitive badly estimated priors 
table   shows overall results table    together obtained using
prior  prior only   results show improvement attained training
sensecorpus prominent unsupervised priors  from          percentage
points   lower improvements  around     percentage points  priors estimated
hand tagged corpora  results show clearly acquired corpus use    note due sparse data problem  test examples might feature common
training data  cases dl algorithm return result  thus coverage
lower     
    results following tables given recall  coverage always      precision
equals recall case 

  

fion use automatically acquired examples all nouns wsd

unsupervised
word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
overall


prior
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

web
prior
    
    
    
    
    
    
   
    
    
   
    
    
    
    
    
   
    
   
    
    
    
    
    
    
   
    
    
    
    
    

autom 
ranking
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

minimally supervised
semcor
prior
    
    
    
   
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
   
    
    

senseval  
prior
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

table    performance  recall  sensecorpus    nouns senseval   lexical sample 
using different priors train dl  best results word bold 

ful information word senses  estimation prior extremely
important 

prior
prior
web prior
autom  ranking
semcor prior
senseval  prior

type
unsupervised
minimallysupervised

prior
    
    
    
    
    

sensecorpus
    
    
    
    
    

diff 
     
    
    
    
    

table    performance  recall  nouns senseval   lexical sample  row  results
given prior own  sensecorpus using prior  difference
both 

  

fimartinez  lopez de lacalle   agirre

method
sensecorpus  semcor prior 
uned
sensecorpus  autom  prior 
kenneth litkowski clr ls
haynes iit 
haynes iit 

type
minimallysupervised
unsupervised

recall
    
    
    
    
    
    

table    results nouns best minimally supervised fully unsupervised systems  in bold  compared unsupervised systems took part senseval  
lexical sample 

    comparison systems
point  important compare performance dl based approach
systems state art  section compare best unsupervised
system  the one using automatic ranking  minimally unsupervised system  using
semcor prior  systems participating senseval   deemed unsupervised  order results systems  used resources available
senseval   competition  answers participating systems different
tasks available     made possible compare results test data 
set nouns occurrences 
  systems presented senseval   lexical sample task unsupervised 
wasp bench system relied lexicographers hand code information semi automatically
 tugwell   kilgarriff         system use training data  uses
manually coded knowledge think falls supervised category 
results   systems shown table    classified
uned system  fernandez amoros  gonzalo    verdejo        minimally supervised 
use hand tagged examples training  heuristics applied
system rely prior information available semcor  distribution senses
used discard low frequency senses  choose first sense back off
strategy  conditions  minimally supervised system attains       recall 
nearly   points better 
rest systems fully unsupervised  perform significantly worse
unsupervised system 

   nouns evaluation
explained introduction  main goal research develop wsd
system able tag nouns context  sample them  previous
section explored different settings system  adjusting according results
handful words lexical sample task 
    http   www senseval org

  

fion use automatically acquired examples all nouns wsd

section test sensecorpus     occurrences polysemous nouns
present senseval   all words task  compare results performance
systems participated competition  present analysis results
according frequency target nouns 
developed three different systems  based sensecorpus  different requirements external information  less informed system unsupervised
system  called sensecorpus u   use hand coded corpus prior extracted therein  system relies examples sensecorpus following automatic ranking  mccarthy et al         train dl  see section       following
system minimally supervised  sensecorpus ms   sense uses priors
obtained semcor define distribution examples sensecorpus
fed dl  lastly  informed system trains dl hand tagged
examples semcor sensecorpus  following semcor prior   known
sensecorpus s  three systems follow widely used distinction among unsupervised 
minimally supervised supervised systems  compare similar
systems participated senseval   
systems respond realistic scenarios  unsupervised system called
case languages all words hand tagged corpus exists  cases priors coming semcor appropriate  domain specific corpora  minimally
supervised system useful hand tagged corpora 
indication distribution senses  lastly  supervised system  sensecorpus s 
shows performance sensecorpus currently available conditions english 
is  all words corpus limited size available 
order measure real contribution sensecorpus  compare three systems
following baselines  sensecorpus u vs  first sense according
automatically obtained ranking  sensecorpus ms vs  frequent sense semcor 
sensecorpus s vs  decision lists trained semcor  order judge
significance improvements  applied one tail paired t test 
    comparison unsupervised systems senseval  
systems participated all words task three rely
hand tagged corpora  not even estimating prior information   compare performance systems unsupervised system sensecorpus u table    order
make fair comparison respect participants  removed answers
correctly guess lemma test instance  discarding errors pre processing
senseval   xml data  
see one participating systems automatic ranking mccarthy
et al         used baseline  although able improve system 
results best unsupervised system  irst ddd lsi   strapparava  gliozzo   
giuliano         surprisingly  unsupervised method able obtain better performance
dataset version relies semcor frequencies  irst ddd    see next
subsection   discrepancy explained authors  reasons
remarkable results irst ddd lsi clear  subsequent publications
authors shed light it 
  

fimartinez  lopez de lacalle   agirre

code
irst ddd lsi
sensecorpus u
autops  baseline 
dlsi ua

method
lsi
decision lists
automatic rank 
wordnet domains

attempt 
   
   
   
   

prec 
    
    
    
    

rec 
    
    
    
    

f
    
    
    
    

p value
     

     
     

table    performance unsupervised systems participating senseval   all words
    polysemous nouns  accompanied p values one tailed paired t test
respect unsupervised system  in bold  

code
sensecorpus ms
mfs  baseline 
irst ddd   
clr   aw
kunlp
irst ddd   

method
dl
mfs
domain driven
dictionary clues
similar relative wordnet
domain driven

attempt 
   
   
   
   
   
   

prec 
    
    
    
    
    
    

rec 
    
    
    
    
    
    

f
    
    
    
    
    
    

p value

     
     
     
     
     

table    performance minimally supervised systems participating senseval   allwords     polysemous nouns  accompanied p values one tailed
paired t test respect sensecorpus ms  in bold  

improvement baseline lower lexical sample case 
significant      level  significance  p value   order explore reasons
this  performed experiments separating words different sets according
frequency semcor  reported section     
    comparison minimally supervised systems senseval  
four systems senseval used semcor estimate sense distribution 
without using examples word training  show performance
systems  together frequent sense baseline table   
results show sensecorpus examples able obtain best performance
kind systems  well rest  improvement semcor mfs baseline
significant      level 
    comparison supervised systems senseval  
systems participated all words task supervised systems
relied mainly semcor  table    present results top    competing systems
system  trained sensecorpus semcor  include dl system
trained semcor  baseline 
results show using sensecorpus able obtain significant improvement
     points f score baseline  score places system second  close
  

fion use automatically acquired examples all nouns wsd

code
senselearner
sensecorpus s
lccaw
kuaw ans
r d english
gambl aw
upv eaw upv eaw 
meaning
upv eaw upv eaw
prob 
semcor baseline
ujaen 

method
syntactic patterns
dl

ensemble
optim  timbl
ensemble

dl

attempt 
   
   
   
   
   
   
   
   
   
   
   
   

prec 
    
    
    
    
    
    
    
    
    
    
    
    

rec 
    
    
    
    
    
    
    
    
    
    
    
    

f
    
    
    
    
    
    
    
    
    
    
    
    

p value
     

     
     
     
     
     
     
     
     
     
     

table     performance top    supervised systems participating senseval   allwords     polysemous nouns  accompanied p values one tailed
paired t test respect sensecorpus s  in bold  

best system all nouns  statistical significance tests score    
top   systems      rest systems  means system performs
similar top three systems  significantly better rest 
    analysis performance word frequency
previous sections observed different words achieve different rates accuracy 
instance  lexical sample experiments showed precision unsupervised
system ranged              cf  table     clearly  words
whose performance low using sensecorpus  section  group
nouns senseval   all nouns task according frequency see whether
correlation frequency words performance system 
goal identify sets words disambiguated higher accuracy
method  process would allow us previously detect type words system
applied to  thus providing better tool work combination wsd systems
exploit properties language 
study  created separate word sets according frequency occurrence
semcor  table    shows different word sets  frequency ranges  number
nouns range  average polysemy  see frequent words
tend polysemous  case supervised systems  polysemy
number training examples tend compensate other  yielding good results
kinds words  is  polysemous words difficult disambiguate 
examples train semcor  agirre   martinez        
table    shows results different frequency ranges top unsupervised systems senseval    together method  see systems
performance low high frequency range  best performing system  irstddd lsi  profits use threshold leaves many instances unanswered 
regarding improvement sensecorpus u automatic ranking baseline  au  

fimartinez  lopez de lacalle   agirre

range
   
    
    
    
    
     
   
overall

 nouns
   
   
  
  
  
  
   
   

avg  polysemy
   
   
   
   
   
   
   
   

table     number noun occurrences frequency ranges  in semcor  
average polysemy 

   
    
    
    
    
     
   
overall

dlsi ua
att 
f sc 
   
     
  
     
  
     
  
     
  
     
  
    
   
     
   
     

irst ddd lsi
att 
f sc 
   
     
  
     
  
     
  
     
  
     
  
     
  
     
   
     

sensecorpus u
att 
f sc 
   
     
  
     
  
     
  
     
  
     
  
     
   
     
   
     

autops
att 
f sc
   
     
  
     
  
     
  
     
  
     
  
     
   
     
   
     

table     results unsupervised systems senseval   words  evaluated
nouns semcor frequency range  att  stands number words
attempted range  best f score per system given bold 

tops   best results obtained low frequency range         baseline
scores        f score range  results sensecorpus lower baseline
words frequency higher     suggests system reliable
low frequency words  simple threshold takes account frequency words
would indicative performance expect  behavior apparent
unsupervised systems  shows weak spot kind
systems  think future research focus high frequency words 

   discussion
work implemented evaluated all words wsd system nouns
able reach state of the art performance three supervised  unsupervised
semi supervised settings  produced different systems combining sensecorpus
different priors actual examples semcor  supervised system  trained
hand tagged  semcor  automatically obtained corpora  reaches f score
       would rank second senseval   all nouns test data  semi supervised
system  using priors semcor manually tagged examples  would rank first
   

fion use automatically acquired examples all nouns wsd

class  unsupervised system second  cases  sensecorpus improves
baselines 
results remarkable  compare system came first
unsupervised supervised settings  see uses completely
different strategy  contrary  system  using primarily automatically acquired
examples  able perform top ranks three settings 
case  deep gap exists among following three kinds systems   i  supervised
systems specific training  e g  senseval lexical sample systems    ii  supervised systems
all words training  e g  trained using semcor    iii  unsupervised systems 
algorithm implemented all words supervised system  unsupervised system  although implementations obtain state of the art performance
categories  different issues could addressed order close gaps 
make all words unsupervised performance closer supervised systems 
identified three main sources error  low quality relatives applied
words  different distributions senses training testing data  low
performance high frequency  and highly polysemous  words  examine
turn 
algorithm suffers noise introduced relatives far target
word  share local context it  better filtering would required
alleviate problem  one way could retrieve examples
share part local context target word discard examples  another
interesting aspect problem would identify type words achieve low
performance sensecorpus  already observed high frequency words obtain low
performance  another study performance according type relatives would
useful better application algorithm 
order deal words close wordnet relatives  another source
examples would use distributionally similar words  words would obtained
methods one presented lin         retrieved examples would
linked target senses using wordnet similarity package  patwardhan   pedersen 
      
second main problem systems rely automatic acquisition fact
sense distributions training test data different  seriously
affects performance  system relies automatically obtained sense ranking
alleviate problem  however  words still get many examples senses
relevant domain  preliminary experiments  observed benefit using
heuristics filter senses  using number close relatives wordnet 
promising results 
finally  third problem observed section      fact high frequency
words profit automatically acquired examples  unsupervised methods 
frequent  and polysemous  words get low performances  threshold based
systems usually discard answering them  straightforward way improve f score
system would apply threshold discard words apply another method
back off strategy them 
all  detecting limitations system give us important clues work
towards accurate unsupervised all words system  literature shows single
   

fimartinez  lopez de lacalle   agirre

unsupervised system able perform well words  able identify type
words suited different algorithms heuristics  integration
algorithms one single combined system could way go  instance  could
detect cases relatives target word different apply sensecorpus
approach  cases automatic ranking enough evidence 
observed simple heuristics number close relatives wordnet
successfully applied sets words  meta learning techniques  vilalta   drissi       
could useful exploit strengths unsupervised systems 

   conclusions future work
paper presents evidence showing proper use automatically acquired examples
allows state of the art performance wsd nouns  gathered examples
nouns wordnet     resource called sensecorpus  amounting     million examples 
made resource publicly available community 
used examples train supervised wsd system  variety settings 
own  combined prior information coming different sources  combined
training examples semcor  depending knowledge used  able build 
respectively  unsupervised system seen hand labeled training data 
semisupervised one sees priors generic hand labeled corpus  semcor  
fully supervised system uses generic hand labeled corpus  semcor 
training data 
evaluation lexical sample all words settings shown sensecorpus
improves commonly used baselines combinations  achieves state of the art
performance all words senseval   evaluation set nouns  previous work automatic example acquisition evaluated handful words  contrast
shown able scale nouns producing excellent results  way 
learned use prior senses crucial apply acquired examples
effectively 
discussion outlined different ways overcome limitations
system  proposed lines could improve significantly current performance 
although recent literature shows unsupervised system performs
high precision words  believe different systems complement
other  usually perform well different sets words  meta learning perspective  could build word expert system able apply best knowledge
source problem  sensecorpus  hand tagged examples  simple heuristics 
unsupervised algorithms incorporated 
future work  aside proposed improvements  think would
interesting apply method testbeds  order applied  monosemous
relative method requires ontology raw corpus  resources found many
specific domains  biomedicine  fine grainedness wordnet 
could lead practical applications 
   

fion use automatically acquired examples all nouns wsd

acknowledgments
work partially financed ministry education  know project  ict             basque government  consolidated research groups grant  it         
oier lopez de lacalle supported phd grant basque government  david
martinez funded australian research council  grant no  dp        

references
abney  s          bootstrapping  proceedings   th annual meeting association computational linguistics  acl    philadelphia 
agirre  e   ansa  o   martinez  d     hovy  e          enriching large ontologies using
www  proceedings ontology learning workshop  organized ecai 
berlin  germany  
agirre  e   ansa  o   martinez  d     hovy  e          enriching wordnet concepts
topic signatures  procceedings siglex workshop wordnet
lexical resources  applications  extensions customizations  conjunction
naacl 
agirre  e     edmonds  p   eds            word sense disambiguation  algorithms
applications  springer 
agirre  e     lopez de lacalle  o          publicly available topic signatures wordnet nominal senses  proceedings  th international conference language
resources evaluation  lrec   lisbon  portugal 
agirre  e     martinez  d          exploring automatic word sense disambiguation decision lists web  procedings coling      workshop semantic
annotation intelligent content  luxembourg 
agirre  e     martinez  d       a   smoothing word sense disambiguation  proceedings expaa natural language processing  estal   alicante  spain 
agirre  e     martinez  d       b   unsupervised wsd based automatically retrieved
examples  importance bias  proceedings conference empirical
methods natural language processing  barcelona  spain 
blum  a     mitchell  t          combining labeled unlabeled data co training 
proceedings   h annual conference computational learning theory  pp 
       new york  acm press 
chan  y     ng  h          scaling word sense disambiguation via parallel texts 
proceedings   th national conference artificial intelligence  aaai       
pittsburgh  pennsylvania  usa 
collins  m     singer  y          unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural
language processing large corpora  emnlp vlc    college park  md 
usa 
   

fimartinez  lopez de lacalle   agirre

cuadros  m   padro  l     rigau  g          empirical study automatic acquisition
topic signatures  proceedings third international wordnet conference  jeju
island  korea  
cuadros  m     rigau  g          quality assessment large scale knowledge resources 
proceedings      conference empirical methods natural language processing  pp          sydney  australia  association computational linguistics 
daconta  m   obrst  l     smith  k          semantic web  guide future
xml  web services  knowledge management  john wiley   sons 
dale  r   moisl  h     somers  h          handbook natural language processing  marcel
dekker inc 
daude  j   padro  l     rigau  g          mapping wordnets using structural information 
  th anual meeting association computational linguistics  acl      
hong kong 
edmonds  p     kilgarriff  a          natural language engineering  special issue word
sense disambiguation systems  no         cambridge university press 
fellbaum  c          wordnet  electronic lexical database  mit press 
fernandez amoros  d   gonzalo  j     verdejo  f          uned systems senseval   proceedings senseval   workshop  conjunction acl  toulouse 
france 
fujii  a   inui  k   tokunaga  t     tanaka  h          selective sampling example based
word sense disambiguation  computational linguistics  no          pp         
humphreys  l   lindberg  d   schoolman  h     barnett  g          unified medical
language system  informatics research collaboration  journal american
medical informatics association        
ide  n     veronis  j          introduction special issue word sense disambiguation 
state art  computational linguistics              
jurafsky  d     martin  j          introduction natural language processing  computational linguistics  speech recognition  prentice hall  upper saddle river 
nj       
kim  s  b   seo  h  c     rim  h  c          information retrieval using word senses  root
sense tagging approach  sigir     proceedings   th annual international
acm sigir conference research development information retrieval  pp 
        new york  ny  usa  acm 
kohomban  u     lee  w          learning semantic classes word sense disambiguation  proceedings   rd annual meeting association computational
linguistics  acl    
leacock  c   chodorow  m     miller  g  a          using corpus statistics wordnet
relations sense identification  computational linguistics  vol      pp         
lin  d          automatic retrieval clustering similar words  proceedings
coling acl  montreal  canada 
   

fion use automatically acquired examples all nouns wsd

liu  s   liu  f   yu  c     meng  w          effective approach document retrieval
via utilizing wordnet recognizing phrases  sigir     proceedings
  th annual international acm sigir conference research development
information retrieval  pp          new york  ny  usa  acm 
manning  c  d     schutze  h          foundations statistical natural language processing  mit press  cambridge  massachusetts 
mccarthy  d   koeling  r   weeds  j     carroll  j          finding predominant word senses
untagged text  proceedings   nd annual meeting association
computational linguistics  acl   barcelona  spain 
mihalcea  r          bootstrapping large sense tagged corpora  proceedings
 rd international conference language resources evaluation  lrec   las
palmas  spain 
mihalcea  r          co training self training word sense disambiguation  proceedings conference natural language learning  conll        boston 
usa 
mihalcea  r     chklovski  t          open mind word expert  creating large annotated
data collections web users help  proceedings eacl      workshop
linguistically annotated corpora  linc        budapest  hungary 
mihalcea  r   chklovski  t     killgariff  a          senseval   english lexical sample
task  proceedings  rd acl workshop evaluation systems
semantic analysis text  senseval   barcelona  spain 
mihalcea  r     edmonds  p          senseval    third international workshop
evaluation systems semantic analysis text  association computational linguistics 
mihalcea  r     moldovan  d          automatic method generating sense tagged
corpora  proceedings aaai     orlando  fl 
miller  g  a   leacock  c   tengi  r     bunker  r          semantic concordance 
proceedings arpa human language technology workshop  pp         
princeton  nj  distributed human language technology san mateo  ca  morgan
kaufmann publishers 
ngai  g     florian  r          transformation based learning fast lane  proceedings second conference north american chapter association
computational linguistics  pp        pittsburgh  pa  usa 
patwardhan  s     pedersen  t          cpan wordnet  similarity package 
http   search cpan org author sid wordnet similarity       



pedersen  t          decision tree bigrams accurate predictor word sense 
proceedings second meeting north american chapter association
computational linguistics  naacl      pittsburgh  pa 
pham  t  p   ng  h  t     lee  w  s          word sense disambiguation semisupervised learning  proceedings   th national conference artificial
intelligence  aaai        pp            pittsburgh  pennsylvania  usa 
   

fimartinez  lopez de lacalle   agirre

ravin  y     leacock  c          polysemy  theoretical computational approaches 
oxford university press 
resnik  p          word sense disambiguation natural language processing applications 
agirre  e     edmonds  p   eds    word sense disambiguation  chap      pp     
     springer 
snyder  b     palmer  m          english all words task  proceedings  rd
acl workshop evaluation systems semantic analysis text  senseval   barcelona  spain 
stevenson  m          word sense disambiguation  case combining knowledge
sources  csli publications  stanford  ca 
stevenson  m     clough  p          eurowordnet resource cross language information retrieval  proceedings fourth international conference language
resources evaluation  lisbon  portugal 
strapparava  c   gliozzo  a     giuliano  c          pattern abstraction term similarity
word sense disambiguation  irst senseval    proceedings  rd acl
workshop evaluation systems semantic analysis text  senseval   barcelona  spain 
tugwell  d     kilgarriff  a          wasp bench  lexicographic tool supporting word
sense disambiguation  proceedings senseval   workshop  conjunction
acl      eacl       toulouse  france 
vickrey  d   biewald  l   teyssier  m     koller  d          word sense disambiguation
machine translation  proceedings human language technology conference
conference empirical methods natural language processing 
vilalta  r     drissi  y          perspective view survey meta learning  artificial
intelligence review  no          pp       
vossen  p   rigau  g   alegra  i   agirre  e   farwell  d     fuentes  m          meaningful
results information retrieval meaning project  proceedings third
international wordnet conference  jeju island  korea 
wang  x     carroll  j          word sense disambiguation using sense examples automatically acquired second language  proceedings joint human language
technologies empirical methods natural language processing conference  vancouver  canada 
wang  x     martinez  d          word sense disambiguation using automatically translated sense examples  proceedings eacl      workshop cross language
knowledge induction  trento  italy 
weeber  m   mork  j     aronson  a          developing test collection biomedical
word sense disambiguation  proceedings amia symposium  pp         
yarowsky  d          decision lists lexical ambiguity resolution  application accent
restoration spanish french  proceedings   nd annual meeting
association computational linguistics  pp        las cruces  nm 
   

fion use automatically acquired examples all nouns wsd

yarowsky  d          unsupervised word sense disambiguation rivaling supervised methods  proceedings   rd annual meeting association computational
linguistics  pp          cambridge  ma 
yarowsky  d     florian  r          evaluating sense disambiguation across diverse parameter spaces  natural language engineering                

   


