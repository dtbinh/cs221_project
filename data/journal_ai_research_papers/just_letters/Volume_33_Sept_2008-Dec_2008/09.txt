journal of artificial intelligence research                  

submitted        published      

learning partially observable deterministic action models
eyal   illinois   edu

eyal amir
computer science department
university of illinois  urbana champaign
urbana  il        usa

allenc      yahoo   com

allen chang
     latham st   apartment   
mountainview  ca        usa

abstract
we present exact algorithms for identifying deterministic actions effects and preconditions in
dynamic partially observable domains  they apply when one does not know the action model  the
way actions affect the world  of a domain and must learn it from partial observations over time 
such scenarios are common in real world applications  they are challenging for ai tasks because
traditional domain structures that underly tractability  e g   conditional independence  fail there
 e g   world features become correlated   our work departs from traditional assumptions about
partial observations and action models  in particular  it focuses on problems in which actions are
deterministic of simple logical structure and observation models have all features observed with
some frequency  we yield tractable algorithms for the modified problem for such domains 
our algorithms take sequences of partial observations over time as input  and output deterministic action models that could have lead to those observations  the algorithms output all or one of
those models  depending on our choice   and are exact in that no model is misclassified given the
observations  our algorithms take polynomial time in the number of time steps and state features
for some traditional action classes examined in the ai planning literature  e g   strips actions  in
contrast  traditional approaches for hmms and reinforcement learning are inexact and exponentially intractable for such domains  our experiments verify the theoretical tractability guarantees 
and show that we identify action models exactly  several applications in planning  autonomous
exploration  and adventure game playing already use these results  they are also promising for
probabilistic settings  partially observable reinforcement learning  and diagnosis 

   introduction
partially observable domains are common in the real world  they involve situations in which one
cannot observe the entire state of the world  many examples of such situations are available from
all walks of life  e g   the physical worlds  we do not observe the position of items in other rooms  
the internet  we do not observe more than a few web pages at a time   and inter personal communications  we do not observe the state of mind of our partners  
autonomous agents actions involve a special kind of partial observability in such domains 
when agents explore a new domain  e g   one goes into a building or meets a new person   they
have limited knowledge about their action models  actions preconditions and effects   these action models do not change with time  but they may depend on state features  such agents can act
intelligently  if they learn how their actions affect the world and use this knowledge to respond to
their goals 
c
    
ai access foundation  all rights reserved 

fia mir   c hang

learning action models is important when goals change  when an agent acted for a while  it can
use its accumulated knowledge about actions in the domain to make better decisions  thus  learning
action models differs from reinforcement learning  it enables reasoning about actions instead of
expensive trials in the world 
learning actions effects and preconditions is difficult in partially observable domains  the
difficulty stems from the absence of useful conditional independence structures in such domains 
most fully observable domains include such structures  e g   the markov property  independence of
the state at time t     from the state at time t     given the  observed  state at time t   these are
fundamental to tractable solutions of learning and decision making 
in partially observable domains those structures fail  e g   the state of the world at time t    
depends on the state at time t    because we do not observe the state at time t   and complex
approximate approaches are the only feasible path  for these reasons  much work so far has been
limited to fully observable domains  e g   wang        pasula  zettlemoyer    kaelbling        
hill climbing  em  approaches that have unbounded error in deterministic domains  e g   ghahramani        boyen  friedman    koller         and approximate action models  dawsey  minsker 
  amir        hill  minsker    amir        kuffner    lavalle        thrun        
this paper examines the application of an old new structure to learning in partially observable
domains  namely  determinism and logical formulation  it focuses on some such deterministic domains in which tractable learning is feasible  and shows that a traditional assumption about the form
of determinism  the strips assumption  generalized to adl  pednault        leads to tractable
learning and state estimation  learning in such domains has immediate applications  e g   exploration by planning  shahaf  chang    amir        chang   amir        and it can also serve as
the basis for learning in stochastic domains  thus  a fundamental advance in the application of
such a structure is important for opening the field to new approaches of broader applicability  the
following details the technical aspects of our advance 
the main contribution of this paper is an approach called slaf  simultaneous learning and
filtering  for exact learning of actions models in partially observable deterministic domains  this
approach determines the set of possible transition relations  given an execution sequence of actions
and partial observations  for example  the input could come from watching another agent act or
from watching the results of our own actions execution  the approach is online  and updates a
propositional logical formula called transition belief formula  this formula represents the possible
transition relations and world states at every time step  in this way  it is similar in spirit to bayesian
learning of hmms  e g   ghahramani        and logical filtering  amir   russell        
the algorithms that we present differ in their range of applicability and their computational
complexity  first  we present a deduction based algorithm that is applicable to any nondeterministic
learning problem  but that takes time that is worst case exponential in the number of domain fluents 
then  we present algorithms that update a logical encoding of all consistent transition relations in
polynomial time per step  but that are limited in applicability to special classes of deterministic
actions 
one set of polynomial time algorithms that we present applies to action learning scenarios in
which actions are adl  pednault         with no conditional effects  and one of the following
holds   a  the action model already has preconditions known  and we observe action failures  e g  
when we perform actions in the domain   or  b  actions execution always succeeds  e g   when an
expert or tutor performs actions  
   

fil earning partially o bservable d eterministic action m odels

our algorithms output a transition belief formula that represents the possible transition relations
and states after partial observations of the state and actions  they do so by updating each component
of the formula separately in linear time  thus  updating the transition belief formula with every
action execution and observation takes linear time in the size of the input formula 
processing a sequence of t action executions and observations takes time o t    n  for case
 b   the main reason for this is a linear growth in the representation size of the transition belief
formula  at time t  the iterative process that updates this formula would process a formula that has
size linear in t 
for case  a  processing a sequence of length t takes polynomial time o t  n k     only if we
observe every feature in the domain every  k steps in expectation  for some fixed k  the reason for
this is that the transition belief formula can be kept in k cnf  k conjunctive normal
v form  
w thus
of size o nk     recall that a propositional formula is in k cnf  if it is of the form im jk li j  
with every li j a propositional variable or its negation   case  b  takes time o t  n  under the same
assumption 
another set of polynomial time algorithms that we present takes linear time in the representation
size  in this case actions are known to be injective  i e   map states      there  we can bound
the computation time for t steps with o t  nk    if we approximate the transition belief formula
representation with a k cnf formula 
in contrast  work on learning in dynamic bayesian networks  e g   boyen et al          reinforcement learning in pomdps  e g   littman         and inductive logic programming  ilp 
 e g   wang        either approximate the solution with unbounded error for deterministic domains 
n
or take time        and are inapplicable in domains larger than    features  our algorithms are
better in this respect  and scale polynomially and practically to domains of    s of features and
more  section   provides a comparison with these and other works 
we conduct a set of experiments that verify these theoretical results  these experiments show
that our algorithms are faster and better qualitatively than related approaches  for example  we can
learn some adl actions effects in domains of       features exactly and efficiently 
an important distinction must be made between learning action models and traditional creation
of ai planning operators  from the perspective of ai planning  action models are the result of
explicit modeling  taking into account modeling decisions  in contrast  learning action models
is deducing all possible transition relations that are compatible with a set of partially observed
execution trajectories 
in particular  action preconditions are typically used by the knowledge engineer to control the
granularity of the action model so as to leave aside from specification unwanted cases  for example 
if driving a truck with insufficient fuel from one site to another might generate unexpected situations
that the modeller does not want to consider  then a simple precondition can be used to avoid considering that case  the intention in this paper is not to mimic this modeling perspective  but instead
find action models that generate sound states when starting from a sound state  sound state is any
state in which the system can be in practice  namely  ones that our observations of real executions
can reflect 
our technical advance for deterministic domains is important for many applications such as
automatic software interfaces  internet agents  virtual worlds  and games  other applications  such
as robotics  human computer interfaces  and program and machine diagnosis can use deterministic
action models as approximations  finally  understanding the deterministic case better can help us
   

fia mir   c hang

develop better results for stochastic domains  e g   using approaches such as those by boutilier 
reiter  and price         hajishirzi and amir        
in the following  section   defines slaf precisely  section   provides a deduction based exact
slaf algorithm  section   presents tractable action model update algorithms  section   gives sufficient conditions and algorithms for keeping the action model representation compact  thus  overall
polynomial time   and section   presents experimental results 

   simultaneous learning and filtering  slaf 
simultaneous learning and filtering  slaf  is the problem of tracking a dynamic system from
a sequence of time steps and partial observations  when we do not have the systems complete
dynamics initially  a solution for slaf is a representation of all combinations of action models
that could possibly have given rise to the observations in the input  and a representation of all the
corresponding states in which the system may now be  after the sequence of time steps that were
given in the input occurs  
computing  the solution for  slaf can be done in a recursive fashion by dynamic programming
in which we determine slaf for time step t   from our solution of slaf for time t  in this section
we define slaf formally in such a recursive fashion 
ignoring stochastic information or assumptions  slaf involves determining the set of possible
ways in which actions can change the world  the possible transition models  defined formally below 
and the set of states the system might be in  any transition model determines a set of possible states 
so a solution to slaf is a transition model and its associated possible states 
we define slaf with the following formal tools  borrowing intuitions from work on bayesian
learning of hidden markov models  hmms   ghahramani        and logical filtering  amir  
russell        
definition     a transition system is a tuple hp  s  a  ri  where
 p is a finite set of propositional fluents 
 s  p ow p  is the set of world states 
 a is a finite set of actions 
 r  s  a  s is the transition relation  transition model  
thus  a world state  s  s  is a subset of p that contains propositions true in this state  omitted
propositions are false in that state   and r s  a  s    means that state s  is a possible result of action a
in state s  our goal in this paper is to find r  given known p  s  a  and a sequence of actions and
partial observations  logical sentences on any subset of p  
another  equivalent  representation for s that we will also use in this paper is the following 
a literal is a proposition  p  p  or its negation  p  a complete term over p is a conjunction of
literals from p such that every fluent appears exactly once  every state corresponds to a complete
term of p and vice versa  for that reason  we sometime identify a state s  s with this term  e g  
for states s    s    s  s  is the disjunction of the complete terms corresponding to s     s    respectively 
a transition belief state is a set of tuples hs  ri where s is a state and r a transition relation 
let r   p ow s  a  s  be the set of all possible transition relations on s  a  let s   s  r 
when we hold a transition belief state   s we consider every tuple hs  ri   possible 

   

fil earning partially o bservable d eterministic action m odels

figure    locked door with unknown key domain 
example     consider a domain where an agent is in a room with a locked door  see figure    
in its possession are three different keys  and suppose the agent cannot tell from observation only
which key opens the door  the goal of the agent is to unlock the door 
this domain can be represented as follows  let the set of variables defining the state space
be p    locked  where locked is true if and only if the door is locked  let the set of states be
s    s    s    where s     locked   the state in which the door is locked  and s        here the
door is unlocked   let a    unlock    unlock    unlock    be the three actions wherein the agent
tries unlocking the door using each of the three keys 
let r     hs    unlock    s  i  hs    unlock    s  i  hs    unlock    s  i  represent a transition relation in which key   unlocks the door and the other keys do not  define r   and r  in a similar
fashion  e g   with r  key   unlocks the door and keys   and   do not   a transition belief state
represents the set of possibilities that we consider consistent with our observations so far  consider
a transition belief state given by     hs    r  i  hs    r  i  hs    r  i   i e   the state of the world is
fully known but the action model is only partially known 
we would like the agent to be able to open the door despite not knowing which key opens it  to
do this  the agent will learn the actual action model  i e   which key opens the door   in general  not
only will learning an action model be useful in achieving an immediate goal  but such knowledge
will be useful as the agent attempts to perform other tasks in the same domain  
definition      slaf semantics  let   s be a transition belief state  the slaf of  with
actions and observations haj   oj i jt is defined by
   slaf  a     
 hs    ri   hs  a  s  i  r  hs  ri    
   slaf  o       hs  ri     o is true in s  
   slaf  haj   oj iijt      
slaf  haj   oj ii  jt   slaf  oi   slaf  ai       
step   is progression with a  and step   filtering with o 
example     consider the domain from example      the progression of  on the action unlock  
is given by slaf  unlock         hs    r  i  hs    r  i  hs    r  i   likewise  the filtering of  on the
observation locked  the door became unlocked  is given by slaf  locked       hs     r  i   
   

fia mir   c hang

example     a slightly more involved example is the following situation presented in figure   
there  we have two rooms  a light bulb  a switch  an action of flipping the switch  and an observation  e  we are in the east room   the real states of the world before and after the action  s   s     
respectively  shown in the top part   are not known to us 
west

east
off

psfrag replacements

west
off

s    sw  lit  e

on

 
sw on

 s  r  

 

east

s     sw  lit  e
 s  r  

 s  r  

 s  r  
 s  r  

 s  r  

on

 s  r  

 

figure    top  two rooms and flipping the light switch  bottom  slaf semantics  progressing
an action  the arrows map state transition pairs  and then filtering with an observation
 crossing out some pairs  
the bottom of figure   demonstrates how knowledge evolves after performing the action sw on 
there       hs    r  i  hs    r  i  hs    r  i  for some s    r    s    r    s     e   and r  that includes hs    sw on  s   i  the identity and full details of r    r    r  are irrelevant here  so we omit
them     is the resulting transition belief state after action sw on and observation e      
slaf  sw on  e        
we assume that observations  and an observation model relating observations to state fluents 
are given to us as logical sentences over fluents after performing an action  they are denoted with
o 
this approach to transition belief states generalizes version spaces of action models  e g  
wang        as follows  if the current state  s  is known  then the version spaces lattice contains
the set of transition relations s    r   hs  ri     thus  from the perspective of version spaces 
slaf semantics is equivalent to a set of version spaces  one for each state in which we might be 
this semantics also generalizes belief states  if the transition relation  r  is known  then the
belief state  set of possible states  is r    s   hs  ri     read  restricted to r   and logical
filtering  amir   russell        of belief state  and action a is equal to  thus  we can define it as 
f ilter a       slaf  a   hs  ri   s     r  
thus  slaf semantics is equivalent to holding a set of belief states  each conditioned on a transition
relation  similar to saying if the transition relation is r  then the belief state  set of states  is  r  

   learning by logical inference
learning transition models using definition     directly is intractable  it requires space       
in many cases  the reason for that is the explicit representation of the very large set of possible
transition state pairs  instead  in this section and the rest of this paper we represent transition belief states more compactly using propositional logic  in many scenarios there is some amount of
structure that can be exploited to make a propositional representation compact 
 p 

   

fil earning partially o bservable d eterministic action m odels

a combinatorial argument implies that no encoding is compact for all sets  nonetheless  we are
motivated by the success of propositional  logical  approaches for logical filtering  amir   russell 
      shahaf   amir        and logical database regression  reiter               and observe that
propositional logic represents compactly some natural sets of exponential size 
in this section we re define slaf as an operation on propositional logical formulas with a
propositional formula as output  slafs input is a propositional formula that represents a transition
belief state  and slaf computes a new transition belief formula from that input and a sequence of
actions and observations 
we want to find algorithms for slaf that manipulate an input formula and produce a correct
output  we use general purpose logical inference for this task in this section  in later sections we
sidestep expensive general purpose inference  and make assumptions that lead to tractable algorithms  for the rest of this paper we focus on deterministic transition relations  namely  transition
relations that are partial functions  every action has at most one outcome state for every state  
    representing transition relations in logic
our initial algorithm for solving slaf  to be presented momentarily  does so with a compact
representation of transition belief states  we present this logical encoding of transition belief states
first  and define a deduction based algorithm in the next section 
we use the following general terminology for propositional logical languages  all the terminological conventions apply with or without subscripts and superscripts   l denotes a vocabulary  i e  
a set of propositional variables that we use in the present context  l denotes a language  i e   a set
of propositional sentences      and other script greek letters stand for propositional formulas in
the language of the present context  f  g also stand for such formulas  but in a restricted context
 see below   l   denotes the vocabulary of   l l  denotes the language built from propositions
in l using the standard propositional connectives             l   is a shorthand for l l    
we represent deterministic transition relations with a propositional vocabulary  l a   whose
propositions are of the form afg   for a  a  f a literal over p  and g a logical formula  f is
the effect of afg   and g is the precondition of afg   when proposition afg takes the truth value true 
this has the intended meaning that if g holds in the present state  then f holds in the state that
results from executing a 
we let f  p   p   p  p  be the set of all effects  f   that we consider  we let g be the
set of all preconditions  g  that we consider  in the rest of this section and section   we assume
that g represents a single state in s  recall that we identify a state with a complete term which is
the conjunction of literals that hold in that state  we use this representation of states and write a fs
instead of afg   later we build on this definition and consider gs that are more general formulas 
from our assumption  g  s for now  as stated above  we conclude that l a has o   p     p  
 a   propositional variables  we prove fundamental results for this language and a set of axioms 
disregarding the size of the language for a moment  section   focuses on decreasing the language
size for computational efficiency 
our semantics for vocabulary la lets every interpretation  truth assignment   m   for la correspond with a transition relation  rm   every transition relation has at least one  possibly more 
interpretation that corresponds to it  so this correspondence is surjective  onto  but not injective
   to     every propositional sentence   l la   specifies a set of transition models as follows 
   

fia mir   c hang

the set of models   satisfying interpretations  of   i      m interpretation to la   m      
specifies the corresponding set of transition relations   rm   m  i    
informally  assume that propositions afs        afs k  la take the value true in m   and that all
other propositions with precondition
v s take the value false  then  r m  with action  a  takes the
 
state s to a state s that satisfies ik fi   and is identical to s otherwise  if no such s exists  e g  
fi   fj   for some i  j  k   then rm takes s to no s   thus  a is not executable in s according to
rm   
the following paragraphs show how interpretations over l a correspond to transition relations 
they culminate with a precise definition of the correspondence between formulas in l l a  p  and
transition belief states   s 
e very i nterpretation of la c orresponds to a u nique t ransition r elation
every interpretations of la defines a unique transition relation rm as follows  let m be an interpretation of la   for every state s  s and an action a  a we either define a unique state s   such
that hs  a  s  i  rm or decide that there is no s  for which hs  a  s  i  rm  
m gives an interpretation for every proposition afs   for f a fluent or its negation  if for any
fluent p  p  m  aps     m  ap
s     true  m    is the truth value of  according to interpretation
m    we decide that there is no s  such that hs  a  s  i  rm   otherwise  define
s     p  p   m    aps     p  s   m    ap
s  
in the left hand side of  we consider the cases of p  p for which m  a ps      m  ap
s    and on the
right hand side of  we treat the cases of p  p for which m  aps     m  ap
s     f alse  this is
called inertia because p keeps its previous value for lack of other specifications   put another way 
 
s   p    m  aps     s p   m  ap
s     if we view s as an interpretation of p  rm is well defined  i e  
there is only one rm for every m  
e very t ransition r elation
la

has at

l east o ne c orresponding i nterpretation

of

it is possible that rm   rm   for m    m     this occurs in two circumstances   a  cases in which
there is no hs  a  s  i  rm for some s  a and  b  when m  aps     m  ap
s     f alse  inertia  and
p
p
 
 
m  as     s p   m  as     s p   not inertia  
for an example of the first circumstance  let p be a fluent  let m be an interpretation such that
 
m  aps     m  ap
s   for some g  define m an interpretation that is identical to m on all propositions
p p
p
 
besides as   as as follows  define m  as   to have the opposite truth assignment to m  aps    false
p
instead of true  and true instead of false   define m    ap
s     m  as   
then  rm   rm   because they map all pairs s  a in the same way  in particular  for state s that
corresponds to g  there is no hs  a  s  i  rm and similarly there is no hs  a  s  i  rm    
finally  every transition relation r has at least one interpretation m such that r   r m   to see
this  define mr for every hs  a  s  i  r the interpretation to aps  p any fluent  mr  aps     t ru e iff
  s    finally  for all s  a for
p  s    also  for the same hs  a  s  i define mr  ap
s     f alse iff p 
p
p
which there is no such s    define mr  as     mr  as     t ru e  then  r   rmr  
   we overload the word model for multiple related meanings  model refers to a satisfying interpretation of a logical
formula  transition model is defined in definition     to be a transition relation in a transition system  action model
is define in the introduction section to be any well defined specification of actions preconditions and effects 

   

fil earning partially o bservable d eterministic action m odels

e very t ransition r elation d efines a f ormula over la
every deterministic transition relation r defines a logical formula whose set of models all map
to r  there are many such possible formulas  and we define the most general one  up to logical
equivalence  that does not make use of inertia 
define t h r  as follows 
f
 
 
t h   r     afs   af
s   as  la   hs  a  s i  r  s    f  
p
p
t h   r     as  as   p  p  s  s 

w
 
 
t h   r      pp  aps  ap
s     s   hs  a  s i  r 
t h r    t h   r   t h   r   t h 

t h  addresses fluent changes  t h  addresses fluent innertia  effectively disallowing innertia in
our definition   and t h  addresses conditions in which actions are not executable  thus  t h r 
includes as a model m every interpretation that satisfies rm   r and that requires no inertia for
its definition of rm   it represents r in that each of its models m satisfies rm   r 
it is illuminating to see how our modeling decisions  above and throughout this section  lead
to the last definition  on the one hand  we choose to have every interpretation of l a correspond
to a transition relation  we do this to simplify later arguments about logical entailment   consequently  we associate interpretations m with m  afs     m  af
s     f alse with transition relations r s  a  s    that keep the value of f fixed between s  s   this is inertia for f in a  s   on the
other hand  when we define t h r  above  we choose axioms that exclude such models  thus  we
avoid models that include inertia  because it simplifies our later discussion of learning algorithms 
in summary  we consider every interpretation of la as representing exactly one transition relation  and we consider the set of axioms defining r as those that define it directly  i e   without inertia
 without m  afs     m  af
s     f alse  
t ransition b elief s tates c orrespond to f ormulas over la  p
thus  for every
w transition belief state  we can define a formula in l l a  p  that corresponds to
it  t h     hs ri  s  t h r    other formulas exist that would characterize  in a similar way 
and they are not all equivalent  this is so because there are stronger formulas   l l a   such that
    t h r  and t h r       and every model  m   of  satisfies rm   r 
similarly  for every formula   l la  p  we define a transition belief state       hm p
  rm i   m         i e   all the state transition pairs that satisfy   m p is m restricted to
p  viewed as a complete term over p   we say that formula  is a transition belief formula  if
t h        note   t h       always holds  
    transition formula filtering
in this section  we show that computing the transition belief formula for slaf  a    for successful
action a and transition belief formula  is equivalent to a logical consequence finding operation 
this characterization of slaf as consequence finding permits using consequence finding as an
algorithm for slaf  and is important later in this paper for proving the correctness of our more
tractable  specialized algorithms 
let cnl    denote the set of logical consequences of  restricted to vocabulary l  that is 
l
cn    contains the set of prime implicates of  that contain only propositions from the set l 
   

fia mir   c hang

recall that an implicate  of a formula  is a clause entailed by          recall that a prime implicate  of a formula  is an implicate of  that is not subsumed  entailed  by any other implicates
of  
consequence finding is any process that computes cnl    for an input    for example  propositional resolution  davis   putnam        chang   lee        is an efficient consequence finder
when used properly  lee        del val         marquis        surveys results about prime implicates and consequence finding algorithms   thus  cn l        l l         
for a set of propositions p  let p   represent the same set of propositions but with every proposition primed  i e   each proposition f is annotated to become f      typically  we will use a primed
fluent to denote the value of the unprimed fluent one step into the future after taking an action  let
 p    p  denote the same formula as   but with all primed fluents replaced by their unprimed counterparts  for example  the formula  a  b    p    p  is equal to a  b when b  p   see section   for a
discussion and comparison with relevant formal verification techniques  
the following lemma shows the logical equivalence of existential quantification of quantified
boolean formulas and consequence finding restricted to a vocabulary  recall that quantified boolean
formulas  qbf  are propositional formulas with the addition of existential and universal quantifiers
over propositions  informally  the qbf x  is true for a given interpretation if and only if there
exists some true false valuation of x that makes  true under the assignment  the lemma will prove
useful for showing the equivalence between slaf and consequence finding 
lemma     x   cnl    x      for any propositional logic formula  and propositional variable x 
p roof
see section b     
the lemma extends easily to the case of multiple variables 
corollary     for any formula  and set of propositional variables x  x   cn l   x    
we present an algorithm for updating transition belief formulas whose output is equivalent to
that of slaf  when slaf is applied to the equivalent transition belief state   our algorithm applies
consequence finding to the input transition belief formula together with a set of axioms that define
transitions between time steps  we present this set of axioms first 
for a deterministic  possibly conditional  action  a  the action model of a  for time t  is axiomatized as
v
teff  a    lf  gg   alg  g   l    
w
v
   
l
 
lf  l    gg  ag  g   

the first part of     says that assuming a executes at time t  and it causes l when g holds  and g
holds at time t  then l holds at time t      the second part says that if l holds after as execution 
then it must be that alg holds and also g holds in the current state  these two parts are very similar
to  in fact  somewhat generalize  effect axioms and explanation closure axioms used in the situation
calculus  see mccarthy   hayes        reiter        
now  we are ready to describe our zeroth level algorithm  slaf     for slaf of a transition
belief formula  let l    p   la be the vocabulary that includes only fluents of time t   and effect
propositions from la   recall  definition      that slaf has two operations  progression  with an
action  and filtering  with an observation   at time t we apply progression for the given action a t
and current transition belief formula  t   and then apply filtering with the current observations 
   

fil earning partially o bservable d eterministic action m odels

 

t     slaf   at   ot   t      cnl  t  teff  at     p    p   ot

   

this is identical to definition      slaf semantics   with the above replacing   and   
 
as stated above  for slaf  we can implement cnl    using consequence finding algorithms
such as resolution and some of its variants  e g   simon   del val        mcilraith   amir       
lee        iwanuma   inoue         the following theorem shows that this formula slaf algorithm is correct and exact 
theorem      representation  for  transition belief formula  a action 
slaf  a   hs  ri  s   hs  ri satisfies     
 hs  ri  s   hs  ri satisfies slaf   a    
p roof
see section b     
this theorem allows us to identify slaf  with slaf   and we do so throughout the rest of the
paper  in particular  we show that polynomial time algorithms for slaf in special cases are correct
by showing that their output is logically equivalent to that of slaf    
u sing

the

o utput

of

slaf 

the output of any algorithm for slaf of a transition belief formula is a logical formula  the way
to use this formula for answering questions about slaf depends on the query and the form of the
output formula  when we wish to find if a transition model and state are possible  we wish to see if
m      for m an interpretation of l   p  la and  the output of slaf   
the answer can be found by a simple model checking algorithm     for example  to check that
an interpretation satisfies a logical formula we assign the truth values of the variables in the interpretation into the formula  computing the truth value of the formula can be done in linear time 
thus  this type of query from slaf takes linear time in the size of the output formula from
slaf because the final query is about a propositional interpretation and a propositional formula 
when we wish to find if a transition model is possible or if a state is possible  we can do so with
propositional satisfiability  sat  solver algorithms  e g   moskewicz  madigan  zhao  zhang   
malik         similarly  when we wish to answer whether all the possible models satisfy a property
we can use a sat solver 
example     recall example     in which we discuss a locked door and three combinations  let
    locked  and let     slaf   unlock    locked       we wish to find if   implies that trying
to unlock the door with key   fails to open it  this is equivalent to asking if all models consistent
with   give the value true to unlock locked
locked  
we can answer such a query by taking the slaf  output formula      and checking if   
unlock locked
locked is sat  has a model    follows from the deduction theorem for propositional logic 
     iff    is not sat  
one example application of this approach is the goal achievement algorithm of chang and amir
        it relies on sat algorithms to find potential plans given partial knowledge encoded as a
transition belief formula 
   this is not model checking in the sense used in the formal verification literature  there  the model is a transition
model  and checking is done by updating a formula in obdd with some transformations

   

fia mir   c hang

our zeroth level algorithm may enable more compact representation  but it does not guarantee
it  nor does it guarantee tractable computation  in fact  no algorithm can maintain compact representation or tractable computation in general  deciding if a clause is true as a result of slaf is
conp hard because the similar decision problem for logical filtering is conp hard  eiter   gottlob        amir   russell        even for deterministic actions   the input representation for
both problems includes an initial belief state formula in cnf  the input representation for filtering
includes further a propositional encoding in cnf of the  known  transition relation  
also  any representation of transition belief states that uses poly  p   propositions grows exponentially  in the number of time steps and  p   for some starting transition belief states and action
sequences  when actions are allowed to be nondeterministic     the question of whether such exponential growth must happen with deterministic actions and flat formula representations  e g   cnf 
dnf  etc   see darwiche   marquis        is open  logical circuits are known to give a solution
for deterministic actions  when a representation is given in terms of fluents at time    shahaf et al  
      

   factored formula update
update of any representation is hard when it must consider the set of interactions between all parts
of the representation  operations like those used in slaf   consider such interactions  manipulate
them  and add many more interactions as a result  when processing can be broken into independent
pieces  computation scales up linearly with the number of pieces  i e   computation time is the
total of times it takes for each piece separately   so  it is important to find decompositions that
enable such independent pieces and computation  hereforth we examine one type of decomposition 
namely  one that follows logical connectives 
learning world models is easier when slaf distributes over logical connectives  a function 
f   distributes over a logical connective               if f       f     f     computation of
slaf becomes tractable  when it distributes over     the bottleneck of computation in that case
becomes computing slaf for each part separately 
in this section we examine conditions that guarantee such distribution  and present a linear time
algorithm that gives an exact solution in those cases  we will also show that the same algorithm
gives a weaker transition belief formula when such distribution is not possible 
distribution properties that always hold for slaf follow from set theoretical considerations and
theorem     
theorem     for    transition belief formulas  a action 
slaf  a       slaf  a     slaf  a   
   slaf  a       slaf  a     slaf  a   
p roof
see appendix b     
stronger distribution properties hold for slaf whenever they hold for logical filtering 
theorem     let       be transition belief states 
slaf  a           slaf  a       slaf  a     
   this follows from a theorem about filtering by amir and russell         even if we provide a proper axiomatization
 note that our axiomatization above is for deterministic actions only  

   

fil earning partially o bservable d eterministic action m odels

iff for every r
r
r
r
f ilter a  r
         f ilter a       f ilter a      

we conclude the following corollary from theorems          and theorems by amir and russell
       
corollary     for    transition belief formulas  a action  slaf  a       slaf  a    
slaf  a    if for every relation r in    one of the following holds 
   a in r maps states    
   a in r has no conditional effects     includes all its prime implicates  and we observe if a
fails
   the state is known for r  for at most one s  hs  ri      
condition   combines semantics and syntax  it is particularly useful in the correct computation
of slaf in later sections  it states when    has a particular syntactic form  namely  together
they include their joint prime implicates   and a is simple enough  but not necessarily       then
computation of slaf can be broken into separate slaf of  and  
figure   presents procedure factored slaf  which computes slaf exactly when the conditions of corollary     hold  consequently  factored slaf returns an exact solution whenever our
actions are known to be      if our actions have no conditional effects and their success failure is
observed  then a modified factored slaf can solve this problem exactly too  see section    
procedure factored slaf hai   oi i  it   
i  ai action  oi observation   transition belief formula 
   for i from   to t do 
 a  set   step slaf oi  ai    
 b  eliminate subsumed clauses in  
   return  
procedure step slaf o a  
o an observation formula in l p   a an action   a transition belief formula 
   if  is a literal  then return oliteral slaf a   
   if           return step slaf o a    step slaf o a     
   if           return step slaf o a    step slaf o a     
procedure literal slaf a  
a an action   a proposition in lt or its negation 
 
   return cnl    teff  a   p    p    

figure    slaf using distribution over   
if we pre compute  and cache  the  n possible responses of literal slaf  then every time step
t in this procedure requires linear time in the representation size of   our transition belief formula
at that time step  this is a significant improvement over the  super exponential  time taken by
a straightforward algorithm  and over the  potentially exponential  time taken by general purpose
consequence finding used in our zeroth level slaf procedure above 
theorem     step slaf a  o    returns a formula   such that slaf  a  o           if every run
of literal slaf takes time c  then step slaf takes time o   c    recall that    is the syntactic 
representation size of    finally  if we assume one of the assumptions of corollary      then
   slaf  a  o    
   

fia mir   c hang

a belief state formula is a transition belief formula that has no effect propositions  i e   it includes only fluent variables and no propositions of the form a fg   this is identical to the traditional
use of the term belief state formula  e g    amir   russell         we can give a closed form solution for the slaf of a belief state formula  procedure literal slaf in figure     this makes
procedure literal slaf tractable  avoiding general purpose inference in filtering a single literal 
and also allows us to examine the structure of belief state formulas in more detail 
v
theorem     for belief state formula   l p   action a  ca   gg lf  alg  al
g    and
g         gm  g all the terms in g such that gi     
slaf  a    

 

m
 

li
 li  ag
   ca
i

l       lm f i  

v

here  l       lm f means a conjunction over all possible  combinations of  selections of m literals
from f 
p roof
see appendix b     
this theorem is significant in that it says that we can can write down the result of slaf in a
prescribed form  while this form is still potentially of exponential size  it boils down to simple
computations  its proof follows by a straightforward  though a little long  derivation of the possible
prime implicates of slaf  a  t   
a consequence of theorem     is that we can implement procedure literal slaf using the
equivalence


theorem    
l l   p
slaf  a  l  
l  slaf  a  true  otherwise

notice that the computation in theorem     for    l a literal is simple because g          gm are
all the complete terms in l p  that include l  this computation does not require a general purpose
consequence finder  and instead we need to answer  n   queries at the initialization phase  namely 
storing in a table the values of slaf  a  l  for all l   p and l   p for p  p and also for
l   true 
in general  m could be as high as   p    the number of complete terms in g  and finding g         gm
may take time exponential in  p   still  the simplicity of this computation and formula provide the
basic ingredients needed for efficient computations in the following sections for restricted cases  it
should also give guidelines on future developments of slaf algorithms 

   compact model representation
previous sections presented algorithms that are potentially intractable for long sequences of actions
and observations  for example  in theorem      m could be as high as    p    the number of complete
terms in g  consequently  clauses may have exponential length  in n    p   and there may be a
super exponential number of clauses in this result 
in this section we focus on learning action models efficiently in the presence of action preconditions and failures  this is important for agents that have only partial domain knowledge and are
therefore likely to attempt some inexecutable actions 
we restrict our attention to deterministic actions with no conditional effects  and provide an
overall polynomial bound on the growth of our representation  its size after many steps  and the
   

fil earning partially o bservable d eterministic action m odels

time taken to compute the resulting model  this class of actions generalizes strips  fikes  hart 
  nilsson         so our results apply to a large part of the ai planning literature 
we give an efficient algorithm for learning both non conditional deterministic action effects and
preconditions  as well as an efficient algorithm for learning such actions effects in the presence of
action failures 
    actions of limited effect
in many domains we can assume that every action a affects at most k fluents  for some small k     
it is also common to assume that our actions are strips  and that they may fail without us knowing 
leaving the world unchanged  those assumptions together allow us to progress slaf with a limited
 polynomial factor  growth in the formula size 
we use a language that is similar to the one in section    but which uses only action propositions
alg with g being
a fluent term of size k  instead of a fluent term of size n in g   semantically 
v
l
al     lk  lk        ln all     ln  
theorem     let   l p  be a belief state formula  and a a strips action with  k fluents
affected or in the precondition term  let g k be the set of all terms of k fluents in l p  that are
consistent with   then 
slaf  a    

 

k
 

li
 li  ag
   ca
i

i  
g         gk  g k
g        gk    
l         lk  f

v
here      refers to a conjunction over all possible  combinations of  selections of m literals from
f and g         gk from g k such that g        gk     
p roof
see section b     
the main practical difference between this theorem and theorem     is the smaller number of
terms that need to be checked for practical computation  the limited language enables and entails
a limited number of terms that are at play here  specifically  when a has at most k literals in its
preconditions  we need to check all combinations of k terms g          gk  g k   computation that is
bounded by o exp k   iterations 
the proof uses two insights  first  if a has only one case in which change occurs  then every
clause in theorem     is subsumed by a clause that is entailed by slaf  a   t    has at most one
algi i per literal li  i e   li    lj for i    j  and gi is a fluent term  has no disjunctions   second  every
alg with g term is equivalent to a formula on algi with gi terms of length k  if a affects only k
fluents 
thus  we can encode all of the clauses in the conjunction using a subset of the  extended  action
effect propositions  alg   with g being a term of size k  there are o nk   such terms  and o nk    
such propositions  every clause is of length  k  with the identity of the clause determined by the
 
first half  the set of action effect propositions   consequently  slaf  a   t   takes o nk  k  k    
 
space to represent using o nk  k   clauses of length   k 
   

fia mir   c hang

    actions of no conditional effects  revised language
in thisssection we reformulate the representation that we presented above in section      let
l f   aa  af   af    af   a f     a f     for every f  p  let the vocabulary for the formulas representing transition belief states be defined as l   p  l f   the intuition behind propositions in this
vocabulary is as follows 
v
 al  a causes l for literal l  formally  al  ss als  
v
 af   a keeps f   formally  af   ss   s  f    afs      s  f    af
s   
v
  
 thus  l is a
 a l   a causes false if l  formally  a l   ss  s  l    als  al
s
precondition for executing a  and it must hold when a executes  
for each model of a transition belief formula over l  the valuation of the fluents from p defines a
state  the valuation of the propositions from all l f defines an unconditional deterministic transition
relation as follows  action proposition af  af   is true if and only if action a in the transition
relation causes f  f   to hold after a is executed  action proposition a f  is true if and only if
action a does not affect fluent f   action proposition a f    a f     is true if and only if f  f   is in
the precondition of a  we assume the existence of logical axioms that disallow inconsistent or
impossible models  these axioms are 
   af  af  af 
    af  af     af  af      af  af   


    a f    a f  

for all possible a  a and f  p  the first two axioms state that in every action model  exactly one
of af   af   or af  must hold  thus  a causes f   its negation  or keeps f unchanged   the last axiom
disallows interpretations where both a f   and a f   hold  we state these axioms so that we do not
need to represent these constraints explicitly in the transition belief formula itself 
we will use the set theoretic and propositional logic notations for transition belief states interchangeably  note that the vocabulary we have defined is sufficient for describing any unconditional
strips action model  but not any deterministic action model in general 
example     consider the domain from example      the transition belief state  can be represented by the transition belief formula 
locked 
  unlock locked  unlock locked  unlock locked   
 unlock locked  unlock locked  unlock locked   
 unlock locked  unlock locked  unlock locked    
 
we provide an axiomatization that is equivalent to slaf and is a special case of t eff     with
our notation above  we do this over p and p     recall that we intend primed fluents to represent the
value of the fluent immediately after action a is taken 
   

fil earning partially o bservable d eterministic action m odels

eff  a  

 

prea f  effa f

f p

prea f



 

 a l   l 

l f f  

effa f



 

  al   af   l    l      l    al   af   l    

l f f  

here prea f describes the precondition of action a  it states that if literal l occurs in the precondition
of a then literal l must have held in the state before taking a  the formula eff a f describes the effects
of action a  it states that the fluents before and after taking the action must be consistent according
to the action model defined by the propositions af   af   and af   
now we can show that the revised axiomatization of action models   eff   leads to an equivalent
definition to slaf within our restricted action models 
 

theorem     for any successful action a  slaf  a     cn lp    eff  a   p    p   
p roof

see appendix b     

    always successful non conditional actions
we are now ready to present an algorithm that learns effects for actions that have no conditional
effects  the algorithm allows actions that have preconditions that are not fully known  still  it
assumes that the filtered actions all executed successfully  without failures   so it cannot effectively
learn those preconditions  e g   it would know only some more than it knew originally about those
preconditions after seeing a sequence of events   such a sequence of actions might  for example  be
generated by an expert agent whose execution traces can be observed 
the algorithm maintains transition belief formulas in a special fluent factored form  defined below  by maintaining formulas in this special form  we will show that certain logical consequence
finding operations can be performed very efficiently  a formula is fluent factored  if it is the conjunction of formulas f such that each f concerns only one fluent  f   and action propositions 
also  for every fluent  f   f is conjunction of a positive element  a negative element  and a
neutral one f   f  explf     f  explf    af   with explf   explf   af formulae over action
propositions af   af   a f     a f     and af   possibly multiple different actions   the intuition here
is that explf and explf are all the possible explanations for f being true and false  respectively 
also  af holds knowledge about actions effects and preconditions on f   knowledge that does
not depend on f s current value  note that any formula in l l f   can be represented as a fluentfactored formula  nonetheless  a translation sometimes leads to a space blowup  so we maintain our
representation in this form by construction 
the new learning algorithm  as strips slaf    is shown in figure    to simplify exposition 
it is described for the case of a single action observation pair  though it should be obvious how to
apply the algorithm to sequences of actions and observations  whenever an action is taken  first
the subformulas af   explf   and explf for each f are updated according to steps    a   c   then 
   as strips slaf extends ae strips slaf  amir        by allowing preconditions for actions

   

fia mir   c hang

algorithm as strips slaf ha  oi   
inputs 
v successful action a  observation term o  and fluent factored transition belief formula   
f p f  

returns  fluent factored transition belief formula for slaf  ha  oi   
   for every f  p
 a  set af   a f    explf     a f    explf    af
 b  set explf   af   af   a f    explf   
 c  set explf   af   af   a f    explf   

 d  if o    f  f is observed  then seta   f   f       f     af  explf
 e  if o    f then set f   f      f      af  explf  note  if o     f and
o     f   we do nothing beyond the earlier steps  
   simplify   e g   eliminate subsumed clauses in   
   return 
a  the term  f     is the new explf   and  f    is the new explf   they appear here without simplification
to conform to step  a of the algorithm  this also emphasizes the syntactic nature of the procedure  and that no
implicit logical simplification is assumed 

figure    slaf algorithm for always successful strips actions 
when an observation is received  each f is updated according to the observation according to steps
   d   e   step   merely indicates that in most implementations  it is likely that some simplification
procedure will be used on the formula such as subsumption elimination  however  the use of any
such simplification procedure is not strictly necessary in order for the theoretical guarantees of our
algorithm to hold 
for example  if we know nothing about actions that affect f  e g   when we start our exploration  
then f    f  t ru e    f  t ru e   t ru e  with this representation  slaf  a  f   is the
conjunction  f explf   f explf  af as computed by step   of procedure as strips slaf 
a similar formula holds for observations 
the following theorem shows the correctness of the algorithm  it shows that the steps taken
by the algorithm produce a result equivalent to the logical consequence finding characterization of
slaf of theorem     
theorem     slaf ha  oi     as strips slaf ha  oi    for any fluent factored formula  
successfully executed action a  and observation term o 
p roof
see appendix b    
the time and space complexity of procedure as strips slaf are given in the following
theorem  as a time guarantee  it is shown that the procedure takes linear time in the size of the input
formula  under the condition that the algorithm receives observations often enoughspecifically
   

fil earning partially o bservable d eterministic action m odels

that every fluent is observed at least once every k calls to the procedureit is possible to show
that the transition belief formula remains in k cnf indefinitely  recall that  is in k cnf for some
fixed k  if  is a conjunction of clauses each of size  k   thus  regardless of the length of actionobservation input sequence  the output of as strips slaf and the value of  throughout its
computation is in k cnf  this amounts to a space guarantee on the size of the formula 
theorem     the following are true of as strips slaf 
   the procedure takes linear time in the size of the input formula for a single action  observation
pair input 
   if for every fluent and every k steps there is an observation of that fluent in one of those steps 
and the input formula is in k cnf  them the resulting formula  after an arbitrary number of
steps  is in k cnf 
   if the input of as strips slaf is fluent factored  then so is its output 
p roof
see appendix b    
the following corollary follows immediately from the above 
corollary     in order to process t steps of actions and observations  as strips slaf requires
o  t   p   time  additionally  if 
every fluentis observed every at most k steps  then the resulting
formula always has size that is o  p    a k  
this corollary holds because theorem        guarantees a bound on the size of our belief state
formula at any point in the algorithm 

    learning actions which may fail
in many partially observable domains  a decision making agent cannot know beforehand whether
each action it decides to take will fail or succeed  in this section we consider possible action failure 
and assume that the agent knows whether each action it attempts fails or succeeds after trying the
action 
more precisely  we assume that there is an additional fluent ok observed by the agent such
that ok is true if and only if the action succeeded  a failed action  in this case  may be viewed as
an extra observation by the agent that the preconditions for the action were not met  that is  an
action failure is equivalent to the observation
 

 a f    f     a f    f   
   
f p

action failures make performing the slaf operation considerably more difficult  in particular 
observations of the form     cause interactions between fluents where the value of a particular fluent
might no longer depend on only the action propositions for that fluent  but on the action propositions
for other fluents as well  transition belief states can no longer be represented by convenient fluentfactored formulas in such cases  and it becomes more difficult to devise algorithms which give
useful time and space performance guarantees 
   

fia mir   c hang

algorithm pre strips slaf a  o   
inputs  action a and observation
term o  the transition belief formula  has the following facv w
tored form     i j i j   where each i j is a fluent factored formula 
returns  filtered transition belief formula 
   if o    ok 
w
 a  set     i f  li   where li are the literals appearing in as precondition  and
f  l  is the v
fluent factored formula equivalent to l  i e   f  l      l       l 
       f p   f       f         

 b  set i j  as strips slaf o  i j   for all i j
   else  o    ok  
 a  for all i j

i  set i j  as strips slaf p   i j    where p is the precondition of a
ii  set i j  as strips slaf ha  oi  i j  
   each i j is factored into ai j  bi j where bi j contains all  and only  clauses containing
a fluent from p  for
w any i such that there exists b such that for all j  b i j  b  replace
w

with
b

j ai j
j i j
   simplify each i j  e g  remove subsumed clauses 
   return 
figure    algorithm for handling action failures when preconditions are known 
as we shall demonstrate  action failures can be dealt with tractably if we assume that the action
preconditions are known by the agent  that is  the agent must learn the effects of the actions it can
take  but does not need to learn the preconditions of these actions  in particular  this means that for
each action a  the algorithm is given access to a formula  more precisely  logical term  p a describing
the precondition of action a  clearly  because the algorithm does not need to learn the preconditions
of its actions  we can restrict the action proposition vocabulary used to describe belief states to the
ones of the forms af   af   and af    as we no longer need action propositions of the forms a f   or
a f    
we present procedure pre strips slaf   figure    that performs slaf on transition belief
formulas in the presence of action failures for actions of non conditional effects  it maintains transition belief
as conjunctions of disjunctions of fluent factored formulas  formulas of the
v formulas
w
form    i j i j where each i j is fluent factored   naturally  such formulas are a superset of
all fluent factored formulas 
   pre strips slaf is essentially identical to cnf slaf  shahaf et al        

   

fil earning partially o bservable d eterministic action m odels

the algorithm operates as follows  when an action executes successfully  and an ensuing observation is received   each of the component fluent factored formulas  i j is filtered separately
according to the as strips slaf procedure on the action observation pair  step     on the other
hand  when an action fails  a disjunction of fluent factored formulas is appended to the transition
belief formula  step     each component of the disjunction corresponds to one of the possible reasons the action failed  i e   to one of the literals occurring in the actions precondition   finally  as
observations are accumulated by the learning algorithm  it collapses disjunctions of fluent factored
formulas occurring in the belief formula together  step    or simplifies them generally  step    
decreasing the total size of the formula  as with the case of as strips slaf  these simplification
steps are not necessary in order for our time and space guarantees to hold 
the proof of correctness of algorithm pre strips slaf relies on our distribution results
from section    theorem     and corollary     
we proceed to show the correctness of pre strips slaf  the following theorem shows that
the procedure always returns a filtered transition belief formula that is logically weaker than the
exact result  so it always produces a safe approximation  additionally  the theorem shows that
under the conditions of corollary      the filtered transition belief formula is an exact result 
theorem     the following are true 
   slaf a  o       pre strips slaf a  o   
   pre strips slaf a  o     slaf a  o    if corollary     holds 
p roof
see appendix b     
now we consider the time and space complexity of the algorithm  the following theorem shows
that     the procedure is time efficient  and     given frequent enough observations  as in theorem
      the algorithm is space efficient because the transition belief formula stays indefinitely compact 
theorem     the following are true of pre strips slaf 
   the procedure takes time linear in the size of the formula for a single action  observation pair
input 
   if every fluent is observed every at most k steps and the input formula is in m  k cnf  then
the filtered formula is in m  k cnf  where m is the maximum number of literals in any action
precondition 
p roof
see appendix b     
therefore  we get the following corollary 
corollary     in order to process t steps of actions and observations  pre strips slaf requires
o  t   p   time  if every fluent is 
observed at least
 as frequently as every k steps  then the resulting
mk
formula always has size that is o  p    a 
 

   building on our results
in this section we describe briefly how one might extend our approach to include an elaborate
observation model  bias  and parametrized actions 
   

fia mir   c hang

    expressive observation model
the observation model that we use throughout this paper is very simple  at every state  if a fluent is
observed to have value v  then this is its value in the current state  we can consider an observation
model that is more general 
an observation model  o  is a set of logical sentences that relates propositions in a set obs with
fluents in p  obs includes propositions that do not appear in p  and which are independent of the
previous and following state  times t    and t      given the fluents at time t 
slaf with o is the result of conjoining t with cnlt  oo   i e   finding the prime implicates of
o  o and conjoining those with t   we can embed this extension into our slaf algorithms above 
if we can maintain the same structures that those algorithms use  if o is in k cnf and at every step
we observe all but  at most    variable  then finding the prime implicates is easy  embedding this
into the transition belief formula is done by conjunction of these prime implicates with the formula 
and removal of subsumed clauses  the resulting formula is still fluent factored  if the input was
fluent factored  then  the algorithms above remain applicable with the same time complexity  by
replacing ot with the prime implicates of ot  ot  
using the model the algorithms we described above provide an exact solution to slaf  and
all the tuples hs  ri that are in this solution are consistent with our observations  they compute a
solution to slaf that is represented as a logical formula  we can use a sat solver  e g   moskewicz
et al         to answer queries over this formula  such as checking if it entails a f   for action a and
fluent f   this would show if in all consistent models action a makes f have the value true 
the number of variables in the result formula is always independent of t   and is linear in  p 
for some of our algorithms  therefore  we can use current sat solvers to treat domains of     
features and more 
preference and probabilistic bias many times we have information that leads us to prefer some
possible action models over others  for example  sometimes we can assume that actions change
only few fluents  or we suspect that an action  e g   open door  does not affect some features  e g  
position  normally  we can represent such bias using a preference model  e g   mccarthy       
ginsberg        or a probabilistic prior over transition relations  e g   robert  celeux    diebolt 
      
we can add this bias at the end of our slaf computation  and get an exact solution if we can
compute the effect of this bias together with a logical formula efficiently  preferential biases were
studied before and fit easily with the result of our algorithms  e g   we can use implementations of
doherty  lukaszewicz    szalas        for inference with such bias  
also  algorithms for inference with probabilistic bias and logical sentences are now emerging
and can be used here too  hajishirzi   amir         there  the challenge is to not enumerate
tentative models explicitly  a challenge that is overcome with some success in the work of hajishirzi
and amir        for the similar task of filtering  we can use such algorithms to apply probabilistic
bias to the resulting logical formula 
for example  given a probabilistic graphical model  e g   bayesian networks  and a set of propositional logical sentences  we can consider the logical sentences as observations  with this approach 


a logical sentence  gives rise to a characteristic function   
x   which is   when 
x satisfies  and
  otherwise  for a conjunction of clauses we get a set of such functions  one per clause   thus 
inference in the combined probabilistic logical system is a probabilistic inference  for example 
   

fil earning partially o bservable d eterministic action m odels

one can consider variable elimination  e g   dechter        in which there are additional potential
functions 
parametrized actions in many systems and situations it is natural to use parametrized actions 
these are action schemas whose effect depend on their parameters  but their definition applies
identically to all instantiations 
for example  move b  x  y  can be an action which moves b from position x to position y  with
b  x  y as parameters of the action  these are very common in planning systems  e g   strips 
pddl  situation calculus   a complete treatment of parameterized actions is outside the scope of
this paper  but we give guidelines for a generalization of our current approach for such actions 
consider a domain with a set of fluent predicates and a universe of named objects  the propositional fluents that are defined in this domain are all ground instantiations of predicate fluents  slaf
can work on this set of propositional fluents and instantiated actions in the same manner as for the


rest of this paper  we have action propositions a  x  lg instantiated for every vector of object names


x 




y  lg  
the different treatment comes in additional axioms that say that  
x 
y  a 
x  lg  a 
inference over a transition belief state with these axioms will be able to join information collected
about different instantiations of those actions  we expect that a more thorough treatment will be
able to provide more efficient algorithms whose time complexity depend on the number of action
schemas instead of the number of instantiated actions 
several approaches already start to address this problem  including the work of nance  vogel 
and amir        for filtering and the work of shahaf and amir        for slaf 

   experimental evaluation
previous sections discussed the problem settings that we consider and algorithms for their solutions  they showed that modifying traditional settings for learning in dynamic partially observable
domains is important  determinism alone does not lead to tractability  but our additional assumptions of simple  logical action structure and bounded from   frequency of observations for all fluents
do  specifically  so far we showed that the time and space for computing slaf of a length t time
sequence over n fluents are polynomial in t and n 
this section considers the practical considerations involved in using our slaf procedures  in
particular  it examines the following questions 
 how much time and space do slaf computations take in practice 
 how much time is required to extract a model from the logical formula in the result of our
slaf procedures 
 what is the quality of the learned model  taking an arbitrary consistent model   how far is it
from the true  generating  model 
 do the conditions for the algorithms correctness hold in practice 
 can the learned model be used for successful planning and execution  how do the learning
procedures fit with planning and execution 
we implemented our algorithms and ran experiments with as strips slaf over the following domains taken from the  rd international planning competition  ipc   drivelog  zenotravel 
blocksworld  and depots  details of the domains and the learning results appear in appendix c  
   

fia mir   c hang

each such experiment involves running a chosen algorithm over a sequence of randomly generated
action observation sequences of      steps  information was recorded every     steps 
the random sequence generator receives the correct description of the domain  specified in
pddl  ghallab  howe  knoblock  mcdermott  ram  veloso  weld    wilkins        fox   long 
       a plannig domain description language   the size of the domain  and a starting state   the
size of the domain is the number of propositional fluents in it  it is set by a specification of the
number of objects in the domain and the number and arity of predicates in the domain   it generates
a valid sequence of actions and observations for this domain and starting state  i e   a sequence that
is consistent with the input pddl to that generator but in which actions may fail  action failure is
consistent with the pddl if the action is attempted in a state in which it canot execute  
for our experiments  we chose to have observations as follows  in every time step we select   
fluents uniformly at random to observe  we applied no additional restrictions  such as making sure
each fluent was observed every fixed k steps  
our slaf algorithm receives only such a sequences of actions and observations  and no domain
information otherwise  e g   it does not receive the size of the domain  the fluents  the starting state 
or the pddl   the starting knowledge for the algorithm is the empty knowledge  true 
for each domain we ran the algorithm over different numbers of propositional fluents     to    
fluents   we collected the time and space taken for each slaf computation and plotted them as a
function of the input sequence length  dividing by t the total computation time for t steps   the
time and space results are shown in figures          and    the graphs are broken into the different
domains and compare the time and space taken for different domain sizes  the time is slaf time
without cnf simplification  e g  we do not remove subsumed clauses 
how much time and space do slaf computations take in practice  we can answer the first
question now  we can observe in these figures that the time per step remains relatively constant
throughout execution  consequently  the time taken to perform slaf of different domains grows
linearly with the number of time steps  also  we see that the time for slaf grows with the domain
size  but scales easily for moderate domain sizes   ms per step of slaf for domains of     fluents  
how much time is required to extract a model from the logical formula in the result of our
slaf procedures  our slaf procedures return a logical formula for each sequence of actions
and observations  we need to apply further work to extract a candidate  consistent  model from that
formula  this computation is done with a sat solver for cnf formulas 
what is the quality of the learned model  taking an arbitrary consistent model   how far is it
from the true  generating  model  there are sometimes many possible models  and with little
further bias we must consider each of them possible and as likely  we decided to introduce one
such bias  namely  that actions are instances of actions schemas  thus  the actions are assumed to
have the same effect on their parameters  or objects   given properties of those parameters  thus 
actions effects are assumed independent of the identity of those parameter 
so  with the vanilla implementation  there are propositions which look like
  stack
  stack
  stack
  stack
etc 

e
e
a
a

g 
g 
b 
a 

causes
causes
causes
causes

 on
 on
 on
 on

e
g
a
a

g  
e  
b  
a  

   

fil earning partially o bservable d eterministic action m odels

slaf time step  blocksworld domain
   

   

   

time  ms 

 

   fluents
   fluents
   fluents
    fluents
    fluents

   

   

   

   

 
   

    

    

    

    

    

    

input sequence length

slaf time step  depots domain
   

   

   

time  ms 

 

   fluents
   fluents
    fluents
    fluents
    fluents

   

   

   

   

 
   

    

    

    

    

    

    

input sequence length

figure    slaf time without cnf simplification for domains blocksworld and depots

instead  we replace ground propositions like those above with schematized propositions 
  stack  x  y  causes  on  x  y  
  stack  x  y  causes  on  y  x  
  stack  x  y  causes  on  x  y  
etc 
thus  the belief state formula looks something like 
   

fia mir   c hang

slaf time step  driverlog domain
   
   
   

time  ms 

   
   fluents
   fluents
    fluents
    fluents
    fluents

 
   
   
   
   
 
   

    

    

    

    

    

    

input sequence length

slaf time step  zeno travel domain
   

   

   

time  ms 

   
   fluents
   fluents
    fluents

   

   

   

   

 
   

    

    

    

    

    

    

input sequence length

figure    slaf time without cnf simplification for domains driverlog and zeno travel

 and
 and
 or  on e g 
 or   stack  x  y  causes  not  on  x  y   
 and   stack  x  y  keeps  on  x  y  
 not   stack  x  y  needs  on  x  y      
   
   

fil earning partially o bservable d eterministic action m odels

slaf space  blocksworld domain
   k

space   lisp symbols 

   k

   k
   fluents
   fluents
   fluents
    fluents
    fluents

   k

   k

  k

k
   

    

    

    

    

    

    

input sequence length

slaf space  depots domain
   k

   k

space   lisp symbols 

   k

   k

   fluents
   fluents
    fluents
    fluents
    fluents

  k

  k

  k

  k

k
   

    

    

    

    

    

    

input sequence length

figure    slaf space for domains blocksworld and depots

an example fragment of a model  the complete output is given in appendix c  that is consistent
with our training data is
blocksworld domain 
      fluents
       randomly selected actions
     fluents observed per step
   schematized  learning
   

converting to cnf
clause count        
variable count     
adding clauses
calling zchaff

fia mir   c hang

slaf space  driverlog domain
   k

space   lisp symbols 

   k

   k

   fluents
   fluents
    fluents
    fluents
    fluents

   k

  k

k
   

    

    

    

    

    

    

input sequence length

slaf space  zeno travel domain
  k

space   lisp symbols 

  k

  k
   fluents
   fluents
    fluents

  k

  k

  k

k
   

    

    

    

    

    

    

input sequence length

figure    slaf space for domains driverlog and zeno travel

      precondition heuristics

parsing result
slaf time       
inference time        
learned model 

 unstack needs  not  clear  underob   
 unstack needs  clear  ob  
 unstack needs  arm empty  
   

fil earning partially o bservable d eterministic action m odels

 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
 unstack
   

needs  not  holding  ob   
needs  on  ob  underob  
causes  clear  underob  
causes  not  clear  ob   
causes  not  arm empty   
causes  holding  ob  
causes  not  on  ob  underob   
keeps  on table  underob  
keeps  on table  ob  
keeps  holding  underob  
keeps  on  underob  underob  
keeps  on  ob  ob  
keeps  on  underob  ob  

sometimes  there are multiple possible schematized propositions that correspond to a ground
action proposition  in which case we disjoin the propositions together when doing the replacement
 i e   a single ground propositional symbol gets replaced by a disjunction of schema propositions  
this replacement is a simple procedure  but one that is effective in both deriving more information for fewer steps and also in speeding up model finding from the slaf formula  we implemented
it to run while slaf runs  one could do this during the sat solving portion of the algorithm  with
an equivalent result 
regarding the latter  we ran into scaling issues with the sat solver  zchaff   moskewicz et al  
      tang  yinlei yu    malik        and the common lisp compiler in large experiments  we got
around these issues by applying the replacement scheme above  thus reducing greatly the number
of variables that the sat solver handles 
another issue that we ran into is that the sat solver tended to choose models with blank preconditions  the sequences that we used in these experiments include no action failure  so no preconditions is never eliminated by our algorithm   to add some bias to the extracted action model 
we added axioms of the following form 
 or  not   a causes  f     a needs  not  f   
 or  not   a causes  not  f      a needs  f  
these axioms state that if an action a causes a fluent f to hold  then a requires f to not hold in
the precondition  similarly  there is an analagous axiom for f    intuitively  these axioms cause
the sat solver to favor     action models  we got the idea for this heuristic from the work of wu 
yang  and jiang         which uses a somewhat similar set of axioms to bias their results in terms
of learning preconditions  clearly  these axioms dont always hold  and in the results  one can see
that the learned preconditions are often inaccurate 
some other inaccuracies in the learned action models are reasonable  for example  if a fluent
never changes over the course of an action sequence  the algorithm may infer that an arbitrary action
causes that fluent to hold 
do the conditions for the algorithms correctness hold in practice  in all of those scenarios
that we report here  the conditions that guarantee correctness for our algorithms hold  in our experiments we assumed that the main conditions of the algorithms hold  namely  that actions are
   

fia mir   c hang

deterministic and of few preconditions  we did not enforce having observations for every fluent
every  fixed  k steps  the latter condition is not necessery for correctness of the algorithm  but is
necessary to guarantee polynomial time computation  our experiments verify that this is not necessary in practice  and it indeed be the case that the algorithms can have a polynomial time guarantee
for our modified observation
an earlier work of hlubocky and amir        has included a modified version of as stripsslaf in their architecture and tested it on a suite of adventure game like virtual environments that
are generated at random  these include arbitrary numbers of places  objects of various kinds  and
configurations and settings of those  there  an agents task is to exit a house  starting with no
knowledge about the state space  available actions and their effects  or characteristics of objects 
their experiments show that the agent learns the effects of its actions very efficiently  this agent
makes decisions using the learned knowledge  and inference with the resulting representation is fast
 a fraction of a second per sat problem in domains including more than    object  modes  and
locations  
can the learned model be used for successful planning and execution  how do the learning
procedures fit with planning and execution  the learned model can be used for planning by
translating it to pddl  however  that model is not always the correct one for the domain  so the
plan may not be feasible or may not lead to the required goal  in those cases  we can interleave
planning  execution  and learning  as was described in the work of chang and amir         there 
one finds a short plan with a consistent action model  executes that plan  collects observations  and
applies slaf to those  when plan failure can be detected  e g   that the goal was not achieved   the
results of chang and amir        guarantee that the joint planning execution learning procedure
would reach the goal in a bounded amount of time  that bounded time is in fact linear in the length
of the longest plan needed for reaching the goal  and is exponential in the complexity of the action
model that we need to learn 

   comparison with related work
hmms  boyen   koller        boyen et al         murphy        ghahramani        can be used
to estimate a stochastic transition model from observations  initially  we expected to compare our
work with the hmm implementation of murphy         which uses em  a hill climbing approach  
unfortunately  hmms require an explicit representation of the state space  and our smallest domain     features  requires a transition matrix of         entries  this prevents initializing hmms
procedures on any current computer 
structure learning approaches in dynamic bayes nets  dbns   e g   ghahramani   jordan 
      friedman  murphy    russell        boyen et al         use em and additional approximations  e g   using factoring  variation  or sampling   and are more tractable  however  they are still
limited to small domains  e g      features   ghahramani   jordan        boyen et al          and
also have unbounded errors in discrete deterministic domains  so are not usable in our settings 
a simple approach for learning transition models was devised in the work of holmes and
charles lee isbell        for deterministic pomdps  there  the transition and observation models
are deterministic  this approach is close to ours in that it represents the hidden state and possible models using a finite structure  a looping prediction suffix tree  this structure can be seen
as related to our representation in that both grow models that relate action histories to possible
transition models  in our work here such interactions are realized in the recursive structure of the
   

fil earning partially o bservable d eterministic action m odels

transition belief formula built by as strips slaf  e g  
 af   af   a f    explf   
where explf refers to a similar formula to this that was created in the previous time step 
the main difference that we should draw between our work and that of holmes and charles
lee isbell        is that the latter refers to states explicitly  whereas our work refers to features 
consequently  our representation is  provably  more compact and our procedures scale to larger
domains both theoretically and in practice  furthermore  our procedure provably maintains a reference to all the possible models when data is insufficient to determine a single model  whereas the
work of holmes and charles lee isbell        focuses only on the limit case of enough information for determining a single consistent model  on the down side  our procedure does not consider
stochasticity in the belief state  and this remains an area for further development 
a similar relationship holds between our work and that of littman  sutton  and singh        
in that work  a model representation is given with a size linear in the number of states  this model 
predictive state representation  psr   is based on action observation histories and predicts behavior based on those histories  that work prefers a low dimensional vector basis instead a featurebased representation of states  one of the traditional hallmarks of the knowledge representation
approach   there is no necessary correspondence between the basis vectors and some intuitive features in the real world necessarily  this enables a representation of the world that is more closely
based on behavior 
learning psrs  james   singh        is nontrivial because one needs to find a good lowdimensional vector basis  the stage called discovery of tests   that stage of learning psrs requires
matrices of size  n     for states spaces of size n 
our work advances over that line of work in providing correct results in time that is polylogarithmic in the number of states  specifically  our work learns  deterministic  transition models
in polynomial time in the state features  thus taking time o poly log n   
reinforcement learning  rl  approaches  sutton   barto        bertsekas   tsitsiklis       
compute a mapping between world states and preferred actions  they are highly intractable in
partially observable domains  kaelbling  littman    cassandra         and approximation  e g  
kearns  mansour    ng        meuleau  peshkin  kim    kaelbling        even dar  kakade   
mansour        mccallum        is practical only for small domains  e g         features  with
small horizon time t  
in contrast to hmms  dbns  and rl  our algorithms are exact and tractable in large domains   
    features   we take advantages of properties common to discrete domains  such as determinism 
limited effects of actions  and observed failure 
previous work on learning deterministic action models in the ai planning literature assumes
fully observable deterministic domains  these learn parametrized strips actions using  e g   version spaces  gil        wang         general classifiers  oates   cohen         or hill climbing
ilp  benson         recently  the work of pasula et al         gave an algorithm that learns stochastic actions with no conditional effects  the work of schmill  oates  and cohen        approximates
partial observability by assuming that the world is fully observable  we can apply these to partially
observable learning problems  sometimes  by using the space of belief states instead of world states 
but this increases the problem size to exponentially  so this is not practical for our problem 
finally  recent research on learning action models in partially observable domains includes the
works of yang  wu  and jiang        and of shahaf and amir         in the works of yang et al 
   

fia mir   c hang

        example plan traces are encoded as a weighted maximum sat problem  from which a
candidate strips action model is extracted  in general  there may be many possible action models
for any given set of example traces  and therefore the approach is by nature approximate  in contrast
to ours  which always identifies the exact set of possible action models   the work also introduces
additional approximations in the form of heuristic rules meant to rule out unlikely action models 
the work of shahaf and amir        presents an approach for solving slaf using logicalcircuit encodings of transition belief states  this approach performs tractable slaf for more general deterministic models than those that we present in section    but it also requires sat solvers
for logical circuits  such sat solvers are not optimized nowadays in comparison with cnf sat
solvers  so their overall performance for answering questions from slaf is lower than ours  most
importantly  the representation given by shahaf and amir        grows  linearly  with the number
of time steps  and this can still hinder long sequences of actions and observations  in comparison 
our transition belief formula has bounded size that is independent of the number of time steps that
we track 
our encoding language  la is typical in methods for software and hardware verification and
testing  relevant books and methods  e g   clarke  grumberg    peled        are closely related
to this representation  and results that we achieve here are applicable there and vice versa  the
main distinction we should draw between our work and that done in formal methods  e g   model
checking and bounded model checking  is that we are able to conclude size bounds for logical formulas involved in the computation  while obdds are used with some success in model checking 
and cnf representations are used with some success in bounded model checking  they have little
bounds for the sizes of formulas in theory or in practice  the conditions available in ai applications
are used in the current manuscript to deliver such bounds and yield tractability and scalability results
that are both theoretical and of practical significance 
interestingly  methods that use linear temporal logics  ltl  cannot distinguish between what
can happen and what actually happens  calvanese  giacomo    vardi         thus  they cannot
consider what causes an occurence  our method is similar in that it does not consider alternate
futures for a state explicitly  however  we use an extended language  namely l a   that makes those
alternatives explicit  this allows us to forego the limitations of ltl and produce the needed result 

   conclusions
we presented a framework for learning the effects and preconditions of deterministic actions in
partially observable domains  this approach differs from earlier methods in that it focuses on determining the exact set of consistent action models  earlier methods do not   we showed that in several
common situations this can be done exactly in time that is polynomial  sometime linear  in the number of time steps and features  we can add bias and compute an exact solution for large domains
 hundreds of features   in many cases  furthermore  we show that the number of action observation
traces that must be seen before convergence is polynomial in the number of features of the domain 
these positive results contrast with the difficulty encountered by many approaches to learning of
dynamic models and reinforcement learning in partially observable domains 
the results that we presented are promising for many applications  including reinforcement
learning  agents in virtual domains  and hmms  already  this work is being applied to autonomous
agents in adventure games  and exploration is guided by the transition belief state that we compute
   

fil earning partially o bservable d eterministic action m odels

and information gain criteria  in the future we plan to extend these results to stochastic domains 
and domains with continuous features 

acknowledgments
we wish to thank megan nance for providing the code and samples for the sequence generator 
and also wish to thank dafna shahaf for an encouraging collaboration that enhanced our development and understanding of these results  the first author also wishes to acknowledge stimulating
discussion with brian hlubocky on related topics  we wish to acknowledge support from daf
air force research laboratory award fa                darpa real program   the second
author wishes to acknowledge support from a university of illinois at urbana champaign  college
of engineering fellowship  finally  the first author acknowledges support from a joint fellowship
            with the center for advanced studies and the beckman institute at the university of
illinois urbana champaign 
earlier versions of the results in this manuscript appeared in conference proceedings  amir 
      shahaf et al         

appendix a  our representation and domain descriptions
the transition relation rm for interpretation m in la is defined in section     in a way that is
similar to the interpretation of domain descriptions  domain descriptions are a common method
for specifying structured deterministic domains  fagin  ullman    vardi        lifschitz       
pednault        gelfond   lifschitz         other methods that are equivalent in our contexts
include successor state axioms  reiter        and the fluent calculus  thielscher         their
relevance and influence on our work merit a separate exposition and relationship to our work 
a domain description d is a finite set of effect rules of the form a causes f if g that describe the effects of actions  for f and g being state formulas  propositional combinations of fluent
names   we say that f is the head or effect and g is the precondition for each such rule  we write
a causes f   for a causes f if true  we denote by ed  a  the set of effect rules in d for action
a  a  for an effect rule e  let ge be its precondition and fe its effect  rule e is active in state s  if
s    ge  s taken here as a interpretation in p  
every domain description d defines a unique transition relation r d  s  a  s    as follows 
v
 let f  a  s  be the conjunction of effects of rules that are active in s for a  i e    fe   e 
ed  a   s    ge    we set f  a  s    t ru e when no rule of a is active in s 
 let i a  s  be the set of fluents that are not affected by a in s  i e   i a  s     f  p d   e 
ed  a   s    ge     f 
  l fe     
 define  recalling that world states are sets of fluents 
  



    s  i a  s      s  i a  s  
rd   hs  a  s i 
   
and s     f  a  s 

thus  if action a has an effect of false in s  then it cannot execute in s 
this definition applies inertia  a fluent keeps its value  to all fluents that appear in no active rule 
in some contexts it is useful to specify this inertia explicitly by extra effect rules of the form a
keeps f if g  for a fluent f  p  it is a shorthand for writing the two rules a causes f if f  g
and a causes f if f  g  when d includes all its inertia  keeps  statements  we say that d is
a complete domain description 
   

fia mir   c hang

example a   consider the scenario of figure   and assume that actions and observations occur as
in figure     actions are assumed deterministic  with no conditional effects  and their preconditions
must holds before they execute successfully  then  every action affects every fluent either negatively 
positively  or not at all  thus  every transition relation has a complete domain description that
includes only rules of the form a causes l or a keeps l  where l is a fluent literal  a fluent or
its negation  
time step
action
location
bulb
switch

 

go w

e
 
sw

 
e
lit
 

go e

 
e
 
sw

sw on

 
e
 
sw

go w

 
e
lit
 

go e

 
e
 
sw

figure     an action observation sequence  table entries are observations   legend  e  east  e 
west  lit  light is on  lit  light is off  sw  switch is on  sw  switch is off 

consequently  every transition relation r is completely defined by some domain description d
such that  viewing a tuple as a set of its elements 

 
 

 a causes e    a causes sw    a causes lit  
y
a causes e  a causes sw  a causes lit
r

 
 
   a keeps e
 
a keeps lit
a keeps sw
  go w  
a

 
 

 
 

go e  
 
 
 
  sw on  

say that initially we know the effects of go e  go w  but do not know what sw on does  then 
transition filtering starts with the product set of r  of    possible relations  and all possible    
states  also  at time step   we know that the world state is exactly  e  lit  sw   we try sw on
and get that f ilter sw on      includes the same set of transition relations but with each of those
transition relations projecting the state  e  lit  sw  to an appropriate choice from s  when we
receive the observations o    e  sw of time step        f ilter o    f ilter sw on      
removes from the transition belief state all the relations that gave rise to e or to sw  we are left
with transition relations satisfying one of the tuples in




 sw on causes lit 


sw on causes e 
sw on causes lit
 sw on causes sw 
sw on keeps e


sw on keeps lit

finally  when we perform action go w  again we update the set of states associated with every
transition relation in the set of pairs     when we receive the observations of time step    we
conclude     f ilter o    f ilter go w        
 




sw on keeps e  
sw on causes e  

 
   e  
 e  






 
  

   
sw on causes sw 
sw on causes sw 
lit
lit
   
 
 
 
sw on causes lit  
sw on causes lit  

  
  







sw
sw
 




go e   
go e   
 
   

fil earning partially o bservable d eterministic action m odels

appendix b  proofs
b   proof of lemma      consequence finding and existential quantification
p roof
consider some cnf form of   suppose the clauses containing the literal x are x 
            x  a where             a are clauses  suppose the clauses containing the literal x are
x              x  b   v
suppose the clauses
not containing x or x are             c   then note
v
l    x 
that cn
       ic i       ia  jb i  j    the formula produced by adding
all resolvents over variable x and then removing clauses not belonging to l l    x     since
resolution is complete for consequence finding 
necessity   x     cnl    x      consider any model m of x   by definition  m can be
extended to l    i e   by assigning some value to m x   such that m        extend m such that
this is the case  now suppose for contradiction m is not a model of cn l    x      it cannot be the
case that m k       for any k  because then m        contradiction  then  it must be the case
that m i  j       for some    i  a and    j  b  therefore  m i       and m j       
but m is a model of   so both m x  i       or m x  j        thus either m i       or
m j        contradiction 
sufficiency   cnl    x        x   consider any model m of cnl    x      suppose for
contradiction m x        that is  if m is extended to l    then m        now  extend m
to l   such that m x       it cannot be the case that m k       for some k since m models
cnl    x      because m x       it cannot be the case that m x   j       for any j 
therefore m x  i       for some    i  a  therefore  m i        because m i  j       for
all    j  n  we must have that m j       for all j  but now if we alter m such that m x      
then m satisfies   contradiction   
b   proof of theorem    
p roof
both sides of the equality relation are sets of state transition relation pairs  we show
that the two sets have the same elements  we show first that the left hand side of the equality is
contained in the right hand side 
take hs    ri  slaf  a   hs  ri  s   hs  ri satisfies     from definition     there is s  s
such that s   s  s   hs  ri satisfies   such that hs  a  s  i  r  in other words  there is s  s
such that hs  ri satisfies  and hs  a  s  i  r 
to prove that hs    ri satisfies slaf   a    we need to show that   teff  at   has a model m
such that rm   r and s  interprets p     let m be such that s interprets p  s  interprets p     and mr
interpreting la as in the previous
section  this interpretation
does not
wsatisfy this formula only if
v
v
one of the conjuncts of   lf  gg   alg  g   l     lf  l     gg  alg  g    is falsified 
this cannot be the case for  by our choice of s 
assume by contradiction that  alg  g   l    fails for some l  then   alg  g  hold in m and l 
is false  the portion of m that interprets la is built according to mr   for our r  since hs  r  s  i
we know that s  satisfies l  by the construction of mr   this is a contradicts l   being false in m
 m interprets p   according to s     and therefore we conclude that  alg  g   l    for every l   
w
similarly  assume by contradiction that  l      gg  alg  g    fails for some l  then  l   holds
in s    and als fails  again  from the way we constructed mr it must be that als in m takes the value
that corresponds to the l   s truth value in s    thus  it must be that als takes the value true in m  
and we are done with the first direction 
   

fia mir   c hang

for the opposite direction  showing the right hand side is contained in the left hand side   take
hs    ri  s that satisfies slaf  a     we show that
hs    ri  slaf  a   hs  ri  s   hs  ri satisfies    
hs    ri    slaf  a    implies  corollary      that there is s  s such that hs     r  si   
  teff  a   s    r  s interpreting p     la   p  respectively   a similar argument to the one we give for
the first part shows that hs  a  s  i  r  and hs    ri  slaf  a   hs  ri  s   hs  ri satisfies    
 
b   proof of theorem      distribution of slaf over connectives
for the first part  we will show that the sets of models of slaf  a      and slaf  a    
slaf  a    are identical 
take a model m of slaf  a       let m   be a model of    for which slaf  a  m      
m   then  m   is a model of  or m is a model of   without loss of generalization  assume m
is a model of   thus  m    slaf  a     and it follows that m is a model of slaf  a    
slaf  a    
for the other direction  take m a model of slaf  a     slaf  a     then  m a model of
slaf  a    or m is a model of slaf  a     without loss of generalization assume that m is a
model of slaf  a     take m   a model of  such that m   slaf  a  m      so  m          it
follows that m    slaf  a      
a similar argument follows for the  case  take a model m of slaf  a       let m   be
a model of    for which slaf  a  m       m   then  m   is a model of  and   thus  m   
slaf  a    and m    slaf  a     it follows that m is a model of slaf  a   slaf  a    
 
b   proof of theorem      closed form for slaf of a belief state formula
p roof sketch
we follow the characterization offered by theorem     and formula      we
take t at teff  a  t  and resolve out the literals of time t  this resolution is guaranteed to generate
a set of consequences that is equivalent to cnlt    t  at  teff  a  t   
v
assuming t at   teff  a  t  is logically equivalent to teff  a  t     lf  gg g     alg gt   
v
lt      lf  gg g    lt     gt  alg     this follows from two observations  first  notice that
t implies that for any g  g such that g      we get gt  alg and  alg  gt    lt    the
antecedent does notwhold  so the formula is true   second  notice that  in
v the second part of the
original teff  a  t     gg g    alg  gt    is equivalent  assuming   to   gg g    gt  alg    
now  resolving out the literals of time t from t  at  teff  a  t   should consider all the
resolutions of clauses  gt is a term  of the form alg  gt  lt   and all the clauses of the form
lt     gt  alg   with each other  this yields the equivalent to
 

m
 

  lit  
i  
g        
wgm  g
    im gi
l         lm  f

m
 
li
 algi i   
  ag
i
i  

because to eliminate all w
the literals of time t we have to resolve together sets of clauses with matching gi s such that     in gi   the formula above encodes all the resulting clauses for a chosen
   

fil earning partially o bservable d eterministic action m odels

li
i
set of g         gm and a chosen set of literals l         lm   the reason for including  al
gi  agi   is
that we can always choose a clause with gi   li of a specific type  either one that includes alg or
one that produces al
g 
finally  we get the formula in our theorem because afg  af
g  g characterizes exactly one
state in s   and the fact that there is one set of g         gm that is stronger than all the rest  it entails
all the rest  because g         gm are complete terms  this set is the complete fluent assignments g i
that satisfy    

b   proof of theorem      closed form when k affected fluents
p roof sketch
for literal l and clause c in the conjunction in theorem      we aggregate all
the action propositions to a single action proposition algl   with g the disjunction of all the complete
preconditions for l in c  notice that there cannot be ls negation because then the c is a tautology  
lemma b   shows that gl is equivalent to a fluent term  first we prove the more restricted lemma
b   
w
li
t  
  be a clause of the formula in theorem      and let gl  
lemma b   let c   m
 ag
i    li
i
w
 gi   li   l  gi    li    for any literal l    assume that as effect is deterministic with only
one case with a term precondition  if that case does not hold  then nothing changes   then  g l is
equivalent to a term 
p roof
gl is a disjunction of complete state terms gi   so it represents the set of states that
corresponds to those gi s  the intuition that we apply is that
w
lit    algi i 
c m
w
vm i  
t  
  i   algi i      m
i   li   
 
l
a gl  l  c
for c   the part of c that does not affect l  the reason is that for complete terms g i we know
li
l
i
that al
gi  agi   thus  our choice of g is such that it includes all the conditions under which l
changes  if we assume the precondition algl  
we now compute the action model of a for l by updating a copy of g l   let li be a fluent literal 
and set glt   gl  
   if there is no gl   glt such that gl     li   then all of the terms in glt include li   thus 
glt  glt  li   and algl  algl li   thus  add li as a conjunct to glt  
   otherwise  if there is no gl   glt such that gl     li   then all of the terms in glt include li  
thus  glt  glt  li   and algl  algl li   thus  add li as a conjunct to glt  
   otherwise  we know that under both li and li the value of the fluent of l changes into l  
true  since we assume that the value of l changes under a single case of its preconditions
 i e   a has no conditional effects   it either succeeds with the change  or fails with the change  
then li cannot be part of those preconditions  i e   for every term g i in glt we can replace li
with li and vice versa  and the precondition would still entail l after the action  thus 
algl  algl   l     for glt    li   the result of replacing both li and li by true 
t

t

i

   not related to the literal l in the proof above 

   

fia mir   c hang

the result of these replacements is a term glt that is consistent with t  it is consistent with the
original gl   and satisfies a has   case    algl  algl    
t

w
li
t  
cl  
lemma b   let c   m
 ag
be a clause of the formula in theorem      and let g
i   li
i
w
 gi   li   l   for any literal l  assume that as effect is deterministic with only one case with a
cl is equivalent
term precondition  if that case does not hold  then nothing changes   then  either g
cl
to a term  or c is subsumed by another clause that is entailed from that formula such that there g
is equivalent to a term 
gl  

cl and not in
p roof
consider gl from lemma b    and let gl  a complete fluent term in g
l
l
l
thus  g     l  let gt the term that is equivalent to g according to lemma b   
clause c is equivalent to algl  al
      however  algl already asserts change from l to l
gl
t

t

 

in the result of action a  and al
asserts a change under a different condition from l to l  thus 
gl
 

  we get a subsuming clause c     c   al
  in this way we can
a has   case    algl  al
gl
gl
t

 

 

remove from c all the literals algi with gi not in gl  

cl   however  clause c is now
after such a process we are left with a clause c that has gl  g
not of the form of theorem     because some of the gi s are missing  how do we represent that in
the theorem  we must allow some of the gi s to be missing 
 
proof of theorem continues thus  the representation of c    the new clause  takes space o n   
 each propositional symbol is encoded with o n  space  assuming constant encoding for every
fluent p  p  
however  the number of propositional symbols is still large  there are o   n   fluent terms  so this
encoding still requires o  n n   all preconditions to all effects  new propositional symbols  now
we notice that we can limit our attention to clauses c that have at most k literals l whose action
proposition algl satisfies    gl  l  this is because if we have more than k such propositions in c 
v
w
l
say  algi l  ik    then c    ki   algi l    agk  
     im lit     which is always subsumed by
l
i
i
k  
v
l
 
the
latter
is
a
sentence
that is always true  if we assume that a can change
  ki   algi l    agk  
l
i

k  

at most k fluents  agk  
asserts that lk   remains the same  and algi l asserts that li changes in
lk  
i
one of the conditions satisfied by gli   
v
l
 which state that we have at most k effects 
using clauses of the form   ki   algi l    agk  
l
l

i

k  

l

we can resolve away agj l for j   k in every clause c of the original conjunction  thus  we are
j
w
v
left with clauses of the form c    ki   algi l    im lit     now  since our choice of literals
i
other than l         lk is independent of the rest of the clause  and we can do so for every polarity of
those fluents  we get that all of these clauses resolve  on these combinations of literals  into  and are
subsumed by 
k
 
 
lit  
   
c    algi l   
i  

i

   

ik

fil earning partially o bservable d eterministic action m odels

thus  we get a conjunction of clauses of the form      with g li  i  k  being a fluent term  so 
the conjunction of clauses in theorem     is equivalent to a conjunction of clauses such that each
clause has at most  k literals  thus  the space required to represent each clause is  kn 
finally  we use the fact that every action is dependent on at most k other fluents  every proposition that asserts no change in li is equivalent to a conjunction of literals stating that none of the
possible k preconditions that are consistent with it affect li   for example alli     lk lu implies

that alli     lk  alli     lk  lu       similarly  each one the elements in the conjunction implies

alli     lk lu    

b   proof of theorem      equivalent restricted definition of action axioms
p roof
let     p  eff  a    now we claim that for any successful actions
a  slaf  a    
 
 p    p    to see this  consider any model of     the valuation to the fluents in f p l f define a
transition relation r and the valuation to the fluents in p   define a state s   s such that hs    ri     
by the definition of     hs    ri    if and only if there exists hs  ri   such that eff  a  is satisfied  finally note that eff  a  is satisfied if and only if the preconditions of action a are met and
s  is consistent with the effects of action a applied to s  that is  eff  a  is satisfied if and only if
hs  a  s  i  r  together  these observations and corollary     yield the theorem   
b   proof of theorem      as strips slaf is correct
p roof
let the shorthand notation c   denote c    cnl    p    p   
by definition  slaf  ha  oi     slaf  o  slaf  a      from theorem      we have
slaf  a     c   eff  a    a formula equivalent to c   eff  a   may be generated by
resolving
v out all fluents from p  by following the procedure from the proof of lemma       suppose
   f p f is in fluent factored form  then we may rewrite c   eff  a   as 
 



slaf  a     










 

f p

 

f p

 

f p

 

f p



c prea f    c effa f    c f   

   



c prea f  effa f   


c f  prea f   


c f  effa f  

this equivalence holds because all resolvents generated by resolving out literals from p in c  
eff  a   will still be generated in the above formula  that is  each pair of clauses that can be possibly resolved together  where a fluent from p is resolved out  in    eff  a  to generate a new
consequence in c    eff  a   will appear together in one of the c    components of      because
   

fia mir   c hang

every clause in   eff  a  contains at most one literal from p  we see that all possible consequences
will be generated 
now we note that effa f may be rewritten as follows 
effa f



 

  al   af   l    l      l    al   af   l   

   

l f f  



 

 l   l   al      l    al  af    

l f f  

it is straightforward to verify the equivalence of the rewritten formula to the original formula  note
that in performing this rewriting  we may discard clauses of the form a f  af  af    as they must
be true in every consistent model  given the axioms described in section      
now consider the consequences that are generated by each component of      we may compute
these consequences by performing resolution  we have that c pre a f    a f    a f     but we
may discard these clauses because only inconsistent action models will violate this clause  by
the definition of fluent factored formulas  c f    af   next  the remaining components can be
computed straightforwardly 
 
c effa f   
 l   al  af   
l f f  

c f  prea f    c f    c prea f   

 

 a l   expll  

l f f  

c prea f  effa f    c prea f    c effa f   

 

 l   al  a l   

l f f  

c f  effa f    c f    c effa f   

 

 l   al  expll  

l f f  

finally  it is not difficult to see that in steps  a   c  the procedure sets  to the following formula  in
fluent factored form  
 
 
slaf  a    
af 
 a l   expll     l   al   af   a l   expll  
f p

l f f  

now  note that slaf  o  slaf  a      slaf  a     o  note that because o is a term 
then slaf  a     o can be made fluent factored by performing unit resolution  this is exactly
what steps    d   e  do   
b   proof of theorem      as strips slaf complexity
p roof
consider the size of the formula returned by as strips slaf  overview  we note
that if a formula is in i cnf  for any integer i      then the filtered formula after one step is in
 i      cnf  then  we note that every observation of fluent f resets the f  part of the belief state
formula to   cnf  thus  to i      
further details  for the first part  this is because each call to the procedure appends at most
one literal to any existing clauses of the formula  and no new clauses of length more than k     are
   

fil earning partially o bservable d eterministic action m odels

generated  additionally  if every fluent is observed every at most k steps  then the transition belief
formula stays in k cnf  i e   indefinitely compact   this is because existing clauses may only grow
in length    literal per timestep  when augmented in steps    a   c   but when the appropriate fluent
is observed in steps    d   e   the clauses stop growing  finally  it is easy to see that each of the
steps    a   e  can be performed in polynomial time   
b   proof of theorem      pre strips slaf is correct
p roof
consider the semantics of slaf when filtering on a strips action with a known precondition  in the case of an action failure  a world is in the filtered transition belief state if and only
if the world did not meet the action precondition  and satisfies the observation   clearly  step   of
the algorithm performs this filtering by conjoining the belief formula with the negation of the action
precondition  converted into a logically equivalent disjunction of fluent factored formulas  
in the case of an action success  filtering can be performed by first removing worlds which
do not satisfy the action precondition  so in all remaining worlds  the action is executable  and
then filtering the remaining worlds using algorithm as strips slaf  moreover  by theorem    
and corollary     it follows that filtering the formula  can be performed by filtering each of the
subformulas i j separately  furthermore  slaf ha  oi       pre strips slaf ha  oi     and
pre strips slaf ha  oi     slaf ha  oi    if any of the conditions of corollary      the
filtering of each subformula is performed by steps   and   of the algorithm 
finally  note that steps   and   serve only to simplify the belief formula and produce a logically
equivalent formula   
b    proof of theorem      pre strips slaf complexity
p roof
note that each call to as strips slaf on a subformula takes time linear in the size
of the subformula  and the steps not involving as strips slaf can be performed in linear time 
thus the the total time complexity is linear  additionally  note that if every fluent is observed every
at most k steps  then every fluent factored subformula i j of the belief formula is in k cnf  by a
theorem of amir
w         if action preconditions contain at most m literals  then each disjunction
of the form j i j contains at most m disjuncts  therefore  the entire belief formula stays in
m  k cnf  indefinitely   

appendix c  experiments and their outputs
our experiments  section    examine properties of our algorithms for learning action models  they
show that learning is tractable and exact  in this appendix section we bring the generating models
and the learned models for more detailed comparison by the reader  recall that our algorithms
output a representation for the set of models that are possible given the input  below we bring only
one satisfying model from the learned formula 
our experiments include the following domains from the international planning competition
 ipc   drivelog  zenotravel  blocksworld  and depots 
c   driverlog domain
the driverlog domain has the following generating pddl 
 define  domain driverlog 

   

fia mir   c hang

  requirements  typing 
  types
location locatable   object
driver truck obj   locatable  
  predicates
 at  obj   locatable  loc   location 
 in  obj    obj  obj   truck 
 driving  d   driver  v   truck 
 path  x  y   location 
 empty  v   truck   
  action load truck
 parameters
  obj   obj
 truck   truck
 loc   location 
 precondition
 and  at  truck  loc   at  obj  loc  
 effect
 and  not  at  obj  loc    in  obj  truck   
  action unload truck
 parameters
  obj   obj
 truck   truck
 loc   location 
 precondition
 and  at  truck  loc   in  obj  truck  
 effect
 and  not  in  obj  truck    at  obj  loc   
  action board truck
 parameters
  driver   driver
 truck   truck
 loc   location 
 precondition
 and  at  truck  loc   at  driver  loc   empty  truck  
 effect
 and  not  at  driver  loc    driving  driver  truck 
 not  empty  truck    
  action disembark truck
 parameters
  driver   driver
 truck   truck
 loc   location 
 precondition
 and  at  truck  loc   driving  driver  truck  
 effect
 and  not  driving  driver  truck    at  driver  loc 
 empty  truck   
  action drive truck
 parameters
  truck   truck
 loc from   location
 loc to location
 driver   driver 
 precondition
 and  at  truck  loc from 
 driving  driver  truck 
 path  loc from  loc to  
 effect
 and  not  at  truck  loc from    at  truck  loc to   
  action walk
 parameters
  driver   driver
 loc from   location
 loc to   location 
 precondition
 and  at  driver  loc from   path  loc from  loc to  
 effect
 and  not  at  driver  loc from    at  driver  loc to     

one learned model  one possible satisfying model of our formula  from our random sequence
input in this driverlog domain is the following  brought together with the experimental parameters  
driverlog domain 
  ipc  problem   
      fluents
       randomly selected actions
     fluents observed per step

   

fil earning partially o bservable d eterministic action m odels

   schematized  learning
      precondition heuristics
  action distribution 
  board truck        drive truck        disembark truck      
 walk         unload truck         load truck        
converting to cnf
clause count       
variable count     
adding clauses
calling zchaff
parsing result
slaf time       
inference time       
learned model 
 walk needs  at  driver  loc from  
 walk needs  not  at  driver  loc to   
 walk causes  at  driver  loc to  
 walk causes  not  at  driver  loc from   
 walk keeps  path  loc from  loc from  
 walk keeps  path  loc to  loc to  
 walk keeps  path  loc to  loc from  
 walk keeps  path  loc from  loc to  
 drive truck needs  not  at  truck  loc to   
 drive truck needs  at  truck  loc from  
 drive truck causes  at  truck  loc to  
 drive truck causes  not  at  truck  loc from   
 drive truck keeps  at  driver  loc to  
 drive truck keeps  at  driver  loc from  
 drive truck keeps  driving  driver  truck  
 drive truck keeps  path  loc to  loc to  
 drive truck keeps  path  loc from  loc from  
 drive truck keeps  path  loc from  loc to  
 drive truck keeps  path  loc to  loc from  
 drive truck keeps  empty  truck  
 disembark truck needs  not  at  driver  loc   
 disembark truck needs  driving  driver  truck  
 disembark truck needs  not  empty  truck   
 disembark truck causes  at  driver  loc  
 disembark truck causes  not  driving  driver  truck   
 disembark truck causes  empty  truck  
 disembark truck keeps  at  truck  loc  
 disembark truck keeps  path  loc  loc  
 board truck needs  at  driver  loc  
 board truck needs  not  driving  driver  truck   
 board truck needs  empty  truck  
 board truck causes  not  at  driver  loc   
 board truck causes  driving  driver  truck  
 board truck causes  not  empty  truck   
 board truck keeps  at  truck  loc  
 board truck keeps  path  loc  loc  
 unload truck needs  not  at  obj  loc   
 unload truck needs  in  obj  truck  
 unload truck causes  at  obj  loc  

   

fia mir   c hang

 unload truck causes  not  in  obj  truck   
 unload truck keeps  at  truck  loc  
 unload truck keeps  path  loc  loc  
 unload truck keeps  empty  truck  
 load truck needs  at  obj  loc  
 load truck needs  not  in  obj  truck   
 load truck causes  not  at  obj  loc   
 load truck causes  in  obj  truck  
 load truck keeps  at  truck  loc  
 load truck keeps  path  loc  loc  
 load truck keeps  empty  truck  

c   zeno travel domain
the zeno travel domain has the following generating pddl 
 define  domain zeno travel 
  requirements  typing 
  types aircraft person city flevel   object 
  predicates  at  x    either person aircraft   c   city 
 in  p   person  a   aircraft 
 fuel level  a   aircraft  l   flevel 
 next  l   l    flevel  
  action board
 parameters   p   person  a   aircraft  c   city 
 precondition  and  at  p  c   at  a  c  
 effect  and  not  at  p  c    in  p  a   
  action debark
 parameters   p   person  a   aircraft  c   city 
 precondition  and  in  p  a   at  a  c  
 effect  and  not  in  p  a    at  p  c   
  action fly
 parameters   a   aircraft  c   c    city  l   l    flevel 
 precondition  and  at  a  c    fuel level  a  l    next  l   l   
 effect  and  not  at  a  c     at  a  c    not  fuel level  a  l   
 fuel level  a  l    
  action zoom
 parameters   a   aircraft  c   c    city  l   l   l    flevel 
 precondition  and  at  a  c    fuel level  a  l    next  l   l  
 next  l   l    
 effect  and  not  at  a  c     at  a  c    not  fuel level  a  l   
 fuel level  a  l     
  action refuel
 parameters   a   aircraft  c   city  l   flevel  l    flevel 
 precondition  and  fuel level  a  l   next  l  l    at  a  c  
 effect  and  fuel level  a  l    not  fuel level  a  l     

one learned model  one possible satisfying model of our formula  from our random sequence
input in this zeno travel domain is the following  brought together with the experimental parameters  
zenotravel domain 

   

fil earning partially o bservable d eterministic action m odels

 
 
 
 
 
 
 

ipc  problem  
   fluents        possible unique actions
     actions in learned action sequence
  observed fluents per step
 schematized  learning
    precondition heuristics
action distribution    zoom        fly         refuel       
 board         debark        

converting to cnf
clause count       
variable count     
adding clauses
calling zchaff
parsing result
slaf time       
inference time        
learned model 
 refuel needs  fuel level  a  l  
 refuel needs  not  fuel level  a  l    
 refuel causes  not  fuel level  a  l   
 refuel causes  fuel level  a  l   
 refuel keeps  next  l  l  
 refuel keeps  next  l  l   
 refuel keeps  next  l   l  
 refuel keeps  next  l   l   
 zoom needs  not  fuel level  a  l    
 zoom needs  fuel level  a  l   
 zoom causes  fuel level  a  l   
 zoom causes  not  fuel level  a  l    
 zoom keeps  fuel level  a  l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 zoom keeps  next  l   l   
 fly needs  not  fuel level  a  l    
 fly needs  fuel level  a  l   
 fly causes  fuel level  a  l   
 fly causes  not  fuel level  a  l    
 fly keeps  next  l   l   
 fly keeps  next  l   l   
 fly keeps  next  l   l   
 fly keeps  next  l   l   
 debark needs  in  p  a  
 debark causes  not  in  p  a   
 board needs  not  in  p  a   
 board causes  in  p  a  

   

fia mir   c hang

c   blocks world domain
the blocksworld domain has the following generating pddl 
 define  domain blocksworld 
  requirements  strips 
  predicates  clear  x   object 
 on table  x   object 
 arm empty 
 holding  x   object 
 on  x  y   object  
  action pickup
 parameters   ob   object 
 precondition  and  clear  ob   on table  ob   arm empty  
 effect  and  holding  ob   not  clear  ob    not  on table  ob  
 not  arm empty    
  action putdown
 parameters   ob   object 
 precondition  holding  ob 
 effect  and  clear  ob   arm empty   on table  ob 
 not  holding  ob    
  action stack
 parameters   ob   object
 underob   object 
 precondition  and  clear  underob   holding  ob  
 effect  and  arm empty   clear  ob   on  ob  underob 
 not  clear  underob    not  holding  ob    
  action unstack
 parameters   ob   object
 underob   object 
 precondition  and  on  ob  underob   clear  ob   arm empty  
 effect  and  holding  ob   clear  underob   not  on  ob  underob  
 not  clear  ob    not  arm empty     

one learned model  one possible satisfying model of our formula  from our random sequence
input in this blocksworld domain is the following  brought together with the experimental parameters  
blocksworld domain 
      fluents
       randomly selected actions
     fluents observed per step
   schematized  learning
      precondition heuristics
converting to cnf
clause count        
variable count     
adding clauses
calling zchaff
parsing result
slaf time       
inference time        

   

fil earning partially o bservable d eterministic action m odels

learned model 
 unstack needs  not  clear  underob   
 unstack needs  clear  ob  
 unstack needs  arm empty  
 unstack needs  not  holding  ob   
 unstack needs  on  ob  underob  
 unstack causes  clear  underob  
 unstack causes  not  clear  ob   
 unstack causes  not  arm empty   
 unstack causes  holding  ob  
 unstack causes  not  on  ob  underob   
 unstack keeps  on table  underob  
 unstack keeps  on table  ob  
 unstack keeps  holding  underob  
 unstack keeps  on  underob  underob  
 unstack keeps  on  ob  ob  
 unstack keeps  on  underob  ob  
 stack needs  clear  underob  
 stack needs  not  clear  ob   
 stack needs  not  arm empty   
 stack needs  holding  ob  
 stack needs  not  on  ob  underob   
 stack causes  not  clear  underob   
 stack causes  clear  ob  
 stack causes  arm empty  
 stack causes  not  holding  ob   
 stack causes  on  ob  underob  
 stack keeps  on table  underob  
 stack keeps  on table  ob  
 stack keeps  holding  underob  
 stack keeps  on  underob  underob  
 stack keeps  on  ob  ob  
 stack keeps  on  underob  ob  
 putdown needs  not  clear  ob   
 putdown needs  not  on table  ob   
 putdown needs  not  arm empty   
 putdown needs  holding  ob  
 putdown causes  clear  ob  
 putdown causes  on table  ob  
 putdown causes  arm empty  
 putdown causes  not  holding  ob   
 putdown keeps  on  ob  ob  
 pickup needs  clear  ob  
 pickup needs  on table  ob  
 pickup needs  arm empty  
 pickup needs  not  holding  ob   
 pickup causes  not  clear  ob   
 pickup causes  not  on table  ob   
 pickup causes  not  arm empty   
 pickup causes  holding  ob  
 pickup keeps  on  ob  ob  

   

fia mir   c hang

c   depot domain
the depot domain has the following generating pddl 
 define  domain depot 
  requirements  typing 
  types place locatable   object
depot distributor   place
truck hoist surface   locatable
pallet crate   surface 
  predicates  at  x   locatable  y   place 
 on  x   crate  y   surface 
 in  x   crate  y   truck 
 lifting  x   hoist  y   crate 
 available  x   hoist 
 clear  x   surface  
  action drive
 parameters   x   truck  y   place  z   place 
 precondition  and  at  x  y  
 effect  and  not  at  x  y    at  x  z   
  action lift
 parameters   x   hoist  y   crate  z   surface  p   place 
 precondition  and  at  x  p   available  x   at  y  p   on  y  z 
 clear  y  
 effect  and  not  at  y  p    lifting  x  y   not  clear  y  
 not  available  x    clear  z   not  on  y  z    
  action drop
 parameters   x   hoist  y   crate  z   surface  p   place 
 precondition  and  at  x  p   at  z  p   clear  z   lifting  x  y  
 effect  and  available  x   not  lifting  x  y    at  y  p 
 not  clear  z    clear  y   on  y  z   
  action load
 parameters   x   hoist  y   crate  z   truck  p   place 
 precondition  and  at  x  p   at  z  p   lifting  x  y  
 effect  and  not  lifting  x  y    in  y  z   available  x   
  action unload
 parameters   x   hoist  y   crate  z   truck  p   place 
 precondition  and  at  x  p   at  z  p   available  x   in  y  z  
 effect  and  not  in  y  z    not  available  x    lifting  x  y     

one learned model  one possible satisfying model of our formula  from our random sequence
input in this depot domain is the following  brought together with the experimental parameters  
depots domain 
  ipc  problem  
      fluents
       randomly selected actions
     fluents observed per step
   schematized  learning
      precondition heuristics
converting to cnf

   

fil earning partially o bservable d eterministic action m odels

clause count       
variable count     
adding clauses
calling zchaff
parsing result
slaf time       
inference time       
learned model 
 unload needs  in  y  z  
 unload needs  not  lifting  x  y   
 unload needs  available  x  
 unload causes  not  in  y  z   
 unload causes  lifting  x  y  
 unload causes  not  available  x   
 unload keeps  at  z  p  
 unload keeps  at  y  p  
 unload keeps  at  x  p  
 unload keeps  on  y  y  
 unload keeps  clear  y  
 load needs  not  in  y  z   
 load needs  lifting  x  y  
 load needs  not  available  x   
 load causes  in  y  z  
 load causes  not  lifting  x  y   
 load causes  available  x  
 load keeps  at  z  p  
 load keeps  at  y  p  
 load keeps  at  x  p  
 load keeps  on  y  y  
 load keeps  clear  y  
 drop needs  not  at  y  p   
 drop needs  not  on  y  z   
 drop needs  lifting  x  y  
 drop needs  not  available  x   
 drop needs  clear  z  
 drop needs  not  clear  y   
 drop causes  at  y  p  
 drop causes  on  z  z  
 drop causes  not  on  z  z   
 drop causes  on  z  y  
 drop causes  not  on  z  y   
 drop causes  on  y  z  
 drop causes  not  lifting  x  y   
 drop causes  lifting  x  z  
 drop causes  not  lifting  x  z   
 drop causes  available  x  
 drop causes  not  clear  z   
 drop causes  clear  y  
 drop keeps  at  z  p  
 drop keeps  at  x  p  
 drop keeps  on  z  z  
 drop keeps  on  z  y  
 drop keeps  on  y  y  
 drop keeps  lifting  x  z  

   

fia mir   c hang

 lift needs  at  y  p  
 lift needs  on  y  z  
 lift needs  not  lifting  x  y   
 lift needs  available  x  
 lift needs  not  clear  z   
 lift needs  clear  y  
 lift causes  not  at  y  p   
 lift causes  not  on  y  z   
 lift causes  on  z  z  
 lift causes  not  on  z  z   
 lift causes  on  z  y  
 lift causes  not  on  z  y   
 lift causes  lifting  x  y  
 lift causes  lifting  x  z  
 lift causes  not  lifting  x  z   
 lift causes  not  available  x   
 lift causes  clear  z  
 lift causes  not  clear  y   
 lift keeps  at  z  p  
 lift keeps  at  x  p  
 lift keeps  on  y  y  
 lift keeps  on  z  z  
 lift keeps  on  z  y  
 lift keeps  lifting  x  z  
 drive needs  at  x  y  
 drive needs  not  at  x  z   
 drive causes  not  at  x  y   
 drive causes  at  x  z  

references
amir  e          learning partially observable deterministic action models  in proc  nineteenth
international joint conference on artificial intelligence  ijcai      pp            international joint conferences on artificial intelligence 
amir  e     russell  s          logical filtering  in proc  eighteenth international joint conference
on artificial intelligence  ijcai      pp        morgan kaufmann 
benson  s          inductive learning of reactive action models  in proceedings of the   th international conference on machine learning  icml     
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
boutilier  c   reiter  r     price  b          symbolic dynamic programming for first order mdps 
in proc  seventeenth international joint conference on artificial intelligence  ijcai      pp 
        morgan kaufmann 
boyen  x   friedman  n     koller  d          discovering the hidden structure of complex dynamic
systems  in proceedings of the   th conference on uncertainty in artificial intelligenceuai
      pp         morgan kaufmann  available at http   www cs stanford edu  xb uai    
boyen  x     koller  d          approximate learning of dynamic models  in kearns  m  s   solla 
s  a     kohn  d  a   eds    advances in neural information processing systems     proceedings of the      conferencenips       pp          cambridge  mit press  available at http   www cs stanford edu  xb nips    
   

fil earning partially o bservable d eterministic action m odels

calvanese  d   giacomo  g  d     vardi  m  y          reasoning about actions and planning in
ltl action theories  in principles of knowledge representation and reasoning  proc  eighth
intl conference  kr        pp          morgan kaufmann 
chang  a     amir  e          goal achievement in partially known  partially observable domains 
in proceedings of the   th intl conf  on automated planning and scheduling  icaps    
aaai press 
chang  c  l     lee  r  c  t          symbolic logic and mechanical theorem proving  academic
press 
clarke  e  m   grumberg  o     peled  d  a          model checking  mit press 
darwiche  a     marquis  p          a knowledge compilation map  journal of artificial intelligence research             
davis  m     putnam  h          a computing procedure for quantification theory  journal of the
acm            
dawsey  w   minsker  b     amir  e          real time assessment of drinking water systems using
bayesian networks  in world environmental and water resources congress 
dechter  r          bucket elimination  a unifying framework for reasoning  artificial intelligence 
              
del val  a          a new method for consequence finding and compilation in restricted language  in proc  national conference on artificial intelligence  aaai      pp         
aaai press mit press 
doherty  p   lukaszewicz  w     szalas  a          computing circumscription revisited  a reduction algorithm  journal of automated reasoning                
eiter  t     gottlob  g          on the complexity of propositional knowledge base revision  updates  and counterfactuals  artificial intelligence                  
even dar  e   kakade  s  m     mansour  y          reinforcement learning in pomdps  in
proc  nineteenth international joint conference on artificial intelligence  ijcai      pp 
        international joint conferences on artificial intelligence 
fagin  r   ullman  j  d     vardi  m  y          on the semantics of updates in databases  in
proceedings of the second acm sigact sigmod symposium on principles of database
systems  pp          atlanta  georgia 
fikes  r   hart  p     nilsson  n          learning and executing generalized robot plans  artificial
intelligence            
fox  m     long  d          pddl     an extension to pddl for expressing temporal planning
domains  http   www dur ac uk d p long ipc pddl html  used in aips   competition 
friedman  n   murphy  k     russell  s          learning the structure of dynamic probabilistic
networks  in proc  fourteenth conference on uncertainty in artificial intelligence  uai     
morgan kaufmann 
gelfond  m     lifschitz  v          action languages  electronic transactions on artificial intelligence  http   www etaij org      nr    
   

fia mir   c hang

ghahramani  z          an introduction to hidden markov models and bayesian networks  international journal of pattern recognition and artificial intelligence             
ghahramani  z     jordan  m  i          factorial hidden markov models  machine learning     
       
ghallab  m   howe  a   knoblock  c   mcdermott  d   ram  a   veloso  m   weld  d     wilkins 
d          pddl  the planning domain definition language  version      tech  rep  cvc
tr        dcs tr       yale center for computational vision and control 
gil  y          learning by experimentation  incremental refinement of incomplete planning domains  in proceedings of the   th international conference on machine learning  icml     
pp       
ginsberg  m  l          readings in nonmonotonic reasoning  chap     pp       morgan kaufmann  los altos  ca 
hajishirzi  h     amir  e          stochastic filtering in probabilistic action models  in proc  national conference on artificial intelligence  aaai     
hill  d  j   minsker  b     amir  e          real time bayesian anomaly detection for environmental
sensor data  in   nd congress of the international association of hydraulic engineering and
research  iahr       
hlubocky  b     amir  e          knowledge gathering agents in adventure games  in aaai   
workshop on challenges in game ai  aaai press 
holmes  m  p     charles lee isbell  j          looping suffix tree based inference of partially
observable hidden state  in proceedings of the   rd international conference on machine
learning  icml      pp          acm press 
iwanuma  k     inoue  k          minimal answer computation and sol  in logics in artificial
intelligence  proceedings of the eighth european conference  vol       of lnai  pp     
     springer verlag 
james  m     singh  s          learning and discovery of predictive state representations in dynamical systems with reset  in proceedings of the   st international conference on machine
learning  icml      pp          acm press 
kaelbling  l  p   littman  m  l     cassandra  a  r          planning and acting in partially
observable stochastic domains  artificial intelligence             
kearns  m   mansour  y     ng  a  y          approximate planning in large pomdps via reusable
trajectories  in proceedings of the   th conference on neural information processing systems
 nips     published       pp            mit press 
kuffner   j  j     lavalle  s  m          rrt connect  an efficient approach to single query path
planning   in ieee international conference on robotics and automation  icra   pp     
     
lee  r  c  t          a completeness theorem and a computer program for finding theorems
derivable from given axioms  ph d  thesis  university of california  berkeley 
lifschitz  v          on the semantics of strips  in allen  j  f   hendler  j     tate  a   eds   
readings in planning  pp          morgan kaufmann  san mateo  california 
   

fil earning partially o bservable d eterministic action m odels

littman  m  l          algorithms for sequential decision making  ph d  thesis  department of
computer science  brown university  technical report cs       
littman  m  l   sutton  r     singh  s          predictive representations of state  in proceedings of
the   th conference on neural information processing systems  nips     published      
mit press 
marquis  p          consequence finding algorithms  in gabbay  d     smets  p   eds    handbook of defeasible reasoning and uncertainty management systems  vol     algorithms for
defeasible and uncertain reasoning  kluwer 
mccallum  r  a          instance based utile distinctions for reinforcement learning with hidden
state  in proceedings of the   th international conference on machine learning  icml     
morgan kaufmann 
mccarthy  j          applications of circumscription to formalizing common sense knowledge 
artificial intelligence            
mccarthy  j     hayes  p  j          some philosophical problems from the standpoint of artificial intelligence  in meltzer  b     michie  d   eds    machine intelligence    pp         
edinburgh university press 
mcilraith  s     amir  e          theorem proving with structured theories  in proc  seventeenth
international joint conference on artificial intelligence  ijcai      pp          morgan
kaufmann 
meuleau  n   peshkin  l   kim  k  e     kaelbling  l  p          learning finite state controllers for
partially observable environments  in proc  fifteenth conference on uncertainty in artificial
intelligence  uai      morgan kaufmann 
moskewicz  m  w   madigan  c  f   zhao  y   zhang  l     malik  s          chaff  engineering an
efficient sat solver  in proceedings of the   th design automation conference  dac    
murphy  k          dynamic bayesian networks  representation  inference and learning  ph d 
thesis  university of california at berkeley 
nance  m   vogel  a     amir  e          reasoning about partially observed actions  in proc  national conference on artificial intelligence  aaai      aaai press 
oates  t     cohen  p  r          searching for planning operators with context dependent and
probabilistic effects  in proc  national conference on artificial intelligence  aaai      pp 
        aaai press 
pasula  h  m   zettlemoyer  l  s     kaelbling  l  p          learning probabilistic relational
planning rules  in proceedings of the   th intl conf  on automated planning and scheduling
 icaps     aaai press 
pednault  e  p  d          adl  exploring the middle ground between strips and the situation
calculus  in proc  first international conference on principles of knowledge representation
and reasoning  kr      pp         
reiter  r          knowledge in action  logical foundations for describing and implementing
dynamical systems  mit press 
   

fia mir   c hang

reiter  r          the frame problem in the situation calculus  a simple solution  sometimes  and
a completeness result for goal regression  in lifschitz  v   ed    artificial intelligence and
mathematical theory of computation  papers in honor of john mccarthy   pp         
academic press 
robert  c  p   celeux  g     diebolt  j          bayesian estimation of hidden markov chains  a
stochastic implementation  statist  prob  letters           
schmill  m  d   oates  t     cohen  p  r          learning planning operators in real world  partially observable environments  in proceedings of the  th intl conf  on ai planning and
scheduling  aips     pp          aaai press 
shahaf  d     amir  e          learning partially observable action schemas  in proc  national
conference on artificial intelligence  aaai      aaai press 
shahaf  d     amir  e          logical circuit filtering  in proc  twentieth international joint conference on artificial intelligence  ijcai      pp            international joint conferences
on artificial intelligence 
shahaf  d   chang  a     amir  e          learning partially observable action models  efficient
algorithms  in proc  national conference on artificial intelligence  aaai      aaai press 
simon  l     del val  a          efficient consequence finding  in proc  seventeenth international
joint conference on artificial intelligence  ijcai      pp          morgan kaufmann 
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
tang  d   yinlei yu  d  r     malik  s          analysis of search based algorithms for satisfiability of quantified boolean formulas arising from circuit state space diameter problems  in
proceedings of the seventh international conference on theory and applications of satisfiability testing  sat      
thielscher  m          introduction to the fluent calculus  electronic transactions on artificial
intelligence  http   www etaij org      nr    
thrun  s          robotic mapping  a survey  in exploring artificial intelligence in the new millennium  pp       morgan kaufmann 
wang  x          learning by observation and practice  an incremental approach for planning operator acquisition  in proceedings of the   th international conference on machine learning
 icml      pp          morgan kaufmann 
wu  k   yang  q     jiang  y          arms  an automatic knowledge engineering tool for learning
action models for ai planning  the knowledge engineering review                
yang  q   wu  k     jiang  y          learning actions models from plan examples with incomplete
knowledge   in biundo  s   myers  k  l     rajan  k   eds    icaps  pp          aaai 

   

fi