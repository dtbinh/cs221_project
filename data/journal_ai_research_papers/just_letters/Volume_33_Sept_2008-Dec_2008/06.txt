journal of artificial intelligence research                 

submitted        published      

learning to reach agreement in a continuous ultimatum game
steven de jong
simon uyttendaele

steven   dejong   micc   unimaas   nl

micc  maastricht university
p o  box           md maastricht  the netherlands

karl tuyls

k   p  tuyls   tue   nl

eindhoven university of technology
p o  box           mb eindhoven  the netherlands

abstract
it is well known that acting in an individually rational manner  according to the principles of classical game theory  may lead to sub optimal solutions in a class of problems named social dilemmas 
in contrast  humans generally do not have much difficulty with social dilemmas  as they are able
to balance personal benefit and group benefit  as agents in multi agent systems are regularly confronted with social dilemmas  for instance in tasks such as resource allocation  these agents may
benefit from the inclusion of mechanisms thought to facilitate human fairness  although many of
such mechanisms have already been implemented in a multi agent systems context  their application is usually limited to rather abstract social dilemmas with a discrete set of available strategies
 usually two   given that many real world examples of social dilemmas are actually continuous in
nature  we extend this previous work to more general dilemmas  in which agents operate in a continuous strategy space  the social dilemma under study here is the well known ultimatum game  in
which an optimal solution is achieved if agents agree on a common strategy  we investigate whether
a scale free interaction network facilitates agents to reach agreement  especially in the presence of
fixed strategy agents that represent a desired  e g  human  outcome  moreover  we study the influence of rewiring in the interaction network  the agents are equipped with continuous action
learning automata and play a large number of random pairwise games in order to establish a common strategy  from our experiments  we may conclude that results obtained in discrete strategy
games can be generalized to continuous strategy games to a certain extent  a scale free interaction network structure allows agents to achieve agreement on a common strategy  and rewiring in
the interaction network greatly enhances the agents ability to reach agreement  however  it also
becomes clear that some alternative mechanisms  such as reputation and volunteering  have many
subtleties involved and do not have convincing beneficial effects in the continuous case 

   introduction
sharing limited resources with others is a common challenge for individuals in human societies as
well as for agents in multi agent systems  chevaleyre et al          often  there is a conflict of
interest between personal benefit and group  or social  benefit  this conflict is most prominently
present in a class of problems named social dilemmas  in which individuals need to consider not
only their personal benefit  but also the effects of their choices on others  as failure to do so may
lead to sub optimal solutions  in such dilemmas  classical game theory  which assumes players or
agents to be completely individually rational in strategic circumstances  seems to be of limited value
 gintis        maynard smith         as individually rational players are not socially conditioned 
humans on the other hand generally show a remarkable ability to address social dilemmas  due
c
    
ai access foundation  all rights reserved 

fid e j ong   u yttendaele   t uyls

to their tendency to consider concepts such as fairness in addition to personal benefit  see  e g 
dannenberg et al         fehr   schmidt        gintis        oosterbeek et al         
a prime example of a social dilemma is modeled in the well known ultimatum game  gueth
et al           in this game  two agents bargain about the division of an amount r  obtained from
an outsider  the first agent proposes an offer r  to the second agent  e g  you receive    of the
      if the second agent accepts  each agent gets his share  i e  the first agent receives r  r    and
the second receives r     however  if the second agent rejects  both agents are left with nothing  an
individually rational first agent would offer the smallest amount possible  knowing that the second
agent can then choose between obtaining this amount by accepting  or nothing by rejecting  thus 
accepting the smallest amount possible is the individually rational response  in contrast  human
players of the game hardly  if ever  offer less than about      and if such an offer occurs  it is not
likely to be accepted  bearden        oosterbeek et al          thus  an individually rational player
that plays as a proposer against a human player will probably not gain any money 
researchers have proposed various mechanisms that may be responsible for the emergence of
fair strategies in human populations playing social dilemmas  as well as the resistance of these
strategies against invasion by individually rational strategies  see  e g  fehr   schmidt        gintis        nowak et al         santos et al       a   often  these mechanisms have been implemented in multi agent systems for validation purposes  i e  if the agents can be shown to prefer
fair strategies over individually rational ones  this makes it more plausible that humans are actually
affected by the underlying mechanisms  however  we argue that multi agent systems driven by fairness mechanisms may not only be used to validate these mechanisms  but also to allow agents to act
in a fair way in real world applications  given that agents often face tasks such as resource sharing
and allocation  if not explicitly  then implicitly  e g  sharing limited computational resources   and
that these tasks regularly contain elements of social dilemmas  it is important to enable agents to
act not only based on individual rationality  but also based on fairness  chevaleyre et al         
unfortunately  existing work usually introduces a number of abstractions that do not allow the resulting multi agent systems to be applied to realistic problems such as resource allocation  most
prominently  most work has focused on social dilemmas with discrete strategy sets  usually limited
to two    this abstraction simplifies the dilemmas at hand and does not reflect their potential realworld nature  since in many dilemmas  especially those related to real world resource allocation 
there is a continuum of strategies  i e  a continuous strategy space  rather than a discrete set of
pure strategies  moreover  in social dilemmas with continuous strategy spaces  qualifications such
as cooperation and defection  which are often used in discrete social dilemmas  are actually relative  a certain strategy may be seen as cooperative  i e  desirable  in a certain context  whereas
it may be either defective or simply naive in another one  it is clear that the dilemma may be far
more complicated in continuous strategy spaces  and that agents may need to use a different way of
determining whether their behavior is desirable 
   the analogy between the ultimatum game and other social dilemmas  such as the public goods game  can be shown
with full mathematical rigor  sigmund et al          de jong   tuyls        report preliminary results of applying
the methodology described in this paper to the public goods game 
   theoretical work in the field of evolutionary game theory occasionally is not limited to discrete strategy sets  worth
mentioning here is the work of peters         which introduces a theoretical extension of the evolutionary stable
strategy concept  ess  for continuous strategy spaces  i e  the extended stability calculus  this extension provides a
theoretical solution concept that can clarify egalitarian outcomes  however  the concept does not shed light on how
learning agents possibly achieve these fair outcomes in social dilemmas  the work is therefore complementary to
this work  which aims at mechanisms enabling agents to find fair outcomes 

   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

in this paper  we generalize existing work on achieving agreement  cooperation and fairness
in social dilemmas to continuous strategy spaces  with the aim of presenting a methodology that
allows agents to reach satisfactory outcomes in these dilemmas  as well as in real world problems
containing elements of these dilemmas  we apply our proposed methodology to the ultimatum
game  an example of a social dilemma  in this game  a population of agents needs to reach agreement  i e  converge to a common strategy  in order to obtain a satisfactory payoff  this agreement
then specifies the populations cooperative  desirable strategy  the precise nature of the agreement
may vary   in our population of agents  any strategy that is agreed upon by a sufficient number of
agents will be successful  and will dictate the culture  or the desirable strategy  of the population 
if we wish our agents to learn a human strategy  we may introduce a number of simulated human
agents  i e  agents that play according to our own desirable strategy  the learning agents should
then be able to imitate this strategy  even if they already reached agreement on a different  possibly
relatively defective  strategy 
the remainder of this paper is structured as follows  first  in    we give a brief overview of the
related work that this paper aims to continue  next  in    we discuss our methodology  aimed at
establishing agreement in large populations of learning agents  in    we present experiments and
results  in    we outline a number of alternative approaches that were proposed for dilemmas with
discrete strategy sets  but fail to impress in a dilemma with a continuous strategy space  we discuss
why this is the case  finally  we conclude this paper in   

   related work
this work basically builds upon two tracks of existing work  we will give an overview of these
tracks here and indicate how they are related to our current work  for a more extensive discussion 
we refer to previous work  de jong et al       b  
    learning fairness in bargaining
de jong et al       a  investigated the behavior of agents playing the ultimatum game and the
nash bargaining game with continuous action learning automata  in both games  all agents were
interacting at the same time  for the ultimatum game  this required an extension to more than two
players in which agents  one after the other  demanded a portion of the reward at hand  the last
player received what was left  the homo egualis utility function  as developed by fehr   schmidt
       and gintis         was used to represent a desired outcome  i e  a required minimal amount
every agent wished to obtain  up to     agents were able to successfully find and maintain agreements in both games  in addition  it was observed that the solutions agreed upon corresponded to
solutions agreed upon by humans  as reported in literature  in this work  we similarly use continuous action learning automata to learn agreement in the ultimatum game  however  our multi agent
system  organized in a network structure  can efficiently be populated with a much larger number of
agents  e g  thousands   in contrast to our previous work  agents play pairwise games  moreover 
   note the analogy with humans  where cultural background is one of the primary influences on what constitutes a fair 
cooperative  or desirable strategy  although there is a general tendency to deviate from pure individual rationality in
favor of more socially aware strategies  the exact implications vary greatly  henrich et al         oosterbeek et al  
      roth et al          in the ultimatum game for instance  the actual amounts offered and minimally accepted
vary between     and as much as      depending on various factors  such as the amount to bargain about  cameron 
      de jong et al       c  slonim   roth        sonnegard         and culture  henrich et al          cultural
differences may persist between groups of agents  but also within these groups  axelrod        

   

fid e j ong   u yttendaele   t uyls

we do not use the homo egualis utility function  instead  the desired  human inspired outcome
offered by the homo egualis utility function is replaced here by  potentially  including agents that
always play according to a certain fixed strategy  i e  simulated human players  
    network topology
santos et al       b  investigated the impact of scale free networks on resulting strategies in social
dilemmas  a scale free network was used in order to randomly determine the two agents  neighbors  that would play together in various social dilemmas  such as the prisoners dilemma and the
snowdrift game  the agents were limited to two strategies  i e   cooperate and defect  which were
initially equally probable  it was observed that  due to the scale free network  defectors could not
spread over the entire network in both games  as they do in other network structures  the authors
identified that the topology of the network contributed to the observed maintained cooperation  in
subsequent research  santos et al       a  introduced rewiring in the network and played many different social dilemmas  once again with two strategies per agent  they concluded that the ease
 measure of individuals inertia to readjust their ties  of rewiring was increasing the rate at which
cooperators efficiently wipe out defectors  in contrast to the work of santos et al       a b   which
used a discrete strategy set  this work uses a continuous strategy space  this requires another view
on fairness  cooperation and agreement  departing from the traditional view that fairness is achieved
by driving all agents to  manually labeled  cooperative strategies  in social dilemmas such as the
ultimatum game  any strategy that agents may agree on leads to satisfactory outcomes 

   methodology
before discussing our methodology in detail  we first outline our basic setting  we continue by
explaining continuous action learning automata  as they are central to our methodology  next  we
discuss the structure and topology of the networks of interaction we use  after this we discuss the
agent types and initial strategies of agents  then we elaborate on how we provide the additional
possibility of rewiring connections between agents  finally  we explain our experimental setup 
    the basic setting
we study a large group of adaptive agents  driven by continuous action learning automata  playing
the ultimatum game in pairwise interactions  pairs are chosen according to a  scale free  network
of interaction  every agent is randomly assigned the role of proposer or responder in the ultimatum
game  agents start with different strategies  for good performance  most of them need to converge
to agreement by playing many pairwise games  i e  they need to learn a common strategy  some
agents may be fixed in their strategies  these agents represent an external strategy that the adaptive
agents need to converge to  for instance a preference dictated by humans  as an addition to this
basic setting  we study the influence of adding the option for agents to rewire in their network of
interaction as a response to an agent that behaved in a defecting manner 
    continuous action learning automata
continuous action learning automata  cala  thathachar   sastry        are learning automata
developed for problems with continuous action spaces  cala are essentially function optimizers 
for every action a from their continuous  one dimensional action space a  they receive a feedback
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

  x   the goal is to optimize this feedback  cala have a proven convergence to  local  optima 
given that the feedback function   x  is sufficiently smooth  the advantage of cala over many
other reinforcement learning techniques  see  e g  sutton   barto         is that it is not necessary
to discretize continuous action spaces  because actions are simply real numbers 
      h ow cala w ork
essentially  cala maintain a gaussian distribution from which actions are pulled  in contrast to
standard learning automata  cala require feedback on two actions  being the action corresponding
to the mean  of the gaussian distribution  and the action corresponding to a sample x  taken from
this distribution  these actions lead to a feedback     and   x   respectively  and in turn  this
feedback is used to update the probability distributions  and   more precisely  the update formula
for cala can be written as 
x
        x   
  
  


 
 x   
x
         
    k    l  
  

   

in this equation   represents the learning rate  k represents a large constant driving down   the
variance  is kept above a threshold l to keep calculations tractable even in case of convergence  
this is implemented using the function 
      max    l  

   

the intuition behind the update formula is quite straightforward  de jong et al       a   using this
update formula  cala rather quickly converge to a  local  optimum  with multiple  e g  n  learning
automata  every automaton i receives feedback with respect to the joint actions  respectively i   
and i  x   with                n and x   x            xn   in this case  there still is convergence to a
 local  optimum  thathachar   sastry        
      m odifications to cala for our p urposes
as has been outlined above  we use cala to enable agents to learn a sensible proposer and responder strategy in the ultimatum game  when playing the ultimatum game  two agents may agree
on only one of their two joint actions  i e  they obtain one high and one very low feedback   or
may even disagree on both of them  i e  they obtain two very low feedbacks   both situations need
additional attention  as their occurrence prevents the cala from converging to correct solutions 
to address these situations  we propose two domain specific modifications to the update formula of
the cala  de jong et al       a   
first  in case both joint actions yield a feedback of    the cala are unable to draw effective
conclusions  even though they may have tried a very ineffective strategy and thus should actually
   we used the following settings after some initial experiments           k         and l         the
precise settings for  and l are not a decisive influence on the outcomes  although other values may lead to slower
convergence  if k is chosen to be large  as  rather vaguely  implied by thathachar   sastry         then  decreases
too fast  i e  usually the cala stop exploring before a sufficient solution has been found 
   note that such modifications are not uncommon in the literature  see  e g  the work of selten   stoecker        on
learning direction theory  grosskopf        successfully applied directional learning to the setting of the ultimatum
game  focusing on responder competition  which is not further addressed in this paper  

   

fid e j ong   u yttendaele   t uyls

learn  in order to counter this problem  we introduce a driving force  which allows agents to update
their strategy even if the feedback received is    this driving force is defined as 

for proposers          x
iff         x     
   
for responders        x  
the effect of this modification  which we call zero feedback avoidance  zfa   is that an agent
playing as the proposer will learn to offer more  and an agent playing as the responder will accept
to lower his expectation  in both roles  this will lead to a more probable agreement 
second  if one joint action yields agreement  but the other a feedback of    the cala may adapt
their strategies too drastically in favor of the first joint action  in fact  shifts of  to values greater
than     were observed  de jong   tuyls        de jong et al       a   to tackle this problem 
we restrict the difference that is possible between the two feedbacks the cala receive in every
iteration  more precisely  we empirically set 
fi
fi
fi        x  fi
fi
fi 
   
fi
fi
   
thus  if there is a large difference in feedback between the  action and the x action  we preserve
the direction indicated by this feedback  but prevent the automaton to jump too far in that direction 
we call this modification strategy update limitation  sul  
    the network of interaction
a scale free network  barabasi   albert        is a network in which the degree distribution follows
a power law  more precisely  the fraction p  k  of nodes in the network having k connections to
other nodes goes for large values of k as p  k   k    the value of the constant  is typically in the
range           scale free networks are noteworthy because many empirically observed networks
appear to be scale free  including the world wide web  protein networks  citation networks  and also
social networks  the mechanism of preferential attachment has been proposed to explain power law
degree distributions in some networks  preferential attachment implies that nodes prefer attaching
themselves to nodes that already have a large number of neighbors  over nodes that have a small
number of neighbors 
previous research has indicated that scale free networks contribute to the emergence of cooperation  santos et al       b   we wish to determine whether this phenomenon still occurs in
continuous strategy spaces and therefore use a scale free topology for our interaction network  using the barabasi albert model  more precisely  the probability pi that a newly introduced node is
connected to an existing node i with degree ki is equal to 
ki
pi   p
   
j kj
when we construct the network  the two first nodes are linked to each other  after which the other
nodes are introduced sequentially and connected to one or more existing nodes  using pi   in this
way  the newly introduced node will more probably connect to a heavily linked hub than to one
having only a few connections  in our simulations  we connect every new node to one  two or three
existing ones  uniform probabilities   this yields networks of interaction that are more realistic than
the acyclic ones obtained by always connecting new nodes to only one existing node  for example 
if the network is modeling a friendship network  avoiding cycles means assuming that all friends of
a certain person are never friends of each other 
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

    agent types and strategies
in order to study how agreement concerning a common strategy emerges  we need to make our
agents learn to reach such a common strategy  starting from a situation in which it is absent  i e 
agents have different strategies   moreover  we need to study whether a common strategy can
be established from example agents  and whether it is robust against agents that use a different 
potentially relatively defective strategy 
      t wo t ypes of agents
we introduce two types of agents  i e  dynamic strategy  ds  agents and fixed strategy  fs  agents 
ds agents are the learning agents  they start with a certain predefined strategy and are allowed to
adapt their strategy constantly  according to the learning mechanism of their learning automaton 
basically  these agents are similar to those used in earlier work  de jong et al       a   fs agents
are  optional  good examples  they model an example strategy that needs to be learned by the
 other  agents in our system  and therefore refuse to adapt this strategy 
      o ne or t wo cala per agent  
as has been outlined above  each agent needs to be able to perform two different roles in the ultimatum game  i e  playing as the proposer as well as playing as the responder  in other words  an
agent is in one of two distinct states  and each state requires it to learn a different strategy  as cala
are stateless learners  each agent therefore would require two cala  nonetheless  in the remainder
of this paper  we equip every ds agent with only one cala  representing both the agents proposer
strategy as well as its responder strategy 
our choice for one cala is motivated by two observations  i e      human behavior  and    
some initial experiments  first  human strategies are often consistent  implying that they generally
accept their own offers  but reject offers that are lower  oosterbeek et al          even with high
amounts at stake  de jong et al       c  sonnegard         second  in a set of initial experiments 
we observed that agents using two cala will generally converge to one single strategy anyway  as
an illustration  three learning curves obtained in a fully connected network of three agents playing
the ultimatum game are displayed in figure    it is clearly visible that agents proposer strategies
 bold lines  are strongly attracted to other agents responder strategies  thin lines   and especially to
the lowest of these responder strategies  in the presence of a fs agent that offers     and accepts at
least    the first strategy is immediately ignored in favor of the  lower  second one  with only ds
agents  once again all strategies are attracted to the lowest responder strategy present  
in future work  we will study this observation from the perspective of evolutionary game theory
and replicator equations  gintis        maynard smith   price         for the current paper  we
use the observation to justify an abstraction  i e  we limit the complexity of our agents by equipping
them with only one cala  this cala then represents the agents proposer strategy as well as
its responder strategy  it is updated when the agent plays as a proposer as well as when it plays
as a responder  according to the cala update formula presented in       and the modifications
presented in        thus  the agents single cala receive twice as much feedback as two separate
cala would  this abstraction therefore increases the efficiency of the learning process 
   the agents more quickly adapt their strategies downward than upward  figure     therefore  when multiple  e g     
ds agents are learning  i e  without any fs agents   their strategy usually converges to    this is due to an artifact of
the learning process  two cala trying to learn each others current strategy tend to be driven downward 

   

fi 

 

   

   

 

 

   

   

 

strategy

strategy

d e j ong   u yttendaele   t uyls

   
 
   

 
   

 

 

   

   

 

 
                                           
iteration

                                           
iteration

  

 top left  two ds agents  one starting at offering and accepting      and one starting at offering and accepting       learn
to play optimally against each other and a fs agent offering
    and accepting    the ds agents rather quickly learn to
offer and accept   

 
 
 
strategy

 
   

 

 top right  two ds agents  both starting at offering     and
accepting    learn to play optimally against each other and
a fs agent also offering     and accepting    again the ds
agents learn to offer and accept   

 
 
 

 bottom left  three ds agents  starting at different initial
strategies  i e  offering         and    and accepting      and
   respectively   quickly learn a single  similar strategy 

 
 
 
                                           
iteration

figure    evolving strategies in a fully connected network of three agents  proposal strategies are
indicated with a bold line  response strategies are indicated with a thin line  agents
converge to a situation in which their two initial strategies become similar 

      agents  s trategies
in our simulations  we use two types of ds agents and one type of fs agents  more precisely 
dsr agents are learning agents that start at a rational solution of offering x  n            and
also accepting their own amount or more   dsh agents start with a more human  fair solution  i e 
of offering x  n           and also accepting their own amount or more   since fs agents are
examples of a desired solution  we equip them with a fair  human inspired solution to see whether
the other agents are able to adapt to this solution  the fs agents always offer      and accept
any offer of     or more  all agents are limited to strategies taken from a continuous interval
c            where    is chosen as the upper bound  instead of the more common    because it is a
common amount of money that needs to be shared in the ultimatum game  if any agents strategy
falls outside the interval c  we round off the strategy to the nearest value within the interval 
    rewiring
agents play together based on their connections in the interaction network  thus  in order to avoid
playing with a certain undesirable neighbor j  agent i may decide to break the connection between
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

him and j and create a new link to a random neighbor of j  santos et al       a    for rewiring 
we use a heuristic proposed by santos et al   agents want to disconnect themselves from  relative 
defectors  as they prefer to play with relative cooperators  thus  the probability that agent i unwires
from agent j  is calculated as 
si  sj
pr  
   
c
here  si and sj are the agents current strategies  more precisely  agent is responder strategy and
agent js proposer strategy   and c is the amount at stake in the ultimatum game  i e      even if
agents determine that they want to unwire because of this probability  they may still not be allowed
to  if this breaks the last link for one of them  if unwiring takes place  agent i creates a new wire to
a random neighbor of agent j 
    experimental setup
using the aforementioned types of agents  we need to determine whether our proposed methodology
possesses the traits that we would like to see  our population can be said to have established a
successful agreement if it manages to reach a common strategy that incorporates the preferences of
the good examples  while at the same time discouraging those agents that try to exploit the dominant
strategy  thus  in a population consisting of only ds agents  any strategy that is shared by most  or
all  agents leads to good performance  since all agents agree in all games  yielding an average payoff
of   per game per agent  our architecture should be able to find such a common strategy  when
using ds as well as fs agents  the fs agents dictate the strategy that the ds agents should converge
to  regardless of whether they start as dsh or as dsr agents 
in order to measure whether the agents achieved a satisfactory outcome  we study four quantities
related to the learning process and the final outcome  viz      the point of convergence      the
learned strategy      the performance and     the resulting network structure  we will briefly explain
these four quantities below  in general  we remark that every simulation lasts for        iterations
per agent  i e        n iterations for n agents  we repeat every simulation    times to obtain reliable
estimates of the quantities of interest 
      p oint of c onvergence
the most important quantity concerning the agents learning process is the point of convergence 
which  if present  tells us how many games the agents needed to play in order to establish an agreement  to determine the point of convergence  we calculate and save the average population strategy
avg t  after each pairwise game  i e  each iteration of the learning process   after t iterations  we
obtain an ordered set of t averages  i e   avg             avg t     initially  the average population
strategy changes over time  as the agents are learning  at a certain point in time t  the agents stop
learning  and as a result  the average population strategy avg t  does not change much anymore 
to estimate this point t  i e  the point of convergence  we find the lowest t for which the standard
deviation on the subset  avg t           avg t    is at most       subsequently  we report the number
of games per agent played at iteration t  i e  nt   in our experiments  every simulation is repeated
   note that we may also choose to allow an agent i to create a new connection to specific other agents instead of only
random neighbors of their neighbor j  however  especially in combination with reputation  see       this allows
 relative  defectors to quickly identify  relative  cooperators  with which they may then connect themselves in an
attempt to exploit  preliminary experiments have shown that this behavior leads to the interaction network losing its
scale freeness  which may seriously impair the emergence of agreement 

   

fid e j ong   u yttendaele   t uyls

avg
std
conv

 

 

  
  

 
  

  
  

  

  

 

 
  

 

  
  

 
  

  
  

 
  

  

  
  

iteration

  

strategy

  
 
 
 
 
 
 
 
 
 
 
 

 
  

 

avg
std
conv

population strategy

  

 
  

  
  

  

  

 

 
  

 

  
  

 
  

  
  

  

  

 
  

  

  

 

strategy

population strategy
  
 
 
 
 
 
 
 
 
 
 

iteration

figure    two examples of the convergence point of a single run  in both graphs  we display the
average strategy of the population  bold line  as well as the standard deviation on this
average  thin line   the dotted vertical line denotes the convergence point  as found by
the analysis detailed in the text 

   times  resulting in    convergence points  we will use a box plot to visualize the distribution of
these    convergence points  
as an example  in figure    left   we see how    fs agents     dsh agents and    dsr agents
converge to agreement  using rewiring  only the first         games are shown  in addition to a
bold line denoting the average population strategy  we also plot a thinner line  denoting the standard
deviation on this average  using the method outlined above  the point of convergence is determined
to be around         games  i e  approximately     games per agent were necessary  in figure
   right   we show similar results for    fs agents and    dsr agents  once again using rewiring 
here  the point of convergence is around         games  i e  approximately     games per agent
were necessary  which means that learning to reach agreement was more difficult 
      l earned s trategy
once we established at which iteration t the agents have converged  we can state that the average
learned strategy is precisely avg t   we repeat every simulation    times to obtain a reliable estimate of this average  once again  in our results  we use a box plot to visualize the distribution of
the average learned strategy 
      p erformance
to measure performance  we first allow our agents to learn from playing        ultimatum games
each  then  we fix the strategies of all ds agents  we let every agent play as a proposer against all
its neighbors  one by one   and count the number of games that were successful   we divide this
   in our box plots  we report the average instead of the median  as the average is a more informative quantity  e g  when
comparing our results with existing work  this may result in the box plots mid point being located outside the box 
   note that the cala update formula prevents agents from converging to an exact strategy  as the standard deviation
of the calas gaussian is kept artificially strictly positive  therefore  there is some noise on the strategies agents
have converged to  to counter this noise while measuring performance  we set responders strategies to     of their
actual strategies  thus  an agent having a strategy of   will propose   and accept any offer of      or more 

   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

number through the total number of games played  i e  twice the number of edges in the interaction
network   the resulting number denotes the performance  which lies between    for utterly catastrophic  and    for complete agreement   human players of the ultimatum game typically achieve
a performance of         fehr   schmidt        oosterbeek et al          once again  the   
repetitions lead to    measures of performance  which are displayed in a box plot in our results 
      r esulting n etwork s tructure
since the network of interaction may be rewired by agents that are not satisfied about their neighbors  we are interested in the network structure resulting from the agents learning processes  we
examine the network structure by looking at the degree distribution of the nodes in the network  i e 
the number of neighbors of the agents   with    repeated simulations  we may draw a single box
plot expressing the average degree distribution 

   experiments and results
we present our experiments and results in two subsections  first  we study a setup without rewiring
and a setup with rewiring  varying the number of agents  while keeping the proportion of dsh  dsr
and fs agents constant and equal  i e      for each type of agent   second  we study the same
two setups with various population sizes  this time varying the proportion of fs agents  where the
remainder of the population is half dsh and half dsr  in general  we remark that every experiment
reports results that are averaged over    simulations  in every simulation  we allow the agents to
play       n random games  where n denotes the number of agents  i e  the population size  
    varying the population size
in many multi agent systems  increasing the number of agents  i e  the population size  causes
difficulties  many mechanisms that work with a relatively low number of agents stop working
well with a high number of agents  for instance due to computational complexity or undesired
emergent properties  according to previous research  this issue of scalability also applies to the
task of learning social dilemmas  indeed  previous research using evolutionary algorithms in games
with discrete strategy sets mentions that the number of games needed to converge to an agreement
 i e  on cooperation  may be prohibitively large  santos et al       a    
since our agents are learning in continuous strategy spaces  we may expect a scalability issue
as well  to determine whether our proposed methodology has such an issue  we vary the population
size between    and          with some steps in between   while keeping the proportion of fs 
dsh and dsr agents constant at one third each  we study a setup without rewiring as well as a
setup with rewiring  and determine     the point of convergence  i e  the number of games per agent
needed to reach convergence      the average learned strategy the agents converged to      the final
performance of the system  and finally     the resulting network structure  especially the first and
third of these quantities give an indication of the scalability of our methodology 
    in order to limit the time taken for learning  santos et al       a  terminate the learning process after     iterations 
while using at most     agents  leading to an average of  more than      games per agent being available  still  with
this high number of games per agent  they report that the agents occasionally do not converge 

   

fid e j ong   u yttendaele   t uyls

w ith rewiring
    

    

    
games per agent

games per agent

n o rewiring
    

    
    
    

    
    
    
   

   

 

 
  

  

           
number of agents

  

          

  

                      
number of agents

figure    games per agent until convergence  without rewiring  left  and with rewiring  right  

      p oint of c onvergence
a setup without rewiring  figure    left  tends to require more games per agent as the total number
of agents increases  at a certain point  i e  around a population size of     agents  this tendency
stops  mainly because the average number of games per agent approaches the maximum  i e        
games per agent  a setup with rewiring  same figure  right  convincingly outperforms one without
rewiring  as increasing the population size hardly affects the number of games per agent required
to reach convergence  independent from the population size  the setup requires approximately    
games per agent to converge  note the difference with previous research  i e  santos et al       a  
which reports requiring     games per agent  or more  
      l earned s trategy
a setup without rewiring  figure    left  on average converges to a strategy of offering as well as
accepting around    where     would be required  as the     fs agents present in the population
all play with this strategy  i e  the     ds agents on average have a strategy of     with increasing
population size  this average strategy is not affected  however  it becomes more and more with
certainty established  once again  a setup with rewiring  same figure  right  shows convincingly
better results  independent from the population size  the learning agents all converge to the desired
strategy  i e      
      p erformance
with a setup without rewiring  figure    left   we already saw that the average learned strategy of
the ds agents is not very good  performance is seriously affected  at around      it indicates that
few ds agents ever agree with fs agents  however  average performance is not influenced by the
population size  as with the learned strategy  the performance of around     only becomes more
certainly established  as expected  a setup with rewiring  same figure  right  shows much more
satisfying results  i e  generally above     agreement  these results are actually positively affected
by the population size  as the average performance increases with an increasing population 
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

n o rewiring

w ith rewiring
 
average converged strategy

average converged strategy

 
 
 
 
 

 
 
 
 
 

 
  

  

   
   
   
number of agents

    

  

     

  

           
number of agents

          

figure    average learned strategy  without rewiring  left  and with rewiring  right  

w ith rewiring

 
   
   
   
   
   
   
   
   
   
 

performance at convergence

performance at convergence

n o rewiring

  

  

   
   
   
number of agents

 
   
   
   
   
   
   
   
   
   
 
  

          

  

           
number of agents

          

figure    final performance  without rewiring  left  and with rewiring  right  

      r esulting n etwork s tructure
we look at the network structure resulting from learning to reach agreement  and determine whether
this structure is influenced by the population size  obviously  a setup without rewiring  figure   
left  does not display any influence here  as the network is static  a setup with rewiring  same
figure  right  shows an interesting tendency  the average degree of the resulting network stays low 
while the maximum degree increases with an increasing population size  clearly  as the population
size increases  the hubs in the scale free network receive more and more preferential attachment 
and correspondingly  less densely connected nodes become even less densely connected  when we
examine the number of times agents actually rewire  we find that this number generally lies below
        i e  a very low percentage of the total number of games played actually made the agents
rewire to a random neighbor of an undesired proposer 
   

fid e j ong   u yttendaele   t uyls

w ith rewiring
   

   

   

degree distribution

degree distribution

n o rewiring
   

  
  
  

  
  
  
 

 
  

  

           
number of agents

  

          

  

           
number of agents

          

figure    resulting network structure  without rewiring  left  and with rewiring  right  
      i n c onclusion
in conclusion to this subsection  we may state that the proposed methodology is not suffering from
severe scalability issues  a setup that does not include rewiring is clearly outperformed by one
that does include rewiring  but neither a setup without rewiring  nor a setup with rewiring  suffer
severely from increasing the number of agents 
    varying the proportion of good examples  fs agents 
in this section  we investigate the behavior of the proposed methodology when the proportion of
good examples in the population  i e  fs agents with a strategy of      is varied  the remainder
of the population consists of dsh and dsr agents in equal proportions  we experimented with a
number of population sizes  ranging from    to     
since the results for each population size are rather similar  we restrict ourselves to graphically
reporting and analyzing the results of our experiments with     agents in the remainder of this
section  a selection of the remaining results is given in table    specifically  for a setup without rewiring and a setup with rewiring  we report on the population size  pop   the percentage fs
agents used   fs   the average number of games per agent needed to converge  games   the average learned strategy  strat   the average performance  perf   and finally  the maximum number of
connections that a single agent has with other agents in the network  netw   as we will discuss
below  the results reported in table   for population sizes other than     are highly similar to those
for a population size of     agents 
      p oint of c onvergence
a setup without rewiring  figure    left  requires more and more games per agent to converge  until
the proportion of fs agents reaches around      then  the required number of games decreases
again  although there is a great deal of uncertainty  introducing rewiring  same figure  right  yields
much better results  the number of games required per agent hardly exceeds      and this number
decreases steadily with an increasing proportion of the population being an fs agent 
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

n o rewiring
pop

  fs

  

w ith rewiring

games

strat

perf

netw

pop

  fs

 
  
  
  

      
        
        
      

    
    
    
    

    
    
    
    

  
  
  
  

  

 
  
  
  

   

 
  
  
  

      
        
        
      

    
    
    
    

    
    
    
    

  
  
  
  

   

   

 
  
  
  

      
        
        
      

    
    
    
    

    
    
    
    

  
  
  
  

   

games

strat

perf

netw

      
      
      
      

    
    
    
    

    
    
    
    

  
  
  
  

 
  
  
  

      
      
      
      

    
    
    
    

    
    
    
    

  
  
  
  

 
  
  
  

      
      
      
      

    
    
    
    

    
    
    
    

  
   
  
  

table    summary of the results of experiments in which the proportion of fs agents is varied  for
details and additional results  see the main text 
w ith rewiring

    

    

    

    
games per agent

games per agent

n o rewiring

    
    
    

    
    
    

   

   

 

 
 

                                
percentage fs agents

                              
percentage fs agents

figure    games per agent until convergence  without rewiring  left  and with rewiring  right  

      l earned s trategy
interestingly  a population consisting of only ds agents tends to converge to offering and accepting
the lowest amount possible  both in a setup that does not use rewiring  figure    left   as well as in
a setup that does  same figure  right   as has been explained in    ds agents tend to adapt their
strategies downward more easily than upward  thus  two ds agents that are having approximately
the same strategy  may slowly pull each others strategy downward  with many ds agents  the
probability that this happens increases  adding fs agents to the population results in different
behavior for the two setups  a setup without rewiring has difficulties moving away from the lowest
amount possible  only with a sufficient number of fs agents  i e      of the population  does the
average learned strategy reflect that the ds agents move towards the strategy dictated by the fs
agents  with rewiring  results are convincingly better  even with only     fs agents  the ds agents
on average converge towards offering and accepting the amount dictated by these agents  i e      
   

fid e j ong   u yttendaele   t uyls

n o rewiring

w ith rewiring
 
average converged strategy

average converged strategy

 
 
 
 
 

 
 
 
 
 

 
 

  

  

              
percentage fs agents

  

 

      

                              
percentage fs agents

figure    average learned strategy  without rewiring  left  and with rewiring  right  
w ith rewiring

performance at convergence

performance at convergence

n o rewiring
 
   
   
   
   
   
   
   
   
   
 
 

 
   
   
   
   
   
   
   
   
   
 
 

                              
percentage fs agents

                              
percentage fs agents

figure    final performance  without rewiring  left  and with rewiring  right  
      p erformance
the observations concerning the learned strategy  as reported above  are reflected in the performance
of the collective of agents  in a setup without rewiring  figure    left   performance decreases
initially with an increasing proportion of fs agents  as the ds agents refuse to adapt to the dictated
strategy  when the proportion of fs agents becomes large enough  the ds agents start picking up
this strategy  resulting in increasing performance  a setup with rewiring  same figure  right  does
better  as performance increases with an increasing number of fs agents  even though the average
learned strategy is close to     for every proportion of fs agents  low proportions of fs agents
still display less performance than higher proportions  this may require additional explanation 
note that the box plot of figure   shows the distribution of the average strategy over    repeated
simulations  i e  it does not show the strategy distribution within a single simulation 
thus  even though the average strategy in a single simulation is always very close to      there is
still variance  with a low number of fs agents  this variance is most prominently caused by inertia 
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

w ith rewiring

degree distribution

degree distribution

n o rewiring
   
  
  
  
  
  
  
  
  
  
 
 

                              
percentage fs agents

   
  
  
  
  
  
  
  
  
  
 
 

                              
percentage fs agents

figure     resulting network structure  without rewiring  left  and with rewiring  right  
i e  not all ds agents are directly connected to an fs agent  which implies that they need to learn
their desired strategy from neighboring agents who are also learning  especially with rewiring  this
may result in two agents playing together that are compatible with most of their neighbors  but not
 yet  with each other 
      r esulting n etwork s tructure
clearly  the network structure of a setup without rewiring  figure     left  is not influenced by
varying the proportion of fs agents  when rewiring is used  same figure  right   we observe an
interesting phenomenon  closely related to our observations in      once again  the number of
times agents actually rewire generally lies below         even though this is a low number  it does
affect the network structure in a useful way  with a low proportion of fs agents  there is a large
tendency for increased preferential attachment  with     fs agents for instance  there is a single
agent that connects to    out of     other agents  with an increasing proportion of fs agents  the
maximum degree of the network decreases  until finally  it closely resembles the original network 
clearly  in the presence of only few examples of the desired strategy  ds agents attempt to connect
to other agents that provide such examples  this is interesting and useful emergent behavior 
      i n c onclusion
when we compare the results obtained in a population of     agents with the results for other
population sizes  as reported in table    we see that these are highly similar  in conclusion to this
subsection  we may state that a setup that is not using rewiring has severe difficulties converging to
a desired example if the proportion of fs agents providing this example is low  only for  e g  half
of the population consisting of examples  does the other half learn the desired behavior  a setup
that is using rewiring has absolutely no problems converging to the desired strategy  even with a
low proportion of fs agents  in both cases  completely omitting the examples leads to the agents
converging to the individually rational solution  this is caused by an artifact of the learning method
used  i e  as mentioned before  two cala trying to learn each others strategy tend to be driven
downward to the lowest value allowed 
   

fid e j ong   u yttendaele   t uyls

   discussion
the results presented in the previous section suggest that mechanisms that lead to cooperative solutions in social dilemmas with only a discrete set of strategies  e g  scale free networks and rewiring  
also lead to agreement in social dilemmas with a continuous strategy space  in this section  however  we show that this is not a trivial issue  more precisely  we discuss a number of mechanisms
that enhance agents abilities to reach cooperation in social dilemmas with discrete strategy sets 
but do not directly enhance agents abilities to reach agreement in continuous strategy spaces  we
empirically analyze why this is the case 
    reputation
reputation is one of the main concepts used in behavioral economics to explain how fairness
emerges  e g  bowles et al         fehr         basically  it is assumed that interactions between
people lead to expectations concerning future interactions  these expectations may be positive or
negative and may be kept to oneself  or actually shared with peers 
in work closely related to our work  nowak et al         show that reputation deters agents
from accepting low offers in the ultimatum game  as this information will spread  leading to the
agents also receiving low offers in return  then  if all agents refuse to accept low offers  they
should provide high offers  thus  nowak et al  argue that the population goes toward providing
and accepting high offers  however  we note that any shared strategy  i e  any agreement  in the
ultimatum game yields an expected payoff of     of the amount at stake for both agents  thus 
reputation may indeed help agents to decide which strategy to play against others  but a preference
for playing cooperatively  i e  providing high offers  does not directly result from reputation 
      s preading r eputation
we study the effects of reputation by optionally adding a second network to our system  as with
the interaction network  we consider the reputation network to be scale free  in contrast to the
interaction network however  the reputation network is assumed to be static  as agents are truthful
concerning reputation  making it unnecessary for agents to consider rewiring  note that two agents
sharing reputation information may or may not be connected as well in the interaction network 
and as a consequence  two agents playing an ultimatum game may or may not share reputation
information with each other  in effect  after every ultimatum game  the responding agent may
broadcast reputation information to its neighbors in the reputation network  the information is sent
by the responder and concerns the offer just done by the proposer  this is the only information that
is guaranteed to be correct  agents receive information with a probability 
pij     

d
h

   

here  d is the distance between the sender and the  potential  receiver j in the reputation network 
thus  reputation information may travel for at most h hops  with a decreasing probability per hop 
in our simulations  we set h      we note that in relatively small networks  this implies that
reputation information is essentially public 
note that reputation information may be helpful only if we allow agents to do something with
this information  in the work of nowak et al          for instance  the reputation of others is used
by agents to determine what to offer to these others  given     the observation that reputation  used
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

in this way  should not necessarily promote cooperative strategies  see above   and     the fact that
we already use cala to determine what agents offer to each other  we want the reputation to affect
something else than agents strategies  we will discuss a number of ways in which agents may use
reputation  as taken from literature  i e  interacting with a preferred neighbor  below  and using
reputation to facilitate voluntary participation       
      u sing r eputation
without reputation  agents play against a random neighbor in the interaction network  reputation
may be used to make agents prefer interacting with specific neighbors  chiang        discusses that
strategies of fairness could evolve to be dominant if agents are allowed to choose preferred partners
to play against  chiang allows agents to select partners that have helped the agent previously 
to determine who is a preferred partner  we use the heuristic proposed by santos et al       a  
i e  an agent prefers playing with  relative  cooperators  as these help it in obtaining a high payoff
if it is the responder  thus  the probability that agent i plays with agent j  ni   where ni is the set
of agent is neighbors  is 
sj  si
pij   p
   
kni sk
here  si   sj and sk are the agents current strategies  for agents other than i  these are estimates
based on reputation and previous experience  
there are two problems with this approach  first  the number of times that an agent i receives
information about an agent j  ni may be rather low  especially with many agents  even with only
   agents  we observe that only around     of the reputation information received by agents actually concerned one of their neighbors  this problem may be addressed by making the reputation
network identical to the interaction network  as neighbor relations in both networks are then identical   however  this may be seen as a considerable abstraction  second  the probability that agent i
has information concerning all of his neighbors is low  so we need to specify default values for s j  
clearly  any default value is more often wrong than right  unless we use a centralized mechanism
to estimate it by  for instance  using the current average population strategy  which is what we do in
our simulations 
with this mechanism in place  we perform the same experiments as in    i e  we vary the
population size between    and         agents  and the proportion of fs agents in steps of      a
statistical analysis reveals no significant difference between a setup that uses reputation and a setup
that does not  when we further analyze the results  we see that  as expected  agents almost always
need to resort to default values for their neighbors strategies  thus  on average  the reputation
system does not often change the probabilities that certain neighbors are selected 
    reputation and rewiring
as we have seen in    rewiring works very well without reputation  i e  purely based on an agents
own experience   adding reputation may be beneficial to agents  as they no longer need to interact
with each other to be allowed to unwire  thus  agents may once again increase their preference
for certain others  reputation information  i e  the amount offered by a certain agent  propagates
through the  static  reputation network  allowing agents receiving such information to potentially
unwire from one of their neighbors if they consider this neighbors behavior to be undesirable  the
same rewiring mechanism is used here as detailed in    i e  equation     we allow the responder
   

fid e j ong   u yttendaele   t uyls

in the ultimatum game to broadcast reputation information through the reputation network  with a
maximum of h     hops 
once again  we perform the same experiments as in    and once again  there is no significant
difference in the main results  we further analyze the number of times agents actually rewired  and
find that this number on average increases by a factor   with respect to a setup in which reputation is
not used  i e  as reported in       however  this increase does not increase performance  on average  agents have only few neighbors  thus  they generally receive reputation information concerning
a neighbor that  in the absence of reputation  they would play against soon anyway 
    volunteering
according to existing research on human fairness  e g  boyd   mathew        hauert et al        
sigmund et al         the mechanism of volunteering may contribute to reaching cooperation in
games with only two strategies  the mechanism of volunteering consists in allowing players not to
participate in certain games  enabling them to fall back on a safe side income that does not depend
on other players strategies  such risk averse optional participation can prevent exploiters from
gaining the upper hand  as they are left empty handed by more cooperative players preferring not to
participate  clearly  the side income must be carefully selected  such that agents are encouraged to
participate if the population is sufficiently cooperative  experiments show that volunteering indeed
allows a collective of players to spend most of its time in a happy state  boyd   mathew       
in which most players are cooperative 
the biggest problem when applying volunteering is that we basically introduce yet another
social dilemma  an agent may refrain from participating to make a statement against the other
agent  which may convince this other agent to become more social in the future  but to make this
statement  the agent must refuse an expected positive payoff  in the ultimatum game with randomly
assigned roles  the expected payoff is always positive  nonetheless  we study whether volunteering
promotes agreement in games with continuous strategy spaces  we once again use the heuristic
proposed by santos et al       a   which has already been applied in various mechanisms in this
paper  if agent i thinks that agent j is a  relative  cooperator  then he agrees on playing  if both
agents agree  then a game is played  to prevent agents from not playing any game  after all  both
agents see each other as a relative cooperator only if they already are playing the same strategy   we
introduce a     probability that games are played anyway  even if one or both agents does not want
to  note that reputation may be used here  as it may allow agents to estimate whether one of their
neighbors is a relative cooperator or not  without having to play with this neighbor 
unfortunately  experimental results point out that agents using volunteering  with and without
reputation  have severe difficulties establishing a common strategy  uyttendaele         as a result  when measuring performance  we see that only around     of the games is played  of the
games played  the performance is similar to a setup with rewiring  e g  above       which may
be expected  as two agents usually only agree to play if their strategies are similar  the reason
why agents do not converge properly is quite simple  they avoid playing with other agents that are
different from them  therefore  they do not learn to behave in a way more similar to these others 
    general discussion
in general  we may state that with the mechanism of rewiring  we clearly find a good balance
between allowing agents to play with more preferred neighbors on the one hand  and forcing agents
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

to learn from those different from them on the other hand  the additions discussed above allow
agents to be too selective  i e  they have too much influence on who they play against  while this
may be in the interest of individual agents  it generally leads to agents not playing against others that
are different from them  instead of learning from these others  as is required to obtain convergence
to agreement 

   conclusion
in this paper  we argue that mechanisms thought to allow humans to find fair  satisfactory solutions
to social dilemmas  may be useful for multi agent systems  as many multi agent systems need to
address tasks that contain elements of social dilemmas  e g  resource allocation  chevaleyre et al  
       existing work concerning  human inspired  fairness in multi agent systems is generally restricted to discrete strategy sets  usually with only two strategies  one of which is deemed to be
cooperative  i e  desirable   however  many real world applications of multi agent systems pose
social dilemmas which require a strategy taken from a continuous strategy space  rather than a discrete strategy set  we observed that the traditional concept of cooperation is not trivially applicable
to continuous strategy spaces  especially since it is no longer feasible to manually label a certain
strategy as cooperative in an absolute manner  a certain strategy may be cooperative in a certain
culture  whereas it may be defective or naive in another  thus  cooperation is a relative rather than
absolute concept in continuous strategy spaces 
we propose that the concept of agreement  as introduced in statistical physics  dallasta et al  
      may be used as an alternative to cooperation  we discuss the emergence of agreement in
continuous strategy spaces  using learning agents that play pairwise ultimatum games  based on
a scale free interaction network and the possibility to rewire in this network  in the ultimatum
game  two agents agree if the offering agent offers at least the minimal amount that satisfies the
responding agent  in this case  the two agents cooperate   thus  for our population of agents to
agree on many random pairwise games  the agents should converge to the same strategy  without
any external influences  any shared strategy is sufficient  with external influences  e g  a preference
dictated by humans  agents should adapt to the dictated strategy  even if they are already agreeing
on a completely different strategy  we propose a methodology  based on continuous action learning
automata  interactions in scale free networks  and rewiring in these networks  aimed at allowing
agents to reach agreement  a set of experiments investigates the usability of this methodology 
in conclusion  we give four statements      our proposed methodology is able to establish agreement on a common strategy  especially when agents are given the option to rewire in their network
of interaction  humans playing the ultimatum game reach an agreement of approximately       
 oosterbeek et al          without rewiring  our agents do worse  generally      of the games are
successful   with rewiring  they do as well as humans  thus  as in games with a discrete strategy set 
rewiring greatly enhances the agents abilities to reach agreement  without compromising the scalefree network structure  this indicates that interactions in scale free networks  as well as rewiring in
these networks  are plausible mechanisms for making agents reach agreement      in comparison to
methodologies reported on in related work  e g  santos et al       b   our methodology facilitates
convergence with only a low number of games per agent needed  e g      instead of          this
indicates that continuous action learning automata are a satisfactory approach when we are aiming
at allowing agents to learn from a relatively low number of examples      the performance of the
collective is not seriously influenced by its size  this is clearly influenced by the characteristics
   

fid e j ong   u yttendaele   t uyls

of a scale free  self similar network      concepts such as reputation or volunteering  which have
been reported to facilitate cooperative outcomes in discrete strategy games  do not seem to have
 additional  benefits in continuous strategy spaces 
although the ultimatum game is only one example of a social dilemma  its core difficulty is
present in all social dilemmas  selecting an individually rational action  which should optimize ones
payoff  actually may hurt this payoff  in the ultimatum game  this problem is caused by the fact
that we may play  as a proposer  against someone who would rather go home empty handed than
accept a deal that is perceived as unfair  similar fairness related problems may arise in various other
interactions  e g  in bargaining about the division of a reward  or in resource allocation  chevaleyre
et al         endriss         many multi agent systems need to allocate resources  if not explicitly
in their assigned task  then implicitly  for instance because multiple agents share a certain  limited
amount of computation time  thus  fair division is an important area of research  which recently is
receiving increasing attention from the multi agent systems community  endriss         as humans
often display adequate and immediate ability to come up with a fair division that is accepted by
most of them  it will definitely pay off if we allow agents to learn to imitate human strategies  in
this paper  we examined how such a task may be executed 

acknowledgments
the authors wish to thank the anonymous referees for their valuable contributions  steven de jong
is funded by the breedtestrategie programme of maastricht university 

references
axelrod  r          the dissemination of culture  a model with local convergence and global
polarization  journal of conflict resolution            
barabasi  a  l  and albert  r          emergence of scaling in random networks  science         
    
bearden  j  n          ultimatum bargaining experiments  the state of the art  ssrn elibrary 
bowles  s   boyd  r   fehr  e   and gintis  h          homo reciprocans  a research initiative on
the origins  dimensions  and policy implications of reciprocal fairness  advances in complex
systems        
boyd  r  and mathew  s          a narrow road to cooperation  science               
cameron  l          raising the stakes in the ultimatum game  evidence from indonesia  journal
of economic inquiry          
chevaleyre  y   dunne  p   endriss  u   lang  j   lematre  m   maudet  n   padget  j   phelps  s  
rodriguez aguilar  j   and sousa  p          issues in multiagent resource allocation  informatica         
chiang  y  s          a path toward fairness  preferential association and the evolution of strategies in the ultimatum game  rationality and society               
   

fil earning to r each agreement in a c ontinuous u ltimatum g ame

dallasta  l   baronchelli  a   barrat  a   and loreto  v          agreement dynamics on smallworld networks  europhysics letters        pp         
dannenberg  a   riechmann  t   sturm  b   and vogt  c          inequity aversion and individual
behavior in public good games  an experimental investigation  ssrn elibrary 
de jong  s  and tuyls  k          learning to cooperate in public goods interactions  presented at
the eumas   workshop  bath  uk  december       
de jong  s   tuyls  k   and verbeeck  k       a   artificial agents learning human fairness 
in proceedings of the international joint conference on autonomous agents and multi agent
systems  aamas     pages        
de jong  s   tuyls  k   and verbeeck  k       b   fairness in multi agent systems  knowledge
engineering review               
de jong  s   tuyls  k   verbeeck  k   and roos  n       c   priority awareness  towards a computational model of human fairness for multi agent systems  adaptive agents and multi agent
systems iii  adaptation and multi agent learning              
endriss  u          fair division  tutorial at the international conference on autonomous agents
and multi agent systems  aamas  
fehr  e          dont lose your reputation  nature             
fehr  e  and schmidt  k          a theory of fairness  competition and cooperation  quarterly
journal of economics             
gintis  h          game theory evolving  a problem centered introduction to modeling strategic
interaction  princeton university press  princeton  usa 
grosskopf  b          reinforcement and directional learning in the ultimatum game with responder competition  experimental economics              
gueth  w   schmittberger  r   and schwarze  b          an experimental analysis of ultimatum
bargaining  journal of economic behavior and organization               
hauert  c   traulsen  a   brandt  h   nowak  m  a   and sigmund  k          via freedom to
coercion  the emergence of costly punishment  science               
henrich  j   boyd  r   bowles  s   camerer  c   fehr  e   and gintis  h          foundations of
human sociality  economic experiments and ethnographic evidence from fifteen small scale
societies  oxford university press  oxford  uk 
maynard smith  j          evolution and the theory of games  cambridge university press 
maynard smith  j  and price  g  r          the logic of animal conflict  nature           
nowak  m  a   page  k  m   and sigmund  k          fairness versus reason in the ultimatum
game  science               
   

fid e j ong   u yttendaele   t uyls

oosterbeek  h   sloof  r   and van de kuilen  g          cultural differences in ultimatum game
experiments  evidence from a meta analysis  experimental economics           
peters  r          evolutionary stability in the ultimatum game  group decision and negotiation 
         
roth  a  e   prasnikar  v   okuno fujiwara  m   and zamir  s          bargaining and market
behavior in jerusalem  ljubljana  pittsburgh  and tokyo  an experimental study  american
economic review               
santos  f  c   pacheco  j  m   and lenaerts  t       a   cooperation prevails when individuals
adjust their social ties  plos comput  biol                  
santos  f  c   pacheco  j  m   and lenaerts  t       b   evolutionary dynamics of social dilemmas
in structured heterogeneous populations  proc  natl  acad  sci  usa               
selten  r  and stoecker  r          end behavior in sequences of finite prisoners dilemma supergames   a learning theory approach  journal of economic behavior   organization         
   
sigmund  k   hauert  c   and nowak  m  a          reward and punishment  proceedings of the
national academy of sciences                    
slonim  r  and roth  a          learning in high stakes ulitmatum games  an experiment in the
slovak republic  econometrica            
sonnegard  j          determination of first movers in sequential bargaining games  an experimental study  journal of economic psychology            
sutton  r  s  and barto  a  g          reinforcement learning  an introduction  mit press 
cambridge  ma  a bradford book 
thathachar  m  a  l  and sastry  p  s          networks of learning automata  techniques for
online stochastic optimization  kluwer academic publishers  dordrecht  the netherlands 
uyttendaele  s          fairness and agreement in complex networks  masters thesis  micc 
maastricht university 

   

fi