journal of artificial intelligence research               

submitted        published      

anytime induction of low cost  low error classifiers 
a sampling based approach
saher esmeir
shaul markovitch

esaher cs technion ac il
shaulm cs technion ac il

computer science department
technionisrael institute of technology
haifa        israel

abstract
machine learning techniques are gaining prevalence in the production of a wide range of
classifiers for complex real world applications with nonuniform testing and misclassification
costs  the increasing complexity of these applications poses a real challenge to resource
management during learning and classification  in this work we introduce act  anytime
cost sensitive tree learner   a novel framework for operating in such complex environments 
act is an anytime algorithm that allows learning time to be increased in return for lower
classification costs  it builds a tree top down and exploits additional time resources to
obtain better estimations for the utility of the different candidate splits  using sampling
techniques  act approximates the cost of the subtree under each candidate split and favors
the one with a minimal cost  as a stochastic algorithm  act is expected to be able to
escape local minima  into which greedy methods may be trapped  experiments with a
variety of datasets were conducted to compare act to the state of the art cost sensitive
tree learners  the results show that for the majority of domains act produces significantly
less costly trees  act also exhibits good anytime behavior with diminishing returns 

   introduction
traditionally  machine learning algorithms have focused on the induction of models with
low expected error  in many real word applications  however  several additional constraints
should be considered  assume  for example  that a medical center has decided to use machine learning techniques to build a diagnostic tool for heart disease  the comprehensibility
of decision tree models  hastie  tibshirani    friedman        chap     makes them the preferred choice on which to base this tool  figure   shows three possible trees  the first tree
 upper left  makes decisions using only the results of cardiac catheterization  heart cath  
this tree is expected to be highly accurate  nevertheless  the high costs and risks associated with the heart cath procedure make this decision tree impractical  the second tree
 lower left  dispenses with the need for cardiac catheterization and reaches a decision based
on a single  simple  inexpensive test  whether or not the patient complains of chest pain 
such a tree would be highly accurate  most people do not experience chest pain and are
indeed healthy  the tree  however  does not distinguish between the costs of different types
of errors  while a false positive prediction might result in extra treatments  a false negative
prediction might put a persons life at risk  therefore  a third tree  right  is preferred  one
that attempts to minimize test costs and misclassification costs simultaneously 
c
    
ai access foundation  all rights reserved 

fiesmeir   markovitch

heart cath
no

normal

chest pain
no

yes

yes

blood
pressure

alerting

no

blood
pressure
no

yes

cardiac
stress

normal

normal

no

normal

normal

heart cath

yes

no

no

yes

yes

alerting

yes

normal

alerting

normal

heart cath

chest pain
no

yes

alerting

figure    three possible decision trees for diagnosis of heart diseases  the upper left tree
bases its decision solely on heart cath and is therefore accurate but prohibitively
expensive  the lower left tree dispenses with the need for heart cath and reaches
a decision using a single  simple  and inexpensive test  whether or not the patient
complains of chest pain  such a tree would be highly accurate but does not
distinguish between the costs of the different error types  the third  right hand 
tree is preferable  it attempts to minimize test costs and misclassification costs
simultaneously 

cost a          

a 
a  
 

 

a 

a  
 

a 

a 

 

cost a         
cost a              

a 

 

a 
 

 

a 
 

 

a 
 

 

 

figure    left  an example of a difficulty greedy learners might face  right  an example of
the importance of context based feature evaluation 

finding a tree with the lowest expected total cost is at least np complete   as in the
cost insensitive case  a greedy heuristic can be used to bias the search towards low cost trees 
decision trees with minimal cost  dtmc   a greedy method that attempts to minimize
   finding the smallest consistent tree  which is an easier problem  is np complete  hyafil   rivest        

 

fianytime induction of low cost  low error classifiers

both types of costs simultaneously  has been recently introduced  ling  yang  wang   
zhang        sheng  ling  ni    zhang         a tree is built top down  and a greedy split
criterion that takes into account both testing and misclassification costs is used  the basic
idea is to estimate the immediate reduction in total cost after each split  and to prefer the
split with the maximal reduction  if no split reduces the cost on the training data  the
induction process is stopped 
although efficient  the dtmc approach can be trapped into a local minimum and
produce trees that are not globally optimal  for example  consider the concept and costs
described in figure    left   there are    attributes  of which only a  and a   are relevant 
the cost of a  and a     however  is significantly higher than the others  such high costs may
hide the usefulness of a  and a     and mislead the learner into repeatedly splitting on a    
which would result in a large  expensive tree  the problem would be intensified if a  and
a   were interdependent  with a low immediate information gain  e g   a   a      in that
case  even if the costs were uniform  a local measure might fail to recognize the relevance
of a  and a    
dtmc is appealing when learning resources are very limited  however  it requires a
fixed runtime and cannot exploit additional resources to escape local minima  in many
real life applications  we are willing to wait longer if a better tree can be induced  esmeir  
markovitch         for example  the importance of the model in saving patients lives may
convince the medical center to allocate   month to learn it  algorithms that can exploit
additional time to produce better solutions are called anytime algorithms  boddy   dean 
      
the icet algorithm  turney        was a pioneer in non greedy search for a tree that
minimizes test and misclassification costs  icet uses genetic search to produce a new
set of costs that reflects both the original costs and the contribution of each attribute in
reducing misclassification costs  then it builds a tree using the eg  algorithm  nunez 
      but with the evolved costs instead of the original ones  eg  is a greedy cost sensitive
algorithm that builds a tree top down and evaluates candidate splits by considering both
the information gain they yield and their measurement costs  it does not  however  take
into account the misclassification cost of the problem 
icet was shown to significantly outperform greedy tree learners  producing trees of
lower total cost  icet can use additional time resources to produce more generations and
hence widen its search in the space of costs  because the genetic operations are randomized 
icet is more likely to escape local minima  into which eg  with the original costs might
be trapped  nevertheless  two shortcomings limit icets ability to benefit from extra time 
first  after the search phase  it uses the greedy eg  algorithm to build the final tree 
but because eg  prefers attributes with high information gain  and low test cost   the
usefulness of highly relevant attributes may be underestimated by the greedy measure in the
case of hard to learn concepts where attribute interdependency is hidden  this will result
in more expensive trees  second  even if icet overcomes the above problem by randomly
reweighting the attributes  it searches the space of parameters globally  regardless of the
context in the tree  this imposes a problem if an attribute is important in one subtree but
useless in another  to better understand these shortcomings  consider the concept described
by the tree in figure    right   there are    attributes with similar costs  the value of a 
determines whether the target concept is a   a  or a   a    the interdependencies result in
 

fiesmeir   markovitch

a low gain for all attributes  because icet assigns costs globally  the attributes will have
similar costs as well  therefore  icet will not be able to recognize which one is relevant in
which context  if the irrelevant attributes are cheaper  the problem is intensified and the
model might end up relying on irrelevant attributes 
recently  we have introduced the cost insensitive lsid  algorithm  which can induce
more accurate trees when allocated more time  esmeir   markovitch      a   the algorithm evaluates a candidate split by estimating the size of the smallest consistent tree under
it  the estimation is based on sampling the space of consistent trees  where the size of the
sample is determined in advance according to the allocated time  lsid  is not designed 
however  to minimize test and misclassification costs  in this work we build on lsid  and
propose act  an anytime cost sensitive tree learner that can exploit additional time to
produce lower cost trees  applying the sampling mechanism in the cost sensitive setup 
however  is not trivial and imposes three major challenges      how to produce the sample 
    how to evaluate the sampled trees  and     how to prune the induced trees  in section
  we show how these obstacles may be overcome 
in section   we report an extensive set of experiments that compares act to several
decision tree learners using a variety of datasets with costs assigned by human experts
or automatically  the results show that act is significantly better for the majority of
problems  in addition  act is shown to exhibit good anytime behavior with diminishing
returns 

   cost sensitive classification
offline concept learning consists of two stages  the learning stage  where a set of labeled
examples is used to induce a classifier  and the classification stage  where the induced
classifier is used to classify unlabeled instances  these two stages involve different types
of costs  turney         our primary goal in this work is to trade learning speed for a
reduction in test and misclassification costs  to make the problem well defined  we need to
specify      how misclassification costs are represented      how test costs are calculated 
and     how we should combine both types of cost 
to answer these questions  we adopt the model described by turney         in a problem
with  c  different classes  a misclassification cost matrix m is a  c    c  matrix whose mi j
entry defines the penalty of assigning the class ci to an instance that actually belongs to the
class cj   typically  entries on the main diagonal of a classification cost matrix  no error 
are all zero 
when classifying an example e using a tree t   we propagate e down the tree along a
single path from the root of t to one of its leaves  let  t  e  be the set of tests along this
path  we denote by cost   the cost of administering the test   the testing cost of e in t
p
is therefore tcost t  e     cost    note that we use sets notation because tests that
appear several times are charged for only once  in addition  the model described by turney
       handles two special test types  namely grouped and delayed tests 
grouped tests  some tests share a common cost  for which we would like to charge only
once  typically  the test also has an extra  possibly different  cost  for example  consider a
tree path with tests like cholesterol level and glucose level  for both values to be measured 
a blood test is needed  taking blood samples to measure the cholesterol level clearly lowers
 

fianytime induction of low cost  low error classifiers

the cost of measuring the glucose level  formally  each test possibly belongs to a group   if
its the first test from the group to be administered  we charge for the full cost  if another
test from the same group has already been administered earlier in the decision path  we
charge only for the marginal cost 
delayed tests  sometimes the outcome of a test cannot be obtained immediately  e g  
lab test results  such tests  called delayed tests  force us to wait until the outcome is
available  alternatively  turney        suggests taking into account all possible outcomes 
when a delayed test is encountered  all the tests in the subtree under it are administered
and charged for  once the result of the delayed test is available  the prediction is at hand 
one problem with this setup is that it follows all paths in the subtree  regardless of the
outcome of non delayed costs  moreover  it is not possible to distinguish between the delays
different tests impose  for example  one result might be ready after several minutes while
another only after a few days  in this work we do not handle delayed tests  but we do
explain how act can be modified to take them into account 
after the test and misclassification costs have been measured  an important question
remains  how should we combine them  following turney         we assume that both
cost types are given in the same scale  a more general model would require a utility function
that combines both types  qin  zhang  and zhang        presented a method to handle the
two kinds of cost scales by setting a maximal budget for one kind of cost and minimizing
the other one  alternatively  patient preferences can be elicited and summarized as a utility
function  lenert   soetikno        
note that the algorithm we introduce in this paper can be adapted to any cost model  an
important property of our cost sensitive setup is that maximizing generalization accuracy 
which is the goal of most existing learners  can be viewed as a special case  when accuracy
is the only objective  test costs are ignored and misclassification cost is uniform 

   the act algorithm
act  our proposed anytime framework for induction of cost sensitive decision trees  builds
on the recently introduced lsid  algorithm  lsid  adopts the general top down scheme
for induction of decision trees  tdidt   it starts from the entire set of training examples 
partitions it into subsets by testing the value of an attribute  and then recursively builds
subtrees  unlike greedy inducers  lsid  invests more time resources for making better split
decisions  for every candidate split  lsid  attempts to estimate the size of the resulting
subtree were the split to take place  following occams razor  blumer  ehrenfeucht  haussler    warmuth        esmeir   markovitch      b   it favors the one with the smallest
expected size 
the estimation is based on a biased sample of the space of trees rooted at the evaluated
attribute  the sample is obtained using a stochastic version of id   quinlan         which
we call sid   in sid   rather than choosing an attribute that maximizes the information
gain i  as in id    we choose the splitting attribute semi randomly  the likelihood that
an attribute will be chosen is proportional to its information gain  due to its randomization 
   in this model each test may belong to a single group  however  it is easy to extend our work to allow
tests that belong to several groups 

 

fiesmeir   markovitch

procedure lsid  choose attribute e  a  r 
if r    
return id  choose attribute e  a 
foreach a  a
foreach vi  domain a 
ei   e  e   a e    vi  
mini  
repeat r times
t  sid  ei   a   a  
mini  min  mini   size t   
p domain a  
totala  i  
mini
return a for which totala is minimal
figure    attribute selection in lsid 
repeated invocations of sid  result in different trees  for each candidate attribute a  lsid 
invokes sid  r times to form a sample of r trees rooted at a  and uses the size of the smallest
tree in the sample to evaluate a  obviously  when r is larger  the resulting size estimations
are expected to be more accurate  improving the final tree  consider  for example  a   xor
concept with several additional irrelevant attributes  for lsid  to prefer one of the relevant
attributes at the root  one of the trees in the samples of the relevant attributes must be the
smallest  the probability for this event increases with the increase in sample size 
lsid  is a contract anytime algorithm parameterized by r  the sample size  additional
time resources can be utilized by forming larger samples  figure   lists the procedure for
attribute selection as applied by lsid   let m    e  be the number of examples and
n    a  be the number of attributes  the runtime complexity of lsid  is o rmn     lsid 
was shown to exhibit good anytime behavior with diminishing returns  when applied to
hard concepts  it produced significantly better trees than id  and c    
act takes the same sampling approach as lsid   the three major components of
lsid  that need to be replaced in order to adapt it for cost sensitive problems are     
sampling the space of trees      evaluating a tree  and     pruning a tree 
    obtaining the sample
lisd  uses sid  to bias the samples towards small trees  in act  however  we would like
to bias our sample towards low cost trees  for this purpose  we designed a stochastic version
of the eg  algorithm  which attempts to build low cost trees greedily  in eg   a tree is
built top down  and the test that maximizes icf is chosen for splitting a node  where 
icf     

 i     
 
 cost        w

i is the information gain  as in id    the parameter w         controls the bias
towards lower cost attributes  when w      test costs are ignored and icf relies solely
 

fianytime induction of low cost  low error classifiers

procedure seg  choose attribute e  a 
foreach a  a
i  a   information gain e  a 
c  a   cost a 
 i a   
p  a    c a    
w
a  choose attribute at random from a 
for each attribute a  the probability
of selecting it is proportional to p  a 
return a
figure    attribute selection in seg 
on the information gain  larger values of w strengthen the effect of test costs on icf  we
discuss setting the value of w in section     
in stochastic eg   seg    we choose splitting attributes semi randomly  proportionally
to their icf  because seg  is stochastic  we expect to be able to escape local minima for at
least some of the trees in the sample  figure   formalizes the attribute selection component
in seg   to obtain a sample of size r  act uses eg  once and seg  r    times  eg 
and seg  are given direct access to context based costs  i e   if an attribute has already
been tested  its cost is zero and if another attribute that belongs to the same group has
been tested  a group discount is applied 
    evaluating a subtree
lsid  is a cost insensitive learning algorithm  as such  its main goal is to maximize the
expected accuracy of the learned tree  occams razor states that given two consistent
hypotheses  the smaller one is likely to be more accurate  following occams razor  lsid 
uses the tree size as a preference bias and favors splits that are expected to reduce its final
size 
in a cost sensitive setup  however  our goal is to minimize the expected total cost of
classification  therefore  rather than choosing an attribute that minimizes the size  we
would like to choose one that minimizes the total cost  given a decision tree  we need to
come up with a procedure that estimates the expected cost of using the tree to classify a
future case  this cost has two components  the test cost and the misclassification cost 
      estimating test costs
assuming that the distribution of future cases would be similar to that of the learning
examples  we can estimate the test costs using the training data  given a tree  we calculate
the average test cost of the training examples and use it to estimate the test cost of new
cases  for a  sub tree t built from e  a set of m training examples  we denote the average
cost of traversing t for an example from e by
tcost t  e   

  x
tcost t  e  
m ee
 

fiesmeir   markovitch

d
the estimated test cost for an unseen example e is therefore tcost t 
e     tcost t  e  
observe that costs are calculated in the relevant context  if an attribute a has already
been tested in upper nodes  we will not charge for testing it again  similarly  if an attribute
from a group g has already been tested  we will apply a group discount to the other attributes
from g  if a delayed attribute is encountered  we sum the cost of the entire subtree 

      estimating misclassification costs
how to go about estimating the cost of misclassification is not obvious  the tree size can
no longer be used as a heuristic for predictive errors  occams razor allows the comparison
of two consistent trees but provides no means for estimating accuracy  moreover  tree size
is measured in a different currency than accuracy and hence cannot be easily incorporated
in the cost function 
rather than using the tree size  we propose a different estimator  the expected error
 quinlan         for a leaf with m training examples  of which s are misclassified  the
expected error is defined as the upper limit on the probability for error  i e   ee m  s  cf    
bin  m  s   where cf is the confidence level and u bin is the upper limit of the confidence
ucf
interval for binomial distribution  the expected error of a tree is the sum of the expected
errors in its leaves 
originally  the expected error was used by c   s error based pruning to predict whether
a subtree performs better than a leaf  although lacking a theoretical basis  it was shown
experimentally to be a good heuristic  in act we use the expected error to approximate
the misclassification cost  assume a problem with  c  classes and a misclassification cost
matrix m   let c be the class label in a leaf l  let ml be the total number of examples in
l and mil be the number of examples in l that belong to class i  when the penalties for
predictive errors are uniform  mi j   mc   the estimated misclassification cost in l is
d  l    ee ml   ml  mc   cf    mc 
mcost
l

in a problem with nonuniform misclassification costs  mc should be replaced by the cost
of the actual errors the leaf is expected to make  these errors are obviously unknown to the
learner  one solution is to estimate each error type separately using confidence intervals
for multinomial distribution and multiply it by the associated cost 
d  l   
mcost

x

mul
ucf
 ml   mil    c    mc 

i  c

such approach  however  would result in an overly pessimistic approximation  mainly
when there are many classes  alternatively  we compute the expected error as in the uniform
case and propose replacing mc by the weighted average of the penalty for classifying an
instance as c while it belongs to another class  the weights are derived from the proportions
mil
ml mc using a generalization of laplaces law of succession  good        chap     
l

d  l    ee ml   ml  mc   cf   
mcost
l

x
i  c



 

mil    
 mc i  
ml  mcl    c    

note that in a problem with c classes  the average is over c    possible penalties
because mc c      hence  in a problem with two classes c    c  if a leaf is marked as c    mc
 

fianytime induction of low cost  low error classifiers

procedure act choose attribute e  a  r 
if r    
return eg  choose attribute e  a 
foreach a  a
foreach vi  domain a 
ei   e  e   a e    vi  
t  eg  a  ei   a   a  
mini  total cost t  ei  
repeat r    times
t  seg  a  ei   a   a  
mini  min  mini   total cost t  ei   
p domain a  
mini
totala  cost a    i  
return a for which totala is minimal
figure    attribute selection in act
would be replaced by m      when classifying a new instance  the expected misclassification
cost of a tree t built from m examples is the sum of the expected misclassification costs in
the leaves divided by m 
  x d
d
mcost l  
mcost t
  
m ll
where l is the set of leaves in t   hence  the expected total cost of t when classifying a
single instance is 
d
d
d
total t 
e    tcost t 
e    mcost t
  

an alternative approach that we intend to explore in future work is to estimate the cost
of the sampled trees using the cost for a set aside validation set  this approach is attractive
mainly when the training set is large and one can afford setting aside a significant part of
it 
    choosing a split
having decided about the sampler and the tree utility function  we are ready to formalize
the tree growing phase in act  a tree is built top down  the procedure for selecting a
splitting test at each node is listed in figure   and illustrated in figure    we give a
detailed example of how act chooses splits and explain how the split selection procedure
is modified for numeric attributes 
      choosing a split  illustrative examples
acts evaluation is cost senstive both in that it considers test and error costs simultaneously
and in that it can take into account different error penalties  to illustrate this let us consider
a two class problem with mc         uniform  and   attributes  a            a    whose costs are
     the training data contains     examples  out of which     are positive and     are
negative 
 

fiesmeir   markovitch

a

g 
se
st     
 

co
 

cost eg  
    
cost seg  
    

cost eg  
    

figure    attribute evaluation in act  assume that the cost of a in the current context is
     the estimated cost of a subtree rooted at a is therefore       min            
min                

test costs
a    
a    
a    
a    
a    
a    
mc costs
fp    
fn    

r  

t 

t 

a 
a 

   
   ee      

 

a 
a 

a 

    
 
ee      

 

   
   ee      

 

    
 
ee      

 

mcost  t                            
tcost  t        
total t          

   
   ee      

 

a 

     
   ee       

 

   
   ee      

 

     
   ee       

 

mcost  t                                          
tcost  t        
total  t          

figure    evaluation of tree samples in act  the leftmost column defines the costs   
attributes with identical cost and uniform error penalties  t   was sampled for a 
and t   for a    ee stands for the expected error  because the total cost of t  is
lower  act would prefer to split on a   

assume that we have to choose between a  and a    and that r      let the trees in
figure    denoted t   and t    be those sampled for a  and a  respectively  the expected
error costs of t  and t  are  
d
mcost t
    

d
mcost t
    

 

 
      
    ee                        
            
   
   
 
    ee                          ee                       
   
                
             
   

when both test and error costs are involved  act considers their sum  since the test
cost of both trees is identical        act would prefer to split on a    if  however  the cost
   in this example we set cf to       as in c     in section     we discuss how to tune cf  

  

fianytime induction of low cost  low error classifiers

test costs
a    
a    
a    
a    
a    
a    
mc costs
fp    
fn    

r  

t 

t 

a 
a 

 

a 

a 

    
 
ee      

   
   ee      

    
 
ee      

   
   ee      

a 

 

 

 

mcost  t                            
tcost  t        
total t          

a 

 

     
   ee       

   
   ee      

     
   ee       

   
   ee      

 

 

 

mcost  t                                          
tcost  t        
total  t          

figure    evaluation of tree samples in act  the leftmost column defines the costs   
attributes with identical cost  except for the expensive a    and uniform error
penalties  t   was sampled for a  and t   for a    because the total cost of t  is
lower  act would prefer to split on a   

test costs
a    
a    
a    
a    
a    
a    
mc costs
fp  
fn    

r  

t 

t 

a 
a 

   
   ee      

 

a 
a 

a 

    
 
ee      

 

   
   ee      

 

    
 
ee      

 

mcost  t                                         
tcost  t        
total  t          

   
   ee      

 

a 

     
   ee       

 

   
   ee      

 

     
   ee       

 

mcost  t                                          
tcost  t        
total  t          

figure    evaluation of tree samples in act  the leftmost column defines the costs   
attributes with identical cost and nonuniform error penalties  t   was sampled
for a  and t   for a    because the total cost of t  is lower  act would prefer to
split on a   

of a  were      as in figure    tcost t    would become     and the total cost of t   would
become        while that of t   would remain        hence  in this case act would split
on a   
to illustrate how act handles nonuniform error penalties  let us assume that the cost
of all attributes is again      while the cost of a false positive  fp   is    and the cost of a
false negative  f n   is       let the trees in figure    denoted t   and t    be those sampled
for a  and a  respectively  as in the first example  only misclassification costs play a role
because the test costs of both trees is the same  although on average the misclassification
  

fiesmeir   markovitch

cost is also       act now evaluates these trees differently 
d
mcost t
    

 

d
mcost t
    

 

 
    ee                         ee                      
   
                           
      
   
 
    ee                          ee                     
   
                          
      
   

therefore  in the nonuniform setup  act would prefer a    this makes sense because in the
given setup we prefer trees that may result in more false positives but reduce the number
of expensive false negatives 
      choosing a split when attributes are numeric
the selection procedure as formalized in figure   must be modified slightly when an attribute is numeric  rather than iterating over the values the attribute can take  we first pick
r tests  split points  with the highest information gain and then invoke eg  once for each
split point  this guarantees that numeric and nominal attributes get the same resources 
chickering  meek  and rounthwaite        introduced several techniques for generating a
small number of candidate split points dynamically with little overhead  in the future we
intend to apply these techniques to select r points  each of which will be evaluated with a
single invocation of eg  
    cost sensitive pruning
pruning plays an important role in decision tree induction  in cost insensitive environments 
the main goal of pruning is to simplify the tree in order to avoid overfitting the training
data  a subtree is pruned if the resulting tree is expected to yield a lower error 
when test costs are taken into account  pruning has another important role  reducing
test costs in a tree  keeping a subtree is worthwhile only if its expected reduction in misclassification costs is larger than the cost of the tests in that subtree  if the misclassification
cost is zero  it makes no sense to keep any split in the tree  if  on the other hand  the misclassification cost is much larger than the test costs  we would expect similar behavior to
the cost insensitive setup 
to handle this challenge  we propose a novel approach for cost sensitive pruning  as in
error based pruning  quinlan         we scan the tree bottom up  then we compare the
expected total cost of each subtree to that of a leaf  if a leaf is expected to perform better 
the subtree is pruned 
the cost of a subtree is estimated as described in section      formally  let e be the
set of training examples that reach a subtree t   and let m be the size of e  assume that
s examples in e do not belong to the default class   let l be the set of leaves in t   we
   if misclassification costs are uniform  the default class is the majority class  otherwise  it is the class
that minimizes the misclassification cost in the node 

  

fianytime induction of low cost  low error classifiers

prune t into a leaf if 
x
 
d
d
mcost l  
 ee m  s  cf    mc  tcost t 
e   
m
ll

the above assumes a uniform misclassification cost mc  in the case of nonuniform penalties 
we multiply the expected error by the average misclassification cost 
an alternative approach for post pruning is early stopping of the growing phase  for
example  one could limit the depth of the tree  require a minimal number of examples in
each child  as in c      or prevent splitting nodes when the splitting criterion fails to exceed
a predetermined threshold  as in dtmc   obviously  any pre pruning condition can also
be applied as part of the post pruning procedure  the advantage of post pruning  however 
is its ability to estimate the effect of a split on the entire subtree below it  and not only on
its immediate successors  horizon effect  
consider for example the   xor problem a  b  splitting on neither a nor b would
have a positive gain and hence the growing would be stopped  if no pre pruning is allowed 
the optimal tree would be found and would not be post pruned because the utility of the
splits is correctly measured  frank        reports a comprehensive study about pruning of
decision trees  in which he compared pre  to post pruning empirically in a cost insensitive
setup  his findings show that the advantage of post pruning on a variety of uci datasets
is not significant  because pre pruning is computationally more efficient  frank concluded
that  in practice  it might be a viable alternative to post pruning  despite these results  we
decided to use post pruning in act  for the following reasons 
   several concepts not represented in the uci repository may appear in real world
problems  for example  parity functions naturally arise in real world problems  such
as the drosophila survival concept  page   ray        
   when costs are involved  the horizon effect may appear more frequently because high
costs may hide good splits 
   in our anytime setup the user is willing to wait longer in order to obtain a good tree 
since post pruning takes even less time than the induction of a single greedy tree  the
extra cost of post pruning is minor 
in the future we plan to add a pre pruning parameter which will allow early stopping
when resources are limited  another interesting direction for future work would be to postprune the final tree but pre prune the lookahead trees that form the samples  this would
reduce the runtime at the cost of less accurate estimations for the utility of each candidate
split 
    setting the parameters of act
in addition to r  the sample size  act is parameterized by w  which controls the weight
of the test costs in eg   and cf   the confidence factor used both for pruning and for error
estimation  icet tunes w and cf using genetic search  in act we considered three different
alternatives      keeping eg s and c   s default values w     and cf             tuning
  

fiesmeir   markovitch

the values using cross validation  and     setting the values a priori  as a function of the
problem costs 
while the first solution is the simplest  it does not exploit the potential of adapting
the sampling mechanism to the specific problem costs  although tuning the values using
grid search would achieve good results  it may be costly in terms of runtime  for example 
if we had   values for each parameter and used   fold cross validation  we would need to
run act     times for the sake of tuning alone  in our anytime setup this time could be
invested to invoke act with a larger r and hence improve the results  furthermore  the
algorithm would not be able to output any valid solution before the tuning stage is finished 
alternatively  we could try to tune the parameters by invoking the much faster eg   but
the results would not be as good because the optimal values for eg  are not necessarily
good for act 
the third approach  which we chose for our experiments  is to set w and cf in advance 
according to the problem specific costs  w is set inverse proportionally to the misclassification cost  a high misclassification cost results in a smaller w  reducing the effect of attribute
costs on the split selection measure  the exact formula is 
w         ex  
where x is the average misclassification cost  over all non diagnoal entries in m   divided by
t c  the cost if we take all tests  formally 
x 

p

mi j
 
  c        c   t c
i  j

in c    the default value of cf is       larger cf values result in less pruning  smaller
cf values lead to more aggressive pruning  therefore  in act we set cf to a value in the
range             the exact value depends on the problem cost  when test costs are dominant 
we prefer aggressive pruning and hence a low value for cf   when test costs are negligible 
we prefer to prune less  the same value of cf is also used to estimate the expected error 
again  when test costs are dominant  we can afford a pessimistic estimate of the error  but
when misclassification costs are dominant  we would prefer that the estimate be closer to
the error rate in the training data  the exact formula for setting cf is 
x 
cf                 
  
x  

   empirical evaluation
we conducted a variety of experiments to test the performance and behavior of act 
first we introduce a novel method for automatic adaption of existing datasets to the costsensitive setup  we then describe our experimental methodology and its motivation  finally
we present and discuss our results 
    datasets
typically  machine learning researchers use datasets from the uci repository  asuncion  
newman         only five uci datasets  however  have assigned test costs   we include
   costs for these datasets have been assigned by human experts  turney        

  

fianytime induction of low cost  low error classifiers

these datasets in our experiments  nevertheless  to gain a wider perspective  we have
developed an automatic method that assigns costs to existing datasets  the method is
parameterized with 
   cr  the cost range 
   g  the number of desired groups as a percentage of the number of attributes  in a
problem with  a  attributes  there are g   a  groups  the probability for an attribute
 
  as is the probability for it not to belong
to belong to each of these groups is g a   
to any of the groups 
   d  the number of delayed tests as a percentage of the number of attributes 
     the group discount as a percentage of the minimal cost in the group  to ensure
positive costs  
     a binary flag which determines whether costs are drawn randomly  uniformly       
or semi randomly         the cost of a test is drawn proportionally to its information
gain  simulating a common case where valuable features tend to have higher costs  in
this case we assume that the cost comes from a truncated normal distribution  with
the mean being proportional to the gain 
using this method  we assigned costs to    datasets     arbitrarily chosen uci datasets 
and   datasets that represent hard concepts and have been used in previous research  appendix a gives detailed descriptions of these datasets 
due to the randomization in the cost assignment process  the same set of parameters
defines an infinite space of possible costs  for each of the    datasets we sampled this space
  times with
cr             g        d                   
these parameters were chosen in an attempt to assign costs in a manner similar to that in
which real costs are assigned  in total  we have     datasets    assigned by human experts
and     with automatically generated costs  
cost insensitive learning algorithms focus on accuracy and therefore are expected to perform well when testing costs are negligible relative to misclassification costs  however  when
testing costs are significant  ignoring them would result in expensive classifiers  therefore 
evaluating cost sensitive learners requires a wide spectrum of misclassification costs  for
each problem out of the      we created   instances  with uniform misclassification costs
mc                                later on  we also consider nonuniform misclassification
costs 
    methodology
we start our experimental evaluation by comparing act  given a fixed resource allocation  with several other cost sensitive and cost insensitive algorithms  next we compare
the anytime behavior of act to that of icet  finally  we evaluate the algorithms with
   the chosen uci datasets vary in size  type of attributes  and dimension 
   the additional     datasets are available at http   www cs technion ac il esaher publications cost 

  

fiesmeir   markovitch

two modifications on the problem instances  random test cost assignment and nonuniform
misclassification costs 
      compared algorithms
act is compared to the following algorithms 
 c      a cost insensitive greedy decision tree learner  the algorithm has been reimplemented following the details in  quinlan        and the default parameters have
been used 
 lsid    a cost insensitive anytime decision tree learner  as such it uses extra time to
induce trees of higher accuracy  it is not able  however  to exploit additional allotted
time to reduce classification costs 
 idx   a greedy top down learner that prefers splits that maximize i
c  norton        
the algorithm does not take into account misclassification costs  idx has been implemented on top of c     by modifying the split selection criteria 
 

 csid    a greedy top down learner that prefers splits that maximize ic  tan  
schlimmer         the algorithm does not take into account misclassification costs 
csid  has been implemented on top of c     by modifying the split selection criteria 
 eg    a greedy top down learner that prefers splits that maximize

 i    

w

 cost     
 nunez         the algorithm does not take into account misclassification costs 
eg  has been implemented on top of c     by modifying the split selection criteria 

 dtmc   dtmc was implemented by following the original pseudo code  ling et al  
      sheng et al          however  the original pseudo code does not support continuous attributes and multiple class problems  we added support to continuous
attributes  as in c   s dynamic binary cut discretization  with the cost reduction
replacing gain ratio for selecting cutting points  the extension to multiple class problems was straightforward  note that dtmc does not post prune the trees but only
pre prunes them 
 icet   icet has been reimplemented following the detailed description given by
turney         to verify the results of the reimplementation  we compared them
with those reported in the literature  we followed the same experimental setup and
used the same   datasets  the results are indeed similar  the basic version of icet
achieved an average cost of      in our reimplementation vs     reported originally 
one possible reason for the slight difference may be that the initial population of
the genetic algorithm is randomized  as are the genetic operators and the process of
partitioning the data into training  validating  and testing sets  in his paper  turney
introduced a seeded version of icet  which includes the true costs in the initial
population  and reported it to perform better than the unseeded version  therefore 
we use the seeded version for our comparison  the other parameters of icet are the
default ones 
  

fianytime induction of low cost  low error classifiers

      normalized cost
as turney        points out  using the average cost of classification for each dataset is
problematic because      the cost differences of the algorithms become relatively small as
the misclassification cost increases      it is difficult to combine the results for multiple
datasets in a fair manner  e g   average   and     it is difficult to combine the average of the
different misclassification costs  to overcome these problems  turney suggests normalizing
the average cost of classification by dividing it by the standard cost  let t c be the cost if
we take all tests  let fi be the frequency of class i in the data  the error if the response is
always class i is therefore     fi    the standard cost is defined as
t c   mini     fi    maxi j  mi j    
the standard cost is an approximation for the maximal cost in a given problem  it
consists of two components  the maximal test cost and the misclassification cost if the
classifier achieves only the baseline accuracy  e g   a majority based classifier when error
costs are uniform   because some classifiers may perform even worse than the baseline
accuracy  the standard cost is not strictly an upper bound on real cost  in most of our
experiments  however  it has not been exceeded 
      statistical significance
for each problem out of the      a single    fold cross validation experiment was conducted 
the same partition to train test sets was used for all compared algorithms  to determine
statistical significance of the performance differences between act  icet  and dtmc we
used two tests 
 paired t test with       confidence  for each problem out of the     and for
each pair of algorithms  we have    pairs of results obtained from the    fold cross
validation runs  we used paired t test to determine weather the difference between
the two algorithms on a given problem is significant  rejecting the null hypothesis that
the algorithms do not differ in their performance   then  we count for each algorithm
how many times it was a significant winner 
 wilcoxon test  demsar         which compares classifiers over multiple datasets and
states whether one method is significantly better than the other         
    fixed time comparison
for each of the        problem instances  we ran the different algorithms  including act
with r      we chose r     so the average runtime of act would be shorter than icet over
all problems  the other methods have much shorter runtime due to their greedy nature 
table   summarizes the results   each pair of numbers represents the average normalized
cost and its associated confidence interval          figure    illustrates the average results
and plots the normalized costs for the different algorithms and misclassification costs 
statistical significance test results for act  icet  and dtmc are given in table   
the algorithms are compared using both the t test and the wilcoxon test  the table lists
   the full results are available at http   www cs technion ac il esaher publications cost 

  

fiesmeir   markovitch

table    average cost of classification as a percentage of the standard cost of classification
for different mc values  the numbers represent the average over all     datasets
and the associated confidence intervals         
mc
   
   
    
    
     

c   
    
    
    
    
    

lsid 

   
   
   
   
   

    
    
    
    
    

idx

   
   
   
   
   

    
    
    
    
    

csid 

   
   
   
   
   

    
    
    
    
    

   
   
   
   
   

eg 
    
    
    
    
    

dtmc

   
   
   
   
   

    
    
    
    
    

   
   
   
   
   

icet
    
    
    
    
    

   
   
   
   
   

act
    
    
    
    
    

   
   
   
   
   

table    dtmc vs  act and icet vs  act using statistical tests  for each mc  the first
column lists the number of t test significant wins while the second column gives
the winner  if any  as implied by a wilcoxon test over all datasets with       
t  test wins
mc
   
   
    
    
     

dtmc vs  act
  
 
 
 
 

 
  
  
  
  

w ilcoxon winner

icet vs  act
 
 
  
  
 

  
  
  
  
  

dtmc vs  act

icet vs  act

dtmc
act
act
act
act

act
act
act
act
 

the number of t test wins for each algorithm out of the     datasets  as well as the winner 
if any  when the wilcoxon test was applied 
when misclassification cost is relatively small  mc         act clearly outperforms
icet  with    significant wins as opposed to icets   significant wins  no significant
difference was found in the remaining runs  in this setup act was able to produce very
small trees  sometimes consisting of one node  the accuracy of the learned model was ignored
in this setup  icet  on the contrary  produced  for some of the datasets  larger and more
costly trees  dtmc achieved the best results  and outperformed act    times  the
wilcoxon test also indicates that dtmc is better than act and that act is better than
icet  further investigation showed that for a few datasets act produced unnecessarily
larger trees  we believe that a better tuning of cf would improve act in this scenario by
making the pruning more aggressive 
at the other extreme  when misclassification costs dominate  mc           the performance of dtmc is worse than act and icet  the t test indicates that act was
significantly better than icet    times and significantly worse only   times  according to
the wilcoxon test with        the difference between act and icet is not significant 
taking           however  would turn the result in favor of act  observe that dtmc 
the winner when mc        becomes the worst algorithm when mc          one reason
  

fianytime induction of low cost  low error classifiers

average   standard cost

  

  

  

  
c   
lsid 
eg 
dtmc
icet
act

  

  
   

    
misclassification cost

     

act cost

figure     average normalized cost as a function of misclassification cost

   

   

   

  

  

  

  

  

  

  

  

  

  

  

  

 

 
 

  

  
  
icet cost

  

   

 
 

  

  
  
icet cost

  

   

 

  

  
  
icet cost

  

   

figure     illustration of the differences in performance between act and icet for mc  
                  from left to right   each point represents a dataset  the x axis
represents the cost of icet while the y axis represents that of act  the dashed
line indicates equality  points are below it if act performs better and above it
if icet is better 

for this phenomenon is that dtmc  as introduced by ling et al          does not perform
post pruning  although doing so might improve accuracy in some domains 
the above two extremes are less interesting  for the first we could use an algorithm that
always outputs a tree of size   while for the second we could use cost insensitive learners 
the middle range  where mc                     requires that the learner carefully balance
the two types of cost  in these cases act has the lowest average cost and the largest
number of t test wins  moreover  the wilcoxon test indicates that it is superior  icet is
the second best method  as reported by turney         icet is clearly better than the
greedy methods eg   idx  and csid  
note that eg   idx  and csid   which are insensitive to misclassification cost  produced the same trees for all values of mc  these trees  however  are judged differently with
the change in misclassification cost 
figure    illustrates the differences between icet and act for mc                    
each point represents one of the     datasets  the x axis represents the cost of icet while
the y axis represents that of act  the dashed line indicates equality  as we can see  the
  

fiesmeir   markovitch

   

average accuracy

  

  

  
c   
lsid 
eg 
dtmc
icet
act

  

  
   

    
misclassification cost

     

figure     average accuracy as a function of misclassification cost

majority of points are below the equality line  indicating that act performs better  for
mc         we can see that there are points located close to the x axis but with large x
value  these points represent the difficult domains  such as xor  which icet could not
learn but act could 
    comparing the accuracy of the learned models
when misclassification costs are low  an optimal algorithm would produce a very shallow
tree  when misclassification costs are dominant  an optimal algorithm would produce a
highly accurate tree  as we can see  acts normalized cost increases with the increase in
misclassification cost  while it is relatively easy to produce shallow trees  some concepts
are not easily learnable and even cost insensitive algorithms fail to achieve perfect accuracy
on them  hence  as the importance of accuracy increases  the normalized cost increases too
because the predictive errors affect it more dramatically 
to learn more about the effect of misclassification costs on accuracy  we compare the
accuracy of the built trees for different misclassification costs  figure    shows the results 
an important property of dtmc  icet  and act is their ability to compromise on accuracy when needed  they produce inaccurate trees when accuracy is insignificant and much
more accurate trees when the penalty for error is high  acts flexibility  however  is more
noteworthy  from the second least accurate method it becomes the most accurate one 
interestingly  when accuracy is extremely important  both icet and act achieve even
better accuracy than c     the reason is their non greedy nature  icet performs an
implicit lookahead by reweighting attributes according to their importance  act performs
lookahead by sampling the space of subtrees before every split  of the two  the results
indicate that acts lookahead is more efficient in terms of accuracy  dtmc is less accurate
than c     the reason is the different split selection criterion and the different pruning
mechanism 
in comparison to our anytime cost insensitive algorithm lsid   act produced less
accurate trees when mc was relatively low  when mc was set to       however  act
achieved comparable accuracy to lsid  and slightly outperformed it for mc         
statistical tests found the differences between the accuracy of the two algorithms in this
  

fianytime induction of low cost  low error classifiers

  
eg 
dtmc
icet
act

  
 
average cost

average cost

  
  
eg 
dtmc
icet
act

  
  

 

 

 

  
  

 
 

   

 

   

 
   
time  sec 

 

   

 

   

 

   

   

   

   
 
time  sec 

   

   

   

   

  
   
  

  
  
average cost

average cost

  
  
  
eg 
dtmc
icet
act

  
  

eg 
dtmc
icet
act

  
  
  
  
  

  

  
 
 

 

 
 
time  sec 

 

 

 

 

 

 
time  sec 

 

 

 

figure     average normalized cost as a function of time for  from top left to bottom right 
breast cancer     monks   multi xor  and xor 

case to be insignificant  acts small advantage on some of the datasets indicates that  for
some problems  expected error is a better heuristic than tree size for maximizing accuracy 
    comparison of anytime behavior
both icet and act  like other typical anytime algorithms  perform better with increased
resource allocation  icet is expected to exploit the extra time by producing more generations and hence better tuning the parameters for the final invocation of eg   act can
use the extra time to acquire larger samples and hence achieve better cost estimations 
to examine the anytime behavior of icet and act  we ran them on   problems 
namely breast cancer     monks    multi xor  and xor   with exponentially increasing
time allocation  mc was set to       icet was run with                generations and act
with a sample size of                 as in the fixed time comparison  we used   instances for
each problem  figure    plots the results averaged over the   instances  we also included
the results for the greedy methods eg  and dtmc 
the results show good anytime behavior of both icet and act  generally it is worthwhile to allocate more time  act dominates icet for the four domains and is able to
produce less costly trees in shorter time 
one advantage of act over icet is that it is able to consider the context in which an
attribute is judged  icet  on the contrary  reassigns the cost of the attributes globally  an
  

fiesmeir   markovitch

average   standard cost

  

  

dtmc

icet

act

  

   
   
    
    
     

  

  

  
   

dtmc
icet
act
    
misclassification cost

    
    
    
    
    

   
   
   
   
   

    
    
    
    
    

   
   
   
   
   

    
    
    
    
    

   
   
   
   
   

     

figure     average cost when test costs are assigned randomly

attribute cannot be assigned a high cost in one subtree and a low cost in another  the multixor dataset exemplifies a concept whose attributes are important only in one sub concept 
the concept is composed of four sub concepts  each of which relies on different attributes
 see appendix a for further details   as we expected  act outperforms icet significantly
because the latter cannot assign context based costs  allowing icet to produce more and
more generations  up to      does not result in trees comparable to those obtained by act 
    random costs
the costs of     out of the     datasets were assigned using a semi random mechanism that
gives higher costs to informative attributes  to ensure that acts success is not due to
this particular cost assignment scheme  we repeated the experiments with the costs drawn
randomly uniformly from the given cost range cr  i e    was set to    figure    shows the
results  as we can see  act maintains its advantage over the other methods  it dominates
them along the scale of mc values 
    nonuniform misclassification costs
so far  we have only used uniform misclassification cost matrices  i e   the cost of each
error type was identical  as explained in section    the act algorithm can also handle
complex misclassification cost matrices where the penalty for one type of error might be
higher than the penalty for another type  our next experiment examines act in the
nonuniform scenario  let fp denote the penalty for a false positive and fn the penalty for
false negative  when there are more than   classes  we split the classes into   equal groups
according to their order  or randomly if no order exists   we then assign a penalty fp for
misclassifying an instance that belongs to the first group and fn for one that belongs to
the second group 
to obtain a wide view  we vary the ratio between fp and fn and also examine different
absolute values  table   and figure    give the average results  table   lists the number
of t test significant wins each algorithm achieved  it is easy to see that act consistently
outperforms the other methods 
  

fianytime induction of low cost  low error classifiers


 


 


 




 


 


 


      

c   
eg 
dtcm
icet
act

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

       

table    comparison of c     eg   dtmc  act  and icet when misclassification costs
are nonuniform  fp denotes the penalty for a false positive and fn the penalty
for a false negative   denotes the basic mc unit 

c   
eg 
dtcm
icet
act

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    
    
    

fp
fn

table    comparing dtmc  act  and icet when misclassification costs are nonuniform 
for each f p f n ratio  the columns list the number of t test significant wins with
       fp denotes the penalty for a false positive and fn the penalty for a
false negative   denotes the basic mc unit 
      
f p f n
     
    
   
 
 
 
 

dtmc vs  act
 
 
  
 
 
 
 

  
  
  
  
  
  
  

       

icet vs  act
  
 
 
 
 
 
 

  
  
  
  
  
  
  

dtmc vs  act
 
  
  
 
 
 
 

  
  
  
  
  
  
  

icet vs  act
  
 
  
  
 
 
 

  
  
  
  
  
  
  

interestingly  the graphs are slightly asymmetric  the reason could be that for some
datasets  for example medical ones  it is more difficult to reduce negative errors than positive
ones  or vice versa  a similar phenomenon is reported by turney        
the highest cost for all algorithms is observed when f p   f n because  when the
ratio between fp and fn is extremely large or extremely small  the learner can easily
build a small tree whose leaves are labeled with the class that minimizes costs  when
misclassification costs are more balanced  however  the learning process becomes much
more complicated 
  

fi  

  

  

  
average   standard cost

average   standard cost

esmeir   markovitch

  
  
  
  
c   
eg 
dtmc
icet
act

  
  
  
  

  

  
 
  
misclassification cost fp fn

  
  
  
  
  
  

c   
eg 
dtmc
icet
act

  
  

  

  
  

  

  

  
 
  
misclassification cost fp fn

  

  

figure     comparison of c     eg   dtmc  act  and icet when misclassification costs
are nonuniform  the misclassification costs are represented as a pair  f p f n   
fp denotes the penalty for a false positive and fn the penalty for a false
negative   denotes the basic mc unit  the figures plot the average cost as
a function of the ratio between fp and fn  for         left  and        
 right  

   related work
in addition to the works referred to earlier in this paper  several related works warrant
discussion here 
cost sensitive trees have been the subject of many research efforts  several works proposed learning algorithms that consider different misclassification costs  breiman  friedman  olshen    stone        pazzani  merz  murphy  ali  hume    brunk        provost
  buchanan        bradford  kunz  kohavi  brunk    brodley        domingos       
elkan        zadrozny  langford    abe        lachiche   flach        abe  zadrozny   
langford        vadera        margineantu        zhu  wu  khoshgoftaar    yong       
sheng   ling         these methods  however  do not consider test costs and hence are
appropriate mainly for domains where test costs are not a constraint 
davis  ha  rossbach  ramadan  and witchel        presented a greedy cost sensitive
decision tree algorithm for forensic classification  the problem of classifying irreproducible
events  in this setup  they assume that all tests that might be used for testing must be
acquired and hence charged for before classification 
one way to exploit additional time when searching for a less costly tree is to widen
the search space  bayer zubek and dietterich        formulated the cost sensitive learning
problem as a markov decision process  mdp   and used a systematic search algorithm
based on the ao  heuristic search procedure to solve the mdp  to make ao  efficient 
the algorithm uses a two step lookahead based heuristic  such limited lookahead is more
informed than immediate heuristics but still insufficient for complex domains and might
cause the search to go astray  esmeir   markovitch      a   the algorithm was shown to
output better diagnostic policies than several greedy methods using reasonable resources 
an optimal solution  however  could not always be found due to time and memory limits 
a nice property of the algorithm is that it can serve as an anytime algorithm by computing
  

fianytime induction of low cost  low error classifiers

the best complete policy found so far  its anytime behavior  nevertheless  is problematic
because policies that are optimal with respect to the train data tend to overfit  as a result 
the performance will eventually start to degrade 
arnt and zilberstein        tackled the problem of time and cost sensitive classification
 tcsc   in tcsc  the utility of labeling an instance depends not only on the correctness of
the labeling  but also the amount of time it takes  therefore the total cost function has an
additional component  which reflects the time needed to measure an attribute  typically 
is has a super linear form  the cost of a quick result is small and fairly constant  but as
the waiting time increases  the time cost grows at an increasing rate  the problem is
further complicated when a sequence of time sensitive classification instances is considered 
where time spent administering tests for one case can adversely affect the costs of future
instances  arnt and zilberstein suggest solving these problems by extending the decision
theoretic approach introduced by bayer zubek and dietterich         in our work  we
assume that the time it takes to administer a test is incorporated into its cost  in the
future  we intend to extend our framework to support time sensitive classification  both for
individual cases and for sequences 
fan  lee  stolfo  and miller        studied the problem of cost sensitive intrusion detection systems  ids   the goal is to maximize security while minimizing costs  each
prediction  action  has a cost  features are categorized into three cost levels according to
amount of information needed to compute their values  to reduce the cost of an ids  high
cost rules are considered only when the predictions of low cost rules are not sufficiently
accurate 
costs are also involved in the learning phase  during example acquisition and during
model learning  the problem of budgeted learning has been studied by lizotte  madani 
and greiner         there is a cost associated with obtaining each attribute value of a
training example  and the task is to determine what attributes to test given a budget 
a related problem is active feature value acquisition  in this setup one tries to reduce
the cost of improving accuracy by identifying highly informative instances  melville  saartsechansky  provost  and mooney        introduced an approach in which instances are
selected for acquisition based on the accuracy of the current model and its confidence in
the prediction 
greiner  grove  and roth        were pioneers in studying classifiers that actively decide
what tests to administer  they defined an active classifier as a classifier that given a
partially specified instance  returns either a class label or a strategy that specifies which
test should be performed next  greiner et al  also analyzed the theoretical aspects of
learning optimal active classifiers using a variant of the probably approximately correct
 pac  model  they showed that the task of learning optimal cost sensitive active classifiers
is often intractable  however  this task is shown to be achievable when the active classifier
is allowed to perform only  at most  a constant number of tests  where the limit is provided
before learning  for this setup they proposed taking a dynamic programming approach to
build trees of at most depth d 
our setup assumed that we are charged for acquiring each of the feature values of the test
cases  the term test strategy  sheng  ling    yang        describes the process of feature
values acquisition  which values to query for and in what order  several test strategies have
been studied  including sequential  single batch and multiple batch  sheng et al         
  

fiesmeir   markovitch

each of which corresponds to a different diagnosis policy  these strategies are orthogonal
to our work because they assume a given decision tree 
bilgic and getoor        tackled the problem of feature subset selection when costs are
involved  the objective is to minimize the sum of the information acquisition cost and the
misclassification cost  unlike greedy approaches that compute the value of features one at
a time  they used a novel data structure called the value of information lattice  voila  
which exploits dependencies between missing features and makes it possible to share information value computations between different feature subsets possible  viola was shown
empirically to achieve dramatic cost improvements without the prohibitive computational
costs of comprehensive search 

   conclusions
machine learning techniques are increasingly being used to produce a wide range of classifiers for complex real world applications that involve nonuniform testing and misclassification costs  the increasing complexity of these applications poses a real challenge to
resource management during learning and classification  in this work we introduced a novel
framework for operating in such complex environments  our framework has four major
advantages 
 it uses a non greedy approach to build a decision tree and therefore is able to overcome
local minima problems 
 it evaluates entire trees during the search  thus  it can be adjusted to any cost scheme
that is defined over trees 
 it exhibits good anytime behavior and allows learning speed to be traded for classification costs  in many applications we are willing to allocate more time than we
would allocate to greedy methods  our proposed framework can exploit such extra
resources 
 the sampling process can easily be parallelized and the method benefit from distributed computer power 
to evaluate act we have designed an extensive set of experiments with a wide range
of costs  since there are only a few publicly available cost oriented datasets  we designed a
parametric scheme that automatically assigns costs for existing datasets  the experimental
results show that act is superior to icet and dtmc  existing cost sensitive algorithms
that attempt to minimize test costs and misclassification costs simultaneously  significance
tests found the differences to be statistically strong  act also exhibited good anytime
behavior  with the increase in time allocation  the cost of the learned models decreased 
act is a contract anytime algorithm that requires its sample size to be predetermined 
in the future we intend to convert act into an interruptible anytime algorithm by adopting
the iidt general framework  esmeir   markovitch      a   in addition  we plan to apply
monitoring techniques  hansen   zilberstein        for optimal scheduling of act and to
examine other strategies for evaluating subtrees 
  

fianytime induction of low cost  low error classifiers

table    characteristics of the datasets used

dataset
breast cancer
bupa
car
flare
glass
heart
hepatitis
iris
krk
monks  
monks  
monks  
multiplexer   
multi xor
multi and or
nursery
pima
tae
tic tac toe
titanic
thyroid
voting
wine
xor  d
xor  

instances
   
   
    
   
   
   
   
   
     
       
       
       
   
   
   
    
   
   
   
    
    
   
   
   
   

attributes
nominal  binary  numeric
     
     
     
      
     
    
      
     
    
     
     
     
       
       
       
    
    
    
     
    
      
       
     
     
       

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 

max attribute
domain

classes

  
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 

 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

acknowledgments
this work was partially supported by funding from the ec sponsored muscle network
of excellence  fp          

appendix a  datasets
table   lists the characteristics of the    datasets we used  below we give a more detailed
description of the non uci datasets used in our experiments 
   multiplexer  the multiplexer task was used by several researchers for evaluating classifiers  e g   quinlan         an instance is a series of bits of length a    a   where a is
a positive integer  the first a bits represent an index into the remaining bits and the
label of the instance is the value of the indexed bit  in our experiments we considered
the    multiplexer  a       the dataset contains     randomly drawn instances 
   boolean xor  parity like functions are known to be problematic for many learning
algorithms  however  they naturally arise in real world data  such as the drosophila
survival concept  page   ray         we considered xor of five variables with five
additional irrelevant attributes 
  

fiesmeir   markovitch

   numeric xor  a xor based numeric dataset that has been used to evaluate learning
algorithms  e g   baram  el yaniv    luz         each example consists of values for
x and y coordinates  the example is labeled   if the product of x and y is positive  and
  otherwise  we generalized this domain for three dimensions and added irrelevant
variables to make the concept harder 
   multi xor   multi and or  these concepts are defined over    binary attributes 
in both cases the target concept is composed of several subconcepts  where the first
two attributes determines which of them is considered  the other    attributes are
used to form the subconcepts  in the multi xor dataset  each subconcept is an xor 
and in the multi and or dataset  each subconcept is either and or or 

references
abe  n   zadrozny  b     langford  j          an iterative method for multi class costsensitive learning  in proceedings of the   th acm sigkdd international conference
on knowledge discovery and data mining  kdd        seattle  wa  usa 
arnt  a     zilberstein  s          learning policies for sequential time and cost sensitive
classification  in proceedings of the  st international workshop on utility based data
mining  ubdm    held with kdd    pp        new york  ny  usa  acm press 
asuncion  a     newman  d         
uci machine learning repository 
university of california  irvine  school of information and computer sciences 
http   www ics uci edu mlearn mlrepository html 
baram  y   el yaniv  r     luz  k          online choice of active learning algorithms  in
proceedings of the    international conference on machine learning  icml       
pp        washington  dc  usa 
bayer zubek  v     dietterich         integrating learning from examples into the search
for diagnostic policies  artificial intelligence             
bilgic  m     getoor  l          voila  efficient feature value acquisition for classification  in
proceedings of the   nd national conference on artificial intelligence  aaai       
blumer  a   ehrenfeucht  a   haussler  d     warmuth  m  k          occams razor 
information processing letters                 
boddy  m     dean  t  l          deliberation scheduling for problem solving in time
constrained environments  artificial intelligence                 
bradford  j   kunz  c   kohavi  r   brunk  c     brodley  c          pruning decision
trees with misclassification costs  in proceedings of the  th european conference on
machine learning  ecml        pp         
breiman  l   friedman  j   olshen  r     stone  c          classification and regression
trees  wadsworth and brooks  monterey  ca 
chickering  d  m   meek  c     rounthwaite  r          efficient determination of dynamic
split points in a decision tree  in proceedings of the  st ieee international conference
  

fianytime induction of low cost  low error classifiers

on data mining  icdm        pp        washington  dc  usa  ieee computer
society 
davis  j  v   ha  j   rossbach  c  j   ramadan  h  e     witchel  e          cost sensitive
decision tree learning for forensic classification  in proceedings of the   th european
conference on machine learning  ecml        pp          berlin  germany 
demsar  j          statistical comparisons of classifiers over multiple data sets  journal of
machine learning research         
domingos  p          metacost  a general method for making classifiers cost sensitive  in
proceedings of the  th international conference on knowledge discovery and data
mining  kdd       pp         
elkan  c          the foundations of cost sensitive learning  in proceedings of the   th
international joint conference on artificial intelligence  ijcai        pp         
seattle  washington  usa 
esmeir  s     markovitch  s          when a decision tree learner has plenty of time  in
proceedings of the   st national conference on artificial intelligence  aaai       
boston  ma  usa 
esmeir  s     markovitch  s       a   anytime learning of decision trees  journal of machine
learning research    
esmeir  s     markovitch  s       b   occams razor just got sharper  in proceedings
of the   th international joint conference in artificial intelligence  ijcai       
hyderabad  india 
fan  w   lee  w   stolfo  s  j     miller  m          a multiple model cost sensitive approach
for intrusion detection  in proceedings of the   th european conference on machine
learning  ecml        pp          barcelona  catalonia  spain 
frank  e          pruning decision trees and lists  ph d  thesis  department of computer
science  university of waikato 
good  i          the estimation of probabilities  an essay on modern bayesian methods 
mit press  usa 
greiner  r   grove  a  j     roth  d          learning cost sensitive active classifiers 
artificial intelligence                  
hansen  e  a     zilberstein  s          monitoring and control of anytime algorithms  a
dynamic programming approach  artificial intelligence                    
hastie  t   tibshirani  r     friedman  j          the elements of statistical learning 
data mining  inference  and prediction  new york  springer verlag 
hyafil  l     rivest  r  l          constructing optimal binary decision trees is npcomplete  information processing letters              
lachiche  n     flach  p          improving accuracy and cost of two class and multiclass probabilistic classifiers using roc curves  in proceedings of the   th international
conference on machine learning  icml       
  

fiesmeir   markovitch

lenert  l     soetikno  r          automated computer interviews to elicit utilities  potential applications in the treatment of deep venous thrombosis  american medical
informatics association              
ling  c  x   yang  q   wang  j     zhang  s          decision trees with minimal costs  in
proceedings of the   st international conference on machine learning  icml       
lizotte  d  j   madani  o     greiner  r          budgeted learning of naive bayes classifiers 
in proceedings of the   th conference on uncertainty in artificial intelligence  uai       acapulco  mexico 
margineantu  d          active cost sensitive learning  in proceedings of the   th international joint conference on artificial intelligence  ijcai        edinburgh  scotland 
melville  p   saar tsechansky  m   provost  f     mooney  r  j          active feature acquisition for classifier induction  in proceedings of the  th ieee international conference
on data mining  icdm        pp          brighton  uk 
norton  s  w          generating better decision trees  in sridharan  n  s   ed    proceedings of the eleventh international joint conference on artificial intelligence  pp 
        detroit  michigan  usa 
nunez  m          the use of background knowledge in decision tree induction  machine
learning            
page  d     ray  s          skewing  an efficient alternative to lookahead for decision
tree induction  in proceedings of the   th international joint conference on artificial
intelligence  ijcai        acapulco  mexico 
pazzani  m   merz  c   murphy  p   ali  k   hume  t     brunk  c          reducing
misclassification costs  knowledge intensive approaches to learning from noisy data 
in proceedings of the   th international conference on machine learning  icml      
provost  f     buchanan  b          inductive policy  the pragmatics of bias selection 
machine learning                 
qin  z   zhang  s     zhang  c          cost sensitive decision trees with multiple cost
scales  lecture notes in computer scienc  ai       advances in artificial intelligence 
volume                   
quinlan  j  r          induction of decision trees  machine learning           
quinlan  j  r          c     programs for machine learning  morgan kaufmann  san
mateo  ca 
sheng  s   ling  c  x   ni  a     zhang  s          cost sensitive test strategies  in
proceedings of the   st national conference on artificial intelligence  aaai       
boston  ma  usa 
sheng  s   ling  c  x     yang  q          simple test strategies for cost sensitive decision
trees  in proceedings of the  th european conference on machine learning  ecml       pp          porto  portugal 
  

fianytime induction of low cost  low error classifiers

sheng  v  s     ling  c  x          roulette sampling for cost sensitive learning  in
proceedings of the   th european conference on machine learning  ecml       
pp          warsaw  poland 
tan  m     schlimmer  j  c          cost sensitive concept learning of sensor use in approach and recognition  in proceedings of the  th international workshop on machine
learning  pp          ithaca  ny 
turney  p          types of cost in inductive concept learning  in proceedings of the
workshop on cost sensitive learning held with the   th international conference on
machine learning  icml        stanford  ca 
turney  p  d          cost sensitive classification  empirical evaluation of a hybrid genetic
decision tree induction algorithm  journal of artificial intelligence research        
    
vadera  s          inducing cost sensitive non linear decision trees  technical report            school of computing  science and engineering  university of salford 
zadrozny  b   langford  j     abe  n          cost sensitive learning by cost proportionate
example weighting  in proceedings of the  rd ieee international conference on data
mining  icdm        melbourne  florida  usa 
zhu  x   wu  x   khoshgoftaar  t     yong  s          an empirical study of the noise
impact on cost sensitive learning  in proceedings of the   th international joint conference on artificial intelligence  ijcai        hyderabad  india 

  

fi