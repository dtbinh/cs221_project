journal of artificial intelligence research                  

submitted        published      

accelerating reinforcement learning
through implicit imitation
bob price

price cs ubc ca

department of computer science
university of british columbia
vancouver  b c   canada v t  z 

craig boutilier

cebly cs toronto edu

department of computer science
university of toronto
toronto  on  canada m s  h 

abstract
imitation can be viewed as a means of enhancing learning in multiagent environments 
it augments an agents ability to learn useful behaviors by making intelligent use of the
knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents  we propose and study a formal model of implicit imitation that can
accelerate reinforcement learning dramatically in certain cases  roughly  by observing a
mentor  a reinforcement learning agent can extract information about its own capabilities
in  and the relative value of  unvisited parts of the state space  we study two specific
instantiations of this model  one in which the learning agent and the mentor have identical
abilities  and one designed to deal with agents and mentors with different action sets  we
illustrate the benefits of implicit imitation by integrating it with prioritized sweeping  and
demonstrating improved performance and convergence through observation of single and
multiple mentors  though we make some stringent assumptions regarding observability
and possible interactions  we briefly comment on extensions of the model that relax these
restricitions 

   introduction
the application of reinforcement learning to multiagent systems offers unique opportunities
and challenges  when agents are viewed as independently trying to achieve their own ends 
interesting issues in the interaction of agent policies  littman        must be resolved  e g  
by appeal to equilibrium concepts   however  the fact that agents may share information
for mutual gain  tan        or distribute their search for optimal policies and communicate reinforcement signals to one another  mataric        offers intriguing possibilities for
accelerating reinforcement learning and enhancing agent performance 
another way in which individual agent performance can be improved is by having a
novice agent learn reasonable behavior from an expert mentor  this type of learning can
be brought about through explicit teaching or demonstration  atkeson   schaal       
lin        whitehead      a   by sharing of privileged information  mataric         or
through an explicit cognitive representation of imitation  bakker   kuniyoshi         in
imitation  the agents own exploration is used to ground its observations of other agents
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fiprice   boutilier

behaviors in its own capabilities and resolve any ambiguities in observations arising from
partial observability and noise  a common thread in all of this work is the use of a mentor
to guide the exploration of the observer  typically  guidance is achieved through some form
of explicit communication between mentor and observer  a less direct form of teaching
involves an observer extracting information from a mentor without the mentor making an
explicit attempt to demonstrate a specific behavior of interest  mitchell  mahadevan   
steinberg        
in this paper we develop an imitation model we call implicit imitation that allows an
agent to accelerate the reinforcement learning process through the observation of an expert
mentor  or mentors   the agent observes the state transitions induced by the mentors
actions and uses the information gleaned from these observations to update the estimated
value of its own states and actions  we will distinguish two settings in which implicit
imitation can occur  homogeneous settings  in which the learning agent and the mentor
have identical actions  and heterogeneous settings  where their capabilities may differ  in
the homogeneous setting  the learner can use the observed mentor transitions directly to
update its own estimated model of its actions  or to update its value function  in addition 
a mentor can provide hints to the observer about the parts of the state space on which it
may be worth focusing attention  the observers attention to an area might take the form of
additional exploration of the area or additional computation brought to bear on the agents
prior beliefs about the area  in the heterogeneous setting  similar benefits accrue  but with
the potential for an agent to be misled by a mentor that possesses abilities different from
its own  in this case  the learner needs some mechanism to detect such situations and to
make efforts to temper the influence of these observations 
we derive several new techniques to support implicit imitation that are largely independent of any specific reinforcement learning algorithm  though they are best suited for use
with model based methods  these include model extraction  augmented backups  feasibility
testing  and k step repair  we first describe implicit imitation in homogeneous domains 
then we describe the extension to heterogeneous settings  we illustrate its effectiveness
empirically by incorporating it into moore and atkesons        prioritized sweeping algorithm 
the implicit imitation model has several advantages over more direct forms of imitation
and teaching  it does not require any agent to explicitly play the role of mentor or teacher 
observers learn simply by watching the behavior of other agents  if an observed mentor
shares certain subtasks with the observer  the observed behavior can be incorporated  indirectly  by the observer to improve its estimate of its own value function  this is important
because there are many situations in which an observer can learn from a mentor that is
unwilling or unable to alter its behavior to accommodate the observer  or even communicate
information to it  for example  common communication protocols may be unavailable to
agents designed by different developers  e g   internet agents   agents may find themselves
in a competitive situation in which there is disincentive to share information or skills  or
there may simply be no incentive for one agent to provide information to another  
another key advantage of our approachwhich arises from formalizing imitation in the
reinforcement learning contextis the fact that the observer is not constrained to directly
   for reasons of consistency  we will use the term mentor to describe any agent from which an observer
can learn  even if the mentor is an unwilling or unwitting participant 

   

fiimplicit imitation

imitate  i e   duplicate the actions of  the mentor  the learner can decide whether such
explicit imitation is worthwhile  implicit imitation can thus be seen as blending the
advantages of explicit teaching or explicit knowledge transfer with those of independent
learning  in addition  because an agent learns by observation  it can exploit the existence
of multiple mentors  essentially distributing its search  finally  we do not assume that
the observer knows the actual actions taken by the mentor  or that the mentor shares a
reward function  or goals  with the mentor  again  this stands in sharp contrast with many
existing models of teaching  imitation  and behavior learning by observation  while we make
some strict assumptions in this paper with respect to observability  complete knowledge of
reward functions  and the existence of mappings between agent state spaces  the model can
be generalized in interesting ways  we will elaborate on some of these generalizations near
the end of the paper 
the remainder of the paper is structured as follows  we provide the necessary background on markov decision processes and reinforcement learning for the development of our
implicit imitation model in section    in section    we describe a general formal framework
for the study of implicit imitation in reinforcement learning  two specific instantiations
of this framework are then developed  in section    a model for homogeneous agents is
developed  the model extraction technique is explained and the augmented bellman backup
is proposed as a mechanism for incorporating observations into model based reinforcement
learning algorithms  model confidence testing is then introduced to ensure that misleading
information does not have undue influence on a learners exploration policy  the use of
mentor observations to to focus attention on interesting parts of the state space is also
introduced  section   develops a model for heterogeneous agents  the model extends
the homogeneous model through feasibility testing  a device by which a learner can detect
whether the mentors abilities are similar to its own  and k step repair  whereby a learner
can attempt to mimic the trajectory of a mentor that cannot be duplicated exactly  both
of these techniques prove crucial in heterogeneous settings  the effectiveness of these models
is demonstrated on a number of carefully chosen navigation problems  section   examines
conditions under which implicit imitation will and will not work well  section   describes
several promising extensions to the model  section   examines the implicit imitation model
in the context of related work and section   considers future work before drawing some
general conclusions about implicit imitation and the field of computational imitation more
broadly 

   reinforcement learning
our aim is to provide a formal model of implicit imitation  whereby an agent can learn
how to act optimally by combining its own experience with its observations of the behavior
of an expert mentor  before doing so  we describe in this section the standard model of
reinforcement learning used in artificial intelligence  our model will build on this singleagent view of learning how to act  we begin by reviewing markov decision processes  which
provide a model for sequential decision making under uncertainty  and then move on to
describe reinforcement learning  with an emphasis on model based methods 
   

fiprice   boutilier

    markov decision processes
markov decision processes  mdps  have proven very useful in modeling stochastic sequential decision problems  and have been widely used in decision theoretic planning to model
domains in which an agents actions have uncertain effects  an agents knowledge of the environment is uncertain  and the agent can have multiple  possibly conflicting objectives  in
this section  we describe the basic mdp model and consider one classical solution procedure 
we do not consider action costs in our formulation of mdps  though these pose no special
complications  finally  we make the assumption of full observability  partially observable
mdps  pomdps   cassandra  kaelbling    littman        lovejoy        smallwood  
sondik        are much more computationally demanding than fully observable mdps  our
imitation model will be based on a fully observable model  though some of the generalizations of our model mentioned in the concluding section build on pomdps  we refer the
reader to bertsekas         boutilier  dean and hanks         and puterman        for
further material on mdps 
an mdp can be viewed as a stochastic automaton in which actions induce transitions
between states  and rewards are obtained depending on the states visited by an agent 
formally  an mdp can be defined as a tuple hs  a  t  ri  where s is a finite set of states or
possible worlds  a is a finite set of actions  t is a state transition function  and r is a reward
function  the agent can control the state of the system to some extent by performing actions
a  a that cause state transitions  movement from the current state to some new state 
actions are stochastic in that the actual transition caused cannot generally be predicted
with certainty  the transition function t   s  a   s  describes the effects of each
action at each state  t  si   a  is a probability distribution over s  specifically  t  si   a  sj  
is the probability of ending up in state sj  s when action a is performed at state si  
we will denote this quantity by pr si   a  sj    we require that    pr si   a  sj      for all
p
si   sj   and that for all si   sj s pr si   a  sj        the components s  a and t determine
the dynamics of the system being controlled  the assumption that the system is fully
observable means that the agent knows the true state at each time t  once that stage is
reached   and its decisions can be based solely on this knowledge  thus  uncertainty lies
only in the prediction of an actions effects  not in determining its actual effect after its
execution 
a  deterministic  stationary  markovian  policy    s  a describes a course of action
to be adopted by an agent controlling the system  an agent adopting such a policy performs
action  s  whenever it finds itself in state s  policies of this form are markovian since the
action choice at any state does not depend on the system history  and are stationary since
action choice does not depend on the stage of the decision problem  for the problems we
consider  optimal stationary markovian policies always exist 
we assume a bounded  real valued reward function r   s     r s  is the instantaneous reward an agent receives for occupying state s  a number of optimality criteria
can be adopted to measure the value of a policy   all measuring in some way the reward
accumulated by an agent as it traverses the state space through the execution of   in this
work  we focus on discounted infinite horizon problems  the current value of a reward received t stages in the future is discounted by some factor  t            this allows simpler
   

fiimplicit imitation

computational methods to be used  as discounted total reward will be finite  discounting
can be justified on other  e g   economic  grounds in many situations as well 
the value function v   s    reflects the value of a policy  at any state s  this is
simply the expected sum of discounted future rewards obtained by executing  beginning
at s  a policy   is optimal if  for all s  s and all policies   we have v  s   v  s  
we are guaranteed that such optimal  stationary  policies exist in our setting  puterman 
       the  optimal  value of a state v   s  is its value v  s  under any optimal policy    
by solving an mdp  we refer to the problem of constructing an optimal policy  value
iteration  bellman        is a simple iterative approximation algorithm for optimal policy
construction  given some arbitrary estimate v   of the true value function v    we iteratively
improve this estimate as follows 
v n  si     r si     max 
aa

x

pr si   a  sj  v n   sj   

   

sj s

the computation of v n  s  given v n  is known as a bellman backup  the sequence of value
functions v n produced by value iteration converges linearly to v    each iteration of value
iteration requires o  s    a   computation time  and the number of iterations is polynomial
in  s  
for some finite n  the actions a that maximize the right hand side of equation   form an
optimal policy  and v n approximates its value  various termination criteria can be applied 
for example  one might terminate the algorithm when
kv i    v i k 

     
 

   

 where kxk   max  x    x  x  denotes the supremum norm   this ensures the resulting
value function v i   is within   of the optimal function v  at any state  and that the induced
policy is  optimal  i e   its value is within  of v     puterman        
a concept that will be useful later is that of a q function  given an arbitrary value
function v   we define qva  si   as
qva  si     r si     

x

pr si   a  sj  v  sj  

   

sj s

intuitively  qva  s  denotes the value of performing action a at state s and then acting in a
manner that has value v  watkins   dayan         in particular  we define qa to be the
q function defined with respect to v    and qna to be the q function defined with respect
to v n    in this manner  we can rewrite equation   as 
v n  s    max qna  s  
aa

   

we define an ergodic mdp as an mdp in which every state is reachable from any other
state in a finite number of steps with non zero probability 
   

fiprice   boutilier

    model based reinforcement learning
one difficulty with the use of mdps is that the construction of an optimal policy requires
that the agent know the exact transition probabilities pr and reward model r  in the specification of a decision problem  these requirements  especially the detailed specification of the
domains dynamics  can impose an undue burden on the agents designer  reinforcement
learning can be viewed as solving an mdp in which the full details of the model  in particular pr and r  are not known to the agent  instead  the agent learns how to act optimally
through experience with its environment  we provide a brief overview of reinforcement
learning in this section  with an emphasis on model based approaches   for further details 
please refer to the texts of sutton and barto        and bertsekas and tsitsiklis        
and the survey of kaelbling  littman and moore        
in the general model  we assume that an agent is controlling an mdp hs  a  t  ri and
initially knows its state and action spaces  s and a  but not the transition model t or
reward function r  the agent acts in its environment  and at each stage of the process
makes a transition hs  a  r  ti  that is  it takes action a at state s  receives reward r and
moves to state t  based on repeated experiences of this type it can determine an optimal
policy in one of two ways   a  in model based reinforcement learning  these experiences can
be used to learn the true nature of t and r  and the mdp can be solved using standard
methods  e g   value iteration   or  b  in model free reinforcement learning  these experiences
can be used to directly update an estimate of the optimal value function or q function 
probably the simplest model based reinforcement learning scheme is the certainty equivalence approach  intuitively  a learning agent is assumed to have some current estimated
c
transition model tb of its environment consisting of estimated probabilities pr s 
a  t  and
b
an estimated rewards model r s   with each experience hs  a  r  ti the agent updates its esc to obtain an policy 
b that would be optimal
timated models  solves the estimated mdp m
if its estimated models were correct  and acts according to that policy 
to make the certainty equivalence approach precise  a specific form of estimated model
and update procedure must be adopted  a common approach is to used the empirical distribution of observed state transitions and rewards as the estimated model  for instance 
if action a has been attempted c s  a  times at state s  and on c s  a  t  of those occasions
c
state t has been reached  then the estimate pr s 
a  t    c s  a  t  c s  a   if c s  a      
some prior estimate is used  e g   one might assume all state transitions are equiprobable   a
bayesian approach  dearden  friedman    andre        uses an explicit prior distribution
over the parameters of the transition distribution pr s  a     and then updates these with
each experienced transition  for instance  we might assume a dirichlet  generalized beta 
distribution  degroot        with parameters n s  a  t  associated with each possible successor state t  the dirichlet parameters are equal to the experience based counts c s  a  t 
plus a prior count p  s  a  t  representing the agents prior beliefs about the distribution
 i e   n s  a  t    c s  a  t    p  s  a  t    the expected transition probability pr s  a  t  is then
p
c can be solved
n s  a  t   t  n s  a  t     assuming parameter independence  the mdp m
using these expected values  furthermore  the model can be updated with ease  simply
increasing n s  a  t  by one with each observation hs  a  r  ti  this model has the advantage
over a counter based approach of allowing a flexible prior model and generally does not
   

fiimplicit imitation

assign probability zero to unobserved transitions  we will adopt this bayesian perspective
in our imitation model 
one difficulty with the certainty equivalence approach is the computational burden of rec with each update of the models tb and r
b  i e   with each experience   one
solving an mdp m
could circumvent this to some extent by batching experiences and updating  and re solving 
the model only periodically  alternatively  one could use computational effort judiciously to
apply bellman backups only at those states whose values  or q values  are likely to change
the most given a change in the model  moore and atkesons        prioritized sweeping
c
algorithm does just this  when tb is updated by changing pr s 
a  t   a bellman backup is
b
b a   suppose the
applied at s to update its estimated value v   as well as the q value q s 
b
b
magnitude of the change in v  s  is given by v  s   for any predecessor w  the q values
b
c
q w 
a   hence values vb  w can change if pr w 
a    s       the magnitude of the change
c
is bounded by pr w 
a    s vb  s   all such predecessors w of s are placed in a priority
 
c
queue with pr w  a   s vb  s  serving as the priority  a fixed number of bellman backups
are applied to states in the order in which they appear in the queue  with each backup 
any change in value can cause new predecessors to be inserted into the queue  in this way 
computational effort is focused on those states where a bellman backup has the greatest
impact due to the model change  furthermore  the backups are applied only to a subset
of states  and are generally only applied a fixed number of times  by way of contrast  in
the certainty equivalence approach  backups are applied until convergence  thus prioritized
sweeping can be viewed as a specific form of asynchronous value iteration  and has appealing
computational properties  moore   atkeson        
under certainty equivalence  the agent acts as if the current approximation of the model
is correct  even though the model is likely to be inaccurate early in the learning process  if
the optimal policy for this inaccurate model prevents the agent from exploring the transitions which form part of the optimal policy for the true model  then the agent will fail to
find the optimal policy  for this reason  explicit exploration policies are invariably used to
ensure that each action is tried at each state sufficiently often  by acting randomly  assuming an ergodic mdp   an agent is assured of sampling each action at each state infinitely
often in the limit  unfortunately  the actions of such an agent will fail to exploit  in fact 
will be completely uninfluenced by  its knowledge of the optimal policy  this explorationexploitation tradeoff refers to the tension between trying new actions in order to find out
more about the environment and executing actions believed to be optimal on the basis of
the current estimated model 
the most common method for exploration is the greedy method in which the agent
chooses a random action a fraction  of the time  where           typically   is decayed
over time to increase the agents exploitation of its knowledge  in the boltzmann approach 
each action is selected with a probability proportional to its value 
prs  a    p

eq s a  
e
a  a

q s a    

   

the proportionality can be adjusted nonlinearly with the temperature parameter    as
    the probability of selecting the action with the highest value tends to    typically 
 is started high so that actions are randomly explored during the early stages of learning 
as the agent gains knowledge about the effects of its actions and the value of these effects 
   

fiprice   boutilier

the parameter  is decayed so that the agent spends more time exploiting actions known to
be valuable and less time randomly exploring actions 
more sophisticated methods attempt to use information about model confidence and
value magnitudes to plan a utility maximizing exploration plan  an early approximation
of this scheme can be found in the interval estimation method  kaelbling         bayesian
methods have also been used to calculate the expected value of information to be gained
from exploration  meuleau   bourgine        dearden et al         
we concentrate in this paper on model based approaches to reinforcement learning 
however  we should point out that model free methodsthose in which an estimate of the
optimal value function or q function is learned directly  without recourse to a domain
modelhave attracted much attention  for example  td methods  sutton        and
q learning  watkins   dayan        have both proven to be among the more popular
methods for reinforcement learning  our methods can be modified to deal with model free
approaches  as we discuss in the concluding section  we also focus on so called tablebased  or explicit  representations of models and value functions  when state and action
spaces are large  table based approaches become unwieldy  and the associated algorithms
are generally intractable  in these situations  approximators are often used to estimate the
values of states  we will discuss ways in which our techniques can be extended to allow for
function approximation in the concluding section 

   a formal framework for implicit imitation
to model the influence that a mentor agent can have on the decision process or the learning
behavior of an observer  we must extend the single agent decision model of mdps to account
for the actions and objectives of multiple agents  in this section  we introduce a formal
framework for studying implicit imitation  we begin by introducing a general model for
stochastic games  shapley        myerson         and then impose various assumptions
and restrictions on this general model that allow us to focus on the key aspects of implicit
imitation  we note that the framework proposed here is useful for the study of other forms
of knowledge transfer in multiagent systems  and we briefly point out various extensions of
the framework that would permit implicit imitation  and other forms of knowledge transfer 
in more general settings 
    non interacting stochastic games
stochastic games can be viewed as a multiagent extension of markov decision processes 
though shapleys        original formulation of stochastic games involved a zero sum  fully
competitive  assumption  various generalizations of the model have been proposed allowing
for arbitrary relationships between agents utility functions  myerson          formally 
an n agent stochastic game hs   ai   i  n   t   ri   i  n i comprises a set of n agents
    i  n   a set of states s  a set of actions ai for each agent i  a state transition function
t   and a reward function ri for each agent i  unlike an mdp  individual agent actions do
not determine state transitions  rather it is the joint action taken by the collection of agents
that determines how the system evolves at any point in time  let a   a       an be
   for example  see the fully cooperative multiagent mdp model proposed by boutilier        

   

fiimplicit imitation

the set of joint actions  then t   s  a   s   with t  si   a  sj     pr si   a  sj   denoting
the probability of ending up in state sj  s when joint action a is performed at state si  
for convenience  we introduce the notation ai to denote the set of joint actions a  
    ai   ai        an involving all agents except i  we use ai  ai to denote the
 full  joint action obtained by conjoining ai  ai with ai  ai  
because the interests of the individual agents may be at odds  strategic reasoning and
notions of equilibrium are generally involved in the solution of stochastic games  because
our aim is to study how a reinforcement agent might learn by observing the behavior of an
expert mentor  we wish to restrict the model in such a way that strategic interactions need
not be considered  we want to focus on settings in which the actions of the observer and
the mentor do not interact  furthermore  we want to assume that the reward functions of
the agents do not conflict in a way that requires strategic reasoning 
we define noninteracting stochastic games by appealing to the notion of an agent projection function which is used to extract an agents local state from the underlying game  in
these games  an agents local state determines all aspects of the global state that are relevant
to its decision making process  while the projection function determines which global states
are identical from an agents local perspective  formally  for each agent i  we assume a local
state space si   and a projection function li   s  si   for any s  t  s  we write s i t
iff li  s    li  t   this equivalence relation partitions s into a set of equivalence classes
such that the elements within a specific class  i e   l 
i  s  for some s  si   need not be
distinguished by agent i for the purposes of individual decision making  we say a stochastic
game is noninteracting if there exists a local state space si and projection function li for
each agent i such that 
   if s i t  then ai  ai   ai  ai   wi  si we have
x

 pr s  ai  ai   w    w  l 
i  wi     

x

 pr t  ai  ai   w    w  l 
i  wi   

   ri  s    ri  t  if s i t
intuitively  condition   above imposes two distinct requirements on the game from the
perspective of agent i  first  if we ignore the existence of other agents  it provides a notion
of state space abstraction suitable for agent i  specifically  li clusters together states
s  s only if each state in an equivalence class has identical dynamics with respect to
the abstraction induced by li   this type of abstraction is a form of bisimulation of the
type studied in automaton minimization  hartmanis   stearns        lee   yannakakis 
      and automatic abstraction methods developed for mdps  dearden   boutilier       
dean   givan         it is not hard to showignoring the presence of other agentsthat
the underlying system is markovian with respect to the abstraction  or equivalently  w r t 
si   if condition   is met  the quantification over all ai imposes a strong noninteraction
requirement  namely  that the dynamics of the game from the perspective of agent i is
independent of the strategies of the other agents  condition   simply requires that all
states within a given equivalence class for agent i have the same reward for agent i  this
means that no states within a class need to be distinguishedeach local state can be viewed
as atomic 
   

fiprice   boutilier

a noninteracting game induces an mdp mi for each agent i where mi   hsi   ai   pri   ri i
where pri is given by condition     above  specifically  for each si   ti  si  
pri  si   ai   ti    

p

 pr s  ai  ai   t    t  l 
i  ti   

where s is any state in l 
i  si   and ai is any element of ai   let i   sa  ai be an
optimal policy for mi   we can extend this to a strategy ig   s  ai for the underlying
stochastic game by simply applying i  si   to every state s  s such that li  s    si   the
following proposition shows that the term noninteracting indeed provides an appropriate
description of such a game 
proposition   let g be a noninteracting stochastic game  mi the induced mdp for agent
i  and i some optimal policy for mi   the strategy ig extending i to g is dominant for
agent i 
thus each agent can solve the noninteracting game by abstracting away irrelevant aspects of the state space  ignoring other agent actions  and solving its personal mdp
mi  
given an arbitrary stochastic game  it can generally be quite difficult to discover whether
it is noninteracting  requiring the construction of appropriate projection functions  in what
follows  we will simply assume that the underlying multiagent system is a noninteracting
game  rather than specifying the game and projection functions  we will specify the individual mdps mi themselves  the noninteracting game induced by the set of individual
mdps is simply the cross product of the individual mdps  such a view is often quite
natural  consider the example of three robots moving in some two dimensional office domain  if we are able to neglect the possibility of interactionfor example  if the robots can
occupy the same   d position  at a suitable level of granularity  and do not require the
same resources to achieve their tasksthen we might specify an individual mdp for each
robot  the local state might be determined by the robots x  y position  orientation  and
the status of its own tasks  the global state space would be the cross product s   s   s 
of the local spaces  the individual components of any joint action would affect only the
local state  and each agent would care  through its reward function ri   only about its local
state 
we note that the projection function li should not be viewed as equivalent to an observation function  we do not assume that agent i can only distinguish elements of si in
fact  observations of other agents states will be crucial for imitation  rather the existence
of li simply means that  from the point of view of decision making with a known model 
the agent need not worry about distinctions other than those made by li   assuming no
computational limitations  an agent i need only solve mi   but may use observations of other
agents in order to improve its knowledge about mi s dynamics  
    implicit imitation
despite the very independent nature of the agent subprocesses in a noninteracting multiagent system  there are circumstances in which the behavior of one agent may be relevant to
   we elaborate on the condition of computational limitations below 

   

fiimplicit imitation

another  to keep the discussion simple  we assume the existence of an expert mentor agent
m  which is implementing some stationary  and presumably optimal  policy m over its
local mdp mm   hsm   am   prm   rm i  we also assume a second agent o  the observer  with
local mdp mo   hso   ao   pro   ro i  while nothing about the mentors behavior is relevant
to the observer if it knows its own mdp  and can solve it without computational difficulty  
the situation can be quite different if o is a reinforcement learner without complete knowledge of the model mo   it may well be that the observed behavior of the mentor provides
valuable information to the observer in its quest to learn how to act optimally within mo  
to take an extreme case  if mentors mdp is identical to the observers  and the mentor
is an expert  in the sense of acting optimally   then the behavior of the mentor indicates
exactly what the observer should do  even if the mentor is not acting optimally  or if the
mentor and observer have different reward functions  mentor state transitions observed by
the learner can provide valuable information about the dynamics of the domain 
thus we see that when one agent is learning how to act  the behavior of another can
potentially be relevant to the learner  even if the underlying multiagent system is noninteracting  similar remarks  of course  apply to the case where the observer knows the mdp
mo   but computational restrictions make solving this difficultobserved mentor transitions
might provide valuable information about where to focus computational effort   the main
motivation underlying our model of implicit imitation is that the behavior of an expert
mentor can provide hints as to appropriate courses of action for a reinforcement learning
agent 
intuitively  implicit imitation is a mechanism by which a learning agent attempts to
incorporate the observed experience of an expert mentor agent into its learning process 
like more classical forms of learning by imitation  the learner considers the effects of the
mentors action  or action sequence  in its own context  unlike direct imitation  however 
we do not assume that the learner must physically attempt to duplicate the mentors
behavior  nor do we assume that the mentors behavior is necessarily appropriate for the
observer  instead  the influence of the mentor is on the agents transition model and its
estimate of value of various states and actions  we elaborate on these points below 
in what follows  we assume a mentor m and associated mdp mm   and a learner or
observer o and associated mdp mo   as described above  these mdps are fully observable 
we focus on the reinforcement learning problem faced by agent o  the extension to multiple
mentors is straightforward and will be discussed below  but for clarity we assume only one
mentor in our description of the abstract framework  it is clear that certain conditions must
be met for the observer to extract useful information from the mentor  we list a number
of assumptions that we make at different points in the development of our model 
observability  we must assume that the learner can observe certain aspects of the mentors behavior  in this work  we assume that state of the mentors mdp is fully
observable to the learner  equivalently  we interpret this as full observability of the
underlying noninteracting game  together with knowledge of the mentors projection
   for instance  algorithms like asynchronous dynamic programming and prioritized sweeping can benefit
from such guidance  indeed  the distinction between reinforcement learning and solving mdps is viewed
by some as rather blurry  sutton   barto        bertsekas   tsitsiklis         our focus is on the
case of an unknown model  i e   the classical reinforcement learning problem  as opposed to one where
computational issues are key 

   

fiprice   boutilier

function lm   a more general partially observable model would require the specification of an observation or signal set z and an observation function o   so sm   z  
where o so   sm   z  denotes the probability with which the observer obtains signal z
when the local states of the observer and mentor are so and sm   respectively  we do
not pursue such a model here  it is important to note that we do not assume that the
observer has access to the action taken by m at any point in time  since actions are
stochastic  the state  even if fully observable  that results from the mentor invoking a
specific control signal is generally insufficient to determine that signal  thus it seems
much more reasonable to assume that states  and transitions  are observable than the
actions that gave rise to them 
analogy  if the observer and the mentor are acting in different local state spaces  it is clear
that observations made of the mentors state transitions can offer no useful information
to the observer unless there is some relationship between the two state spaces  there
are several ways in which this relationship can be specified  dautenhahn and nehaniv
       use a homomorphism to define the relationship between mentor and observer
for a specific family of trajectories  see section   for further discussion  
a slightly different notion might involve the use of some analogical mapping h   sm 
so such that an observed state transition s  t provides some information to the
observer about the dynamics or value of state h s   so   in certain circumstances  we
might require the mapping h to be homomorphic with respect to pr   a     for some 
or all  a   and perhaps even with respect to r  we discuss these issues in further detail
below  in order to simplify our model and avoid undue attention to the  admittedly
important  topic of constructing suitable analogical mappings  we will simply assume
that the mentor and the observer have identical state spaces  that is  sm and so are
in some sense isomorphic  the precise sense in which the spaces are isomorphicor
in some cases  presumed to be isomorphic until proven otherwiseis elaborated below
when we discuss the relationship between agent abilities  thus from this point we
simply refer to the state s without distinguishing the mentors local space sm from
the observers so  
abilities  even with a mapping between states  observations of a mentors state transitions
only tell the observer something about the mentors abilities  not its own  we must
assume that the observer can in some way duplicate the actions taken by the mentor
to induce analogous transitions in its own local state space  in other words  there must
be some presumption that the mentor and the observer have similar abilities  it is
in this sense that the analogical mapping between state spaces can be taken to be
a homomorphism  specifically  we might assume that the mentor and the observer
have the same actions available to them  i e   am   ao   a  and that h   sm  so
is homomorphic with respect to pr   a    for all a  a  this requirement can be
weakened substantially  without diminishing its utility  by requiring only that the
observer be able to implement the actions actually taken by the mentor at a given
state s  finally  we might have an observer that assumes that it can duplicate the
actions taken by the mentor until it finds evidence to the contrary  in this case 
there is a presumed homomorphism between the state spaces  in what follows  we
will distinguish between implicit imitation in homogeneous action settingsdomains
   

fiimplicit imitation

in which the analogical mapping is indeed homomorphicand heterogeneous action
settingswhere the mapping may not be a homomorphism 
there are more general ways of defining similarity of ability  for example  by assuming
that the observer may be able to move through state space in a similar fashion to the
mentor without following the same trajectories  nehaniv   dautenhahn         for
instance  the mentor may have a way of moving directly between key locations in state
space  while the observer may be able to move between analogous locations in a less
direct fashion  in such a case  the analogy between states may not be determined by
single actions  but rather by sequences of actions or local policies  we will suggest
ways for dealing with restricted forms of analogy of this type in section   
objectives  even when the observer and mentor have similar or identical abilities  the
value to the observer of the information gleaned from the mentor may depend on
the actual policy being implemented by the mentor  we might suppose that the
more closely related a mentors policy is to the optimal policy of the observer  the
more useful the information will be  thus  to some extent  we expect that the more
closely aligned the objectives of the mentor and the observer are  the more valuable
the guidance provided by the mentor  unlike in existing teaching models  we do
not suppose that the mentor is making any explicit efforts to instruct the observer 
and because their objectives may not be identical  we do not force the observer to
 attempt to  explicitly imitate the behavior of the mentor  in general  we will make
no explicit assumptions about the relationship between the objectives of the mentor
and the observer  however  we will see that  to some extent  the closer they are 
the more utility can be derived from implicit imitation 
finally  we remark on an important assumption we make throughout the remainder
of this paper  the observer knows its reward function ro   that is  for each state s  the
observer can evaluate ro  s  without having visited state s  this view is consistent
with view of reinforcement learning as automatic programming  a user may easily
specify a reward function  e g   in the form of a set of predicates that can be evaluated
at any state  prior to learning  it may be more difficult to specify a domain model
or optimal policy  in such a setting  the only unknown component of the mdp mo is
the transition function pro   we believe this approach to reinforcement learning is  in
fact  more common in practice than the approach in which the reward function must
be sampled 
to reiterate  our aim is to describe a mechanism by which the observer can accelerate
its learning  but we emphasize our position that implicit imitationin contrast to explicit
imitationis not merely replicating the behaviors  or state trajectories  observed in another
agent  nor even attempting to reach similar states  we believe the agent must learn
about its own capabilities and adapt the information contained in observed behavior to
these  agents must also explore the appropriate application  if any  of observed behaviors 
integrating these with their own  as appropriate  to achieve their own ends  we therefore
see imitation as an interactive process in which the behavior of one agent is used to guide
the learning of another 
   

fiprice   boutilier

given this setting  we can list possible ways in which an observer and a mentor can  and
cannot  interact  contrasting along the way our perspective and assumptions with those of
existing models in the literature   first  the observer could attempt to directly infer a
policy from its observations of mentor state action pairs  this model has a conceptual
simplicity and intuitive appeal  and forms the basis of the behavioral cloning paradigm
 sammut  hurst  kedzier    michie        urbancic   bratko         however  it assumes
that the observer and mentor share the same reward function and action capabilities  it
also assumes that complete and unambiguous trajectories  including action choices  can be
observed  a related approach attempts to deduce constraints on the value function from
the inferred action preferences of the mentor agent  utgoff   clouse        suc   bratko 
       again  however  this approach assumes congruity of objectives  our model is also
distinct from models of explicit teaching  lin        whitehead      b   we do not assume
that the mentor has any incentive to move through its environment in a way that explicitly
guides the learner to explore its own environment and action space more effectively 
instead of trying to directly learn a policy  an observer could attempt to use observed
state transitions of other agents to improve its own environment model pro  s  a  t   with
a more accurate model and its own reward function  the observer could calculate more
accurate values for states  the state values could then be used to guide the agent towards
distant rewards and reduce the need for random exploration  this insight forms the core
of our implicit imitation model  this approach has not been developed in the literature 
and is appropriate under the conditions listed above  specifically  under conditions where
the mentors actions are unobservable  and the mentor and observer have different reward
functions or objectives  thus  this approach is applicable under more general conditions
than many existing models of imitation learning and teaching 
in addition to model information  mentors may also communicate information about the
relevance or irrelevance of regions of the state space for certain classes of reward functions 
an observer can use the set of states visited by the mentor as heuristic guidance about
where to perform backup computations in the state space 
in the next two sections  we develop specific algorithms from our insights about how
agents can use observations of others to both improve their own models and assess the
relevance of regions within their state spaces  we first focus on the homogeneous action
case  then extend the model to deal with heterogeneous actions 

   implicit imitation in homogeneous settings
we begin by describing implicit imitation in homogeneous action settingsthe extension
to heterogeneous settings will build on the insights developed in this section  we develop
a technique called implicit imitation through which observations of a mentor can be used
to accelerate reinforcement learning  first  we define the homogeneous setting  then we
develop the implicit imitation algorithm  finally  we demonstrate how implicit imitation
works on a number of simple problems designed to illustrate the role of the various mechanisms we describe 
   we will describe other models in more detail in section   

   

fiimplicit imitation

    homogeneous actions
the homogeneous action setting is defined as follows  we assume a single mentor m and
observer o  with individual mdps mm   hs  am   prm   rm i and mo   hs  ao   pro   ro i 
respectively  note that the agents share the same state space  more precisely  we assume
a trivial isomorphic mapping that allows us to identify their local states   we also assume
that the mentor is executing some stationary policy m   we will often treat this policy as
deterministic  but most of our remarks apply to stochastic policies as well  let the support
set supp m   s  for m at state s be the set of actions a  am accorded nonzero probability
by m at state s  we assume that the observer has the same abilities as the mentor in
the following sense  s  t  s  am  supp m   s   there exists an action ao  ao such that
pro  s  ao   t    prm  s  am   t   in other words  the observer is able to duplicate  in a the sense
of inducing the same distribution over successor states  the actual behavior of the mentor 
or equivalently  the agents local state spaces are isomorphic with respect to the actions
actually taken by the mentor at the subset of states where those actions might be taken 
this is much weaker than requiring a full homomorphism from sm to so   of course  the
existence of a full homomorphism is sufficient from our perspective  but our results do not
require this 
    the implicit imitation algorithm
the implicit imitation algorithm can be understood in terms of its component processes 
first  we extract action models from a mentor  then we integrate this information into
the observers own value estimates by augmenting the usual bellman backup with mentor
action models  a confidence testing procedure ensures that we only use this augmented
model when the observers model of the mentor is more reliable than the observers model
of its own behavior  we also extract occupancy information from the observations of mentor
trajectories in order to focus the observers computational effort  to some extent  in specific
parts of the state space  finally  we augment our action selection process to choose actions
that will explore high value regions revealed by the mentor  the remainder of this section
expands upon each of these processes and how they fit together 
      model extraction
the information available to the observer in its quest to learn how to act optimally can be
divided into two categories  first  with each action it takes  it receives an experience tuple
hs  a  r  ti  in fact  we will often ignore the sampled reward r  since we assume the reward
function r is known in advance  as in standard model based learning  each such experience
can be used to update its own transition model pro  s  a    
second  with each mentor transition  the observer obtains an experience tuple hs  ti 
note again that the observer does not have direct access to the action taken by the mentor 
only the induced state transition  assume the mentor is implementing a deterministic 
stationary policy m   with m  s  denoting the mentors choice of action at state s  this
policy induces a markov chain prm      over s  with prm  s  t    pr s  m  s   t  denoting
   

fiprice   boutilier

the probability of a transition from s to t   since the learner observes the mentors state
c m of this chain  pr
c m  s  t  is simply estimated by
transitions  it can construct an estimate pr
the relative observed frequency of mentor transitions s  t  w r t  all transitions taken from
s   if the observer has some prior over the possible mentor transitions  standard bayesian
update techniques can be used instead  we use the term model extraction for this process
of estimating the mentors markov chain 
      augmented bellman backups
c m of the mentors markov chain  by
suppose the observer has constructed an estimate pr
the homogeneity assumption  the action m  s  can be replicated exactly by the observer at
state s  thus  the policy m can  in principle  be duplicated by the observer  were it able
to identify the actual actions used   as such  we can define the value of the mentors policy
from the observers perspective 

vm  s    ro  s    

x

prm  s  t vm  t 

   

ts

notice that equation   uses the mentors dynamics but the observers reward function 
letting v denote the optimal  observers  value function  clearly v  s   vm  s   so vm
provides a lower bound on the observers value function 
more importantly  the terms making up vm  s  can be integrated directly into the bellman equation for the observers mdp  forming the augmented bellman equation 
 

 

v  s    ro  s     max max
aao

x

 

pro  s  a  t v  t   

ts

x

 

prm  s  t v  t 

   

ts

this is the usual bellman equation with an extra term added  namely  the second
p
summation  ts prm  s  t v  t  denoting the expected value of duplicating the mentors
action am   since this  unknown  action is identical to one of the observers actions  the
term is redundant and the augmented value equation is valid  of course  the observer using
the augmented backup operation must rely on estimates of these quantities  if the observer
exploration policy ensures that each state is visited infinitely often  the estimates of the pro
terms will converge to their true values  if the mentors policy is ergodic over state space s 
then prm will also converge to its true value  if the mentors policy is restricted to a subset
of states s    s  those forming the basis of its markov chain   then the estimates of prm
for the subset will converge correctly with respect to s   if the chain is ergodic  the states
in s  s   will remain unvisited and the estimates will remain uninformed by data  since
the mentors policy is not under the control of the observer  there is no way for the observer
to influence the distribution of samples attained for prm   an observer must therefore be
able to reason about the accuracy of the estimated model prm for any s and restrict the
application of the augmented equation to those states where prm is known with sufficient
accuracy 
   this is somewhat imprecise  since the initial distribution of the markov chain is unknown  for our
purposes  it is only the dynamics that are relevant to the observer  so only the transition probabilities
are used 

   

fiimplicit imitation

while prm cannot be used indiscriminately  we argue that it can be highly informative
early in the learning process  assuming that the mentor is pursuing an optimal policy  or at
least is behaving in some way so that it tends to visit certain states more frequently   there
will be many states for which the observer has much more accurate estimates of prm  s  t 
than it does for pro  s  a  t  for any specific a  since the observer is learning  it must explore
both its state spacecausing less frequent visits to sand its action spacethus spreading
its experience at s over all actions a  this generally ensures that the sample size upon which
prm is based is greater than that for pro for any action that forms part of the mentors
policy  apart from being more accurate  the use of prm  s  t  can often give more informed
value estimates at state s  since prior action models are often flat or uniform  and only
become distinguishable at a given state when the observer has sufficient experience at state
s 
we note that the reasoning above holds even if the mentor is implementing a  stationary  stochastic policy  since the expected value of stochastic policy for a fully observable
mdp cannot be greater than that of an optimal deterministic policy   while the direction offered by a mentor implementing a deterministic policy tends to be more focused 
empirically we have found that mentors offer broader guidance in moderately stochastic
environments or when they implement stochastic policies  since they tend to visit more of
the state space  we note that the extension to multiple mentors is straightforwardeach
mentor model can be incorporated into the augmented bellman equation without difficulty 
      model confidence
when the mentors markov chain is not ergodic  or if the mixing rate  is sufficiently low  the
mentor may visit a certain state s relatively infrequently  the estimated mentor transition
model corresponding to a state that is rarely  or never  visited by the mentor may provide a
very misleading estimatebased on the small sample or the prior for the mentors chainof
the value of the mentors  unknown  action at s  and since the mentors policy is not under
the control of the observer  this misleading value may persist for an extended period  since
the augmented bellman equation does not consider relative reliability of the mentor and
observer models  the value of such a state s may be overestimated   that is  the observer can
be tricked into overvaluing the mentors  unknown  action  and consequently overestimating
the value of state s 
to overcome this  we incorporate an estimate of model confidence into our augmented
backups  for both the mentors markov chain and the observers action transitions  we
assume a dirichlet prior over the parameters of each of these multinomial distributions
 degroot         these reflect the observers initial uncertainty about the possible transition probabilities  from sample counts of mentor and observer transitions  we update
these distributions  with this information  we could attempt to perform optimal bayesian
estimation of the value function  but when the sample counts are small  and normal approximations are not appropriate   there is no simple  closed form expression for the resultant
distributions over values  we could attempt to employ sampling methods  but in the in   the mixing rate refers to how quickly a markov chain approaches its stationary distribution 
   note that underestimates based on such considerations are not problematic  since the augmented bellman
equation then reduces to the usual bellman equation 

   

fiprice   boutilier

v
m

vo 

vo vm

figure    lower bounds on action values incorporate uncertainty penalty
terest of simplicity we have employed an approximate method for combining information
sources inspired by kaelblings        interval estimation method 
let v denote the current estimated augmented value function  and pro and prm denote
  denote the variance
the estimated observer and mentor transition models  we let o  and m
in these model parameters 
an augmented bellman backup with respect to v using confidence testing proceeds
as follows  we first compute the observers optimal action ao based on the estimated
augmented values for each of the observers actions  let q ao   s    vo  s  denote its value 
for the best action  we use the model uncertainty encoded by the dirichlet distribution to
construct a lower bound vo  s  on the value of the state to the observer using the model
 at state s  derived from its own behavior  i e   ignoring its observations of the mentor  
we employ transition counts no  s  a  t  and nm  s  t  to denote the number of times the
observer has made the transition from state s to state t when the action a was performed 
and the number of times the mentor was observed making the transition from state s to
t  respectively  from these counts  we estimate the uncertainty in the model using the
p
variance of a dirichlet distribution  let    no  s  a  t  and    t  st no  s  a  t     then
the model variance is 
 
model
 s  a  t   

   

  

 
           

   

the variance in the q value of an action due to the uncertainty in the local model
can be found by simple application of the rule for combining linear combinations of variances  v ar cx   dy     c  v ar x    d  v ar y   to the expression for the bellman backup 
p
v ar r s     t p r t s  a v  t   the result is 
    s  a      

x
t

 
model
 s  a  t v t  

   

using chebychevs inequality   we can obtain a confidence level even though the dirichlet
distributions for small sample counts are highly non normal  the lower bound is then
vo  s    vo  s co  s  ao   for some suitable constant c  one may interpret this as penalizing
   chebychevs inequality states that    k   of the probability mass for an arbitrary distribution will be
within k standard deviations of the mean 

   

fiimplicit imitation

 
 
function augmentedbackup  v  pro  omodel
 prm  mmodel
 s c 

a   arg maxaao

p
ts

pr s  a  t v  t 

p

vo  s    ro  s     pts pro  s  a   t v  t 
vm  s    ro  s     ts prm  s  t v  t 

p

 
 

o   s  a     
 t  
p ts  omodel  s  a   t v
 
 
 
m  s    

 s  t v  t 
ts mmodel

vo  s    vo  s   c  o  s  a  

vm
 s    vm  s   c  m  s 
if vo  s    vm  s  then
v  s    vo  s 
else
v  s    vm  s 
end

table    implicit backup
the value of a state by subtracting its uncertainty from it  see figure       the value
vm  s  of the mentors action m  s  is estimated similarly and an analogous lower bound
vm  s  on it is also constructed  if vo  s    vm  s   then we say that vo  s  supersedes
vm  s  and we write vo  s   vm  s   when vo  s   vm  s  then either the mentor inspired
model has  in fact  a lower expected value  within a specified degree of confidence  and
uses a nonoptimal action  from the observers perspective   or the mentor inspired model
has lower confidence  in either case  we reject the information provided by the mentor and
use a standard bellman backup using the action model derived solely from the observers
experience  thus suppressing the augmented backup the backed up value is vo  s  in this
case 
an algorithm for computing an augmented backup using this confidence test is shown
in table    the algorithm parameters include the current estimate of the augmented value
 
function v   the current estimated model pro and its associated local variance omodel
 
 
and the model of the mentors markov chain prm and its associated variance mmodel   it
calculates lower bounds and returns the mean value  vo or vm   with the greatest lower
bound  the parameter c determines the width of the confidence interval used in the mentor
rejection test 
      focusing
the augmented bellman backups improves the accuracy of the observers model  a second
way in which an observer can exploit its observations of the mentor is to focus attention on
the states visited by the mentor  in a model based approach  the specific focusing mecha    ideally  we would like to take not only the uncertainty of the model at the current state into account 
but also the uncertainty of future states as well  meuleau   bourgine        

   

fiprice   boutilier

nism we adopt is to require the observer to perform a  possibly augmented  bellman backup
at state s whenever the mentor makes a transition from s  this has three effects  first  if
the mentor tends to visit interesting regions of space  e g   if it shares a certain reward structure with the observer   then the significant values backed up from mentor visited states
will bias the observers exploration towards these regions  second  computational effort will
c m  s  t  changes 
be concentrated toward parts of state space where the estimated model pr
and hence where the estimated value of one of the observers actions may change  third 
computation is focused where the model is likely to be more accurate  as discussed above  
      action selection
the integration of exploration techniques in the action selection policy is important for any
reinforcement learning algorithm to guarantee convergence  in implicit imitation  it plays a
second  crucial role in helping the agent exploit the information extracted from the mentor 
our improved convergence results rely on the greedy quality of the exploration strategy to
bias an observer towards the higher valued trajectories revealed by the mentor 
for expediency  we have adopted the  greedy action selection method  using an exploration rate  that decays over time  we could easily have employed other semi greedy
methods such as boltzmann exploration  in the presence of a mentor  greedy action selection becomes more complex  the observer examines its own actions at state s in the usual
way and obtains a best action ao which has a corresponding value vo  s   a value is also
calculated for the mentors action vm  s   if vo  s   vm  s   then the observers own action
model is used and the greedy action is defined exactly as if the mentor were not present 
if  however  vm  s   vo  s  then we would like to define the greedy action to be the action
dictated by the mentors policy at state s  unfortunately  the observer does not know which
action this is  so we define the greedy action to be the observers action closest to the
mentors action according to the observers current model estimates at s  more precisely 
the action most similar to the mentors at state s  denoted m  s   is that whose outcome
distribution has minimum kullback leibler divergence from the mentors action outcome
distribution 
 

m  s    argmina 

x

 

pro  s  a  t  log prm  s  t 

    

t

the observers own experience based action models will be poor early in training  so there
is a chance that the closest action computation will select the wrong action  we rely on the
exploration policy to ensure that each of the observers actions is sampled appropriately in
the long run   
in our present work we have assumed that the state space is large and that the agent
will therefore not be able to completely update the q function over the whole space   the
intractability of updating the entire state space is one of the motivations for using imitation
techniques   in the absence of information about the states true values  we would like to
bias the value of the states along the mentors trajectories so that they look worthwhile to
explore  we do this by assuming bounds on the reward function and setting the initial qvalues over the entire space below this bound  in our simple examples  rewards are strictly
    if the mentor is executing a stochastic policy  the test based on kl divergence can mislead the learner 

   

fiimplicit imitation

positive so we set the bounds to zero  if mentor trajectories intersect any states valued by
the observing agent  backups will cause the states on these trajectories to have a higher
value than the surrounding states  this causes the greedy step in the exploration method
to prefer actions that lead to mentor visited states over actions for which the agent has no
information 
      model extraction in specific reinforcement learning algorithms
model extraction  augmented backups  the focusing mechanism  and our extended notion of
the greedy action selection  can be integrated into model based reinforcement learning algorithms with relative ease  generically  our implicit imitation algorithm requires that   a 
c m  s  t  of the markov chain induced by the mentors
the observer maintain an estimate pr
policythis estimate is updated with every observed transition  and  b  that all backups
performed to estimate its value function use the augmented backup  equation    with confic o  s  a  t 
dence testing  of course  these backups are implemented using estimated models pr
c
and prm  s  t   in addition  the focusing mechanism requires that an augmented backup be
performed at any state visited by the mentor 
we demonstrate the generality of these mechanisms by combining them with the wellknown and efficient prioritized sweeping algorithm  moore   atkeson         as outlined
c
in section      prioritized sweeping works by maintaining an estimated transition model pr
b whenever an experience tuple hs  a  r  ti is sampled  the estimated
and reward model r 
model at state s can change  a bellman backup is performed at s to incorporate the revised
model and some  usually fixed  number of additional backups are performed at selected
states  states are selected using a priority that estimates the potential change in their values
based on the changes precipitated by earlier backups  effectively  computational resources
 backups  are focused on those states that can most benefit from those backups 
incorporating our ideas into prioritized sweeping simply requires the following changes 
c o  s  a  t  is up with each transition hs  a  ti the observer takes  the estimated model pr
dated and an augmented backup is performed at state s  augmented backups are then
performed at a fixed number of states using the usual priority queue implementation 
c m  s  t  is updated
 with each observed mentor transition hs  ti  the estimated model pr
and an augmented backup is performed at s  augmented backups are then performed
at a fixed number of states using the usual priority queue implementation 

keeping samples of mentor behavior implements model extraction  augmented backups
integrate this information into the observers value function  and performing augmented
backups at observed transitions  in addition to experienced transitions  incorporates our
focusing mechanism  the observer is not forced to follow or otherwise mimic the actions
of the mentor directly  but it does back up value information along the mentors trajectory
as if it had  ultimately  the observer must move to those states to discover which actions
are to be used  in the meantime  important value information is being propagated that can
guide its exploration 
implicit imitation does not alter the long run theoretical convergence properties of the
underlying reinforcement learning algorithm  the implicit imitation framework is orthogonal to  greedy exploration  as it alters only the definition of the greedy action  not when
   

fiprice   boutilier

the greedy action is taken  given a theoretically appropriate decay factor  the  greedy
strategy will thus ensure that the distributions for the action models at each state are
sampled infinitely often in the limit and converge to their true values  since the extracted
model from the mentor corresponds to one of the observers own actions  its effect on the
value function calculations is no different than the effect of the observers own sampled
action models  the confidence mechanism ensures that the model with more samples will
eventually come to dominate if it is  in fact  better  we can therefore be sure that the convergence properties of reinforcement learning with implicit imitation are identical to that
of the underlying reinforcement learning algorithm 
the benefit of implicit imitation lies in the way in which the models extracted from the
mentor allow the observer to calculate a lower bound on the value function and use this
lower bound to choose its greedy actions to move the agent towards higher valued regions
of state space  the result is quicker convergence to optimal policies and better short term
practical performance with respect to accumulated discounted reward while learning 
      extensions
the implicit imitation model can easily be extended to extract model information from
multiple mentors  mixing and matching pieces extracted from each mentor to achieve good
results  it does this by searching  at each state  the set of mentors it knows about to find
the mentor with the highest value estimate  the value estimate of the best mentor is then
compared using the confidence test described above with the observers own value estimate 
the formal expression of the algorithm is given by the multi augmented bellman equation 
 

 

v  s    ro  s     max max
aao

max

mm

x

x

 

pro  s  a  t v  t 

 

ts

 

prm  s  t v  t 

    

ts

where m is the set of candidate mentors  ideally  confidence estimates should be taken
into account when comparing mentor estimates with each other  as we may get a mentor
with a high mean value estimate but large variance  if the observer has any experience
with the state at all  this mentor will likely be rejected as having poorer quality information
than the observer already has from its own experience  the observer might have been
better off picking a mentor with a lower mean but more confident estimate that would
have succeeded in the test against the observers own model  in the interests of simplicity 
however  we investigate multiple mentor combination without confidence testing 
up to now  we have assumed no action costs  i e   the agents rewards depend only on
the state and not on the action selected in the state   however  we can use more general
reward functions  e g   where reward has the form r s  a    the difficulty lies in backing
up action costs when the mentors chosen action is unknown  in section       we defined
the closest action function   the  function can be used to choose the appropriate reward 
the augmented bellman equation with generalized rewards takes the following form 
 

 

v  s    max max ro  s  a    
aao

x
ts

   

 

pro  s  a  t v  t 

 

fiimplicit imitation

ro  s   s     

x

 

prm  s  t v  t 

ts

we note that bayesian methods could be used could be used to estimate action costs
in the mentors chain as well  in any case  the generalized reward augmented equation can
readily be amended to use confidence estimates in a similar fashion to the transition model 
    empirical demonstrations
the following empirical tests incorporate model extraction and our focusing mechanism into
prioritized sweeping  the results illustrate the types of problems and scenarios in which
implicit imitation can provide advantages to a reinforcement learning agent  in each of the
experiments  an expert mentor is introduced into the experiment to serve as a model for the
observer  in each case  the mentor is following an  greedy policy with a very small   on
the order of        this tends to cause the mentors trajectories to lie within a cluster
surrounding optimal trajectories  and reflect good if not optimal policies   even with a
small amount of exploration and some environment stochasticity  mentors generally do not
cover the entire state space  so confidence testing is important 
in all of these experiments  prioritized sweeping is used with a fixed number of backups per observed or experienced sample     greedy exploration is used with decaying  
observer agents are given uniform dirichlet priors and q values are initialized to zero  observer agents are compared to control agents that do not benefit from a mentors experience 
but are otherwise identical  implementing prioritized sweeping with similar parameters and
exploration policies   the tests are all performed on stochastic grid world domains  since
these make it clear to what extent the observers and mentors optimal policies overlap  or
fail to   in figure    a simple        example shows a start and end state on a grid 
a typical optimal mentor trajectory is illustrated by the solid line between the start and
end states  the dotted line shows that a typical mentor influenced trajectory will be quite
similar to the observed mentor trajectory  we assume eight connectivity between cells so
that any state in the grid has nine neighbors including itself  but agents have only four
possible actions  in most experiments  the four actions move the agent in the compass
directions  north  south  east and west   although the agent will not initially know which
action does which  we focus primarily on whether imitation improves performance during
learning  since the learner will converge to an optimal policy whether it uses imitation or
not 
      experiment    the imitation effect
in our first experiment we compare the performance of an observer using model extraction
and an expert mentor with the performance of a control agent using independent reinforcement learning  given the uniform nature of this grid world and the lack of intermediate
rewards  confidence testing is not required  both agents attempt to learn a policy that
maximizes discounted return in a        grid world  they start in the upper left corner
and seek a goal with value     in the lower right corner  upon reaching the goal  the agents
    generally  the number of backups was set to be roughly equal to the length of the optimal noise free
path 

   

fiprice   boutilier

s

x

figure    a simple grid world with start state s and goal state x

  
obs
fa series
ctrl

average reward per      steps

  

  

  

  

delta

 

  

 

   

    

    

    

    

    

    

    

    

simulation steps

figure    basic observer and control agent comparisons

are restarted in the upper left corner  generally the mentor will follow a similar if not identical trajectory each run  as the mentors were trained using a greedy strategy that leaves
one path slightly more highly valued than the rest  action dynamics are noisy  with the
intended direction being realized     of the time  and one of the other directions taken
otherwise  uniformly   the discount factor is      in figure    we plot the cumulative
number of goals obtained over the previous      time steps for the observer obs and
control ctrl agents  results are averaged over ten runs   the observer is able to quickly
incorporate a policy learned from the mentor into its value estimates  this results in a
steeper learning curve  in contrast  the control agent slowly explores the space to build a
model first  the delta curve shows the difference in performance between the agents 
both agents converge to the same optimal value function 
   

fiimplicit imitation

  

  

average reward per      steps

  

basic
scale

  

  
stoch
 

 

 

 

    

    

    

    

    

    

simulation steps

figure    delta curves showing the influence of domain size and noise

      experiment    scaling and noise
the next experiment illustrates the sensitivity of imitation to the size of the state space and
action noise level  again  the observer uses model extraction but not confidence testing 
in figure    we plot the delta curves  i e   difference in performance between observer and
control agents  for the basic scenario just described  the scale scenario in which the
state space size is increased    percent  to a        grid   and the stoch scenario in
which the noise level is increased to    percent  results are averaged over ten runs   the
total gain represented by the area under the curves for the observer and the non imitating
prioritized sweeping agent increases with the state space size  this reflects whiteheads
     a  observation that for grid worlds  exploration requirements can increase quickly
with state space size  but that the optimal path length increases only linearly  here we see
that the guidance of the mentor can help more in larger state spaces 
increasing the noise level reduces the observers ability to act upon the information
received from the mentor and therefore erodes its advantage over the control agent  we
note  however  that the benefit of imitation degrades gracefully with increased noise and is
present even at this relatively extreme noise level 
      experiment    confidence testing
sometimes the observers prior beliefs about the transition probabilities of the mentor can
mislead the observer and cause it to generate inappropriate values  the confidence mechanism proposed in the previous section can prevent the observer from being fooled by
misleading priors over the mentors transition probabilities  to demonstrate the role of the
confidence mechanism in implicit imitation  we designed an experiment based on the scenario illustrated in figure    again  the agents task is to navigate from the top left corner
to the bottom right corner of a        grid in order to attain a reward of     we have cre   

fiprice   boutilier

  

  

  

  

figure    an environment with misleading priors
ated a pathological scenario in which islands of high reward      are enclosed by obstacles 
since the observers priors reflect eight connectivity and are uniform  the high valued cells
in the middle of each island are believed to be reachable from the states diagonally adjacent
with some small prior probability  in reality  however  the agents action set precludes this
and the agent will therefore never be able to realize this value  the four islands in this
scenario thus create a fairly large region in the center of the space with a high estimated
value  which could potentially trap an observer if it persisted in its prior beliefs 
notice that a standard reinforcement learner will quickly learn that none of its actions
take it to the rewarding islands  in contrast  an implicit imitator using augmented backups
could be fooled by its prior mentor model  if the mentor does not visit the states neighboring
the island  the observer will not have any evidence upon which to change its prior belief that
the mentor actions are equally likely to take one in any of the eight possible directions  the
imitator may falsely conclude on the basis of the mentor action model that an action does
exist which would allow it to access the islands of value  the observer therefore needs a
confidence mechanism to detect when the mentor model is less reliable than its own model 
to test the confidence mechanism  we have the mentor follows a path around the outside
of the obstacles so that its path cannot lead the observer out of the trap  i e   it provides
no evidence to the observer that the diagonal moves into the islands are not feasible   the
combination of a high initial exploration rate and the ability of prioritized sweeping to
spread value across large distances then virtually guarantees that the observer will be led to
the trap  given this scenario  we ran two observer agents and a control  the first observer
used a confidence interval with width given by    which  according to the chebychev rule 
should cover approximately    percent of an arbitrary distribution  the second observer
was given a   interval  which effectively disables confidence testing  the observer with no
confidence testing consistently became stuck  examination of the value function revealed
consistent peaks within the trap region  and inspection of the agent state trajectories showed
that it was stuck in the trap  the observer with confidence testing consistently escaped the
trap  observation of its value function over time shows that the trap formed  but faded
away as the observer gained enough experience to with its own actions to allow it to ignore
   

fiimplicit imitation

  
cr series

  

ctrl

  

average reward per      steps

obs
  

  

  

  

  

 

 

 

delta

 

    

    

    

    

     

     

simulation steps

figure    misleading priors may degrade performance
overcome erroneous priors over the mentor actions  in figure    the performance of the
observer with confidence testing is shown with the performance of the control agent  results
are averaged over    runs   we see that the observers performance is only slightly degraded
from that of the unaugmented control agent even in this pathological case 
      experiment    qualitative difficulty
the next experiment demonstrates how the potential gains of imitation can increase with
the  qualitative  difficulty of the problem  the observer employs both model extraction and
confidence testing  though confidence testing will not play a significant role here    in the
maze scenario  we introduce obstacles in order to increase the difficulty of the learning
problem  the maze is set on a        grid  figure    with     obstacles complicating the
agents journey from the top left to the bottom right corner  the optimal solution takes
the form of a snaking     step path  with distracting paths  up to length     branching
off from the solution path necessitating frequent backtracking  the discount factor is      
with    percent noise  the optimal goal attainment rate is about six goals per      steps 
from the graph in figure    with results averaged over ten runs   we see that the control
agent takes on the order of         steps to build a decent value function that reliably leads
to the goal  at this point  it is only achieving four goals per      steps on average  as its
exploration rate is still reasonably high  unfortunately  decreasing exploration more quickly
leads to slower value function formation   the imitation agent is able to take advantage of
the mentors expertise to build a reliable value function in about        steps  since the
control agent has been unable to reach the goal at all in the first        steps  the delta
between the control and the imitator is simply equal to the imitators performance  the
    the mentor does not provide evidence about some path choices in this problem  but there are no
intermediate rewards which would cause the observer to make use of the misleading mentor priors at
these states 

   

fiprice   boutilier

figure    a complex maze
 

cmb series
 

average reward per      steps

 

 

 

obs

 

delta

 

 

 

ctrl

   

 

   

 

   
 

x   

simulation steps

figure    imitation in a complex space
imitator can quickly achieve the optimal goal attainment rate of six goals per      steps 
as its exploration rate decays much more quickly 
      experiment    improving suboptimal policies by imitation
the augmented backup rule does not require that the reward structure of the mentor and
observer be identical  there are many useful scenarios where rewards are dissimilar but
the value functions and policies induced share some structure  in this experiment  we
demonstrate one interesting scenario in which it is relatively easy to find a suboptimal
solution  but difficult to find the optimal solution  once the observer finds this suboptimal
path  however  it is able to exploit its observations of the mentor to see that there is a
   

fiimplicit imitation

 

 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 

 
 

figure    a maze with a perilous shortcut

shortcut that significantly shortens the path to the goal  the structure of the scenario
is shown in figure    the suboptimal solution lies on the path from location   around
the scenic route to location   and on to the goal at location    the mentor takes the
vertical path from location   to location   through the shortcut    to discourage the
use of the shortcut by novice agents  it is lined with cells  marked    from which the
agent immediately jumps back to the start state  it is therefore difficult for a novice agent
executing random exploratory moves to make it all the way to the end of the shortcut
and obtain the value which would reinforce its future use  both the observer and control
therefore generally find the scenic route first 
in figure     the performance  measured using goals reached over the previous     
steps  of the control and observer are compared  averaged over ten runs   indicating the
value of these observations  we see that the observer and control agent both find the longer
scenic route  though the control agent takes longer to find it  the observer goes on to find
the shortcut and increases its return to almost double the goal rate  this experiment shows
that mentors can improve observer policies even when the observers goals are not on the
mentors path 
      experiment    multiple mentors
the final experiment illustrates how model extraction can be readily extended so that the
observer can extract models from multiple mentors and exploit the most valuable parts of
each  again  the observer employs model extraction and confidence testing  in figure    
the learner must move from start location   to goal location    two expert agents with
different start and goal states serve as potential mentors  one mentor repeatedly moves from
location   to location   along the dotted line  while a second mentor departs from location
  and ends at location   along the dashed line  in this experiment  the observer must
    a mentor proceeding from   to   would not provide guidance without prior knowledge that actions are
reversible 

   

fiprice   boutilier

  

csb series
  

average reward per      steps

  

obs

  
delta
  

  
ctrl
 

 

 

   

 

   

 

   

 
 

x   

simulation steps

figure     transfer with non identical rewards

 

 

 

 

 

figure     multiple mentors scenario

combine the information from the examples provided by the two mentors with independent
exploration of its own in order to solve the problem 
in figure     we see that the observer successfully pulls together these information
sources in order to learn much more quickly than the control agent  results are averaged
over    runs   we see that the use of a value based technique allows the observer to choose
which mentors influence to use on a state by state basis in order to get the best solution
to the problem 
   

fiimplicit imitation

  

obs
cmm series

average reward per      steps

  

  

ctrl
  

  
delta

  

 

 

    

    

    

    

    

    

simulation steps

figure     learning from multiple mentors

   implicit imitation in heterogeneous settings
when the homogeneity assumption is violated  the implicit imitation framework described
above can cause the learners convergence rate to slow dramatically and  in some cases 
cause the learner to become stuck in a small neighborhood of state space  in particular 
if the learner is unable to make the same state transition  or a transition with the same
probability  as the mentor at a given state  it may drastically overestimate the value of
that state  the inflated value estimate causes the learner to return repeatedly to this state
even though its exploration will never produce a feasible action that attains the inflated
estimated value  there is no mechanism for removing the influence of the mentors markov
chain on value estimatesthe observer can be extremely  and correctly  confident in its
observations about the mentors model  the problem lies in the fact that the augmented
bellman backup is justified by the assumption that the observer can duplicate every mentor
action  that is  at each state s  there is some a  a such that pro  s  a  t    prm  s  t  for
all t  when an equivalent action a does not exist  there is no guarantee that the value
calculated using the mentor action model can  in fact  be achieved 
    feasibility testing
in such heterogeneous settings  we can prevent lock up and poor convergence through the
use of an explicit action feasibility test  before an augmented backup is performed at s  the
observer tests whether the mentors action am differs from each of its actions at s  given
its current estimated models  if so  the augmented backup is suppressed and a standard
bellman backup is used to update the value function     by default  mentor actions are
    the decision is binary  but we could envision a smoother decision criterion that measures the extent to
which the mentors action can be duplicated 

   

fiprice   boutilier

assumed to be feasible for the observer  however  once the observer is reasonably confident
that am is infeasible at state s  augmented backups are suppressed at s 
recall that uncertainty about the agents true transition probabilities are captured by a
dirichlet distribution derived from sampled transitions  comparing am with ao is effected by
a difference of means test with respect to the corresponding dirichlets  this is complicated
by the fact that dirichlets are highly non normal for small parameter values and transition
distributions are multinomial  we deal with the non normality by requiring a minimum
number of samples and using robust chebychev bounds on the pooled variance of the
distributions to be compared  conceptually  we will evaluate equation    
r

  pro  s  ao   t   prm  s  t  
 
 
no  s ao  t omodel
 s ao  t  nm  s t mmodel
 s t 
no  s ao  t  nm  s t 

  z  

    

here z   is the critical value of the test  the parameter  is the significance of the test 
or the probability that we will falsely reject two actions as being different when they are
actually the same  given our highly non normal distributions early in the training process 
the appropriate z value for a given  can be computed from chebychevs bound by solving
       z   for z    
when we have too few samples to do an accurate test  we persist with augmented
backups  embodying our default assumption of homogeneity   if the value estimate is
inflated by these backups  the agent will be biased to obtain additional samples  which will
then allow the agent to perform the required feasibility test  our assumption is therefore
self correcting  we deal with the multivariate complications by performing the bonferroni
test  seber         which has been shown to give good results in practice  mi   sampson 
       is efficient to compute  and is known to be robust to dependence between variables  a
bonferroni hypothesis test is obtained by conjoining several single variable tests  suppose
the actions ao and am result in r possible successor states  s         sr  i e   r transition
probabilities to compare   for each si   the hypothesis ei denotes that ao and am have the
same transition probability to successor state si   that is pr s  am   si     pr s  ao   si    we let
ei denote the complementary hypothesis  i e   that the transition probabilities differ   the
bonferroni inequality states 
 

pr

r
 

 

ei    

i  

r
x



pr ei



i  

t

thus we can test the joint hypothesis ri   ei the two action models are the sameby
testing each of the r complementary hypotheses ei at confidence level  r  if we reject
any of the hypotheses we reject the notion that the two actions are equal with confidence
at least   the mentor action am is deemed infeasible if for every observer action ao   the
multivariate bonferroni test just described rejects the hypothesis that the action is the same
as the mentors 
pseudo code for the bonferroni component of the feasibility test appears in table    it
assumes a sufficient number of samples  for efficiency reasons  we cache the results of the
feasibility testing  when the duplication of the mentors action at state s is first determined
to be infeasible  we set a flag for state s to this effect 
   

fiimplicit imitation

function feasible m s    boolean
for each a in ao do
allsuccessorprobssimilar   true
for each t in successors s  do
    p roq
 s  a  t   p rm  s  t  
no  s a t   

 s a t  nm  s t   

omodel
z     
no  s a t  nm  s t 
if z   z  r
allsuccessorprobssimilar   false
end for
if allsuccessorprobssimilar
return true
end for
return false

mmodel

 s t 

table    action feasibility testing
    k step similarity and repair
action feasibility testing essentially makes a strict decision as to whether the agent can
duplicate the mentors action at a specific state  once it is decided that the mentors action is
infeasible  augmented backups are suppressed and all potential guidance offered is eliminated
at that state  unfortunately  the strictness of the test results in a somewhat impoverished
notion of similarity between mentor and observer  this  in turn  unnecessarily limits the
transfer between mentor and observer  we propose a mechanism whereby the mentors
influence may persist even if the specific action it chooses is not feasible for the mentor  we
instead rely on the possibility that the observer may approximately duplicate the mentors
trajectory instead of exactly duplicating it 
suppose an observer has previously constructed an estimated value function using augmented backups  using the mentor action model  i e   the mentors chain prm  s  t    a high
value has been calculated for state s  subsequently  suppose the mentors action at state s
is judged to be infeasible  this is illustrated in figure     where the estimated value at state
s is originally due to the mentors action m  s   which for the sake of illustration moves
with high probability to state t  which itself can lead to some highly rewarding region of
state space  after some number of experiences at state s  however  the learner concludes
that the action m  s and the associated high probability transition to tis not feasible 
at this point  one of two things must occur  either  a  the value calculated for state
s and its predecessors will collapse and all exploration towards highly valued regions
beyond state s ceases  or  b  the estimated value drops slightly but exploration continues
towards the highly valued regions  the latter case may arise as follows  if the observer
has previously explored in the vicinity of state s  the observers own action model may be
sufficiently developed that they still connect the higher value regions beyond state s to state
s through bellman backups  for example  if the learner has sufficient experience to have
learned that the highly valued region can be reached through the alternative trajectory
s  u  v  w  the newly discovered infeasibility of the mentors transition s  t will not have
a deleterious effect on the value estimate at s  if s is highly valued  it is likely that states
close to the mentors trajectory will be explored to some degree  in this case  state s will
   

fiprice   boutilier

infeasible transition

s

high value
state

t

w
 bridge 

u

v

figure     an alternative path can bridge value backups around infeasible paths
not be as highly valued as it was when using the mentors action model  but it will still be
valued highly enough that it will likely to guide further exploration toward the area  we call
this alternative  in this case s  u  v  w  to the mentors action a bridge  because it allows
value from higher value regions to flow over an infeasible mentor transition  because
the bridge was formed without the intention of the agent  we call this process spontaneous
bridging 
where a spontaneous bridge does not exist  the observers own action models are generally undeveloped  e g   they are close to their uniform prior distributions   typically  these
undeveloped models assign a small probability to every possible outcome and therefore diffuse value from higher valued regions and lead to a very poor value estimate for state s 
the result is often a dramatic drop in the value of state s and all of its predecessors  and
exploration towards the highly valued region through the neighborhood of state s ceases 
in our example  this could occur if the observers transition models at state s assign low
probability  e g   close to prior probability  of moving to state u due to lack of experience
 or similarly if the surrounding states  such as u or v  have been insufficiently explored  
the spontaneous bridging effect motivates a broader notion of similarity  when the
observer can find a short sequence of actions that bridges an infeasible action on the
mentors trajectory  the mentors example can still provide extremely useful guidance  for
the moment  we assume a short path is any path of length no greater than some given
integer k  we say an observer is k step similar to a mentor at state s if the observer can
duplicate in k or fewer steps the mentors nominal transition at state s with sufficiently
high probability 
given this notion of similarity  an observer can now test whether a spontaneous bridge
exists and determine whether the observer is in danger of value function collapse and the
concomitant loss of guidance if it decides to suppress an augmented backup at state s  to do
this  the observer initiates a reachability analysis starting from state s using its own action
model pro  s  a  t  to determine if there is a sequence of actions with leads with sufficiently
high probability from state s to some state t on the mentors trajectory downstream of
the infeasible action    if a k step bridge already exists  augmented backups can be safely
suppressed at state s  for efficiency  we maintain a flag at each state to mark it as bridged 
once a state is known to be bridged  the k step reachability analysis need not be repeated 
if a spontaneous bridge cannot be found  it might still be possible to intentionally set out
to build one  to build a bridge  the observer must explore from state s up to k steps away 
hoping to make contact with the mentors trajectory downstream of the infeasible mentor
    in a more general state space where ergodicity is lacking  the agent must consider predecessors of state
s up to k steps before s to guarantee that all k step paths are checked 

   

fiimplicit imitation

action  we implement a single search attempt as a k   step random walk  which will result
in a trajectory on average k steps away from s as long ergodicity and local connectivity
assumptions are satisfied  in order for the search to occur  we must motivate the observer to
return to the state s and engage in repeated exploration  we could provide motivation to the
observer by asking the observer to assume that the infeasible action will be repairable  the
observer will therefore continue the augmented backups which support high value estimates
at the state s and the observer will repeatedly engage in exploration from this point  the
danger  of course  is that there may not in fact be a bridge  in which case the observer will
repeat this search for a bridge indefinitely  we therefore need a mechanism to terminate
the repair process when a k step repair is infeasible  we could attempt to explicitly keep
track of all of the possible paths open to the observer and all of the paths explicitly tried by
the observer and determine the repair possibilities had been exhausted  instead  we elect
to follow a probabilistic search that eliminates the need for bookkeeping  if a bridge cannot
be constructed within n attempts of k step random walk  the repairability assumption is
judged falsified  the augmented backup at state s is suppressed and the observers bias to
explore the vicinity of state s is eliminated  if no bridge is found for state s  a flag is used
to mark the state as irreparable 
this approach is  of course  a very nave heuristic strategy  but it illustrates the basic
import of bridging  more systematic strategies could be used  involving explicit planning
to find a bridge using  say  local search  alissandrakis  nehaniv    dautenhahn        
another aspect of this problem that we do not address is the persistence of search for
bridges  in a specific domain  after some number of unsuccessful attempts to find bridges 
a learner may conclude that it is unable to reconstruct a mentors behavior  in which case
the search for bridges may be abandoned  this involves simple  higher level inference  and
some notion of  or prior beliefs about  similarity of capabilities  these notions could also
be used to automatically determine parameter settings  discussed below  
the parameters k and n must be tuned empirically  but can be estimated given knowledge of the connectivity of the domain and prior beliefs about how similar  in terms of
length of average repair  the trajectories of the mentor and observer will be  for instance 
n    k    seems suitable in an   connected grid world with low noise  based on the number
of trajectories required to cover the perimeter states of a k step rectangle around a state 
we note that very large values of n can reduce performance below that of non imitating
agents as it results in temporary lock up 
feasibility and k step repair are easily integrated into the homogeneous implicit imitation framework  essentially  we simply elaborate the conditions under which the augmented
backup will be employed  of course  some additional representation will be introduced to
keep track of whether a state is feasible  bridged  or repairable  and how many repair attempts have been made  the action selection mechanism will also be overridden by the
bridge building algorithm when required in order to search for a bridge  bridge building
always terminates after n attempts  however  so it cannot affect long run convergence  all
other aspects of the algorithm  however  such as the exploration policy  are unchanged 
the complete elaborated decision procedure used to determine when augmented backups
will be employed at state s with respect to mentor m appears in table    it uses some
internal state to make its decisions  as in the original model  we first check to see if the
observers experience based calculation for the value of the state supersedes the mentor   

fiprice   boutilier

function use augmented  s m    boolean
if vo  s   vm  s  then return false
else if f easible s  m  then return true
else if bridged s  m  then return false
else if reachable s  m  then
bridged s m     true
return false
else if not repairable s  m  then return false
else   we are searching
if     search steps s  m    k then   search in progress
return true
if search steps s  m    k then   search failed
if attempts s    n then
repairable s    false
return false
else
reset search s m 
attempts s     attempts s     
return true
attempts s        initiate first attempt of a search
initiate search s 
return true

table    elaborated augmented backup test

based calculation  if so  then the observer uses its own experience based calculation  if
the mentors action is feasible  then we accept the value calculated using the observationbased value function  if the action is infeasible we check to see if the state is bridged  the
first time the test is requested  a reachability analysis is performed  but the results will be
drawn from a cache for subsequent requests  if the state has been bridged  we suppress
augmented backups  confident that this will not cause value function collapse  if the state
is not bridged  we ask if it is repairable  for the first n requests  the agent will attempt a
k step repair  if the repair succeeds  the state is marked as bridged  if we cannot repair
the infeasible transition  we mark it not repairable and suppress augmented backups 
we may wish to employ implicit imitation with feasibility testing in a multiple mentor
scenario  the key change from implicit imitation without feasibility testing is that the
observer will only imitate feasible actions  when the observer searches through the set of
mentors for the one with the action that results in the highest value estimate  the observer
must consider only those mentors whose actions are still considered feasible  or assumed to
be repairable  
    empirical demonstrations
in this section  we empirically demonstrate the utility of feasibility testing and k step repair
and show how the techniques can be used to surmount both differences in actions between
agents and small local differences in state space topology  the problems here have been
   

fiimplicit imitation

chosen specifically to demonstrate the necessity and utility of both feasibility testing and
k step repair 
      experiment    necessity of feasibility testing
our first experiment shows the importance of feasibility testing in implicit imitation when
agents have heterogeneous actions  in this scenario  all agents must navigate across an
obstacle free         grid world from the upper left corner to a goal location in the lowerright  the agent is then reset to the upper left corner  the first agent is a mentor with
the news action set  north  south  east  and west movement actions   the mentor
is given an optimal stationary policy for this problem  we study the performance of three
learners  each with the skew action set  n  s  ne  sw  and unable to duplicate the mentor
exactly  e g   duplicating a mentors e move requires the learner to move ne followed by
s  or move se then n   due to the nature of the grid world  the control and imitation
agents will actually have to execute more actions to get to the goal than the mentor and
the optimal goal rate for both the control and imitator are therefore lower than that of the
mentor  the first learner employs implicit imitation with feasibility testing  the second uses
imitation without feasibility testing  and the third control agent uses no imitation  i e   is a
standard reinforcement learning agent   all agents experience limited stochasticity in the
form of a    chance that their action will be randomly perturbed  as in the last section 
the agents use model based reinforcement learning with prioritized sweeping  we set k    
and n      
the effectiveness of feasibility testing in implicit imitation can be seen in figure    
the horizontal axis represents time in simulation steps and the vertical axis represents the
average number of goals achieved per      time steps  averaged over    runs   we see that
the imitation agent with feasibility testing converges much more quickly to the optimal
goal attainment rate than the other agents  the agent without feasibility testing achieves
sporadic success early on  but frequently locks up due to repeated attempts to duplicate
infeasible mentor actions  the agent still manages to reach the goal from time to time  as
the stochastic actions do not permit the agent to become permanently stuck in this obstaclefree scenario  the control agent without any form of imitation demonstrates a significant
delay in convergence relative to the imitation agents due to the lack of any form of guidance 
but easily surpasses the agent without feasibility testing in the long run  the more gradual
slope of the control agent is due to the higher variance in the control agents discovery time
for the optimal path  but both the feasibility testing imitator and the control agent converge
to optimal solutions  as shown by the comparison of the two imitation agents  feasibility
testing is necessary to adapt implicit imitation to contexts involving heterogeneous actions 
      experiment    changes to state space
we developed feasibility testing and bridging primarily to deal with the problem of adapting
to agents with heterogeneous actions  the same techniques  however  can be applied to
agents with differences in their state space connectivity  ultimately  these are equivalent
notions   to test this  we constructed a domain where all agents have the same news
action set  but we alter the environment of the learners by introducing obstacles that arent
present for the mentor  in figure     the learners find that the mentors path is obstructed
   

fiprice   boutilier

  

feas

  
fs series

average reward per      steps

  

  

ctrl

  

  
nofeas
  

 

 

 

   

    

    

    
    
simulation steps

    

    

    

    

figure     utility of feasibility testing
s

x

figure     obstacle map and mentor path
by obstacles  movement toward an obstacle causes a learner to remain in its current state 
in this sense  its action has a different effect than the mentors 
in figure     we see that the results are qualitatively similar to the previous experiment 
in contrast to the previous experiment  both imitator and control use the news action set
and therefore have a shortest path with the same length as that of the mentor  consequently 
the optimal goal rate of the imitators and control is higher than in the previous experiment 
the observer without feasibility testing has difficulty with the maze  as the value function
augmented by mentor observations consistently leads the observer to states whose path to
the goal is directly blocked  the agent with feasibility testing quickly discovers that the
mentors influence is inappropriate at such states  we conclude that local differences in
state are well handled by feasibility testing 
next  we demonstrate how feasibility testing can completely generalize the mentors
trajectory  here  the mentor follows a path which is completely infeasible for the imitating
agent  we fix the mentors path for all runs and give the imitating agent the maze shown
   

fiimplicit imitation

  
feas
fo series

  

average reward per      steps

  

  

ctrl

  

  

  

  

  

 

 

nofeas

 

   

    

    

    
    
simulation steps

    

    

    

    

figure     interpolating around obstacles

s

observer
mentor
x

figure     parallel generalization
in figure    in which all but two of the states the mentor visits are blocked by an obstacle 
the imitating agent is able to use the mentors trajectory for guidance and builds its own
parallel trajectory which is completely disjoint from the mentors 
the results in figure    show that gain of the imitator with feasibility testing over the
control agent diminishes  but still exists marginally when the imitator is forced to generalize
a completely infeasible mentor trajectory  the agent without feasibility testing does very
poorly  even when compared to the control agent  this is because it gets stuck around the
doorway  the high value gradient backed up along the mentors path becomes accessible to
the agents at the doorway  the imitation agent with feasibility will conclude that it cannot
proceed south from the doorway  into the wall  and it will then try a different strategy 
the imitator without feasibility testing never explores far enough away from the doorway
to setup an independent value gradient that will guide it to the goal  with a slower decay
schedule for exploration  the imitator without feasibility testing would find the goal  but this
   

fiprice   boutilier

  

feas
  

fp series

average reward per      steps

ctrl

  

  

  

  

nofeas
 

 

   

    

    

    
    
simulation steps

    

    

    

    

figure     parallel generalization results

would still reduce its performance below that of the imitator with feasibility testing  the
imitator with feasibility testing makes use of its prior beliefs that it can follow the mentor
to backup value perpendicular to the mentors path  a value gradient will therefore form
parallel to the infeasible mentor path and the imitator can follow along side the infeasible
path towards the doorway where it makes the necessary feasibility test and then proceeds
to the goal 
as explained earlier  in simple problems there is a good chance that the informal effects
of prior value leakage and stochastic exploration may form bridges before feasibility testing
cuts off the value propagation that guides exploration  in more difficult problems where the
agent spends a lot more time exploring  it will accumulate sufficient samples to conclude
that the mentors actions are infeasible long before the agent has constructed its own bridge 
the imitators performance would then drop down to that of an unaugmented reinforcement
learner 
to demonstrate bridging  we devised a domain in which agents must navigate from the
upper left corner to the bottom right corner  across a river which is three steps wide and
exacts a penalty of     per step  see figure      the goal state is worth      in the figure 
the path of the mentor is shown starting from the top corner  proceeding along the edge of
the river and then crossing the river to the goal  the mentor employs the news action
set  the observer uses the skew action set  n  ne  s  sw  and attempts to reproduce
the mentor trajectory  it will fail to reproduce the critical transition at the border of the
river  because the east action is infeasible for a skew agent   the mentor action can
no longer be used to backup value from the rewarding state and there will be no alternative
paths because the river blocks greedy exploration in this region  without bridging or an
optimistic and lengthly exploration phase  observer agents quickly discover the negative
states of the river and curtail exploration in this direction before actually making it across 
   

fiimplicit imitation

figure     river scenario
if we examine the value function estimate  after      steps  of an imitator with feasibility
testing but no repair capabilities  we see that  due to suppression by feasibility testing  the
darkly shaded high value states in figure     backed up from the goal  terminate abruptly
at an infeasible transition without making it across the river  in fact  they are dominated by
the lighter grey circles showing negative values  in this experiment  we show that bridging
can prolong the exploration phase in just the right way  we employ the k step repair
procedure with k     
examining the graph in figure     we see that both imitation agents experience an early
negative dip as they are guided deep into the river by the mentors influence  the agent
without repair eventually decides the mentors action is infeasible  and thereafter avoids
the river  and the possibility of finding the goal   the imitator with repair also discovers
the mentors action to be infeasible  but does not immediately dispense with the mentors
guidance  it keeps exploring in the area of the mentors trajectory using a random walk 
all the while accumulating a negative reward until it suddenly finds a bridge and rapidly
converges on the optimal solution    the control agent discovers the goal only once in the
ten runs 

   applicability
the simple experiments presented above demonstrate the major qualitative issues confronting an implicit imitation agent and how the specific mechanisms of implicit imitation
address these issues  in this section  we examine how the assumptions and the mechanisms we presented in the previous sections determine the types of problems suitable for
implicit imitation  we then present several dimensions that prove useful for predicting the
performance of implicit imitation in these types of problems 
    while repair steps take place in an area of negative reward in this scenario  this need not be the case 
repair doesnt imply short term negative return 

   

fiprice   boutilier

  

average reward per      steps

  

fb series

 
ctrl
ctrl
 
norepair

repair
 

norepair
  

  

  

repair

 

    

    

    
simulation steps

    

    

    

figure     utility of bridging
we have already identified a number of assumptions under which implicit imitation is
applicablesome assumptions under which other models of imitation or teaching cannot be
applied  and some assumptions that restrict the applicability of our model  these include 
lack of explicit communication between mentors and observer  independent objectives for
mentors and observer  full observability of mentors by observer  unobservability of mentors
actions  and  bounded  heterogeneity  assumptions such as full observability are necessary
for our modelas formulatedto work  though we discuss extension to the partially observable case in section     assumptions of lack of communication and unobservable actions
extend the applicability of implicit imitation beyond other models in the literature  if these
conditions do not hold  a simpler form of explicit communication may be preferable  finally 
the assumptions of bounded heterogeneity and independent objectives also ensure implicit
imitation can be applied widely  however  the degree to which rewards are the same and
actions are homogeneous can have an impact on the utility  i e   the acceleration of learning offered by  implicit imitation  we turn our attention to predicting the performance of
implicit imitation as a function of certain domain characteristics 
    predicting performance
in this section we examine two questions  first  given that implicit imitation is applicable 
when can implicit imitation bias an agent to a suboptimal solution  and second  how will
the performance of implicit imitation vary with structural characteristics of the domains
one might want to apply it to  we show how analysis of the internal structure of state space
can be used to motivate a metric that  roughly  predicts implicit imitation performance 
we conclude with an analysis of how the problem space can be understood in terms of
distinct regions playing different roles within an imitation context 
   

fiimplicit imitation

in the implicit imitation model  we use observations of other agents to improve the observers knowledge about its environment and then rely on a sensible exploration policy to
exploit this additional knowledge  a clear understanding of how knowledge of the environment affects exploration is therefore central to understanding how implicit imitation will
perform in a domain 
within the implicit imitation framework  agents know their reward functions  so knowledge of the environment consists solely of knowledge about the agents action models  in
general  these models can take any form  for simplicity  we have restricted ourselves to
models that can be decomposed into local models for each possible combination of a system
state and agent action 
the local models for state action pairs allow the prediction of a j step successor state
distribution given any initial state and sequence of actions or local policy  the quality of
the j step state predictions will be a function of every action model encountered between
the initial state and the states at time j     unfortunately  the quality of the j step
estimate can be drastically altered by the quality of even a single intermediate state action
model  this suggests that connected regions of state space  the states of which all have
fairly accurate models  will allow reasonably accurate future state predictions 
since the estimated value of a state s is based on both the immediate reward and the
reward expected to be received in subsequent states  the quality of this value estimate
will also depend on the quality of the action models in those states connected to s  now 
since greedy exploration methods bias their exploration according to the estimated value
of actions  the exploratory choices of an agent at state s will also be dependent on the
connectivity of reliable action models at those states reachable from s  our analysis of
implicit imitation performance with respect to domain characteristics is therefore organized
around the idea of state space connectivity and the regions such connectivity defines 
      the imitation regions framework
since connected regions play an important role in implicit imitation  we introduce a classification of different regions within the state space shown graphically in figure     in what
follows  we describe of how these regions affect imitation performance in our model 
we first observe that many tasks can be carried out by an agent in a small subset of
states within the state space defined for the problem  more precisely  in many mdps  the
optimal policy will ensure that an agent remains in a small subspace of state space  this
leads us to the definition of our first regional distinction  relevant vs  irrelevant regions  the
relevant region is the set of states with non zero probability of occupancy under the optimal
policy    an  relevant region is a natural generalization in which the optimal policy keeps
the system within the region a fraction     of the time 
within the relevant region  we distinguish three additional subregions  the explored
region contains those states where the observer has formulated reliable action models on the
basis of its own experience  the augmented region contains those states where the observer
lacks reliable action models but has improved value estimates due to mentor observations 
    one often assumes that the system starts in one of a small set of states  if the markov chain induced by
the optimal policy then is not ergodic  then the irrelevant region will be nonempty  otherwise it will be
empty 

   

fiprice   boutilier

observer
explored
region
blind
region

mentor
augmented
region

irrelevant
region

reward
figure     classification of regions of state space
note that both the explored and augmented regions are created as the result of observations
made by the learner  of either its own transitions or those of a mentor   these regions will
therefore have significant connected components  that is  contiguous regions of state space
where reliable action or mentor models are available  finally  the blind region designates
those states where the observer has neither  significant  personal experience nor the benefit
of mentor observations  any information about states within the blind region will come
 largely  from the agents prior beliefs   
we can now ask how these regions interact with an imitation agent  first we consider the
impact of relevance  implicit imitation makes the assumption that more accurate dynamics
models allow an observer to make better decisions which will  in turn  result in higher returns
sooner in the learning process  however  not all model information is equally helpful  the
imitator needs only enough information about the irrelevant region to be able to avoid it 
since action choices are influenced by the relative value of actions  the irrelevant region will
be avoided when it looks worse than the relevant region  given diffuse priors on action
models  none of the actions open to an agent will initially appear particularly attractive 
however  a mentor that provides observations within the relevant region can quickly make
the relevant region look much more promising as a method of achieving higher returns and
therefore constrain exploration significantly  therefore  considering problems just from the
point of view of relevance  a problem with a small relevant region relative to the entire space
combined with a mentor that operates within the relevant region will result in maximum
advantage for an imitation agent over a non imitating agent 
in the explored region  the observer has sufficiently accurate models to compute a good
policy with respect to rewards within the explored region  additional observations on
    our partitioning of states into explored  blind and augmented regions bears some resemblance to kearns
and singhs        partitioning of state space into known and unknown regions  unlike kearns and singh 
however  we use the partitions only for analysis  the implicit imitation algorithm does not explicitly
maintain these partitions or use them in any way to compute its policy 

   

fiimplicit imitation

the states within the explored region provided by the mentor can still improve performance
somewhat if significant evidence is required to accurately discriminate between the expected
value of two actions  hence  mentor observations in the explored region can help  but will
not result in dramatic speedups in convergence 
now  we consider the augmented region in which the observers q values have been
augmented with observations of a mentor  in experiments in previous sections  we have
seen that an observer entering an augmented region can experience significant speedups in
convergence due to the information inherent in the augmented value function about the
location of rewards in the region  characteristics of the augmented zone  however  can
affect the degree to which augmentation improves convergence speed 
since the observer receives observations of only the mentors state  and not its actions 
the observer has improved value estimates for states in the augmented region  but no policy 
the observer must therefore infer which actions should be taken to duplicate the mentors
behavior  where the observer has prior beliefs about the effects of its actions  it may be able
to perform immediate inference about the mentors actual choice of action  perhaps using
kl divergence or maximum likelihood   where the observers prior model is uninformative 
the observer will have to explore the local action space  in exploring a local action space 
however  the agent must take an action and this action will have an effect  since there is no
guarantee that the agent took the action that duplicates the mentors action  it may end up
somewhere different than the mentor  if the action causes the observer to fall outside of the
augmented region  the observer will lose the guidance that the augmented value function
provides and fall back to the performance level of a non imitating agent 
an important consideration  then  is the probability that the observer will remain in
augmented regions and continue to receive guidance  one quality of the augmented region
that affects the observers probability of staying within its boundaries is its relative coverage
of the state space  the policy of the mentor may be sparse or complete  in a relatively
deterministic domain with defined begin and end states  a sparse policy covering few states
may be adequate  in a highly stochastic domain with many start and end states  an agent
may need a complete policy  i e   covering every state   implicit imitation will provide
more guidance to the agent in domains that are more stochastic and require more complete
policies  since the policy will cover a larger part of the state space 
as important as the completeness of a policy is in predicting its guidance  we must
also take into account the probability of transitions into and out of the augmented region 
where the actions in a domain are largely invertible  directly  or effectively so   the agent
has a chance of re entering the augmented region  where ergodicity is lacking  however 
the agent may have to wait until the process undergoes some form of reset before it has
the opportunity to gather additional evidence regarding the identity of the mentors actions
in the augmented region  the reset places the agent back into the explored region  from
which it can make its way to the frontier where it last explored  the lack of ergodicity
would reduce the agents ability to make progress towards high value regions before resets 
but the agent is still guided on each attempt by the augmented region  effectively  the
agent will concentrate its exploration on the boundary between the explored region and the
mentor augmented region 
the utility of mentor observations will depend on the probability of the augmented
and explored regions overlapping in the course of the agents exploration  in the explored
   

fiprice   boutilier

regions  accurate action models allow the agent to move as quickly as possible to high
value regions  in augmented regions  augmented q values inform agents about which states
lead to highly valued outcomes  when an augmented region abuts an explored region  the
improved value estimates from the augmented region are rapidly communicated across the
explored region by accurate action models  the observer can use the resultant improved
value estimates in the explored region  together with the accurate action models in the
explored region  to rapidly move towards the most promising states on the frontier of the
explored region  from these states  the observer can explore outward and thereby eventually
expand the explored region to encompass the augmented region 
in the case where the explored region and augmented region do not overlap  we have a
blind region  since the observer has no information beyond its priors for the blind region 
the observer is reduced to random exploration  in a non imitation context  any states that
are not explored are blind  however  in an imitation context  the blind area is reduced in
effective size by the augmented area  hence  implicit imitation effectively shrinks the size
of the search space of the problem even when there is no overlap between explored and
augmented spaces 
the most challenging case for implicit imitation transfer occurs when the region augmented by mentor observations fails to connect to both the observer explored region and
the regions with significant reward values  in this case  the augmented region will initially
provide no guidance  once the observer has independently located rewarding states  the
augmented regions can be used to highlight shortcuts  these shortcuts represent improvements on the agents policy  in domains where a feasible solution is easy to find  but
optimal solutions are difficult  implicit imitation can be used to convert a feasible solution
to an increasingly optimal solution 
      cross regional textures
we have seen how distinctive regions can be used to provide a certain level of insight into how
imitation will perform in various domains  we can also analyze imitation performance in
terms of properties that cut across the state space  in our analysis of how model information
impacts imitation performance  we saw that regions connected by accurate action models
allowed an observer to use mentor observations to learn about the most promising direction
for exploration  we see  then  that any set of mentor observations will be more useful
if it is concentrated on a connected region and less useful if dispersed about the state
space in unconnected components  we are fortunate in completely observable environments
that observations of mentors tend to capture continuous trajectories  thereby providing
continuous regions of augmented states  in partially observable environments  occlusion
and noise could lessen the value of mentor observations in the absence of a model to predict
the mentors state 
the effects of heterogeneity  whether due to differences in action capabilities in the
mentor and observer or due to differences in the environment of the two agents  can also
be understood in terms of the connectivity of action models  value can propagate along
chains of action models until we hit a state in which the mentor and observer have different
action capabilities  at this state  it may not be possible to achieve the mentors value
and therefore  value propagation is blocked  again  the sequential decision making aspect
   

fiimplicit imitation

of reinforcement learning leads to the conclusion that many scattered differences between
mentor and observer will create discontinuity throughout the problem space  whereas a
contiguous region of differences between mentor and observer will cause discontinuity in
a region  but leave other large regions fully connected  hence  the distribution pattern of
differences between mentor and observer capabilities is as important as the prevalence of
difference  we will explore this pattern in the next section 
    the fracture metric
we now try to characterize connectivity in the form of a metric  since differences in reward structure  environment dynamics and action models that affect connectivity all would
manifest themselves as differences in policies between mentor and observer  we designed a
metric based on differences in the agents optimal policies  we call this metric fracture 
essentially  it computes the average minimum distance from a state in which a mentor and
observer disagree on a policy to a state in which mentor and observer agree on the policy  this measure roughly captures the difficulty the observer faces in profitably exploiting
mentor observations to reduce its exploration demands 
more formally  let m be the mentors optimal policy and o be the observers  let s
be the state space and sm   o be the set of disputed states where the mentor and observer
have different optimal actions  a set of neighboring disputed states constitutes a disputed
region  the set s  sm   o will be called the undisputed states  let m be a distance
metric on the space s  this metric corresponds to the number of transitions along the
minimal length path between states  i e   the shortest path using nonzero probability
observer transitions     in a standard grid world  it will correspond to the manhattan
distance  we define the fracture  s  of state space s to be the average minimal distance
between a disputed state and the closest undisputed state 
 s   

 

x

 sm   o   ss

m   o

min

tssm   o

m  s  t  

    

other things being equal  a lower fracture value will tend to increase the propagation
of value information across the state space  potentially resulting in less exploration being
required  to test our metric  we applied it to a number of scenarios with varying fracture
coefficients  it is difficult to construct scenarios which vary in their fracture coefficient yet
have the same expected value  the scenarios in figure    have been constructed so that the
length of all possible paths from the start state s to the goal state x are the same in each
scenario  in each scenario  however  there is an upper path and a lower path  the mentor
is trained in a scenario that penalizes the lower path and so the mentor learns to take the
upper path  the imitator is trained in a scenario in which the upper path is penalized
and should therefore take the lower path  we equalized the difficulty of these problems
as follows  using a generic  greedy learning agent with a fixed exploration schedule  i e  
a fixed initial rate and decay  in one scenario  we tuned the magnitude of penalties and
their exact placement along loops in their other scenarios so that a learner using the same
exploration policy would converge to the optimal policy in roughly the same number of
steps in each 
    the expected distance would give a more accurate estimate of fracture  but is more difficult to calculate 

   

fiprice   boutilier

x

x

s

s

 a        

x

s

 b        

s

 c        

x

 d        

figure     fracture metric scenarios


   
   
   
   

      
   

      
   

observer initial exploration rate i
                           
   
  
   
   
    
   
     
    
    

      

      

     

     

figure     percentage of runs  of ten  converging to optimal policy given fracture  and
initial exploration rate i

in figure    a   the mentor takes the top of each loop and in an optimal run  the imitator
would take the bottom of each loop  since the loops are short and the length of the common
path is long  the average fracture is low  when we compare this to figure    d   we see
that the loops are very longthe majority of states in the scenario are on loops  each of
these states on a loop has a distance to the nearest state where the observer and mentor
policies agree  namely  a state not on the loop  this scenario therefore has a high average
fracture coefficient 
since the loops in the various scenarios differ in length  penalties inserted in the loops
vary with respect to their distance from the goal state and therefore affect the total discounted expected reward in different ways  the penalties may also cause the agent to
become stuck in a local minimum in order to avoid the penalties if the exploration rate is
too low  in this set of experiments  we therefore compare observer agents on the basis of
how likely they are to converge to the optimal solution given the mentor example 
figure    presents the percentage of runs  out of ten  in which the imitator converged
to the optimal solution  i e   taking only the lower loops  as a function of exploration rate
and scenario fracture    we can see a distinct diagonal trend in the table illustrating that
increasing fracture requires the imitator to increase its levels of exploration in order to find
    for reasons of computational expediency  only the entries near the diagonal have been computed  sampling of other entries confirms the trend 

   

fiimplicit imitation

the optimal policy  this suggests that fracture reflects a feature of rl domains that is may
be important in predicting the efficacy of implicit imitation 
    suboptimality and bias
implicit imitation is fundamentally about biasing the exploration of the observer  as such 
it is worthwhile to ask when this has a positive effect on observer performance  the short
answer is that a mentor following an optimal policy for an observer will cause an observer to
explore in the neighborhood of the optimal policy and this will generally bias the observer
towards finding the optimal policy 
a more detailed answer requires looking explicitly at exploration in reinforcement learning  in theory  an  greedy exploration policy with a suitable rate of decay will cause
implicit imitators to eventually converge to the same optimal solution as their unassisted
counterparts  however  in practice  the exploration rate is typically decayed more quickly
in order to improve early exploitation of mentor input  given practical  but theoretically
unsound exploration rates  an observer may settle for a mentor strategy that is feasible 
but non optimal  we can easily imagine examples  consider a situation in which an agent
is observing a mentor following some policy  early in the learning process  the value of the
policy followed by the mentor may look better than the estimated value of the alternative
policies available to the observer  it could be the case that the mentors policy actually
is the optimal policy  on the other hand  it may be the case that one of the alternative
policies  with which the observer has neither personal experience  nor observations from a
mentor  is actually superior  given the lack of information  an aggressive exploitation policy might lead the observer to falsely conclude that the mentors policy is optimal  while
implicit imitation can bias the agent to a suboptimal policy  we have no reason to expect
that an agent learning in a domain sufficiently challenging to warrant the use of imitation
would have discovered a better alternative  we emphasize that even if the mentors policy
is suboptimal  it still provides a feasible solution which will be preferable to no solution for
many practical problems 
in this regard  we see that the classic exploration exploitation tradeoff has an additional
interpretation in the implicit imitation setting  a component of the exploration rate will
correspond to the observers belief about the sufficiency of the mentors policy  in this
paradigm  then  it seems somewhat misleading to think in terms of a decision about whether
to follow a specific mentor or not  it is more a question of how much exploration to
perform in addition to that required to reconstruct the mentors policy 
    specific applications
we see applications for implicit imitation in a variety of contexts  the emerging electronic
commerce and information infrastructure is driving the development of vast networks of
multi agent systems  in networks used for competitive purposes such as trade  implicit
imitation can be used by an rl agent to learn about buying strategies or information
filtering policies of other agents in order to improve its own behavior 
in control  implicit imitation could be used to transfer knowledge from an existing
learned controller which has already adapted to its clients to a new learning controller with
a completely different architecture  many modern products such as elevator controllers
   

fiprice   boutilier

 crites   barto         cell traffic routers  singh   bertsekas        and automotive fuel
injection systems use adaptive controllers to optimize the performance of a system for
specific user profiles  when upgrading the technology of the underlying system  it is quite
possible that sensors  actuators and the internal representation of the new system will be
incompatible with the old system  implicit imitation provides a method of transferring
valuable user information between systems without any explicit communication 
a traditional application for imitation like technologies lies in the area of bootstrapping
intelligent artifacts using traces of human behavior  research within the behavioral cloning
paradigm has investigated transfer in applications such as piloting aircraft  sammut et al  
      and controlling loading cranes  suc   bratko         other researchers have investigated the use of imitation to simplify the programming of robots  kuniyoshi  inaba   
inoue         the ability of imitation to transfer complex  nonlinear and dynamic behaviors
from existing human agents makes it particularly attractive for control problems 

   extensions
the model of implicit imitation presented above makes certain restrictive assumptions regarding the structure of the decision problem being solved  e g   full observability  knowledge
of reward function  discrete state and action space   while these simplifying assumptions
aided the detailed development of the model  we believe the basic intuitions and much of
the technical development can be extended to richer problem classes  we suggest several
possible extensions in this section  each of which provides a very interesting avenue for
future research 
    unknown reward functions
our current paradigm assumes that the observer knows its own reward function  this
assumption is consistent with the view of rl as a form of automatic programming  we
can  however  relax this constraint assuming some ability to generalize observed rewards 
suppose that the expected reward can be expressed in terms of a probability distribution
over features of the observers state  pr r f  so     in model based rl  this distribution
can be learned by the agent through its own experience  if the same features can be
applied to the mentors state sm   then the observer can use what it has learned about the
reward distribution to estimate expected reward for mentor states as well  this extends
the paradigm to domains in which rewards are unknown  but preserves the ability of the
observer to evaluate mentor experiences on its own terms 
imitation techniques designed around the assumption that the observer and the mentor
share identical rewards  such as utgoffs         would of course work in the absence of a
reward function  the notion of inverse reinforcement learning  ng   russell        could be
adapted to this case as well  a challenge for future research would be to explore a synthesis
between implicit imitation and reward inversion approaches to handle an observers prior
beliefs about some intermediate level of correlation between the reward function of observer
and mentor 
   

fiimplicit imitation

    interaction of agents
while we cast the general imitation model in the framework of stochastic games  the restriction of the model presented thus far to noninteracting games essentially means that the
standard issues associated with multiagent interaction do not arise  there are  of course 
many tasks that require interactions between agents  in such cases  implicit imitation offers
the potential to accelerate learning  a general solution requires the integration of imitation
into more general models for multiagent rl based on stochastic or markov games  littman 
      hu   wellman        bowling   veloso         this would no doubt be a rather
challenging  yet rewarding endeavor 
to take a simple example  in simple coordination problems  e g   two mobile agents
trying to avoid each other while carrying out related tasks  we might imagine an imitator
learning from a mentor by reversing the roles of their roles when considering how the
observed state transition is influenced by their joint action  in this and more general
settings  learning typically requires great care  since agents learning in a nonstationary
environment may not converge  say  to equilibrium   again  imitation techniques offer
certain advantages  for instance  mentor expertise can suggest means of coordinating with
other agents  e g   by providing a focal point for equilibrium selection  or by making clear
a specific convention such as always passing to the right to avoid collision  
other challenges and opportunities present themselves when imitation is used in multiagent settings  for example  in competitive or educational domains  agents not only have to
choose actions that maximize information from exploration and returns from exploitation 
they must also reason about how their actions communicate information to other agents 
in a competitive setting  one agent may wish to disguise its intentions  while in the context
of teaching  a mentor may wish to choose actions whose purpose is abundantly clear  these
considerations must become part of any action selection process 
    partially observable domains
the extension of this model to partially observable domains is critical  since it is unrealistic
in many settings to suppose that a learner can constantly monitor the activities of a mentor 
the central idea of implicit imitation is to extract model information from observations of
the mentor  rather than duplicating mentor behavior  this means that the mentors internal
belief state and policy are not  directly  relevant to the learner  we take a somewhat
behaviorist stance and concern ourselves only with what the mentors observed behaviors
tell us about the possibilities inherent in the environment  the observer does have to keep a
belief state about the mentors current state  but this can be done using the same estimated
world model the observer uses to update its own belief state 
preliminary investigation of such a model suggests that dealing with partial observability
is viable  we have derived update rules for augmented partially observable updates  these
updates are based on a bayesian formulation of implicit imitation which is  in turn  based
on bayesian rl  dearden et al          in fully observable contexts  we have seen that more
effective exploration using mentor observations is possible in fully observable domains when
this bayesian model of imitation is used  price   boutilier         the extension of this
model to cases where the mentors state is partially observable is reasonably straightforward 
we anticipate that updates performed using a belief state about the mentors state and
   

fiprice   boutilier

action will help to alleviate fracture that could be caused by incomplete observation of
behavior 
more interesting is dealing with an additional factor in the usual exploration exploitation
tradeoff  determining whether it is worthwhile to take actions that render the mentor more
visible  e g   ensuring the mentor remains in view so that this source of information remains
available while learning  
    continuous and model free learning
in many realistic domains  continuous attributes and large state and action spaces prohibit
the use of explicit table based representations  reinforcement learning in these domains
is typically modified to make use of function approximators to estimate the q function
at points where no direct evidence has been received  two important approaches are
parameter based models  e g   neural networks   bertsekas   tsitsiklis        and the
memory based approaches  atkeson  moore    schaal         in both these approaches 
model free learning is generally employed  that is  the agent keeps a value function but uses
the environment as an implicit model to perform backups using the sampling distribution
provided by environment observations 
one straightforward approach to casting implicit imitation in a continuous setting would
employ a model free learning paradigm  watkins   dayan         first  recall the augmented bellman backup function used in implicit imitation 
 

 

v  s    ro  s     max max
aao

x

 

pro  s  a  t v  t   

ts

 

x

prm  s  t v  t 

    

ts

when we examine the augmented backup equation  we see that it can be converted to a
model free form in much the same way as the ordinary bellman backup  we use a standard
q function with observer actions  but we will add one additional action which corresponds
to the action am taken by the mentor    now imagine that the observer was in state so  
took action ao and ended up in state s o   at the same time  the mentor made the transition
from state sm to s m   we can then write 


q so   ao          q so   ao      ro  so   ao      max

max

a  ao





 
q s o   a     

q sm   am          q sm   am      ro  sm   am      max max
 

a ao





q s o   am  

 
q s m   a     

    


q s m   am  

as discussed earlier  the relative quality of mentor and observer estimates of the qfunction at specific states may vary  again  in order to avoid having inaccurate prior beliefs
about the mentors action models bias exploration  we need to employ a confidence measure
to decide when to apply these augmented equations  we feel the most natural setting for
these kind of tests is in the memory based approaches to function approximation  memorybased approaches  such as locally weighted regression  atkeson et al          not only provide estimates for functions at points previously unvisited  they also maintain the evidence
    this doesnt imply the observer knows which of its actions corresponds to am  

   

fiimplicit imitation

set used to generate these estimates  we note that the implicit bias of memory based approaches assumes smoothness between points unless additional data proves otherwise  on
the basis of this bias  we propose to compare the average squared distance of the query from
the exemplars used in the estimate of the mentors q value to the average squared distance
from the query to the exemplars used in the observer based estimate to heuristically decide
which agent has the more reliable q value 
the approach suggested here does not benefit from prioritized sweeping  prioritizedsweeping  has however  been adapted to continuous settings  forbes   andre         we
feel a reasonably efficient technique could be made to work 

   related work
research into imitation spans a broad range of dimensions  from ethological studies  to
abstract algebraic formulations  to industrial control algorithms  as these fields have crossfertilized and informed each other  we have come to stronger conceptual definitions and
a better understanding of the limits and capabilities of imitation  many computational
models have been proposed to exploit specialized niches in a variety of control paradigms 
and imitation techniques have been applied to a variety of real world control problems 
the conceptual foundations of imitation have been clarified by work on natural imitation  from work on apes  russon   galdikas         octopi  fiorito   scotto         and
other animals  we know that socially facilitated learning is widespread throughout the animal kingdom  a number of researchers have pointed out  however  that social facilitation
can take many forms  conte        noble   todd         for instance  a mentors attention
to an object can draw an observers attention to it and thereby lead the observer to manipulate the object independently of the model provided by the mentor  true imitation
is therefore typically defined in a more restrictive fashion  visalberghi and fragazy       
cite mitchells definition 
   something c  the copy of the behavior  is produced by an organism
   where c is similar to something else m  the model behavior 
   observation of m is necessary for the production of c  above baseline levels of c
occurring spontaneously 
   c is designed to be similar to m
   the behavior c must be a novel behavior not already organized in that precise way in
the organisms repertoire 
this definition perhaps presupposes a cognitive stance towards imitation in which an
agent explicitly reasons about the behaviors of other agents and how these behaviors relate
to its own action capabilities and goals 
imitation can be further analyzed in terms of the type of correspondence demonstrated
by the mentors behavior and the observers acquired behavior  nehaniv   dautenhahn 
      byrne   russon         correspondence types are distinguished by level  at the
action level  there is a correspondence between actions  at the program level  the actions
   

fiprice   boutilier

may be completely different but correspondence may be found between subgoals  at the
effect level  the agent plans a set of actions that achieve the same effect as the demonstrated
behavior but there is no direct correspondence between subcomponents of the observers
actions and the mentors actions  the term abstract imitation has been proposed in the
case where agents imitate behaviors which come from imitating the mental state of other
agents  demiris   hayes        
the study of specific computational models of imitation has yielded insights into the
nature of the observer mentor relationship and how it affects the acquisition of behaviors by
observers  for instance  in the related field of behavioral cloning  it has been observed that
mentors that implement conservative policies generally yield more reliable clones  urbancic
  bratko         highly trained mentors following an optimal policy with small coverage of
the state space yield less reliable clones than those that make more mistakes  sammut et al  
       for partially observable problems  learning from perfect oracles can be disastrous  as
they may choose policies based on perceptions not available to the observer  the observer is
therefore incorrectly biased away from less risky policies that do not require the additional
perceptual capabilities  scheffer  greiner    darken         finally  it has been observed
that successful clones would often outperform the original mentor due to the cleanup effect
 sammut et al         
one of the original goals of behavioral cloning  michie        was to extract knowledge
from humans to speed up the design of controllers  for the extracted knowledge to be
useful  it has been argued that rule based systems offer the best chance of intelligibility
 van lent   laird         it has become clear  however  that symbolic representations are
not a complete answer  representational capacity is also an issue  humans often organize
control tasks by time  which is typically lacking in state and perception based approaches
to control  humans also naturally break tasks down into independent components and
subgoals  urbancic   bratko         studies have also demonstrated that humans will
give verbal descriptions of their control policies which do not match their actual actions
 urbancic   bratko         the potential for saving time in acquisition has been borne out
by one study which explicitly compared the time to extract rules with the time required to
program a controller  van lent   laird        
in addition to what has traditionally been considered imitation  an agent may also face
the problem of learning to imitate or finding a correspondence between the actions and
states of the observer and mentor  nehaniv   dautenhahn         a fully credible approach
to learning by observation in the absence of communication protocols will have to deal with
this issue 
the theoretical developments in imitation research have been accompanied by a number
of practical implementations  these implementations take advantage of properties of different control paradigms to demonstrate various aspects of imitation  early behavioral cloning
research took advantage of supervised learning techniques such as decision trees  sammut
et al          the decision tree was used to learn how a human operator mapped perceptions to actions  perceptions were encoded as discrete values  a time delay was inserted in
order to synchronize perceptions with the actions they trigger  learning apprentice systems
 mitchell et al         also attempted to extract useful knowledge by watching users  but the
goal of apprentices is not to independently solve problems  learning apprentices are closely
related to programming by demonstration systems  lieberman         later efforts used
   

fiimplicit imitation

more sophisticated techniques to extract actions from visual perceptions and abstract these
actions for future use  kuniyoshi et al          work on associative and recurrent learning models has allowed work in the area to be extended to learning of temporal sequences
 billard   hayes         associative learning has been used together with innate following
behaviors to acquire navigation expertise from other agents  billard   hayes        
a related but slightly different form of imitation has been studied in the multi agent
reinforcement learning community  an early precursor to imitation can be found in work
on sharing of perceptions between agents  tan         closer to imitation is the idea of
replaying the perceptions and actions of one agent for a second agent  lin        whitehead 
    a   here  the transfer is from one agent to another  in contrast to behavioral clonings
transfer from human to agent  the representation is also different  reinforcement learning
provides agents with the ability to reason about the effects of current actions on expected
future utility so agents can integrate their own knowledge with knowledge extracted from
other agents by comparing the relative utility of the actions suggested by each knowledge
source  the seeding approaches are closely related  trajectories recorded from human
subjects are used to initialize a planner which subsequently optimizes the plan in order
to account for differences between the human effector and the robotic effector  atkeson  
schaal         this technique has been extended to handle the notion of subgoals within
a task  atkeson   schaal         subgoals are also addressed by others  suc   bratko 
       our own work is based on the idea of an agent extracting a model from a mentor
and using this model information to place bounds on the value of actions using its own
reward function  agents can therefore learn from mentors with reward functions different
than their own 
another approach in this family is based on the assumption that the mentor is rational
 i e   follows an optimal policy   has the same reward function as the observer and chooses
from the same set of actions  given these assumptions  we can conclude that the action
chosen by a mentor in a particular state must have higher value to the mentor than the
alternatives open to the mentor  utgoff   clouse        and therefore higher value to the
observer than any alternative  the system of utgoff and clouse therefore iteratively adjusts
the values of the actions until this constraint is satisfied in its model  a related approach
uses the methodology of linear quadratic control   suc   bratko         first a model of
the system is constructed  then the inverse control problem is solved to find a cost matrix
that would result in the observed controller behavior given an environment model  recent
work on inverse reinforcement learning takes a related approach to reconstructing reward
functions from observed behavior  ng   russell         it is similar to the inversion of the
quadratic control approach  but is formulated for discrete domains 
several researchers have picked up on the idea of common representations for perceptual functions and action planning  one approach to using the same representation for
perception and control is based on the pid controller model  the pid controller represents
the behavior  its output is compared with observed behaviors in order to select the action
which is closest to the observed behavior  demiris   hayes         explicit motor action
schema have also been investigated in the dual role of perceptual and motor representations
 mataric  williamson  demiris    mohan        
imitation techniques have been applied in a diverse collection of applications  classical control applications include control systems for robot arms  kuniyoshi et al        
   

fiprice   boutilier

friedrich  munch  dillmann  bocionek    sassin         aeration plants  scheffer et al  
       and container loading cranes  suc   bratko        urbancic   bratko         imitation learning has also been applied to acceleration of generic reinforcement learning  lin 
      whitehead      a   less traditional applications include transfer of musical style
 canamero  arcos    de mantaras        and the support of a social atmosphere  billard  hayes    dautenhahn        breazeal        scassellati         imitation has also
been investigated as a route to language acquisition and transmission  billard et al        
oliphant        

   concluding remarks
we have described a formal and principled approach to imitation called implicit imitation 
for stochastic problems in which explicit forms of communication are not possible  the
underlying model based framework combined with model extraction provides an alternative
to other imitation and learning by observation systems  our new approach makes use of a
model to compute the actions an imitator should take without requiring that the observer
duplicate the mentors actions exactly  we have shown implicit imitation to offer significant
transfer capability on several test problems  where it proves to be robust in the face of
noise  capable of integrating subskills from multiple mentors  and able to provide benefits
that increase with the difficulty of the problem 
we have seen that feasibility testing extends implicit imitation in a principled manner
to deal with the situations where the homogeneous action assumption is invalid  adding
bridging capabilities preserves and extends the mentors guidance in the presence of infeasible actions  whether due to differences in action capabilities or local differences in state
spaces  our approach also relates to the idea of following in the sense that the imitator
uses local search in its model to repair discontinuities in its augmented value function before acting in the world  in the process of applying imitation to various domains  we have
learned more about its properties  in particular we have developed the fracture metric to
characterize the effectiveness of a mentor for a given observer in a specific domain  we have
also made considerable progress in extending imitation to new problem classes  the model
we have developed is rather flexible and can be extended in several ways  for example  a
bayesian approach to imitation building on this work shows great potential         and we
have initial formulations of promising approaches to extending implicit imitation to multiagent problems  partially observable domains and domains in which the reward function is
not specified a priori 
a number of challenges remain in the field of imitation  bakker and kuniyoshi       
describe a number of these  among the more intriguing problems unique to imitation are 
the evaluation of the expected payoff for observing a mentor  inferring useful state and
reward mappings between the domains of mentors and those of observers  and repairing
or locally searching in order to fit observed behaviors to an observers own capabilities
and goals  we have also raised the possibility of agents attempting to reason about the
information revealed by their actions in addition to whatever concrete value the actions
have for the agent 
model based reinforcement has been applied to numerous problems  since implicit imitation can be added to model based reinforcement learning with relatively little effort  we
   

fiimplicit imitation

expect that it can be applied to many of the same problems  its basis in the simple but
elegant theory of markov decision processes makes it easy to describe and analyze  though
we have focused on some simple examples designed to illustrate the different mechanisms
required for implicit imitation  we expect that variations on our approach will provide interesting directions for future research 

acknowledgments
thanks to the anonymous referees for their suggestions and comments on earlier versions of
this work and michael littman for editorial suggestions  price was supported by nce irisiii project bac  boutilier was supported by nserc research grant ogp         and
the nce iris iii project bac  some parts of this paper were presented in implicit imitation in reinforcement learning  proceedings of the sixteenth international conference
on machine learning  icml      bled  slovenia  pp               and imitation and
reinforcement learning in agents with heterogeneous actions  proceedings fourteenth
biennial conference of the canadian society for computational studies of intelligence  ai
       ottawa  pp               

references
alissandrakis  a   nehaniv  c  l     dautenhahn  k          learning how to do things with
imitation  in bauer  m     rich  c   eds    aaai fall symposium on learning how to do
things  pp     cape cod  ma 
atkeson  c  g     schaal  s          robot learning from demonstration  in proceedings of the
fourteenth international conference on machine learning  pp       nashville  tn 
atkeson  c  g   moore  a  w     schaal  s          locally weighted learning for control  artificial
intelligence review                  
bakker  p     kuniyoshi  y          robot see  robot do  an overview of robot imitation  in aisb  
workshop on learning in robots and animals  pp      brighton uk 
bellman  r  e          dynamic programming  princeton university press  princeton 
bertsekas  d  p          dynamic programming  deterministic and stochastic models  prentice hall 
englewood cliffs 
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena  belmont  ma 
billard  a     hayes  g          learning to communicate through imitation in autonomous robots 
in proceedings of the seventh international conference on artificial neural networks  pp 
      lausanne  switzerland 
billard  a     hayes  g          drama  a connectionist architecturefor control and learning in
autonomous robots  adaptive behavior journal          
billard  a   hayes  g     dautenhahn  k          imitation skills as a means to enhance learning of a
synthetic proto language in an autonomous robot  in proceedings of the aisb   symposium
on imitation in animals and artifacts  pp       edinburgh 
boutilier  c          sequential optimality and coordination in multiagent systems  in proceedings of
the sixteenth international joint conference on artificial intelligence  pp         stockholm 
   

fiprice   boutilier

boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions
and computational leverage  journal of artificial intelligence research          
bowling  m     veloso  m          rational and convergent learning in stochastic games  in proceedings of the seventeenth international joint conference on artificial intelligence  pp          
seattle 
breazeal  c          imitation as social exchange between humans and robot  in proceedings of the
aisb   symposium on imitation in animals and artifacts  pp        edinburgh 
byrne  r  w     russon  a  e          learning by imitation  a hierarchical approach  behavioral
and brain sciences             
canamero  d   arcos  j  l     de mantaras  r  l          imitating human performances to automatically generate expressive jazz ballads  in proceedings of the aisb   symposium on
imitation in animals and artifacts  pp        edinburgh 
cassandra  a  r   kaelbling  l  p     littman  m  l          acting optimally in partially observable stochastic domains  in proceedings of the twelfth national conference on artificial
intelligence  pp           seattle 
conte  r          intelligent social learning  in proceedings of the aisb   symposium on starting
from society  the applications of social analogies to computational systems birmingham 
crites  r     barto  a  g          elevator group control using multiple reinforcement learning
agents  machine learning                 
dean  t     givan  r          model minimization in markov decision processes  in proceedings of
the fourteenth national conference on artificial intelligence  pp         providence 
dearden  r     boutilier  c          abstraction and approximate decision theoretic planning 
artificial intelligence             
dearden  r   friedman  n     andre  d          model based bayesian exploration  in proceedings
of the fifteenth conference on uncertainty in artificial intelligence  pp         stockholm 
degroot  m  h          probability and statistics  addison wesley  reading  ma 
demiris  j     hayes  g          do robots ape   in proceedings of the aaai fall symposium on
socially intelligent agents  pp       cambridge  ma 
demiris  j     hayes  g          active and passive routes to imitation  in proceedings of the
aisb   symposium on imitation in animals and artifacts  pp       edinburgh 
fiorito  g     scotto  p          observational learning in octopus vulgaris  science             
forbes  j     andre  d          practical reinforcement learning in continuous domains  tech  rep 
ucb csd          computer science division  university of california  berkeley 
friedrich  h   munch  s   dillmann  r   bocionek  s     sassin  m          robot programming by
demonstration  rpd   support the induction by human interaction  machine learning     
       
hartmanis  j     stearns  r  e          algebraic structure theory of sequential machines  prenticehall  englewood cliffs 
hu  j     wellman  m  p          multiagent reinforcement learning  theoretical framework and an
algorithm  in proceedings of the fifthteenth international conference on machine learning 
pp         madison  wi 
kaelbling  l  p          learning in embedded systems  mit press  cambridge ma 
   

fiimplicit imitation

kaelbling  l  p   littman  m  l     moore  a  w          reinforcement learning  a survey  journal
of artificial intelligence research            
kearns  m     singh  s          near optimal reinforcement learning in polynomial time  in proceedings of the fifthteenth international conference on machine learning  pp         madison 
wi 
kuniyoshi  y   inaba  m     inoue  h          learning by watching  extracting reusable task
knowledge from visual observation of human performance  ieee transactions on robotics
and automation                 
lee  d     yannakakis  m          online miminization of transition systems  in proceedings of the
  th annual acm symposium on the theory of computing  stoc      pp         victoria 
bc 
lieberman  h          mondrian  a teachable graphical editor  in cypher  a   ed    watch what
i do  programming by demonstration  pp          mit press  cambridge  ma 
lin  l  j          self improvement based on reinforcement learning  planning and teaching  machine
learning  proceedings of the eighth international workshop  ml              
lin  l  j          self improving reactive agents based on reinforcement learning  planning and
teaching  machine learning            
littman  m  l          markov games as a framework for multi agent reinforcement learning  in
proceedings of the eleventh international conference on machine learning  pp         new
brunswick  nj 
lovejoy  w  s          a survey of algorithmic methods for partially observed markov decision
processes  annals of operations research           
mataric  m  j          using communication to reduce locality in distributed multi agent learning 
journal experimental and theoretical artificial intelligence                 
mataric  m  j   williamson  m   demiris  j     mohan  a          behaviour based primitives for
articulated control  in r  pfiefer  b  blumberg  j  a  m    s  w  w   ed    fifth international
conference on simulation of adaptive behavior sab    pp         zurich  mit press 
meuleau  n     bourgine  p          exploration of multi state environments  local mesures and
back propagation of uncertainty  machine learning                 
mi  j     sampson  a  r          a comparison of the bonferroni and scheffe bounds  journal of
statistical planning and inference             
michie  d          knowledge  learning and machine intelligence  in sterling  l   ed    intelligent
systems  plenum press  new york 
mitchell  t  m   mahadevan  s     steinberg  l          leap  a learning apprentice for vlsi
design  in proceedings of the ninth international joint conference on artificial intelligence 
pp         los altos  california  morgan kaufmann publishers  inc 
moore  a  w     atkeson  c  g          prioritized sweeping  reinforcement learning with less
data and less real time  machine learning                
myerson  r  b          game theory  analysis of conflict  harvard university press  cambridge 
nehaniv  c     dautenhahn  k          mapping between dissimilar bodies  affordances and the
algebraic foundations of imitation  in proceedings of the seventh european workshop on
learning robots  pp       edinburgh 
   

fiprice   boutilier

ng  a  y     russell  s          algorithms for inverse reinforcement learning  in proceedings of the
seventeenth international conference on machine learning  pp         stanford 
noble  j     todd  p  m          is it really imitation  a review of simple mechanisms in social
information gathering  in proceedings of the aisb   symposium on imitation in animals
and artifacts  pp       edinburgh 
oliphant  m          cultural transmission of communications systems  comparing observational
and reinforcement learning models  in proceedings of the aisb   symposium on imitation
in animals and artifacts  pp       edinburgh 
price  b     boutilier  c          a bayesian approach to imitation in reinforcement learning  in proceedings of the eighteenth international joint conference on artificial intelligence acapulco 
to appear 
puterman  m  l          markov decision processes  discrete stochastic dynamic programming 
john wiley and sons  inc   new york 
russon  a     galdikas  b          imitation in free ranging rehabilitant orangutans  pongopygmaeus   journal of comparative psychology                  
sammut  c   hurst  s   kedzier  d     michie  d          learning to fly  in proceedings of the
ninth international conference on machine learning  pp         aberdeen  uk 
scassellati  b          knowing what to imitate and knowing when you succeed  in proceedings of
the aisb   symposium on imitation in animals and artifacts  pp         edinburgh 
scheffer  t   greiner  r     darken  c          why experimentation can be better than perfect
guidance  in proceedings of the fourteenth international conference on machine learning 
pp         nashville 
seber  g  a  f          multivariate observations  wiley  new york 
shapley  l  s          stochastic games  proceedings of the national academy of sciences     
       
singh  s  p     bertsekas  d          reinforcement learning for dynamic channel allocation in
cellular telephone systems  in advances in neural information processing systems  pp     
    cambridge  ma  mit press 
smallwood  r  d     sondik  e  j          the optimal control of partially observable markov
processes over a finite horizon  operations research               
suc  d     bratko  i          skill reconstruction as induction of lq controllers with subgoals 
in proceedings of the fifteenth international joint conference on artificial intelligence  pp 
       nagoya 
sutton  r  s          learning to predict by the method of temporal differences  machine learning 
       
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
cambridge  ma 
tan  m          multi agent reinforcement learning  independent vs  cooperative agents  in icml    pp        
urbancic  t     bratko  i          reconstruction human skill with machine learning  in eleventh
european conference on artificial intelligence  pp         amsterdam 
   

fiimplicit imitation

utgoff  p  e     clouse  j  a          two kinds of training information for evaluation function
learning  in proceedings of the ninth national conference on artificial intelligence  pp     
    anaheim  ca 
van lent  m     laird  j          learning hierarchical performance knowledge by observation 
in proceedings of the sixteenth international conference on machine learning  pp        
bled  slovenia 
visalberghi  e     fragazy  d          do monkeys ape   in parker  s     gibson  k   eds   
language and intelligence in monkeys and apes  pp          cambridge university press 
cambridge 
watkins  c  j  c  h     dayan  p          q learning  machine learning            
whitehead  s  d       a   complexity analysis of cooperative mechanisms in reinforcement learning  in proceedings of the ninth national conference on artificial intelligence  pp        
anaheim 
whitehead  s  d       b   complexity and cooperation in q learning  in machine learning  proceedings of the eighth international workshop  ml     pp         

   

fi