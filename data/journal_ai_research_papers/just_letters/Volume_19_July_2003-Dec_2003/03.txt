journal of artificial intelligence research                  

submitted       published      

efficient solution algorithms for factored mdps
carlos guestrin

guestrin cs stanford edu

computer science dept   stanford university

daphne koller

koller cs stanford edu

computer science dept   stanford university

ronald parr

parr cs duke edu

computer science dept   duke university

shobha venkataraman

shobha cs cmu edu

computer science dept   carnegie mellon university

abstract
this paper addresses the problem of planning under uncertainty in large markov decision
processes  mdps   factored mdps represent a complex state space using state variables and
the transition model using a dynamic bayesian network  this representation often allows an
exponential reduction in the representation size of structured mdps  but the complexity of exact
solution algorithms for such mdps can grow exponentially in the representation size  in this paper 
we present two approximate solution algorithms that exploit structure in factored mdps  both
use an approximate value function represented as a linear combination of basis functions  where
each basis function involves only a small subset of the domain variables  a key contribution of this
paper is that it shows how the basic operations of both algorithms can be performed efficiently
in closed form  by exploiting both additive and context specific structure in a factored mdp  a
central element of our algorithms is a novel linear program decomposition technique  analogous to
variable elimination in bayesian networks  which reduces an exponentially large lp to a provably
equivalent  polynomial sized one  one algorithm uses approximate linear programming  and the
second approximate dynamic programming  our dynamic programming algorithm is novel in that
it uses an approximation based on max norm  a technique that more directly minimizes the terms
that appear in error bounds for approximate mdp algorithms  we provide experimental results
on problems with over      states  demonstrating a promising indication of the scalability of our
approach  and compare our algorithm to an existing state of the art approach  showing  in some
problems  exponential gains in computation time 

   introduction
over the last few years  markov decision processes  mdps  have been used as the basic
semantics for optimal planning for decision theoretic agents in stochastic environments  in
the mdp framework  the system is modeled via a set of states which evolve stochastically 
the main problem with this representation is that  in virtually any real life domain  the
state space is quite large  however  many large mdps have significant internal structure 
and can be modeled compactly if the structure is exploited in the representation 
factored mdps  boutilier  dearden    goldszmidt        are one approach to representing large  structured mdps compactly  in this framework  a state is implicitly described
by an assignment to some set of state variables  a dynamic bayesian network  dbn   dean
  kanazawa        can then allow a compact representation of the transition model  by
exploiting the fact that the transition of a variable often depends only on a small number
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

figuestrin  koller  parr   venkataraman

of other variables  furthermore  the momentary rewards can often also be decomposed as
a sum of rewards related to individual variables or small clusters of variables 
there are two main types of structure that can simultaneously be exploited in factored
mdps  additive and context specific structure  additive structure captures the fact that
typical large scale systems can often be decomposed into a combination of locally interacting components  for example  consider the management of a large factory with many
production cells  of course  in the long run  if a cell positioned early in the production line
generates faulty parts  then the whole factory may be affected  however  the quality of the
parts a cell generates depends directly only on the state of this cell and the quality of the
parts it receives from neighboring cells  such additive structure can also be present in the
reward function  for example  the cost of running the factory depends  among other things 
on the sum of the costs of maintaining each local cell 
context specific structure encodes a different type of locality of influence  although a
part of a large system may  in general  be influenced by the state of every other part of this
system  at any given point in time only a small number of parts may influence it directly 
in our factory example  a cell responsible for anodization may receive parts directly from
any other cell in the factory  however  a work order for a cylindrical part may restrict this
dependency only to cells that have a lathe  thus  in the context of producing cylindrical
parts  the quality of the anodized parts depends directly only on the state of cells with a
lathe 
even when a large mdp can be represented compactly  for example  by using a factored
representation  solving it exactly may still be intractable  typical exact mdp solution algorithms require the manipulation of a value function  whose representation is linear in the
number of states  which is exponential in the number of state variables  one approach is
to approximate the solution using an approximate value function with a compact representation  a common choice is the use of linear value functions as an approximation  value
functions that are a linear combination of potentially non linear basis functions  bellman 
kalaba    kotkin        sutton        tsitsiklis   van roy      b   our work builds on
the ideas of koller and parr               by using factored  linear  value functions  where
each basis function is restricted to some small subset of the domain variables 
this paper presents two new algorithms for computing linear value function approximations for factored mdps  one that uses approximate dynamic programming and another
that uses approximate linear programming  both algorithms are based on the use of factored linear value functions  a highly expressive function approximation method  this
representation allows the algorithms to take advantage of both additive and context specific
structure  in order to produce high quality approximate solutions very efficiently  the capability to exploit both types of structure distinguishes these algorithms differ from earlier
approaches  boutilier et al          which only exploit context specific structure  we provide
a more detailed discussion of the differences in section    
we show that  for a factored mdp and factored value functions  various critical operations for our planning algorithms can be implemented in closed form without necessarily
enumerating the entire state space  in particular  both our new algorithms build upon a
novel linear programming decomposition technique  this technique reduces structured lps
with exponentially many constraints to equivalent  polynomially sized ones  this decomposition follows a procedure analogous to variable elimination that applies both to additively
   

fiefficient solution algorithms for factored mdps

structured value functions  bertele   brioschi        and to value functions that also exploit context specific structure  zhang   poole         using these basic operations  our
planning algorithms can be implemented efficiently  even though the size of the state space
grows exponentially in the number of variables 
our first method is based on the approximate linear programming algorithm  schweitzer
  seidmann         this algorithm generates a linear  approximate value function by
solving a single linear program  unfortunately  the number of constraints in the lp proposed
by schweitzer and seidmann grows exponentially in the number of variables  using our lp
decomposition technique  we exploit structure in factored mdps to represent exactly the
same optimization problem with exponentially fewer constraints 
in terms of approximate dynamic programming  this paper makes a twofold contribution 
first  we provide a new approach for approximately solving mdps using a linear value
function  previous approaches to linear function approximation typically have utilized a
least squares  l   norm  approximation to the value function  least squares approximations
are incompatible with most convergence analyses for mdps  which are based on max norm 
we provide the first mdp solution algorithms  both value iteration and policy iteration 
that use a linear max norm projection to approximate the value function  thereby directly
optimizing the quantity that appears in our provided error bounds  second  we show how
to exploit the structure of the problem to apply this technique to factored mdps  by again
leveraging on our lp decomposition technique 
although approximate dynamic programming currently possesses stronger theoretical
guarantees  our experimental results suggest that approximate linear programming is a
good alternative  whereas the former tends to generate better policies for the same set of
basis functions  due to the simplicity and computational advantages of approximate linear
programming  we can add more basis functions  obtaining a better policy and still requiring
less computation than the approximate dynamic programming approach 
finally  we present experimental results comparing our approach to the work of boutilier
et al          illustrating some of the tradeoffs between the two methods  in particular  for
problems with significant context specific structure in the value function  their approach
can be faster due to their efficient handling of their value function representation  however 
there are cases with significant context specific structure in the problem  rather than in
the value function  in which their algorithm requires an exponentially large value function
representation  in such classes of problems  we demonstrate that by using a value function that exploits both additive and context specific structure  our algorithm can obtain a
polynomial time near optimal approximation of the true value function 
this paper starts with a presentation of factored mdps and approximate solution algorithms for mdps  in section    we describe the basic operations used in our algorithms 
including our lp decomposition technique  in section    we present the first of our two
algorithms  the approximate linear programming algorithm for factored mdps  the second
algorithm  approximate policy iteration with max norm projection  is presented in section   
section   describes an approach for efficiently computing bounds on policy quality based on
the bellman error  section   shows how to extend our methods to deal with context specific
structure  our paper concludes with an empirical evaluation in section   and a discussion
of related work in section    
   

figuestrin  koller  parr   venkataraman

this paper is a greatly expanded version of work that was published before in guestrin
et al       a   and some of the work presented in guestrin et al       b        

   factored markov decision processes
a markov decision process  mdp  is a mathematical framework for sequential decision
problems in stochastic domains  it thus provides an underlying semantics for the task of
planning under uncertainty  we begin with a concise overview of the mdp framework  and
then describe the representation of factored mdps 
    markov decision processes
we briefly review the mdp framework  referring the reader to the books by bertsekas and
tsitsiklis        or puterman        for a more in depth review  a markov decision process
 mdp  is defined as a   tuple  x  a  r  p   where  x is a finite set of  x    n states  a is
a finite set of actions  r is a reward function r   x  a   r  such that r x  a  represents
the reward obtained by the agent in state x after taking action a  and p is a markovian
transition model where p  x    x  a  represents the probability of going from state x to state
x  with action a  we assume that the rewards are bounded  that is  there exists rmax such
that rmax   r x  a     x  a 
example     consider the problem of optimizing the behavior of a system administrator
 sysadmin  maintaining a network of m computers  in this network  each machine is
connected to some subset of the other machines  various possible network topologies can be
defined in this manner  see figure   for some examples   in one simple network  we might
connect the machines in a ring  with machine i connected to machines i     and i      in
this example  we assume addition and subtraction are performed modulo m  
each machine is associated with a binary random variable xi   representing whether it
is working or has failed  at every time step  the sysadmin receives a certain amount of
money  reward  for each working machine  the job of the sysadmin is to decide which
machine to reboot  thus  there are m     possible actions at each time step  reboot one of the
m machines or do nothing  only one machine can be rebooted per time step   if a machine
is rebooted  it will be working with high probability at the next time step  every machine
has a small probability of failing at each time step  however  if a neighboring machine fails 
this probability increases dramatically  these failure probabilities define the transition model
p  x    x  a   where x is a particular assignment describing which machines are working or
have failed in the current time step  a is the sysadmins choice of machine to reboot and x 
is the resulting state in the next time step 
we assume that the mdp has an infinite horizon and that future rewards are discounted
exponentially with a discount factor           a stationary policy  for an mdp is a
mapping    x   a  where  x  is the action the agent takes at state x  in the computer
network problem  for each possible configuration of working and failing machines  the policy
would tell the sysadmin which machine to reboot  each policy is associated with a value
function v  rn   where v  x  is the discounted cumulative value that the agent gets if
it starts at state x and follows policy   more precisely  the value v of a state x under
   

fiefficient solution algorithms for factored mdps

server

server

star

bidirectional ring

ring and star

server

  legs

ring of rings

figure    network topologies tested  the status of a machine is influence by the status of
its parent in the network 

policy  is given by 
v  x    e

  
x

t



 t 

 t 

 r x    x

t  


 

    
  x   x  


where x t  is a random variable representing the state of the system after t steps  in our
running example  the value function represents how much money the sysadmin expects to
collect if she starts acting according to  when the network is at state x  the value function
for a fixed policy is the fixed point of a set of linear equations that define the value of a
state in terms of the value of its possible successor states  more formally  we define 
definition     the dp operator  t   for a stationary policy  is 
t v x    r x   x     

x

p  x    x   x  v x    

x 

the value function of policy   v   is the fixed point of the t operator  v   t v  
the optimal value function v  describes the optimal value the agent can achieve for
each starting state  v  is also defined by a set of non linear equations  in this case  the
value of a state must be the maximal expected value achievable by any policy starting at
that state  more precisely  we define 
definition     the bellman operator  t    is 
t  v x    max r x  a    
a

x

p  x    x  a v x     

x 

the optimal value function v  is the fixed point of t    v    t  v   
for any value function v  we can define the policy obtained by acting greedily relative
to v  in other words  at each state  the agent takes the action that maximizes the one step
   

figuestrin  koller  parr   venkataraman

utility  assuming that v represents our long term utility achieved at the next state  more
precisely  we define 
greedy v  x    arg max r x  a    
a

x

p  x    x  a v x     

   

x 

the greedy policy relative to the optimal value function v  is the optimal policy    
greedy v    
    factored mdps
factored mdps are a representation language that allows us to exploit problem structure
to represent exponentially large mdps very compactly  the idea of representing a large
mdp using a factored model was first proposed by boutilier et al         
in a factored mdp  the set of states is described via a set of random variables x  
 x            xn    where each xi takes on values in some finite domain dom xi    a state x
defines a value xi  dom xi   for each variable xi   in general  we use upper case letters
 e g   x  to denote random variables  and lower case  e g   x  to denote their values  we
use boldface to denote vectors of variables  e g   x  or their values  x   for an instantiation
y  dom y  and a subset of these variables z  y  we use y z  to denote the value of the
variables z in the instantiation y 
in a factored mdp  we define a state transition model  using a dynamic bayesian
network  dbn   dean   kanazawa         let xi denote the variable xi at the current
time and xi    the same variable at the next step  the transition graph of a dbn is a
two layer directed acyclic graph g whose nodes are  x            xn   x             xn     we denote
the parents of xi  in the graph by parents  xi     for simplicity of exposition  we assume
that parents  xi     x  thus  all arcs in the dbn are between variables in consecutive
time slices   this assumption is used for expository purposes only  intra time slice arcs
are handled by a small modification presented in section       each node xi  is associated
with a conditional probability distribution  cpd  p  xi    parents  xi      the transition
probability p  x    x  is then defined to be 
p  x    x   

y

p  x i   ui    

i

where ui is the value in x of the variables in parents  xi    
example     consider an instance of the sysadmin problem with four computers  labelled
m            m    in an unidirectional ring topology as shown in figure   a   our first task in
modeling this problem as a factored mdp is to define the state space x  each machine
is associated with a binary random variable xi   representing whether it is working or has
failed  thus  our state space is represented by four random variables   x    x    x    x    
the next task is to define the transition model  represented as a dbn  the parents of the
next time step variables xi  depend on the network topology  specifically  the probability that
machine i will fail at the next time step depends on whether it is working at the current
time step and on the status of its direct neighbors  parents in the topology  in the network
at the current time step  as shown in figure   b   the parents of xi  in this example are xi
and xi    the cpd of xi  is such that if xi   false  then xi    false with high probability 
   

fiefficient solution algorithms for factored mdps

x 

x 
r

x 

m 

r

m 

m 

m 

 a 

 

x 
r

x 
r

 

 

p  xi    t   xi   xi    a  
h
 

h

 

x 

x 
h

 

x 
h

 

 b 

action is reboot 
machine i other machine

 

xi 
xi
xi 
xi
xi 
xi
xi 
xi

 f
 f
 f
 t
 t
 f
 t
 t






 

      

 

     

 

      

 

    

 c 

figure    factored mdp example  from a network topology  a  we obtain the factored
mdp representation  b  with the cpds described in  c  

that is  failures tend to persist  if xi   true  then xi  is a noisy or of its other parents  in
the unidirectional ring topology xi  has only one other parent xi     that is  a failure in any
of its neighbors can independently cause machine i to fail 
we have described how to represent factored the markovian transition dynamics arising
from an mdp as a dbn  but we have not directly addressed the representation of actions 
generally  we can define the transition dynamics of an mdp by defining a separate dbn
model a   hga   pa i for each action a 
example     in our system administrator example  we have an action ai for rebooting
each one of the machines  and a default action d for doing nothing  the transition model
described above corresponds to the do nothing action  the transition model for ai is
different from d only in the transition model for the variable xi    which is now xi    true
with probability one  regardless of the status of the neighboring machines  figure   c  shows
the actual cpd for p  xi    w orking   xi   xi    a   with one entry for each assignment to
the state variables xi and xi    and to the action a 
to fully specify an mdp  we also need to provide a compact representation of the reward
function  we assume that the reward function is factored additively into a set of localized
reward functions  each of which only depends on a small set of variables  in our example  we
might have a reward function associated with each machine i  which depends on xi   that
is  the sysadmin is paid on a per machine basis  at every time step  she receives money for
machine i only if it is working  we can formalize this concept of localized functions 
definition     a function f has a scope scope f     c  x if f   dom c    r 
if f has scope y and y  z  we use f  z  as shorthand for f  y  where y is the part of the
instantiation z that corresponds to variables in y 
   

figuestrin  koller  parr   venkataraman

we can now characterize the concept of local rewards  let r a           rra be a set of
functions  where the scope of each ria is restricted to variable cluster uai   x            xn   
p
the reward for taking action a at state x is defined to be ra  x    ri   ria  uai    r  in
our example  we have a reward function ri associated with each machine i  which depends
only xi   and does not depend on the action choice  these local rewards are represented
by the diamonds in figure   b   in the usual notation for influence diagrams  howard  
matheson        

   approximate solution algorithms
there are several algorithms to compute the optimal policy in an mdp  the three most
commonly used are value iteration  policy iteration  and linear programming  a key component in all three algorithms is the computation of value functions  as defined in section     
recall that a value function defines a value for each state x in the state space  with an
explicit representation of the value function as a vector of values for the different states 
the solution algorithms all can be implemented as a series of simple algebraic steps  thus 
in this case  all three can be implemented very efficiently 
unfortunately  in the case of factored mdps  the state space is exponential in the number
of variables in the domain  in the sysadmin problem  for example  the state x of the system
is an assignment describing which machines are working or have failed  that is  a state x
is an assignment to each random variable xi   thus  the number of states is exponential in
the number m of machines in the network   x    n    m    hence  even representing an
explicit value function in problems with more than about ten machines is infeasible  one
might be tempted to believe that factored transition dynamics and rewards would result in
a factored value function  which can thereby be represented compactly  unfortunately  even
in trivial factored mdps  there is no guarantee that structure in the model is preserved in
the value function  koller   parr        
in this section  we discuss the use of an approximate value function  that admits a
compact representation  we also describe approximate versions of these exact algorithms 
that use approximate value functions  our description in this section is somewhat abstract 
and does not specify how the basic operations required by the algorithms can be performed
explicitly  in later sections  we elaborate on these issues  and describe the algorithms in
detail  for brevity  we choose to focus on policy iteration and linear programming  our
techniques easily extend to value iteration 
    linear value functions
a very popular choice for approximating value functions is by using linear regression  as first
proposed by bellman et al          here  we define our space of allowable value functions
v  h  rn via a set of basis functions 
definition     a linear value function over a set of basis functions h    h            hk  
p
is a function v that can be written as v x    kj   wj hj  x  for some coefficients w  
 w            wk     
we can now define h to be the linear subspace of rn spanned by the basis functions h 
it is useful to define an n  k matrix h whose columns are the k basis functions viewed as
   

fiefficient solution algorithms for factored mdps

vectors  in a more compact notation  our approximate value function is then represented
by hw 
the expressive power of this linear representation is equivalent  for example  to that
of a single layer neural network with features corresponding to the basis functions defining
h  once the features are defined  we must optimize the coefficients w in order to obtain a
good approximation for the true value function  we can view this approach as separating
the problem of defining a reasonable space of features and the induced space h  from the
problem of searching within the space  the former problem is typically the purview of
domain experts  while the latter is the focus of analysis and algorithmic design  clearly 
feature selection is an important issue for essentially all areas of learning and approximation 
we offer some simple methods for selecting good features for mdps in section     but it is
not our goal to address this large and important topic in this paper 
once we have a chosen a linear value function representation and a set of basis functions 
the problem becomes one of finding values for the weights w such that hw will yield
a good approximation of the true value function  in this paper  we consider two such
approaches  approximate dynamic programming using policy iteration and approximate
linear programming  in this section  we present these two approaches  in section    we
show how we can exploit problem structure to transform these approaches into practical
algorithms that can deal with exponentially large state spaces 
    policy iteration
      the exact algorithm
the exact policy iteration algorithm iterates over policies  producing an improved policy at
each iteration  starting with some initial policy        each iteration consists of two phases 
value determination computes  for a policy   t    the value function v t    by finding the
fixed point of the equation t t  v t    v t    that is  the unique solution to the set of linear
equations 
x
p  x    x    t   x  v t   x     x 
v t   x    r x    t   x     
x 

the policy improvement step defines the next policy as
  t      greedy v t    
it can be shown that this process converges to the optimal policy  bertsekas   tsitsiklis 
       furthermore  in practice  the convergence to the optimal policy is often very quick 
      approximate policy iteration
the steps in the policy iteration algorithm require a manipulation of both value functions
and policies  both of which often cannot be represented explicitly in large mdps  to define
a version of the policy iteration algorithm that uses approximate value functions  we use
the following basic idea  we restrict the algorithm to using only value functions within the
provided h  whenever the algorithm takes a step that results in a value function v that is
outside this space  we project the result back into the space by finding the value function
within the space which is closest to v  more precisely 
   

figuestrin  koller  parr   venkataraman

definition     a projection operator  is a mapping    rn  h   is said to be a
projection w r t  a norm kk if v   hw such that w  arg minw khw  vk 
that is  v is the linear combination of the basis functions  that is closest to v with respect
to the chosen norm 
our approximate policy iteration algorithm performs the policy improvement step exactly  in the value determination step  the value function  the value of acting according to
the current policy   t   is approximated through a linear combination of basis functions 
we now consider the problem of value determination for a policy   t    at this point 
it is useful to introduce some notation  although the rewards are a function of the state
and action choice  once the policy is fixed  the rewards become a function of the state
only  which we denote as r t    where r t   x    r x    t   x    similarly  for the transition
model  p t   x    x    p  x    x    t   x    we can now rewrite the value determination step
in terms of matrices and vectors  if we view v t  and r t  as n  vectors  and p t  as an
n  n matrix  we have the equations 
v t    r t    p t  v t   
this is a system of linear equations with one equation for each state  which can only be
solved exactly for relatively small n   our goal is to provide an approximate solution  within
h  more precisely  we want to find 
w t    arg min khw   r t    p t  hw k  
w







  arg min  h  p t  h  w t   r t    
w

thus  our approximate policy iteration alternates between two steps 
w t    arg min khw   r t    p t  hw k  
w

  t      greedy hw t    

   
   

      max norm projection
an approach along the lines described above has been used in various papers  with several
recent theoretical and algorithmic results  schweitzer   seidmann        tsitsiklis   van
roy      b  van roy        koller   parr               however  these approaches suffer
from a problem that we might call norm incompatibility  when computing the projection 
they utilize the standard euclidean projection operator with respect to the l  norm or a
weighted l  norm   on the other hand  most of the convergence and error analyses for mdp
algorithms utilize max norm  l    this incompatibility has made it difficult to provide
error guarantees 
we can tie the projection operator more closely to the error bounds through the use
of a projection operator in l norm  the problem of minimizing the l norm has been
studied in the optimization literature as the problem of finding the chebyshev solution  to
   weighted l  norm projections are stable and have meaningful error bounds when the weights correspond
to the stationary distribution of a fixed policy under evaluation  value determination   van roy        
but they are not stable when combined with t    averagers  gordon        are stable and non expansive
in l   but require that the mixture weights be determined a priori  thus  they do not  in general 
minimize l error 
   the chebyshev norm is also referred to as max  supremum and l norms and the minimax solution 

   

fiefficient solution algorithms for factored mdps

an overdetermined linear system of equations  cheney         the problem is defined as
finding w such that 
w  arg min kcw  bk  
   
w

we use an algorithm due to stiefel         that solves this problem by linear programming 
variables  w            wk     
minimize    
p
   
subject to    kj   cij wj  bi and
pk
  bi  j   cij wj   i       n 



p


the constraints in this linear program imply that    kj   cij wj  bi  for each i  or
equivalently  that   kcw  bk   the objective of the lp is to minimize   thus  at the
solution  w      of this linear program  w is the solution of equation     and  is the l
projection error 
we can use the l projection in the context of the approximate policy iteration in the
obvious way  when implementing the projection operation of equation      we can use
the l projection  as in equation       where c    h  p t  h  and b   r t    this
minimization can be solved using the linear program of     
a key point is that this lp only has k     variables  however  there are  n constraints 
which makes it impractical for large state spaces  in the sysadmin problem  for example 
the number of constraints in this lp is exponential in the number of machines in the network
 a total of     m constraints for m machines   in section    we show that  in factored mdps
with linear value functions  all the  n constraints can be represented efficiently  leading to
a tractable algorithm 
      error analysis
we motivated our use of the max norm projection within the approximate policy iteration
algorithm via its compatibility with standard error analysis techniques for mdp algorithms 
we now provide a careful analysis of the impact of the l error introduced by the projection step  the analysis provides motivation for the use of a projection step that directly
minimizes this quantity  we acknowledge  however  that the main impact of this analysis
is motivational  in practice  we cannot provide a priori guarantees that an l projection
will outperform other methods 
our goal is to analyze approximate policy iteration in terms of the amount of error
introduced at each step by the projection operation  if the error is zero  then we are
performing exact value determination  and no error should accrue  if the error is small  we
should get an approximation that is accurate  this result follows from the analysis below 
more precisely  we define the projection error as the error resulting from the approximate
value determination step 








  t    hw t   r t    p t  hw t    


note that  by using our max norm projection  we are finding the set of weights w t  that
exactly minimizes the one step projection error   t    that is  we are choosing the best
   

figuestrin  koller  parr   venkataraman

possible weights with respect to this error measure  furthermore  this is exactly the error
measure that is going to appear in the bounds of our theorem  thus  we can now make the
bounds for each step as tight as possible 
we first show that the projection error accrued in each step is bounded 
lemma     the value determination error is bounded  there exists a constant p  rmax
such that p    t  for all iterations t of the algorithm 
proof  see appendix a   
due to the contraction property of the bellman operator  the overall accumulated error
is a decaying average of the projection error incurred throughout all iterations 
definition     the discounted value determination error at iteration t is defined as  
 t  
   
  t    
       

 t 

 

lemma     implies that the accumulated error remains bounded in approximate policy
 t 
   t  
iteration    p  
  we can now bound the loss incurred when acting according
to the policy generated by our approximate policy iteration algorithm  as opposed to the
optimal policy 
theorem     in the approximate policy iteration algorithm  let   t  be the policy generated
at iteration t  furthermore  let v t  be the actual value of acting according to this policy 
the loss incurred by using policy   t  as opposed to the optimal policy   with value v  is
bounded by 
 t 
 
t


kv  v t  k   kv  v    k  
 
   
      
proof  see appendix a   
in words  equation     shows that the difference between our approximation at iteration
t and the optimal value function is bounded by the sum of two terms  the first term is
present in standard policy iteration and goes to zero exponentially fast  the second is the
discounted accumulated projection error and  as lemma     shows  is bounded  this second
term can be minimized by choosing w t  as the one that minimizes 





hw t   r t    p t  hw t  



 

which is exactly the computation performed by the max norm projection  therefore  this
theorem motivates the use of max norm projections to minimize the error term that appears
in our bound 
the bounds we have provided so far may seem fairly trivial  as we have not provided
a strong a priori bound on   t    fortunately  several factors make these bounds interesting despite the lack of a priori guarantees  if approximate policy iteration converges  as
b is the policy
occurred in all of our experiments  we can obtain a much tighter bound  if 
after convergence  then 
 


v  v    b
 
b
 
     
where b is the one step max norm projection error associated with estimating the value
b   since the max norm projection operation provides b
of 
   we can easily obtain an a
   

fiefficient solution algorithms for factored mdps

posteriori bound as part of the policy iteration procedure  more details are provided in
section   
one could rewrite the bound in theorem     in terms of the worst case projection error p   or the worst projection error in a cycle of policies  if approximate policy iteration
gets stuck in a cycle  these formulations would be closer to the analysis of bertsekas and
tsitsiklis        proposition      p       however  consider the case where most policies
 or most policies in the final cycle  have a low projection error  but there are a few policies
that cannot be approximated well using the projection operation  so that they have a large
one step projection error  a worst case bound would be very loose  because it would be
dictated by the error of the most difficult policy to approximate  on the other hand  using
our discounted accumulated error formulation  errors introduced by policies that are hard
to approximate decay very rapidly  thus  the error bound represents an average case
analysis  a decaying average of the projection errors for policies encountered at the successive iterations of the algorithm  as in the convergent case  this bound can be computed
easily as part of the policy iteration procedure when max norm projection is used 
the practical benefit of a posteriori bounds is that they can give meaningful feedback on
the impact of the choice of the value function approximation architecture  while we are not
explicitly addressing the difficult and general problem of feature selection in this paper  our
error bounds motivate algorithms that aim to minimize the error given an approximation
architecture and provide feedback that could be useful in future efforts to automatically
discover or improve approximation architectures 
    approximate linear programming
      the exact algorithm
linear programming provides an alternative method for solving mdps  it formulates the
problem of finding a value function as a linear program  lp   here the lp variables are
v            vn   where vi represents v xi    the value of starting at the ith state of the system 
the lp is given by 
variables  v            vn  
p
minimize 
xi  xi   vi  
p
subject to  vi   r xi   a     j p  xj   xi   a vj   xi  x  a  a 

   

where the state relevance weights  are positive  note that  in this exact case  the solution
obtained is the same for any positive weight vector  it is interesting to note that steps of
the simplex algorithm correspond to policy changes at single states  while steps of policy
iteration can involve policy changes at multiple states  in practice  policy iteration tends
to be faster than the linear programming approach  puterman        
      approximate linear program
the approximate formulation for the lp approach  first proposed by schweitzer and seidmann         restricts the space of allowable value functions to the linear space spanned
by our basis functions  in this approximate formulation  the variables are w            wk   the
weights for our basis functions  the lp is given by 
   

figuestrin  koller  parr   venkataraman

variables  w            wk  
p
p
minimize 
 x  i wi hi  x   
px
p
p
 
 
subject to 
i wi hi  x    r x  a    
x  p  x   x  a 
i wi hi  x    x  x  a  a 
   
in other words  this formulation takes the lp in     and substitutes the explicit state
p
value function by a linear value function representation i wi hi  x   or  in our more compact
notation  v is replaced by hw  this linear program is guaranteed to be feasible if a constant
function  a function with the same constant value for all states  is included in the set
of basis functions 
in this approximate linear programming formulation  the choice of state relevance weights 
  becomes important  intuitively  not all constraints in this lp are binding  that is  the
constraints are tighter for some states than for others  for each state x  the relevance
weight  x  indicates the relative importance of a tight constraint  therefore  unlike the
exact case  the solution obtained may differ for different choices of the positive weight vector
  furthermore  there is  in general  no guarantee as to the quality of the greedy policy
generated from the approximation hw  however  the recent work of de farias and van
roy      a  provides some analysis of the error relative to that of the best possible approximation in the subspace  and some guidance as to selecting  so as to improve the quality
of the approximation  in particular  their analysis shows that this lp provides the best
approximation hw of the optimal value function v  in a weighted l  sense subject to the
constraint that hw  t  hw   where the weights in the l  norm are the state relevance
weights  
the transformation from an exact to an approximate problem formulation has the effect of reducing the number of free variables in the lp to k  one for each basis function
coefficient   but the number of constraints remains n   a   in our sysadmin problem  for
example  the number of constraints in the lp in     is  m        m   where m is the number
of machines in the network  thus  the process of generating the constraints and solving the
lp still seems unmanageable for more than a few machines  in the next section  we discuss
how we can use the structure of a factored mdp to provide for a compact representation
and an efficient solution to this lp 

   factored value functions
the linear value function approach  and the algorithms described in section    apply to any
choice of basis functions  in the context of factored mdps  koller and parr        suggest
a particular type of basis function  that is particularly compatible with the structure of a
factored mdp  they suggest that  although the value function is typically not structured 
there are many cases where it might be close to structured  that is  it might be wellapproximated using a linear combination of functions each of which refers only to a small
number of variables  more precisely  we define 
definition     a factored  linear  value function is a linear function over the basis set
h            hk   where the scope of each hi is restricted to some subset of variables ci  
value functions of this type have a long history in the area of multi attribute utility theory  keeney   raiffa         in our example  we might have a basis function hi for each
   

fiefficient solution algorithms for factored mdps

machine  indicating whether it is working or not  each basis function has scope restricted
to xi   these are represented as diamonds in the next time step in figure   b  
factored value functions provide the key to performing efficient computations over the
exponential sized state spaces we have in factored mdps  the main insight is that restricted scope functions  including our basis functions  allow for certain basic operations to
be implemented very efficiently  in the remainder of this section  we show how structure in
factored mdps can be exploited to perform two crucial operations very efficiently  one step
lookahead  backprojection   and the representation of exponentially many constraints in
the lps  then  we use these basic building blocks to formulate very efficient approximation algorithms for factored mdps  each presented in its own self contained section  the
approximate linear programming for factored mdps in section    and approximate policy
iteration with max norm projection in section   
    one step lookahead
a key step in all of our algorithms is the computation of the one step lookahead value of
some action a  this is necessary  for example  when computing the greedy policy as in
equation      lets consider the computation of a q function  qa  x   which represents the
expected value the agent obtains after taking action a at the current time step and receiving
a long term value v thereafter  this q function can be computed by 
qa  x    r x  a    

x

p  x    x  a v x  

   

x 

that is  qa  x  is given by the current reward plus the discounted expected future value 
using this notation  we can express the greedy policy as  greedy v  x    maxa qa  x  
recall that we are estimating the long term value of our policy using a set of basis
p
functions  v x    i wi hi  x   thus  we can rewrite equation     as 
qa  x    r x  a    

x

p  x    x  a 

x

x 

wi hi  x  

    

i

p

the size of the state space is exponential  so that computing the expectation x  p  x   
p
x  a  i wi hi  x  seems infeasible  fortunately  as discussed by koller and parr        
this expectation operation  or backprojection  can be performed efficiently if the transition
model and the value function are both factored appropriately  the linearity of the value
function permits a linear decomposition  where each summand in the expectation can be
viewed as an independent value function and updated in a manner similar to the value
iteration procedure used by boutilier et al          we now recap the construction briefly 
by first defining 
ga  x   

x
x 

p  x    x  a 

x

wi hi  x     

i

x
i

wi

x

p  x    x  a hi  x    

x 

thus  we can compute the expectation of each basis function separately 
gia  x   

x

p  x    x  a hi  x    

x 

   

figuestrin  koller  parr   venkataraman

p

and then weight them by wi to obtain the total expectation ga  x    i wi gia  x   the
intermediate function gia is called the backprojection of the basis function hi through the
transition model pa   which we denote by gia   pa hi   note that  in factored mdps  the
transition model pa is factored  represented as a dbn  and the basis functions hi have
scope restricted to a small set of variables  these two important properties allow us to
compute the backprojections very efficiently 
we now show how some restricted scope function h  such as our basis functions 
can be backprojected through some transition model p represented as a dbn    here
h has scope restricted to y  our goal is to compute g   p h  we define the backprojected scope of y through  as the set of parents of y  in the transition graph g  
  y      yi  y  parents  yi     if intra time slice arcs are included  so that parents  xi    
 x            xn   x             xn     then the only change in our algorithm is in the definition of backprojected scope of y through    the definition now includes not only direct parents of y    
but also all variables in  x            xn   that are ancestors of y    
  y       xj   there exist a directed path from xj to any xi   y    
thus  the backprojected scope may become larger  but the functions are still factored 
we can now show that  if h has scope restricted to y  then its backprojection g has
scope restricted to the parents of y    i e     y     furthermore  each backprojection can
be computed by only enumerating settings of variables in   y     rather than settings of
all variables x 
g x     p h  x  
 

x

p  x    x h x    

x 

 

x

p  x    x h y    

x 

 

x
y 

 

x

x

p  y    x h y   

p  u    x  

u   x  y   

p  y    z h y    

y 

  g z  
p

where z is the value of   y    in x and the term u   x  y    p  u    x      as it is the
sum of a probability distribution over a complete domain  therefore  we see that  p h  is a
function whose scope is restricted to   y     note that the cost of the computation depends
linearly on  dom   y       which depends on y  the scope of h  and on the complexity of
the process dynamics  this backprojection procedure is summarized in figure   
returning to our example  consider a basis function hi that is an indicator of variable xi  
it takes value   if the ith machine is working and   otherwise  each hi has scope restricted to
xi    thus  its backprojection gi has scope restricted to parents  xi       xi       xi    xi   
    representing exponentially many constraints
as seen in section    both our approximation algorithms require the solution of linear programs  the lp in     for approximate policy iteration  and the lp in     for the approximate
   

fiefficient solution algorithms for factored mdps

backproja  h  

where basis function h has scope c 

define the scope of the backprojection  a  c      xi  c  parentsa  xi    
 
for each assignment
p
q y  a  c      
a
g  y    c  c  i x   c  pa  c  xi     y h c    
i

return g a  

figure    backprojection of basis function h 
linear programming algorithm  these lps have some common characteristics  they have
a small number of free variables  for k basis functions there are k     free variables in approximate policy iteration and k in approximate linear programming   but the number of
constraints is still exponential in the number of state variables  however  in factored mdps 
these lp constraints have another very useful property  the functionals in the constraints
have restricted scope  this key observation allows us to represent these constraints very
compactly 
first  observe that the constraints in the linear programs are all of the form 


x

wi ci  x   b x   x 

    

i

where only  and w            wk are free variables in the lp and x ranges over all states  this
general form represents both the type of constraint in the max norm projection lp in    
and the approximate linear programming formulation in      
the first insight in our construction is that we can replace the entire set of constraints
in equation      by one equivalent non linear constraint 
  max
x

x

wi ci  x   b x  

    

i

the second insight is that this new non linear constraint can be implemented by a set of
linear constraints using a construction that follows the structure of variable elimination in
cost networks  this insight allows us to exploit structure in factored mdps to represent
this constraint compactly 
we tackle the problem of representing the constraint in equation      in two steps 
first  computing the maximum assignment for a fixed set of weights  then  representing the
non linear constraint by small set of linear constraints  using a construction we call the
factored lp 
      maximizing over the state space
the key computation in our algorithms is to represent a non linear constraint of the form
in equation      efficiently by a small set of linear constraints  before presenting this construction  lets first consider a simpler problem  given some fixed weights wi   we would
p
like to compute the maximization     maxx i wi ci  x   b x   that is  the state x  such
p

   the complementary constraints in        b x   i wi ci  x   can be formulated using an analogous
construction to the one we present in this section by changing the sign of ci  x  and b x   the approximate
linear programming constraints of     can also be formulated in this form  as we show in section   

   

figuestrin  koller  parr   venkataraman

p

that the difference between i wi ci  x  and b x  is maximal  however  we cannot explicitly enumerate the exponential number of states and compute the difference  fortunately 
structure in factored mdps allows us to compute this maximum efficiently 
in the case of factored mdps  our state space is a set of vectors x which are assignments to the state variables x    x            xn    we can view both cw and b as functions
of these state variables  and hence also their difference  thus  we can define a function
p
f w  x            xn   such that f w  x    i wi ci  x   b x   note that we have executed a
representation shift  we are viewing f w as a function of the variables x  which is parameterized by w  recall that the size of the state space is exponential in the number
of variables  hence  our goal in this section is to compute maxx f w  x  without explicitly
considering each of the exponentially many states  the solution is to use the fact that f w
p
has a factored representation  more precisely  cw has the form i wi ci  zi    where zi is
a subset of x  for example  we might have c   x    x    which takes value   in states where
x    true and x    false and   otherwise  similarly  the vector b in our case is also a sum
p
of restricted scope functions  thus  we can express f w as a sum j fjw  zj    where fjw may
or may not depend on w  in the future  we sometimes drop the superscript w when it is
clear from context 
p
using our more compact notation  our goal here is simply to compute maxx i wi ci  x 
b x    maxx f w  x   that is  to find the state x over which f w is maximized  recall that
p
w
fw   m
j   fj  zj    we can maximize such a function  f   without enumerating every state
using non serial dynamic programming  bertele   brioschi         the idea is virtually
identical to variable elimination in a bayesian network  we review this construction here 
as it is a central component in our solution lp 
our goal is to compute
x
max
fj  x zj    
x       xn

j

the main idea is that  rather than summing all functions and then doing the maximization 
we maximize over variables one at a time  when maximizing over xl   only summands
involving xl participate in the maximization 
example     assume
f   f   x    x      f   x    x      f   x    x      f   x    x    
we therefore wish to compute 
max

x   x   x   x 

f   x    x      f   x    x      f   x    x      f   x    x    

we can first compute the maximum over x    the functions f  and f  are irrelevant  so we
can push them out  we get
max f   x    x      f   x    x      max f   x    x      f   x    x     

x   x   x 

x 

the result of the internal maximization depends on the values of x    x    thus  we can introduce a new function e   x    x    whose value at the point x    x  is the value of the internal
max expression  our problem now reduces to computing
max f   x    x      f   x    x      e   x    x    

x   x   x 

   

fiefficient solution algorithms for factored mdps

variableelimination  f  o 
  f    f            fm   is the set of functions to be maximized 
  o stores the elimination order 

for i     to number of variables 
  select the next variable to be eliminated 

let l   o i   
  select the relevant functions 

let e            el be the functions in f whose scope contains xl  
  maximize over current variable xl  

define a new function e   maxxl
l
j   scope ej     xl   

pl

j   ej

  note that scope e   

  update set of functions 

update the set of functions f   f   e     e            el   
  now  all functions have empty scope
p and their sum is the maximum value of f         fm  

return the maximum value

ei f

ei  

figure    variable elimination procedure for computing the maximum value f         fm  
where each fi is a restricted scope function 

having one fewer variable  next  we eliminate another variable  say x    with the resulting
expression reducing to 
max f   x    x      e   x    x    
x   x 

where

e   x    x      max f   x    x      e   x    x     
x 

finally  we define
e    max f   x    x      e   x    x    
x   x 

the result at this point is a number  which is the desired maximum over x            x    while
the naive approach of enumerating all states requires    arithmetic operations if all variables
are binary  using variable elimination we only need to perform    operations 
the general variable elimination algorithm is described in figure    the inputs to
the algorithm are the functions to be maximized f    f            fm   and an elimination
ordering o on the variables  where o i  returns the ith variable to be eliminated  as in
the example above  for each variable xl to be eliminated  we select the relevant functions
e            el   those whose scope contains xl   these functions are removed from the set f and
p
we introduce a new function e   maxxl l
j   ej   at this point  the scope of the functions in
f no longer depends on xl   that is  xl has been eliminated  this procedure is repeated
until all variables have been eliminated  the remaining functions in f thus have empty
scope  the desired maximum is therefore given by the sum of these remaining functions 
the computational cost of this algorithm is linear in the number of new function
values introduced in the elimination process  more precisely  consider the computation of
a new function e whose scope is z  to compute this function  we need to compute  dom z  
different values  the cost of the algorithm is linear in the overall number of these values 
introduced throughout the execution  as shown by dechter         this cost is exponential
   

figuestrin  koller  parr   venkataraman

in the induced width of the cost network  the undirected graph defined over the variables
x            xn   with an edge between xl and xm if they appear together in one of the original
functions fj   the complexity of this algorithm is  of course  dependent on the variable
elimination order and the problem structure  computing the optimal elimination order
is an np hard problem  arnborg  corneil    proskurowski        and elimination orders
yielding low induced tree width do not exist for some problems  these issues have been
confronted successfully for a large variety of practical problems in the bayesian network
community  which has benefited from a large variety of good heuristics which have been
developed for the variable elimination ordering problem  bertele   brioschi        kjaerulff 
      reed        becker   geiger        
      factored lp
in this section  we present the centerpiece of our planning algorithms  a new  general
approach for compactly representing exponentially large sets of lp constraints in problems
with factored structure  those where the functions in the constraints can be decomposed
as the sum of restricted scope functions  consider our original problem of representing
the non linear constraint in equation      compactly  recall that we wish to represent
p
the non linear constraint   maxx i wi ci  x   b x   or equivalently    maxx f w  x  
without generating one constraint for each state as in equation       the new  key insight
is that this non linear constraint can be implemented using a construction that follows the
structure of variable elimination in cost networks 
consider any function e used within f  including the original fi s   and let z be its scope 
for any assignment z to z  we introduce variable uez   whose value represents ez   into the
linear program  for the initial functions fiw   we include the constraint that ufzi   fiw  z   as
fiw is linear in w  this constraint is linear in the lp variables  now  consider a new function
e introduced into f by eliminating a variable xl   let e            el be the functions extracted
from f  and let z be the scope of the resulting e  we introduce a set of constraints 
uez



l
x
ej
j  

u z xl   zj  

xl  

    

let en be the last function generated in the elimination  and recall that its scope is empty 
hence  we have only a single variable uen   we introduce the additional constraint   uen  
the complete algorithm  presented in figure    is divided into three parts  first  we
generate equality constraints for functions that depend on the weights wi  basis functions  
in the second part  we add the equality constraints for functions that do not depend on the
weights  target functions   these equality constraints let us abstract away the differences
between these two types of functions and manage them in a unified fashion in the third
part of the algorithm  this third part follows a procedure similar to variable elimination
described in figure    however  unlike standard variable elimination where we would inp
troduce a new function e  such that e   maxxl l
j   ej   in our factored lp procedure we
introduce new lp variables uez   to enforce the definition of e as the maximum over xl of
pl
j   ej   we introduce the new lp constraints in equation      
example     to understand this construction  consider our simple example above  and
assume we want to express the fact that   maxx f w  x   we first introduce a set of
   

fiefficient solution algorithms for factored mdps

factoredlp  c  b o 
   c    c            ck   is the set of basis functions 
   b    b            bm   is the set of target functions 
  o stores the elimination order 
p
p
  return a  polynomial  set of constraints  equivalent to   i wi ci  x    j bj  x   x  
  data structure for the constraints in factored lp 

let        
  data structure for the intermediate functions generated in variable elimination 

let f       
  generate equality constraint to abstract away basis functions 

for each ci  c 
let z   scope ci   
for each assignment z  z  create a new lp variable ufzi and add a
constraint to  
ufzi   wi ci  z  
store new function fi to use in variable elimination step  f   f   fi   
  generate equality constraint to abstract away target functions 

for each bj  b 
let z   scope bj   
f
for each assignment z  z  create a new lp variable uzj and add a
constraint to  
f
uzj   bj  z  
store new function fj to use in variable elimination step  f   f   fj   
  now  f contains all of the functions involved in the lp  our constraints become   
p
e  x   x   which we represent compactly using a variable elimination procedure 
e f i
i

for i     to number of variables 

  select the next variable to be eliminated 

let l   o i   
  select the relevant functions 

let e            el be the functions in f whose scope contains xl   and let
zj   scope ej   
  introduce linear constraints for the maximum over current variable xl  

define a new function e with scope z   l
j   zj   xl   to represent
pl
maxxl j   ej  
add constraints to  to enforce maximum  for each assignment z  z 
uez 

l
x

e

j
u z x
l   zj  

xl  

j  

  update set of functions 

update the set of functions f   f   e     e            el   
  now  all variables have been eliminated and all functions have empty scope 

add last constraint to  


x

ei  

ei f

return  

figure    factored lp algorithm for the compact representation of the exponential set of
p
p
constraints   i wi ci  x    j bj  x   x 
   

figuestrin  koller  parr   venkataraman

variables ufx    x  for every instantiation of values x    x  to the variables x    x    thus  if
x  and x  are both binary  we have four such variables  we then introduce a constraint
defining the value of ufx    x  appropriately  for example  for our f  above  we have uft t     
and uft f    w    we have similar variables and constraints for each fj and each value z in
zj   note that each of the constraints is a simple equality constraint involving numerical
constants and perhaps the weight variables w 
next  we introduce variables for each of the intermediate expressions generated by variable elimination  for example  when eliminating x    we introduce a set of lp variables
uex    x    for each of them  we have a set of constraints
uex    x   ufx    x    ufx    x 
one for each value x  of x    we have a similar set of constraint for uex    x  in terms of
ufx    x  and uex    x    note that each constraint is a simple linear inequality 
we can now prove that our factored lp construction represents the same constraint as
non linear constraint in equation      
theorem     the constraints generated by the factored lp construction are equivalent to
the non linear constraint in equation       that is  an assignment to    w  satisfies the
factored lp constraints if and only if it satisfies the constraint in equation      
proof  see appendix a   
p
returning to our original formulation  we have that j fjw is cw  b in the original
set of constraints  hence our new set of constraints is equivalent to the original set   
p
maxx i wi ci  x   b x  in equation       which in turn is equivalent to the exponential
p
set of constraints   i wi ci  x   b x   x in equation       thus  we can represent this
exponential set of constraints by a new set of constraints and lp variables  the size of
this new set  as in variable elimination  is exponential only in the induced width of the cost
network  rather than in the total number of variables 
in this section  we presented a new  general approach for compactly representing exponentially large sets of lp constraints in problems with factored structure  in the remainder
of this paper  we exploit this construction to design efficient planning algorithms for factored
mdps 
      factored max norm projection
we can now use our procedure for representing the exponential number of constraints in
equation      compactly to compute efficient max norm projections  as in equation     
w  arg min kcw  bk  
w

the max norm projection is computed by the linear program in      there are two sets
p
p
of constraints in this lp    kj   cij wj  bi   i and   bi  kj   cij wj   i  each of
these sets is an instance of the constraints in equation       which we have just addressed
in the previous section  thus  if each of the k basis functions in c is a restricted scope
function and the target function b is the sum of restricted scope functions  then we can
use our factored lp technique to represent the constraints in the max norm projection lp
compactly  the correctness of our algorithm is a corollary of theorem     
   

fiefficient solution algorithms for factored mdps

corollary     the solution     w   of a linear program that minimizes  subject to the
constraints in factoredlp c  b o  and factoredlp c  b o   for any elimination
order o satisfies 
w  arg min kcw  bk  
w

and

   min kcw  bk  
w

the original max norm projection lp had k     variables and two constraints for each
state x  thus  the number of constraints is exponential in the number of state variables 
on the other hand  our new factored max norm projection lp has more variables  but
exponentially fewer constraints  the number of variables and constraints in the new factored
lp is exponential only in the number of state variables in the largest factor in the cost
network  rather than exponential in the total number of state variables  as we show in
section    this exponential gain allows us to compute max norm projections efficiently when
solving very large factored mdps 

   approximate linear programming
we begin with the simplest of our approximate mdp solution algorithms  based on the
approximate linear programming formulation in section      using the basic operations
described in section    we can formulate an algorithm that is both simple and efficient 
    the algorithm
as discussed in section      approximate linear program formulation is based on the linear
programming approach to solving mdps presented in section      however  in this approximate version  we restrict the space of value functions to the linear space defined by
our basis functions  more precisely  in this approximate lp formulation  the variables are
w            wk  the weights for our basis functions  the lp is given by 
variables  w            wk  
p
p
minimize 
 x  i wi hi  x   
x
p
p
p
 
 
subject to 
i wi hi  x    r x  a    
x  p  x   x  a 
i wi hi  x    x  x  a  a 
    
in other words  this formulation takes the lp in     and substitutes the explicit state value
p
function with a linear value function representation i wi hi  x   this transformation from
an exact to an approximate problem formulation has the effect of reducing the number
of free variables in the lp to k  one for each basis function coefficient   but the number
of constraints remains  x    a   in our sysadmin problem  for example  the number of
constraints in the lp in      is  m        m   where m is the number of machines in the
network  however  using our algorithm for representing exponentially large constraint sets
compactly we are able to compute the solution to this approximate linear programming
algorithm in closed form with an exponentially smaller lp  as in section     
p
p
first  consider the objective function x  x  i wi hi  x  of the lp       naively
representing this objective function requires a summation over a exponentially large state
space  however  we can rewrite the objective and obtain a compact representation  we
first reorder the terms 
   

figuestrin  koller  parr   venkataraman

factoredalp  p   r    h  o   
  p is the factored transition model 
  r is the set of factored reward functions 
   is the discount factor 
  h is the set of basis functions h    h            hk   
  o stores the elimination order 
   are the state relevance weights 
  return the basis function weights w computed by approximate linear programming 
  cache the backprojections of the basis functions 

for each basis function hi  h  for each action a 
let gia   backproja  hi   
  compute factored state relevance weights 

for each basis function hi   compute the factored state relevance weights
i as in equation       
  generate approximate linear programming constraints

let       
for each action a 
   ra   o  
let      factoredlp  g a  h            gka  hkp

p

  so far  our constraints guarantee that   r x  a     x  p  x    x  a  i wi hi  x    
p
w hi  x   to satisfy the approximate linear programming solution in      we must add
i i
a final constraint 

let             
  we can now obtain the solution weights by solving an lp 

let w be the solution of the linear program  minimize
the constraints  
return w 

p
i

i wi   subject to

figure    factored approximate linear programming algorithm 

   

fiefficient solution algorithms for factored mdps

x

 x 

x

x

wi hi  x   

x

i

x

wi

 x  hi  x  

x

i

now  consider the state relevance weights  x  as a distribution over states  so that  x     
p
and x  x       as in backprojections  we can now write 
i  

x

x

 x  hi  x   

x

 ci   hi  ci   

    

ci ci

where  ci   represents the marginal of the state relevance weights  over the domain
dom ci   of the basis function hi   for example  if we use uniform state relevance weights as
 
in our experiments   x     x 
 then the marginals become  ci      c i     thus  we can
p
rewrite the objective function as i wi i   where each basis weight i is computed as shown
in equation       if the state relevance weights are represented by marginals  then the cost
of computing each i depends exponentially on the size of the scope of ci only  rather than
exponentially on the number of state variables  on the other hand  if the state relevance
weights are represented by arbitrary distributions  we need to obtain the marginals over the
ci s  which may not be an efficient computation  thus  greatest efficiency is achieved by
using a compact representation  such as a bayesian network  for the state relevance weights 
second  note that the right side of the constraints in the lp      correspond to the qa
functions 
x
x
qa  x    ra  x    
p  x    x  a 
wi hi  x    
x 

i

using the efficient backprojection operation in factored mdps described in section     we
can rewrite the qa functions as 
qa  x    ra  x    

x

wi gia  x  

i

gia

where
is the backprojection of basis function hi through the transition model pa   as we
discussed  if hi has scope restricted to ci   then gia is a restricted scope function of a  c i   
we can precompute the backprojections gia and the basis relevance weights i   the
approximate linear programming lp of      can be written as 
variables  w            wk  
p
minimize 
 w  
pi i i
p
a
a
subject to 
w
i i hi  x    r  x    
i wi gi  x   x  x  a  a 

    

finally  we can rewrite this lp to use constraints of the same form as the one in equation      
variables  w            wk  
p
minimize 
i i wi  
p
subject to     maxx  ra  x    i wi  gia  x   hi  x    a  a 

    

we can now use our factored lp construction in section     to represent these non linear
constraints compactly  basically  there is one set of factored lp constraints for each action
a  specifically  we can write the non linear constraint in the same form as those in equation      by expressing the functions c as  ci  x    hi  x gia  x   each ci  x  is a restricted
   

figuestrin  koller  parr   venkataraman

scope function  that is  if hi  x  has scope restricted to ci   then gia  x  has scope restricted
to a  c i    which means that ci  x  has scope restricted to ci  a  c i    next  the target
function b becomes the reward function ra  x  which  by assumption  is factored  finally 
in the constraint in equation        is a free variable  on the other hand  in the lp in     
the maximum in the right hand side must be less than zero  this final condition can be
achieved by adding a constraint       thus  our algorithm generates a set of factored
lp constraints  one for each action  the total number of constraints and variables in this
new lp is linear in the number of actions  a  and only exponential in the induced width
of each cost network  rather than in the total number of variables  the complete factored
approximate linear programming algorithm is outlined in figure   
    an example
we now present a complete example of the operations required by the approximate lp algorithm to solve the factored mdp shown in figure   a   our presentation follows four steps 
problem representation  basis function selection  backprojections and lp construction 
problem representation  first  we must fully specify the factored mdp model for the
problem  the structure of the dbn is shown in figure   b   this structure is maintained
for all action choices  next  we must define the transition probabilities for each action 
there are   actions in this problem  do nothing  or reboot one of the   machines in the
network  the cpds for these actions are shown in figure   c   finally  we must define the
reward function  we decompose the global reward as the sum of   local reward functions 
one for each machine  such that there is a reward if the machine is working  specifically 
ri  xi   true      and ri  xi   false       breaking symmetry by setting r   x    true   
   we use a discount factor of        
in this simple example  we use five simple basis functions 
basis function selection 
first  we include the constant function h       next  we add indicators for each machine
which take value   if the machine is working  hi  xi   true      and hi  xi   false      
backprojections 
the first algorithmic step is computing the backprojection of the
basis functions  as defined in section      the backprojection of the constant basis is
simple 
g a  

x

pa  x    x h   

x 

 

x

pa  x    x     

x 

    
next  we must backproject our indicator basis functions hi  
gia  

x
x 

 

pa  x    x hi  x i    

x

y

pa  x j   xj    xj  hi  x i    

x    x    x    x   j

   

fiefficient solution algorithms for factored mdps

 

x
x i

 

x

x

pa  x i   xi    xi  hi  x i  

y

pa  x j   xj    xj    

x   x   xi     j  i

pa  x i   xi    xi  hi  x i    

x i

  pa  xi    true   xi    xi       pa  xi    false   xi    xi      
  pa  xi    true   xi    xi    
thus  gia is a restricted scope function of  xi    xi    we can now use the cpds in figure   c  to specify gia  
reboot   i

 xi    xi    

reboot    i

 xi    xi    

gi

gi

xi   true xi   false
xi    true
 
 
 
xi    false
 
 
xi    true
xi    false

xi   true xi   false
   
    
 
   
    

lp construction 
to illustrate the factored lps constructed by our algorithms  we
define the constraints for the approximate linear programming approach presented above 
first  we define the functions cai   gia  hi   as shown in equation       in our example 
these functions are ca              for the constant basis  and for the indicator bases 
reboot   i

 xi    xi    

reboot    i

 xi    xi    

ci

ci

xi   true xi   false
xi    true
   
   
 
xi    false
   
   
xi    true
xi    false

xi   true xi   false
    
     
 
    
     

using this definition of cai   the approximate linear programming constraints are given by 
   max
x

x

ri  

x

i

wj caj   a  

    

j

we present the lp construction for one of the   actions  reboot      analogous constructions
can be made for the other actions 
in the first set of constraints  we abstract away the difference between rewards and basis
functions by introducing lp variables u and equality constraints  we begin with the reward
functions 
r 
 
ur
x        ux       

r 
 
ur
x        ux       

r 
 
ur
x        ux       

r 
 
ur
x        ux       

we now represent the equality constraints for the caj functions for the reboot     action  note
that the appropriate basis function weight from equation      appears in these constraints 

   

figuestrin  koller  parr   venkataraman

uc        w   
ucx    x        w   
ucx    x        w   
ucx    x        w   
ucx    x        w   
c
c
c
ux    x         w    ux    x         w    ux    x          w    ucx    x          w   
ucx    x         w    ucx    x         w    ucx    x          w    ucx    x          w   
ucx    x         w    ucx    x         w    ucx    x          w    ucx    x          w   
using these new lp variables  our lp constraint from equation      for the reboot     action
becomes 
 

max

x   x   x   x 

 
 
x
x
c
c 
i
ur
 
u
 
uxjj   xj  
xi
i  

j  

we are now ready for the variable elimination process  we illustrate the elimination of
variable x   
 

max

x   x   x 

 
 
h
i
x
x
c
ri
c 
c 
c 
 
uxi   u  
uxjj   xj   max ur
x    ux   x    ux   x   
i  

x 

j  

h

i

c 
c 
 
we can represent the term maxx  ur
x    ux   x    ux   x  by a set of linear constraints 
one for each assignment of x  and x    using the new lp variables uex    x  to represent this
maximum 

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

uex    x 

c 
c 
 
 ur
x    ux   x    ux   x   

we have now eliminated variable x  and our global non linear constraint becomes 
 

 
 
x
x
c
c 
i
ur
 
u
 
uxjj   xj   uex    x   
xi
x   x   x 

max

j  

i  

next  we eliminate variable x    the new lp constraints and variables have the form 
c 
e 
 
uex    x   ur
x    ux   x    ux   x     x    x    x   

thus  removing x  from the global non linear constraint 
 
x
c 
e 
c 
i
ur
xi   u   ux   x    ux   x   
x   x 

   max

i  

   

fiefficient solution algorithms for factored mdps

      

number of lp constraints

      

  explicit constraints  
 n      n

explicit lp
factored lp
      

      

     

  factored constraints  
  n     n    
 
 

 

 

 
 
  
number of machines in ring

  

  

  

figure    number of constraints in the lp generated by the explicit state representation
versus the factored lp construction for the solution of the ring problem with
basis functions over single variables and approximate linear programming as the
solution algorithm 

we can now eliminate x    generating the linear constraints 
c 
e 
 
uex    ur
x    ux   x    ux   x     x    x   

now  our global non linear constraint involves only x   
e 
c 
 
   max ur
x    u   ux   
x 

as x  is the last variable to be eliminated  the scope of the new lp variable is empty and
the linear constraints are given by 
e 
 
u e   u r
x    ux     x   

all of the state variables have now been eliminated  turning our global non linear constraint
into a simple linear constraint 
   uc    ue   
which completes the lp description for the approximate linear programming solution to
the problem in figure   
in this small example with only four state variables  our factored lp technique generates
a total of    equality constraints      inequality constraints and     lp variables  while
the explicit state representation in equation     generates only    inequality constraints
and   lp variables  however  as the problem size increases  the number of constraints and
lp variables in our factored lp approach grow as o n     while the explicit state approach
grows exponentially  at o n n    this scaling effect is illustrated in figure   

   approximate policy iteration with max norm projection
the factored approximate linear programming approach described in the previous section
is both elegant and easy to implement  however  we cannot  in general  provide strong
   

figuestrin  koller  parr   venkataraman

guarantees about the error it achieves  an alternative is to use the approximate policy
iteration described in section      which does offer certain bounds on the error  however 
as we shall see  this algorithm is significantly more complicated  and requires that we place
additional restrictions on the factored mdp 
in particular  approximate policy iteration requires a representation of the policy at each
iteration  in order to obtain a compact policy representation  we must make an additional
assumption  each action only affects a small number of state variables  we first state this
assumption formally  then  we show how to obtain a compact representation of the greedy
policy with respect to a factored value function  under this assumption  finally  we describe
our factored approximate policy iteration algorithm using max norm projections 
    default action model
in section      we presented the factored mdp model  where each action is associated with
its own factored transition model represented as a dbn and with its own factored reward
function  however  different actions often have very similar transition dynamics  only differing in their effect on some small set of variables  in particular  in many cases a variable
has a default evolution model  which only changes if an action affects it directly  boutilier
et al         
this type of structure turns out to be useful for compactly representing policies  a property which is important in our approximate policy iteration algorithm  thus  in this section
of the paper  we restrict attention to factored mdps that are defined using a default transition model d   hgd   pd i  koller   parr         for each action a  we define effects a   x 
to be the variables in the next state whose local probability model is different from d   i e  
those variables xi  such that pa  xi    parentsa  xi        pd  xi    parentsd  xi     
example     in our system administrator example  we have an action ai for rebooting
each one of the machines  and a default action d for doing nothing  the transition model
described above corresponds to the do nothing action  which is also the default transition
model  the transition model for ai is different from d only in the transition model for the
variable xi    which is now xi    true with probability one  regardless of the status of the
neighboring machines  thus  in this example  effects ai     xi   
as in the transition dynamics  we can also define the notion of default reward model  in
p
this case  there is a set of reward functions ri   ri  ui   associated with the default action
d  in addition  each action a can have a reward function ra  ua    here  the extra reward of
action a has scope restricted to rewards a    uai   x            xn    thus  the total reward
p
associated with action a is given by ra   ri   ri   note that ra can also be factored as a
linear combination of smaller terms for an even more compact representation 
we can now build on this additional assumption to define the complete algorithm 
recall that the approximate policy iteration algorithm iterates through two steps  policy
improvement and approximate value determination  we now discuss each of these steps 
    computing greedy policies
the policy improvement step computes the greedy policy relative to a value function v  t    
  t    greedy v  t     
   

fiefficient solution algorithms for factored mdps

recall that our value function estimates have the linear form hw  as we described in
section      the greedy policy for this type of value function is given by 
greedy hw  x    arg max qa  x  
a

p

where each qa can be represented by  qa  x    r x  a    i wi gia  x  
if we attempt to represent this policy naively  we are again faced with the problem
of exponentially large state spaces  fortunately  as shown by koller and parr         the
greedy policy relative to a factored value function has the form of a decision list  more
precisely  the policy can be written in the form ht    a  i  ht    a  i          htl   al i  where each ti
is an assignment of values to some small subset ti of variables  and each ai is an action 
the greedy action to take in state x is the action aj corresponding to the first event tj in
the list with which x is consistent  for completeness  we now review the construction of
this decision list policy 
the critical assumption that allows us to represent the policy as a compact decision list
is the default action assumption described in section      under this assumption  the qa
functions can be written as 
a

qa  x    r  x   

r
x

ri  x   

i  

x

wi gia  x  

i

where ra has scope restricted to ua   the q function for the default action d is just 
p
p
qd  x    ri   ri  x    i wi gid  x  
we now have a set of linear q functions which implicitly describes a policy   it is
not immediately obvious that these q functions result in a compactly expressible policy 
an important insight is that most of the components in the weighted combination are
identical  so that gia is equal to gid for most i  intuitively  a component gia corresponding
to the backprojection of basis function hi  ci   is only different if the action a influences
one of the variables in ci   more formally  assume that effects a   ci     in this case 
all of the variables in ci have the same transition model in a and d   thus  we have
that gia  x    gid  x   in other words  the ith component of the qa function is irrelevant
when deciding whether action a is better than the default action d  we can define which
components are actually relevant  let ia be the set of indices i such that effects a   ci     
these are the indices of those basis functions whose backprojection differs in pa and pd  
in our example dbn of figure    actions and basis functions involve single variables  so
iai   i 
let us now consider the impact of taking action a over the default action d  we can
define the impact  the difference in value  as 
a  x    qa  x   qd  x  
  ra  x   

x

h

i

wi gia  x   gid  x   

    

iia

this analysis shows that a  x  is a function whose scope is restricted to




ta   ua  iia a  c i    
   

    

figuestrin  koller  parr   venkataraman

decisionlistpolicy  qa  
  qa is the set of q functions  one for each action 
  return the decision list policy  
  initialize decision list 

let       
  compute the bonus functions 

for each action a  other than the default action d 
compute the bonus for taking action a 
a  x    qa  x   qd  x  
as in equation       note that a has scope restricted to ta   as in
equation      
  add states with positive bonuses to the  unsorted  decision list 

for each assignment t  ta  
if a  t       add branch to decision list 
      ht  a  a  t i  
  add the default action to the  unsorted  decision list 

let       h  d   i  
  sort decision list to obtain final policy 

sort the decision list  in decreasing order on the  element of ht  a  i 
return  

figure    method for computing the decision list policy  from the factored representation
of the qa functions 

in our example dbn  ta     x    x    
intuitively  we now have a situation where we have a baseline value function qd  x 
which defines a value for each state x  each action a changes that baseline by adding or
subtracting an amount from each state  the point is that this amount depends only on ta  
so that it is the same for all states in which the variables in ta take the same values 
we can now define the greedy policy relative to our q functions  for each action a  define
a set of conditionals ht  a  i  where each t is some assignment of values to the variables ta  
and  is a  t   now  sort the conditionals for all of the actions by order of decreasing  
ht    a      i  ht    a      i          htl   al   l i 
consider our optimal action in a state x  we would like to get the largest possible bonus
over the default value  if x is consistent with t    we should clearly take action a    as it
gives us bonus     if not  then we should try to get     thus  we should check if x is
consistent with t    and if so  take a    using this procedure  we can compute the decisionlist policy associated with our linear estimate of the value function  the complete algorithm
for computing the decision list policy is summarized in figure   
p
note that the number of conditionals in the list is a  dom ta     ta   in turn  depends
on the set of basis function clusters that intersect with the effects of a  thus  the size
of the policy depends in a natural way on the interaction between the structure of our
   

fiefficient solution algorithms for factored mdps

process description and the structure of our basis functions  in problems where the actions
modify a large number of variables  the policy representation could become unwieldy  the
approximate linear programming approach in section   is more appropriate in such cases 
as it does not require an explicit representation of the policy 
    value determination
in the approximate value determination step our algorithm computes 
w t    arg min khw   r t    p t  hw k  
w

by rearranging the expression  we get 
w t    arg min k h  p t  h  w  r t  k  
w

this equation is an instance of the optimization in equation      if p t  is factored  we can
conclude that c    h  p t  h  is also a matrix whose columns correspond to restrictedscope functions  more specifically 
 t 

ci  x    hi  x   gi  x  
 t 

where gi is the backprojection of the basis function hi through the transition model p t   
as described in section      the target b   r t  corresponds to the reward function  which
for the moment is assumed to be factored  thus  we can again apply our factored lp in
section       to estimate the value of the policy   t   
unfortunately  the transition model p t  is not factored  as a decision list representation for the policy   t  will  in general  induce a transition model p t  which cannot be
represented by a compact dbn  nonetheless  we can still generate a compact lp by exploiting the decision list structure of the policy  the basic idea is to introduce cost networks
corresponding to each branch in the decision list  ensuring  additionally  that only states
consistent with this branch are considered in the cost network maximization  specifically 
we have a factored lp construction for each branch hti   ai i  the ith cost network only
considers a subset of the states that is consistent with the ith branch of the decision list 
let si be the set of states x such that ti is the first event in the decision list for which x
is consistent  that is  for each state x  si   x is consistent with ti   but it is not consistent
with any tj with j   i 
recall that  as in equation       our lp construction defines a set of constraints that
p
imply that   i wi ci  x   b x  for each state x  instead  we have a separate set of
constraints for the states in each subset si   for each state in si   we know that action ai is
taken  hence  we can apply our construction above using pai  a transition model which is
factored by assumption  in place of the non factored p t    similarly  the reward function
p
becomes rai  x    ri   ri  x  for this subset of states 
the only issue is to guarantee that the cost network constraints derived from this transition model are applied only to states in si   specifically  we must guarantee that they are
applied only to states consistent with ti   but not to states that are consistent with some
tj for j   i  to guarantee the first condition  we simply instantiate the variables in ti to
take the values specified in ti   that is  our cost network now considers only the variables in
   

figuestrin  koller  parr   venkataraman

factoredapi  p   r    h  o    tmax  
  p is the factored transition model 
  r is the set of factored reward functions 
   is the discount factor 
  h is the set of basis functions h    h            hk   
  o stores the elimination order 
   bellman error precision 
  tmax maximum number of iterations 
  return the basis function weights w computed by approximate policy iteration 
  initialize weights

let w        
  cache the backprojections of the basis functions 

for each basis function hi  h  for each action a 
let gia   backproja  hi   
  main approximate policy iteration loop 

let t     
repeat
  policy improvement part of the loop 
  compute decision list policy for iteration t weights 

let  t    decisionlistpolicy ra   

p

i

 t 

wi gia   

  value determination part of the loop 
  initialize constraints for max norm projection lp 

let        and       
  initialize indicators 

let i      
  for every branch of the decision list policy  generate the relevant set of constraints  and
update the indicators to constraint the state space for future branches 

for each branch htj   aj i in the decision list policy  t   
  instantiate the variables in tj to the assignment given in tj  
a

a

instantiate the set of functions  h   g  j           hk  gk j   with the
partial state assignment tj and store in c 
instantiate the target functions raj with the partial state assignment tj and store in b 
instantiate the indicator functions i with the partial state assignment tj and store in i    
  generate the factored lp constraints for the current decision list branch 

let        factoredlp c  b   i     o  
let      factoredlp c  b   i     o  
  update the indicator functions 

let ij  x      x   tj   and update the indicators i   i  ij  
  we can now obtain the new set of weights by solving an lp  which corresponds to the
max norm projection 

let w t    be the solution of the linear program  minimize   subject
to the constraints         
let t   t     
until bellmanerr hw t      or t  tmax or w t     w t   
return w t   

figure    factored approximate policy iteration with max norm projection algorithm 

   

fiefficient solution algorithms for factored mdps

 x            xn  ti   and computes the maximum only over the states consistent with ti   ti  
to guarantee the second condition  we ensure that we do not impose any constraints on
states associated with previous decisions  this is achieved by adding indicators ij for each
previous decision tj   with weight   more specifically  ij is a function that takes value
 for states consistent with tj and zero for other all assignments of tj   the constraints
for the ith branch will be of the form 
  r x  ai    

x

wl  gl  x  ai    h x    

x

  x   tj   

x   ti   

    

j i

l

where x   ti   defines the assignments of x consistent with ti   the introduction of these
indicators causes the constraints associated with ti to be trivially satisfied by states in sj
for j   i  note that each of these indicators is a restricted scope function of tj and can
be handled in the same fashion as all other terms in the factored lp  thus  for a decision
list of size l  our factored lp contains constraints from  l cost networks  the complete
approximate policy iteration with max norm projection algorithm is outlined in figure   
    comparisons
it is instructive to compare our max norm policy iteration algorithm to the l   projection
policy iteration algorithm of koller and parr        in terms of computational costs per
iteration and implementation complexity  computing the l  projection requires  among
other things  a series of dot product operations between basis functions and backprojected
basis functions hhi gj i  these expressions are easy to compute if p refers to the transition
model of a particular action a  however  if the policy  is represented as a decision list  as is
the result of the policy improvement step  then this step becomes much more complicated 
in particular  for every branch of the decision list  for every pair of basis functions i and j 
and for each assignment to the variables in scope hi    scope gja    it requires the solution of
a counting problem which is  p  complete in general  although koller and parr show that
this computation can be performed using a bayesian network  bn  inference  the algorithm
still requires a bn inference for each one of those assignments at each branch of the decision
list  this makes the algorithm very difficult to implement efficiently in practice 
the max norm projection  on the other hand  relies on solving a linear program at every
iteration  the size of the linear program depends on the cost networks generated  as we
discuss  two cost networks are needed for each point in the decision list  the complexity
of each of these cost networks is approximately the same as only one of the bn inferences
in the counting problem for the l  projection  overall  for each branch in the decision
list  we have a total of two of these inferences  as opposed to one for each assignment of
scope hi    scope gja   for every pair of basis functions i and j  thus  the max norm policy
iteration algorithm is substantially less complex computationally than the approach based
on l   projection  furthermore  the use of linear programming allows us to rely on existing
lp packages  such as cplex   which are very highly optimized 
it is also interesting to compare the approximate policy iteration algorithm to the approximate linear programming algorithm we presented in section    in the approximate
linear programming algorithm  we never need to compute the decision list policy  the
policy is always represented implicitly by the qa functions  thus  this algorithm does not
   

figuestrin  koller  parr   venkataraman

require explicit computation or manipulation of the greedy policy  this difference has two
important consequences  one computational and the other in terms of generality 
first  not having to compute or consider the decision lists makes approximate linear
programming faster and easier to implement  in this algorithm  we generate a single lp
with one cost network for each action and never need to compute a decision list policy  on
the other hand  in each iteration  approximate policy iteration needs to generate two lps
for every branch of the decision list of size l  which is usually significantly longer than  a  
with a total of  l cost networks  in terms of representation  we do not require the policies
to be compact  thus  we do not need to make the default action assumption  therefore  the
approximate linear programming algorithm can deal with a more general class of problems 
where each action can have its own independent dbn transition model  on the other hand 
as described in section      approximate policy iteration has stronger guarantees in terms
of error bounds  these differences will be highlighted further in our experimental results
presented in section   

   computing bounds on policy quality
we have presented two algorithms for computing approximate solutions to factored mdps 
b where w
b
all these algorithms generate linear value functions which can be denoted by hw 
are the resulting basis function weights  in practice  the agent will define its behavior by
b one issue that remains is how this
b   greedy hw  
acting according to the greedy policy 

b compares to the true optimal policy    that is  how the actual value vb
policy 
 of policy
b compares to v   

in section    we showed some a priori bounds for the quality of the policy  another
possible procedure is to compute an a posteriori bound  that is  given our resulting weights
b we compute a bound on the loss of acting according to the greedy policy 
b rather than
w 
the optimal policy  this can be achieved by using the bellman error analysis of williams
and baird        
the bellman error is defined as bellmanerr v    kt  v  vk   given the greedy
b   greedy v   their analysis provides the bound 
policy 
 

v  v    bellmanerr v   
b
 
 

    

b to evaluate the quality of our resulting
thus  we can use the bellman error bellmanerr hw 
greedy policy 
note that computing the bellman error involves a maximization over the state space 
thus  the complexity of this computation grows exponentially with the number of state
variables  koller and parr        suggested that structure in the factored mdp can be
exploited to compute the bellman error efficiently  here  we show how this error bound can
be computed by a set of cost networks using a similar construction to the one in our maxb that can be represented
norm projection algorithms  this technique can be used for any 
as a decision list and does not depend on the algorithm used to determine the policy  thus 
we can apply this technique to solutions determined approximate linear programming if the
action descriptions permit a decision list representation of the policy 
b the bellman error is given by 
for some set of weights w 
   

fiefficient solution algorithms for factored mdps

b
factoredbellmanerr  p   r    h  o  w 
  p is the factored transition model 
  r is the set of factored reward functions 
   is the discount factor 
  h is the set of basis functions h    h            hk   
  o stores the elimination order 
  w
b are the weights for the linear value function 
  return the bellman error for the value function hw 
b
  cache the backprojections of the basis functions 

for each basis function hi  h  for each action a 
let gia   backproja  hi   
  compute decision list policy for value function
hw 
b
a
b   decisionlistpolicy ra    p w
let 
i bi gi   
  initialize indicators 

let i      
  initialize bellman error 

let      
  for every branch of the decision list policy  generate the relevant cost networks  solve it with
variable elimination  and update the indicators to constraint the state space for future branches 

b
for each branch htj   aj i in the decision list policy  

  instantiate the variables in tj to the assignment given in tj  
a

a

instantiate the set of functions  w
b   h  g  j            w
bk  hk gk j    with the
partial state assignment tj and store in c 
instantiate the target functions raj with the partial state assignment
tj and store in b 
instantiate the indicator functions i with the partial state assignment
tj and store in i    
  use variable elimination to solve first cost network  and update bellman error  if error
for this branch is larger 

let    max    variableelimination c  b   i     o   
  use variable elimination to solve second cost network  and update bellman error  if error
for this branch is larger 

let    max    variableelimination c   b   i     o   
  update the indicator functions 

let ij  x      x   tj   and update the indicators i   i  ij  
return  
b
figure     algorithm for computing bellman error for factored value function hw 

   

figuestrin  koller  parr   venkataraman

b
b  hwk
b  
bellmanerr hw 
  kt  hw


  max

p

p

p

maxx i wi hi  x   rb  x    x  pb  x    x  j wj hj  x     
p
p
p
maxx rb  x     x  pb  x    x  j wj hj  x     i wi hi  x 

 

if the rewards rb and the transition model pb are factored appropriately  then we can
compute each one of these two maximizations  maxx   using variable elimination in a cost
b is a decision list policy and it does not
network as described in section        however  
induce a factored transition model  fortunately  as in the approximate policy iteration
algorithm in section    we can exploit the structure in the decision list to perform such
maximization efficiently  in particular  as in approximate policy iteration  we will generate
two cost networks for each branch in the decision list  to guarantee that our maximization
is performed only over states where this branch is relevant  we include the same type of
indicator functions  which will force irrelevant states to have a value of   thus guaranteeing that at each point of the decision list policy we obtain the corresponding state with
the maximum error  the state with the overall largest bellman error will be the maximum
over the ones generated for each point the in the decision list policy  the complete factored
algorithm for computing the bellman error is outlined in figure    
one last interesting note concerns our approximate policy iteration algorithm with maxnorm projection of section    in all our experiments  this algorithm converged  so that
w t    w t    after some iterations  if such convergence occurs  then the objective function
 t    of the linear program in our last iteration is equal to the bellman error of the final
policy 
lemma     if approximate policy iteration with max norm projection converges  so that
w t    w t    for some iteration t  then the max norm projection error  t    of the last
b   hw t   
iteration is equal to the bellman error for the final value function estimate hw
b    t     
bellmanerr hw 

proof  see appendix a   
thus  we can bound the loss of acting according to the final policy   t    by substituting
 t   

into the bellman error bound 
corollary     if approximate policy iteration with max norm projection converges after
b associated with a greedy policy 
b  
t iterations to a final value function estimate hw
b
b instead of the optimal policy   is
greedy hw  
then the loss of acting according to 
bounded by 
 t   
 

v  v    
 
b
 
 

b 
where vb is the actual value of the policy 

therefore  when approximate policy iteration converges we can obtain a bound on the
quality of the resulting policy without needing to compute the bellman error explicitly 
   

 

fiefficient solution algorithms for factored mdps

   exploiting context specific structure
thus far  we have presented a suite of algorithms which exploit additive structure in the
reward and basis functions and sparse connectivity in the dbn representing the transition
model  however  there exists another important type of structure that should also be
exploited for efficient decision making  context specific independence  csi   for example 
consider an agent responsible for building and maintaining a house  if the painting task can
only be completed after the plumbing and the electrical wiring have been installed  then
the probability that the painting is done is    in all contexts where plumbing or electricity
are not done  independently of the agents action  the representation we have used so far in
this paper would use a table to represent this type of function  this table is exponentially
large in the number of variables in the scope of the function  and ignores the context specific
structure inherent in the problem definition 
boutilier et al   boutilier et al         dearden   boutilier        boutilier  dean   
hanks        boutilier et al         have developed a set of algorithms which can exploit csi
in the transition and reward models to perform efficient  approximate  planning  although
this approach is often successful in problems where the value function contains sufficient
context specific structure  the approach is not able to exploit the additive structure which
is also often present in real world problems 
in this section  we extend the factored mdp model to include context specific structure 
we present a simple  yet effective extension of our algorithms which can exploit both csi
and additive structure to obtain efficient approximations for factored mdps  we first extend
the factored mdp representation to include context specific structure and then show how
the basic operations from section   required by our algorithms can be performed efficiently
in this new representation 
    factored mdps with context specific and additive structure
there are several representations for context specific functions  the most common are
decision trees  boutilier et al          algebraic decision diagrams  adds   hoey  st aubin 
hu    boutilier         and rules  zhang   poole         we choose to use rules as our
basic representation  for two main reasons  first  the rule based representation allows a
fairly simple algorithm for variable elimination  which is a key operation in our framework 
second  rules are not required to be mutually exclusive and exhaustive  a requirement that
can be restrictive if we want to exploit additive independence  where functions can be
represented as a linear combination of a set of non mutually exclusive functions 
we begin by describing the rule based representation  along the lines of zhang and
pooles presentation         for the probabilistic transition model  in particular  the cpds
of our dbn model  roughly speaking  each rule corresponds to some set of cpd entries
that are all associated with a particular probability value  these entries with the same
value are referred to as consistent contexts 
definition     let c   x  x    and c  dom c   we say that c is consistent with
b  dom b   for b   x  x     if c and b have the same assignment for the variables in
c  b 
the probability of these consistent contexts will be represented by probability rules 
   

figuestrin  koller  parr   venkataraman

electrical

electrical

done

not done

done

not done

plumbing

p painting     

plumbing

p painting     
not done

done

not done

painting

p painting     

done

not done

p painting     

p painting        

p painting     

 a 

done
p painting       

 b 

    helectrical    i
    helectrical  plumbing    i
    helectrical  plumbing  painting    i
    helectrical  plumbing  painting      i
 d 

    helectrical    i
    helectrical   plumbing    i
    helectrical  plumbing       i
 c 

figure     example cpds for variable the painting   true represented as decision trees 
 a  when the action is paint   b  when the action is not paint  the same cpds
can be represented by probability rules as shown in  c  and  d   respectively 

definition     a probability rule    hc   pi is a function     x  x              where the
context c  dom c  for c   x  x    and p          such that  x  x      p if  x  x    is
consistent with c and is equal to   otherwise 
in this case  it is convenient to require that the rules be mutually exclusive and exhaustive  so that each cpd entry is uniquely defined by its association with a single rule 
definition     a rule based conditional probability distribution  rule cpd  pa is a function pa     xi     x            composed of a set of probability rules                  m   whose
contexts are mutually exclusive and exhaustive  we define 
pa  x i   x    j  x  x    
where j is the unique rule in pa for which cj is consistent with  x i   x   we require that 
for all x 
x
pa  x i   x      
x i

we can define parentsa  xi    to be the union of the contexts of the rules in pa  xi    x   an
example of a cpd represented by a set of probability rules is shown in figure    
rules can also be used to represent additive functions  such as reward or basis functions 
we represent such context specific value dependencies using value rules 
   

fiefficient solution algorithms for factored mdps

definition     a value rule    hc   vi is a function    x   r such that  x    v when
x is consistent with c and   otherwise 
note that a value rule hc   vi has a scope c 
it is important to note that value rules are not required to be mutually exclusive and
exhaustive  each value rule represents a  weighted  indicator function  which takes on a
value v in states consistent with some context c  and   in all other states  in any given state 
the values of the zero or more rules consistent with that state are simply added together 
example     in our construction example  we might have a set of rules 
    hplumbing   done      i 
    helectricity   done      i 
    hpainting   done      i 
    haction   plumb     i 
  
 
which  when summed together  define the reward function r                      
in general  our reward function ra is represented as a rule based function 
definition     a rule based function f   x   r is composed of a set of rules              n  
p
such that f  x    ni   i  x  
in the same manner  each one of our basis functions hj is now represented as a rule based
function 
this notion of a rule based function is related to the tree structure functions used by
boutilier et al          but is substantially more general  in the tree structure value functions  the rules corresponding to the different leaves are mutually exclusive and exhaustive 
thus  the total number of different values represented in the tree is equal to the number
of leaves  or rules   in the rule based function representation  the rules are not mutually
exclusive  and their values are added to form the overall function value for different settings
of the variables  different rules are added in different settings  and  in fact  with k rules 
one can easily generate  k different possible values  as is demonstrated in section    thus 
the rule based functions can provide a compact representation for a much richer class of
value functions 
using this rule based representation  we can exploit both csi and additive independence
in the representation of our factored mdp and basis functions  we now show how the basic
operations in section   can be adapted to exploit our rule based representation 
    adding  multiplying and maximizing consistent rules
in our table based algorithms  we relied on standard sum and product operators applied to
tables  in order to exploit csi using a rule based representation  we must redefine these
standard operations  in particular  the algorithms will need to add or multiply rules that
ascribe values to overlapping sets of states 
we will start by defining these operations for rules with the same context 
   

figuestrin  koller  parr   venkataraman

definition     let     hc   v  i and     hc   v  i be two rules with context c  define the
rule product as        hc   v   v  i  and the rule sum as         hc   v    v  i 
note that this definition is restricted to rules with the same context  we will address this
issue in a moment  first  we will introduce an additional operation which maximizes a
variable from a set of rules  which otherwise share a common context 
definition     let y be a variable with dom y      y            yk    and let i   for each i  
           k  be a rule of the form i   hc  y   yi   vi i  then for the rule based function
f            k   define the rule maximization over y as maxy f   hc   maxi vi i  
after this operation  y has been maximized out from the scope of the function f  
these three operations we have just described can only be applied to sets of rules that
satisfy very stringent conditions  to make our set of rules amenable to the application
of these operations  we might need to refine some of these rules  we therefore define the
following operation 
definition     let    hc   vi be a rule  and y be a variable  define the rule split
split   y   of  on a variable y as follows  if y  scope c   then split   y        
otherwise 
split   y      hc  y   yi   vi   yi  dom y     
thus  if we split a rule  on variable y that is not in the scope of the context of   then we
generate a new set of rules  with one for each assignment in the domain of y  
in general  the purpose of rule splitting is to extend the context c of one rule  coincide
with the context c  of another consistent rule     naively  we might take all variables in
scope c     scope c  and split  recursively on each one of them  however  this process
creates unnecessarily many rules  if y is a variable in scope c     scope c  and we split 
on y   then only one of the  dom y    new rules generated will remain consistent with     the
one which has the same assignment for y as the one in c    thus  only this consistent rule
needs to be split further  we can now define the recursive splitting procedure that achieves
this more parsimonious representation 
definition      let    hc   vi be a rule  and b be a context such that b  dom b  
define the recursive rule split split   b  of  on a context b as follows 
       if c is not consistent with b  else 
       if scope b   scope c   else 
    split i   b    i  split   y     for some variable y  scope b   scope c   

in this definition  each variable y  scope b   scope c  leads to the generation of k  
 dom y    rules at the step in which it is split  however  only one of these k rules is used
in the next recursive step because only one is consistent with b  therefore  the size of the
p
split set is simply     y scope b scope c    dom y         this size is independent of the
order in which the variables are split within the operation 
   

fiefficient solution algorithms for factored mdps

note that only one of the rules in split   b  is consistent with b  the one with context
c  b  thus  if we want to add two consistent rules     hc    v  i and     hc    v  i  then
all we need to do is replace these rules by the set 
split     c     split     c    
and then simply replace the resulting rules hc   c    v  i and hc   c    v  i by their sum
hc   c    v    v  i  multiplication is performed in an analogous manner 
example      consider adding the following set of consistent rules 
    ha  b    i 
    ha  c  d    i 
in these rules  the context c  of   is a  b  and the context c  of   is a  c  d 
rules   and   are consistent  therefore  we must split them to perform the addition
operation 


 ha  b  c    i 
ha  b  c  d    i 
split     c     

 ha  b  c  d    i 
likewise 

 

split    

c     

ha  b  c  d    i 
ha  b  c  d    i 

the result of adding rules   and   is
ha  b  c    i 
ha  b  c  d    i 
ha  b  c  d    i 
ha  b  c  d    i 

    rule based one step lookahead
using this compact rule based representation  we are able to compute a one step lookahead
plan efficiently for models with significant context specific or additive independence 
as in section     for the table based case  the rule based qa function can be represented
as the sum of the reward function and the discounted expected value of the next state 
due to our linear approximation of the value function  the expectation term is  in turn 
represented as the linear combination of the backprojections of our basis functions  to
exploit csi  we are representing the rewards and basis functions as rule based functions 
to represent qa as a rule based function  it is sufficient for us to show how to represent the
backprojection gj of the basis function hj as a rule based function 
p  h  
each hj is a rule based
function 
which can be written as hj  x    i i j  x   where
d
e
 h  
 h  
 h  
i j has the form ci j   vi j   each rule is a restricted scope function  thus  we can
simplify the backprojection as 
   

figuestrin  koller  parr   venkataraman

rulebackproja     

where  is given by hc   vi  with c  dom c  

let g      
select the set p of relevant probability rules 
p    j  p  xi    parents xi       xi   c and c is consistent with cj   
remove the x  assignments from the context of all rules in p 
   multiply consistent rules 
while there are two consistent rules     hc    p  i and     hc    p  i 
if c    c    replace these two rules by hc    p  p  i 
else replace these two rules by the set  split     c     split     c    
   generate value rules 
for each rule i in p 
update the backprojection g   g   hci   pi vi  
return g 

figure     rule based backprojection 
gja  x   

x

pa  x    x hj  x     

x 

 

x

pa  x    x 

x 

 

xx
i

 

x  hj  

i

 x    

i
 hj  

 

pa  x   x i

 x    

x 

x  hj  

vi

 hj  

pa  ci

  x  

i
 h  

 h  

where the term vi j pa  ci j   x  can be written as a rule function  we denote this back h  
projection operation by rulebackproja  i j   
the backprojection procedure  described in figure     follows three steps  first  the
relevant rules are selected  in the cpds for the variables that appear in the context of  
we select the rules consistent with this context  as these are the only rules that play a role
in the backprojection computation  second  we multiply all consistent probability rules to
form a local set of mutually exclusive rules  this procedure is analogous to the addition
procedure described in section      now that we have represented the probabilities that
can affect  by a mutually exclusive set  we can simply represent the backprojection of 
by the product of these probabilities with the value of   that is  the backprojection of  is
a rule based function with one rule for each one of the mutually exclusive probability rules
i   the context of this new value rule is the same as that of i   and the value is the product
of the probability of i and the value of  
example      for example  consider the backprojection of a simple rule 
   h painting   done      i 
through the cpd in figure    c  for the paint action 
rulebackprojpaint     

x

ppaint  x    x  x    

x 

   

fiefficient solution algorithms for factored mdps

x

 

ppaint  painting    x  painting    

painting 

     

 
y

i  painting   done  x   

i  

note that the product of these simple rules is equivalent to the decision tree cpd shown in
figure    a   hence  this product is equal to   in most contexts  for example  when electricity
is not done at time t  the product in non zero only in one context  in the context associated
with rule     thus  we can express the result of the backprojection operation by a rule based
function with a single rule 
rulebackprojpaint      hplumbing  electrical     i 
similarly  the backprojection of  when the action is not paint can also be represented by a
single rule 
rulebackprojpaint      hplumbing  electrical  painting     i 
using this algorithm  we can now write the backprojection of the rule based basis function hj as 
gja  x   

x

 hj  

rulebackproja  i

  

    

i

where gja is a sum of rule based functions  and therefore also a rule based function  for
simplicity of notation  we use gja   rulebackproja  hj   to refer to this definition of backprop
jection  using this notation  we can write qa  x    ra  x     j wj gja  x   which is again a
rule based function 
    rule based maximization over the state space
the second key operation required to extend our planning algorithms to exploit csi is to
modify the variable elimination algorithm in section       to handle the rule based representation  in section        we showed that the maximization of a linear combination
of table based functions with restricted scope can be performed efficiently using non serial
dynamic programming  bertele   brioschi         or variable elimination  to exploit structure in rules  we use an algorithm similar to variable elimination in a bayesian network with
context specific independence  zhang   poole        
intuitively  the algorithm operates by selecting the value rules relevant to the variable
being maximized in the current iteration  then  a local maximization is performed over
this subset of the rules  generating a new set of rules without the current variable  the
procedure is then repeated recursively until all variables have been eliminated 
more precisely  our algorithm eliminates variables one by one  where the elimination process performs a maximization step over the variables domain  suppose that we
are eliminating xi   whose collected value rules lead to a rule function f   and f involves
additional variables in some set b  so that f s scope is b   xi    we need to compute
the maximum value for xi for each choice of b  dom b   we use maxout  f  xi   to denote a procedure that takes a rule function f  b  xi   and returns a rule function g b  such
   

figuestrin  koller  parr   venkataraman

maxout  f  b 
let g      
add completing rules to f   hb   bi    i  i              k 
   summing consistent rules 
while there are two consistent rules     hc    v  i and     hc    v  i 
if c    c    then replace these two rules by hc    v    v  i 
else replace these two rules by the set  split     c     split     c    
   maximizing out variable b 
repeat until f is empty 
if there are rules hc  b   bi   vi i  bi  dom b   
then remove these rules from f and add rule hc   maxi vi i to g 
else select two rules  i   hci  b   bi   vi i and j   hcj  b   bj   vj i
such that ci is consistent with cj   but not identical  and replace
them with split i   cj    split j   ci    
return g 

figure     maximizing out variable b from rule function f  
that  g b    maxxi f  b  xi    such a procedure is an extension of the variable elimination
algorithm of zhang and poole  zhang   poole        
the rule based variable elimination algorithm maintains a set f of value rules  initially
containing the set of rules to be maximized  the algorithm then repeats the following steps
for each variable xi until all variables have been eliminated 
   collect all rules which depend on xi into fi  fi    hc   vi  f   xi  c   and
remove these rules from f 
   perform the local maximization step over xi   gi   maxout  fi   xi   
   add the rules in gi to f  now  xi has been eliminated 
the cost of this algorithm is polynomial in the number of new rules generated in the
maximization operation maxout  fi   xi    the number of rules is never larger and in many
cases exponentially smaller than the complexity bounds on the table based maximization in
section        which  in turn  was exponential only in the induced width of the cost network
graph  dechter         however  the computational costs involved in managing sets of rules
usually imply that the computational advantage of the rule based approach over the tablebased one will only be significant in problems that possess a fair amount of context specific
structure 
in the remainder of this section  we present the algorithm for computing the local
maximization maxout  fi   xi    in the next section  we show how these ideas can be applied
to extending the algorithm in section       to exploit csi in the lp representation for
planning in factored mdps 
the procedure  presented in figure     is divided into two parts  first  all consistent
rules are added together as described in section      then  variable b is maximized  this
maximization is performed by generating a set of rules  one for each assignment of b  whose
contexts have the same assignment for all variables except for b  as in definition      this
set is then substituted by a single rule without a b assignment in its context and with value
equal to the maximum of the values of the rules in the original set  note that  to simplify
   

fiefficient solution algorithms for factored mdps

the algorithm  we initially need to add a set of value rules with   value  which guarantee
that our rule function f is complete  i e   there is at least one rule consistent with every
context  
the correctness of this procedure follows directly from the correctness of the rule based
variable elimination procedure described by zhang and poole  merely by replacing summations with product with max  and products with products with sums  we conclude this
section with a small example to illustrate the algorithm 
example      suppose we are maximizing a for the following set of rules 
 
 
 
 

  ha    i 
  ha  b    i 
  ha  b  c    i 
  ha  b    i 

when we add completing rules  we get 
    ha    i 
    ha    i 
in the first part of the algorithm  we need to add consistent rules  we add   to    which
remains unchanged   combine   with       with     and then the split of   on the context
of     to get the following inconsistent set of rules 
 
 
 
 
 

  ha  b    i 
  ha  b  c    i 
  ha  b    i 
 from adding   to the consistent rule from split     b  
  ha  b    i 
 from split     b  
  ha  b  c    i 
 from split     a  b  c   

note that several rules with value   are also generated  but not shown here because they are
added to other rules with consistent contexts  we can move to the second stage  repeat loop 
of maxout  we remove     and     and maximize a out of them  to give 
     hb    i 
we then select rules   and   and split   on c    is split on the empty set and is not
changed  
     ha  b  c    i 
     ha  b  c    i 
maximizing out a from rules    and     we get 
     hb  c    i 
we are left with      which maximized over its counterpart   gives
     hb  c    i 
notice that  throughout this maximization  we have not split on the variable c when b  ci  
giving us only   distinct rules in the final result  this is not possible in a table based
representation  since our functions would then be over the   variables a b c  and therefore
must have   entries 
   

figuestrin  koller  parr   venkataraman

    rule based factored lp
in section        we showed that the lps used in our algorithms have exponentially many
p
constraints of the form    i wi ci  x   b x   x  which can be substituted by a single 
p
equivalent  non linear constraint    maxx i wi ci  x   b x   we then showed that  using
variable elimination  we can represent this non linear constraint by an equivalent set of
linear constraints in a construction we called the factored lp  the number of constraints in
the factored lp is linear in the size of the largest table generated in the variable elimination
procedure  this table based algorithm can only exploit additive independence  we now
extend the algorithm in section       to exploit both additive and context specific structure 
by using the rule based variable elimination described in the previous section 
suppose we wish to enforce the more general constraint    maxy f w  y   where f w  y   
p w
j fj  y  such that each fj is a rule  as in the table based version  the superscript w means
that fj might depend on w  specifically  if fj comes from basis function hi   it is multiplied
by the weight wi   if fj is a rule from the reward function  it is not 
in our rule based factored linear program  we generate lp variables associated with
contexts  we call these lp rules  an lp rule has the form hc   ui  it is associated with a
context c and a variable u in the linear program  we begin by transforming all our original
rules fjw into lp rules as follows  if rule fj has the form hcj   vj i and comes from basis
function hi   we introduce an lp rule ej   hcj   uj i and the equality constraint uj   wi vj  
if fj has the same form but comes from a reward function  we introduce an lp rule of the
same form  but the equality constraint becomes uj   vj  
p
now  we have only lp rules and need to represent the constraint     maxy j ej  y  
to represent such a constraint  we follow an algorithm very similar to the variable elimination procedure in section      the main difference occurs in the maxout  f  b  operation in
figure     instead of generating new value rules  we generate new lp rules  with associated
new variables and new constraints  the simplest case occurs when computing a split or
adding two lp rules  for example  when we add two value rules in the original algorithm 
we instead perform the following operation on their associated lp rules  if the lp rules
are hc   ui i and hc   uj i  we replace these by a new rule hc   uk i  associated with a new lp
variable uk with context c  whose value should be ui   uj   to enforce this value constraint 
we simply add an additional constraint to the lp  uk   ui   uj   a similar procedure can
be followed when computing the split 
more interesting constraints are generated when we perform a maximization  in the
rule based variable elimination algorithm in figure     this maximization occurs when we
replace a set of rules 
hc  b   bi   vi i  bi  dom b  
by a new rule





c   max vi  
i

following the same process as in the lp rule summation above  if we are maximizing
ei   hc  b   bi   ui i  bi  dom b  
we generate a new lp variable uk associated with the rule ek   hc   uk i  however  we
cannot add the nonlinear constraint uk   maxi ui   but we can add a set of equivalent linear
   

fiefficient solution algorithms for factored mdps

constraints
uk  ui   i 
therefore  using these simple operations  we can exploit structure in the rule functions
p
to represent the nonlinear constraint en  maxy j ej  y   where en is the very last lp
rule we generate  a final constraint un    implies that we are representing exactly the
constraints in equation       without having to enumerate every state 
the correctness of our rule based factored lp construction is a corollary of theorem    
and of the correctness of the rule based variable elimination algorithm  zhang   poole 
       
corollary      the constraints generated by the rule based factored lp construction are
equivalent to the non linear constraint in equation       that is  an assignment to    w 
satisfies the rule based factored lp constraints if and only if it satisfies the constraint in
equation      
the number of variables and constraints in the rule based factored lp is linear in the
number of rules generated by the variable elimination process  in turn  the number of rules
is no larger  and often exponentially smaller  than the number of entries in the table based
approach 
to illustrate the generation of lp constraints as just described  we now present a small
example 
example      let e    e    e    and e  be the set of lp rules which depend on the variable
b being maximized  here  rule ei is associated with the lp variable ui  
e 
e 
e 
e 

  ha  b   u  i 
  ha  b  c   u  i 
  ha  b   u  i 
  ha  b  c   u  i 

in this set  note that rules e  and e  are consistent  we combine them to generate the
following rules 
e    ha  b  c   u  i 
e    ha  b  c   u  i 
and the constraint u    u    u    similarly  e  and e  may be combined  resulting in 
e    ha  b  c   u  i 
with the constraint u    u    u    now  we have the following three inconsistent rules for
the maximization 
e    ha  b   u  i 
e    ha  b  c   u  i 
e    ha  b  c   u  i 
following the maximization procedure  since no pair of rules can be eliminated right away 
we split e  and e  to generate the following rules 
e    ha  b  c   u  i 
e    ha  b  c   u  i 
e    ha  b  c   u  i 
   

figuestrin  koller  parr   venkataraman

we can now maximize b out from e  and e    resulting in the following rule and constraints
respectively 
e     ha  c   u  i 
u   u   
u   u   
likewise  maximizing b out from e  and e    we get 
e     ha  c   u  i 
u   u   
u   u   
which completes the elimination of variable b in our rule based factored lp 
we have presented an algorithm for exploiting both additive and context specific structure in the lp construction steps of our planning algorithms  this rule based factored lp
approach can now be applied directly in our approximate linear programming and approximate policy iteration algorithms  which were presented in sections   and   
the only additional modification required concerns the manipulation of the decision
list policies presented in section      although approximate linear programming does not
require any explicit policy representation  or the default action model   approximate policy iteration require us to represent such policy  fortunately  no major modifications are
required in the rule based case  in particular  the conditionals hti   ai   i i in the decision
list policies are already context specific rules  thus  the policy representation algorithm in
section     can be applied directly with our new rule based representation  therefore  we
now have a complete framework for exploiting both additive and context specific structure
for efficient planning in factored mdps 

   experimental results
the factored representation of a value function is most appropriate in certain types of
systems  systems that involve many variables  but where the strong interactions between
the variables are fairly sparse  so that the decoupling of the influence between variables
does not induce an unacceptable loss in accuracy  as argued by herbert simon       
in architecture of complexity  many complex systems have a nearly decomposable 
hierarchical structure  with the subsystems interacting only weakly between themselves  to
evaluate our algorithm  we selected problems that we believe exhibit this type of structure 
in this section  we perform various experiments intended to explore the performance
of our algorithms  first  we compare our factored approximate linear programming  lp 
and approximate policy iteration  pi  algorithms  we also compare to the l   projection
algorithm of koller and parr         our second evaluation compares a table based implementation to a rule based implementation that can exploit csi  finally  we present
comparisons between our approach and the algorithms of boutilier et al         
    approximate lp and approximate pi
in order to compare our approximate lp and approximate pi algorithms  we tested both on
the sysadmin problem described in detail in section      this problem relates to a system
   

fiefficient solution algorithms for factored mdps

administrator who has to maintain a network of computers  we experimented with various
network architectures  shown in figure    machines fail randomly  and a faulty machine
increases the probability that its neighboring machines will fail  at every time step  the
sysadmin can go to one machine and reboot it  causing it to be working in the next time
step with high probability  recall that the state space in this problem grows exponentially
in the number of machines in the network  that is  a problem with m machines has  m states 
each machine receives a reward of   when working  except in the ring  where one machine
receives a reward of    to introduce some asymmetry   a zero reward is given to faulty
machines  and the discount factor is          the optimal strategy for rebooting machines
will depend upon the topology  the discount factor  and the status of the machines in the
network  if machine i and machine j are both faulty  the benefit of rebooting i must be
weighed against the expected discounted impact of delaying rebooting j on js successors 
for topologies such as rings  this policy may be a function of the status of every single
machine in the network 
the basis functions used included independent indicators for each machine  with value
  if it is working and zero otherwise  i e   each one is a restricted scope function of a single
variable   and the constant basis  whose value is   for all states  we selected straightforward
variable elimination orders  for the star and three legs topologies  we first eliminated
the variables corresponding to computers in the legs  and the center computer  server  was
eliminated last  for ring  we started with an arbitrary computer and followed the ring
order  for ring and star  the ring machines were eliminated first and then the center one 
finally  for the ring of rings topology  we eliminated the computers in the outer rings
first and then the ones in the inner ring 
we implemented the factored policy iteration and linear programming algorithms in
matlab  using cplex as the lp solver  experiments were performed on a sun ultrasparcii      mhz with    mb of ram  to evaluate the complexity of the approximate policy
iteration with max norm projection algorithm  tests were performed with increasing the
number of states  that is  increasing number of machines on the network  figure    shows
the running time for increasing problem sizes  for various architectures  the simplest one
is the star  where the backprojection of each basis function has scope restricted to two
variables and the largest factor in the cost network has scope restricted to two variables 
the most difficult one was the bidirectional ring  where factors contain five variables 
note that the number of states is growing exponentially  indicated by the log scale in
figure      but running times increase only logarithmically in the number of states  or
polynomially in the number of variables  we illustrate this behavior in figure    d   where
we fit a  rd order polynomial to the running times for the unidirectional ring  note that
the size of the problem description grows quadratically with the number of variables  adding
a machine to the network also adds the possible action of fixing that machine  for this
problem 
the computation
cost of our factored algorithm empirically grows approximately


as o  n   a        for a problem with n variables  as opposed to the exponential complexity
 poly   n    a    of the explicit algorithm 
for further evaluation  we measured the error in our approximate value function relative
to the true optimal value function v    note that it is only possible to compute v  for small
problems  in our case  we were only able to go up to    machines  for comparison  we
also evaluated the error in the approximate value function produced by the l   projection
   

figuestrin  koller  parr   venkataraman

   

   

ring
  legs

   

ring of rings

   
total time  minutes 

total time  minutes 

   

star

   

ring and star
   

   

   

 

 
 e   

 e   

 e   

 e     e     e   
number of states

 e   

 

 e   

   

     
       
number of states

 a 
    

   

    

fitting a polynomial 

   

time          x           x   
       x          

 

ring 

total time  minutes 

total time  minutes 

 e   

 b 

   

   

         

unidirectional
bidirectional

   
   

 

 

quality of the fit  r        
   

   

   

   
 
 e   

 e   

 e   

 e   

 e   

 e   

 e   

 e   

 
 

number of state s

 c 

  

  
  
  
number of variables  x 

  

  

 d 

figure      a  c  running times for policy iteration with max norm projection on variants
of the sysadmin problem   d  fitting a polynomial to the running time for the
ring topology 

algorithm of koller and parr         as we discussed in section      the l  projections in
factored mdps by koller and parr are difficult and time consuming  hence  we were only
able to compare the two algorithms for smaller problems  where an equivalent l   projection
can be implemented using an explicit state space formulation  results for both algorithms
are presented in figure    a   showing the relative error of the approximate solutions to
the true value function for increasing problem sizes  the results indicate that  for larger
problems  the max norm formulation generates a better approximation of the true optimal
value function v  than the l   projection  here  we used two types of basis functions  the
same single variable functions  and pairwise basis functions  the pairwise basis functions
contain indicators for neighboring pairs of machines  i e   functions of two variables   as
expected  the use of pairwise basis functions resulted in better approximations 
   

fiefficient solution algorithms for factored mdps

   

max norm  single basis
l   single basis

   

bellman error   rmax

max norm  pair basis
l   pair basis

relative error 

   

   

 
 

 

 

 

 

 

 

  

number of variables

   

   

ring
  legs

   

 
 e   

star

 e   

 e   

 e   

 e   

 e   

 e   

 e   

numbe r of sta tes

 a 

 b 

figure      a  relative error to optimal value function v  and comparison to l  projection
for ring   b  for large models  measuring bellman error after convergence 

for these small problems  we can also compare the actual value of the policy generated
by our algorithm to the value of the optimal policy  here  the value of the policy generated
by our algorithm is much closer to the value of the optimal policy than the error implied by
the difference between our approximate value function and v    for example  for the star
architecture with one server and up to   clients  our approximation with single variable
basis functions had relative error of      but the policy we generated had the same value
as the optimal policy  in this case  the same was true for the policy generated by the l 
projection  in a unidirectional ring with   machines and pairwise basis  the relative
error between our approximation and v  was about      but the resulting policy only had
a    loss over the optimal policy  for the same problem  the l  approximation has a value
function error of      and a true policy loss was     in other words  both methods induce
policies that have lower errors than the errors in the approximate value function  at least
for small problems   however  our algorithm continues to outperform the l  algorithm 
even with respect to actual policy loss 
for large models  we can no longer compute the correct value function  so we cannot
evaluate our results by computing kv   hwk   fortunately  as discussed in section   
the bellman error can be used to provide a bound on the approximation error and can be
computed efficiently by exploiting problem specific structure  figure    b  shows that the
bellman error increases very slowly with the number of states 
it is also valuable to look at the actual decision list policies generated in our experiments 
first  we noted that the lists tended to be short  the length of the final decision list policy
grew approximately linearly with the number of machines  furthermore  the policy itself
is often fairly intuitive  in the ring and star architecture  for example  the decision list
says  if the server is faulty  fix the server  else  if another machine is faulty  fix it 
thus far  we have presented scaling results for running times and approximation error for
our approximate pi approach  we now compare this algorithm to the simpler approximate
   

figuestrin  koller  parr   venkataraman

   

   

pi single basis
pi single basis

   

lp single basis

   

lp pair basis

   

lp triple basis

discounted reward of final policy
 averaged over    trials of     steps 

total running time  minutes 

   

   
  
  
  
  
 
 

 

  

  

  

  

  

  

numbe r of machine s

lp single basis
lp pair basis

   

lp triple basis

   

   

 
 

  

  

  

  

numbe r of machine s

 a 

 b 

figure     approximate lp versus approximate pi on the sysadmin problem with a ring
topology   a  running time   b  estimated value of policy 

lp approach of section    as shown in figure    a   the approximate lp algorithm for
factored mdps is significantly faster than the approximate pi algorithm  in fact  approximate pi with single variable basis functions variables is more costly computationally than
the lp approach using basis functions over consecutive triples of variables  as shown in
figure    b   for singleton basis functions  the approximate pi policy obtains slightly better
performance for some problem sizes  however  as we increase the number of basis functions
for the approximate lp formulation  the value of the resulting policy is much better  thus 
in this problem  our factored approximate linear programming formulation allows us to use
more basis functions and to obtain a resulting policy of higher value  while still maintaining
a faster running time  these results  along with the simpler implementation  suggest that
in practice one may first try to apply the approximate linear programming algorithm before
deciding to move to the more elaborate approximate policy iteration approach 
    comparing table based and rule based implementations
our next evaluation compares a table based representation  which exploits only additive
independence  to the rule based representation presented in section    which can exploit
both additive and context specific independence  for these experiments  we implemented
our factored approximate linear programming algorithm with table based and rule based
representations in c    using cplex as the lp solver  experiments were performed on
a sun ultrasparc ii      mhz with  gb of ram 
to evaluate and compare the algorithms  we utilized a more complex extension of the
sysadmin problem  this problem  dubbed the process sysadmin problem  contains three
state variables for each machine i in the network  loadi   statusi and selectori   each computer runs processes and receives rewards when the processes terminate  these processes
are represented by the loadi variable  which takes values in  idle  loaded  success   and the
computer receives a reward when the assignment of loadi is success  the statusi variable 
   

fitotal running time  minutes 

efficient solution algorithms for factored mdps

   
table based  single  basis
rule based  single  basis

   

table based  pair basis
   

rule based  pair basis

  
 
 e   

 e   

 e   

 e   

 e   

 e   

 e   

number of states

total running time  minutes 

 a 
   
   

table based  single  basis
rule based  single  basis

   

table based  pair basis
   
rule based  pair basis
  
 
 e     e     e     e     e     e     e     e   
number of states

 b 

total running time  minutes 

   
 
 x   

 x   

y    e     x         e     x     
        
 
r        

   

table based  single  basis
rule based  single  basis

   
   

 

 

y         x         x  
      x         
r          

   
   
 
 

 

  
number of machines

  

  

 c 
figure     running time for process sysadmin problem for various topologies   a  star 
 b  ring   c  reverse star  with fit function  

   

figuestrin  koller  parr   venkataraman

cplex time   total time

 
   
table based  single  basis
   
rule based  single  basis
   
   
 
 

 

  

  

  

number of machines

figure     fraction of total running time spent in cplex for the process sysadmin problem with a ring topology 

representing the status of machine i  takes values in  good  faulty  dead   if its value is
faulty  then processes have a smaller probability of terminating and if its value is dead 
then any running process is lost and loadi becomes idle  the status of machine i can become faulty and eventually dead at random  however  if machine i receives a packet from
a dead machine  then the probability that statusi becomes faulty and then dead increases 
the selectori variable represents this communication by selecting one of the neighbors of i
uniformly at random at every time step  the sysadmin can select at most one computer
to reboot at every time step  if computer i is rebooted  then its status becomes good
with probability    but any running process is lost  i e   the loadi variable becomes idle 
thus  in this problem  the sysadmin must balance several conflicting goals  rebooting a
machine kills processes  but not rebooting a machine may cause cascading faults in network 
furthermore  the sysadmin can only choose one machine to reboot  which imposes the additional tradeoff of selecting only one of the  potentially many  faulty or dead machines in
the network to reboot 
we experimented with two types of basis functions  single  includes indicators over
all of the joint assignments of loadi   statusi and selectori   and pair which  in addition 
includes a set of indicators over statusi   statusj   and selectori   j  for each neighbor j
of machine i in the network  the discount factor was          the variable elimination
order eliminated all of the loadi variables first  and then followed the same patterns as in
the simple sysadmin problem  eliminating first statusi and then selectori when machine i
is eliminated 
figure    compares the running times for the table based implementation to the ones
for the rule based representation for three topologies  star  ring  and reverse star 
the reverse star topology reverses the direction of the influences in the star  rather
than the central machine influencing all machines in the topology  all machines influence
the central one  these three topologies demonstrate three different levels of csi  in the
   

fiefficient solution algorithms for factored mdps

star topology  the factors generated by variable elimination are small  thus  although the
running times are polynomial in the number of state variables for both methods  the tablebased representation is significantly faster than the rule based one  due to the overhead of
managing the rules  the ring topology illustrates an intermediate behavior  single 
basis functions induce relatively small variable elimination factors  thus the table based
approach is faster  however  with pair basis the factors are larger and the rule based
approach starts to demonstrate faster running times in larger problems  finally  the reverse star topology represents the worst case scenario for the table based approach  here 
the scope of the backprojection of a basis function for the central machine will involve all
computers in the network  as all machines can potentially influence the central one in the
next time step  thus  the size of the factors in the table based variable elimination approach are exponential in the number of machines in the network  which is illustrated by
the exponential growth in figure    c   the rule based approach can exploit the csi in this
problem  for example  the status of the central machine status  only depends on machine
j if the value selector is j  i e   if selector    j  by exploiting csi  we can solve the same
problem in polynomial time in the number of state variables  as seen in the second curve in
figure    c  
it is also instructive to compare the portion of the total running time spent in cplex
for the table based as compared to the rule based approach  figure    illustrates this
comparison  note that amount of time spent in cplex is significantly higher for the
table based approach  there are two reasons for this difference  first  due to csi  the lps
generated by the rule based approach are smaller than the table based ones  second  rulebased variable elimination is more complex than the table based one  due to the overhead
introduced by rule management  interestingly  the proportion of cplex time increases as
the problem size increases  indicating that the asymptotic complexity of the lp solution is
higher than that of variable elimination  thus suggesting that  for larger problems  additional
large scale lp optimization procedures  such as constraint generation  may be helpful 
    comparison to apricodd
the most closely related work to ours is a line of research that began with the work of
boutilier et al          in particular  the approximate apricodd algorithm of hoey et
al          which uses analytic decision diagrams  adds  to represent the value function
is a strong alternative approach for solving factored mdps  as discussed in detail in section     the apricodd algorithm can successfully exploit context specific structure in the
value function  by representing it with the set of mutually exclusive and exhaustive branches
of the add  on the other hand  our approach can exploit both additive and context specific
structure in the problem  by using a linear combination of non mutually exclusive rules  to
better understand this difference  we evaluated both our rule based approximate linear
programming algorithm and apricodd in two problems  linear and expon  designed by
boutilier et al         to illustrate respectively the best case and the worst case behavior
of their algorithm  in these experiments  we used the web distributed version of apricodd  hoey  st aubin  hu    boutilier         running it locally on a linux pentium iii
   mhz with  gb of ram 
   

figuestrin  koller  parr   venkataraman

   

rule based

  
  

 

 

y         x         x         x         
 

r         

  

apricodd
 

y         x         x
        

  

apricodd

   
time  in seconds 

time  in seconds 

  

x

 

x

y    e                             
r          

   
   

rule based
y        x         x   
      x        
r     

   

 

r         

 

 

 

 

  
  
  
  
number of variables

  

 

  

 

  

  

number of variables

 a 

 b 

figure     comparing apricodd to rule based approximate linear programming on the  a 
linear and  b  expon problems 

these two problems involve n binary variables x            xn and n deterministic actions
a            an   the reward is   when all variables xk are true  and is   otherwise  the problem
is discounted by a factor          the difference between the linear and the expon
problems is in the transition probabilities  in the linear problem  the action ak sets the
variable xk to true and makes all succeeding variables  xi for i   k  false  if the state space
of the linear problem is seen as a binary number  the optimal policy is to set repeatedly the
largest bit  xk variable  which has all preceding bits set to true  using an add  the optimal
value function for this problem can be represented in linear space  with n   leaves  boutilier
et al          this is the best case for apricodd  and the algorithm can compute this value
function quite efficiently  figure    a  compares the running time of apricodd to that of
one of our algorithms with indicator basis functions between pairs of consecutive variables 
note that both algorithms obtain the same policy in polynomial time in the number of
variables  however  in such structured problems  the efficient implementation of the add
package used in apricodd makes it faster in this problem 
on the other hand  the expon problem illustrates the worst case for apricodd  in this
problem  the action ak sets the variable xk to true  if all preceding variables  xi for i   k  are
true  and it makes all preceding variables false  if the state space is seen as a binary number 
the optimal policy goes through all binary numbers in sequence  by repeatedly setting the
largest bit  xk variable  which has all preceding bits set to true  due to discounting  the
n
optimal value function assigns a value of    j  to the jth binary number  so that the
value function contains exponentially many different values  using an add  the optimal
value function for this problem requires an exponential number of leaves  boutilier et al  
       which is illustrated by the exponential running time in figure    b   however 
the same value function can be approximated very compactly as a factored linear value
function using n     basis functions  an indicator over each variable xk and the constant
base  as shown in figure    b   using this representation  our factored approximate linear
programming algorithm computes the value function in polynomial time  furthermore  the
   

fiefficient solution algorithms for factored mdps

  

  

running time  minutes 

discounted value of policy
 avg     runs of     steps 

rule based lp

  

apricodd
  
  
  
  
 

rule based lp

  

apricodd
  
  
  
 
 

 

 

 
 
 
number of machines

  

  

 

 

 
 
 
number of machines

 a 

  

  

  

 b 

  

  

  

rule based lp

rule based lp

  

discounted value of policy
 avg     runs of     steps 

running time  minutes 

  

apricodd

  
  
  
  
  
  

  

apricodd

  
  
  
 

 
 

 
 

 

 
 
 
number of machines

  

  

 c 

 

 

 
 
 
number of machines

 d 

figure     comparing apricodd to rule based approximate linear programming with single  basis functions on the process sysadmin problem with ring topology
 a  running time and  b  value of the resulting policy  and with star topology
 c  running time and  d  value of the resulting policy 

policy obtained by our approach was optimal for this problem  thus  in this problem  the
ability to exploit additive independence allows an efficient polynomial time solution 
we have also compared apricodd to our rule based approximate linear programming
algorithm on the process sysadmin problem  this problem has significant additive structure in the reward function and factorization in the transition model  although this type of
structure is not exploited directly by apricodd  the add approximation steps performed by
the algorithm can  in principle  allow apricodd to find approximate solutions to the problem  we spent a significant amount of time attempting to find the best set of parameters
for apricodd for these problems   we settled on the sift method of variable reordering
and the round approximation method with the size  maximum add size  criteria  to
   we are very grateful to jesse hoey and robert st aubin for their assistance in selecting the parameters 

   

figuestrin  koller  parr   venkataraman

allow the value function representation to scale with the problem size  we set the maximum
add size to           n for a network with n machines   we experimented with a variety
of different growth rates for the maximum add size  here  as for the other parameters 
we selected the choice that gave the best results for apricodd   we compared apricodd
with these parameters to our rule based approximate linear programming algorithm with
single  basis functions on a pentium iii    mhz with  gb of ram  these results are
summarized in figure    
on very small problems  up to    machines   the performance of the two algorithms is
fairly similar in terms of both the running time and the quality of the policies generated 
however  as the problem size grows  the running time of apricodd increases rapidly  and
becomes significantly higher than that of our algorithm   furthermore  as the problem size
increases  the quality of the policies generated by apricodd also deteriorates  this difference
in policy quality is caused by the different value function representation used by the two
algorithms  the adds used in apricodd represent k different values with k leaves  thus 
they are forced to agglomerate many different states and represent them using a single value 
for smaller problems  such agglomeration can still represent good policies  unfortunately 
as the problem size increases and the state space grows exponentially  apricodds policy
representation becomes inadequate  and the quality of the policies decreases  on the other
hand  our linear value functions can represent exponentially many values with only k basis
functions  which allows our approach to scale up to significantly larger problems 

    related work
the most closely related work to ours is a line of research that began with the work of
boutilier et al          we address this comparison separately below  but we begin this
section with some broader background references 
     approximate mdp solutions
the field of mdps  as it is popularly known  was formalized by bellman        in the
    s  the importance of value function approximation was recognized at an early stage
by bellman himself         in the early     s the mdp framework was recognized by ai
researchers as a formal framework that could be used to address the problem of planning
under uncertainty  dean  kaelbling  kirman    nicholson        
within the ai community  value function approximation developed concomitantly with
the notion of value function representations for markov chains  suttons seminal paper on
temporal difference learning         which addressed the use of value functions for prediction
but not planning  assumed a very general representation of the value function and noted
the connection to general function approximators such as neural networks  however  the
stability of this combination was not directly addressed at that time 
several important developments gave the ai community deeper insight into the relationship between function approximation and dynamic programming  tsitsiklis and van
roy      a  and  independently  gordon        popularized the analysis of approximate
mdp methods via the contraction properties of the dynamic programming operator and
function approximator  tsitsiklis and van roy      b  later established a general convergence result for linear value function approximators and t d    and bertsekas and
   

fiefficient solution algorithms for factored mdps

tsitsiklis        unified a large body of work on approximate dynamic programming under
the name of neuro dynamic programming  also providing many novel and general error
analyses 
approximate linear programming for mdps using linear value function approximation
was introduced by schweitzer and seidmann         although the approach was somewhat
deprecated until fairly recently due the lack of compelling error analyses and the lack of an
effective method for handling the large number of constraints  recent work by de farias
and van roy      a      b  has started to address these concerns with new error bounds
and constraint sampling methods  our approach  rather than sampling constraints  utilizes
structure in the model and value function to represent all of the constraints compactly 
     factored approaches
tatman and shachter        considered the additive decomposition of value nodes in influence diagrams  a number of approaches to factoring of general mdps have been explored in
the literature  techniques for exploiting reward functions that decompose additively were
studied by meuleau et al          and by singh and cohn        
the use of factored representations such as dynamic bayesian networks was pioneered
by boutilier et al         and has developed steadily in recent years  these methods rely
on the use of context specific structures such as decision trees or analytic decision diagrams
 adds   hoey et al         to represent both the transition dynamics of the dbn and
the value function  the algorithms use dynamic programming to partition the state space 
representing the partition using a tree like structure that branches on state variables and
assigns values at the leaves  the tree is grown dynamically as part of the dynamic programming process and the algorithm creates new leaves as needed  a leaf is split by the
application of a dp operator when two states associated with that leaf turn out to have
different values in the backprojected value function  this process can also be interpreted
as a form of model minimization  dean   givan        
the number of leaves in a tree used to represent a value function determines the computational complexity of the algorithm  it also limits the number of distinct values that can
be assigned to states  since the leaves represent a partitioning of the state space  every state
maps to exactly one leaf  however  as was recognized early on  there are trivial mdps which
require exponentially large value functions  this observation led to a line of approximation
algorithms aimed at limiting the tree size  boutilier   dearden        and  later  limiting
the add size  st aubin  hoey    boutilier         kim and dean        also explored
techniques for discovering tree structured value functions for factored mdps  while these
methods permit good approximate solutions to some large mdps  their complexity is still
determined by the number of leaves in the representation and the number of distinct values
than can be assigned to states is still limited as well 
tadepalli and ok        were the first to apply linear value function approximation
to factored mdps  linear value function approximation is a potentially more expressive
approximation method because it can assign unique values to every state in an mdp without
requiring storage space that is exponential in the number of state variables  the expressive
power of a tree with k leaves can be captured by a linear function approximator with k basis
functions such that basis function hi is an indicator function that tests if a state belongs
   

figuestrin  koller  parr   venkataraman

in the partition of leaf i  thus  the set of value functions that can be represented by a
tree with k leaves is a subset of the set of value functions that can be represented by a
value function with k basis functions  our experimental results in section     highlight this
difference by showing an example problem that requires exponentially many leaves in the
value function  but that can be approximated well using a linear value function 
the main advantage of tree based value functions is that their structure is determined
dynamically during the solution of the mdp  in principle  as the value function representation is derived automatically from the model description  this approach requires less insight
from the user  in problems for which the value function can be well approximated by a relatively small number of values  this approach provides an excellent solution to the problem 
our method of linear value function approximation aims to address what we believe to be
the more common case  where a large range of distinct values is required to achieve a good
approximation 
finally  we note that schuurmans and patrascu         based on our earlier work on
max norm projection using cost networks and linear programs  independently developed
an alternative approach to approximate linear programming using a cost network  our
method embeds a cost network inside a single linear program  by contrast  their method
is based on a constraint generation approach  using a cost network to detect constraint
violations  when constraint violations are found  a new constraint is added  repeatedly
generating and attempting to solve lps until a feasible solution is found  interestingly 
as the approach of schuurmans and patrascu uses multiple calls to variable elimination in
order to speed up the lp solution step  it will be most successful when the time spent
solving the lp is significantly larger than the time required for variable elimination  as
suggested in section      the lp solution time is larger for the table based approach  thus 
schuurmans and patrascus constraint generation method will probably be more successful
in table based problems than in rule based ones 

    conclusions
in this paper  we presented new algorithms for approximate linear programming and approximate dynamic programming  value and policy iteration  for factored mdps  both
of these algorithms leverage on a novel lp decomposition technique  analogous to variable elimination in cost networks  which reduces an exponentially large lp to a provably
equivalent  polynomial sized one 
our approximate dynamic programming algorithms are motivated by error analyses
showing the importance of minimizing l error  these algorithms are more efficient and
substantially easier to implement than previous algorithms based on the l   projection  our
experimental results suggest that they also perform better in practice 
our approximate linear programming algorithm for factored mdps is simpler  easier to
implement and more general than the dynamic programming approaches  unlike our policy
iteration algorithm  it does not rely on the default action assumption  which states that
actions only affect a small number of state variables  although this algorithm does not have
the same theoretical guarantees as max norm projection approaches  empirically it seems to
be a favorable option  our experiments suggest that approximate policy iteration tends to
generate better policies for the same set of basis functions  however  due to the computa   

fiefficient solution algorithms for factored mdps

tional advantages  we can add more basis functions to the approximate linear programming
algorithm  obtaining a better policy and still maintaining a much faster running time than
approximate policy iteration 
unlike previous approaches  our algorithms can exploit both additive and contextspecific structure in the factored mdp model  typical real world systems possess both
of these types of structure  thus  this feature of our algorithms will increase the applicability of factored mdps to more practical problems  we demonstrated that exploiting
context specific independence  by using a rule based representation instead of the standard
table based one  can yield exponential improvements in computational time when the problem has significant amounts of csi  however  the overhead of managing sets of rules make
it less well suited for simpler problems  we also compared our approach to the work of
boutilier et al          which exploits only context specific structure  for problems with
significant context specific structure in the value function  their approach can be faster due
to their efficient handling of the add representation  however  there are problems with
significant context specific structure in the problem representation  rather than in the value
function  which require exponentially large adds  in some such problems  we demonstrated that by using a linear value function our algorithm can obtain a polynomial time
near optimal approximation of the true value function 
the success of our algorithm depends on our ability to capture the most important
structure in the value function using a linear  factored approximation  this ability  in turn 
depends on the choice of the basis functions and on the properties of the domain  the
algorithms currently require the designer to specify the factored basis functions  this is a
limitation compared to the algorithms of boutilier et al          which are fully automated 
however  our experiments suggest that a few simple rules can be quite successful for designing a basis  first  we ensure that the reward function is representable by our basis  a
simple basis that  in addition  contained a separate set of indicators for each variable often
did quite well  we can also add indicators over pairs of variables  most simply  we can choose
these according to the dbn transition model  where an indicator is added between variables
xi and each one of the variables in parents xi    thus representing one step influences  this
procedure can be extended  adding more basis functions to represent more influences as
required  thus  the structure of the dbn gives us indications of how to choose the basis
functions  other sources of prior knowledge can also be included for further specifying the
basis 
nonetheless  a general algorithm for choosing good factored basis functions still does
not exist  however  there are some potential approaches  first  in problems with csi  one
could apply the algorithms of boutilier et al  for a few iterations to generate partial treestructured solutions  indicators defined over the variables in backprojection of the leaves
could  in turn  be used to generate a basis set for such problems  second  the bellman
error computation  which can be performed efficiently as shown in section    does not only
provide a bound on the quality of the policy  but also the actual state where the error is
largest  this knowledge can be used to create a mechanism to incrementally increase the
basis set  adding new basis functions to tackle states with high bellman error 
there are many other possible extensions to this work  we have already pursued extensions to collaborative multiagent systems  where multiple agents act simultaneously to
maximize the global reward  guestrin et al       b   and factored pomdps  where the
   

figuestrin  koller  parr   venkataraman

full state is not observed directly  but indirectly through observation variables  guestrin 
koller    parr      c   however  there are other settings that remain to be explored  in
particular  we hope to address the problem of learning a factored mdp and planning in a
competitive multiagent system 
additionally  in this paper we have tackled problems where the induced width of the cost
network is sufficiently low or that possess sufficient context specific structure to allow for
the exact solution of our factored lps  unfortunately  some practical problems may have
prohibitively large induced width  we plan to leverage on ideas from loopy belief propagation algorithms for approximate inference in bayesian networks  pearl        yedidia 
freeman    weiss        to address this issue 
we believe that the methods described herein significantly further extend the efficiency 
applicability and general usability of factored models and value functions for the control of
practical dynamic systems 

acknowledgements
we are very grateful to craig boutilier  dirk ormoneit and uri lerner for many useful
discussions  and to the anonymous reviewers for their detailed and thorough comments  we
also would like to thank jesse hoey  robert st aubin  alan hu  and craig boutilier for
distributing their algorithm and for their very useful assistance in using apricodd and in
selecting its parameters  this work was supported by the dod muri program  administered by the office of naval research under grant n                 by air force contract
f                under darpas task program  and by the sloan foundation  the first
author was also supported by a siebel scholarship 

appendix a  proofs
a   proof of lemma    
there exists a setting to the weights  the all zero setting  that yields a bounded maxnorm projection error p for any policy  p  rmax    our max norm projection operator
chooses the set of weights that minimizes the projection error   t  for each policy   t    thus 
the projection error   t  must be at least as low as the one given by the zero weights p
 which is bounded   thus  the error remains bounded for all iterations 
a   proof of theorem    
first  we need to bound our approximation of v t   




v t   hw t  












 t t  hw t   hw t  




  v t   t t  hw t  








 t 
 t  
 t t  hw  hw     v t   hw t  






   triangle inequality  

   t t  is a contraction  

moving the second term to the right hand side and dividing through by      we obtain 




v t   hw t  






  
  t 


 
t t  hw t   hw t    

 
 
   

    

fiefficient solution algorithms for factored mdps

for the next part of the proof  we adapt a lemma of bertsekas and tsitsiklis        lemma
     p      to fit into our framework  after some manipulation  this lemma can be reformulated as 
kv   v t    k   kv   v t  k  


  


v t   hw t    

 

    

the proof is concluded by substituting equation      into equation      and  finally  induction on t 
a   proof of theorem    
first  note that the equality constraints represent a simple change of variable  thus  we
can rewrite equation      in terms of these new lp variables ufzii as 
  max

x

x

ufzii  

    

i

where any assignment to the weights w implies an assignment for each ufzii   after this stage 
we only have lp variables 
it remains to show that the factored lp construction is equivalent to the constraint in
equation       for a system with n variables  x            xn    we assume  without loss of
generality  that variables are eliminated starting from xn down to x    we now prove the
equivalence by induction on the number of variables 
the base case is n      so that the functions ci  x  and b x  in equation      all have
empty scope  in this case  equation      can be written as 


x

uei  

    

i

in this case  no transformation is done on the constraint  and equivalence is immediate 
now  we assume the result holds for systems with i  variables and prove the equivalence
for a system with i variables  in such a system  the maximization can be decomposed into
two terms  one with the factors that do not depend on xi   which are irrelevant to the
maximization over xi   and another term with all the factors that depend on xi   using this
decomposition  we can write equation      as 
 



max

x ej

x       xi

uzj  

j

max 

x       xi 




x

x

uezll   max
xi

l   xi  zl

e
uzjj   

    

j   xi zj

at this point we can define new lp variables uez corresponding to the second term on
the right hand side of the constraint  these new lp variables must satisfy the following
constraint 
uez  max
xi

x ej
j  

   

u z xi   zj    

    

figuestrin  koller  parr   venkataraman

this new non linear constraint is again represented in the factored lp construction by a
set of equivalent linear constraints 
uez 

x ej
j  

u z xi   zj     z  xi  

    

the equivalence between the non linear constraint equation      and the set of linear constraints in equation      can be shown by considering binding constraints  for each new
lp variable created uez   there are  xi   new constraints created  one for each value xi of xi  
for any assignment to the lp variables in the right hand side of the constraint in equap
ej
tion       only one of these  xi   constraints is relevant  that is  one where  j   u z x
i   zj  
is maximal  which corresponds to the maximum over xi   again  if for each value of z more
than one assignment to xi achieves the maximum  then any of  and only  the constraints
corresponding to those maximizing assignments could be binding  thus  equation      and
equation      are equivalent 
substituting the new lp variables uez into equation       we get 


max

x       xi 

x

uezll   uez  

l   xi  zl

which does not depend on xi anymore  thus  it is equivalent to a system with i  variables 
concluding the induction step and the proof 
a   proof of lemma    
first note that at iteration t     the objective function  t    of the max norm projection
lp is given by 








 t      hw t     r t      p t    hw t      


however  by convergence the value function estimates are equal for both iterations 
w t      w t   
so we have that 








 t      hw t   r t      p t    hw t    


in operator notation  this term is equivalent to 






 t      hw t   t t    hw t    


note that    t      greedy hw t    by definition  thus  we have that 
t t    hw t    t  hw t   
finally  substituting into the previous expression  we obtain the result 






 t      hw t   t  hw t    


   

fiefficient solution algorithms for factored mdps

references
arnborg  s   corneil  d  g     proskurowski  a          complexity of finding embeddings
in a k tree  siam journal of algebraic and discrete methods                  
becker  a     geiger  d          a sufficiently fast algorithm for finding close to optimal
clique trees  artificial intelligence                 
bellman  r   kalaba  r     kotkin  b          polynomial approximation  a new computational technique in dynamic programming  math  comp                  
bellman  r  e          dynamic programming  princeton university press  princeton  new
jersey 
bertele  u     brioschi  f          nonserial dynamic programming  academic press  new
york 
bertsekas  d     tsitsiklis  j          neuro dynamic programming  athena scientific 
belmont  massachusetts 
boutilier  c   dean  t     hanks  s          decision theoretic planning  structural assumptions and computational leverage  journal of artificial intelligence research        
   
boutilier  c     dearden  r          approximating value trees in structured dynamic
programming  in proc  icml  pp       
boutilier  c   dearden  r     goldszmidt  m          exploiting structure in policy construction  in proc  ijcai  pp           
boutilier  c   dearden  r     goldszmidt  m          stochastic dynamic programming
with factored representations  artificial intelligence                   
cheney  e  w          approximation theory   nd edition   chelsea publishing co   new
york  ny 
de farias  d     van roy  b       a   the linear programming approach to approximate
dynamic programming  submitted to operations research 
de farias  d     van roy  b       b   on constraint sampling for the linear programming approach to approximate dynamic programming  to appear in mathematics of
operations research 
dean  t   kaelbling  l  p   kirman  j     nicholson  a          planning with deadlines in
stochastic domains  in proceedings of the eleventh national conference on artificial
intelligence  aaai      pp          washington  d c  aaai press 
dean  t     kanazawa  k          a model for reasoning about persistence and causation 
computational intelligence                
dean  t     givan  r          model minimization in markov decision processes  in
proceedings of the fourteenth national conference on artificial intelligence  aaai     pp          providence  rhode island  oregon  aaai press 
dearden  r     boutilier  c          abstraction and approximate decision theoretic planning  artificial intelligence                 
   

figuestrin  koller  parr   venkataraman

dechter  r          bucket elimination  a unifying framework for reasoning  artificial
intelligence                 
gordon  g          stable function approximation in dynamic programming  in proceedings
of the twelfth international conference on machine learning  pp          tahoe
city  ca  morgan kaufmann 
guestrin  c  e   koller  d     parr  r       a   max norm projections for factored mdps 
in proceedings of the seventeenth international joint conference on artificial intelligence  ijcai      pp            seattle  washington  morgan kaufmann 
guestrin  c  e   koller  d     parr  r       b   multiagent planning with factored mdps 
in   th neural information processing systems  nips      pp            vancouver 
canada 
guestrin  c  e   koller  d     parr  r       c   solving factored pomdps with linear value
functions  in seventeenth international joint conference on artificial intelligence
 ijcai     workshop on planning under uncertainty and incomplete information 
pp          seattle  washington 
guestrin  c  e   venkataraman  s     koller  d          context specific multiagent coordination and planning with factored mdps  in the eighteenth national conference
on artificial intelligence  aaai        pp          edmonton  canada 
hoey  j   st aubin  r   hu  a     boutilier  c          spudd  stochastic planning using decision diagrams  in proceedings of the fifteenth conference on uncertainty in
artificial intelligence  uai      pp          stockholm  sweden  morgan kaufmann 
hoey  j   st aubin  r   hu  a     boutilier  c          stochastic planning using decision
diagrams  c implementation  http   www cs ubc ca spider staubin spudd  
howard  r  a     matheson  j  e          influence diagrams  in howard  r  a     matheson  j  e   eds    readings on the principles and applications of decision analysis 
pp          strategic decisions group  menlo park  california 
keeney  r  l     raiffa  h          decisions with multiple objectives  preferences and
value tradeoffs  wiley  new york 
kim  k  e     dean  t          solving factored mdps using non homogeneous partitioning  in proceedings of the seventeenth international joint conference on artificial
intelligence  ijcai      pp            seattle  washington  morgan kaufmann 
kjaerulff  u          triangulation of graphs  algorithms giving small total state space 
tech  rep  tr r        department of mathematics and computer science  strandvejen  aalborg  denmark 
koller  d     parr  r          computing factored value functions for policies in structured
mdps  in proceedings of the sixteenth international joint conference on artificial
intelligence  ijcai      pp              morgan kaufmann 
koller  d     parr  r          policy iteration for factored mdps  in proceedings of the
sixteenth conference on uncertainty in artificial intelligence  uai      pp      
     stanford  california  morgan kaufmann 
   

fiefficient solution algorithms for factored mdps

meuleau  n   hauskrecht  m   kim  k   peshkin  l   kaelbling  l   dean  t     boutilier  c 
        solving very large weakly coupled markov decision processes  in proceedings
of the   th national conference on artificial intelligence  pp          madison  wi 
pearl  j          probabilistic reasoning in intelligent systems  networks of plausible inference  morgan kaufmann  san mateo  california 
puterman  m  l          markov decision processes  discrete stochastic dynamic programming  wiley  new york 
reed  b          finding approximate separators and computing tree width quickly  in
  th annual symposium on theory of computing  pp          acm 
schuurmans  d     patrascu  r          direct value approximation for factored mdps 
in advances in neural information processing systems  nips      pp           
vancouver  canada 
schweitzer  p     seidmann  a          generalized polynomial approximations in markovian decision processes  journal of mathematical analysis and applications          
     
simon  h  a          the sciences of the artificial  second edition   mit press  cambridge 
massachusetts 
singh  s     cohn  d          how to dynamically merge markov decision processes  in
jordan  m  i   kearns  m  j     solla  s  a   eds    advances in neural information
processing systems  vol      the mit press 
st aubin  r   hoey  j     boutilier  c          apricodd  approximate policy construction using decision diagrams  in advances in neural information processing systems
    proceedings of the      conference  pp            denver  colorado  mit press 
stiefel  e          note on jordan elimination  linear programming and tchebycheff approximation  numerische mathematik           
sutton  r  s          learning to predict by the methods of temporal differences  machine
learning         
tadepalli  p     ok  d          scaling up average reward reinforcmeent learning by approximating the domain models and the value function  in proceedings of the thirteenth
international conference on machine learning  bari  italy  morgan kaufmann 
tatman  j  a     shachter  r  d          dynamic programming and influence diagrams 
ieee transactions on systems  man and cybernetics                 
tsitsiklis  j  n     van roy  b       a   feature based methods for large scale dynamic
programming  machine learning           
tsitsiklis  j  n     van roy  b       b   an analysis of temporal difference learning with
function approximation  technical report lids p       laboratory for information
and decision systems  massachusetts institute of technology 
van roy  b          learning and value function approximation in complex decision
processes  ph d  thesis  massachusetts institute of technology 
   

figuestrin  koller  parr   venkataraman

williams  r  j     baird  l  c  i          tight performance bounds on greedy policies based
on imperfect value functions  tech  rep   college of computer science  northeastern
university  boston  massachusetts 
yedidia  j   freeman  w     weiss  y          generalized belief propagation  in advances
in neural information processing systems     proceedings of the      conference 
pp          denver  colorado  mit press 
zhang  n     poole  d          on the role of context specific independence in probabilistic
reasoning  in proceedings of the sixteenth international joint conference on artificial
intelligence  ijcai      pp            morgan kaufmann 

   

fi