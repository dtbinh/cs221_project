journal of artificial intelligence research                  

submitted        published     

backbone fragility and the local search cost peak
josh singer

joshuas dai ed ac uk

division of informatics  university of edinburgh
   south bridge  edinburgh eh   hn  united kingdom

ian p  gent

ipg dcs st and ac uk

school of computer science  university of st  andrews
north haugh  st  andrews  fife ky    ss  united kingdom

alan smaill

a smaill ed ac uk

division of informatics  university of edinburgh
   south bridge  edinburgh eh   hn  united kingdom

abstract
the local search algorithm wsat is one of the most successful algorithms for solving
the satisfiability  sat  problem  it is notably effective at solving hard random   sat
instances near the so called satisfiability threshold  but still shows a peak in search cost
near the threshold and large variations in cost over different instances  we make a number
of significant contributions to the analysis of wsat on high cost random instances  using
the recently introduced concept of the backbone of a sat instance  the backbone is the set
of literals which are entailed by an instance  we find that the number of solutions predicts
the cost well for small backbone instances but is much less relevant for the large backbone
instances which appear near the threshold and dominate in the overconstrained region 
we show a very strong correlation between search cost and the hamming distance to the
nearest solution early in wsats search  this pattern leads us to introduce a measure of the
backbone fragility of an instance  which indicates how persistent the backbone is as clauses
are removed  we propose that high cost random instances for local search are those with
very large backbones which are also backbone fragile  we suggest that the decay in cost
beyond the satisfiability threshold is due to increasing backbone robustness  the opposite
of backbone fragility   our hypothesis makes three correct predictions  first  that the
backbone robustness of an instance is negatively correlated with the local search cost when
other factors are controlled for  second  that backbone minimal instances  which are   sat
instances altered so as to be more backbone fragile  are unusually hard for wsat  third 
that the clauses most often unsatisfied during search are those whose deletion has the most
effect on the backbone  in understanding the pathologies of local search methods  we hope
to contribute to the development of new and better techniques 

   introduction
why do some problem instances require such a high computational cost for algorithms to
solve  answering this question will help us to understand the interaction between search
algorithms and problem instance structure and can potentially suggest principled improvements  for example the minimise kappa heuristic  gent  macintyre  prosser    walsh 
      walsh        
in this paper we study the propositional satisfiability problem  sat   sat is important
as it was the first and is perhaps the archetypal np complete problem  furthermore  many
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fisinger  gent   smaill

ai tasks of practical interest such as constraint satisfaction  planning and timetabling can
be naturally encoded as sat instances 
a sat instance c is a propositional formula in conjunctive normal form  c is a bag
of m clauses which represents their conjunction  a clause is a disjunction of literals  which
are boolean variables or their negations  the variables constitute a set of n symbols v   an
assignment is a mapping from v to  true  false   the decision question for sat asks whether
there exists an assignment which makes c true under the standard logical interpretation
of the connectives  such an assignment is a solution of the instance  if there is a solution 
the sat instance is said to be satisfiable  in this study  assignments where a few clauses
are unsatisfied are also important  we refer to these as quasi solutions  k sat is the sat
problem restricted to clauses containing k literals  notably  the k sat decision problem
is np hard for k     cook         in several np hard decision problems  such as  sat  certain probabilistic distributions of instances parameterised by a control parameter
exhibit a sharp threshold or phase transition in the probability of there being a solution
 cheeseman  kanefsky    taylor        gent et al         mitchell  selman    levesque 
       there is a critical value of the control parameter such that instances generated
with the parameter in the region lower than the critical value  the underconstrained region 
almost always have solutions  those generated from the overconstrained region where the
control parameter is higher than the critical value almost always have no solutions 
in many problem distributions  this threshold is associated with a peak in search cost
for a wide range of algorithms  instances generated from the distribution with the control
parameter near the critical value are hardest and cost decays as we move from this value
to lower or higher values  this behaviour is of interest to basic ai research  being devoid
of any regularities  random instances represent the challenge faced by an algorithm in the
absence of any assumptions about the problem domain  or once all knowledge about it has
been exploited in the design of the algorithm or transformation of the problem instance 
random k sat is a parameterised distribution of k sat instances  in random k sat 
n is fixed and the control parameter is m n  varying m n produces a sharp threshold in the
probability of satisfiability and an associated cost peak for a range of complete algorithms
 crawford   auton        larrabee   tsuji         the cost peak pattern in random
k sat has been conjectured to extend to all reasonable complete methods by cook and
mitchell         who also give an overview of analytic and experimental results on the
average case hardness of sat distributions 
in this paper we study the behaviour of local search on random k sat  the term local
search encompasses a class of procedures in which a series of assignments are examined with
the objective of finding a solution  the first assignment is typically selected at random 
local search then proceeds by moving from one assignment to another by flipping  i e 
inverting  the truth value of a single variable  the variable to flip is chosen using a heuristic
which may include randomness  an element of hill climbing  for example on the number of
satisfied clauses  and memory  usually  local search is incomplete for the sat decision
problem  there is no guarantee that if a solution exists  it will be found within any time
bound  unlike complete procedures  local search cannot usually prove for certain that no
solution exists 
it is a relatively recent discovery  e g  selman  levesque and mitchell        that the
average cost for local search procedures scales much better than that of the best complete
   

fibackbone fragility and the local search cost peak

procedures at the critical value of m n in random   sat  more recent studies   e g  parkes
and walser        have confirmed this in detail  therefore in any system where completeness
may be sacrificed  local search procedures have an important role to play  and this is why
they have generated so much interest in recent years 
if we restrict ourselves to those instances of the distribution which are satisfiable and
increase the control parameter  there is a peak in the cost for local search procedures to
solve the instances near the critical value in several constraint like problems  clark  frank 
gent  macintyre  tomov    walsh        hogg   williams         in the underconstrained
region  the average cost increases with m n due to the decreasing number of solutions per
instance  clark et al          however  in the overconstrained region  the cost decreases with
m n although the number of solutions per instance continues to fall  several researchers
have noted this fact with surprise  clark et al         parkes        yokoo        since the
number of solutions does not change in any special way near the critical value  why  then 
should the cost of satisfiable instances peak near the critical value  and then decay 
parkes        provided an appealing answer to the first part of this question in his study
of the backbone of satisfiable random   sat instances  for satisfiable sat instances  the
backbone is the set of literals which are logically entailed by the clauses of the instance   
variables which appear in these entailed literals are each forced to take a particular truth
value in all solutions  parkes study demonstrated that in instances from the underconstrained region  only a small fraction of the variables  if any  appear in the backbone 
however  as the control parameter is increased towards the critical value  a subclass of
instances which have large backbones  mentioning around        of the variables  rapidly
emerges  soon after the control parameter is increased into the overconstrained region these
large backbone instances account for all but a few of the satisfiable instances  parkes also
showed that for a fixed value of the control parameter  the cost for the local search procedure wsat is strongly influenced by the size of the backbone  this suggests that the peak in
average wsat cost near the critical value as the control parameter is increased may be due
to the emergence of large backbone instances at this point  parkes noted that for any given
size of backbone  the cost is actually higher for instances from the underconstrained region
and falls as the control parameter is increased  he also identified this fall as indicative of
another factor which produces the overall peak in cost  the main aim of this paper is to
identify the factor responsible for this pattern  why are some instances with a certain size
of backbone more costly to solve than others 
the remainder of the paper is organised as follows  in section   we review the details
of the wsat algorithm and the random k sat distribution and discuss the experimental
conditions which were used  we also elucidate the patterns in cost which we intend to
explain and show how the number of solutions and the backbone size interact  in section
  we identify a remarkable pattern in wsats search behaviour which clearly distinguishes
high cost from lower cost instances of a certain backbone size  wsat is usually drawn
early on in the search to quasi solutions where a few clauses are unsatisfied  on high cost
instances  these quasi solutions are distant from the nearest solution  while on lower cost
instances of equal backbone size  they are less distant  in section   we develop a causal
hypothesis  postulating a structural property of instances which induces a search space
   here  our use of the term backbone follows monasson  zecchina  kirkpatrick  selman and troyansky
     a      b  whose definition of the backbone is equivalent to ours for satisfiable instances 

   

fisinger  gent   smaill

structure which in turn causes the observed search behaviour and thus the cost pattern  we
suggest that instances of a certain backbone size are of high cost when they are backbonefragile  i e  when the removal of a few clauses at random results in an instance with a
greatly reduced backbone size  we discuss how this property may be measured and show
how as the control parameter is increased  instances of a certain backbone size become less
backbone fragile 
a hypothesis is only of true scientific merit if it makes correct predictions  our hypothesis made three correct predictions for which we provide experimental evidence  in
section   we show that the degree to which an instance is backbone fragile accounts for
some of the variance in cost when the control parameter and the backbone size are fixed 
in section   we consider the generation of instances which are very backbone fragile  if
clauses are removed such that the backbone is unaffected  we found that the resulting instances became progressively more backbone fragile  eventually  no more clauses can be
removed without affecting the backbone and the instance is said to be backbone minimal 
our hypothesis correctly predicts that as clauses are removed in this way from random
  sat instances  the cost becomes considerably higher  in section   we show that the hypothesis makes a correct prediction relating to the search behaviour  the clauses which are
most often unsatisfied during search are those whose removal most affects the backbone  in
section   we relate this study to previous research and give suggestions for further work 
finally  section   concludes 

   background
in this section we discuss the local search algorithm wsat  the measurement of computational cost for it and its representativeness of local search algorithms in general  we also
review the random k sat distribution and the overall cost pattern for wsat on random
k sat  finally we look at how backbone size and the number of solutions interact to affect
the cost 
    the wsat algorithm
the term wsat was first introduced by selman et al          it refers to a local search
architecture which has also been the subject of a number of subsequent empirical studies
 hoos      a  mcallester  selman    kautz        parkes   walser        parkes        
a pseudocode outline of the wsat algorithm is given in figure    an important feature
of wsat is that  unlike earlier local search algorithms  it chooses an unsatisfied clause
and then flips a variable appearing in that clause  select variable from clause must
return a variable mentioned in clause  this architecture was first seen in the random walk
algorithm due to papadimitriou         wsat may use different strategies for selectvariable from clause  in this study  we used the skc strategy introduced by selman 
kautz and cohen         we refer to this combination simply as wsat  pseudocode for
the skc strategy is given in figure   
we follow hoos        in our approach to measuring the computational cost of sat
instances for our local search algorithm wsat  rather than run times  we measure runlengths   the number of flips taken to find a solution  we set the noise level p to       which
hoos found to be approximately optimal on random   sat  hoos and stutzle        showed
   

fibackbone fragility and the local search cost peak

wsat c  max tries  max flips  p 
for i     to max tries
t    a random assignment
for j     to max flips
clause    an unsatisfied clause of c  selected at random
v    select variable from clause clause  c  p 
t    t with vs value flipped
if t is satisfying
return t
end if
end for
end for
return no satisfying assignment found

figure    the wsat local search algorithm

select variable from clause clause  c  p 
for each variable x mentioned in clause
breaks x     the number of clauses in c which would
become unsatisfied if x were flipped
end for
if there is some variable y from clause such that breaks y     
return such a variable  breaking ties randomly
else
with probability    p
return a variable z from clause
which minimises breaks z   breaking ties randomly
with probability p
return a variable z from clause
chosen randomly
end if

figure    the skc variable selection strategy

that run lengths on all but the easiest instances are exponentially distributed for many local
search variants  this implies that the random restart mechanism  the re randomisation
of t after max flips flips  is not significantly worthwhile 
   

fisinger  gent   smaill

it is not known to date whether  without using restart  wsat will almost surely  i e  with
probability approaching    find a solution on satisfiable   sat instances if given unlimited
flips  if a local search algorithm will eventually find a solution under these conditions  it is
said to be probabilistically approximately complete  pac   hoos      a  proved whether
several local search algorithms were pac and culberson and gent      a  proved that
wsat is pac for the   sat case  hoos        observed that his data suggested wsat
could be pac  we set max tries to   and max flips infinite on all runs reported in this
paper  a solution was found in every run  which is further evidence that wsat may be
pac 
another implication of the exponential distribution of run lengths is that a large number
of samples must be taken to obtain a good estimate of the mean  following hoos  we use
the median of      wsat runs on each instance as our descriptive statistic representing
wsats search cost on that instance  this appears to give a stable estimate of the cost
 as it is less sensitive to the long tail than the mean  with only a moderate amount of
computational effort 
one objection to studying a single algorithm from the local search class is that it may not
be representative  results obtained for the algorithm may not generalise to other members
of the class  while we accept this objection  there is evidence that under certain conditions 
one local search algorithm is actually to a large extent representative of the whole class 
for example hoos        found a very high correlation between the computational costs of
random instances of pairs of different local search algorithms  including wsat  this also
suggests that there is some algorithm independent property of these instances which results
in high cost for this class of algorithms 
    random k sat
we use the well studied random k sat distribution  franco   paull        mitchell et al  
      with k      random k sat is a distribution of k sat instances  parameterised by
the ratio of clauses to variables m n  let v be the fixed set of boolean variable symbols of
size n  to generate an instance from random k sat with m clauses and n variables  each
clause in c is independently chosen by randomly selecting as its literals k distinct variables
from v and independently negating each with probability      there is no guarantee that all
variables are mentioned in the instance or that it will not contain duplicate clauses 
as local search cannot solve unsatisfiable instances  we filter these out using a complete
sat procedure  in order to control for the effects of the backbone size  we will also need
to isolate the portion of the satisfiable part of the distribution for which the backbone size
is of a certain value  this is obtained by calculating the backbone size of each satisfiable
instance and discarding those whose backbone is not of the required size  we will term this
controlling the backbone size  satisfiable instances with certain backbone sizes are rare at
certain values of m n  for example when m n is       we found that only   in about       
generated instances is satisfiable with a backbone size of     hence generation of instances
in this way can be somewhat costly in computational terms  this was therefore one of the
limits on the value of n for which data could be collected 
   

fibackbone fragility and the local search cost peak

we were primarily interested in the threshold region of the control parameter  where the
cost peak occurs  the region near the point at which     of the instances are satisfiable 
we looked at the region between     and     satisfiability 
    a pattern in wsat cost for random   sat
in figure   we show the peak in wsat cost which has been mentioned e g  by parkes
        the peak is slightly above the     point        for the median but appears to shift
down for higher percentiles  a similar pattern was noticed by hogg and williams        in
local search cost for graph colouring 

    

    
  th

    

cost

    

    

  th

    

    
  th
    
  th
    

 

  th

 

   

   

   
m n

   

   

figure    the cost peak for wsat as m n is varied  at each level of m n  we generated
     satisfiable instances  we measured per instance wsat cost for each of these 
each line in the plot gives a different point in the cost distribution  e g  the   th
percentile is the difficulty of the    th most costly instance for wsat 

both parkes        and yokoo        suggest that the local search cost peak shown for
wsat in figure   is a result of two competing factors  as m n is increased the number
of solutions per instance falls and this causes the onset of high cost  however  the number
of solutions continues to fall in the overconstrained region but the cost decreases  there
must therefore be a second factor whose effect outweighs that of the number of solutions in
the overconstrained region so as to cause the fall in cost  the main aim of this paper is to
identify this factor  a pattern in wsat cost on random   sat identified by parkes       
   

fisinger  gent   smaill

is our starting point  parkes observed that for a given backbone size and n  the average
cost falls as m n is increased 
figure   shows the fall in wsat cost for n       random   sat instances  each
point in the plot is the median cost of      instances  and the length of the bars is the
interquartile range of instance cost  the fall in cost is an approximately exponential decay
for a range of m n near the threshold and for a range of backbone sizes  the rate of decay
is affected by the backbone size  with the cost of large backbone instances decaying fastest 
the length of the error bars in figure   along with the log scale of the cost axis indicates
that the distribution of per instance cost is also positively skewed even once backbone size
is controlled  for example at the point where m n is      and backbone size is    n the
difference between the   th percentile and the median is about      whereas between the
median and the   th percentile it is about half that  the spread of cost is large  particularly
relative to the effect of the control parameter  we do not think that a significant portion
of this variance in the cost among instances is due to errors in our estimates of the cost for
each instance 

backbone size       n
backbone size       n
backbone size       n

 

cost

  

 

  

 

    

   

    

   

    
m n

   

    

   

    

   

figure    the effect of varying m n on cost while backbone size is controlled 

   the cost of each instance is defined as the median run length of      runs so each point in figure   is a
median of medians 

   

fibackbone fragility and the local search cost peak

    the number of solutions when backbone size is controlled
we studied the effect of the number of solutions on wsat cost  the number of solutions
was determined using a modified complete procedure  for small backbone instances  there
was some evidence that the number of solutions actually increases with m n  at least in the
overconstrained region  figure   shows a plot of the number of solutions  with backbone
size controlled at    n  each point is the median of      instances and the bars show the
interquartile range  this possible increase in the number of solutions may help to explain
the fall in cost for small backbone instances  but it appears to be too weak an effect to
account for it in full 
 

  

number of solutions

backbone size       n

 

  

 

  

 

    

   

    

   

    
m n

   

    

   

    

   

figure    number of solutions with n        m n varied  and backbone size controlled at
   n 

we studied the relationship between the number of solutions and the wsat cost with
backbone size controlled at different values  figure   shows a log log plot of the number of
solutions against cost  where m n is      and backbone size is    n  a linear least squares
regression  lsr  fit is superimposed  table   gives summary data on the log log scatter plot
for different backbone sizes through the transition   the gradient and intercept of lsr fits 
the product moment correlation r and the rank correlation 
the number of solutions is strongly and negatively related to the cost for smaller backbone sizes through the transition and the strength of the relationship is fairly constant as
m n is varied  we speculate that the strong relationship on these instances arises because
   

fisinger  gent   smaill

m n
    

    

    

    

    

    

    

    

backbone
size
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n

intercept
of lsr fit
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

gradient
of lsr fit
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

r

rank corr 

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       
       

table    data on log log correlations between number of solutions and cost with n       
m n varied and backbone size fixed at different values 

   

fibackbone fragility and the local search cost peak

m n         backbone size       n

 

cost

  

 

  

 

  
 
  

 

  

 

  

 

  
number of solutions

 

  

 

  

 

  

figure    scatter plot of number of solutions and cost with n        m n        and
backbone size fixed at    n 

finding the backbone is straightforward and the main difficulty is encountering a solution
once the backbone has been satisfied  the density of solutions in the region satisfying the
backbone is then important  for larger backbone sizes  the number of solutions is less
relevant to the cost  no significant change in the number of solutions for large backbone
instances was observed as m n was varied  that the number of solutions and the cost are
not strongly related for these instances is unsurprising  as the large backbone size implies
that the solutions lie in a compact cluster and local searchs main difficulty is finding this
cluster  i e  satisfying the backbone   therefore we expect that the density of solutions
within the cluster is not so important  hoos        observed that the correlation between
number of solutions and local search cost becomes small in the overconstrained region  this
can now be explained simply by the fact that the large backbone instances dominate in this
region 

   search behaviour  the hamming distance to the nearest solution
in order to suggest the cause of the cost decay for large backbone instances which was
observed in section      we made a detailed study of wsats search behaviour  i e  the
assignments visited during search  we report on this exploratory part of the research in
   

fisinger  gent   smaill

this section  we explain the somewhat novel search behaviour metrics which were used
before giving results and our discussion of them 
    definitions and methods
assuming a local search algorithm is pac  in any given run of unlimited length  fb   the
number of flips taken to find the first assignment where at most b clauses are unsatisfied  is
well defined for b     f  is then equal to the run length 
a particular run of a local search algorithm then consists of a series of assignments
t    t         tf    where ti is the assignment visited after i flips have been made  we found that
on random   sat with n        an assignment satisfying all but a few clauses was quickly
found and that during the remainder of the search  few clauses          were unsatisfied 
as shown by gent and walsh        in gsat  there is a rapid hill climbing phase  which is
also suggested by hoos         followed by a long plateau like phase in which the number
of unsatisfied clauses is low but constantly changing  in our experiments we used f  as an
arbitrary indicator of the length of the hill climbing phase  unlike in gsat  in wsat there
is no well defined end point for the hill climbing phase  since short bursts of hill climbing
continue to occur for the rest of the search  we think that using fb as the indicator with
any value of b between   and    would give similar results 
local search proceeds by flipping variable values and so we might expect that the hamming distance between the current assignment and the nearest solution may also be of
interest  the hamming distance between two assignments hd t    t    is simply the number
of variables which t  and t  assign differently  we studied the hamming distance between
the current assignment t and the solution tsol of c for which hd t  tsol   is minimised  we
abbreviate this hdns t  c   hamming distance to nearest solution   for any assignment
t   hdns t  c  may be calculated by using a complete sat procedure which is modified so
that every solution to c is visited and its hamming distance from t calculated 
    results
in this section  data is based on random   sat instances with n       and backbone size
controlled at various values between    n and    n  recall that to control backbone at
a certain value  we generate satisfiable random   sat instances as usual and discard all
those whose backbone is not of the required size  we varied m n from the point of    
satisfiability        to the point of     satisfiability         hdns tf    c  is the hamming
distance between the first assignment at which no more than   clauses are unsatisfied and
the nearest solution  for each instance we calculated the median value for f  and the mean
value for hdns tf    c  based on      runs of wsat  in the plots in figures   and    each
point is the median of      instances 
figure   shows the effect of varying m n on f  when backbone size is controlled  the
values for f  are low compared to the cost and the range is very small  so although the
cost to find a solution varies considerably from instance to instance  quasi solutions are
quickly found no matter what the overall cost  however  there are some notable effects of
backbone size and m n on f    as might be expected  on the larger backbone instances  for
which overall cost is generally higher  wsat takes slightly longer to find a quasi solution 
the effect of m n is unexpected  if backbone size is controlled at    n or more  as m n is
   

fibackbone fragility and the local search cost peak

increased wsat takes slightly longer to find a quasi solution  although simultaneously cost
is decreasing as we have seen in figure   

   

backbone size       n
backbone size       n
backbone size       n
backbone size       n
backbone size       n

   

   

   

f 

   

   

   

   

  

  

 

    

   

    

   

    
m n

   

    

   

    

   

figure    the effect of varying m n on f  while backbone size is controlled 
figure   shows the effect of varying m n on hdns tf    c  when the effects of backbone
size are controlled for  in this plot  the bars give the interquartile range  the spread of
values for mean hdns tf    c  at each point is also small relative to the effect of varying
m n  again the positive effect of backbone size on hdns tf    c  is as one might expect
since backbone size affects cost 
with backbone size controlled  as m n is increased through the satisfiability threshold 
mean hdns tf    c  decreases linearly for a wide range of backbone values  hence  although
a quasi solution  tf    is usually quickly found  on the instances of lower m n this quasisolution is considerably hamming distant from the nearest solution  as m n is increased 
while the backbone size is controlled  this effect is gradually lessened 
we also looked at the relationship between the search behaviour and the cost when
m n was fixed and the backbone size was controlled  we found that in this case variance
in hdns tf    c  accounts for most of the cost variance  figure   shows a plot of the mean
hdns tf    c  against the cost with backbone size controlled at    n and m n fixed at      
an lsr fit is superimposed  the plot suggests hdns tf    c  is linearly related to log of cost 
table   gives the intercept and gradient for lsr fits and r values with backbone size
controlled at three values and m n varied  variance in hdns tf    c  accounts for most of
the variance in cost at three different backbone sizes and this is consistent through the
   

fisinger  gent   smaill

  

backbone size       n
backbone size       n
backbone size       n
  

 

hdns tf  c 

  

  

  

  

  

 

    

   

    

   

    
m n

   

    

   

    

   

figure    the effect of varying m n on hdns tf    c  while backbone size is controlled 

threshold  the scatter plots  not shown  and linear lsr fits to the data were similar in
shape to that of figure   and so are consistent with a linear relationship  the r values are
greatest for small backbone instances but the reasons for this are unclear  possibly  since
the search is shorter on the small backbone instances  success follows quickly after f  and
so hdns tf    c  is a better indicator of the likelihood of finding a solution 
figure   showed that while backbone size is controlled  hdns tf    c  falls linearly as m n
is increased  the gradient of the fall is about     table   showed that if backbone size is
controlled and m n fixed  hdns tf    c  is linearly related to log of cost  with the gradient
of the fit being around       given that this linear relationship continues to hold with a
constant gradient as m n is varied  in fact the gradient decreases slightly  and assuming
that increasing m n is not affecting the cost by other means  we would expect a linear
decrease in log of mean cost with gradient       which is only slightly steeper than the
observed decrease in log of median cost shown in figure   
so the results are consistent with the idea that whatever factor causes the cost to decay
exponentially as m n is varied does so largely by causing hdns tf    c  to fall linearly 
    discussion
we have identified a pattern in search behaviour which is strongly related to the pattern
in cost discussed in section      our interpretation of this pattern is as follows  in each
   

fibackbone fragility and the local search cost peak

m n

backbone size

    

   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n

    

    

    

    

    

    

    

intercept
of lsr fit
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

gradient
of lsr fit
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

r
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

table    data on correlations between hdns tf    c  and log   cost with n       and m n
and backbone size fixed at different values 

   

fisinger  gent   smaill

 

  

 

cost

  

 

  

 

  

  

  

  

  

  

  
hdns tf  c 

  

  

  

  

  

 

figure    the relationship of hdns tf    c  to log of cost when backbone size is controlled
at    n and m n is fixed at      

instance the quasi solutions which wsat visits form interconnected areas of the search space
such that local search can always move to a solution from them  without often moving to an
assignment where many clauses are unsatisfied  the evidence for this is simply that wsat
runs are apparently always successful but visit the assignments where more clauses are
unsatisfied very infrequently  frank  cheeseman and stutz        also mentioned in their
analysis of gsat search spaces that in random   sat  local minima where few clauses
were unsatisfied can usually be escaped by unsatisfying just one clause 
we believe that in instances of higher cost this quasi solution area extends to parts of the
search space which are hamming distant from solutions  whereas in instances of lower cost
the area is less extensive  the mean hamming distance between the early quasi solution
tf  and the nearest solution is an accurate indicator of how extensive the quasi solution
area is  this interpretation suggests why hdns tf    c  is so strongly correlated with cost 
the extensiveness of the quasi solution area determines how costly it is to search  it also
suggests why  on instances of higher cost  quasi solutions are found slightly more quickly 
when the quasi solution area is extensive  from a random starting point a shorter series of
hill climbing flips is required to find a quasi solution 
   

fibackbone fragility and the local search cost peak

the mean hdns tf    c  decreases linearly as m n is increased while backbone size is
controlled  at the same time  cost decays exponentially  we think this is because as m n
is increased  the quasi solution area becomes progressively less extensive 

   a causal hypothesis
the pattern in search behaviour from section   and our interpretation of it suggested a
causal hypothesis to account for the decay in cost discussed in section     and hence the
overall peak  the key to this hypothesis is a property of sat instances  backbone fragility 
this property is qualitatively consistent with the above observations  most importantly 
although backbone fragility has implications for an instances search space topology  it is
a property based on the logical structure of the sat instance  in this section we motivate
and define backbone fragility  discuss how it may be measured and show how it relates to
the patterns reported in sections     and   
    backbone fragility   motivation
suppose b is a small sub bag of the clauses of a satisfiable sat instance c  such that
there exists a set of quasi solutions qb where at most the clauses b are unsatisfied  what
structural property of c would cause the quasi solutions qb to be attractive to wsat 
we already know that if the backbone of a random   sat instance is small  its solutions
are found with little search  parkes         the solutions to c  b  c  b denotes c with
one copy of each member of b removed  are either solutions to c or members of qb   if
we assume that the assignments which are attractive to wsat on c are approximately
the same assignments which are attractive on c  b  then the members of qb  which are
solutions of c b  will be attractive in c when the backbone of c b is small  particularly
if cs backbone is large  furthermore for any tb  qb   the number of variables which do
not appear in the backbone of c b is an upper bound on hdns tb   c   so a large reduction
in the backbone size allows for high hdns tb   c   to summarise  if the removal of a certain
small sub bag of clauses causes the backbone size to be greatly reduced  we can expect that
quasi solutions where only these clauses are unsatisfied will be attractive to wsat and
possibly hamming distant from the nearest solution 
we are interested in quasi solutions in general rather than those in qb   if removing a
random small set of clauses on average causes a large reduction in the backbone size  we
say that the instance is backbone fragile  where the effect on the backbone is smaller on
average  the instance is backbone robust  if a large backbone instance is backbone fragile  by
extension of the above argument we expect that in general quasi solutions will be attractive
and they may be hamming distant from the nearest solution  hence this idea is consistent
with our observations and interpretation in section    backbone fragility approximately
corresponds to how extensive the quasi solution area is 
the idea that backbone fragility is the underlying factor causing the search behaviour
pattern is appealing for other reasons  for each entailed literal l of c  there must be a
sub bag of clauses in c whose conjunction entails l  for any given backbone size  as clauses
are added  for any given entailed literal l we expect that the extra clauses allow alternative
combinations of clauses which entail l  hence after adding clauses whilst controlling the
backbone size  the random removal of clauses will have less effect on the backbone since
   

fisinger  gent   smaill

the fact that a literal is entailed depends less on the presence of particular sub bags  as
clauses are added  we expect that instances will become less backbone fragile  given the
hypothetical relationship between backbone fragility and the search behaviour  this would
then explain qualitatively why the search behaviour changes as it does when m n is varied 
we think that because backbone fragility is a property of the instances logical structure  its
study may also lead to further results about complexity issues  but we postpone discussion
of this until section   
    the measurement of backbone robustness
we now define a measure of the backbone robustness of an instance which will allow us to
test predictions of the hypothesis  we take the instance c and delete clauses at random 
halting the process when the backbone size is reduced by at least half  at this point we
record as the result the number of deleted clauses  this constitutes one robustness trial 
our metric for backbone robustness is the mean result of all such possible trials  i e  the
average number of random deletions of clauses which must be made so as to reduce the
backbone size by half 
it is infeasible to compute the results of all possible robustness trials  therefore  when
measuring backbone robustness of an instance we estimated it by computing the average of
a random sample of trials  we used at least     robustness trials in each case and in order
to ensure a reasonably accurate estimate  we continued to sample more robustness trials
until the standard error was less than       the sample mean  in which case our estimate
of the mean was accurate to within about     at the     confidence level   with n       
using satisfiable instances from near the satisfiability threshold whose backbone size was
controlled at     usually less than     robustness trials were required for the estimate to
converge in this way  even then  backbone robustness was costly to compute 
there were different possible metrics for backbone fragility robustness  but we found
that the metric described above gave the clearest results for our purposes without an unnecessarily complicated definition  other metrics  such as the reduction in backbone size when
a random fixed fraction of clauses are removed  may be more suitable in other contexts 
    the change in backbone robustness as the control parameter is varied
as discussed in section     we expect that if backbone size is controlled  backbone robustness
increases as m n is increased  since our measure of backbone robustness is defined in terms
of the size of the backbone  it is most useful when comparing instances of equal backbone
size 
we found that increasing the control parameter made instances more backbone robust 
as expected  figure    shows the effect on backbone robustness of increasing m n through
the satisfiability threshold while n       and backbone size is controlled  each point is the
median of      instances 
we note that backbone robustness as defined by our measure is generally higher for
instances with larger backbones  we think that this is because on the large backbone
instances  the backbone must be reduced by a larger number of literals in each fragility trial
and that this requires more clauses to be removed 
   

fibackbone fragility and the local search cost peak

  

  

  

backbone robustness

  

  

  

  

 

backbone size       n
backbone size       n
backbone size       n

 

 

 

    

   

    

   

    
m n

   

    

   

    

   

figure     backbone robustness through the satisfiability transition  with backbone size
fixed at    n    n and    n 

   a correct prediction about cost variance
we may assert that the fall of cost observed with the increase in the control parameter
is due to the change in some other factor f   as for example yokoo        has  such an
assertion makes an important and testable prediction  that any variation in f when the
control parameter is fixed accounts for some of the variation in cost  however there may
be other factors whose influence on the cost is so great as to obscure the effect of f when
the control parameter is fixed  to best reveal the effect of f   if there is any  the effects of
some other factors may have to be controlled for 
backbone robustness is our proposed factor f   the backbone size is another factor
which strongly influences the cost  our result in this section is that when the effects of
m n and backbone size are controlled for  i e  when they are fixed  the effects of backbone
robustness can be seen quite clearly for large backbone instances 
    correlation data
figure    shows a plot of the log cost against the measure of backbone robustness for
random   sat instances with n        m n      and backbone size controlled at    n 
   n and    n  a linear lsr fit is superimposed in each case  table   gives the intercept 
   

fisinger  gent   smaill

m n

backbone size

    

   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n
   n

    

    

    

    

    

    

    

intercept
of lsr fit
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

gradient
of lsr fit
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

r

r    

r     

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

rank corr 
coefficient
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

table    data on the correlation between backbone robustness and log   cost with n      
and m n and backbone size fixed at different values 

gradient and r values for lsr fits with backbone size controlled at the same three values and
with m n varied through the threshold 
the r values suggest an effect of backbone robustness on cost  particularly for large
backbone sizes  for smaller backbone sizes  we imagine that finding the backbone is less of
an issue and so backbone fragility  which hinders this  has less of an effect  for the larger
backbone sizes  we think the main difficulty for wsat is satisfying the backbone  backbone
fragility is then important  however  given the somewhat unclear shape of the scatter plots 
there are several concerns as to the significance of the correlation  which we now address
using some simple statistical methods 
   

fibackbone fragility and the local search cost peak

m n         backbone size       n

 

cost

  

 

  

 

 

  

  
  
  
backbone robustness

  

  

  

  

  

  

  

m n         backbone size       n

 

cost

  

 

  

 

  

 

 

  

  
  
  
backbone robustness

  

m n         backbone size       n
 

  

cost

 

  

 

  

 

  

 

 

  

  
  
  
backbone robustness

  

figure     scatter plot of backbone robustness and cost with n        m n        and
backbone size fixed at    n     n and    n 

   

fisinger  gent   smaill

    an artifact of the distributions of the variables 
one concern is that the observed r could also have arisen simply because of the distributions
of the two variables rather than because of any relationship between them  this is a serious
concern here as the distributions are unknown 
the null hypothesis  h  is that the value of r which results from the distributions of the
two variables is equal to the observed r  a randomisation method can be used to test h   
see appendix a for details of this method  for each data set presented       randomised
pairings of the data were constructed  in each case  we found that the observed r does
not fall within the range of the sampling distribution of r for randomised pairings  h  can
therefore be rejected at the       confidence level 
the r coefficient  given above  can be greatly affected by outliers  therefore the rank
correlation coefficient  which is less affected  was also calculated  the rank correlation is
also given in table    we found that in each case the rank correlation coefficient is not
considerably different from the r coefficient  this demonstrates that the observed r was not
greatly affected by outliers 
    confidence intervals for the correlation
given that there is a relationship between the two variables which is not merely an artifact
of the distributions or of outliers  how accurate is our measurement of r  a bootstrap
method can be used to obtain bounds on a confidence interval for this statistic  again  the
reader is referred to appendix a for details of this method  using this method with     
pseudo samples we obtained lower and upper bounds on the     confidence interval for r 
which are also given in table   as r     and r      respectively  the data implies with    
confidence  the upper bounds on the amount of error in our estimates of r   are between
about      and      

   a correct prediction about very backbone fragile instances
our hypothesis proposes that high backbone fragility of instances quite accurately represents
the factor which  via the search behaviour patterns uncovered in section    causes high
wsat cost for those instances  however  it is plausible that the high backbone fragility is
a by product of some unmeasured latent factor and that it is not causally related to the
cost 
to help establish the causal link between backbone fragility and cost  we therefore created sets of random sat instances which had higher backbone fragility than usual random
  sat instances  this is to some degree following the methodological precedent of bayardo
and schrag         who created random instances which contained small unsatisfiable subinstances but which had few constraints overall  these were often found to be exceptionally
hard for the complete procedure ntab  their experiments thereby helped establish that
this feature of instance structure was the cause of exceptionally high cost for complete
procedures 
we cannot easily set backbone fragility directly  since it is not a generation parameter 
one manipulation experiment which is possible is the use of an instance generation procedure which results in instances with a higher backbone fragility  our hypothesis predicts that
   

fibackbone fragility and the local search cost peak

instances generated using such a procedure will be harder than random   sat instances 
in this section we define such a procedure and test the prediction  it may be that our procedure is also manipulating the latent factor  however  since the procedure is specifically
designed to increase backbone fragility  a correct prediction here still lends credibility to
our hypothesis 
    backbone minimal sub instances
suppose we have a sat instance c and we remove a clause such that the backbone is not
affected by the removal of the clause  if such clauses are repeatedly removed  eventually
the instance will be such that no clause can be removed without disturbing the backbone 
in this case we have a backbone minimal sub instance  bms  of c  more formally  we have
the following definition 
definition a sat instance c   is a bms of c iff
 c   is a sub instance of c  i e  a sub bag of the clauses  such that c   has the same
backbone as c 
 for each clause c of c   there exists a literal l such that 
   c    l
    c     c    l is satisfiable
i e  every strict sub instance of c   has a strictly smaller backbone than the backbone of c    
bmss can be seen as satisfiable analogues of the minimal unsatisfiable sub instances
 muss  of unsatisfiable instances studied by amongst others culberson and gent      b 
in the context of graph colouring and gent and walsh        and bayardo and schrag
       in satisfiability  an mus of an instance c is a sub instance which is unsatisfiable 
but such that the removal of any one clause from the sub instance renders it satisfiable 
just as all unsatisfiable instances must have an mus  all satisfiable sat instances must
have a bms  having a bms does not depend on having a non empty backbone  if the
backbone of the instance is empty  its bms is the empty sub instance  an instance can
have more than one bms  different bmss of an instance may share clauses  one bms of
an instance cannot be a strict sub instance of another 
suppose the backbone of a satisfiable instance c is the set of literals  l    l            lk    let
d be the clause l   l          lk   then we have the following useful fact 
theorem c   is a bms of c iff c    d is an mus of c  d  
a simple proof of the above is given in appendix b  due to this fact  methods for studying
muss can be applied to the study of bmss  we can study the bmss of a satisfiable
instance c by finding the backbone of c and then studying the muss of c  d  each of
these corresponds to a bms of c since d must be present in every mus of c  d 
to find a bms of c we determine the backbone  then find a random mus of c  d using
the same mus finding method as gent and walsh        and remove d from the result 
   

fisinger  gent   smaill

instances
preserve backbone c     c    
preserve backbone c     c    
preserve backbone c      c    
preserve backbone c      c    
preserve backbone c      c    
preserve backbone c      c    
bms

backbone robustness
  th percentile median   th percentile
              
       
              
       
              
       
      
      
       
      
      
      
      
      
      
      
      
      

table    the effect of preserve backbone on backbone robustness 

    interpolating between an instance and one of its bmss
once a bms c   has been established  we can also study the effects of interpolation between
c and c   by removing at random from c some of the clauses which do not appear in
c     this is equivalent to removing clauses at random such that the backbone is preserved 
preserve backbone c  mr   c     will denote c with mr clauses  which do not appear in
the bms c     removed at random  the resulting instance will have the same backbone as
c 
just as increasing m n while controlling the backbone size causes backbone robustness
to increase  we have found that deleting clauses such that the backbone is unaffected causes
backbone robustness  as measured above  to decrease  as one might expect 
we used     random   sat instances with n       and m n         for each instance
we found one bms  we then used preserve backbone to interpolate with mr set at
various values  table   shows the effect of increasing mr on backbone robustness  the
bmss of the threshold instances are so backbone fragile that the removal of just one clause
from them is likely to reduce the backbone by a half or more 
our hypothesis predicts that as this interpolation from c to c   proceeds  the cost
for local search increases because the backbone robustness decreases  it is conceivable 
although it would be very surprising  that removing any clauses from random instances
near the threshold generally makes their cost for local search increase  if this were the
case  any increase in cost during interpolation towards a bms could merely be due to the
removal of clauses per se rather than the removal of clauses whilst preserving the backbone 
to control for this possibility we also removed clauses according to two other procedures 
the procedure random c  mr   removes mr clauses from c at random  the procedure
reduce backbone c  mr   removes mr clauses such that each time a clause is removed 
the size of the backbone is reduced  the clause to be removed is chosen randomly from all
such clauses  this procedure therefore uses the opposite removal criterion to preservebackbone  if the backbone becomes empty  no further clauses are removed 
figure    shows the effect on per instance cost of applying the three clause removal
procedures to the same set of     random   sat threshold instances  each plot is the
median per instance cost 
   

fibackbone fragility and the local search cost peak

 

  

cost

preservebackbone
reducebackbone
random

 

  

 

  

 

  

  

  

  
mr

  

  

  

  

figure     the effect of the three clause removal procedures on median per instance cost 

we observe that removing clauses randomly or such that the backbone is strictly reduced  causes cost to be reduced  so the removal of clauses does not in itself cause higher
cost  the reduce backbone procedure causes a greater initial fall in cost  as the backbone size is reduced more quickly than with random  however  the cost then stabilises for
reduce backbone because the backbone becomes empty and thereafter no more clauses
are removed 
removing clauses according to preserve backbone causes the local search cost to
increase by an amount approximately exponential in the number of clauses removed  table
  gives more data on this effect and also cost data for bmss  the interpolation shifts the
whole distribution up  not just the median  the median cost of the bmss  which are the
most backbone fragile of all the instances  is more than three times that of the   th cost
percentile of random   sat instances 
the bmss of these instances had between     and     clauses  the above results
therefore demonstrate the existence of instances in the underconstrained region which are
much harder than the typical instances from near the satisfiability threshold  however since
these were not obtained by sampling from random   sat directly  we do not know how
often they occur  as far as we know  they are vanishingly rare and therefore  in contrast
to exceptionally hard instances for complete algorithms  it seems unlikely that they affect
the mean cost  also  while gent and walsh        showed that the exceptionally hard
   

fisinger  gent   smaill

instances
preserve backbone c     c    
preserve backbone c     c    
preserve backbone c      c    
preserve backbone c      c    
preserve backbone c      c    
preserve backbone c      c    
bms

per instance cost
  th percentile median   th percentile
   
    
    
   
    
    
   
    
    
   
    
    
   
    
     
   
    
     
    
     
      

table    the effect of preserve backbone on per instance cost 

instances for complete algorithms are hard for a different reason from that of threshold
instances  bmss are apparently hard for the same reason  because they are backbonefragile 
one useful by product of this section is a means of generating harder test instances for
local search variants without increasing n  however these instances do require o m   n 
complete searches to generate  o n  to determine satisfiability and the backbone and o m 
to reduce to a bms 

   a correct prediction about search behaviour
recall that in the motivating discussion of section     it was suggested that the quasisolutions in qb would be attractive if the backbone of c  b was small  that is to say that
the clauses of b are more likely to be the set of unsatisfied clauses if the removal of the
clauses of b has a large effect on the backbone  this part of the hypothesis also makes a
prediction about search behaviour  that clauses most often unsatisfied by wsat should be
those whose removal reduces the backbone size most  in this section we show this prediction
to be correct 
we looked at individual instances which were cost percentiles from a set of      random
k sat instances with n       and m n         per instance cost was determined as in
previous sections  for each clause in the instance  we calculated the number of backbone
literals which were no longer entailed if the clause was removed  this is a simple measure of
the backbone contribution  bc  of the clause  how much the backbone size depends on the
presence of the clauses  if a clauses backbone contribution is high  it is termed a backbonecritical clause  we made      runs of wsat on each instance under the same conditions as
in previous sections  during search  each time the current assignment changed we recorded
whether each clause was unsatisfied  the result of averaging the number of times the clause
was unsatisfied over all runs gives the unsatisfaction frequency  uf   of that clause 
figure    shows a plot of these two quantities for the clauses of the instance whose cost
was median of      threshold instances  we note from this figure that the clauses whose
presence contributes the most to the backbone are more often unsatisfied than average
during wsat search 
   

fibackbone fragility and the local search cost peak

 

  

unsatisfaction frequency

 

  

 

  

 

  

 

 

  
  
backbone contribution

  

  

figure     scatter plot of unsatisfaction frequency against backbone contribution for the
clauses of the cost median of      instances  m n         n       

table   confirms this pattern  each row of the table gives data for one instance  we
selected cost percentiles  individual instances of varying degrees of difficulty  for example
the row labelled   th corresponds to the instance whose cost is the     th in rank from
the easiest to the most difficult of the      instances  while the   th percentile instance
is the one used to produce figure     the third and fourth columns give the mean and
standard deviation of the unsatisfaction frequency over all clauses in the instance and the
last two columns give the same statistics for the sub bag of the clauses which were most
backbone critical  their backbone contribution was in the top      
table   shows that the converse effect is also present  the clauses which are most often
unsatisfied  their unsatisfaction frequency is in the top      are more backbone critical than
average  although an effect is quite clear from the means  there are sometimes particularly
large standard deviations in bc values for the most frequently unsatisfied clauses  this
is because  as can be seen from figure     some clauses are very often unsatisfied even
though removing them on their own does not affect the backbone size at all  we have
found in other experiments that the removal of these clauses along with other small random
bags of clauses does on average reduce the backbone size considerably  the large standard
deviations therefore arise because the true backbone contribution of these clauses is not
apparent when using this simple measure 
   

fisinger  gent   smaill

cost
percentile

backbone
size

  th
  th
  th
  th
  th
  th
  th
  th
  th

  
  
  
  
  
  
  
  
  

all clauses
uf mean
       
       
       
       
       
       
       
       
        

uf std  dev 
      
       
       
       
       
       
       
       
        

most backbonecritical clauses
uf mean uf std  dev 
       
      
       
       
       
       
       
       
       
       
       
       
        
       
        
        
        
        

table    unsatisfaction frequencies of clauses in different cost percentile instances 

cost
percentile
  th
  th
  th
  th
  th
  th
  th
  th
  th

backbone
size
  
  
  
  
  
  
  
  
  

all clauses
bc mean
      
      
      
      
      
      
      
      
      

bc std  dev 
      
      
      
      
      
      
      
      
       

most often
unsatisfied clauses
bc mean bc std  dev 
      
      
      
      
      
      
      
      
      
      
      
      
       
       
      
      
       
       

table    backbone contributions of clauses in different cost percentile instances 

   

fibackbone fragility and the local search cost peak

for instances of different costs at the satisfiability threshold  the clauses which are most
likely to be unsatisfied during search have a higher backbone contribution than average 
conversely  the clauses which have the largest backbone contribution are more likely to be
unsatisfied during search  this section therefore demonstrates that as well as explaining
differences in cost between instances  the backbone fragility hypothesis can also explain
differences in the difficulty of satisfying particular clauses during search 

   related and further work
clark et al         showed that the number of solutions is correlated with search cost for
a number of local search algorithms on random instances of different constraint problems 
including random   sat  the pattern was confirmed by hoos        using an improved
methodology  clark et al s work was the first step towards understanding the variance in
cost when the number of constraints is fixed  we have followed their approach both by
looking at the number of solutions and by using linear regression to estimate strengths of
relationships between factors 
schrag and crawford        made an early empirical study of the clauses  including
literals  which were entailed by random   sat instances  parkes         whose study is
also discussed in section    looked in detail at backbone size in random   sat and its effect
on local search cost  he also linked the position of the cost peak to that of the satisfiability
threshold by the emergence of large backbone instances which occurs at that point  parkes
also identified the fall in wsat cost for instances of a given backbone size  this was
therefore the basis for our study  parkes conjectured that the presence of a failed cluster
may be the cause of high wsat cost for some large backbone random   sat instances 
according to this hypothesis  the addition of a single clause could remove a group of solutions
which is hamming distant from the remaining solutions  reducing the size of the backbone
dramatically  such a clause would then have a large backbone contribution  therefore our
explanation for the general high cost of the threshold region has certain features in common
with parkes conjecture  in particular we agree that it is the presence of clauses with a
large backbone contribution which causes high cost  this is especially demonstrated by our
results from section   
frank et al         studied in detail the topology of the gsat search space induced by
different classes of random sat instances  their study discussed the implications of search
space structure for future algorithms  as well as the effects of these structures on algorithms
such as gsat  they also noted that some local search algorithms such as wsat may be
blind to the structures they studied because they search in different ways to gsat 
yokoo        also addressed the question of why there is a cost peak for local search
as m n is increased  the approach was to analyse the whole search space of small satisfiable random instances  while in this study  we have only examined sat  yokoo also
showed his results generalised to the colourability problem  yokoo used a deterministic
hill climbing algorithm  he studied the number of assignments from which a solution is
reachable  solution reachable assignments  via the algorithms deterministic moves  which
largely determines the cost for the algorithm 
we followed yokoo in looking for a factor competing with the number of solutions whose
effect on cost changes as m n is increased  the factor which yokoo proposed as the cause
   

fisinger  gent   smaill

of the overall fall in cost was the decrease in the number of local minima  assignments
from which no local move decreased the number of unsatisfied clauses  the decrease in
this number was demonstrated as m n is increased  the decrease was attributed to the
decreasing size of basins  interconnected regions of local minima with the same number
of unsatisfied clauses   yokoo claimed  p       that 
adding constraints       makes the  instance  easier by decreasing the number
of local minima 
however  we do not think it is clear a priori what the relationship between the number
of local minima and the cost is in a given instance and yokoo did not study it sufficiently
to convince us of his explanation  in contrast with yokoo  we have studied in detail the
relationship between the backbone fragility of instances and wsats cost on these instances
and confirmed it by testing predictions of our hypothesis  also  we studied instance properties that related to the logical structure of the clauses rather than the search space topology
which was induced as we think this has more potential to generalise across algorithms and
even to address complexity issues  as we explain towards the end of this section 
hoos        also analysed the search spaces of sat instances in relation to local search
cost by looking at two new measures of the induced objective function which he defined 
including one based on local minima  although via these measures  hoos was not able to
account for the random   sat cost peak  he found that the features were correlated with
cost for some sat encodings of other problems and has also shown  hoos      b  that his
measures can help distinguish between alternative encodings of the same problem 
how does the pattern we have uncovered fit in to other work on what makes instances
require a high cost to solve  gent and walsh        looked at the probability that an unsatisfiable sat instance became satisfiable if a fixed number of clauses are removed at random 
the unsatisfiable instances which had the highest computational cost for a complete procedure were found to be those which were unsatisfiability fragile  their unsatisfiability was
sensitive to the random removal of clauses  it may therefore be that the fragility of an instances unsatisfiability or backbone size is the cause of high computational cost both in the
context of complete procedures and incomplete local search  which would be an interesting
link between the two algorithm classes  this link may form the basis of a possible explanation of the reasons why threshold random   sat instances may be universally hard in
the average case  as opposed to merely costly for some class of algorithms  recent work by
monasson et al       a      b  has suggested that parameterised distributions of instances
which are hard in the average case  e g  random   sat  exhibit a discontinuity in the
backbone size  as the control parameter is varied  whereas in polynomial time average case
distributions  such as random   sat  the backbone size changes smoothly  they propose
that the complexity of the distribution is linked to the presence of this discontinuity  we
conjecture that this may be because in the asymptotic limit  instances which are backboneor unsatisfiability fragile only persist as n is increased where there is such a discontinuity 
this line of research may therefore establish a testable causal mechanism for this pattern 
showing how the properties of the instance distributions affect algorithm performance 
it would be interesting to compare backbone fragility in different random distributions
of   sat instances  such as those introduced by bayardo and schrag        and by iwama 
   monasson et al s definition of the backbone also extends to unsatisfiable instances 

   

fibackbone fragility and the local search cost peak

miyano and asahiro        to see whether differences in local search cost could be explained 
a method which generates satisfiable instances which are quickly solved by local search is
analysed by koutsoupias and papadimitriou        and gent         random clauses are
added to the formula as in random   sat but only if they do not conflict with a certain
solution which is set in advance  we conjecture that overconstrained examples of these are
quickly solved by local search because they are very backbone robust 
an interesting possibility mentioned by hoos and stutzle        suggested by the exponential run length distribution  was that local search is equivalent to random generate andtest in a drastically reduced search space  we conjecture that this reduced search space
corresponds to the quasi solution area  measurements of hdns tb   c  for quasi solutions
tb may therefore be indicative of the extensiveness of this reduced search space  especially
since this metric is linearly correlated with log cost  further experimentation in this vein
may therefore reveal more about the topology of the reduced search space which could in
turn lead to better local search algorithms designed to exploit this knowledge 
finally  we should emphasise that the notions of backbone and backbone fragility are
equally applicable to non random sat instances  in future we may be able to confirm that
the results we have shown for random sat instances apply equally to benchmark and realworld sat instances  however  one caveat here is that entailed literals may be uncommon
in these instances and we may need to study the fragility of other sets of entailed formulas 

   conclusion
we have reconsidered the question of why cost for local search peaks near the random
  sat satisfiability threshold  the overall pattern is one of two competing factors  the
cause of the onset of high cost as the control parameter is increased has been previously
established as the decreasing number of solutions  we have proposed that the cause of the
subsequent fall in cost is falling backbone fragility 
we found a striking pattern in the search behaviour of the local search algorithm wsat 
for instances of a given backbone size  in the underconstrained region of the control parameter  wsat is attracted early on to quasi solutions which are hamming distant from the
nearest solution  this distance is also very strongly related to search cost  as the control
parameter is increased  the distance decreases  we suggested backbone fragility was the
cause of this pattern 
we defined a measure of backbone robustness  backbone fragile instances have low
robustness  we were then able to test predictions of the hypothesis that the fall in backbone
fragility is the cause of the overall decay in cost as the control parameter is increased  we
found that the hypothesis made three correct predictions  firstly that the degree to which
an instance is backbone fragile is correlated with the cost when the effects of other factors
are controlled for  secondly  that when random   sat instances are altered so as to be
more backbone fragile  by removing clauses without disrupting the backbone  their cost
increases  thirdly  that the clauses most often unsatisfied during search are those whose
deletion has most effect on the backbone 
we now summarise our interpretation of the evidence  in the underconstrained region 
instances with small backbones are predominant  in this region  the rapid hill climbing
phase typically results in an assignment which is close to the nearest solution  and probably
   

fisinger  gent   smaill

satisfies the backbone   since finding the small backbone is largely accomplished by hillclimbing  typical cost for wsat is low in this region and variance in cost is due to variance
in the density of solutions in the region of the search space where the backbone is satisfied 
in the threshold region  large backbone instances quickly appear in large quantities 
for large backbone instances  the main difficulty for local search is to identify the backbone
rather than to find a solution once the backbone has been identified  the identification of a
large backbone may be accomplished by the rapid hill climbing phase to a greater or lesser
extent  we think that this extent is determined by the backbone fragility of the instance 
if a large backbone instance is backbone fragile the hill climbing phase is ineffective and
results in an assignment which is hamming distant from the nearest solution  probably
implying that much of the backbone has not been identified   then a costly plateau search
is required to find a solution  hence when the rare large backbone instances do occur in the
underconstrained region  they are extremely costly to solve because of their high backbone
fragility 
if a large backbone instance is more backbone robust  the rapid hill climbing phase is
more effective in determining the backbone and the plateau phase is shorter  so overall
the instance is less costly for wsat to solve  hence for large backbone instances  since
backbone fragility increases as we add clauses  cost decreases  in the overconstrained region 
large backbone instances are dominant and so backbone fragility becomes the main factor
determining cost  hence cost decreases in this region  our hypothesis proposes the following
explanation for the cost peak  typical cost peaks in the threshold region because of the
appearance of many large backbone instances which are still moderately backbone fragile 
followed by the increasing backbone robustness of these instances 

acknowledgments
this research was supported by uk engineering and physical sciences research council
studentship          to the first author  the first two authors are members of the crossuniversity apes research group  http   www cs strath ac uk  apes    we would like
to thank the other members of the apes group  the anonymous reviewers of this and an
earlier paper and andrew tuson for invaluable comments and discussions 

   

fibackbone fragility and the local search cost peak

appendix a  randomisation and bootstrap tests
we summarise the methods as used in this context  further explanation of these methods
is given in cohen        
a   randomisation for estimating the correlation coefficient due to the
distributions of the variables
randomisation can be used to estimate the correlation coefficient between the two variables
which results simply from their distributions rather than from any relationship  we start
with the two vectors of data x   hx    x            xn i and y   hy    y            yn i  if the correlation
coefficient is merely due to the distributions of x and y  then it is not dependent on any
particular xi being paired with yi   therefore to calculate the correlation coefficient resulting
merely from the distributions we pair the x and y data randomly 
we construct k randomisations  each randomisation consists of a vector y     which is
simply a random permutation of y  for each randomisation  we calculate the correlation
coefficient between x and y    note that each value xi is now paired with a random value
from y  these randomised correlation coefficients give us an estimate of the correlation
coefficients resulting from the distributions of the variables  if k is large enough  we will
have an accurate estimate which can be compared with the correlation coefficient in the
observed data 
a   bootstrap estimation of confidence intervals for correlation coefficients
we have an original sample h x    y      x    y            xn   yn  i of n pairs  a pseudo sample from
the original also consists of n pairs  the j th pair in the pseudo sample  xbj   yjb      xq   yq  
where q is a random number between   and n   each pair in the pseudo sample is chosen
independently i e  pairs are sampled from the original with replacement  we assume that
our original sample of pairs of data is representative of the whole population of such pairs 
given this  composing pseudo samples is just like sampling from the whole population 
therefore by measuring the correlation coefficient of many pseudo samples  we can study
what the correlation coefficient would have looked like had we taken many sets of samples
from the whole population  from the distribution of the correlation coefficient among many
pseudo samples  the bootstrap sampling distribution  we can infer bounds on the confidence
interval for the observed correlation coefficients 
many pseudo samples are taken  and the correlation coefficient is calculated for each of
the pseudo samples  this gives the bootstrap sampling distribution of the correlation coefficient  the     th percentile of this distribution is an upper bound on the     confidence
interval for the correlation coefficient  and the    th percentile is a lower bound 

appendix b  the relationship between bmss and muss
let c be a satisfiable sat instance and  l    l            lk   be the set of all literals entailed by
c  let d be the clause l   l          lk  
theorem c   is a bms of c iff c    d is an mus of c  d  
   

fisinger  gent   smaill

proof suppose c   is a bms of c  then c    d  which is a sub instance of c  d  must
be unsatisfiable  as d violates every literal in the backbone of c     if d is removed from c   d 
the result c   is satisfiable  if any other clause c is removed from c    d  there must be some
literal from the backbone of c     li say  such that  c     c    li is satisfiable  therefore 
since li is also a literal of d   c     c    d is satisfiable  therefore c    d is an mus of
c  d 
conversely  suppose c    d is an mus of c  d  since c    d is minimally unsatisfiable 
 
c is satisfiable  since c   is a sub instance of c  the backbone of c   must be a subset of the
backbone of c  suppose there were some literal lj which was in the backbone of c but not
in the backbone of c     then there would be a solution to c    lj   this would then also be
a solution to c    d  since lj is one literal of d  this contradicts c    d being unsatisfiable
and so there can be no lj i e  c   and c must have the same backbone 
c    d is minimally unsatisfiable  therefore for any clause c of c      c     c    d is
satisfiable  any solution to  c     c    d must make some literal lk of d true  and must
therefore also be a solution to  c     c    lk   therefore lk   which is in the backbone of
c     is not in the backbone of  c     c    hence c   is a bms of c  

references
bayardo  r  j     schrag  r          using csp look back techniques to solve exceptionally hard sat instances  in proceedings of the second international conference
on the principles and practice of constraint programming  pp        springer 
cheeseman  p   kanefsky  b     taylor  w          where the really hard problems are 
in proceedings of ijcai     pp          morgan kaufmann 
clark  d   frank  j   gent  i  p   macintyre  e   tomov  n     walsh  t          local search
and the number of solutions  in proceedings of the second international conference
on the principles and practice of constraint programming  pp          springer 
cohen  p          empirical methods for artificial intelligence  the mit press 
cook  s          the complexity of theorem proving procedures  in proc   rd ann  acm
symp  on theory of computing  pp         
cook  s     mitchell  d          finding hard instances of the satisfiability problem  a
survey  in satisfiability problem  theory and applications  vol     of dimacs series
in discrete mathematics and theoretical computer science  pp         american
mathematical society 
crawford  j  m     auton  l  d          experimental results on the crossover point in
random  sat  artificial intelligence           
culberson  j     gent  i  p       a   on the completeness of walksat for   sat  tech 
rep  apes          apes research group 
available from http   apes cs strath ac uk apesreports html 
   

fibackbone fragility and the local search cost peak

culberson  j     gent  i  p       b   well out of reach  why hard problems are hard  tech 
rep  apes          apes research group 
available from http   apes cs strath ac uk apesreports html 
franco  j     paull  m          probabilistic analysis of the davis putnam procedure for
solving the satisfiability problem  discrete applied math           
frank  j   cheeseman  p     stutz  j          when gravity fails  local search topology 
j  artificial intelligence research            
gent  i  p          on the stupid algorithm for satisfiability  tech  rep  apes         
apes research group 
available from http   apes cs strath ac uk apesreports html 
gent  i  p   macintyre  e   prosser  p     walsh  t          the constrainedness of search 
in proceedings of aaai     pp          aaai press   the mit press 
gent  i  p     walsh  t          an empirical analysis of search in gsat  j  artificial
intelligence research          
gent  i  p     walsh  t          the satisfiability constraint gap  artificial intelligence 
         
hogg  t     williams  c  p          the hardest constraint problems  a double phase
transition  artificial intelligence             
hoos  h          stochastic local search   methods  models  applications  ph d  thesis 
darmstadt university of technology 
hoos  h       a   on the run time behaviour of stochastic local search algorithms for
sat  in proceedings of aaai     pp          aaai press   the mit press 
hoos  h       b   sat encodings  search space structure  and local search performance 
in proceedings of ijcai     pp          morgan kaufmann 
hoos  h     stutzle  t          characterising the run time behaviour of stochastic local
search  tech  rep  aida        darmstadt university of technology 
iwama  k   miyano  e     asahiro  y          random generation of test instances with controlled attributes  in cliques  coloring  and satisfiability  vol     of dimacs series
in discrete mathematics and theoretical computer science  pp          american
mathematical society 
koutsoupias  e     papadimitriou  c  h          on the greedy algorithm for satisfiability 
information processing letters                 
larrabee  t     tsuji  y          evidence for a satisfiability threshold for random  cnf
formulas  tech  rep  ucsc crl        jack baskin school of engineering  university
of california  santa cruz 
   

fisinger  gent   smaill

mcallester  d   selman  b     kautz  h          evidence for invariants in local search 
in proceedings of aaai     pp          aaai press   the mit press 
mitchell  d   selman  b     levesque  h          hard and easy distributions of sat
problems  in proceedings of aaai     pp          aaai press   the mit press 
monasson  r   zecchina  r   kirkpatrick  s   selman  b     troyansky  l       a     psat  relation of typical case complexity to the nature of the phase transition 
random structures and algorithms               
monasson  r   zecchina  r   kirkpatrick  s   selman  b     troyansky  l       b   determining computational complexity from characteristic phase transitions  nature      
       
papadimitriou  c  h          on selecting a satisfying truth assignment  in proc    nd
ieee symp  on the foundations of comp  sci   pp         
parkes  a          clustering at the phase transition  in proceedings of aaai     pp 
        aaai press   the mit press 
parkes  a     walser  j          tuning local search for satisfiability testing  in proceedings of aaai     pp          aaai press   the mit press 
schrag  r     crawford  j          implicates and prime implicates in random   sat 
artificial intelligence             
selman  b   kautz  h     cohen  b          noise strategies for improving local search 
in proceedings of aaai     pp          aaai press   the mit press 
walsh  t          the constrainedness knife edge  in proceedings of aaai     pp     
     aaai press   the mit press 
yokoo  m          why adding more constraints makes a problem easier for hill climbing
algorithms  analysing landscapes of csps  in proceedings of the third international
conference on the principles and practice of constraint programming  pp         
springer 

   

fi