journal artificial intelligence research                 

submitted        published     

model inductive bias learning
jonathan baxter

j onathan  baxter   anu   edu   au

research school information sciences engineering
australian national university  canberra       australia

abstract
major problem machine learning inductive bias  choose learners hypothesis space large enough contain solution problem learnt  yet small
enough ensure reliable generalization reasonably sized training sets  typically bias
supplied hand skill insights experts  paper model automatically
learning bias investigated  central assumption model learner embedded
within environment related learning tasks  within environment learner sample
multiple tasks  hence search hypothesis space contains good solutions
many problems environment  certain restrictions set hypothesis
spaces available learner  show hypothesis space performs well sufficiently
large number training tasks perform well learning novel tasks environment  explicit bounds derived demonstrating learning multiple tasks within
environment related tasks potentially give much better generalization learning single
task 

   introduction
often hardest problem machine learning task initial choice hypothesis space 
large enough contain solution problem hand  yet small enough ensure
good generalization small number examples  mitchell         suitable bias
found  actual learning task often straightforward  existing methods bias generally
require input human expert form heuristics domain knowledge  for example 
selection appropriate set features   despite successes  methods
clearly limited accuracy reliability experts knowledge extent
knowledge transferred learner  thus natural search methods
automatically learning bias 
paper introduce analyze formal model bias learning builds upon
pac model machine learning variants  vapnik        valiant        blumer 
ehrenfeucht  haussler    warmuth        haussler         models typically take
training data
following general form  learner supplied hypothesis space
drawn independently according underlying distribution

  based information contained   learners goal select hypothesis
minimizing measure
expected loss respect  for example  case squared loss
   models learners
bias represented choice   contain good solution problem  then 
regardless much data learner receives  cannot learn 
course  best way bias learner supply containing single optimal hypothesis  finding hypothesis precisely original learning problem 



fifi
  
     
   

 



 

 

 
 
 
 
                         
      

c      ai access foundation morgan kaufmann publishers  rights reserved 





fibaxter

pac model distinction bias learning ordinary learning  put differently 
pac model model process inductive bias  simply takes hypothesis space
given proceeds there  overcome problem  paper assume instead
faced single learning task  learner embedded within environment
related learning tasks  learner supplied family hypothesis spaces
 
  appropriate entire environment 
goal find bias  i e  hypothesis space
simple example problem handwritten character recognition  preprocessing stage
identifies removes  small  rotations  dilations translations image character
advantageous recognizing characters  set individual character recognition
problems viewed environment learning problems  that is  set problems
form distinguish characters  distinguish b characters 
on   preprocessor represents bias appropriate problems environment 
likely many currently unknown biases appropriate
environment  would able learn automatically 

dc

b

many examples learning problems viewed belonging environments related problems  example  individual face recognition problem belongs
 essentially infinite  set related learning problems  all individual face recognition problems   set individual spoken word recognition problems forms another large environment 
set fingerprint recognition problems  printed chinese japanese character recognition problems  stock price prediction problems on  even medical diagnostic prognostic
problems  multitude diseases predicted pathology tests  constitute
environment related learning problems 
many cases environments normally modeled such  instead treated
single  multiple category learning problems  example  recognizing group faces would
normally viewed single learning problem multiple class labels  one face
group   multiple individual learning problems  however  reliable classifier
individual face group constructed easily combined produce
classifier whole group  furthermore  viewing faces environment related
learning problems  results presented show bias learnt good
learning novel faces  claim cannot made traditional approach 
point goes heart model  concerned adjusting learners
bias performs better fixed set learning problems  process fact
ordinary learning richer hypothesis space components labelled bias
able varied  instead  suppose learner faced  potentially infinite  stream
tasks  adjusting bias subset tasks improves learning performance
future  yet unseen tasks 
bias appropriate problems environment must learnt sampling
many tasks  single task learnt bias extracted likely specific
task  rest paper  general theory bias learning developed based upon idea
learning multiple related tasks  loosely speaking  formal results stated section    
two main conclusions theory presented here 

e

learning multiple related tasks reduces sampling burden required good generalization 
least number of examples required per task basis 
   

fia odel nductive b ias l earning

e

bias learnt sufficiently many training tasks likely good learning novel
tasks drawn environment 

second point shows form meta generalization possible bias learning  ordinarily  say learner generalizes well if  seeing sufficiently many training examples 
produces hypothesis high probability perform well future examples
task  however  bias learner generalizes well if  seeing sufficiently many training tasks produces hypothesis space high probability contains good solutions novel tasks  another
term used process learning learn  thrun   pratt        
main theorems stated agnostic setting  that is 
necessarily contain
hypothesis space solutions problems environment   give improved
bounds realizable case  sample complexity bounds appearing results stated
terms combinatorial parameters related complexity set hypothesis spaces
available bias learner  boolean learning problems  pattern classification  parameters
bias learning analogue vapnik chervonenkis dimension  vapnik        blumer et al  
      
application general theory  problem learning appropriate set neuralnetwork features environment related tasks formulated bias learning problem 
case continuous neural network features able prove upper bounds number
training tasks number examples training task required ensure set features
works well training tasks will  high probability  work well novel tasks drawn

environment  upper bound number tasks scales
measure complexity possible feature sets available learner  upper

number
bound number examples task scales
examples required learn task true set features  that is  correct bias  already
known  number tasks  thus  case see number related tasks
learnt increases  number examples required task good generalization decays
minimum possible  boolean neural network feature maps able show matching
lower bound number examples required per task form 





f jilkmgonpq

p

f hg
f jir

g

    related work
large body previous algorithmic experimental work machine learning
statistics literature addressing problems inductive bias learning improving generalization
multiple task learning  approaches seen special cases of  least
closely aligned with  model described here  others orthogonal  without
completely exhaustive  section present overview main contributions  see thrun
pratt        chapter    comprehensive treatment 

e

hierarchical bayes  earliest approaches bias learning come hierarchical bayesian
methods statistics  berger        good        gelman  carlin  stern    rubim        
contrast bayesian methodology  present paper takes essentially empirical
process approach modeling problem bias learning  however  model using mixture
hierarchical bayesian information theoretic ideas presented baxter      a  
similar conclusions found here  empirical study showing utility
hierarchical bayes approach domain containing large number related tasks given
heskes        
   

fie

baxter

e

early machine learning work  rendell  seshu  tcheng        vbms variable bias
management system introduced mechanism selecting amongst different learning
algorithms tackling new learning problem  stabb shift better bias  utgoff        another early scheme adjusting bias  unlike vbms  stabb
primarily focussed searching bias applicable large problem domains  use
environment related tasks paper may interpreted environment
analogous tasks sense conclusions one task arrived analogy
 sufficiently many of  tasks  early discussion analogy context  see russell        s      particular observation analogous problems
sampling burden per task reduced 
metric based approaches  metric used nearest neighbour classification  vector
quantization determine nearest code book vector  represents form inductive bias 
using model present paper  extra assumptions tasks
environment  specifically  marginal input space distributions identical
differ conditional probabilities assign class labels   shown
optimal metric distance measure use vector quantization onenearest neighbour classification  baxter      a      b  baxter   bartlett         metric
learnt sampling subset tasks environment  used
distance measure learning novel tasks drawn environment  bounds
number tasks examples task required ensure good performance novel
tasks given baxter bartlett         along experiment metric
successfully trained examples subset     japanese characters used
fixed distance measure learning      yet unseen characters 
similar approach described thrun mitchell         thrun        
neural networks output trained match labels novel task  simultaneously
forced match gradient derivative information generated distance metric
trained previous  related tasks  performance novel tasks improved substantially
use derivative information 

e

e

note many adaptive metric techniques used machine learning 
focus exclusively adjusting metric fixed set problems rather learning
metric suitable learning novel  related tasks  bias learning  
feature learning learning internal representations  adaptive metric techniques 
many approaches feature learning focus adapting features fixed task
rather learning features used novel tasks  one cases features
learnt subset tasks explicit aim using novel tasks
intrator edelman        low dimensional representation learnt set
multiple related image recognition tasks used successfully learn novel tasks
kind  experiments reported baxter      a  chapter    baxter      b  
baxter bartlett        nature 
bias learning inductive logic programming  ilp   predicate invention refers process ilp whereby new predicates thought useful classification task hand
added learners domain knowledge  using new predicates background domain knowledge learning novel tasks  predicate invention may viewed form
   

fia odel nductive b ias l earning

inductive bias learning  preliminary results approach chess domain reported
khan  muggleton  parson        

e

e

improving performance fixed reference task  multi task learning  caruana       
trains extra neural network outputs match related tasks order improve generalization
performance fixed reference task  although approach explicitly identify
extra bias generated related tasks way used learn novel tasks 
example exploiting bias provided set related tasks improve generalization
performance  similar approaches include suddarth kergosien         suddarth
holden         abu mostafa        

e

bias computational complexity  paper consider inductive bias samplecomplexity perspective  learnt bias decrease number examples required
novel tasks good generalization  natural alternative line enquiry runningtime computational complexity learning algorithm may improved training
related tasks  early algorithms neural networks vein contained sharkey
sharkey         pratt        
reinforcement learning  many control tasks appropriately viewed elements sets
related tasks  learning navigate different goal states  learning set
complex motor control tasks  number papers reinforcement learning literature
proposed algorithms sharing information related tasks improve average
generalization performance across tasks singh         ring         learning bias
set tasks improve performance future tasks sutton         thrun schwartz
       

    overview paper
section   bias learning model formally defined  main sample complexity results
given showing utility learning multiple related tasks feasibility bias learning 
results show sample complexity controlled size certain covering numbers
associated set hypothesis spaces available bias learner  much way
sample complexity learning boolean functions controlled vapnik chervonenkis
dimension  vapnik        blumer et al          results section   upper bounds
sample complexity required good generalization learning multiple tasks learning
inductive bias 
general results section   specialized case feature learning neural networks section    algorithm training features gradient descent presented 
special case able show matching lower bounds sample complexity
multiple task learning  section   present concluding remarks directions future
research  many proofs quite lengthy moved appendices
interrupt flow main text 
following tables contain glossary mathematical symbols used paper 
   

fibaxter

symbol



u

 

description
input space
output space
distribution
 learning task 
loss function
hypothesis space
hypothesis
error hypothesis distribution
training set
learning algorithm
empirical error training set
set learning tasks
distribution learning tasks
family hypothesis spaces
loss hypothesis space environment
 sample
empirical loss
bias learning algorithm
function induced
set
average

set
set
function probability distributions
set
pseudo metric
pseudo metric
covering number
capacity
covering number
capacity
sequence hypotheses
sequence distributions
average loss
average loss
set feature maps
output class composed feature maps
hypothesis space associated
loss function class associated
covering number
capacity
pseudo metric feature maps
covering number

st 

 
 
       
v
 
 y w  x  

z

     
p fi  
w  a 
 
v
 b
 
b
 b
 
  b fifi   c   b
 
fifi  rc b
 
fifi  rc b
dqb c
bc
  c
fifi   c b
b
b
tea
e
e
cb
fa g
f 
e

h jijfi e f  
e
k jijfi e c
ea
cb
h jijfi c b flg

c
jijfi b
ap b

n
p
g  
n
 
w    

p
psr q
pb
h jijfi p b f  
pb
k hijfi p b
pb
ft     uvxw q qzy
h jijfi f t     uv w






u

 
fifi   c

fifi c

   

qp

q qzy

q

z

first referenced
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fia odel nductive b ias l earning

hsymbol

jijfi f     uv w  description
covering number
k uv jijfi

capacity
  
neural network hypothesis space
   
restricted vector
  
growth function

vapnik chervonenkis dimension
 
restricted matrix
  p fi  
restricted matrix
growth function
f pq
dimension function
f
upper dimension function
f c
lower dimension function cof
n
  g
optimal performance

f

 
    c metric
 
 rc
average
   
c
 
 
 
c

 
set
 
     c  
permutations integer pairs
 j
permuted  
f  
u

empirical
metric functions
w   g
n
optimal average error

first referenced
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   bias learning model
section bias learning model formally introduced  motivate definitions  first
describe main features ordinary  single task  supervised learning models 
    single task learning
computational learning theory models supervised learning usually include following ingredients 

e

input space

e



ss 
e loss function u   s  d  
probability distribution

e

hypothesis space

 

output space

 

 

set hypotheses functions

     

 

example  problem learn recognize images marys face using neural network 
would set images  typically represented subset
component
pixel intensity   would set
  distribution would peaked images
different faces correct class labels  learners hypothesis space would class
neural networks mapping input space

  loss case would discrete loss 



 





u zfi   l






   



 

   

fibaxter

u
u   l   

 

 

using loss function allows us present unified treatment pattern recognition  
  above   real valued function learning  e g  regression 
usually
 
goal learner select hypothesis
minimum expected loss 

  ct

   
         b u   fi  f qfir
 
minimizing
course  learner know cannot search
         practice  learner samples repeatedly   according distribution
generate training set
 

fifi j
   
  ct   hence  general
based information contained learner produces hypothesis
v
learner simply map set training samples hypothesis space  
v   dt   

v
 stochastic learners treated assuming distribution valued   
 
many algorithms seek minimize empirical loss   defined by 


w   x       u  
   


course  intelligent things data simply minimizing empirical
errorfor example one add regularisation terms avoid over fitting 
however learner chooses hypothesis   uniform bound  over
 
probability large deviation

  bound learners genas function empirical loss training set
  whether
eralization error
bound holds depends upon richness   conditions ensuring convergence

well understood  boolean function learning  
  discrete
loss   convergence controlled vc dimension   

  w   x  

     
       

 
w   x  

  c

      

  w  x  
  bfi

suppose
probability distribution


 fifi fibe isanygenerated
 
times according   let
f      probabilityby atsampling
least    over choice training set   
  c satisfy

 
 
 
 

f


w
   
        x ffko  f k

theorem    let

proofs result may found vapnik         blumer et al         
reproduced here 

aj 













   vc dimension class boolean functions
largest integer exists subset
restriction contains boolean functions  

   



fia odel nductive b ias l earning

     

      

  w  x  

theorem   provides conditions deviation


actually small 
likely small  guarantee true error
governed choice   contains solution small error learner minimizes
error training set  high probability
small  however  bad choice
mean hope achieving small error  thus  bias learner model 
represented choice hypothesis space  

     

    bias learning model
main extra assumption bias learning model introduced learner embedded environment related tasks  sample environment generate multiple
training sets belonging multiple different tasks  model ordinary  single task 

  bias learning
learning  learning task represented distribution
model  environment learning problems represented pair
set
 i e   set possible learning problems  
probability distributions
distribution   controls learning problems learner likely see    example 
learner face recognition environment  highly peaked face recognition type
problems  whereas learner character recognition environment peaked
character recognition type problems  here  introduction  view environments
sets individual classification problems  rather single  multiple class classification problems  
recall last paragraph previous section learners bias represented
choice hypothesis space   enable learner learn bias  supply family
 
set hypothesis spaces
putting together  formally learning learn bias learning problem consists of 



 

z



  z

z



z

z

 

e

input space

e

loss function



output space

u   s  d  
e environment z

distribution  
e

hypothesis space family

 

 both separable metric spaces  

set probability distributions





u

u

c

assume loss function range
assume bounded 



d 

set functions



      

z



 

    equivalently  rescaling 

   bias governed learner uses hypothesis space  example  circumstances
learner may choose use full power  a neural network example early stopping   simplicity
paper abstract away features algorithm assume uses entire hypothesis space
 
   domain  algebra subsets   suitable one purposes borel  algebra
generated
topology weak convergence   assume separable metric spaces 
separable metric space prohorov metric  which metrizes topology weak convergence   parthasarathy 
       problem existence measures
  see appendix discussion 
particularly proof part   lemma    













   








q



fibaxter

define goal bias learner find hypothesis space
following loss 

c

minimizing

               f z
   
  u   fi  f fi  f z
z
 
way     small if  high  probability  contains good solution
z
problem drawn random according   sense     measures appropriate
z
bias embodied environment  
z
general learner know   able find minimizing    

z

times according yield 
c


fifi  





e sample   times s  according yield 
b

 
e resulting p training setshenceforth called p fi    sample generated
processare supplied learner  sequel  p fi    sample denoted
  written matrix 






j

  
  
  
  
   
   
 
 
 
c
c
c c    c
c
p fi    sample simply p training sets
fifia sampled p different learning tasks
c

  task selected according environmental probability distribution z  
size training set kept primarily facilitate analysis 
 c  
based information contained     learner must choose hypothesis space
 
directly  however  learner sample environment following way 

e

sample

p

one way would learner find
defined by 

minimizing empirical loss  

  w  a  

 

 
 
c



c





  w       p
  r  w   x   

   

    

note
simply average best possible empirical error achievable
training set   using function   biased estimate
  unbiased estimate
would require choosing minimal average error distributions
  defined
 
ordinary learning  likely intelligent things training data
minimizing      denoting set
 samples
  general bias
learner map takes
 samples input produces hypothesis spaces

output 

 

v

p fi  


c c
r    
    
p fi  

  c    
v  

s 
 
 
c
   

ss    c    

p

sc

   

fia odel nductive b ias l earning

v

 as stated  deterministic bias learner  however trivial extend results stochastic
learners  
note paper concerned sample complexity properties bias
learner   discuss issues computability  
since searching entire hypothesis spaces within family hypothesis spaces
  extra representational question model bias learning present
represented searched   defer
ordinary learning  family
discussion section      main sample complexity results model bias learning
introduced  specific case learning set features suitable environment
related learning problems  see section   
regardless learner chooses hypothesis space   uniform bound  over

  probability large deviation

  compute
upper bound
  bound bias learners generalization error
 
view  question generalization within bias learning model becomes  many
tasks     many examples task     required ensure

close high probability  uniformly
  or  informally  many tasks
many examples task required ensure hypothesis space good solutions
training tasks contain good solutions novel tasks drawn environment 
turns kind uniform convergence bias learning controlled size
certain function classes derived hypothesis space family   much way
vc dimension hypothesis space
controls uniform convergence case boolean
function learning  theorem     size measures auxiliary definitions needed
state main theorem introduced following subsection 

v



v

v

v



c
p

  w  a 

  w   

 

    

  w   

c

   
   



     d    define   b       
 b fi    u    fir
    
hypothesis space hypothesis space family   define
bff  b   b   ct j
    
c
 
  c   define  
fifi   c b  dt      
sequence p hypotheses
fifi
c

 
fifi  rc b

fifi c c   p u  
    


db
 
  c b   hypothesis space family   define
use denote
fifi
cb   b  
fifi   c b  
fifi   c ct j
    
    covering numbers

definition    hypothesis

define

cb   cb


   

    

fibaxter

      
b
b
p

     

 b

first part definition above  hypotheses
turned functions
mapping
composition loss function 
collection
functions original hypotheses come  
often called loss function class 
case interested average loss across tasks  hypotheses
chosen fixed hypothesis space   motivates definition

  finally 
collection
  restriction
belong single
hypothesis space
 

c
b

c

definition   

 
  c b

c

  define

hypothesis space family



te    
   
e     r       

  define

db
 
fifi   c

p c
b

    

e  b
e   c j

    
cb
e controls large p fi    sample   must ensure
size

 w  a       close uniformly c   size defined terms
certain covering
cb numbers  neede define measure distance
elements
elements  
n
fifi c sequence p probability distributions dt   
definition    let
c
db b c b   define

flg dqb yb       db

fifi c c    yb

fifi c c
    
f


f c c c
z
e e c e   define
similarly  distribution
 
f   e
e   e
   e f z
    
 
 
fg f   pseudo metrics cb e respectively 
easily verified
e f
te te
edc e  


definition     cover   set
fifi
f   te e ti     note require te contained
h

e f
e   measurable functions
  let jijfi   denote size smallest
e
cover  define capacity

k jilfi e  
h jilfi e f  
    
 
  h jijfi cb f g defined similar
supremum probability measures
c
fg place f     define capacity b by 
way  using
k jijfi cb   g h jilfi cb fg
    
supremum sequences p probability measures s   
r
 
   pseudo metric metric without condition
 

   

fia odel nductive b ias l earning

    uniform convergence bias learners
enough machinery state main theorem  theorem hypothesis space
family required permissible  permissibility discussed detail appendix d  note
weak measure theoretic condition satisfied almost real world hypothesis space
families  logarithms base  





 



z

 
p fi  

 




z
c

p
 

 fififip










































p

k e






p
     
number examples   task satisfies
k cb






  fi
pffi     fi i     
c satisfy
probability least    over p fi    sample     
       w  a   k

theorem    suppose
separable metric spaces let probability distribution   set distributions
  suppose
 sample generated
sampling times according give
  sampling times
generate
 
  let
permissible
hypothesis space family  number tasks satisfies





    

    

    

proof  see appendix a 

k jijfi cb
p fi  

several important points note theorem   

k j ijfi e

    
  cs
  o 

   provided capacities

finite  theorem shows bias
bound generalisation error

learner selects hypothesis spaces
terms
sufficiently large
 samples   bias learners find
exact value
involves finding smallest error hypothesis
training sets   upper bound
 found  example
gradient descent error function  still give upper bound
  see
section       brief discussion achieved feature learning setting 

  w  a  w
  ap  

  w   

 



 

 
 
p

c

 

  w  a 

 


close uniformly
   order learn bias  in sense
   number tasks number examples task
must
sufficiently large  intuitively reasonable bias learner must see
sufficiently many tasks confident nature environment  sufficiently
many examples task confident nature task 

c
z
 

  w    

   learner found
small value
  use

learn novel tasks drawn according   one following theorem bounding
sample complexity required good generalisation learning  the proof
similar proof bound theorem    



   

fibaxter



fifi


ilfi   ijfi
 

k   b
   
     
 
    
  ct satisfy
probability least     
         w  x   k ij
k
capacity jilfi appearing equation      defined analogous
 b fito the 
f  b   b     fashion
capacities definition    we use pseudo metric    

  yb qfir f fi     important thing note theorem   number
ex 

theorem    let
training set generated sampling
according distribution   let permissible hypothesis space 
  
 
  number training examples satisfies

amples required good generalisation learning novel tasks proportional logarithm capacity learnt hypothesis space   contrast  learner
bias learning  reason select one hypothesis space

consequently would view candidate solution hypothesis
hypothesis spaces
  thus  sample complexity proportional
capacity  
  general considerably larger capacity
  learning learner learnt learn environment
individual
sense needs far smaller training sets learn novel tasks 

z

c

c
b b

c

  
w   a  k
   
 
      r          fi

w  a 
 

       

   learnt hypothesis space
small value
  theorem   tells us
probability least
  expected value
novel task
less
  course  rule really bad performance tasks
  however  probability generating bad tasks bounded  particular 
note
expected value function
  markovs
inequality       



e

   

   
  
 

 
 
 
 

w
   

e

  e

 


  ki


 







 with probability

     

ijfi

   keeping accuracy confidence parameters
fixed  note number examples
required task good generalisation obeys

  f p k ijfi cb
    
c
k jilfi b increases sublinearly p   upper bound number
provided

examples required task decrease number tasks increases  shows
suitably constructed hypothesis space families possible share information
tasks  discussed theorem   below 
   

fia odel nductive b ias l earning

    choosing hypothesis space family

    
p fi  
 



 

  w    

    
c
p  

theorem   provides conditions

close  guarantee
actually small  governed choice   contains hypothesis
space small value
learner able find
minimizing error
sample  i e   minimizing
   then  sufficiently large   theorem   enthe
sures high probability
small  however  bad choice mean
hope finding small error  sense choice represents hyper bias
learner 
note sample complexity point view  optimal hypothesis space family choose
contains good solutions
one containing single  minimal hypothesis space
problems environment  or least set problems high  probability   more 
bias learning  because choice made hypothesis
spaces   output bias learning algorithm guaranteed good hypothesis space
environment  since hypothesis space minimal  learning problem within environment using require smallest possible number examples  however  scenario
analagous trivial scenario ordinary learning learning algorithm contains
single  optimal hypothesis problem learnt  case learning done 
bias learning done correct hypothesis space already known 
extreme  contains single hypothesis space consisting possible functions
bias learning impossible bias learner cannot produce
restricted hypothesis space output  hence cannot produce hypothesis space improved
sample complexity requirements yet unseen tasks 
focussing two extremes highlights minimal requirements successful bias
must strictly smaller space
learning occur  hypothesis spaces
functions
  small skewed none contain good solutions
large majority problems environment 
may seem simply replaced problem selecting right bias  i e   selecting
right hypothesis space   equally difficult problem selecting right hyper bias  i e  
right hypothesis space family    however  many cases selecting right hyper bias far
easier selecting right bias  example  section   see feature selection
problem may viewed bias selection problem  selecting right features extremely
difficult one knows little environment  intelligent trial and error typically best
one do  however  bias learning scenario  one specify set features
exist  find loosely parameterised set features  for example neural networks   learn
features sampling multiple related tasks 

   
   



w  a 
 







z

   





c

   



    learning multiple tasks

p

z


may learner interested learning learn  wants learn fixed set
  previous section  assume learner starts
tasks environment
hypothesis space family   receives
 sample generated
distributions
  time  however  learner simply looking hypotheses
  contained hypothesis space   average generalization
error hypotheses minimal  denoting
writing
 

p
 
fifi  rc

p


c

p fi  

 
fifi   c

   



 

p

n
c

fibaxter

c






    g   p
      
    
c


p u   fi  f fi fi



empirical loss  
c


w  a    p  w  x   
    

c
p   u      

 

 
  c   prove uniform bound
before  regardless learner chooses
fifi
g


 
  c perform
w
probability large deviation        
fifi
well training sets   high probability perform well future examples

error given by 

tasks 

n

fifi c p
 

 


 



p fi  

theorem    let
probability distributions
let
according   let

sample generated sampling times
permissible hypothesis space family  number examples task satisfies

 

 

k
    cb






ffp  
 
  fi
    
   
c c satisfy
probability least    over choice     
    
   g    w  a  ffk
c
k b
 recall definition   meaning jilfi   
proof  omitted  follow proof bound   theorem    
bound   theorem   virtually identical bound   theorem    note
depends inversely number tasks p  assuming first part max
k cb
expression dominate one   whether helps depends rate growth  
 
function p   following lemma shows growth always small enough ensure
never worse learning multiple tasks  at least terms upper bound number
examples required per task  

 


k li b k h ilfi cb k li b
c

lemma    hypothesis space family

   

    

fia odel nductive b ias l earning


 
 
r
 
c
b
proof  let   denote set functions
fifi c
k hijfi cb member
k jijfi    
 c  recall definition    
  
b
c

 



hypothesis space


k
k

ilfi

b
h

j





lemma    appendix b 
 
right hand inequality follows 
n meafor first inequality 
let probability measure   let
c
sure     obtained using first copy   
cb flg product  and
b c ignoring
b

elements product  let
 cover
  pick

c b c fg     fifi   b fi  
fifi   c b   construction 
let   
fifi  
flg     fi fi   b fi   
fi fi   c b f     fi   
b   establishes first inequality 
k ji b
k jijfi cb p k ijfi b

    
keeping accuracy parameters fixed  plugging            see upper
bound number examples required task never increases number tasks 
best decreases f npq   although upper bound  provides strong hint
lemma  

learning multiple related tasks advantageous number examples required per task
basis  section   shown feature learning types behavior possible 
decrease 
advantage
    dependence



f npq

ni  

ni

theorems        bounds sample complexity scale
  behavior
improved
empirical loss always guaranteed zero  i e   realizable
case   behavior results interested relative deviation empirical
true loss  rather absolute deviation  formal theorems along lines stated appendix
a   

   feature learning
use restricted feature sets nearly ubiquitous method encoding bias many areas
machine learning statistics  including classification  regression density estimation 
section show problem choosing set features environment
related tasks recast bias learning problem  explicit bounds

calculated general feature classes section      bounds applied problem
learning neural network feature set section     

k e fiai 

k cb fiai 

    feature learning model
consider following quote vapnik        
classical approach estimating multidimensional functional dependencies
based following belief 
real life problems exists small number strong features  simple
functions  say linear combinations  approximate well unknown function 
therefore  necessary carefully choose low dimensional feature space
use regular statistical techniques construct approximation 
   

fibaxter

q   


q














q
q
q

q
p
 
q c o
ptr q   b r q   c p


p
r
 
 

c

    
q q j
problem carefully choosing right features q equivalent bias learning
c   hence  provided
problem find right hypothesis space
k e
k cb learner embedded within
environment related tasks  capacities fiai fiai finite  theorem   tells
us feature set q learnt rather carefully chosen  represents important
simplification  choosing set features often difficult part machine learning
problem 
k e
k cb
section     give theorem bounding fiai fiai  general feature classes 
theorem specialized neural network classes section     
p
note forced function class feature maps q   although
p
necessary  indeed variants results follow obtained allowed vary
q  

general set strong features may viewed function
   mapping input
 typically lower  dimensional space     let
set feature
space
 
maps  each may viewed set features
   must
    
carefully chosen quote  general  simple functions features may
represented class functions mapping    
define hypothesis
  
 
  hypothesis space family
space

    capacity bounds general feature classes

fi   
q


s 

s 
p
u
c p
q fi 
u
  fir   fi  u
b  
pb
b
p b r   r q   c p b q co
pb
k jilfi p b  
e h jilfi p b f  
 
f
   zcbfir 
supremum probability measures          fi    gf
  cbfi  f
cbfi   
define capacity p ofb first define pseudo metric f     uvxw
pulling back h metric follows 
ft     uv w q q   u v   r q fi       r q qfir f fi 
    



f t
ft
easily verified     uvxw pseudo metric  note     uv w well defined suprep
b
mum integrand must measurable  guaranteed theh hypothesis space family
ft
p ib r q   q c permissible  lemma
    part     define jilfi     uv w 

pb
f


smallest  cover pseudo metric space     u v w  capacity  with respect  

k uv jilfi  
h jilfi f     uv w
 
supremum probability measures      state main theorem
a 
 

notationally easier view feature maps mapping
  absorb loss function definition viewing  

a 
   cb

via cb
  previously latter function would
map  
denoted   follows drop subscript cause confusion 
class   belongs still denoted  
  
 
definitions let
  define capacity
usual way 

section 

   

fia odel nductive b ias l earning

ii
k  

theorem    let
 



hypothesis space family equation      

k j ilfi cb
k jijfi e

k h
p b c k u v ji  
k uv hilfi

ijfiai
fiai    


    
    

proof  see appendix b 
    learning neural network features

f

general  set features may viewed map  typically high dimensional  input
 
much smaller dimensional space
  jlk
   section consider approximatspace
ing feature map one hidden layer neural network input nodes j output nodes
qp r

n
 
 figure     denote set feature maps

r
bounded subset ts   u number weights  parameters  first two layers  
set previous section 
feature n
 
j defined











f
 
   
fifi       c

           fifi
b


 n       vwx b       kyb b
 z
    

 


 
b
output
 
  output
    node first hidden layer  cb
fifi b

 
node parameters th feature v sigmoid squashing function v      
     s    fifi u   computes
first layer hidden node






      v wx

z 
    k
    






 
 
 



hidden nodes parameters  assume v lipschitz  weight

fifi
 
 
vector entire feature map thus
p

fifi


fifi b
fifi b
 fi b

fi fi b
b
 fi b  
 fi b   b




 
 
 
 
uf
u
total number feature parameters u kffk j k  
p
arguments sake  assume simple functions features  the class previous
 

section  squashed affine maps using sigmoid function v  in keeping
p
neural network flavor features   thus  setting feature weights generates
hypothesis space 

   



 


n     k


f  


c r
 
 

ed





r
 
bounded subset   set hypothesis spaces 
  b   q  p c r
  k   h lmgnh   k h hkporq  
   lipschitz exists constant g h j
vcb

   

    

    

fibaxter

multiple output classes
n

k

l

feature
map


input

p

p

p fi  

figure    neural network feature learning  feature map implemented first two
hidden layers  output nodes correspond different tasks
sample   node network computes squashed linear function nodes
previous layer 

 


fifi



 
feature
hypothesis space family  restrictions output layer weights
p


weights   restriction lipschitz squashing function needed obtain finite upper
bounds covering numbers theorem   
finding good set features environment
equivalent finding good hyp
  turn means finding good set feature map parameters  
pothesis space
theorem    correct set features may learnt finding hypothesis space
small error sufficiently large
 sample   specializing squared loss  present
framework empirical loss
 equation      given

z

  c

p fi  
 
 
c

 

 









b
b
    
 w      p
 tsu    w vwrvwv   s x    y k    
 z vyb b
n        k f       
since sigmoid function v range     restrict outputs   range 
 

      lgorithms



f inding



g ood et



f eatures

provided squashing function v differentiable  gradient descent  with small variation
p
backpropagation compute derivatives  used find feature weights minimizing     
 or least local minimum        extra difficulty ordinary gradient
descent appearance definition
  solution perform gradient
p
  node feature weights  
descent output parameters


details see baxter      b  baxter      a  chapter     empirical results supporting
theoretical results presented given 

r

w      
 

fifi
   

fia odel nductive b ias l earning

      ample c omplexity b ounds

 
k j ijfi cb



n eural  n etwork f eature l earning

size ensuring resulting features good learning novel tasks
environment given theorem    compute logarithm covering
numbers

 

k j ijfi e
   ap c s  hypothesis space family    form
theorem    let   


 




    v b
n     k
f  
  c  






  n    
n       neural network u weights mapping    

p
feature weights output weights  fi
fifi   bounded  squashing function v
u



lipschitz  squared loss  output space      any bounded subset do  
exist constants  y  independent ilfi u j      
k jijfi cb j kp k u
    

k jijfi e u ri
    
 recall specialized squared loss here  
proof  see appendix b 
noting neural network hypothesis space family
theorem   gives following theorem 



permissible  plugging          

 

 

theorem    let
hypothesis space family hypothesis space

set squashed linear maps composed neural network feature map  above  suppose
number features j   total number feature weights w  assume feature weights
 sample
output weights bounded  squashing function v lipschitz  let
generated environment
 

 

z

p f   u k   r

p fi  

    



  fi f   j k k u p k p r
  c satisfy
probability least  
         w          k il
   

    

    

fibaxter

      iscussion



f k



npq

   keeping accuracy confidence parameters fixed  upper bound number
examples required task behaves j
u
  learner simply learning
fixed tasks  rather learning learn   upper bound applies  recall
theorem    

p
 

f

p




upper bound
   note away feature map altogether u
becomes j   independent  apart less important term   terms
upper bound  learning tasks becomes hard learning one task  extreme 
fix output weights effectively j
number examples required
task decreases u
  thus range behavior number examples required
decrease number
task possible  improvement
tasks increases  recall discussion end section      

p



f npq

p

f npq

   feature map learnt  which achieved using techniques outlined baxter 
    b  baxter   bartlett        baxter      a  chapter     output weights
estimated learn novel task  keeping accuracy parameters fixed  requires
j examples  thus  number tasks learnt increases  upper bound
number examples required task decays minimum possible  j  

f

f

   small number strong features assumption correct  j small  however 
typically little idea features are  confident neural
network capable implementing good feature set need large  implying
uj  
j
u
decreases rapidly increasing uj   least
terms upper bound number examples required per task  learning small feature
sets ideal application bias learning  however  upper bound number
tasks fare well scales u  

f k

npq

p

f


special case multi task framework one marginal distribution input
  task  fifip   varies tasks conditional
space
distribution output space     example would multi class problem face
l fifip  p number faces recognized
recognition   

marginal distribution simply natural distribution images faces 
case  every example   havein addition sample   th tasks conditional
distribution   samples remaining p  conditional distributions    
view p training sets containing   examples one large training set multi class
problem  tp examples altogether  bound   theorem   states  tp
f p j k u   proportional total number parameters network  result would
      c omparison



raditional ultiple  c lass c lassification

expect from   haussler        
specialized traditional multiple class  single task framework  theorem   consistent bounds already known  however  already argued  problems face
recognition really single task  multiple class problems  appropriately viewed
   example classified large margin naive parameter counting improved upon  bartlett 
      

   

fia odel nductive b ias l earning

p

p

 potentially infinite  collection distinct binary classification problems  case  goal
bias learning find single  output network classify subset faces
well  learn set features reliably used fixed preprocessing distinguishing single face faces  new thing provided theorem    tells us
provided trained  output neural network sufficiently many examples sufficiently
many tasks  confident common feature map learnt tasks good
learning new  yet unseen task  provided new task drawn distribution
generated training tasks  addition  learning new task requires estimating j
output node parameters task  vastly easier problem estimating parameters
entire network  sample computational complexity perspective  also  since
high confidence learnt features good learning novel tasks drawn
environment  features candidate study learn
nature environment  claim could made features learnt
small set tasks guarantee generalization novel tasks  likely features
would implement idiosyncrasies specific tasks  rather invariances apply across
tasks 

p

p

 

p

viewed bias  or feature  learning perspective  rather traditional  class
classification perspective  bound number examples required task takes
somewhat different meaning  tells us provided large  i e   collecting examples
large number tasks   really need collect examples would
examples vs  j examples  
otherwise collect feature map already known   j u
tells us burden imposed feature learning made negligibly small  least
viewed perspective sampling burden required task 

p

k np

    learning multiple tasks boolean feature maps



p



ignoring accuracy confidence parameters   theorem   shows number
examples required task learning tasks common neural network feature map
j
u
bounded
  j number features u number
adjustable parameters feature map  since
j examples required learn single task
true features known  shows upper bound number examples
required task decays  in order  minimum possible number tasks increases 
suggests learning multiple tasks advantageous  truly convincing need
 
prove lower bound form  proving lower bounds real valued setting  
complicated fact single example convey infinite amount information 
one typically make extra assumptions  targets
corrupted
noise process  rather concern complications  section restrict
attention boolean hypothesis space families  meaning hypothesis
maps

measure error discrete loss


otherwise  

f k

npq

f

p

c  

 

  c

u    fir     u    fir
 
show sample complexity learning p tasks boolean hypothesis space family
f  pq  that is  give nearly matching upper
type parameter
controlled vc dimension
f pq    derive bounds f pq hypothesis space
lower bounds involving

family considered previous section lipschitz sigmoid function v replaced hard
threshold  linear threshold networks  
   

fibaxter

f

well bound number examples required per task good generalization across
tasks  theorem   shows features performing well u
tasks generalize well
novel tasks  u number parameters feature map  given many feature
learning problems u likely quite large  recall note   section         would useful
know
u
tasks fact necessary without restrictions environmental
distributions generating tasks  unfortunately  yet able show lower
bound 
empirical evidence suggesting practice upper bound number
tasks may weak  example  baxter bartlett        reported experiments
set neural network features learnt subset     japanese characters turned
good enough classifying      unseen characters  even though features contained
several hundred thousand parameters  similar results may found intrator edelman       
experiments reported thrun        thrun pratt        chapter    
gap experiment theory may another example looseness inherent
general bounds  may analysis tightened  particular  bound
number tasks insensitive size class output functions  the class section      
may looseness arisen 

zf



p

      u pper l ower b ounds
pace families



l earning tasks



b oolean h ypothesis


fifi c    

      b  
fifi      ct j
 
 
clearly       say shatters   growth function defined
       l      
size largest set shattered  
vapnik chervonenkis dimension
           j

first recall concepts theory boolean function learning  let
class
 
set binary vectors obtainable
boolean functions
applying functions  



important result theory learning boolean functions sauers lemma  sauer        
make use 
lemma    sauers lemma   boolean function class

positive integers

 



f  

 
     f


 

generalize concepts learning

p

   

tasks boolean hypothesis space family 

fia odel nductive b ias l earning

definition    let
input space
matrices 



  matrices
denote p
  bec   a  boolean hypothesis
  c   space
  family 
c
c
 



  define set  binary 
 

 



 



 z 
  c ct
    
  
  
  

 
 
 
  c c
 rc c




      
p fi  
p   fi      define
p fi     l  

c   c




 
p


 
 
 


note
matrix
  say shatters
c

f pq       p fi   j
define

define

lemma    

 

p
 



let

f    

f    
f f
f pq f p f
 

f k f
p
proof  first inequality trivial definitions  get second term maximum
c   f construct matrix
second
inequality  choose
c
 
 
 

f
c
whose rows length shattered   clearly
shatters  

first term maximum take sequence
fifi     shattered  the hypothesis

space consisting union hypothesis spaces    distribute elements equally
among rows  throw away leftovers   set matrices
 


 





 
 
c
  
  

  


 
 
 
 
c
 
c

















c

f
  size  
  np subset
c  c  
lemma    
 
p fi   f  pq  
   







fibaxter

p p fi   p   

fifi c
 

  
 
f pq   p f qp

p

 
fifi  rc

proof  observe  
collection boolean
obtained first choosing functions

functions sequences
  applying
first examples 
second examples on 
definition
 
  hence result follows lemma   applied
 

c

 

k cb fiai 

p fi   


one follows proof theorem    in particular proof theorem    appendix
a  clear  
 
may replaced
boolean
e
case  making replacement theorem     using choices
discussion

following theorem     obtain following bound probability large deviation
empirical true performance boolean setting 

n
fifi c p
let  


p fi  
 


  let b
 
      d c c      g  w a    ffk ij p fi          p   n

    
corollary     conditions theorem     number examples   task


probability distributions
theorem     let

 sample generated sampling times
according
permissible boolean hypothesis space family   
 

satisfies

    f pq k p
 

probability least    over choice     
   g    w  a  ffk



c c

    
satisfy
    

proof  applying theorem     require

p fi          p   n

satisfied

  fi   f pq f   pq k p

fim  
used lemma     now 
  k k ifi



f pqni          satisfied
  fii     setting il
  fi   f pq k p
   

    

fia odel nductive b ias l earning

corollary    shows algorithm learning
requires

p

tasks using hypothesis space family

  f    f pq k p r

c





    

 

p

examples task ensure high probability average true error hypotheses
selects
within average empirical error sample   give
theorem showing learning algorithm required produce hypotheses whose average
true error within best possible error  achievable using
  arbitrary sequence
distributions
  within
factor number examples equation     
necessary 

sequence
probability distributions
  define






fifi c

  g c




n
c p
g c   r    g

c p






contains least two
pbe afi boolean
hypothesis space family
fifi let v c learning algorithm taking input p fi  c  samples
c
 
 
 


c
    producing output p hypotheses  
  c c  
 i mn    mn  
      f pq km i   p    
 



c
n

fifi probability least  over
exist distributions
 
random choice   
g   v c j      g c ffk

theorem     let
functions 

proof  see appendix c

ni 
f pq

      l inear hreshold n etworks

p
f pq

factor  sample complexity
theorems       show within constants
learning tasks using boolean hypothesis space family controlled complexity parameter
  section derive bounds
hypothesis space families constructed
thresholded linear combinations boolean feature maps  specifically  assume
form given                        squashing function v replaced hard
threshold 


v
otherwise

   

rfi





ry

dont restrict range feature output layer weights  note case
proof theorem   carry constants theorem   depend
lipschitz bound v  



f u

theorem     let hypothesis space family form given                       
hard threshold sigmoid function v   recall parameters   j input dimension 
number hidden nodes feature map number features  output nodes feature map 
   

fibaxter

  u f k k j u k  the number adjustable parameters feature
f pq u k j k j k u kz
p
 
proof  recall
  c   for  eachm   p   c ts            denotes feature map parameters p  
c


  let
denote matrix
 

 
 

 
 

 
 m    c
         c
  set binary p   matrices obtainable composing thresholded linear
note
   

respectively  let u
map   then 

functions elements
  restriction function must applied
element row  but functions may differ rows   slight abuse notation 
define

p fi    


 


     ap c









  c    
c
fix
  sauers
lemma  node first hidden layer feature map computes

f   functions p   input vectors   thus 
 tpqnb   k
 tpqn f k
distinct functions input output first hidden layer
p   points   fixing first hidden layer
u b
parameters  node second layer of  b the 
feature map computes  tpqn k functions image produced output
u  

first hidden layer  thus second hidden layer computes  tpqn k
functions output first hidden layer p   points   so  total 
b  
   tp     b
 

 
p
p fi   f kr u k

      number functions computable row    
now  possible matrix


 
thresholded linear combination output feature map   n j k   hence 
c    
  obtainable applying linear threshold functions
number binary sign assignments
  thus 
rows   n j k
b 
 
c  
 
 b
 
p fi   f  tkp u  tkp   p  tkp  
j
 

q convex function  hence ifigfi    
ik u gy
k

j
q j k u k j k u k jrq jiffk u q hgffk q
b
b
u
 
  
k

k








j


j ik u g kc
g
u k   g f k p j k shows
substituting
c   
 
u

 

p

k

k






p fi   k j p k
    
u

j

   

fia odel nductive b ias l earning

hence 

 tp j k u k





k
k

    
  p
j
  u k p j k
p fi    c definition f pq        observe    


u kn u k p j k j k u k shows
    setting  tp j k

u k  
     satisfied   u np k j k   j k
f
u
theorem     let theorem    following extra restrictions    j
f
 
j
f pq u p k j k



f
f
proof  bound apply lemma     present setting contains
f
u
three layer linear threshold networks input nodes  hidden nodes first hidden layer  j
 

u

hidden nodes second hidden layer one output node  theorem    bartlett        


 
lf u k u j   k fi

f
restrictions stated greater u n   hence
f ufi

u

n 

f  

j

j choose feature weight assignment feature map
j
identity j components input vector insensitive setting reminaing
components  hence generate j
points
whose image feature map
j
shattered linear threshold output node 
 

k

f   k

combining theorem    corrolary    shows

  fi f   u p k j k k p
examples task suffice learning p tasks using linear threshold hypothesis space family 
combining theorem    theorem    shows

    u p k j k k p
learning algorithm fail set p tasks 
   conclusion
problem inductive bias one broad significance machine learning  paper
introduced formal model inductive bias learning applies learner able
sample multiple related tasks  proved provided certain covering numbers computed
set hypothesis spaces available bias learner finite  hypothesis space
contains good solutions sufficiently many training tasks likely contain good solutions
novel tasks drawn environment 
specific case learning set features  showed number examples
j
u
required task  task training set obeys
  j number

p

  f k

   

npq

 

fibaxter

features u measure complexity feature class  showed bound
essentially tight boolean feature maps constructed linear threshold networks  addition 
proved number tasks required ensure good performance features novel
tasks u   showed good set features may found gradient
descent 
model paper represents first step towards formal model hierarchical approaches
learning  modelling learners uncertainty concerning environment probabilistic terms 
shown learning occur simultaneously base levellearn tasks
handand meta levellearn bias transferred novel tasks  technical
perspective  assumption tasks distributed probabilstically allows performance guarantees proved  practical perspective  many problem domains
viewed probabilistically distributed sets related tasks  example  speech recognition
may decomposed along many different axes  words  speakers  accents  etc  face recognition
represents potentially infinite domain related tasks  medical diagnosis prognosis problems
using pathology tests yet another example  domains benefit
tackled bias learning approach 
natural avenues enquiry include 

e

f



alternative constructions   although widely applicable  specific example feature
learning via gradient descent represents one possible way generating searching
hypothesis space family   would interesting investigate alternative methods 
including decision tree approaches  approaches inductive logic programming  khan
et al          whether general learning techniques boosting applied
bias learning setting 

e





algorithms automatically determining hypothesis space family   model
structure
fixed apriori represents hyper bias bias learner  would
interesting see extent structure learnt 

e

e



algorithms automatically determining task relatedness  ordinary learning usually little doubt whether individual example belongs learning task not 
analogous question bias learning whether individual learning task belongs
given set related tasks  contrast ordinary learning  always
clear cut answer  examples discussed here  speech
face recognition  task relatedness question  cases medical
problems clear  grouping large subset tasks together related tasks could
clearly detrimental impact bias learning multi task learning  emprical evidence support  caruana         thus  algorithms automatically determining
task relatedness potentially useful avenue research  context  see silver
mercer         thrun osullivan         note question task relatedness
clearly meaningful relative particular hypothesis space family  for example 
possible collections tasks related contains every possible hypothesis space  





extended hierarchies  extension two level approach arbitrarily deep hierarchies 
see langford         interesting question extent hierarchy
inferred data  somewhat related question automatic induction
structure graphical models 
   

fia odel nductive b ias l earning

acknowledgements
work supported various times australian postgraduate award  shell australia postgraduate fellowship  u k engineering physical sciences research council grants
k      k       australian postdoctoral fellowship  along way  many people
contributed helpful comments suggestions improvement including martin anthony 
peter bartlett  rich caruana  john langford  stuart russell  john shawe taylor  sebastian thrun
several anonymous referees 

appendix a  uniform convergence results
theorem   provides bound  uniform   probability large deviation
  
 p

  obtain general result  follow haussler        introduce
following parameterized class metrics  



 c


    e


  main theorem uniform bound probability large values
     
 p

 e 
  


  theorem   follow corollary 

  rather

 e 
better bounds realizable case
 appendix a    
p



lemma     following three properties

  

easily established 

e

  
  




n

l


p e
 

   e     


 



e

 

  e  

 





    p  


 




ease exposition dealing explicitly hypothesis spaces
q 
  j 
containing functions
  constructing loss functions q mapping
 
 
a  j 
q
  however  general view



loss function
 
j 
q function abstract set  
 
ignore particular construction

terms loss function   remainder section  unless otherwise stated 
j 
  considerably
hypothesis spaces sets functions mapping
 



convenient transpose notation c
 samples  writing
training sets columns
instead rows 


            

 equation   prior discussion  

fiff
  recalling definition






transposition lives
  following definition generalizes quantities



 
new setting 

definition    let
functions mapping

 
  let orsetssimply
map
denote















 







 











































   







j 





fibaxter

  let     denote set functions  given
      elements

   or equivalently element
writing
rows   define




 

 recall equation
  define      similarly  product probability measure        
        

 recall equation          
 not necessarily form       
define
            
  define
 recall equation        class functions mapping
                 
 
supremum product probability measures
 
size smallest    cover  recall definition    




 









































  

















 















































j 

































j 



























following theorem main result rest uniform convergence results
paper derived 

ba c






edf  g
ji g
     h 
gqp
 r    g st vu   g w     


permissible class functions mapping
theorem     let


j 








  let
generated
independent trials





according


product
probability
measure
 


 
 


k ml








n     





p  
 

 



 























following immediate corollary use later 

yx z u   g h      r  bac gd fe g ih

corollary     conditions theorem    


 





k l










j     















p   
 

 



 

 



    

gp g


    

a   proof theorem   
proof via double symmetrization argument kind given chapter   pollard        
borrowed ideas proof theorem   haussler        
   

fia odel nductive b ias l earning

a     f irst ymmetrization





  let







  
  
  
 
 
k  







extra piece notation 
bottom half  viz 













top half

l
  
  
  
 
 
l  















following lemma first symmetrization trick  relate probability large deviation
empirical estimate loss true loss probability large deviation
two independent empirical estimates loss 



mi g





lemma     let permissible set functions


j

 
 
probability measure


k l











n     



po

gp
n      rq


cfn



 

p  
   
 






yd k l










p

 






 


j 

let

 



ts g p








    

q
rq ts g uo g
q g zd
 w
q uo g
q uo g

proof  note first permissibility guarantees measurability suprema
p
   
   






 lemma    part     triangle inequality
 

   

 





 
 

 

 
  thus 







 q
k  v

vo ts g
xw
k v





















q 





 









 












  

 




 

 









   
   





    



chebyshevs
inequality  fixed  


k v











q  
 



 i g
k l













 df  g







   




gives result 








 









r



g


 r




 




  





   




gd p

 

  substituting last expression right hand side     

   

fibaxter

a     econd ymmetrization
second symmetrization trick bounds probability large deviation two empirical
estimates loss  i e  right hand side       computing probability large deviation elements randomly permuted first second sample  following
definition introduces appropriate permutation group purpose 

 
 









     
 

  h    let

f l
           
  l  

  permissible set functions
lemma     let
mapping
let  w
 as statement theorem      fix
st  cover      
 
 

rows
g
  then 
k l   h  j   o    q     ts g p
k v   h  q

ts gr     

 
  h  chosen uniformly random 
q     ts g  if
proof  fix     let

g st   without loss
  already done   choose






    now 
generality assume form


  ffu

fiff





  ffu



  ffp q



  ffu q



  ffp q

ts

  ffu q

ts
q q







definition    integers
  let
denote set permutations

 
 




 
sequence pairs integers



 

































  either


 
















j 




















p







 

 






























 








q



p









 











j





 



 




 










 




















   








cp




















 










 

 


















j









 
















j





















 




 













 





















 










 














fia odel nductive b ias l earning



p

hence  triangle inequality

 

q
q ts
q    
 q


q g   r construction
g  r   thus 
     implies
rq ts
v   h  xw
v   h  xw


p





 
 







 



 





 
 
 
 



 




c  
 
c  



 


 





p  



 


 


   
 









 

 
 

 





q
q ts
    
ts g assumption 
gd
q gry



 




 












 







 



 












 
















gives      



 be function written form
  
k mv   b   q ts gr yd vu   g
    
  h  chosen uniformly random 


proof    f   
q

ts





p




q ts


    








ffu

    let
simplify notation denote
fiff
fiff   pair  


lff independent random variable










fiff
probability



lff

fiff probability        
k mv   h  q gr




l




fiffyl
k   h  q

ts gr

  ffp


  ffp



fiff  
k
fiff gr

  ffu

ffu


  bounded ranges


  hofor zero mean independent random variables
effdings inequality  devroye  gyorfi    lugosi       


k  
h yd vu   d 


 



 
bound probability term right hand side      





lemma     let
 














 







j









 


 












j





































 


 











 






j 



q 





























 







 












 








j





















































   



 

















fibaxter


fiff


fiff
 

fiff  yd vu   g
ffu
fiffv
k l
fiff gr

ffp
fiff


  ffp

ffu

let

ffp
fiff  
fiff  
ffu
fiff
fiff   hence 

vu   g
  ffu
 
lff ffu

lff yd vu   g


r   hence
giving value
minimized setting
k v   f  q ts gr yd vu   g


noting range












j











 


























j





e





dj















j

















e




































j

j



 j



 


























required 



      give 
k l   h  j   o    q     ts g p
yd    g st
vu  w g
      
empirical distribution
note
simply  




 recall definition     hence 
puts point mass


n      q g p
k l   h 

yd    g st vu  w g
now  random choice  
fiff independently  but identically  distributed  
 
ever swaps
fiff
 so   swaps
fiff drawn according another component
drawn according distribution   thus integrate respect choice
  write
j      q g p
k l

yd    g st vu  w g
a     p utting


fixed




ogether


  lemmas





 

 






 










 









g



 



















e



e





e



 



j





























 

 




 




























j



















p

 






 








applying lemma    expression gives theorem    
   









j



fia odel nductive b ias l earning

a   proof theorem  

    

 

another piece notation required proof  hypothesis space



measures
  let



 






  o 

     





 








    
k  
 



probability



 

another empirical
note used   rather   indicate

  
estimate
 
 
c
generated se  sampling process  addition
sample



although supplied learner 
quence probability measures 






means
notion used
following lemma 

n
probability generating sequence measures environment

c  
 sample according
holds 

 

 

lemma    



k l  










 




  f n    





k l         
       
k l







   




  x
p

     
 





   






g p dg





















    p
  





proof  follows directly triangle inequality





g p dg







    



    

g p g


 

treat two inequalities lemma    separately 

a     nequality     





following lemma replace supremum
   
lemma    


k ml  











inequality      supremum

f j     
qg p


f n       
k    


















p   
 









   









p  ep
 

 



 



gh

    

fibaxter


 
  




     

p





g
ji  

n

 

 e 
 
proof  suppose
  let satisfy


 

  
  

  definition
 

equality  suppose first


 
  
 e 
exists  
  hence property    






    
 e 




metric 
  exists

  pick arbitrary
 
 
 
 
 e 
satisfying inequality  definition 

 



   
 
 


 by assumption   compatibility
ordering




p    
 
 


p


 
reals 
  say  triangle inequality  





 

u



g



 




g g g





g g


thus

  g g g     satisfying inequality
g


found  choosing
shows exists

 

instead 
  identical argument run role  
interchanged  thus cases 
     
g w

g
p   p
 



p    
 

 
 








 



 

qc   p
 e 
p    
 














    p
 












 





     
 





  







 

qp    
 

 





 





 







completes proof lemma 
 
nature c
sampling process 


        


j       
    k  

 
ay

k    
















p   
 

 





 

gh

 

p g h       
h permissible assumed

permissibility  lemma     appendix d   hence
satisfies conditions corollary   
g g g corollary
combining lemma     equation      substituting g
















gq

 
 



































   gives following lemma sample size required ensure      holds 

 x z u l g f         g g g p
f j     



lemma    



k l  


 



























p   
 




 
 








 
g

g
g
 



 x z u l g          g g   g p

a     nequality     














 



g p dg


f  



 


note  




  i e expectation

distributed according   bound left hand side      apply corollary

  
  replaced   replaced   replaced

respectively 
replaced replaced   note
permissible whenever  lemma     
thus 

 





















 



   



g

    

fia odel nductive b ias l earning

inequality      satisfied 
now  putting together lemma     lemma    equation     proved following

general version theorem   

 i g g
yx z u l g          g g g p
   g   p

 
x
z
u
l
g

g      
g

  j     
k l
gp g

 m
get theorem    observe




 
   r
setting g
maximizing g
gives
  substituting g
p

 
 sample gentheorem     let permissible hypothesis space family let c
n










erated environment
 

 





 

















p    
  
 


































 





















 
 
 






 












  ed


  



 


theorem    gives theorem   

 
g

a   realizable case

 


 


g g
g
g
g   g  
g
ji g g  

corollary     conditions theorem      
x z u l g g           g g   g g   p
  g   p


 
x
z
u
l
g

g g        
g g  

g
  n     
k l
 p g
g


bounds particularly useful know
  set g
g   
 which maximizes g




 

theorem   sample complexity
scales
  improved

  



 
 ep





instead requiring
  require  












 




  see this  observe  






     
   







  setting
theorem    treating constant
gives 





 







 





 







 





  



















  z b
composition two function classes note



write
r




form given       written



r























recalling definition   






 


 p





appendix b  proof theorem  










 p









 



























 



 

 







   





 









 







define

fibaxter

bt z   thus  setting b  


    
   
following two lemmas enable us bound   






lemma     let
form

     
 
      
         

 

 
 




proof  fix measure

let minimum size  cover
 

 

 
 


 
 

 
 



definition
b let
measure
defined

  set in 



is  measurable  
   algebra
  measurable


 
f


     let

let

  definition again 
 
 
  andsize  cover
  note

minimum
      lemma





 
 




shown
 cover
  so  given
choose
proved
  h       v       now 
           
   
   
line follows
first line follows triangle inequality second






f









facts 
 
   
    thus
     cover    
result follows 
 definition     following lemma 
recalling definition  
lemma    
     m      



  let
   
proof  fix product probability measure
 
 
   covers   let     given  
     choose    o 

    now 





 
 
 
  

 


 

 



  o 


 

 
 
result follows 
thus    cover   



 









j 



j 









































 





l



























c












 









































c

























c



e

















 











































 































 



















j

 

 



































































 









 

























e























   

































 















fia odel nductive b ias l earning

b   bounding

    






j

      e            e
lemma    
         
    
 

using similar techniques used prove lemmas       
satisfy
          
lemma    


































 















    







    


shown





    

equations                       together imply inequality      

 

 
   hypothesis space
wish prove   


x

family form

 
f    note f corresponds
 
    

  defined
probability measure induces probability measure
zz       
   algebra
  note
  bounded  positive functions

arbitrary set  
               
    






 




 






  let f
let probability measure space probability measures





  then 
two elements corresponding hypothesis spaces


          
            by      above 


   
        




      guaranteed
permissibility  lemma    part    apthe measurability



 


have 
pendix d  
f
    
    
      m  e
 

b   bounding

























 












   

























j









































 

















   





   





 



















   

















 













 





















gives inequality      
   















   

 





























fibaxter

b   proof theorem  
order prove bounds theorem   apply theorem   neural network
hypothesis space family equation       case structure




g

g  g g g
 







 
 
bounded

subset
lipschitz squashing function     feature class

set one hidden layer neural networks inputs  hidden nodes  outputs   
squashing function weights
fiff bounded subset   lipschitz
restriction   bounded restrictions weights ensure lipschitz
b



classes  hence exists
     
  w  



    norm
 
case  loss function squared loss 
  hence   probability measures

  onnow   recall assumed
output space
  
     l
 
 
yd     
    

 
 
   

marginal distribution
derived   similarly 
 
probability measures
 


  yd      
    
define
    e      o         e

supremum probability measures  the borel subsets of 
 

 
 
 
 
e






size


smallest
 cover



metric 
similarly
set 
 
    e      o         e
supremum probability measures   equations           imply
        d
    
        
d 
    

applying theorem    haussler         find







 

    
  d      
w











 





j 



























m 

 









 



j 






 

 



























 


















































 


























































j 









m 





j 































































































substituting two expressions           applying theorem   yields theorem
  
   

fia odel nductive b ias l earning

appendix c  proof theorem   
proof follows similar argument one presented anthony bartlett       
ordinary boolean function learning 
first need technical lemma 

 

g
   
k  




k              g  r                 n  n  
 
  denote number occurences random sequence        
proof  let


function viewed decision rule  i e  based observations     tries guess














whether probability
bayes

dis   wd   and 
  optimal
rule isd otherwise 
  decision
estimator     
hence 
k      g k   g


k   g
kd   g







half probability binomial
random variable least wd  
sluds inequality  slud        
k      g k


g









lemma     let random variable uniformly distributed




 
  let
i i d 
 valued random variables g
  



  function mapping
 






















 

 



























 

















 





























 





 







 



 















 





























 





 

 







n





















j

  tates inequality  tate        states
normal
 

k






  









   n

 

 





  shattered     row   let
set
let  

 
   
distributions

contained
 

 


 



 
fiff









th   row ofd     f  
 

  let
 
 
 
 

   achieved sequence







note  
  optimal error  


fiff  

fiff
 
always contains sequence shatters     optimal error

       

    

 

  ffu
combining last two inequalities completes proof 













 


 

c





 



 

 














c








 







c















 



 
 















c





 

 



c



 













 











g





 

   







 

 

c





 







fibaxter








 
p       










    


fiff   

fiff

 sample   let element
fiff array
  
  
  
 
  
 
 
  
equal number occurrences
fiff  
     uniformly random   generate now  select  

   the output learning algorithm  have 
sample using    


fiff   

lff c b     

fiff   

lff  
b         
fiff   
fiff
lff

  ffu
  probability generating configuration  
fiff  sampling
  
process sum possible configurations  lemma    
 
lff   
fiff
fiff er df  g        h   nn ji
 
 

c  





 







































c  



























































fiff   




k




c













































j 

j 

 



b  
























c

r  


  




 



 

 






























 

 




 



    r

  ffp
m  l  n   l
n


k






















h
         n n ij
 





    


      

g









   

df

g

 

 k   k     

      g





    


  plugging      shows


k  



c  

 valued random variable   g














lff   

g
o  l  n   l
 
 

g nr  
  k   k    n  






jensens inequality  since
implies 











c

















hence



































 















c





g


fia odel nductive b ias l earning

 

 



 

 

since inequality holds random choice   must hold specific choice

algorithm
sequence distributions
  hence learning

k

 



 




setting

assuming equality       get











g  




g



e



    

        g




 

g







 

 





e



k



g g



ensures

     g



 









    

 g



solving        substituting expressions g shows      satisfied
provided
  g          g g
    
g
r   r since g  r g g    assuming
setting
g  somefor
yd        becomes

    

     
r right hand side      approximately maximized
subject constraint
qpsr     point value  exceeds
cdf d d     thus      g

r
d d e
    
 
g













 









c







 












e










j

 



c

 





















c









 



c





q



 

 





 













      g
 
contains least two
g
obtain  dependence theorem    observe assumption
 ut two distributions
functions
  hence exists
  
  let
 



 t

 

 
concentrated



       let     
b       h  product distributions

v
 
   generated  t     note

    one  
learning algorithm
chooses wrong hypothesis  

        
k




















 

 













 









 





 











 






























 

















   



























 





fibaxter

 

 

    generate
  n
       nr         x  w w n  


now  choose uniformly random
cording   lemma    shows

k  
g
least









 



 






 

























c





 





 sample

ac 





r





 





ji g  r   combining two constraints
x z u finishes proof 


c  



    f      g g


provided





    


        with





p         using


appendix d  measurability
order theorems      hold full generality impose constraint called
permissibility hypothesis space family   permissibility introduced pollard       
ordinary hypothesis classes   definition similar dudleys image admissible
suslin  dudley         extending definition cover hypothesis space families 
throughout section assume functions map  the complete separable metric
j 
  let denote borel  algebra topological space   section
space 
     view   set probability measures   topological space equipping
topology weak convergence   algebra generated topology 
following two definitions taken  with minor modifications  pollard        







 





j 



 

 valued functions indexed set
definition    set
r

j 





definition    set
  











 



zff

q







permissible indexed set







exists function



analytic subset polish  space  


 


   function
 algebra









j 

indexing



   y  
analytic subset polish space




measurable respect product


simply continuous image borel subset
another polish space   analytic subsets polish space include borel sets 
important projections analytic sets analytic  measured complete
measure space whereas projections borel sets necessarily borel  hence cannot
measured borel measure  details see dudley         section      
lemma    

  









j 

permissible








permissible 

proof  omitted 
define permissibility hypothesis space families 
   topological space called polish metrizable complete separable metric space 

   

fia odel nductive b ias l earning

w







definition     hypothesis space family
permissible exist sets





analytic subsets polish spaces respectively  function


measurable respect

 

    y    y
ba



fe hg





  e






zff







dc






 



j 

 



let
analytic subset polish space  let
measure space

denote analytic subsets   following three facts analytic sets taken
pollard         appendix c 

fe hg


 a 

 
 b 









complete

 



e




 

e  
   
  projection onto

 

contains product  algebra

 c  set





 



 

 



 




 








recall definition   definition   following lemma assume
 
n
  complete
completed respect probability measure  
respect environmental measure  





lemma     permissible hypothesis space family  
   permissible 


f permissible 
  
permissible
 

 
 
 
 
   measurable  
  
   measurable
 
   permissible 
simply set
proof  absorbed loss function hypotheses  
 
 fold products
  thus     follows lemma            
   g







 

 



 





















immediate definitions  permissible       proved
identical argument used measurable suprema section pollard         appendix
c 


j 


j 
  function
defined
     note borel measurable








borel measurable kechris        chapter      now  permissibility




 

measurable
automatically implies permissibility

 
    
r



j 
appropriate way  prove     
let indexed




j 
 
 

 

e





 e



  fubinis theorem
define




j 
e






 measurable function  let
defined

   e
indexes
appropriate way
permissible  provided
 

 measurable  analyticity becomes important  let
shown




   e  e
  property  b  analytic sets 
 


contains

 
e



 
e








set
projection
onto
  property  c 
n

analytic 

assumed complete 
measurable  property  a   thus
measurable function permissibility
follows 

 

kj  

 

 




  m   fiff
qp  

  on     r fiy
g
c c  
 
p


 

g





w

lj  
 





 

   

c

c

f    f
 










 



c

fibaxter

references
abu mostafa  y          method learning hints  hanson  s  j   cowan  j  d     giles 
c  l   eds    advances neural information processing systems    pp       san mateo 
ca  morgan kaufmann 
anthony  m     bartlett  p  l          neural network learning  theoretical foundations  cambridge university press  cambridge  uk 
bartlett  p  l          lower bounds vc dimension multi layer threshold networks 
proccedings sixth acm conference computational learning theory  pp       
new york  acm press  summary appeared neural computation     no    
bartlett  p  l          sample complexity pattern classification neural networks 
size weights important size network  ieee transactions
information theory                
baxter  j       a   learning internal representations  ph d  thesis  department mathematics statistics  flinders university south australia  copy available
http   wwwsyseng anu edu au  jon papers thesis ps gz 



baxter  j       b   learning internal representations  proceedings eighth international
conference computational learning theory  pp          acm press  copy available
http   wwwsyseng anu edu au  jon papers colt   ps gz 



baxter  j       a   bayesian information theoretic model learning learn via multiple task
sampling  machine learning          
baxter  j       b   canonical distortion measure vector quantization function approximation  proceedings fourteenth international conference machine learning 
pp        morgan kaufmann 
baxter  j     bartlett  p  l          canonical distortion measure feature space   nn
classification  advances neural information processing systems     pp          mit
press 
berger  j  o          statistical decision theory bayesian analysis  springer verlag  new
york 
blumer  a   ehrenfeucht  a   haussler  d     warmuth  m  k          learnability vapnikchervonenkis dimension  journal acm             
caruana  r          multitask learning  machine learning           
devroye  l   gyorfi  l     lugosi  g          probabilistic theory pattern recognition 
springer  new york 
dudley  r  m          course empirical processes  vol       lecture notes mathematics  pp        springer verlag 
dudley  r  m          real analysis probability  wadsworth   brooks cole  california 
   

fia odel nductive b ias l earning

gelman  a   carlin  j  b   stern  h  s     rubim  d  b   eds            bayesian data analysis 
chapman hall 
good  i  j          history hierarchical bayesian methodology  bernardo  j  m  
groot  m  h  d   lindley  d  v     smith  a  f  m   eds    bayesian statistics ii  university
press  valencia 
haussler  d          decision theoretic generalizations pac model neural net
learning applications  information computation             
heskes  t          solving huge number similar tasks  combination multi task learning
hierarchical bayesian approach  shavlik  j   ed    proceedings   th international
conference machine learning  icml      pp          morgan kaufmann 
intrator  n     edelman  s          make low dimensional representation suitable
diverse tasks  connection science    
kechris  a  s          classical descriptive set theory  springer verlag  new york 
khan  k   muggleton  s     parson  r          repeat learning using predicate invention  page 
c  d   ed    proceedings  th international workshop inductive logic programming
 ilp      lnai       pp         springer verlag 
langford  j  c          staged learning  tech  rep   cmu  school computer science 
http   www cs cmu edu  jcl research ltol staged latest ps 



mitchell  t  m          need biases learning generalisations  dietterich  t  g    
shavlik  j   eds    readings machine learning  morgan kaufmann 
parthasarathy  k  r          probabiliity measures metric spaces  academic press  london 
pollard  d          convergence stochastic processes  springer verlag  new york 
pratt  l  y          discriminability based transfer neural networks  hanson  s  j  
cowan  j  d     giles  c  l   eds    advances neural information processing systems   
pp          morgan kaufmann 
rendell  l   seshu  r     tcheng  d          layered concept learning dynamically variable
bias management  proceedings tenth international joint conference artificial
intelligence  ijcai      pp          ijcai   inc 
ring  m  b          continual learning reinforcement environments  r  oldenbourg verlag 
russell  s          use knowledge analogy induction  morgan kaufmann 
sauer  n          density families sets  journal combinatorial theory a     
       
sharkey  n  e     sharkey  a  j  c          adaptive generalisation transfer knowledge 
artificial intelligence review            
   

fibaxter

silver  d  l     mercer  r  e          parallel transfer task knowledge using dynamic
learning rates based measure relatedness  connection science            
singh  s          transfer learning composing solutions elemental sequential tasks  machine learning            
slud  e          distribution inequalities binomial law  annals probability            
suddarth  s  c     holden  a  d  c          symolic neural systems use hints developing complex systems  international journal man machine studies             
suddarth  s  c     kergosien  y  l          rule injection hints means improving network performance learning time  proceedings eurasip workshop neural
networks portugal  eurasip 
sutton  r          adapting bias gradient descent  incremental version delta bar delta 
proceedings tenth national conference artificial intelligence  pp          mit
press 
tate  r  f          double inequality normal distribution  annals mathematical
statistics             
thrun  s          learning n th thing easier learning first   advances neural
information processing systems    pp          mit press 
thrun  s     mitchell  t  m          learning one thing  proceedings international
joint conference artificial intelligence  pp            morgan kaufmann 
thrun  s     osullivan  j          discovering structure multiple learning tasks  tc algorithm  saitta  l   ed    proceedings   th international conference machine
learning  icml      pp          morgen kaufmann 
thrun  s     pratt  l   eds            learning learn  kluwer academic 
thrun  s     schwartz  a          finding structure reinforcement learning  tesauro  g  
touretzky  d     leen  t   eds    advances neural information processing systems  vol    
pp          mit press 
utgoff  p  e          shift bias inductive concept learning  machine learning  artificial
intelligence approach  pp          morgan kaufmann 
valiant  l  g          theory learnable  comm  acm               
vapnik  v  n          estimation dependences based empirical data  springer verlag  new
york 
vapnik  v  n          nature statistical learning theory  springer verlag  new york 

   


