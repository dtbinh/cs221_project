journal of artificial intelligence research              

submitted       published     

planning graph as a  dynamic  csp 
exploiting ebl  ddb and other csp search techniques in graphplan
subbarao kambhampati

rao   asu   edu

department of computer science and engineering
arizona state university  tempe az           

abstract
this paper reviews the connections between graphplans planning graph and the dynamic
constraint satisfaction problem and motivates the need for adapting csp search techniques to the
graphplan algorithm  it then describes how explanation based learning  dependency directed backtracking  dynamic variable ordering  forward checking  sticky values and random restart search
strategies can be adapted to graphplan  empirical results are provided to demonstrate that these
augmentations improve graphplans performance significantly  up to     x speedups on several
benchmark problems  special attention is paid to the explanation based learning and dependency
directed backtracking techniques as they are empirically found to be most useful in improving the
performance of graphplan 

   introduction
graphplan  blum   furst        is currently one of the more efficient algorithms for solving classical planning problems  four of the five competing systems in the recent aips    planning competition were based on the graphplan algorithm  mcdermott         extending the efficiency of
the graphplan algorithm thus seems to be a worth while activity  in  kambhampati  parker   
lambrecht         we provided a reconstruction of graphplan algorithm to explicate its links to
previous work in classical planning and constraint satisfaction  one specific link that was discussed
is the connection between the process of searching graphplans planning graph  and solving a dynamic constraint satisfaction problem  dcsp   mittal   falkenhainer         seen from the dcsp
perspective  the standard backward search proposed by blum and furst        lacks a variety of ingredients that are thought to make up efficient csp search mechanisms  frost   dechter       
bayardo   schrag         these include forward checking  dynamic variable ordering  dependency directed backtracking and explanation based learning  tsang        kambhampati        
in  kambhampati et al          i have suggested that it would be beneficial to study the impact of
these extensions on the effectiveness of graphplans backward search 
in this paper  i describe my experiences with adding a variety of csp search techniques to improve graphplan backward searchincluding explanation based learning  ebl  and dependencydirected backtracking capabilities  ddb   dynamic variable ordering  forward checking  sticky
values  and random restart search strategies  of these  the addition of ebl and ddb capabilities
turned out to be empirically the most useful  both ebl and ddb are based on explaining failures
at the leaf nodes of a search tree  and propagating those explanations upwards through the search
tree  kambhampati         ddb involves using the propagation of failure explanations to support
intelligent backtracking  while ebl involves storing interior node failure explanations  for pruning
future search nodes  graphplan does use a weak form of failure driven learning that it calls mem 

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fik ambhampati

oization  as we shall see in this paper  graphplans brand of learning is quite limited as there is
no explicit analysis of the reasons for failure  instead the explanation of failure of a search node is
taken to be all the constraints in that search node  as explained in  kambhampati         this not
only eliminates the opportunities for dependency directed backtracking  it also adversely effects the
utility of the stored memos 
adding full fledged ebl and ddb capabilities in effect gives graphplan both the ability to
do intelligent backtracking  and the ability to learn generalized memos that are more likely to be
applicable in other situations  technically  this involves generalizing conflict directed backjumping
 prosser         a specialized version of ebl ddb strategy applicable for binary csp problems 
to work in the context of dynamic constraint satisfaction problems  as discussed in  kambhampati          empirically  the ebl ddb capabilities improve graphplans search efficiency quite
dramaticallygiving rise to up to     x speedups  and allowing graphplan to easily solve several
problems that have hither to been hard or unsolvable  in particular  i will report on my experiments
with the bench mark problems described by kautz and selman         as well as   other domains 
some of which were used in the recent aips planning competition  mcdermott        
i discuss the utility issues involved in storing and using memos  and point out that the graphplan
memoization strategy can be seen as a very conservative form of csp no good learning  while this
conservative strategy keeps the storage and retrieval costs of no goods the usual bane of no good
learning strategiesunder control  it also loses some learning opportunities  i then present the use
of sticky values as a way of recouping some of these losses  empirical studies show that sticky
values lead to a further    x improvement over ebl 
in addition to ebl and ddb  i also investigated the utility of forward checking and dynamic
variable ordering  both in isolation and in concert with ebl and ddb  my empirical studies show
that these capabilities typically lead to an additional    x speedup over ebl ddb  but are not by
themselves competitive with ebl ddb 
finally  i consider the utility of the ebl ddb strategies in the context of random restart search
strategies  gomes  selman    kautz        that have recently been shown to be good at solving hard combinatorial problems  including planning problems  my results show that ebl ddb
strategies retain their advantages even in the context of such random restart strategies  specifically 
ebl ddb strategies enable graphplan to use the backtrack limits more effectivelyallowing it to
achieve higher solvability rates  and more optimal plans with significantly smaller backtrack and
restart limits 
this paper is organized as follows  in the next section  i provide some background on viewing
graphplans backward search as a  dynamic  constraint satisfaction problem  and review some of
the opportunities this view presents  in section    i discuss some inefficiencies of the backtracking
and learning methods used in normal graphplan that motivate the need for ebl ddb capabilities 
section   describes how ebl and ddb are added to graphplan  section   presents empirical studies
demonstrating the usefulness of these augmentations  section   investigates the utility of forward
checking and dynamic variable ordering strategies for graphplan  section   investigates the utility
of ebl ddb strategies in the context of random restart search  section   discusses related work
and section    presents conclusions and some directions for further work 
   binary csp problems are those problems where all initial constraints are between pairs of variables 

 

fip lanning g raph

action list
level k  

proposition list
level k  

action list
level k

proposition list
level k

g         g    p     p 
domains  g    fa  g  g    fa  gg    fa  gg    fa  g
p    fa  gp    fa    a   gp    fa  gp    fa    a  g
p    fa   gp    fa   g
constraints  normal  p    a    p     a 
p    a    p     a 
p    a     p     a 
constraints  activity   g    a    activefp    p    p  g
g    a    activefp  g
g    a    activefp  g
g    a    activefp    p  g
init state  activefg    g    g    g  g

p 
a 

a 

x
a 
a 

p 

a   

a 

g 
g 

p 
p 

a   

g 

p 

a 

x

csp

variables 

a 

x

as a

a 

g 

p 
a 

 a  planning graph

 b  dcsp

figure    a planning graph and the dcsp corresponding to it

   review of graphplan algorithm and its connections to dcsp
    review of graphplan algorithm
graphplan algorithm  blum   furst        can be seen as a disjunctive version of the forward
state space planners  kambhampati et al         kambhampati         it consists of two interleaved
phases  a forward phase  where a data structure called planning graph is incrementally extended 
and a backward phase where the planning graph is searched to extract a valid plan  the planninggraph consists of two alternating structures  called proposition lists and action lists  figure   shows
a partial planning graph structure  we start with the initial state as the zeroth level proposition list 
given a k level planning graph  the extension of structure to level k     involves introducing all
actions whose preconditions are present in the k th level proposition list  in addition to the actions
given in the domain model  we consider a set of dummy persist actions  one for each condition
in the k th level proposition list  a persist c action has c as its precondition and c as its effect 
once the actions are introduced  the proposition list at level k     is constructed as just the union of
the effects of all the introduced actions  planning graph maintains the dependency links between the
actions at level k     and their preconditions in level k proposition list and their effects in level k    
proposition list  the planning graph construction also involves computation and propagation of
mutex constraints  the propagation starts at level    with the actions that are statically interfering
with each other  i e   their preconditions and effects are inconsistent  labeled mutex  mutexes are
then propagated from this level forward by using a two simple rules  two propositions at level k are
marked mutex if all actions at level k that support one proposition are mutex with all actions that
support the second proposition  two actions at level k     are mutex if they are statically interfering
or if one of the propositions  preconditions  supporting the first action is mutually exclusive with
one of the propositions supporting the second action 
the search phase on a k level planning graph involves checking to see if there is a sub graph
of the planning graph that corresponds to a valid solution to the problem  this involves starting
with the propositions corresponding to goals at level k  if all the goals are not present  or if they are
present but a pair of them are marked mutually exclusive  the search is abandoned right away  and
planning grap is grown another level   for each of the goal propositions  we then select an action
 

fik ambhampati

g         g    p     p 
g    fa  g  g    fa  gg    fa  gg    fa  g
p    fa  gp    fa    a   gp    fa  gp    fa    a  g
p    fa   gp    fa   g
constraints  normal  p    a    p     a 
p    a    p     a 
p    a     p     a 
constraints  activity   g    a    activefp    p    p  g
g    a    activefp  g
g    a    activefp  g
g    a    activefp    p  g
init state  activefg    g    g    g  g

g         g    p     p 
g    fa     g  g    fa     gg    fa     gg    fa     g
p    fa     gp    fa    a      gp    fa     gp    fa    a     g
p    fa      gp    fa      g
constraints  normal  p    a    p     a 
p    a    p     a 
p    a     p     a 
constraints  activity   g    a    p       p       p     
g    a    p     
g    a    p     
g    a    p       p     
init state  g       g       g       g     

variables 

variables 

domains 

domains 

 a  dcsp

 b  csp

figure    compiling a dcsp to a standard csp
from the level k action list that supports it  such that no two actions selected for supporting two
different goals are mutually exclusive  if they are  we backtrack and try to change the selection of
actions   at this point  we recursively call the same search process on the k     level planning graph 
with the preconditions of the actions selected at level k as the goals for the k     level search  the
search succeeds when we reach level    corresponding to the initial state  
consider the  partial  planning graph shown in figure   that graphplan may have generated
and is about to search for a solution  g     g  are the top level goals that we want to satisfy 
and a     a  are the actions that support these goals in the planning graph  the specific actionprecondition dependencies are shown by the straight line connections  the actions a     a   at the
left most level support the conditions p     p  in the planning graph  notice that the conditions p 
and p  at level k     are supported by two actions each  the x marked connections between the
actions a    a    a    a  and a    a   denote that those action pairs are mutually exclusive   notice
that given these mutually exclusive relations alone  graphplan cannot derive any mutual exclusion
relations at the proposition level p     p    
    connections between graphplan and csp
the graphplan algorithm as described above bears little resemblance to previous classical planning
algorithms  in  kambhampati et al          we explicate a number of important links between
the graphplan algorithm and previous work in planning and constraint satisfaction communities 
specifically  i show that a planning graph of length k can be thought of  to a first approximation  as a
disjunctive  unioned  version of a k  level search tree generated by a forward state space refinement 
with the action lists corresponding to the union of all actions appearing at k th level  and proposition
lists corresponding to the union of all states appearing at the k th level  the mutex constraints
can be seen as providing  partial  information about which subsets of a proposition list actually
correspond to legal states in the corresponding forward state space search  the process of searching
the planning graph to extract a valid plan from it can be seen as a dynamic constraint satisfaction
problem  since this last link is most relevant to the work described in this paper  i will review it
further below 
the dynamic constraint satisfaction problem  dcsp   mittal   falkenhainer        is a generalization of the constraint satisfaction problem  tsang         that is specified by a set of variables 
 

fip lanning g raph

as a

csp

activity flags for the variables  the domains of the variables  and the constraints on the legal variablevalue combinations  in a dcsp  initially only a subset of the variables is active  and the objective is
to find assignments for all active variables that is consistent with the constraints among those variables  in addition  the dcsp specification also contains a set of activity constraints  an activity
constraint is of the form  if variable x takes on the value vx   then the variables y  z  w    become
active 
the correspondence between the planning graph and the dcsp should now be clear  specifically  the propositions at various levels correspond to the dcsp variables    and the actions supporting them correspond to the variable domains  there are three types of constraints  action mutex
constraints  fact  proposition  mutex constraints and subgoal activation constraints 
since actions are modeled as values rather than variables  action mutex constraints have to be
modeled indirectly as constraints between propositions  if two actions a  and a  are marked mutex
with each other in the planning graph  then for every pair of propositions p   and p   where a  is
one of the possible supporting actions for p   and a  is one of the possible supporting actions for
p     we have the constraint 

   p     a    p     a   
fact mutex constraints are modeled as constraints that prohibit the simultaneous activation of
the two facts  specifically  if two propositions p   and p   are marked mutex in the planning graph 
we have the constraint 

   active p       active p     

subgoal activation constraints are implicitly specified by action preconditions  supporting an
active proposition p with an action a makes all the propositions in the previous level corresponding
to the preconditions of a active 
finally  only the propositions corresponding to the goals of the problem are active in the beginning  figure   shows the dynamic constraint satisfaction problem corresponding to the example
planning graph that we discussed 
      s olving

a

dcsp

there are two ways of solving a dcsp problem  the first  direct  approach  mittal   falkenhainer 
      involves starting with the initially active variables  and finding a satisfying assignment for
them  this assignment may activate some new variables  and these newly activated variables are
assigned in the second epoch  this process continues until we reach an epoch where no more new
variables are activated  which implies success   or we are unable to give a satisfying assignment to
the activated variables at a given epoch  in this latter case  we backtrack to the previous epoch and
try to find an alternative satisfying assignment to those variables  backtracking further  if no other
assignment is possible   the backward search process used by the graphplan algorithm  blum  
furst        can be seen as solving the dcsp corresponding to the planning graph in this direct
fashion 
the second approach for solving a dcsp is to first compile it into a standard csp  and use
the standard csp algorithms  this compilation process is quite straightforward and is illustrated in
   note that the same literal appearing in different levels corresponds to different dcsp variables  thus  strictly speaking  a literal p in the proposition list at level i is converted into a dcsp variable pi   to keep matters simple  the
example in figure   contains syntactically different literals in different levels of the graph 

 

fik ambhampati

figure    the main idea is to introduce a new null value  denoted by    into the domains of
each of the dcsp variables  we then model an inactive dcsp variable as a csp variable which
takes the value    the constraint that a particular variable p be active is modeled as p      thus 
activity constraint of the form

g    a    activefp    p    p  g
is compiled to the standard csp constraint

g    a    p       p       p     
it is worth noting here that the activation constraints above are only concerned about ensuring
that propositions that are preconditions of a selected action do take non   values  they thus allow
for the possibility that propositions can become active  take non   values  even though they are
strictly not supporting preconditions of any selected action  although this can lead to inoptimal
plans  the mutex constraints ensure that no unsound plans will be produced  kautz   selman 
       to avoid unnecessary activation of variables  we need to add constraints to the effect that
unless one of the actions needing that variable as a precondition has been selected as the value for
some variable in the earlier  higher  level  the variable must have   value  such constraints are
typically going to have very high arity  as they wind up mentioning a large number of variables in
the previous level   and may thus be harder to handle during search 
finally  a mutex constraint between two propositions

   active p       active p     
is compiled into

   p        p         

since action mutex constraints are already in the standard csp form  with this compilation  all
the activity constraints are converted into standard constraints and thus the entire csp is now a
standard csp  it can now be solved by any of the standard csp search techniques  tsang         
the direct method has the advantage that it closely mirrors the graphplans planning graph
structure and its backward search  because of this  it is possible to implement the approach on the
plan graph structure without explicitly representing all the constraints  furthermore  as i will discuss in section    there are some distinct advantages for adopting the dcsp view in implementing
ebl ddb on graphplan  the compilation to csp requires that plan graph be first converted into
an extensional csp  it does however allow the use of standard algorithms  as well as supports nondirectional search  in that one does not have to follow the epoch by epoch approach in assigning
variables    since my main aim is to illustrate the utility of csp search techniques in the context of
the graphplan algorithm  i will adopt the direct solution method for the dcsp  for a study of the
tradeoffs offered by the technique of compiling the planning graph into a csp  the reader is referred
to  do   kambhampati        
   it is also possible to compile any csp problem to a propositional satisfiability problem  i e   a csp problem with
boolean variables   this is accomplished by compiling every csp variable p that has the domain fv    v         vn g
into n boolean variables of the form p is v    p is vn   every constraint of the form p
vj           is compiled
to p is vj            this is essentially what is done by the blackbox system  kautz   selman        
   compilation to csp is not a strict requirement for doing non directional search  in  zimmerman   kambhampati 
       we describe a technique that allows the backward search of graphplan to be non directional  see the discussion
in section    

 

 

fip lanning g raph

as a

csp

    interpreting mutex propagation from the csp view
viewing the planning graph as a constraint satisfaction problem helps put the mutex propagation
in a clearer perspective  see  kambhampati et al           specifically  the way graphplan constructs its planning graph  it winds up enforcing partial directed   consistency and partial directed
  consistency  tsang         the partial   consistency is ensured by the graph building procedure
which introduces an action at level l only if the actions preconditions are present in the proposition
list at level l     and are not mutually exclusive  partial   consistency is ensured by the mutual
exclusion propagation procedure 
in particular  the graphplan planning graph construction implicitly derives no good  constraints of the form 

 active pmi    or pmi     

i will be simply removed from  or will not be put into  the level i  and the mutex
in which case pm
constraints of the form 




  active pmi     active pni  

or pmi      pni     

 

i and p i are marked mutually exclusive 
in which case pm
n
both procedures are directed in that they only use reachability analysis in enforcing the consistency  and are partial in that they do not enforce either full   consistency or full   consistency 
lack of full   consistency is verified by the fact that the appearance of a goal at level k does not
necessarily mean that the goal is actually achievable by level k  i e   there is a solution for the csp
that assigns a non    value to that goal at that level   similarly  lack of full   consistency is verified by the fact that appearance of a pair of goals at level k does not imply that there is a plan for
achieving both goals by that level 
there is another  somewhat less obvious  way in which the consistency enforcement used in
graphplan is partial  and very conservative it concentrates only on whether a single goal variable
or a pair of goal variables can simultaneously have non    values  be active  in a solution  it may
be that a goal can have a non    value  but not all non    values are feasible  similarly  it may be
that a pair of goals are achievable  but not necessarily achievable with every possible pair of actions
in their respective domains 
this interpretation of mutex propagation procedure in graphplan brings to fore several possible
extensions worth considering for graphplan 
   explore the utility of directional consistency enforcement procedures that are not based solely
on reachability analysis  kambhampati et  al         argue for extending this analysis using
relevance information  and do et  al         provide an empirical analysis of the effectiveness
of consistency enforcement through relevance information 
   explore the utility of enforcing higher level consistency  as pointed out in  kambhampati
et al         kambhampati         the memoization strategies can be seen as failure driven
procedures that incrementally enforce partial higher level consistency 
   normally  in the csp literature  a no good is seen as a compound assignment that can not be part of any feasible
i      pni     correspond to a conjunction of nogoods of
solution  with this view  mutex constraints of the form pm
i
i
i and pni  
the the form pm au   pn av where au and av are values in the domains of pm

 

 

 

 

 

fik ambhampati

   consider relaxing the focus on non    values alone  and allow derivation of no goods of the
form

pmi   au   pni   av

this is not guaranteed to be a winning idea as the number of derived no goods can increase
quite dramatically  in particular  assuming that there are l levels in the planning graph  and an
average of m goals per level  and an average of d actions supporting each goal  the maximum
number of graphplan style pair wise mutexes will be o  l  m    while the   size no goods of
type discussed here will be o  l   m   d           we consider a similar issue in the context
of graphplan memoization strategy in section   

   some inefficiencies of graphplans backward search
to motivate the need for ebl and ddb  we shall first review the details of graphplans backward
search  and pinpoint some of its inefficiencies  we shall base our discussion on the example planning
graph from figure    which is reproduced for convenience from figure     assuming that g     g 
are the top level goals of the problem we are interested in solving  we start at level k   and select
actions to support the goals g     g    to keep matters simple  we shall assume that the search
assigns the conditions  variables  at each level from top to bottom  i e   g  first  then g  and so
on   further  when there is a choice in the actions  values  that can support a condition  we will
consider the top actions first  since there is only one choice for each of the conditions at this level 
and none of the actions are mutually exclusive with each other  we select the actions a    a    a 
and a  for supporting the conditions at level k   we now have to make sure that the preconditions
of a    a    a    a  are satisfied at level k      we thus subgoal on the conditions p     p  at level
k      and recursively start the action selection for them  we select the action a  for p    for p   
we have two supporting actions  and using our convention  we select a  first  for p    a  is the
only choice  when we get down to selecting a support for p    we again have a choice  suppose
we select a  first  we find that this choice is infeasible as a  is mutually exclusive with a  that is
already chosen  so  we backtrack and choose a    and find that it too is mutually exclusive with a
previously selected action  a    we now are stymied as there are no other choices for p    so  we
have to backtrack and undo choices for the previous conditions  graphplan uses a chronological
backtracking approach  whereby  it first tries to see if p  can be re assigned  and then p  and so on 
notice the first indication of inefficiency here  the failure to assign p  had nothing to do with the
assignment for p    and yet  chronological backtracking will try to re assign p  in the vain hope of
averting the failure  this can lead to a large amount of wasted effort had it been the case that p  did
indeed have other choices 
as it turns out  we find that p  has no other choices and backtrack over it  p  does have another
choice  a     we try to continue the search forward with this value for p    but hit an impasse at p  
since the only value of p    a  is mutex with a     at this point  we backtrack over p    and continue
backtracking over p  and p    as they too have no other remaining choices  when we backtrack over
p    we need to go back to level k and try to re assign the goals at that level  before this is done  the
graphplan search algorithm makes a memo signifying the fact that it failed to satisfy the goals
p     p  at this level  with the hope that if the search ever subgoals on these same set of goals in
future  we can scuttle it right away with the help of the remembered memo  here is the second
indication of inefficiency  we are remembering all the subgoals p     p  even though we can see
that the problem lies in trying to assign p    p    p  and p  simultaneously  and has nothing to do
 

fip lanning g raph

action list
level k  

proposition list
level k  

as a

csp

action list
level k

proposition list
level k

a 
p 
a 

x

a 

x
a 
a 

p 
a 

p 

g 
a 

p 
a   

g 

p 

a 

x

g 

g 

p 
a 

a   

figure    the running example used to illustrate ebl ddb in graphplan
with the other subgoals  if we remember fp    p    p    p  g as the memo as against fp     p  g  the
remembered memo would be more general  and would have a much better chance of being useful
in the future 
after the memo is stored  the backtracking continues into level k  once again in a chronological
fashion  trying to reassign g    g    g  and g  in that order  here we see the third indication of inefficiency caused by chronological backtracking  g  really has no role in the failure we encountered
in assigning p  and p   since it only spawns the condition p  at level k      yet  the backtracking
scheme of graphplan considers reassigning g    a somewhat more subtle point is that reassigning
g  is not going to avert the failure either  although g  requires p  one of the conditions taking
part in the failure  p  is also required by g  and unless g  gets reassigned  considering further
assignments to g  is not going to avert the failure 
for this example  we continue backtracking over g  and g  too  since they too have no alternative supports  and finally memoize fg    g    g    g  g at this level  at this point the backward search
fails  and graphplan extends the planning graph by another level before re initiating the backward
search on the extended graph 

   improving backward search with ebl and ddb
i will now describe how graphplans backward search can be augmented with full fledged ebl
and ddb capabilities to eliminate the inefficiencies pointed out in the previous section  informally 
ebl ddb strategies involve explanation of failures at leaf nodes  and regression and propagation
of leaf node failure explanations to compute interior node failure explanations  along the lines described in  kambhampati         the specific extensions i propose to the backward search can

 

fik ambhampati

essentially be seen as adapting conflict directed backjumping strategy  prosser         and generalizing it to work with dynamic constraint satisfaction problems 
the algorithm is shown in pseudo code form in figure    it contains two mutually recursive
procedures find plan and assign goals  the former is called once for each level of the
planning graph  it then calls assign goals to assign values to all the required conditions at that
level  assign goals picks a condition  selects a value for it  and recursively calls itself with
the remaining conditions  when it is invoked with empty set of conditions to be assigned  it calls
find plan to initiate the search at the next  previous  level 
in order to illustrate how ebl ddb capabilities are added  lets retrace the previous example 
and pick up at the point where we are about to assign p  at level k      having assigned p    p  and
p    when we try to assign the value a  to p   we violate the mutex constraint between a  and a  
an explanation of failure for a search node is a set of constraints from which false can be derived 
the complete explanation for this failure can thus be stated as 

p    a    p    a     p    a    p     a   
of this  the part p    a    p     a  can be stripped from the explanation since the mutual
exclusion relation will hold as long as we are solving this particular problem with these particular
actions  further  we can take a cue from the conflict directed backjumping algorithm  prosser 
       and represent the remaining explanation compactly in terms of conflict sets  specifically 
whenever the search reaches a condition c  and is about to find an assignment for it   its conflict
set is initialized as fcg  whenever one of the possible assignments to c is inconsistent  mutually
exclusive  with the current assignment of a previous variable c    we add c  to the conflict set of c  in
the current example  we start with fp  g as the conflict set of p    and expand it by adding p  after
we find that a  cannot be assigned to p  because of the choice of a  to support p    informally 
the conflict set representation can be seen as an incrementally maintained  partial  explanation of
failure  indicating that there is a conflict between the current value of p  and one of the possible
values of p   kambhampati        
we now consider the second possible value of p    viz   a    and find that it is mutually exclusive
with a  which is currently supporting p    following our practice  we add p  to the conflict set of
p    at this point  there are no further choices for p    and so we backtrack from p    passing the
conflict set of p    viz   fp    p    p  g as the reason for its failure  in essence  the conflict set is a
shorthand notation for the following complete failure explanation  kambhampati         
  

p    a       p    a        p    a    p     a       p    a    p     a      p    a    p    a 

it is worth noting at this point that when p  is revisited in the future with different assignments
to the preceding variables  its conflict set will be re initialized to fp  g before considering any assignments to it 
the first advantage of the conflict set is that it allows a transparent way of supporting dependency directed backtracking  kambhampati         in the current example  having failed to assign
p    we have to start backtracking  we do not need to do this in a chronological fashion however 
   we strip the first  disjunctive  clause since it is present in the graph structure  and the next two implicative clauses
since they are part of the mutual exclusion relations that will not change for this problem  the conflict set representation just keeps the condition  variable  names of the last two clauses  denoting  in essence  that it is the current
assignments of the variables p  and p  that are causing the failure to assign p   

  

fip lanning g raph

as a

csp

find plan g goals  pg   plan graph  k   level 
if k      return an empty subplan p with success 
if there is a memo m such that m  g 
fail  and return m as the conflict set
call assign goals g  pg  k     
if assign goals fails and returns a conflict set m  
store m as a memo
regress m over actions selected at level k     to get r
fail and return r as the conflict set
if assign goals succeeds  and returns a k  level subplan p  
return p with success
assign goals g goals  pg   plan graph  k   level  a  actions 
if g    
let u be the union of preconditions of the actions in a
call find plan u  pg  k     
if find plan fails and returns a conflict set r 
fail and return r
if find plan succeeds and returns a subplan p of length k    
succeed and return a k length subplan p  a
else    g      
select a goal g   g
let cs
fgg  and ag be the set of actions from level k in pg that support g
l  
if ag      fail and return cs as the conflict set
ag   a
else  pick an action a   ag   and set ag
if a is mutually exclusive with some action b   a
let l be the goal that b was selected to support
cs   flg
set cs
goto l 
else  a is not mutually exclusive with any action in a 
call assign goals g   fg g  pg  k  a   fag 
if the call fails and returns a conflict set c
if g   c
set cs   cs   c  conflict set absorption
goto l 
else   g    c  
fail and return c as the conflict set
 dependency directed backjumping

figure    a pseudo code description of graphplan backward search enhanced with ebl ddb capabilities  the backward search at level k in a planning graph pg is initiated with the call
find plan g  pg  k   where g is the set of top level goals of the problem 
  

fik ambhampati

instead  we jump back to the most recent variable  condition  taking part in the conflict set of p  
in this case p    by doing so  we are avoiding considering other alternatives at p    and thus avoiding
one of the inefficiencies of the standard backward search  it is easy to see that such backjumping is
sound since p  is not causing the failure at p  and thus re assigning it wont avert the failure 
continuing along  whenever the search backtracks to a condition c  the backtrack conflict is
absorbed into the current conflict set of c  in our example  we absorb fp    p    p  g into the conflict
set of p    which is currently fp  g  making fp    p    p  g the new conflict set of p     we now assign
a     the only remaining value  to p    next we try to assign p  and find that its only value a  is
mutex with a     thus  we set conflict set of p  to be fp    p  g and backtrack with this conflict
set  when the backtracking reaches p    this conflict set is absorbed into the current conflict set of
p   as described earlier   giving rise to fp    p    p    p  g as the current combined failure reason for
p    this step illustrates how the conflict set of a condition is incrementally expanded to collect the
reasons for failure of the various possible values of the condition 
at this point  p  has no further choices  so we backtrack over p  with its current conflict set 
fp    p    p    p  g  at p    we first absorb the conflict set fp    p    p    p  g into p  s current conflict
set  and then re initiate backtracking since p  has no further choices 
now  we have reached the end of the current level  k       any backtracking over p  must
involve undoing assignments of the conditions at the k th level  before we do that however  we do
two steps  memoization and regression 
    memoization
before we backtrack over the first assigned variable at a given level  we store the conflict set of that
variable as a memo at that level  we store the conflict set fp    p    p    p  g of p  as a memo at this
level  notice that the memo we store is shorter  and thus more general  than the one stored by the
normal graphplan  as we do not include p  and p    which did not have anything to do with the
failure 
    regression
before we backtrack out of level k     to level k   we need to convert the conflict set of  the first
assigned variable in  level k     so that it refers to the conditions in level k   this conversion
process involves regressing the conflict set over the actions selected at the k th level  kambhampati 
       in essence  the regression step computes the  smallest  set of conditions  variables  at the
kth level whose supporting actions spawned  activated  in dcsp terms  the conditions  variables 
in the conflict set at level k      in the current case  our conflict set is fp    p    p    p  g  we can
see that p    p  are required because of the condition g  at level k   and the condition p  is required
because of the condition g   
in the case of condition p    both g  and g  are responsible for it  as both their supporting
actions needed p    in such cases we have two heuristics for computing the regression      prefer
choices that help the conflict set to regress to a smaller set of conditions     if we still have a choice
between multiple conditions at level k   we pick the one that has been assigned earlier  the motivation for the first rule is to keep the failure explanations as compact  and thus as general  as possible 
   while in the current example  the memo includes all the conditions up to p   which is the farthest we have gone in
this level   even this is not always necessary  we can verify that p  would not have been in the memo set if a   were
not one of the supporters of p   

  

fip lanning g raph

as a

csp

and the motivation for the second rule is to support deeper dependency directed backtracking  it
is important to note that these heuristics are aimed at improving the performance of the ebl ddb
and do not affect the soundness and completeness of the approach 
in the current example  the first of these rules applies  since p  is already required by g    which
is also requiring p  and p    even if this was not the case  i e   g  only required p     we still would
have selected g  over g  as the regression of p    since g  was assigned earlier in the search 
the result of regressing fp    p    p    p  g over the actions at k th level is thus fg    g  g  we start
backtracking at level k with this as the conflict set  we jump back to g  right away  since it is the
most recent variable named in the conflict set  this avoids the inefficiency of re considering the
choices at g  and g    as done by the normal backward search  at g    the backtrack conflict set
is absorbed  and the backtracking continues since there are no other choices  same procedure is
repeated at g    at this point  we are once again at the end of a leveland we memoize fg    g  g
as the memo at level k   since there are no other levels to backtrack to  graphplan is called on to
extend the planning graph by one more level 
notice that the memos based on ebl analysis capture failures that may require a significant
amount of search to rediscover  in our example  we are able to discover that fg    g  g is a failing
goal set despite the fact that there are no mutex relations between the choices of the goals g  and
g   
    using the memos
before we end this section  there are a couple of observations regarding the use of the stored memos 
in the standard graphplan  memos at each level are stored in a level specific hash table  whenever
backward search reaches a level k with a set of conditions to be satisfied  it consults the hash table
to see if this exact set of conditions is stored as a memo  search is terminated only if an exact hit
occurs  since ebl analysis allows us to store compact memos  it is not likely that a complete goal
set at some level k is going to exactly match a stored memo  what is more likely is that a stored
memo is a subset of the goal set at level k  which is sufficient to declare that goal set a failure  
in other words  the memo checking routine in graphplan needs to be modified so that it checks to
see if some subset of the current goal set is stored as a memo  the naive way of doing it  which
involves enumerating all the subsets of the current goal set and checking if any of them are in the
hash table  turns out to be very costly  one needs more efficient data structures  such as the setenumeration trees  rymon         indeed  koehler and her co workers  koehler  nebel  hoffman 
  dimopoulos        have developed a data structure called ub trees for storing the memos  the
ub tree structures can be seen as a specialized version of the set enumeration trees  and they can
efficiently check if any subset of the current goal set has been stored as a memo 
the second observation regarding memos is that they can often serve as a failure explanation
in themselves  suppose we are at some level k   and find that the goal set at this level subsumes
some stored memo m   we can then use m as the failure explanation for this level  and regress it
back to the previous level  such a process can provide us with valuable opportunities for further
back jumping at levels above k   it also allows us to learn new compact memos at those levels  note
that none of this would have been possible with normal memos stored by graphplan  as the only
way a memo can declare a goal set at level k as failing is if the memo is exactly equal to the goal
set  in such a case regression will just get us all the goals at level k      and does not buy us any
backjumping or learning power  kambhampati        

  

fik ambhampati

   empirical evaluation of the effectiveness of ebl ddb
we have now seen the way ebl and ddb capabilities are added to the backward search by maintaining and updating conflict sets  we also noted that ebl and ddb capabilities avoid a variety
of inefficiencies in the standard graphplan backward search  that these augmentations are soundness and completeness preserving follows from the corresponding properties of conflict directed
backjumping  kambhampati         the remaining  million dollar  question is whether these capabilities make a difference in practice  i now present a set of empirical results to answer this
question 
i implemented the ebl ddb approach described in the previous section on top of a graphplan
implementation in lisp   the changes needed to the code to add ebl ddb capability were relatively minor  only two functions needed non trivial changes    i also added the ub tree subset
memo checking code described in  koehler et al          i then ran several comparative experiments
on the benchmark problems from  kautz   selman         as well as from four other domains 
the specific domains included blocks world  rocket world  logistics domain  gripper domain  ferry
domain  traveling salesperson domain  and towers of hanoi  some of these domains  including the
blocks world  the logistics domain and the gripper domain were used in the recent ai planning
systems competition  the specifications of the problems as well as domains are publicly available 
table   shows the statistics on the times taken and number of backtracks made by normal graphplan  and graphplan with ebl ddb capabilities   
    run time improvement
the first thing we note is that ebl ddb techniques can offer quite dramatic speedups  from    x
in blocks world all the way to    x in the logistics domain  the att log a problem is unsolvable by
normal graphplan after over    hours of cpu time    we also note that the number of backtracks
reduces significantly and consistently with ebl ddb  given the lengh of some of the runs  the time
lisp spends doing garbage collection becomes an important issue  i thus report the cumulative time
 including cpu time and garbage collection time  for graphplan with ebl ddb  while i separate
the cpu time from cumulative time for the plain graphplan  in cases where the total time spent
was large enough that garbage collection time is a significant fraction   specifically  there are two
entrys in the column corresponding to total time for the normal graphplan  the first entry is the
cpu time spent  while the second entry in parenthesis is the cumulative time  cpu time and garbage
collection time  spent  the speedup is computed with respect to the cumulative time of graphplan
with ebl ddb and cpu time of plain graphplan     the reported speedups should thus be seen as
conservative estimates 
   the original lisp implementation of graphplan was done by mark peot  the implementation was subsequently
improved by david smith 
   assign goals and find plan
    in the earlier versions of this paper  including the paper presented at ijcai  kambhampati        i have reported
experiments on a sun sparc ultra   running allegro common lisp      the linux machine run time statistics
seem to be approximately    x faster than those from the sparc machine 
    it is interesting to note that the percentage of time spent doing garbage collection is highly problem dependent  for
example  in the case of att log a  only    minutes out of    hours  or about    of the cumulative time  was spent
doing garbage collection  while in the case of tower        hours out of     hours  or about     of the cumulative
time  was spent on garbage collection 

  

fiproblem

speedup
   x
   x
  x
  x
     x
  x
  x
   x
  x
   x
  x
  x
   x
  x
   x

table    empirical performance of ebl ddb  unless otherwise noted  times are in cpu minutes on a pentium iii     mhz machine with
   meg ra running linux and allegro common lisp    compiled for speed  tt is total time  mt is the time used in checking
memos and btks is the number of backtracks done during search  the times for graphplan with ebl ddb include both the cpu
and garbage collection time  while the cpu time is separated from the total time in the case of normal graphplan  the numbers in
parentheses next to the problem names list the number of time steps and number of actions respectively in the solution  avln and
avfm denote the average memo length and average number of failures detected per stored memo respectively 

csp

avfm
    
    
   
    
   
   
   
   
 
 

as a

normal graphplan
tt  mt 
  btks avln
        
    k
    
         
    k      
          
    k
    
    
         k
    
     hr     hr 
  
   
   
    k
    
        
    
    hr    hr 
               k
    
    hr      hr 
    
      
        k
    
       
        k
  
  hr      hr 
                 k
  
   hr       hr 
 

p lanning g raph

  

huge fact        
bw large b        
rocket ext a       
rocket ext b       
att log a       
gripper          
gripper          
gripper          
tower          
tower          
ferry           
ferry          
ferry         
tsp           
tsp          

graphplan with ebl ddb
tt
mt
  btks avln avfm
         
    k
    
    
         
   k      
    
  
   
   k
   
  
  
   
   k
   
   
    
   
    k
          
        
   k
   
   
   
   
    k
 
    
               k      
   
        
   k
   
   
         
    k
   
   
        
   k
   
    
    
   
    k
   
    
     
         k
    
   
        
    k
   
  
               k
   
    

fik ambhampati

    reduction in memo length
the results also highlight the fact that the speedups offered by ebl ddb are problem domain
dependent  they are quite meager in blocks world problems  and are quite dramatic in many other
domains including the rocket world  logistics  ferry  gripper  tsp and hanoi domains  the statistics
on the memos  shown in table   shed light on the reasons for this variation  of particular interest
is the average length of the stored memos  given in the columns labeled avln   in general  we
expect that the ebl analysis reduces the length of stored memos  as conditions that are not part of
the failure explanation are not stored in the memo  however  the advantage of this depends on the
likelihood that only a small subset of the goals at a given level are actually taking part in the failure 
this likelihood in turn depends on the amount of inter dependencies between the goals at a given
level  from the table  we note that the average length reduces quite dramatically in the rocket world
and logistics     while the reduction is much less pronounced in the blocks world  this variation can
be traced back to a larger degree of inter dependency between goals at a given level in the blocks
world problems 
the reduction in average memo length is correlated perfectly with the speedups offered by ebl
on the corresponding problems  let me put this in perspective  the fact that the average length
of memos for rocket ext a problem is     with ebl and    without
ebl  shows in essence that
  
normal graphplan is re discovering an   sized failure embedded in   
  possible ways in the worst
case in a    sized goal set  storing a new memo each time  incurring both increased backtracking
and matching costs   it is thus no wonder that normal graphplan performs badly compared to
graphplan with ebl ddb 
    utility of stored memos
the statistics in table   also show the increased utility of the memos stored by graphplan with
ebl ddb  since ebl ddb store more general  smaller  memos than normal graphplan  they
should  in theory  generate fewer memos and use them more often  the columns labeled avfm
give the ratio of the number of failures discovered through the use of memos to the number of memos
generated in the first place  this can be seen as a measure of the average utility of the stored
memos  we note that the utility is consistently higher with ebl ddb  as an example  in rocketext b  we see that on the average an ebl ddb generated memo was used to discover failures    
times  while the number was only     for the memos generated by the normal graphplan   
    relative utility of ebl vs  ddb
from the statistics in table    we see that even though ebl can make significant improvements in
run time  a significant fraction of the run time with ebl  as well as normal graphplan  is spent in
memo checking  this raises the possibility that the overall savings are mostly from the ddb part
and that the ebl part  i e  the part involving storing and checking memos  is in fact a net drain
 kambhampati  katukam    qu         to see if this is true  i ran some problems with ebl  i e  
memo checking  disabled  the ddb capability as well as the standard graphplan memoization
    for the case of att log a  i took the memo statistics by interrupting the search after about   hours
    the statistics for att log aseem to suggest that memo usage was not as bad for normal graphplan  however  it should
be noted that att log a was not solved by normal graphplan to begin with  the improved usage factor may be due
mostly to the fact that the search went for a considerably longer time  giving graphplan more opportunity to use its
memos 

  

fip lanning g raph

problem
att log a
tower  
rocket ext a
gripper  
tsp   
huge fct

ebl ddb
btks
time
    k     
    k     
   k
   
    k     
    k
   
    k     

as a

csp

ddb
btks
time
      k    
     k
   
    k
     
    k
    
    k
   
    k
    

speedup
   x
  x
  x
    x
    x
    x

table    utility of storing and using ebl memos over just doing ddb
strategies were left in    the results are shown in table    and demonstrate that the ability to store
smaller memos  as afforded by ebl  is quite helpfulgiving rise to    x speedup over ddb alone
in the att log a problem  and   x speedup in tower   problem  of course  the results also show that
ddb is an important capability in itself  indeed  att log aand tower   could not even be solved by
the standard graphplan  while with ddb  these problems become solvable  in summary  the results
show that both ebl and ddb can have a net positive utility 
    utility of memoization
another minor  but not well recognized  point brought out by the statistics in table   is that the
memo checking can sometimes be a significant fraction of the run time of standard graphplan  for
example  in the case of rocket ext a  standard graphplan takes      minutes of which      minutes 
or over half the time  is spent in memo checking  in hash tables   this raises the possibility that if
we just disable the memoization  perhaps we can do just as well as the version with ebl ddb  to
see if this is the case  i ran some of the problems with memoization disabled  the results show that
in general disabling memo checking leads to worsened performance  while i came across some
cases where the disablement reduces the overall run time  the run time is still much higher than
what you get with ebl ddb  as an example  in the case of rocket ext a  if we disable the memo
checking completely  graphplan takes      minutes  which while lower than the      minutes taken
by standard graphplan  is still much higher than the    minutes taken by the version of graphplan
with ebl ddb capabilities added  if we add ddb capability  while still disabling the memochecking  the run time becomes     minutes  which is still   times higher than that afforded with
ebl capability 
    the c vs  lisp question
given that most existing implementations of graphplan are done in c with many optimizations 
one nagging doubt is whether the dramatic speedups due to ebl ddb are somehow dependent
on the moderately optimized lisp implementation i have used in my experiments  thankfully  the
ebl ddb techniques described in this paper have also been  re implemented by maria fox and
derek long on their stan system  stan is a highly optimized implementation of graphplan
that fared well in the recent aips planning competition  they have found that ebl ddb resulted
in similar dramatic speedups on their system too  fox        fox   long         for example 
    i also considered removing the memoization completely  but the results were even poorer 

  

fik ambhampati

they were unable to solve att log a with plain graphplan  but could solve it easily with ebl ddb
added 
finally  it is worth pointing out that even with ebl ddb capabilities  i was unable to solve
some larger problems in the at t benchmarks  such as bw large c and att log b  this is however
not an indictment against ebl ddb since to my knowledge the only planners that solved these
problems have all used either local search strategies such as gsat  randomized re start strategies 
or have used additional domain specific knowledge and pre processing  at the very least  i am not
aware of any existing implementations of graphplan that solve these problems 

   on the utility of graphplan memos
one important issue in using ebl is managing the costs of storage and matching  indeed  as discussed in  kambhampati         naive implementations of ebl ddb are known to lose the gains
made in pruning power in the matching and storage costs  consequently  several techniques have
been invented to reduce these costs through selective learning as well as selective forgetting  it is
interesting to see why these costs have not been as prominent an issue for ebl ddb on graphplan 
i think this is mostly because of two characteristics of graphplan memoization strategy 
   graphplans memoization strategy provides a very compact representation for no goods  as
well as a very selective strategy for remembering no goods  seen as dcsp  it only remembers
subsets of activated variables that do not have a satisfying assignment  seen as a csp  c f 
figure     graphplan only remembers no goods of the form

p i      p i        pmi    
 where the superscripts correspond to the level of the planning graph to which the proposition
belongs   while normal ebl implementations learn no goods of the form

p i   a    p j   a     pmk   am
suppose a planning graph contains n propositions divided into l levels  and each proposition
p at level j has at most d actions supporting it  a csp compilation of the planning graph will
have n variables  each with d     values  the extra one for     a normal ebl implementation
for such a csp can learn  in the worst case   d     n no goods    in contrast  graphplan
n
remembers only l    l memos   a very dramatic reduction  this reduction is a result of two
factors 
 a  each individual memo stored by graphplan corresponds to an exponentially large set of
normal no goods  the memo

p i      p i        pmi    
is a shorthand notation for the conjunction of dm no goods corresponding to all possible
i 
non    assignments to p i    pm

    each variable v may either not be present in a no good  or be present with one of d
d possibilities for each of n variables 
    at each level  each of nl propositions either occurs in a memo or does not occur

  

  

    possible assignmentsgiving

fip lanning g raph

as a

csp

 b  memos only subsume no goods made up of proposition variables from the same planning graph level 
   the matching cost is reduced by both the fact that considerably fewer no goods are ever
learned  and the fact that graphplan stores no goods  memos  separately for each level  and
only consults the memos stored at level j   while doing backwards search at a level j  
the above discussion throws some light on why the so called ebl utility problem is not as
critical for graphplan as it is for ebl done on normal csps 
    scenarios where memoization is too conservative to avoid rediscovery of the same
failures
the discussion above also raises the possibility that graphplan  even with ebl ddb  memoization
is too conservative and may be losing some useful learning opportunities only because they are not
in the required syntactic form  specifically  before graphplan can learn a memo of the form

p i      p i        pmi     
it must be the case that each of the dm possible assignments to the m propositional variables must
be a no good  even if one of them is not a no good  graphplan avoids learning the memo  thus
potentially repeating the failing searches at a later time  although the loss is made up to some extent
by learning several memos at a lower level  
i    p i at some
consider for example the following scenario  we have a set of variables p i    pm
n
level i that are being assigned by backward search  suppose the search has found a legal partial asi   and the domain of p i contains the k values fv     vk g  in
signment for the variables p i    pm
m
  
i
trying to assign the variables pm    pni   suppose we repeatedly fail and backtrack up to the variable
pmi   re assigning it and eventually settling at the value v   at this point once again backtracking
i to higher level variables  p i    p i   and re assigning
occurs  but this time we backtrack over pm
m
 
them  at this point  it would have been useful to remember some no goods to the effect that none
i are going to work so all that backtracking does not have to be repeated 
of the first   values of pm
such no goods will take the form 

pmi   vj   pmi         pmi           pni    
i that were tried and found to lead to failure while
where j ranges over         for all the values of pm

assigning the later variables  unfortunately  such no goods are not in the syntactic form of memos
and so the memoization procedure cannot remember them  the search is thus forced to rediscover
the same failures 
    sticky values as a partial antidote
one way of staying with the standard memoization  but avoiding rediscovery of the failing search
paths  such as those in the case of the example above  is to use the sticky values heuristic  frost
  dechter        kambhampati         this involves remembering the current value of a variable
while skipping over it during ddb  and trying that value first when the search comes back to that
variable  the heuristic is motivated by the fact that when we skip over a variable during ddb  it
means that the variable and its current assignment have not contributed to the failure that caused
  

fik ambhampati

the backtrackingso it makes sense to restore this value upon re visit  in the example above  this
i when we backtracked over it  and tries
heuristic will remember that v  was the current value of pm
that as the first value when it is re visited  a variation on this technique is to re arrange or fold the
domain of the variable such that all the values that precede the current value are sent to the back
of the domain  so that these values will be tried only if other previously untried values are found to
fail  this makes the assumption that the values that led to failure once are likely to do so again  in
i so it becomes fv    v     vk   v    v     v  g 
the example above  this heuristic folds the domain of pm
notice that both these heuristics make sense only if we employ ddb  as otherwise we will never
skip over any variable during backtracking 
i implemented both sticky value heuristics on top of ebl ddb for graphplan  the statistics
in table   show the results of experiments with this extension  as can be seen  the sticky values
approach is able to give up to    x additional speedup over ebl ddb depending on the problem 
further  while the folding heuristic dominates the simple version in terms of number of backtracks 
the difference is quite small in terms of run time 

   forward checking   dynamic variable ordering
ddb and ebl are considered look back techniques in that they analyze the failures by looking
back at the past variables that may have played a part in those failures  there is a different class
of techniques known as look forward techniques for improving search  prominent among these
latter are forward checking and dynamic variable ordering  supporting forward checking involves
filtering out the conflicting actions from the domains of the remaining goals  as soon as a particular
goal is assigned  in the example in figure    forward checking will filter a  from the domain of p 
as soon as p  is assigned a    dynamic variable ordering  dvo  involves selecting for assignment
the goal that has the least number of remaining establishers    when dvo is combined with forward checking  the variables are ordered according to their live domain sizes  where live domain
is comprised of values from the domain that are not yet pruned by forward checking   our experiments   show that these techniques can bring about reasonable  albeit non dramatic  improvements
in graphplans performance  table   shows the statistics for some benchmark problems  with dynamic variable ordering alone  and with forward checking and dynamic variable ordering  we note
that while the backtracks reduce by up to    x in the case of dynamic variable ordering  and  x in the
case of dynamic variable ordering and forward checking  the speedups in time are somewhat smaller 
ranging only from    x to    x  times can perhaps be improved further with a more efficient implementation of forward checking    the results also seem to suggest that no amount of optimization
is going to make dynamic variable ordering and forward checking competitive with ebl ddb on
other problems  for one thing  there are several problems  including att log a  tsp     ferry   etc 
which just could not be solved even with forward checking and dynamic variable ordering  second 
even on the problems that could be solved  the reduction in backtracks provided by ebl ddb is far
greater than that provided by fc dvo strategies  for example  on tsp     the fc dvo strategies
    i have also experimented with a variation of this heuristic  known as the brelaz heuristic  gomes et al          where
the ties among variables with the same sized live domains are broken by picking variables that take part in most
number of constraints  this variation did not however lead to any appreciable improvement in performance 
    the study of forward checking and dynamic variable ordering was initiated with dan weld 
    my current implementation physically removes the pruned values of a variable during forward checking phase  and
restores values on backtracks  there are better implementations  including use of in out flags on the values as well as
use of indexed arrays  c f   bacchus   van run        

  

fi  

ebl ddb sticky
btks
speedup
   k
   x     x 
   k
   x    x 
     k     x     x 
     k    x     x 
     k     x     x 
    k
 x     x 

ebl ddb sticky fold
time
btks
speedup
   
   k
   x     x 
    
   k
   x     x 
          k     x     x 
           k    x     x 
           k     x     x 
   
   k
   x    x 

table    utility of using sticky values along with ebl ddb 

csp

time
   
   
    
     
    
   

as a

rocket ext a      
rocket ext b      
gripper          
ferry  
tsp          
att log a       

plain ebl ddb
time
btks
  
   k
  
   k
           k
           k
           k
    
    k

p lanning g raph

problem

fik ambhampati

problem
huge fact        
bw large b        
rocket ext a       
rocket ext b       
att log a       
gripper         
tsp          
tower         

gp
        k 
         k 
          k 
          k 
   hr
        k 
        k 
   hr

gp dvo
          k 
         k 
         k 
         k 
   hr
        k 
        k 
   hr

speedup
   x    x 
   x  x 
   x    x 
   x    x 
   x    x 
    x    x 
 

gp dvo fc
          k 
        k 
         k 
      k 
   hr 
        k 
        k 
   hr 

speedup
    x  x 
     x 
   x    x 
   x  x 
   x    x 
    x    x 

table    impact of forward checking and dynamic variable ordering routines on graphplan  times
are in cpu minutes as measured on a     mhz pentium iii running linux and franz
allegro common lisp    the numbers in parentheses next to times are the number of
backtracks  the speedup columns report two factorsthe first is the speedup in time  and
the second is the speedup in terms of number of backtracks  while fc and dvo tend to
reduce the number of backtracks  the reduction does not always seem to show up in the
time savings 

reduce number of backtracks from      k to      k  a    x improvement  however  this pales
in comparison to     k backtracks  or   x improvement  given by by ebl ddb  see the entry in
table     notice that these results only say that variable ordering strategies do not make a dramatic
difference for graphplans backward search  or a dcsp compilation of the planning graph   they do
not make any claims about the utility of fc and dvo for a csp compilation of the planning graph 
    complementing ebl ddb with forward checking and dynamic variable ordering
although forward checking and dynamic variable ordering approaches were not found to be particularly effective in isolation for graphplans backward search  i thought that it would be interesting
to revisit them in the context of a graphplan enhanced with ebl ddb strategies  part of the original reasoning underlying the expectation that goal  variable  ordering will not have a significant
effect on graphplan performance is based on the fact that all the failing goal sets are stored in toto
as memos  blum   furst        pp        this reason no longer holds when we use ebl ddb 
further more  there exists some difference of opinion as to whether or not forward checking and
ddb can fruitfully co exist  the results of  prosser        suggest that domain filteringsuch as
the one afforded by forward checking  degrades intelligent backtracking  the more recent work
 frost   dechter        bayardo   schrag        however seems to suggest however that best csp
algorithms should have both capabilities 
while adding plain dvo capability on top of ebl ddb presents no difficulties  adding forward
checking does require some changes to the algorithm in figure    the difficulty arises because a
failure may have occurred as a combined effect of the forward checking and backtracking  for
example  suppose we have four variables v     v  that are being considered for assignment in that
order  suppose v  has the domain f       g  and v  cannot be   if v  is a  and cannot be   if v  is
b  suppose further that v s domain only contains d  and there is a constraint saying that v  cant
  

fip lanning g raph

problem
huge fct
bw large b
rocket ext a
rocket ext b
att log a
tower  
tsp   

ebl
time btks 
         k 
        k 
      k 
      k 
         k 
         k 
        k 

as a

csp

ebl dvo
time btks 
speedup
        k 
 x     x 
        k 
    x    x 
      k 
 x    x 
       k 
    x     x 
         k     x     x 
         k 
   x    x 
         k     x     x 

ebl fc dvo
time btks 
speedup
        k 
   x  x 
        k     x     x 
       k 
    x    x 
       k 
   x    x 
         k    x     x 
        k 
   x    x 
        k 
   x    x 

table    effect of complementing ebl ddb with dynamic variable ordering and forward checking
strategies  the speedup columns report two factorsthe first is the speedup in time  and
the second is the speedup in terms of number of backtracks  while fc and dvo tend to
reduce the number of backtracks  the reduction does not always seem to show up in the
time savings 

be d if v  is a and v  is    suppose we are using forward checking  and have assigned v    v  the
values a and b  forward checking prunes   and   from v  s domain  leaving only the value    at
this point  we try to assign v  and fail  if we use the algorithm in figure    the conflict set for v 
would be fv    v    v  g  as the constraint that is violated is v    a   v        v    d  however this
is not sufficient since the failure at v  may not have occurred if forward checking had not stripped
the value   from the domain of v    this problem can be handled by pushing v  and v    the variables
whose assignment stripped some values from v    into v  s conflict set    specifically  the conflict
set of every variable v is initialized to fv g to begin with  and whenever v loses a value during
forward checking with respect to the assignment of v     v   is added to the conflict set of v   whenever
a future variable  such as v    conflicts with v    we add the conflict set of v   rather than just v    to
the conflict set of v    specifically the line
set cs   cs   f l g
in the procedure in figure   is replaced with the line
set cs   cs   conflict set l 
i have incorporated the above changes into my implementation  so it can support support forward checking  dynamic variable ordering as well as ebl on graphplan  table   shows the performance of this version on the experimental test suite  as can be seen from the numbers  the number
of backtracks are reduced by up to    x in the case of ebl dvo  and up to  x in the case of
ebl fc dvo  the cpu time improvements are somewhat lower  while we got up to    x speedup
    notice that it is possible that the values that were stripped off from v  s domain may not have had any impact on the
failure to assign v    for example  perhaps there is another constraint that says that v  cant be d if v  is b  and in
that case  strictly speaking  the assignment of v  cannot really be blamed for the failure at v    while this leads to
non minimal explanations  there is no reason to expect that strict minimization of explanations is a pre requisite for
the effectiveness of ebl ddb  see  kambhampati       

  

fik ambhampati

with ebl dvo  and up to    x speedup with ebl fc dvo  in several cases  the cpu times increase with fc and dvo  once again  i attribute this to the overheads of forward checking  and to
a lesser extent  of dynamic variable ordering   most importantly  by comparing the results in the
tables   and    we can see that ebl ddb capabilities are able to bring about significant speedups
even over a graphplan implementation using fc and dvo 

   ebl ddb   randomized search
recent years have seen increased use of randomized search strategies in planning  these include
both purely local search strategies  gerevini        selman  levesque    mitchell        as well
as hybrid strategies that introduce a random restart scheme on top of a systematic search strategy
 gomes et al          the blackbox planning system  kautz   selman        supports a variety
of random restart strategies on top of a sat compilation of the planning graph  and empirical
studies show that these strategies can  probabilistically speaking  scale up much better than purely
systematic search strategies 
i wanted to investigate if  and by how much  ebl   ddb techniques will help graphplan
even in the presence of these newer search strategies  while ebl and ddb techniques have little
applicability to purely local search strategies  they could in theory help random restart systematic
search strategies  random restart strategies are motivated by an attempt to exploit the heavytail distribution  gomes et al         of the solution nodes in the search trees of many problems 
intuitively  in problems where there are a non trivial percentage of very easy to find solutions as
well as very hard to find solutions  it makes sense to restart the search when we find that we are
spending too much effort for a solution  by restarting this way  we hope to  probabilistically  hit on
the easier to find solutions 
i implemented a random restart strategy on top of graphplan by making the following simple
modifications to the backward search 
   we keep track of the number of times the backward search backtracks from one level of the
plan graph to a previous level  a level closer to the goal state   and whenever this number
exceeds a given limit  called backtrack limit   the search is restarted  by going back to the last
level of the plan graph   assuming that the number of restarts has not also exceeded the given
limit  the search process between any two restarts is referred to as an epoch 
   the supporting actions  values  for a proposition variable are considered in a randomized
order  it is this randomization that ensures that when the search is restarted  we will look at
the values of each variable in a different order   
notice that random restart strategy still allows the application of ebl and ddb strategies  since
during any given epoch  the behavior of the search is identical to that of the standard backward
search algorithm  indeed  as the backtrack limit and the number of restarts are made larger and
larger  the whole search becomes identical to standard backward search 
    reordering values of a variable doesnt make a whole lot of sense in blackbox which is based on sat encodings
and thus has only boolean variables  thus  the randomization in blackbox is done on the order in which the goals
are considered for assignment  this typically tends to clash with the built in goal ordering strategies  such as dvo
and sat z  li   anbulagan          and they get around this conflict by breaking ties among variables randomly 
to avoid such clashes  i decided to randomize graphplan by reordering values of a variable  i also picked inter level
backtracks as a more natural parameter characterizing the difficulty of a problem for graphplans backward search 

  

fiproblem

 sol
  
   
   
   
   
  
  
  
  
  
   
   
    

normal graphplan
length
time av  mfsl
       
   
  k    k 
                
   k   k 
         
 
 k   k 
        
 
  k    k 
           
  
  k    k 
  k  k 
   k   k 
       
 
 k    k 
         
   
  k  k 
       
 
   k    k 
           
 
  k  k 
        
   
   k   k 
          
  
  k    k 

table    effect of ebl ddb on random restart graphplan  time is measured in cpu minutes on allegro common lisp     running on a
linux    mhz pentium machine  the numbers next to the problem names are the number of steps and actions in the shortest
plans reported for those problems in the literature  the r b l parameters in the second column refer to the limits on the number
of restarts  number of backtracks and the number levels to which the plan graph is expanded  all the statistics are averaged over
multiple runs  typically     or      the mfsl column gives the average number of memo based failures per searched level of
the plan graph  the numbers in parentheses are the total number of memo based failures averaged over all runs  plan lengths were
averaged only over the successful runs 

csp

graphplan with ebl ddb
length
time av  mfsl
      
   
   k   k 
          
   
    k   k 
          
   
    k   k 
        
       k    k 
        
       k    k 
         
    
 k   k 
        
       k    k 
         
       k    k 
               
 k    k 
         
  
  k    k 
          
   
  k    k 
       
       k    k 
       
       k    k 

as a

 sol
   
    
    
    
    
   
   
    
   
    
    
    
    

p lanning g raph

  

att log a       
att log a       
att log a       
att log a       
att log a       
att log b       
att log b       
att log b       
att log c       
att log c       
rocket ext a      
rocket ext a      
rocket ext a      

parameters
r b l
       
         
         
         
         
       
         
         
       
         
         
         
         

fik ambhampati

to check if my intuitions about the effectiveness of ebl ddb in randomized search were indeed correct  i conducted an empirical investigation comparing the performance of random search
on standard graphplan as well as graphplan with ebl ddb capabilities  since the search is randomized  each problem is solved multiple number of times      times in most cases   and the runtime  plan length and other statistics were averaged over all the runs  the experiments are conducted
with a given backtrack limit  a given restart limit  as well as a limit on the number of levels to which
the planning graph is extended  this last one is needed as in randomized search  a solution may be
missed at the first level it appears  leading to a prolonged extension of the planning graph until a
 inoptimal  solution is found at a later level  when the limit on the number of levels is expanded 
the probability of finding solution increases  but at the same time  the cpu time spent searching the
graph also increases 
having implemented this random restart search  the first thing i noticed is an improvement in
the solvability horizon  as expected  given the results in  gomes et al           table   shows these
results  one important point to note is that the results in the table above talk about average plan
lengths and cpu times  this is needed as due to randomization potentially each run can produce
a different outcome  plan   secondly  while graphplan with systematic search guarantees shortest
plans  measured in the number of steps   the randomized search will not have such a guarantee 
in particular  the randomized version might consider a particular planning graph to be barren of
solutions  based simply on the fact that no solution could be found within the confines of the given
backtrack limit and number of restarts 
graphplan  with or without ebl ddb  is more likely to solve larger problems with randomized
search strategies  for example  in the logistics domain  only the att log a problem was solvable
 within    hours real time  with ebl and systematic search  with the randomization added  my
implementation was able to solve both att log b and att log c quite frequently  as the limits on the
number of restarts  backtracks and levels is increased  the likelihood of finding a solution as well as
the average length of the solution found improves  for example  graphplan with ebl ddb is able
to solve att log b in every trial for    restarts      backtracks and    levels as the limits  although
the plans are quite inoptimal  
the next  and perhaps more interesting  question i wanted to investigate is whether ebl and
ddb will continue to be useful for graphplan when it uses randomized search  at first blush 
it seems as if they will not be as importantafter all even graphplan with standard search may
luck out and be able to find solutions quickly in the presence of randomization  further thought
however suggests that ebl and ddb may still be able to help graphplan  specifically  they can
help graphplan in using the given backtrack limit in a more judicious fashion  to elaborate  suppose
the random restart search is being conducted with     backtracks and    restarts  with ebl and
ddb  graphplan is able to pinpoint the cause of the failure more accurately than without ebl and
ddb  this means that when the search backtracks  the chance that it will have to backtrack again
for the same  or similar  reasons is reduced  this in turn gives the search more of a chance on
catching a success during one of the number of epochs allowed  all this is in addition to the more
direct benefit of being able to use the stored memos across epochs to cut down search 
as can be seen from the data in table    for a given set of limits on number of restarts  number
of backtracks  and number of levels expanded  graphplan with ebl ddb is able to get a higher
percentage of solvability as well as significantly shorter length solutions  both in terms of levels and
in terms of actions   to get comparable results on the standard graphplan  i had to significantly
increase the input parameters  restarts  backtracks and levels expanded   which in turn led to dra  

fip lanning g raph

as a

csp

matic increases in the average run time  for example  for the att log a problem  with   restarts and
   backtracks  and    levels limit  graphplan was able to solve the problem     of the time  with
an average plan length of    steps and    actions  in contrast  without ebl ddb  graphplan was
able to solve the problem in only    of the cases  with an average plan length of    steps and    
actions  if we double the restarts and backtracks  the ebl ddb version goes to      solvability
with an average plan length of       steps and       actions  the standard graphplan goes to    
solvability and a plan length of      steps and     actions  if we increase the number of levels to    
then the standard graphplan solves     of the problems with an average plan length of      steps
and     actions  it takes    restarts and     backtracks  as well as a    level limit before standard
graphplan is able to cross     solvability  by this time  the average run time is    minutes  and
the average plan length is    steps and     actions  the contrast between this and the     solvability in     minutes with    step    action plans provided by graphplan with ebl and   restarts
and    backtracks is significant  similar results were observed in other problems  both in logistics
 att log b  att log c  and other domains  rocket ext a  rocket ext b  
the results also show that graphplan with ebl ddb is able to generate and reuse memos effectively across different restart epochs  specifically  the numbers in the columns titled av  mfsl
give the average number of memo based failures per search level    we note that in all cases  the
average number of memo based failures are significantly higher for graphplan with ebl than for
normal graphplan  this shows that ebl ddb analysis is helping graphplan reduce wasted effort
significantly  and thus reap better benefits out of the given backtrack and restart limits 

   related work
in their original implementation of graphplan  blum and furst experimented with a variation of the
memoization strategy called subset memoization  in this strategy  they keep the memo generation
techniques the same  but change the way memos are used  declaring a failure when a stored memo
is found to be a subset of the current goal set  since complete subset checking is costly  they
experimented with a partial subset memoization where only subsets of length n and n     are
considered for an n sized goal set 
as we mentioned earlier  koehler and her co workers  koehler et al         have re visited the
subset memoization strategy  and developed a more effective solution to complete subset checking
that involves storing the memos in a data structure called ub tree  instead of in hash tables  the
results from their experiments with subset memoization are mixed  indicating that subset memoization does not seem to improve the cpu time performance significantly  the reason for this is quite
easy to understand  while they improved the memo checking time with the ub tree data structure 
they are still generating and storing the same old long memos  in contrast  the ebl ddb extension
described here supports dependency directed backtracking  and by reducing the average length of
stored memos  increases their utility significantly  thus offering dramatic speedups 
to verify that the main source of power in the ebl ddb graphplan is in the ebl ddb part
and not in the ub tree based memo checking  i re ran my experiments with ebl ddb turned off 
    notice that the number of search levels may be different from  and smaller than  the number of planning graph levels 
because graphplan initiates a search only when none of the goals are pair wise mutex with each other  in att log a 
att log b and att log c  this happens starting at level    for rocket ext a it happens starting at level    the numbers
in parentheses are the total number of memo based failures  we divide this number by the average number of levels
in which search was conducted to get the av  mfsl statistic 

  

fik ambhampati

problem
huge fact
bw large b
rocket ext a
rocket ext b
att log a

tt
    
    
    
    
    hrs

mt
 
    
    
    
 

 btks
    k
    k
    k
    k
 

ebl x 
    x
    x
  x
   x
    x

 gen
     
     
     
     
 

 fail
     
     
      
      
 

avfm
    
    
   
   
 

avln
     
     
     
     
 

table    performance of subset memoization with ub tree data structure  without ebl ddb   the
tt is the total cpu time and mt is the time taken for checking memos   btks is the
number of backtracks  eblx is the amount of speedup offered by ebl ddb over subset
memoization  gen lists the number of memos generated  and stored    fail lists the
number of memo based failures  avfm is the average number of failures identified per
generated memo and avln is the average length of stored memos 

but with subset memo checking with ub tree data structure still enabled  the results are shown in
in table    the columns labeled avfm show that as expected subset memoization does improve
the utility of stored memos over normal graphplan  since it uses a memo in more scenarios than
normal graphplan can   however  we also note that subset memoization by itself does not have any
dramatic impact on the performance of graphplan  and that ebl ddb capability can significantly
enhance the savings offered by subset memoization 
in  kambhampati         i describe the general principles underlying the ebl ddb techniques
and sketch how they can be extended to dynamic constraint satisfaction problems  the development
in this paper can be seen as an application of the ideas there  readers needing more background
on ebl ddb are thus encouraged to review that paper  other related work includes previous attempts at applying ebl ddb to planning algorithms  such as the work on ucpop ebl system
 kambhampati et al          one interesting contrast is the ease with which ebl ddb can be added
to graphplan as compared to ucpop system  part of the difference comes from the fact that the
search in graphplan is ultimately on a propositional dynamic csp  while in ucpops search is a
variablized problem solving search 
as i mentioned in section    graphplan planning graph can also be compiled into a normal csp
representation  rather than the dynamic csp representation  i used the dynamic csp representation as it corresponds quite directly to the backward search used by graphplan  we saw that the
model provides a clearer picture of the mutex propagation and memoization strategies  and helps us
unearth some of the sources of strength in the graphplan memoization strategyincluding the fact
that memos are a very conservative form of no good learning that obviate the need for the no good
management strategies to a large extent 
the dynamic csp model may also account for some of the peculiarities of the results of my
empirical studies  for example  it is widely believed in the csp literature that forward checking and
dynamic variable ordering are either as critical as  or perhaps even more critical than  the ebl ddb
strategies  bacchus   van run        frost   dechter         our results however show that for
graphplan  which uses the dynamic csp model of search  dvo and fc are largely ineffective
compared to ebl ddb on the standard graphplan  to some extent  this may be due to the fact that

  

fip lanning g raph

as a

csp

graphplan already has a primitive form of ebl built into its memoization strategy  in fact  blum
  furst        argue that with memoization and a minimal action set selection  an action set is
considered minimal if it is not possible to remove an action from the set and still support all the
goals for which the actions were selected   the ordering of goals will have little effect  especially in
the earlier levels that do not contain a solution  
another reason for the ineffectiveness of the dynamic variable ordering heuristic may have to
do with the differences between the csp and dcsp problems  in dcsp  the main aim is not just to
quickly find an assignment for the the current level variables  but rather to find an assignment for
the current level which is likely to activate fewer and easier to assign variables  whose assignment
in turn leads to fewer and easier to assign variables and so on  the general heuristic of picking the
variable with the smallest  live  domain does not necessarily make sense in dcsp  since a variable
with two actions supporting it may actually be much harder to handle than another with many
actions supporting it  if each of the actions supporting the first one eventually lead to activation of
many more and harder to assign new variables  it may thus be worth considering ordering strategies
that are more customized to the dynamic csp modelse g  orderings that are based on the number
 and difficulty  of variables that get activated by a given variable  or value  choice 
we have recently experimented with a value ordering heuristic that picks the value to be assigned to a variable using the distance estimates of the variables that will be activated by that choice
 kambhampati   nigenda         the planning graph provides a variety of ways of obtaining these
distance estimates  the simplest idea would be to say that the distance of a proposition p is the level
at which p enters the planning graph for the first time  this distance estimate can then be used
to rank variables and their values  variables can be ranked simply in terms of their distancesthe
variables that have the highest distance are chosen first  akin to fail first principle   value ordering
is a bit trickierfor a given variable  we need to pick an action whose precondition set has the lowest
distance  the distance of the precondition set can be computed from the distance of the individual
preconditions in several ways 





maximum of the distances of the individual propositions making up the preconditions 
sum of the distances of the individual propositions making up the preconditions 
the first level at which the set of propositions making up the preconditions are present and
are non mutex 

in  kambhampati   nigenda         we evaluate goal and value ordering strategies based on
these ideas  and show that they can lead to quite impressive  upto   orders of magnitude in our
tests  speedups in solution bearing planning graphs  we also relate the distances computed through
planning graph to the distance transforms computed by planners like hsp  bonet  loerincs   
geffner        and unpop  mcdermott         this idea of using the planning graph as a basis
for computing heuristic distance metrics is further investigated in the context of state space search
in  nguyen   kambhampati         an interesting finding in that paper is that even when one is
using state space instead of csp style solution extraction  ebl can still be useful as a lazy demanddriven approach for discovering n ary mutexes that can improve the informedness of the heuristic 
specifically  long   kambhampati describe a method where a limited run of graphplans backward search  armed with ebl ddb is used as a pre processing stage to explicate memos  n ary
mutexes  which are then used to significantly improve the effectiveness of the heuristic on the
state search 
  

fik ambhampati

the general importance of ebl   ddb for csp and sat problems is well recognized  indeed 
one of the best systematic solvers for propositional satisfiability problems is relsat  bayardo  
schrag         which uses ebl  ddb  and forward checking  a randomized version of relsat is
one of the solvers supported by the blackbox system  kautz   selman         which compiles
the planning graph into a sat encoding  and ships it to various solvers  blackbox thus offers
a way of indirectly comparing the dynamic csp and static csp models for solving the planning
graph  as discussed in section      the main differences are that blackbox needs to compile
the planning graph into an extensional sat representation  this makes it harder for blackbox
to exploit the results of searches in previous levels  as graphplan does with its stored memos  
and also leads to memory blowups  the latter is particularly problematic as the techniques for
condensing planning graphs  such as the bi level representation discussed in  fox   long       
smith   weld        will not be effective when we compile the planning graph to sat  on the
flip side  blackbox allows non directional search  and the opportunity to exploit existing sat
solvers  rather than develop customized solvers for the planning graph  at present  it is not clear
whether either of these approaches dominates the other  in my own informal experiments  i found
that certain problems  such as att log x  are easier to solve with non directional search offered by
blackbox  while others  such as gripper x  are easier to solve with the graphplan backward
search  the results of the recent aips planning competition are also inconclusive in this respect
 mcdermott        
while my main rationale for focusing on dynamic csp model of the planning graph is due to
its closeness to graphplans backward search  gelle        argues that keeping activity constraints
distinct from value constraints has several advantages in terms of modularity of the representation 
in graphplan  this advantage becomes apparent when not all activation constraints are known a
priori  but are posted dynamically during search   this is the case in several extensions of the
graphplan algorithm that handle conditional effects  kambhampati et al         anderson  smith 
  weld        koehler et al          and incomplete initial states  weld  anderson    smith        
although ebl and ddb strategies try to exploit the symmetry in the search space to improve the
search performance  they do not go far enough in many cases  for example  in the gripper domain 
the real difficulty is that search gets lost in the combinatorics of deciding which hand should be used
to pick which ball for transfer into the next rooma decision which is completely irrelevant for the
quality of the solution  or the search failures  for that matter   while ebl ddb allow graphplan
to cut the search down a bit  allowing transfer of up to    balls from one room to another  they
are over come beyond    balls  there are two possible ways of scaling further  the first is to
variablize memos  and realize that certain types of failures would have occurred irrespective of
the actual identity of the hand that is used  variablization  also called generalization is a part
of ebl methods  kambhampati        kambhampati et al          another way of scaling up
in such situations would be to recognize the symmetry inherent in the problem and abstract the
resources from the search  in  srivastava   kambhampati         we describe this type of resource
abstraction approach for graphplan 

    conclusion and future work
in this paper  i traced the connections between the graphplan planning graph and csp  and motivated the need for exploiting csp techniques to improve the performance of graphplan backward search  i then adapted and evaluated several csp search techniques in the contest of graph 

  

fip lanning g raph

as a

csp

plan  these included ebl  ddb  forward checking  dynamic variable ordering  sticky values  and
random restart search  my empirical studies show the ebl ddb is particularly useful in dramatically speeding up graphplans backward search  by up tp     x in some instances   the speedups
can be improved further  by up to  x  with the addition of forward checking  dynamic variable ordering and sticky values on top of ebl ddb  i also showed that ebl ddb techniques are equally
effective in helping graphplan  even if random restart search strategies are used 
a secondary contribution of this paper is a clear description of the connections between the
graphplan planning graph  and the  dynamic  constraint satisfaction problem  these connections
help us understand some unique properties of the graphplan memoization strategy  when viewed
from csp standpoint  see section    
there are several possible ways of extending this work  the first would be to support the
use of learned memos across problems  or when the specification of a problem changes  as is the
case during replanning   blum   furst        suggest this as a promising future direction  and
the ebl framework described here makes the extension feasible  as discussed in  kambhampati 
      schiex   verfaillie         supporting such inter problem usage involves contextualizing
the learned no goods  in particular  since the soundness of memos depends only on the initial state
of the problem  given that operators do not change from problem to problem   inter problem usage
of memos can be supported by tagging each learned memo with the specific initial state literals that
supported that memo  memos can then be used at the corresponding level of a new problem as
long as their initial state justification holds in the new problem  the initial state justification for
the memos can be computed incrementally by a procedure that first justifies the propagated mutex
relations in terms of the initial state  and then justifies individual memos in terms of the justifications
of the mutexes and other memos from which they are derived 
the success of ebl ddb approaches in graphplan is in part due to the high degree of redundancy in the planning graph structure  for example  the propositions  actions  at level l in a
planning graph are a superset of the propositions  actions  at level l      the mutexes  memos  at
level l are a subset of the mutexes  memos  at level l       while the ebl ddb techniques help
graphplan exploit some of this redundancy by avoiding previous failures  the exploitation of redundancy can be pushed further  indeed  the search that graphplan does on a planning graph of size l
is almost a re play of the search it did on the planning graph of size l      with a few additional
choices   in  zimmerman   kambhampati         we present a complementary technique called
explanation guided backward search that attempts to exploit this deja vu property of the graphplans backward search  our technique involves keeping track of an elaborate trace of the search at
a level l  along with the failure information   termed the pilot explanation for level l  and using the
pilot explanation to guide the search at level l      the way ebl ddb help in this process is that
they significantly reduce the size of the pilot explanations that need to be maintained  preliminary
results with this technique shows that it complements ebl ddb and provides significant further
savings in search 
acknowledgements
this research is supported in part by nsf young investigator award  nyi  iri          arpa rome
laboratory planning initiative grant f         c       army aasert grant daah             afosr grant f                and nsf grant iri          i thank maria fox and derek
long for taking the time to implement and experiment with ebl ddb on their stan system  i

  

fik ambhampati

would also like to thank them  as well as terry zimmerman  biplav srivastava  dan weld  avrim
blum and steve minton for comments on previous drafts of this paper  special thanks are due to
dan weld  who hosted me at university of washington in summer       and spent time discussing
the connections between csp and graphplan  finally  i thank mark peot and david smith for their
clean lisp implementation of the graphplan algorithm  which served as the basis for my extensions 

references
anderson  c   smith  d     weld  d          conditional effects in graphplan  in proc  ai planning
systems conference 
bacchus  f     van run  p          dynamic variable ordering in csps  in proc  principles and
practice of constraint programming  cp      published as lecture notes in artificial intelligence  no       springer verlag 
bayardo  r     schrag  r          using csp look back techniques to solve real world sat instances  in proc  aaai    
blum  a     furst  m          fast planning through planning graph analysis  artificial intelligence 
        
bonet  b   loerincs  g     geffner  h          a robust and fast action selection mechanism for
planning  in in proc  aaai    
do  b     kambhampati  s          solving planning graph by compiling it into csp  in proc   th
international conference on ai planning and scheduling 
do  b   srivastava  b     kambhampati  s          investigating the effect of relevance and reachability constraints on the sat encodings of planning  in proc   th international conference on
ai planning and scheduling 
fox  m          private correspondence  
fox  m     long  d          efficient implementation of plan graph  journal of artificial intelligence research     
frost  d     dechter  r          in search of the best constraint satisfactions earch  in proc  aaai   
gelle  e          on the generation of locally consistent solution spaces in mixed dynamic constraint problems  ph d  thesis  ingenieure informaticienne epfl de nationalite suisse  lausanne  switzerland 
gerevini  a          fast planning through greedy planning graphs  in proc  aaai    
gomes  c   selman  b     kautz  h          boosting combinatorial search through randomization 
in proc  aaai     pp         
kambhampati  s          challenges in bridging plan synthesis paradigms  in proc  ijcai    

  

fip lanning g raph

as a

csp

kambhampati  s          on the relations between intelligent backtracking and explanation based
learning in planning in constraint satisfaction  artifical intelligence           
kambhampati  s          improving graphplans search with ebl   ddb techniques  in proc  ijcai   
kambhampati  s   katukam  s     qu  y          failure driven dynamic search control for partial
order planners  an explanation based approach  artificial intelligence                  
kambhampati  s     nigenda  r          distance based goal ordering heuristics for graphplan  in
proc   th international conference on ai planning and scheduling 
kambhampati  s   parker  e     lambrecht  e          understanding and extending graphplan 
in proceedings of  th european conference on planning  url  rakaposhi eas asu edu ewspgraphplan ps 
kautz  h     selman  b          pushing the envelope  plannng  propositional logic and stochastic
search  in proc  aaai    
kautz  h     selman  b          blackbox  unifying sat based and graph based planning  in proc 
ijcai    
koehler  j   nebel  b   hoffman  j     dimopoulos  y          extending planning graphs to an adl
subset  tech  rep      albert ludwigs university 
li  c     anbulagan         heuristics based on unit propagation for satisfiability problems  in
proc  ijcai    
mcdermott 
d 
       
aips   
planning
ftp cs yale edu pub mcdermott aipscomp results html 

competition

results 

mcdermott  d          using regression graphs to control search in planning  aritificial intelligence                   
mittal  s     falkenhainer  b          dynamic constraint satisfaction problems  in proc  aaai    
nguyen  x     kambhampati  s          extracting effective and admissible state space heuristics
from the planning graph  tech  rep  asu cse tr        arizona state university 
prosser  p          domain filtering can degrade intelligent backtracking search  in proc  ijcai    
rymon  r          set enumeration trees  in proc  krr    
schiex  t     verfaillie  g          nogood recording for static and dynamic constraint satisfaction
problems  in proc   th intl  conference on tools with artificial intelligence 
selman  b   levesque  h     mitchell  d          gsat  a new method for solving hard satisfiability
problems  in in proc  aaai    
smith  d     weld  d          temporal planning with mutual exclusion reasoning  in proc 
ijcai    
  

fik ambhampati

srivastava  b     kambhampati  s          scaling up planning by teasing out resource scheduling 
in proc  european conference on planning 
tsang  e          foundations of constraint satisfaction  academic press  san diego  california 
weld  d   anderson  c     smith  d          extending graphplan to handle uncertainty   sensing
actions  in proc  aaai    
zimmerman  t     kambhampati  s          exploiting symmetry in the plan graph via
explanation guided search  in proc  aaai    

  

fi