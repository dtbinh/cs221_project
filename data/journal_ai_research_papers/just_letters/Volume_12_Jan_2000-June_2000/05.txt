journal of artificial intelligence research                 

submitted        published     

a model of inductive bias learning
jonathan baxter

j onathan  baxter   anu   edu   au

research school of information sciences and engineering
australian national university  canberra       australia

abstract
a major problem in machine learning is that of inductive bias  how to choose a learners hypothesis space so that it is large enough to contain a solution to the problem being learnt  yet small
enough to ensure reliable generalization from reasonably sized training sets  typically such bias is
supplied by hand through the skill and insights of experts  in this paper a model for automatically
learning bias is investigated  the central assumption of the model is that the learner is embedded
within an environment of related learning tasks  within such an environment the learner can sample
from multiple tasks  and hence it can search for a hypothesis space that contains good solutions to
many of the problems in the environment  under certain restrictions on the set of all hypothesis
spaces available to the learner  we show that a hypothesis space that performs well on a sufficiently
large number of training tasks will also perform well when learning novel tasks in the same environment  explicit bounds are also derived demonstrating that learning multiple tasks within an
environment of related tasks can potentially give much better generalization than learning a single
task 

   introduction
often the hardest problem in any machine learning task is the initial choice of hypothesis space 
it has to be large enough to contain a solution to the problem at hand  yet small enough to ensure
good generalization from a small number of examples  mitchell         once a suitable bias has
been found  the actual learning task is often straightforward  existing methods of bias generally
require the input of a human expert in the form of heuristics and domain knowledge  for example 
through the selection of an appropriate set of features   despite their successes  such methods are
clearly limited by the accuracy and reliability of the experts knowledge and also by the extent to
which that knowledge can be transferred to the learner  thus it is natural to search for methods for
automatically learning the bias 
in this paper we introduce and analyze a formal model of bias learning that builds upon
the pac model of machine learning and its variants  vapnik        valiant        blumer 
ehrenfeucht  haussler    warmuth        haussler         these models typically take the
and training data
following general form  the learner is supplied with a hypothesis space
drawn independently according to some underlying distribution
on
  based on the information contained in   the learners goal is to select a hypothesis
from minimizing some measure
of expected loss with respect to  for example  in the case of squared loss
   in such models the learners
bias is represented by the choice of   if does not contain a good solution to the problem  then 
regardless of how much data the learner receives  it cannot learn 
of course  the best way to bias the learner is to supply it with an containing just a single optimal hypothesis  but finding such a hypothesis is precisely the original learning problem  so in the

 ff
fi
fifi fi
  
     
   

 



  

 

 
 
 
 
                             
        

c      ai access foundation and morgan kaufmann publishers  all rights reserved 





fibaxter

pac model there is no distinction between bias learning and ordinary learning  or put differently 
the pac model does not model the process of inductive bias  it simply takes the hypothesis space
as given and proceeds from there  to overcome this problem  in this paper we assume that instead
of being faced with just a single learning task  the learner is embedded within an environment of
related learning tasks  the learner is supplied with a family of hypothesis spaces
  and its
  that is appropriate for the entire environment 
goal is to find a bias  i e  hypothesis space
a simple example is the problem of handwritten character recognition  a preprocessing stage that
identifies and removes any  small  rotations  dilations and translations of an image of a character
will be advantageous for recognizing all characters  if the set of all individual character recognition
problems is viewed as an environment of learning problems  that is  the set of all problems of the
form distinguish a from all other characters  distinguish b from all other characters  and
so on   this preprocessor represents a bias that is appropriate for all problems in the environment 
it is likely that there are many other currently unknown biases that are also appropriate for this
environment  we would like to be able to learn these automatically 

dc a

a b 

there are many other examples of learning problems that can be viewed as belonging to environments of related problems  for example  each individual face recognition problem belongs to an
 essentially infinite  set of related learning problems  all the other individual face recognition problems   the set of all individual spoken word recognition problems forms another large environment 
as does the set of all fingerprint recognition problems  printed chinese and japanese character recognition problems  stock price prediction problems and so on  even medical diagnostic and prognostic
problems  where a multitude of diseases are predicted from the same pathology tests  constitute an
environment of related learning problems 
in many cases these environments are not normally modeled as such  instead they are treated
as single  multiple category learning problems  for example  recognizing a group of faces would
normally be viewed as a single learning problem with multiple class labels  one for each face in
the group   not as multiple individual learning problems  however  if a reliable classifier for each
individual face in the group can be constructed then they can easily be combined to produce a
classifier for the whole group  furthermore  by viewing the faces as an environment of related
learning problems  the results presented here show that bias can be learnt that will be good for
learning novel faces  a claim that cannot be made for the traditional approach 
this point goes to the heart of our model  we are not not concerned with adjusting a learners
bias so it performs better on some fixed set of learning problems  such a process is in fact just
ordinary learning but with a richer hypothesis space in which some components labelled bias are
also able to be varied  instead  we suppose the learner is faced with a  potentially infinite  stream of
tasks  and that by adjusting its bias on some subset of the tasks it improves its learning performance
on future  as yet unseen tasks 
bias that is appropriate for all problems in an environment must be learnt by sampling from
many tasks  if only a single task is learnt then the bias extracted is likely to be specific to that
task  in the rest of this paper  a general theory of bias learning is developed based upon the idea of
learning multiple related tasks  loosely speaking  formal results are stated in section     there are
two main conclusions of the theory presented here 

e

learning multiple related tasks reduces the sampling burden required for good generalization 
at least on a number of examples required per task basis 
   

fia m odel of i nductive b ias l earning

e

bias that is learnt on sufficiently many training tasks is likely to be good for learning novel
tasks drawn from the same environment 

the second point shows that a form of meta generalization is possible in bias learning  ordinarily  we say a learner generalizes well if  after seeing sufficiently many training examples  it
produces a hypothesis that with high probability will perform well on future examples of the same
task  however  a bias learner generalizes well if  after seeing sufficiently many training tasks it produces a hypothesis space that with high probability contains good solutions to novel tasks  another
term that has been used for this process is learning to learn  thrun   pratt        
our main theorems are stated in an agnostic setting  that is 
does not necessarily contain a
hypothesis space with solutions to all the problems in the environment   but we also give improved
bounds in the realizable case  the sample complexity bounds appearing in these results are stated
in terms of combinatorial parameters related to the complexity of the set of all hypothesis spaces
available to the bias learner  for boolean learning problems  pattern classification  these parameters
are the bias learning analogue of the vapnik chervonenkis dimension  vapnik        blumer et al  
      
as an application of the general theory  the problem of learning an appropriate set of neuralnetwork features for an environment of related tasks is formulated as a bias learning problem  in
the case of continuous neural network features we are able to prove upper bounds on the number
of training tasks and number of examples of each training task required to ensure a set of features
that works well for the training tasks will  with high probability  work well on novel tasks drawn
where is
from the same environment  the upper bound on the number of tasks scales as
a measure of the complexity of the possible feature sets available to the learner  while the upper
where
is the number
bound on the number of examples of each task scales as
of examples required to learn a task if the true set of features  that is  the correct bias  is already
known  and is the number of tasks  thus  in this case we see that as the number of related tasks
learnt increases  the number of examples required of each task for good generalization decays to
the minimum possible  for boolean neural network feature maps we are able to show a matching
lower bound on the number of examples required per task of the same form 

a

a

f jilkmgonpq

p

f hg
f jir

g

    related work
there is a large body of previous algorithmic and experimental work in the machine learning and
statistics literature addressing the problems of inductive bias learning and improving generalization
through multiple task learning  some of these approaches can be seen as special cases of  or at least
closely aligned with  the model described here  while others are more orthogonal  without being
completely exhaustive  in this section we present an overview of the main contributions  see thrun
and pratt        chapter    for a more comprehensive treatment 

e

hierarchical bayes  the earliest approaches to bias learning come from hierarchical bayesian
methods in statistics  berger        good        gelman  carlin  stern    rubim        
in contrast to the bayesian methodology  the present paper takes an essentially empirical
process approach to modeling the problem of bias learning  however  a model using a mixture
of hierarchical bayesian and information theoretic ideas was presented in baxter      a  
with similar conclusions to those found here  an empirical study showing the utility of the
hierarchical bayes approach in a domain containing a large number of related tasks was given
in heskes        
   

fie

baxter

e

early machine learning work  in rendell  seshu  and tcheng        vbms or variable bias
management system was introduced as a mechanism for selecting amongst different learning
algorithms when tackling a new learning problem  stabb or shift to a better bias  utgoff        was another early scheme for adjusting bias  but unlike vbms  stabb was not
primarily focussed on searching for bias applicable to large problem domains  our use of an
environment of related tasks in this paper may also be interpreted as an environment of
analogous tasks in the sense that conclusions about one task can be arrived at by analogy
with  sufficiently many of  the other tasks  for an early discussion of analogy in this context  see russell        s      in particular the observation that for analogous problems the
sampling burden per task can be reduced 
metric based approaches  the metric used in nearest neighbour classification  and in vector
quantization to determine the nearest code book vector  represents a form of inductive bias 
using the model of the present paper  and under some extra assumptions on the tasks in
the environment  specifically  that their marginal input space distributions are identical and
they only differ in the conditional probabilities they assign to class labels   it can be shown
that there is an optimal metric or distance measure to use for vector quantization and onenearest neighbour classification  baxter      a      b  baxter   bartlett         this metric
can be learnt by sampling from a subset of tasks from the environment  and then used as a
distance measure when learning novel tasks drawn from the same environment  bounds on
the number of tasks and examples of each task required to ensure good performance on novel
tasks were given in baxter and bartlett         along with an experiment in which a metric
was successfully trained on examples of a subset of     japanese characters and then used as
a fixed distance measure when learning      as yet unseen characters 
a similar approach is described in thrun and mitchell         thrun         in which a
neural networks output was trained to match labels on a novel task  while simultaneously
being forced to match its gradient to derivative information generated from a distance metric
trained on previous  related tasks  performance on the novel tasks improved substantially
with the use of the derivative information 

e

e

note that there are many other adaptive metric techniques used in machine learning  but these
all focus exclusively on adjusting the metric for a fixed set of problems rather than learning a
metric suitable for learning novel  related tasks  bias learning  
feature learning or learning internal representations  as with adaptive metric techniques 
there are many approaches to feature learning that focus on adapting features for a fixed task
rather than learning features to be used in novel tasks  one of the few cases where features
have been learnt on a subset of tasks with the explicit aim of using them on novel tasks was
intrator and edelman        in which a low dimensional representation was learnt for a set
of multiple related image recognition tasks and then used to successfully learn novel tasks of
the same kind  the experiments reported in baxter      a  chapter    and baxter      b  
baxter and bartlett        are also of this nature 
bias learning in inductive logic programming  ilp   predicate invention refers to the process in ilp whereby new predicates thought to be useful for the classification task at hand
are added to the learners domain knowledge  by using the new predicates as background domain knowledge when learning novel tasks  predicate invention may be viewed as a form of
   

fia m odel of i nductive b ias l earning

inductive bias learning  preliminary results with this approach on a chess domain are reported
in khan  muggleton  and parson        

e

e

improving performance on a fixed reference task  multi task learning  caruana       
trains extra neural network outputs to match related tasks in order to improve generalization
performance on a fixed reference task  although this approach does not explicitly identify the
extra bias generated by the related tasks in a way that can be used to learn novel tasks  it is
an example of exploiting the bias provided by a set of related tasks to improve generalization
performance  other similar approaches include suddarth and kergosien         suddarth and
holden         abu mostafa        

e

bias as computational complexity  in this paper we consider inductive bias from a samplecomplexity perspective  how does the learnt bias decrease the number of examples required of
novel tasks for good generalization  a natural alternative line of enquiry is how the runningtime or computational complexity of a learning algorithm may be improved by training on
related tasks  some early algorithms for neural networks in this vein are contained in sharkey
and sharkey         pratt        
reinforcement learning  many control tasks can appropriately be viewed as elements of sets
of related tasks  such as learning to navigate to different goal states  or learning a set of
complex motor control tasks  a number of papers in the reinforcement learning literature
have proposed algorithms for both sharing the information in related tasks to improve average
generalization performance across those tasks singh         ring         or learning bias
from a set of tasks to improve performance on future tasks sutton         thrun and schwartz
       

    overview of the paper
in section   the bias learning model is formally defined  and the main sample complexity results
are given showing the utility of learning multiple related tasks and the feasibility of bias learning 
these results show that the sample complexity is controlled by the size of certain covering numbers
associated with the set of all hypothesis spaces available to the bias learner  in much the same way
as the sample complexity in learning boolean functions is controlled by the vapnik chervonenkis
dimension  vapnik        blumer et al          the results of section   are upper bounds on
the sample complexity required for good generalization when learning multiple tasks and learning
inductive bias 
the general results of section   are specialized to the case of feature learning with neural networks in section    where an algorithm for training features by gradient descent is also presented 
for this special case we are able to show matching lower bounds for the sample complexity of
multiple task learning  in section   we present some concluding remarks and directions for future
research  many of the proofs are quite lengthy and have been moved to the appendices so as not to
interrupt the flow of the main text 
the following tables contain a glossary of the mathematical symbols used in the paper 
   

fibaxter

symbol



u

 

description
input space
output space
distribution on
 learning task 
loss function
hypothesis space
hypothesis
error of hypothesis on distribution
training set
learning algorithm
empirical error of on training set
set of all learning tasks
distribution over learning tasks
family of hypothesis spaces
loss of hypothesis space on environment
 sample
empirical loss of on
bias learning algorithm
function induced by and
set of
average of
same as
set of
set of
function on probability distributions
set of
pseudo metric on
pseudo metric on
covering number of
capacity of
covering number of
capacity of
sequence of hypotheses
sequence of distributions
average loss of on
average loss of on
set of feature maps
output class composed with feature maps
hypothesis space associated with
loss function class associated with
covering number of
capacity of
pseudo metric on feature maps
covering number of

st 

 
 
         
v
 
 y w  x    

z
a
       
 p fi  
w  a   
 
v
 b
 
b
 b
  
   b fifi   c   b
   
 fifi  rc  b
  
 fifi  rc  b
dqb c
bc
   c 
 fifi   c  b
b
b
tea
e
e
cb
fa g
f 
e
a
h jijfi a e fi f   
a e
k jijfi a e c 
ea
cb
h jijfi a c b fi flg 
a
c
m jijfi b 
ap b
d a
n
p
g    d 
d n
d  
w      d 
o
p
psr q
pb
h jijfi p b fi f   
pb
k hijfi p b 
pb
ft     uvxw  q fi qzy 
h jijfi o fi f t     uv w 
o





u

   
 fifi   c 
  
 fifi  c 

   

qp

q fi qzy

q

z

first referenced
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

fia m odel of i nductive b ias l earning

hsymbol
o
jijfi o fi f t     uv w  description
covering number of
k uv jijfi o 
o
capacity of
  
neural network hypothesis space
   
restricted to vector 
    
growth function of
  
vapnik chervonenkis dimension of
  
restricted to matrix 
a     p fi  
a restricted to matrix 
growth function of a
f   pq
dimension function of a
f a 
upper dimension function of a
f a  c
lower dimension function cof a
n
  g  a 
optimal performance of a
on
f
on 
  
    c metric
 
 rc
average of 
     
c
 
 
 
c

 
set of 
 
       c  
permutations on integer pairs
 j
permuted  
f   d fi d y
u
d
empirical 
 metric on functions
w   g  
n
optimal average error of on

first referenced
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   the bias learning model
in this section the bias learning model is formally introduced  to motivate the definitions  we first
describe the main features of ordinary  single task  supervised learning models 
    single task learning
computational learning theory models of supervised learning usually include the following ingredients 

e

an input space

e



 on ss 
e a loss function u   s  d   and
a probability distribution

e

a hypothesis space

 

and an output space

 

 

which is a set of hypotheses or functions

     

 

as an example  if the problem is to learn to recognize images of marys face using a neural network 
then would be the set of all images  typically represented as a subset of
where each component
is a pixel intensity   would be the set
  and the distribution would be peaked over images
of different faces and the correct class labels  the learners hypothesis space would be a class of
neural networks mapping the input space
to
  the loss in this case would be discrete loss 



 

fi

   fi
 
u  zfi y      if l
 y



if

   

y

 

   

fibaxter

fi u
u  fi y   l  y   

  

   

using the loss function allows us to present a unified treatment of both pattern recognition  
  as above   and real valued function learning  e g  regression  in which
and usually
 
the goal of the learner is to select a hypothesis
with minimum expected loss 

  ct

   
           b u     fi  f   qfir
 
for an minimizing
of course  the learner does not know  and so it cannot search through
           in practice  the learner samples repeatedly from   according to the distribution  to
generate a training set
     
 fi 
 fifi   fi  j
   
  ct   hence  in general
based on the information contained in  the learner produces a hypothesis
v
a learner is simply a map from the set of all training samples to the hypothesis space  
v    dt     

v
 stochastic learners can be treated by assuming a distribution valued   
 
many algorithms seek to minimize the empirical loss of on    where this is defined by 
 

w   x             u      fi 
   


of course  there are more intelligent things to do with the data than simply minimizing empirical
errorfor example one can add regularisation terms to avoid over fitting 
however the learner chooses its hypothesis   if we have a uniform bound  over all
  on
the probability of large deviation between
and
  then we can bound the learners genas a function of its empirical loss on the training set
  whether such
eralization error
a bound holds depends upon the richness of   the conditions ensuring convergence between
and
are by now well understood  for boolean function learning  
  discrete
loss   convergence is controlled by the vc dimension  of  

  w   x    

       
         

 
w   x    

  c

        

  w  x    
  bfi

fi and suppose  
probability distribution on 
  ff
fi
 fifi  fibe isanygenerated
 
times from  fi according to    let
f         then with probabilityby atsampling
least    over the choice of the training set     all
  c will satisfy

  
 
 
 

f


w
   
          x  ffko      f k 

theorem    let

proofs of this result may be found in vapnik         blumer et al          and will not be
reproduced here 

aj 

 











   the vc dimension of a class of boolean functions
is the largest integer such that there exists a subset
such that the restriction of to contains all boolean functions on  

   

 

fia m odel of i nductive b ias l earning

       

        

  w  x    

theorem   only provides conditions under which the deviation between
and
is
will actually be small  this is
likely to be small  it does not guarantee that the true error
governed by the choice of   if contains a solution with small error and the learner minimizes
error on the training set  then with high probability
will be small  however  a bad choice of
will mean there is no hope of achieving small error  thus  the bias of the learner in this model 
is represented by the choice of hypothesis space  

       

    the bias learning model
the main extra assumption of the bias learning model introduced here is that the learner is embedded in an environment of related tasks  and can sample from the environment to generate multiple
training sets belonging to multiple different tasks  in the above model of ordinary  single task 
on
  so in the bias learning
learning  a learning task is represented by a distribution
model  an environment of learning problems is represented by a pair
where is the set of
 i e   is the set of all possible learning problems   and is a
all probability distributions on
distribution on   controls which learning problems the learner is likely to see    for example  if
the learner is in a face recognition environment  will be highly peaked over face recognition type
problems  whereas if the learner is in a character recognition environment will be peaked over
character recognition type problems  here  as in the introduction  we view these environments as
sets of individual classification problems  rather than single  multiple class classification problems  
recall from the last paragraph of the previous section that the learners bias is represented by its
choice of hypothesis space   so to enable the learner to learn the bias  we supply it with a family
 
or set of hypothesis spaces
putting all this together  formally a learning to learn or bias learning problem consists of 

y

 

y z



  y fi z 

z

y

z

z

a    

e

an input space

e

a loss function



and an output space

u   s  d  
e an environment  y fi z  where y
y
a distribution on  
e

a hypothesis space family

 

 both of which are separable metric spaces  

is the set of all probability distributions on

a  

where each

u

u

c a

from now on we will assume the loss function has range
we assume that is bounded 



d 

is a set of functions

and

      

z

is

 

 fi    or equivalently  with rescaling 

   the bias is also governed by how the learner uses the hypothesis space  for example  under some circumstances the
learner may choose not to use the full power of  a neural network example is early stopping   for simplicity in
this paper we abstract away from such features of the algorithm and assume that it uses the entire hypothesis space
 
   s domain is a  algebra of subsets of   a suitable one for our purposes is the borel  algebra
generated
by the topology of weak convergence on   if we assume that and are separable metric spaces  then is also
a separable metric space in the prohorov metric  which metrizes the topology of weak convergence   parthasarathy 
       so there is no problem with the existence of measures on
  see appendix d for further discussion 
particularly the proof of part   in lemma    













   








q



fibaxter

we define the goal of a bias learner to be to find a hypothesis space
following loss 

c a

minimizing the

                         f z   
   
        u     fi  f   fi  f z   
z
 
the only way       can be small is if  with high  probability  contains a good solution to
z
any problem  drawn at random according to   in this sense       measures how appropriate
y z
the bias embodied by is for the environment  fi   
z
in general the learner will not know   so it will not be able to find an minimizing      

z
y
times from according to to yield 
c

 
 fifi   





e sample   times from s  according to each  to yield 
 b  
 fi 
 fi   fi    
e the resulting p training setshenceforth called an  p fi    sample if they are generated by the
above processare supplied to the learner  in the sequel  an  p fi    sample will be denoted
by   and written as a matrix 
 ff

fi

   ff
fi
 j

  
  
  
  
   
   
 
 
 
  c 
 fi c 
     c  fi c     c
c
an  p fi    sample is simply p training sets  
 fifia sampled from p different learning tasks
c
 
fifi    where each task is selected according to the environmental probability distribution z  
the size of each training set is kept the same primarily to facilitate the analysis 
 c a  
based on the information contained in     the learner must choose a hypothesis space
 
directly  however  the learner can sample from the environment in the following way 

e

sample

p

one way to do this would be for the learner to find an
this is defined by 

minimizing the empirical loss on   where

  w  a     
 
 

 
 
c
 
 fifi 

c





  w          p 
   r   w   x     

   

      

note that
is simply the average of the best possible empirical error achievable on each
training set   using a function from   it is a biased estimate of
  an unbiased estimate of
would require choosing an with minimal average error over the distributions
  where this is defined by
 
as with ordinary learning  it is likely there are more intelligent things to do with the training data
than minimizing      denoting the set of all
 samples by
  a general bias
learner is a map that takes
 samples as input and produces hypothesis spaces
as
output 

 

v

 p fi  


c  c  
  r        
    
 p fi  

  c     
v    d

 s 
  a
 
c  
   

 ss     c     

p

sc a

   

fia m odel of i nductive b ias l earning

v

 as stated  is a deterministic bias learner  however it is trivial to extend our results to stochastic
learners  
note that in this paper we are concerned only with the sample complexity properties of a bias
learner   we do not discuss issues of the computability of  
since is searching for entire hypothesis spaces within a family of such hypothesis spaces
  there is an extra representational question in our model of bias learning that is not present in
is represented and searched by   we defer this
ordinary learning  and that is how the family
discussion until section      after the main sample complexity results for this model of bias learning
have been introduced  for the specific case of learning a set of features suitable for an environment
of related learning problems  see section   
regardless of how the learner chooses its hypothesis space   if we have a uniform bound  over
all
  on the probability of large deviation between
and
  and we can compute
an upper bound on
  then we can bound the bias learners generalization error
 
with this view  the question of generalization within our bias learning model becomes  how many
tasks     and how many examples of each task     are required to ensure that
and
are close with high probability  uniformly over all
  or  informally  how many tasks and how
many examples of each task are required to ensure that a hypothesis space with good solutions to
all the training tasks will contain good solutions to novel tasks drawn from the same environment 
it turns out that this kind of uniform convergence for bias learning is controlled by the size
of certain function classes derived from the hypothesis space family   in much the same way as
the vc dimension of a hypothesis space
controls uniform convergence in the case of boolean
function learning  theorem     these size measures and other auxiliary definitions needed to
state the main theorem are introduced in the following subsection 

v

a

v

v

v

a

c a
p

  w  a   

  w     

 

      

  w     

c a

     
     

a

     d    define   b      fi  by
 b  fi     u      fir
    
for any hypothesis space in the hypothesis space family a   define
bff  b   b   ct j
    
c
 
  c    define    
fifi   c  b   dt      fi  by
for any sequence of p hypotheses  
fifi
c   

   
 fifi  rc  b   
 fi 
 fifi c fi c     p   u      fi 
    


db
 
  c  b   for any in the hypothesis space family a   define
we will also use to denote  
 fifi
cb   b   
fifi   c  b   
fifi   c ct j
    
    covering numbers

definition    for any hypothesis

define

cb     cb 
a
 
   

    

fibaxter

       
b
b
p

    fi 

 b

in the first part of the definition above  hypotheses
are turned into functions
mapping
by composition with the loss function 
is then just the collection of all
such functions where the original hypotheses come from  
is often called a loss function class 
in our case we are interested in the average loss across tasks  where each of the hypotheses
is chosen from a fixed hypothesis space   this motivates the definition of
and
  finally 
is the collection of all
  with the restriction that all
belong to a single
hypothesis space
 

c
a b

c a

definition    for each

   
fifi   c  b

c a

  define

for the hypothesis space family

a

te   y  
   fi  by
e         r          

  define

db
  
fifi   c

p c
b

    

e  b
  e   c a j
a
    
cb
e that controls how large the  p fi    sample   must be to ensure
it is the size of a
and a
 w  a    and        are close uniformly over all c a   their size will be defined in terms of
certain covering
cb numbers  and for this we neede to define how to measure the distance between
elements of a
and also between elements of a  
n   
 fifi  c  be any sequence of p probability distributions on dt    for
definition    let
c
db d b c a b   define
any fi y
flg  dqb fi d yb    m      db   
 fi 
 fifi c fi c    d yb   
 fi 
 fifi c fi c  
    
f  
   
 fi 
  f  c   c fi c 
z y
e e c a e   define
similarly  for any distribution on and any 
 fi  
f    e
 fi e        e
      e     f z   
    
 
 
fg and f   are pseudo metrics on a cb and a e respectively 
it is easily verified that
e f
te te
edc a e  


definition    an i  cover of  a fi    is a set  
 fifi   such that for all
f    te fi e ti for some      note that we do not require the te to be contained in
h
y
e f
a e   just that they be measurable functions
on   let jijfi a fi    denote the size of the smallest
e
such cover  define the capacity of a
by
k jilfi a e    
   h jilfi a e fi f   
    
 
y   h jijfi a cb fi f g  is defined in a similar
where the supremum is over all probability measures on
c
fg in place of f     define the capacity of a b by 
way  using
k jijfi a cb    g  h jilfi a cb fi fg 
    
where now the supremum is over all sequences of p probability measures on s   
  r  ff
     
   a pseudo metric  is a metric without the condition that 
 

   

fia m odel of i nductive b ias l earning

    uniform convergence for bias learners
now we have enough machinery to state the main theorem  in the theorem the hypothesis space
family is required to be permissible  permissibility is discussed in detail in appendix d  but note
that it is a weak measure theoretic condition satisfied by almost all real world hypothesis space
families  all logarithms are to base  

y



 



z

 
 p fi  

 




z
c
y
p
 
 
 fi  fififip




 



fi




fi







fi
 



fi














a
p 

k  e 






pfi
i         fi a fi i    fi
and the number of examples   of each task satisfies
k    fi a cb 






  fi
pffi         fi i      fi
c a will satisfy
then with probability at least    over the  p fi    sample      all
        w  a    k i

theorem    suppose
and are separable metric spaces and let be any probability distribution on   the set of all distributions on
  suppose is an
 sample generated by
sampling times from according to to give
  and then sampling times from each
to generate
 
  let
be any permissible
hypothesis space family  if the number of tasks satisfies





    

    

    

proof  see appendix a 

k jijfi a cb 
 p fi   a

there are several important points to note about theorem   

k j ijfi a e 

      
  cs
  o   

   provided the capacities
and
are finite  the theorem shows that any bias
can bound its generalisation error
in
learner that selects hypothesis spaces from
terms of
for sufficiently large
 samples   most bias learners will not find the
exact value of
because it involves finding the smallest error of any hypothesis
on each of the training sets in   but any upper bound on
 found  for example
by gradient descent on some error function  will still give an upper bound on
  see
section       for a brief discussion on how this can be achieved in a feature learning setting 

  w  a   w  
  ap  

  w     

 

 

 

 
 
p

c a

 

  w  a   

 

and
are close uniformly over all
   in order to learn bias  in the sense that
   both the number of tasks and the number of examples of each task
must
be sufficiently large  this is intuitively reasonable because the bias learner must see both
sufficiently many tasks to be confident of the nature of the environment  and sufficiently
many examples of each task to be confident of the nature of each task 

c a
z
 

  w      

   once the learner has found an
with a small value of
  it can then use
to
learn novel tasks drawn according to   one then has the following theorem bounding the
sample complexity required for good generalisation when learning with  the proof is very
similar to the proof of the bound on in theorem    



   

fibaxter

  
 fi 
 fifi   fi  


ilfi    ijfi m
 

k   b 
  fi   i      
    fi fi i   
 
    
  ct will satisfy
then with probability at least       all
          w  x    k ij
k
the capacity jilfi  appearing in equation      is defined in an analogous
  b  fito the 
f  b   b      fashion
capacities in definition    we just use the pseudo metric    fi y   

  yb  qfir  f   fi     the important thing to note about theorem   is that the number
of ex 

theorem    let
be a training set generated by sampling from
according to some distribution   let be a permissible hypothesis space  for all
  
with  
  if the number of training examples satisfies

amples required for good generalisation when learning novel tasks is proportional to the logarithm of the capacity of the learnt hypothesis space   in contrast  if the learner does not
do any bias learning  it will have no reason to select one hypothesis space
over any
other and consequently it would have to view as a candidate solution any hypothesis in any
of the hypothesis spaces
  thus  its sample complexity will be proportional to the
capacity of  
  which in general will be considerably larger than the capacity
  so by learning the learner has learnt to learn in the environment
of any individual
in the sense that it needs far smaller training sets to learn novel tasks 

y fi z 

c a

c
    b  a b
 a
c a

   
w   a   ff k i
      
 
        r             fi

w  a   
 

            

   having learnt a hypothesis space
with a small value of
  theorem   tells us that
with probability at least
  the expected value of
on a novel task will be
less than
  of course  this does not rule out really bad performance on some tasks
  however  the probability of generating such bad tasks can be bounded  in particular 
note that
is just the expected value of the function
over   and so by markovs
inequality  for      



e

   

     
   
 

 
 
 
 
 
w
     

e

  e  

 


  ki

fi
 



y



 with probability

     

ijfi

   keeping the accuracy and confidence parameters
fixed  note that the number of examples
required of each task for good generalisation obeys

   f  p    k  ijfi a cb   
    
c
 k jilfi a b  increases sublinearly with p   the upper bound on the number of
so provided  

examples required of each task will decrease as the number of tasks increases  this shows
that for suitably constructed hypothesis space families it is possible to share information
between tasks  this is discussed further after theorem   below 
   

fia m odel of i nductive b ias l earning

    choosing the hypothesis space family

      
 p fi  
 

a

 

  w      

      
a c a
p a  

theorem   only provides conditions under which
and
are close  it does not guarantee that
is actually small  this is governed by the choice of   if contains a hypothesis
space with a small value of
and the learner is able to find an
minimizing error on
sample  i e   minimizing
   then  for sufficiently large and   theorem   enthe
sures that with high probability
will be small  however  a bad choice of will mean there
is no hope of finding an with small error  in this sense the choice of represents the hyper bias
of the learner 
note that from a sample complexity point of view  the optimal hypothesis space family to choose
that contains good solutions to all of the
is one containing a single  minimal hypothesis space
problems in the environment  or at least a set of problems with high  probability   and no more 
for then there is no bias learning to do  because there is no choice to be made between hypothesis
spaces   the output of the bias learning algorithm is guaranteed to be a good hypothesis space for
the environment  and since the hypothesis space is minimal  learning any problem within the environment using will require the smallest possible number of examples  however  this scenario
is analagous to the trivial scenario in ordinary learning in which the learning algorithm contains a
single  optimal hypothesis for the problem being learnt  in that case there is no learning to be done 
just as there is no bias learning to be done if the correct hypothesis space is already known 
at the other extreme  if contains a single hypothesis space consisting of all possible functions from
then bias learning is impossible because the bias learner cannot produce a
restricted hypothesis space as output  and hence cannot produce a hypothesis space with improved
sample complexity requirements on as yet unseen tasks 
focussing on these two extremes highlights the minimal requirements on for successful bias
must be strictly smaller than the space of all
learning to occur  the hypothesis spaces
functions
  but not so small or so skewed that none of them contain good solutions to a
large majority of the problems in the environment 
it may seem that we have simply replaced the problem of selecting the right bias  i e   selecting
the right hypothesis space   with the equally difficult problem of selecting the right hyper bias  i e  
the right hypothesis space family    however  in many cases selecting the right hyper bias is far
easier than selecting the right bias  for example  in section   we will see how the feature selection
problem may be viewed as a bias selection problem  selecting the right features can be extremely
difficult if one knows little about the environment  with intelligent trial and error typically the best
one can do  however  in a bias learning scenario  one only has to specify that a set of features should
exist  find a loosely parameterised set of features  for example neural networks   and then learn the
features by sampling from multiple related tasks 

    
   



w  a   
 

 

a

a

z

    

a

a

c a

   

a

    learning multiple tasks

p

y fi z 
a

it may be that the learner is not interested in learning to learn  but just wants to learn a fixed set
  as in the previous section  we assume the learner starts
of tasks from the environment
out with a hypothesis space family   and also that it receives an
 sample generated from
the distributions
  this time  however  the learner is simply looking for hypotheses
  all contained in the same hypothesis space   such that the average generalization
error of the hypotheses is minimal  denoting
by and writing
 

p
  
 fifi  rc 

p

 
 fifi  c

 p fi  

   
 fifi   c 

   

d

 

p

n    
 fifi  c 

fibaxter

c






    g  d     p 
         
    
c


 p     u     fi  f    fi fi


d
and the empirical loss of on   is
c


w  a   d     p    w  x     
    

c    
 p         u        fi   

   

 
  c    if we can prove a uniform bound on
as before  regardless of how the learner chooses  
 fifi
g
d
d
 
  c  that perform
w
the probability of large deviation between        and      then any  
fifi
well on the training sets   will with high probability perform well on future examples of the same

this error is given by 

tasks 

n 
   
fifi  c  p
 

  


 

a   

 p fi  

theorem    let
be probability distributions on
and let be an
according to each   let
be any
sample generated by sampling times from
permissible hypothesis space family  if the number of examples of each task satisfies

 

 

k  
    fi a cb  






 ffp  i    
 
  fi
    
 fi i    
d c a c will satisfy
then with probability at least    over the choice of      any
    
   g  d    w  a   d ffk i
c
k b
 recall definition   for the meaning of jilfi a    
proof  omitted  follow the proof of the bound on   in theorem    
the bound on   in theorem   is virtually identical to the bound on   in theorem    and note
again that it depends inversely on the number of tasks p  assuming that the first part of the max
k  cb
expression is the dominate one   whether this helps depends on the rate of growth of   
   fi a  as
a function of p   the following lemma shows that this growth is always small enough to ensure that
we never do worse by learning multiple tasks  at least in terms of the upper bound on the number of
examples required per task  

a  


k  li fi a b   k h ilfi a cb   k  li fi a b
  c 

lemma    for any hypothesis space family

   

    

fia m odel of i nductive b ias l earning


 
 
r
 
c
b
proof  let   denote the set of all functions  
 fifi c  where each can be
k hijfi a cb  a member
k jijfi   of   any
 c a  recall definition     then
  
b
c
a
 
and
so
by
hypothesis space


k
k
 
 ilfi

b
h

j
i
fi



lemma    in appendix b 
 
a and so the right hand inequality follows 
n be the meafor the first inequality 
let  be any probability measure on    and let
c
sure on       obtained by using  on the first copy of    
cb flg in the product  and
b c ignoring
b
 and
all other elements of the product  let
be an i  cover for  a fi
  pick any
a
c b c be such that fg    fi   fifi    b fi   
 fifi   c  b  i   but by construction 
let    
 fifi   
flg    fi   fi fi    b fi    
 fi fi   c  b  f      fi    
  b    which establishes the first inequality 
  k  ji fi a b
     k jijfi a cb p   k  ijfi a b
  
    
so keeping the accuracy parameters i and  fixed  and plugging      into       we see that the upper
bound on the number of examples required of each task never increases with the number of tasks 
and at best decreases as f npq   although only an upper bound  this provides a strong hint that
by lemma  

learning multiple related tasks should be advantageous on a number of examples required per task
basis  in section   it will be shown that for feature learning all types of behavior are possible  from
decrease 
no advantage at all to
    dependence on

i

f npq

ni  

ni

in theorems      and   the bounds on sample complexity all scale as
  this behavior can be
improved to
if the empirical loss is always guaranteed to be zero  i e   we are in the realizable
case   the same behavior results if we are interested in relative deviation between empirical and
true loss  rather than absolute deviation  formal theorems along these lines are stated in appendix
a   

   feature learning
the use of restricted feature sets is nearly ubiquitous as a method of encoding bias in many areas of
machine learning and statistics  including classification  regression and density estimation 
in this section we show how the problem of choosing a set of features for an environment of
related tasks can be recast as a bias learning problem  explicit bounds on
and
are calculated for general feature classes in section      these bounds are applied to the problem of
learning a neural network feature set in section     

k  a e fiai 

k  a cb fiai 

    the feature learning model
consider the following quote from vapnik        
the classical approach to estimating multidimensional functional dependencies is
based on the following belief 
real life problems are such that there exists a small number of strong features  simple
functions of which  say linear combinations  approximate well the unknown function 
therefore  it is necessary to carefully choose a low dimensional feature space and then
to use regular statistical techniques to construct an approximation 
   

fibaxter

q    
o

q


fi






fi




q
q
q

q
p
 
q c o
ptr q   b r q   c p 
a
s
p
r
 
 

c
o
    
a  q q j
now the problem of carefully choosing the right features q is equivalent to the bias learning
c a   hence  provided the
problem find the right hypothesis space
k e
k cb learner is embedded within
an environment of related tasks  and the capacities  a fiai and  a fiai are finite  theorem   tells
us that the feature set q can be learnt rather than carefully chosen  this represents an important
simplification  as choosing a set of features is often the most difficult part of any machine learning
problem 
k e
k cb
in section     we give a theorem bounding  a fiai and  a fiai  for general feature classes 
the theorem is specialized to neural network classes in section     
p
note that we have forced the function class to be the same for all feature maps q   although
p
this is not necessary  indeed variants of the results to follow can be obtained if is allowed to vary
with q  

in general a set of strong features may be viewed as a function
   mapping the input
into some  typically lower  dimensional space     let
be a set of such feature
space
 
maps  each may be viewed as a set of features
   it is the that must be
   if  
carefully chosen in the above quote  in general  the simple functions of the features may be
represented as a class of functions mapping   to   if for each
we define the hypothesis
  
 
  then we have the hypothesis space family
space

    capacity bounds for general feature classes

 fi   
q


s 

s 
p
u
c p
 q  fi 
u
 fi   fir     fi  u
b  
pb
b
p b r o    r q   c p b fi q co 
pb
k jilfi p b    d
 e  h jilfi p b fi f   
 
f
     zcbfir 
where the supremum is over all probability measures on      and     fi   y    gf
  cbfi  f
y   cbfi    to
 define the capacity p ofb o we first define a pseudo metric f t     uvxw on o by
pulling back the h metric on  through as follows 
ft     uv w  q fi q y       u v   r q  fi       r q y  qfir f   fi 
    


i 
f t
ft
it is easily verified that     uvxw is a pseudo metric  note that for     uv w to be well defined the suprep
b
mum over in the integrand must be measurable  this is guaranteed if theh hypothesis space family
o ft
a   p ib r q   q c o  is permissible  lemma
    part     now define jilfi fi     uv w  to be the
t
pb
f
o
o
smallest  cover of the pseudo metric space  fi     u v w  and the i  capacity of  with respect to  
as
k uv jilfi o    d
   h jilfi o fi f t     uv w 
 
where the supremum is over all probability measures on      now we can state the main theorem
a 
to  
by
notationally it is easier to view the feature maps as mapping from
  and also to absorb the loss function into the definition of by viewing each  
as a
a 
   cb
into
via cb
  previously this latter function would have been
map from  
denoted   but in what follows we will drop the subscript where this does not cause confusion  the
class to which   belongs will still be denoted by  
  
 
with the above definitions let
  define the capacity of in
the usual way 

of this section 

   

fia m odel of i nductive b ias l earning

ii 
 k i  

theorem    let
 

a

be a hypothesis space family as in equation       then for all

k j ilfi a cb 
k jijfi a e  

k h i 
 fi p b  c k u v ji   fi o 
k uv hilfi o 

ijfiai 
 fiai     

with
    
    

proof  see appendix b 
    learning neural network features

f

in general  a set of features may be viewed as a map from the  typically high dimensional  input
 
to a much smaller dimensional space
  jlk
   in this section we consider approximatspace
ing such a feature map by a one hidden layer neural network with input nodes and j output nodes
qp r
on
n
 
 figure     we denote the set of all such feature maps by m
where
r
is a bounded subset of ts   u is the number of weights  parameters  in the first two layers  
this set is the of the previous section 
each feature n
 
j is defined by



o







f
 
      
 fifi        c 

           fi    fifi
b 


 n        vwx   b         kyb b 
 z
    
 
  


 
b 
  are the output
 
where     is the output
of the     node in the first hidden layer  cb 
 fifi b

 
node parameters for the  th feature and v is a sigmoid squashing function v    fi    each
       s     fifi u   computes
first layer hidden node






       v wx

 z 
     k
    





a
 
 
 



  are the hidden nodes parameters  we assume v is lipschitz  the weight
where  
fifi
 
  
vector for the entire feature map is thus
p  

fifi 


 fifi b 
 fifi b  
  fi b

fi fi b 
 b  
 fi fi b   
 fi fi b   b  
 



 
 
 
 
uf
u
and the total number of feature parameters u   kffk j  k   
p
for arguments sake  assume the simple functions of the features  the class of the previous
 

section  are squashed affine maps using the same sigmoid function v above  in keeping with the
p
neural network flavor of the features   thus  each setting of the feature weights generates a
hypothesis space 

    



   

 
n     k


 f    
fifi

   c r y  fi
 
 

ed
d 
d
d 


r
 
where y is a bounded subset of     the set of all such hypothesis spaces 
a   b   q  p c r 
  i   k   h lmgnh  i   k h for all hkporq  
    is lipschitz if there exists a constant g such that h  j
vcb

   

    

    

fibaxter

multiple output classes
n

k

l

feature
map

d
input

p

p

 p fi  

figure    neural network for feature learning  the feature map is implemented by the first two
hidden layers  the output nodes correspond to the different tasks in the
sample   each node in the network computes a squashed linear function of the nodes in
the previous layer 

 

 
 fifi 
 


 
and feature
is a hypothesis space family  the restrictions on the output layer weights
p
d
d
weights   and the restriction to a lipschitz squashing function are needed to obtain finite upper
bounds on the covering numbers in theorem   
finding a good set of features for the environment
is equivalent to finding a good hyp
  which in turn means finding a good set of feature map parameters  
pothesis space
as in theorem    the correct set of features may be learnt by finding a hypothesis space with
small error on a sufficiently large
 sample   specializing to squared loss  in the present
framework the empirical loss of
on  equation      is given by

y fi z 

  c a

 p fi  
 
 
c

  

 









b
b
    
 w        p 
  tsu   s   w vwrvwv   s x    y k     
 z vyb b 
 d n          k d  f       
since our sigmoid function v only has range  fi    we also restrict the outputs   to this range 
 

      a lgorithms

for

f inding

a

g ood s et

of

f eatures

provided the squashing function v is differentiable  gradient descent  with a small variation on
p
backpropagation to compute the derivatives  can be used to find feature weights minimizing     
 or at least a local minimum of        the only extra difficulty over and above ordinary gradient
descent is the appearance of   in the definition of
  the solution is to perform gradient
p
  for each node and the feature weights   for
descent over both the output parameters
d
d
more details see baxter      b  and baxter      a  chapter     where empirical results supporting
the theoretical results presented here are also given 

r

w        
 

  fifi 
   

fia m odel of i nductive b ias l earning

      s ample c omplexity b ounds

 
k j ijfi a cb 

for

n eural  n etwork f eature l earning

the size of ensuring that the resulting features will be good for learning novel tasks from the same
environment is given by theorem    all we have to do is compute the logarithm of the covering
numbers
and
 

k j ijfi a e 
   ap c ff s  be a hypothesis space family where each    is of the form
theorem    let a   
 

 




     v b
n       k
f    
fifi    c     fi

d

d
d
d
   o n     
 fifi n        is a neural network with u weights mapping from   to      if the
where m
p
feature weights and the output weights  fi 
fifi   are bounded  the squashing function v is
u
d
d
d
lipschitz  is squared loss  and the output space     fi   any bounded subset of  will do   then
there exist constants  fi  y  independent of ilfi u and j   such that for all i     
  k jijfi a cb     j kp k u    
    
i
  k jijfi a e    u   ri y
    
 recall that we have specialized to squared loss here  
proof  see appendix b 
noting that our neural network hypothesis space family
into theorem   gives the following theorem 

a

is permissible  plugging      and     

a    

 

theorem    let
be a hypothesis space family where each hypothesis space
is a
set of squashed linear maps composed with a neural network feature map  as above  suppose the
number of features is j   and the total number of feature weights is w  assume all feature weights and
 sample
output weights are bounded  and the squashing function v is lipschitz  let be an
generated from the environment
  if

 

y fi z 

pfi f  i     u   i k     r fi

 p fi  

    

and

  fi f  i      j k k u p    i k p      r 
  c a will satisfy
then with probability at least   any
           w           k il
   

    

    

fibaxter

      d iscussion

i

f  k



npq

   keeping the accuracy and confidence parameters and fixed  the upper bound on the number
of examples required of each task behaves like j
u
  if the learner is simply learning
fixed tasks  rather than learning to learn   then the same upper bound also applies  recall
theorem    

p
 

f  

p




and the upper bound on
   note that if we do away with the feature map altogether then u
becomes j   independent of  apart from the less important term   so in terms of the
upper bound  learning tasks becomes just as hard as learning one task  at the other extreme 
if we fix the output weights then effectively j
and the number of examples required of
each task decreases as u
  thus a range of behavior in the number of examples required
decrease as the number of
of each task is possible  from no improvement at all to an
tasks increases  recall the discussion at the end of section      

p



f  npq

p

f npq

   once the feature map is learnt  which can be achieved using the techniques outlined in baxter 
    b  baxter   bartlett        baxter      a  chapter     only the output weights have to be
estimated to learn a novel task  again keeping the accuracy parameters fixed  this requires no
more that j examples  thus  as the number of tasks learnt increases  the upper bound on
the number of examples required of each task decays to the minimum possible  j  

f  

f  

   if the small number of strong features assumption is correct  then j will be small  however 
typically we will have very little idea of what the features are  so to be confident that the neural
network is capable of implementing a good feature set it will need to be very large  implying
uj  
j
u
decreases most rapidly with increasing when uj   so at least in
terms of the upper bound on the number of examples required per task  learning small feature
sets is an ideal application for bias learning  however  the upper bound on the number of
tasks does not fare so well as it scales as u  

f  k

npq

p

f  


a special case of this multi task framework is one in which the marginal distribution on the input
  is the same for each task   fifip   and all that varies between tasks is the conditional
space 
distribution over the output space     an example would be a multi class problem such as face
l fifip  where p is the number of faces to be recognized and the
recognition  in which   s

marginal distribution on  is simply the natural distribution over images of those faces  in that
case  if for every example    we havein addition to the sample    from the  th tasks conditional
distribution on   samples from the remaining p   conditional distributions on     then we can
view the p training sets containing   examples each as one large training set for the multi class
problem with  tp examples altogether  the bound on   in theorem   states that  tp should be
f  p j k u    or proportional to the total number of parameters in the network  a result we would
      c omparison

with

t raditional m ultiple  c lass c lassification

expect from   haussler        
so when specialized to the traditional multiple class  single task framework  theorem   is consistent with the bounds already known  however  as we have already argued  problems such as face
recognition are not really single task  multiple class problems  they are more appropriately viewed
   if each example can be classified with a large margin then naive parameter counting can be improved upon  bartlett 
      

   

fia m odel of i nductive b ias l earning

p

p

as a  potentially infinite  collection of distinct binary classification problems  in that case  the goal
of bias learning is not to find a single  output network that can classify some subset of faces
well  it is to learn a set of features that can reliably be used as a fixed preprocessing for distinguishing any single face from other faces  this is the new thing provided by theorem    it tells us that
provided we have trained our  output neural network on sufficiently many examples of sufficiently
many tasks  we can be confident that the common feature map learnt for those tasks will be good
for learning any new  as yet unseen task  provided the new task is drawn from the same distribution
that generated the training tasks  in addition  learning the new task only requires estimating the j
output node parameters for that task  a vastly easier problem than estimating the parameters of the
entire network  from both a sample and computational complexity perspective  also  since we have
high confidence that the learnt features will be good for learning novel tasks drawn from the same
environment  those features are themselves a candidate for further study to learn more about the
nature of the environment  the same claim could not be made if the features had been learnt on too
small a set of tasks to guarantee generalization to novel tasks  for then it is likely that the features
would implement idiosyncrasies specific to those tasks  rather than invariances that apply across
all tasks 

p

p

 

p

when viewed from a bias  or feature  learning perspective  rather than a traditional  class
classification perspective  the bound on the number of examples required of each task takes on
a somewhat different meaning  it tells us that provided is large  i e   we are collecting examples
of a large number tasks   then we really only need to collect a few more examples than we would
examples vs  j examples  
otherwise have to collect if the feature map was already known   j u
so it tells us that the burden imposed by feature learning can be made negligibly small  at least when
viewed from the perspective of the sampling burden required of each task 

p

k np

    learning multiple tasks with boolean feature maps

i

p



ignoring the accuracy and confidence parameters and   theorem   shows that the number of
examples required of each task when learning tasks with a common neural network feature map
j
u
is bounded above by
  where j is the number of features and u is the number of
adjustable parameters in the feature map  since
j examples are required to learn a single task
once the true features are known  this shows that the upper bound on the number of examples
required of each task decays  in order  to the minimum possible as the number of tasks increases 
this suggests that learning multiple tasks is advantageous  but to be truly convincing we need to
 
prove a lower bound of the same form  proving lower bounds in a real valued setting  
is complicated by the fact that a single example can convey an infinite amount of information  so
one typically has to make extra assumptions  such as that the targets
are corrupted by a
noise process  rather than concern ourselves with such complications  in this section we restrict
our attention to boolean hypothesis space families  meaning each hypothesis
maps to

and we measure error by discrete loss
if
and
otherwise  

f  k

npq

f  

p

 c  

   

  c a 

u      fir       u       fir
    
we show that the sample complexity for learning p tasks with a boolean hypothesis space family
f   pq  that is  we give nearly matching upper
type parameter
a is controlled by a vc dimension
f   pq    we then derive bounds on f   pq for the hypothesis space
and lower bounds involving

family considered in the previous section with the lipschitz sigmoid function v replaced by a hard
threshold  linear threshold networks  
   

fibaxter

f  

as well as the bound on the number of examples required per task for good generalization across
those tasks  theorem   also shows that features performing well on u
tasks will generalize well
to novel tasks  where u is the number of parameters in the feature map  given that for many feature
learning problems u is likely to be quite large  recall note   in section         it would be useful
to know that
u
tasks are in fact necessary without further restrictions on the environmental
distributions generating the tasks  unfortunately  we have not yet been able to show such a lower
bound 
there is some empirical evidence suggesting that in practice the upper bound on the number of
tasks may be very weak  for example  in baxter and bartlett        we reported experiments in
which a set of neural network features learnt on a subset of only     japanese characters turned out
to be good enough for classifying some      unseen characters  even though the features contained
several hundred thousand parameters  similar results may be found in intrator and edelman       
and in the experiments reported in thrun        and thrun and pratt        chapter     while
this gap between experiment and theory may be just another example of the looseness inherent in
general bounds  it may also be that the analysis can be tightened  in particular  the bound on the
number of tasks is insensitive to the size of the class of output functions  the class in section      
which may be where the looseness has arisen 

zf

 

p

      u pper and l ower b ounds
s pace families

for

l earning  tasks

with

b oolean h ypothesis

t  
 fifi   c      

      b     
 fifi          ct j
  
  
clearly         if       we say shatters    the growth function of is defined by
           l           
   is the size of the largest set shattered by  
the vapnik chervonenkis dimension
                    j

first we recall some concepts from the theory of boolean function learning  let
be a class of
 
is the set of all binary vectors obtainable
boolean functions on and
by applying functions in to  



an important result in the theory of learning boolean functions is sauers lemma  sauer         of
which we will also make use 
lemma    sauers lemma   for a boolean function class

for all positive integers

 

with

   f  

  fi
               f

 
 

we now generalize these concepts to learning

p

   

tasks with a boolean hypothesis space family 

fia m odel of i nductive b ias l earning

definition    let
input space by
matrices 



  matrices over the
denote the p
a   bec    a  boolean hypothesis
  c    space
  family 
c
c
 



a   define to be the set of  binary 
  for each 
and
   
   

 
   
   
 

 z  
fifi   c ct  
       
  
  
  

 
 
 
  c   c 
    rc   c   t




a              
   p fi   by
now for each p   fi       define
  p fi       l     a     

c       c 


 

 
p

fi
 
 
 


note that
matrix 
   if  a   we say a shatters the
c

f   pq             p fi    j
define

define

lemma    

  for each

p
 



let

f  a         a 
  and
f  a            
f a  fi f a 
f  pq fi    f  p a   fi f  a 
 

f a  k f  
  p  a 
proof  the first inequality is trivial from the definitions  to get the second term in the maximum
c a with     f  a  and construct a matrix
in the second
inequality  choose an
c
 
 
 

f
 c 
whose rows are of length  a  and are shattered by   then clearly a 
 shatters    for

the first term in the maximum take a sequence t  
 fifi      shattered by a  the hypothesis

space consisting of the union over all hypothesis spaces from a    and distribute its elements equally
among the rows of   throw away any leftovers   the set of matrices
    ff

o
    ff



   

 
 
c
  
  

  
a

 
 
 
 
c
 
c   t

 



 











c

f
   and has size   
where     a np is a subset of a
c  c  
lemma    
 
  p fi    f    pq   
   

fi





fibaxter

p   p fi        p   
 
 fifi c 
 

  
 
f    pq     p f   qp 

p

  
 fifi  rc

proof  observe that for each  
where is the collection of all boolean
obtained by first choosing functions
from some
functions on sequences
  and then applying
to the first examples 
to the second examples and so on  by
the definition of
 
  hence the result follows from lemma   applied to
 

c a

 

 k  a cb fiai 

   p fi    
fi

if one follows the proof of theorem    in particular the proof of theorem    in appendix
a  then it is clear that for all  
 
may be replaced by
in the boolean
e
case  making this replacement in theorem     and using the choices of
from the discussion
d
following theorem     we obtain the following bound on the probability of large deviation between
empirical and true performance in this boolean setting 

n    
 fifi  c  p
   and let   be an


 p fi  
 
     

   let a b 
 
      d c a c      g  d  fi  w a     d ffk ij     p fi              p   n   

    
corollary     under the conditions of theorem     if the number of examples   of each task


be probability distributions on
theorem     let

 sample generated by sampling times from
according to each
be any permissible boolean hypothesis space family  for all   
 

satisfies

  fi i     f   pq   i  k p   
 
d
then with probability at least    over the choice of      any
   g  d    w  a   d ffk i



c a c

    
will satisfy
    

proof  applying theorem     we require

     p fi              p   n    fi

which is satisfied if

  fi     f    pq  f      pq k p      fi

i fim   if
where we have used lemma     now  for all m
     k   i    k   ifi



f   pqni          is satisfied if
then   fii      so setting il  
  fi i     f   pq   i  k p      
   

    

fia m odel of i nductive b ias l earning

corollary    shows that any algorithm learning
requires no more than

p

tasks using the hypothesis space family

   f  i      f    pq   i k p      r

c

i

a

    

 

p

examples of each task to ensure that with high probability the average true error of any hypotheses
it selects from
is within of their average empirical error on the sample   we now give a
theorem showing that if the learning algorithm is required to produce hypotheses whose average
true error is within of the best possible error  achievable using
  for an arbitrary sequence of
distributions
  then within a
 factor the number of examples in equation      is also
necessary 

for any sequence
of probability distributions on
  define
by

a

i
 
fifi  c

  g  a c 

a
  

n    
 fifi  c  p
 g  a c      r      g  d 

c p



 


 contains at least two
a pbe afi boolean
hypothesis space family such that a
fifi let v c be any learning algorithm taking as input  p fi  c   samples
c
 
 
 


c
       and producing as output p hypotheses d s   
 fifi   c  c a   for all
 i mn   and      mn     if
    i     f    pq km i    p          
 

 

c
n
   
fifi   such that with probability at least   over the
then there exist distributions
 
random choice of   
g    v c j      g  a c ffk i

theorem     let
functions  for each

proof  see appendix c

   ni 
f   pq a

      l inear t hreshold n etworks

p
f   pq

factor  the sample complexity of
theorems    and    show that within constants and a
learning tasks using the boolean hypothesis space family is controlled by the complexity parameter
  in this section we derive bounds on
for hypothesis space families constructed
as thresholded linear combinations of boolean feature maps  specifically  we assume is of the
form given by                  and       where now the squashing function v is replaced with a hard
threshold 

if fi
v
otherwise

        

rfi

a

fi

fi ry

and we dont restrict the range of the feature and output layer weights  note that in this case the
proof of theorem   does not carry through because the constants   in theorem   depend on the
lipschitz bound on v  

a

f u

theorem     let be a hypothesis space family of the form given in                  and       with
a hard threshold sigmoid function v   recall that the parameters   and j are the input dimension 
number of hidden nodes in the feature map and number of features  output nodes in the feature map 
   

fibaxter

   u  f k k j  u k  the number of adjustable parameters in the feature
f   pq    u k j k       j k u kz
p
 
proof  recall that
  c    for  eachm   p    c ts   m            denotes the feature map with parameters p  
c

for each 
  let
denote the matrix
 m    

 
 m    

 
 

 
 m     c 
      m        c  t 
   is the set of all binary p   matrices obtainable by composing thresholded linear
note that a
m    

respectively  let u
map   then 

functions with the elements of
  with the restriction that the same function must be applied to
each element in a row  but the functions may differ between rows   with a slight abuse of notation 
define

  p fi           


  m


      ap c 

s



 



  c    
c
fix 
  by sauers
 lemma  each node in the first hidden layer of the feature map computes

f     functions on the p   input vectors in    thus  there can be at most
at most    tpqnb    k
   tpqn f k  
 distinct functions from the input to the output of the first hidden layer on
the p   points in    fixing the first hidden layer
u b 
 parameters  each node in the second layer of  b the 
feature map computes at most    tpqn k  functions on the image of  produced at the output
u   

of the first hidden layer  thus the second hidden layer computes no more than    tpqn k 
functions on the output of the first hidden layer on the p   points in    so  in total 
b   
    tp     b 
  
t
 
p
   p fi     f  kr   u  k  

       the number of functions computable on each row of m      by a
now  for each possible matrix m


 
thresholded linear combination of the output of the feature map is at most     n j k    hence 
c     
   obtainable by applying linear threshold functions to all the
the number of binary sign assignments
   thus 
rows is at most     n j k 
b  
 
c   
 
 b 
  
   p fi    f   tkp     u  tkp       p   tkp      
j
 

q      is a convex function  hence for all ifigfi      
ik u gy
k 

j
q  j k u k   j k u k  jrq jiffk u q hgffk q  
b
b 
u
 
  
k

k








j

fi
 j ik u g kc 
i g  
u k   g f k and  p j k shows that
substituting i 
c    
  
u
t
 

p

k

k




 
s
  p fi     k j p k 
    
u

j

   

fia m odel of i nductive b ias l earning

hence  if

 tp j k u k 





k
k

    
   p
j
    u k p j k 
  p fi     c  and so by definition f   pq    for all i      observe that    i     
then

u kn u k p j k and i    j k u k shows that
if t  i     i   setting t   tp j k

u k  
     is satisfied if      u np k j k       j k
f
u
theorem     let a be as in theorem    with the following extra restrictions  fi    fi j and
f
   then
j
f   pq fi    u p  k j k 
 


f
f
proof  we bound  a  and  a  and then apply lemma     in the present setting a contains all
f
u
three layer linear threshold networks with input nodes  hidden nodes in the first hidden layer  j
 

u

hidden nodes in the second hidden layer and one output node  from theorem    in bartlett        
we have

    a 
 fi lf u k u  j   k fi

f
which under the restrictions stated above is greater than u n    hence  a fi
 f ufi

u

n 

f  

as j
and
j we can choose a feature weight assignment so that the feature map is the
j
identity on j components of the input vector and insensitive to the setting of the reminaing
components  hence we can generate j
points in
whose image under the feature map is
j
shattered by the linear threshold output node  and so
 

k

f  a     k

combining theorem    with corrolary    shows that

  fi f  i      u p k j k    i k p     
examples of each task suffice when learning p tasks using a linear threshold hypothesis space family 
while combining theorem    with theorem    shows that if

    i      u p k j k   k p      
then any learning algorithm will fail on some set of p tasks 
   conclusion
the problem of inductive bias is one that has broad significance in machine learning  in this paper
we have introduced a formal model of inductive bias learning that applies when the learner is able
to sample from multiple related tasks  we proved that provided certain covering numbers computed
from the set of all hypothesis spaces available to the bias learner are finite  any hypothesis space
that contains good solutions to sufficiently many training tasks is likely to contain good solutions to
novel tasks drawn from the same environment 
in the specific case of learning a set of features  we showed that the number of examples
j
u
required of each task in an  task training set obeys
  where j is the number of

p

  f  k

   

npq

 

fibaxter

features and u is a measure of the complexity of the feature class  we showed that this bound is
essentially tight for boolean feature maps constructed from linear threshold networks  in addition 
we proved that the number of tasks required to ensure good performance from the features on novel
tasks is no more than u   we also showed how a good set of features may be found by gradient
descent 
the model of this paper represents a first step towards a formal model of hierarchical approaches
to learning  by modelling a learners uncertainty concerning its environment in probabilistic terms 
we have shown how learning can occur simultaneously at both the base levellearn the tasks at
handand at the meta levellearn bias that can be transferred to novel tasks  from a technical
perspective  it is the assumption that tasks are distributed probabilstically that allows the performance guarantees to be proved  from a practical perspective  there are many problem domains that
can be viewed as probabilistically distributed sets of related tasks  for example  speech recognition
may be decomposed along many different axes  words  speakers  accents  etc  face recognition
represents a potentially infinite domain of related tasks  medical diagnosis and prognosis problems
using the same pathology tests are yet another example  all of these domains should benefit from
being tackled with a bias learning approach 
natural avenues for further enquiry include 

e

f  

a

alternative constructions for   although widely applicable  the specific example on feature
learning via gradient descent represents just one possible way of generating and searching
the hypothesis space family   it would be interesting to investigate alternative methods 
including decision tree approaches  approaches from inductive logic programming  khan
et al          and whether more general learning techniques such as boosting can be applied
in a bias learning setting 

e

a

a

algorithms for automatically determining the hypothesis space family   in our model the
structure of
is fixed apriori and represents the hyper bias of the bias learner  it would
be interesting to see to what extent this structure can also be learnt 

e

e

a

algorithms for automatically determining task relatedness  in ordinary learning there is usually little doubt whether an individual example belongs to the same learning task or not 
the analogous question in bias learning is whether an individual learning task belongs to a
given set of related tasks  which in contrast to ordinary learning  does not always have such
a clear cut answer  for most of the examples we have discussed here  such as speech and
face recognition  the task relatedness is not in question  but in other cases such as medical
problems it is not so clear  grouping too large a subset of tasks together as related tasks could
clearly have a detrimental impact on bias learning or multi task learning  and there is emprical evidence to support this  caruana         thus  algorithms for automatically determining
task relatedness are a potentially useful avenue for further research  in this context  see silver
and mercer         thrun and osullivan         note that the question of task relatedness
is clearly only meaningful relative to a particular hypothesis space family  for example  all
possible collections of tasks are related if contains every possible hypothesis space  

a

a

extended hierarchies  for an extension of our two level approach to arbitrarily deep hierarchies 
see langford         an interesting further question is to what extent the hierarchy can
be inferred from data  this is somewhat related to the question of automatic induction of
structure in graphical models 
   

fia m odel of i nductive b ias l earning

acknowledgements
this work was supported at various times by an australian postgraduate award  a shell australia postgraduate fellowship  u k engineering and physical sciences research council grants
k      and k       and an australian postdoctoral fellowship  along the way  many people
have contributed helpful comments and suggestions for improvement including martin anthony 
peter bartlett  rich caruana  john langford  stuart russell  john shawe taylor  sebastian thrun
and several anonymous referees 

appendix a  uniform convergence results
theorem   provides a bound  uniform over all    on the probability of large deviation between
  
 p
  and 
    to obtain a more general result  we follow haussler        and introduce the
following parameterized class of metrics on a  
m

 
  c


     e


  our main theorem will be a uniform bound on the probability of large values
     
  p

  e 
  
 

    theorem   will then follow as a corollary 
 
    rather than 
d
 e 
will better bounds for the realizable case  
 appendix a    
p

where

lemma     the following three properties of

   for

are easily established 

e

   for all 
   for all

of
as

n

l


p e 
 

    e     


 



e

 

   e  

  



and

    p    


 


 

for ease of exposition we have up until now been dealing explicitly with hypothesis spaces 
q 
   j 
containing functions 
  and then constructing loss functions q mapping
  
     
a  j 
by q
  however  in general we can view



 for some loss function
 
 j 
q just as a function from an abstract set   
  to
and ignore its particular construction

in terms of the loss function   so for the remainder of this section  unless otherwise stated  all
 j 
  it will also be considerably more
hypothesis spaces  will be sets of functions mapping  to
 



convenient to transpose our notation for c
  samples  writing the
training sets as columns
instead of rows 


              
      
  equation   and prior discussion  
where each 
fiff
  recalling the definition
of





with this transposition lives in
  the following definition now generalizes quantities


like
 
and so on to this new setting 

definition    let  
of functions mapping
into
  for any 
      let  be  orsetssimply
the map
 denote

    
   
 





 





   

 





 











































   







 j 





fibaxter

        let       denote the set of all such functions  given
            and elements
of
        or equivalently an element of
by writing the  
 as rows   define
 



    

 recall equation
   define      similarly  for any product probability measure            on
             

 recall equation        for any    
 not necessarily of the form        
define
                    
 to   define
 recall equation        for any class of functions mapping
                  
 and    is the
where the supremum is over all product probability measures on
 
size of the smallest    cover of under   recall definition    


for all

  
 























 
















  













 



 







 









 





























 j 















 



 



 









 j 













 













the following theorem is the main result from which the rest of the uniform convergence results
in this paper are derived 

ba   c    






edf  g
ji g i
       h  
   gqp
 r    g st vu   g w t      


be a permissible class of functions mapping 
theorem     let  
 into

 j 
  


 




  let 
be generated by
  independent trials from



 

according
to
some
product
probability
measure
 
for
all
 
 


k ml








     n     
o




p   
  

 



 











 

 





  



the following immediate corollary will also be of use later 

yx z u   g t h      r  bac gd fe g d ih

corollary     under the same conditions as theorem     if


 



then

k l





  


 

     j     
o












  

p   
  

 



 

 



    

gp g


    

a   proof of theorem   
the proof is via a double symmetrization argument of the kind given in chapter   of pollard        
i have also borrowed some ideas from the proof of theorem   in haussler        
   

fia m odel of i nductive b ias l earning

a     f irst s ymmetrization





       let


 

 

    
  
  
  
 
 
 k    




 


an extra piece of notation  for all
bottom half  viz 
 






d

d

and   be the

be the top half of

  l    
  
  
  
 
 
 l      















the following lemma is the first symmetrization trick  we relate the probability of large deviation
between an empirical estimate of the loss and the true loss to the probability of large deviation
between two independent empirical estimates of the loss 



mi g i



 

lemma     let  be a permissible set of functions from 


 j

 
   for all
probability measure on 
and

k l











     n     
o


po

gp
     n      rq   
o

cfn

into

  

p   
    
 



 


yd k l










p

  






   


 j 

and let

 

be a

  ts g d p 








    

q
rq      ts g d    uo s g
q      s g zd y
      w
q    uo s g and
q   uo s i g d y 

proof  note first that permissibility of  guarantees the measurability of suprema over 
p
    
    




 

 lemma    part     by the triangle inequality for
  if
and
    

 





  
  

 

  then
  thus 




  


 q  
k  v

vo ts i g  d
     xw
k v







 



 





 



q 



  

  







 

   












  

  




 

  









    
    





    



by chebyshevs
inequality  for any fixed   


k v






 


   

q      
  



o  i g d y
k l













 df  g

o





   
as
  and
 

gives the result 





d


 

o





 

 r



g

   
 r o




 




  
  


o

   


 

i gd p

 

  substituting this last expression into the right hand side of     

   

fibaxter

a     s econd s ymmetrization
the second symmetrization trick bounds the probability of large deviation between two empirical
estimates of the loss  i e  the right hand side of       by computing the probability of large deviation when elements are randomly permuted between the first and second sample  the following
definition introduces the appropriate permutation group for this purpose 

        d  d
 





 



               
     

     and any    h         let 
for any
f  l      
             
   l         
 into
   be a permissible set of  functions
lemma     let
mapping
   and let     w be
 as in the statement of theorem      fix 
st  cover for   where       
      
     
 where the  
 are the rows
an g
of   then 
k l   h       j   o    q            ts g d p
  k v   h       q      
     
 ts gr y     

  
where each   h       is chosen uniformly at random 
q            ts g d  if there is
proof  fix          and let 
be such that
g st   without loss
no such  for any   we are already done   choose 
such that





      now 
of generality we can assume  is of the form 
d

     ffu   ff 
fi ff  ff 
fiff  



 
 
      ffu   ff  
  ff    ff  
  ff   
  
     ffp    q ff  
  ff   ff  
  ff  s 
  
    ffu  q ff  
  ff   ff  
  ff  s
  
      ffp    q ff  
  ff   ff  
  ff  ts 
  
      ffu  q ff  
  ff   ff  
  ff  ts
q       s q         s 







definition    for all integers
  let
denote the set of all permutations of the
  
 
   
    
   
   
 
 
sequence of pairs of integers



 such that for all  



























 and

 or

 and

  either 

 
   
 



 











 j 
















 



 p







 

  


















 











   








q 



p



  





 










 
j





 

a

  




  
  

  







 




















   








cp




















 
  









 

   


















j





 



  
















j







 



 









  




 







 

 



 





















  










  
  













fia m odel of i nductive b ias l earning

d

p

hence  by the triangle inequality for

 

q        s
q         ts
q            s 
 q     
but
and
  q g    r   by construction
     s g  r   thus 
     implies
rq          ts
v   h       xw 
a v   h       xw 


  p
 




 
 

 

  



 



  





    
  
  
    



  


  

c   
  
c    
  


   
 

   


  


p   


 
 


   


    
  


 


  



    

  
 

 





q       s
q          ts
    
    ts g d by assumption  so
gd y
q          s gry

  

  




   












 







 

 

  












   










 





which gives      



      be any function that can be written in the form 
     
k mv   b        q          ts gr y yd vu   g t 
    
where each   h       is chosen uniformly at random 


proof  for any   f        
   q ff  
  ff   ff   
  ff  ts 





p
ff



q          ts 
 

    


ff
fi


ff
 
    ffu 

       let
to simplify the notation denote ff 
fiff by  
fiff   for each pair   
d and

lff be an independent random variable
fi


ff


ff


such that 
fiff
with probability



lff   
  ff  
fiff with probability d   from      
k mv   h       q          s gr y




l



     
fiffyl
k    h          q ff  
  ff   ff   
  ff  ts  gr
 
   ffp 


   ffp  


     
fiff   
k        
fiff  gr
 
   ffu  

   ffu  


  with bounded ranges  
 
  
   hofor zero mean independent random variables  
effdings inequality  devroye  gyorfi    lugosi        is


k    
   h yd vu    d  


    
 
   
  
  
now we bound the probability of each term in the right hand side of      

 



lemma     let
  for any














 







j









    
 

  












j





























y







   


 









 

  






 j 

 

q 



 









y

d













 







  












   








j

























a



























   



 









 







fibaxter


fiff is  fi
 ff  
   ff  
fiff  
   ff   we have
     
fiff   yd vu     g d     
     ffu    
fiffv  
k l      
fiff  gr
  
    ffp   
fiff  
   ff
 
   ffp  

   ffu  
  
let 
 
    ffp   
fiff   as  
fiff    
    ffu    
fiff   
fiff    hence 

d vu     g d  
  ffu  
   
lff  ffu 
   
lff ff    yd vu    g  d   

    
r   hence
giving a value of
   is minimized by setting 
k v   f       q          ts gr y yd vu    g t 


noting that the range of each












j











  












 







 





 j





e 

 



dj

 













j











 





e 












  























 j

j



 j



  
  



















a





as required 


   
   and    give 
k l   h       j   o    q            ts g d p
yd    g st
vu  w g t  
        and each   
 is the empirical distribution that
note that
is simply  where  




 d  recall definition     hence 
puts point mass
on each ff

     n      q          s g d p
k l   h      
o
yd    g st vu  w g t  
now  for a random choice of   each 
fiff in is independently  but not identically  distributed and  
 
only ever swaps 
fiff and 
   ff  so that   swaps a 
fiff drawn according to ff with another component
drawn according to the same distribution   thus we can integrate out with respect to the choice of
  and write
     j      q        s g d p
k l
o
yd    g st vu  w g t  
a     p utting


for fixed




it t ogether
 
 
  lemmas





 

  






   










 



 





 g

 

 













 





e



e





e



 



j





 



















 



 

  




   


























 

j















 



p

  






   








applying lemma    to this expression gives theorem    
   







 

j



fia m odel of i nductive b ias l earning

a   proof of theorem  

       

 

another piece of notation is required for the proof  for any hypothesis space 
 


measures
 on    let



 




 

    o o  

     





 








       
k   
 



and any probability



  

 is another empirical
note that we have used       rather than     to indicate that

  
estimate of
   
 
with the c
there is also generated a se  sampling process  in addition to the
 sample
 


 although these are not supplied to the learner 
quence of probability measures 






means
this notion is used
in the following lemma  where    

 n
the probability of generating a sequence of measures from the environment
 and then an
c  
  sample according to
such that a holds 

 

 

lemma     if



k l  


and







 

 


    f  n    





k l            
         
k l







    




      x 
 p

     
  

  

 

    

 




g d p dg







then







 





    p
   


 
 

proof  follows directly from the triangle inequality for

 

 

g d p dg







    



    

g p g


 

we treat the two inequalities in lemma    separately 

a     i nequality     





in the following lemma we replace the supremum over 
over    
lemma    


k ml  






 

 


in inequality      with a supremum

   f  j     
qg p


    f  n       
k    
  

















p   
  

 

 





   









p   ep
  

 



 



gh

    

fibaxter


 
  




      

p 





g
ji  

n

 

 e 
  
proof  suppose that   are such that
  let  satisfy this in
 

   
 
  
  

   by the definition of 
   for all
there
equality  suppose first that  


 
  
 e 
exists   
  hence by property     of the

 such that 



 
     
  e 


 
 
metric  for all
  there exists y
such that
  pick an arbitrary
   
   
   
  
 e 
satisfying this inequality  by definition  
 
   and so 



   
      
   

 
 by assumption   by the compatibility of
with the ordering on the
as

 

p    
   
 


p


 
reals 
  say  by the triangle inequality for  





 

u



g

 

 
  i

  

g g g

 



  g g
  

thus
can be
    g g g   and for any   an  satisfying this inequality
g


found  choosing
shows that there exists 
such that
 
i
if instead  
  then an identical argument can be run with the role of and  
interchanged  thus in both cases 
     
g w  

   g
p   p
  



p    
  

 
 








 



 

qc    p
  e 
p    
  

 
 











    p
  
 

  



 





 





     
  



 

  

 





 

qp    
  

 





 





 







which completes the proof of the lemma 
 
by the nature of the c
 sampling process 


             


    j       
    k  

  
 ay   where  

k    
 





 







 

p   
  

 





 

gh

 

 p  g h          
h  and  is permissible by the assumed
now
permissibility of  lemma     appendix d   hence
satisfies the conditions of corollary   
d for g and g d for g in corollary
and so combining lemma     equation      and substituting g






 









gq

      
  



































   gives the following lemma on the sample size required to ensure      holds 

 x z u l g  d f      t    g s g   g t p
   f  j     



lemma     if

then

k l  


  





 













  







p   
  
 

 

    
       








 o   
g
d
g
g
 



 x z u l g  d       t    g s g    g t p

a     i nequality     




 









 



g d p dg 


f  



   


note that     

 and
 

   i e the expectation of 

where is distributed according to   so to bound the left hand side of      we can apply corollary
  
   with 
  replaced by     replaced by    and replaced by
and
respectively 
replaced by and  replaced by   note that 
is permissible whenever  is  lemma     
thus  if

 





















  



   



g d

    

fia m odel of i nductive b ias l earning

then inequality      is satisfied 
now  putting together lemma     lemma    and equation     we have proved the following

more general version of theorem   

 i g g i
yx z u l g  d       t    g s g   g t p
 d t    g     t p
y
 
x
z
u
l
g
and
g      
g
then
    j     
k l
gp g

 m
to get theorem    observe that

d

d
 
   r
setting g
and maximizing g
gives
  substituting g
 p

 
  sample gentheorem     let  be a permissible hypothesis space family and let be an c
n










erated from the environment 
   for all
and
  if



  

   

 







  







p    
   
 


















 



 











 





















      
    
 

 
 



   








and



   ed d


   



 
into

theorem    gives theorem   

  
g 

a   the realizable case

  


 
s

g g
g 
g
g   g  
g
ji g g i   if
and
corollary     under the same conditions as theorem     for all  
x z u l g  d g         t   g g      g t g   p
t   g      t p
d

 
x
z
u
l
g
and
g g        
g g  
then
g
    n     
k l
 p g
g

d
these bounds are particularly useful if we know that
  for then we can set g
g   
 which maximizes g




 

in theorem   the sample complexity for both
and  scales as
   this can be improved to

   



 
 ep
 

 


if instead of requiring
  we require only that     
 











   



for some
  to see this  observe that     
 

 



     
    

  


 
 

  so setting
in theorem    and treating as a constant
gives 





  







   





   



  



  





  





 

  











       z       b and   
 as a composition of two function classes note that if for each 
   by
             

to write  
r 
 



of the form given in         can be written



r























recalling definition    for 






  


 p

 

 

appendix b  proof of theorem  



 






 p

 

  





 



 

















 





 



 

 







   





 









 







we define

fibaxter

   bt  i     z     thus  setting    b  and      

  
    
   
the following two lemmas will enable us to bound   

 




lemma     let
be of the form
where
  for all     
 
       
               

 

 
 
o



proof  fix a measure
on
and let  be a minimum size  cover for 
  by

 

 
 


 
 

 
 



definition
each b let
be the measure on
defined by

      for any set   in  for



 is  measurable  
the    algebra on
  is measurable so


 
f
o

      let

let 
for 
  by definition again  
 
 
   andsize  cover
    note

 be a minimum
         so the lemma
that
be

 will

 
 
o



can be shown to be an
 cover for
  so  given any  
choose
proved if
   h such that  o          and    v such that o          now 
o           o          o            
 o       o      
    
    line follows from
where the first line follows from the triangle inequality for o and the second





o
f
o

o
o





the facts 
 
   and    
     thus is an
      cover for     o and
so the result follows 
  definition     we have the following lemma 
recalling the definition of  
lemma    
     m          


 
   let      be
       on
proof  fix a product probability measure
 
 
   covers of  o   o   and let          given    
      choose       such that o  
  
   for each     now 





 
 
 
        
 
   
 
 
    
 
      

 

   o  
  

 

 and as    
    
 the result follows 
thus is an    cover for   


then
 









 j 

 

 j 





























 







 



  





l

 





















 



c








 



 







y











m











o









c

























c



e

 















 













 









 









m









 































  



















j

  

 

















 

































  







 







 









 

























e























   

































 









 





fia m odel of i nductive b ias l earning

b   bounding

     






j

  a          e               a    e
and from lemma    
                
     
 

using similar techniques to those used to prove lemmas    and     
satisfy
                 
from lemma    


































 















    







    


can be shown to

m



    

equations                  and      together imply inequality      

s   

 
    when is a hypothesis space
we wish to prove that    


x

 family of the form

 
f    note that each f  corresponds
to some     and that
         o    
 
i on   defined by
any probability measure on induces a probability measure
zz           
for any  in the    algebra on
  note also that if
  are bounded  positive functions on an

arbitrary set   then
                          
    






 




 






  let f 
let be any probability measure on the space of probability measures on





   then 
be two elements of  with corresponding hypothesis spaces 


            o          o       
           o     o         by      above 


    
               



         
          is guaranteed
   by the permissibility of  lemma    part    apthe measurability of

m

 


   we have   
pendix d   from
 f
    
     
  a      m    e
 

b   bounding
























m
 












   

























j







a





















a





a





  

















   





   





  



















   



 













  











 

  









 











which gives inequality      
   



 







 



   

  



 







 

















fibaxter

b   proof of theorem  
in order to prove the bounds in theorem   we have to apply theorem   to the neural network
hypothesis space family of equation       in this case the structure is

  


  g 
 
 g   g g   g 
 







       
  
 for some bounded
where 
subset  of
and some lipschitz squashing function     the feature class 

is the set of all one hidden layer neural networks with inputs  hidden nodes    outputs    as
the squashing function and weights 
 fiff where ff is a bounded subset of    the lipschitz
restriction on   and the bounded restrictions on the weights ensure that  and  are lipschitz
b
i 

for all  and
classes  hence there exists
             i
     and for all  w and   such that
y
i

       where   is the  norm
  
in each case  the loss function is squared loss 
  hence for all     and all probability measures

  onnow    recall that we assumed
the output space was
  
o            l    
   
   
yd                
    
  
  
 
      and
where
is the marginal distribution on
derived from   similarly  for all
 
probability measures on 
 


 o       yd              
    
define
  a      e      o     a        e

where the  supremum is over all probability measures on  the borel subsets of 
  and
a
 
 
 
 
e




is
the
size
of
the
smallest
 cover
of
under
the
metric 
similarly
set 
 
  a      e      o     a        e
where now the supremum is over all probability measures on    equations      and      imply
          d    
    
           
d   
    
 
applying theorem    from haussler         we find

d





 

  d       
   d         d      
w











 





  j 







 











 







m 

 

m







  
 


 j 






 

 



























 





 














 













 















 


























































 j 









m 





 j 

 



 

























 





























m























 









substituting these two expressions into      and      and applying theorem   yields theorem
  
   

fia m odel of i nductive b ias l earning

appendix c  proof of theorem   
this proof follows a similar argument to the one presented in anthony and bartlett        for
ordinary boolean function learning 
first we need a technical lemma 

d  d d  d    with

g for all
       
   d d d d k   


   
 
k                     g   r                 n  n   
 
   denote the number of occurences of in the random sequence           
proof  let


the function can be viewed as a decision rule  i e  based on the observations     tries to guess



d


d



d


d
whether the probability of
the bayes

dis  d if    or wd   and  the
    optimal
d rule isd otherwise 
    decision
estimator        
hence 
k       g d k     d  g d  d 


d k    i d  g d  d
kd     d  g  d  d 
 d



d


which is half the probability that a binomial
 random variable is at least wd   by
sluds inequality  slud        
k       g d k 

 
g

i  i







lemma     let be a random variable uniformly distributed on




 
  let
be i i d 
 valued random variables with g
  

 
 
  for any function mapping
 






















 

 



























 

















 





























 





 







 



 







 







 





























 





 

 







n





















j

   tates inequality  tate        states that for all
where  is normal 
 

k






  

d





 

   n 

 

 





    be shattered by   with    for each row  in   let  
 be the set of
let  
d
 
   
all  distributions on
such that
if is not contained in the
 

 


 

d

 and   
fiff
fi


ff





 th   row  ofd     and for each    f   
 

  let
 
 
 
 

    is achieved by any sequence







note that for  
  the optimal error   
     such that 
 
fiff if and only if   
 
fiff
  d    and
always contains such a sequence because shatters     the optimal error is then

               
  
            d   d 

  

   ffu 
combining the last two inequalities completes the proof 


 










 


  

c





 



 

 














c

  






 







c













 
 
 


 
  
 















c

  



 

  



c



 













 











g





 

   







 

 

c





 







fibaxter

and for any






     
p          








 

    
     
 
fiff    
 
fiff  
for any
 sample   let each element 
fiff in the array
        
  
  
 
  
 
 
        
equal the number of occurrences of 
fiff in  
        uniformly at random from    and generate an now  if we select  
a
    the output of the learning algorithm  we have 
sample using     then for 
    
 
fiff    
 
lff  c b          
 
fiff    
 
lff   
 b                 
fiff     
fiff 
lff

   ffu 
  is the probability of generating a configuration   of the 
fiff under the  sampling
where   
process and the sum is over all possible configurations  from lemma    
  
lff     
fiff 
fiff er df  g        h   nn ji
 
 

c  





  









 



 









 















c  







 















































 

   
 
fiff    
 fi
 ff 
 

k  


 

c





 























where



 











 j 

 j 

 



b  






















 

c

r  


  




 

 

  

 





 

 









 












  

  




 



         r

   ffp 
m  l  n   l
n


k



















y


h
          n n ij
 





    


      

 g

  







   

df

g

 

 k   k     

         g m





    


  plugging this into      shows
that

k  



c  

 valued random variable    g  


a









   
 
lff    
 fi
 ff   g 
 o  l  n   l
 
 

g nr  
  k   k    n  






by jensens inequality  since for any
implies 











c









and 







hence



































 



 











c



  

 g


fia m odel of i nductive b ias l earning

 

 



 

 

since the inequality holds over the random choice of   it must also hold for some specific choice

algorithm
there is some sequence of distributions such that
of   hence for any learning

k 

  



  




setting

assuming equality in       we get











g  




 g

  

e



    

           g 




 

g



and



  

  





e



k 



 g g

  

ensures

        g m



 









    

 g  



solving      for   and substituting the above expressions for g and  shows that      is satisfied
provided
    g               t g   d g
    
g
r   r since g i  r and g g      and assuming
setting 
g     for  somefor  some
yd         becomes
  d t  d 
    

        
r the right hand side of      is approximately maximized at 
subject to the constraint 
t qpsr     at which point the value  exceeds
cdf    d d     thus  for all     if   g

 r   and
 d d a  e
    
 
g







 





 









c





 

 



  
 

 





e 






 



j

 



c

 





















c









 

 

c





q

 

 

 





 













         g 
 
 contains at least two
g
to obtain the  dependence in theorem    observe that by assumption
 ut be two distributions
functions 
  hence there exists an
such that    
  let
 


d
 t
t
 

 
concentrated on
and
such that
and
     d   let     
t  b  and       h  be the product distributions on

v
 
     generated by  t and             note that   and 
are both in
  if   is one of  
and the learning algorithm
chooses the wrong hypothesis   
then
             
k 

then







 










  

  






 






  









 





 











 


























 

  

 






 

 








   









 

















 





fibaxter

 

 

     and generate an
  n
          nr         x  w w n  


now  if we choose uniformly at random from 
cording to   lemma    shows that

k  
g
which is at least if









  



  






 

























c





 





 sample

ac 





 r





 





ji g i   r   combining the two constraints on
x z u     finishes the proof 


c  



i     f      t g d g


provided





    


        with



 

p   and       and using


appendix d  measurability
in order for theorems   and    to hold in full generality we had to impose a constraint called
permissibility on the hypothesis space family    permissibility was introduced by pollard       
for ordinary hypothesis classes    his definition is very similar to dudleys image admissible
suslin  dudley         we will be extending this definition to cover hypothesis space families 
throughout this section we assume all functions  map from  the complete separable metric
 j 
  let   denote the borel  algebra of any topological space   as in section
space   into
     we view   the set of all probability measures on    as a topological space by equipping it
with the topology of weak convergence    is then the  algebra generated by this topology  the
following two definitions are taken  with minor modifications  from pollard        





y ff

 



y

 j 

ff

 

 valued functions on  is indexed by the set
definition    a set  of
r

 j 
such that


ff

definition    the set 
  

ff



 





 



zff

q







is permissible if it can be indexed by a set

ff

ff

ff

if there exists a function

such that

is an analytic subset of a polish  space   and


 


   the function
 algebra  

y

ff





 j 

indexing 

by

   y ff   
an analytic subset ff of a polish space ff


ff

is measurable with respect to the product


is simply the continuous image of a borel subset
of another polish space   the analytic subsets of a polish space include the borel sets  they
are important because projections of analytic sets are analytic  and can be measured in a complete
measure space whereas projections of borel sets are not necessarily borel  and hence cannot be
measured with a borel measure  for more details see dudley         section      
lemma     

   


  






 j 

is permissible if 

  






are all permissible 

proof  omitted 
we now define permissibility of hypothesis space families 
   a topological space is called polish if it is metrizable such that it is a complete separable metric space 

   

fia m odel of i nductive b ias l earning

 w







definition     a hypothesis space family 
is permissible if there exist sets





are analytic subsets of polish spaces and respectively  and a function


measurable with respect to

   such that

    y ff    y
ba  



fe hg



 ff

  e




ff

zff





ff

dc 



 and ff

that
 



 j 

 

 

let 
be an analytic subset of a polish space  let  
 be a measure space and

denote the analytic subsets of   the following three facts about analytic sets are taken from
pollard         appendix c 

fe hg

 
 a  if 

 
 b 







ff

is complete then

 



e

a


 

e  
  y  ff   
ff    the projection i   of  onto 

 

contains the product  algebra

 c  for any set



in

 

 

 

 

is in

 




 

 y 



y


recall definition   for the definition of    in the following lemma we assume that  
 
 n 
  is complete
has been completed with respect to any probability measure   and also that
with respect to the environmental measure  





lemma     for any permissible hypothesis space family   
     is permissible 


f is permissible 
  
is permissible for all
 

 
 
 
 
o and    o are measurable for all  
  
    is measurable for all
 
    is permissible 
 is simply the set of all
proof  as we have absorbed the loss function into the hypotheses  
  such that
 fold products
  thus     follows from lemma         and    
   g







 

 



 





















are immediate from the definitions  as  is permissible for all         can be proved by an
identical argument to that used in the measurable suprema section of pollard         appendix
c 


 j 


 j 
  the function 
defined
for      note that for any borel measurable  








 is borel measurable kechris        chapter      now  permissibility of
by   



  and 
so 
is measurable
 automatically implies permissibility of 

  
by     
r



 j 
in the appropriate way  to prove     
now let  be indexed by

 


 j 
 
 

 

e





  e 



   by fubinis theorem is a
define
by 



 j 
e






  measurable function  let
be defined by 

    e
indexes 
in the appropriate way for 
to be permissible  provided it can
  

  measurable  this is where analyticity becomes important  let
be shown that is  




    e    e
  by property  b  of analytic sets  
 


 contains

 
e



 
e








the set
is the projection of
onto
  which by property  c  is
n 

also analytic  as 

 is assumed complete 
is measurable  by property  a   thus is
a measurable function and the permissibility of 
follows 

 

kj  

 

 




y   m y ff   fiff y 
qp   

  on     y    r fiy  
g
 c  c  
 
     p
y

 

g

ff



w

 lj   
 





 

   

c

c

f    o f
 

 



ff




  



c

fibaxter

references
abu mostafa  y          a method for learning from hints  in hanson  s  j   cowan  j  d     giles 
c  l   eds    advances in neural information processing systems    pp       san mateo 
ca  morgan kaufmann 
anthony  m     bartlett  p  l          neural network learning  theoretical foundations  cambridge university press  cambridge  uk 
bartlett  p  l          lower bounds on the vc dimension of multi layer threshold networks  in
proccedings of the sixth acm conference on computational learning theory  pp       
new york  acm press  summary appeared in neural computation     no    
bartlett  p  l          the sample complexity of pattern classification with neural networks  the
size of the weights is more important than the size of the network  ieee transactions on
information theory                
baxter  j       a   learning internal representations  ph d  thesis  department of mathematics and statistics  the flinders university of south australia  copy available from
http   wwwsyseng anu edu au  jon papers thesis ps gz 

s

baxter  j       b   learning internal representations  in proceedings of the eighth international
conference on computational learning theory  pp          acm press  copy available
from http   wwwsyseng anu edu au  jon papers colt   ps gz 

s

baxter  j       a   a bayesian information theoretic model of learning to learn via multiple task
sampling  machine learning          
baxter  j       b   the canonical distortion measure for vector quantization and function approximation  in proceedings of the fourteenth international conference on machine learning 
pp        morgan kaufmann 
baxter  j     bartlett  p  l          the canonical distortion measure in feature space and   nn
classification  in advances in neural information processing systems     pp          mit
press 
berger  j  o          statistical decision theory and bayesian analysis  springer verlag  new
york 
blumer  a   ehrenfeucht  a   haussler  d     warmuth  m  k          learnability and the vapnikchervonenkis dimension  journal of the acm             
caruana  r          multitask learning  machine learning           
devroye  l   gyorfi  l     lugosi  g          a probabilistic theory of pattern recognition 
springer  new york 
dudley  r  m          a course on empirical processes  vol       of lecture notes in mathematics  pp        springer verlag 
dudley  r  m          real analysis and probability  wadsworth   brooks cole  california 
   

fia m odel of i nductive b ias l earning

gelman  a   carlin  j  b   stern  h  s     rubim  d  b   eds            bayesian data analysis 
chapman and hall 
good  i  j          some history of the hierarchical bayesian methodology  in bernardo  j  m  
groot  m  h  d   lindley  d  v     smith  a  f  m   eds    bayesian statistics ii  university
press  valencia 
haussler  d          decision theoretic generalizations of the pac model for neural net and other
learning applications  information and computation             
heskes  t          solving a huge number of similar tasks  a combination of multi task learning and
a hierarchical bayesian approach  in shavlik  j   ed    proceedings of the   th international
conference on machine learning  icml      pp          morgan kaufmann 
intrator  n     edelman  s          how to make a low dimensional representation suitable for
diverse tasks  connection science    
kechris  a  s          classical descriptive set theory  springer verlag  new york 
khan  k   muggleton  s     parson  r          repeat learning using predicate invention  in page 
c  d   ed    proceedings of the  th international workshop on inductive logic programming
 ilp      lnai       pp         springer verlag 
langford  j  c          staged learning  tech  rep   cmu  school of computer science 
http   www cs cmu edu  jcl research ltol staged latest ps 

s

mitchell  t  m          the need for biases in learning generalisations  in dietterich  t  g    
shavlik  j   eds    readings in machine learning  morgan kaufmann 
parthasarathy  k  r          probabiliity measures on metric spaces  academic press  london 
pollard  d          convergence of stochastic processes  springer verlag  new york 
pratt  l  y          discriminability based transfer between neural networks  in hanson  s  j  
cowan  j  d     giles  c  l   eds    advances in neural information processing systems   
pp          morgan kaufmann 
rendell  l   seshu  r     tcheng  d          layered concept learning and dynamically variable
bias management  in proceedings of the tenth international joint conference on artificial
intelligence  ijcai      pp          ijcai   inc 
ring  m  b          continual learning in reinforcement environments  r  oldenbourg verlag 
russell  s          the use of knowledge in analogy and induction  morgan kaufmann 
sauer  n          on the density of families of sets  journal of combinatorial theory a     
       
sharkey  n  e     sharkey  a  j  c          adaptive generalisation and the transfer of knowledge 
artificial intelligence review            
   

fibaxter

silver  d  l     mercer  r  e          the parallel transfer of task knowledge using dynamic
learning rates based on a measure of relatedness  connection science            
singh  s          transfer of learning by composing solutions of elemental sequential tasks  machine learning            
slud  e          distribution inequalities for the binomial law  annals of probability            
suddarth  s  c     holden  a  d  c          symolic neural systems and the use of hints in developing complex systems  international journal of man machine studies             
suddarth  s  c     kergosien  y  l          rule injection hints as a means of improving network performance and learning time  in proceedings of the eurasip workshop on neural
networks portugal  eurasip 
sutton  r          adapting bias by gradient descent  an incremental version of delta bar delta  in
proceedings of the tenth national conference on artificial intelligence  pp          mit
press 
tate  r  f          on a double inequality of the normal distribution  annals of mathematical
statistics             
thrun  s          is learning the n th thing any easier than learning the first   in advances in neural
information processing systems    pp          mit press 
thrun  s     mitchell  t  m          learning one more thing  in proceedings of the international
joint conference on artificial intelligence  pp            morgan kaufmann 
thrun  s     osullivan  j          discovering structure in multiple learning tasks  the tc algorithm  in saitta  l   ed    proceedings of the   th international conference on machine
learning  icml      pp          morgen kaufmann 
thrun  s     pratt  l   eds            learning to learn  kluwer academic 
thrun  s     schwartz  a          finding structure in reinforcement learning  in tesauro  g  
touretzky  d     leen  t   eds    advances in neural information processing systems  vol    
pp          mit press 
utgoff  p  e          shift of bias for inductive concept learning  in machine learning  an artificial
intelligence approach  pp          morgan kaufmann 
valiant  l  g          a theory of the learnable  comm  acm               
vapnik  v  n          estimation of dependences based on empirical data  springer verlag  new
york 
vapnik  v  n          the nature of statistical learning theory  springer verlag  new york 

   

fi