journal of artificial intelligence research                  

submitted       published      

parameter learning of logic programs for
symbolic statistical modeling

taisuke sato
yoshitaka kameya

sato mi cs titech ac jp
kame mi cs titech ac jp

dept  of computer science  graduate school of information
science and engineering  tokyo institute of technology
       ookayama meguro ku tokyo japan         

abstract

we propose a logical mathematical framework for statistical parameter learning of parameterized logic programs  i e  definite clause programs containing probabilistic facts with
a parameterized distribution  it extends the traditional least herbrand model semantics in
logic programming to distribution semantics   possible world semantics with a probability
distribution which is unconditionally applicable to arbitrary logic programs including ones
for hmms  pcfgs and bayesian networks 
we also propose a new em algorithm  the graphical em algorithm  that runs for a
class of parameterized logic programs representing sequential decision processes where each
decision is exclusive and independent  it runs on a new data structure called support graph s
describing the logical relationship between observations and their explanations  and learns
parameters by computing inside and outside probability generalized for logic programs 
the complexity analysis shows that when combined with oldt search for all explanations for observations  the graphical em algorithm  despite its generality  has the same
time complexity as existing em algorithms  i e  the baum welch algorithm for hmms  the
inside outside algorithm for pcfgs  and the one for singly connected bayesian networks
that have been developed independently in each research field  learning experiments with
pcfgs using two corpora of moderate size indicate that the graphical em algorithm can
significantly outperform the inside outside algorithm 
   introduction

parameter learning is common in various fields from neural networks to reinforcement learning to statistics  it is used to tune up systems for their best performance  be they classifiers
or statistical models  unlike these numerical systems described by mathematical formulas however  symbolic systems  typically programs  do not seem amenable to any kind of
parameter learning  actually there has been little literature on parameter learning of programs 
this paper is an attempt to incorporate parameter learning into computer programs 
the reason is twofold  theoretically we wish to add the ability of learning to computer
programs  which the authors believe is a necessary step toward building intelligent systems 
practically it broadens the class of probability distributions  beyond traditionally used numerical ones  which are available for modeling complex phenomena such as gene inheritance 
consumer behavior  natural language processing and so on 
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fisato   kameya

the type of learning we consider here is statistical parameter learning applied to logic
programs   we assume that facts  unit clauses  in a program are probabilistically true and
have a parameterized distribution   other clauses  non unit definite clauses  are always
true as they encode laws such as  if one has a pair of blood type genes a and b  one s
blood type is ab   we call logic programs of this type a parameterized logic program and
use for statistical modeling in which ground atoms  provable from the program represent
our observations such as  one s blood type is ab  and the parameters of the program are
inferred by performing ml  maximum likelihood  estimation on the observed atoms 
the probabilistic first order framework sketched above is termed statistical abduction
 sato   kameya        as it is an amalgamation of statistical inference and abduction
where probabilistic facts play the role of abducible s  i e  primitive hypotheses   statistical
abduction is powerful in that it not only subsumes diverse symbolic statistical frameworks
such as hmms  hidden markov models  rabiner         pcfgs  probabilistic context free
grammars  wetherell        manning   schutze        and  discrete  bayesian networks
 pearl        castillo  gutierrez    hadi        but gives us freedom of using arbitrarily
complex logic programs for modeling  
the semantic basis for statistical abduction is distribution semantics introduced by sato
        it defines a parameterized distribution  actually a probability measure  over the set
of possible truth assignments to ground atoms and enables us to derive a new em algorithm 
for ml estimation called the graphical em algorithm  kameya   sato        
parameter learning in statistical abduction is done in two phases  search and em learning  given a parameterized logic program and observations  the first phase searches for all
explanations for the observations  redundancy in the first phase is eliminated by tabulating
partial explanations using oldt search  tamaki   sato        warren        sagonas  t  
  warren        ramakrishnan  rao  sagonas  swift    warren        shen  yuan  you   
zhou         it returns a support graph which is a compact representation of the discovered
explanations  in the second phase  we run the graphical em algorithm on the support graph
   in this paper  logic programs mean definite clause programs  a definite clause program is a set of definite
clauses  a definite clause is a clause of the form a l            ln     n  where a  l            ln are atoms 
a is called the head  l            ln the body  all variables are universally quantified  it reads if l  and
      and ln hold  then a holds  in case of n      the clause is called a unit clause  a general clause is
one whose body may contain negated atoms  a program including general clauses is sometimes called a
general program  lloyd        doets        
   throughout this paper  for familiarity and readability  we will somewhat loosely use  distribution  as a
synonym for  probability measure  
   in logic programming  the adjective  ground  means no variables contained 
   abduction means inference to the best explanation for a set of observations  logically  it is formalized as
a search for an explanation e such that e  kb   g where g is an atom representing our observation  kb
a knowledge base and e a conjunction of atoms chosen from abducible s  i e  a class of formulas allowed
as primitive hypotheses  kakas  kowalski    toni        flach   kakas         e must be consistent
with kb 
   existing symbolic statistical modeling frameworks have restrictions and limitations of various types compared with arbitrary logic programs  see section   for details   for example  bayesian networks do not
allow recursion  hmms and pcfgs  stochastic grammars  allow recursion but lack variables and data
structures  recursive logic programs are allowed in ngo and haddawy s        framework but they
assume domains are finite and function symbols seem prohibited 
    em algorithm  stands for a class of iterative algorithms for ml estimation with incomplete data
 mclachlan   krishnan        
   

fiparameter learning of logic programs for symbolic statistical modeling

and learn the parameters of the distribution associated with the program  redundancy in
the second phase is removed by the introduction of inside and outside probability for logic
programs computed from the support graph 
the graphical em algorithm has accomplished  when combined with oldt search for
all explanations  the same time complexity as the specialized ones  e g  the baum welch
algorithm for hmms  rabiner        and the inside outside algorithm for pcfgs  baker 
       despite its generality  what is surprising is that  when we conducted learning experiments with pcfgs using real corpora  it outperformed the inside outside algorithm by
orders of magnitudes in terms of time for one iteration to update parameters  these experimental results enhance the prospect for symbolic statistical modeling by parameterized
logic programs of even more complex systems than stochastic grammars whose modeling
has been dicult simply because of the lack of an appropriate modeling tool and their sheer
complexities  the contributions of this paper therefore are
 distribution semantics for parameterized logic programs which unifies existing symbolicstatistical frameworks 
 the graphical em algorithm  combined with tabulated search   a general yet ecient
em algorithm that runs on support graphs and
 the prospect suggested by the learning experiments for modeling and learning complex
symbolic statistical phenomena 
the rest of this paper is organized as follows  after preliminaries in section    a probability space for parameterized logic programs is constructed in section   as a mathematical
basis for the subsequent sections  we then propose a new em algorithm  the graphical
em algorithm  for parameterized logic programs in section    complexity analysis of the
graphical em algorithm is presented in section   for hmms  pcfgs  pseudo pcsgs and
sc bns   section   contains experimental results of parameter learning with pcfgs by the
graphical em algorithm using real corpora that demonstrate the eciency of the graphical
em algorithm  we state related work in section    followed by conclusion in section    the
reader is assumed to be familiar with the basics of logic programming  lloyd        doets 
       probability theory  chow   teicher         bayesian networks  pearl        castillo
et al         and stochastic grammars  rabiner        manning   schutze        
   preliminaries

since our subject intersects logic programming and em learning which are quite different
in nature  we separate preliminaries 

    logic programming and oldt

in logic programming  a program db is a set of definite clauses  and the execution is search
for an sld refutation of a given goal g  the top down interpreter recursively selects the
   pseudo pcsgs  probabilistic context sensitive grammars  are a context sensitive extension of pcfgs
proposed by charniak and carroll         sc bn is a shorthand for a singly connected bayesian network
 pearl        
   we do not deal with general logic programs in this paper 
   

fisato   kameya

next goal and unfolds it  tamaki   sato        into subgoals using a nondeterministically
chosen clause  the computed result by the sld refutation  i e  a solution  is an answer
substitution  variable binding   such that db   g   usually there is more than one
refutation for g  and the search space for all refutations is described by an sld tree
which may be infinite depending on the program and the goal  lloyd        doets        
more often than not  applications require all solutions  in natural language processing
for instance  a parser must be able to find all possible parse trees for a given sentence as
every one of them is syntactically correct  similarly in statistical abduction  we need to
examine all explanations to determine the most likely one  all solutions are obtained by
searching the entire sld tree  and there is a choice of the search strategy  in prolog  the
standard logic programming language  backtracking is used to search for all solutions in
conjunction with a fixed search order for goals  textually from left to right  and clauses
 textually top to bottom  due to the ease and simplicity of implementation 
the problem with backtracking is that it forgets everything until up to the previous
choice point  and hence it is quite likely to prove the same goal again and again  resulting in
exponential search time  one answer to avoid this problem is to store computed results and
reuse them whenever necessary  oldt is such an instance of memoizing scheme  tamaki
  sato        warren        sagonas et al         ramakrishnan et al         shen et al  
       reuse of proved subgoals in oldt search often drastically reduces search time
for all solutions  especially when refutations of the top goal include many common subrefutations  take as an example a logic program coding an hmm  for a given string s 
there exist exponentially many transition paths that output s  oldt search applied to
the program however only takes time linear in the length of s to find all of them unlike
exponential time by prolog s backtracking search 
what does oldt have to do with statistical abduction  from the viewpoint of statistical abduction  reuse of proved subgoals  or equivalently  structure sharing of sub refutations
for the top goal g brings about structure sharing of explanations for g  in addition to the
reduction of search time mentioned above  thereby producing a highly compact representation of all explanations for g 

    em learning

parameterized distributions such as the multinomial distribution and the normal distribution provide convenient modeling devices in statistics  suppose a random sample x           xt
of size t on a random variable x drawn from a distribution p  x   x j   parameterized
by unknown   is observed  the value of  is determined by ml estimation as the mle
 maximum likelihood estimate  of   i e  as the maximizer of the likelihood  it p  xi j   
things get much more dicult when data are incomplete  think of a probabilistic
relationship between non observable cause x and observable effect y such as one between
diseases and symptoms in medicine and assume that y does not uniquely determine the
cause x   then y is incomplete in the sense that y does not carry enough information to
completely determine x   let p  x   x  y   y j   be a parameterized joint distribution
over x and y   our task is to perform ml estimation on  under the condition that x is
q

   by a solution we ambiguously mean both the answer substitution  itself and the proved atom g  as
one gives the other 
   

fiparameter learning of logic programs for symbolic statistical modeling

non observable while y is observable  let y            yt be a random sample of size t drawn
from the marginal distribution p  y   y j     x p  x   x  y   y j    the mle of  is
obtained by maximizing the likelihood  it p  yi j   as a function of  
while mathematical formulation looks alike in both cases  the latter  ml estimation with
incomplete data  is far more complicated and direct maximization is practically impossible
in many cases  people therefore looked to indirect approaches to tackle the problem of
ml estimation with incomplete data to which the em algorithm has been a standard
solution  dempster  laird    rubin        mclachlan   krishnan         it is an iterative
algorithm applicable to a wide class of parameterized distributions including the multinomial
distribution and the normal distribution such that the mle computation is replaced by the
iteration of two easier  more tractable steps  at n th iteration  it first calculates the value
of q function introduced below using current parameter value  n   e step     
p

q

q  j   n   def
 

x

x

p  x j y   n    ln p  x  y j    

   

next  it maximizes q  j  n   as a function of  and updates  n   m step  
 n      argmax q  j  n    
   
since the old value  n  and the updated value  n    do not necessarily coincide  the e steps
and m steps are iterated until convergence  during which the  log  likelihood is assured to
increase monotonically  mclachlan   krishnan        
although the em algorithm merely performs local maximization  it is used in a variety
of settings due to its simplicity and relatively good performance  one must notice however
that the em algorithm is just a class name  taking different form depending on distributions
and applications  the development of a concrete em algorithm such as the baum welch
algorithm for hmms  rabiner        and the inside outside algorithm for pcfgs  baker 
      requires individual effort for each case 
    q function is related to ml estimation as follows  we assume here only one data  y   is observed  from
jensen s inequality  chow   teicher        and the concavity of ln function  it follows that
x

p  x j y   n    ln p  x j y      

x

x

p  x j y   n    ln p  x j y    n      

x

and hence that
q  j  n      q  n  j  n   
x
x
 
p  x j y    n    ln p  x j y     
p  x j y    n    ln p  x j y    n      ln p  y j     ln p  y j  n   
x

 ln p  y j     ln p  y j  n    

x

consequently  we have
q  j  n     q  n  j  n      ln p y j    ln p y j  n      p y j    p y j  n    
   

fisato   kameya
   distribution semantics

in this section  we introduce parameterized logic programs and define their declarative semantics  the basic idea is as follows  we start with a set f of probabilistic facts  atoms 
and a set r of non unit definite clauses  sampling from f determines a set f   of true
atoms  and the least herbrand model of f     r determines the truth value of every atom
in db   f   r  hence every atom can be considered as a random variable  taking on  
 true  or    false   in what follows  we formalize this process and construct the underlying
probability space for the denotation of db 

    basic distribution pf

let db   f  r be a definite clause program in a first order language l with countably many
variables  function symbols and predicate symbols where f is a set of unit clauses  facts 
and r a set of non unit clauses  rules   in the sequel  unless otherwise stated  we consider
for simplicity db as the set of all ground instances of the clauses in db  and assume that
f and r consist of countably infinite ground clauses  the finite case is similarly treated  
we then construct a probability space for db in two steps  first we introduce a probability
space over the herbrand interpretations   of f i e  the truth assignments to ground atoms
in f   next we extend it to a probability space over the herbrand interpretations of all
ground atoms in l by using the least model semantics  lloyd        doets        
let a    a          be a fixed enumeration of atoms in f   we regard an infinite vector    
hx   x        i of  s and  s as a herbrand interpretation of f in such a way that for i              
ai is true  resp  false  if and only if xi      resp  xi       under this isomorphism  the set
of all possible herbrand interpretations of f coincides with the cartesian product 
 
def

f   f    gi 
y

i  

we construct a probability measure pf over the sample space 
f    from a collection of
finite joint distributions pf n  a    x           an   xn   n                 xi   f    g     i  n 
such that
   pf n  a    x            an   xn    
 n 
   
x      x pf  a    x            an   xn      
 
n   
 
n 
pf  a    x            an     xn       pf  a    x            an   xn   
x
the last equation is called the compatibility condition  it can be proved  chow   teicher 
      from the compatibility condition that there exists a probability space  
f   f   pf  
where pf is a probability measure on f   the minimal  algebra containing open sets of 
f  
such that for any n 
pf  a    x            an   xn     pf n   a    x            an   xn   
 
 
 
 
 
 
 

p

p

 

n

n  

    a herbrand interpretation interprets a function symbol uniquely as a function on ground terms and
assigns truth values to ground atoms  since the interpretation of function symbols is common to all
herbrand interpretations  given l  they have a one to one correspondence with truth assignments to
ground atoms in l  so we do not distinguish them 
    we regard 
f as a topological space with the product topology such that each f    g is equipped with
the discrete topology 
   

fiparameter learning of logic programs for symbolic statistical modeling

we call pf a basic distribution
   
 
n 
the choice of pf is free as long as the compatibility condition is met  if we want all
interpretations to be equiprobable  we should set pf n  a    x           an   xn       n for
every hx           xn i  the resulting pf is a uniform distribution over 
f just like the one
over the unit interval         if  on the other hand  we stipulate no interpretation except
     hc    c        i should be possible  we put  for each n 
pf n   a    x            an   xn        ifo w   i xi   ci     i  n 
then pf places all probability mass on    and gives probability   to the rest 
define a parameterized logic program as a definite clause program   db   f   r where
f is a set of unit clauses  r is a set of non unit clauses such that no clause head in r is
unifiable with a unit clause in f and a parameterized basic distribution pf is associated with
f   a parameterized pf is obtained from a collection of parameterized joint distributions
satisfying the compatibility condition  generally  the more complex pf n  s are  the more
exible pf is  but at the cost of tractability  the choice of parameterized finite distributions
made by sato        was simple 
pf  n   on     x   off     x            off  n   x n j             n 
n
  pbs  on  i     x i    off  i   x i j i 
i  
where
pbs  on  i     x i     off  i   x i j i  
  if x i     x i
 
i if x i        x i    
   
    i if x i        x i     
pbs  on  i     x i     off  i   x i j i       i  n  represents a probabilistic binary switch 
i e  a bernoulli trial  using two exclusive atoms on  i   and off  i in such a way that either
one of them is true on each trial but never both  i is a parameter specifying the probability
that the switch i is on  the resulting pf is a probability measure over the infinite product of
independent binary outcomes  it might look too simple but expressive enough for bayesian
networks  markov chains and hmms  sato        sato   kameya        
 

y

 
 
 
 
 

    extending pf to pdb

in this subsection  we extend pf to a probability measure pdb over the possible world s
for l  i e  the set of all possible truth assignments to ground atoms in l through the least
    this naming of pf   despite its being a probability measure  partly reects the observation that it behaves
like an infinite joint distribution pf  a    x    a    x           for an infinite random vector ha    a         i
of which pf n   a    x            an   xn    n                are marginal distributions  another reason is
intuitiveness  these considerations apply to pdb defined in the next subsection as well 
    here clauses are not necessarily ground 
   

fisato   kameya

herbrand model  lloyd        doets         before proceeding however  we need a couple
of notations  for an atom a  define ax by
ax   a if x    
ax    a if x     
next take a herbrand interpretation    
f of f   it makes some atoms in f true and
others false  let f be the set of atoms made true by    then imagine a definite clause
program db    r   f and its least herbrand model mdb   lloyd        doets        
mdb  is characterized as the least fixed point of a mapping tdb      below
is some a b            bk   db      k 
tdb   i   def
  a there
such that fb            bk g  i
where i is a set of ground atoms    or equivalently  it is inductively defined by
i     
in     tdb   in  
mdb   
in  
 

 

fi
fi
fi
fi
fi

 

 

n

taking into account the fact that mdb  is a function of    
f   we henceforth employ a
functional notation mdb     to denote mdb   
turning back  let a    a          be again an enumeration  but of all ground atoms in l   
form 
db   similarly to 
f   as the cartesian product of denumerably many f    g s and identify it with the set of all possible herbrand interpretations of the ground atoms a    a         
in l  i e  the possible world s for l  then extend pf to a probability
measure pdb over 
db
 
n 
as follows  introduce a series of finite joint distributions pdb  a    x            an   xn   for
n               by
 ax            axn  f def
  f   
f j mdb     j  ax           axn g
def
 n   a   x           a   x     p   ax          ax     
pdb
 
 
n
n
f
n f
 
 

n

 

 

n

n

 n   s satisfy the
note that the set  ax            axn  f is pf  measurable and by definition  pdb
compatibility condition
 n     a   x           a   x     p  n   a   x           a   x   
pdb
 
 
n  
n  
 
n
n
db  
 

n

x

xn  

hence there exists a probability measure pdb over 
db which is an extension of pf such
that
pdb  a    x            an   xn     pf  a    x            an   xn  
    i defines  mutually  a herbrand interpretation such that a ground atom a is true if and only if a   i  
a herbrand model of a program is a herbrand interpretation that makes every ground instance of every
clause in the program true 
    note that this enumeration enumerates ground atoms in f as well 
   

fiparameter learning of logic programs for symbolic statistical modeling

for any finite atoms a           an in f and for every binary vector hx            xni  xi   f    g    
i  n   define the denotation of the program db   f   r w r t  pf to be pdb   the denotational semantics of parameterized logic programs defined above is called distribution
semantics  as remarked before  we regard pdb as a kind of infinite joint distribution
pdb  a    x    a    x            mathematical properties of pdb are listed in appendix a
where our semantics is proved to be an extension of the standard least model semantics in
logic programming to possible world semantics with a probability measure 

    programs as distributions

distribution semantics views parameterized logic programs as expressing distributions  traditionally distributions have been expressed by using mathematical formulas but the use of
programs as  discrete  distributions gives us far more freedom and exibility than mathematical formulas in the construction of distributions because they have recursion and arbitrary composition  in particular a program can contain infinitely many random variables
as probabilistic atoms through recursion  and hence can describe stochastic processes that
potentially involve infinitely many random variables such as markov chains and derivations
in pcfgs  manning   schutze          
programs also enable us to procedurally express complicated constraints on distributions
such as  the sum of occurrences of alphabets a or b in an output string of an hmm must be
a multiple of three   this feature  procedural expression of arbitrarily complex  discrete 
distributions  seems quite helpful in symbolic statistical modeling 
finally  providing mathematically sound semantics for parameterized logic programs
is one thing  and implementing distribution semantics in a tractable way is another  in
the next section  we investigate conditions on parameterized logic programs which make
probability computation tractable  thereby making them usable as a means for large scale
symbolic statistical modeling 
   graphical em algorithm

according to the preceding section  a parameterized logic program db   f   r in a
first order language l with a parameterized basic distribution pf    j   over the herbrand
interpretations of ground atoms in f specifies a parameterized distribution pdb    j   over
the herbrand interpretations for l  in this section  we develop  step by step  an ecient em
algorithm for the parameter learning of parameterized logic programs by interpreting pdb
as a distribution over the observable and non observable events  the new em algorithm is
termed the graphical em algorithm  it is applicable to arbitrary logic programs satisfying
certain conditions described later provided the basic distribution is a direct product of
multi ary random switches  which is a slight complication of the binary ones introduced in
section     
from this section on  we assume that db consists of usual definite clauses containing
 universally quantified  variables  definitions and changes relating to this assumption are
    an infinite derivation can occur in pcfgs  take a simple pcfg fp   s   a  q   s   ss g where s is a
start symbol  a a terminal symbol  p   q     and p  q      in this pcfg  s is rewritten either to a with
probability p or to ss with probability q   the probability of the occurrence of an infinite derivation is
calculated as max f        p q g which is non zero when q   p  chi   geman        
   

fisato   kameya

listed below  for a predicate p  we introduce iff  p   the iff definition of p by
iff p  def
   x  p x     y  x   t    w              yn  x   tn   wn     
here x is a vector of new variables of length equal to the arity of p  p ti  wi     i 
n     n   an enumeration of clauses about p in db  and yi   a vector of variables occurring
in p ti  wi  then define comp r  as follows 
head r  def
  fb j b is a ground instance of a clause head appearing in rg
iff  r  def
  fiff  p  j p appears in a clause head in rg
eq def
  ff  x    f  y    x   y j f is a function symbolg
  ff  x     g y  j f and g are different function symbolsg
  ft    x j t is a term properly containing xg
comp r  def
  iff  r    eq
eq   clark s equational theory  clark         deductively simulates unification  likewise
comp r  is a first order theory which deductively simulates sld refutation with the help
of eq by replacing a clause head atom with the clause body  lloyd        doets        
we here introduce some definitions which will be frequently used  let b be an atom 
an explanation for b w r t  db   f   r is a conjunction s such that s  r   b  and as a
set comprised of its conjuncts  s  f holds and no proper subset of s satisfies this  the
set of all explanations for b is called the support set for b and designated by db  b    

    motivating example

first of all  we review distribution semantics by a concrete example  consider the following
program dbb   fb   rb in figure   modeling how one s blood type is determined by blood
type genes probabilistically inherited from the parents   
the first four clauses in rb state a blood type is determined by a genotype  i e  a pair of
blood type genes a  b and o  for instance  btype  a      gtype a a    gtype a o   
gtype o a   says that one s blood type is a if his  her  genotype is ha  ai  ha  oi or ho  ai 
these are propositional rules 
succeeding clauses state general rules in terms of logical variables  the fifth clause
says that regardless of the values of x and y  event gtype x y   one s having genotype
hx  yi  is caused by two events  gene father x   inheriting gene x from the father  and
gene mother y   inheriting gene y from the mother   gene p g    msw gene p g  is a
clause connecting rules in rb with probabilistic facts in fb  it tells us that the gene g
is inherited from a parent p if a choice represented by msw gene p g    is made  the

    this definition of a support set differs from the one used by sato        and kameya and sato        
    when we implicitly emphasize the procedural reading of logic programs  prolog conventions are employed
 sterling   shapiro         thus    stands for  or      and      implied by  respectively  strings
beginning with a capital letter are  universally quantified  variables  but quoted ones such as  a  are
constants  the underscore is an anonymous variable 
    msw is an abbreviation of  multi ary random switch  and msw          expresses a probabilistic choice from
finite alternatives  in the framework of statistical abduction  msw atoms are abducibles from which
explanations are constructed as a conjunction 
   

fiparameter learning of logic programs for symbolic statistical modeling

 
 
 
 
 
 
 
 
 

btype  a  
btype  b  
btype  o  
btype  ab  
gtype x y 
gene p g 

       

 gtype a a    gtype a o    gtype o a   
 gtype b b    gtype b o    gtype o b   
gtype o o  
 gtype a b    gtype b a   
gene father x   gene mother y  
msw gene p g  

rb

 

fb

  fmsw gene father a   msw gene father b   msw gene father o  

 
 
 
 
 
 
 
 

msw gene mother a   msw gene mother b   msw gene mother o g

figure    abo blood type program dbb
genetic knowledge that the choice of g is by chance and made from fa  b  og is expressed by
specifying a joint distribution fb as follows 
pf  msw gene t a    x  msw gene t b    y  msw gene t o    z j a   b   o   def
  axby oz
where x  y  z   f    g  x   y   z      a  b  o           a   b   o     and t is either
father or mother  thus a is the probability of inheriting gene a from a parent  statistical
independence of the choice of gene  once from father and once from mother  is expressed
by putting
pf   msw gene father a    x  msw gene father b    y  msw gene father o    z 
msw gene mother a    x   msw gene mother b    y    msw gene mother o    z  
j a   b   o  
  pf  x  y  z j a  b  o pf  x   y    z  j a  b   o  
in this setting  atoms representing our observation are obs dbb     fbtype  a    btype  b   
btype  o    btype  ab  g  we observe one of them  say btype  a    and infer a possible
explanation s   i e  a minimal conjunction of abducibles msw gene      such that
s  rb   btype  a   
s is obtained by applying a special sld refutation procedure to the goal btype  a  
which preserves msw atoms resolved upon in the refutation  three explanations are found 
s    msw gene father a    msw gene mother a 
s    msw gene father a    msw gene mother o 
s    msw gene father o    msw gene mother a 
so db  btype a    the support set for btype a   is fs    s    s g  the probability of each
explanation is respectively computed as pf  s     a  and pf  s      pf  s     ao  from
proposition a   in appendix a  it follows that pdb  btype  a      pdb  s    s    s    
pf  s    s    s    and that
pdb  btype  a   j a   b   o     pf  s      pf  s      pf  s   
  a     ao 
b

b

b

b

b

b

b

b

b

b

b

b

b

   

b

b

fisato   kameya

here we used the fact that s   s  and s  are mutually exclusive as the choice of gene is
exclusive  parameters  i e  a  b and o are determined by ml estimation performed on a
random sample such as fbtype  a    btype  o    btype  ab  g of btype as follows 
ha   b   oi   argmaxh     i pdb  btype  a   pdb  btype  o   pdb  btype  ab   
  argmaxh     i  a     ao o  ab
this program contains neither function symbol nor recursion though our semantics
allows for them  later we see an example containing both  a program for an hmm  rabiner
  juang        
a

b o

a

b o

b

b

b

    four simplifying conditions

in figure   is simple and probability computation is easy  this is not generally the
case  since our primary interest is learning  especially ecient parameter learning of parameterized logic programs  we hereafter concentrate on identifying what property of a program
makes probability computation easy like dbb  thereby makes ecient parameter learning
possible 
to answer this question precisely  let us formulate the whole modeling process  suppose
there exist symbolic statistical phenomena such as gene inheritance for which we hope
to construct a probabilistic computational model  we first specify a target predicate p
whose ground atom p s  represents our observation of the phenomena  then to explain
the empirical distribution of p  we write down a parameterized logic program db   f   r
having a basic distribution pf with parameter  that can reproduce all observable patterns
of p s   finally  observing a random sample p s            p st   of ground atoms of p  we
adjust  by ml estimation  i e  by maximizing the likelihood l     tt   pdb  p st  j   so
that pdb  p    j   approximates as closely to the empirically observed distribution of p as
possible 
at first sight  this formulation looks right  but in reality it is not  suppose two events
p s  and p s     s    s   are observed  we put l     pdb  p s  j   pdb  p s   j    but this
cannot be a likelihood at all simply because in distribution semantics  p s  and p s    are
two different random variables  not two realizations of the same random variable 
a quick remedy is to note that in the case of blood type program dbb where obs dbb   
fbtype  a    btype  b    btype  o    btype  ab  g are observable atoms  only one of
them is true for each observation  and if some atom is true  others must be false  in other
words  these atoms collectively behave as a single random variable having the distribution
pdb whose values are obs dbb   
keeping this in mind  we introduce the following condition  let obs db    head r  
be a set of ground atoms which represent observable events  we call them observable atom s 
dbb

q

b

uniqueness condition 
 
pdb  g   g      

for any g    g    obs db   and

   

p

g obs db  pdb

 g      

fiparameter learning of logic programs for symbolic statistical modeling

the uniqueness condition enables us to introduce a new random variable yo representing
our observation  fix an enumeration g    g          of observable atoms in obs db  and define
yo by  
y o        k

iff   j  gk for     
db  k     
   
let gk t  gk           gk   obs db  be a random sample of size t   then l     tt   pdb  gk j
    t   pdb  yo   kt j   qualifies for the likelihood function w r t  yo  
the second condition concerns the reduction of probability computation to addition 
take again the blood type exmaple  the computation of pdb  btype  a    is decomposed
into a summation because explanations in the support set are mutualy exclusive  so we
introduce
q

q 

 

t

t

b

exclusiveness condition 

for every g   obs db  and the support set db  g   pdb  s   s        for any s   
s     db  g  
using the exclusiveness condition  and proposition a   in appendix a   we have
pdb  g   
pf  s   
x

s 

db

 g 

from a modeling point of view  it means that while a single event  or a single observation 
g  may have several  or even infinite  explanations db  g   only one of db  g  is allowed
to be true for each observation 
now introduce  db   i e  the set of all explanations relevant to obs db  by
 db def
 
db  g 
 

g obs db 

and fix an enumeration s   s          of explanations in  db   it follows from proposition a   
the uniqueness condition and the exclusiveness condition that
pdb  si   sj       for i    j
and
pdb  s    
pdb  s  
x

s   db

x

x

g obs db  s  

 g 
pdb  g 
db

 
g obs db 
    
so we are able to introduce under the uniqueness condition and the exclusiveness condition
yet another random variable xe  representing an explanation for g  defined by
xe       k iff   j  sk for     
db  
   
the third condition concerns termination 

   

x

g obs db  pdb  g      only guarantees that the measure of f   j   j  gk for some k     g is one  so
there can be some   satisfying no gk  s  in such case  we put yo          but values on a set of measure
zero do not affect any part of the discussion that follows  this also applies to the definition of xe in     
p

   

fisato   kameya

finite support condition 

for every g   obs db  db  g  is finite 
pdb  g  is then computed from the support set db  g    fs            sm g     m   with
the help of the exclusiveness condition  as a finite summation mi   pf  si   this condition
prevents an infinite summation that is hardly computable 
the fourth condition simplifies the probability computation to multiplication  recall
that an explanation s for g   obs db  is a conjunction a           am of some abducibles
fa           amg  f     m   in order to reduce the computation of pf  s     pf  a         am 
to the multiplication pf  a        pf  am    we assume
p

distribution condition 

f is a set fmsw of ground atoms with a parameterized distribution pmsw specified below 

here atom msw i n v  is intended to simulate a multi ary random switch whose name is i
and whose outcome is v on trial n  it is a generalization of primitive probabilistic events
such as coin tossing and dice rolling 
   fmsw consists of probabilistic atoms msw i n v   the arguments i  n and v are ground
terms called switch name  trial id and a value  of the switch i   respectively  we
assume that a finite set vi of ground terms called the value set of i is associated with
each i  and v   vi holds 
   write vi as fv    v            vmg  m   jvi j   then  one of the ground atoms f msw i n v   
msw i n v            msw i n vm  g becomes exclusively true  takes on value    on each
trial  with each i  a parameter i v          such that v v i v     is associated  i v
is the probability of msw i   v  being true  v   vi  
   for each ground terms i  i    n  n   v   vi and v    vi    random variable msw i n v  is
independent of msw i   n   v    if n    n  or i    i   
in other words  we introduce a family of parameterized finite distributions p i n  such that
p i n  msw i n v      x            msw i n vm     xm j i v           i v  
x
x
if mk   xk    
def
   i v      i v o w 
   
p

i

 

 

 

 

m
m

p

m

where m   jvij  xk   f    g     k  m   and define pmsw as their infinite product
pmsw def
  p i n   
y

i n

under this condition  we can compute pmsw s    the probability of an explanation s  as the
product of parameters  suppose msw ij  n v  and msw ij   n  v   are different conjuncts in
an explanation s   msw i   n  v            msw ik  nk  vk    if either j    j   or n    n  holds 
they are independent by construction  else if j   j   and n   n  but v    v    they are not
independent but pmsw s       by construction  as a result  whichever condition may hold 
pmsw  s   is computed from the parameters 
   

fiparameter learning of logic programs for symbolic statistical modeling

    modeling principle

up to this point  we have introduced four conditions  the uniqueness condition  the exclusiveness condition  the finite support condition and the distribution condition  to simplify
probability computation  the last one is easy to satisfy  we just adopt fmsw together with
pmsw   so  from here on  we always assume that fmsw has a parameterized distribution pmsw
introduced in the previous subsection  unfortunately the rest are not satisfied automatically  according to our modeling experiences however  it is only mildly dicult to satisfy
the uniqueness condition and the exclusiveness condition as long as we obey the following
modeling principle 
modeling principle  db   fmsw   r describes a sequential decision process
 modulo auxiliary computations  that uniquely produces an observable atom
g   obs db  where each decision is expressed by some msw atom   
translated into programming level  it says that we must take care when writing a program so that for any sample f   from pmsw  there must uniquely exist goal g  g   obs db  
which has a successful refutation from db    f     r  we can confirm the principle by the
blood type program dbb   fb   rb  it describes a process of gene inheritance  and for
an arbitrary sample fb  from pmsw  say fb    fmsw gene father a   msw gene mother o g 
there exists a unique goal  btype  a   in this case  that has a successful sld refutation
from fb    rb 
the idea behind this principle is that a decision process always produces some result  an
observable atom   and different decision processes must differ at some msw thereby entailing
mutually exclusive observable atoms  so the uniqueness condition and the exclusiveness
condition will be automatically satisfied 
satisfying the finite support condition is more dicult as it is virtually equivalent to
writing a program db for which all solution search for g  g   obs db   always terminates  apparently we have no general solution to this problem  but as far as specific models
such as hmms  pcfgs and bayesian networks are concerned  it can be met  all programs
for these models satisfy the finite support condition  and other conditions as well  

    four conditions revisited

in this subsection  we discuss how to relax the four simplifying conditions introduced in subsection     for the purpose of exible modeling  we first examine the uniqueness condition
considering its crucial role in the adaptation of the em algorithm to our semantics 
the uniqueness condition guarantees that there exists a  many to one  mapping from
explanations to observations so that the em algorithm is applicable  dempster et al         
it is possible  however  to relax the uniqueness condition while justifying the application
of the em algorithm  we assume the mar  missing at random  condition introduced by
rubin        which is a statistical condition on how a complete data  explanation  becomes an incomplete data  observation   and is customarily assumed implicitly or explicitly
in statistics  see appendix b   by assuming the mar condition  we can apply our em
    decisions made in the process are a finite subset of fmsw  
   

fisato   kameya

algorithm to non exclusive observations o such that o p  o     where the uniqueness
condition is seemingly destroyed 
let us see the mar condition in action with a simple example  imagine we walk along
a road in front of a lawn  we occasionally observe their state such as  the road is dry but
the lawn is wet   assume that the lawn is watered by a sprinkler running probabilistically 
the program dbrl   rrl   frl in figure   describes a sequential process which outputs
an observation observed road x  lawn y     the road is x and the lawn is y   where
x  y   fwet  dryg 
rrl     observed road x  lawn y   p

frl

 

msw rain once a  
  a   yes  x   wet  y   wet
  a   no  msw sprinkler once b  
  b   on  x   dry  y   wet
  b   off  x   dry  y   dry       
  msw rain once yes   msw rain once no  
msw sprinkler once on   msw sprinkler once off   

figure    dbrl
the basic distribution over frl is specified like pf     in subsection      so we omit it 
msw rain once a  in the program determines whether it rains  a   yes  or not  a   no  
whereas msw sprinkler once b  determines whether the sprinkler works fine  b   on 
or not  b   off   since for each sampled values of a   a  a   fyes  nog  and b   b
 b   fon  offg   there uniquely exists an observation observed road x  lawn y    x  y  
fwet  dryg   there is a many to one mapping     a  b    hx  yi  in other words  we
can apply the em algorithm to the observations observed road x  lawn y    x  y  
fwet  dryg   what would happen if we observe exclusively either a state of the road or
that of the lawn  logically  this means we observe  y observed road x  lawn y   or
 x observed road x  lawn y    apparently the uniqueness condition is not met  because
 y observed road wet  lawn y   and  x observed road x  lawn wet   are compatible
 they are true when it rains   despite the non exclusiveness of the observations  we can still
apply the em algorithm to them under the mar condition  which in this case translates
into that we observe either the lawn or the road randomly regardless of their state 
we now briey check other conditions  basically they can be relaxed at the cost of
increased computation  without the exclusiveness condition for instance  we would need an
additional process of transforming the support set db  g  for a goal g into a set of exclusive
explanations  for instance  if g has explanations fmsw a n v   msw b m w g  we have to
transform it into fmsw a n v    msw a n v    msw b m w g and so on    clearly  this
transformation is exponential in the number of msw atoms and eciency concern leads to
assuming the exclusiveness condition 
the finite support condition is in practice equivalent to the condition that the sld tree
for g is finite  so relaxing this condition might induce infinite computation 
b

     msw a n v   is further transformed to a disjunction of exclusive msw atoms like
   

w

 

 
  msw a n v   

v    v v   va

fiparameter learning of logic programs for symbolic statistical modeling

relaxing the distribution condition and accepting probability distributions other than
serve to expand the horizon of the applicability of parameterized logic programs  in
particular the introduction of parameterized joint distributions p  v           vk   like boltzmann distributions over switches msw            mswk where v           vk are values of the switches 
makes them correlated  such distributions facilitate writing parameterized logic programs
for complicated decision processes in which decisions are not independent but interdependent  obviously  on the other hand  they increase learning time  and whether the added
exibility of distributions deserves the increased learning time or not is yet to be seen 
pmsw

    naive approach to em learning

in this subsection  we derive a concrete em algorithm for parameterized logic programs
db   fmsw   r assuming that they satisfy the uniqueness condition  the exclusiveness
condition and the finite support condition 
to start  we introduce yo  a random variable representing our observations according
to     based on a fixed enumeration of observable atoms in obs db   we also introduce
another random variable xe representing their explanations according to     based on some
fixed enumeration of explanations in  db   our understanding is that xe is non observable
while yo is observable  and they have a joint distribution pdb  xe   x  yo   y j   where
 denotes relevant parameters  it is then immediate  following     and     in section    to
derive a concrete em algorithm from the q function defined by q  j     def
  x pdb  x j
y      ln pdb  x  y j   whose input is a random sample of observable atoms and whose output
is the mle of  
in the following  for the sake of readability  we substitute an observable atom g  g  
obs db   for yo   y and write pdb  g j   instead of pdb  yo   y j    likewise we
substitute an explanation s  s    db   for xe   x and write pdb  s  g j   instead of
pdb  xe   x  yo   y j    then it follows from the uniqueness condition that
 
if s    db  g 
pdb  s  g j    
pmsw  s j   if s   db  g  
we need yet another notation here  for an explanation s  define the count of msw i n v 
in s by
i v  s   def
  jf n j msw i n v    s gj  
we have done all preparations now  suppose we make some observations g   g            gt
where gt   obs db      t  t    put
i def
  fi j msw i n v    s   db  gt      t  t g
 def
  fi v j msw i n v    s   db  gt      t  t g 
i is a set of switch names that appear in some explanation for one of the gt  s and  denotes
parameters associated with these switches   is finite due to the finite support condition 
p

 

   

fisato   kameya

various probabilities and the q function are computed by using proposition a   in
appendix a together with our assumptions as follows 
pdb  gt j     pdb
 
pmsw  s j   
   
db  gt   
fi
fi
fi

 

pmsw  s j   

 

q  j     def
 

 
where
  i  v     def
 

y

t   s  db
i i v vi
t
x
t  

x

s 

i v  s  
i v

i i v vi
t
x
x

x



db

 gt  

pdb  s j gt       ln pdb  s  gt j  

  i  v     ln i v 

 

pdb  gt j   s  

x

i i v vi

x

db

 gt  

 
 i  v     
 
 i  v     ln p
   
v   vi   i  v     

   

pmsw  s j  i v  s  

here we used jensen s inequality to obtain      note that pdb  gt j     s   g  
pmsw  s j   i v  s   is the expected count of msw i   v  in an sld refutation of gt  speaking
of the likelihood function l     tt   pdb  gt j    it is already shown in subsection    
 footnote  that q  j      q   j     implies l    l      hence from      we reach
the procedure learn naive   g  below that finds the mle of the parameters  the array
variable  i  v  stores  i  v    under the current  
p

db

t

q

db

  
  
  
  
  
  

procedure

learn naive db  g  

begin

initialize t with appropriate values and   with a small positive number  
       t   ln pdb  gt j    
  compute the log likelihood 
p

repeat
foreach

i   i  v   vi do

 i  v     

t
x

 

pdb  gt j    s  
foreach i   i  v   vi do
 i  v 
i v    p
  
 
v  vi   i  v  
m    m  p  
 m     tt   ln pdb  gt j  
until  m     m       

  
  
  
   
   
    end

t  

x

db

 gt  

pmsw  s j   i v  s   

  update the parameters 
  compute the log likelihood again 
  terminate if converged 

this em algorithm is simple and correctly calculates the mle of   but the calculation of pdb  gt j   and  i  v  line      and     may suffer a combinatorial explosion of
explanations  that is  j db  gt j often grows exponentially in the complexity of the model 
for instance  j db  gt j for an hmm with n states is o n l   exponential in the length l
of an input output string  nonetheless  suppressing the explosion to realize ecient computation in a polynomial order is possible  under suitable conditions  by avoiding multiple
computations of the same subgoal as we see next 
   

fiparameter learning of logic programs for symbolic statistical modeling

    inside probability and outside probability for logic programs

in this subsection  we generalize the notion of inside probability and outside probability
 baker        lari   young        to logic programs  major computations in learn naive   g 
are those of two terms in line    pdb  gt j   and s   g   pmsw s j  i v  s   computational redundancy lurks in the naive computation of both terms  we show it by an example 
suppose there is a propositional program dbp   fp   rp where fp   fa  b  c  d  mg and
db

p

db

 
 
 
 
 
 
 

rp    
 
 
 
 
 

f
f
g
g
h

t

a g
b g
c
d h
m 

    

here f is an observable atom  we assume that a  b  c  d and m are independent and also
that fa  bg and fc  dg are pair wise exclusive  then the support set for f is calculated as
db  f    fa   c  a   d   m  b   c  b   d   m g 
hence  in light of      we may compute pdb  f  as
pdb  f    pf  a   c    pf  a   d   m    pf  b   c    pf  b   d   m  
    
this computation requires   multiplications  because pf  a   c    pf  a pf  c  etc   and
  additions  on the other hand  it is possible to compute pdb  f  much more eciently by
factoring out common computations  let a be a ground atom  define the inside probability
fi  a  of a as
fi  a  def
  pdb  a j     
    
then by applying theorem a   in appendix a to
comp rp    f    a   g     b   g   g   c    d   h   h   m
    
which unconditionally holds in our semantics  and by using the independent and the exclusiveness assumption made on fp  the following equations about inside probability are
derived 
fi  f    fi  a fi  g    fi  b fi  g 
fi  g    fi  c    fi  d fi  h 
    
fi  h    fi  m 
pdb  f    fi  f   is obtained by solving      about fi  f   for which only   multiplications
and   additions are required 
it is quite straightforward to generalize      but before proceeding  look at a program
dbq   fmg   fg  m   m  g  mg where g is an observable atom and m the only msw atom 
we have g    m   m    m in our semantics  but to compute p  g    p  m p  m    p  m  is
clearly wrong as it ignores the fact that clause bodies for g  i e  m m and m are not mutually
exclusive  and atoms in the clause body m m are not independent  here p       pdb      
similarly  if we set a   b   c   d   m  the equation      will be totally incorrect 
p

p

p

p

p

p

p

p

p

p

p

 
 
 
 
 

p

q

    note that if a is a fact in f   fi  a    pmsw  a j   

   

fisato   kameya

we therefore add  temporarily in this subsection  two assumptions on top of the exclusiveness condition and the finite support condition so that equations like      become
mathematically correct  the first assumption is that  clause  bodies are mutually exclusive i e  if there are two clauses b w and b w     pdb  w   w   j        and the
second assumption is that body atoms are independent  i e  if a b            bk is a rule 
pdb  b           bk j     pdb  b  j        pdb  bk j   holds 
please note that  clause  used in this subsection has a special meaning  it is intended to
mean g  where g is a goal and  is a tabled explanation for g obtained by oldt search
both of which will be explained in the next subsection    in other words  these additional
conditions are not imposed on a source program but on the result of oldt search  so
clauses for auxiliary computations do not need to satisfy them 
now suppose clauses about a occur in db like
a
a

b              b  i 

    

bl            bl il

where bh j     h  l     j  ih  is an atom  theorem a   in appendix a and the above
assumptions ensure
i
i
fi  a    fi  b  j            fi  bl j   
    
 
y

l
y

j   

j   

     suggests that fi  gt  can be considered as a function of fi  a  if these equations about
inside probabilities are hierarchically organized in such a way that fi gt  belongs to the top
layer and any fi a  appearing on the left hand side only refers to fi  b  s which belong to the
lower layers  we refer to this condition as the acyclic support condition  under the acyclic
support condition  equations of the form      have a unique solution  and the computation
of pdb  g j   via inside probabilities allows us to take advantage of reusing intermediate
results stored as fi  a   thereby contributing to faster computation of pdb  gt j   
next we tackle a more intricate problem  the computation of s   g   pmsw s j
 i v  s    since the sum equals n msw i n v   s    g   pmsw  s j    we concentrate
on the computation of
 gt  m  def
 
pmsw  s j   
p

p

db

p

db

t

t

x

   

m s   db gt

where m   msw i n v   first we note that if an explanation s contains m like s   a        
ah   m  then we have fi  s     fi  a          fi  ah fi  m   so  gt   m  is expressed as
 gt  m    ff gt   m fi  m 

    
where ff gt  m      fi g m  m  and ff gt  m  does not depend on fi  m   generalizing this observation to arbitrary ground atoms  we introduce the outside probability of ground atom a
w r t  gt by
 gt 
ff gt  a  def
   fi
 fi  a 
t

    the logical relationship      corresponds to      where f  g and h are table atoms 
   

fiparameter learning of logic programs for symbolic statistical modeling

assuming the same conditions as inside probability  in view of       the problem of computing  gt  m  is now reduced to that of computing ff gt  m   which is recursively computable
as follows  suppose a occurs in the ground program db like
b 
bk

a   w             b 

    

a   wk            bk

a   w  i 
a   wk ik  

as fi  gt  is a function of fi  b             fi bk   by our assumption  the chain rule of derivatives
leads to
 fi  gt  
 fi  a   wk i  
 fi  gt    fi  a   w     
 
  
 
 
ff gt   a   
 fi  b   
 fi  a 
 fi  bk  
 fi  a 
and hence to  
ff gt  gt      
    
i
i
ff gt   a    ff gt   b    fi  w  j            ff gt   bk   fi  wk j   
    












k

 
x

k
x

j   

j   

therefore if all inside probabilities have already been computed  outside probabilities are
recursively computed from the top      using      downward along the program layers  in
the case of dbp with f and m being chosen atoms  we compute
ff f  f     
ff f  g    fi  a    fi  b 
    
ff f  h    ff f  g fi  d 
ff f  m    ff f  h  
from       the desired sum  f  m  is calculated as
 f  m    ff f  m fi  m     fi  a    fi  b  fi  d fi  m 
which requires only two multiplications and one addition compared to four multiplications
and one addition in the naive computation 
gains obtained by computing inside and outside probability may be small for this case 
but as the problem size grows  they become enormous  and compensate enough for additional restrictions imposed on the result of oldt search 
 
 
 
 
 
 
 
 
 

    oldt search

to compute inside and outside probability recursively like      or      and       we need
at programming level a tabulation mechanism for structure sharing of partial explanations
    because of the independence assumption on body atoms  wh j     h  k     j  ih   and a are
independent  therefore
 fi  a   wh j      fi  a fi  wh j     fi  w   
h j
 fi a 
 fi  a 
   

fisato   kameya

between subgoals  we henceforth deal with programs db in which a set table db  of table
predicate s are declared in advance  a ground atom containing a table predicate is called
a table atom  the purpose of table atoms is to store their support sets and eliminate the
need of recomputation  and by doing so  to construct hierarchically organized explanations
made up of the table atoms and the msw atoms 
let db   fmsw   r be a parameterized logic program which satisfies the finite support
condition and the uniqueness condition  also let g    g           gt be a random sample of
observable atoms in obs db   we make the following additional assumptions 

assumptions 

for each t     t  t    there exists a finite set f t          kt g of table atoms associated
t     k  k      j  m   such that
with conjunctions sk j
t
k
t

e

comp r 





  gt   s t             s t m
   t   s t            s t m          kt t   skt t             skt t  mkt
e



e

e

 

e





e

 

e

    



where
t     k  k      j  m   is  as a set  a subset of f
 each sk j
t
msw   fk             k g
k
 acyclic support condition   as a convention  we put     gt and call respectively
t def
t  k     a t explanation
db
  f     t          kt g the set of table atoms for gt and sk j
for kt     the set of all t explanations for k is denoted by db  kt   and we consider
db     as a function of table atoms 
t   st    
 t explanations are mutually exclusive  i e  for each k     k  kt    pdb  sk j
k j  
      j    j    mk    t exclusiveness condition  
t     k  k      j  m   is a conjunction of independent atoms  independent
 sk j
t
k
condition    
these assumptions are aimed at ecient probability computation  namely  the acyclic
support condition makes dynamic programming possible  the t exclusiveness condition reduces pdb  a   b  to pdb  a   pdb  b  and the independent condition reduces pdb  a   b  
to pdb  a pdb  b   there is one more point concerning eciency however  note that the
t    imcomputation in dynamic programming proceeds following the partial order on db
posed by the acyclic support condition and access to the table atoms will be much simplified
t respecting the said partial
if they are linearly ordered  we therefore topologically sort db
t
order and call the linearized db satisfying the three assumptions  the acyclic support condition  the t exclusiveness condition and the independent condition  a hierarchical system
t   h t    t            t i     g   assuming
of t explanations for gt   we write it as db
 
t
db     is
   
k
  
implicitly given  once a hierarchical system of t explanations for gt is successfully built
e

t

e

t

e

e

e

e

e

t

e

    prefix  t   is an abbreviation of  tabled   
    the independence mentioned here only concerns positive propositions  for b    b    head db   we say
b  and b  are independent if pdb  b    b  j     pdb  b  j  pdb  b  j   for any  
    i precedes j if and only if the top down execution of i w r t  db invokes j directly or indirectly 
    so now it holds that if i precedes j then i   j  
   

fiparameter learning of logic programs for symbolic statistical modeling

from the source program  equations on inside probability and outside probability such as
     and      are automatically derived and solved in time proportional to the size of the
equations  it plays a central role in our approach to ecient em learning 
one way to obtain such t explanations is to use oldt search  tamaki   sato       
warren         a complete refutation method for logic programs  in oldt search  when
a goal g is called for the first time  we set up an entry for g in a solution table and store
its answer substitutions g there  when a call to an instance g  of g occurs later  we stop
solving g  and instead try to retrieve an answer substitution g stored in the solution table
by unifying g  with g  to record the remaining answer substitutions of g  we prepare a
lookup table for g  and hold a pointer to them 
for self containedness  we look at details of oldt search using a sample program
dbh   fh  rh in figure     which depicts an hmm   in figure    this hmm has two states
fs   s g  at a state transition  it probabilistically chooses the next destination from fs   s g
a b
s 

s 

a b

a b

a b

figure    two state hmm
fh

rh

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

f   values init   s  s    
f   values out     a b   
f   values tr      s  s    

h   hmm cs  msw init once si  
hmm   si cs  
h   hmm t s  c cs     t    
msw out s  t c  
msw tr s  t nexts  
t  is t   
hmm t  nexts cs  
h   hmm t         t   

 
 
 
 
 
 
 
 
 

to generate a string  chars  cs 
set initial state to si  and then
enter the loop with clock     
loop 
output c in state s 
transit from s to nexts 
put the clock ahead 
continue the loop  recursion  
finish the loop if clock     

figure    two state hmm program dbh
    f   f   f   h   h  and h  are temporary marks  not part of the program 
    an hmm defines a probability distribution over strings in the given set of alphabets  and works as a
stochastic string generator  rabiner   juang        such that an output string is a sample from the
defined distribution 
   

fisato   kameya

and also an alphabet from fa  bg to emit  note that to specify a fact set fh and the associated distribution compactly  we introduce here a new notation values i  v      vm    it
declares that fh contains msw atoms of the form msw i n v   v   fv            vmg  whose distribution is p i n  given by     in subsection      for example   f    values tr     s  s   
introduces msw tr t  n v  atoms into the program such that t can be any ground term 
v   fs   s g and for a ground term n  they have a distribution
x y
p tr t  n   msw tr t  n s     x  msw tr t  n s     y j i s    i s     i s
  i s 
where i   tr t   x  y   f    g and x   y     
this program runs like a prolog program  for a non ground top goal hmm s   it functions as a stochastic string generator returning a list of alphabets such as  a b a  in the
variable s as follows  the top goal calls clause  h   and  h   selects the initial state by executing subgoal msw init once si    which returns in si an initial state probabilistically
chosen from fs   s g  the second clause  h   is called from  h   with ground s and ground
t  it makes a probabilistic choice of an output alphabet c by asking msw out s  t c  and
then determines nexts  the next state  by asking msw tr s  t nexts    h   is there to
stop the transition  for simplicity  the length of output strings is fixed to three  this way
of execution is termed as sampling execution because it corresponds to a random sampling
from pdb   if the top goal is ground like hmm  a b a    it works as an acceptor  i e 
returning success  yes  or failure  no  
if all explanations for hmm  a b a   are sought for  we keep all msw atoms resolved upon
during the refutation as a conjunction  explanation   and repeat this process by backtracking until no more refutation is found  if we need t explanations however  backtracking must
be abandoned because sharing of partial explanations through t explanations  the purpose
of t explanations itself  becomes impossible  we therefore instead use oldt search for all
h

t  
t  
t  
t  
t   
 
t  
t  
t  

top hmm cs ans    tab hmm cs ans     
tab hmm cs  hmm cs  x  x    hmm cs       
tab hmm t s cs  hmm t s cs  x  x    hmm t s cs       
e msw init t s   msw init t s   x  x  
e msw init t s   msw init t s   x  x  
hmm cs x  x     e msw init once si x  x    tab hmm   si cs x  x   
hmm t s  c cs  x  x   t     e msw out s  t c x  x    e msw tr s  t nexts x  x   
t  is t    tab hmm t  nexts cs x  x   
hmm t s    x x    t   

figure    translated program of dbh
    if msw i n v  is called with ground i and ground n  v  a logical variable  behaves like a random variable 
it is instantiated to some term v with probability i v selected from the value set vi declared by a values
atom  if  on the other hand  v is a ground term v when called  the procedural semantics of msw i n v 
is equal to that of msw i n v    v   v 
   

fiparameter learning of logic programs for symbolic statistical modeling

t explanation search  in the case of the hmm program for example  to build a hierarchical
system of t explanations for hmm  a b a   by oldt search  we first declare hmm   and
hmm   as table predicate    so a t explanation will be a conjunction of hmm   atoms  hmm  
atoms and msw atoms  we then translate the program into another logic program  analogously to the translation of definite clause grammars  dcgs  in prolog  sterling   shapiro 
       we add two arguments  which forms a d list  to each predicate for the purpose of
accumulating msw atoms and table atoms as conjuncts in a t explanation  the translation
applied to dbh yields the program in figure   
in the translated program  clause  t   corresponds to the top goal hmm l  with an
input string l  and a t explanation for the table atom hmm l  will be returned in ans   t  
and  t   are auxiliary clauses to add to the callee s d list a table atom of the form hmm l 
and hmm t s l  respectively  t  time step  s  state   in general  if p n is a table predicate
in the original program  p  n      becomes a table predicate in the translated program and
an auxiliary predicate tab p  n     is inserted to signal the oldt interpreter to check the
solution table for p n  i e  to check if there already exist t explanations for p n  likewise
clauses  t   and  t    are a pair corresponding to  f   which insert msw init t    to the
callee s d list with t   once  clauses  t     t   and  t   respectively correspond to  h   
 h   and  h   
hmm  a b a    hmm  a b a   
   msw init once s    hmm   s   a b a    
 msw init once s    hmm   s   a b a     
hmm   s   a b a    hmm   s   a b a   
   msw out s     a   msw tr s     q    hmm   s   b a    
 msw out s     a   msw tr s     s    hmm   s   b a     
hmm   s   a b a    hmm   s   a b a   
   msw out s     a   msw tr s     s    hmm   s   b a    
 msw out s     a   msw tr s     s    hmm   s   b a     
hmm   s   b a    hmm   s   b a   
   msw out s     b   msw tr s     s    hmm   s   a    
 msw out s     b   msw tr s     s    hmm   s   a     
hmm   s   b a    hmm   s   b a   
   msw out s     b   msw tr s     s    hmm   s   a    
 msw out s     b   msw tr s     s    hmm   s   a     
hmm   s   a    hmm   s   a   
   msw out s     a   msw tr s     s    hmm   s       
 msw out s     a   msw tr s     s    hmm   s        
hmm   s   a    hmm   s   a   
   msw out s     a   msw tr s     s    hmm   s       
 msw out s     a   msw tr s     s    hmm   s        
hmm   s       hmm   s      
    
hmm   s       hmm   s      
    

figure    solution table
    in general  p n means a predicate p with arity n  so although hmm   and hmm   share the predicate name
hmm  they are different predicates 
   

fisato   kameya

then after translation  we apply oldt search to top hmm  a b a  ans  while noting  i  the added d list does not inuence the oldt procedure  and  ii  we associate with
each solution of a table atom in the solution table a list of t explanations  the resulting
solution table is shown in figure    the first row reads that a call to hmm  a b a   occurred and entered the solution table and its solution  hmm  a b a    no variable binding generated   has two t explanations  msw init once s     hmm   s   a b a   and
msw init once s     hmm   s   a b a    the remaining task is the topological sorting of the table atoms stored in the solution table respecting the acyclic support condition 
this can be done by using depth first search  trace  of t explanations from the top goal for
example  thus we obtain a hierarchical system of t explanations for hmm  a b a   

    support graphs

looking back  all we need to compute inside and outside probability is a hierarchical system
of t explanations  which essentially is a boolean combination of primitive events  msw atoms 
and compound events  table atoms  and as such can be more intuitively representable as a
graph  for this reason  and to help visualizing our learning algorithm  we introduce a new
data structure termed support graphs  though the new em algorithm in the next subsection
itself is described solely by the hierarchical system of t explanations 
as illustrated in figure    a   the support graph for gt is a graphical representation of
t   h t    t            t i   t   g   for g in      
the hierarchical system of t explanations db
t
t
   
 
k
it consists of totally ordered disconnected subgraphs  each of which is labeled with the
t     k  k    a subgraph labeled  t comprises two
corresponding table atom kt in db
t
k
special nodes  the start node and the end node  and explanation graphs  each corresponding
t in
t
to a t explanation sk j
db  k       j  mk   
t is a linear graph in which a node is labeled either with a
an explanation graph of sk j
t   they are called a table node and a switch
table atom  or with a switch msw        in sk j
node respectively  figure    b  is the support graph for hmm  a b a   obtained from the
solution table in figure    each table node labeled  refers to the subgraph labeled    so
data sharing is achieved through the distinct table nodes referring to the same subgraph 
t

e

e

e

e

    graphical em algorithm

we describe here an ecient em learning algorithm termed the graphical em algorithm
 figure    introduced by kameya and sato         that runs on support graphs  suppose
we have a random sample g   g           gt of observable atoms  also suppose support
graphs for gt     t  t    i e  hierarchical systems of t explanations satisfying the acyclic
support condition  the t exclusiveness condition and the independent condition  have been
successfully constructed from a parameterized logic program db satisfying the uniqueness
condition and the finite support condition 
the graphical em algorithm refines learn naive   g   by introducing two subroutines 
get inside probs 
  g   to compute inside probabilities and get expectations 
  g   to compute outside probabilities  they are called from the main routine learn gem   g    when
learning  we prepare four arrays for each support graph for gt in g  
 p  t     for the inside probability of    i e  fi       pdb   j    see      
db

db

db

db

   

fiparameter learning of logic programs for symbolic statistical modeling
k

 a 

explanation graph

msw

gt 
start

k

msw

end

msw

msw

msw

 
k  
start

end
msw

msw

 

 b 
msw init once s  

hmm   s   a b a  

hmm  a b a   

start

end

msw init once s  

msw out s     a 

hmm   s   a b a  

msw tr s     s  

hmm   s   b a  

hmm   s   a b a   

end

start

msw out s     a 

msw tr s     s  

hmm   s   b a  

msw out s     a 

msw tr s     s  

hmm   s   b a  

hmm   s   a b a   

end

start

msw out s     a 

msw tr s     s  

hmm   s   b a  

figure    a support graph  a  in general form   b  for gt   hmm  a b a   in the hmm
program dbh  a double circled node refers to a table node 
 q t     for the outside probability of  w r t  gt   i e  ff gt      see      and      
 r t    s   for the explanation probability of s    db  kt     i e  pdb  s j  
e

e

   

e

e

fisato   kameya
   procedure learn gem  db  g  
   begin
   select some  as initial
  
  
  
  
  
  

   procedure get inside probs  db  g  
   begin
   for t      to t do begin
  
let  t   gt 
  
for k    kt downto   do begin
  
p  t  kt        
  
foreach se   edb  kt   do begin
  
let se   fa    a            ajsejg 
  
r t  kt   se       
   
for l      to jsej do
   
if al   msw i   v   then
   
r t  kt   se      i v
   
else r t  kt   se      p  t  al   
   
p  t  kt     r t  kt   se 
   
end    foreach se   
   
end    for k   
    end    for t   
    end 

parameters 

get inside probs
 db  g  
p
       tt   ln p  t  gt   
repeat

get expectations  db  g   
i   i  v   vi do
  i  vp
    
t  t  i  v  p  t  g   
t
t  
foreach i   i  v p
  vi do
i v     i  v   v   vi   i  v    
get inside probs  db  g   
m    m  
  
p
 m     tt   ln p  t  gt  
until  m     m       
foreach

   
   
   
   
   
   
    end 

   procedure get expectations  db  g   begin
   for t      to t do begin
  
foreach i   i  v   vi do   t  i  v       
  
let  t   gt  q t   t         
  
for k      to kt do q t  kt        
  
for k      to kt do
  
foreach se   edb  kt   do begin
  
let se   fa    a            ajsejg 
  
for l      to jsej do
   
if al   msw i   v   then   t  i  v     q t  kt     r t  kt   se 
   
else q t  al      q t  kt     r t  kt   se  p  t  al  
   
end    foreach se   
    end    for t   
    end 

figure    graphical em algorithm 
  t  i  v  for the expected count of msw i   v   i e 

p

s 

db

 gt  pmsw  s j  i v  s  

and call the procedure learn gem   g  in figure    the main routine learn gem   g  initially computes all inside probabilities  line    and enters a loop in which get expectations   g  
is called first to compute the expected count  t  i  v  of msw i   v  and parameters are updated  line      inside probabilities are renewed by using the updated parameters before
entering the next loop  line     
db

db

db

   

fiparameter learning of logic programs for symbolic statistical modeling

the subroutine get inside probs   g   computes the inside probability fi       pdb   j  
 and stores it in p  t      of a table atom  from the bottom layer up to the topmost layer    
gt  line    of the hierarchical system of t explanations for gt  see      in subsection      
it takes a t explanation s in db  kt   one by one  line     decomposes s into conjuncts and
multiplies their inside probabilities which are either known  line     or already computed
 line     
the other subroutine get expectations   g   computes outside probabilities following the
recursive definitions      and      in subsection     and stores the outside probability
ff gt     of a table atom  in q t      it first sets the outside probability of the top goal
    gt to      line    and computes the rest of outside probabilities  line    going down
the layers of the t explanation for gt described by      in subsection       line     adds
q t  kt     r t  kt   s     ff gt   kt     fi s   to  t  i  v   the expected count of msw i   v   as a
contribution of msw i   v  in s through kt to  t  i  v    line     increments the outside
probability q t  al     ff gt  al   of al according to the equation       notice that q t  kt  
has already been computed and r t  kt   s  p  t  al     fi w   for s   al   w   as shown in
subsection      learn naive   g   is the mle procedure  hence the following theorem holds 
theorem     let db be a parameterized logic program  and g   g           gt a randb

e

e

e

db

e

e

e

e

e

db

dom sample of observable atoms  suppose the five conditions  uniqueness  finite support
 subsection       acyclic support  t exclusiveness and independence  subsection       are
met  thenqlearn gem  db  g   finds the mle   which  locally  maximizes the likelihood
l g j     tt   pdb  gt j    

 proof  sketch    since the main routine learn gem   g   is the same as learn naive   g 
except the computation of  i  v    tt    t  i  v   we show that  t  i  v    s   g   pmsw s j
 i v  s      n msw i n v   s   g   pmsw  s j     however 
db

p

p

db

p

db

 t  i  v 

db

p

 

x

x

 kkt

t

t

x

n msw i n v   se  e   t  
db k

ff gt   kt  fi  se 

 see  line     in get expectations db  g  
  ff gt  msw i n v  fi msw i n v  
n
   gt  msw i n v    see the equation      
n
 
pmsw  s j    
q e d 
x

x

x

x

n msw i n v   s 

db

 gt  

here we used the fact that if s contains msw i n v  like s   s    msw i n v   fi s   
fi  s    fi  msw i n v    holds  and hence
ff gt   kt  fi  s     ff gt   kt  fi  s    fi  msw i n v   
   contribution of msw i n v  in s through kt to ff gt  msw i n v   fi  msw i n v   
e

e

e

e

e

e

e

e

    a formal proof is given by kameya         it is proved there that under the common parameters     i  v 
in learn naive db g   coincides with  i  v  in learn gem db g    so  the parameters are updated to
the same values  hence  starting with the same initial values  the parameters converge to the same
values 
   

fisato   kameya

the five conditions on the applicability of the graphical em algorithm may look hard
to satisfy at once  fortunately  the modeling principle in section     still stands  and with
due care in modeling  it is likely to lead us to a program that meets all of them  actually 
we will see in the next section  programs for standard symbolic statistical frameworks such
as bayesian networks  hmms and pcfgs all satisfy the five conditions 
   complexity

in this section  we analyze the time complexity of the graphical em algorithm applied
to various symbolic statistical frameworks including hmms  pcfgs  pseudo pcsgs and
bayesian networks  the results show that the graphical em algorithm is competitive with
these specialized em algorithms developed independently in each research field 

    basic property

since the em algorithm is an iterative algorithm and since we are unable to predict when
it converges  we measure time complexity by the time taken for one iteration  we therefore
estimate time per iteration on the repeat loop of learn gem  db  g   g   g            gt    we
observe that in one iteration  each support graph for gt     t  t   is scanned twice  once
by get inside probs  db  g   and once by get expectations  db  g   in the scan  addition is
performed on the t explanations  and multiplication  possibly with division  is performed
on the msw atoms and table atoms once for each  so time spent for gt per iteration by the
graphical em algorithm is linear in the size of the support graph  i e  the number of nodes
in the support graph for gt  put
 tdb def
 
db    
e

 

e

t
  db

num def
   max
j e t j
tt db
maxsize def
 
max
jsej 
t
e
e
 tt s   db
t is the set of table atoms for g   and hence  t is the set of all t explanations
recall that db
t
db
appearing in the right hand side of      in subsection      so num is the maximum number
of t explanations in a support graph for the gt s and maxsize the maximum size of a texplanation for the gt s respectively  the following is obvious 
e

proposition     the time complexity of the graphical em algorithm per iteration is linear

in the total size of support graphs  o  nummaxsize t   in notation  which coincides with the
space complexity because the graphical em algorithm runs on support graphs 

this is a rather general result  but when we compare the graphical em algorithm with
other em algorithms  we must remember that the input to the graphical em algorithm is
support graphs  one for each observed atom  and our actual total learning time is
oldt time    the number of iterations   o nummaxsizet  
   

fiparameter learning of logic programs for symbolic statistical modeling

where  oldt time  denotes time to construct all support graphs for g   it is the sum of
time for oldt search and time for the topological sorting of the table atoms  but because
the latter is part of the former order wise    we represent  oldt time  by time for oldt
search  also observe that the total size of support graphs does not exceed time for oldt
search for g order wise 
to evaluate oldt time for a specific class of models such as hmms  we need to know
time for table operations  observe that our oldt search in this paper is special in the
sense that table atoms are always ground when called and there is no resolution with solved
goals  accordingly a solution table is used only
 to check if a goal g already has an entry in the solution table  i e  if it was called
before  and
 to add a new searched t explanation for g to the list of discovered t explanations
under g s entry 
the time complexity of these operations is equal to that of table access which depends
both on the program and on the implementation of the solution table    we first suppose
programs are carefully written in such a way that the arguments of table atoms used as indecies for table access are integers  actually all programs used in the subsequent complexity
analysis  dbh in subsection      dbg and dbg  in subsection      dbg in subsection     
satisfy or can satisfy this condition by replacing non integer terms with appropriate integers  we also suppose that the solution table is implemented using an array so that table
access can be done in o    time   
in what follows  we present a detailed analysis of the time complexity of the graphical
em algorithm applied to hmms  pcfgs  pseudo pcsgs and bayesian networks  assuming
o    time access to the solution table  we remark by the way that their space complexity
is just the total size of solution tables  support graphs  


    hmms

the standard em algorithm for hmms is the baum welch algorithm  rabiner        rabiner   juang         an example of hmm is shown in figure   in subsection        given
t observations w            wt of output string of length l  it computes in o  n   lt   time in
each iteration the forward probability fftm q    p  ot  ot       otm    q j   and the backward
probability fimt  q    p  otm otm         otl j q    for each state q   q  time step m     m  l 
and a string wt   ot  ot       otl     t  t    where q is the set of states and n the number of
states  the factor n   comes from the fact that every state has n possible destinations and

    think of oldt search for a top goal gt   it searches for msw atoms and table atoms to create a solution table  while doing some auxiliary computations  therefore its time complexity is never less
than o jthe number of msw atoms and table atoms in the support graph for gt j   which coincides with
the time we need to topologically sort table atoms in the solution table by depth first search from     gt  
    sagonas et al         and ramakrishnan et al         discuss about the implementation of oldt 
    if arrays are not available  we may be able to use balanced trees  giving o log n  access time where n
is the number data in the solution table  or we may be able to use hashing  giving average o     time
access under a certain condition  cormen  leiserson    rivest        
    we treat here only  state emission hmms  which emit a symbol depending on the state  another type 
 arc emission hmms  in which the emitted symbol depends on the transition arc  is treated similarly 
   

fisato   kameya

we have to compute the forward and backward probability for every destination and every
state  after computing all ffmt  q  s and fimt  q  s  parameters are updated  so  the total
computation time in each iteration of the baum welch algorithm is estimated as o n  lt  
 rabiner   juang        manning   schutze        
to compare this result with the graphical em algorithm  we use the hmm program
dbh in figure   with appropriate modifications to l  the length of a string  q  the
state set  and declarations in fh for the output alphabets  for a string w   o o       ol 
hmm n q   om   om             ol    in dbh reads that the hmm is in state q   q at time n and
has to output  om  om        ol  until it reaches the final state  after declaring hmm   and
hmm   as table predicate and translation  see figure     we apply oldt search to the goal
top hmm  o      ol   ans  w r t  the translated program to obtain all t explanations
for hmm  o      ol    for a complexity argument however  the translated program and
dbh are the same  so we talk in terms of dbh for the sake of simplicity  in the search 
we fix the search strategy to multi stage depth first strategy  tamaki   sato         we
assume that the solution table is accessible in o    time    since the length of the list in
the third argument of hmm   decreases by one on each recursion  and there are only finitely
many choices of the state transition and the output alphabet  the search terminates  leaving
finitely many t explanations in the solution table like figure   that satisfy the acyclic support condition respectively  also the sampling execution of hmm l  w r t  dbh is nothing
but a sequential decision process such that decisions made by msw atoms are exclusive 
independent and generate a unique string  which means dbh satisfies the t exclusiveness
condition  the independence condition and the uniqueness condition respectively  so  the
graphical em algorithm is applicable to the set of hierarchical systems of t explanations for
hmm wt       t  t   produced by oldt search for t observations w            wt of output
string  put wt   ot  ot       otl  it follows from
t
db
  fhmm m q  otm      otl   j    m  l      q   qg   fhmm  ot      otl  g
h

dbh

 

 

msw out q  m om    msw tr q  m q    
hmm m     q    otm        otl   

fi
fi
fi
fi
fi

 

   
    m  l 
that for a top goal hmm  ot       otl    there are at most o nl  calling patterns of hmm  
and each call causes at most n calls to hmm    implying there occur o nl   n     o n  l 
calls to hmm    since each call is computed once due to the tabling mechanism  we have
num   o n   l   also maxsize      applying proposition      we reach
e

hmm m q   otm      otl   

q    q

proposition     suppose we have t strings of length l  also suppose
each table operation
 

in oldt search is done in o     time  oldt time by dbh is o n lt   and the graphical
em algorithm takes o n   lt   time per iteration where n is the number of states 

o n  lt   is the time complexity of the baum welch algorithm 
algorithm runs as eciently as the baum welch algorithm   

so the graphical em

    o    is possible because in the translated program dbh in section      we can identify a goal pattern of
hmm            by the first two arguments which are constants  integers  
    besides  the baum welch algorithm and the graphical em algorithm whose input are support graphs
generated by dbh update parameters to the same value if initial values are the same 
   

fiparameter learning of logic programs for symbolic statistical modeling

by the way  the viterbi algorithm  rabiner        rabiner   juang        provides for
hmms an ecient way of finding the most likely transition path for a given input output
string  a similar algorithm for parameterized logic programs that determines the most
likely explanation for a given goal can be derived  it runs in time linear in the size of the
support graph  thereby o n   l  in the case of hmms  the same complexity as the viterbi
algorithm  sato   kameya        

    pcfgs

we now compare the graphical em algorithm with the inside outside algorithm  baker 
      lari   young         the inside outside algorithm is a well known em algorithm
for pcfgs  wetherell        manning   schutze           it takes a grammar in chomsky
normal form  given n nonterminals  a production rule in the grammar takes the form
i   j  k     i  j  k  n    nonterminals are named by numbers from   to n and   is a
starting symbol  or the form i   w where    i  n and w is a terminal  in each iteration 
it computes the inside probability and the outside probability of every partial parse tree
of the given sentence to update parameters for these production rules  time complexity is
measured by time per iteration  and is described by n   the number of nonterminals  and
l  the number of terminals in a sentence  it is o n   l t   for t observed sentences  lari
  young        
to compare the graphical em algorithm with the inside outside algorithm  we start
from a propositional program dbg   fg   rg below representing the largest grammar
containing all possible rules i   j  k in n nonterminals where nonterminal   is a starting
symbol  i e  sentence 
fg
rg

d d    j  k   j    i  j  k  n  d  d  are numbersg
  fmsw  if  msw 
i d w   j    i  n  d is a number  w is a terminalg

 

 
 
 
 
 

s

q i d  d       msw i  d  d     j  k    
q j  d  d    
q k  d   d   
n

q i d d        msw i d wd     

fi
fi
fi

fi
fi
fi
fi
fi
fi
fi

   i  j  k  n 
   d    d    d   l

   i  n     d  l    

 
 
 
 
 

o

figure    pcfg program dbg
db g is an artificial parsing program whose sole purpose is to measure the size of an
oldt tree   created by the oldt interpreter when it parses a sentence w  w       wl  so
    a pcfg  probabilistic context free grammar  is a backbone cfg with probabilities  parameters  assigned to each production rule  for a nonterminal a having
n production rules fa   ffi j    i  ng  a
p
probability pi is assigned to a   ffi     i  n  where ni   pi      the probability of a sentence s is
the sum of probabilities of each  leftmost  derivation of s  the latter is the product of probabilities of
rules used in the derivation 
    to be more precise  an oldt structure  but in this case  it is a tree because dbg contains only constants
 datalog program  and there never occurs the need of creating a new root node 
   

fisato   kameya
   

td

q   d l 
  j n

  k n
q   d d    
q   d   l 

q   d   l 
   

 note  q
  i

  k n

q   d d    
q k d   l 

q j d d    
q   d   l 

q j d d    
q k d   l 

q k d   l 

q   d   l 

q k d   l 

 k 

td  
q i d d  already appears
d   d d l    i
d d l d  

td  

d   e l  
  j n

  k n
q j d e  
q   e l 

q j d e  
q k e l 

d   e e
  i n 
  j n 

q i d e  
q j e e  
q   e l 

td

q k e l 

n

q

n

q j e e  
q   e l 
q   e l 

   
p i 

p    p    p n 

figure     oldt tree for the query

q   d l 

the input sentence w  w       wl is embedded in the program separately as msw i d wd   
    d  l     in the second clauses of rg  this treatment does not affect the complexity argument   q i d  d   reads that the i th nonterminal spans from position d  to position d  
i e  the substring wd         wd   the first clauses q i d   d       msw         q j  d   d   
q k d   d   are supposed to be textually ordered according to the lexicographic order for
tuples hi  j  k  d    d    d i  as a parser  the top goal is set to q     l     it asks the
parser to parse the whole sentence w  w        wl as the syntactic category      sentence  
we make an exhaustive search for this query by oldt search    as before  the multistage depth first search strategy and o    time access to the solution table are assumed 
then the time complexity of oldt search is measured by the number of nodes in the
oldt tree  let td k  be the oldt tree for q k d l   figure    illustrates td    for d
    d  l      where msw atoms are omitted  as can be seen  the tree has many similar
subtrees  so we put them together  see note in figure      due to the depth first strategy 
td    has a recursive structure and contains td   
   as a subtree  nodes whose leftmost atom
is not underlined are solution nodes  i e  they solve their leftmost atoms for the first time in
the entire refutation process  the underlined atoms are already computed in the subtrees
to their left    they only check the solution table if there are their entries    already
 

 

    l here is not a prolog variable but a constant denoting the sentence length 
    q is a table predicate 
    
 
    it can be inductively proved that td   
   contains every computed q i d  d       d  l      d      d  
d    l     i  n  d     d   l   d      
   

fiparameter learning of logic programs for symbolic statistical modeling

computed  in o    time  since all clauses are ground  such execution only generates a
single child node 
   
 k 
we enumerate h   
d   the number of nodes in td but not in td       k  n    from
 k 
 
    
figure     we see h   
d   o n  l   d     let hd     k  n   be the number of nodes in
k 
td   
not contained in td       it is estimated as o n    l   d        consequently  the number
 k 
n
 
 
of nodes that are newly created in td    is h   
d   k   hd   o  n  l   d     as a result 
l
 
 
 
 
  
total time for oldt search is computed as d   hd   o n l   which is also the size of
the support graph 
we now consider a non propositional parsing program dbg    fg    rg  in figure   
whose ground instances constitute the propositional program dbg   dbg  is a probabilistic
variant of dcg program  pereira   warren        in which q     q    and between   are
declared as table predicate  semantically dbg  specifies a probability distribution over the
atoms of the form fq  l  j l is a list of terminalsg 
p

p

fg 

rg 

t  sj  sk    j    i  j  k  n  t is a numberg
  fmsw  sfi msw 
si  t w  j    i  n  t is a number  w is a terminalg

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

q  s     length s d   q  s     d     s     
q  i d  d  c  c  l  l      between d  d  d   
msw i c   j k   
q  j d  d  s c   c  l  l   
q  k d  d  s c   c  l  l   
q  i d s d  c  s c    w x  x     msw i c  w  

figure     probabilistic dcg like program dbg 
the top goal to parse a sentence s    w           wl  is q   w           wl    it invokes
q  s     d      w         wl       after measuring the length d of an input sentence s by
calling length          in general  q  i d  d   c   c   l  l   works identically to q i d   d   
but three arguments  c    c  and l  l   are added  c  supplies a unique trial id for msws to be
used in the body  c  the latest trial id in the current computation  and l   l  a d list holding
a substring between d  and d    since the added arguments do not affect the shape of the

    we here focus on the subtree td    j   i  and j   range from   to n   and fif e  e    j d      e    e  l     gfi  
o  l   d      hence  the number of nodes in td  is o n    l   d      the number of nodes in td    but
   
 
neither in td   
  o  n    l   d     
   nor in td is negligible  therefore hd
   
   
    the number of nodes in tl   and tl   is negligible 
    to make the program as simple as possible  we assume that an integer n is represented by a ground term
fi

 n 
z     

fi

s      s            we also assume that when d  and d  are ground  the goal between d   d   d  
returns an integer d  between them in time proportional to jd    d j 
    we omit an obvious program for length l sn   which computes the length sn of a list l in o jlj  time 

sn  

def

   

fisato   kameya

search tree in figure    and the extra computation caused by length   is o l  and the
one by the insertion of between d  d  d   is o nl   respectively    oldt time remains
o n   l    and hence so is the size of the support graph 
to apply the graphical em algorithm correctly  we need to confirm the five conditions
on its applicability  it is rather apparent however that the oldt refutation of any topgoal of the form q   w         wl   w r t  dbg  terminates  and leaves a support graph
satisfying the finite support condition and the acyclic support condition  the t exclusiveness
condition and the independent condition also hold because the refutation process faithfully
simulates the leftmost stochastic derivation of w       wl in which the choice of a production
rule made by msw si  sc  sj  sk    is exclusive and independent  trial ids are different on
different choices  
what remains is the uniqueness condition  to confirm it  let us consider another program dbg     a modification of dbg  such that the first goal length s d  in the body of the
first clause and the first goal between d  d  d   in the second clause of rg  are moved to
the last position in their bodies respectively  dbg   and dbg  are logically equivalent  and
semantically equivalent as well from the viewpoint of distribution semantics  then think of
the sampling execution by the oldt interpreter of a top goal q  s  w r t  dbg   where
s is a variable  using the multi stage depth first search strategy  it is easy to see first that
the execution never fails  and second that when the oldt refutation terminates  a sentence
 w            wl   is returned in s  and third that conversely  the set of msw atoms resolved upon
in the refutation uniquely determines the output sentence  w            wl     hence  if the
sampling execution is guaranteed to always terminate  every sampling from pf       pf    
uniquely generates a sentence  an observable atom  so the uniqueness condition is satisfied
by dbg     and hence by dbg   
then when is the sampling execution guaranteed to always terminate  in other words 
when does the grammar only generate finite sentences  giving a general answer seems
dicult  but it is known that if the parameter values in a pcfg are obtained by learning
from finite sentences  the stochastic derivation by the pcfg terminates with probability
one  chi   geman         in summary  assuming appropriate parameter values  we can
say that the parameterized logic program dbg  for the largest pcfg with n nonterminal
symbols satisfies all applicability conditions  and the oldt time for a sentence of length
l is o n   l     and this is also the size of the support graph  from proposition      we
conclude
g

g

proposition     let db be a  parameterized logic program representing a pcfg with n

nonterminals in the form of dbg in figure     and g   g    g            gt be the sampled atoms
representing sentences of length l  we suppose each table operation in oldt search is done
in o    time  then oldt search for g and one iteration in learn gem are respectively
done in o n   l t   time 

   

between d  d  d  

is called o n  l   d     times in td      so it is called

  o n  l   d       o nl   

pl  
d  

times in t  
    because the trial ids used in the refutation record which rule is used at what step in the derivation of
w        wl  
    in dbg   we represent integers by ground terms made out of   and s    to keep the program short  if
we use integers instead of ground terms however  the first three arguments of q               are enough
to check whether the goal is previously called or not  and this check can be done in o    time 
   
 

 

   

fiparameter learning of logic programs for symbolic statistical modeling
o  n   l   t  

is also the time complexity of the inside outside algorithm per iteration
 lari   young         hence our algorithm is as ecient as the inside outside algorithm 

    pseudo pcsgs

pcfgs can be improved by making choices context sensitive  and one of such attempts is
pseudo pcsgs  pseudo probabilistic context sensitive grammars  in which a rule is chosen probabilistically depending on both the nonterminal to be expanded and its parent
nonterminal  charniak   carroll        
a pseudo pcsg is easily programmed  we add one extra argument  n  representing
the parent node  to the predicate q  i d  d  c  c  l  l   in figure    and replace
msw i c   j k   with msw  n i  c   j k    since the  leftmost  derivation of a sentence
from a pseudo pcsg is still a sequential decision process described by the modified program 
the graphical em algorithm applied to the support graphs generated from the modified
program and observed sentences correctly performs the ml estimation of parameters in the
pseudo pcsg 
a pseudo pcsg is thought to be a pcfg with rules of the form  n  i     i  j   i  k 
    n  i  j  k  n   where n is the parent nonterminal of i  so the arguments in the previous
subsection are carried over with minor changes  we therefore have  details omitted 

proposition     let db be a parameterized logic program for a pseudo pcsg with n

nonterminals as shown above  and g   g   g            gt the observed atoms representing
sampled sentences of length l  suppose each table operation in oldt search can be done
in o    time  then oldt search for g and each iteration in learn gem is completed in
o n   l t   time 

    bayesian networks

a relationship between cause c and its effect e is often probabilistic such as the one between diseases and symptoms  and as such it is mathematically captured as the conditional
probability p  e   e j c   c  of effect e given the cause c  what we wish to know however is
the inverse  i e  the probability of a candidate cause c given evidence e  i e  p  c   c j e   e 
which is calculated by bayes  theorem as p  e   e j c   c p  c   c   c  p  e   e j c  
c  p  c   c     bayesian networks are a representational computational framework that fits
best this type of probabilistic inference  pearl        castillo et al         
a bayesian network is a graphical representation of a joint distribution p  x    x           
xn   xn   of finitely many random variables x            xn   the graph is a dag  directed
acyclic graph  such as ones in figure     and each node is a random variable   
in the graph  a conditional probability table  cpt  representing p  xi   xi j  i   ui 
    i  n   is associated with each node xi where  i represents xi s parent nodes and ui
their values  when xi has no parent  i e  a topmost node in the graph  the table is just a
marginal distribution p  xi   xi   the whole joint distribution is defined as the product of
p

    we only deal with discrete cases 
   

fisato   kameya

a

b

a

d

c

b

c

e

d

f

e

  g     singly connected

f

  g     multiply connected

figure     bayesian networks
these conditional distributions 
p  x    x            xn

  xn      

n
y
i  

p  xi   xi j  i   ui   

    

thus the graph g  in figure    defines
pg  a  b  c  d  e  f     pg  a pg  b pg  c j a pg  d j a  b pg  e j d pg  f j d 
where a  b  c  d  e and f are values of corresponding random variables a  b  c   d  e
and f   respectively    as mentioned before  one of the basic tasks of bayesian networks
is to compute marginal probabilities  for example  the marginal distribution pg  c  d  is
computed either by      or      below 
pg  c  d   
pg  a pg  b pg  c j a pg  d j a  b pg  e j d p  f j d 
    
 

 

 

 

 

 

 

 

x

 

a b e f

 

 

 

 

 

 

x
 

a b

 

  

pg   a pg   b pg   c j a pg   d j a  b a  

 
x

e f

pg   e j d pg   f j d a     

     is clearly more ecient than       observe that if the graph were like g  in
figure     there would be no way to factorize computations like      but to use      requiring
exponentially many operations  the problem is that computing marginal probabilities is
np hard in general  and factorization such as      is assured only when the graph is singly
connected like g    i e  has no loop when viewed as an undirected graph  in such case  the
computation is possible in o jv j  time where v is the set of vertices in the graph  pearl 
       otherwise  the graph is called multiply connected  and might need exponential time
to compute marginal probabilities  in the sequel  we show the following 
 for any discrete bayesian network g defining a distribution pg  x            xn    there is a
parameterized logic program dbg for a predicate bn    such that pdb  bn x        xn   
  pg x            xn   
g

    thanks to the acyclicity of the graph  without losing generality  we may assume that if xi is an ancestor
node of xj   then i   j holds 
    for notational simplicity  we shall omit random variables when no confusion arises 
   

fiparameter learning of logic programs for symbolic statistical modeling

 for arbitrary factorizations and their order to compute a marginal distribution  there

exists a tabled program that accomplishes the same computation in the specified way 
 when the graph is singly connected and evidence e is given  there exists a tabled
program dbg such that oldt time for bn e  is o jv j   and hence the time
complexity per iteration of the graphical em algorithm is o jv j  as well 
let g be a bayesian network defining a joint distribution pg x            xn   and fpg  xi  
xi j  i   ui   j    i  n  xi   val xi    ui   val  i  g the conditional probabilities
associated with g where val xi  is the set of xi s possible values and val  i  denotes
the set of possible values of the parent nodes  i as a random vector  respectively  we
construct a parameterized logic program that defines the same distribution pg  x            xn   
our program dbg   fg   rg is shown in figure    


fg

  f msw par i ui  once xi  j    i  n  ui   val  i   xi   val xi  g

rg

  f bn x         xn    

 i  once xi   g

vn

i   msw par i 

figure     bayesian network program dbg
fg is comprised of msw atoms of the form msw par i ui   once xi   whose probability is
exactly the conditional probability pg  xi   xi j  i   ui   when xi has no parents  ui is
the empty list     rg is a singleton  containing only one clause whose body is a conjunction
of msw atoms which corresponds to the product of conditional probabilities  note that we
intentionally identify random variables x            xn with logical variables x            xn for
convenience 

proposition     dbg denotes the same distributions as g 

 proof  let hx            xn i be a realization of the random vector hx           xn i  it holds by
construction that
pdbg  bn x         xn   

 

n
y
h  
n
y

pmsw  msw par i ui   once xi   

 
pg  xi   xi j  i   ui  
h  
  pg  x            xn   
q e d 
in the case of g  in figure     the program becomes  
bn a b c d e f 

  

msw par  a      once a  
msw par  c   a   once c  
msw par  e   d   once e  

      a      b          are prolog constants used in place of integers 
   

msw par  b      once b  
msw par  d   a b   once d  
msw par  f   d   once f  

fisato   kameya

and the left to right sampling execution gives a sample realization of the random vector
h a  b  c  d  e  f i  a marginal distribution is computed from bn x         xn   by adding a new
clause to dbg  for example  to compute pg  c  d   we add bn c d    bn a b c d e f 
to dbg  let the result be db g   and then compute pdb   bn c d   which is equal to
pg  c  d  because
pdb   bn c d     pdb    a  b  e  f bn a b c d e f   
 
pdb  bn a b c d e f   
a b e f
  pg  c  d  
regrettably this computation corresponds to       not to the factorization       ecient
probability computation using factorization is made possible by carrying out summations
in a proper order 
we next sketch by an example how to carry out specified summations in a specified
order by introducing new clauses  suppose we have a joint distribution p  x  y  z  w   
   x  y     y  z  w   x  z  w  such that   x  y      y  z  w  and    x  z  w  are respectively
computed by atoms p   x y   p   y z w  and p   x z w   suppose also that we hope to
compute the sum
p  x      x  y  
   y  z  w     x  z  w 
 

 

 

g 

 

g 

x

g 

g 

 

x

x

y

z w

 

in which we first eliminate z  w and then y  corresponding to each elimination  we introduce
two new predicates  q x y  to compute    x  y    z w    y  z  w    x  z  w  and p x  to
compute p  x    y    x  y   x  y  as follows 
p

p

p x 
q x y 

   

p  x y   q x y  
p  y z w   p   x z w  

note that the clause body of q   contains z and w as  existentially quantified  local variables
and the clause head q x y  contains variables shared with other atoms  in view of the
correspondence between and    it is easy to confirm that this program realizes the
required computation  it is also easy to see by generalizing this example  though we do
not prove here  that there exists a parameterized logic program that carries out the given
summations in the given order for an arbitrary bayesian network  in particular we are
able to simulate ve  variable elimination  zhang   poole        d ambrosio        in our
approach 
ecient computation of marginal distributions is not always possible but there is a
well known class of bayesian networks  singly connected bayesian networks  for which there
exists an ecient algorithm to compute marginal distributions by message passing  pearl 
      castillo et al          we here show that when the graph is singly connected  we can
construct an ecient tabled bayesian network program dbg assigning a table predicate
to each node  to avoid complications  we explain the construction procedure informally
and concentrate on the case where we have only one interested variable  let g be a singly
p



   

fiparameter learning of logic programs for symbolic statistical modeling

connected graph  first we pick up a node u whose probability pg  u  is what we seek  we
construct a tree g with the root node u from g  by letting other nodes dangling from u  
figure    shows how g  is transformed to a tree when we select node b as the root node 
b

d

a

e

f

c


transformed graph g 

figure     transforming g  to a tree
then we examine each node in g one by one  we add for each node x in the graph a
corresponding clause to dbg whose purpose is to visit all nodes connected to x except the
one that calls x   suppose we started from the root node u  in figure    where evidence
u is given  and have generated clause       now we proceed to an inner node x  u  calls
x    in the original graph g  x has parent nodes fu    u   u g and child nodes fv    v  g  u 
is a topmost node in g 


u 
x
u 

v 
u 

v 

tree g 

figure     general situation
for node x in figure     we add clause       when it is called from the parent node
u  with u  being ground  we first generate possible values of u  by calling val u   u   
and then call call x u   u    to visit all nodes connected to x through u    u  is similary
treated  after visiting all nodes in g connecting to x through the parent nodes u  and
u   nodes connected to u  have already been visited   the value of random variable x is
determined by sampling the msw atom jointly indexed by  x  and the values of u    u  and
   

fisato   kameya
u    then we visit x  s children  v  and v    for a topmost node u  in the original graph 
we add clause      
tbn u       msw par  u        once u     call
call

u  x  u    

u  x  u       val u   u    call x u   u    
val u   u    call x u   u    
msw par  x   u   u   u     once x  
call x v   x   call x v  x  

    

    

    
let dbg be the final program containing clauses like            and       apparently
dbg can be constructed in time linear in the number of nodes in the network  also
note that successive unfolding  tamaki   sato        of atoms of the form call       
in the clause bodies that starts from      yields a program db g similar to the one in
figure    which contains msw atoms but no call        s  as dbg and db g define the
same distribution    it can be proved from proposition     that pg  u    pdb   bn u    
pdb  tbn u   holds  details omitted   by the way  in figure    we assume the construction
starts from the topmost node u  where the evidence u is given  but this is not necessary 
suppose we change to start from the inner node x  in that case  we replace clause     
with call x u  u       msw par  u       once u    just like       at the same time we
replace the head of clause      with tbn   and add a goal call x u   u  to the body
and so on  for the changed program db  g   it is rather straightforward to prove that
pdb    tbn      pg u  holds  it is true that the construction of the tabled program
dbg shown here is very crude and there is a lot of room for optimization  but it suces
to show that a parameterized logic program for a singly connected bayesian network runs
in o jv j  time where v is the set of nodes 
to estimate time complexity of oldt search w r t  dbg   we declare tbn and every
predicate of the form call        as table predicate and verify the five conditions on the
applicability of the graphical em algorithm  details omitted   we now estimate the time
complexity of oldt search for the goal tbn u  w r t  dbg     we notice that calls occur
according to the pre order scan  parents   the node   children  of the tree g   and calls
to call y x     occur val y   times  each call to call y x     invokes calls to the rest of
nodes  x  s parents and x  s children in the graph g except the caller node  with diffrent
set of variable instantiations  but from the second call on  every call only refers to solutions
stored in the solution table in o    time  thus  the number of added computation steps in
call

x u  u       msw par  u        once u    







g

g



g






    since distribution semantics is based on the least model semantics  and because unfold fold transformation  tamaki   sato        preserves the least herbrand model of the transformed program  unfold fold
transformation applied to parameterized logic programs preserves the denotation of the transformed
program 
    dbg is further transformed for the oldt interpreter to collect msw atoms like the case of the hmm
program 
   

fiparameter learning of logic programs for symbolic statistical modeling

oldt search by x is bounded from above  by constant o val u   val u   val u   val x   
in the case of figure     as a result oldt time is proportional to the number of nodes
in the original graph g  and so is the size of the support graph  provided that the number
of edges connecting to a node  and that of values of a random variable are bounded from
above  so we have

proposition     let g be a singly connected bayesian network defining distribution pg  

v the set of nodes  and dbg the tabled program derived as above  suppose the number of
edges connecting to a node  and that of values of a random variable are bounded from above
by some constant  also suppose table access can be done in o    time  then  oldt time
for computing pg u  for an observed value u of a random variable u by means of dbg is
o jv j  and so is time per iteration required by the graphical em algorithm  if there are t
observations  time complexity is o jv jt   

o jv j  is the time complexity required to compute a marignal distribution for a singly
connected bayesian network by a standard algorithm  pearl        castillo et al         
and also is that of the em algorithm using it  we therefore conclude that the graphical
em algorithm is as ecient as a specialzed em algorithm for singly connected bayesian
networks    we must also quickly add that the graphical em algorithm is applicable to
arbitrary bayesian networks    and what proposition     says is that an explosion of the
support graph can be avoided by appropriate programming in the case of singly connected
bayesian networks 
to summarize  the graphical em algorithm  a single generic em algorithm  is proved to
have the same time complexity as specialized em algorithms  i e  the baum welch algorithm
for hmms  the inside outside algorithm for pcfgs  and the one for singly connected
bayesian networks that have been developed independently in each research field 
table   summarizes the time complexity of em learning using oldt search and the
graphical em algorithm in the case of one observation  in the first column   sc bns 
represents singly connected bayesian networks  the second column shows a program to use 
dbh is an hmm proram in subsection      dbg  a pcfg program in subsection     and
dbg a transformed bayesian network program in subsection      respectively  oldt time
in the third column is time for oldt search to complete the search of all t explanations 
gem in the fourth column is time in one iteration taken by the graphical em algorithm
to update parameters  we use n   m   l and v respectively for the number of states in
an hmm  the number of nonterminals in a pcfg  the length of an input string and the
number of nodes in a bayesian network  the last column is a standard  specialized  em
algorithm for each model 


    when a marginal distribution of pg for more than one variable is required  we can construct a similar
tabled program that computes marginal probabilities still in o jv j  time by adding extra arguments
that convey other evidence or by embedding other evidnece in the program 
    we check the five conditions with dbg in figure     the uniqueness condition is obvious as sampling
always uniquely generates a sampled value for each random variable  the finite support condition is
satisfied because there are only a finite number of random variables and their values  the acyclic support
condition is immediate because of the acyclicity of bayesian networks  the t exclusiveness condition and
the independent condition are easy to verify 
   

fisato   kameya

model
program
hmms
dbh
pcfgs
dbg 
dbg
sc bns
user model


oldt time
o  n  l 
o  m   l    
o jv j 
o joldt treej 

gem
specialized em
 
o n l 
baum welch
 
 
o  m l  
inside outside
o jv j 
 castillo et al        
o  jsupport graphj 

table    time complexity of em learning by oldt search and the graphical em algorithm

    modeling language prism

we have been developing a symbolic statistical modeling laguage prism since       url
  http   mi cs titech ac jp prism   as an implementation of distribution semantics
 sato        sato   kameya        sato         the language is intented for modeling
complex symbolic statistical phenomena such as discourse interpretation in natural language
processing and gene inheritance interacting with social rules  as a programming language 
it looks like an extension of prolog with new built in predicates including the msw predicate
and other special predicates for manipulating msw atoms and their parameters 
a prism program is comprised of three parts  one for directives  one for modeling and
one for utilities  the directive part contains declarations such as values  telling the system
what msw atoms will be used in the execution  the modeling part is a set of non unit definite
clauses that define the distribution  denotation  of the program by using msw atoms  the
last part  the utility part  is an arbitary prolog program which refers to predicates defined
in the modeling part  we can use in the utility part learn built in predicate to carry out
em learning from observed atoms 
prism provides three modes of execution  the sampling execution correponds to a
random sampling drawn from the distribution defined by the modeling part  the second
one computes the probability of a given atom  the third one returns the support set for a
given goal  these execution modes are available through built in predicates 
we must report however that while the implementation of the graphical em algorithm
with a simpified oldt search mechanism has been under way  it is not completed yet  so
currently  only prolog search and learn naive db  g  in section   are available for em learning though we realized  partially  structrure sharing of explanations in the implemention
of learn naive db  g    putting computational eciecy aside however  there is no problem
in expressing and learning hmms  pcfgs  pseudo pcsgs  bayesian networks and other
probailistic models by the current version  the learning experiments in the next section
used a parser as a substitute for the oldt interpreter  and the independently implemented
graphical em algorithm 
   learning experiments

after complexity analysis of the graphical em algorithm for popular symbolic probabilistic
models in the previous section  we look at an actual behavior of the graphical em algorithm
with real data in this section  we conducted learning experiments with pcfgs using two
   

fiparameter learning of logic programs for symbolic statistical modeling

corpora which have contrasting characters  and compared the performance of the graphical
em algorithm against that of the inside outside algorithm in terms of time per iteration
   time for updating parameters   the results indicate that the graphical em algorithm
can outperform the inside outside algorithm by orders of magnigude  detalis are reported
by sato  kameya  abe  and shirai         before proceeding  we review the inside outside
algorithm for completeness 

    the inside outside algorithm

the inside outside algorithm was proposed by baker        as a generalization of the
baum welch algorithm to pcfgs  the algorithm is designed to estimate parameters for a
cfg grammar in chomsky normal form containing rules expressed by numbers like i   j  k
    i  j  k  n for n nonterminals  where   is a starting symbol   suppose an input
sentence w            wl is given  in each
iteration  it first computes in a bottom up manner
 
inside probabilities  e s  t  i    p  i   ws          wt  and then computes outside probabilities
f  s  t  i    p  s  
w            ws   i wt             wl   in a top down manner for every s  t and
i     s  t  l     i  n    after computing both probabilities  parameters are
updated by using them  and this process iterates until some predetermined criterion such
as a convergence of the likelihood of the input sentence is achieved  although baker did
not give any analysis of the inside outside algorithm  lari and young        showed that it
takes o n   l    time in one iteration and lafferty        proved that it is the em algorithm 
while it is true that the inside outside algorithm has been recognized as a standard em
algortihm for training pcfgs  it is notoriously slow  although there is not much literature
explicitly stating time required by the inside outside algorithm  carroll   rooth       
beil  carroll  prescher  riezler    rooth         beil et al         reported for example
that when they trained a pcfg with       rules for a corpus of         german subordinate clauses whose average ambiguity is       trees clause using four machines     mhz
sun ultrasparc   and    mhz sun ultrasparc ii     it took     hours to complete
one iteration  we discuss later why the inside outside algorithm is slow 

    learning experiments using two corpora

we report here parameter learning of existing pcfgs using two corpora of moderate size
and compare the graphical em algorithm against the inside outside algorithm in terms
of time per iteration  as mentioned before  support graphs  input to the garphical em
algorithm  were generated by a parser  i e  mslr parser    all measurements were made
on a    mhz sun ultrasparc ii with  gb memory under solaris     and the threshold
for an increase of the log likelihood of input sentences was set to      as a stopping criterion
for the em algorithms 
in the experiments  we used atr corpus and edr corpus  each converted to a pos
 part of speech  tagged corpus   they are similar in size  about         but contrasting in
their characters  sentence length and ambiguity of their grammars  the first experiment
employed atr corpus which is a japanese english corpus  we used only the japanese part 
developed by atr  uratani  takezawa  matsuo    morita         it contains        short
    mslr parser is a tomita  generalized lr  parser developed by tanaka tokunaga laboratory in tokyo
institute of technology  tanaka  takezawa    etoh        
   

fisato   kameya

conversational sentences  whose minimum length  average length and maximum length are
respectively         and     as a skeleton of pcfg  we employed a context free grammar
gatr comprising     rules      nonterminals and     terminals  manually developed for
atr corpus  tanaka et al         which yields     parses sentence 
because the inside outside algorithm only accepts a cfg in chomsky normal form  we
converted gatr into chomsky normal form g atr   g atr contains       rules      nonterminals and     terminals   we then divided the corpus into subgroups of similar length
like  l           l                   l            each containing randomly chosen     sentences 
after these preparations  we compare at each length the graphical em algorithm applied to
gatr and g atr against the inside outside algorithm applied to g atr in terms of time per
iteration by running them until convergence 
 sec 

 sec 

 sec 

  
i o
  

   

    

   

     

   

  

   

    

i o
gem  original 
gem  chomsky nf 

     
    

  
   
  

     

   

  

    

   

     

 

  

  

  

  

l

l

l
 

gem  original 
gem  chomsky nf 

 

 

  

  

  

  

 

 

           

figure     time per iteration   i o vs  gem  atr 
curves in figure    show the learning results where an x axis is the length l of an input
sentence and a y axis is average time taken by the em algorithm in one iteration to update
all parameters contained in the support graphs generated from the chosen     sentences
 other parameters in the grammar do not change   in the left graph  the inside outside
algorithm plots a cubic curve labeled  i o   we omitted a curve drawn by the graphical
em algorithm as it drew the x axis  the middle graph magnifies the left graph  the curve
labeled  gem  original   is plotted by the graphical em algorithm applied to the original
grammar gatr whereas the one labeled  gem  chomsky nf   used g atr  at length     the
average sentence length  it is measured that whichever grammar is employed  the graphical
em algorithm runs several hundreds times faster      times faster in the case of gatr
and     times faster in the case of g atr  than the inside outside algorithm per iteration 
the right graph shows  almost  linear dependency of updating time by the graphical em
algorithm within the measuared sentence length 
although some difference is anticipated in their learning speed  the speed gap between
the inside outside algorithm and the graphical em algorithm is unexpectedly large  the
most conceivable reason is that atr corpus only contains short sentences and gatr is not
   

fiparameter learning of logic programs for symbolic statistical modeling

much ambiguous so that parse trees are sparse and generated support graphs are small 
which affects favorably the perforamnce of the graphical em algorithm 
we therefore conducted the same experiment with another corpus which contains much
longer sentences using a more ambiguous grammar that generates dense parse trees  we
used edr japanese corpus  japan edr        containing         japanese news article
sentences  it is however under the process of re annotation  and only part of it  randomly
sampled       sentences  has recently been made available as a labeled corpus  compared
with atr corpus  sentences are much longer  the average length of       sentences is    
the minimum length    the maximum length     and a cfg grammar gedr        rules 
converted to chomsky normal form grammar g edr containing        rules  developed for
it is very ambiguous  to keep a coverage rate   having           parses sentence at length
   and            parses sentence at length    
 sec 

    

 sec 

 sec 
 

  

    
i o

 

i o
gem  original 

   

gem  original 

 

    

 
   

    

 
 

    

 

    
 

l
                      

 

   

l
                      

 

l
                      

figure     time per iteration   i o vs  gem  edr 
figure    shows the obtained curves from the experiments with edr corpus  the graphical em algorithm applied to gedr vs  the inside outside algorithm applied to g edr  under
the same condition as atr corpus  i e  plotting average time per iteration to process    
sentences of the designated length  except that the plotted time for the inside outside algorithm is the average of    iterations whereas that for the graphical em algorithm is the
average of     iterations  as is clear from the middle graph  this time again  the graphical
em algorithm runs orders of magnitude faster than the inside outside algorithm  at average sentence length     the former takes       second whereas the latter takes     seconds 
giving a speed ratio of       to    at sentence length     the former takes       seconds but
the latter takes       seconds  giving a speed ratio of       to    thus the speed ratio even
widens compared to atr corpus  this can be explained by the mixed effects of o l    
time complexity of the inside outside algorithm  and a moderate increase in the total size
of support graphs w r t  l  notice that the right graph shows how the total size of support
graphs grows with sentence length l as time per iteration by the graphical em algorithm
is linear in the total size of support graphs 

   

fisato   kameya

since we implemented the inside outside algorithm faithfully to baker         lari and
young         there is much room for improvement  actually kita gave a refined insideoutside algorithm  kita         there is also an implementation by mark johnson of the
inside outside algorithm down loadable from http   www cog brown edu   emj   the
use of such implementations may lead to different conclusions  we therefore conducted
learning experiments with the entire atr corpus using these two implementations and
measured updating time per iteration  sato et al          it turned out that both implementations run twice as fast as our naive implementation and take about     seconds per
iteration while the graphical em algorithm takes       second per iteration  which is still
orders of magnitude faster than the former two  regrettably a similar comparison using
the entire edr corpus available at the moment was abandoned due to memory overow
during parsing for the construction of support graphs 
learning experiments so far only compared time per iteration which ignore extra time
for search  parsing  required by the graphical em algorithm  so a question naturally arises
w r t  comparison in terms of total learning time  assuming     iterations for learning
atr corpus however  it is estimated that even considering parsing time  the graphical
em algorithm combined with mslr parser runs orders of magnitude faster than the three
implementations  ours  kita s and johnson s  of the inside outside algorithm  sato et al  
       of course this estimation does not directly apply to the graphical em algorithm
combined with oldt search  as the oldt interpreter will take more time than a parser
and how much more time is needed depends on the implementaiton of oldt search   
conversely  however  we may be able to take it as a rough indication of how far our approach 
the graphical em algorithm combined with oldt search via support graphs  can go in the
domain of em learning of pcfgs 

    examing the performance gap

in the previous subsection  we compared the performance of the graphical em algorithm
against the inside outside algorithm when pcfgs are given  using two corpora and three
implementations of the inside outside algorithm  in all experiments  the graphical em
algorithm considerably outperformed the inside outside algorithm despite the fact that
both have the same time complexity  now we look into what causes such a performance
gap 
simply put  the inside outside algorithm is slow  primarily  because it lacks parsing 
even when a backbone cfg grammar is explicitly given  it does not take any advantage of
the constraints imposed by the grammar  to see it  it might help to review how the inside
probability e s  t  a   i e  p nonterminal a spans from s th word to t th word   s  t   is
calculated by the inside outside algorithm for the given grammar 
e s  t  a   

r 
t  
x

p a   bc  e s  r  b e r      t  c  
s t  a bc in the grammar r s
here p a   bc   is a probability associated with a production rule a   bc   note that for
a fixed triplet  s  t  a   it is usual that the term p a   bc  e s  r  b  e r     t  c   is non zero
x

b c

    we cannnot answer this question right now as the implementation of oldt search is not completed 
   

fiparameter learning of logic programs for symbolic statistical modeling

only for a relatively small number of  b  c  r  s determined from successful parses and the
rest of combinations always give   to the term  nonetheless the inside outside algorithm
attempts to compute the term in every iteration for all possible combinations of b  c and
r and this is repeated for every possible  s  t  a   resulting in a lot of redundancy  the same
kind of redundancy occurs in the computation of outside probability by the inside outside
algorithm 
the graphical em algorithm is free of such redundancy because it runs on parse trees  a
parse forest  represented by the support graph    it must be added  on the other hand  that
superiority in learning speed of the graphical em algorithm is realized at the cost of space
complexity because while the inside outside algorithm merely requires o nl    space for
its array to store probabilities  the graphical em algorithm needs o n   l    space to store
the support graph where n is the number of nonterminals and l is the sentence length 
this trade off is understandable if one notices that the graphical em algorithm applied
to a pcfg can be considered as partial evaluation of the inside outside algorithm by the
grammar  and the introduction of appropriate data structure for the output  
finally we remark that the use of parsing as a preprocess for em learning of pcfgs is
not unique to the graphical em algorithm  fujisaki  jelinek  cocke  black    nishino       
stolcke         these approaches however still seem to contain redundancies compared with
the graphical em algorithm  for instance stolcke        uses an earley chart to compute
inside and outside probability  but parses are implicitly reconstructed in each iteration
dynamically by combining completed items 
   related work and discussion

    related work

the work presented in this paper is at the crossroads of logic programming and probability
theory  and considering an enormous body of work done in these fields  incompleteness is
unavoidable when reviewing related work  having said that  we look at various attempts
made to integrate probability with computational logic or logic programming    in reviewing  one can immediately notice there are two types of usage of probability  one type 
constraint approach  emphasizes the role of probability as constraints and does not necessarily seek for a unique probability distribution over logical formulas  the other type 
distribution approach  explicitly defines a unique distribution by model theoretical means
or proof theoretical means  to compute various probabilities of propositions 
a typical constraint approach is seen in the early work of probabilistic logic by nilsson
        his central problem   probabilistic entailment problem   is to compute the upper
and lower bound of probability p   of a target sentence  in such a way that the bounds
are compatible with a given knowledge base containing logical sentences  not necessarily
logic programs  annotated with a probability  these probabilities work as constraints on
    we emphasize that the difference between the inside outside algorithm and the graphical em algorithm
is solely computational eciency  and they converge to the same parameter values when starting from the
same initial values  linguistic evaluations of the estimated parameters by the graphical em algorithm
are also reported by sato et al         
    we omit literature leaning strongly toward logic  for logic s  concerning uncertainty  see an overview
by kyburg        
   

fisato   kameya

the possible range of p    he used the linear programming technique to solve this problem
that inevitably delimits the applicability of his approach to finite domains 
later lukasiewicz        investigated the computational complexity of the probabilistic
entailment problem in a slightly different setting  his knowledge base comprises statements
of the form  h j g  u    u    representing u   p h j g   u    he showed that inferring
 tight  u    u  is np hard in general  and proposed a tractable class of knowledge base called
conditional constraint trees 
after the inuential work of nilsson  frish and haddawy        introduced a deductive system for probabilistic logic that remedies  drawbacks  of nilsson s approach  that
of computational intractability and the lack of a proof system  their system deduces a
probability range of a proposition by rules of probabilistic inferences about unconditional
and conditional probabilities  for instance  one of the rules infers p  ff j        y  from
p  ff   fi j      x y   where ff fi and  are propositional variables and  x y   x  y   designates
a probability range 
turning to logic programming  probabilistic logic programming formalized by ng and
subrahmanian        and dekhtyar and subrahmanian        was also a constraint approach  their program is a set of annotated clauses of the form a    f               fn   n
where a is an atom  fi     i  n  a basic formula  i e  a conjunction or a disjunction of
atoms  and j     j  n  a sub interval of        indicating a probability range  a query
   f                fn   n  is answered by an extension of sld refutation  on formalization 
it is assumed that their language contains only a finite number of constant and predicate
symbols  and no function symbol is allowed 
a similar framework was proposed by lakshmanan and sadri        under the same syntactic restrictions  finitely many constant and predicate symbols but no function
symbols 
in a different uncertainty setting  they used annotated clauses of the form a c b            bn
where a and bi     i  n  are atoms and c   h ff  fi        i  a confidence level  represents
a belief interval  ff  fi       ff  fi     and a doubt interval                  which
an expert has in the clause 
as seen above  defining a unique probability distribution is of secondary or no concern
to the constraint approach  this is in sharp contrast with bayesian networks as the whole
discipline rests on the ability of the networks to define a unique probability distribution
 pearl        castillo et al          researchers in bayesian networks have been seeking for
a way of mixing bayesian networks with a logical representation to increase their inherently
propositional expressive power 
breese        used logic programs to automatically build a bayesian network from a
query  in breese s approach  a program is the union of a definite clause program and a set
of conditional dependencies of the form p p j q           qn   where p and qi s are atoms 
given a query  a bayesian network is constructed dynamically that connects the query and
relevant atoms in the program  which in turn defines a local distribution for the connected
atoms  logical variables can appear in atoms but no function symbol is allowed 
ngo and haddawy        extended breese s approach by incorporating a mechanism
reecting context  they used a clause of the form p a  j a            an     ff l            lk  
where ai s are called p atoms  probabilistic atoms  whereas lj  s are context atoms disjoint
from p atoms  and computed by another general logic program  satisfying certain restric   

fiparameter learning of logic programs for symbolic statistical modeling

tions   given a query  a set of evidence and context atoms  relevant ground p atoms are
identified by resolving context atoms away by sldnf resolution  and a local bayesian network is built to calculate the probability of the query  they proved the soundness and
completeness of their query evaluation procedure under the condition that programs are
acyclic   and domains are finite 
instead of defining a local distribution for each query  poole        defined a global distribution in his  probabilistic horn abduction   his program consists of definite clauses and
disjoint declarations of the form disjoint  h   p      hn pn   which specifies a probability distribution over the hypotheses  abducibles  fh           hn g  he assigned probabilities to
all ground atoms with the help of the theory of logic programming  and furthermore proved
that bayesian networks are representable in his framework  unlike previous approaches  his
language contains function symbols  but the acyclicity condition imposed on the programs
for his semantics to be definable seems to be a severe restriction  also  probabilities are not
defined for quantified formulas 
bacchus et al         used a much more powerful first order probabilistic language than
clauses annotated with probabilities  their language allows a statistically quantified term
such as k  x j x  kx to denote the ratio of individuals in a finite domain satisfying  x   
 x  to those satisfying  x   assuming that every world  interpretation for their language 
is equally likely  they define the probability of a sentence   under the given knowledge
   kb 

base kb as the limit limn      worlds
worlds  kb  where  worldsn    is the number of
possible worlds containing n individuals satisfying   and  parameters used in judging
approximations  although the limit does not necessarily exist and the domain must be finite 
they showed that their method can cope with diculties arising from  direct inference  and
default reasoning 
in a more linguistic vein  muggleton        and others  formulated slps  stochastic
logic programs  procedurally  as an extension of pcfgs to probabilistic logic programs 
so  a clause c   which must be range restricted    is annotated with a probability p like
p   c   the probability of a goal g is the product of such ps appearing in its refutation but
with a modification such that if a subgoal g can invoke n clauses  pi   ci     i  n  at
some refutation step  the probability of choosing k th clause is normalized to pk   ni   pi 
more recently  cussens              enriched slps by introducing a special class of
log linear models for sld refutations w r t  a given goal  he for example considers all
possible sld refutations for the most general goal s x   and defines probability p r 
of a refutation r as p r    z    exp  i i  r  i    here i is a number associated with
a clause ci and   r  i  is a feature  i e  the number of occurrences of ci in r  z is the
normalizing constant  then  the probability assigned to s a  is the sum of probabilities of
refutation for s a  



n

n



p

p

    the condition says that every ground atom a must be assigned a unique integer n a  such that n a   
n b             n bn   holds for any ground instance of a clause of the form a b            bn   under this
condition  when a program includes p x   q x  y    we cannot write recursive clauses about q such as
q  x   h jy    q x  y   
    a syntactic property that variables appearing in the head also appear in the body of a clause  a unit
clause must be ground 
   

fisato   kameya

    limitations and potential problems

approaches described so far have more or less similar limitations and potential problems 
descriptive power confined to finite domains is the most common limitation  which is due
to the use of the linear programming technique  nilsson         or due to the syntactic
restrictions not allowing for infinitely many constant  function or predicate symbols  ng
  subrahmanian        lakshmanan   sadri         bayesian networks have the same
limitation as well  only a finite number of random variables are representable     also there
are various semantic syntactic restrictions on logic programs  for instance the acyclicity
condition imposed by poole        and ngo and haddawy        prevents the unconditional
use of clauses with local variables  and the range restrictedness imposed by muggleton
       and cussens        excludes programs such as the usual membership prolog program 
there is another type of problem  the possibility of assigning conicting probabilities
to logically equivalent formulas  in slps  p a  and p a   a  do not necessarily coincide
because a and a   a may have different refutations  muggleton        cussens              
consequently in slps  we would be in trouble if we naively interpret p a  as the probability
of a s being true  also assigning probabilities to arbitrary quantified formulas seems out of
scope of both approaches to slps 
last but not least  there is a big problem common to any approach using probabilities 
where do the numbers come from  generally speaking  if we use n binary random variables in
a model  we have to determine  n probabilities to completely specify their joint distribution 
and fulfilling this requirement with reliable numbers quickly becomes impossible as n grows 
the situation is even worse when there are unobservable variables in the model such as
possible causes of a disease  apparently parameter learning from observed data is a natural
solution to this problem  but parameter learning of logic programs has not been well studied 
distribution semantics proposed by sato        was an attempt to solve these problems
along the line of the global distribution approach  it defines a distribution  probability
measure  over the possible interpretations of ground atoms for an arbitrary logic program
in any first order language and assigns consistent probabilities to all closed formulas  also
distribution semantics enabled us to derive an em algorithm for the parameter learning of
logic programs for the first time  as it was a naive algorithm however  dealing with large
problems was dicult when there are exponentially many explanations for an observation
like hmms  we believe that the eciency problem is solved to a large extent by the
graphical em algorithm presented in this paper 

    em learning

since em learning is one of the central issues in this paper  we separately mention work
related to em learning for symbolic frameworks  koller and pfeffer        used in their
approach to kbmc  knowledge based model construction  em learning to estimate parameters labeling clauses  they express probabilistic dependencies among events by definite clauses annotated with probabilities  similarly to ngo and haddawy s        approach 
and locally build a bayesian network relevant to the context and evidence as well as the
    however  rpms  recursive probability models  proposed by pfeffer and koller        as an extension
of bayesian networks allow for infinitely many random variables  they are organized as attributes of
classes and a probability measure over attribute values is introduced 
   

fiparameter learning of logic programs for symbolic statistical modeling

query  parameters are learned by applying to the constructed network the specialized em
algorithm for bayesian networks  castillo et al         
dealing with a pcfg by a statically constructed bayesian network was proposed pynadath and wellman         and it is possible to combine the em algorithm with their method
to estimate parameters in the pcfg  unfortunately  the constructed network is not singly
connected  and time complexity of probability computation is potentially exponential in the
length of an input sentence 
closely related to our em learning is parameter learning of log linear models  riezler        proposed the im algorithm in his approach to probabilistic constraint programming  the im algorithm is a general parameter estimation algorithm from incomplete data for
log linear models whose probability function p x  takes the form p x   
z    exp  ni   i i  x   p   x  where              n   are parameters to be estimated  i  x  the
i th feature of an observed object x and z the normalizing constant  since a feature can
be any function of x  the log linear model is highly exible and includes our distribution
pmsw as a special case of z      there is a price to pay however  the computational cost
of z   it requires a summation over exponentially many terms  to avoid the cost of exact
computation  approximate computation by a monte carlo method is possible  whichever
one may choose however  learning time increases compared to the em algorithm for z     
the fam  failure adjusted maximization  algorithm proposed by cussens        is an
em algorithm applicable to pure normalized slps that may fail  it deals with a special
class of log linear models but is more ecient than the im algorithm  because the statistical
framework of the fam is rather different from distribution semantics  comparison with the
graphical em algorithm seems dicult 
being slightly tangential to em learning  koller et al         developed a functional
modeling language defining a probability distribution over symbolic structures in which
they showed  cashing  of computed results leads to ecient probability computation of
singly connected bayesian networks and pcfgs  their cashing corresponds to the computation of inside probability in the inside outside algorithm and the computation of outside
probability is untouched 
p

    future directions

parameterized logic programs are expected to be a useful modeling tool for complex symbolicstatistical phenomena  we have tried various types of modeling  besides stochastic grammars and bayesian networks  such as the modeling of gene inheritance in the kariera tribe
 white        where the rules of bi lateral cross cousin marriage for four clans interact with
the rules of genetic inheritance  sato         the model was quite interdisciplinary  but
the exibility of combining msw atoms by means of definite clauses greatly facilitated the
modeling process 
although satisfying the five conditions in section  
 the uniqueness condition  roughly  one cause yields one effect 
 the finite support condition  there are a finite number of explanations for one observation 
 the acyclic support condition  explanations must not be cyclic 
   

fisato   kameya

 the t exclusiveness condition  explanations must be mutually exclusive 
 the independence condition  events in an explanation must be independent 

for the applicability of the graphical em algorithm seems daunting  our modeling experiences so far tell us that the modeling principle in section   effectively guides us to successful
modeling  in return  we can obtain a declarative model described compactly by a high level
language whose parameters are eciently learnable by the graphical em algorithm as shown
in the preceding section 
one of the future directions is however to relax some of the applicability conditions 
especially the uniqueness condition that prohibits a generative model from failure or from
generating multiple observable events  although we pointed out in section     that the mar
condition in appendix b adapted to our semantics can replace the uniqueness condition and
validates the use of the graphical em algorithm even when a complete data does not uniquely
determine the observed data just like the case of  partially bracketed corpora   pereira  
schabes         we feel the need to do more research on this topic  also investigating
the role of the acyclicity condition seems theoretically interesting as the acyclicity is often
related to the learning of logic programs  arimura        reddy   tadepalli        
in this paper we only scratched the surface of individual research fields such as hmms 
pcfgs and bayesian networks  therefore  there remains much to be done about clarifying
how experiences in each research field are reected in the framework of parameterized logic
programs  for example  we need to clarify the relationship between symbolic approaches
to bayesian networks such as spi  li  z    d ambrosio  b         and our approach 
also it is unclear how a compiled approach using the junction tree algorithm for bayesian
networks can be incorporated into our approach  aside from exact methods  approximate
methods of probability computation specialized for parameterized logic programs must also
be developed 
there is also a direction of improving learning ability by introducing priors instead of ml
estimation to cope with data sparseness  the introduction of basic distributions that make
probabilistic switches correlated seems worth trying in the near future  it is also important
to take advantage of the logical nature of our approach to handle uncertainty  for example 
it is already shown by sato        that we can learn parameters from negative examples
such as  the grass is not wet  but the treatment of negative examples in parameterized
logic programs is still in its infancy 
concerning developing complex statistical models based on the  programs as distributions  scheme  stochastic natural language processing which exploits semantic information
seems promising  for instance  unification based grammars such as hpsgs  abney       
may be a good target beyond pcfgs because they use feature structures logically describable  and the ambiguity of feature values seems to be expressible by a probability
distribution 
also building a mathematical basis for logic programs with continuous random variables
is a challenging research topic 

   

fiparameter learning of logic programs for symbolic statistical modeling
   conclusion

we have proposed a logical mathematical framework for statistical parameter learning of
parameterized logic programs  i e  definite clause programs containing probabilistic facts
with a parameterized probability distribution  it extends the traditional least herbrand
model semantics in logic programming to distribution semantics   possible world semantics
with a probability distribution over possible worlds  herbrand interpretations  which is
unconditionally applicable to arbitrary logic programs including ones for hmms  pcfgs
and bayesian networks 
we also have presented a new em algorithm  the graphical em algorithm in section   
which learns statistical parameters from observations for a class of parameterized logic programs representing a sequential decision process in which each decision is exclusive and
independent  it works on support graph s  a new data structure specifying a logical relationship between an observed goal and its explanations  and estimates parameters by computing
inside and outside probability generalized for logic programs 
the complexity analysis in section   showed that when oldt search  a complete tabled
refutation method for logic programs  is employed for the support graph construction and
table access is done in o    time  the graphical em algorithm  despite its generality  has
the same time complexity as existing em algorithms  i e  the baum welch algorithm for
hmms  the inside outside algorithm for pcfgs and the one for singly connected bayesian
networks that have been developed independently in each research field  in addition  for
pseudo probabilistic context sensitive grammars with n nonterminals  we showed that the
graphical em algorithm runs in time o n   l   for a sentence of length l 
to compare actual performance of the graphical em algorithm against the insideoutside algorithm  we conducted learning experiments with pcfgs in section   using two
real corpora with contrasting characters  one is atr corpus containing short sentences for
which the grammar is not much ambiguous      parses sentence   and the other is edr
corpus containing long sentences for which the grammar is rather ambiguous           
at average sentence length      in both cases  the graphical em algorithm outperformed
the inside outside algorithm by orders of magnitude in terms of time per iteration  which
suggests the effectiveness of our approach to em learning by the graphical em algorithm 
since our semantics is not limited to finite domains or finitely many random variables
but applicable to any logic programs of arbitrary complexity  the graphical em algorithm is
expected to give a general yet ecient method of parameter learning for models of complex
symbolic statistical phenomena governed by rules and probabilities 
acknowledgments

the authors wish to thank three anonymous referees for their comments and suggestions 
special thanks go to takashi mori and shigeru abe for stimulating discussions and learning
experiments  and also to tanaka tokunaga laboratory for kindly allowing them to use
mslr parser and the linguistic data 

   

fisato   kameya
appendix a  properties of

pdb

in this appendix  we list some properties of pdb defined by a parameterized logic program
db   f   r in a countable first order language l    first of all  pdb assigns consistent
probabilities   to every closed formula  in l by
pdb    def
  pdb  f    
db j   j  g 
while guaranteeing continuity in the sense that
limn   pdb   t              tn     pdb   x x  
limn   pdb   t              tn     pdb   x x  
where t    t          is an enumeration of ground terms in l 
the next proposition  proposition a    relates pdb to the herbrand model  to prove
it  we need some terminology  a factor is a closed formula in prenex disjunctive normal
form q        qnm where qi     i  n  is either an existential quantification or a universal
quantification and m a matrix  the length of quantifications n is called the rank of the
factor  define   as a set of formulas made out of factors  conjunctions and disjunctions 
associate with each formula  in   a multi set r   of ranks by
 
if  is a factor with no quantification
r     
fng
if  is a factor with rank n
r      r      if          or          
here   stands for the union of two multi sets  for instance f       g f       g   f                g 
we use the multi set ordering in the proof of proposition a   because the usual induction
on the complexity of formulas does not work 
lemma a   let  be a boolean formula made out of ground atoms in l  pdb    
pf  f   
f j mdb     j  g  
 proof  we have only to prove the lemma about a conjunction of atoms of the form d x  
       dnx  xi   f    g     i  n  
pdb  d x          dnx     pdb  f    
db j   j  d x          dnx g 
  pdb  d    x            dn   xn 
  pf  f   
f j mdb     j  d x          dnx g  q e d 
 
 
 
 
 

 

n

 

n

n

 

 

n

proposition a   let  be a closed formula in l  pdb     pf  f   
f j mdb    j  g  
    for definitions of 
f   pf   mdb      
db   pdb and others used below  see section   
    by consistent  we mean probabilities assigned to logical formulas respect the laws of probability such as
   p  a      p   a        p  a  and p  a   b     p  a    p  b     p  a   b   
   

fiparameter learning of logic programs for symbolic statistical modeling

 proof  recall that a closed formula has an equivalent prenex disjunctive normal form
that belongs to    we prove the proposition for formulas in   by using induction on the
multi set ordering over fr   j     g  if r         has no quantification  so the
proposition is correct by lemma a    suppose otherwise  write    g q  q       qn f  
where q  q       qnf indicates a single occurrence of a factor in g    we assume q     x
 q     x is similarly treated   we also assume that bound variables are renamed to avoid
name clash  then g  xq       qn f   is equivalent to  xg q       qnf   in light of the validity
of   xa    b    x a   b  and   xa    b    x a   b  when b contains no free x 
pdb      pdb  g q  q       qnf   
  pdb   xg  q       qn f  x   
  klim
p  g  q       qn f  t              g q       qn f  tk    
   db
  klim
p  g  q       qn f  t             q       qn f  tk    
   db
  klim
p  f   
f j mdb     j  g  q       qnf  t            q       qnf  tk    g 
   f
 by induction hypothesis 
  pf  f   
f j mdb     j   xg q       qnf  x  g 
  pf  f   
f j mdb     j  g 
q e d 
we next prove a theorem on the iff definition introduced in section    distribution
semantics considers the program db   f   r as a set of infinitely many ground definite
clauses such that f is a set of facts  with a probability measure pf   and r a set of rules 
and no clause head in r appears in f   put
head r  def
  fb j b appears in r as a clause headg 
for b   head r   let b wi  i                be an enumeration of clauses about b in r 
define iff  b   the iff  if and only if  form of rules about b in db   by
iff  b  def
  b   w    w         
since mdb     is a least herbrand model  the following is obvious 
lemma a   for b in head r  and    
f   mdb    j  iff  b  
theorem a   below is about iff  b   it states that at general level  both sides of the iff
definition p x     y   x   t    w              yn  x   tn   wn  of p    coincide as random
variables whenever x is instantiated to a ground term 
theorem a   let iff  b     b   w    w        be the iff form of rules about b   head r  
pdb  iff  b        and pdb  b     pdb  w    w          
    for an expression e   e     means that  may occur in the specified positions of e   if       in e         
indicates a single occurrence of       in a positive boolean formula e   e            e        e      holds 
    this definition is different from the usual one  lloyd        doets        as we are here talking at ground
level  w    w          is true if and only if one of the disjuncts is true 
   

fisato   kameya

 proof 

pdb  iff  b   

 

pdb  f    
db j   j  b    w    w         g 
 pdb  f    
db j   j   b     w    w         g 

  klim
p  f    
db j   j  b  
   db

k
 

i  

wi g 

  klim
p  f    
db j   j   b    
   db
  klim
p  f   
f j mdb     j  b  
   f

k
 
i  

k
 
i  

wi g 

wi g 
k
 

  klim
p  f   
f j mdb     j   b     wi g 
   f
i  
 lemma a   
  pf  f   
f j mdb     j  iff  b g 
  pf  
f    lemma a   
   
it follows from pdb  iff  b       that
pdb  b     pdb  b   iff b      pdb  w    w           
q e d 
we then prove a proposition useful in probability computation  let db  b   be the
support set for an atom b introduced in section    it is the set of all explanations for b   in
the sequel  b is a ground atom  write db  b    fs    s        g and db  b    s   s         
define a set  b by
 b def
  f    
db j   j  b   db  b g 
proposition a   for every b   head r   pdb  b       and pdb  b    pdb  db  b   
 proof  we first prove pdb   b       but the proof exactly parallels that of theorem a  
except that w    w          is replaced by s    s         using the fact that b   s    s        
is true in every least herbrand model of the form mdb      then from pdb   b        we
have
pdb  b     pdb  b    b  
db  b    
  pdb   db  b   
q e d 
finally  we show that distribution semantics is a probabilistic extension of the traditional
least herbrand model semantics in logic programming by proving theorem a    it says that
the probability mass is distributed exclusively over possible least herbrand models 
define   as the set of least herbrand models generated by fixing r and varying a subset
of f in the program db   f   r  in symbols 
w

 

w

 

 

w

    for a set k   fe    e         g of formulas  k denotes a   n infinite  disjunction e    e         
   

fiparameter learning of logic programs for symbolic statistical modeling

  def
  f    
db j     mdb    for some    
f g 
note that as   is merely a subset of 
db   we cannot conclude pdb         a priori  but the
next theorem  theorem a    states pdb          i e  distribution semantics distributes the
probability mass exclusively over    i e  possible least herbrand models 
to prove the theorem  we need some preparations  recalling that atoms outside head r  
f have no chance of being proved from db  we introduce
   def
  f    
db j   j   d for every ground atom d    head r    f g 
for a herbrand interpretation     
db    jf    
f   is the restriction of   to those atoms
in f  
lemma a   let     
db be a herbrand
interpretation 
 
    mdb     for some    
f iff       and   j  b   db  b   for every b   head r  
 proof  only if part is immediate from the property of the least herbrand model  for
if part  suppose   satisfies the right hand side  we show that     mdb   jf    as   and
mdb    jf   coincide w r t  atoms not in head r   it is enough to prove that they also give
the same truth values to atoms in head r   take b   head r  and write db  b   
s    s         suppose   j  b   s    s         then if   j  b   we have   j  sj for some j  
thereby  jf j  sj   and hence mdb   jf   j  sj   which implies mdb   jf   j  b  otherwise
  j   b   so   j   sj for every j   it follows that mdb    jf   j   b   since b is arbitrary 
we conclude that   and mdb   jf   agree on the truth values assigned to atoms in head r 
as well 
q e d 
w

w

theorem a   pdb        

 proof  from lemma a    we have
    f    
db j     mdb     for some    
f g
      
 b  
 

b head r 

pdb   b       by proposition a    to prove pdb            let d   d          be an enumeration
of atoms not belonging to head r    f   they are not provable from db   f   r  and
hence false in every least herbrand model mdb         
f    so
pdb      

  mlim
   pdb  f    
db j   j   d            dm g 
  mlim
   pf  f   
f j mdb     j   d             dm g 
  pf  
f       
since a countable conjunction of measurable sets of probability measure one has also
probability measure one  it follows from pdb   b       for every b   head r  and pdb       
  that pdb         
q e d 
   

fisato   kameya
appendix b  the mar  missing at random  condition

in the original formulation of the em algorithm by dempster et al          it is assumed
that there exists a many to one mapping y    x  from a complete data x to an incomplete
 observed  data y  in the case of parsing  x is a parse tree and y is the input sentence and x
uniquely determines y  in this paper  the uniqueness condition ensures the existence of such
a many to one mapping from explanations to observations  we however sometimes face a
situation where there is no such many to one mapping from complete data to incomplete
data but nonetheless we wish to apply the em algorithm 
this dilemma can be solved by the introduction of a missing data mechanism which
makes a complete data incomplete  the missing data mechanism  m  has a distribution
g  m j x  parameterized by  and y   the observed data  is described as y   m  x   it says
x becomes incomplete y by m  the correspondence between x and y   i e  fhx  y i j  m y  
m  x  g naturally becomes many to many 
rubin        derived two conditions on g  data are missing at random and data are
observed at random  collectively called the mar  missing at random  condition  and showed
that if we assume a missing data mechanism behind our observations that satisfies the mar
condition  we may estimate parameters of the distribution over x by simply applying the
em algorithm to y  the observed data 
we adapt the mar condition to parameterized logic programs as follows  we keep a
generative model satisfying the uniqueness condition that outputs goals g such as parse
trees  we further extend the model by additionally inserting a missing data mechanism
m between g and our observation o like o   m  g  and assume m satisfies the mar
condition  then the extended model has a many to many correspondence between explanations and observations  and generates non exclusive observations such that p  o   o       
 o    o     which causes o p  o     where p  o    g  m o   g  pdb  g   thanks to
the mar condition however  we are still allowed to apply the em algorithm to such nonexclusive observations  put it differently  even if the uniqueness condition is seemingly
destroyed  the em algorithm is applicable just by  imaginarily  assuming a missing data
mechanism satisfying the mar condition 
p

p

m

references

abney  s          stochastic attribute value grammars  computational linguistics         
        
arimura  h          learning acyclic first order horn sentences from entailment  in
proceedings of the eighth international workshop on algorithmic learning theory 
ohmsha springer verlag 
bacchus  f   grove  a   halpern  j     koller  d          from statistical knowledge bases
to degrees of belief  artificial intelligence             
baker  j  k          trainable grammars for speech recognition  in proceedings of spring
conference of the acoustical society of america  pp          

   

fiparameter learning of logic programs for symbolic statistical modeling

beil  f   carroll  g   prescher  d   riezler  s     rooth  m          inside outside estimation
of a lexicalized pcfg for german  in proceedings of the   th annual meeting of the
association for computational linguistics  acl      pp          
breese  j  s          construction of belief and decision networks  computational intelligence                 
carroll  g     rooth  m          valence induction with a head lexicalized pcfg  in proceedings of the  rd conference on empirical methods in natural language processing
 emnlp    
castillo  e   gutierrez  j  m     hadi  a  s          expert systems and probabilistic
network models  springer verlag 
charniak  e     carroll  g          context sensitive statistics for improved grammatical language models  in proceedings of the   th national conference on artificial
intelligence  aaai      pp          
chi  z     geman  s          estimation of probabilistic context free grammars  computational linguistics                  
chow  y     teicher  h          probability theory   rd ed    springer 
clark  k          negation as failure  in gallaire  h     minker  j   eds    logic and
databases  pp           plenum press 
cormen  t   leiserson  c     rivest  r          introduction to algorithms  the mit press 
cussens  j          loglinear models for first order probabilistic reasoning  in proceedings of
the   th conference on uncertainty in artificial intelligence  uai      pp          
cussens  j          parameter estimation in stochastic logic programs  machine learning 
                
d ambrosio  b          inference in bayesian networks  ai magazine  summer        
dekhtyar  a     subrahmanian  v  s          hybrid probabilistic programs  in proceedings
of the   th international conference on logic programming  iclp      pp          
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood from incomplete
data via the em algorithm  royal statistical society  b             
doets  k          from logic to logic programming  the mit press 
flach  p     kakas  a   eds            abduction and induction   essays on their relation
and integration  kluwer academic publishers 
frish  a     haddawy  p          anytime deduction for probabilistic logic  journal of
artificial intelligence             
fujisaki  t   jelinek  f   cocke  j   black  e     nishino  t          a probabilistic parsing
method for sentence disambiguation  in proceedings of the  st international workshop
on parsing technologies  pp        
japan edr  l          edr electronic dictionary technical guide   nd edition   technical
report  japan electronic dictionary research institute  ltd 
   

fisato   kameya

kakas  a  c   kowalski  r  a     toni  f          abductive logic programming  journal
of logic and computation                 
kameya  y          learning and representation of symbolic statistical knowledge  in
japanese   ph  d  dissertation  tokyo institute of technology 
kameya  y     sato  t          ecient em learning for parameterized logic programs 
in proceedings of the  st conference on computational logic  cl       vol       of
lecture notes in artificial intelligence  pp           springer 
kita  k          probabilistic language models  in japanese   tokyo daigaku syuppan kai 
koller  d   mcallester  d     pfeffer  a          effective bayesian inference for stochastic programs  in proceedings of   th national conference on artificial intelligence
 aaai      pp          
koller  d     pfeffer  a          learning probabilities for noisy first order rules  in proceedings of the   th international joint conference on artificial intelligence  ijcai     
pp            
kyburg  h          uncertainty logics  in gabbay  d   hogger  c     robinson  j   eds   
handbook of logics in artificial intelligence and logic programming  pp          
oxford science publications 
lafferty  j          a derivation of the inside outside algorithm from the em algorithm 
technical report  ibm t j watson research center 
lakshmanan  l  v  s     sadri  f          probabilistic deductive databases  in proceedings
of the      international symposium on logic programming  ilps      pp          
lari  k     young  s  j          the estimation of stochastic context free grammars using
the inside outside algorithm  computer speech and language           
li  z     d ambrosio  b          ecient inference in bayes networks as a combinatorial
optimization problem  international journal of approximate reasoning            
lloyd  j  w          foundations of logic programming  springer verlag 
lukasiewicz  t          probabilistic deduction with conditional constraints over basic
events  journal of artificial intelligence research              
manning  c  d     schutze  h          foundations of statistical natural language processing  the mit press 
mclachlan  g  j     krishnan  t          the em algorithm and extensions  wiley
interscience 
muggleton  s          stochastic logic programs  in de raedt  l   ed    advances in
inductive logic programming  pp           ios press 
ng  r     subrahmanian  v  s          probabilistic logic programming  information and
computation               
ngo  l     haddawy  p          answering queries from context sensitive probabilistic
knowledge bases  theoretical computer science               
nilsson  n  j          probabilistic logic  artificial intelligence            
   

fiparameter learning of logic programs for symbolic statistical modeling

pearl  j          probabilistic reasoning in intelligent systems  morgan kaufmann 
pereira  f  c  n     schabes  y          inside outside reestimation from partially bracketed
corpora  in proceedings of the   th annual meeting of the association for computational linguistics  acl      pp          
pereira  f  c  n     warren  d  h  d          definite clause grammars for language analysis
  a survey of the formalism and a comparison with augmented transition networks 
artificial intelligence              
pfeffer  a     koller  d          semantics and inference for recursive probability models  in
proceedings of the seventh national conference on artificial intelligence  aaai     
pp          
poole  d          probabilistic horn abduction and bayesian networks  artificial intelligence                 
pynadath  d  v     wellman  m  p          generalized queries on probabilistic context free
grammars  ieee transaction on pattern analysis and machine intelligence         
      
rabiner  l  r          a tutorial on hidden markov models and selected applications in
speech recognition  proceedings of the ieee                  
rabiner  l  r     juang  b          foundations of speech recognition  prentice hall 
ramakrishnan  i   rao  p   sagonas  k   swift  t     warren  d          ecient tabling
mechanisms for logic programs  in proceedings of the   th international conference
on logic programming  iclp      pp           the mit press 
reddy  c     tadepalli  p          learning first order acyclic horn programs from entailment  in proceedings of the   th international conference on machine learning 
 and proceedings of the  th international conference on inductive logic programming   morgan kaufmann 
riezler  s          probabilistic constraint logic programming  ph d  thesis  universitat
tubingen 
rubin  d          inference and missing data  biometrika                  
sagonas  k   t   s     warren  d          xsb as an ecient deductive database engine 
in proceedings of the      acm sigmod international conference on management
of data  pp          
sato  t          a statistical learning method for logic programs with distribution semantics 
in proceedings of the   th international conference on logic programming  iclp     
pp          
sato  t          modeling scientific theories as prism programs  in proceedings of ecai   
workshop on machine discovery  pp        
sato  t          minimum likelihood estimation from negative examples in statistical abduction  in proceedings of ijcai    workshop on abductive reasoning  pp        
sato  t     kameya  y          prism  a language for symbolic statistical modeling 
in proceedings of the   th international joint conference on artificial intelligence
 ijcai      pp            
   

fisato   kameya

sato  t     kameya  y          a viterbi like algorithm and em learning for statistical
abduction  in proceedings of uai     workshop on fusion of domain knowledge
with data for decision support 
sato  t   kameya  y   abe  s     shirai  k          fast em learning of a family of pcfgs 
titech technical report  dept  of cs  tr         tokyo institute of technology 
shen  y   yuan  l   you  j     zhou  n          linear tabulated resolution based on prolog
control strategy  theory and practice of logic programming                
sterling  l     shapiro  e          the art of prolog  the mit press 
stolcke  a          an ecient probabilistic context free parsing algorithm that computes
prefix probabilities  computational linguistics                  
tamaki  h     sato  t          unfold fold transformation of logic programs  in proceedings
of the  nd international conference on logic programming  iclp      lecture notes
in computer science  pp           springer 
tamaki  h     sato  t          old resolution with tabulation  in proceedings of the  rd
international conference on logic programming  iclp      vol      of lecture notes
in computer science  pp         springer 
tanaka  h   takezawa  t     etoh  j          japanese grammar for speech recognition
considering the mslr method  in proceedings of the meeting of sig slp  spoken
language processing      slp        pp           information processing society of
japan  in japanese 
uratani  n   takezawa  t   matsuo  h     morita  c          atr integrated speech and
language database  technical report tr it       atr interpreting telecommunications research laboratories  in japanese 
warren  d  s          memoing for logic programs  communications of the acm         
       
wetherell  c  s          probabilistic languages  a review and some open questions  computing surveys                  
white  h  c          an anatomy of kinship  prentice hall 
zhang  n     poole  d          exploiting causal independence in bayesian network inference  journal of artificial intelligence research             

   

fi