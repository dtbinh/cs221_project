journal artificial intelligence research                  

submitted       published      

parameter learning logic programs
symbolic statistical modeling

taisuke sato
yoshitaka kameya

sato mi cs titech ac jp
kame mi cs titech ac jp

dept  computer science  graduate school information
science engineering  tokyo institute technology
       ookayama meguro ku tokyo japan         

abstract

propose logical mathematical framework statistical parameter learning parameterized logic programs  i e  definite clause programs containing probabilistic facts
parameterized distribution  extends traditional least herbrand model semantics
logic programming distribution semantics   possible world semantics probability
distribution unconditionally applicable arbitrary logic programs including ones
hmms  pcfgs bayesian networks 
propose new em algorithm  graphical em algorithm  runs
class parameterized logic programs representing sequential decision processes
decision exclusive independent  runs new data structure called support graph
describing logical relationship observations explanations  learns
parameters computing inside outside probability generalized logic programs 
complexity analysis shows combined oldt search explanations observations  graphical em algorithm  despite generality 
time complexity existing em algorithms  i e  baum welch algorithm hmms 
inside outside algorithm pcfgs  one singly connected bayesian networks
developed independently research field  learning experiments
pcfgs using two corpora moderate size indicate graphical em algorithm
significantly outperform inside outside algorithm 
   introduction

parameter learning common various fields neural networks reinforcement learning statistics  used tune systems best performance  classifiers
statistical models  unlike numerical systems described mathematical formulas however  symbolic systems  typically programs  seem amenable kind
parameter learning  actually little literature parameter learning programs 
paper attempt incorporate parameter learning computer programs 
reason twofold  theoretically wish add ability learning computer
programs  authors believe necessary step toward building intelligent systems 
practically broadens class probability distributions  beyond traditionally used numerical ones  available modeling complex phenomena gene inheritance 
consumer behavior  natural language processing on 
c      ai access foundation morgan kaufmann publishers  rights reserved 

fisato   kameya

type learning consider statistical parameter learning applied logic
programs   assume facts  unit clauses  program probabilistically true
parameterized distribution   clauses  non unit definite clauses  always
true encode laws  if one pair blood type genes b  one s
blood type ab   call logic programs type parameterized logic program
use statistical modeling ground atoms  provable program represent
observations  one s blood type ab  parameters program
inferred performing ml  maximum likelihood  estimation observed atoms 
probabilistic first order framework sketched termed statistical abduction
 sato   kameya        amalgamation statistical inference abduction
probabilistic facts play role abducible s  i e  primitive hypotheses   statistical
abduction powerful subsumes diverse symbolic statistical frameworks
hmms  hidden markov models  rabiner         pcfgs  probabilistic context free
grammars  wetherell        manning   schutze         discrete  bayesian networks
 pearl        castillo  gutierrez    hadi        gives us freedom using arbitrarily
complex logic programs modeling  
semantic basis statistical abduction distribution semantics introduced sato
        defines parameterized distribution  actually probability measure  set
possible truth assignments ground atoms enables us derive new em algorithm 
ml estimation called graphical em algorithm  kameya   sato        
parameter learning statistical abduction done two phases  search em learning  given parameterized logic program observations  first phase searches
explanations observations  redundancy first phase eliminated tabulating
partial explanations using oldt search  tamaki   sato        warren        sagonas  t  
  warren        ramakrishnan  rao  sagonas  swift    warren        shen  yuan  you   
zhou         returns support graph compact representation discovered
explanations  second phase  run graphical em algorithm support graph
   paper  logic programs mean definite clause programs  definite clause program set definite
clauses  definite clause clause form l            ln    n  a  l            ln atoms 
called head  l            ln body  variables universally quantified  reads l 
      ln hold  holds  case n      clause called unit clause  general clause
one whose body may contain negated atoms  program including general clauses sometimes called
general program  lloyd        doets        
   throughout paper  familiarity readability  somewhat loosely use  distribution 
synonym  probability measure  
   logic programming  adjective  ground  means variables contained 
   abduction means inference best explanation set observations  logically  formalized
search explanation e e  kb   g g atom representing observation  kb
knowledge base e conjunction atoms chosen abducible s  i e  class formulas allowed
primitive hypotheses  kakas  kowalski    toni        flach   kakas         e must consistent
kb 
   existing symbolic statistical modeling frameworks restrictions limitations various types compared arbitrary logic programs  see section   details   example  bayesian networks
allow recursion  hmms pcfgs  stochastic grammars  allow recursion lack variables data
structures  recursive logic programs allowed ngo haddawy s        framework
assume domains finite function symbols seem prohibited 
    em algorithm  stands class iterative algorithms ml estimation incomplete data
 mclachlan   krishnan        
   

fiparameter learning logic programs symbolic statistical modeling

learn parameters distribution associated program  redundancy
second phase removed introduction inside outside probability logic
programs computed support graph 
graphical em algorithm accomplished  combined oldt search
explanations  time complexity specialized ones  e g  baum welch
algorithm hmms  rabiner        inside outside algorithm pcfgs  baker 
       despite generality  surprising that  conducted learning experiments pcfgs using real corpora  outperformed inside outside algorithm
orders magnitudes terms time one iteration update parameters  experimental results enhance prospect symbolic statistical modeling parameterized
logic programs even complex systems stochastic grammars whose modeling
dicult simply lack appropriate modeling tool sheer
complexities  contributions paper therefore
distribution semantics parameterized logic programs unifies existing symbolicstatistical frameworks 
graphical em algorithm  combined tabulated search   general yet ecient
em algorithm runs support graphs
prospect suggested learning experiments modeling learning complex
symbolic statistical phenomena 
rest paper organized follows  preliminaries section    probability space parameterized logic programs constructed section   mathematical
basis subsequent sections  propose new em algorithm  graphical
em algorithm  parameterized logic programs section    complexity analysis
graphical em algorithm presented section   hmms  pcfgs  pseudo pcsgs
sc bns   section   contains experimental results parameter learning pcfgs
graphical em algorithm using real corpora demonstrate eciency graphical
em algorithm  state related work section    followed conclusion section   
reader assumed familiar basics logic programming  lloyd        doets 
       probability theory  chow   teicher         bayesian networks  pearl        castillo
et al         stochastic grammars  rabiner        manning   schutze        
   preliminaries

since subject intersects logic programming em learning quite different
nature  separate preliminaries 

    logic programming oldt

logic programming  program db set definite clauses  execution search
sld refutation given goal g  top down interpreter recursively selects
   pseudo pcsgs  probabilistic context sensitive grammars  context sensitive extension pcfgs
proposed charniak carroll         sc bn shorthand singly connected bayesian network
 pearl        
   deal general logic programs paper 
   

fisato   kameya

next goal unfolds  tamaki   sato        subgoals using nondeterministically
chosen clause  computed result sld refutation  i e  solution  answer
substitution  variable binding  db   g   usually one
refutation g  search space refutations described sld tree
may infinite depending program goal  lloyd        doets        
often not  applications require solutions  natural language processing
instance  parser must able find possible parse trees given sentence
every one syntactically correct  similarly statistical abduction  need
examine explanations determine likely one  solutions obtained
searching entire sld tree  choice search strategy  prolog 
standard logic programming language  backtracking used search solutions
conjunction fixed search order goals  textually left to right  clauses
 textually top to bottom  due ease simplicity implementation 
problem backtracking forgets everything previous
choice point  hence quite likely prove goal again  resulting
exponential search time  one answer avoid problem store computed results
reuse whenever necessary  oldt instance memoizing scheme  tamaki
  sato        warren        sagonas et al         ramakrishnan et al         shen et al  
       reuse proved subgoals oldt search often drastically reduces search time
solutions  especially refutations top goal include many common subrefutations  take example logic program coding hmm  given string s 
exist exponentially many transition paths output s  oldt search applied
program however takes time linear length find unlike
exponential time prolog s backtracking search 
oldt statistical abduction  viewpoint statistical abduction  reuse proved subgoals  equivalently  structure sharing sub refutations
top goal g brings structure sharing explanations g  addition
reduction search time mentioned above  thereby producing highly compact representation explanations g 

    em learning

parameterized distributions multinomial distribution normal distribution provide convenient modeling devices statistics  suppose random sample x           xt
size random variable x drawn distribution p  x   x j   parameterized
unknown   observed  value determined ml estimation mle
 maximum likelihood estimate    i e  maximizer likelihood  it p  xi j   
things get much dicult data incomplete  think probabilistic
relationship non observable cause x observable effect one
diseases symptoms medicine assume uniquely determine
cause x   incomplete sense carry enough information
completely determine x   let p  x   x    j   parameterized joint distribution
x   task perform ml estimation condition x
q

   solution ambiguously mean answer substitution proved atom g 
one gives other 
   

fiparameter learning logic programs symbolic statistical modeling

non observable observable  let y            yt random sample size drawn
marginal distribution p  y   j     x p  x   x    j    mle
obtained maximizing likelihood  it p  yi j   function  
mathematical formulation looks alike cases  latter  ml estimation
incomplete data  far complicated direct maximization practically impossible
many cases  people therefore looked indirect approaches tackle problem
ml estimation incomplete data em algorithm standard
solution  dempster  laird    rubin        mclachlan   krishnan         iterative
algorithm applicable wide class parameterized distributions including multinomial
distribution normal distribution mle computation replaced
iteration two easier  tractable steps  n th iteration  first calculates value
q function introduced using current parameter value  n   e step     
p

q

q  j  n   def
 

x

x

p  x j y   n    ln p  x  j   

   

next  maximizes q  j  n   function updates  n   m step  
 n      argmax q  j  n    
   
since old value  n  updated value  n    necessarily coincide  e steps
m steps iterated convergence   log  likelihood assured
increase monotonically  mclachlan   krishnan        
although em algorithm merely performs local maximization  used variety
settings due simplicity relatively good performance  one must notice however
em algorithm class name  taking different form depending distributions
applications  development concrete em algorithm baum welch
algorithm hmms  rabiner        inside outside algorithm pcfgs  baker 
      requires individual effort case 
    q function related ml estimation follows  assume one data    observed 
jensen s inequality  chow   teicher        concavity ln function  follows
x

p  x j y   n    ln p  x j y     

x

x

p  x j y   n    ln p  x j y   n     

x

hence
q  j  n      q  n  j  n   
x
x
 
p  x j y   n    ln p  x j y     
p  x j y   n    ln p  x j y   n      ln p  y j     ln p  y j  n   
x

ln p  y j     ln p  y j  n    

x

consequently 
q  j  n    q  n  j  n      ln p y j   ln p y j  n      p y j   p y j  n    
   

fisato   kameya
   distribution semantics

section  introduce parameterized logic programs define declarative semantics  basic idea follows  start set f probabilistic facts  atoms 
set r non unit definite clauses  sampling f determines set f   true
atoms  least herbrand model f     r determines truth value every atom
db   f   r  hence every atom considered random variable  taking  
 true     false   follows  formalize process construct underlying
probability space denotation db 

    basic distribution pf

let db   f  r definite clause program first order language l countably many
variables  function symbols predicate symbols f set unit clauses  facts 
r set non unit clauses  rules   sequel  unless otherwise stated  consider
simplicity db set ground instances clauses db  assume
f r consist countably infinite ground clauses  the finite case similarly treated  
construct probability space db two steps  first introduce probability
space herbrand interpretations   f i e  truth assignments ground atoms
f   next extend probability space herbrand interpretations
ground atoms l using least model semantics  lloyd        doets        
let a    a          fixed enumeration atoms f   regard infinite vector    
hx   x        i  s  s herbrand interpretation f way              
ai true  resp  false  xi      resp  xi       isomorphism  set
possible herbrand interpretations f coincides cartesian product 
 
def

f   f    gi 


i  

construct probability measure pf sample space
f    collection
finite joint distributions pf n  a    x             xn   n                 xi   f    g    n 

  pf n  a    x              xn   
 n 
   
x      x pf  a    x              xn      
 
n   
 
n 
pf  a    x            an     xn       pf  a    x              xn   
x
last equation called compatibility condition  proved  chow   teicher 
      compatibility condition exists probability space  
f   f   pf  
pf probability measure f   minimal algebra containing open sets
f  
n 
pf  a    x              xn     pf n   a    x              xn   
 
 
 
 
 
 
 

p

p

 

n

n  

    herbrand interpretation interprets function symbol uniquely function ground terms
assigns truth values ground atoms  since interpretation function symbols common
herbrand interpretations  given l  one to one correspondence truth assignments
ground atoms l  distinguish them 
    regard
f topological space product topology f    g equipped
discrete topology 
   

fiparameter learning logic programs symbolic statistical modeling

call pf basic distribution
   
 
n 
choice pf free long compatibility condition met  want
interpretations equiprobable  set pf n  a    x             xn       n
every hx           xn i  resulting pf uniform distribution
f one
unit interval         if  hand  stipulate interpretation except
     hc    c        i possible  put  n 
pf n   a    x              xn        ifo w   i xi   ci    n 
pf places probability mass    gives probability   rest 
define parameterized logic program definite clause program   db   f   r
f set unit clauses  r set non unit clauses clause head r
unifiable unit clause f parameterized basic distribution pf associated
f   parameterized pf obtained collection parameterized joint distributions
satisfying compatibility condition  generally  complex pf n  s are 
exible pf is  cost tractability  choice parameterized finite distributions
made sato        simple 
pf  n   on     x       x             n   x n j             n 
n
  pbs  on  i     x i     i   x i j i 
i  

pbs  on  i     x i      i   x i j  
  x i     x i
 
x i        x i    
   
    x i        x i     
pbs  on  i     x i      i   x i j      n  represents probabilistic binary switch 
i e  bernoulli trial  using two exclusive atoms  i    i way either
one true trial never both  parameter specifying probability
switch on  resulting pf probability measure infinite product
independent binary outcomes  might look simple expressive enough bayesian
networks  markov chains hmms  sato        sato   kameya        
 



 
 
 
 
 

    extending pf pdb

subsection  extend pf probability measure pdb possible world
l  i e  set possible truth assignments ground atoms l least
    naming pf   despite probability measure  partly ects observation behaves
infinite joint distribution pf  a    x    a    x           infinite random vector ha    a         i
pf n   a    x              xn    n                marginal distributions  another reason
intuitiveness  considerations apply pdb defined next subsection well 
    clauses necessarily ground 
   

fisato   kameya

herbrand model  lloyd        doets         proceeding however  need couple
notations  atom a  define ax
ax   x    
ax    a x     
next take herbrand interpretation  
f f   makes atoms f true
others false  let f set atoms made true   imagine definite clause
program db    r   f least herbrand model mdb   lloyd        doets        
mdb  characterized least fixed point mapping tdb     
b            bk   db     k 
tdb   i   def
 
fb            bk g
set ground atoms    equivalently  inductively defined
i     
in     tdb   in  
mdb   
 
 

 







 

 

n

taking account fact mdb  function  
f   henceforth employ
functional notation mdb     denote mdb   
turning back  let a    a          enumeration  ground atoms l   
form
db   similarly
f   cartesian product denumerably many f    g s identify set possible herbrand interpretations ground atoms a    a         
l  i e  possible world l  extend pf probability
measure pdb
db
 
n 
follows  introduce series finite joint distributions pdb  a    x              xn  
n              
 ax            axn  f def
  f  
f j mdb     j  ax           axn g
def
 n   a   x             x     p   ax          ax     
pdb
 
 
n
n
f
n f
 
 

n

 

 

n

n

 n   s satisfy
note set  ax            axn  f pf  measurable definition  pdb
compatibility condition
 n     a   x             x     p  n   a   x             x   
pdb
 
 
n  
n  
 
n
n
db  
 

n

x

xn  

hence exists probability measure pdb
db extension pf

pdb  a    x              xn     pf  a    x              xn  
    defines  mutually  herbrand interpretation ground atom true    
herbrand model program herbrand interpretation makes every ground instance every
clause program true 
    note enumeration enumerates ground atoms f well 
   

fiparameter learning logic programs symbolic statistical modeling

finite atoms a           f every binary vector hx            xni  xi   f    g   
n   define denotation program db   f   r w r t  pf pdb   denotational semantics parameterized logic programs defined called distribution
semantics  remarked before  regard pdb kind infinite joint distribution
pdb  a    x    a    x            mathematical properties pdb listed appendix
semantics proved extension standard least model semantics
logic programming possible world semantics probability measure 

    programs distributions

distribution semantics views parameterized logic programs expressing distributions  traditionally distributions expressed using mathematical formulas use
programs  discrete  distributions gives us far freedom exibility mathematical formulas construction distributions recursion arbitrary composition  particular program contain infinitely many random variables
probabilistic atoms recursion  hence describe stochastic processes
potentially involve infinitely many random variables markov chains derivations
pcfgs  manning   schutze          
programs enable us procedurally express complicated constraints distributions
 the sum occurrences alphabets b output string hmm must
multiple three   feature  procedural expression arbitrarily complex  discrete 
distributions  seems quite helpful symbolic statistical modeling 
finally  providing mathematically sound semantics parameterized logic programs
one thing  implementing distribution semantics tractable way another 
next section  investigate conditions parameterized logic programs make
probability computation tractable  thereby making usable means large scale
symbolic statistical modeling 
   graphical em algorithm

according preceding section  parameterized logic program db   f   r
first order language l parameterized basic distribution pf    j   herbrand
interpretations ground atoms f specifies parameterized distribution pdb    j  
herbrand interpretations l  section  develop  step step  ecient em
algorithm parameter learning parameterized logic programs interpreting pdb
distribution observable non observable events  new em algorithm
termed graphical em algorithm  applicable arbitrary logic programs satisfying
certain conditions described later provided basic distribution direct product
multi ary random switches  slight complication binary ones introduced
section     
section on  assume db consists usual definite clauses containing
 universally quantified  variables  definitions changes relating assumption
    infinite derivation occur pcfgs  take simple pcfg fp     a  q     ss g
start symbol  terminal symbol  p   q     p  q      pcfg  rewritten either
probability p ss probability q   probability occurrence infinite derivation
calculated max f        p q g non zero q   p  chi   geman        
   

fisato   kameya

listed below  predicate p  introduce iff  p   iff definition p
iff p  def
   x  p x     y  x   t    w              yn  x   tn   wn     
x vector new variables length equal arity p  p ti  wi   
n    n   enumeration clauses p db  yi   vector variables occurring
p ti  wi  define comp r  follows 
head r  def
  fb j b ground instance clause head appearing rg
iff  r  def
  fiff  p  j p appears clause head rg
eq def
   x    f  y    x   j f function symbolg
   x     g y  j f g different function symbolsg
  ft    x j term properly containing xg
comp r  def
  iff  r    eq
eq   clark s equational theory  clark         deductively simulates unification  likewise
comp r  first order theory deductively simulates sld refutation help
eq replacing clause head atom clause body  lloyd        doets        
introduce definitions frequently used  let b atom 
explanation b w r t  db   f   r conjunction s  r   b 
set comprised conjuncts  f holds proper subset satisfies this 
set explanations b called support set b designated db  b    

    motivating example

first all  review distribution semantics concrete example  consider following
program dbb   fb   rb figure   modeling one s blood type determined blood
type genes probabilistically inherited parents   
first four clauses rb state blood type determined genotype  i e  pair
blood type genes a  b o  instance  btype  a      gtype a a    gtype a o   
gtype o a   says one s blood type  her  genotype ha  ai  ha  oi ho  ai 
propositional rules 
succeeding clauses state general rules terms logical variables  fifth clause
says regardless values x y  event gtype x y   one s genotype
hx  yi  caused two events  gene father x   inheriting gene x father 
gene mother y   inheriting gene mother   gene p g    msw gene p g 
clause connecting rules rb probabilistic facts fb  tells us gene g
inherited parent p choice represented msw gene p g    made 

    definition support set differs one used sato        kameya sato        
    implicitly emphasize procedural reading logic programs  prolog conventions employed
 sterling   shapiro         thus    stands  or      and      implied by  respectively  strings
beginning capital letter  universally quantified  variables  quoted ones  a 
constants  underscore anonymous variable 
    msw abbreviation  multi ary random switch  msw          expresses probabilistic choice
finite alternatives  framework statistical abduction  msw atoms abducibles
explanations constructed conjunction 
   

fiparameter learning logic programs symbolic statistical modeling

 
 
 
 
 
 
 
 
 

btype  a  
btype  b  
btype  o  
btype  ab  
gtype x y 
gene p g 

       

 gtype a a    gtype a o    gtype o a   
 gtype b b    gtype b o    gtype o b   
gtype o o  
 gtype a b    gtype b a   
gene father x   gene mother y  
msw gene p g  

rb

 

fb

  fmsw gene father a   msw gene father b   msw gene father o  

 
 
 
 
 
 
 
 

msw gene mother a   msw gene mother b   msw gene mother o g

figure    abo blood type program dbb
genetic knowledge choice g chance made fa  b  og expressed
specifying joint distribution fb follows 
pf  msw gene t a    x  msw gene t b    y  msw gene t o    z j   b     def
  axby oz
x  y  z   f    g  x     z      a  b              b       either
father mother  thus probability inheriting gene parent  statistical
independence choice gene  father mother  expressed
putting
pf   msw gene father a    x  msw gene father b    y  msw gene father o    z 
msw gene mother a    x   msw gene mother b       msw gene mother o    z  
j   b    
  pf  x  y  z j a  b  o pf  x   y    z  j a  b   o  
setting  atoms representing observation obs dbb     fbtype  a    btype  b   
btype  o    btype  ab  g  observe one them  say btype  a    infer possible
explanation   i e  minimal conjunction abducibles msw gene     
s  rb   btype  a   
obtained applying special sld refutation procedure goal btype  a  
preserves msw atoms resolved upon refutation  three explanations found 
s    msw gene father a    msw gene mother a 
s    msw gene father a    msw gene mother o 
s    msw gene father o    msw gene mother a 
db  btype a    support set btype a   fs    s    s g  probability
explanation respectively computed pf  s     a  pf  s      pf  s     ao 
proposition a   appendix a  follows pdb  btype  a      pdb  s    s    s    
pf  s    s    s   
pdb  btype  a   j   b       pf  s      pf  s      pf  s   
  a     ao 
b

b

b

b

b

b

b

b

b

b

b

b

b

   

b

b

fisato   kameya

used fact s   s  s  mutually exclusive choice gene
exclusive  parameters  i e  a  b determined ml estimation performed
random sample fbtype  a    btype  o    btype  ab  g btype follows 
ha   b   oi   argmaxh     pdb  btype  a   pdb  btype  o   pdb  btype  ab   
  argmaxh      a     ao o  ab
program contains neither function symbol recursion though semantics
allows them  later see example containing both  program hmm  rabiner
  juang        


b



b

b

b

b

    four simplifying conditions

figure   simple probability computation easy  generally
case  since primary interest learning  especially ecient parameter learning parameterized logic programs  hereafter concentrate identifying property program
makes probability computation easy dbb  thereby makes ecient parameter learning
possible 
answer question precisely  let us formulate whole modeling process  suppose
exist symbolic statistical phenomena gene inheritance hope
construct probabilistic computational model  first specify target predicate p
whose ground atom p s  represents observation phenomena  explain
empirical distribution p  write parameterized logic program db   f   r
basic distribution pf parameter reproduce observable patterns
p s   finally  observing random sample p s            p st   ground atoms p 
adjust ml estimation  i e  maximizing likelihood l     tt   pdb  p st  j  
pdb  p    j   approximates closely empirically observed distribution p
possible 
first sight  formulation looks right  reality not  suppose two events
p s  p s     s    s   observed  put l     pdb  p s  j  pdb  p s   j   
cannot likelihood simply distribution semantics  p s  p s   
two different random variables  two realizations random variable 
quick remedy note case blood type program dbb obs dbb   
fbtype  a    btype  b    btype  o    btype  ab  g observable atoms  one
true observation  atom true  others must false 
words  atoms collectively behave single random variable distribution
pdb whose values obs dbb   
keeping mind  introduce following condition  let obs db    head r  
set ground atoms represent observable events  call observable atom s 
dbb

q

b

uniqueness condition 
 
pdb  g   g      

g    g    obs db  

   

p

g obs db  pdb

 g      

fiparameter learning logic programs symbolic statistical modeling

uniqueness condition enables us introduce new random variable yo representing
observation  fix enumeration g    g          observable atoms obs db  define
yo by  
       k

iff   j  gk    
db  k    
   
let gk t  gk           gk   obs db  random sample size   l     tt   pdb  gk j
    t   pdb  yo   kt j   qualifies likelihood function w r t  yo  
second condition concerns reduction probability computation addition 
take blood type exmaple  computation pdb  btype  a    decomposed
summation explanations support set mutualy exclusive 
introduce
q

q 

 





b

exclusiveness condition 

every g   obs db  support set db  g   pdb  s            
    db  g  
using exclusiveness condition  and proposition a   appendix a  
pdb  g   
pf  s   
x

s 

db

 g 

modeling point view  means single event  single observation 
g  may several  or even infinite  explanations db  g   one db  g  allowed
true observation 
introduce  db   i e  set explanations relevant obs db 
 db def
 
db  g 
 

g obs db 

fix enumeration s   s          explanations  db   follows proposition a   
uniqueness condition exclusiveness condition
pdb  si   sj          j

pdb  s    
pdb  s  
x

  db

x

x

g obs db   

 g 
pdb  g 
db

 
g obs db 
    
able introduce uniqueness condition exclusiveness condition
yet another random variable xe  representing explanation g  defined
xe       k iff   j  sk    
db  
   
third condition concerns termination 

   

x

g obs db  pdb  g      guarantees measure f   j   j  gk k     g one 
  satisfying gk  s  case  put yo          values set measure
zero affect part discussion follows  applies definition xe     
p

   

fisato   kameya

finite support condition 

every g   obs db  db  g  finite 
pdb  g  computed support set db  g    fs            sm g    m  
help exclusiveness condition  finite summation mi   pf  si   condition
prevents infinite summation hardly computable 
fourth condition simplifies probability computation multiplication  recall
explanation g   obs db  conjunction a           abducibles
fa           amg f    m   order reduce computation pf  s     pf  a         am 
multiplication pf  a        pf  am    assume
p

distribution condition 

f set fmsw ground atoms parameterized distribution pmsw specified below 

atom msw i n v  intended simulate multi ary random switch whose name
whose outcome v trial n  generalization primitive probabilistic events
coin tossing dice rolling 
   fmsw consists probabilistic atoms msw i n v   arguments i  n v ground
terms called switch name  trial id value  of switch i   respectively 
assume finite set vi ground terms called value set associated
i  v   vi holds 
   write vi fv    v            vmg  m   jvi j   then  one ground atoms f msw i n v   
msw i n v            msw i n vm  g becomes exclusively true  takes value   
trial  i  parameter i v          v v i v     associated  i v
probability msw i   v  true  v   vi  
   ground terms i  i    n  n   v   vi v    vi    random variable msw i n v 
independent msw i   n   v    n    n     i   
words  introduce family parameterized finite distributions p i n 
p i n  msw i n v      x            msw i n vm     xm j i v           i v  
x
x
mk   xk    
def
   i v      i v o w 
   
p



 

 

 

 




p



  jvij  xk   f    g    k m   define pmsw infinite product
pmsw def
  p i n   


i n

condition  compute pmsw s    probability explanation s 
product parameters  suppose msw ij  n v  msw ij   n  v   different conjuncts
explanation   msw i   n  v            msw ik  nk  vk    either j    j   n    n  holds 
independent construction  else j   j   n   n  v    v   
independent pmsw s       construction  result  whichever condition may hold 
pmsw  s   computed parameters 
   

fiparameter learning logic programs symbolic statistical modeling

    modeling principle

point  introduced four conditions  uniqueness condition  exclusiveness condition  finite support condition distribution condition  simplify
probability computation  last one easy satisfy  adopt fmsw together
pmsw   so  on  always assume fmsw parameterized distribution pmsw
introduced previous subsection  unfortunately rest satisfied automatically  according modeling experiences however  mildly dicult satisfy
uniqueness condition exclusiveness condition long obey following
modeling principle 
modeling principle  db   fmsw   r describes sequential decision process
 modulo auxiliary computations  uniquely produces observable atom
g   obs db  decision expressed msw atom   
translated programming level  says must take care writing program sample f   pmsw  must uniquely exist goal g  g   obs db  
successful refutation db    f     r  confirm principle
blood type program dbb   fb   rb  describes process gene inheritance 
arbitrary sample fb  pmsw  say fb    fmsw gene father a   msw gene mother o g 
exists unique goal  btype  a   case  successful sld refutation
fb    rb 
idea behind principle decision process always produces result  an
observable atom   different decision processes must differ msw thereby entailing
mutually exclusive observable atoms  uniqueness condition exclusiveness
condition automatically satisfied 
satisfying finite support condition dicult virtually equivalent
writing program db solution search g  g   obs db   always terminates  apparently general solution problem  far specific models
hmms  pcfgs bayesian networks concerned  met  programs
models satisfy finite support condition  and conditions well  

    four conditions revisited

subsection  discuss relax four simplifying conditions introduced subsection     purpose exible modeling  first examine uniqueness condition
considering crucial role adaptation em algorithm semantics 
uniqueness condition guarantees exists  many to one  mapping
explanations observations em algorithm applicable  dempster et al         
possible  however  relax uniqueness condition justifying application
em algorithm  assume mar  missing random  condition introduced
rubin        statistical condition complete data  explanation  becomes incomplete data  observation   customarily assumed implicitly explicitly
statistics  see appendix b   assuming mar condition  apply em
    decisions made process finite subset fmsw  
   

fisato   kameya

algorithm non exclusive observations p  o    uniqueness
condition seemingly destroyed 
let us see mar condition action simple example  imagine walk along
road front lawn  occasionally observe state  the road dry
lawn wet   assume lawn watered sprinkler running probabilistically 
program dbrl   rrl   frl figure   describes sequential process outputs
observation observed road x  lawn y     the road x lawn y  
x    fwet  dryg 
rrl     observed road x  lawn y   p

frl

 

msw rain once a  
    yes  x   wet    wet
    no  msw sprinkler once b  
  b   on  x   dry    wet
  b   off  x   dry    dry       
  msw rain once yes   msw rain once no  
msw sprinkler once on   msw sprinkler once off   

figure    dbrl
basic distribution frl specified pf     subsection      omit it 
msw rain once a  program determines whether rains  a   yes   a   no  
whereas msw sprinkler once b  determines whether sprinkler works fine  b   on 
 b   off   since sampled values    a   fyes  nog  b   b
 b   fon  offg   uniquely exists observation observed road x  lawn y    x   
fwet  dryg   many to one mapping    a  b    hx  yi  words 
apply em algorithm observations observed road x  lawn y    x   
fwet  dryg   would happen observe exclusively either state road
lawn  logically  means observe  y observed road x  lawn y  
 x observed road x  lawn y    apparently uniqueness condition met 
 y observed road wet  lawn y    x observed road x  lawn wet   compatible
 they true rains   despite non exclusiveness observations  still
apply em algorithm mar condition  case translates
observe either lawn road randomly regardless state 
brie check conditions  basically relaxed cost
increased computation  without exclusiveness condition instance  would need
additional process transforming support set db  g  goal g set exclusive
explanations  instance  g explanations fmsw a n v   msw b m w g 
transform fmsw a n v    msw a n v    msw b m w g on    clearly 
transformation exponential number msw atoms eciency concern leads
assuming exclusiveness condition 
finite support condition practice equivalent condition sld tree
g finite  relaxing condition might induce infinite computation 
b

     msw a n v   transformed disjunction exclusive msw atoms
   

w

 

 
  msw a n v   

v    v v   va

fiparameter learning logic programs symbolic statistical modeling

relaxing distribution condition accepting probability distributions
serve expand horizon applicability parameterized logic programs 
particular introduction parameterized joint distributions p  v           vk   boltzmann distributions switches msw            mswk v           vk values switches 
makes correlated  distributions facilitate writing parameterized logic programs
complicated decision processes decisions independent interdependent  obviously  hand  increase learning time  whether added
exibility distributions deserves increased learning time yet seen 
pmsw

    naive approach em learning

subsection  derive concrete em algorithm parameterized logic programs
db   fmsw   r assuming satisfy uniqueness condition  exclusiveness
condition finite support condition 
start  introduce yo  random variable representing observations according
    based fixed enumeration observable atoms obs db   introduce
another random variable xe representing explanations according     based
fixed enumeration explanations  db   understanding xe non observable
yo observable  joint distribution pdb  xe   x  yo   j  
denotes relevant parameters  immediate  following         section   
derive concrete em algorithm q function defined q  j     def
  x pdb  x j
y     ln pdb  x  j   whose input random sample observable atoms whose output
mle  
following  sake readability  substitute observable atom g  g  
obs db   yo   write pdb  g j   instead pdb  yo   j    likewise
substitute explanation  s    db   xe   x write pdb  s  g j   instead
pdb  xe   x  yo   j    follows uniqueness condition
 
   db  g 
pdb  s  g j    
pmsw  s j     db  g  
need yet another notation here  explanation s  define count msw i n v 

i v  s   def
  jf n j msw i n v    gj  
done preparations now  suppose make observations g   g            gt
gt   obs db        put
def
  j msw i n v      db  gt     g
def
  fi v j msw i n v      db  gt     g 
set switch names appear explanation one gt  s denotes
parameters associated switches  finite due finite support condition 
p

 

   

fisato   kameya

various probabilities q function computed using proposition a  
appendix together assumptions follows 
pdb  gt j     pdb
 
pmsw  s j  
   
db  gt  




 

pmsw  s j  

 

q  j     def
 

 

 i  v    def
 



t   s  db
i i v vi

x
t  

x

s 

i v  s  
i v

i i v vi

x
x

x



db

 gt  

pdb  s j gt       ln pdb  s  gt j  

 i  v     ln i v

 

pdb  gt j    

x

i i v vi

x

db

 gt  

 
 i  v     
 
 i  v    ln p
   
v   vi  i  v    

   

pmsw  s j  i v  s  

used jensen s inequality obtain      note pdb  gt j     s   g  
pmsw  s j  i v  s   expected count msw i   v  sld refutation gt  speaking
likelihood function l     tt   pdb  gt j    already shown subsection    
 footnote  q  j     q   j     implies l   l      hence      reach
procedure learn naive   g  finds mle parameters  array
variable  i  v  stores  i  v    current  
p

db



q

db

  
  
  
  
  
  

procedure

learn naive db  g  

begin

initialize appropriate values   small positive number  
       t   ln pdb  gt j   
  compute log likelihood 
p

repeat
foreach

  i  v   vi

 i  v     


x

 

pdb  gt j    
foreach   i  v   vi
 i  v 
i v    p
  
 
v  vi  i  v  
    p  
 m     tt   ln pdb  gt j  
 m     m       

  
  
  
   
   
    end

t  

x

db

 gt  

pmsw  s j  i v  s   

  update parameters 
  compute log likelihood again 
  terminate converged 

em algorithm simple correctly calculates mle   calculation pdb  gt j    i  v  line          may suffer combinatorial explosion
explanations  is  j db  gt j often grows exponentially complexity model 
instance  j db  gt j hmm n states o n l   exponential length l
input output string  nonetheless  suppressing explosion realize ecient computation polynomial order possible  suitable conditions  avoiding multiple
computations subgoal see next 
   

fiparameter learning logic programs symbolic statistical modeling

    inside probability outside probability logic programs

subsection  generalize notion inside probability outside probability
 baker        lari   young        logic programs  major computations learn naive   g 
two terms line    pdb  gt j   s   g   pmsw s j  i v  s   computational redundancy lurks naive computation terms  show example 
suppose propositional program dbp   fp   rp fp   fa  b  c  d  mg
db

p

db

 
 
 
 
 
 
 

rp    
 
 
 
 
 

f
f
g
g
h



a g
b g
c
d h
m 

    

f observable atom  assume a  b  c  independent
fa  bg fc  dg pair wise exclusive  support set f calculated
db  f    fa   c      m  b   c  b     g 
hence  light      may compute pdb  f 
pdb  f    pf  a   c    pf  a     m    pf  b   c    pf  b     m  
    
computation requires   multiplications  because pf  a   c    pf  a pf  c  etc  
  additions  hand  possible compute pdb  f  much eciently
factoring common computations  let ground atom  define inside probability
 a 
 a  def
  pdb  a j     
    
applying theorem a   appendix
comp rp    f    a   g     b   g   g   c    d   h   h  
    
unconditionally holds semantics  using independent exclusiveness assumption made fp  following equations inside probability
derived 
 f     a fi  g     b fi  g 
 g     c     d fi  h 
    
 h     m 
pdb  f     f   obtained solving       f     multiplications
  additions required 
quite straightforward generalize      proceeding  look program
dbq   fmg   fg  m   m  g  mg g observable atom msw atom 
g    m   m    semantics  compute p  g    p  m p  m    p  m 
clearly wrong ignores fact clause bodies g  i e  m m mutually
exclusive  atoms clause body m m independent  here p       pdb      
similarly  set   b   c     m  equation      totally incorrect 
p

p

p

p

p

p

p

p

p

p

p

 
 
 
 
 

p

q

    note fact f    a    pmsw  a j   

   

fisato   kameya

therefore add  temporarily subsection  two assumptions top exclusiveness condition finite support condition equations      become
mathematically correct  first assumption  clause  bodies mutually exclusive i e  two clauses b w b w     pdb  w   w   j       
second assumption body atoms independent  i e  b            bk rule 
pdb  b           bk j     pdb  b  j        pdb  bk j   holds 
please note  clause  used subsection special meaning  intended
mean g g goal tabled explanation g obtained oldt search
explained next subsection    words  additional
conditions imposed source program result oldt search 
clauses auxiliary computations need satisfy them 
suppose clauses occur db



b              b  i 

    

bl            bl il

bh j    h l    j ih  atom  theorem a   appendix
assumptions ensure


 a     b  j             bl j   
    
 


l


j   

j   

     suggests  gt  considered function  a  equations
inside probabilities hierarchically organized way fi gt  belongs top
layer fi a  appearing left hand side refers  b  s belong
lower layers  refer condition acyclic support condition  acyclic
support condition  equations form      unique solution  computation
pdb  g j   via inside probabilities allows us take advantage reusing intermediate
results stored  a   thereby contributing faster computation pdb  gt j   
next tackle intricate problem  computation s   g   pmsw s j
 i v  s    since sum equals n msw i n v   s    g   pmsw  s j    concentrate
computation
 gt  m  def
 
pmsw  s j  
p

p

db

p

db





x

   

m s   db gt

  msw i n v   first note explanation contains   a        
ah   m   s      a           ah fi  m    gt   m  expressed
 gt  m    ff gt   m fi  m 

    
ff gt  m      fi g m  m  ff gt  m  depend  m   generalizing observation arbitrary ground atoms  introduce outside probability ground atom
w r t  gt
 gt 
ff gt  a  def
   fi
 fi  a 


    logical relationship      corresponds      f  g h table atoms 
   

fiparameter learning logic programs symbolic statistical modeling

assuming conditions inside probability  view       problem computing  gt  m  reduced computing ff gt  m   recursively computable
follows  suppose occurs ground program db
b 
bk

  w             b 

    

  wk            bk

  w  i 
  wk ik  

 gt  function  b             fi bk   assumption  chain rule derivatives
leads
 fi  gt  
 fi  a   wk i  
 fi  gt    fi  a   w     
 
  
 
 
ff gt   a   
 fi  b   
 fi  a 
 fi  bk  
 fi  a 
hence to  
ff gt  gt      
    


ff gt   a    ff gt   b     w  j            ff gt   bk    wk j   
    












k

 
x

k
x

j   

j   

therefore inside probabilities already computed  outside probabilities
recursively computed top      using      downward along program layers 
case dbp f chosen atoms  compute
ff f  f     
ff f  g     a     b 
    
ff f  h    ff f  g fi  d 
ff f  m    ff f  h  
      desired sum  f  m  calculated
 f  m    ff f  m fi  m     fi  a     b  fi  d fi  m 
requires two multiplications one addition compared four multiplications
one addition naive computation 
gains obtained computing inside outside probability may small case 
problem size grows  become enormous  compensate enough additional restrictions imposed result oldt search 
 
 
 
 
 
 
 
 
 

    oldt search

compute inside outside probability recursively                 need
programming level tabulation mechanism structure sharing partial explanations
    independence assumption body atoms  wh j    h k    j ih  
independent  therefore
 fi  a   wh j      fi  a fi  wh j      w   
h j
 fi a 
 fi  a 
   

fisato   kameya

subgoals  henceforth deal programs db set table db  table
predicate declared advance  ground atom containing table predicate called
table atom  purpose table atoms store support sets eliminate
need recomputation  so  construct hierarchically organized explanations
made table atoms msw atoms 
let db   fmsw   r parameterized logic program satisfies finite support
condition uniqueness condition  let g    g           gt random sample
observable atoms obs db   make following additional assumptions 

assumptions 

      exists finite set f t          kt g table atoms associated
   k k     j  
conjunctions sk j

k


e

comp r 





  gt   s t             s t m
   t   s t            s t m          kt   skt             skt  mkt
e



e

e

 

e





e

 

e

    




   k k     j   is  set  subset f
sk j

msw   fk             k g
k
 acyclic support condition   convention  put     gt call respectively
def
 k    t explanation
db
  f     t          kt g set table atoms gt sk j
kt     set t explanations k denoted db  kt   consider
db     function table atoms 
  st    
t explanations mutually exclusive  i e  k    k kt    pdb  sk j
k j  
     j    j   mk    t exclusiveness condition  
   k k     j   conjunction independent atoms  independent
sk j

k
condition    
assumptions aimed ecient probability computation  namely  acyclic
support condition makes dynamic programming possible  t exclusiveness condition reduces pdb  a   b  pdb  a   pdb  b  independent condition reduces pdb  a   b  
pdb  a pdb  b   one point concerning eciency however  note
   imcomputation dynamic programming proceeds following partial order db
posed acyclic support condition access table atoms much simplified
respecting said partial
linearly ordered  therefore topologically sort db

order call linearized db satisfying three assumptions  the acyclic support condition  t exclusiveness condition independent condition  hierarchical system
  h                 g   assuming
t explanations gt   write db
 

db    
   
k
  
implicitly given  hierarchical system t explanations gt successfully built
e



e



e

e

e

e

e



e

    prefix  t   abbreviation  tabled   
    independence mentioned concerns positive propositions  b    b    head db   say
b  b  independent pdb  b    b  j     pdb  b  j  pdb  b  j    
    precedes j top down execution w r t  db invokes j directly indirectly 
    holds precedes j   j  
   

fiparameter learning logic programs symbolic statistical modeling

source program  equations inside probability outside probability
          automatically derived solved time proportional size
equations  plays central role approach ecient em learning 
one way obtain t explanations use oldt search  tamaki   sato       
warren         complete refutation method logic programs  oldt search 
goal g called first time  set entry g solution table store
answer substitutions g there  call instance g  g occurs later  stop
solving g  instead try retrieve answer substitution g stored solution table
unifying g  g  record remaining answer substitutions g  prepare
lookup table g  hold pointer them 
self containedness  look details oldt search using sample program
dbh   fh  rh figure     depicts hmm   figure    hmm two states
fs   s g  state transition  probabilistically chooses next destination fs   s g
a b
s 

s 

a b

a b

a b

figure    two state hmm
fh

rh

 

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

f   values init   s  s    
f   values out     a b   
f   values tr      s  s    

h   hmm cs  msw init once si  
hmm   si cs  
h   hmm t s  c cs     t    
msw out s  t c  
msw tr s  t nexts  
t  t   
hmm t  nexts cs  
h   hmm t         t   

 
 
 
 
 
 
 
 
 

generate string  chars  cs 
set initial state si 
enter loop clock     
loop 
output c state s 
transit nexts 
put clock ahead 
continue loop  recursion  
finish loop clock     

figure    two state hmm program dbh
    f   f   f   h   h  h  temporary marks  part program 
    hmm defines probability distribution strings given set alphabets  works
stochastic string generator  rabiner   juang        output string sample
defined distribution 
   

fisato   kameya

alphabet fa  bg emit  note specify fact set fh associated distribution compactly  introduce new notation values i  v      vm   
declares fh contains msw atoms form msw i n v   v   fv            vmg  whose distribution p i n  given     subsection      example   f    values tr     s  s   
introduces msw tr t  n v  atoms program ground term 
v   fs   s g ground term n  distribution
x
p tr t  n   msw tr t  n s     x  msw tr t  n s     j i s    i s     i s
  i s 
  tr t   x    f    g x       
program runs prolog program  non ground top goal hmm s   functions stochastic string generator returning list alphabets  a b a 
variable follows  top goal calls clause  h    h   selects initial state executing subgoal msw init once si    returns si initial state probabilistically
chosen fs   s g  second clause  h   called  h   ground ground
t  makes probabilistic choice output alphabet c asking msw out s  t c 
determines nexts  next state  asking msw tr s  t nexts    h  
stop transition  simplicity  length output strings fixed three  way
execution termed sampling execution corresponds random sampling
pdb   top goal ground hmm  a b a    works acceptor  i e 
returning success  yes  failure  no  
explanations hmm  a b a   sought for  keep msw atoms resolved upon
refutation conjunction  explanation   repeat process backtracking refutation found  need t explanations however  backtracking must
abandoned sharing partial explanations t explanations  purpose
t explanations itself  becomes impossible  therefore instead use oldt search
h

t  
t  
t  
t  
t   
 
t  
t  
t  

top hmm cs ans    tab hmm cs ans     
tab hmm cs  hmm cs  x  x    hmm cs       
tab hmm t s cs  hmm t s cs  x  x    hmm t s cs       
e msw init t s   msw init t s   x  x  
e msw init t s   msw init t s   x  x  
hmm cs x  x     e msw init once si x  x    tab hmm   si cs x  x   
hmm t s  c cs  x  x   t     e msw out s  t c x  x    e msw tr s  t nexts x  x   
t  t    tab hmm t  nexts cs x  x   
hmm t s    x x    t   

figure    translated program dbh
    msw i n v  called ground ground n  v  logical variable  behaves random variable 
instantiated term v probability i v selected value set vi declared values
atom  if  hand  v ground term v called  procedural semantics msw i n v 
equal msw i n v    v   v 
   

fiparameter learning logic programs symbolic statistical modeling

t explanation search  case hmm program example  build hierarchical
system t explanations hmm  a b a   oldt search  first declare hmm  
hmm   table predicate    t explanation conjunction hmm   atoms  hmm  
atoms msw atoms  translate program another logic program  analogously translation definite clause grammars  dcgs  prolog  sterling   shapiro 
       add two arguments  which forms d list  predicate purpose
accumulating msw atoms table atoms conjuncts t explanation  translation
applied dbh yields program figure   
translated program  clause  t   corresponds top goal hmm l 
input string l  t explanation table atom hmm l  returned ans   t  
 t   auxiliary clauses add callee s d list table atom form hmm l 
hmm t s l  respectively  t  time step  s  state   general  p n table predicate
original program  p  n      becomes table predicate translated program
auxiliary predicate tab p  n     inserted signal oldt interpreter check
solution table p n  i e  check already exist t explanations p n  likewise
clauses  t    t    pair corresponding  f   insert msw init t   
callee s d list   once  clauses  t     t    t   respectively correspond  h   
 h    h   
hmm  a b a    hmm  a b a   
   msw init once s    hmm   s   a b a    
 msw init once s    hmm   s   a b a     
hmm   s   a b a    hmm   s   a b a   
   msw out s     a   msw tr s     q    hmm   s   b a    
 msw out s     a   msw tr s     s    hmm   s   b a     
hmm   s   a b a    hmm   s   a b a   
   msw out s     a   msw tr s     s    hmm   s   b a    
 msw out s     a   msw tr s     s    hmm   s   b a     
hmm   s   b a    hmm   s   b a   
   msw out s     b   msw tr s     s    hmm   s   a    
 msw out s     b   msw tr s     s    hmm   s   a     
hmm   s   b a    hmm   s   b a   
   msw out s     b   msw tr s     s    hmm   s   a    
 msw out s     b   msw tr s     s    hmm   s   a     
hmm   s   a    hmm   s   a   
   msw out s     a   msw tr s     s    hmm   s       
 msw out s     a   msw tr s     s    hmm   s        
hmm   s   a    hmm   s   a   
   msw out s     a   msw tr s     s    hmm   s       
 msw out s     a   msw tr s     s    hmm   s        
hmm   s       hmm   s      
    
hmm   s       hmm   s      
    

figure    solution table
    general  p n means predicate p arity n  although hmm   hmm   share predicate name
hmm  different predicates 
   

fisato   kameya

translation  apply oldt search top hmm  a b a  ans  noting  i  added d list uence oldt procedure   ii  associate
solution table atom solution table list t explanations  resulting
solution table shown figure    first row reads call hmm  a b a   occurred entered solution table solution  hmm  a b a    no variable binding generated   two t explanations  msw init once s     hmm   s   a b a  
msw init once s     hmm   s   a b a    remaining task topological sorting table atoms stored solution table respecting acyclic support condition 
done using depth first search  trace  t explanations top goal
example  thus obtain hierarchical system t explanations hmm  a b a   

    support graphs

looking back  need compute inside outside probability hierarchical system
t explanations  essentially boolean combination primitive events  msw atoms 
compound events  table atoms  intuitively representable
graph  reason  help visualizing learning algorithm  introduce new
data structure termed support graphs  though new em algorithm next subsection
described solely hierarchical system t explanations 
illustrated figure    a   support graph gt graphical representation
  h                 g   g      
hierarchical system t explanations db


   
 
k
consists totally ordered disconnected subgraphs  labeled
   k k    subgraph labeled comprises two
corresponding table atom kt db

k
special nodes  the start node end node  explanation graphs  corresponding


t explanation sk j
db  k      j mk   
linear graph node labeled either
explanation graph sk j
  called table node switch
table atom switch msw        sk j
node respectively  figure    b  support graph hmm  a b a   obtained
solution table figure    table node labeled refers subgraph labeled  
data sharing achieved distinct table nodes referring subgraph 


e

e

e

e

    graphical em algorithm

describe ecient em learning algorithm termed graphical em algorithm
 figure    introduced kameya sato         runs support graphs  suppose
random sample g   g           gt observable atoms  suppose support
graphs gt       i e  hierarchical systems t explanations satisfying acyclic
support condition  t exclusiveness condition independent condition 
successfully constructed parameterized logic program db satisfying uniqueness
condition finite support condition 
graphical em algorithm refines learn naive   g   introducing two subroutines 
get inside probs 
  g   compute inside probabilities get expectations 
  g   compute outside probabilities  called main routine learn gem   g   
learning  prepare four arrays support graph gt g  
p  t    inside probability   i e        pdb   j    see      
db

db

db

db

   

fiparameter learning logic programs symbolic statistical modeling
k

 a 

explanation graph

msw

gt 
start

k

msw

end

msw

msw

msw

 
k  
start

end
msw

msw

 

 b 
msw init once s  

hmm   s   a b a  

hmm  a b a   

start

end

msw init once s  

msw out s     a 

hmm   s   a b a  

msw tr s     s  

hmm   s   b a  

hmm   s   a b a   

end

start

msw out s     a 

msw tr s     s  

hmm   s   b a  

msw out s     a 

msw tr s     s  

hmm   s   b a  

hmm   s   a b a   

end

start

msw out s     a 

msw tr s     s  

hmm   s   b a  

figure    support graph  a  general form   b  gt   hmm  a b a   hmm
program dbh  double circled node refers table node 
q t    outside probability w r t  gt   i e  ff gt     see           
r t      explanation probability    db  kt     i e  pdb  s j  
e

e

   

e

e

fisato   kameya
   procedure learn gem  db  g  
   begin
   select initial
  
  
  
  
  
  

   procedure get inside probs  db  g  
   begin
        begin
  
let  t   gt 
  
k    kt downto   begin
  
p  t  kt        
  
foreach se   edb  kt   begin
  
let se   fa    a            ajsejg 
  
r t  kt   se       
   
l      jsej
   
al   msw i   v  
   
r t  kt   se      i v
   
else r t  kt   se      p  t  al   
   
p  t  kt     r t  kt   se 
   
end    foreach se   
   
end    k   
    end      
    end 

parameters 

get inside probs
 db  g  
p
       tt   ln p  t  gt   
repeat

get expectations  db  g   
  i  v   vi
 i  vp
    
 t  i  v  p  t  g   

t  
foreach   i  v p
  vi
i v     i  v   v   vi  i  v    
get inside probs  db  g   
    
  
p
 m     tt   ln p  t  gt  
 m     m       
foreach

   
   
   
   
   
   
    end 

   procedure get expectations  db  g   begin
        begin
  
foreach   i  v   vi  t  i  v       
  
let  t   gt  q t   t         
  
k      kt q t  kt        
  
k      kt
  
foreach se   edb  kt   begin
  
let se   fa    a            ajsejg 
  
l      jsej
   
al   msw i   v    t  i  v     q t  kt     r t  kt   se 
   
else q t  al      q t  kt     r t  kt   se  p  t  al  
   
end    foreach se   
    end      
    end 

figure    graphical em algorithm 
 t  i  v  expected count msw i   v   i e 

p

s 

db

 gt  pmsw  s j  i v  s  

call procedure learn gem   g  figure    main routine learn gem   g  initially computes inside probabilities  line    enters loop get expectations   g  
called first compute expected count  t  i  v  msw i   v  parameters updated  line      inside probabilities renewed using updated parameters
entering next loop  line     
db

db

db

   

fiparameter learning logic programs symbolic statistical modeling

subroutine get inside probs   g   computes inside probability       pdb   j  
 and stores p  t     table atom bottom layer topmost layer    
gt  line    hierarchical system t explanations gt  see      subsection      
takes t explanation db  kt   one one  line     decomposes conjuncts
multiplies inside probabilities either known  line     already computed
 line     
subroutine get expectations   g   computes outside probabilities following
recursive definitions           subsection     stores outside probability
ff gt    table atom q t     first sets outside probability top goal
    gt      line    computes rest outside probabilities  line    going
layers t explanation gt described      subsection       line     adds
q t  kt     r t  kt       ff gt   kt     fi s    t  i  v   expected count msw i   v  
contribution msw i   v  kt  t  i  v    line     increments outside
probability q t  al     ff gt  al   al according equation       notice q t  kt  
already computed r t  kt   s  p  t  al     fi w     al   w   shown
subsection      learn naive   g   mle procedure  hence following theorem holds 
theorem     let db parameterized logic program  g   g           gt randb

e

e

e

db

e

e

e

e

e

db

dom sample observable atoms  suppose five conditions  uniqueness  finite support
 subsection       acyclic support  t exclusiveness independence  subsection      
met  thenqlearn gem  db  g   finds mle    locally  maximizes likelihood
l g j     tt   pdb  gt j   

 proof  sketch    since main routine learn gem   g   learn naive   g 
except computation  i  v    tt    t  i  v   show  t  i  v    s   g   pmsw s j
 i v  s      n msw i n v   s   g   pmsw  s j     however 
db

p

p

db

p

db

 t  i  v 

db

p

 

x

x

 kkt





x

n msw i n v   se  e    
db k

ff gt   kt  fi  se 

 see  line     get expectations db  g  
  ff gt  msw i n v  fi msw i n v  
n
   gt  msw i n v    see equation      
n
 
pmsw  s j   
q e d 
x

x

x

x

n msw i n v   s 

db

 gt  

used fact contains msw i n v    s    msw i n v   fi s   
 s    fi  msw i n v    holds  hence
ff gt   kt  fi  s     ff gt   kt  fi  s    fi  msw i n v   
   contribution msw i n v  kt ff gt  msw i n v   fi  msw i n v   
e

e

e

e

e

e

e

e

    formal proof given kameya         proved common parameters    i  v 
learn naive db g   coincides  i  v  learn gem db g    so  parameters updated
values  hence  starting initial values  parameters converge
values 
   

fisato   kameya

five conditions applicability graphical em algorithm may look hard
satisfy once  fortunately  modeling principle section     still stands 
due care modeling  likely lead us program meets them  actually 
see next section  programs standard symbolic statistical frameworks
bayesian networks  hmms pcfgs satisfy five conditions 
   complexity

section  analyze time complexity graphical em algorithm applied
various symbolic statistical frameworks including hmms  pcfgs  pseudo pcsgs
bayesian networks  results show graphical em algorithm competitive
specialized em algorithms developed independently research field 

    basic property

since em algorithm iterative algorithm since unable predict
converges  measure time complexity time taken one iteration  therefore
estimate time per iteration repeat loop learn gem  db  g   g   g            gt   
observe one iteration  support graph gt      scanned twice 
get inside probs  db  g   get expectations  db  g   scan  addition
performed t explanations  multiplication  possibly division  performed
msw atoms table atoms each  time spent gt per iteration
graphical em algorithm linear size support graph  i e  number nodes
support graph gt  put
 tdb def
 
db    
e

 

e


 db

num def
   max
j e j
tt db
maxsize def
 
max
jsej 

e
e
 tt s   db
set table atoms g   hence  t set t explanations
recall db

db
appearing right hand side      subsection      num maximum number
t explanations support graph gt s maxsize maximum size texplanation gt s respectively  following obvious 
e

proposition     time complexity graphical em algorithm per iteration linear

total size support graphs   nummaxsize   notation  coincides
space complexity graphical em algorithm runs support graphs 

rather general result  compare graphical em algorithm
em algorithms  must remember input graphical em algorithm
support graphs  one observed atom  actual total learning time
oldt time    the number iterations   o nummaxsizet  
   

fiparameter learning logic programs symbolic statistical modeling

 oldt time  denotes time construct support graphs g   sum
time oldt search time topological sorting table atoms 
latter part former order wise    represent  oldt time  time oldt
search  observe total size support graphs exceed time oldt
search g order wise 
evaluate oldt time specific class models hmms  need know
time table operations  observe oldt search paper special
sense table atoms always ground called resolution solved
goals  accordingly solution table used
check goal g already entry solution table  i e  called
before 
add new searched t explanation g list discovered t explanations
g s entry 
time complexity operations equal table access depends
program implementation solution table    first suppose
programs carefully written way arguments table atoms used indecies table access integers  actually programs used subsequent complexity
analysis  dbh subsection      dbg dbg  subsection      dbg subsection     
satisfy satisfy condition replacing non integer terms appropriate integers  suppose solution table implemented using array table
access done o    time   
follows  present detailed analysis time complexity graphical
em algorithm applied hmms  pcfgs  pseudo pcsgs bayesian networks  assuming
o    time access solution table  remark way space complexity
total size solution tables  support graphs  


    hmms

standard em algorithm hmms baum welch algorithm  rabiner        rabiner   juang         example hmm shown figure   subsection        given
observations w            wt output string length l  computes  n   lt   time
iteration forward probability fftm q    p  ot  ot       otm    q j   backward
probability fimt  q    p  otm otm         otl j q    state q   q  time step    l 
string wt   ot  ot       otl       q set states n number
states  factor n   comes fact every state n possible destinations

    think oldt search top goal gt   searches msw atoms table atoms create solution table  auxiliary computations  therefore time complexity never less
o jthe number msw atoms table atoms support graph gt j   coincides
time need topologically sort table atoms solution table depth first search     gt  
    sagonas et al         ramakrishnan et al         discuss implementation oldt 
    arrays available  may able use balanced trees  giving o log n  access time n
number data solution table  may able use hashing  giving average     time
access certain condition  cormen  leiserson    rivest        
    treat  state emission hmms  emit symbol depending state  another type 
 arc emission hmms  emitted symbol depends transition arc  treated similarly 
   

fisato   kameya

compute forward backward probability every destination every
state  computing ffmt  q  s fimt  q  s  parameters updated  so  total
computation time iteration baum welch algorithm estimated o n  lt  
 rabiner   juang        manning   schutze        
compare result graphical em algorithm  use hmm program
dbh figure   appropriate modifications l  length string  q 
state set  declarations fh output alphabets  string w   o o       ol 
hmm n q   om   om             ol    dbh reads hmm state q   q time n
output  om  om        ol  reaches final state  declaring hmm  
hmm   table predicate translation  see figure     apply oldt search goal
top hmm  o      ol   ans  w r t  translated program obtain t explanations
hmm  o      ol    complexity argument however  translated program
dbh same  talk terms dbh sake simplicity  search 
fix search strategy multi stage depth first strategy  tamaki   sato        
assume solution table accessible o    time    since length list
third argument hmm   decreases one recursion  finitely
many choices state transition output alphabet  search terminates  leaving
finitely many t explanations solution table figure   satisfy acyclic support condition respectively  sampling execution hmm l  w r t  dbh nothing
sequential decision process decisions made msw atoms exclusive 
independent generate unique string  means dbh satisfies t exclusiveness
condition  independence condition uniqueness condition respectively  so 
graphical em algorithm applicable set hierarchical systems t explanations
hmm wt        produced oldt search observations w            wt output
string  put wt   ot  ot       otl  follows

db
  fhmm m q  otm      otl   j   l      q   qg   fhmm  ot      otl  g
h

dbh

 

 

msw out q  m om    msw tr q  m q    
hmm m     q    otm        otl   







 

   
   l 
top goal hmm  ot       otl    o nl  calling patterns hmm  
call causes n calls hmm    implying occur o nl   n     o n  l 
calls hmm    since call computed due tabling mechanism 
num   o n   l   maxsize      applying proposition      reach
e

hmm m q   otm      otl   

q    q

proposition     suppose strings length l  suppose
table operation
 

oldt search done     time  oldt time dbh o n lt   graphical
em algorithm takes o n   lt   time per iteration n number states 

o n  lt   time complexity baum welch algorithm 
algorithm runs eciently baum welch algorithm   

graphical em

    o    possible translated program dbh section      identify goal pattern
hmm            first two arguments constants  integers  
    besides  baum welch algorithm graphical em algorithm whose input support graphs
generated dbh update parameters value initial values same 
   

fiparameter learning logic programs symbolic statistical modeling

way  viterbi algorithm  rabiner        rabiner   juang        provides
hmms ecient way finding likely transition path given input output
string  similar algorithm parameterized logic programs determines
likely explanation given goal derived  runs time linear size
support graph  thereby o n   l  case hmms  complexity viterbi
algorithm  sato   kameya        

    pcfgs

compare graphical em algorithm inside outside algorithm  baker 
      lari   young         inside outside algorithm well known em algorithm
pcfgs  wetherell        manning   schutze           takes grammar chomsky
normal form  given n nonterminals  production rule grammar takes form
  j  k    i  j  k n    nonterminals named numbers   n  
starting symbol  form   w   n w terminal  iteration 
computes inside probability outside probability every partial parse tree
given sentence update parameters production rules  time complexity
measured time per iteration  described n   number nonterminals 
l  number terminals sentence  o n   l t   observed sentences  lari
  young        
compare graphical em algorithm inside outside algorithm  start
propositional program dbg   fg   rg representing largest grammar
containing possible rules   j  k n nonterminals nonterminal   starting
symbol  i e  sentence 
fg
rg

d d    j  k   j   i  j  k n  d  d  numbersg
  fmsw  if  msw 
i d w   j   n  number  w terminalg

 

 
 
 
 
 



q i d  d       msw i  d  d     j  k    
q j  d  d    
q k  d   d   
n

q i d d        msw i d wd     













  i  j  k n 
  d    d    d  l

  n    l    

 
 
 
 
 



figure    pcfg program dbg
db g artificial parsing program whose sole purpose measure size
oldt tree   created oldt interpreter parses sentence w  w       wl 
    pcfg  probabilistic context free grammar  backbone cfg probabilities  parameters  assigned production rule  nonterminal
n production rules fa   ffi j   ng 
p
probability pi assigned   ffi    n  ni   pi      probability sentence
sum probabilities  leftmost  derivation s  latter product probabilities
rules used derivation 
    precise  oldt structure  case  tree dbg contains constants
 datalog program  never occurs need creating new root node 
   

fisato   kameya
   

td

q   d l 
  j n

  k n
q   d d    
q   d   l 

q   d   l 
   

 note  q
 

  k n

q   d d    
q k d   l 

q j d d    
q   d   l 

q j d d    
q k d   l 

q k d   l 

q   d   l 

q k d   l 

 k 

td  
q i d d  already appears
d   l   
d d l d  

td  

d   e l  
  j n

  k n
q j d e  
q   e l 

q j d e  
q k e l 

d   e e
  n 
  j n 

q i d e  
q j e e  
q   e l 

td

q k e l 

n

q

n

q j e e  
q   e l 
q   e l 

   
p i 

p    p    p n 

figure     oldt tree query

q   d l 

input sentence w  w       wl embedded program separately msw i d wd   
   l     second clauses rg  this treatment affect complexity argument   q i d  d   reads i th nonterminal spans position d  position d  
i e  substring wd         wd   first clauses q i d   d       msw         q j  d   d   
q k d   d   supposed textually ordered according lexicographic order
tuples hi  j  k  d    d    d i  parser  top goal set q     l     asks
parser parse whole sentence w  w        wl syntactic category      sentence  
make exhaustive search query oldt search    before  multistage depth first search strategy o    time access solution table assumed 
time complexity oldt search measured number nodes
oldt tree  let td k  oldt tree q k d l   figure    illustrates td   
   l      msw atoms omitted  seen  tree many similar
subtrees  put together  see note figure      due depth first strategy 
td    recursive structure contains td   
   subtree  nodes whose leftmost atom
underlined solution nodes  i e  solve leftmost atoms first time
entire refutation process  underlined atoms already computed subtrees
left    check solution table entries    already
 

 

    l prolog variable constant denoting sentence length 
    q table predicate 
    
 
    inductively proved td   
   contains every computed q i d  d      l           
d   l    n  d     d  l        
   

fiparameter learning logic programs symbolic statistical modeling

computed  o    time  since clauses ground  execution generates
single child node 
   
 k 
enumerate h   
  number nodes td td      k n   
 k 
 
    
figure     see h   
  o n  l   d     let hd    k n   number nodes
k 
td   
contained td       estimated o n    l          consequently  number
 k 
n
 
 
nodes newly created td    h   
  k   hd    n  l   d     result 
l
 
 
 
 
  
total time oldt search computed d   hd   o n l   size
support graph 
consider non propositional parsing program dbg    fg    rg  figure   
whose ground instances constitute propositional program dbg   dbg  probabilistic
variant dcg program  pereira   warren        q     q    between  
declared table predicate  semantically dbg  specifies probability distribution
atoms form fq  l  j l list terminalsg 
p

p

fg 

rg 

t  sj  sk    j   i  j  k n  numberg
  fmsw  sfi msw 
si  t w  j   n  number  w terminalg

 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

q  s     length s d   q  s     d     s     
q  i d  d  c  c  l  l      between d  d  d   
msw i c   j k   
q  j d  d  s c   c  l  l   
q  k d  d  s c   c  l  l   
q  i d s d  c  s c    w x  x     msw i c  w  

figure     probabilistic dcg program dbg 
top goal parse sentence    w           wl  q   w           wl    invokes
q  s     d      w         wl       measuring length input sentence
calling length          general  q  i d  d   c   c   l  l   works identically q i d   d   
three arguments  c    c  l  l   added  c  supplies unique trial id msws
used body  c  latest trial id current computation  l   l  d list holding
substring d  d    since added arguments affect shape

    focus subtree td    j   i  j   range   n   fif e  e    j     e    e l     gfi  
o  l   d      hence  number nodes td  o n    l   d      number nodes td   
   
 
neither td   
   n    l   d     
   td negligible  therefore hd
   
   
    number nodes tl   tl   negligible 
    make program simple possible  assume integer n represented ground term


 n 
z     



s      s            assume d  d  ground  goal between d   d   d  
returns integer d  time proportional jd    d j 
    omit obvious program length l sn   computes length sn list l o jlj  time 

sn  

def

   

fisato   kameya

search tree figure    extra computation caused length   o l 
one insertion between d  d  d   o nl   respectively    oldt time remains
o n   l    hence size support graph 
apply graphical em algorithm correctly  need confirm five conditions
applicability  rather apparent however oldt refutation topgoal form q   w         wl   w r t  dbg  terminates  leaves support graph
satisfying finite support condition acyclic support condition  t exclusiveness
condition independent condition hold refutation process faithfully
simulates leftmost stochastic derivation w       wl choice production
rule made msw si  sc  sj  sk    exclusive independent  trial ids different
different choices  
remains uniqueness condition  confirm it  let us consider another program dbg     modification dbg  first goal length s d  body
first clause first goal between d  d  d   second clause rg  moved
last position bodies respectively  dbg   dbg  logically equivalent 
semantically equivalent well viewpoint distribution semantics  think
sampling execution oldt interpreter top goal q  s  w r t  dbg  
variable  using multi stage depth first search strategy  easy see first
execution never fails  second oldt refutation terminates  sentence
 w            wl   returned s  third conversely  set msw atoms resolved upon
refutation uniquely determines output sentence  w            wl     hence 
sampling execution guaranteed always terminate  every sampling pf       pf    
uniquely generates sentence  observable atom  uniqueness condition satisfied
dbg     hence dbg   
sampling execution guaranteed always terminate  words 
grammar generate finite sentences  giving general answer seems
dicult  known parameter values pcfg obtained learning
finite sentences  stochastic derivation pcfg terminates probability
one  chi   geman         summary  assuming appropriate parameter values 
say parameterized logic program dbg  largest pcfg n nonterminal
symbols satisfies applicability conditions  oldt time sentence length
l o n   l     size support graph  proposition     
conclude
g

g

proposition     let db a  parameterized logic program representing pcfg n

nonterminals form dbg figure     g   g    g            gt sampled atoms
representing sentences length l  suppose table operation oldt search done
o    time  oldt search g one iteration learn gem respectively
done o n   l t   time 

   

between d  d  d  

called o n  l   d     times td      called

  o n  l   d       o nl   

pl  
d  

times  
    trial ids used refutation record rule used step derivation
w        wl  
    dbg   represent integers ground terms made   s    keep program short 
use integers instead ground terms however  first three arguments q               enough
check whether goal previously called not  check done o    time 
   
 

 

   

fiparameter learning logic programs symbolic statistical modeling
 n   l    

time complexity inside outside algorithm per iteration
 lari   young         hence algorithm ecient inside outside algorithm 

    pseudo pcsgs

pcfgs improved making choices context sensitive  one attempts
pseudo pcsgs  pseudo probabilistic context sensitive grammars  rule chosen probabilistically depending nonterminal expanded parent
nonterminal  charniak   carroll        
pseudo pcsg easily programmed  add one extra argument  n  representing
parent node  predicate q  i d  d  c  c  l  l   figure    replace
msw i c   j k   msw  n i  c   j k    since  leftmost  derivation sentence
pseudo pcsg still sequential decision process described modified program 
graphical em algorithm applied support graphs generated modified
program observed sentences correctly performs ml estimation parameters
pseudo pcsg 
pseudo pcsg thought pcfg rules form  n  i     i  j   i  k 
   n  i  j  k n   n parent nonterminal i  arguments previous
subsection carried minor changes  therefore  details omitted 

proposition     let db parameterized logic program pseudo pcsg n

nonterminals shown above  g   g   g            gt observed atoms representing
sampled sentences length l  suppose table operation oldt search done
o    time  oldt search g iteration learn gem completed
o n   l t   time 

    bayesian networks

relationship cause c effect e often probabilistic one diseases symptoms  mathematically captured conditional
probability p  e   e j c   c  effect e given cause c  wish know however
inverse  i e  probability candidate cause c given evidence e  i e  p  c   c j e   e 
calculated bayes  theorem p  e   e j c   c p  c   c   c  p  e   e j c  
c  p  c   c     bayesian networks representational computational framework fits
best type probabilistic inference  pearl        castillo et al         
bayesian network graphical representation joint distribution p  x    x           
xn   xn   finitely many random variables x            xn   graph dag  directed
acyclic graph  ones figure     node random variable   
graph  conditional probability table  cpt  representing p  xi   xi j  i   ui 
   n   associated node xi  i represents xi s parent nodes ui
values  xi parent  i e  topmost node graph  table
marginal distribution p  xi   xi   whole joint distribution defined product
p

    deal discrete cases 
   

fisato   kameya



b





c

b

c

e



f

e

  g     singly connected

f

  g     multiply connected

figure     bayesian networks
conditional distributions 
p  x    x            xn

  xn      

n

i  

p  xi   xi j  i   ui   

    

thus graph g  figure    defines
pg  a  b  c  d  e  f     pg  a pg  b pg  c j a pg  d j a  b pg  e j d pg  f j d 
a  b  c  d  e f values corresponding random variables a  b  c   d  e
f   respectively    mentioned before  one basic tasks bayesian networks
compute marginal probabilities  example  marginal distribution pg  c  d 
computed either           below 
pg  c  d   
pg  a pg  b pg  c j a pg  d j a  b pg  e j d p  f j d 
    
 

 

 

 

 

 

 

 

x

 

a b e f

 

 

 

 

 

 

x
 

a b

 

  

pg   a pg   b pg   c j a pg   d j a  b a  

 
x

e f

pg   e j d pg   f j d a     

     clearly ecient       observe graph g 
figure     would way factorize computations      use      requiring
exponentially many operations  problem computing marginal probabilities
np hard general  factorization      assured graph singly
connected g    i e  loop viewed undirected graph  case 
computation possible o jv j  time v set vertices graph  pearl 
       otherwise  graph called multiply connected  might need exponential time
compute marginal probabilities  sequel  show following 
discrete bayesian network g defining distribution pg  x            xn   
parameterized logic program dbg predicate bn    pdb  bn x        xn   
  pg x            xn   
g

    thanks acyclicity graph  without losing generality  may assume xi ancestor
node xj     j holds 
    notational simplicity  shall omit random variables confusion arises 
   

fiparameter learning logic programs symbolic statistical modeling

arbitrary factorizations order compute marginal distribution 

exists tabled program accomplishes computation specified way 
graph singly connected evidence e given  exists tabled
program dbg oldt time bn e  o jv j   hence time
complexity per iteration graphical em algorithm o jv j  well 
let g bayesian network defining joint distribution pg x            xn   fpg  xi  
xi j  i   ui   j   n  xi   val xi    ui   val  i  g conditional probabilities
associated g val xi  set xi s possible values val  i  denotes
set possible values parent nodes  i random vector  respectively 
construct parameterized logic program defines distribution pg  x            xn   
program dbg   fg   rg shown figure    


fg

  f msw par i ui  once xi  j   n  ui   val  i   xi   val xi  g

rg

  f bn x         xn    

 i  once xi   g

vn

i   msw par i 

figure     bayesian network program dbg
fg comprised msw atoms form msw par i ui   once xi   whose probability
exactly conditional probability pg  xi   xi j  i   ui   xi parents  ui
empty list     rg singleton  containing one clause whose body conjunction
msw atoms corresponds product conditional probabilities  note
intentionally identify random variables x            xn logical variables x            xn
convenience 

proposition     dbg denotes distributions g 

 proof  let hx            xn realization random vector hx           xn i  holds
construction
pdbg  bn x         xn   

 

n

h  
n


pmsw  msw par i ui   once xi   

 
pg  xi   xi j  i   ui  
h  
  pg  x            xn   
q e d 
case g  figure     program becomes  
bn a b c d e f 

  

msw par  a      once a  
msw par  c   a   once c  
msw par  e   d   once e  

      a      b          prolog constants used place integers 
   

msw par  b      once b  
msw par  d   a b   once d  
msw par  f   d   once f  

fisato   kameya

left to right sampling execution gives sample realization random vector
h a  b  c  d  e  f i  marginal distribution computed bn x         xn   adding new
clause dbg  example  compute pg  c  d   add bn c d    bn a b c d e f 
dbg  let result db g   compute pdb   bn c d   equal
pg  c  d 
pdb   bn c d     pdb    a  b  e  f bn a b c d e f   
 
pdb  bn a b c d e f   
a b e f
  pg  c  d  
regrettably computation corresponds       factorization       ecient
probability computation using factorization made possible carrying summations
proper order 
next sketch example carry specified summations specified
order introducing new clauses  suppose joint distribution p  x  y  z  w   
   x      y  z  w   x  z  w    x       y  z  w     x  z  w  respectively
computed atoms p   x y   p   y z w  p   x z w   suppose hope
compute sum
p  x      x   
   y  z  w     x  z  w 
 

 

 

g 

 

g 

x

g 

g 

 

x

x



z w

 

first eliminate z  w y  corresponding elimination  introduce
two new predicates  q x y  compute    x  y    z w    y  z  w    x  z  w  p x 
compute p  x       x  y   x  y  follows 
p

p

p x 
q x y 

   

p  x y   q x y  
p  y z w   p   x z w  

note clause body q   contains z w  existentially quantified  local variables
clause head q x y  contains variables shared atoms  view
correspondence    easy confirm program realizes
required computation  easy see generalizing example  though
prove here  exists parameterized logic program carries given
summations given order arbitrary bayesian network  particular
able simulate  variable elimination  zhang   poole        d ambrosio       
approach 
ecient computation marginal distributions always possible
well known class bayesian networks  singly connected bayesian networks 
exists ecient algorithm compute marginal distributions message passing  pearl 
      castillo et al          show graph singly connected 
construct ecient tabled bayesian network program dbg assigning table predicate
node  avoid complications  explain construction procedure informally
concentrate case one interested variable  let g singly
p



   

fiparameter learning logic programs symbolic statistical modeling

connected graph  first pick node u whose probability pg  u  seek 
construct tree g root node u g  letting nodes dangling u  
figure    shows g  transformed tree select node b root node 
b





e

f

c


transformed graph g 

figure     transforming g  tree
examine node g one one  add node x graph
corresponding clause dbg whose purpose visit nodes connected x except
one calls x   suppose started root node u  figure    evidence
u given  generated clause       proceed inner node x  u  calls
x    original graph g  x parent nodes fu    u   u g child nodes fv    v  g  u 
topmost node g 


u 
x
u 

v 
u 

v 

tree g

figure     general situation
node x figure     add clause       called parent node
u  u  ground  first generate possible values u  calling val u   u   
call call x u   u    visit nodes connected x u    u  similary
treated  visiting nodes g connecting x parent nodes u 
u   nodes connected u  already visited   value random variable x
determined sampling msw atom jointly indexed  x  values u    u 
   

fisato   kameya
u    visit x  s children  v  v    topmost node u  original graph 
add clause      
tbn u       msw par  u        once u     call
call

u  x  u    

u  x  u       val u   u    call x u   u    
val u   u    call x u   u    
msw par  x   u   u   u     once x  
call x v   x   call x v  x  

    

    

    
let dbg final program containing clauses                  apparently
dbg constructed time linear number nodes network 
note successive unfolding  tamaki   sato        atoms form call       
clause bodies starts      yields program db g similar one
figure    contains msw atoms call        s  dbg db g define
distribution    proved proposition     pg  u    pdb   bn u    
pdb  tbn u   holds  details omitted   way  figure    assume construction
starts topmost node u  evidence u given  necessary 
suppose change start inner node x  case  replace clause     
call x u  u       msw par  u       once u          time
replace head clause      tbn   add goal call x u   u  body
on  changed program db  g   rather straightforward prove
pdb    tbn      pg u  holds  true construction tabled program
dbg shown crude lot room optimization  suces
show parameterized logic program singly connected bayesian network runs
o jv j  time v set nodes 
estimate time complexity oldt search w r t  dbg   declare tbn every
predicate form call        table predicate verify five conditions
applicability graphical em algorithm  details omitted   estimate time
complexity oldt search goal tbn u  w r t  dbg     notice calls occur
according pre order scan  parents   node   children  tree g   calls
call x     occur val y   times  call call x     invokes calls rest
nodes  x  s parents x  s children graph g except caller node  diffrent
set variable instantiations  second call on  every call refers solutions
stored solution table o    time  thus  number added computation steps
call

x u  u       msw par  u        once u    







g

g



g






    since distribution semantics based least model semantics  unfold fold transformation  tamaki   sato        preserves least herbrand model transformed program  unfold fold
transformation applied parameterized logic programs preserves denotation transformed
program 
    dbg transformed oldt interpreter collect msw atoms case hmm
program 
   

fiparameter learning logic programs symbolic statistical modeling

oldt search x bounded above  constant o val u   val u   val u   val x   
case figure     result oldt time proportional number nodes
original graph g  and size support graph  provided number
edges connecting node  values random variable bounded
above 

proposition     let g singly connected bayesian network defining distribution pg  

v set nodes  dbg tabled program derived above  suppose number
edges connecting node  values random variable bounded
constant  suppose table access done o    time  then  oldt time
computing pg u  observed value u random variable u means dbg
o jv j  time per iteration required graphical em algorithm 
observations  time complexity o jv jt   

o jv j  time complexity required compute marignal distribution singly
connected bayesian network standard algorithm  pearl        castillo et al         
em algorithm using it  therefore conclude graphical
em algorithm ecient specialzed em algorithm singly connected bayesian
networks    must quickly add graphical em algorithm applicable
arbitrary bayesian networks    proposition     says explosion
support graph avoided appropriate programming case singly connected
bayesian networks 
summarize  graphical em algorithm  single generic em algorithm  proved
time complexity specialized em algorithms  i e  baum welch algorithm
hmms  inside outside algorithm pcfgs  one singly connected
bayesian networks developed independently research field 
table   summarizes time complexity em learning using oldt search
graphical em algorithm case one observation  first column   sc bns 
represents singly connected bayesian networks  second column shows program use 
dbh hmm proram subsection      dbg  pcfg program subsection    
dbg transformed bayesian network program subsection      respectively  oldt time
third column time oldt search complete search t explanations 
gem fourth column time one iteration taken graphical em algorithm
update parameters  use n     l v respectively number states
hmm  number nonterminals pcfg  length input string
number nodes bayesian network  last column standard  specialized  em
algorithm model 


    marginal distribution pg one variable required  construct similar
tabled program computes marginal probabilities still o jv j  time adding extra arguments
convey evidence embedding evidnece program 
    check five conditions dbg figure     uniqueness condition obvious sampling
always uniquely generates sampled value random variable  finite support condition
satisfied finite number random variables values  acyclic support
condition immediate acyclicity bayesian networks  t exclusiveness condition
independent condition easy verify 
   

fisato   kameya

model
program
hmms
dbh
pcfgs
dbg 
dbg
sc bns
user model


oldt time
 n  l 
 m   l    
o jv j 
o joldt treej 

gem
specialized em
 
o n l 
baum welch
 
 
 m l  
inside outside
o jv j 
 castillo et al        
 jsupport graphj 

table    time complexity em learning oldt search graphical em algorithm

    modeling language prism

developing symbolic statistical modeling laguage prism since       url
  http   mi cs titech ac jp prism   implementation distribution semantics
 sato        sato   kameya        sato         language intented modeling
complex symbolic statistical phenomena discourse interpretation natural language
processing gene inheritance interacting social rules  programming language 
looks extension prolog new built in predicates including msw predicate
special predicates manipulating msw atoms parameters 
prism program comprised three parts  one directives  one modeling
one utilities  directive part contains declarations values  telling system
msw atoms used execution  modeling part set non unit definite
clauses define distribution  denotation  program using msw atoms 
last part  utility part  arbitary prolog program refers predicates defined
modeling part  use utility part learn built in predicate carry
em learning observed atoms 
prism provides three modes execution  sampling execution correponds
random sampling drawn distribution defined modeling part  second
one computes probability given atom  third one returns support set
given goal  execution modes available built in predicates 
must report however implementation graphical em algorithm
simpified oldt search mechanism way  completed yet 
currently  prolog search learn naive db  g  section   available em learning though realized  partially  structrure sharing explanations implemention
learn naive db  g    putting computational eciecy aside however  problem
expressing learning hmms  pcfgs  pseudo pcsgs  bayesian networks
probailistic models current version  learning experiments next section
used parser substitute oldt interpreter  independently implemented
graphical em algorithm 
   learning experiments

complexity analysis graphical em algorithm popular symbolic probabilistic
models previous section  look actual behavior graphical em algorithm
real data section  conducted learning experiments pcfgs using two
   

fiparameter learning logic programs symbolic statistical modeling

corpora contrasting characters  compared performance graphical
em algorithm inside outside algorithm terms time per iteration
   time updating parameters   results indicate graphical em algorithm
outperform inside outside algorithm orders magnigude  detalis reported
sato  kameya  abe  shirai         proceeding  review inside outside
algorithm completeness 

    inside outside algorithm

inside outside algorithm proposed baker        generalization
baum welch algorithm pcfgs  algorithm designed estimate parameters
cfg grammar chomsky normal form containing rules expressed numbers   j  k
   i  j  k n n nonterminals    starting symbol   suppose input
sentence w            wl given 
iteration  first computes bottom manner
 
inside probabilities  e s  t  i    p  i   ws          wt  computes outside probabilities
f  s  t  i    p  s  
w            ws   wt             wl   top down manner every s 
   l    n    computing probabilities  parameters
updated using them  process iterates predetermined criterion
convergence likelihood input sentence achieved  although baker
give analysis inside outside algorithm  lari young        showed
takes o n   l    time one iteration lafferty        proved em algorithm 
true inside outside algorithm recognized standard em
algortihm training pcfgs  notoriously slow  although much literature
explicitly stating time required inside outside algorithm  carroll   rooth       
beil  carroll  prescher  riezler    rooth         beil et al         reported example
trained pcfg       rules corpus         german subordinate clauses whose average ambiguity       trees clause using four machines     mhz
sun ultrasparc      mhz sun ultrasparc ii     took     hours complete
one iteration  discuss later inside outside algorithm slow 

    learning experiments using two corpora

report parameter learning existing pcfgs using two corpora moderate size
compare graphical em algorithm inside outside algorithm terms
time per iteration  mentioned before  support graphs  input garphical em
algorithm  generated parser  i e  mslr parser    measurements made
   mhz sun ultrasparc ii  gb memory solaris     threshold
increase log likelihood input sentences set      stopping criterion
em algorithms 
experiments  used atr corpus edr corpus  each converted pos
 part speech  tagged corpus   similar size  about         contrasting
characters  sentence length ambiguity grammars  first experiment
employed atr corpus japanese english corpus  we used japanese part 
developed atr  uratani  takezawa  matsuo    morita         contains        short
    mslr parser tomita  generalized lr  parser developed tanaka tokunaga laboratory tokyo
institute technology  tanaka  takezawa    etoh        
   

fisato   kameya

conversational sentences  whose minimum length  average length maximum length
respectively             skeleton pcfg  employed context free grammar
gatr comprising     rules      nonterminals     terminals  manually developed
atr corpus  tanaka et al         yields     parses sentence 
inside outside algorithm accepts cfg chomsky normal form 
converted gatr chomsky normal form g atr   g atr contains       rules      nonterminals     terminals   divided corpus subgroups similar length
 l           l                   l            containing randomly chosen     sentences 
preparations  compare length graphical em algorithm applied
gatr g atr inside outside algorithm applied g atr terms time per
iteration running convergence 
 sec 

 sec 

 sec 

  
i o
  

   

    

   

     

   

  

   

    

i o
gem  original 
gem  chomsky nf 

     
    

  
   
  

     

   

  

    

   

     

 

  

  

  

  

l

l

l
 

gem  original 
gem  chomsky nf 

 

 

  

  

  

  

 

 

           

figure     time per iteration   i o vs  gem  atr 
curves figure    show learning results x axis length l input
sentence y axis average time taken em algorithm one iteration update
parameters contained support graphs generated chosen     sentences
 other parameters grammar change   left graph  inside outside
algorithm plots cubic curve labeled  i o   omitted curve drawn graphical
em algorithm drew x axis  middle graph magnifies left graph  curve
labeled  gem  original   plotted graphical em algorithm applied original
grammar gatr whereas one labeled  gem  chomsky nf   used g atr  length    
average sentence length  measured whichever grammar employed  graphical
em algorithm runs several hundreds times faster      times faster case gatr
    times faster case g atr  inside outside algorithm per iteration 
right graph shows  almost  linear dependency updating time graphical em
algorithm within measuared sentence length 
although difference anticipated learning speed  speed gap
inside outside algorithm graphical em algorithm unexpectedly large 
conceivable reason atr corpus contains short sentences gatr
   

fiparameter learning logic programs symbolic statistical modeling

much ambiguous parse trees sparse generated support graphs small 
affects favorably perforamnce graphical em algorithm 
therefore conducted experiment another corpus contains much
longer sentences using ambiguous grammar generates dense parse trees 
used edr japanese corpus  japan edr        containing         japanese news article
sentences  however process re annotation  part  randomly
sampled       sentences  recently made available labeled corpus  compared
atr corpus  sentences much longer  the average length       sentences    
minimum length    maximum length     cfg grammar gedr        rules 
converted chomsky normal form grammar g edr containing        rules  developed
ambiguous  to keep coverage rate             parses sentence length
              parses sentence length    
 sec 

    

 sec 

 sec 
 

  

    
i o

 

i o
gem  original 

   

gem  original 

 

    

 
   

    

 
 

    

 

    
 

l
                      

 

   

l
                      

 

l
                      

figure     time per iteration   i o vs  gem  edr 
figure    shows obtained curves experiments edr corpus  the graphical em algorithm applied gedr vs  inside outside algorithm applied g edr 
condition atr corpus  i e  plotting average time per iteration process    
sentences designated length  except plotted time inside outside algorithm average    iterations whereas graphical em algorithm
average     iterations  clear middle graph  time again  graphical
em algorithm runs orders magnitude faster inside outside algorithm  average sentence length     former takes       second whereas latter takes     seconds 
giving speed ratio          sentence length     former takes       seconds
latter takes       seconds  giving speed ratio          thus speed ratio even
widens compared atr corpus  explained mixed effects o l    
time complexity inside outside algorithm  moderate increase total size
support graphs w r t  l  notice right graph shows total size support
graphs grows sentence length l time per iteration graphical em algorithm
linear total size support graphs 

   

fisato   kameya

since implemented inside outside algorithm faithfully baker         lari
young         much room improvement  actually kita gave refined insideoutside algorithm  kita         implementation mark johnson
inside outside algorithm down loadable http   www cog brown edu   emj  
use implementations may lead different conclusions  therefore conducted
learning experiments entire atr corpus using two implementations
measured updating time per iteration  sato et al          turned implementations run twice fast naive implementation take     seconds per
iteration graphical em algorithm takes       second per iteration  still
orders magnitude faster former two  regrettably similar comparison using
entire edr corpus available moment abandoned due memory ow
parsing construction support graphs 
learning experiments far compared time per iteration ignore extra time
search  parsing  required graphical em algorithm  question naturally arises
w r t  comparison terms total learning time  assuming     iterations learning
atr corpus however  estimated even considering parsing time  graphical
em algorithm combined mslr parser runs orders magnitude faster three
implementations  ours  kita s johnson s  inside outside algorithm  sato et al  
       course estimation directly apply graphical em algorithm
combined oldt search  oldt interpreter take time parser
much time needed depends implementaiton oldt search   
conversely  however  may able take rough indication far approach 
graphical em algorithm combined oldt search via support graphs  go
domain em learning pcfgs 

    examing performance gap

previous subsection  compared performance graphical em algorithm
inside outside algorithm pcfgs given  using two corpora three
implementations inside outside algorithm  experiments  graphical em
algorithm considerably outperformed inside outside algorithm despite fact
time complexity  look causes performance
gap 
simply put  inside outside algorithm slow  primarily  lacks parsing 
even backbone cfg grammar explicitly given  take advantage
constraints imposed grammar  see it  might help review inside
probability e s  t  a   i e  p nonterminal spans s th word t th word   s t  
calculated inside outside algorithm given grammar 
e s  t  a   

r 
t  
x

p a   bc  e s  r  b e r      t  c  
s t  a bc grammar r s
p a   bc   probability associated production rule   bc   note
fixed triplet  s  t  a   usual term p a   bc  e s  r  b  e r     t  c   non zero
x

b c

    cannnot answer question right implementation oldt search completed 
   

fiparameter learning logic programs symbolic statistical modeling

relatively small number  b  c  r  s determined successful parses
rest combinations always give   term  nonetheless inside outside algorithm
attempts compute term every iteration possible combinations b  c
r repeated every possible  s  t  a   resulting lot redundancy 
kind redundancy occurs computation outside probability inside outside
algorithm 
graphical em algorithm free redundancy runs parse trees  a
parse forest  represented support graph    must added  hand 
superiority learning speed graphical em algorithm realized cost space
complexity inside outside algorithm merely requires o nl    space
array store probabilities  graphical em algorithm needs o n   l    space store
support graph n number nonterminals l sentence length 
trade off understandable one notices graphical em algorithm applied
pcfg considered partial evaluation inside outside algorithm
grammar  and introduction appropriate data structure output  
finally remark use parsing preprocess em learning pcfgs
unique graphical em algorithm  fujisaki  jelinek  cocke  black    nishino       
stolcke         approaches however still seem contain redundancies compared
graphical em algorithm  instance stolcke        uses earley chart compute
inside outside probability  parses implicitly reconstructed iteration
dynamically combining completed items 
   related work discussion

    related work

work presented paper crossroads logic programming probability
theory  considering enormous body work done fields  incompleteness
unavoidable reviewing related work  said that  look various attempts
made integrate probability computational logic logic programming    reviewing  one immediately notice two types usage probability  one type 
constraint approach  emphasizes role probability constraints necessarily seek unique probability distribution logical formulas  type 
distribution approach  explicitly defines unique distribution model theoretical means
proof theoretical means  compute various probabilities propositions 
typical constraint approach seen early work probabilistic logic nilsson
        central problem   probabilistic entailment problem   compute upper
lower bound probability p   target sentence way bounds
compatible given knowledge base containing logical sentences  not necessarily
logic programs  annotated probability  probabilities work constraints
    emphasize difference inside outside algorithm graphical em algorithm
solely computational eciency  converge parameter values starting
initial values  linguistic evaluations estimated parameters graphical em algorithm
reported sato et al         
    omit literature leaning strongly toward logic  logic s  concerning uncertainty  see overview
kyburg        
   

fisato   kameya

possible range p    used linear programming technique solve problem
inevitably delimits applicability approach finite domains 
later lukasiewicz        investigated computational complexity probabilistic
entailment problem slightly different setting  knowledge base comprises statements
form  h j g  u    u    representing u  p h j g  u    showed inferring
 tight  u    u  np hard general  proposed tractable class knowledge base called
conditional constraint trees 
uential work nilsson  frish haddawy        introduced deductive system probabilistic logic remedies  drawbacks  nilsson s approach 
computational intractability lack proof system  system deduces
probability range proposition rules probabilistic inferences unconditional
conditional probabilities  instance  one rules infers p  ff j        y 
p  ff   j      x   ff fi propositional variables  x y   x   designates
probability range 
turning logic programming  probabilistic logic programming formalized ng
subrahmanian        dekhtyar subrahmanian        constraint approach  program set annotated clauses form   f               fn   n
atom     n  basic formula  i e  conjunction disjunction
atoms  j    j n  sub interval        indicating probability range  query
   f                fn   n  answered extension sld refutation  formalization 
assumed language contains finite number constant predicate
symbols  function symbol allowed 
similar framework proposed lakshmanan sadri        syntactic restrictions  finitely many constant predicate symbols function
symbols 
different uncertainty setting  used annotated clauses form c b            bn
bi    n  atoms c   h ff          i  confidence level  represents
belief interval  ff          doubt interval             
expert clause 
seen above  defining unique probability distribution secondary concern
constraint approach  sharp contrast bayesian networks whole
discipline rests ability networks define unique probability distribution
 pearl        castillo et al          researchers bayesian networks seeking
way mixing bayesian networks logical representation increase inherently
propositional expressive power 
breese        used logic programs automatically build bayesian network
query  breese s approach  program union definite clause program set
conditional dependencies form p p j q           qn   p qi atoms 
given query  bayesian network constructed dynamically connects query
relevant atoms program  turn defines local distribution connected
atoms  logical variables appear atoms function symbol allowed 
ngo haddawy        extended breese s approach incorporating mechanism
ecting context  used clause form p a  j a                l            lk  
ai s called p atoms  probabilistic atoms  whereas lj  s context atoms disjoint
p atoms  computed another general logic program  satisfying certain restric   

fiparameter learning logic programs symbolic statistical modeling

tions   given query  set evidence context atoms  relevant ground p atoms
identified resolving context atoms away sldnf resolution  local bayesian network built calculate probability query  proved soundness
completeness query evaluation procedure condition programs
acyclic   domains finite 
instead defining local distribution query  poole        defined global distribution  probabilistic horn abduction   program consists definite clauses
disjoint declarations form disjoint  h   p      hn pn   specifies probability distribution hypotheses  abducibles  fh           hn g  assigned probabilities
ground atoms help theory logic programming  furthermore proved
bayesian networks representable framework  unlike previous approaches 
language contains function symbols  acyclicity condition imposed programs
semantics definable seems severe restriction  also  probabilities
defined quantified formulas 
bacchus et al         used much powerful first order probabilistic language
clauses annotated probabilities  language allows statistically quantified term
k  x j x  kx denote ratio individuals finite domain satisfying  x   
 x  satisfying  x   assuming every world  interpretation language 
equally likely  define probability sentence   given knowledge
   kb 

base kb limit limn      worlds
worlds  kb   worldsn    number
possible worlds containing n individuals satisfying   parameters used judging
approximations  although limit necessarily exist domain must finite 
showed method cope diculties arising  direct inference 
default reasoning 
linguistic vein  muggleton        others  formulated slps  stochastic
logic programs  procedurally  extension pcfgs probabilistic logic programs 
so  clause c   must range restricted    annotated probability p
p   c   probability goal g product ps appearing refutation
modification subgoal g invoke n clauses  pi   ci    n 
refutation step  probability choosing k th clause normalized pk   ni   pi 
recently  cussens              enriched slps introducing special class
log linear models sld refutations w r t  given goal  example considers
possible sld refutations general goal s x   defines probability p r 
refutation r p r    z    exp   r  i    number associated
clause ci  r  i  feature  i e  number occurrences ci r  z
normalizing constant  then  probability assigned s a  sum probabilities
refutation s a  



n

n



p

p

    condition says every ground atom must assigned unique integer n a  n a   
n b             n bn   holds ground instance clause form b            bn  
condition  program includes p x   q x     cannot write recursive clauses q
q  x   h jy    q x    
    syntactic property variables appearing head appear body clause  unit
clause must ground 
   

fisato   kameya

    limitations potential problems

approaches described far less similar limitations potential problems 
descriptive power confined finite domains common limitation  due
use linear programming technique  nilsson         due syntactic
restrictions allowing infinitely many constant  function predicate symbols  ng
  subrahmanian        lakshmanan   sadri         bayesian networks
limitation well  only finite number random variables representable    
various semantic syntactic restrictions logic programs  instance acyclicity
condition imposed poole        ngo haddawy        prevents unconditional
use clauses local variables  range restrictedness imposed muggleton
       cussens        excludes programs usual membership prolog program 
another type problem  possibility assigning con icting probabilities
logically equivalent formulas  slps  p a  p a   a  necessarily coincide
  may different refutations  muggleton        cussens              
consequently slps  would trouble naively interpret p a  probability
a s true  assigning probabilities arbitrary quantified formulas seems
scope approaches slps 
last least  big problem common approach using probabilities 
numbers come from  generally speaking  use n binary random variables
model  determine  n probabilities completely specify joint distribution 
fulfilling requirement reliable numbers quickly becomes impossible n grows 
situation even worse unobservable variables model
possible causes disease  apparently parameter learning observed data natural
solution problem  parameter learning logic programs well studied 
distribution semantics proposed sato        attempt solve problems
along line global distribution approach  defines distribution  probability
measure  possible interpretations ground atoms arbitrary logic program
first order language assigns consistent probabilities closed formulas 
distribution semantics enabled us derive em algorithm parameter learning
logic programs first time  naive algorithm however  dealing large
problems dicult exponentially many explanations observation
hmms  believe eciency problem solved large extent
graphical em algorithm presented paper 

    em learning

since em learning one central issues paper  separately mention work
related em learning symbolic frameworks  koller pfeffer        used
approach kbmc  knowledge based model construction  em learning estimate parameters labeling clauses  express probabilistic dependencies among events definite clauses annotated probabilities  similarly ngo haddawy s        approach 
locally build bayesian network relevant context evidence well
    however  rpms  recursive probability models  proposed pfeffer koller        extension
bayesian networks allow infinitely many random variables  organized attributes
classes probability measure attribute values introduced 
   

fiparameter learning logic programs symbolic statistical modeling

query  parameters learned applying constructed network specialized em
algorithm bayesian networks  castillo et al         
dealing pcfg statically constructed bayesian network proposed pynadath wellman         possible combine em algorithm method
estimate parameters pcfg  unfortunately  constructed network singly
connected  time complexity probability computation potentially exponential
length input sentence 
closely related em learning parameter learning log linear models  riezler        proposed im algorithm approach probabilistic constraint programming  im algorithm general parameter estimation algorithm incomplete data
log linear models whose probability function p x  takes form p x   
z    exp  ni    x   p   x               n   parameters estimated   x 
i th feature observed object x z normalizing constant  since feature
function x  log linear model highly exible includes distribution
pmsw special case z      price pay however  computational cost
z   requires summation exponentially many terms  avoid cost exact
computation  approximate computation monte carlo method possible  whichever
one may choose however  learning time increases compared em algorithm z     
fam  failure adjusted maximization  algorithm proposed cussens       
em algorithm applicable pure normalized slps may fail  deals special
class log linear models ecient im algorithm  statistical
framework fam rather different distribution semantics  comparison
graphical em algorithm seems dicult 
slightly tangential em learning  koller et al         developed functional
modeling language defining probability distribution symbolic structures
showed  cashing  computed results leads ecient probability computation
singly connected bayesian networks pcfgs  cashing corresponds computation inside probability inside outside algorithm computation outside
probability untouched 
p

    future directions

parameterized logic programs expected useful modeling tool complex symbolicstatistical phenomena  tried various types modeling  besides stochastic grammars bayesian networks  modeling gene inheritance kariera tribe
 white        rules bi lateral cross cousin marriage four clans interact
rules genetic inheritance  sato         model quite interdisciplinary 
exibility combining msw atoms means definite clauses greatly facilitated
modeling process 
although satisfying five conditions section  
uniqueness condition  roughly  one cause yields one effect 
finite support condition  there finite number explanations one observation 
acyclic support condition  explanations must cyclic 
   

fisato   kameya

t exclusiveness condition  explanations must mutually exclusive 
independence condition  events explanation must independent 

applicability graphical em algorithm seems daunting  modeling experiences far tell us modeling principle section   effectively guides us successful
modeling  return  obtain declarative model described compactly high level
language whose parameters eciently learnable graphical em algorithm shown
preceding section 
one future directions however relax applicability conditions 
especially uniqueness condition prohibits generative model failure
generating multiple observable events  although pointed section     mar
condition appendix b adapted semantics replace uniqueness condition
validates use graphical em algorithm even complete data uniquely
determine observed data case  partially bracketed corpora   pereira  
schabes         feel need research topic  investigating
role acyclicity condition seems theoretically interesting acyclicity often
related learning logic programs  arimura        reddy   tadepalli        
paper scratched surface individual research fields hmms 
pcfgs bayesian networks  therefore  remains much done clarifying
experiences research field ected framework parameterized logic
programs  example  need clarify relationship symbolic approaches
bayesian networks spi  li  z    d ambrosio  b         approach 
unclear compiled approach using junction tree algorithm bayesian
networks incorporated approach  aside exact methods  approximate
methods probability computation specialized parameterized logic programs must
developed 
direction improving learning ability introducing priors instead ml
estimation cope data sparseness  introduction basic distributions make
probabilistic switches correlated seems worth trying near future  important
take advantage logical nature approach handle uncertainty  example 
already shown sato        learn parameters negative examples
 the grass wet  treatment negative examples parameterized
logic programs still infancy 
concerning developing complex statistical models based  programs distributions  scheme  stochastic natural language processing exploits semantic information
seems promising  instance  unification based grammars hpsgs  abney       
may good target beyond pcfgs use feature structures logically describable  ambiguity feature values seems expressible probability
distribution 
building mathematical basis logic programs continuous random variables
challenging research topic 

   

fiparameter learning logic programs symbolic statistical modeling
   conclusion

proposed logical mathematical framework statistical parameter learning
parameterized logic programs  i e  definite clause programs containing probabilistic facts
parameterized probability distribution  extends traditional least herbrand
model semantics logic programming distribution semantics   possible world semantics
probability distribution possible worlds  herbrand interpretations 
unconditionally applicable arbitrary logic programs including ones hmms  pcfgs
bayesian networks 
presented new em algorithm  graphical em algorithm section   
learns statistical parameters observations class parameterized logic programs representing sequential decision process decision exclusive
independent  works support graph s  new data structure specifying logical relationship observed goal explanations  estimates parameters computing
inside outside probability generalized logic programs 
complexity analysis section   showed oldt search  complete tabled
refutation method logic programs  employed support graph construction
table access done o    time  graphical em algorithm  despite generality 
time complexity existing em algorithms  i e  baum welch algorithm
hmms  inside outside algorithm pcfgs one singly connected bayesian
networks developed independently research field  addition 
pseudo probabilistic context sensitive grammars n nonterminals  showed
graphical em algorithm runs time o n   l   sentence length l 
compare actual performance graphical em algorithm insideoutside algorithm  conducted learning experiments pcfgs section   using two
real corpora contrasting characters  one atr corpus containing short sentences
grammar much ambiguous      parses sentence   edr
corpus containing long sentences grammar rather ambiguous           
average sentence length      cases  graphical em algorithm outperformed
inside outside algorithm orders magnitude terms time per iteration 
suggests effectiveness approach em learning graphical em algorithm 
since semantics limited finite domains finitely many random variables
applicable logic programs arbitrary complexity  graphical em algorithm
expected give general yet ecient method parameter learning models complex
symbolic statistical phenomena governed rules probabilities 
acknowledgments

authors wish thank three anonymous referees comments suggestions 
special thanks go takashi mori shigeru abe stimulating discussions learning
experiments  tanaka tokunaga laboratory kindly allowing use
mslr parser linguistic data 

   

fisato   kameya
appendix a  properties

pdb

appendix  list properties pdb defined parameterized logic program
db   f   r countable first order language l    first all  pdb assigns consistent
probabilities   every closed formula l
pdb    def
  pdb  f   
db j   j  g 
guaranteeing continuity sense
limn   pdb   t              tn     pdb   x x  
limn   pdb   t              tn     pdb   x x  
t    t          enumeration ground terms l 
next proposition  proposition a    relates pdb herbrand model  prove
it  need terminology  factor closed formula prenex disjunctive normal
form q        qnm qi    n  either existential quantification universal
quantification matrix  length quantifications n called rank
factor  define   set formulas made factors  conjunctions disjunctions 
associate formula   multi set r   ranks
 
factor quantification
r     
fng
factor rank n
r      r                      
  stands union two multi sets  instance f       g f       g   f                g 
use multi set ordering proof proposition a   usual induction
complexity formulas work 
lemma a   let boolean formula made ground atoms l  pdb    
pf  f  
f j mdb     j  g  
 proof  prove lemma conjunction atoms form d x  
       dnx  xi   f    g    n  
pdb  d x          dnx     pdb  f   
db j   j  d x          dnx g 
  pdb  d    x            dn   xn 
  pf  f  
f j mdb     j  d x          dnx g  q e d 
 
 
 
 
 

 

n

 

n

n

 

 

n

proposition a   let closed formula l  pdb     pf  f  
f j mdb    j  g  
    definitions
f   pf   mdb     
db   pdb others used below  see section   
    consistent  mean probabilities assigned logical formulas respect laws probability
  p  a     p   a        p  a  p  a   b     p  a    p  b     p  a   b   
   

fiparameter learning logic programs symbolic statistical modeling

 proof  recall closed formula equivalent prenex disjunctive normal form
belongs    prove proposition formulas   using induction
multi set ordering fr   j    g  r        quantification 
proposition correct lemma a    suppose otherwise  write   g q  q       qn f  
q  q       qnf indicates single occurrence factor g    assume q     x
 q     x similarly treated   assume bound variables renamed avoid
name clash  g  xq       qn f   equivalent  xg q       qnf   light validity
  xa    b    x a   b    xa    b    x a   b  b contains free x 
pdb      pdb  g q  q       qnf   
  pdb   xg  q       qn f  x   
  klim
p  g  q       qn f  t              g q       qn f  tk    
   db
  klim
p  g  q       qn f  t             q       qn f  tk    
   db
  klim
p  f  
f j mdb     j  g  q       qnf  t            q       qnf  tk    g 
   f
 by induction hypothesis 
  pf  f  
f j mdb     j   xg q       qnf  x  g 
  pf  f  
f j mdb     j  g 
q e d 
next prove theorem iff definition introduced section    distribution
semantics considers program db   f   r set infinitely many ground definite
clauses f set facts  with probability measure pf   r set rules 
clause head r appears f   put
head r  def
  fb j b appears r clause headg 
b   head r   let b wi  i                enumeration clauses b r 
define iff  b   iff  if and only if  form rules b db  
iff  b  def
  b   w    w         
since mdb     least herbrand model  following obvious 
lemma a   b head r   
f   mdb    j  iff  b  
theorem a   iff  b   states general level  sides iff
definition p x     y   x   t    w              yn  x   tn   wn  p    coincide random
variables whenever x instantiated ground term 
theorem a   let iff  b     b   w    w        iff form rules b   head r  
pdb  iff  b        pdb  b     pdb  w    w          
    expression e   e     means may occur specified positions e         e          
indicates single occurrence       positive boolean formula e   e             e         e       holds 
    definition different usual one  lloyd        doets        talking ground
level  w    w          true one disjuncts true 
   

fisato   kameya

 proof 

pdb  iff  b   

 

pdb  f   
db j   j  b    w    w         g 
 pdb  f   
db j   j   b     w    w         g 

  klim
p  f   
db j   j  b  
   db

k
 

i  

wi g 

  klim
p  f   
db j   j   b    
   db
  klim
p  f  
f j mdb     j  b  
   f

k
 
i  

k
 
i  

wi g 

wi g 
k
 

  klim
p  f  
f j mdb     j   b     wi g 
   f
i  
 lemma a   
  pf  f  
f j mdb     j  iff  b g 
  pf  
f    lemma a   
   
follows pdb  iff  b      
pdb  b     pdb  b   iff b      pdb  w    w           
q e d 
prove proposition useful probability computation  let db  b  
support set atom b introduced section    it set explanations b  
sequel  b ground atom  write db  b    fs    s        g db  b    s   s         
define set  b
 b def
  f   
db j   j  b   db  b g 
proposition a   every b   head r   pdb  b       pdb  b    pdb  db  b   
 proof  first prove pdb   b       proof exactly parallels theorem a  
except w    w          replaced s    s         using fact b   s    s        
true every least herbrand model form mdb      pdb   b       

pdb  b     pdb  b    b  
db  b    
  pdb   db  b   
q e d 
finally  show distribution semantics probabilistic extension traditional
least herbrand model semantics logic programming proving theorem a    says
probability mass distributed exclusively possible least herbrand models 
define   set least herbrand models generated fixing r varying subset
f program db   f   r  symbols 
w

 

w

 

 

w

    set k   fe    e         g formulas  k denotes   n infinite  disjunction e    e         
   

fiparameter learning logic programs symbolic statistical modeling

  def
  f   
db j     mdb     
f g 
note   merely subset
db   cannot conclude pdb         priori 
next theorem  theorem a    states pdb          i e  distribution semantics distributes
probability mass exclusively    i e  possible least herbrand models 
prove theorem  need preparations  recalling atoms outside head r  
f chance proved db  introduce
   def
  f   
db j   j   d every ground atom    head r    f g 
herbrand interpretation    
db    jf   
f   restriction   atoms
f  
lemma a   let    
db herbrand
interpretation 
 
    mdb      
f iff         j  b   db  b   every b   head r  
 proof  only if part immediate property least herbrand model 
if part  suppose   satisfies right hand side  show     mdb   jf     
mdb    jf   coincide w r t  atoms head r   enough prove give
truth values atoms head r   take b   head r  write db  b   
s    s         suppose   j  b   s    s           j  b     j  sj j  
thereby  jf j  sj   hence mdb   jf   j  sj   implies mdb   jf   j  b  otherwise
  j   b     j   sj every j   follows mdb    jf   j   b   since b arbitrary 
conclude   mdb   jf   agree truth values assigned atoms head r 
well 
q e d 
w

w

theorem a   pdb        

 proof  lemma a   
    f   
db j     mdb      
f g
      
 b  
 

b head r 

pdb   b       proposition a    prove pdb            let d   d          enumeration
atoms belonging head r    f   provable db   f   r 
hence false every least herbrand model mdb        
f   
pdb      

  mlim
   pdb  f   
db j   j   d            dm g 
  mlim
   pf  f  
f j mdb     j   d             dm g 
  pf  
f       
since countable conjunction measurable sets probability measure one
probability measure one  follows pdb   b       every b   head r  pdb       
  pdb         
q e d 
   

fisato   kameya
appendix b  mar  missing random  condition

original formulation em algorithm dempster et al          assumed
exists many to one mapping    x  complete data x incomplete
 observed  data y  case parsing  x parse tree input sentence x
uniquely determines y  paper  uniqueness condition ensures existence
many to one mapping explanations observations  however sometimes face
situation many to one mapping complete data incomplete
data nonetheless wish apply em algorithm 
dilemma solved introduction missing data mechanism
makes complete data incomplete  missing data mechanism  m  distribution
g  m j x  parameterized   observed data  described    x   says
x becomes incomplete m  correspondence x   i e  fhx  j  m y  
 x  g naturally becomes many to many 
rubin        derived two conditions g  data missing random data
observed random  collectively called mar  missing random  condition  showed
assume missing data mechanism behind observations satisfies mar
condition  may estimate parameters distribution x simply applying
em algorithm y  observed data 
adapt mar condition parameterized logic programs follows  keep
generative model satisfying uniqueness condition outputs goals g parse
trees  extend model additionally inserting missing data mechanism
g observation    g  assume satisfies mar
condition  extended model many to many correspondence explanations observations  generates non exclusive observations p  o   o       
 o    o     causes p  o    p  o    g  m o   g  pdb  g   thanks
mar condition however  still allowed apply em algorithm nonexclusive observations  put differently  even uniqueness condition seemingly
destroyed  em algorithm applicable  imaginarily  assuming missing data
mechanism satisfying mar condition 
p

p



references

abney  s          stochastic attribute value grammars  computational linguistics         
        
arimura  h          learning acyclic first order horn sentences entailment 
proceedings eighth international workshop algorithmic learning theory 
ohmsha springer verlag 
bacchus  f   grove  a   halpern  j     koller  d          statistical knowledge bases
degrees belief  artificial intelligence             
baker  j  k          trainable grammars speech recognition  proceedings spring
conference acoustical society america  pp          

   

fiparameter learning logic programs symbolic statistical modeling

beil  f   carroll  g   prescher  d   riezler  s     rooth  m          inside outside estimation
lexicalized pcfg german  proceedings   th annual meeting
association computational linguistics  acl      pp          
breese  j  s          construction belief decision networks  computational intelligence                 
carroll  g     rooth  m          valence induction head lexicalized pcfg  proceedings  rd conference empirical methods natural language processing
 emnlp    
castillo  e   gutierrez  j  m     hadi  a  s          expert systems probabilistic
network models  springer verlag 
charniak  e     carroll  g          context sensitive statistics improved grammatical language models  proceedings   th national conference artificial
intelligence  aaai      pp          
chi  z     geman  s          estimation probabilistic context free grammars  computational linguistics                  
chow  y     teicher  h          probability theory   rd ed    springer 
clark  k          negation failure  gallaire  h     minker  j   eds    logic
databases  pp           plenum press 
cormen  t   leiserson  c     rivest  r          introduction algorithms  mit press 
cussens  j          loglinear models first order probabilistic reasoning  proceedings
  th conference uncertainty artificial intelligence  uai      pp          
cussens  j          parameter estimation stochastic logic programs  machine learning 
                
d ambrosio  b          inference bayesian networks  ai magazine  summer        
dekhtyar  a     subrahmanian  v  s          hybrid probabilistic programs  proceedings
  th international conference logic programming  iclp      pp          
dempster  a  p   laird  n  m     rubin  d  b          maximum likelihood incomplete
data via em algorithm  royal statistical society  b             
doets  k          logic logic programming  mit press 
flach  p     kakas  a   eds            abduction induction   essays relation
integration  kluwer academic publishers 
frish  a     haddawy  p          anytime deduction probabilistic logic  journal
artificial intelligence             
fujisaki  t   jelinek  f   cocke  j   black  e     nishino  t          probabilistic parsing
method sentence disambiguation  proceedings  st international workshop
parsing technologies  pp        
japan edr  l          edr electronic dictionary technical guide   nd edition   technical
report  japan electronic dictionary research institute  ltd 
   

fisato   kameya

kakas  a  c   kowalski  r  a     toni  f          abductive logic programming  journal
logic computation                 
kameya  y          learning representation symbolic statistical knowledge  in
japanese   ph  d  dissertation  tokyo institute technology 
kameya  y     sato  t          ecient em learning parameterized logic programs 
proceedings  st conference computational logic  cl       vol      
lecture notes artificial intelligence  pp           springer 
kita  k          probabilistic language models  in japanese   tokyo daigaku syuppan kai 
koller  d   mcallester  d     pfeffer  a          effective bayesian inference stochastic programs  proceedings   th national conference artificial intelligence
 aaai      pp          
koller  d     pfeffer  a          learning probabilities noisy first order rules  proceedings   th international joint conference artificial intelligence  ijcai     
pp            
kyburg  h          uncertainty logics  gabbay  d   hogger  c     robinson  j   eds   
handbook logics artificial intelligence logic programming  pp          
oxford science publications 
lafferty  j          derivation inside outside algorithm em algorithm 
technical report  ibm t j watson research center 
lakshmanan  l  v  s     sadri  f          probabilistic deductive databases  proceedings
     international symposium logic programming  ilps      pp          
lari  k     young  s  j          estimation stochastic context free grammars using
inside outside algorithm  computer speech language           
li  z     d ambrosio  b          ecient inference bayes networks combinatorial
optimization problem  international journal approximate reasoning            
lloyd  j  w          foundations logic programming  springer verlag 
lukasiewicz  t          probabilistic deduction conditional constraints basic
events  journal artificial intelligence research              
manning  c  d     schutze  h          foundations statistical natural language processing  mit press 
mclachlan  g  j     krishnan  t          em algorithm extensions  wiley
interscience 
muggleton  s          stochastic logic programs  de raedt  l   ed    advances
inductive logic programming  pp           ios press 
ng  r     subrahmanian  v  s          probabilistic logic programming  information
computation               
ngo  l     haddawy  p          answering queries context sensitive probabilistic
knowledge bases  theoretical computer science               
nilsson  n  j          probabilistic logic  artificial intelligence            
   

fiparameter learning logic programs symbolic statistical modeling

pearl  j          probabilistic reasoning intelligent systems  morgan kaufmann 
pereira  f  c  n     schabes  y          inside outside reestimation partially bracketed
corpora  proceedings   th annual meeting association computational linguistics  acl      pp          
pereira  f  c  n     warren  d  h  d          definite clause grammars language analysis
  survey formalism comparison augmented transition networks 
artificial intelligence              
pfeffer  a     koller  d          semantics inference recursive probability models 
proceedings seventh national conference artificial intelligence  aaai     
pp          
poole  d          probabilistic horn abduction bayesian networks  artificial intelligence                 
pynadath  d  v     wellman  m  p          generalized queries probabilistic context free
grammars  ieee transaction pattern analysis machine intelligence         
      
rabiner  l  r          tutorial hidden markov models selected applications
speech recognition  proceedings ieee                  
rabiner  l  r     juang  b          foundations speech recognition  prentice hall 
ramakrishnan  i   rao  p   sagonas  k   swift  t     warren  d          ecient tabling
mechanisms logic programs  proceedings   th international conference
logic programming  iclp      pp           mit press 
reddy  c     tadepalli  p          learning first order acyclic horn programs entailment  proceedings   th international conference machine learning 
 and proceedings  th international conference inductive logic programming   morgan kaufmann 
riezler  s          probabilistic constraint logic programming  ph d  thesis  universitat
tubingen 
rubin  d          inference missing data  biometrika                  
sagonas  k   t   s     warren  d          xsb ecient deductive database engine 
proceedings      acm sigmod international conference management
data  pp          
sato  t          statistical learning method logic programs distribution semantics 
proceedings   th international conference logic programming  iclp     
pp          
sato  t          modeling scientific theories prism programs  proceedings ecai   
workshop machine discovery  pp        
sato  t          minimum likelihood estimation negative examples statistical abduction  proceedings ijcai    workshop abductive reasoning  pp        
sato  t     kameya  y          prism  language symbolic statistical modeling 
proceedings   th international joint conference artificial intelligence
 ijcai      pp            
   

fisato   kameya

sato  t     kameya  y          viterbi like algorithm em learning statistical
abduction  proceedings uai     workshop fusion domain knowledge
data decision support 
sato  t   kameya  y   abe  s     shirai  k          fast em learning family pcfgs 
titech technical report  dept  cs  tr         tokyo institute technology 
shen  y   yuan  l   you  j     zhou  n          linear tabulated resolution based prolog
control strategy  theory practice logic programming                
sterling  l     shapiro  e          art prolog  mit press 
stolcke  a          ecient probabilistic context free parsing algorithm computes
prefix probabilities  computational linguistics                  
tamaki  h     sato  t          unfold fold transformation logic programs  proceedings
 nd international conference logic programming  iclp      lecture notes
computer science  pp           springer 
tamaki  h     sato  t          old resolution tabulation  proceedings  rd
international conference logic programming  iclp      vol      lecture notes
computer science  pp         springer 
tanaka  h   takezawa  t     etoh  j          japanese grammar speech recognition
considering mslr method  proceedings meeting sig slp  spoken
language processing      slp        pp           information processing society
japan  japanese 
uratani  n   takezawa  t   matsuo  h     morita  c          atr integrated speech
language database  technical report tr it       atr interpreting telecommunications research laboratories  japanese 
warren  d  s          memoing logic programs  communications acm         
       
wetherell  c  s          probabilistic languages  review open questions  computing surveys                  
white  h  c          anatomy kinship  prentice hall 
zhang  n     poole  d          exploiting causal independence bayesian network inference  journal artificial intelligence research             

   


