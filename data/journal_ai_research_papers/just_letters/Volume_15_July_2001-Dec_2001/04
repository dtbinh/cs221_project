journal articial intelligence research                  

submitted        published      

analysis reduced error pruning
elomaa cs helsinki fi
matti kaariainen cs helsinki fi

tapio elomaa
matti kriinen
department computer science
p  o  box     teollisuuskatu    
fin       university helsinki  finland

abstract

top down induction decision trees observed suer inadequate
functioning pruning phase  particular  known size resulting
tree grows linearly sample size  even though accuracy tree
improve  reduced error pruning algorithm used representative
technique attempts explain problems decision tree learning 
paper present analyses reduced error pruning three dierent settings 
first study basic algorithmic properties method  properties hold independent input decision tree pruning examples  examine situation
intuitively lead subtree consideration replaced leaf node 
one class label attribute values pruning examples independent
other  analysis conducted two dierent assumptions  general
analysis shows pruning probability node tting pure noise bounded
function decreases exponentially size tree grows  specic analysis
assume examples distributed uniformly tree  assumption lets
us approximate number subtrees pruned receive
pruning examples 
paper claries dierent variants reduced error pruning algorithm 
brings new insight algorithmic properties  analyses algorithm less imposed
assumptions before  includes previously overlooked empty subtrees
analysis 
   introduction
decision tree learning usually two phase process  breiman  friedman  olshen    stone 
      quinlan        

training examples

first tree reecting given sample faithfully possible

constructed  noise prevails  accuracy tree perfect

used build tree  practice  however  data tends noisy 
may introduce contradicting examples training set 

overtted

necessarily obtained even training set 


hence       accuracy cannot

case  resulting decision tree

sample  addition general trends data  encodes

pruned

peculiarities particularities training data  makes poor predictor
class label future instances  second phase induction  decision tree

order reduce dependency training data  pruning aims removing
tree parts likely due chance properties training set 
problems two phased top down induction decision trees well known
extensively reported  catlett        oates   jensen               size

c      ai access foundation morgan kaufmann publishers  rights reserved 

fielomaa   kriinen
tree grows linearly size training set  even though
accuracy gained increased tree complexity  obviously  pruning intended
ght eect  another defect observed data contains relevant attributes 
i e   class labels examples independent attribute values  clearly 
single node tree predicting majority label examples result case 
since help obtained querying attribute values  practice  though  often
large decision trees built data 
many alternative pruning schemes exist  mingers      a  esposito  malerba    semeraro        frank        

pruning examples

dier  e g   whether single pruned tree series

pruned trees produced  whether separate set

used  aspects

 classication error tree complexity  taken account pruning decisions 
aspects determined  whether single scan tree suces whether
iterative processing required 

basic pruning operation applied tree

replacement internal node together subtree rooted leaf 
elaborated tree restructuring operations used pruning techniques

majority leaf
pruning tree

 quinlan               paper  pruning operation considered
replacement subtree
examples reaching it  hence 

  i e   leaf labeled majority class
subtree original tree

zero  one  internal nodes changed leaves 

reduced error pruning

 subsequently

rep short  introduced quinlan       

context decision tree learning  subsequently adapted rule set learning
well  pagallo   haussler        cohen        
practical decision tree pruning

overprunes

rep

rep one simplest pruning strategies 

seldom used  disadvantage

requiring separate set examples pruning  moreover  considered aggressive
pruning strategy

decision tree  deleting relevant parts  quinlan 

      esposito et al          need pruning set often considered harmful
scarceness data  however  data mining context examples often
abundant setting part aside pruning purposes presents problem 
despite shortcomings

rep

baseline method performance

pruning algorithms compared  mingers      a  esposito  malerba    semeraro       
esposito et al         

presents good starting point understanding strengths

weaknesses two phased decision tree learning oers insight decision tree
pruning 

rep advantage producing smallest pruning among

accurate respect pruning set  recently  oates jensen        analyzed

rep attempt explain decision tree pruning fails control growth

tree  even though data warrant increased size  approach
subject  try avoid restricting analysis unnecessary assumptions 



consider explanation unwarranted growth size decision tree 

rep three dierent settings  first  explore basic algorep  apply regardless distribution examples presented

paper analyze
rithmic properties

learning algorithm  second  study  probabilistic setting  situation
attribute values independent classication example  even though
pure noise tting situation expected arise whole pruning set considered 
encountered lower levels tree  relevant attributes already
exhausted  assume subtrees receive least one pruning example 

   

fian analysis reduced error pruning
none directly pruned due receiving examples  class value
assigned random pruning examples  third analysis assumed
pruning example equal chance end one subtrees tree
pruned  rather theoretical setting lets us take account subtrees
receive examples  left without attention earlier analyses 
rest paper organized follows  next section discusses dierent
versions

rep

algorithm xes one analyzed subsequently  section

  review earlier analyses

rep 

basic algorithmic properties

section    then  section    carry probabilistic analysis
assumptions distribution examples 

rep examined
rep  without making

derive bound pruning

probability tree depends exponentially relation number pruning
examples size tree  section   presents analysis  assumes
pruning examples distribute uniformly subtrees tree  assumption lets us
sharpen preceding analysis certain aspects  however  bounds section   hold
certainty  section   approximate results  related research
briey reviewed section   and  nally  section   present concluding remarks
study 

   reduced error pruning algorithm

rep

never introduced algorithmically quinlan         source much

confusion  even though

rep

considered appears simple  almost trivial 

algorithm pruning  many dierent algorithms go name 
consensus exists whether

rep bottom up algorithm

iterative method  neither

obvious whether training set pruning set used decide labels leaves
result pruning 

    high level control
quinlan s        p          original description

rep clearly specify pruning

algorithm leaves room interpretation  includes  e g   following characterizations 
every non leaf subtree







examine change misclassications

test set would occur



replaced best possible leaf 

new tree would give equal fewer number errors
subtree property 

replaced leaf 

contains

process continues

replacements would increase number errors test
set 
      nal tree accurate subtree original tree respect
test set smallest tree accuracy 
quinlan        p       later continues give following description 
method  pessimistic pruning  two advantages  much faster
either preceding methods  cost complexity reduced error pruning 
since subtree examined once 

   

fielomaa   kriinen
one hand description requires nodes processed bottom up manner 
since subtrees must checked property pruning node but 
hand  last quotation would indicate

rep

rep

iterative method 

take

following single scan bottom up control strategy studies

 oates   jensen                    esposito et al               kearns   mansour        
nodes pruned single bottom up sweep decision tree  pruning node considered encountered 

nodes processed

postorder 
order node processing  tree candidate pruning cannot
contain subtree could still pruned without increasing tree s error 
due ambiguity
    a  mitchell        

rep s denition  dierent version rep lives  mingers 

probably due mingers         interpretation quinlan s

ambiguous denition 
nodes pruned iteratively  always choosing node whose removal
increases decision tree accuracy pruning set  process continues
pruning harmful 
however  algorithm appears incorrect  esposito et al               shown
tree produced algorithm meet objective accurate
subtree respect pruning set 

moreover  algorithm overlooks explicit

requirement checking whether subtree would lead reduction classication
error 
iterative algorithms could induced quinlan s original description  however  explicit requirement checking whether subtree could pruned pruning supertree obeyed  versions

rep

reduce ecient

bottom up algorithm 

    leaf labeling
another source confusion quinlan s        description

rep

clearly

specied choose labels leaves introduced tree

training

pruning  oates jensen        interpreted intended algorithm would label
new leaves according majority class

pruning

examples  analyzed

version algorithm new leaves obtain labels majority
examples  oates jensen motivated choice empirical observation

practice little dierence choosing leaf labels either way 
however  choosing labels pruned leaves according majority pruning examples
set leaves dierent status original leaves  label
majority class training examples 

example

figure   shows decision tree pruned single leaf

training examples used label pruned leaves  negative leaf replaces root
tree makes two mistakes pruning examples  original tree makes three
mistakes 

tree illustrate important dierence using training

   

fian analysis reduced error pruning





 



 



 

   

   

   

   

figure     part a  decision tree  labels inside nodes denote majority classes
training examples arriving nodes  leaves numbers pruning
examples two classes given 

x y

means

x negative





positive instances reach leaf 

pruning examples label pruned leaves  using training examples proceeding bottomup  observe neither subtree pruned  since left one replaced negative leaf
would make two mistakes instead original one mistake  similarly  right subtree
replaced positive leaf would result increased number classication errors 
nevertheless  root node even though subtrees pruned still
pruned 
pruning examples used label pruned leaves  node two non trivial
subtrees cannot pruned unless subtrees collapsed leaves 

next

section prove this  tree figure   subtrees would collapsed zeroerror leaves  however  case root node pruned 

possibility labeling leaf nodes would take training
pruning examples account deciding label pruned leaf 

depending

relation numbers training pruning examples strategy resembles one
above described approaches  usually training examples numerous
pruning examples  thus dominate  practice impossible discern
labeling strategy using majority training examples 

    empty subtrees
since

rep

uses dierent sets examples construct prune decision tree 

possible parts tree receive examples pruning phase 
parts decision tree  naturally  replaced single leaf without changing
number classication errors tree makes pruning examples 
words  subtrees obtain pruning examples always pruned  quinlan       
already noted parts original tree correspond rarer special cases 
represented pruning set  may excised 

   

fielomaa   kriinen
decisiontree rep  decisiontree t  examplearray  
        s length     classify  t  s i    
return prune      
void classify  decisiontree t  example e  
  t total      e label        t pos   
   update node counters
   leaf t   
  t test e         classify  t left  e   
else classify  t right  e     
int prune  decisiontree      output classification error pruning
    leaf t   
  t label        return t total   t pos 
else return t pos 
else
  error   prune  t left     prune  t right   
  error   min  t pos  t total   t pos    
return error 
else
  replace leaf 
  t pos   t total   t pos  
  t label      return t total   t pos   
else
  t label      return t pos         
table   

rep

algorithm  algorithm rst classies pruning examples top 

pass using method
tree using method

prune 

classify

bottom up pass prunes

intuitively  clear best founded strategy handling

empty subtrees

 

receive examples  one hand obtain support training
set  usually numerous pruning set but  hand  fact
pruning example corresponds parts tree would justify drawing
conclusion parts decision tree built chance properties training
data 

rep 

consistently preferring smaller prunings otherwise  latter view

adopted 
problem empty subtrees connected problem
learning algorithms  holte  acker    porter        
number training examples 

small disjuncts

machine

small disjunct covers small

collectively small disjuncts responsible

small number classication decisions  accumulate error whole
concept  nevertheless  small disjuncts cannot eliminated altogether  without adversely
aecting disjuncts concept 

   

fian analysis reduced error pruning
    analyzed pruning algorithm
let us briey reiterate details

rep algorithm analyzed subsequently 

al 

ready stated  control strategy algorithm single sweep bottom up processing 
first  top down traversal drives pruning examples tree appropriate
leaves  counters nodes en route updated  second  bottom up traversal pruning operations indicated classication errors executed 

errors

determined basis node counter values  bottom up traversal
node visited once  pruned leaves labeled majority pruning set
 see table    

   previous work
pruning decision trees recently received lot analytical attention  existing pruning
methods analyzed  esposito et al               oates   jensen             
      new analytically founded pruning techniques developed  helmbold  
schapire        pereira   singer        mansour        kearns   mansour        



many empirical comparisons pruning appeared  mingers      a  malerba  esposito 
  semeraro        frank         section review earlier work concerns

rep

algorithm  related research considered section   

esposito et al         viewed
search process state space 

rep

algorithm  among pruning methods 

addition noting iterative version

rep

cannot produce optimal result required quinlan         observed even
though
tree

rep linear time algorithm size tree  respect height
rep requires exponential time worst case  subsequent comparative

analysis esposito et al         sketched proof quinlan s        claim pruning
produced

rep

tree 
bias

smallest among accurate prunings given decision

rep briey examined oates jensen              

observed

rl   best majority leaf could replace subtree depends
 the class distribution   examples reach root n   words  tree
structure n decides error rl   let rt denote error subtree
moment pruning sweep reaches n   i e   pruning may already
taken place   pruning operations performed led either rt decrease
initial situation stay unchanged  case  pruning taken place
potentially decreases rt   aect rl   hence  probability rt   rl i e  
pruned increases pruning   error propagation bias
error 

inherent

rep 

oates jensen              conjecture larger original

tree smaller pruning set  larger eect  large tree provides
pruning opportunities high variance small pruning set oers random
chances

rl rt  

subsequently study eects exactly 

follow up study oates jensen        used

rep

vehicle explaining

problems observed pruning phase top down induction decision
trees  analyzed

rep situation

decision node consideration ts

noise i e   class examples independent value attribute tested
node hand built statistical model

   

rep

situation  indicates 

fielomaa   kriinen
consistently earlier considerations  even though probability pruning
node ts noise prior pruning beneath close    pruning occurs beneath
node reduces pruning probability close    particular  model shows even
one descendant node

n

depth

pruned  n

pruned  assuming

     consequence result increasing depth
leads exponential decrease node s pruning probability 
leaves depth

rst part oates jensen s        analysis easy comprehend  significance uncertain  situation rise bottom up pruning strategy 
statistical model based assumption number 

n  pruning instances

pass node consideration large  case independence assumptions prevailing errors committed node approximated normal
distribution  expected error original tree mean distribution  while 
pruned leaf  tree would misclassify proportion

n examples corresponds

minority class  oates jensen show latter number always less
mean standard distribution errors  hence  probability pruning
    approaches  

n grows 

second part analysis  considering pruning probability node

n

pruning taken place beneath it  oates jensen assume proportion
positive examples descendant
assuming

n

n

depth

n  

setting 


pruned 

positive majority  descendants level

positive majority  directly follows descendants level

replaced positive leaf  hence  function represented pruning identically
positive  majority leaf would replace
smaller pruning  therefore 

n

represents function

rep choose single leaf pruning 



n depth pruned 
n   subtrees maintained nodes

hand  one descendants
pruning tree rooted
level



pruned positive leaves  accurate majority leaf 

case tree pruned 
oates jensen        assume starting node level

proba 

bility routing example positive leaf same  following analyses try
rid unnecessary assumptions  results obtained without knowledge
example distribution 

   basic properties

rep

going detailed probabilistic analysis
basic algorithmic properties 

rep

algorithm  examine

throughout paper review binary case

simplicity  results  however  apply many valued attributes several classes 
processing control

rep

algorithm settled  actually

prove quinlan s        claim optimality pruning produced

rep 

observe

following result holds true independent leaf labeling strategy 

theorem   applying rep set pruning examples    decision tree produces
  pruning smallest prunings minimal
error respect example set s 
   

fian analysis reduced error pruning
proof prove claim induction size tree  observe decision
full binary tree   l t     nodes  l t   number leaves

tree

tree 

base case

 

l t       



original tree



consists single leaf node 





possible pruning itself  thus  is  trivially  smallest among
accurate prunings

t 

inductive hypothesis
inductive step l t     k

  claim holds

 

  let

n

right subtree  respectively  subtrees
pruning decision

n

l t     k 

t 

left

must strictly less



root tree

t 



t 

prunings trees 

t  



t    

k leaves 

inductive hypothesis 

smallest possible among accurate

 i   accuracy
n



taken  bottom up recursive control strategy

rep t  t  already processed algorithm 
subtrees pruning 

t 

  pruning decision node

n consists choosing whether collapse

tree rooted majority leaf  whether maintain whole

tree  alternatives make number errors 

n

collapsed

original accuracy respect pruning set retained  otherwise 

rep

algorithm  pruning decision based resulting trees would make

  hence  whichever choice made 
  make smaller number errors respect  
  
let us assume pruning makes even less errors respect
 
  
  
  
  must consist root n two subtrees t  t   
 
  
majority leaf cannot accurate   since accurate pruning
    must either t    accurate pruning t  t   t   
 
accurate pruning t  t    inductive hypothesis possibilities
 
false  therefore  accurate pruning  

less errors respect pruning set
resulting tree

 ii   size
 


  see chosen alternative small possible  rst assume

consists single leaf  tree smallest pruning

claim follows 

t   t    

 
otherwise  consists root node n

  case

two pruned subtrees

since tree collapsed  tree must accurate

tree consisting single majority leaf  assume exists pruning



    smaller  majority leaf less accurate
 



  must consist root node n two subtrees t  t    then  either
t  smaller pruning t  t     accurate  t  smaller pruning
t  t     accurate  cases contradict inductive hypothesis  hence 
  smallest among accurate prunings  




accurate

thus  case  claim follows

t 

 

consider next situation internal node tree  bottom up
pruning sweep reaches node 

committed leaf labeling

majority pruning examples 

   

fielomaa   kriinen

internal node  prior pruning leaves children 
pruned rep non trivial subtree bottom up pruning sweep reaches
it 
theorem  
proof

internal node

n

two possible cases non trivial

subtrees  either subtrees non trivial  non leaf   one trivial  let us
review cases 
let

rt

denote error  sub tree



respect part pruning set

  rl denote misclassication rate majority leaf l
  chosen pruned 

reaches root
would replace

case i  let two subtrees

  t  t    non trivial 

hence 

rt    rl  rt    rl   
t  t    respectively 
pruned  rt   rt    rt    must rt   rl    rl   
t  t  majority class  majority class  
rl   rl    rl    l majority leaf corresponding   otherwise 
rl rl    rl    case  rl rl    rl    combining fact
rt   rl    rl  means rt   rl   hence  pruned 

retained pruning sweep passed them  thus 

l 



case ii  let





l 

majority leaves would replace

one trivial subtree  produced pruning  one non 

t  non trivial l 
t  pruning process  then  rt    rl    hence 
rt   rt    rl    rl    rl   
way case i  deduce rl rl    rl    therefore 
rt   rl retained pruned tree 

trivial subtree  assume  without loss generality 
majority leaf replaced



cannot pruned either case  pruning process stopped branch



containing
node

unless original leaf appears along path root

n

 

t 

original leaf  may pruned even subtree

non trivial 

n

n

two trivial subtrees  may pruned  whether pruning

takes place depends class distribution examples reaching

n

subtrees 

analysis oates jensen        shown prerequisite pruning
node

n

tree descendants depth



pruned 

depth rst  original  leaf subtree rooted
result situation  corroborate nding
descendants depth

retained 

n

n 





apply

pruned one

applying theorem   recursively gives

result 

tree rooted node n retained rep one
descendants n depth pruned 
corollary  

avoid analysis restricted leaf globally closest root  need

fringe

able consider set leaves closest root branches tree  let us
dene

decision tree contains node prior pruning leaf

   

fian analysis reduced error pruning

figure    fringe  black gray nodes   interior  white nodes   safe nodes
 black ones  decision tree  triangles denote non trivial subtrees 

child  furthermore  node subtree rooted node belonging

interior

safe nodes

fringe tree fringe  nodes belonging fringe make
tree 

belong fringe tree 

parent interior tree  see figure     fringe decision tree closed
downwards  safe nodes tree correspond leaves pruning it  observe
along path root safe node leaves  therefore 
pruning process ever reaches safe node  theorem   applies corresponding branch
on 
decision tree consideration pruned single majority leaf  safe
nodes need turned leaves point  necessarily simultaneously 
pruning sweep continues safe nodes  question whether node
pruned settled solely basis whether nodes path root
majority class  pruning whole tree characterized below 
let



tree pruned

set pruning examples  js j   n 

assume 

without loss generality  least half pruning examples positive  let
proportion positive examples

  p     





p

replaced majority

leaf  leaf would positive class label  assumptions prove
following 

tree pruned single leaf
subtrees rooted safe nodes pruned
least many positive negative pruning examples reach safe node  

theorem  

   

fielomaa   kriinen
proof

begin show two conditions necessary pruning

show former condition fullled 
leaf  second  prove neither
latter not 
hold 





t 

first 

cannot pruned single

pruned former condition holds 

third  show suciency conditions  i e   prove



pruned single leaf 



 i   let us rst assume

safe node

denition safe node  parent
therefore  theorem   
neither root



p

p



n

n

pruned 

originally leaves children 

pruned 

easy see  inductively 

pruned 

 ii   let us assume subtrees rooted safe nodes get pruned
one safe nodes



negative positive pruning examples

fall  observe safe nodes cannot such  let us consider pruning



leaves situated place safe nodes  leaves receive
examples original safe nodes 

safe nodes internal nodes 

rep

corresponding pruned leaves labeled majority pruning examples 
particular  safe nodes receive negative positive examples
replaced negative leaves  leaves labeled positive  pruning
original tree accurate majority leaf  hence  theorem   
prune



rep



single leaf tree 

 iii   let us assume subtrees rooted safe nodes



pruned

least many positive negative pruning examples reach safe node 
interior nodes must majority positive pruning examples  otherwise 

negative positive examples  thus 
n majority negative examples  carrying
induction way safe nodes shows node n exist  
hence  interior prunings represent function  identically positive 
error respect   majority leaf unique 

interior node

n



least one children

smallest prunings will  theorem    chosen 

 
   probabilistic analysis

rep

let us turn attention question prerequisites pruning decision
tree



single majority leaf are  since  theorem   

rep

produces pruning



accurate respect pruning set small
possible  show



reduce single leaf suces nd pruning

better prediction accuracy pruning examples majority leaf has 
following class example assumed independent attribute
values  obviously  decision tree node assumption holds
examples arriving it  would pruning algorithm turn majority leaf 
make assumptions decision tree  however  similar analysis
oates jensen         obtained bounds tight  shortest path
root tree leaf short 

   

fian analysis reduced error pruning
    probability theoretical preliminaries
let us recall basic probabilistic concepts results used subsequently 
denote probability event

p

x
x b  n  p  

 integer valued  random variable
  denoted

e



prfe g

ee  

binomially distributed parameters n
expectation

said

discrete

 

n k
prf x   k g  
p    p n k   k                 n 
k


x b  n  p   expected
p value mean ex     np  variance varx   np   p  
  np   p  

indicator variable

standard deviation


   indicator variable


a           

discrete random variable takes values  



used denote occurrence non occurrence event 

pn

independent events

x 

ia

prfai g   p ia            ia

n

respective

i  
bernoulli
p
density function fx           
x
fx  x    prf x   x g
cumulative
distribution
function
f
 

 
      
x
p
indicator variables 

ia



called



binomially distributed parameters

random variable parameter



discrete random variable

 

n p 

 



dened

fx  y    prf x g   xy fx  x  
let x b  n  p  random variable mean   np standard
p
  np   p   normalized random variable corresponding x



x



dened

x

xe  





deviation

 

central limit theorem approximate cumulative distribution function

e
x



normal gaussian distribution


n

fxe  y    pr xe



 y  

cumulative distribution function bell curve density function e
respectively  apply
able

x

normal approximation

fx  y    prf x g   fxe

x      

fxe

p

   

corresponding random vari 





 



    bounding pruning probability tree
now  pruning set considered sample distribution class
attribute independent attributes 

assume class attribute

 p  distribution  i e   class positive probability
p negative probability   p  assume p       
distributed according bernoulli

following analyze situation subtrees rooted safe nodes
already pruned leaves  bound pruning probability tree starting
initial conguration 

since bottom up pruning may already come

halt situation  following results actually give high probability
pruning  hence  following upper bounds tight possible 

   

fielomaa   kriinen
consider pruning decision tree

rep

trial whose result decided set



pruning examples  theorem   approximate probability tree
pruned majority leaf approximating probability

safe nodes get

positive majority negative majority  latter alternative probable
assumption

p      

safe assume never happens 

consider sampling pruning examples two phases  first attribute values
assigned 

decides leaf example falls 

second phase

independently assign class label example 

z  t     fz            zk g let number examples
pruning set js j   n  number pruning examples falling safe node zi
pk
denoted ni  
i   ni   n  time assume ni     i  number
positive examples falling safe node zi sum independent bernoulli variables
and  thus  binomially distributed parameters ni p  respectively  number
negative pruning examples safe node zi xi b  ni    
p   probability
majority negative examples safe node zi prf xi   ni    g  bound
let safe nodes tree





probability using following inequality  slud        

lemma    slud s inequality 
m   q  h mq 

let x b m  q  random variable q     
 

h mq
 
prf x h g   p
mq   q 
p      random variable corresponding number negative examples
zi xi b  ni    p   rst condition slud s inequality holds  furthermore 
see condition m  
q  h mq holds safe node zi substitute h   ni       ni  
q    
p obtain ni p ni   ni   p   thus 
since

safe node



pr xi  


ni

 

 

 

   ni   p 
    p p     ni  
  nip
ni p   p 
ni p   p 

   

ni   number pruning instances reaching safe node zi   grows  standard

normal distribution term bound grows  hence  bound probability
majority pruning examples reaching

zi

negative smaller

pruning examples reach it  probability negative majority reduces
growing probability positive class example 

p 

reected

pruning probabilities whole tree 
roughly approximate probability
majority leaf follows  theorem   
node








pruned single

pruned leaf safe

receives majority positive examples 





k

safe nodes

n pruning examples  according pigeon hole principle least half safe
r    n k examples  safe node zi ni r examples has 

nodes receive

inequality    negative majority least probability

 

  p p     r  
rp   p 
   

fian analysis reduced error pruning
observe inequality   holds

ni   r  becausepthe cumulative

distribution

increasing function  argument ni  p       ni p   p  canpbe rewritten
p

ni cp   cp ispa positive constant depending value p  since   ni cp   grows
ni grows   
  ni cp   grows decreasing ni   hence  lower bound inequality
  applies values     ni   r  

function

thus  probability half safe nodes receive

r

examples

positive majority

p p     r
rp   p 

  k  

 

   

upper bound probability whole tree



pruned single

leaf  distribution assumption made reach result

p      

order obtain tighter bounds  one make assumptions shape tree
distribution examples 
bound equation   depends size decision tree  reected

n

p





k  

number     class distribution     pruning examples  keeping parameters
constant letting

k

grow reduces pruning probability exponentially  number

pruning examples grows proportion

r    n k

stays constant 

pruning probability still falls exponentially  class distribution pruning examples
aects pruning probability smaller  closer

p value    

    implications analysis
empirically observed size decision tree grows linearly
training set size  even trees pruned  catlett        oates   jensen       
       analysis gives us possibility explain behavior  however  let us
rst prove correlation attribute values class label
example  size tree perfectly ts training data depends linearly
size sample 
setting simple be  one real valued attribute
attribute

y  whose value independent



x 

before 



x class

two possible values 

     tree built using binary splits numerical value range  i e   propositions
type

x   r

assigned internal leaves tree 

analysis duplicate

instances occur probability   

let training examples  x  y  drawn distribution  x uniformly distributed range        obtains value    independent x  probability
p  value   probability   p  expected size decision tree ts
data linear size sample 
theorem  

  h x    y              xt   yt  i sample described distribution 
xi    xj      j   probability complement event   
let us  further  assume examples indexed x    x            xt  
let ai indicator variable event instances     dierent class
labels  i e   yi    yi          eai   prf ai     g   p   p     p p    p   p  
proof

let

may assume

   

fielomaa   kriinen
yi     probability p  time event yi      
  p  vice versa  number class alternations   pti    ai

event
probability

expectation

ea  
let



 
x
i  

eai  

 
x
i  

 p   p     p   p 

 
x
i  

      t   p   p  

decision tree grown sample

continued training error    leaf

 a  b         



yi    yy    



xi



xi  



s 

   

growing

corresponds half open interval

must fall dierent leaves

t 



  thus  upper boundary b
xi falls must value less xi    

otherwise one example falsely classied
interval corresponding leaf

repetitively applying observation scanning examples left

must least one leaf x  one leaf class alternation 
    leaves total  using equation   see expected number leaves

right  see
i e  






ea         t   p   p      
particular  linear size sample   js j   t 

 

theorem concerns zero training error trees built rst phase
decision tree induction  empirical observations catlett        oates jensen
              however  concern decision trees pruned second phase
induction  come back topic pruned trees shortly 
consider

rep used practice 

amount  classied  data available

application domain  let total

examples available  part
  reserved

data used tree growing remaining portion
separate pruning set 

        

quite common practice use two thirds data

growing one third pruning nine tenths growing one tenth pruning
 ten fold  cross validation used  decision tree construction phase tree
tted

fft examples perfectly possible 

hypothesize previous result

holds noisy real world data sets  empirical evidence would appear
case  number safe nodes grows linearly number leaves 
tree grown contain



safe nodes 

     since pruning set size
r    n k stays constant setting 

linear fraction training set size  ratio

hence  equation    growing data set size forces pruning probability zero  even
quite fast  reduction probability exponential 

    limitations analysis
empty subtrees  receive pruning examples  left without attention
above  assumed

ni    



i 

empty subtrees  however  decisively aect

analysis  automatically pruned away  unfortunately  one cannot derive non trivial
upper bound number empty subtrees  worst case pruning examples
routed safe node  leaves

k   empty safe nodes tree 

subsequently

review case examples distributed uniformly safe nodes 
better approximations obtained 

   

fian analysis reduced error pruning
even though assume pruning example positive higher probability
    guarantees majority examples positive 

however 

probability majority examples changes small  even negligible 
cherno  s inequality  cherno        hagerup   rb        number pruning

n  high p extremely close one half 
prf x h g  used bound
probability prf x   h g  continuity correction could used compensate this 
examples 

slud s inequality bounds probability

practice  inexactness make dierence 
even though would appear number safe nodes increases proportion leaves size training set grows  proved
result  theorem   essentially uses leaf nodes  lend modication 
safe nodes could substituted place leaves 
relation number safe nodes leaves decision tree depends
shape tree  hence  splitting criterion used tree growing decisively
aects relation  splitting criteria aim keeping produced split balanced
possible  others aim separating small class coherent subsets data  quinlan 
      mingers      b   example  common entropy based criteria bias
favors balanced splits  breiman         using balanced splitting criterion would seem
imply number safe nodes tree depends linearly number leaves
tree 

case reasoning would explain empirically observed linear

growth pruned decision trees 

   pruning probability uniform distribution
assume


k

n

pruning examples equal probability end

safe nodes  i e   pruning example falls safe node

zi

probability

  k 

contrary normal uniform distribution assumption analysis  analysis
best case  best distribution examples safe nodes would one pruning
example safe nodes except one  remaining pruning instances
would gather 

nevertheless  uniformity lets us sharpen general approximation

using standard techniques 

n k  let us calculate
cn k examples  c
event safe node zi receives

expected number examples falling safe node
expected number safe nodes receive

qpi indicator
k q number safe nodes receive less
i  
pk

linearity expectation eq  
i   eqi   keq   
last equality follows fact qi  s identically distributed 
let y  number examples reaching safe node z    n examples reaches z  probability   k independent examples  y  binomially
distributed parameters n   k   clearly eq    prf y  cn k g  approxiarbitrary positive constant  let


cn k examples 
cn k examples 



q 

mate last probability normal approximation  obtain

 
 


cn k
n k
 
c
  
n k
cn
pn   k      k    pn k     k   
pr y 
k
   

fielomaa   kriinen
hence  observation 

 

 c   n k  
eq   keq  k p
n k     k 


   

use approximation   determine probability whole decision tree
pruned single leaf  let

denote



p

random variable represent number

cn k examples least one example 
r number empty safe nodes  p   q r  hence  ep   e q r   

safe nodes

receive

eq er 

following result  kamath  motwani  palem    spirakis        motwani   raghavan 
      lets us approximate number empty safe nodes

theorem  

bins 

n k 

let z number empty bins balls thrown randomly h


  ez   h  

    

prf jz

 

h

j g   exp

m h

 

   h     
 
h   

result expected number empty safe nodes approximately
number small

ke

n k  



k relatively small compared n 
eq  equation    using pre 

substituting obtained approximation
vious result  get

 c   n k
ep   eq er k p
n k     k 

 

e

n k

 

 

applying slud s inequality can  before  bound probability
majority class change safe node receives
since

p

cn k

pruning examples 

safe nodes class distribution examples within

independent  event majority class change safe node receives
least one

cn k examples

upper bound

p p    r
rp   p 


r   cn k 

replacing

p

  p

 

   

expected value equation approxi 

mation pruning probability  approximation valid
expected value  consider deviation

p

p

deviate lot

expected value below 

upper bound pruning probability similar upper bound
obtained without assumptions distribution examples  however 
earlier constant   replaced new  controllable parameter
explicitly taken account 

c  empty subtrees

c chosen suitably  upper bound strict

one obtained general case 

   

fian analysis reduced error pruning

upper bound pruning probability
   
    

 

   

 

   
   
   

   
 

figure    eect parameters

p

   

   

c

   

p c upper bound pruning probability

tree     safe nodes     pruning examples used  curves
depicting          upper bounds shown 

    illustration upper bound
figure   plots upper bound pruning probability tree     safe nodes
    pruning examples used  value parameter

c varies     p varies

       observe surface corresponding upper bound stays
close   class distribution skewed parameter

c

small value  probability example positive class label

c approaches    upper bound climbs steeply  least
parameter c due inexactness approximation

hits value      value
part
extreme values 
probability

p

example positive class approaches    error

committed single positive leaf falls    hence  accuracy non trivial pruning
better  closer

p   beat majority leaf 

intuitively  probability

pruning exists i e   root node pruned drop zero

p increases 

bound reects intuition 

value parameter

c falls close    safe nodes taken account

upper bound receive pruning examples  number nodes

   

fielomaa   kriinen
small  hand 

c

increased  number nodes consideration

grows together upper limit number examples reaching single one
them  thus  small large values
value

c somewhere

c

yield loose bounds  strictest bounds

middle  example around values        

bound equation   argument cumulative distribution function
zero value

c small 

tends towards

time exponent decreases 

approaches      argument goes zero  hand  c
large value  approaches value   exponent p increases 
value

    exactness approximation
used expected value p analysis  ep   eq
er  probe
deviation p expected value  deviation r directly available
theorem   



 

   k     
 
k  e  r

prf jr erj g   exp

q similar result yet 

lipschitz condition

section provide one 

let us rst recapitulate denition

 

f   d  dm   ir real valued function arguments
f said satisfy lipchitz condition
x    d            xm   dm     f           mg  yi   di  
denition

let

possibly distinct domains  function

jf  x            xi     xi   xi            xm   f  x           xi     yi  xi             xm  j   

hence  function satises lipschitz condition arbitrary change value
one argument change value function   

martingales

following result  mcdiarmid        holds functions satisfying lipschitz condition  general results kind obtained using
 motwani   raghavan         

 see e g  

theorem    mcdiarmid  let x            xm independent random variables taking values
set v   let f   v   ir that               m 

sup

x       xm  yi  v

jf  x           xi    xi   xi            xm   f  x           xi     yi  xi             xm  j ci  

    
prf jf  x            xm   ef  x            xm  j g   exp

 
p 

c 
i  

 

 

wi                n  random variable wi   j i th example
directed safe node zj   uniform distribution assumption wi  s independent 
values within set f           k g  let us dene function f
f  w            wn   number safe nodes receive r   cn k examples 
i th example directed safe node zw   is 
let



f  w            wn     jf   f            k g j jsi j r gj 
   

fian analysis reduced error pruning


si set examples directed safe node zi  
si   f h   f            n g j wh   g 
q   f  w            wn   

hence 

moving one example one safe node another  chang 

ing value one argument
dition


f

pn

wi    change one safe node zi

jsij r  one less safe node fulll it  time 

fulll conthus  value

changes    hence  function fullls lipschitz condition  therefore 

apply mcdiarmid s inequality substituting

c    n 

i  

ci     observing

 
prf jf  w            wn   ef  w            wn  j g  e    n  
equally

 
prf jq eqj g  e    n  

unfortunately  concentration bound tight  nevertheless  combining
concentration bounds

q r p

following deviation expected

value 

since jp
ep j   jq r e q r j   jq eq   er rj jq eqj   jr erj 
jq r e q r j implies jq eqj    jr erj     thus 
prf jp ep j g   prf jq r e q r j g




pr jq eqj     pr jr erj  

 

 
 n     exp

  exp

 

   k     
 
  k  e  r 

   related work
traditional pruning algorithms cost complexity pruning  breiman et al          pessimistic pruning  quinlan         minimum error pruning  niblett   bratko        cestnik
  bratko         critical value pruning  mingers      a   error based pruning  quinlan 
      already covered extensively earlier work  mingers      a  esposito
et al         frank         thus touch methods further  instead 
review recent work pruning 

rep

produces optimal pruning given decision tree respect pruning

set  approaches producing optimal prunings presented  breiman
et al         bohanec   bratko        oliver   hand        almuallim         however 
often optimality measured training set  possible maintain
initial accuracy  assuming noise present  neither usually possible reduce
size decision tree without sacricing classication accuracy  example 
work bohanec bratko        studied eciently nd optimal
pruning sense output decision tree smallest pruning satises
given accuracy requirement  somewhat improved algorithm problem
presented subsequently almuallim        

   

fielomaa   kriinen
high level control kearns mansour s        pruning algorithm

cost complexity

bottom up sweep

rep 

however  pruning criterion method kind

condition  breiman et al         takes observed classication

error  sub tree complexity account 

moreover  pruning scheme

pessimistic

require pruning set separate training set  mansour s       
kearns mansour s        algorithms
 sub tree training error 

  try bound true error

since training error nature optimistic 

pruning criterion compensate pessimistic error approximation 
consider yet another variant

rep  one

otherwise similar one analyzed

above  exception original leaves put special status 
relabeled majority pruning examples internal nodes  version


rep

produces optimal pruning respect performance kearns

mansour s        algorithm measured  pessimistic pruning produces decision tree
smaller produced

rep 

kearns mansour        able prove algorithm strong performance guarantee  generalization error produced pruning bounded
best pruning given tree plus complexity penalty 
local sense

rep

pruning decisions

basic pruning operation replacing

subtree leaf used pruning algorithm 

   conclusion
paper

rep

algorithm analyzed three dierent settings 

first 

rep alone  without assuming anything input
setting possible prove rep fullls

studied algorithmic properties
decision tree pruning set 



intended task produces optimal pruning given tree  algorithm proceeds
prune nodes branch long subtrees internal node pruned
stops immediately even one subtree kept  moreover  prunes interior node
descendants level



pruned  furthermore 

rep

either halts

safe nodes reached prunes whole tree case safe nodes
majority class 
second setting tree consideration assumed noise  i e  
assumed class label pruning examples independent attribute
values  setting pruning probability tree could bound equation
depends exponentially size tree linearly number class
distribution pruning examples  thus  analysis corroborates main nding
oates jensen       

rep fails control growth decision tree extreme

case tree ts pure noise  moreover  analysis opened possibility initially
explain learned decision tree grows linearly increasing data set  bound
pruning probability tree based bounding probability safe nodes
majority class  surprisingly  essentially property  whose probability
try bound close    assumed hold probability   analysis oates
jensen        


rep

may happen pruning examples directed given subtree 

subtrees taken account earlier analyses 

   

nal analysis

fian analysis reduced error pruning
included empty subtrees equation tree s pruning probability 

taking empty

subtrees account gives realistic bound pruning probability tree 
unfortunately  one cannot draw denite general conclusions two phased topdown induction decision trees basis analyses
bias quite unique among pruning algorithms 

fact

rep algorithm 
rep penalize

size tree  rests classication error pruning examples makes
method sensitive small changes class distribution pruning set  decision
tree pruning algorithms individual characteristics  therefore  unied analysis
decision tree pruning may impossible 
version

rep 

one allowed relabel original leaves  well  used

performance objective kearns mansour s        pruning algorithm 

thus 

performance pruning algorithms use error size penalty related
use error estimation  version

rep

used kearns mansour

analysis based safe nodes applies leaves place safe nodes  hence
algorithm derived bounds stricter 
leave detailed analysis important pruning algorithms future work 
investigation possible disclose dierences similarities
pruning algorithms 

empirical examination managed reveal clear performance

dierences methods 

also  relationship number safe nodes

leaves tree ought examined analytically empirically  particular  one
study whether number safe nodes increase linearly growing training set 
conjectured paper  deeper understanding existing pruning algorithms may help
overcome problems associated pruning phase decision tree learning 

references

intelligence   

almuallim  h          ecient algorithm optimal pruning decision trees 
 

learning   

         

bohanec  m     bratko  i          trading accuracy simplicity decision trees 
 

            

regression trees

breiman  l   friedman  j  h   olshen  r  a     stone  c  j         
  wadsworth  pacic grove  ca 

tional joint conference articial intelligence

machine

classication

machine learning   
proceedings twelfth interna 

breiman  l          properties splitting criteria 
catlett  j          overpruning large decision trees 

articial

 

          

  pp          san mateo  ca  morgan

kaufmann 

machine learningewsl     proceedings fifth european working
lecture notes articial intelligence

cestnik  b     bratko  i          estimating probabilities tree pruning  kodrato 

session

y   ed   

  vol     

  pp          berlin  hei 

delberg  new york  springer verlag 

annals mathematical statistics   

cherno  h          measure asymptotic eciency tests hypothesis based
sum observations 

 

   

            

fielomaa   kriinen

proceedings thirteenth international joint conference articial

cohen  w  w         

intelligence

systems 

ecient pruning methods separate and conquer rule learning

  pp          san mateo  ca  morgan kaufmann 

machine learning  ecml     proceedings
lecture notes articial intelligence

esposito  f   malerba  d     semeraro  g         

sixth european conference

state space  brazdil  p  b   ed   

decision tree pruning search

  vol     

  pp 

        berlin  heidelberg  new york  springer verlag 

ieee transactions pattern analysis machine intelligence   
pruning decision trees lists
information processing
letters   
machine learning   
proceedings eleventh international joint conference articial
intelligence
proceedings thirty fifth annual
ieee symposium foundations computer science

esposito  f   malerba  d     semeraro  g          comparative analysis methods
pruning decision trees 
 

            

frank  e         

  ph d  thesis  university waikato 

department computer science  hamilton  new zealand 

hagerup  t     rb  c          guided tour cherno bounds 
 

            

helmbold  d  p     schapire  r  e          predicting nearly well best pruning
decision tree 

 

          

holte  r  c   acker  l     porter  b         

concept learning problem small

disjuncts 

  pp          san mateo  ca  morgan kaufmann 

kamath  a   motwani  r   palem  k     spirakis  p         

tail bounds occupancy

satisability threshold conjecture 

  pp          los alamitos 

ca  ieee press 

proceedings fifteenth inter 

kearns  m     mansour  y          fast  bottom up decision tree pruning algorithm

national conference machine learning

near optimal generalization  shavlik  j   ed   

  pp          san francisco  ca  morgan

kaufmann 

learning

malerba  d   esposito  f     semeraro  g          comparison simplication

data  ai statistics v
proceedings fourteenth international conference machine learning

methods decision tree induction  fisher  d     lenz  h  j   eds   

  pp          berlin  heidelberg  new york  springer verlag 

mansour  y          pessimistic decision tree pruning based tree size  fisher  d  h 
 ed   

 

pp          san francisco  ca  morgan kaufmann 

surveys combinatorics  invited papers   th british combinatorial conference
machine learning  
machine learning  
machine learning

mcdiarmid  c  j  h          method bounded dierences  siemons  j   ed   
  pp          cambridge  u k  cambridge university press 

mingers  j       a   empirical comparison pruning methods decision tree induction 
 

            

mingers  j       b   empirical comparison selection measures decision tree induction 

mitchell  t  m         

 

            

  mcgraw hill  new york 

   

fian analysis reduced error pruning
motwani  r     raghavan  p         
new york 

randomized algorithms

  cambridge university press 

research development expert systems iii

niblett  t     bratko  i          learning decision rules noisy domains  bramer  m  a 
 ed   

  pp        cambridge  uk 

cambridge university press 

proceedings fourteenth international conference

oates  t     jensen  d          eects training set size decision tree complexity 

machine learning

fisher  d  h   ed   

  pp          san francisco  ca  morgan kaufmann 

oates  t     jensen  d          large datasets lead overly complex models  expla 

proceedings fourth international conference knowledge discovery data
mining
proceedings sixteenth national conference articial intelligence
nation solution 

agrawal  r   stolorz  p     piatetsky shapiro  g   eds   

  pp          menlo park  ca  aaai press 

oates  t     jensen  d         

toward theoretical understanding

decision tree pruning algorithms fail 

  pp          menlo park  ca cambridge  ma  aaai

press mit press 

proceedings twelfth international conference machine
machine

oliver  j  j     hand  d  j          pruning averaging decision trees  prieditis  a  

learning
learning  

  russell  s   eds   

  pp          san francisco  ca  morgan kaufmann 

pagallo  g     haussler  d          boolean feature discovery empirical learning 
 

          

machine learning   

pereira  f     singer  y          ecient extension mixture techniques prediction
decision trees 

 

            

machine learning  
international journal man machine
c     programs machine learning
annals probability

quinlan  j  r          induction decision trees 

studies   

quinlan  j  r         
 

 

        

simplifying decision trees 

            

quinlan  j  r         

 

morgan kaufmann  san

slud  e  v          distribution inequalities binomial law 

 

mateo  ca 

 

            

   


