journal of artificial intelligence research                  

submitted       published      

infinite horizon policy gradient estimation
jonathan baxter

jbaxter   whizbang   com

whizbang  labs 
     henry street pittsburgh  pa      

peter l  bartlett

bartlett   barnhilltechnologies   com

biowulf technologies 
     addison street  suite      berkeley  ca      

abstract
gradient based approaches to direct policy search in reinforcement learning have received
much recent attention as a means to solve problems of partial observability and to avoid some of
the problems associated with policy degradation in value function methods  in this paper we introduce gpomdp  a simulation based algorithm for generating a biased estimate of the gradient of
the average reward in partially observable markov decision processes  pomdps  controlled by
parameterized stochastic policies  a similar algorithm was proposed by kimura  yamamura  and
kobayashi         the algorithms chief advantages are that it requires storage of only twice the
number of policy parameters  uses one free parameter fi           which has a natural interpretation
in terms of bias variance trade off   and requires no knowledge of the underlying state  we prove
convergence of gpomdp  and show how the correct choice of the parameter fi is related to the
mixing time of the controlled pomdp  we briefly describe extensions of gpomdp to controlled
markov chains  continuous state  observation and control spaces  multiple agents  higher order
derivatives  and a version for training stochastic policies with internal states  in a companion paper
 baxter  bartlett    weaver        we show how the gradient estimates generated by gpomdp
can be used in both a traditional stochastic gradient algorithm and a conjugate gradient procedure
to find local optima of the average reward 

   introduction
dynamic programming is the method of choice for solving problems of decision making under
uncertainty  bertsekas         however  the application of dynamic programming becomes problematic in large or infinite state spaces  in situations where the system dynamics are unknown  or
when the state is only partially observed  in such cases one looks for approximate techniques that
rely on simulation  rather than an explicit model  and parametric representations of either the valuefunction or the policy  rather than exact representations 
simulation based methods that rely on a parametric form of the value function tend to go by
the name reinforcement learning  and have been extensively studied in the machine learning
literature  bertsekas   tsitsiklis        sutton   barto         this approach has yielded some
remarkable empirical successes in a number of different domains  including learning to play checkers  samuel         backgammon  tesauro               and chess  baxter  tridgell    weaver 
       job shop scheduling  zhang   dietterich        and dynamic channel allocation  singh  
bertsekas        
despite this success  most algorithms for training approximate value functions suffer from the
same theoretical flaw  the performance of the greedy policy derived from the approximate valuefunction is not guaranteed to improve on each iteration  and in fact can be worse than the old policy

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fibaxter   bartlett

by an amount equal to the maximum approximation error over all states  this can happen even when
the parametric class contains a value function whose corresponding greedy policy is optimal  we
illustrate this with a concrete and very simple example in appendix a 
an alternative approach that circumvents this problemthe approach we pursue hereis to
consider a class of stochastic policies parameterized by    r k   compute the gradient with respect
to  of the average reward  and then improve the policy by adjusting the parameters in the gradient
direction  note that the policy could be directly parameterized  or it could be generated indirectly
from a value function  in the latter case the value function parameters are the parameters of the
policy  but instead of being adjusted to minimize error between the approximate and true value
function  the parameters are adjusted to directly improve the performance of the policy generated
by the value function 
these policy gradient algorithms have a long history in operations research  statistics  control theory  discrete event systems and machine learning  before describing the contribution of
the present paper  it seems appropriate to introduce some background material explaining this approach  readers already familiar with this material may want to skip directly to section      where
the contributions of the present paper are described 
    a brief history of policy gradient algorithms
for large scale problems or problems where the system dynamics are unknown  the performance
gradient will not be computable in closed form    thus the challenging aspect of the policy gradient
approach is to find an algorithm for estimating the gradient via simulation  naively  the gradient
can be calculated numerically by adjusting each parameter in turn and estimating the effect on performance via simulation  the so called crude monte carlo technique   but that will be prohibitively
inefficient for most problems  somewhat surprisingly  under mild regularity conditions  it turns out
that the full gradient can be estimated from a single simulation of the system  the technique is
called the score function or likelihood ratio method and appears to have been first proposed in the
sixties  aleksandrov  sysoyev    shemeneva        rubinstein        for computing performance
gradients in i i d   independently and identically distributed  processes 
specifically  suppose r x is a performance function that depends on some random variable
x   and q   x is the probability that x x  parameterized by    rk   under mild regularity
conditions  the gradient with respect to  of the expected performance 

   

   

may be written

to see this  rewrite     as a sum

 

     er x   

   

r     er x   rqq    xx    

   

    

x

r    

x

r x q   x  

x
differentiate  one source of the requirement of mild regularity conditions  to obtain
x

r x rq   x  

   see equation      for a closed form expression for the performance gradient 

   

fip olicy g radient e stimation

rewrite as

x

r    

r x 

rq   x  q   x  

   

q   x
x
and observe that this formula is equivalent to     
if a simulator is available to generate samples x distributed according to q   x   then any
sequence x    x            xn generated i i d  according to q   x gives an unbiased estimate 

   

   
n
x
r       n  r xi  rqq    xx i   
   
i
i
       r   with probability one  the quantity
of r      by the law of large numbers  r
rq   x   q   x   is known as the likelihood ratio or score function in classical statistics  if
the performance function r  x   also depends on    then r  x  rq    x   q    x   is replaced by
rr   x     r   x  rq   x   q   x   in     
  

      u nbiased e stimates
p rocesses

of the

p erformance g radient

for

r egenerative

extensions of the likelihood ratio method to regenerative processes  including markov decision
processes or mdps  were given by glynn               glynn and lecuyer        and reiman
and weiss               and independently for episodic partially observable markov decision
processes  pomdps  by williams         who introduced the reinforce algorithm    here the
i i d  samples x of the previous section are sequences of states x            xt  of random length 
encountered between visits to some designated recurrent state i   or sequences of states from some
start state to a goal state  in this case rq   x  q   x can be written as a sum

 

   

 

rq   x     tx rpxtxt       
 

q   x  

  

t  

pxt xt     

   

where pxt xt    is the transition probability from xt to xt   given parameters    equation    
admits a recursive computation over the course of a regenerative cycle of the form z 
  rk  
and after each state transition xt   xt    

  

zt     zt  

     
 

   

 

rpxtxt       
pxt xt     

so that each term r x rq   x  q   x in the estimate     is of the form 
in addition  r x            xt can be recursively computed by

 

   

r x            xt  zt   if 

r x            xt        r x            xt    xt    
for some function   then the estimate r  x            xt  zt for each cycle can be computed using
storage of only k     parameters  k for zt and   parameter to update the performance function
r   hence  the entire estimate     can be computed with storage of only  k     real parameters  as
follows 

   a thresholded version of these algorithms for neuron like elements was described earlier in barto  sutton  and anderson        
   the vector zt is known in reinforcement learning as an eligibility trace  this terminology is used in barto et al 
       

   

fibaxter   bartlett

algorithm      policy gradient algorithm for regenerative processes 
   set j

     r      z      and       z      rk   
 

 

 

   for each state transition xt




   if j

 

  xt

  

 

 

if the episode is finished  that is  xt  
j   
j rt zt  
j j  
zt    
rt    

    
    
  
  
otherwise  set

rp

  i   set



zt     zt   pxxtxtxt      
t  
rt      rt   xt     
   

  n return n  n   otherwise goto   

examples of recursive
performance functions include the sum of a scalar reward over a cycle 
p

r x            xt     tt   r xt   where r i  is a scalar reward associated with state i  this corresponds to      being the average reward multiplied by the expected recurrence time e  t     the

 
  

negative length of the cycle  which can be implemented by assigning a reward of
to each state 
and is used when the task is to mimimize time taken to get to a goal state  since
  in this case is
pt
t
just e t    the discounted reward from the start state  r x            xt
t   ff r xt   where
ff     is the discount factor  and so on 
as williams        pointed out  a further simplification is possible in the case that rt
r x            xt is a sum of scalar rewards r xt   t depending on the state and possibly the time
t since the starting state  such as r xt   t r xt   or r xt   t fft r xt as above   in that case 
the update from a single regenerative cycle may be written as

   
     

 

 

 

 



 

tx 
t  

pxt xt     

  t
x

s  

   

 

   
        

rpxtxt     

  

  

  

r xs   s   

t
x
s t  

   

 

r xs   s   

 

 

because changes in pxt xt    have no influence on the rewards r xs   s associated with earlier
states  s  t   we should be able to drop the first term in the parentheses on the right hand side and
write
tx 
t
rpxtxt    x
r x  s  
   
p
 s t   s
t   xt xt  

 

  
  

 

 

although the proof is not entirely trivial  this intuition can indeed be shown to be correct 
equation     allows an even simpler recursive formula for estimating the performance gradient  set z 
  and introduce a new variable s
  as before  set zt  
zt
 
rpxtxt     pxtxt    and s s if xt     i   or s
and zt  
otherwise  but
now  on each iteration  set t   r xt   s zt
t   then t  t is our estimate of r    since t
is updated on every iteration  this suggests that we can do away with t altogether and simply update  directly  t   t t r xt   s zt   where the t are suitable step sizes    proving convergence

  

p

    
  

    
 
        
       



  
  

  



   the usual requirements on t for convergence of a stochastic gradient algorithm are t     
       
t   t

 

   

 

  

p t   t

 

 


   and

fip olicy g radient e stimation

of such an algorithm is not as straightforward as normal stochastic gradient algorithms because the
updates r xt zt are not in the gradient direction  in expectation   although the sum of these updates
over a regenerative cycle are  marbach and tsitsiklis        provide the only convergence proof that
we know of  albeit for a slightly different update of the form t  
t t r xt   s  t zt  
where  t is a moving estimate of the expected performance  and is also updated on line  this
update was first suggested in the context of pomdps by jaakkola et al          
marbach and tsitsiklis        also considered the case of   dependent rewards  recall the discussion after       as did baird and moore        with their vaps algorithm  value and policy
search   this last paper contains an interesting insight  through suitable choices of the performance
function r x            xt      one can combine policy gradient search with approximate value function methods  the resulting algorithms can be viewed as actor critic techniques in the spirit of barto
et al          the policy is the actor and the value function is the critic  the primary motivation is
to reduce variance in the policy gradient estimates  experimental evidence for this phenomenon
has been presented by a number of authors  including barto et al          kimura and kobayashi
     a   and baird and moore         more recent work on this subject includes that of sutton
et al         and konda and tsitsiklis         we discuss the use of vaps style updates further in
section     
so far we have not addressed the question of how the parameterized state transition probabilities pxt xt    arise  of course  they could simply be generated by parameterizing the matrix of
transition probabilities directly  alternatively  in the case of mdps or pomdps  state transitions
are typically generated by feeding an observation yt that depends stochastically on the state xt
into a parameterized stochastic policy  which selects a control ut at random from a set of available controls  approximate value function based approaches that generate controls stochastically
via some form of lookahead also fall into this category   the distribution over successor states
pxt xt   ut is then a fixed function of the control  if we denote the probability of control ut given
parameters  and observation yt by ut   yt   then all of the above discussion carries through with
rpxtxt     pxtxt    replaced by rut   yt  ut   yt   in that case  algorithm     is precisely williams reinforce algorithm 
algorithm     and the variants above have been extended to cover multiple agents  peshkin
et al          policies with internal state  meuleau et al          and importance sampling methods
 meuleau et al          we also refer the reader to the work of rubinstein and shapiro       
and rubinstein and melamed        for in depth analysis of the application of the likelihood ratio
method to discrete event systems  des   in particular networks of queues  also worth mentioning
is the large literature on infinitesimal perturbation analysis  ipa   which seeks a similar goal of estimating performance gradients  but operates under more restrictive assumptions than the likelihoodratio approach  see  for example  ho and cao        

   

      

    

 

       

 

  

   
  

 

  

      b iased e stimates

of the

 
 

 

 

 

p erformance g radient

all the algorithms described in the previous section rely on an identifiable recurrent state i   either
to update the gradient estimate  or in the case of the on line algorithm  to zero the eligibility trace
z   this reliance on a recurrent state can be problematic for two main reasons 
   the variance of the algorithms is related to the recurrence time between visits to i   which
will typically grow as the state space grows  furthermore  the time between visits depends on
   

fibaxter   bartlett

the parameters of the policy  and states that are frequently visited for the initial value of the
parameters may become very rare as performance improves 
   in situations of partial observability it may be difficult to estimate the underlying states  and
therefore to determine when the gradient estimate should be updated  or the eligibility trace
zeroed 
if the system is available only through simulation  it seems difficult  if not impossible  to obtain
unbiased estimates of the gradient direction without access to a recurrent state  thus  to solve  
and    we must look to biased estimates  two principle techniques for introducing bias have been
proposed  both of which may be viewed as artificial truncations of the eligibility trace z   the first
method takes as a starting point the formula  for the eligibility trace at time t 

zt  

t  
x

rpxsxs     
pxs xs     

s  

and simply truncates it at some  fixed  not random  number of terms n looking backwards  glynn 
      rubinstein              cao   wan        

zt  n    

t  
x
s t n

rpxsxs       
pxs xs     

   

 n  is then updated after each transition xt   xt by
rp
   rpxt nxt n       
zt  n    zt  n    xtxt  
pxt xt     
pxt n xt n     
and in the case of state based rewards r  xt    the estimated gradient direction after t steps is
t
 rn        x zt  n r xt   
t n   t n
the eligibility trace zt

  

  

   

   

 

unless n exceeds the maximum recurrence time  which is infinite in an ergodic markov chain  
rn  is a biased estimate of the gradient direction  although as n      the bias approaches zero 
however the variance of rn   diverges in the limit of large n  this illustrates a natural trade off
in the selection of the parameter n  it should be large enough to ensure the bias is acceptable  the
expectation of rn   should at least be within  of the true gradient direction   but not so large
that the variance is prohibitive  experimental results by cao and wan        illustrate nicely this
bias variance trade off 
one potential difficulty with this method is that the likelihood ratios rpxs xs     pxs xs   
must be remembered for the previous n time steps  requiring storage of kn parameters  thus 
to obtain small bias  the memory may have to grow without bound  an alternative approach that
requires a fixed amount of memory is to discount the eligibility trace  rather than truncating it 

    

    

    

  

  

zt    fi      fizt  fi    

rpxtxt       
pxt xt     

  

    

r

   for ease of exposition  we have kept the expression for z in terms of the likelihood ratios pxs xs      pxs xs     
which rely on the availability of the underlying state xs   if xs is not available  pxs xs      pxs xs      should
be replaced with us    ys   us    ys   

r

r

   

fip olicy g radient e stimation

     

where z  fi
and fi
after t steps is simply

         is a discount factor 
r fi       t 

tx 
t  

in this case the estimated gradient direction

r xt  zt  fi   

    

      
      

this is precisely the estimate we analyze in the present paper  a similar estimate with r xt zt fi
replaced by r xt
b zt fi where b is a reward baseline was proposed by kimura et al        
      and for continuous control by kimura and kobayashi      b   in fact the use of r xt
b
in place of r xt does not affect the expectation of the estimates of the algorithm  although judicious choice of the reward baseline b can reduce the variance of the estimates   while the algorithm
presented by kimura et al         provides estimates of the expectation under the stationary distribution of the gradient of the discounted reward  we will show that these are in fact biased estimates
of the gradient of the expected discounted reward  this arises because the stationary distribution
itself depends on the parameters  a similar estimate to      was also proposed by marbach and
tsitsiklis         but this time with r xt zt fi replaced by r xt
  zt fi   where   is an
estimate of the average reward  and with zt zeroed on visits to an identifiable recurrent state 
as a final note  observe that the eligibility traces zt fi and zt n defined by      and     are
simply filtered versions of the sequence rpxt xt     pxt xt      a first order  infinite impulse
response filter in the case of zt fi and an n th order  finite impulse response filter in the case of
zt n   this raises the question  not addressed in this paper  of whether there is an interesting theory
of optimal filtering for policy gradient estimators 

    
   

    

      

               

  

  

  

  

    

  
  

    our contribution
we describe gpomdp  a general algorithm based upon      for generating a biased estimate of the
performance gradient r  in general pomdps controlled by parameterized stochastic policies 
here   denotes the average reward of the policy with parameters    rk   gpomdp does
not rely on access to an underlying recurrent state  writing rfi   for the expectation of the estimate produced by gpomdp  we show that
r    and more quantitatively that
fi    rfi  
rfi   is close to the true gradient provided   fi exceeds the mixing time of the markov chain
induced by the pomdp    as with the truncated estimate above  the trade off preventing the setting
of fi arbitrarily close to is that the variance of the algorithms estimates increase as fi approaches
  we prove convergence with probability   of gpomdp for both discrete and continuous observation and control spaces  we present algorithms for both general parameterized markov chains and
pomdps controlled by parameterized stochastic policies 
there are several extensions to gpomdp that we have investigated since the first version of
this paper was written  we outline these developments briefly in section   
in a companion paper we show how the gradient estimates produced by gpomdp can be used
to perform gradient ascent on the average reward    baxter et al          we describe both
traditional stochastic gradient algorithms  and a conjugate gradient algorithm that utilizes gradient
estimates in a novel way to perform line searches  experimental results are presented illustrat 

  

  

  
lim
        
      

  

 

 

  

   the mixing time result in this paper applies only to markov chains with distinct eigenvalues  better estimates of the
bias and variance of gpomdp may be found in bartlett and baxter         for more general markov chains than
those treated here  and for more refined notions of the mixing time  roughly speaking  the variance of gpomdp
grows with      fi    while the bias decreases as a function of      fi   

   

fibaxter   bartlett

ing both the theoretical results of the present paper on a toy problem  and practical aspects of the
algorithms on a number of more realistic problems 

   the reinforcement learning problem
we model reinforcement learning as a markov decision process  mdp  with a finite state space
s f           ng  and a stochastic matrix  p pij giving the probability of transition from state
i to state j   each state i has an associated reward  r i   the matrix p belongs to a parameterized
class of stochastic matrices  p
fp     rk g  denote the markov chain corresponding to
p  by m    we assume that these markov chains and rewards satisfy the following assumptions 

   

  

    

  

       

  

  

assumption    each p    p has a unique stationary distribution 
satisfying the balance equations

                         n   

    p          
 throughout    denotes the transpose of    
assumption    the magnitudes of the rewards  jr  i j  are uniformly bounded by r  
states i 

    

  for all

assumption   ensures that the markov chain forms a single recurrent class for all parameters   
since any finite state markov chain always ends up in a recurrent class  and it is the properties of
this class that determine the long term average reward  this assumption is mainly for convenience
so that we do not have to include the recurrence class as a quantifier in our theorems  however 
when we consider gradient ascent algorithms baxter et al          this assumption becomes more
restrictive since it guarantees that the recurrence class cannot change as the parameters are adjusted 
ordinarily  a discussion of mdps would not be complete without some mention of the actions
available in each state and the space of policies available to the learner  in particular  the parameters
 would usually determine a policy  either directly or indirectly via a value function   which would
then determine the transition probabilities p    however  for our purposes we do not care how
the dependence of p on  arises  just that it satisfies assumption    and some differentiability
assumptions that we shall meet in the next section   note also that it is easy to extend this setup
to the case where the rewards also depend on the parameters  or on the transitions i   j   it is
equally straightforward to extend our algorithms and results to these cases  see section     for an
illustration 
the goal is to find a    r k maximizing the average reward 
fi
 
 
tx 
fi
fi

e
r xt fi x  i  
fi
t     t
t  
where e denotes the expectation over all sequences x    x            with transitions generated according to p    under assumption      is independent of the starting state i and is equal to
n
x

   i r i    r 
    
i  

  

 

       lim

  

 

  

    

where r

   

           

   r             r n     bertsekas        


p

   a stochastic matrix p    pij   has pij   for all i  j and n
j    pij     for all i 
   all the results in the present paper apply to bounded stochastic rewards  in which case r i  is the expectation of the
reward in state i 

   

fip olicy g radient e stimation

   computing the gradient of the average reward

  

for general mdps little will be known about the average reward     hence finding its optimum
will be problematic  however  in this section we will see that under general assumptions the gradient
r  exists  and so local optimization of   is possible 
to ensure the existence of suitable gradients  and the boundedness of certain random variables  
we require that the parameterized class of stochastic matrices satisfies the following additional assumption 

  

  

assumption    the derivatives 

rp      
  rk   the ratios

exist for all 



 pij   
 k

i j      n k     k

  fifi  p    fifi  
ij
fi  k fi
 
 

pij   

are uniformly bounded by b



i j      n k     k

    for all    rk  

the second part of this assumption allows zero probability transitions pij

       only if

rpij    is also zero  in which case we set          one example is if i   j is a forbidden
transition  so that pij         for all    rk   another example satisfying the assumption is
pij     
where 

   

  

           n           nn     rn 

are the parameters of p

 pij     ij
pij   
 pij     kl
pij   

assuming for the moment that r
dependencies 

eij
ij  
j    e

pn

  
 

pij    

    for then

and

pkl    

   exists  this will be justified shortly   then  suppressing 
r   r r 
    

since the reward r does not depend on    note that our convention for r in this paper is that it takes
precedence over all other operations  so rg  f 
rg  f    equations like      should be
regarded as shorthand notation for k equations of the form

                    

   
 k

where k

 





      
    n 
     
 r             r n   
 k
 k

             k   to compute r  first differentiate the balance equations      to obtain
r p     rp   r  
   

fibaxter   bartlett

and hence

r  i p       rp 

    

the system of equations defined by      is under constrained because i p is not invertible  the
balance equations show that i p has a left eigenvector with zero eigenvalue   however  let e
denote the n dimensional column vector consisting of all s  so that e   is the n  n matrix with the
stationary distribution    in each row  since r   e r    e
r
  we can rewrite      as

 

               





r  i  p e      rp 
to see that the inverse
then we can write

 i  p
 

lim  i

e    
a 

t   

t
x
t  

 

exists  let a be any matrix satisfying
 

  tlim
  

at

 

t  

  i tlim
  
  i 

thus 

 i

  t
x

a 

 

 

 
x
t  

tx
  

at

at   

t  

limt   at     

 

at

at  

   

 

it is easy to prove by induction that p e   t
p t ep  which
to as t     by
 tconverges
 
 
    hence  we can write
assumption    so i
p e
exists and is equal to  
p
e
t  

 

 

  



r     rp i p   e 
and so 



r    rp i p   e 





 

    

r 

    

 

 

for mdps with a sufficiently small number of states       could be solved exactly to yield the precise
gradient direction  however  in general  if the state space is small enough that an exact solution of
     is possible  then it will be small enough to derive the optimal policy using policy iteration and
table lookup  and there would be no point in pursuing a gradient based approach in the first place    
thus  for problems of practical interest       will be intractable and we will need to find some
other way of computing the gradient  one approximate technique for doing this is presented in the
next section 
   the argument leading to      coupled with the fact that     is the unique solution to      can be used to justify the
existence of    specifically  we can run through the same steps computing the value of         for small  and
show that the expression      for  is the unique matrix satisfying                        o       
    equation      may still be useful for pomdps  since in that case there is no tractable dynamic programming
algorithm 

r

r

r

   

kk

fip olicy g radient e stimation

   approximating the gradient in parameterized markov chains
in this section  we show that the gradient can be split into two components  one of which becomes
negligible as a discount factor fi approaches  
for all fi       let jfi 
jfi             jfi   n denote the vector of expected discounted
rewards from each state i 

     

            

jfi    i     e

 

 

    

 
x
t  

fitr

fi
fi
xt fifi x 
fi

   

 

 i

 

    

where the  dependence is obvious  we just write jfi  

         
r      fi  r  jfi   fi rp jfi  

proposition    for all    r k and fi

    

proof  observe that jfi satisfies the bellman equations 

jfi   r   fip jfi  

    

 bertsekas         hence 

r   r r
  r   jfi fip jfi  
  r jfi fi r jfi   fi  rp jfi
     fi  r  jfi   fi rp jfi  

by     

we shall see in the next section that the second term in      can be estimated from a single sample path of the markov chain  in fact  theorem   in  kimura et al         shows that the gradient
estimates of the algorithm presented in that paper converge to
fi   rjfi   by the bellman equations       this is equal to
fi fi   rp jfi   rjfi   which implies
fi   rjfi fi  rp jfi  
thus the algorithm of kimura et al         also estimates the second term in the expression for
r  given by       it is important to note that  rjfi   r   jfi the two quantities disagree
by the first term in       this arises because the the stationary distribution itself depends on the
parameters  hence  the algorithm of kimura et al         does not estimate the gradient of the expected discounted reward  in fact  the expected discounted reward is simply  
fi times the
average reward    singh et al         fact     so the gradient of the expected discounted reward
is proportional to the gradient of the average reward 
the following theorem shows that the first term in      becomes negligible as fi approaches  
notice that this is not immediate from proposition    since jfi can become arbitrarily large in the
limit fi    

      

 

  

 

   

 

 

  

    

 

    

  

 

 

 

theorem    for all    rk  

r   filim
r  
  fi

    

rfi       rp jfi  

    

 

where

   

fibaxter   bartlett

proof  recalling equation      and the discussion preceeding it  we have   

r    rp
but rp e

  
x



e  r 

pt

t  

    

  r p e    r        since p is a stochastic matrix  so      can be rewritten as
r    

 
 
x

 

rp p t r 

t  

    

         be a discount factor and consider the expression

now let fi

f  fi       

  lim
    

  

 
 
x

t  

 

rp  fip  t r

    

    

clearly r
rfi  
fi    f fi   to complete the proof we just need to show that f fi
t
t
t
t
 
since fip
fi p   fi e     we can invoke the observation before      to write

 

 
x
t  
p

in particular   
t  
of      and write  

 fip  t    i

 fip  t converges  so we can take rp back out of the sum in the right hand side
f  fi       rp

but

p 

t  


fitp t r

fip      

  jfi   thus f  fi      rp jfi

  
x

t  

 

fitp t

r 

    

  rfi  

 

theorem   shows that rfi  is a good approximation to the gradient as fi approaches   but it
turns out that values of fi very close to lead to large variance in the estimates of rfi  that we
describe in the next section  however  the following theorem shows that
fi need not be too
small  provided the transition probability matrix p  has distinct eigenvalues  and the markov
chain has a short mixing time  from any initial state  the distribution over states of a markov chain
converges to the stationary distribution  provided the assumption  assumption    about the existence
and uniqueness of the stationary distribution is satisfied  see  for example  lancaster   tismenetsky 
      theorem         p        the spectral resolution theorem  lancaster   tismenetsky       
theorem        p       implies that the distribution converges to stationarity at an exponential rate 
and the time constant in this convergence rate  the mixing time  depends on the eigenvalues of
the transition probability matrix  the existence of a unique stationary distribution implies that the

 

 

  

    since e   r   e        motivates a different kind of algorithm for estimating  based on differential rewards
 marbach   tsitsiklis        
    we cannot back p out of the sum in the right hand side of      because  
p t diverges  p t e      the reason
  p p t converges is that p t becomes orthogonal to p in the limit tof  large t  thus  we can view   p t
t  
t  
as a sum of two orthogonal components  an infinite one in the direction e and a finite one in the direction e    it
 
t
t
is the finite component that we need to estimate  approximating  
t   p with t    fip   is a way of rendering
the e component finite while hopefully not altering the e   component too much  there should be other substitutions
that lead to better approximations  in this context  see the final paragraph in section      

p

r

r

r

   

p

p

r

 

p

p

fip olicy g radient e stimation

 

 

largest magnitude eigenvalue is and has multiplicity   and the corresponding left eigenvector is
the stationary distribution  we sort the eigenvalues i in decreasing order of magnitude  so that
    j  j        js j for some  s  n  it turns out that j  j determines the mixing time
of the chain 
the following theorem shows that if
fi is small compared to
j j  the gradient approximation described above is accurate  since we will be using the estimate as a direction in which to
update the parameters  the theorem compares the directions of the gradient and its estimate  in this
theorem    a denotes the spectral condition number of a nonsingular matrix a  which is defined
as the product of the spectral norms of the matrices a and a    

  

 

 

 

   

   a    kak  ka

where

 

k 
 

kak   x max
kaxk 
kxk
 

 

  

and kxk denotes the euclidean norm of the vector x 

  

theorem    suppose that the transition probability matrix p  satisfies assumption   with stationary distribution   
            n   and has n distinct eigenvalues  let s
x  x     xn be
the matrix of right eigenvectors of p corresponding  in order  to the eigenvalues
    j  j 
    jn j  then the normalized inner product between r and fi rfi  satisfies

  

 

  
  

 


 kr p           p  k p
  fi rfi 
  fi
n
  rkr

  s
r r
    
k
krk
  fi j j  
where    diag            n   
notice that r   r is the expectation under the stationary distribution of r  x    
as well as the mixing time  via j j   the bound in the theorem depends on another parameter of
the markov chain  the spectral condition number of    s   if the markov chain is reversible  which
 

   

 

 

 

 

 

 

   

implies that the eigenvectors x            xn are orthogonal   this is equal to the ratio of the maximum
to the minimum probability of states under the stationary distribution  however  the eigenvectors
do not need to be nearly orthogonal  in fact  the condition that the transition probability matrix
have n distinct eigenvalues is not necessary  without it  the condition number is replaced by a more
complicated expression involving spectral norms of matrices of the form p i i  

 

 



proof  the existence of n distinct eigenvalues implies that p can be expressed as s s     where
            n  lancaster   tismenetsky        theorem         p       it follows that for
any polynomial f   we can write f p
sf s    
now  proposition   shows that r fi rfi  r  
fi jfi   but

   diag 

 

        
      
   fi  jfi      fi   r   fip r   fi p r      
     fi   i   fip   fi p      r
 
x
     fi  s
fi t t s r
 

 

 

 

 

    

fi 

n
x
j   

t  

xj y  

   

j

 
x
t  

 fij  

 

t

r 

fibaxter   bartlett

  
 

 

where s  
y            yn    
it is easy to verify that yi is the left eigenvector corresponding to i   and that we can choose
y   and x  e  thus we can write

 

  

fi  jfi      fi  e  r  

n
x

xj yj 

  

 

fi   fij  t r

t  
j   


n
x
fi
r
xj yj 
fi
j
j   

    

fi  e  

    

fi  e   sms   r 


 
 



 
m   diag   
 

where

 
x

fi
  fi  
     
fi 
  fin

it follows from this and proposition   that

  fi rfi 
r   r r   
  rkr
 
 
k
krk
 
  r  r    fi  jfi
 

 

fi  jfi  

krk

r
  r     fi  e   sms r
 
krk
 
 sms r
  r  rkr
k 

r   sms r 

krk  
p 
 
since r   r
       we can apply the cauchy 

 

 

 

 

 

by the cauchy schwartz inequality 
schwartz inequality again to obtain

   

  fi rfi 
  rkr

k





r



p

 

 



   sms
   

 

krk

 



r

 

    

we use spectral norms to bound the second factor in the numerator  it is clear from the definition
that the spectral norm of a product of nonsingular matrices satisfies kab k   kak  kb k    and that
the spectral norm of a diagonal matrix is given by k
d            dn k 
i jdi j  it follows that




   sms
   

 

diag 
    max
 

r      sms       r

 
 

    s  s        r km k

p
     s r r     fi jfi j  
   

 

   

 

 

 

   

   

   

   

 

   

 

combining with equation      proves      
   

 

fip olicy g radient e stimation

   estimating the gradient in parameterized markov chains
algorithm   introduces mcg  markov chain gradient   an algorithm for estimating the approximate gradient rfi  from a single on line sample path x    x          from the markov chain m   
mcg requires only k reals to be stored  where k is the dimension of the parameter space  k
parameters for the eligibility trace zt   and k parameters for the gradient estimate t   note that
after t time steps t is the average so far of r xt zt  

  

 





   

tx
 
t  
zt r xt   
 

t

t  

algorithm   the mcg  markov chain gradient  algorithm
   given 




parameter    r k  
parameterized class of stochastic matrices p
  and   

  fp        rk g satisfying assumptions

 fi          
 arbitrary starting state x  
 state sequence x   x        
 

generated by m      i e  the markov chain with transition
    
 reward sequence r x    r x          satisfying assumption   
set z     and       z      rk   
for each state xt visited do
rp
   
zt   fizt   xtxt  
pxt xt     
t   t   t  r xt  zt t  
probabilities p

 

 

 

  
  
  
  
  

 

 

 

 

 

  

  

 
  

  

  

  

end for

theorem    under assumptions      and    the mcg algorithm starting from any initial state x 
will generate a sequence                 t         satisfying

 



lim t   rfi 

t  

 

w p   

    

  

proof  let fxt g fx    x          g denote the random process corresponding to m    if x   
then the entire process is stationary  the proof can easily be generalized to arbitrary initial distributions using the fact that under assumption    fxt g is asymptotically stationary  when fxt g is
   

fibaxter   bartlett

stationary  we can write

  rp jfi  

x

 

x

 

x

i j
i j
i j

 i rpij   jfi  j  
 i pij   

rpij    j  j  
p    fi
ij

pr xt   i pr xt   j jxt   i  rppij     e j  t     jxt   j   
  

  

ij

where the first probability is with respect to the stationary distribution and

j  t       

        

    

 
x
s t  

fis

t

 

    

j  t      is the process

r xs   

 

the fact that e j t
jxt   jfi xt   for all xt   follows from the boundedness of the
magnitudes of the rewards  assumption    and lebesgues dominated convergence theorem  we
can rewrite equation      as




x
rp   
  rp jfi   e i  xt  j  xt     ij j  t       

pij   

i j

where i

   denotes the indicator function for state i 
 
  if xt   i 
i  xt     
  otherwise 

and the expectation is again with respect to the stationary distribution  when xt is chosen according
to the stationary distribution  the process fxt g is ergodic  since the process fzt g defined by

zt    i  xt  j  xt    

rpij    j  t     
pij   

is obtained by taking a fixed
of fxt g  fzt g is also stationary and ergodic  breiman       
fi function
fi
fi rpij    fi
proposition        since fi pij    fi is bounded by assumption    from the ergodic theorem we have
 almost surely  

  rp jfi

tx
 
rp   
  tlim
i  xt  j  xt   ij j  t     
   t t
pij   
i j
tx
rpxtxt      j  t     
 
  tlim
   t t
pxtxt     
  t
tx
 
x
r
pxtxt      x
 
  tlim
fi s t r xs    
   t t
pxtxt      s t
s t
x

 

  

  

 

  

 

 

  

    

   

    

 

fis t

 

r xs    

    

fip olicy g radient e stimation

concentrating on the second term in the right hand side of       observe that 
fi
fi tx 
fi
fi
fit

 

t  

rpxtxt     
pxt xt     

 
x

fis t

s t   
tx  fifi

 
t

fi
fi

 

fi
fi
xs fifi
fi

r 

 

pxt xt     

t  
 
br tx  x

 t

  br
t

 
x

rpxtxt      fififi

t   s t   
tx  t t

fi

t  

 

fis

fi

t

s t   

fis

t

 

jr xs j

 

fi

  fit
  brfi
t    fi   
    as t     



jrp j
where r and b are the bounds on the magnitudes of the rewards and pijij from assumptions  
and    hence 
tx 
t
rpxtxt    x
  rp jfi
    
fi s t   r xs  
t    t
p

x
x
t t  
t  
s t  
unrolling the equation for t in the mcg algorithm shows it is equal to

  
  

  lim  


t
  tx rpxtxt      x
t t pxt xt      s t
 

  

hence

fis

   

t

 

r is   

    

t    rp jfi w p   as required 

   estimating the gradient in partially observable markov decision processes

  

algorithm   applies to any parameterized class of stochastic matrices p  for which we can compute the gradients rpij    in this section we consider the special case of p  that arise from a
parameterized class of randomized policies controlling a partially observable markov decision process  pomdp   the partially observable qualification means we assume that these policies have
access to an observation process that depends on the state  but in general they may not see the state 
specifically  assume that there are n controls u
f           n g and m observations y
f           m g  each u   u determines a stochastic matrix p u which does not depend on the
parameters    for each state i   s   an observation y   y is generated independently according to
a probability distribution  i over observations in y   we denote the probability of observation y
by y i   a randomized policy is simply a function  mapping observations y   y into probability
distributions over the controls u   that is  for each observation y    y is a distribution over the
controls in u   denote the probability under  of control u given observation y by u y  
to each randomized policy   and observation distribution   there corresponds a markov
chain in which state transitions are generated by first selecting an observation y in state i according

  

   

 

  

  

 

  

  

  
  

  

   

  

fibaxter   bartlett

  

  

to the distribution  i   then selecting a control u according to the distribution  y   and then generating a transition to state j according to the probability pij u   to parameterize these chains we
parameterize the policies  so that  now becomes a function    y of a set of parameters    r k as
well as the observation y   the markov chain corresponding to  has state transition matrix pij 
given by

  
   

      

pij      ey   i  eu   y   pij  u   

equation      implies

rpij     

x

    

y  i pij  u ru    y  

u y

    

algorithm   introduces the gpomdp algorithm  for gradient of a partially observable markov
decision process   a modified form of algorithm   in which updates of zt are based on ut   yt  
rather than pxt xt      note that algorithm   does not require knowledge of the transition probability matrix p   nor of the observation process    it only requires knowledge of the randomized
policy   gpomdp is essentially the algorithm proposed by kimura et al         without the
reward baseline 
the algorithm gpomdp assumes that the policy  is a function only of the current observation 
it is immediate that the same algorithm works for any finite history of observations  in general  an
optimal policy needs to be a function of the entire observation history  gpomdp can be extended
to apply to policies with internal state  aberdeen   baxter        

 

  

 

algorithm   the gpomdp algorithm 
   given 




parameterized class of randomized policies



          rk

 

satisfying assumption   

partially observable markov decision process which when controlled by the randomized
policies     corresponds to a parameterized class of markov chains satisfying assumption   

   

 fi          
 arbitrary  unknown  starting state x  
 observation sequence y   y         generated by the pomdp with controls u   u        
 

 

 

 

   yt  

generated randomly according to 


  
  
  
  
  

       

reward sequence r x    r x          satisfying assumption    where
 hidden  sequence of states of the markov decision process 

  

    
         
          

set z 
and  
 z        rk   
for each observation yt   control ut   and subsequent reward r
rut   yt
zt   fizt
ut   yt
 
t  
t t   r xt   zt  
t
end for

 

   

 xt   do
  

x    x         

 

is the

fip olicy g radient e stimation

for convergence of algorithm   we need to replace assumption   with a similar bound on the
gradient of  
assumption    the derivatives 

exist for all u   u   y

 u    y 
 k

  y and    rk   the ratios
fi
  fifi
 u   y  fi  
fi  
fi
k
 
 

u    y 

are uniformly bounded by b

y     m  u     n  k     k

    for all    rk  

theorem    under assumptions      and    algorithm   starting from any initial state
generate a sequence                 t         satisfying

 



lim t   rfi 

w p   

t  

x 

will

    

proof  the proof follows the same lines as the proof of theorem    in this case 

  rp jfi  

 
 
 

x

i j

 i rpij   jfi  j  

x

i j y u
x

i j y u
x

i j y u

 i pij  u y  i ru    y jfi  j   from     
 i pij  u y  i 

ru    y      y j  j   
fi
    y  u
u

ezt  

where the expectation is with respect to the stationary distribution of fxt g  and the process fzt  g is
defined by
ru   y j t  
zt  i xt j xt   u ut y yt
u   y

        

                      

where ut is the control process and yt is the observation process  the result follows from the same
arguments used in the proof of theorem   
    control dependent rewards

there are many circumstances in which the rewards may themselves depend on the controls u  for
example  some controls may consume more energy than others and so we may wish to add a penalty
term to the reward function in order to conserve energy  the simplest way to deal with this is to
define for each state i the expected reward r i by

   

r i    ey   i  eu   y   r u  i  
   

    

fibaxter   bartlett



and then redefine jfi in terms of r 

jfi    i    

lim e

 

n   

n
x
t  

fi
fi
xt fifi x 
fi

   

fitr

 

 i

 

    

x    x            the performance gradient then becomes
r   r r    rr 

where the expectation is over all trajectories

which can be approximated by

rfi      rp jfi   rr  








due to the fact that jfi satisfies the bellman equations      with r replaced by r  
for gpomdp to take account of the dependence of r on the controls  its fifth line should be
replaced by

 
t   t   t     r ut
  



r
ut      yt  
  xt   zt  
t  
    y  


  

  

  

  

ut  
t  
it is straightforward to extend the proofs of theorems      and   to this setting 

    parameter dependent rewards
it is possible to modify gpomdp when the rewards themselves depend directly on    in this case 
the fifth line of gpomdp is replaced with

t   t   t       r   xt  zt   rr   xt   t   
  

  

  

    

  

   

again  the convergence and approximation theorems will carry through  provided rr   i is uniformly bounded  parameter dependent rewards have been considered by glynn         marbach
and tsitsiklis         and baird and moore         in particular  baird and moore        showed
how suitable choices of r   i lead to a combination of value and policy search  or vaps  for
example  if j   i is an approximate value function  then setting  

   

    

h
i
 
 
 
r   xt   xt    
  r xt     ffj    xt   j    xt    
where r  xt   is the usual reward and ff          is a discount factor  gives an update that seeks to
 

 

 

minimize the expected bellman error
n
x
i  

 

   i   r i    ff

n
x
j   

  

pij   j    j   j    i    

    

    

this will have the effect of both minimizing the bellman error in j   i   and driving the system
 via the policy  to states with small bellman error  the motivation behind such an approach can
be understood if one considers a j that has zero bellman error for all states  in that case a greedy
policy derived from j will be optimal  and regardless of how the actual policy is parameterized  the
expectation of zt r   xt   xt   will be zero and so will be the gradient computed by gpomdp 
this kind of update is known as an actor critic algorithm  barto et al          with the policy playing
the role of the actor  and the value function playing the role of the critic 

 

 

    the use of rewards r   xt   xt
analysis 

 

 

    that depend on the current and previous

   

state does not substantially alter the

fip olicy g radient e stimation

    extensions to infinite state  observation  and control spaces
the convergence proof for algorithm   relied on finite state  s    observation  y   and control  u  
spaces  however  it should be clear that with no modification algorithm   can be applied immediately to pomdps with countably or uncountably infinite s and y   and countable u   all that
changes is that pij u becomes a kernel p x  x    u and  i becomes a density on observations  in
addition  with the appropriate interpretation of r   it can be applied to uncountable u   specifically  if u is a subset of r n then  y   will be a probability density function on u with u y  
the density at u  if u and y are subsets of euclidean space  but s is a finite set   theorem   can be
extended to show that the estimates produced by this algorithm converge almost surely to rfi    in
fact  we can prove a more general result that implies both this case of densities on subsets of r n as
well as the finite case of theorem    we allow u and y to be general spaces satisfying the following
topological assumption   for definitions see  for example   dudley         

  

 

 

  

   

   

assumption    the control space u has an associated topology that is separable  hausdorff  and
first countable  for the corresponding borel   algebra b generated by this topology  there is a
 finite measure  defined on the measurable space u   b   we say that  is the reference measure
for u  
similarly  the observation space y has a topology  borel   algebra  and reference measure
satisfying the same conditions 

 

 

in the case of theorem    where u and y are finite  the associated reference measure is the
counting measure  for u
rn and y rm   the reference measure is lebesgue measure  we
assume that the distributions  i and    y are absolutely continuous with respect to the reference
measures  and the corresponding radon nikodym derivatives  probability masses in the finite case 
densities in the euclidean case  satisfy the following assumption 

 

  

 

   

   

assumption    for every y   y and    r k   the probability measure    y is absolutely continuous with respect to the reference measure for u   for every i   s   the probability measure  i is
absolutely continuous with respect to the reference measure for y  
let  be the reference measure for u   for all u   u   y   y      r k   and k   f           k g  the
derivatives

  d   y 
 u 
 k d

fi
fi   du   y 
fi  k d

exist and the ratios

    

 

fi

 u fifi
du  y
d  u 
 

are bounded by b

  

 

with these assumptions  we can replace  in algorithm   with the radon nikodym derivative
of  with respect to the reference measure on u   in this case  we have the following convergence
result  this generalizes theorem    and also applies to densities  on a euclidean space u  

theorem    suppose the control space u and the observation space y satisfy assumption   and let

 be the reference measure on the control space u   consider algorithm   with
rut    yt 
ut    yt  
   

fibaxter   bartlett

replaced by

r d d yt  ut    
 

 

   

d  yt  
ut
d
under assumptions      and    this algorithm  starting from any initial state
sequence                 t         satisfying

 



lim t   rfi 

t  

x 

will generate a

w p   

proof  see appendix b

   new results
since the first version of this paper  we have extended gpomdp to several new settings  and also
proved some new properties of the algorithm  in this section we briefly outline these results 
    multiple agents

instead of a single agent generating actions according to    y    suppose we have multiple agents
i              na   each with their own parameter set i and distinct observation of the environment
yi   and that generate their own actions ui according to a policy ui  i   yi    if the agents all receive the same reward signal r  xt    they may be cooperating to solve the same task  for example  

then gpomdp can be applied to the collective pomdp obtained by concatenating
   the nobserva
 
na   u
tions  controls  and
parameters
into
single
vectors
y
y
 
 
 
 
 
y
u           u a   and


            na respectively  an easy calculation shows that the gradient estimate generated
by gpomdp in the collective case is precisely the same as that obtained by applying gpomdp
to

 
each agent independently  and then concatenating the results  that is 
          na   where
i is the estimate produced by gpomdp applied to agent i  this leads to an on line algorithm
in which the agents adjust their parameters independently and without any explicit communication 
yet collectively the adjustments are maximizing the global average reward  for similar observations in the context of reinforce and vaps  see peshkin et al          this algorithm gives a
biologically plausible synaptic weight update rule when applied to networks of spiking neurons in
which the neurons are regarded as independent agents  bartlett   baxter         and has shown
some promise in a network routing application  tao  baxter    weaver        

 

 

 

  






    policies with internal states
so far we have only considered purely reactive or memoryless policies in which the chosen control
is a function of only the current observation  gpomdp is easily extended to cover the case of
policies that depend on finite histories of observations yt   yt             yt k   but in general  for optimal
control of pomdps  the policy must be a function of the entire observation history  fortunately  the
observation history may be summarized in the form of a belief state  the current distribution over
states   which is itself updated based only upon the current observation  and knowledge of which
is sufficient for optimal behaviour  smallwood   sondik        sondik         an extension of
gpomdp to policies with parameterized internal belief states is described by aberdeen and baxter
        similar in spirit to the extension of vaps and reinforce described by meuleau et al 
       
   

fip olicy g radient e stimation

    higher order derivatives
can be generalized to compute estimates of second and higher order derivatives of the
average reward  assuming they exist   still from a single sample rpath of the underlying pomdp 
to see this for second order derivatives  observe that if  
q   x r x dx for some twicedifferentiable density q   x and performance measure r x   then

gpomdp

          
  
z
r      r x  rq q   x x  q   x  dx

   

 

 

where r  denotes the matrix of second derivatives  hessian   it can be verified that

r q   x    r log q   x     r log q   x  
q   x 
 

 

 

    

log    

where the second term on the right hand side is the outer product between r
q   x and itself
 that is  the matrix with entries    i
q   x    j q   x    taking x to be a sequence of
states x    x            xt between visits to a recurrent state i in a parameterized markov chain  recall
t  p
section         we have q   x
t   xt xt      which combined with      yields

log    
log    
  
  

 

r q   x     tx r pxtxt     
 

q   x  

 

tx  

 

pxt xt     

t  

rpxtxt        
p
  
 

xt xt  

t  

 t  
x

rpxtxt     

  

pxtxt     

t  

 the squared terms in this expression are also outer products   from this expression we can derive
a gpomdp like algorithm for computing a biased estimate of the hessian r      which involves
maintainingin addition to the usual eligibility trace zt a second matrix trace updated as follows 

  

zt  

  fizt   rp pxtxt      
 



xt xt  

rpxtxt        
p
   
 

xt xt  

     



after t time steps the algorithm returns the average so far of r xt zt zt  where the second term
is again an outer product  computation of higher order derivatives could be used in second order
gradient methods for optimization of policy parameters 
    bias and variance bounds

  

  

theorem   provides a bound on the bias of rfi   relative to r  that applies when the underlying markov chain has distinct eigenvalues  we have extended this result to arbitrary markov chains
 bartlett   baxter         however  the extra generality comes at a price  since the latter bound involves the number of states in the chain  whereas theorem   does not  the same paper also supplies
a proof that the variance of gpomdp scales as  
fi     providing a formal justification for the
interpretation of fi in terms of bias variance trade off 

    

 

   conclusion
we have presented a general algorithm  mcg  for computing arbitrarily accurate approximations
to the gradient of the average reward in a parameterized markov chain  when the chains transition
matrix has distinct eigenvalues  the accuracy of the approximation was shown to be controlled by the
   

fibaxter   bartlett

size of the subdominant eigenvalue j  j  we showed how the algorithm could be modified to apply
to partially observable markov decision processes controlled by parameterized stochastic policies 
with both discrete and continuous control  observation and state spaces  gpomdp   for the finite
state case  we proved convergence with probability   of both algorithms 
we briefly described extensions to multi agent problems  policies with internal state  estimating
higher order derivatives  generalizations of the bias result to chains with non distinct eigenvalues 
and a new variance result  there are many avenues for further research  continuous time results
should follow as extensions of the results presented here  the mcg and gpomdp algorithms can
be applied to countably or uncountably infinite state spaces  convergence results are also needed in
these cases 
in the companion paper  baxter et al          we present experimental results showing rapid
convergence of the estimates generated by gpomdp to the true gradient r   we give on line
variants of the algorithms of the present paper  and also variants of gradient ascent that make use of
the estimates of rfi    we present experimental results showing the effectiveness of these algorithms
in a variety of problems  including a three state mdp  a nonlinear physical control problem  and a
call admission problem 
acknowledgements
this work was supported by the australian research council  and benefited from the comments of
several anonymous referees  most of this research was performed while the authors were with the
research school of information sciences and engineering  australian national university 

appendix a  a simple example of policy degradation in value function learning
approximate value function approaches to reinforcement work by minimizing some form of error
between the approximate value function and the true value function  it has long been known that this
may not necessarily lead to improved policy performance from the new value function  we include
this appendix because it illustrates that this phenomenon can occur in the simplest possible system 
a two state mdp  and also provides some geometric intuition for why the phenomenon arises 
consider the two state markov decision process  mdp  in figure    there are two controls
u    u  with corresponding transition probability matrices

p  u     

 
 
 
 

 

 
 
 
 



  p  u     

 

 
 
 

  

 
 
 
 



 

so that u  always takes the system to state with probability     regardless of the starting state  and
therefore to state with probability      and u  does the opposite  since state has a reward of  
while state has a reward of   the optimal policy is to always select action u    under this policy
the stationary distribution on states is      
        while the infinite horizon discounted
value of each state i
  with discount value ff     is

 

 

 

    

  
 

jff  i    e

             
     

 
x

fft r

fi
fi
xt fifi x 
fi

   

 i

 

 

 

 

t  
where the expectation is over all state sequences x    x    x          with state transitions generated according to p u    solving bellmans equations  jff r ffp u  jff   where jff
jff   jff  
 ff
 ff
and r
r   r   yields jff
and jff
 
    ff 
    ff 

   
            

       
         

     

   

            

fip olicy g radient e stimation

r       

r       

 

 

figure    two state markov decsision process

 

      

now  suppose we are trying to learn an approximate value function j for this mdp  i e    j i
w i for each state i   and some scalar feature    must have dimensionality to ensure that
j really is approximate   here w   r is the parameter being learnt  for the greedy policy obtained
from j to be optimal  j must value state above state   for the purposes of this illustration choose

 
  so that for j
  j   w must be negative 
temporal difference learning  or
   is one of the most popular techniques for training
approximate value functions  sutton   barto         it has been shown that for linear functions 
converges to a parameter w minimizing the expected squared loss under the stationary
distribution  tsitsikilis   van roy        

  
    
 
 
 
         
               
td   
td   
 

w   argmin

w

 

 

 
x

i  

i  w i  jff  i     

    

substituting the previous expressions for          and jff under the optimal policy and solving
  ff
for w   yields w
  hence w   for all values of ff       which is the wrong
    ff 
sign  so we have a situation where the optimal policy is implementable as a greedy policy based
on an approximate value function in the class  just choose any w      yet
observing the
optimal policy will converge to a value function whose corresponding greedy policy implements the
suboptimal policy 
a geometrical illustration of why this occurs is shown in figure    in this figure  points on the
graph
represent
p
p the values of the states  the scales of the state   and state   axes are weighted by
 and  respectively  in this way  the squared euclidean distance on the graph between
two points j and j corresponds to the expectation under the stationary distribution of the squared
difference between values 

 

 

     

 

   

hp




td   

   
 

   j     

p

   j    

i

hp

   j         j    
p

i 





 

  e j  x   j  x  

 

for any value function in the shaded region  the corresponding greedy policy is optimal  since
those value functions rank state   above state    the bold line represents the set of all realizable
approximate value functions w   w
  the solution to      is then the approximate value
function found by projecting the point corresponding to the true value function jff   jff
onto
this line  this is illustrated in the figure for ff
    the projection is suboptimal because weighted
mean squared distance in value function space does not take account of the policy boundary 

     

    
    

           

appendix b  proof of theorem  
the proof needs the following topological lemma  for definitions see  for example   dudley       
pp        
   

fibaxter   bartlett

 

 

 

 

 

               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
 j      j     
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
 w        w       
               
               
               
               
               
               
               
               
               
               
legend
               
               
               
               
               
               
               
               
optimal policy 
               
               
               
               
               
               
approximate
               
               
               
               
value function 
               
               
               
               
               
               
               
               
               
               
               
               
 

 

 

 

 

  
  
  
  
  
  
  
  

 

 

figure    plot of value function space for the two state system  note that the scale of each axis has
been weighted by the square root of the stationary probability of the corresponding state
under the optimal policy  the solution found by td    is simply the projection of the true
value function onto the set of approximate value functions 

 

 

lemma    let x  t be a topological space that is hausdorff  separable  and first countable 
let b be the borel   algebra generated by t   then the measurable space x  b has a sequence
s   s           b of sets that satisfies the following conditions 

 

   each si is a partition of x  that is  x
have empty intersection  

   for all x   x   fxg   b and

 
 

 

  sfs   s   sig and any two distinct elements of si

fs   si   x   s g   fxg 

i  

 

proof  since x is separable  it has a countable dense subset s
fx    x         g  since x is firstcountable  each of these xi has ascountable neighbourhood base  ni   now  construct the partitions
si using the countable set n  
           define
i   ni as follows  let s  x and  for i

 

 

    

si   fs   ni   s   si g   fs    x ni    s   si g  
 

 

   

fip olicy g radient e stimation

clearly  each si is a measurable partition of x   since x is hausdorff  for each pair x  x  of distinct
points from x   there is a pair of disjoint open sets a and a  such that x   a and x    a    since s
is dense  there is a pair s  s  from s with s   a and s    a    also  n contains neighbourhoods ns
and ns  with ns  a and ns   a    so ns and ns  are disjoint  thus  for sufficiently large i  x
and x  fall in distinct elements of the partition si   since this is true for any pair x  x    it follows that

 
 

fs   si   x   s g  fxg 

i  

the reverse inclusion
is trivial  the measurability of all singletons fxg follows from the measuras
bility of sx
i fs   si s   fxg g and the fact that fxg x sx  

  

 

 

 

we shall use lemma   together with the following result to show that we can approximate
expectations of certain random variables using a single sample path of the markov chain 

 

 

lemma    let x  b be a measurable space satisfying the conditions of lemma    and let s    s         
be a suitable sequence of partitions as in that lemma  let  be a probability measure defined on this
space  let f be an absolutely integrable function on x   for an event s   define

f  s    
for each x   x and
almost all x in x  

k

r

s f d  

 s  

               let sk  x  be the unique element of sk containing x  then for
lim f  sk  x     f  x  
k  

proof  clearly  the signed finite measure  defined by

 e    

z

e

fd

is absolutely continuous with respect to   and equation      defines
derivative of  with respect to   this derivative can also be defined as

    

f

as the radon nikodym

 sk  x  
d
 x    klim
 
    sk  x  
d

see  for example   shilov   gurevich        section        by the radon nikodym theorem  dudley        theorem        p        these two expressions are equal a e     
proof   theorem     from the definitions 

rfi      rp jfi

 

n x
n
x
i   j   

 i rpij   jfi  j   

    

for every y    is absolutely continuous with respect to the reference measure   hence for any i and
j we can write
z z
d   y 
pij     
pij  u 
 u  d u  d  i  y  
d
y u
   

fibaxter   bartlett

since  and  do not depend on
under the integral to obtain

rpij     

 and d   y  d is absolutely integrable 

z z

y u

pij  u  r

d   y 
 u  d u  d  i  y  
d

to avoid cluttering the notation  we shall use  to denote the distribution
denote the distribution  i on y   with this notation  we have

  

rpij     

we can differentiate

z z

y u

pij

   y  on u   and 

to

r d
d d d 
d
d

now  let  be the probability measure on y  u generated by  and    we can write      as

rfi   

x

i j

 i jfi  j  

z

yu

pij

r d
d d 
d
d

using the notation of lemma    we define

pij  s    

r

s pij d  

 s  

z
d
 
r s      s   rdd d 
s d

for a measurable set s

 y  u   notice that  for a given i  j   and s  
pij  s     pr xt     j jxt   i   y  u    s  
fi

 

d fi
r
r s     e dd fififi xt   i   yt   ut     s  
d

let s    s          be a sequence of partitions of y  u as in lemma    and let sk
element of sk containing y  u   using lemma    we have

   

z

yu

pij

z
r d
d d  

d
d

lim pij  sk  y  u   r  sk  y  u   d y  u 

yu k  

  klim
  

 y  u  denote the

x z

s  sk

s

pij  s   r s   d 

   

fip olicy g radient e stimation

where we have used assumption   and the lebesgue dominated convergence theorem to interchange
the integral and the limit  hence 

rfi    klim
  

  klim
  

x x

i j s  sk

x

i j s

 i  s  pij  s  jfi  j  r s  

pr xt   i pr  yt   ut     s  pr xt   j jxt   i   yt   ut     s  
  

fi

e  j  t     jxt

  

  klim
  

x

i j s

 

d fi
r
  j   e dd fififi xt   i   yt  ut     s
d

 

 

d
e i xt  s  yt  ut  j  xt  j  t      rdd  
  

d

where probabilities and expectations are with respect to the stationary distribution  of xt   and the
distributions on yt   ut   now  the random process inside the expectation is asymptotically stationary
and ergodic  from the ergodic theorem  we have  almost surely 
d
x tx
r
 
rfi    klim
lim
i  xt  s  yt   ut  j  xt  j  t      dd  
   t    t
 

  

i j s t  

d

it is easy to see that the double limit also exists when the order is reversed  so
tx
d
x
r
 
rfi    tlim
lim i xt  s  yt   ut  j  xt  j  t      dd
   t
k  
 

 
  tlim
   t

t  
tx 
t  

  

i j s
d
  yt  
r d ut
d  yt   u
t
d

    j  t      
   

the same argument as in the proof of theorem   shows that the tails of
when
fi
fi
fi r d  yt   u fi
t fi
fi
d
fi
fi d  y
t 
fi
fi
u
t
d

d

j  t      can be ignored

   
   
and jr  xt  j are uniformly bounded  it follows that t      rp jfi w p    as required 
references
aberdeen  d     baxter  j          policy gradient learning of controllers with internal state  tech 
rep   australian national university 
aleksandrov  v  m   sysoyev  v  i     shemeneva  v  v          stochastic optimaization  engineering cybernetics          
baird  l     moore  a          gradient descent for general reinforcement learning  in advances
in neural information processing systems     mit press 
   

fibaxter   bartlett

bartlett  p  l     baxter  j          hebbian synaptic modifications in spiking neurons that learn 
tech  rep   research school of information sciences and engineering  australian national
university  http   csl anu edu au bartlett papers bartlettbaxter nov   ps gz 
bartlett  p  l     baxter  j          estimation and approximation bounds for gradient based reinforcement learning  journal of computer and systems sciences      invited paper  special
issue on colt      
barto  a  g   sutton  r  s     anderson  c  w          neuronlike adaptive elements that can solve
difficult learning control problems  ieee transactions on systems  man  and cybernetics 
smc            
baxter  j   bartlett  p  l     weaver  l          experiments with infinite horizon  policy gradient
estimation  journal of artificial intelligence research  to appear 
baxter  j   tridgell  a     weaver  l          learning to play chess using temporal differences 
machine learning                
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
bertsekas  d  p          dynamic programming and optimal control  vol ii  athena scientific 
breiman  l          probability  addison wesley 
cao  x  r     wan  y  w          algorithms for sensitivity analysis of markov chains through
potentials and perturbation realization  ieee transactions on control systems technology 
          
dudley  r  m          real analysis and probability  wadsworth   brooks cole  belmont  california 
glynn  p  w          stochastic approximation for monte carlo optimization  in proceedings of the
     winter simulation conference  pp         
glynn  p  w          likelihood ratio gradient estimation for stochastic systems  communications
of the acm           
glynn  p  w     lecuyer  p          likelihood ratio gradient estimation for regenerative stochastic
recursions  advances in applied probability                             
ho  y  c     cao  x  r          perturbation analysis of discrete event dynamic systems  kluwer
academic  boston 
jaakkola  t   singh  s  p     jordan  m  i          reinforcement learning algorithm for partially
observable markov decision problems  in tesauro  g   touretzky  d     leen  t   eds   
advances in neural information processing systems  vol     mit press  cambridge  ma 
kimura  h     kobayashi  s       a   an analysis of actor critic algorithms using eligibility traces 
reinforcement learning with imperfect value functions  in fifteenth international conference
on machine learning  pp         
   

fip olicy g radient e stimation

kimura  h     kobayashi  s       b   reinforcement learning for continuous action using stochastic gradient ascent  in intelligent autonomous systems  ias     pp         
kimura  h   miyazaki  k     kobayashi  s          reinforcement learning in pomdps with
function approximation  in fisher  d  h   ed    proceedings of the fourteenth international
conference on machine learning  icml     pp         
kimura  h   yamamura  m     kobayashi  s          reinforcement learning by stochastic hill
climbing on discounted reward  in proceedings of the twelfth international conference on
machine learning  icml     pp         
konda  v  r     tsitsiklis  j  n          actor critic algorithms  in neural information processing
systems       mit press 
lancaster  p     tismenetsky  m          the theory of matrices  academic press  san diego  ca 
marbach  p     tsitsiklis  j  n          simulation based optimization of markov reward processes  tech  rep   mit 
meuleau  n   peshkin  l   kaelbling  l  p     kim  k  e          off policy policy search  tech 
rep   mit artificical intelligence laboratory 
meuleau  n   peshkin  l   kim  k  e     kaelbling  l  p          learning finite state controllers for
partially observable environments  in proceedings of the fifteenth international conference
on uncertainty in artificial intelligence 
peshkin  l   kim  k  e   meuleau  n     kaelbling  l  p          learning to cooperate via policy
search  in proceedings of the sixteenth international conference on uncertainty in artificial
intelligence 
reiman  m  i     weiss  a          sensitivity analysis via likelihood ratios  in proceedings of the
     winter simulation conference 
reiman  m  i     weiss  a          sensitivity analysis for simulations via likelihood ratios  operations research     
rubinstein  r  y          some problems in monte carlo optimization  ph d  thesis 
rubinstein  r  y          how to optimize complex stochastic systems from a single sample path
by the score function method  annals of operations research             
rubinstein  r  y          decomposable score function estimators for sensitivity analysis and optimization of queueing networks  annals of operations research             
rubinstein  r  y     melamed  b          modern simulation and modeling  wiley  new york 
rubinstein  r  y     shapiro  a          discrete event systems  wiley  new york 
samuel  a  l          some studies in machine learning using the game of checkers  ibm
journal of research and development            
   

fibaxter   bartlett

shilov  g  e     gurevich  b  l          integral  measure and derivative  a unified approach 
prentice hall  englewood cliffs  n j 
singh  s  p   jaakkola  t     jordan  m  i          learning without state estimation in partially
observable markovian decision processes  in proceedings of the eleventh international
conference on machine learning 
singh  s     bertsekas  d          reinforcement learning for dynamic channel allocation in cellular telephone systems  in advances in neural information processing systems  proceedings
of the      conference  pp          mit press 
smallwood  r  d     sondik  e  j          the optimal control of partially observable markov
decision processes over a finite horizon  operations research               
sondik  e  j          the optimal control of partially observable markov decision processes over
the infinite horizon  discounted costs  operations research     
sutton  r  s     barto  a  g          reinforcement learning  an introduction  mit press 
cambridge ma  isbn               
sutton  r  s   mcallester  d   singh  s     mansour  y          policy gradient methods for
reinforcement learning with function approximation  in neural information processing
systems       mit press 
tao  n   baxter  j     weaver  l          a multi agent  policy gradient approach to network
routing  tech  rep   australian national university 
tesauro  g          practical issues in temporal difference learning  machine learning        
    
tesauro  g          td gammon  a self teaching backgammon program  achieves master level
play  neural computation            
tsitsikilis  j  n     van roy  b          an analysis of temporal difference learning with function approximation  ieee transactions on automatic control                
williams  r  j          simple statistical gradient following algorithms for connectionist reinforcement learning  machine learning            
zhang  w     dietterich  t          a reinforcement learning approach to job shop scheduling  in
proceedings of the fourteenth international joint conference on artificial intelligence  pp 
          morgan kaufmann 

   

fi