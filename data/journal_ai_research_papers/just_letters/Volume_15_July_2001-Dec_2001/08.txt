journal of artificial intelligence research                  

submitted       published      

ecient methods for qualitative spatial reasoning
renz dbai tuwien ac at

jochen renz
institut f
ur informationssysteme  technische universit
at wien
favoritenstr    a      wien  austria

nebel informatik uni freiburg de

bernhard nebel
institut f
ur informatik  albert ludwigs universit
at
am flughafen     d       freiburg  germany

abstract

the theoretical properties of qualitative spatial reasoning in the rcc   framework
have been analyzed extensively  however  no empirical investigation has been made yet 
our experiments show that the adaption of the algorithms used for qualitative temporal
reasoning can solve large rcc   instances  even if they are in the phase transition region
  provided that one uses the maximal tractable subsets of rcc   that have been identified
by us  in particular  we demonstrate that the orthogonal combination of heuristic methods
is successful in solving almost all apparently hard instances in the phase transition region
up to a certain size in reasonable time 
   introduction

representing qualitative spatial information and reasoning with such information is an
important subproblem in many applications  such as natural language understanding  document interpretation  and geographical information systems  the rcc   calculus  randell 
cui    cohn      b  is well suited for representing topological relationships between spatial
regions  inference in the full calculus is  however  np hard  grigni  papadias    papadimitriou        renz   nebel         while this means that it is unlikely that very large
instances can be solved in reasonable time  this result does not rule out the possibility that
we can solve instances up to a certain size in reasonable time  recently  maximal tractable
subsets of rcc   were identified  renz   nebel        renz        which can be used to
speed up backtracking search for the general np complete reasoning problem by reducing
the search space considerably 
in this paper we address several questions that emerge from previous theoretical results
on rcc    renz   nebel        renz         up to which size is it possible to solve
instances in reasonable time  which heuristic is the best  is it really so much more ecient
to use the maximal tractable subsets for solving instances of the np complete consistency
problem as the theoretical savings given by the smaller branching factors indicate or is
this effect out balanced by the forward checking power of the interleaved path consistency
computations  this was the case for similar temporal problems  pointisable vs  ord horn
relations   nebel         is it possible to combine the different heuristics in such a way that
more instances can be solved in reasonable time than by each heuristic alone 
we treat these questions by randomly generating instances and solving them using
different heuristics  in doing so  we are particularly interested in the hardest randomly
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

firenz   nebel

generated instances which leads to the question of phase transitions  cheeseman  kanefsky 
  taylor         is there a parameter for randomly generating instances of the consistency
problem of rcc   that results in a phase transition behavior  if so  is it the case that the
hardest instances are mainly located in the phase transition region while the instances not
contained in the phase transition region are easily solvable  in order to generate instances
which are harder with a higher probability  we generate two different kinds of instances  on
the one hand we generated instances which contain constraints over all rcc   relations  on
the other hand we generated instances which contain only constraints over relations which
are not contained in any of the maximal tractable subsets  we expect these instances to be
harder on average than the former instances 
the algorithmic techniques we use for solving these randomly generated instances are
borrowed from similar work on qualitative temporal reasoning  nebel        van beek  
manchak        ladkin   reinefeld         additionally  we make use of the fragments
of rcc    named hb    q    and c   that permit polynomial time inferences  renz   nebel 
      renz         in the backtracking algorithm  which is used to solve the reasoning
problem for full rcc    we decompose every disjunctive relation into relations of one of
these tractable subsets instead of decomposing them into its base relations  this reduces
the average branching factor of the backtracking tree from     for the base relations to
       for hb    to       for c   and to       for q    although these theoretical savings
cannot be observed in our experiments  using the maximal tractable subsets instead of the
base relations leads to significant performance improvements 
this paper is structured as follows  in section    we give a brief sketch of the rcc  
calculus and of the algorithms used for solving instances of rcc    in section   we describe
the procedure for randomly generating instances  the different heuristics we apply for solving
these instances  and how we measure the quality of the heuristics  in section   we evaluate
different path consistency algorithms in order to find the most ecient one to be used for
forward checking in the backtracking search  in section   we observe a phase transition
behavior of the randomly generated instances and show that the instances in the phasetransition region are harder to solve than the other instances  in section   we report on
the outcome of running the different heuristics for solving the instances and identify several
hard instances which are mainly located in the phase transition region  in section   we try
to solve the hard instances by orthogonally combining the different heuristics  this turns
out to be very effective and leads to a very ecient solution strategy  finally  in section  
we evaluate this strategy by trying to solve very large instances  
   the region connection calculus rcc  

the region connection calculus  rcc  is a first order language for representation of and
reasoning about topological relationships between extended spatial regions  randell et al  
    b   spatial regions in rcc are non empty regular subsets of some topological space
which do not have to be internally connected  i e   a spatial region may consist of different
disconnected pieces  different relationships between spatial regions can be defined based on
one dyadic relation  the connected relation c    which is true if the topological closures
of the spatial regions and share a common point 
a  b

a

b

   the programs are available as an online appendix 

   

fiefficient methods for qualitative spatial reasoning

fi fi


fi
fi


fi fi


  



fi
fi


 


x

x

y

y

dc x  y 

x

y

y

x

tpp x  y  tpp    x  y 

ec x  y 

x

x y

y

po x  y 

x

y

y

x

ntpp x  y  ntpp    x  y 

eq x  y 

figure    two dimensional examples for the eight base relations of rcc  
the region connection calculus rcc   is a constraint language formed by the eight
jointly exhaustive and pairwise disjoint base relations dc  ec  po  eq  tpp  ntpp  tpp    
and ntpp   definable in the rcc theory and by all possible unions of the base relations 
giving a total number of          different relations  the base relations have the meaning
of disconnected  externally connected  partial overlap  equal  tangential proper part 
non tangential proper part  and their converses  examples for these relations are shown
in figure    constraints are written in the form
where are variables for spatial
regions and is an rcc   relation  we write the union of base relations as f g  the
union of all base relations  the universal relation  is written as fg  apart from union
     other operations on relations are defined  namely  converse      intersection      and
composition     the formal definitions of these operations are 
xry

x  y

r

r  s

 

 
 
 
 

x  y
x  y
x  y
x  y

 
 
 
 

     
     

x r

s y

x r

s y

xr

 

    

x r

y

s y

 
 
 
   

 
 
 
  

xry

xs y

xry

xs y

yrx
z

xrz

 
 

 

 

zsy  

the composition of base relations can be computed from the semantics of the relations and is
usually provided as a composition table  randell  cohn    cui      a  bennett         the
rcc   composition table corresponds to the given extensional definition of composition only
if the universal region is not permitted  bennett         based on this table  compositions
of disjunctive relations can be easily computed  in the following  sb denotes the closure of
a set of rcc   relations s under composition  intersection  and converse 
a finite set of rcc   constraints  describing the topological relationships of different
regions can be represented by an  matrix   where each entry represents the rcc  
relation holding between region and region   without loss of generality    feqg and
 
can be assumed  the fundamental reasoning problem  named rsat  in this
framework is deciding consistency of a set of spatial formulas   i e   whether there is a
spatial configuration where the relations between the regions can be described by   all
other interesting reasoning problem can be reduced to it in polynomial time  golumbic
  shamir         unfortunately  rsat is np complete  renz   nebel         i e   it is
unlikely that there is any polynomial algorithm for deciding consistency  however  it was
shown in nebel s        paper that there are subsets s of rcc   for which the consistency
n

n

i

mji

n

m

j

 

mij

   

mij

mii

firenz   nebel

problem  written rsat s   can be decided in polynomial time   in particular the set of
eight base relations b was shown to be tractable  from that it follows that bb consisting of
   relations is also tractable  an even larger tractable subset containing all base relations
is hb    renz   nebel         which contains     out of the     rcc   relations  this set
was also shown to be maximal with respect to tractability  i e   if any other rcc   relation
is added  the consistency problem becomes np complete  renz        made a complete
analysis of tractability of rsat by identifying all maximal tractable subsets which contain
all base relations  altogether three subsets hb    q       relations   and c       relations  
np   is the set of relations that by themselves result in np completeness when combined
with the set of base relations  it contains the following    relations which are not contained
in one of hb   q  or c   renz        
np     f j fpog   and  fntppg  or ftppg   
and  fntpp  g  or ftpp  g   g
  ffec ntpp eqg fdc ec ntpp eqg
fec ntpp   eqg fdc ec ntpp   eqgg
the maximal tractable subsets contain the following relations  renz        
hb      rcc   n np     n f j  feq ntppg  and ftppg    
or  feq ntpp   g  and ftpp  g    g
c     rcc   n np     n f j fecg  and fpog   and
  ftpp ntpp tpp   ntpp   eqg     g
q     rcc   n np     n f j feqg  and fpog   and
  ftpp ntpp tpp   ntpp   g     g
 

 

r

r

r

r

r

 

 

 

 

 

 

 

 

 

 

 

 

 

r

r

r

 

 

r

r

 

r

 

 

 

r

r

r

r

 

 

r

r

r

 

r

 

all relations of q  are contained in one of hb   or c   i e   hb     c    rcc   n np    
although hb   is the smallest of the three maximal tractable subsets  it best decomposes the
rcc   relations  when decomposing an rcc   relation
into sub relations of one of the
maximal tractable subsets  i e             one needs on average        hb   relations 
      q  relations  and       c  relations for decomposing all rcc   relations  renz       
gives a detailed enumeration of the relations of the three sets 
r

r

s

   

si

sk

    the path consistency algorithm

as in the area of qualitative temporal reasoning based on allen s interval calculus  allen 
       the path consistency algorithm  montanari        mackworth        mackworth  
freuder        can be used to approximate consistency and to realize forward checking
 haralick   elliot        in a backtracking algorithm 
the path consistency algorithm checks the consistency of all triples of relations and
eliminates relations that are impossible  this is done by iteratively performing the following
operation
  
mij

mij

mik

mkj

   strictly speaking  this applies only to systems of regions that do not require regularity 

   

fiefficient methods for qualitative spatial reasoning

algorithm  path consistency
input  a set  of binary constraints over the variables x    x            x

n

of 

represented by an  matrix  
path consistent set equivalent to   fail  if such a set does not
exist 
n

output 

n

m

      f        j   

      g 
  indicates the  th variable of   analogously for and  
   while      do
   select and delete a path     from  
   if revise    then
  
if
    then return fail
  
else      f 
      j           g 
q

i  j  k  

k  i  j

i

i  j  k

n  i   j  k

i  k

j

i

j

k

q

p  r  q

q

p  r  q

mpq

q

q

p  q  s  

s  p  q

s

n  s

p  s

q

function  revise i  k  j  
input  three labels i  k and j indicating the variables x   x   x of 
output  true  if m is revised  false otherwise 
side effects  m and m revised using the operations   and 
i

j

k

ij

ij

ji

over the constraints involving     and  
xi

xk

xj

   oldm     
  
          
   if  oldm     then return false 
  
    
   return true 
mij

mij

mij

mj i

mij

mik

mkj

mij

 

figure    path consistency algorithm 
for all triples of regions
until a fixed point is reached  if
    for a pair
  then we know that is inconsistent  otherwise is path consistent  computing
can be done in      time  see figure     this is achieved by using a queue of triples of
regions for which the relations should be recomputed  mackworth   freuder         pathconsistency does not imply consistency  for instance  the following set of spatial constraints
is path consistent but not consistent 
i  j  k

i  j

m

m ij

m

m

m

o n

lhh   l
hhh
l  hhhj  l 

x

dc   tpp

z

tpp   tpp  
ec   tpp
ec   tpp

eq   ntpp

y

ec   ntpp

w

on the other hand  consistency does not imply path consistency  since path consistency is
not a form of consistency  in its logical sense   but a form of disjunctive non redundancy 
nevertheless  path consistency can be enforced to any consistent set of constraints by ap   

firenz   nebel

algorithm  consistency
input  a set  of rcc   constraints over the variables x    x            x

and a subset s  rcc   that contains all base relations
and for which decide is a sound and complete decision
procedure 
output  true  iff  is consistent 
   path consistency  
   if  contains the empty relation then return false
   else choose an unprocessed constraint
and
split into  
  s such that        
   if no constraint can be split then return decide  
   for all refinements        do
  
replace
with
in 
  
if consistency   then return true

n

xi rxj

r

s           sk

sl

xi rxj

l

s

   

sk

r

k

xi sl xj

figure    backtracking algorithm for deciding consistency 
plying a path consistency algorithm  if only relations in hb    q    or c  are used  however 
the path consistency algorithm is sucient for deciding consistency  i e   path consistency
decides rsat hb     rsat q     and rsat c      renz   nebel        renz        
    the backtracking algorithm

in order to solve an instance  of rsat  we have to explore the corresponding search space
using some sort of backtracking  in our experiments  we used a backtracking algorithm
employed for solving qualitative temporal reasoning problems  nebel         which is based
on the algorithm proposed by ladkin and reinefeld         for this algorithm  see figure   
it is necessary to have a subset s  rcc   for which consistency can be decided by using a
sound and complete  and preferably polynomial  decision procedure decide  if s contains
all base relations  thens each relation   rcc   can be decomposed into sub relations
  s such that  
  the size of a particular decomposition is the minimal number
of sub relations which is used to decompose   the backtracking algorithm successively
selects constraints of   backtracks over all sub relations of the constraints according to
their decomposition and decides sub instances which contain only constraints over s using
decide 
the  optional  procedure path consistency in line   is used for forward checking
and restricts the remaining search space  nebel        showed that this restriction does
not effect soundness and completeness of the algorithm  if enforcing path consistency is
sucient for deciding rsat s   decide   in line   is not necessary  instead it is possible
to always return true there 
the eciency of the backtracking algorithm depends on several factors  one of them is 
of course  the size of the search space which has to be explored  a common way of measuring
r

si

r

i

si

si

r

   

fiefficient methods for qualitative spatial reasoning

the size of the search space is the average branching factor of the search space  i e   the
average number of branches each node in the search space has  a node is a recursive  call of
    
consistency   then the average size of the search space can be computed as  
 
where  
    is the number of constraints which have to be split when variables are
given  for the backtracking algorithm described in figure   the branching factor depends
on the average number of relations of the split set s into which a relation has to be split 
the less splits on average the better  i e   it is to be expected that the eciency of the
backtracking algorithm depends on the split set s and its branching factor  another factor
is how the search space is explored  the backtracking algorithm of figure   offers two
possibilities for applying heuristics  one is in line   where the next unprocessed constraint
can be chosen  the other is in line   where the next refinement can be chosen  these two
choices inuence the search space and the path through the search space 
b

b

n

n  

n

n  

n

   test instances  heuristics  and measurement

there is no previous work on empirical evaluation of algorithms for reasoning with rcc  
and no benchmark problems are known  therefore we randomly generated our test instances
with a given number of regions   an average label size   and an average degree of
the constraint graph  further  we used two different sets of relations for generating test
instances  the set of all rcc   relations and the set of hard rcc   relations np    i e   those
   relations which are not contained in any of the maximal tractable subsets hb    c    or
q    based on these sets of relations  we used two models to generate instances  denoted
by     and      the former model uses all relations to generate instances  the
latter only the relations in np    the instances are generated as follows 
   a constraint graph with nodes and an average degree of for each node is generated 
this is accomplished by selecting   out of the        possible edges using a
uniform distribution 
   if there is no edge between the th and th node  we set   to be the universal
relation 
   otherwise a non universal relation is selected according to the parameter such that
the average size of relations for selected edges is   this is accomplished by selecting
one of the base relations with uniform distribution and out of the remaining   relations
each one with probability          if this results in an allowed relation  i e   a relation
of np   for      any rcc   relation for       we assign this relation to the
edge  otherwise we repeat the process 
the reason for also generating instances using only relations of np   is that we assume
that these instances are dicult to solve since every relation has to be split during the
backtracking search  even if we use a maximal tractable subclass as the split set  we only
generated instances of average label size        since in this case the relations are equally
distributed 
n

a n  d  l

l

d

h n  d  l

n

d

nd 

i

n n

j

 

mij

mji

l

l

l

 

h n  d  l

a n  d  l

l

 

   this method could result in the assignment of a universal constraint to a selected link  thereby changing
the degree of the node  however  since the probability of getting the universal relation is very low  we
ignore this in the following 

   

firenz   nebel

this way of generating random instances is very similar to the way random csp instances over finite domains are usually generated  gent  macintyre  prosser  smith   
walsh         achlioptas et al         found that the standard models for generating
random csp instances over finite domains lead to trivially awed instances for     
i e   instances become locally inconsistent without having to propagate constraints  since
we are using csp instances over infinite domains  achlioptas et al  s result does not necessarily hold for our random instances  we  therefore  analyze in the following whether
our instances are also trivially awed for      in order to obtain a csp over a finite domain  we first have to transform our constraint graph into its dual graph where
each of the        edges
of our constraint graph corresponds to a node in the
dual graph  moreover  each of the variables of the constraint graph corresponds to
  edges in the dual graph  i e   the dual graph contains      edges and       
nodes  in the dual graph  each node corresponds to a variable over the eight valued domain
d   fdc ec po tpp tpp   ntpp ntpp   eqg  ternary constraints over these variables are imposed by the composition table  i e   the composition rules


must hold for all connected
of the dual graph    
  triples of nodes
n
for all    there are                connected triples in the dual graph  the


n n     
overall number of triples in the dual graph is
 
  unary constraints on
 
 
the domain of the variables are given  i e   there are nd     triples in the dual graph
where all nodes are restricted by unary constraints  therefore  the expected number
of connected triples for which unary constraints are given can be computed as
 
 
 
    
 
 
      
 
for      the expected number of triples   tends to      for the instances generated
according to the model      the probability that the unary constraints which are
assigned to a triple lead to a local inconsistency is about          only        out of the
                  possible assignments are inconsistent   since one locally inconsistent
triple makes the whole instance inconsistent  we are interested in the average degree for
which the expected number of locally inconsistent triples is equal to one  for the model
    this occurs for a value of          and         for         for       
the expected number of locally inconsistent triples is one for          and           for
         for the model      none of the possible assignments of the triples leads to
a local inconsistency  i e   all triples of the randomly generated instances of the    
model are locally consistent   this analysis shows that contrary to what achlioptas et
al  found for randomly generated csp instances over finite domains  the model     
and the model     for small do not suffer from trivial local inconsistencies 
n

n

n n

 

mij

n

n

n n

 

 

 

 

 

 

n n

 

mij

mik

mij   mik   mkj

i  j

n n

 

n

mkj

 

mij

mj i

 

nd 

mij

n
ct

e

n

nd 

n

 

ec t

n n

n

 

ec t

d  

a n  d  l

 

 

 

d

n
e
it

a n  d  l

d

 

e

it

 

d

d

d

 

 

 

n

e

it

 

h n  d  l

h n  d  l

h n  d  l

a n  d  l

d

   this is similar to the result for csps over finite domains that by restricting the constraint type  e g   if
only  not equal  constraints as in graph coloring are used  it is possible to ensure that problems cannot
be trivially awed 

   

fiefficient methods for qualitative spatial reasoning

we solve the randomly generated instances using the backtracking algorithm described
in the previous section  the search space on which backtracking is performed depends on
the split set  i e   the set of sub relations that is allowed in the decompositions  choosing the
right split set inuences the search noticeably as it inuences the average branching factor
of the search space  we choose five different split sets  the three maximal tractable subsets
hb   q  and c   the set of base relations b and the closure of this set bb which consists of
   relations  these sets have the following branching factors b       b b       hb          
c          q           this is  of course  a worst case measure because the interleaved pathconsistency computations reduce the branching factor considerably  ladkin   reinefeld 
      
apart from the choice of the split set there are other heuristics which inuence the eciency of the search  in general it is the best search strategy to proceed with the constraint
with the most constraining relation  line   of figure    and the least constraining choice of
a sub relation  line   of figure     we investigated two different aspects for choosing the
next constraint to be processed  nebel        
static dynamic  constraints are processed according to a heuristic evaluation of their
constrainedness which is determined statically before the backtracking starts or dynamically during the search 
local global  the evaluation of the constrainedness is based on a local heuristic weight
criterion or on a global heuristic criterion  van beek   manchak        
this gives us four possibilities we can combine with the five different split sets  i e   a
total number of    different heuristics  the evaluation of constrainedness as well as how
relations are decomposed into relations of different split sets depends on the restrictiveness
of relations  which is a heuristic criterion  van beek   manchak         restrictiveness
of a relation is a measure of how a relation restricts its neighborhood  for instance  the
universal relation given in a constraint network does not restrict its neighboring relations at
all  the result of the composition of any relation with the universal relation is the universal
relation  the identity relation  in contrast  restricts its neighborhood a lot  in every triple
of variables where one relation is the identity relation  the other two relations must be equal 
therefore  the universal relation is usually the least restricting relation  while the identity
relation is usually the most restricting relation  restrictiveness of relations is represented
as a weight in the range of   to    assigned to every relation  where   is the value of the
most and    the value of the least restricting relation  we discuss in the following section
in detail how the restrictiveness and the weight of a relation is determined 
given the weights assigned to every relation  we compute decompositions and estimate
constrainedness as follows  for each split set s and for each rcc   relation we compute
the smallest decomposition of into sub relations of s  i e   the decomposition which requires the least number of sub relations of s  if there is more than one possibility  we choose
the decomposition with the least restricting sub relations  in line   of the backtracking algorithm  see figure     the least restricting sub relation of each decomposition is processed
first  for the local strategy  the constrainedness of a constraint is determined by the size of
its decomposition  which can be different for every split set  and by its weight  we choose
the constraint with the smallest decomposition larger than one and  if there is more than
 

 

r

r

   

firenz   nebel

one such constraint  the one with the smallest weight  the reason for choosing the relation
with the smallest decomposition is that it is expected that forward checking refines relations with a larger decomposition into relations with a smaller decomposition  this reduces
the backtracking effort  for the global strategy  the constrainedness of a constraint
is
determined by adding the weights of all neighboring relations
with
and
to
the weight of   the idea behind this strategy is that when refining the relation with
the most restricted neighborhood  an inconsistency is detected faster than when refining a
relation with a less restricted neighborhood 
in order to evaluate the quality of the different heuristics  we measured the run time used
for solving instances as well as the number of visited nodes in the search space  comparing
different approaches by their run time is often not very reliable as it depends on several
factors such as the implementation of the algorithms  the used hardware  or the current
load of the used machine which makes results sometimes not reproducible  for this reason 
we ran all our run time experiments on the same machine  a sun ultra   with     mb of
main memory  nevertheless  we suggest to use the run time results mainly for qualitatively
comparing different heuristics and for getting a rough idea of the order of magnitude for
which instances can be solved 
in contrast to this  the number of visited nodes for solving an instance with a particular
heuristic is always the same on every machine  this allows comparing the path through the
search space taken by the single heuristics and to judge which heuristic makes the better
choices on average  however  this does not take into account the time that is needed to make
a choice at a single node  computing the local constrainedness of a constraint is certainly
faster than computing its global constrainedness  similarly  computing constrainedness
statically should be faster than computing it dynamically  furthermore  larger instances
require more time at the nodes than smaller instances  be it for computing path consistency
or for computing the constrainedness  taking running time and the number of visited nodes
together gives good indications of the quality of the heuristics 
a further choice we make in evaluating our measurements is that of how to aggregate
the measurements of the single instances to a total picture  some possibilities are to use
either the average or different percentiles such as the median  i e   the     percentile  the
  percentile for a value  
    is obtained by sorting the measurements in increasing
order and picking the measurement of the   element  i e     of the values are less than
that value  suppose that most instances have a low value  e g  running time  and only a
few instances have a very large value  then the average might be larger than the values of
almost all instances  while in this case the median is a better indication of the distribution
of the values  in this case the     percentile  for instance  gives a good indication of the
value of the hardest among the  normal  instances  we have chosen to use the average
value when the measurements are well distributed and to use both     and     percentile
when there are only a few exceptional values in the distribution of the measurements 
xry

s  t

r

d

xs z

zt y

r

  d  

d

d

   empirical evaluation of the path consistency algorithm

since the eciency of the backtracking algorithm depends on the eciency of the underlying
path consistency algorithm  we will first compare different implementations of the pathconsistency algorithm  in previous empirical investigations  van beek   manchak        of
   

fiefficient methods for qualitative spatial reasoning

reasoning with allen s interval relations  allen         different methods for computing the
composition of two relations were evaluated  this was mainly because the full composition
table for the interval relations contains                     entries  which was too large
at that time to be stored in the main memory  in our setting  we simply use a composition
table that specifies the compositions of all rcc   relations  which is a          table
consuming approximately     kb of main memory  this means that the composition of
two arbitrary relations is done by a simple table lookup 
van beek and manchak        also studied the effect of weighting the relations in
the queue according to their restrictiveness and process the most restricting relation first 
restrictiveness was measured for each base relation by successively composing the base
relation with every possible label  summing up the cardinalities  i e   the number of base
relations contained in the result of the composition  and suitably scaling the result  the
reason for doing so is that the most restricting relation restricts the other relations on
average most and therefore decreases the probability that they have to be processed again 
restrictiveness of a complex relation was approximated by summing up the restrictiveness
of the involved base relations  van beek and manchak        found that their method of
weighting the triples in the queue is much more ecient than randomly picking an arbitrary
triple  because of the relatively small number of rcc   relations  we computed the exact
restrictiveness by composing each relation with every other relation and summing up the
cardinalities of the resulting compositions  we scaled the result into weights from    the
most restricting relation  to     the least restricting relations  
this gives us three different implementations of the path consistency algorithm  one in
which the entries in the queue are not weighted  one with approximated restrictiveness as
done by van beek and manchak  and one with exact restrictiveness   in order to compare
these implementations  we randomly generated instances with    to       regions  for
each value of the average degree ranging from     stepping with     to      we generated
   different instances  figure   displays the average cpu time of the different methods
for applying the path consistency algorithm to the same generated instances  it can be
seen that the positive effect of using a weighted queue is much greater for our problem
than for the temporal problem  about    faster than using an ordinary queue without
weights compared to only about   faster  van beek   manchak          determining the
weights of every relation using their exact restrictiveness does not have much advantage over
approximating their restrictiveness using the approach by van beek and manchak        
however  for our further experiments we always used the  exact weights  method because
determining the restrictiveness amounts to just one table lookup 
as mentioned in the previous section  one way of measuring the quality of the heuristics
is to count the number of visited nodes in the backtrack search  in our backtracking
algorithm  path consistency is enforced in every visited node  note that it is not adequate
to multiply the average running time for enforcing path consistency of an instance of a
particular size with the number of visited nodes in order to obtain an approximation of
the required running time for that instance  the average running time for enforcing pathconsistency as given in figure   holds only when all possible paths are entered into the
queue at the beginning of the computation  see line   of figure     these are the paths
   for the weighted versions we select a path  i  k  j   from the queue q in line   of the algorithm of figure  
according to the weights of the different paths in q which are computed as specified above 

   

firenz   nebel

average cpu time of pca using different queue methods for a n d     
    
   

 exact  weights
 approx   weights
no weights

cpu time  sec 

  
 
   
    
     
   

   

   

   

   
   
nodes

   

   

   

    

figure    comparing the performance of the path consistency algorithm using different
methods for weighting the queue     instances data point             
d

 

 

which have to be checked by the algorithm  the path consistency computation during the
backtracking search is different  however  there  only the paths involving the currently
changed constraint are entered in the queue  since only these paths might result in changes
of the constraint graph  this is much faster than the full computation of path consistency
which is only done once at the beginning of the backtrack search 
   the phase transition of rcc  

when randomly generating problem instances there is usually a problem dependent parameter which determines the solubility of the instances  in one parameter range instances are
underconstrained and are therefore soluble with a very high probability  in another range 
problems are overconstrained and soluble with a very low probability  in between these
ranges is the phase transition region where the probability of solubility changes abruptly
from very high to very low values  cheeseman et al          in order to study the quality
of different heuristics and algorithms with randomly generated instances of an np complete
problem  it is very important to be aware of the phase transition behavior of the problem 
this is because instances which are not contained in the phase transition region are often
very easily solvable by most algorithms and heuristics and are  thus  not very useful for
comparing their quality  conversely  hard instances which are better suited for comparing
the quality of algorithms and heuristics are usually found in the phase transition region 
in this section we identify the phase transition region of randomly generated instances
of the rsat problem  both for instances using all rcc   relations and for instances using
only relations of np    similarly to the empirical analysis of qualitative temporal reasoning
problems  nebel         it turns out that the phase transition depends most strongly on the
average degree of the nodes in the constraint graph  if all relations are allowed  the phased

   

fiefficient methods for qualitative spatial reasoning

probability of satisfiability for a n d     

median cpu time for a n d     
cpu time s 

probability    
   

   
   
   
   
   
   
 

  
   
  
  
   
    
     
average degree
     

  

nodes

   
  
  

   
    
     
average degree
     

  

  

nodes

  

figure    probability of satisfiability and median cpu time for  
hb   static global heuristic      instances per data point 

a n  d 

     using the
 

transition is around     to      depending on the instance size  see figure     because
of the result of our theoretical analysis of the occurrence of trivial aws  see section     it can
be expected that for larger instance sizes the phase transition behavior will be overlaid and
mainly determined by the expected number of locally inconsistent triples which also depends
on the average degree   thus  although it seems that the phase transition shifts towards
larger values of as the instance size increases  the phase transition is asymptotically below
        the theoretical value for      see section     instances which are not pathconsistent can be solved very fast by just one application of the path consistency algorithm
without further need for backtracking  when looking at the median cpu times given in
figure    one notices that there is a sharp decline of the median cpu times at the phase
transition  this indicates that for values of the average degree which are higher than where
the phase transition occurs  at least     of the instances are not path consistent 
when using only  hard  relations  i e   relations in np    the phase transition appears
at higher values for   namely  between      and       see figure     as the median
runtime shows  these instances are much harder in the phase transition than in the former
case  as in the previous case  but even more strongly  it seems that the phase transition
shifts towards larger values of as the instance size increases  and also that the phasetransition region narrows 
in order to evaluate the quality of the path consistency method as an approximation to
consistency  we counted the number of instances that are inconsistent but path consistent
 see figure     i e   those instances where the approximation of the path consistency algorithm to consistency is wrong  first of all  one notes that all such instances are close to the
phase transition region  in the general case  i e   when constraints over all rcc   relations
are employed  only a very low percentage of instances are path consistent but inconsistent 
therefore  the figure looks very erratic  more data points would be required in order to
obtain a smooth curve  however  a few important observations can be made from this
figure  namely  that path consistency gives an excellent approximation to consistency even
for instances of a large size  except for very few instances in the phase transition region 
almost all instances which are path consistent are also consistent  this picture changes
d

d

d

d

d

 

n

d

d

d

d

   

firenz   nebel

probability of satisfiability for h n d     

median cpu time for h n d     

probability    

cpu time s 

   

 
   

  

 
  
  
  

   
     
     
average degree
     

  

   

  

 

nodes

   
     
     
average degree
     

  

  

nodes

  

figure    probability of satisfiability and median cpu time for  
hb   static global heuristic      instances per data point 

h n  d 

percentage points of incorrect pca answers for a n d     

     using the
 

percentage points of incorrect pca answers for h n d     

pc failures    
pc failures    
   
   

  
  
  
  
  
  
  
  
 

   
   
   

   

   

  
  

 
 

  

 

    
     
average degree
     

nodes

  
  
  

 

    
     
     
average degree
  

  

nodes

  

figure    percentage points of incorrect answers of the path consistency algorithm for
       and       
a n  d 

 

h n  d 

 

when looking at the        case  here almost all instances in the phase transition
region and many instances in the mostly insoluble region are path consistent  though only
a few of them are consistent 
for the following evaluation of the different heuristics we will randomly generate instances with an average degree between     and      in the        case and
between     and      in the        case  this covers a large area around the
phase transition  we expect the instances in the phase transition region of        to
be particularly hard which makes them very interesting for comparing the quality of the
different heuristics 
h n  d 

 

d

d

d

h n  d 

d

a n  d 

 

 

h n  d 

 

   empirical evaluation of the heuristics

in this section we compare the different heuristics by running them on the same randomly
generated instances  for the instances of        we ran all    different heuristics
a n  d 

   

 

fiefficient methods for qualitative spatial reasoning

number of hard instances for a n d     

number of hard instances for h n d     

 hard instances

 hard instances

  
 
 
 
 
 
 
 
 
 
 

   
   
   
   
   
   
   
   
  
 

   
  
  

   
    
     
     
average degree
  

  

nodes

  
  
  

   
     
     
average degree
     

  

nodes

  

figure    number of instances using more than        visited nodes for some heuristic for
       and       
a n  d 

 

h n  d 

 

 static dynamic and local global combined with the five split sets b bb hb   c  q    on the
same randomly generated instances of size      up to        for the instances of
       we restricted ourselves to instances with up to      regions because larger
ones appeared to be too dicult 
in first experiments we found that most of the instances were solved very fast with
less than       visited nodes in the search space when using one of the maximal tractable
subsets for splitting  however  some instances turned out to be extremely hard  they could
not be solved within our limit of   million visited nodes  which is about     hours of cpu
time  therefore  we ran all our programs up to a maximal number of        visited nodes
and stored all instances for which at least one of the different heuristics used more than
       visited nodes for further experiments  see next section   we call those instances
the hard instances  the distribution of the hard instances is shown in figure    it turned
out that for the heuristics using b as a split set and for the heuristics using dynamic and
global evaluation of the constrainedness many more instances were hard than for the other
combinations of heuristics  we  therefore  did not include in figure   the hard instances
of the b dynamic global heuristic for        and the hard instances for the heuristics
b dynamic global heuristic for        
using b as a split set and the b 
as figure   shows  almost all of the hard instances are in the phase transition region 
for        only a few of the     instances per data point are hard while for       
almost all instances in the phase transition are hard  altogether there are     hard instances
for         out of a total number of         generated instances  and        hard
instances for         out of a total number of         generated instances   table  
shows the number of hard instances for each heuristic except for those which were excluded
as mentioned above  the heuristics using hb   as a split set solve more instances than the
heuristics using other split sets  using c  or q  as a split set does not seem to be an
improvement over using b b among the different ways of computing constrainedness  static
and global appears to be the most effective combination when using one of the maximal
tractable subsets as a split set  for some split sets  dynamic and local also seems to be an
 

n

h n  d 

 

 

 

n

 

n

a n  d 

 

h n  d 

a n  d 

 

a n  d 

 

 

h n  d 

h n  d 

 

   

 

firenz   nebel

heuristics

hb   sta loc
hb   sta glo
hb   dyn loc
hb   dyn glo
c   sta loc
c   sta glo
c   dyn loc
c   dyn glo
q   sta loc
q   sta glo
q   dyn loc
q   dyn glo
bb sta loc
bb sta glo
bb dyn loc
bb dyn glo
b sta loc
b sta glo
b dyn loc
b dyn glo
total

 

a n  d 

    

  
  
  
   
  
  
  
   
  
  
  
   
  
  
  
   
   
   
   
     
   

 
    
       
       
      
       
       
       
       
       
       
       
       
       
       
       
       
 
 
 
 
 
       

h n  d 

h

               
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
 
 
 
 
 
   

table    number of hard instances for each heuristic
effective combination while combining dynamic and global is in all cases the worst choice
with respect to the number of solved instances 
in figure   we compare the     and     percentiles of the different heuristics on
        we do not give the average run times since we ran all heuristics only up to at
most        visited nodes which reduces the real average run time values  each data point
is the average of the values for     to       we took the average of the different degrees
in order to cover the whole phase transition region which is about     for instances of
size      and      for instances of size        for all different combinations of
computing constrainedness  the ordering of the run times is the same for the different split
sets  b  bb c  hb   q    the run times of using static local  static global  or dynamic local
for computing constrainedness are almost the same when combined with the same split set
while they are longer for all split sets when using dynamic global  about   times longer
when using bb as a split set and about     times longer when using the other split sets  
the     percentile run times are only about     times longer than the     percentile run
times  thus  even the harder among the  normal  instances can be solved easily  i e   apart
from a few hard instances  most instances can be solved eciently within the size range
we analyzed  the erratic behavior of the median curves results from an aggregation of the
effect which can be observed in figure    namely  that some of the median elements in the
phase transition are inconsistent and easily solvable 
a n  d 

 

d

d

d

n

d

 

 

n

 

   

fiefficient methods for qualitative spatial reasoning

median cpu time using static local for a n d     

    percentile cpu time using static local for a n d     

   

   
b split
b  split
c split
h   split
q split

   
   
   
   

  

  

  

  
  
nodes

  

  

  

   

   

  

median cpu time using static global for a n d     

  

  

  

  
  
nodes

  

  

  

   

    percentile cpu time using static global for a n d     

   

   
b split
b  split
c split
h   split
q split

   

b split
b  split
c split
h   split
q split

   
cpu time  sec 

   
cpu time  sec 

   

 
  

   
   
   

   
   
   
   

 

 
  

  

  

  

  
  
nodes

  

  

  

   

  

median cpu time using dynamic local for a n d     

  

  

  

  
  
nodes

  

  

  

   

    percentile cpu time using dynamic local for a n d     

   

   
b split
b  split
c split
h   split
q split

   

b split
b  split
c split
h   split
q split

   
cpu time  sec 

   
cpu time  sec 

   

   

 

   
   
   

   
   
   
   

 

 
  

  

  

  

  
  
nodes

  

  

  

   

  

median cpu time using dynamic global for a n d     

  

  

  

  
  
nodes

  

  

  

   

    percentile cpu time using dynamic global for a n d     

   

   
b split
b  split
c split
h   split
q split

   

b split
b  split
c split
h   split
q split

   
cpu time  sec 

   
cpu time  sec 

b split
b  split
c split
h   split
q split

   
cpu time  sec 

cpu time  sec 

   

   
   
   

   
   
   
   

 

 
  

  

  

  

  
  
nodes

  

  

  

   

  

  

  

  

  
  
nodes

  

  

  

   

figure    percentile     and     cpu time of the different heuristics for solving
               to               instances per data point 
a n  d 

 

d

 

d

 

   

firenz   nebel

for the runtime studies for        we noticed that there are many hard instances
for     see figure     for      almost all instances in the phase transition region are
hard  see last column of table     also  as table   shows  the number of hard instances
varies a lot for the different heuristics  therefore  it is not possible to compare the percentile
running times of the different heuristics for     for      and       see last column
of table     for instance  the     and     percentile element of the c  dynamic global
heuristic is element no    and element no     while it is element no     and element no    
of the hb   dynamic local heuristic  out of the     sorted elements   respectively 
for this reason we show the results only up to a size of       see figure      again  we
took the average of the different degrees from      to      in order to cover the whole
phase transition region  the order of the run times is the same for different combinations
of computing constrainedness  b  bb c   q  hb    while hb   is in most cases the fastest 
as for the        instances  the run times for dynamic global were much longer than
the other combinations  the     percentile run times of the static global combination
and for hb   and q  of the dynamic local combination are faster than those of the other
combinations  although the median cpu times are about the same as for        for
    the percentile     cpu times are much longer  as it was already shown in figure  
and    this is further evidence that there are very hard instances in the phase transition
region of        
h n  d 

n  

 

n

n  

n

d

n

d

 

a n  d 

d

 

 

a n  d 

 

n  

h n  d 

 

   orthogonal combination of the heuristics

in the previous section we studied the quality of different heuristics for solving randomly
generated rsat instances  we found that several instances which are mainly located in the
phase transition region could not be solved by some heuristics within our limit of       
visited nodes in the search space  since the different heuristics have a different search space
 depending on the split set  and use a different path through the search space  determined
by the different possibilities of computing constrainedness   it is possible that instances are
hard for some heuristics but easily solvable for other heuristics  nebel        observed that
running different heuristics in parallel can solve more instances of a particular hard set of
temporal reasoning instances proposed by van beek and manchak        than any single
heuristic alone can solve  when using altogether the same number of visited nodes as for
each heuristic alone  an open question of nebel s investigation  nebel        was whether
this is also the case for the hard instances in the phase transition region 
in this section we evaluate the power of  orthogonally combining  the different heuristics
for solving rsat instances  i e   running the different heuristics for each instance in parallel
until one of the heuristics solves the instance  there are different ways for simulating
this parallel processing on a single processor machine  one is to use time slicing between
the different heuristics  another is to run the heuristics in a fixed or random order until
a certain number of nodes in the search space is visited and if unsuccessful try the next
heuristic  cf  huberman  lukose    hogg         which possibility is chosen and with
which parameters  e g   the order in which the heuristics are run and the number of visited
nodes which is spent for each heuristic  determines the eciency of the single processor
simulation of the orthogonal combination  in order to find the best parameters  we ran all
heuristics using at most        visited nodes for each heuristic on the set of hard instances
   

fiefficient methods for qualitative spatial reasoning

median cpu time using static local for h n d     

    percentile cpu time using static local for h n d     

     

 
b split
c split
b  split
q split
h   split

cpu time  sec 

     
    

b split
c split
b  split
q split
h   split

 
cpu time  sec 

    

     
    
     
    

 

 

 

     
 

 
  

  

  

  
nodes

  

  

  

  

median cpu time using static global for h n d     

  

  
nodes

  

  

  

    percentile cpu time using static global for h n d     

     

 
b split
c split
b  split
q split
h   split

     
    

b split
c split
b  split
q split
h   split

 
cpu time  sec 

    
cpu time  sec 

  

     
    
     
    

 

 

 

     
 

 
  

  

  

  
nodes

  

  

  

  

median cpu time using dynamic local for h n d     

  

  
nodes

  

  

  

    percentile cpu time using dynamic local for h n d     

     

 
b split
c split
b  split
q split
h   split

     
    

b split
c split
b  split
q split
h   split

 
cpu time  sec 

    
cpu time  sec 

  

     
    
     
    

 

 

 

     
 

 
  

  

  

  
nodes

  

  

  

  

median cpu time using dynamic global for h n d     

  

  
nodes

  

  

  

    percentile cpu time using dynamic global for h n d     

     

 
b split
c split
b  split
q split
h   split

     
    

b split
c split
b  split
q split
h   split

 
cpu time  sec 

    
cpu time  sec 

  

     
    
     
    

 

 

 

     
 

 
  

  

  

  
nodes

  

  

  

  

  

  

  
nodes

  

  

  

figure     percentile     and     cpu time of the different heuristics for solving
                to               instances per data point 
h n  d 

 

d

 

d

 

   

firenz   nebel

a n  d      
heuristics solved instances    response
      
      
hb   sta loc
hb   sta glo
      
      
hb   dyn loc
      
      
hb   dyn glo
      
      
c   sta loc
      
     
c   sta glo
      
     
c   dyn loc
      
     
c   dyn glo
      
     
q   sta loc
      
     
q   sta glo
      
      
q   dyn loc
      
      
q   dyn glo
      
      
      
     
bb sta loc
bb sta glo
      
     
bb dyn loc
      
     
bb dyn glo
      
     
b sta loc
      
     
b sta glo
      
     
b dyn loc
      
     
b dyn glo
 
     
combined
      

h  n  d      
solved instances    response
      
     
      
      
      
      
      
      
      
     
      
     
      
     
      
     
      
     
      
     
      
     
      
     
      
     
      
     
      
     
 
     
 
     
 
     
 
     
 
     
      

table    percentage of solved hard instances for each heuristic and percentage of first response when orthogonally running all heuristics  note that sometimes different
heuristics are equally fast  therefore the sum is more than      

identified in the previous section  those instances for which at least one heuristic required
more than        visited nodes  and compared their behavior  since we ran all heuristics
on all instances already for the experiments of the previous section  we only had to evaluate
their outcomes  this led to a very surprising result for the        instances  namely  all
of the     hard instances except for a single one were solved by at least one of the heuristics
using less than        visited nodes  in table   we list the percentage of hard instances
that could be solved by the different heuristics and the percentage of first response by each
of them when running the heuristics in parallel  i e   which heuristic required the smallest
number of visited nodes for solving the instance   it turns out that the heuristics using hb  
as a split set did not only solve more instances than the other heuristics  they were also
more often the fastest in finding a solution  although the heuristics using the other two
maximal tractable subsets q  and c  as a split set did not solve significantly more instances
than the heuristics using b b they were much faster in finding a solution  despite solving
the least number of instances  the heuristics using b as a split set were in some cases the
fastest in producing a solution 
a n  d 

   

 

fiefficient methods for qualitative spatial reasoning

first response for solving the hard instances of a n d     

first response for solving the hard instances of h n d     

  
   
number of solved instances

number of solved instances

inconsistent
consistent
  

  

 

inconsistent
consistent

   
   
   
   
   
   

 

 
 

  
   
    
minimal number of visited nodes

     

 

  
   
    
minimal number of visited nodes

     

figure     fastest solution of the hard instances when running all heuristics in parallel
when comparing the minimal number of visited nodes of all the heuristics for all the
hard instances  we found that only five of them  which were all inconsistent  required
more than     visited nodes  this is particularly remarkable as all these instances are
from the phase transition region of an np hard problem  i e   instances which are usually
considered to be the most dicult ones  further note that about           of the      pathconsistent  instances were inconsistent  which is much higher than usual  cf  figure    
interestingly  most of those inconsistent instances were solved faster than the consistent
instances  at this point  it should be noted that combining heuristics orthogonally is very
similar to randomized search techniques with restarts  selman  levesque    mitchell        
however  in contrast to randomized search  our method can also determine whether an
instance is inconsistent  in figure    we chart the number of hard instances solved with the
smallest number of visited nodes with respect to their solubility  due to the low number
of hard instances of         the figure on the left looks a bit ugly but one can at least
approximate the behavior of the curves when comparing it with the second figure on the
right which is the same curve for         see below   the oscillating behavior of the
inconsistent instances  more instances are solved with an odd than with an even number
of visited nodes  might be due to the sizes of the instances we generated instances with
an even number of nodes only  the most dicult instance              was solved
b static global heuristic using about        visited nodes while all
as inconsistent with the b 
heuristics using one of the maximal tractable subsets as a split set failed to solve it even
when each was allowed to visit            nodes in the search space 
we did the same examination for the set of        hard instances of               of
these instances could not be solved by any of the    different heuristics using        visited
nodes each  their distribution is shown in figure    a   similar to the hard instances
of         the heuristics using hb   as a split set were the most successful ones for
solving the hard instances of         as shown in table    they solved more of the
hard instances than any other heuristics and produced the fastest response of more than
    of the hard instances  there is no significant difference between using c  q  or bb
as a split set  neither in the number of solved instances nor in the percentage of first
response  like in the previous case  computing constrainedness using the static global or the
dynamic local heuristics resulted in more successful paths through the search space by which
a n  d 

 

h n  d 

 

n

 d

h n  d 

a n  d 

 

 

h n  d 

 

 

   

 

firenz   nebel

first response for solving the hard instances of h n d     
number of hard instances for h n d      using orthogonal combination
inconsistent
consistent

 hard instances
   
  
  
  
  
 

  
  
  

   
     
     
average degree
     

nodes

number of solved instances

   

  

  

  

  

  
 
     

     
     
     
number of visited nodes

      

 a 
 b 
figure     hard instances using orthogonal combination of all heuristic for        
 a  shows their distribution   b  shows their fastest solution when using up to
        visited nodes per heuristic
h n  d 

 

more instances were solved within        visited nodes than by the other combinations  on
average they produced faster solutions than the other combinations 
the same observations as for        can be made when charting the fastest solutions
of the hard instances of         see figure      about              of the solved
instances are inconsistent  most of them were  again  solved faster than the consistent
instances  more than     of the hard instances can be solved with at most     visited nodes 
    can be solved with at most       visited nodes  since the hb   dynamic local heuristic
alone solves more than     of the instances  it seems dicult to combine different heuristics
in a way that more hard instances can be solved while using not more than        visited
nodes altogether  however  when orthogonally combining the two best performing heuristics
 hb    dynamic local and hb   static global  allowing each of them a maximal number of      
visitable nodes  we can solve              of the hard instances 
we tried to solve the       hard instances of        which are not solvable using
orthogonal combination of heuristics with at most        visited nodes by using a maximal
number of         visited nodes      of these instances are still not solvable  more than
    of the solved instances are inconsistent  the fastest response for the solved instances
is charted in figure    b   the most successful heuristics in giving the fastest response
are hb    dynamic local         and hb   static global          the three heuristics using
static global computation of constrainedness combined with using q  c  and bb as a split
set gave the fastest response for       of the solved instances where the bb strategy was by
far the best among the three        
a n  d 

h n  d 

 

 

h n  d 

 

 

 

   combining heuristics for solving large instances

in the previous section we found that combining different heuristics orthogonally can solve
more instances using the same amount of visited nodes than any heuristic alone can solve  in
this section we use these results in order to identify the size of randomly generated instances
   

fiefficient methods for qualitative spatial reasoning

up to which almost all of them  especially those in the phase transition region  can still be
solved in acceptable time  since many instances of        are already too dicult for
a size of       see figure      we restrict our analysis to the instances of        and
study randomly generated instances with a size of more than       nodes 
for instances of a large size allowing a maximal number of        visited nodes in
the search space is too much for obtaining an acceptable runtime         visited nodes for
instances of size       corresponds to a runtime of more than    seconds on a sun ultra  
for larger instances it gets much slower  therefore  we have to restrict the maximal number
of visited nodes in order to achieve an acceptable runtime  given a multi processor machine 
the different heuristics can be run orthogonally on different processors using the maximal
number of visited nodes each  if the orthogonal combination of the different heuristics is
simulated on a single processor machine  the maximal number of nodes has to be divided
by the number of used heuristics to obtain the available number of visitable nodes for each
heuristic  thus  the more different heuristics we use  the less visitable nodes are available
for each heuristic  therefore  in order to achieve the best performance  we have to find
the combination of heuristics that solves most instances within a given number of visitable
nodes  the chosen heuristics should not only solve many instances alone  they should also
complement each other well  i e   instances which cannot be solved by one heuristic should
be solvable by the other heuristic 
we started by finding the optimal combination of heuristics for the set of     hard
instances of         from our empirical evaluation given in section   we know how
many visited nodes each heuristic needs in order to solve each of the     hard instances 
therefore  we computed the number of solved instances for all     possible combinations
of the heuristics using an increasing maximal number of visitable nodes for all heuristics
together  since we only tried to find the combination which solves the most instances 
this can be computed quite fast  the results are given in table    they show that a
good performance can be obtained with a maximal number of     visited nodes  in this
case four heuristics were involved  i e       visitable nodes are spent on each of the four
heuristics  since the same combination of heuristics  hb   static global  hb    dynamic local 
b static local  is also the best for up to       visitable nodes  we choose
c  dynamic local  b 
this combination for our further analysis  we choose the order in which they are processed to
b static local according
be    hb   dynamic local     hb    static global     c  dynamic local     b 
to their first response behavior given in table    note that although the two heuristics
b static local do not show a particularly good performance when
c  dynamic local and b 
running them alone  see table     they seem to best complement the other two heuristics 
what we have to find next is the maximal number of visitable nodes we spend for the
heuristics  for this we ran the best performing heuristic  hb   dynamic local  on instances
of the phase transition region of varying sizes  it turned out that for almost all consistent
instances the number of visited nodes required for solving them was slightly less than twice
the size of the instances while most inconsistent instances are also not path consistent and 
thus  solvable with only one visited node  therefore  we ran the four heuristics in the
following allowing   visited nodes each  where is the size of the instance  i e   together
we allow at most   visitable nodes  we randomly generated test instances according to
the        model for a size of       regions up to a size of       regions with
a step of    regions and     instances for each size and each average degree ranging from
h n  d 

 

n

a n  d 

n

n

a n  d 

 

n

n

n

a n  d 

 

n

n

   

 

firenz   nebel

max nodes solved instances
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
    
   
    
   
    
   

combination of heuristics
hb   d l
hb   s g
hb   s g  hb   d l
hb   s g  c   d l
hb   s g  hb   d l  c   d l
b
hb   s g  hb   d l  c   d l  b s l
b
b
b
h  s g  h  d l  c   d l  b s l
b
hb   s g  hb   d l  c   d l  b s l
b
b
b
h  s g  h  d l  c   d l  b s l
b b s g
b
hb   s g  hb   d l  c   d l  b s l 
b b s g
b
hb   s g  hb   d l  b s l 
b
b
b
h  s g  h  d l  b d l

table    best performance of combining different heuristics for solving the     solvable hard
instances of        with a fixed maximal number of visited nodes
a n  d 

 

probability of satisfiability for a n d     

probability    

average number of visited nodes for a n d     

visited nodes

   

    
   
   
   
   
   
   
   
   
   

  

   
    
     
average degree
     

   
   
   
   
   
    nodes
   
   

   
    
     
average degree
     

   
   
   
   
   
    nodes
   
   

figure     probability of satisfiability for             instances per data point  and
average number of visited nodes of the path consistent instances when using
orthogonal combination of the four selected heuristics
a n  d 

 

      to        with a step of      a total number of         instances  since solving
large instances using backtracking requires a lot of memory  we solved the instances on a
sun ultra   with  gb of main memory 
the generated instances display a phase transition behavior which continues the one
given in figure    the phase transition ranges from        for       to       
for        see figure      apart from     instances  all other instances we generated were solvable by orthogonal combination of the four heuristics  hb    static global 
b static local  spending less than  n visited nodes
hb   dynamic local  c  dynamic local  b 
d

 

d

 

d

n

   

 

n

d

 

fiefficient methods for qualitative spatial reasoning

percentile     cpu time using orthogonal combination for a n d     

percentile     cpu time using orthogonal combination for a n d     
cpu time  s 

cpu time  s 
   
  
  
  
  
 
 

  
  

   
    
     
average degree
     

  

   
   
   
   
   
    nodes
   
   

  
 
   
    
     
average degree
     

   
   
   
   
   
    nodes
   
   

figure     percentile     and     cpu time of the orthogonal combination of four different heuristics for solving large randomly generated instances of       
a n  d 

 

each  in figure    we give the average number of visited nodes of the path consistent
instances  it can be seen that for our test instances the average number of visited nodes
is linear in the size of the instances  the percentile     cpu time for instances of the
phase transition with a size of       regions is about    seconds  the percentile     cpu
time is about    seconds  up to a size of       regions  the percentile     cpu time is
less than a minute  see figure     
        of our test instances were already solved by the hb   static global heuristic 
for    instances the hb   dynamic local heuristic was required and for     instances the
c  dynamic local heuristic produced the solution  none of the     instances which were
b static local heuristic  we
not solved by one of those three heuristics were solved by the b 
tried to solve these instances using the other heuristics  again using a maximal number of  
visited nodes each  the best performing among those heuristics was the c  dynamic global
heuristic which solved    of the     instances followed by the c   static global heuristic     
and the q   dynamic global heuristic         instances were not solved by any heuristic
within a maximal number of   visited nodes 
n

n

n

n

   discussion

we empirically studied the behavior of solving randomly generated rsat instances using
different backtracking heuristics some of which make use of the maximal tractable subsets
identified in previous work  we generated instances according to two different models of
which the  general model  allows all     rcc   relations to be used while the  hard
model  allows only relations which are not contained in any of the maximal tractable
subsets  a theoretical analysis of the two models showed that the model and the model
for a small average degree of the nodes in the constraint graph do not suffer from trivial
local inconsistencies as it is the case for similar generation procedures for csps with finite
domains  achlioptas et al          it turned out that randomly generated instances of
both models show a phase transition behavior which depends most strongly on the average
degree of the instances  while most instances outside the phase transition region can be
a

h

h

a

   

firenz   nebel

solved eciently by each of our heuristics  instances in the phase transition region can be
extremely hard  for the instances of the general model  most path consistent instances
are also consistent  conversely  path consistency is a bad approximation to consistency for
instances of the hard model  these instances are also much harder to solve than instances
of the general model 
when comparing the different heuristics  we found that the heuristics using one of the
maximal tractable subsets as a split set are not as much faster in deciding consistency of
rsat instances as their theoretical advantage given by the reduced average branching factor
and the resulting exponentially smaller size of the search space indicates  this is because
using path consistency as a forward checking method considerably reduces the search space
in all cases  nevertheless  using one of the maximal tractable subsets as a split set  in
particular hb    still leads to a much faster solution and solves more instances in reasonable
time than the other heuristics  although the two maximal tractable subsets q  and c 
contain more relations than hb     their average branching factor is lower  i e   when using
hb   one has to decompose more relations                 than when using the other two
sets     and    relations  respectively   but hb   splits the relations better than the other
two sets  most relations can be decomposed into only two hb   sub relations  while many
relations must be decomposed into three c  sub relations or into three q  sub relations 
this explains the superior performance of heuristics involving hb   for decomposition 
among the instances we generated  we stored those which could not be solved by all
heuristics within a maximum number of        visited nodes in the search space in order to
find out how the different heuristics perform on these hard instances  we found that almost
all hard instances are located in the phase transition region and that there are many more
hard instances in the hard model than in the general model  we orthogonally combined all
heuristics and ran them on all hard instances  this turned out to be very successful  apart
from one instance  all hard instances of the general model could be solved  most of them
with a very low number of visited nodes  the hard instances of the hard model were much
more dicult  many of them could not be solved by any of the heuristics  nevertheless 
many more instances were solved by orthogonally combining the heuristics than by each
heuristic alone  again  most of them were solved using a low number of visited nodes 
based on our observations on orthogonally combining different heuristics  we tried to
identify the combination of heuristics which is most successful in eciently solving many
instances and used this combination for solving very large instances  it turned out that
the best combination involves only heuristics which use maximal tractable subsets for decomposition  with this combination we were able to solve almost all randomly generated
instances of the phase transition region of the general model up to a size of       regions
very eciently  this seems to be impossible when considering the enormous size of the
search space  which is on average         for instances of size       when using hb   as a
split set 
our results show that despite its np hardness  we were able to solve almost all randomly generated rsat instances of the general model eciently  this is neither due to the
low number of different rcc   relations  instances generated according to the hard model
are very hard in the phase transition region  nor to our generation procedure for random
instances which does not lead to trivially awed instances asymptotically  it is mainly due
to the maximal tractable subsets which cover a large fraction of rcc   and which lead
n

n

   

fiefficient methods for qualitative spatial reasoning

to extremely low branching factors  since there are different maximal tractable subsets 
they allow choosing between many different backtracking heuristics which further increases
eciency  some instances can be solved easily by one heuristic  other instances by other
heuristics  heuristics involving maximal tractable subclasses showed the best behavior but
some instances can be solved faster when other tractable subsets are used  the full classification of tractable subsets gives the possibility of generating hard instances with a high
probability  many randomly generated instances of the phase transition region are very
hard when using only relations which are not contained in any of the tractable subsets
and consist of more than      regions  the next step in developing ecient reasoning
methods for rcc   is to find methods which are also successful in solving most of the hard
instances of the hard model 
n

the results of our empirical evaluation of reasoning with rcc   suggest that analyzing
the computational properties of a reasoning problem and identifying tractable subclasses of
the problem is an excellent way for achieving ecient reasoning mechanisms  in particular
maximal tractable subclasses can be used to develop more ecient methods for solving
the full problem since their average branching factor is the lowest  using the refinement
method developed in renz s        paper  tractable subclasses of a set of relations forming
a relation algebra can be identified almost automatically  this method makes it very easy
to develop ecient algorithms  a further indication of our empirical evaluation is that it
can be much more effective  even and especially for hard instances of the phase transition
region  to orthogonally combine different heuristics than to try to get the final epsilon out of
a single heuristic  this answers a question raised by nebel        of whether the orthogonal
combination of heuristics is also useful in the phase transition region  in our experiments
this lead to much better results even when simulating the orthogonal combination of different
heuristics on a single processor machine and spending altogether the same resources as
for any one heuristic alone  in contrast to the method of time slicing between different
heuristics  we started a new heuristic only if the previous heuristic failed after a certain
number of visited nodes in the search space  the order in which we ran the heuristics
depended on their performance and on how well they complemented each other  more
successful heuristics were used first  this is similar to using algorithm portfolios as proposed
by huberman et al          which heuristics perform better and which combination is the
most successful one is a matter of empirical evaluation and depends on the particular
problem  heuristics depending on maximal tractable subclasses  however  should lead to
the best performance 
for csps with finite domains there are many theoretical results about localizing the
phase transition behavior and about predicting where hard instances are located  in contrast to this  there are basically no such theoretical results for csps with infinite domains
as used in spatial and temporal reasoning  as our initial theoretical analysis shows  theoretical results on csps with finite domains do not necessarily extend to csps with infinite
domains  it would be very interesting to develop a more general theory for csps with
infinite domains  possibly similar to williams and hogg s  deep structure   williams  
hogg        or gent et al  s  kappa  theory  gent  macintyre  prosser    walsh        
   

firenz   nebel
acknowledgments

we would like to thank ronny fehling for his assistance in developing the programs  malte
helmert for proof reading the paper  and the three anonymous reviewers for their very
helpful comments 
this research has been supported by dfg as part of the project fast qual space  which
is part of the dfg special research effort on  spatial cognition   the first author has been
partially supported by a marie curie fellowship of the european community programme
 improving human potential  under contract number hpmf ct             a preliminary version of this paper appeared in the proceedings of the   th european conference on
artificial intelligence  renz   nebel        
references

achlioptas  d   kirousis  l   kranakis  e   krizanc  d   molloy  m     stamatiou  y         
random constraint satisfaction  a more accurate picture  in  rd conference on the
principles and practice of constraint programming  cp      vol       of lncs  pp 
         springer verlag 
allen  j  f          maintaining knowledge about temporal intervals  communications of
the acm                   
bennett  b          spatial reasoning with propositional logic  in doyle  j   sandewall 
e     torasso  p   eds    principles of knowledge representation and reasoning 
proceedings of the  th international conference  pp         bonn  germany  morgan
kaufmann 
bennett  b          logical representations for automated reasoning about spatial relationships  ph d  thesis  school of computer studies  the university of leeds 
cheeseman  p   kanefsky  b     taylor  w  m          where the really hard problems are 
in proceedings of the   th international joint conference on artificial intelligence 
pp           sydney  australia  morgan kaufmann 
gent  i   macintyre  e   prosser  p   smith  b     walsh  t          random constraint
satisfaction  flaws and structure  constraints                 
gent  i   macintyre  e   prosser  p     walsh  t          the constrainedness of search  in
proceedings of the   th national conference on ai  aaai      pp          
golumbic  m  c     shamir  r          complexity and algorithms for reasoning about time 
a graph theoretic approach  journal of the association for computing machinery 
                  
grigni  m   papadias  d     papadimitriou  c          topological inference  in proceedings
of the   th international joint conference on artificial intelligence  pp          
montreal  canada 
   

fiefficient methods for qualitative spatial reasoning

haralick  r  m     elliot  g  l          increasing tree search eciency for constraint
satisfaction problems  artificial intelligence              
huberman  b   lukose  r     hogg  t          an economics approach to hard computational problems  science             
ladkin  p  b     reinefeld  a          effective solution of qualitative interval constraint
problems  artificial intelligence                  
ladkin  p  b     reinefeld  a          fast algebraic methods for interval constraint problems  annals of mathematics and artificial intelligence           
mackworth  a  k          consistency in networks of relations  artificial intelligence    
       
mackworth  a  k     freuder  e  c          the complexity of some polynomial network
consistency algorithms for constraint satisfaction problems  artificial intelligence     
      
montanari  u          networks of constraints  fundamental properties and applications to
picture processing  information science            
nebel  b          computational properties of qualitative spatial reasoning  first results  in
wachsmuth  i   rollinger  c  r     brauer  w   eds    ki     advances in artificial
intelligence  vol      of lecture notes in artificial intelligence  pp           bielefeld 
germany  springer verlag 
nebel  b          solving hard qualitative temporal reasoning problems  evaluating the
eciency of using the ord horn class  constraints                 
randell  d  a   cohn  a  g     cui  z       a   computing transitivity tables  a challenge
for automated theorem provers  in proceedings of the   th cade  springer verlag 
randell  d  a   cui  z     cohn  a  g       b   a spatial logic based on regions and
connection  in nebel  b   swartout  w     rich  c   eds    principles of knowledge
representation and reasoning  proceedings of the  rd international conference  pp 
         cambridge  ma  morgan kaufmann 
renz  j          maximal tractable fragments of the region connection calculus  a complete analysis  in proceedings of the   th international joint conference on artificial
intelligence  pp           stockholm  sweden 
renz  j          qualitative spatial reasoning with topological information  ph d  thesis 
institut fur informatik  albert ludwigs universitat freiburg 
renz  j     nebel  b          ecient methods for qualitative spatial reasoning  in proceedings of the   th european conference on artificial intelligence  pp           amsterdam  the netherlands  wiley 
   

firenz   nebel

renz  j     nebel  b          on the complexity of qualitative spatial reasoning  a maximal
tractable fragment of the region connection calculus  artificial intelligence                   
selman  b   levesque  h  j     mitchell  d          a new method for solving hard satisfiability problems  in proceedings of the   th national conference of the american
association for artificial intelligence  pp           san jose  ca  mit press 
van beek  p     manchak  d  w          the design and experimental analysis of algorithms
for temporal reasoning  journal of artificial intelligence research          
williams  c  p     hogg  t          exploiting the deep structure of constraint problems 
artificial intelligence             

   

fi