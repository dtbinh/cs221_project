journal artificial intelligence research                  

submitted       published      

infinite horizon policy gradient estimation
jonathan baxter

jbaxter   whizbang   com

whizbang  labs 
     henry street pittsburgh  pa      

peter l  bartlett

bartlett   barnhilltechnologies   com

biowulf technologies 
     addison street  suite      berkeley  ca      

abstract
gradient based approaches direct policy search reinforcement learning received
much recent attention means solve problems partial observability avoid
problems associated policy degradation value function methods  paper introduce gpomdp  simulation based algorithm generating biased estimate gradient
average reward partially observable markov decision processes  pomdps  controlled
parameterized stochastic policies  similar algorithm proposed kimura  yamamura 
kobayashi         algorithms chief advantages requires storage twice
number policy parameters  uses one free parameter           which natural interpretation
terms bias variance trade off   requires knowledge underlying state  prove
convergence gpomdp  show correct choice parameter related
mixing time controlled pomdp  briefly describe extensions gpomdp controlled
markov chains  continuous state  observation control spaces  multiple agents  higher order
derivatives  version training stochastic policies internal states  companion paper
 baxter  bartlett    weaver        show gradient estimates generated gpomdp
used traditional stochastic gradient algorithm conjugate gradient procedure
find local optima average reward 

   introduction
dynamic programming method choice solving problems decision making
uncertainty  bertsekas         however  application dynamic programming becomes problematic large infinite state spaces  situations system dynamics unknown 
state partially observed  cases one looks approximate techniques
rely simulation  rather explicit model  parametric representations either valuefunction policy  rather exact representations 
simulation based methods rely parametric form value function tend go
name reinforcement learning  extensively studied machine learning
literature  bertsekas   tsitsiklis        sutton   barto         approach yielded
remarkable empirical successes number different domains  including learning play checkers  samuel         backgammon  tesauro               chess  baxter  tridgell    weaver 
       job shop scheduling  zhang   dietterich        dynamic channel allocation  singh  
bertsekas        
despite success  algorithms training approximate value functions suffer
theoretical flaw  performance greedy policy derived approximate valuefunction guaranteed improve iteration  fact worse old policy

c      ai access foundation morgan kaufmann publishers  rights reserved 

fibaxter   bartlett

amount equal maximum approximation error states  happen even
parametric class contains value function whose corresponding greedy policy optimal 
illustrate concrete simple example appendix a 
alternative approach circumvents problemthe approach pursue hereis
consider class stochastic policies parameterized   r k   compute gradient respect
average reward  improve policy adjusting parameters gradient
direction  note policy could directly parameterized  could generated indirectly
value function  latter case value function parameters parameters
policy  instead adjusted minimize error approximate true value
function  parameters adjusted directly improve performance policy generated
value function 
policy gradient algorithms long history operations research  statistics  control theory  discrete event systems machine learning  describing contribution
present paper  seems appropriate introduce background material explaining approach  readers already familiar material may want skip directly section     
contributions present paper described 
    brief history policy gradient algorithms
large scale problems problems system dynamics unknown  performance
gradient computable closed form    thus challenging aspect policy gradient
approach find algorithm estimating gradient via simulation  naively  gradient
calculated numerically adjusting parameter turn estimating effect performance via simulation  the so called crude monte carlo technique   prohibitively
inefficient problems  somewhat surprisingly  mild regularity conditions  turns
full gradient estimated single simulation system  technique
called score function likelihood ratio method appears first proposed
sixties  aleksandrov  sysoyev    shemeneva        rubinstein        computing performance
gradients i i d   independently identically distributed  processes 
specifically  suppose r x performance function depends random variable
x   q   x probability x x  parameterized   rk   mild regularity
conditions  gradient respect expected performance 

   

   

may written

see this  rewrite     sum

 

     er x   

   

r     er x   rqq    xx    

   

    

x

r    

x

r x q   x  

x
differentiate  one source requirement mild regularity conditions  obtain
x

r x rq   x  

   see equation      closed form expression performance gradient 

   

fip olicy g radient e stimation

rewrite

x

r    

r x 

rq   x  q   x  

   

q   x
x
observe formula equivalent     
simulator available generate samples x distributed according q   x  
sequence x    x            xn generated i i d  according q   x gives unbiased estimate 

   

   
n
x
r       n  r xi  rqq    xx i   
   


       r   probability one  quantity
r      law large numbers  r
rq   x   q   x   known likelihood ratio score function classical statistics 
performance function r  x   depends   r  x  rq    x   q    x   replaced
rr   x     r   x  rq   x   q   x       
  

      u nbiased e stimates
p rocesses



p erformance g radient



r egenerative

extensions likelihood ratio method regenerative processes  including markov decision
processes mdps  given glynn               glynn lecuyer        reiman
weiss               independently episodic partially observable markov decision
processes  pomdps  williams         introduced reinforce algorithm   
i i d  samples x previous section sequences states x            xt  of random length 
encountered visits designated recurrent state   sequences states
start state goal state  case rq   x  q   x written sum

 

   

 

rq   x     tx rpxtxt       
 

q   x  

  

t  

pxt xt     

   

pxt xt   transition probability xt xt   given parameters   equation    
admits recursive computation course regenerative cycle form z 
  rk  
state transition xt   xt    

  

zt     zt  

     
 

   

 

rpxtxt       
pxt xt     

term r x rq   x  q   x estimate     form 
addition  r x            xt recursively computed

 

   

r x            xt  zt   if 

r x            xt        r x            xt    xt    
function   estimate r  x            xt  zt cycle computed using
storage k     parameters  k zt   parameter update performance function
r   hence  entire estimate     computed storage  k     real parameters 
follows 

   thresholded version algorithms neuron like elements described earlier barto  sutton  anderson        
   vector zt known reinforcement learning eligibility trace  terminology used barto et al 
       

   

fibaxter   bartlett

algorithm      policy gradient algorithm regenerative processes 
   set j

     r      z           z     rk   
 

 

 

   state transition xt




   j

 

  xt

  

 

 

episode finished  that is  xt  
j   
j rt zt  
j j  
zt    
rt    

   
    
  
  
otherwise  set

rp

  i   set



zt     zt   pxxtxtxt      
t  
rt      rt   xt     
   

  n return n  n   otherwise goto   

examples recursive
performance functions include sum scalar reward cycle 
p

r x            xt     tt   r xt   r i  scalar reward associated state  this corresponds     average reward multiplied expected recurrence time e  t    

 
  

negative length cycle  which implemented assigning reward
state 
used task mimimize time taken get goal state  since
case
pt

e    discounted reward start state  r x            xt
t   r xt  
    discount factor  on 
williams        pointed out  simplification possible case rt
r x            xt sum scalar rewards r xt   depending state possibly time
since starting state  such r xt   r xt   r xt   fft r xt above   case 
update single regenerative cycle may written

   
     

 

 

 

 



 

tx 
t  

pxt xt     

 
x

s  

   

 

   
        

rpxtxt     

  

  

  

r xs   s   


x
s t  

   

 

r xs   s   

 

 

changes pxt xt   influence rewards r xs   associated earlier
states  s t   able drop first term parentheses right hand side
write
tx 

rpxtxt   x
r x  s  
   
p
s t  
t   xt xt  

 

  
  

 

 

although proof entirely trivial  intuition indeed shown correct 
equation     allows even simpler recursive formula estimating performance gradient  set z 
  introduce new variable
  before  set zt  
zt
 
rpxtxt    pxtxt   xt      
zt  
otherwise 
now  iteration  set t   r xt   zt
   t estimate r   since
updated every iteration  suggests away altogether simply update directly  t   r xt   zt   suitable step sizes    proving convergence

  

p

    
  

    
 
       
       



  
  

  



   usual requirements convergence stochastic gradient algorithm     
       
t  

 

   

 

  

p t  

 

 


  

fip olicy g radient e stimation

algorithm straightforward normal stochastic gradient algorithms
updates r xt zt gradient direction  in expectation   although sum updates
regenerative cycle are  marbach tsitsiklis        provide convergence proof
know of  albeit slightly different update form t  
r xt   zt  
moving estimate expected performance  updated on line  this
update first suggested context pomdps jaakkola et al          
marbach tsitsiklis        considered case  dependent rewards  recall discussion       baird moore        vaps algorithm  value policy
search   last paper contains interesting insight  suitable choices performance
function r x            xt     one combine policy gradient search approximate value function methods  resulting algorithms viewed actor critic techniques spirit barto
et al          policy actor value function critic  primary motivation
reduce variance policy gradient estimates  experimental evidence phenomenon
presented number authors  including barto et al          kimura kobayashi
     a   baird moore         recent work subject includes sutton
et al         konda tsitsiklis         discuss use vaps style updates
section     
far addressed question parameterized state transition probabilities pxt xt   arise  course  could simply generated parameterizing matrix
transition probabilities directly  alternatively  case mdps pomdps  state transitions
typically generated feeding observation yt depends stochastically state xt
parameterized stochastic policy  selects control ut random set available controls  approximate value function based approaches generate controls stochastically
via form lookahead fall category   distribution successor states
pxt xt   ut fixed function control  denote probability control ut given
parameters observation yt ut   yt   discussion carries
rpxtxt    pxtxt   replaced rut   yt  ut   yt   case  algorithm     precisely williams reinforce algorithm 
algorithm     variants extended cover multiple agents  peshkin
et al          policies internal state  meuleau et al          importance sampling methods
 meuleau et al          refer reader work rubinstein shapiro       
rubinstein melamed        in depth analysis application likelihood ratio
method discrete event systems  des   particular networks queues  worth mentioning
large literature infinitesimal perturbation analysis  ipa   seeks similar goal estimating performance gradients  operates restrictive assumptions likelihoodratio approach  see  example  ho cao        

   

      

    

 

       

 

  

   
  

 

  

      b iased e stimates



 
 

 

 

 

p erformance g radient

algorithms described previous section rely identifiable recurrent state   either
update gradient estimate  case on line algorithm  zero eligibility trace
z   reliance recurrent state problematic two main reasons 
   variance algorithms related recurrence time visits  
typically grow state space grows  furthermore  time visits depends
   

fibaxter   bartlett

parameters policy  states frequently visited initial value
parameters may become rare performance improves 
   situations partial observability may difficult estimate underlying states 
therefore determine gradient estimate updated  eligibility trace
zeroed 
system available simulation  seems difficult  if impossible  obtain
unbiased estimates gradient direction without access recurrent state  thus  solve  
   must look biased estimates  two principle techniques introducing bias
proposed  may viewed artificial truncations eligibility trace z   first
method takes starting point formula  eligibility trace time t 

zt  

 
x

rpxsxs     
pxs xs     

s  

simply truncates  fixed  random  number terms n looking backwards  glynn 
      rubinstein              cao   wan        

zt  n    

 
x
s t n

rpxsxs       
pxs xs     

   

 n  updated transition xt   xt
rp
   rpxt nxt n       
zt  n    zt  n    xtxt  
pxt xt     
pxt n xt n     
case state based rewards r  xt    estimated gradient direction steps

 rn        x zt  n r xt   
n   n
eligibility trace zt

  

  

   

   

 

unless n exceeds maximum recurrence time  which infinite ergodic markov chain  
rn biased estimate gradient direction  although n      bias approaches zero 
however variance rn diverges limit large n  illustrates natural trade off
selection parameter n  large enough ensure bias acceptable  the
expectation rn least within true gradient direction   large
variance prohibitive  experimental results cao wan        illustrate nicely
bias variance trade off 
one potential difficulty method likelihood ratios rpxs xs    pxs xs  
must remembered previous n time steps  requiring storage kn parameters  thus 
obtain small bias  memory may grow without bound  alternative approach
requires fixed amount memory discount eligibility trace  rather truncating it 

    

    

    

  

  

zt    fi      fizt  fi    

rpxtxt       
pxt xt     

  

    

r

   ease exposition  kept expression z terms likelihood ratios pxs xs      pxs xs     
rely availability underlying state xs   xs available  pxs xs      pxs xs     
replaced us    ys   us    ys   

r

r

   

fip olicy g radient e stimation

     

z 

steps simply

         discount factor 
r fi       t 

tx 
t  

case estimated gradient direction

r xt  zt  fi   

    

      
      

precisely estimate analyze present paper  similar estimate r xt zt
replaced r xt
b zt b reward baseline proposed kimura et al        
      continuous control kimura kobayashi      b   fact use r xt
b
place r xt affect expectation estimates algorithm  although judicious choice reward baseline b reduce variance estimates   algorithm
presented kimura et al         provides estimates expectation stationary distribution gradient discounted reward  show fact biased estimates
gradient expected discounted reward  arises stationary distribution
depends parameters  similar estimate      proposed marbach
tsitsiklis         time r xt zt replaced r xt
zt  
estimate average reward  zt zeroed visits identifiable recurrent state 
final note  observe eligibility traces zt zt n defined         
simply filtered versions sequence rpxt xt    pxt xt     first order  infinite impulse
response filter case zt n th order  finite impulse response filter case
zt n   raises question  addressed paper  whether interesting theory
optimal filtering policy gradient estimators 

    
   

    

      

               

  

  

  

  

    

  
  

    contribution
describe gpomdp  general algorithm based upon      generating biased estimate
performance gradient r general pomdps controlled parameterized stochastic policies 
denotes average reward policy parameters   rk   gpomdp
rely access underlying recurrent state  writing rfi expectation estimate produced gpomdp  show
r   quantitatively
   rfi
rfi close true gradient provided   exceeds mixing time markov chain
induced pomdp    truncated estimate above  trade off preventing setting
arbitrarily close variance algorithms estimates increase approaches
  prove convergence probability   gpomdp discrete continuous observation control spaces  present algorithms general parameterized markov chains
pomdps controlled parameterized stochastic policies 
several extensions gpomdp investigated since first version
paper written  outline developments briefly section   
companion paper show gradient estimates produced gpomdp used
perform gradient ascent average reward  baxter et al          describe
traditional stochastic gradient algorithms  conjugate gradient algorithm utilizes gradient
estimates novel way perform line searches  experimental results presented illustrat 

  

  

  
lim
        
      

  

 

 

  

   mixing time result paper applies markov chains distinct eigenvalues  better estimates
bias variance gpomdp may found bartlett baxter         general markov chains
treated here  refined notions mixing time  roughly speaking  variance gpomdp
grows         bias decreases function        

   

fibaxter   bartlett

ing theoretical results present paper toy problem  practical aspects
algorithms number realistic problems 

   reinforcement learning problem
model reinforcement learning markov decision process  mdp  finite state space
f           ng  stochastic matrix  p pij giving probability transition state
state j   state associated reward  r   matrix p belongs parameterized
class stochastic matrices  p
fp   rk g  denote markov chain corresponding
p   assume markov chains rewards satisfy following assumptions 

   

  

    

  

       

  

  

assumption    p   p unique stationary distribution
satisfying balance equations

                         n   

    p          
 throughout   denotes transpose   
assumption    magnitudes rewards  jr  i j  uniformly bounded r  
states i 

    

 

assumption   ensures markov chain forms single recurrent class parameters  
since finite state markov chain always ends recurrent class  properties
class determine long term average reward  assumption mainly convenience
include recurrence class quantifier theorems  however 
consider gradient ascent algorithms baxter et al          assumption becomes
restrictive since guarantees recurrence class cannot change parameters adjusted 
ordinarily  discussion mdps would complete without mention actions
available state space policies available learner  particular  parameters
would usually determine policy  either directly indirectly via value function   would
determine transition probabilities p   however  purposes care
dependence p arises  satisfies assumption    and differentiability
assumptions shall meet next section   note easy extend setup
case rewards depend parameters transitions   j  
equally straightforward extend algorithms results cases  see section    
illustration 
goal find   r k maximizing average reward 

 
 
tx 



e
r xt x   

  
t  
e denotes expectation sequences x    x            transitions generated according p   assumption    independent starting state equal
n
x

  r   r 
    
i  

  

 

       lim

  

 

  

    

r

   

           

   r             r n     bertsekas        


p

   stochastic matrix p    pij   pij   i  j n
j    pij     i 
   results present paper apply bounded stochastic rewards  case r i  expectation
reward state i 

   

fip olicy g radient e stimation

   computing gradient average reward

  

general mdps little known average reward   hence finding optimum
problematic  however  section see general assumptions gradient
r exists  local optimization possible 
ensure existence suitable gradients  and boundedness certain random variables  
require parameterized class stochastic matrices satisfies following additional assumption 

  

  

assumption    derivatives 

rp      
  rk   ratios

exist



 pij   
 k

i j      n k     k

  fifi  p    fifi  
ij
 k
 
 

pij   

uniformly bounded b



i j      n k     k

      rk  

second part assumption allows zero probability transitions pij

      

rpij    zero  case set          one example   j forbidden
transition  pij           rk   another example satisfying assumption
pij     


   

  

           n           nn     rn 

parameters p

 pij     ij
pij   
 pij     kl
pij   

assuming moment r
dependencies 

eij
ij  
j    e

pn

  
 

pij    

   



pkl    

   exists  this justified shortly   then  suppressing
r   r r 
    

since reward r depend   note convention r paper takes
precedence operations  rg f
rg f   equations     
regarded shorthand notation k equations form

                    

   
 k

k

 





      
    n 
     
 r             r n   
 k
 k

             k   compute r  first differentiate balance equations      obtain
r p     rp   r  
   

fibaxter   bartlett

hence

r  i p       rp 

    

system equations defined      under constrained p invertible  the
balance equations show p left eigenvector zero eigenvalue   however  let e
denote n dimensional column vector consisting s  e   n n matrix
stationary distribution   row  since r   e r   e
r
  rewrite     

 

               





r   p e      rp 
see inverse
write

 i  p
 

lim  i

e    
a 

  


x
t  

 

exists  let matrix satisfying
 

  tlim
  



 

t  

  tlim
  
  i 

thus 

 i

 
x

a 

 

 

 
x
t  

tx
  



  

t  

limt       

 



 

   

 

easy prove induction p e  
p ep 
   
tconverges
 
 
    hence  write
assumption   
p e
exists equal  
p
e
t  

 

 

  



r     rp p   e 
so 



r    rp p   e 





 

    

r 

    

 

 

mdps sufficiently small number states       could solved exactly yield precise
gradient direction  however  general  state space small enough exact solution
     possible  small enough derive optimal policy using policy iteration
table lookup  would point pursuing gradient based approach first place    
thus  problems practical interest       intractable need find
way computing gradient  one approximate technique presented
next section 
   argument leading      coupled fact    unique solution      used justify
existence   specifically  run steps computing value       small
show expression      unique matrix satisfying                   o      
    equation      may still useful pomdps  since case tractable dynamic programming
algorithm 

r

r

r

   

kk

fip olicy g radient e stimation

   approximating gradient parameterized markov chains
section  show gradient split two components  one becomes
negligible discount factor approaches  
      let jfi
jfi             jfi   n denote vector expected discounted
rewards state i 

     

            

jfi    i     e

 

 

    

 
x
t  

fitr



xt fifi x 


   

 

 i

 

    

dependence obvious  write jfi  

         
r       r  jfi   fi rp jfi  

proposition      r k

    

proof  observe jfi satisfies bellman equations 

jfi   r   fip jfi  

    

 bertsekas         hence 

r   r r
  r   jfi fip jfi  
  r jfi r jfi   fi  rp jfi
      r  jfi   fi rp jfi  

    

shall see next section second term      estimated single sample path markov chain  fact  theorem    kimura et al         shows gradient
estimates algorithm presented paper converge
  rjfi   bellman equations       equal
  rp jfi   rjfi   implies
  rjfi fi  rp jfi  
thus algorithm kimura et al         estimates second term expression
r given       important note  rjfi   r   jfi two quantities disagree
first term       arises stationary distribution depends
parameters  hence  algorithm kimura et al         estimate gradient expected discounted reward  fact  expected discounted reward simply  
times
average reward  singh et al         fact     gradient expected discounted reward
proportional gradient average reward 
following theorem shows first term      becomes negligible approaches  
notice immediate proposition    since jfi become arbitrarily large
limit    

      

 

  

 

   

 

 

  

    

 

    

  

 

 

 

theorem      rk  

r   filim
r  
 

    

rfi      rp jfi  

    

 



   

fibaxter   bartlett

proof  recalling equation      discussion preceeding it    

r    rp
rp e

 
x



e  r 

pt

t  

    

  r p e    r        since p stochastic matrix       rewritten
r    

 
 
x

 

rp p r 

t  

    

         discount factor consider expression

let

f  fi       

  lim
    

  

 
 
x

t  

 

rp  fip  t r

    

    

clearly r
rfi  
   f   complete proof need show f




 
since fip
p   e     invoke observation      write

 

 
x
t  
p

particular   
t  
     write  

 fip  t    i

 fip  t converges  take rp back sum right hand side
f  fi       rp



p 

t  


fitp r

fip      

  jfi   thus f  fi      rp jfi

  
x

t  

 

fitp

r 

    

  rfi  

 

theorem   shows rfi good approximation gradient approaches  
turns values close lead large variance estimates rfi
describe next section  however  following theorem shows
need
small  provided transition probability matrix p distinct eigenvalues  markov
chain short mixing time  initial state  distribution states markov chain
converges stationary distribution  provided assumption  assumption    existence
uniqueness stationary distribution satisfied  see  example  lancaster   tismenetsky 
      theorem         p        spectral resolution theorem  lancaster   tismenetsky       
theorem        p       implies distribution converges stationarity exponential rate 
time constant convergence rate  the mixing time  depends eigenvalues
transition probability matrix  existence unique stationary distribution implies

 

 

  

    since e   r   e        motivates different kind algorithm estimating based differential rewards
 marbach   tsitsiklis        
    cannot back p sum right hand side       
p diverges  p e      reason
  p p converges p becomes orthogonal p limit tof  large t  thus  view   p
t  
t  
sum two orthogonal components  infinite one direction e finite one direction e   
 


finite component need estimate  approximating  
t   p t    fip   way rendering
e component finite hopefully altering e   component much  substitutions
lead better approximations  in context  see final paragraph section      

p

r

r

r

   

p

p

r

 

p

p

fip olicy g radient e stimation

 

 

largest magnitude eigenvalue multiplicity   corresponding left eigenvector
stationary distribution  sort eigenvalues decreasing order magnitude 
    j  j     js j n  turns j  j determines mixing time
chain 
following theorem shows
small compared
j j  gradient approximation described accurate  since using estimate direction
update parameters  theorem compares directions gradient estimate 
theorem    denotes spectral condition number nonsingular matrix a  defined
product spectral norms matrices    

  

 

 

 

   

   a    kak  ka



 

k 
 

kak   x max
kaxk 
kxk
 

 

  

kxk denotes euclidean norm vector x 

  

theorem    suppose transition probability matrix p satisfies assumption   stationary distribution  
            n   n distinct eigenvalues  let
x  x  xn
matrix right eigenvectors p corresponding  order  eigenvalues
    j  j
jn j  normalized inner product r rfi satisfies

  

 

  
  

 


kr p           p  k p
rfi
 
n
  rkr

 s
r r
    
k
krk
  j j  
  diag            n   
notice r   r expectation stationary distribution r  x    
well mixing time  via j j   bound theorem depends another parameter
markov chain  spectral condition number     markov chain reversible  which
 

   

 

 

 

 

 

 

   

implies eigenvectors x            xn orthogonal   equal ratio maximum
minimum probability states stationary distribution  however  eigenvectors
need nearly orthogonal  fact  condition transition probability matrix
n distinct eigenvalues necessary  without it  condition number replaced
complicated expression involving spectral norms matrices form p  

 

 



proof  existence n distinct eigenvalues implies p expressed    
            n  lancaster   tismenetsky        theorem         p       follows
polynomial f   write f p
sf    
now  proposition   shows r rfi r  
jfi  

  diag 

 

        
      
    jfi        r   fip r   p r  
         fip   fi p   r
 
x
      s
r
 

 

 

 

 

    

fi 

n
x
j   

t  

xj  

   

j

 
x
t  

 fij  

 



r 

fibaxter   bartlett

  
 

 

 
y            yn    
easy verify yi left eigenvector corresponding   choose
y  x  e  thus write

 

  

 jfi       e  r  

n
x

xj yj 

  

 

  fij  t r

t  
j   


n
x

r
xj yj 

j
j   

    

 e  

    

 e   sms   r 


 
 



 
  diag   
 



 
x


   
     
fi 
  fin

follows proposition  

rfi
r  r r   
  rkr
 
 
k
krk
 
  r r     jfi
 

 

 jfi  

krk

r
r      e   sms r
 
krk
 
sms r
  r rkr
k

r   sms r

krk  
p
 
since r   r
      apply cauchy 

 

 

 

 

 

cauchy schwartz inequality 
schwartz inequality obtain

   

rfi
  rkr

k





r



p

 





  sms
   

 

krk

 



r

 

    

use spectral norms bound second factor numerator  clear definition
spectral norm product nonsingular matrices satisfies kab k  kak  kb k   
spectral norm diagonal matrix given k
d            dn k 
jdi j  follows




  sms
   

 

diag 
    max


r     sms     r




      r km k

p
  r r     jfi j  
   

 

   

 

 

 

   

   

   

   

 

   

 

combining equation      proves      
   

 

fip olicy g radient e stimation

   estimating gradient parameterized markov chains
algorithm   introduces mcg  markov chain gradient   algorithm estimating approximate gradient rfi single on line sample path x    x          markov chain  
mcg requires k reals stored  k dimension parameter space  k
parameters eligibility trace zt   k parameters gradient estimate   note
time steps average far r xt zt  

  

 





   

tx
 
 
zt r xt   
 



t  

algorithm   mcg  markov chain gradient  algorithm
   given 




parameter   r k  
parameterized class stochastic matrices p
    

  fp       rk g satisfying assumptions

         
arbitrary starting state x  
state sequence x   x        
 

generated      i e  markov chain transition
    
reward sequence r x    r x          satisfying assumption   
set z          z     rk   
state xt visited
rp
   
zt   fizt   xtxt  
pxt xt     
     r xt  zt  
probabilities p

 

 

 

  
  
  
  
  

 

 

 

 

 

  

  

 
  

  

  

  

end

theorem    assumptions         mcg algorithm starting initial state x 
generate sequence                         satisfying





lim   rfi

t  

 

w p   

    

  

proof  let fxt g fx    x          g denote random process corresponding   x 
entire process stationary  proof easily generalized arbitrary initial distributions using fact assumption    fxt g asymptotically stationary  fxt g
   

fibaxter   bartlett

stationary  write

  rp jfi  

x

 

x

 

x

i j
i j
i j

 i rpij   jfi  j  
 i pij   

rpij    j  j  
p   
ij

pr xt   i pr xt   j jxt   i  rppij     e j  t     jxt   j   
  

  

ij

first probability respect stationary distribution

j  t       

        

    

 
x
s t  

fis



 

    

j  t      process

r xs   

 

fact e j
jxt   jfi xt   xt   follows boundedness
magnitudes rewards  assumption    lebesgues dominated convergence theorem 
rewrite equation     




x
rp   
  rp jfi   e  xt  j  xt     ij j  t       

pij   

i j



   denotes indicator function state i 
 
  xt   i 
 xt     
  otherwise 

expectation respect stationary distribution  xt chosen according
stationary distribution  process fxt g ergodic  since process fzt g defined

zt     xt  j  xt    

rpij    j  t     
pij   

obtained taking fixed
fxt g  fzt g stationary ergodic  breiman       
function

rpij   
proposition        since pij    bounded assumption    ergodic theorem
 almost surely  

  rp jfi

tx
 
rp   
  tlim
 xt  j  xt   ij j  t     
  
pij   
i j
tx
rpxtxt      j  t     
 
  tlim
  
pxtxt     
 
tx
 
x
r
pxtxt      x
 
  tlim
r xs    
  
pxtxt     

x

 

  

  

 

  

 

 

  

    

   

    

 

fis

 

r xs    

    

fip olicy g radient e stimation

concentrating second term right hand side       observe that 

tx 


fit

 

t  

rpxtxt     
pxt xt     

 
x

fis

s t   
tx  fifi

 





 



xs fifi


r 

 

pxt xt     

t  
 
br tx  x



  br


 
x

rpxtxt      fififi

t   s t   
tx 



t  

 

fis





s t   

fis



 

jr xs j

 



  fit
  brfi
     
        



jrp j
r b bounds magnitudes rewards pijij assumptions  
   hence 
tx 

rpxtxt   x
  rp jfi
    
  r xs  
  
p

x
x
t  
t  
s t  
unrolling equation mcg algorithm shows equal

  
  

  lim  



  tx rpxtxt      x
pxt xt     
 

  

hence

fis

   



 

r is   

    

   rp jfi w p   required 

   estimating gradient partially observable markov decision processes

  

algorithm   applies parameterized class stochastic matrices p compute gradients rpij   section consider special case p arise
parameterized class randomized policies controlling partially observable markov decision process  pomdp   partially observable qualification means assume policies
access observation process depends state  general may see state 
specifically  assume n controls u
f           n g observations
f           g  u   u determines stochastic matrix p u depend
parameters   state     observation   generated independently according
probability distribution observations   denote probability observation
  randomized policy simply function mapping observations   probability
distributions controls u   is  observation   distribution
controls u   denote probability control u given observation u  
randomized policy observation distribution corresponds markov
chain state transitions generated first selecting observation state according

  

   

 

  

  

 

  

  

  
  

  

   

  

fibaxter   bartlett

  

  

distribution   selecting control u according distribution   generating transition state j according probability pij u   parameterize chains
parameterize policies  becomes function   set parameters   r k
well observation   markov chain corresponding state transition matrix pij
given

  
   

      

pij      ey  i  eu   y   pij  u   

equation      implies

rpij     

x

    

 i pij  u ru    y  

u y

    

algorithm   introduces gpomdp algorithm  for gradient partially observable markov
decision process   modified form algorithm   updates zt based ut   yt  
rather pxt xt     note algorithm   require knowledge transition probability matrix p   observation process   requires knowledge randomized
policy   gpomdp essentially algorithm proposed kimura et al         without
reward baseline 
algorithm gpomdp assumes policy function current observation 
immediate algorithm works finite history observations  general 
optimal policy needs function entire observation history  gpomdp extended
apply policies internal state  aberdeen   baxter        

 

  

 

algorithm   gpomdp algorithm 
   given 




parameterized class randomized policies



         rk



satisfying assumption   

partially observable markov decision process controlled randomized
policies   corresponds parameterized class markov chains satisfying assumption   

   

         
arbitrary  unknown  starting state x  
observation sequence           generated pomdp controls u   u        
 

 

 

 

   yt  

generated randomly according


  
  
  
  
  

       

reward sequence r x    r x          satisfying assumption   
 hidden  sequence states markov decision process 

  

  
         
         

set z 
 
 z        rk   
observation yt   control ut   subsequent reward r
rut   yt
zt   fizt
ut   yt
 
t  
t   r xt   zt  

end

 

   

 xt  
  

x    x         

 



fip olicy g radient e stimation

convergence algorithm   need replace assumption   similar bound
gradient  
assumption    derivatives 

exist u   u  

 u    y 
 k

    rk   ratios

  fifi
 u   y   
 

k
 
 

u    y 

uniformly bounded b

y     m  u     n  k     k

      rk  

theorem    assumptions         algorithm   starting initial state
generate sequence                         satisfying





lim   rfi

w p   

t  

x 



    

proof  proof follows lines proof theorem    case 

  rp jfi  

 
 
 

x

i j

 i rpij   jfi  j  

x

i j y u
x

i j y u
x

i j y u

 i pij  u y  i ru    y jfi  j       
 i pij  u y  i 

ru    y     y j  j   

   y  u
u

ezt  

expectation respect stationary distribution fxt g  process fzt  g
defined
ru   j  
zt  xt j xt   u ut yt
u  

        

                      

ut control process yt observation process  result follows
arguments used proof theorem   
    control dependent rewards

many circumstances rewards may depend controls u 
example  controls may consume energy others may wish add penalty
term reward function order conserve energy  simplest way deal
define state expected reward r

   

r i    ey  i  eu   y   r u  i  
   

    

fibaxter   bartlett



redefine jfi terms r 

jfi    i    

lim e

 

n   

n
x
t  



xt fifi x 


   

fitr

 

 i

 

    

x    x            performance gradient becomes
r   r r    rr 

expectation trajectories

approximated

rfi     rp jfi   rr  








due fact jfi satisfies bellman equations      r replaced r  
gpomdp take account dependence r controls  fifth line
replaced

 
        r ut
  



r
ut      yt  
  xt   zt  
 
    


  

  

  

  

ut  
t  
straightforward extend proofs theorems        setting 

    parameter dependent rewards
possible modify gpomdp rewards depend directly   case 
fifth line gpomdp replaced

          r   xt  zt   rr   xt   t   
  

  

  

    

  

   

again  convergence approximation theorems carry through  provided rr   uniformly bounded  parameter dependent rewards considered glynn         marbach
tsitsiklis         baird moore         particular  baird moore        showed
suitable choices r   lead combination value policy search  vaps 
example  j   approximate value function  setting  

   

    

h

 
 
 
r   xt   xt    
  r xt     ffj    xt   j    xt    
r  xt   usual reward          discount factor  gives update seeks
 

 

 

minimize expected bellman error
n
x
i  

 

   i   r i   

n
x
j   

  

pij   j    j   j    i    

    

    

effect minimizing bellman error j     driving system
 via policy  states small bellman error  motivation behind approach
understood one considers j zero bellman error states  case greedy
policy derived j optimal  regardless actual policy parameterized 
expectation zt r   xt   xt   zero gradient computed gpomdp 
kind update known actor critic algorithm  barto et al          policy playing
role actor  value function playing role critic 

 

 

    use rewards r   xt   xt
analysis 

 

 

    depend current previous

   

state substantially alter

fip olicy g radient e stimation

    extensions infinite state  observation  control spaces
convergence proof algorithm   relied finite state  s    observation  y   control  u  
spaces  however  clear modification algorithm   applied immediately pomdps countably uncountably infinite   countable u  
changes pij u becomes kernel p x  x    u becomes density observations 
addition  appropriate interpretation r   applied uncountable u   specifically  u subset r n y  probability density function u u y 
density u  u subsets euclidean space  but finite set   theorem  
extended show estimates produced algorithm converge almost surely rfi  
fact  prove general result implies case densities subsets r n
well finite case theorem    allow u general spaces satisfying following
topological assumption   for definitions see  example   dudley         

  

 

 

  

   

   

assumption    control space u associated topology separable  hausdorff 
first countable  corresponding borel  algebra b generated topology 
 finite measure defined measurable space u   b   say reference measure
u  
similarly  observation space topology  borel  algebra  reference measure
satisfying conditions 

 

 

case theorem    u finite  associated reference measure
counting measure  u
rn rm   reference measure lebesgue measure 
assume distributions   absolutely continuous respect reference
measures  corresponding radon nikodym derivatives  probability masses finite case 
densities euclidean case  satisfy following assumption 

 

  

 

   

   

assumption    every     r k   probability measure   absolutely continuous respect reference measure u   every     probability measure
absolutely continuous respect reference measure  
let reference measure u   u   u         r k   k   f           k g 
derivatives

  d   y 
 u 
 k


  du   y 
 k

exist ratios

    

 



 u fifi
du  y
 u 
 

bounded b

  

 

assumptions  replace algorithm   radon nikodym derivative
respect reference measure u   case  following convergence
result  generalizes theorem    applies densities euclidean space u  

theorem    suppose control space u observation space satisfy assumption   let

reference measure control space u   consider algorithm  
rut    yt 
ut    yt  
   

fibaxter   bartlett

replaced

r d yt  ut    
 

 

   

d  yt  
ut

assumptions         algorithm  starting initial state
sequence                         satisfying





lim   rfi

t  

x 

generate

w p   

proof  see appendix b

   new results
since first version paper  extended gpomdp several new settings 
proved new properties algorithm  section briefly outline results 
    multiple agents

instead single agent generating actions according       suppose multiple agents
             na   parameter set distinct observation environment
yi   generate actions ui according policy ui  i   yi    agents receive reward signal r  xt    they may cooperating solve task  example  

gpomdp applied collective pomdp obtained concatenating
  nobserva
 
na   u
tions  controls 
parameters

single
vectors


 
 
 
 
 

u           u  


            na respectively  easy calculation shows gradient estimate generated
gpomdp collective case precisely obtained applying gpomdp


 
agent independently  concatenating results  is 
          na  
estimate produced gpomdp applied agent i  leads on line algorithm
agents adjust parameters independently without explicit communication 
yet collectively adjustments maximizing global average reward  similar observations context reinforce vaps  see peshkin et al          algorithm gives
biologically plausible synaptic weight update rule applied networks spiking neurons
neurons regarded independent agents  bartlett   baxter         shown
promise network routing application  tao  baxter    weaver        

 

 

 

 






    policies internal states
far considered purely reactive memoryless policies chosen control
function current observation  gpomdp easily extended cover case
policies depend finite histories observations yt   yt             yt k   general  optimal
control pomdps  policy must function entire observation history  fortunately 
observation history may summarized form belief state  the current distribution
states   updated based upon current observation  knowledge
sufficient optimal behaviour  smallwood   sondik        sondik         extension
gpomdp policies parameterized internal belief states described aberdeen baxter
        similar spirit extension vaps reinforce described meuleau et al 
       
   

fip olicy g radient e stimation

    higher order derivatives
generalized compute estimates second higher order derivatives
average reward  assuming exist   still single sample rpath underlying pomdp 
see second order derivatives  observe
q   x r x dx twicedifferentiable density q   x performance measure r x  

gpomdp

          
  
z
r      r x  rq q   x x  q   x  dx

   

 

 

r  denotes matrix second derivatives  hessian   verified

r q   x    r log q   x     r log q   x  
q   x 
 

 

 

    

log    

second term right hand side outer product r
q   x
 that is  matrix entries    i
q   x    j q   x    taking x sequence
states x    x            xt visits recurrent state parameterized markov chain  recall
 p
section         q   x
t   xt xt     combined      yields

log    
log    
  
  

 

r q   x     tx r pxtxt     
 

q   x  

 

tx 

 

pxt xt     

t  

rpxtxt       
p
  
 

xt xt  

t  

 t  
x

rpxtxt     

  

pxtxt     

t  

 the squared terms expression outer products   expression derive
gpomdp like algorithm computing biased estimate hessian r    involves
maintainingin addition usual eligibility trace zt second matrix trace updated follows 

  

zt  

  fizt   rp pxtxt      
 



xt xt  

rpxtxt       
p
   
 

xt xt  

     



time steps algorithm returns average far r xt zt zt  second term
outer product  computation higher order derivatives could used second order
gradient methods optimization policy parameters 
    bias variance bounds

  

  

theorem   provides bound bias rfi relative r applies underlying markov chain distinct eigenvalues  extended result arbitrary markov chains
 bartlett   baxter         however  extra generality comes price  since latter bound involves number states chain  whereas theorem   not  paper supplies
proof variance gpomdp scales  
    providing formal justification
interpretation terms bias variance trade off 

    

 

   conclusion
presented general algorithm  mcg  computing arbitrarily accurate approximations
gradient average reward parameterized markov chain  chains transition
matrix distinct eigenvalues  accuracy approximation shown controlled
   

fibaxter   bartlett

size subdominant eigenvalue j  j  showed algorithm could modified apply
partially observable markov decision processes controlled parameterized stochastic policies 
discrete continuous control  observation state spaces  gpomdp   finite
state case  proved convergence probability   algorithms 
briefly described extensions multi agent problems  policies internal state  estimating
higher order derivatives  generalizations bias result chains non distinct eigenvalues 
new variance result  many avenues research  continuous time results
follow extensions results presented here  mcg gpomdp algorithms
applied countably uncountably infinite state spaces  convergence results needed
cases 
companion paper  baxter et al          present experimental results showing rapid
convergence estimates generated gpomdp true gradient r   give on line
variants algorithms present paper  variants gradient ascent make use
estimates rfi   present experimental results showing effectiveness algorithms
variety problems  including three state mdp  nonlinear physical control problem 
call admission problem 
acknowledgements
work supported australian research council  benefited comments
several anonymous referees  research performed authors
research school information sciences engineering  australian national university 

appendix a  simple example policy degradation value function learning
approximate value function approaches reinforcement work minimizing form error
approximate value function true value function  long known
may necessarily lead improved policy performance new value function  include
appendix illustrates phenomenon occur simplest possible system 
two state mdp  provides geometric intuition phenomenon arises 
consider two state markov decision process  mdp  figure    two controls
u    u  corresponding transition probability matrices

p  u     

 
 
 
 

 

 
 
 
 



  p  u     

 

 
 
 

  

 
 
 
 



 

u  always takes system state probability     regardless starting state  and
therefore state probability      u  opposite  since state reward  
state reward   optimal policy always select action u    policy
stationary distribution states      
        infinite horizon discounted
value state
  discount value    

 

 

 

    

  
 

jff  i    e

             
     

 
x

fft r



xt fifi x 


   

 i

 

 

 

 

t  
expectation state sequences x    x    x          state transitions generated according p u    solving bellmans equations  jff r ffp u  jff   jff
jff   jff  
 ff
 ff
r
r   r   yields jff
jff
 
    ff 
    ff 

   
            

       
         

     

   

            

fip olicy g radient e stimation

r       

r       

 

 

figure    two state markov decsision process

 

      

now  suppose trying learn approximate value function j mdp  i e    j
w state   scalar feature   must dimensionality ensure
j really approximate   w   r parameter learnt  greedy policy obtained
j optimal  j must value state state   purposes illustration choose

 
  j
  j   w must negative 
temporal difference learning  or
  one popular techniques training
approximate value functions  sutton   barto         shown linear functions 
converges parameter w minimizing expected squared loss stationary
distribution  tsitsikilis   van roy        

  
    
 
 
 
         
               
td   
td   
 

w   argmin

w

 

 

 
x

i  

 w i  jff  i     

    

substituting previous expressions         jff optimal policy solving
  ff
w   yields w
  hence w   values       wrong
    ff 
sign  situation optimal policy implementable greedy policy based
approximate value function class  just choose w      yet
observing
optimal policy converge value function whose corresponding greedy policy implements
suboptimal policy 
geometrical illustration occurs shown figure    figure  points
graph
represent
p
p values states  scales state   state   axes weighted
respectively  way  squared euclidean distance graph
two points j j corresponds expectation stationary distribution squared
difference values 

 

 

     

 

   

hp




td   

   
 

   j     

p

   j    



hp

   j         j    
p

 





 

  e j  x   j  x  

 

value function shaded region  corresponding greedy policy optimal  since
value functions rank state   state    bold line represents set realizable
approximate value functions w   w
  solution      approximate value
function found projecting point corresponding true value function jff   jff
onto
line  illustrated figure
    projection suboptimal weighted
mean squared distance value function space take account policy boundary 

     

    
    

           

appendix b  proof theorem  
proof needs following topological lemma  definitions see  example   dudley       
pp        
   

fibaxter   bartlett

 

 

 

 

 

               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
 j      j     
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
 w        w       
               
               
               
               
               
               
               
               
               
               
legend
               
               
               
               
               
               
               
               
optimal policy 
               
               
               
               
               
               
approximate
               
               
               
               
value function 
               
               
               
               
               
               
               
               
               
               
               
               
 

 

 

 

 

  
  
  
  
  
  
  
  

 

 

figure    plot value function space two state system  note scale axis
weighted square root stationary probability corresponding state
optimal policy  solution found td    simply projection true
value function onto set approximate value functions 

 

 

lemma    let x  topological space hausdorff  separable  first countable 
let b borel  algebra generated   measurable space x  b sequence
s   s          b sets satisfies following conditions 

 

   si partition x  that is  x
empty intersection  

   x   x   fxg   b

 
 

 

  sfs     sig two distinct elements si

fs   si   x   g   fxg 

i  

 

proof  since x separable  countable dense subset
fx    x         g  since x firstcountable  xi ascountable neighbourhood base  ni   now  construct partitions
si using countable set n  
           define
i   ni follows  let s  x and 

 

 

    

si   fs   ni     si g   fs    x ni      si g  
 

 

   

fip olicy g radient e stimation

clearly  si measurable partition x   since x hausdorff  pair x  x  distinct
points x   pair disjoint open sets a  x   x    a    since
dense  pair s  s    s    a    also  n contains neighbourhoods ns
ns  ns ns  a    ns ns  disjoint  thus  sufficiently large i  x
x  fall distinct elements partition si   since true pair x  x    follows

 
 

fs   si   x   g fxg 

i  

reverse inclusion
trivial  measurability singletons fxg follows measuras
bility sx
fs   si   fxg g fact fxg x sx  

  

 

 

 

shall use lemma   together following result show approximate
expectations certain random variables using single sample path markov chain 

 

 

lemma    let x  b measurable space satisfying conditions lemma    let s    s         
suitable sequence partitions lemma  let probability measure defined
space  let f absolutely integrable function x   event   define

f  s    
x   x
almost x x  

k

r

f  

 s  

               let sk  x  unique element sk containing x 
lim f  sk  x     f  x  
k  

proof  clearly  signed finite measure defined

 e    

z

e

fd

absolutely continuous respect   equation      defines
derivative respect   derivative defined

    

f

radon nikodym

 sk  x  

 x    klim
 
    sk  x  


see  example   shilov   gurevich        section        radon nikodym theorem  dudley        theorem        p        two expressions equal a e     
proof   theorem     definitions 

rfi     rp jfi

 

n x
n
x
i   j   

 i rpij   jfi  j   

    

every   absolutely continuous respect reference measure   hence
j write
z z
d   y 
pij     
pij  u 
 u  d u   i  y  

u
   

fibaxter   bartlett

since depend
integral obtain

rpij     

d   y  d absolutely integrable 

z z

u

pij  u  r

d   y 
 u  d u   i  y  


avoid cluttering notation  shall use denote distribution
denote distribution   notation 

  

rpij     

differentiate

z z

u

pij

   y  u  



r
d 



now  let probability measure u generated   write     

rfi  

x

i j

 i jfi  j  

z

yu

pij

r
d 



using notation lemma    define

pij  s    

r

pij  

 s  

z

 
r s      s   rdd d 


measurable set

u   notice that  given i  j    
pij  s     pr xt     j jxt   i   y  u     


 


r
r s     e dd fififi xt   i   yt   ut      


let s    s          sequence partitions u lemma    let sk
element sk containing y  u   using lemma   

   

z

yu

pij

z
r
 




lim pij  sk  y  u   r  sk  y  u   d y  u 

yu k  

  klim
  

 y  u  denote

x z

 sk



pij  s   r s   d 

   

fip olicy g radient e stimation

used assumption   lebesgue dominated convergence theorem interchange
integral limit  hence 

rfi   klim
  

  klim
  

x x

i j  sk

x

i j s

 i  s  pij  s  jfi  j  r s  

pr xt   i pr  yt   ut      pr xt   j jxt   i   yt   ut      
  



e  j  t     jxt

  

  klim
  

x

i j s

 


r
  j   e dd fififi xt   i   yt  ut    


 

 


e i xt  s  yt  ut  j  xt  j  t      rdd  
  



probabilities expectations respect stationary distribution xt  
distributions yt   ut   now  random process inside expectation asymptotically stationary
ergodic  ergodic theorem   almost surely 

x tx
r
 
rfi   klim
lim
 xt  s  yt   ut  j  xt  j  t      dd  
     
 

  

i j s t  



easy see double limit exists order reversed 
tx

x
r
 
rfi   tlim
lim i xt  s  yt   ut  j  xt  j  t      dd
  
k  
 

 
  tlim
  

t  
tx 
t  

  

i j s

  yt  
r ut
d  yt   u



    j  t      
   

argument proof theorem   shows tails



r d  yt   u




d  y
t 


u





j  t      ignored

   
   
jr  xt  j uniformly bounded  follows     rp jfi w p    required 
references
aberdeen  d     baxter  j          policy gradient learning controllers internal state  tech 
rep   australian national university 
aleksandrov  v  m   sysoyev  v  i     shemeneva  v  v          stochastic optimaization  engineering cybernetics          
baird  l     moore  a          gradient descent general reinforcement learning  advances
neural information processing systems     mit press 
   

fibaxter   bartlett

bartlett  p  l     baxter  j          hebbian synaptic modifications spiking neurons learn 
tech  rep   research school information sciences engineering  australian national
university  http   csl anu edu au bartlett papers bartlettbaxter nov   ps gz 
bartlett  p  l     baxter  j          estimation approximation bounds gradient based reinforcement learning  journal computer systems sciences      invited paper  special
issue colt      
barto  a  g   sutton  r  s     anderson  c  w          neuronlike adaptive elements solve
difficult learning control problems  ieee transactions systems  man  cybernetics 
smc            
baxter  j   bartlett  p  l     weaver  l          experiments infinite horizon  policy gradient
estimation  journal artificial intelligence research  appear 
baxter  j   tridgell  a     weaver  l          learning play chess using temporal differences 
machine learning                
bertsekas  d  p     tsitsiklis  j  n          neuro dynamic programming  athena scientific 
bertsekas  d  p          dynamic programming optimal control  vol ii  athena scientific 
breiman  l          probability  addison wesley 
cao  x  r     wan  y  w          algorithms sensitivity analysis markov chains
potentials perturbation realization  ieee transactions control systems technology 
          
dudley  r  m          real analysis probability  wadsworth   brooks cole  belmont  california 
glynn  p  w          stochastic approximation monte carlo optimization  proceedings
     winter simulation conference  pp         
glynn  p  w          likelihood ratio gradient estimation stochastic systems  communications
acm           
glynn  p  w     lecuyer  p          likelihood ratio gradient estimation regenerative stochastic
recursions  advances applied probability                             
ho  y  c     cao  x  r          perturbation analysis discrete event dynamic systems  kluwer
academic  boston 
jaakkola  t   singh  s  p     jordan  m  i          reinforcement learning algorithm partially
observable markov decision problems  tesauro  g   touretzky  d     leen  t   eds   
advances neural information processing systems  vol     mit press  cambridge  ma 
kimura  h     kobayashi  s       a   analysis actor critic algorithms using eligibility traces 
reinforcement learning imperfect value functions  fifteenth international conference
machine learning  pp         
   

fip olicy g radient e stimation

kimura  h     kobayashi  s       b   reinforcement learning continuous action using stochastic gradient ascent  intelligent autonomous systems  ias     pp         
kimura  h   miyazaki  k     kobayashi  s          reinforcement learning pomdps
function approximation  fisher  d  h   ed    proceedings fourteenth international
conference machine learning  icml     pp         
kimura  h   yamamura  m     kobayashi  s          reinforcement learning stochastic hill
climbing discounted reward  proceedings twelfth international conference
machine learning  icml     pp         
konda  v  r     tsitsiklis  j  n          actor critic algorithms  neural information processing
systems       mit press 
lancaster  p     tismenetsky  m          theory matrices  academic press  san diego  ca 
marbach  p     tsitsiklis  j  n          simulation based optimization markov reward processes  tech  rep   mit 
meuleau  n   peshkin  l   kaelbling  l  p     kim  k  e          off policy policy search  tech 
rep   mit artificical intelligence laboratory 
meuleau  n   peshkin  l   kim  k  e     kaelbling  l  p          learning finite state controllers
partially observable environments  proceedings fifteenth international conference
uncertainty artificial intelligence 
peshkin  l   kim  k  e   meuleau  n     kaelbling  l  p          learning cooperate via policy
search  proceedings sixteenth international conference uncertainty artificial
intelligence 
reiman  m  i     weiss  a          sensitivity analysis via likelihood ratios  proceedings
     winter simulation conference 
reiman  m  i     weiss  a          sensitivity analysis simulations via likelihood ratios  operations research     
rubinstein  r  y          problems monte carlo optimization  ph d  thesis 
rubinstein  r  y          optimize complex stochastic systems single sample path
score function method  annals operations research             
rubinstein  r  y          decomposable score function estimators sensitivity analysis optimization queueing networks  annals operations research             
rubinstein  r  y     melamed  b          modern simulation modeling  wiley  new york 
rubinstein  r  y     shapiro  a          discrete event systems  wiley  new york 
samuel  a  l          studies machine learning using game checkers  ibm
journal research development            
   

fibaxter   bartlett

shilov  g  e     gurevich  b  l          integral  measure derivative  unified approach 
prentice hall  englewood cliffs  n j 
singh  s  p   jaakkola  t     jordan  m  i          learning without state estimation partially
observable markovian decision processes  proceedings eleventh international
conference machine learning 
singh  s     bertsekas  d          reinforcement learning dynamic channel allocation cellular telephone systems  advances neural information processing systems  proceedings
     conference  pp          mit press 
smallwood  r  d     sondik  e  j          optimal control partially observable markov
decision processes finite horizon  operations research               
sondik  e  j          optimal control partially observable markov decision processes
infinite horizon  discounted costs  operations research     
sutton  r  s     barto  a  g          reinforcement learning  introduction  mit press 
cambridge ma  isbn               
sutton  r  s   mcallester  d   singh  s     mansour  y          policy gradient methods
reinforcement learning function approximation  neural information processing
systems       mit press 
tao  n   baxter  j     weaver  l          multi agent  policy gradient approach network
routing  tech  rep   australian national university 
tesauro  g          practical issues temporal difference learning  machine learning        
    
tesauro  g          td gammon  self teaching backgammon program  achieves master level
play  neural computation            
tsitsikilis  j  n     van roy  b          analysis temporal difference learning function approximation  ieee transactions automatic control                
williams  r  j          simple statistical gradient following algorithms connectionist reinforcement learning  machine learning            
zhang  w     dietterich  t          reinforcement learning approach job shop scheduling 
proceedings fourteenth international joint conference artificial intelligence  pp 
          morgan kaufmann 

   


