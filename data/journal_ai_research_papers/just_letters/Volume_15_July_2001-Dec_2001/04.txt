journal of articial intelligence research                  

submitted        published      

an analysis of reduced error pruning
elomaa cs helsinki fi
matti kaariainen cs helsinki fi

tapio elomaa
matti kriinen
department of computer science
p  o  box     teollisuuskatu    
fin       university of helsinki  finland

abstract

top down induction of decision trees has been observed to suer from the inadequate
functioning of the pruning phase  in particular  it is known that the size of the resulting
tree grows linearly with the sample size  even though the accuracy of the tree does not
improve  reduced error pruning is an algorithm that has been used as a representative
technique in attempts to explain the problems of decision tree learning 
in this paper we present analyses of reduced error pruning in three dierent settings 
first we study the basic algorithmic properties of the method  properties that hold independent of the input decision tree and pruning examples  then we examine a situation that
intuitively should lead to the subtree under consideration to be replaced by a leaf node 
one in which the class label and attribute values of the pruning examples are independent
of each other  this analysis is conducted under two dierent assumptions  the general
analysis shows that the pruning probability of a node tting pure noise is bounded by a
function that decreases exponentially as the size of the tree grows  in a specic analysis
we assume that the examples are distributed uniformly to the tree  this assumption lets
us approximate the number of subtrees that are pruned because they do not receive any
pruning examples 
this paper claries the dierent variants of the reduced error pruning algorithm 
brings new insight to its algorithmic properties  analyses the algorithm with less imposed
assumptions than before  and includes the previously overlooked empty subtrees to the
analysis 
   introduction
decision tree learning is usually a two phase process  breiman  friedman  olshen    stone 
      quinlan        

training examples

first a tree reecting the given sample as faithfully as possible is

constructed  if no noise prevails  the accuracy of the tree is perfect on the

that were used to build the tree  in practice  however  the data tends to be noisy  which
may introduce contradicting examples to the training set 

overtted

necessarily be obtained even on the training set 
is

hence       accuracy cannot

in any case  the resulting decision tree

to the sample  in addition to the general trends of the data  it encodes the

pruned

peculiarities and particularities of the training data  which makes it a poor predictor of the
class label of future instances  in the second phase of induction  the decision tree is

in order to reduce its dependency on the training data  pruning aims at removing from the
tree those parts that are likely to only be due to the chance properties of the training set 
the problems of the two phased top down induction of decision trees are well known
and have been extensively reported  catlett        oates   jensen               the size

c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fielomaa   kriinen
of the tree grows linearly with the size of the training set  even though after a while no
accuracy is gained through the increased tree complexity  obviously  pruning is intended to
ght this eect  another defect is observed when the data contains no relevant attributes 
i e   when the class labels of the examples are independent of their attribute values  clearly 
a single node tree predicting the majority label of the examples should result in this case 
since no help can be obtained by querying the attribute values  in practice  though  often
large decision trees are built from such data 
many alternative pruning schemes exist  mingers      a  esposito  malerba    semeraro        frank        

pruning examples

they dier  e g   on whether a single pruned tree or a series of

pruned trees is produced  whether a separate set of

is used  which aspects

 classication error and tree complexity  are taken into account in pruning decisions  how
these aspects are determined  and whether a single scan through the tree suces or whether
iterative processing is required 

the basic pruning operation that is applied to the tree

is the replacement of an internal node together with the subtree rooted at it with a leaf 
also more elaborated tree restructuring operations are used by some pruning techniques

majority leaf
pruning of a tree

 quinlan               in this paper  the only pruning operation that is considered is the
replacement of a subtree by the
examples reaching it  hence  a

  i e   a leaf labeled by the majority class of the
is a subtree of the original tree with just

zero  one  or more internal nodes changed into leaves 

reduced error pruning

 subsequently

rep for short  was introduced by quinlan        in

the context of decision tree learning  it has subsequently been adapted to rule set learning as
well  pagallo   haussler        cohen        
in practical decision tree pruning

overprunes

rep

rep is one of the simplest pruning strategies 

is seldom used  because it has the disadvantage of

requiring a separate set of examples for pruning  moreover  it is considered too aggressive a
pruning strategy that

the decision tree  deleting relevant parts from it  quinlan 

      esposito et al          the need for a pruning set is often considered harmful because
of the scarceness of the data  however  in the data mining context the examples are often
abundant and setting a part of them aside for pruning purposes presents no problem 
despite its shortcomings

rep

is a baseline method to which the performance of other

pruning algorithms is compared  mingers      a  esposito  malerba    semeraro       
esposito et al         

it presents a good starting point for understanding the strengths

and weaknesses of the two phased decision tree learning and oers insight to decision tree
pruning 

rep has the advantage of producing the smallest pruning among those that are the

most accurate with respect to the pruning set  recently  oates and jensen        analyzed

rep in an attempt to explain why and when decision tree pruning fails to control the growth

of the tree  even though the data do not warrant the increased size  we approach the same
subject  but try to avoid restricting the analysis with unnecessary assumptions 

we also

consider an explanation for the unwarranted growth of the size of the decision tree 

rep in three dierent settings  first  we explore the basic algorep  which apply regardless of the distribution of examples presented

in this paper we analyze
rithmic properties of

to the learning algorithm  second  we study  in a probabilistic setting  the situation in which
the attribute values are independent of the classication of an example  even though this
pure noise tting situation is not expected to arise when the whole pruning set is considered 
it is encountered at lower levels of the tree  when all relevant attributes have already been
exhausted  we further assume that all subtrees receive at least one pruning example  so that

   

fian analysis of reduced error pruning
none of them can be directly pruned due to not receiving any examples  the class value is
also assigned at random to the pruning examples  in our third analysis it is assumed that
each pruning example has an equal chance to end up in any one of the subtrees of the tree
being pruned  this rather theoretical setting lets us take into account those subtrees that
do not receive any examples  they have been left without attention in earlier analyses 
the rest of this paper is organized as follows  the next section discusses the dierent
versions of the

rep

algorithm and xes the one that is analyzed subsequently  in section

  we review earlier analyses of

rep 

basic algorithmic properties of

section    then  in section    we carry out a probabilistic analysis of
any assumptions about the distribution of examples 

rep are examined in
rep  without making

we derive a bound for the pruning

probability of a tree which depends exponentially on the relation of the number of pruning
examples and the size of the tree  section   presents an analysis  which assumes that the
pruning examples distribute uniformly to the subtrees of the tree  this assumption lets us
sharpen the preceding analysis on certain aspects  however  the bounds of section   hold
with certainty  while those of section   are approximate results  further related research is
briey reviewed in section   and  nally  in section   we present the concluding remarks of
this study 

   reduced error pruning algorithm

rep

was never introduced algorithmically by quinlan         which is a source of much

confusion  even though

rep

is considered and appears to be a very simple  almost trivial 

algorithm for pruning  there are many dierent algorithms that go under the same name 
no consensus exists whether

rep is a bottom up algorithm

or an iterative method  neither

is it obvious whether the training set or pruning set is used to decide the labels of the leaves
that result from pruning 

    high level control
quinlan s        p          original description of

rep does not clearly specify the pruning

algorithm and leaves room for interpretation  it includes  e g   the following characterizations 
for every non leaf subtree

s

of

t

we examine the change in misclassications

over the test set that would occur if

s

were replaced by the best possible leaf 

if the new tree would give an equal or fewer number of errors and
subtree with the same property 

s is replaced by the leaf 

s contains no

the process continues

until any further replacements would increase the number of errors over the test
set 
      the nal tree is the most accurate subtree of the original tree with respect
to the test set and the smallest tree with that accuracy 
quinlan        p       also later continues to give the following description 
this method  pessimistic pruning  has two advantages  it is much faster than
either of the preceding methods  cost complexity and reduced error pruning 
since each subtree is examined at most once 

   

fielomaa   kriinen
on one hand this description requires the nodes to be processed in a bottom up manner 
since subtrees must be checked for the same property before pruning a node but  on the
other hand  the last quotation would indicate

rep

rep

to be an iterative method 

we take

to have the following single scan bottom up control strategy like in most other studies

 oates   jensen                    esposito et al               kearns   mansour        
nodes are pruned in a single bottom up sweep through the decision tree  pruning each node is considered as it is encountered 

the nodes are processed in

postorder 
by this order of node processing  any tree that is a candidate for pruning itself cannot
contain a subtree that could still be pruned without increasing the tree s error 
due to the ambiguity of
    a  mitchell        

rep s denition  a dierent version of rep also lives on  mingers 

it is probably due to mingers         interpretation of quinlan s

ambiguous denition 
nodes are pruned iteratively  always choosing the node whose removal most
increases the decision tree accuracy over the pruning set  the process continues
until further pruning is harmful 
however  this algorithm appears to be incorrect  esposito et al               have shown
that a tree produced by this algorithm does not meet the objective of being the most accurate
subtree with respect to the pruning set 

moreover  this algorithm overlooks the explicit

requirement of checking whether a subtree would lead to reduction of the classication
error 
other iterative algorithms could be induced from quinlan s original description  however  if the explicit requirement of checking whether a subtree could be pruned before pruning a supertree is obeyed  then these versions of

rep

will all reduce to the more ecient

bottom up algorithm 

    leaf labeling
another source of confusion in quinlan s        description of

rep

is that it is not clearly

specied how to choose the labels for the leaves that are introduced to the tree through

training

pruning  oates and jensen        interpreted that the intended algorithm would label the
new leaves according to the majority class of the

pruning

examples  but themselves analyzed

a version of the algorithm where the new leaves obtain as their labels the majority of the
examples  oates and jensen motivated their choice by the empirical observation

that in practice there is very little dierence between choosing the leaf labels in either way 
however  choosing the labels of pruned leaves according to the majority of pruning examples
will set such leaves into a dierent status than the original leaves  which have as their label
the majority class of training examples 

example

figure   shows a decision tree that will be pruned into a single leaf if the

training examples are used to label pruned leaves  a negative leaf replaces the root of the
tree and makes two mistakes on the pruning examples  while the original tree makes three
mistakes 

with this tree we can illustrate an important dierence in using training and

   

fian analysis of reduced error pruning





 



 



 

   

   

   

   

figure    a  part of a  decision tree  the labels inside the nodes denote the majority classes
of training examples arriving to these nodes  for leaves the numbers of pruning
examples from the two classes are also given 

x y

means that

x negative

and

y

positive instances reach the leaf 

pruning examples to label pruned leaves  using training examples and proceeding bottomup  observe that neither subtree is pruned  since the left one replaced with a negative leaf
would make two mistakes instead of the original one mistake  similarly  the right subtree
replaced with a positive leaf would result in an increased number of classication errors 
nevertheless  the root node  even though its subtrees have not been pruned  can still be
pruned 
when pruning examples are used to label pruned leaves  a node with two non trivial
subtrees cannot be pruned unless both its subtrees are collapsed into leaves 

the next

section will prove this  in the tree of figure   both subtrees would be collapsed into zeroerror leaves  however  in this case the root node will not be pruned 

a further possibility for labeling the leaf nodes would be to take both training and
pruning examples into account in deciding the label of a pruned leaf 

depending on the

relation of the numbers of training and pruning examples this strategy resembles one or the
other of the above described approaches  usually the training examples are more numerous
than the pruning examples  and will thus dominate  in practice it is impossible to discern
this labeling strategy from that of using the majority of training examples 

    empty subtrees
since

rep

uses dierent sets of examples to construct and to prune a decision tree  it is

possible that some parts of the tree do not receive any examples in the pruning phase  such
parts of the decision tree  naturally  can be replaced with a single leaf without changing
the number of classication errors that the tree makes on the pruning examples  in other
words  subtrees that do not obtain any pruning examples are always pruned  quinlan       
already noted that the parts of the original tree that correspond to rarer special cases  which
are not represented in the pruning set  may be excised 

   

fielomaa   kriinen
decisiontree rep  decisiontree t  examplearray s  
  for   i     to s length     classify  t  s i    
return prune  t     
void classify  decisiontree t  example e  
  t total    if   e label        t pos   
   update node counters
if    leaf t   
if   t test e         classify  t left  e   
else classify  t right  e     
int prune  decisiontree t      output classification error after pruning t
  if   leaf t   
if   t label        return t total   t pos 
else return t pos 
else
  error   prune  t left     prune  t right   
if   error   min  t pos  t total   t pos    
return error 
else
  replace t with a leaf 
if   t pos   t total   t pos  
  t label      return t total   t pos   
else
  t label      return t pos         
table    the

rep

algorithm  the algorithm rst classies the pruning examples in a top 

down pass using method
tree using method

prune 

classify

and then during a bottom up pass prunes the

intuitively  it is not clear which is the best founded strategy for handling

empty subtrees

 

those that do not receive any examples  on one hand they obtain support from the training
set  which usually is more numerous than the pruning set but  on the other hand  the fact
that no pruning example corresponds to these parts of the tree would justify drawing the
conclusion that these parts of the decision tree were built by chance properties of the training
data  in

rep 

consistently with preferring smaller prunings also otherwise  the latter view

is adopted 
the problem of empty subtrees is connected to the problem of
learning algorithms  holte  acker    porter        
number of the training examples 

small disjuncts

in machine

a small disjunct covers only a small

collectively the small disjuncts are responsible for a

small number of classication decisions  but they accumulate most of the error of the whole
concept  nevertheless  small disjuncts cannot be eliminated altogether  without adversely
aecting other disjuncts in the concept 

   

fian analysis of reduced error pruning
    the analyzed pruning algorithm
let us briey reiterate the details of the

rep algorithm that is analyzed subsequently 

as al 

ready stated  the control strategy of the algorithm is the single sweep bottom up processing 
first  a top down traversal drives the pruning examples through the tree to the appropriate
leaves  the counters of the nodes en route are updated  second  during a bottom up traversal the pruning operations indicated by the classication errors are executed 

the errors

can be determined on the basis of the node counter values  in the bottom up traversal each
node is visited only once  the pruned leaves are labeled by the majority of the pruning set
 see table    

   previous work
pruning of decision trees has recently received a lot of analytical attention  existing pruning
methods have been analyzed  esposito et al               oates   jensen             
      and new analytically founded pruning techniques have been developed  helmbold  
schapire        pereira   singer        mansour        kearns   mansour        

also

many empirical comparisons of pruning have appeared  mingers      a  malerba  esposito 
  semeraro        frank         in this section we review earlier work that concerns the

rep

algorithm  further related research is considered in section   

esposito et al         viewed the
search process in the state space 

rep

algorithm  among other pruning methods  as a

in addition to noting that the iterative version of

rep

cannot produce the optimal result required by quinlan         they also observed that even
though
the tree

rep is a linear time algorithm in the size of the tree  with respect to the height of
rep requires exponential time in the worst case  in their subsequent comparative

analysis esposito et al         sketched a proof for quinlan s        claim that the pruning
produced by

rep

tree 
the bias of

is the smallest among the most accurate prunings of the given decision

rep was briey examined by oates and jensen              

they observed

rl   of the best majority leaf that could replace a subtree t only depends on
 the class distribution of   the examples that reach the root n of t   in other words  the tree
structure above t and n decides the error rl   let rt denote the error of the subtree t at
the moment when the pruning sweep reaches n   i e   when some pruning may already have
taken place in t   all pruning operations performed in t have led either rt to decrease from
the initial situation or to stay unchanged  in any case  pruning that has taken place in t
potentially decreases rt   but does not aect rl   hence  the probability that rt   rl  i e  
that t will not be pruned  increases through pruning in t   this error propagation bias
that the error 

is inherent to

rep 

oates and jensen              conjecture that the larger the original

tree and the smaller the pruning set  the larger this eect  because a large tree provides
more pruning opportunities and the high variance of a small pruning set oers more random
chances for

rl  rt  

subsequently we study some of these eects exactly 

in a follow up study oates and jensen        used

rep

as a vehicle for explaining the

problems that have been observed in the pruning phase of top down induction of decision
trees  they analyzed

rep in a situation in which

the decision node under consideration ts

noise  i e   when the class of the examples is independent of the value of the attribute tested
in the node at hand  and built a statistical model of

   

rep

in this situation  it indicates 

fielomaa   kriinen
consistently with their earlier considerations  that even though the probability of pruning a
node that ts noise prior to pruning beneath it is close to    pruning that occurs beneath the
node reduces its pruning probability close to    in particular  this model shows that if even
one descendant of node

n

at depth

d is not pruned  then n

will not be pruned  assuming

d      the consequence of this result is that increasing depth
d leads to an exponential decrease of the node s pruning probability 
there are no leaves until depth

the rst part of oates and jensen s        analysis is easy to comprehend  but its significance is uncertain  because this situation does not rise in any bottom up pruning strategy 
the statistical model is based on the assumption that the number 

n  of pruning instances

that pass through the node under consideration is large  in which case  independence assumptions prevailing  the errors committed by the node can be approximated by the normal
distribution  the expected error of the original tree is the mean of the distribution  while  if
pruned to a leaf  the tree would misclassify a proportion of the

n examples that corresponds

to that of the minority class  oates and jensen show that the latter number is always less
than the mean of the standard distribution of errors  hence  the probability of pruning is
over     and approaches   as

n grows 

in the second part of the analysis  in considering the pruning probability of a node

n

after pruning has taken place beneath it  oates and jensen assume that the proportion of
positive examples in any descendant of
assuming further that

n

n

at depth

d is the same as in n  

in this setting 

d also have a
d are pruned  they are

has a positive majority  all its descendants at level

positive majority  it directly follows that if all descendants at level

all replaced by a positive leaf  hence  the function represented by this pruning is identically
positive  the majority leaf that would replace
smaller than the above pruning  therefore 

n

also represents the same function and is

rep will choose the single leaf pruning 

on the

n at depth d are not pruned  then the
n   in which these subtrees are maintained and all other nodes

other hand  if one or more of the descendants of
pruning of the tree rooted at
at level

d are

pruned into positive leaves  is more accurate than the majority leaf  in this

case the tree will not be pruned 
oates and jensen        also assume that starting from any node at level

d the proba 

bility of routing an example to a positive leaf is the same  in the following analyses we try
to rid all unnecessary assumptions  the same results can be obtained without any knowledge
of the example distribution 

   basic properties of

rep

before going to the detailed probabilistic analysis of the
of its basic algorithmic properties 

rep

algorithm  we examine some

throughout this paper we review the binary case for

simplicity  the results  however  also apply with many valued attributes and several classes 
now that the processing control of the

rep

algorithm has been settled  we can actually

prove quinlan s        claim of the optimality of the pruning produced by

rep 

observe

that the following result holds true independent of the leaf labeling strategy 

theorem   applying rep with a set of pruning examples  s   to a decision tree t produces
t   a pruning of t such that it is the smallest of those prunings of t that have minimal
error with respect to the example set s 
   

fian analysis of reduced error pruning
proof we prove the claim by induction over the size of the tree  observe that a decision
t is a full binary tree  which has  l t     nodes  where l t   is the number of leaves

tree

in the tree 

base case

 

l t       

if

then the original tree

t

consists of a single leaf node 

t

is

the only possible pruning of itself  thus  it is  trivially  also the smallest among the most
accurate prunings of

t 

inductive hypothesis
inductive step l t     k

  the claim holds when

 

  let

n

right subtree  respectively  subtrees
the pruning decision for

n

l t     k 

t 

the left and the

must have strictly less than

when

be the root of the tree and

t 

and

t 

prunings of these trees 

t  

and

t    

k leaves 

by the inductive hypothesis 

are the smallest possible among the most accurate

 i   accuracy
n

and

is taken  then  by the bottom up recursive control strategy of

rep  t  and t  have already been processed by the algorithm 
the subtrees after pruning 

t 

  the pruning decision for the node

n consists of choosing whether to collapse

and the tree rooted at it into a majority leaf  or whether to maintain the whole

tree  if both alternatives make the same number of errors  then

n

is collapsed and the

original accuracy with respect to the pruning set is retained  otherwise  by the

rep

algorithm  the pruning decision is based on which of the resulting trees would make

s   hence  whichever choice is made  the
t   will make the smaller number of errors with respect to s  
  
let us now assume that a pruning t of t makes even less errors with respect to s
 
  
  
  
than t   then t must consist of the root n and two subtrees t  and t    because the
 
  
majority leaf cannot be more accurate than t   since t is a more accurate pruning of
t than t     it must be that either t    is a more accurate pruning of t  than t   or t    is
 
a more accurate pruning of t  than t    by the inductive hypothesis both possibilities
 
are false  therefore  t is the most accurate pruning of t  

less errors with respect to the pruning set
resulting tree

 ii   size
 
t

  to see that the chosen alternative is also as small as possible  rst assume that

consists of a single leaf  such a tree is the smallest pruning of

claim follows 

t   and t    

 
otherwise  t consists of the root node n

t   and in this case the

and the two pruned subtrees

since this tree was not collapsed  the tree must be more accurate than the

tree consisting of a single majority leaf  now assume that there exists a pruning

t

t     but smaller  because the majority leaf is less accurate
 



than t   t must consist of the root node n and two subtrees t  and t    then  either
t  is a smaller pruning of t  than t     but as accurate  or t  is a smaller pruning of
t  than t     but as accurate  both cases contradict the inductive hypothesis  hence 
t   is the smallest among the most accurate prunings of t  
of

t

that is as accurate as

thus  in any case  the claim follows for

t 

 

we consider next the situation in an internal node of the tree  when the bottom up
pruning sweep reaches the node 

from now on we are committed to leaf labeling by the

majority of the pruning examples 

   

fielomaa   kriinen

an internal node  which prior to pruning had no leaves as its children  will not
be pruned by rep if it has a non trivial subtree when the bottom up pruning sweep reaches
it 
theorem  
proof

for an internal node

n

we have two possible cases in which it has non trivial

subtrees  either both its subtrees are non trivial  non leaf   or one of them is trivial  let us
review these cases 
let

rt

denote the error of  sub tree

t

with respect to the part of the pruning set that

t   by rl we denote the misclassication rate of the majority leaf l that
t   if t was chosen to be pruned 

reaches the root of
would replace

case i  let the two subtrees of

t   t  and t    be non trivial 

hence  both of them have been

rt    rl  and rt    rl   
t  and t    respectively 
if pruned  because rt   rt    rt    it must be that rt   rl    rl   
if t  and t  have the same the majority class  then it is also the majority class of t  
then rl   rl    rl    where l is the majority leaf corresponding to t   otherwise 
rl  rl    rl    in any case  rl  rl    rl    combining this with the fact that
rt   rl    rl  means that rt   rl   hence  t is not pruned 

retained when the pruning sweep has passed them  thus 

l 

where

case ii  let

t

and

l 

are the majority leaves that would replace

have one trivial subtree  which was produced by pruning  and one non 

t  is non trivial and l  is
t  in the pruning process  then  rt    rl    hence 
we have that rt   rt    rl    rl    rl   
in the same way as in the case i  we can deduce that rl  rl    rl    therefore 
rt   rl and t will be retained in the pruned tree 

trivial subtree  we assume  without loss of generality  that
a majority leaf which has replaced

t

cannot be pruned in either case  and the pruning process can be stopped on the branch

t

containing
if node

unless an original leaf appears along the path from the root to

n

 

t 

has an original leaf  then it may be pruned even if the other subtree of

is non trivial  also when

n

n

has two trivial subtrees  it may be pruned  whether pruning

takes place depends on the class distribution of examples reaching

n

and its subtrees 

in the analysis of oates and jensen        it was shown that the prerequisite for pruning
a node

n

from the tree is that all its descendants at depth

d

have been pruned 

depth just above the rst  original  leaf in the subtree rooted at
result to this situation  we can corroborate their nding that
more of its descendants at depth

d are retained 

n

n 

d

is the

if we apply the above

will not be pruned if one or

applying theorem   recursively gives the

result 

a tree t rooted at node n will be retained by rep if one or more of the
descendants of n at depth d are not pruned 
corollary  

to avoid the analysis being restricted by the leaf globally closest to the root  we need to

fringe

be able to consider the set of leaves closest to the root on all branches of the tree  let us
dene that the

of a decision tree contains any node that prior to pruning had a leaf

   

fian analysis of reduced error pruning

figure    the fringe  black and gray nodes   interior  white nodes   and the safe nodes
 black ones  of a decision tree  the triangles denote non trivial subtrees 

as its child  furthermore  any node that is in a subtree rooted at a node belonging to the

interior

safe nodes

fringe of the tree is also in the fringe  those nodes not belonging to the fringe make up the
of the tree 

themselves belong to the fringe of the tree  but have their

parent in the interior of the tree  see figure     because the fringe of a decision tree is closed
downwards  the safe nodes of a tree correspond to the leaves of some pruning of it  observe
also that along the path from the root to a safe node there are no leaves  therefore  if the
pruning process ever reaches a safe node  theorem   applies on the corresponding branch
from there on 
if the decision tree under consideration will be pruned into a single majority leaf  safe
nodes also need to be turned into leaves at some point  not necessarily simultaneously  if
the pruning sweep continues to the safe nodes  from then on the question whether a node is
pruned is settled solely on the basis of whether all nodes on the path to the root have the
same majority class  the pruning of the whole tree can be characterized as below 
let

t

be the tree to be pruned and

s the set of pruning examples  js j   n 

we assume 

without loss of generality  that at least half of the pruning examples are positive  let
the proportion of positive examples in

s   p      

if

t

p be

was to be replaced by a majority

leaf  that leaf would have a positive class label  under these assumptions we can prove the
following 

a tree t will be pruned into a single leaf if and only if
 all subtrees rooted at the safe nodes of t are pruned and
 at least as many positive as negative pruning examples reach each safe node in t  

theorem  

   

fielomaa   kriinen
proof

to begin we show that the two conditions are necessary for the pruning of

we show that if the former condition is not fullled  then
leaf  second  we prove that neither will
latter not 
hold  then

t

t

t 

first 

cannot be pruned into a single

be pruned if the former condition holds  but the

third  we show the suciency of the conditions  i e   prove that if they both

t

will be pruned into a single leaf 

t

 i   let us rst assume that in

there is a safe node

the denition of a safe node  the parent
therefore  by theorem   
neither will the root of

t

p

p

of

n

n

such that it will not be pruned  by

originally had no leaves as its children 

will not be pruned 

it is easy to see  inductively  that

be pruned 

 ii   let us then assume that all subtrees rooted at safe nodes get pruned and that there are
one or more safe nodes in

t

into which more negative than positive pruning examples

fall  observe that all safe nodes cannot be such  let us now consider the pruning of

t

in which the leaves are situated in place of the safe nodes  the leaves receive the same
examples as the original safe nodes 

because safe nodes are internal nodes  in

rep

the corresponding pruned leaves are labeled by the majority of the pruning examples 
in particular  the safe nodes that receive more negative than positive examples are
replaced by negative leaves  all other leaves are labeled positive  this pruning of the
original tree is more accurate than the majority leaf  hence  by theorem   
not prune

t

rep

will

into a single leaf tree 

 iii   let us now assume that all subtrees rooted at the safe nodes of

t

are pruned and that

at least as many positive as negative pruning examples reach each safe node  then
all interior nodes must also have a majority of positive pruning examples  otherwise 

t that has more negative than positive examples  thus 
n has a majority of negative examples  carrying the
induction all the way to the safe nodes shows that no such node n can exist in t  
hence  all interior prunings of t represent the same function  identically positive  and
all of them have the same error with respect to s   the majority leaf is the unique 

there is an interior node

n

in

at least one of the children of

smallest of these prunings and will  by theorem    be chosen 

 
   a probabilistic analysis of

rep

let us now turn our attention to the question of what the prerequisites for pruning a decision
tree

t

into a single majority leaf are  since  by theorem   

rep

produces a pruning of

t

which is the most accurate with respect to the pruning set and such that it is as small as
possible  to show that

t

does not reduce to a single leaf it suces to nd its pruning that

has a better prediction accuracy on the pruning examples than the majority leaf has 
in the following the class of an example is assumed to be independent of its attribute
values  obviously  if in a decision tree there is a node where this assumption holds for the
examples arriving to it  we would like the pruning algorithm to turn it into a majority leaf 
we do not make any assumptions about the decision tree  however  similar to the analysis
of oates and jensen         for the obtained bounds to be tight  the shortest path from the
root of the tree to a leaf should not be too short 

   

fian analysis of reduced error pruning
    probability theoretical preliminaries
let us recall some basic probabilistic concepts and results that are used subsequently  we
denote the probability of an event

and p

x
x  b  n  p   if

 integer valued  random variable
  denoted by

e

by

prfe g

ee  

binomially distributed with parameters n
and its expectation by

is said to be

a discrete

 

n k
prf x   k g  
p    p n k   k                 n 
k
if

x  b  n  p   then its expected
p value or mean is ex      np  variance varx   np   p  
   np   p  

indicator variable

and standard deviation
an

   an indicator variable
if

a            an

is is a discrete random variable that takes on only the values   and

i

is used to denote the occurrence or non occurrence of an event 

pn

are independent events with

x 

ia

prfai g   p and ia            ia

n

are the respective

i  
bernoulli
p
density function fx   in         
x
fx  x    prf x   x g
cumulative
distribution
function
f
 
in
 
      
x
p
indicator variables  then

ia

i

is called a

i

is binomially distributed with parameters

random variable with parameter

the

for a discrete random variable

 

n and p 

 

the

is dened as

fx  y    prf x  y g   xy fx  x  
let x  b  n  p  be a random variable with mean    np and standard
p
   np   p   the normalized random variable corresponding to x is

for

x

is

dened as

x

xe  





deviation

 

by the central limit theorem we can approximate the cumulative distribution function

e
of x

by the

normal gaussian distribution
or

n

fxe  y    pr xe  y

o

  y  

 is the cumulative distribution function of the bell curve density function e
respectively  we can apply the
able

x

normal approximation

fx  y    prf x  y g   fxe

x      

fxe

p

   

to the corresponding random vari 

y 
y 


 



    bounding the pruning probability of a tree
now  the pruning set is considered to be a sample from a distribution in which the class
attribute is independent of the other attributes 

we assume that the class attribute is

 p  distribution  i e   the class is positive with probability
p and negative with probability   p  we assume that p       
distributed according to bernoulli

in the following we will analyze the situation in which the subtrees rooted at safe nodes
have already been pruned into leaves  we bound the pruning probability of the tree starting
from this initial conguration 

since the bottom up pruning may already have come to

a halt before that situation  the following results actually give too high a probability for
pruning  hence  the following upper bounds are not as tight as possible 

   

fielomaa   kriinen
we consider pruning a decision tree by

rep

as a trial whose result is decided by the set

all

of pruning examples  by theorem   we can approximate the probability that a tree will
be pruned into a majority leaf by approximating the probability that

safe nodes get a

positive majority or a negative majority  the latter alternative is not very probable under
the assumption

p      

it is safe to assume that it never happens 

we can consider sampling the pruning examples in two phases  first the attribute values
are assigned 

this decides the leaf into which the example falls 

in the second phase we

independently assign the class label for the example 

z  t     fz            zk g and let the number of examples in
the pruning set s be js j   n  the number of pruning examples falling to a safe node zi is
pk
denoted by ni  
i   ni   n  for the time being we assume that ni     for all i  the number
of positive examples falling to safe node zi is the sum of independent bernoulli variables
and  thus  it is binomially distributed with parameters ni and p  respectively  the number
of negative pruning examples in safe node zi is xi  b  ni    
p   the probability that there
is a majority of negative examples in safe node zi is prf xi   ni    g  we can bound this
let the safe nodes of tree

t

be

probability from below by using the following inequality  slud        

lemma    slud s inequality 
for m   q   h  mq 

let x  b m  q  be a random variable with q       then
 

h mq
 
prf x  h g     p
mq   q 
p      and the random variable corresponding to the number of negative examples
zi is xi  b  ni    p   the rst condition of slud s inequality holds  furthermore 
to see that condition m  
q   h  mq holds in safe node zi substitute h   ni     m   ni  
and q    
p to obtain ni p  ni    ni   p   thus 
since

in safe node



pr xi  
as

ni 

 

 

 

   ni   p 
     p p     ni  
    nip
ni p   p 
ni p   p 

   

ni   the number of pruning instances reaching safe node zi   grows  then the standard

normal distribution term in the above bound also grows  hence  the bound on the probability
that the majority of the pruning examples reaching

zi

is negative is the smaller the more

pruning examples reach it  the probability of a negative majority also reduces through the
growing probability of positive class for an example 

p 

these both are also reected in the

pruning probabilities of the whole tree 
we can now roughly approximate the probability that
majority leaf as follows  by theorem   
node in
are

t

t

t

will be pruned into a single

will be pruned into a leaf if and only if each safe

receives a majority of positive examples  because

t

has

k

safe nodes and there

n pruning examples  then according to the pigeon hole principle at least half of the safe
r    n k examples  each safe node zi with ni  r examples has  by

nodes receive at most

inequality    a negative majority at least with probability

 

   p p     r  
rp   p 
   

fian analysis of reduced error pruning
observe that inequality   also holds when

ni   r  becausepthe cumulative

distribution

 is an increasing function  the argument ni  p       ni p   p  canpbe rewritten
p
as
ni cp   where cp ispa positive constant depending on the value of p  since   ni cp   grows
as ni grows   
  ni cp   grows with decreasing ni   hence  the lower bound of inequality
  also applies for values     ni   r  

function

thus  the probability that the half of the safe nodes that receive at most

r

examples

have a positive majority is at most

 p p     r
rp   p 

  k  

 

   

this is an upper bound for the probability that the whole tree

t

will be pruned into a single

leaf  the only distribution assumption that was made to reach the result is that

p      

order to obtain tighter bounds  one has to make assumptions about the shape of the tree
and the distribution of examples 
the bound of equation   depends on the size of the decision tree  reected by

n

p

in

t

k   the

number     and the class distribution     of the pruning examples  keeping other parameters
constant and letting

k

grow reduces the pruning probability exponentially  if the number

of pruning examples grows in the same proportion so that

r    n k

stays constant  the

pruning probability still falls exponentially  class distribution of the pruning examples also
aects the pruning probability which is the smaller  the closer

p is to value    

    implications of the analysis
it has been empirically observed that the size of the decision tree grows linearly with the
training set size  even when the trees are pruned  catlett        oates   jensen       
       the above analysis gives us a possibility to explain this behavior  however  let us
rst prove that when there is no correlation between the attribute values and the class label
of an example  the size of the tree that perfectly ts the training data depends linearly on
the size of the sample 
our setting is as simple as can be  we only have one real valued attribute
attribute

y  whose value is independent

of that of

x 

as before 

y

x and the class

has two possible values 

  and    the tree is built using binary splits of a numerical value range  i e   propositions
of type 

x   r

are assigned to the internal leaves of the tree 

in this analysis duplicate

instances occur with probability   

let the training examples  x  y  be drawn from a distribution  where x is uniformly distributed in the range        and y obtains value    independent of x  with probability
p  and value   with probability   p  then the expected size of the decision tree that ts the
data is linear in the size of the sample 
theorem  

s   h x    y              xt   yt  i be a sample of the above described distribution  we
xi    xj   when i    j   because the probability of the complement event is   
let us  further  assume that the examples of s have been indexed so that x    x            xt  
let ai be the indicator variable for the event that instances i and i     have dierent class
labels  i e   yi    yi        i  t    then eai   prf ai     g   p   p     p p    p   p  
proof

let

may assume that

   

fielomaa   kriinen
yi     has probability p  at the same time the event yi       has
  p  and vice versa  now the number of class alternations is a   pti    ai and

because when the event
probability

its expectation is

ea  
let

t

t  
x
i  

eai  

t  
x
i  

 p   p     p   p 

t  
x
i  

      t   p   p  

be a decision tree that has been grown on the sample

continued until the training error is    each leaf in

 a  b  in        

if

yi    yy    

then

xi

and

xi  

t

s 

   

the growing has been

corresponds to a half open interval

must fall into dierent leaves of

t 

because

t   thus  the upper boundary b of
xi falls in must have a value less than xi    

otherwise one or the other example is falsely classied by
the interval corresponding to the leaf into which

repetitively applying this observation when scanning through the examples from left to

t must at least have one leaf for x  and one leaf for each class alternation 
a     leaves in total  by using equation   we see that the expected number of leaves

right  we see that
i e  
in

t

is

ea         t   p   p      
in particular  this is linear in the size of the sample s   js j   t 

 

the above theorem only concerns zero training error trees built in the rst phase of
decision tree induction  the empirical observations of catlett        and oates and jensen
              however  concern decision trees that have been pruned in the second phase of
induction  we come back to the topic of pruned trees shortly 
consider how

rep is used in practice 

there is some amount of  classied  data available

from the application domain  let there be a total of

t examples available  some part ff of
  ff of it is reserved as the

the data is used for tree growing and the remaining portion
separate pruning set 

    ff     

quite a common practice is to use two thirds of the data

for growing and one third for pruning or nine tenths for growing and one tenth for pruning
when  ten fold  cross validation is used  in the decision tree construction phase the tree is
tted to the

fft examples as perfectly as possible 

if we hypothesize that the previous result

holds for noisy real world data sets  which by empirical evidence would appear to be the
case  and that the number of safe nodes also grows linearly with the number of leaves  then
the tree grown will contain

t

safe nodes  where

      since the pruning set size also is
r    n k stays constant in this setting 

a linear fraction of the training set size  the ratio

hence  by equation    the growing data set size forces the pruning probability to zero  even
quite fast  because the reduction in the probability is exponential 

    limitations of the analysis
empty subtrees  which do not receive any pruning examples  were left without attention
above  we assumed that

ni    

for each

i 

empty subtrees  however  decisively aect the

analysis  they are automatically pruned away  unfortunately  one cannot derive a non trivial
upper bound for the number of empty subtrees  in the worst case all pruning examples are
routed to the same safe node  which leaves

k   empty safe nodes to the tree 

subsequently

we review the case where the examples are distributed uniformly to the safe nodes  then
better approximations can be obtained 

   

fian analysis of reduced error pruning
even though we assume that each pruning example is positive with a higher probability
than     there are no guarantees that the majority of all examples is positive 

however 

the probability that the majority of all examples changes is very small  even negligible  by
cherno  s inequality  cherno        hagerup   rb        when the number of pruning

n  is high and p is not extremely close to one half 
prf x  h g  but above we used it to bound
the probability prf x   h g  some continuity correction could be used to compensate this 
examples 

slud s inequality bounds the probability

in practice  the inexactness does not make any dierence 
even though it would appear that the number of safe nodes increases in the same proportion as that of leaves when the size of the training set grows  we have not proved this
result  theorem   essentially uses leaf nodes  and does not lend itself to modication  where
safe nodes could be substituted in place of leaves 
the relation between the number of safe nodes and leaves in a decision tree depends on
the shape of the tree  hence  the splitting criterion that was used in tree growing decisively
aects this relation  some splitting criteria aim at keeping the produced split as balanced as
possible  while others aim at separating small class coherent subsets from the data  quinlan 
      mingers      b   for example  the common entropy based criteria have a bias that
favors balanced splits  breiman         using a balanced splitting criterion would seem to
imply that the number of safe nodes in a tree depends linearly on the number of leaves in
the tree 

in that case the above reasoning would explain the empirically observed linear

growth of pruned decision trees 

   pruning probability under uniform distribution
we now assume that all
of the

k

n

pruning examples have an equal probability to end up in each

safe nodes  i e   a pruning example falls to the safe node

zi

with probability

  k 

contrary to the normal uniform distribution assumption analysis  for our analysis this is not
the best case  here the best distribution of examples into safe nodes would have one pruning
example in each of the safe nodes except one  into which all remaining pruning instances
would gather 

nevertheless  the uniformity lets us sharpen the general approximation by

using standard techniques 

n k  let us calculate
cn k examples  where c is an
for the event safe node zi receives at

the expected number of examples falling into any safe node is
the expected number of those safe nodes that receive at most

qpi be the indicator
k q is the number of those safe nodes that receive less
i   i
pk
than
by the linearity of expectation eq  
i   eqi   keq    in which
the last equality follows from the fact that the qi  s are identically distributed 
let y  be the number of examples reaching safe node z    because each of the n examples reaches z  with probability   k independent of the other examples  y  is binomially
distributed with parameters n and   k   clearly eq    prf y   cn k g  we can approxiarbitrary positive constant  let
most

cn k examples 
cn k examples 

then

q 

mate the last probability by the normal approximation  from which we obtain

 
 


cn k
n k
 
c
  
n k
cn
  pn    k       k     pn k     k   
pr y  
k
   

fielomaa   kriinen
hence  by the above observation 

 

 c   n k  
eq   keq   k p
n k     k 
t

   

we now use approximation   to determine the probability that the whole decision tree
will be pruned into a single leaf  let

denote by

t

p

be a random variable that represent the number

cn k examples and at least one example  if we
r the number of empty safe nodes  we have p   q r  hence  ep   e q r   

of those safe nodes in

that receive at most

eq er 

the following result  kamath  motwani  palem    spirakis        motwani   raghavan 
      lets us approximate the number of empty safe nodes when

theorem  

bins  then

n  k 

let z be the number of empty bins when m balls are thrown randomly into h


   ez   h  

and for      

prf jz

  m  he

h

j   g    exp

m h

 

   h     
 
h   

by this result the expected number of empty safe nodes is approximately
number is small when

ke

n k  

this

k is relatively small compared to n 
eq  equation    and using the pre 

substituting the above obtained approximation for
vious result  we get

 c   n k
ep   eq er  k  p
n k     k 

 

e

n k

 

 

applying slud s inequality we can  as before  bound from above the probability that
the majority class does not change in a safe node that receives
since there are

p

cn k

pruning examples 

such safe nodes and the class distribution of examples within them is

independent  the event majority class does not change in any safe node that receives at
least one and at most

cn k examples

has the upper bound

 p p    r
rp   p 
where

r   cn k 

replacing

p

  p

 

   

with its expected value in this equation we have an approxi 

mation for the pruning probability  this approximation is valid if
from its expected value  we consider the deviation of

p

p

does not deviate a lot

from its expected value below 

the above upper bound for the pruning probability is similar to the upper bound that
was obtained without any assumptions about the distribution of the examples  however  the
earlier constant   has been replaced by a new  controllable parameter
are now explicitly taken into account  if

c  and empty subtrees

c is chosen suitably  this upper bound is more strict

than the one obtained in the general case 

   

fian analysis of reduced error pruning

upper bound for the pruning probability
   
    

 

   

 

   
   
   

   
 

figure    the eect of parameters

p

   

   

c

   

p and c on the upper bound of the pruning probability

of a tree with     safe nodes when     pruning examples are used  the curves
depicting the      and     upper bounds are also shown 

    an illustration of the upper bound
figure   plots the upper bound of the pruning probability of a tree with     safe nodes when
    pruning examples are used  the value of the parameter

c varies from   to   and p varies

from     to    we can observe that the surface corresponding to the upper bound stays very
close to   when the class distribution is not too skewed and when the parameter

c does not

have a very small value  when the probability of an example having a positive class label

c approaches    the upper bound climbs very steeply  at least
parameter c this is due to the inexactness of the approximation on the

hits value      or the value of
on the part of the
extreme values 
when the probability

p

that an example has a positive class approaches    the error

committed by a single positive leaf falls to    hence  the accuracy of a non trivial pruning
has to be better  the closer

p is to   for it to beat the majority leaf 

intuitively  the probability

that such a pruning exists  i e   that the root node is not pruned  should drop to zero as

p increases 

the bound reects this intuition 

when the value of parameter

c falls close to    the safe nodes that are taken into account

in the upper bound only receive very few pruning examples  the number of such nodes is

   

fielomaa   kriinen
small  on the other hand  when

c

is increased  the number of nodes under consideration

grows together with the upper limit on the number of examples reaching each single one of
them  thus  both small and large values of
the value of

c is somewhere

c

yield loose bounds  in the strictest bounds

in the middle  in our example around values         in the

bound of equation   the argument of the cumulative distribution function
zero when the value of

c is very small 

 tends towards

but at the same time the exponent decreases  the

 approaches      when its argument goes to zero  on the other hand  when c has
a large value   approaches value   and the exponent p also increases 
value of

    on the exactness of the approximation
above we used the expected value of p in the analysis  ep   eq
er  we now probe into
the deviation of p from its expected value  the deviation of r is directly available from
theorem   

for

 

   k     
 
k  e  r

prf jr erj   g    exp

q we do not have a similar result yet 

lipschitz condition

in this section we provide one 

let us rst recapitulate the denition of the

 

f   d       dm   ir be a real valued function with m arguments from
f is said to satisfy the lipchitz condition if for any
x    d            xm   dm   any i   f           mg  and any yi   di  
denition

let

possibly distinct domains  the function

jf  x            xi     xi   xi            xm   f  x           xi     yi  xi             xm  j    

hence  a function satises the lipschitz condition if an arbitrary change in the value of
any one argument does not change the value of the function more than   

martingales

the following result  mcdiarmid        holds for functions satisfying the lipschitz condition  more general results of the same kind can be obtained using
 motwani   raghavan         

 see e g  

theorem    mcdiarmid  let x            xm be independent random variables taking values
in a set v   let f   v m   ir be such that  for i              m 

sup

x       xm  yi  v

jf  x           xi    xi   xi            xm   f  x           xi     yi  xi             xm  j  ci  

then for      
prf jf  x            xm   ef  x            xm  j   g    exp

 
p 

m c 
i   i

 

 

wi   i              n  be a random variable such that wi   j if the i th example is
directed to the safe node zj   by the uniform distribution assumption wi  s are independent 
they have their values within the set f           k g  let us dene the function f so that
f  w            wn   is the number of those safe nodes that receive at most r   cn k examples 
when the i th example is directed to the safe node zw   that is 
let

i

f  w            wn     jf i   f            k g j jsi j  r gj 
   

fian analysis of reduced error pruning
where

si is the set of those examples that are directed to safe node zi  
si   f h   f            n g j wh   i g 
q   f  w            wn   

hence 

moving any one example from one safe node to another  chang 

ing the value of any one argument
dition
of

f

pn

wi    can change one more safe node zi

jsij  r  one less safe node to fulll it  or both at the same time 

to fulll the conthus  the value

changes by at most    hence  the function fullls the lipschitz condition  therefore 

we can apply mcdiarmid s inequality to it by substituting

c    n 

i   i

ci     and observing that then

 
prf jf  w            wn   ef  w            wn  j   g   e    n  
or equally

 
prf jq eqj   g   e    n  

unfortunately  this concentration bound is not very tight  nevertheless  combining the
concentration bounds for

q and r we have for p

the following deviation from its expected

value 

since jp
ep j   jq r e q r j   jq eq   er rj  jq eqj   jr erj 
jq r e q r j   implies that jq eqj     or jr erj      thus 
prf jp ep j   g   prf jq r e q r j   g




 pr jq eqj      pr jr erj   

 

 
 n     exp

   exp

 

   k     
 
  k  e  r 

   related work
traditional pruning algorithms  like cost complexity pruning  breiman et al          pessimistic pruning  quinlan         minimum error pruning  niblett   bratko        cestnik
  bratko         critical value pruning  mingers      a   and error based pruning  quinlan 
       have already been covered extensively in earlier work  mingers      a  esposito
et al         frank         thus we will not touch on these methods any further  instead 
we review some of the more recent work on pruning 

rep

produces an optimal pruning of the given decision tree with respect to the pruning

set  other approaches for producing optimal prunings have also been presented  breiman
et al         bohanec   bratko        oliver   hand        almuallim         however 
often optimality is measured over the training set  then it is only possible to maintain the
initial accuracy  assuming that no noise is present  neither is it usually possible to reduce
the size of the decision tree without sacricing the classication accuracy  for example  in
the work of bohanec and bratko        it was studied how to eciently nd the optimal
pruning in the sense that the output decision tree is the smallest pruning which satises
a given accuracy requirement  a somewhat improved algorithm for the same problem was
presented subsequently by almuallim        

   

fielomaa   kriinen
the high level control of kearns and mansour s        pruning algorithm is the same

cost complexity

bottom up sweep as in

rep 

however  the pruning criterion in their method is a kind of a

condition  breiman et al         that takes both the observed classication

error and  sub tree complexity into account 

moreover  their pruning scheme does not

pessimistic

require the pruning set to be separate from the training set  both mansour s        and
kearns and mansour s        algorithms are
of a  sub tree by its training error 

  they try to bound the true error

since the training error is by nature optimistic  the

pruning criterion has to compensate it by being pessimistic about the error approximation 
consider yet another variant of

rep  one which

is otherwise similar to the one analyzed

above  with the exception that the original leaves are not put to a special status  but can
be relabeled by the majority of the pruning examples just like internal nodes  this version
of

rep

produces the optimal pruning with respect to which the performance of kearns and

mansour s        algorithm is measured  their pessimistic pruning produces a decision tree
that is smaller than that produced by

rep 

kearns and mansour        are able to prove that their algorithm has a strong performance guarantee  the generalization error of the produced pruning is bounded by that of
the best pruning of the given tree plus a complexity penalty 
local in the same sense as those of

rep

the pruning decisions are

and only the basic pruning operation of replacing a

subtree with a leaf is used in this pruning algorithm 

   conclusion
in this paper the

rep

algorithm has been analyzed in three dierent settings 

first  we

rep alone  without assuming anything about the input
this setting it is possible to prove that rep fullls its

studied the algorithmic properties of
decision tree nor pruning set 

in

intended task and produces an optimal pruning of the given tree  the algorithm proceeds
to prune the nodes of a branch as long as both subtrees of an internal node are pruned and
stops immediately if even one subtree is kept  moreover  it prunes an interior node only if
all its descendants at level

d

have been pruned  furthermore 

rep

either halts before the

safe nodes are reached or prunes the whole tree only in case all safe nodes have the same
majority class 
in the second setting the tree under consideration was assumed to t noise  i e   it
was assumed that the class label of the pruning examples is independent of their attribute
values  in this setting the pruning probability of the tree could be bound by an equation
that depends exponentially on the size of the tree and linearly on the number and class
distribution of the pruning examples  thus  our analysis corroborates the main nding of
oates and jensen        that

rep fails to control the growth of a decision tree in the extreme

case that the tree ts pure noise  moreover  our analysis opened a possibility to initially
explain why the learned decision tree grows linearly with an increasing data set  our bound
on the pruning probability of a tree is based on bounding the probability that all safe nodes
have the same majority class  surprisingly  essentially the same property  whose probability
we try to bound close to    is assumed to hold with probability   in the analysis of oates
and jensen        
in

rep

it may happen that no pruning examples are directed to a given subtree  such

subtrees have not been taken into account in earlier analyses 

   

in our nal analysis we

fian analysis of reduced error pruning
included empty subtrees in the equation for a tree s pruning probability 

taking empty

subtrees into account gives a more realistic bound for the pruning probability of a tree 
unfortunately  one cannot draw very denite general conclusions on the two phased topdown induction of decision trees on the basis of analyses on the
bias is quite unique among pruning algorithms 

the fact that

rep algorithm  because its
rep does not penalize the

size of a tree  but only rests on the classication error on the pruning examples makes the
method sensitive to small changes in the class distribution of the pruning set  other decision
tree pruning algorithms also have their individual characteristics  therefore  unied analysis
of decision tree pruning may be impossible 
the version of

rep 

in which one is allowed to relabel original leaves  as well  is used

as the performance objective in kearns and mansour s        pruning algorithm 

thus 

the performance of pruning algorithms that use both error and size penalty is related to
those that use only error estimation  in the version of

rep

used by kearns and mansour

our analysis based on safe nodes applies with leaves in place of safe nodes  hence for this
algorithm the derived bounds are stricter 
we leave the detailed analysis of other important pruning algorithms as future work 
only through such investigation is it possible to disclose the dierences and similarities of
pruning algorithms 

empirical examination has not managed to reveal clear performance

dierences between the methods 

also  the relationship of the number of safe nodes and

leaves of a tree ought to be examined analytically and empirically  in particular  one should
study whether the number of safe nodes does increase linearly with a growing training set 
as conjectured in this paper  deeper understanding of existing pruning algorithms may help
to overcome the problems associated with the pruning phase of decision tree learning 

references

intelligence   

almuallim  h          an ecient algorithm for optimal pruning of decision trees 
 

learning   

         

bohanec  m     bratko  i          trading accuracy for simplicity in decision trees 
 

            

regression trees

breiman  l   friedman  j  h   olshen  r  a     stone  c  j         
  wadsworth  pacic grove  ca 

tional joint conference on articial intelligence

machine

classication and

machine learning   
proceedings of the twelfth interna 

breiman  l          some properties of splitting criteria 
catlett  j          overpruning large decision trees  in

articial

 

          

  pp          san mateo  ca  morgan

kaufmann 

machine learningewsl     proceedings of the fifth european working
lecture notes in articial intelligence

cestnik  b     bratko  i          on estimating probabilities in tree pruning  in kodrato 

session

y   ed   

  vol      of

  pp          berlin  hei 

delberg  new york  springer verlag 

annals of mathematical statistics   

cherno  h          a measure of asymptotic eciency for tests of a hypothesis based on
the sum of observations 

 

   

            

fielomaa   kriinen

proceedings of the thirteenth international joint conference on articial

cohen  w  w         

intelligence

systems  in

ecient pruning methods for separate and conquer rule learning

  pp          san mateo  ca  morgan kaufmann 

machine learning  ecml     proceedings of
lecture notes in articial intelligence

esposito  f   malerba  d     semeraro  g         

the sixth european conference

the state space  in brazdil  p  b   ed   

decision tree pruning as a search in

  vol      of

  pp 

        berlin  heidelberg  new york  springer verlag 

ieee transactions on pattern analysis and machine intelligence   
pruning decision trees and lists
information processing
letters   
machine learning   
proceedings of the eleventh international joint conference on articial
intelligence
proceedings of the thirty fifth annual
ieee symposium on foundations of computer science

esposito  f   malerba  d     semeraro  g          a comparative analysis of methods for
pruning decision trees 
 

            

frank  e         

  ph d  thesis  university of waikato 

department of computer science  hamilton  new zealand 

hagerup  t     rb  c          a guided tour of cherno bounds 
 

            

helmbold  d  p     schapire  r  e          predicting nearly as well as the best pruning of
a decision tree 

 

          

holte  r  c   acker  l     porter  b         

concept learning and the problem of small

disjuncts  in

  pp          san mateo  ca  morgan kaufmann 

kamath  a   motwani  r   palem  k     spirakis  p         

tail bounds for occupancy

and the satisability threshold conjecture  in

  pp          los alamitos 

ca  ieee press 

proceedings of the fifteenth inter 

kearns  m     mansour  y          a fast  bottom up decision tree pruning algorithm with

national conference on machine learning

near optimal generalization  in shavlik  j   ed   

  pp          san francisco  ca  morgan

kaufmann 

learning from

malerba  d   esposito  f     semeraro  g          a further comparison of simplication

data  ai and statistics v
proceedings of the fourteenth international conference on machine learning

methods for decision tree induction  in fisher  d     lenz  h  j   eds   

  pp          berlin  heidelberg  new york  springer verlag 

mansour  y          pessimistic decision tree pruning based on tree size  in fisher  d  h 
 ed   

 

pp          san francisco  ca  morgan kaufmann 

surveys in combinatorics  invited papers of the   th british combinatorial conference
machine learning  
machine learning  
machine learning

mcdiarmid  c  j  h          on the method of bounded dierences  in siemons  j   ed   
  pp          cambridge  u k  cambridge university press 

mingers  j       a   an empirical comparison of pruning methods for decision tree induction 
 

            

mingers  j       b   an empirical comparison of selection measures for decision tree induction 

mitchell  t  m         

 

            

  mcgraw hill  new york 

   

fian analysis of reduced error pruning
motwani  r     raghavan  p         
new york 

randomized algorithms

  cambridge university press 

research and development in expert systems iii

niblett  t     bratko  i          learning decision rules in noisy domains  in bramer  m  a 
 ed   

  pp        cambridge  uk 

cambridge university press 

proceedings of the fourteenth international conference on

oates  t     jensen  d          the eects of training set size on decision tree complexity 

machine learning

in fisher  d  h   ed   

  pp          san francisco  ca  morgan kaufmann 

oates  t     jensen  d          large datasets lead to overly complex models  an expla 

proceedings of the fourth international conference on knowledge discovery and data
mining
proceedings of the sixteenth national conference on articial intelligence
nation and a solution 

in agrawal  r   stolorz  p     piatetsky shapiro  g   eds   

  pp          menlo park  ca  aaai press 

oates  t     jensen  d         

toward a theoretical understanding of why and when

decision tree pruning algorithms fail  in

  pp          menlo park  ca cambridge  ma  aaai

press mit press 

proceedings of the twelfth international conference on machine
machine

oliver  j  j     hand  d  j          on pruning and averaging decision trees  in prieditis  a  

learning
learning  

  russell  s   eds   

  pp          san francisco  ca  morgan kaufmann 

pagallo  g     haussler  d          boolean feature discovery in empirical learning 
 

          

machine learning   

pereira  f     singer  y          an ecient extension to mixture techniques for prediction
and decision trees 

 

            

machine learning  
international journal of man machine
c     programs for machine learning
the annals of probability

quinlan  j  r          induction of decision trees 

studies   

quinlan  j  r         
 

 

        

simplifying decision trees 

            

quinlan  j  r         

 

morgan kaufmann  san

slud  e  v          distribution inequalities for the binomial law 

 

mateo  ca 

 

            

   

fi