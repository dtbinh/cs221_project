journal of artificial intelligence research              

submitted       published     

identifying hierarchical structure in sequences 
a linear time algorithm
craig g  nevill manning
ian h  witten
department of computer science
university of waikato  hamilton  new zealand 

cgn cs waikato ac  nz
ihw cs waikato ac  nz

abstract
s equitur is an algorithm that infers a hierarchical structure from a sequence of discrete symbols
by replacing repeated phrases with a grammatical rule that generates the phrase  and continuing
this process recursively  the result is a hierarchical representation of the original sequence  which
offers insights into its lexical structure  the algorithm is driven by two constraints that reduce the
size of the grammar  and produce structure as a by product  s equitur breaks new ground by
operating incrementally  moreover  the methods simple structure permits a proof that it operates
in space and time that is linear in the size of the input  our implementation can process       
symbols per second and has been applied to an extensive range of real world sequences 

   introduction
many sequences of discrete symbols exhibit natural hierarchical structure  text is made up of
paragraphs  sentences  phrases  and words  music is composed from major sections  motifs  bars 
and notes  records of user interface behavior encode the hierarchical structure of tasks that users
perform  computer programs constitute modules  procedures  and statements  discovering the
natural structure that underlies sequences is a challenging and interesting problem that has a wide
range of applications  from phrase discovery to music analysis  from programming by
demonstration to code optimization 
the search for structure in sequences occurs in many different fields  adaptive text
compression seeks models of sequences that can be used to predict upcoming symbols so that
they can be encoded efficiently  bell et al          however  text compression models are
extremely opaque  and do not illuminate any hierarchical structure in the sequence  grammatical
inference techniques induce grammars from a set of example sentences  possibly along with a set
of negative examples  gold        angluin        berwick and pilato         however  it is
crucial to their operation that the input is not a continuous stream but is segmented into sentences 
which are  in effect  independent examples of the structure being sought  a brief review of
pertinent systems appears in section    techniques of markov modeling and hidden markov
modeling make no attempt to abstract information in hierarchical form  rabiner and juang       
laird and saul         sequence learning also occurs in areas such as automaton modeling
 gaines         adaptive systems  andreae         programming by demonstration  cypher 
       and human performance studies  cohen et al          but generally plays only a peripheral
role 
in this paper we describe sequitur  an algorithm that infers a hierarchical structure from a
sequence of discrete symbols  the basic insight is that phrases which appear more than once can
be replaced by a grammatical rule that generates the phrase  and that this process can be
continued recursively  producing a hierarchical representation of the original sequence  the result
      ai access foundation and morgan kaufmann publishers  all rights reserved

finevill manning   witten

is not strictly a grammar  for the rules are not generalized and generate only one string   it does
provide a good basis for inferring a grammar  but that is beyond the scope of this paper   a
scheme that resembles the one developed here arose from work in language acquisition  wolff 
                         but it operated in time that is quadratic with respect to the length of the
input sequence  whereas the algorithm we describe takes linear time  this has let us investigate
sequences containing several million tokensin previous work the examples were much smaller 
the largest mentioned being a few thousand tokens  another difference  which is of crucial
importance in some practical applications  is that the new algorithm works incrementally  we
return to wolffs scheme  and compare it with sequitur  in section   
the ability to deal easily with long sequences has greatly extended the range of sequiturs
application  we have applied it to artificially generated fractal like sequences produced by lsystems  and  along with a unification based rule generalizer  used it to recover the original lsystem  the same method has inferred relatively compact deterministic  context free grammars
for million symbol sequences representing biological objects obtained from stochastic  contextsensitive  l systems  which has in turn greatly speeded the graphical rendering of such objects 
we have applied sequitur to    mbyte segments of a digital library to generate hierarchical
phrase indexes for the text  which provides a novel method of browsing  nevill manning et al  
       the algorithm compresses multi megabyte dna sequences more effectively than other
general purpose compression algorithms  finally  with some post processing  it has elicited
structure from a two million word extract of a genealogical database  successfully identifying the
structure of the database and compressing it much more efficiently than the best known
algorithms  we touch on some of these applications in section   below  nevill manning       
describes them all 
this paper describes the sequitur algorithm and evaluates its behavior  the next section
gives a concise description of the algorithm in terms of constraints on the form of the output
grammar  section   gives a taste of the kind of hierarchies that sequitur is capable of inferring
from realistic sequences  section   describes the implementation in more detail  with particular
emphasis on how it achieves efficiency  section   shows that the run time and storage
requirements are linear in the number of input symbols  while section   discusses the algorithms
behavior on extreme input strings  we end with a quantitative analysis of sequiturs
performance on several example sequences  and a review of related research 

   the sequitur algorithm
sequitur forms a grammar from a sequence based on repeated phrases in that sequence  each
repetition gives rise to a rule in the grammar  and the repeated subsequence is replaced by a nonterminal symbol  producing a more concise representation of the overall sequence  it is this
pursuit of brevity that drives the algorithm to form and maintain the grammar  and  as a byproduct  provide a hierarchical structure for the sequence 
at the left of figure  a is a sequence that contains the repeating string bc  note that the
sequence is already a grammara trivial one with a single rule  to compress it  sequitur forms
a new rule a  bc  and a replaces both occurrences of bc  the new grammar appears at the right
of figure  a 
the sequence in figure  b shows how rules can be reused in longer rules  the longer
sequence consists of two copies of the sequence in figure  a  since it represents an exact

  

fii nferring sequential structure from sequences

repetition  compression can be achieved by forming the rule a  abcdbc to replace both halves
of the sequence  further gains can be made by forming rule b  bc to compress rule a  this
demonstrates the advantage of treating the sequence  rule s  as part of the grammarrules may
be formed from rule a in an analogous way to rules formed from rule s  these rules within rules
constitute the grammars hierarchical structure 
the grammars in figures  a and  b share two properties 
p     no pair of adjacent symbols appears more than once in the grammar 
p     every rule is used more than once 
property p   requires that every digram in the grammar be unique  and will be referred to as
digram uniqueness  property p  ensures that each rule is useful  and will be called rule utility 
these two constraints exactly characterize the grammars that sequitur generates 
figure  c shows what happens when these properties are violated  the first grammar contains
two occurrences of bc  so p  does not hold  this introduces redundancy because bc appears twice 
in the second grammar  rule b is used only once  so p   does not hold  if it were removed  the
grammar would become more concise  the grammars in figures  a and  b are the only ones for
which both properties hold for each sequence  however  there is not always a unique grammar
with these properties  for example  the sequence in figure  d can be represented by both of the
grammars on its right  and they both obey p   and p    we deem either grammar to be acceptable 
repetitions cannot overlap  so the string aaa does not give rise to any rule  despite containing two
digrams aa 
sequiturs operation consists of ensuring that both properties hold  when describing the
algorithm  the properties act as constraints  the algorithm operates by enforcing the constraints
on a grammar  when the digram uniqueness constraint is violated  a new rule is formed  and when
the rule utility constraint is violated  the useless rule is deleted  the next two subsections describe
how this occurs 
    digram uniqueness
when sequitur observes a new symbol  it appends it to rule s  the last two symbols of rule s
the new symbol and its predecessorform a new digram  if this digram occurs elsewhere in the
grammar  the first constraint has been violated  to restore it  a new rule is formed with the digram
on the right hand side  headed by a new non terminal symbol  the two original digrams are
replaced by this non terminal symbol 
sequence

grammar

a

s  abcdbc

s  aada
a  bc

c

s  abcdbcabcdbc

s  aa
a  abcdbc

sequence

grammar

b

s  abcdbcabcdbc

s  aa
a  abdb
b  bc

d

s  aabaaab

s  aaa
a  aab

s  cc
a  bc
b  aa
c  bda

s  abaab
a  aa

figure   example sequences and grammars that reproduce them   a  a
sequence with one repetition   b  a sequence with a nested repetition   c 
two grammars that violate the two constraints   d  two different grammars
for the same sequence that obey the constraints 

  

finevill manning   witten

table   shows the grammars that result when successive symbols of the sequence
abcdbcabcd are processed  the second column shows the sequence observed so far  the third
column gives the grammar created from the sequence  and the fourth column notes constraints
that have been violated  and actions that are taken to resolve the violations 
when sequitur adds the final c at symbol    the digram bc appears twice  sequitur creates
the new rule a  with bc as its right hand side  and replaces the two occurrences of bc by a  this
illustrates the basic procedure for dealing with duplicate digrams 
the appearance of a duplicate digram does not always result in a new rule  if the new digram
appears as the right hand side of an existing rule  then no new rule need be created  the nonterminal symbol that heads the existing rule replaces the digram  after symbol   in table    a
third bc appears  and the existing non terminal symbol a replaces the third occurrence of bc  this
results in a new pair of repeating digrams  aa  shown in the next line of table    sequitur
accordingly forms a new rule b  which replaces the two occurrences of aa  sequitur creates and
maintains the hierarchy by an iterative process  the substitution of a for bc results in the new
digram aa  which is itself replaced by b  for larger sequences  these changes ripple through the
grammar  forming and matching longer rules higher in the hierarchy 
symbol
number

the string so
far

resulting
grammar

remarks

 

a

sa

 

ab

s  ab

 

abc

s  abc

 

abcd

s  abcd

 

abcdb

s  abcdb

 

abcdbc

s  abcdbc

bc appears twice

s  aada
a  bc

enforce digram uniqueness

 

abcdbca

s  aadaa
a  bc

 

abcdbcab

s  aadaab
a  bc

 

abcdbcabc

s  aadaabc
a  bc

bc appears twice

s  aadaaa
a  bc

enforce digram uniqueness 
aa appears twice

s  bdab
a  bc
b  aa

enforce digram uniqueness

s  bdabd
a  bc
b  aa

bd appears twice

s  cac
a  bc
b  aa
c  bd

enforce digram uniqueness 
b is only used once

s  cac
a  bc
c  aad

enforce rule utility

  

abcdbcabcd

table   operation of the two grammar constraints 
digram uniqueness and rule utility

  

fii nferring sequential structure from sequences

    rule utility
until now  the right hand sides of rules in the grammar have been only two symbols long  longer
rules are formed by the effect of the rule utility constraint  which ensures that every rule is used
more than once  in table    symbol    demonstrates this idea  when d is appended to rule s  the
new digram bd causes a new rule  c  to be formed  however  forming this rule leaves only one
appearance of rule b  violating the second constraint  for this reason  b is removed from the
grammar  and its right hand side is substituted in the one place where it occurs  removing b
means that rule c now contains three symbols  this is the mechanism for forming long rules 
form a short rule temporarily  and if subsequent symbols continue the match  allow a new rule to
supersede the shorter one and delete the latter  although this creation and deletion of rules
appears inefficient at first glance  it can be performed efficiently with the appropriate data
structures  more importantly  it keeps track of long matches within the grammar  obviating the
need for external data structures  this simplifies the algorithm considerably  permitting a concise
proof of its linear time complexity  see section    

   structures inferred from realistic sequences
having described the mechanism by which sequitur builds a grammar  and before embarking
on efficiency issues  it is instructive to consider the structures that this simple technique can infer
from realistic sequences  in each case we applied sequitur to a large sample and then excerpted
part of the structure for illustrative purposes 
figures  a   b and  c show parts of three hierarchies inferred from the text of the bible in
english  french  and german  the hierarchies are formed without any knowledge of the preferred
structure of words and phrases  but nevertheless capture many meaningful regularities  in
figure  a  the word beginning is split into begin and ninga root word and a suffix  many words
and word groups appear as distinct parts in the hierarchy  spaces have been made visible by
replacing them with bullets   the same algorithm produces the french version in figure  b 
where commencement is split in an analogous way to beginninginto the root commence and the
suffix ment  again  words such as au  dieu and cieux are distinct units in the hierarchy  the
german version in figure  c correctly identifies all words in the sentence  as well as the phrase
die himmel und die  in fact  the hierarchy for the heaven and the in figure  a bears some
similarity to the german equivalent 
the london oslo bergen corpus  johansson et al         contains     million words tagged
with word classes  for example  the sentence most labour sentiment would still favour the
abolition of the house of lords is tagged with the classes determiner noun noun auxiliary adverb
verb article noun preposition article noun preposition noun  the hierarchy that sequitur infers
from the word classes corresponds to a possible parse of each sentence in the corpus  because it is
a tree expressed in terms of parts of speech  figure  d shows part of the inferred hierarchy  where
the tags have been replaced by the actual words from the text  sequitur identifies the middle
part of the sentence  sentiment would still favour the abolition as a large block  and this part could
stand on its own as a grammatical sentence  the adjectival phrase of the house of lords also
appears as a distinct unit  as does most labour  an adjectival phrase that precedes the subject 
figure  e shows two bach chorales from which sequitur has detected both internal
repetitionsthe light gray boxes show that the two halves of the first chorale are almost
identicaland repetitions between chorales  as denoted by the gray box in the second half of the

  

finevill manning   witten

a

i n  t h e  b e g i n n i n g  g o d  c r e a t e d  t h e  h e a v e n  a n d  t h e  e a r t h

b

 a u  c o m m e n c e m e n t    d i e u  c r  a  l e s  c i e u x  e t  l a  t e r r e

c

xx

 i m  a n f a n g  s c h u f  g o t t  d i e  h i m m e l  u n d  d i e  e r d e

d

most labour sentiment would still favour the abolition of the house of lords

e

imperfect

perfect

figure   hierarchies for various sequences  genesis     in  a  english   b  french  and  c  german 
 d  grammatical parse inferred from a sequence of word classes  and  e  repetitions within and
between two chorales harmonized by j s  bach 
second chorale  the section in the white box occurs in all four halves  also  by detecting repeated
motifs between many chorales  sequitur identifies the imperfect and perfect cadences at the end
of the first and second halves  respectively  in general  sequitur is capable of making plausible
inferences of lexical structure in sequences  so that the hierarchies it produces aid comprehension
of the sequence 

   implementation issues
the sequitur algorithm operates by enforcing the digram uniqueness and rule utility constraints 
it is essential that any violation of these constraints be detected efficiently  and in this section we
will describe the mechanisms that fulfill this requirement 
the choice of an appropriate data structure depends on the kind of operations that need to be
performed to modify the grammar  for sequitur these are 
 appending a symbol to rule s 
 using an existing rule 
 creating a new rule  and
 deleting a rule 
appending a symbol involves lengthening rule s  using an existing rule involves substituting a
non terminal symbol for two symbols  thereby shortening the rules containing the digrams 
creating a new rule involves creating a new non terminal symbol for the left hand side  and
  

fii nferring sequential structure from sequences

a

b

c

b

a

b

d
cd
digram
bc
index
ab

figure   data structures for rules and digram index
inserting two new symbols as the right hand side  after creating the rule  substitutions are made
as for an existing rule by replacing the two digrams with the new non terminal symbol  deleting a
rule involves moving its contents to replace a non terminal symbol  which lengthens the rule
containing the non terminal symbol  the left hand side of the rule must then be deleted 
to ensure that rules can be lengthened and shortened efficiently  sequitur represents a rule
using a doubly linked list whose start and end are connected to a single guard node  shown for
two rules a and b in figure    the guard node also serves as an attachment point for the left hand
side of the rule  because it remains constant even when the rule contents change  each nonterminal symbol also points to the rule it heads  shown in figure   by the pointer from the nonterminal symbol b in rule a to the head of rule b  with these pointers  no arrays are necessary for
accessing rules or symbols  because operations only affect adjacent symbols or rules headed by a
non terminal 
the rule utility constraint demands that a rule be deleted if it is referred to only once  each
rule has an associated reference count  which is incremented when a non terminal symbol that
references the rule is created  and decremented when the non terminal symbol is deleted  when
the reference count falls to one  the rule is deleted 
the digram uniqueness constraint is more difficult to enforce  when a new digram appears 
sequitur must search the grammar for any other occurrence of it  one simple solution would be
to scan the entire grammar each time looking for a match  but this is inefficient and leads to a
quadratic time algorithm  a better solution requires an index that is efficient to search 
the data structure for storing the digram index must permit fast access and efficient addition
and deletion of entries  a hash table provides constant time access  and adding and deleting
entries requires little extra work  because no digram appears more than once  the table need only
contain a pointer to the first symbol of the single matching digram in the grammar data structure 
as shown in figure    the hash table consists of a simple array of pointers  and collisions are
handled by open addressing to avoid the allocation of memory that chaining requires  knuth 
      
every time a new digram appears in the grammar  sequitur adds it to the index  a new
digram appears as a result of two pointer assignments linking two symbols together in the doublylinked list  one forward pointer and one back pointer   thus updating the index can be
incorporated into the low level pointer assignments  a digram also disappears from the grammar
when a pointer assignment is madethe pointer value that is overwritten by the assignment
represents a digram that no longer exists 
to demonstrate the mechanism for updating the hash table when a new rule is created  table
  shows the example in figure  a  with the addition of the contents of the digram index  when
the second c is appended to rule s  the digram table shows that bc already exists in the grammar 
so the rule a  bc is created  creating the link between b and c in the right hand side for rule a
updates the entry in the index for bc to point to its new locationthe hash table now contains a
pointer to the symbol b at the start of rule a  next  the first bc is removed  this breaks the link
between the b in the digram and the preceding symbol a  so ab is removed from the index  it also
  

finevill manning   witten

action

grammar

observe symbol c

s  abcdbc

change in digrams

digram index

make new rule a

s  abcdbc
a  bc

bc updated

 ab  bc  cd  db 

substitute a for bc

s  aadbc
a  bc

ab  cd removed 
aa  ad added

 bc  db  aa  ad 

substitute a for bc

s  aada
a  bc

db removed 
da added

 bc  da  aa  ad 

 ab  bc  cd  db 

table   updating the digram index as links are made and broken
breaks the link between c and the following d  so cd is removed from the index  next  a replaces
bc  creating links between a and a  as well as between a and d  adding these digrams to the index 
this process continues  resulting in a correct index of digram pointers  but costing just one
indexing operation per two pointer operations 
next  sequitur requires an efficient strategy for checking the digram index  rechecking the
entire grammar whenever a symbol is added is infeasible  and inefficient if large portions of the
grammar are unchanged since the last check  in fact  the only parts that need checking are those
where links have been made or broken  that is  when any of the actions that affect the
maintenance of the digram table are performed  the newly created digrams should be checked in
the index  of course  every time a link is created  the digram is entered into the index  and this is
the very time to check for a duplicate  if an entry is found to be already present while attempting
to add a new digram to the index  a duplicate digram has been detected and the appropriate
actions should be performed  therefore  only one hash table lookup is required for both accessing
and updating the digram index 

   computational complexity
in this section  we show that the sequitur algorithm is linear in space and time  the complexity
proof is an amortized oneit does not put a bound on the time required to process one symbol 
but rather bounds the time taken for the whole sequence  the processing time for one symbol can
in fact be as large as o  n    where n is the number of input symbols so far  however  the
pathological sequence that produces this worst case requires that the preceding o  n   symbols
involve no formation or matching of rules 
the basic idea of the proof is that the two constraints both have the effect of reducing the
action
 

as each new input symbol is observed  append it to rule s 

 

 
 
 
 
 
 
 
 

each time a link is made between two symbols
if the new digram is repeated elsewhere and the repetitions do not overlap 
if the other occurrence is a complete rule 
replace the new digram with the non terminal symbol that heads the rule 
otherwise 
form a new rule and replace both digrams with the new non terminal symbol
otherwise 
insert the digram into the index

 

  
  
  

each time a digram is replaced by a non terminal symbol
if either symbol is a non terminal symbol that only occurs once elsewhere 
remove the rule  substituting its contents in place of the other non terminal symbol

table   the sequitur algorithm

  

 
 

 

fii nferring sequential structure from sequences

number of symbols in the grammar  so the work done satisfying the constraints is bounded by the
compression achieved on the sequence  the savings cannot exceed the original size of the input
sequence  so the algorithm is linear in the number of input symbols 
table   gives pseudo code for the sequitur algorithm  line   deals with new observations
in the sequence  lines   through   enforce the digram utility constraint  and lines    through   
enforce rule utility  the on line appendix contains an implementation of sequitur in java  which
requires about     lines for the algorithm 
the numbers at the right of table   identify the main sections of the algorithm  and the proof
will demonstrate bounds on the number of times that each of them executes  action   appends
symbols to rules s and is performed exactly n times  once for every symbol in the input  link
creation triggers action    action   uses an existing rule  action   forms a new rule  and action  
removes a rule 
table   shows examples of actions       and    and the associated savings in grammar size 
the savings are calculated by counting the number of symbols in the grammar before and after
the action  the non terminal symbols that head rules are not counted  because they can be
recreated based on the order in which the rules occur  actions   and   are the only actions that
reduce the number of symbols  there are no actions that increase the size of the grammar  so the
difference between the size of the input and the size of the grammar must equal the number of
times that both these actions have been taken 
now that we have set the stage  we can proceed with the proof  more formally  let
n be the size of the input string 
o be the size of the final grammar 
r be the number of rules in the final grammar 
a   be the number of times new symbol is seen  action    
a   be the number of times a new digram is seen  action    
a   be the number of times an existing rule is used  action    
a   be the number of times a new rule is formed  action     and
a   be the number of times a rule is removed  action    
according to the reasoning above  the reduction in the size of the grammar is the number of times
actions   and   are executed  that is 
   
n  o   a    a  
next  the number of times a new rule is created  action    must be bounded  the two actions that
affect the number of rules are    which creates rules  and    which deletes them  the number of
rules in the final grammar must be the difference between the frequencies of these actions 
r   a   a  
in this equation  r is known and a  is bounded by equation      but a   is unknown  noting that a   
the number of times a new symbol is seen  is equal to n  the total work is
action

before

after

saving

matching existing rule

 

   ab   
a  ab

   a   
a  ab

 

creating new rule

 

   ab   ab   

   a   a   
a  ab

 

deleting a rule

 

   a   
a  ab

   ab   

 

table   reduction in grammar size for the three grammar operations

  

finevill manning   witten

a     a     a     a     a     n   a      n  o     r   a      
to bound this expression  note that the number of rules must be less than the number of symbols
in the final grammar  because each rule contains at least two symbols  so that
r   o 
also  from expression     above  we have
a    n  o  a    n 
consequently 
a     a     a     a     a      n    r  o    a    a      n  a    
the final operation to bound is action    which checks for duplicate digrams  searching the
grammar is done by hash table lookup  assuming an occupancy less than  say      gives an
average lookup time bounded by a constant  knuth         this occupancy can be assured if the
size of the sequence is known in advance  or by enlarging the table and recreating the entries
whenever occupancy exceeds      the number of entries in the table is just the number of
digrams in the grammar  which is the number of symbols in the grammar minus the number of
rules in the grammar  because symbols at the end of a rule do not form the left hand side of any
digram  thus the size of the hash table is less than the size of the grammar  which is bounded by
the size of the input  this means that the memory requirements of the algorithm are linear  in
practice  the linear growth of memory poses a problem  one strategy that we are currently
investigating is to break the input into small segments  form grammars from each of them  and
merge the resulting grammar 
as for the number of times that action   is carried out  a digram is only checked when a new
link is created  links are only created by actions         and    which have already been shown to
be bounded by  n  so the time required for action   is also o n  
thus we have shown that the algorithm is linear in space and time  however  this claim must
be qualified  it is based on a register model of computation rather than a bitwise one  we have
assumed that the average lookup time for the hash table of digrams is bounded by a constant 
however  as the length of the input increases  the number of rules increases without bound  and
for unstructured  e g   random  input  the digram table will grow without bound  thus the time
required to execute the hash function and perform addressing will not be constant  but will
increase logarithmically with the input size  our proof ignores this effect  it assumes that hash
function operations are register based and therefore constant time  in practice  with a    bit
architecture  the linearity proof remains valid for sequences of up to around     symbols  and for
a    bit architecture up to      symbols 

   exploring the extremes
having described sequitur algorithmically  we now characterize its behavior in a variety of
domains  this section explores how large or small a grammar can be for a given sequence length 
as well as determining the minimum and maximum amount of work the algorithm can carry out
and the amount of work required to process one symbol  figure   summarizes these extreme
cases  giving part of an example sequence and the grammar that results  bounds are given in
terms of n  the number of symbols in the input 
the deepest hierarchy that can be formed has depth o  n    and an example of a sequence
that creates such a hierarchy is shown in figure  a  in order for the hierarchy to deepen at every
rule  each rule must contain a non terminal symbol  furthermore  no rule need be longer than two

  

fii nferring sequential structure from sequences

symbols  therefore  to produce a deep hierarchy from a short string  each rule should be one
terminal symbol longer than the one on which it builds  in order to create these rules  the string
represented must appear in two different contexts  otherwise the rule will be incorporated into a
longer rule  one context is the deepest hierarchy  in which it must participate  the other context
should not be in any hierarchy  to reduce the size of the input string  so it should appear in rule s 
note that every rule in figure  a appears both in the hierarchy and in rule s  at each repetition of
the sequence  one terminal symbol is appended  producing a new level in the hierarchy  there is
no point in including a repetition of length one  so the mth repetition has length m      this
repetition gives rise to the mth rule  counting rule s   the total length of the sequence for a
hierarchy of depth m is therefore
n                      m        o m  
and the deepest hierarchy has depth m   o  n   
at the other end of the spectrum  the grammar with the shallowest hierarchy  shown in
figure  b  has no rules apart from rule s  this grammar is also the largest possible one for a
sequence of a given length  precisely because no rules can be formed from it  the sequence that
gives rise to it is one in which no digram ever recurs  of course  in a sequence with an alphabet of
size     there are only o      different digrams  which bounds the length of such a sequence 
this kind of sequence produces the worst case compression  there are no repetitions  and
therefore sequitur detects no structure 
bound

example sequence

example grammar

a

deepest hierarchy

o n 

ababcabcdabcdeabcdef

s  abcddf
a  ab
b  ac
c  bd
d  ce

b

largest grammar 
shallowest hierarchy

n

aabacadae   bbcbdbe   

s  aabacadae   

c

smallest grammar

o log n 

aaaaaaaaaaaaaa   

s  dd
a  aa
b  aa
c  bb
d  cc

d

largest number of rules

n  

aaaaababacacadad   

s  aabbccdd
a  aa
b  ab
c  ac
d  ad

e

maximum processing for
one symbol

o n 

yzxyzwxyzvwxy

s  abwbvwxy
a  yz
b  xa

f

greatest number of rule
creations and deletions

n new rules
n deleted rules

abcdeabcdeabcde   

s  aaa   
a  abcde

figure   some extreme cases for the algorithm

  

finevill manning   witten

turning from the largest grammar to the smallest  figure  c depicts the grammar formed
from the most ordered sequence possibleone consisting entirely of the same symbol  when four
contiguous symbols appear  such as aaaa  a rule b  aa is formed  when another four appear 
rule s contains bbbb  forming a new rule c  bb  every time the number of symbols doubles  a
new rule is created  the hierarchy is thus o log n  deep  and the grammar is o log n  in size  this
represents the greatest data compression possible  although it is not necessary to have a sequence
of only one symbol to achieve this logarithmic lower boundany recursive structure will do 
to produce the grammar with the greatest number of rules  each rule should only include
terminal symbols  because building a hierarchy will reduce the number of rules required to cover
a sequence of a given size  furthermore  no rule should be longer than two symbols or occur
more than twice  therefore each rule requires four symbols for its creation  so the maximum
number of rules for a sequence of length n is n    as shown in figure  d 
having discussed the size of grammars  we can now consider the effort involved in
maintaining them  we have shown that the upper bound for processing a sequence is linear in the
length of the sequence  however  it is still useful to characterize the amount of processing
involved for each new symbol  figure  e shows a sequence where the repetition is built up as yz 
then xyz  then wxyz  and so forth  just before the second occurrence of wxyz appears  no matches
have been possible for the w  x  and y  when z appears  yz matches rule a  then xa matches rule b 
finally  sequitur forms a new rule for wb  this cascading effect can be arbitrarily large if the
repetitions continue to build up in this right to left fashion  the amount of processing required to
deal with the last z is proportional to the depth of the deepest hierarchy  as the matching cascades
up the hierarchy  the maximum time to process one symbol is therefore o  n    the fact that w 
x  and y fail to match means that they require little time to process  preserving the overall linear
time bound 
although the bound is linear  sequences certainly differ in the proportion of work to sequence
length  the sequence in figure  b  where no repetitions exist and no grammar is formed 
minimizes this ratio  the sequence in figure  f  which consists of multiple repetitions of a multisymbol sequence  maximizes it  each time the repetition appears there are several rule deletions
and creations as the match lengthens  in fact  every symbol except a incurs a rule creation and a
subsequent deletion  so there are o n  creations and deletions  if m is the length of the repetition 
the proportion of symbols that do not incur this work is   m  which tends toward zero as the
repetition length approaches infinity 

   behavior in practice
to give an idea of how sequitur behaves on realistic sequences  we turn from artificial cases to
a sequence of english text  figure  a plots the number of rules in the grammar against the number
of input symbols for a         character english novel  and shows that the increase is
approximately linear  figure  b shows the approximately linear growth of the total number of
symbols in the grammar  the growth of the number of unique words in the text  shown in
figure  c  is high at the start and drops off toward the end  zobel  et al         have observed in
much larger samples of english text thatsurprisinglynew words continue to appear at a fairly
constant rate  corresponding not just to neologisms but to names  acronyms  and typographical
errors  in this example  the number of rules grows linearly because  once words have been
recognized  multi word phrases are constructed  and the number of such phrases is unbounded 

  

fii nferring sequential structure from sequences

the nearly linear growth of the number of symbols in the grammar seems disappointing  but is in
fact an inevitable consequence of the information content of english  since symbols at the end of
the text convey a similar amount of information as symbols at the beginning  there is a lower
bound on the achievable compression rate  for english text  this corresponds to the entropy of
english 
s equitur operates very quicklyas shown in figure  d  the         character novel is
processed in    seconds  a rate of        symbols per second  or   mb per minute  the figure
also illustrates sequiturs linear time behavior in practice  the sequence in figure  b  where no
repetitions exist and no rules are formed  should be fast to process  and indeed it is processed at a
rate of         symbols per secondthree times faster than the novel  the sequence in figure  f 
which consists of multiple repetitions of a multi symbol sequence  slows performance to       
symbols per seconda ten fold decrease from the fastest sequence  the sequence in figure  c 
which consists of many repetitions of a single character and forms a concise grammar  comes in
at        symbols per second  about the same as the novel  all measurements were performed on
a silicon graphics indigo   
s equitur is an effective data compression scheme that outperforms other schemes that
achieve compression by factoring out repetition  and approaches the performance of schemes that
a

b

      

symbols in grammar

rules in grammar

     

     

     

      

      

     

 

 
 

c

      

             
input symbols

      

d

     

    

      

             
input symbols

      

 

      

             
input symbols

      

  

time  seconds 

vocabulary size

     

 

  

 

 

 
 

      

             
input symbols

      

figure   growth rates on english text   a  rules in the grammar   b  symbols in the
grammar   c  vocabulary size in the input  and  d  time

  

finevill manning   witten

compress based on probabilistic predictions  sequiturs implementation and evaluation as a
compression scheme is described by nevill manning and witten        

   related work
as mentioned in the introduction  this research resembles work by wolff         having
described sequitur  it is now possible to contrast it with wolffs system  mk    which processes
a sequence from left to right  and forms a chunk  equivalent to a sequitur rule  whenever a
digram is seen more than    times  when this happens  all occurrences of the digram are replaced
with a non terminal symbol  and the system either carries on in the sequence  or restarts from the
beginning  in either case  digram frequencies are discarded and the process starts over  the worst
case for this algorithm corresponds to the sequence in figure  f  where there are long exact
repetitions  each symbol in the repeated segment gives rise to a chunk  and the process starts
over  in figure  f  the length of the repetition is linear in the length of the sequence  and the
number of restarts is the length of the repetition  so the algorithm is quadratic in the length of the
sequence  this makes processing of million symbol sequences impractical 
a number of systems  by langley         stolcke and omohundro         and cook et al 
        form new grammar rules from repeated sequences  and also merge rules to generalize
grammars  however  they operate in a different domainas input  they expect a set of sentences
drawn from a language  rather than a single long sequence  this allows them to make inferences
based on directly comparing corresponding parts of different sequences  furthermore  the small
size of the training data means that efficiency is of lesser concern  the performance of these
algorithms is measured by their ability to accept test sentences from the language  and to reject
new sentences that are not in the target languages  in sequiturs case  where there is only one
sequence available  this metric does not apply 
vanlehn and ball        also infer grammars from sets of sentences  their algorithm
enforces three constraints on grammars for the purpose of making a version space finite  they
are      no rule has an empty right side      if a rule has just one symbol on its right side  the
symbol is a terminal  and     every non terminal appears in a derivation of some string  these
constraints are reminiscent of sequitursfor example  the third constraint is a weaker form of
sequiturs rule utilitybut serve a different purpose  in sequitur  they are operational  they
drive the formation of the grammar  in vanlehn and balls work  they make the version space
tractable by providing sensible restrictions on the form of the grammar  and the algorithm itself is
a search through the space 

   conclusion
this paper has presented s equitur   an algorithm for identifying hierarchical structure in
sequences  based on the idea of abstracting subsequences that occur more than once into rules
and continuing this operation recursively  the algorithm works by maintaining two constraints 
every digram in the grammar must be unique  and every rule must be used more than once 
s equitur operates incrementally and  subject to a caveat about the register model of
computation used  in linear space and time  this efficiency has permitted its application to long
sequencesup to    mbytein many different domains 
we have not evaluated the prediction accuracy of s equitur in this paper  evaluating
prediction accuracy is a fairly complex business  it is not adequate simply to give a count of
  

fii nferring sequential structure from sequences

correct versus incorrect predictions  because doing this begs the question of the likelihood of
different ones occurring  prediction schemes can assign probabilities to all the predictions they
might offer  and should be judged on the discrepancy between their probabilistic predictions and
the true upcoming symbols  the whole question of accurate probabilistic prediction of sequences
is tantamount to the compression of sequences  a substantial field in its own right  bell et al  
       we have in fact evaluated sequiturs performance in compression and found that it vies
with the best compression algorithms  particularly when a large amount of text is available
 nevill manning and witten         but the point of the present paper is a different one  that
sequitur re represents a sequence in a way that exposes its underlying structure  it is fair to say
that no other compression algorithm produces a representation that is in any way perspicuous 
perhaps the greatest drawback to the sequitur algorithm is its memory usage  which is
linear in the size of grammar  linear memory complexity is ordinarily considered intractable 
although in practice s equitur works well on sequences of rather impressive size  there is
clearly room for approximate versions of the algorithm that partition the input and re merge the
grammars formed from them  and this could perhaps be applied recursively to create an algorithm
with logarithmic memory requirements  we conjecture  however  that while such approximations
will no doubt turn out to be very useful in practice  they will inevitably sacrifice the property of
digram uniqueness that is an appealing feature of the original algorithm 

acknowledgments
we are grateful for many detailed suggestions from pat langley and the anonymous referees 

references
andreae  j h         thinking with the teachable machine  london  academic press 
angluin  d         inference of reversible languages  journal of the association for computing
machinery             
bell  t c   cleary  j g   and witten  i h         text compression  englewood cliffs  nj 
prentice hall 
berwick  r c   and pilato  s         learning syntax by automata induction  machine learning 
       
cohen  a   ivry  r i   and keele  s w         attention and structure in sequence learning 
journal of experimental psychology              
cook  c m   rosenfeld  a     aronson  a          grammatical inference by hill climbing 
informational sciences            
cypher  a   editor        watch what i do  programming by demonstration  cambridge 
massachusetts  mit press 
gaines  b r         behaviour structure transformations under uncertainty  international journal
of man machine studies            
gold  m         language identification in the limit  information and control             
johansson  s   leech  g   and goodluck  h         manual of information to accompany the
lancaster oslo bergen corpus of british english  for use with digital computers 
oslo  department of english  university of oslo 

  

finevill manning   witten

knuth  d e         the art of computer programming    fundamental algorithms  addisonwesley 
laird  p    saul  r         discrete sequence prediction and its applications  machine learning
         
langley  p          simplicity and representation change in grammar induction  unpublished
manuscript  robotics laboratory  computer science department  stanford university 
stanford  ca 
nevill manning  c g    witten  i h  compression and explanation using hierarchical grammars 
computer journal  in press 
nevill manning  c g         inferring sequential structure  ph d  thesis  department of
computer science  university of waikato  new zealand 
nevill manning  c g   witten  i h    paynter  g w         browsing in digital libraries  a
phrase based approach  proc  second acm international conference on digital
libraries          philadelphia  pa 
rabiner  l r  and juang  b h         an introduction to hidden markov models  ieee assp
magazine            
stolcke  a     omohundro  s          inducing probabilistic grammars by bayesian model
merging  proc  second international conference on grammatical inference and
applications          alicante  spain  springer verlag 
vanlehn  k     ball  w          a version space approach to learning context free grammars 
machine learning          
wharton  r  m          grammar enumeration and inference  information and control             

wolff  j g         an algorithm for the segmentation of an artificial language analogue  british
journal of psychology           
wolff  j g         the discovery of segments in natural language  british journal of psychology 
          
wolff  j g         language acquisition and the discovery of phrase structure  language and
speech                
wolff  j g         language acquisition  data compression and generalization  language and
communication             

  

fi