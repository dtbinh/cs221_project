journal of artificial intelligence research               

submitted        published     

a new look at the easy hard easy pattern of
combinatorial search diculty
dorothy l  mammen

mammen cs umass edu

tad hogg

hogg parc xerox com

department of computer science
university of massachusetts
amherst  ma        u s a 
xerox palo alto research center
     coyote hill road
palo alto  ca        u s a 

abstract

the easy hard easy pattern in the diculty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in
number of solutions and increased pruning  we test the generality of this explanation by
examining one of its predictions  if the number of solutions is held fixed by the choice
of problems  then increased pruning should lead to a monotonic decrease in search cost 
instead  we find the easy hard easy pattern in median search cost even when the number of
solutions is held constant  for some search methods  this generalizes previous observations
of this pattern and shows that the existing theory does not explain the full range of the
peak in search cost  in these cases the pattern appears to be due to changes in the size of
the minimal unsolvable subproblems  rather than changing numbers of solutions 

   introduction

recently  many authors have shown that the solution cost for various kinds of combinatorial
search problems follows a pattern of easy hard easy as a function of how tightly constrained
the problems are  for example  this pattern appears for graph coloring as a function of
the average graph connectivity  cheeseman  kanefsky    taylor        hogg   williams 
       for propositional satisfiability  sat  as a function of the ratio of number of clauses to
number of variables  cheeseman et al         mitchell  selman    levesque        crawford
  auton        gent   walsh      b   and for constraint satisfaction problems  csps  as a
function of the number of nogoods  williams   hogg        and constraint tightness  smith 
      prosser        
this regularity raises the possibility of determining  prior to search  the likely diculty
of problems  unfortunately  this is not yet possible because of the high variance associated with the observations  this is compounded by the fact that a single problem can be
viewed as belonging to a variety of problem classes  each with somewhat different transition points  thus one important direction for improvement is to investigate whether there
are simple additional parameters that can reduce this variance and allow predictions with
higher confidence 
one approach to this question is based on the explanation of the easy hard easy pattern
as a competition between changes in the number of solutions and pruning of unproductive
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fimammen   hogg

search paths as a function of some measure of the degree to which the problems are constrained  in particular this predicts that problems with many solutions tend to be easier 
on average  than those with fewer for a given number of constraints  thus  at least one
aspect of the high variance in search cost appears to be due to the variance in number of
solutions in the problems of a fixed degree of constraint  this observation has motivated
the introduction of additional parameters describing problem structure based on a more
precise specification of the number of solutions  hogg        
in this paper we investigate the generality of this explanation by examining problems for
which the number of solutions is restricted  including cases where the number is specified
exactly to be either zero or one  if the peak in search cost in fact arises generally from a
competition between changes in the number of solutions and pruning  cases with a fixed
number of solutions should not show a peak  however  we find that a peak continues to
appear in these cases for some sophisticated search algorithms  while it fails to appear in
other cases  this calls into question the generality of the explanation based on number of
solutions  and also suggests that a search for additional problem structure parameters based
solely on reducing the variance in the number of solutions is not likely to be sucient to
accurately predict search cost  however  some structural aspect of problems is likely to be
involved  specifically  we present data showing that the smallest of the problem s minimal
unsolvable subproblems correlates well with search cost 
in the next section we describe some classes of search problems  we then review the
pattern of search behavior and the current theoretical explanation for it  in the following
section we uncover some limitations of this explanation by examining problems with some
specification on their number of solutions  this shows the easy hard easy pattern is a more
general phenomenon than suggested by current explanations  we then suggest an alternative explanation related to problem structure  and present data for unsolvable problems
showing a positive relationship between this problem structure parameter  the minimum
size of minimal unsolvable subproblem  and search cost  this same problem structure parameter may explain differences in search cost among solvable problems with equal numbers
of solutions  as well  finally  we discuss some of the implications of these observations and
make suggestions for obtaining a better understanding and greater predictability for hard
search problems 

   some classes of search problems

in common with many previous studies of the transition phenomenon  we use random binary
csps and graph coloring as example classes of search problems  this section describes how
the problems were generated and searched 

    random csps

the constraint satisfaction problems used in most of our experimental results consist of   
variables with three possible values for each one  and in some cases  we repeated experiments
with problems of    variables  problem constraints are specified by a number of binary
nogoods  i e   assignments to a pair of variables that are considered to be inconsistent 
the search problem is then to find a consistent complete assignment  i e   a value for each
variable that does not include any of the inconsistent pairs 
  

fithe easy hard easy pattern of combinatorial search difficulty

we generated problems in a number of ways to fully sample the range of behaviors  in
the first method   generate select   we generate csps by randomly selecting the specified
number of binary nogoods  to produce classes of problems with restrictions on their number
of solutions  we determine the number of solutions of these randomly generated problems
and retain only those satisfying these restrictions  for example  to produce a class of
solvable problems  only those with a solution are included  similarly  to produce a class of
problems with a fixed number of solutions  only those problems with exactly the specified
number of solutions are retained 
this random generation method gives a simple  uniform selection from the various problem classes  however  it can also be very inecient in generating problems  for instance 
with few nogoods  most randomly generated problems are solvable  hence requiring a large
number of random trials to obtain even a few unsolvable cases 
to address this problem  we also used more ecient   hill climbing   methods  specifically  for generating solvable problems with many nogoods  starting with a randomly generated unsolvable problem  we removed constraints at random until the problem became
solvable  then restored the number of constraints removed with constraints chosen randomly 
but with the requirement that the problem not become unsolvable again 
for generating unsolvable problems with few nogoods  the hill climbing method started
with a randomly generated solvable problem  removed the constraint that constrained the
problem the least  the one whose removal increased the number of solutions the least   and
added a randomly chosen constraint that resulted in a problem with fewer solutions than
the problem had before the constraint removal  if  having removed one constraint  no other
constraint could decrease the number of solutions  the constraint that increased the number
of solutions the least was chosen   a slightly backwards step  to speed this process up  we
checked only one third of the possible constraints before giving up  choosing the one that
increased the number of solutions the least  and starting another iteration 
other methods for generating problems with specified requirements on the number of
solutions have also been studied  one popular method for solvable problems is to randomly
select an assignment to all of the variables  a pre specified solution  and then  during the
random selection of nogoods  avoid any that are inconsistent with this pre specified solution 
this tends to emphasize problems with many solutions and results in instances that are
somewhat easier than uniform random selection  cha   iwama        have also used the
approach of generating problems with specific attributes  for sat problems  using the aim
generators  asahiro  iwama    miyano        
we solved these problems using dynamic backtracking  ginsberg        in most cases 
using random variable and value ordering  for comparison  we also did some searches with
simple chronological backtrack instead  the search cost is measured as the number of nodes
explored 

    graph coloring

we also experimented with the   coloring problem  this constraint satisfaction problem
consists of a graph and the requirement to assign each node one of three colors so that no
pair of nodes linked by an edge have the same color  each edge in the graph defines some
binary nogoods for the problem  namely all pairs of assignments giving the same color to the
  

fimammen   hogg

two nodes connected by the edge  thus each edge in the graph gives three binary nogoods 
a convenient measure of the number of constraints is    the connectivity or average degree
of the nodes in the graph  this is equal to twice the number of edges in the graph divided
by the number of nodes  because each edge is incident on two nodes  for the     node
graphs we studied  the number of binary nogoods is given by      
in this case  we used a simple chronological backtrack search in combination with the
brelaz heuristic for variable and value ordering  johnson  aragon  mcgeoch    schevon 
       this heuristic assigns the most constrained nodes first  i e   those with the most
distinctly colored neighbors   breaking ties by choosing nodes with the most uncolored
neighbors  and with any remaining ties broken randomly  the colors are considered in a
fixed ordering for all of the nodes in the search  as a simple optimization  the search never
changes the colors selected for the first two nodes  any such changes would amount to
unnecessarily repeating the search with a permutation of the colors for unsolvable cases 
search cost is measured by the number of nodes explored 

   the easy hard easy pattern

in this section  we present an example of the how search cost varies with the tightness
of constraints for a class of problems  and describe how this behavior can be understood
in terms of changes in the structure of the problems  independent of particular search
algorithms  this review and summary of previous studies of the transition then forms a
basis for comparison with the new results presented in subsequent sections 

    an example

figure   shows a typical example of the easy hard easy pattern as a function of the constrainedness of the problem  problems with few or many constraints tend to be easy to
solve while those with an intermediate number are more dicult  the fraction of solvable
problems is also shown in figure    scaled from     on the left to     on the right  this
illustrates that the hard problems are concentrated in the so called  mushy region   smith
  dyer        where the probability of a solution is changing from     to      in particular 
the peak in search cost is near the  crossover point   the point at which half the problems
are solvable and half unsolvable  for this problem class  the crossover point occurs at just
over    binary nogoods  and the peak in dynamic backtracking solution cost occurs at about
   binary nogoods 
in all of our results in this paper  we include     confidence intervals  snedecor  
cochran         these intervals for the estimate of the p
median obtained from our samples
are given approximately by the percentiles          n of the data  where n is the
number
of samples  for the estimate of fractions the intervals are given approximately by
p
of the fraction  finally  for the estimate
f    f      f   n   where f is the estimated value p
of means the intervals are approximately x        n where x is the estimate of the mean
and  the standard deviation of the sample  in many cases in this paper  there are sucient
samples to make these intervals smaller than the size of the plotted points 
a key point from examples such as this is that the dicult instances within a class of
search problems tend to be concentrated near a particular value of the constraint tightness
 here measured by the number of binary nogoods   because this behavior is seen for a
  

fithe easy hard easy pattern of combinatorial search difficulty

cost
   

   

   

  

  

  

  

  

   

   

   

nogoods

figure    typical transition pattern  median solution cost for dynamic backtracking  solid line 

and probability of a solution  dashed line  as a function of number of nogoods  each
point represents      problems of    variables and domain size    each solved     times 
error bars showing     confidence intervals are included  but are in some cases smaller
than the size of the plotted points 

variety of search methods  it indicates this concentration does not depend much on the
details of the search algorithm  instead  it appears to be associated with a change in the
properties of the problems themselves  namely their solvability 

    an explanation

these observations raise a number of questions  such as why a peak in search cost exists 
why the peak occurs near the transition from mostly solvable to mostly unsolvable problems
and is thus independent of the particular search algorithm  and why this behavior is seen
for a large variety of constraint satisfaction problems 
the existing explanation for the concentration of hard problems relies on a competition
between changes in the number of solutions and the amount of pruning provided by the
problem constraints  williams   hogg         with few constraints  there are many solutions so the search is usually easy  as constraints are added the number of solutions drops
rapidly  making problems harder  but the new constraints also increase the pruning of unproductive search choices  tending to make search easier  when there are few constraints 
the decrease in the number of solutions overwhelms the increase in pruning  giving harder
problems on average  eventually the last solution is eliminated and all that remains is the
increased pruning from additional constraints  leading to easier problems  thus the phase
transition  the point at which there is a precipitous change from solvability to unsolvability  more or less coincides with the peak in solution cost  all these effects become more
pronounced as larger problems are considered  leading to sharper peaks and more abrupt
  

fimammen   hogg

transitions  this qualitative description explains many features of the observed behavior 
this pruning explanation was also offered by cheeseman et al         with respect to
finding hamiltonian circuits in highly constrained problems 
this explanation can also be used to obtain a quantitative understanding of the behavior 
for instance  the location of the transition region can be understood by an approximate
theory predicting that the cost peak occurs when the expected number of solutions equals
one  smith   dyer        williams   hogg         in our example
there are     possible
     
assignments to the    variables in the problem  there are           possible binary
nogoods for the problem  which counts the number of ways to select a pair of variables and
the different assignments for that pair  a given complete assignment for the    variables will
be a solution provided each of the selected binary nogoods does not use the  same
assignment

for its pair of variables as in the given complete assignment  this leaves                   
possible choices for the binary nogoods  thus the expected number of solutions is given by
    
  
m
       
m

for problems with m randomly selected binary nogoods  this expression equals one at
m         the location of the observed cost peak  furthermore  because the expected
number of solutions grows exponentially with the number of variables when m is smaller
than this threshold value and decreases exponentially to zero when m is larger  the range
of m values over which the expected number of solutions is near one rapidly decreases as
variables are added  this accounts for the observed sharpening of the transition for larger
problems 
a further quantitative success of relating the search cost peak to transition phenomena
is the evaluation of scaling behavior of the transition and search cost peak  kirkpatrick  
selman        gent  macintyer  prosser    walsh        

   search diculty and solvability

in this section we take a closer look at the behavior of the search cost  specifically  by
examining how the behavior depends on whether the problem has a solution and  if so  the
number of solutions 

    search behavior

figure   shows the median dynamic backtracking solution cost for solvable and unsolvable
random csps generated as described above  for problems with number of variables n     
and n       with domain size three  except where specified otherwise in the figure caption 
for problems of    variables we generated      solvable and      unsolvable problems for
each point  and for problems of    variables we generated     solvable and     unsolvable
problems for each point  using the  generate select  method  we also generated unsolvable
problems of    variables with    to    nogoods using the  hill climbing  method  we
overlap the range of problems generated by the two methods to show how the different
generation methods affect search cost 
this figure clearly shows the easy hard easy pattern of solution cost for both solvable
and unsolvable problems  for both problem sizes  the two methods of generating unsolvable
  

fithe easy hard easy pattern of combinatorial search difficulty

cost

 
  

 
  

 

 

 

 

  

  

  

m n

figure    median solution cost using dynamic backtracking for solvable  solid lines  and unsolvable
 dashed and dotted lines  problems with number of variables n       black lines  and
n       gray lines  as a function of number of nogoods divided by problem size  m n 

all problems were generated using the  generate select  method except for the unsolvable problems shown by the dotted line  which were generated using the  hill climbing 
method  for problems of size     each point is the median of      problems solved    
times  except for unsolvable problems generated by  generate select  at m n        
nogoods  and solvable problems at m n           nogoods   which are based on    
problems  for problems of size     each point is the median of     problems solved    
times  except for unsolvable problems at m n          nogoods  and solvable problems
at m n           nogoods   which are based on    and    problems  respectively  error
bars showing     confidence intervals are included  but in most cases are smaller than
the size of the plotted points 

problems give distinct curves  the unsolvable problems generated by the  hill climbing 
method are harder than those generated by the  generate select  method  nonetheless 
both sets of problems show the same easy hard easy pattern 
another example with the same behavior is shown in figure   for the median search
cost for instances of   coloring of random graphs  in contrast to figure    the solvable
and unsolvable cases have similar median search costs near the peaks  this is because  as
described above  the graph coloring searches for unsolvable cases used the symmetry with
respect to permutations of the colors to avoid unnecessary search  without this optimization  the costs for unsolvable cases would be six times greater than the values shown in the
figure  similar peaks are seen for other classes of graphs  such as connected ones  although
at somewhat different values of   
these data show that both random csps and graph coloring problems exhibit an easyhard easy pattern for solvable and unsolvable problems considered separately 
  

fimammen   hogg

cost
   
   
   
   
  
 

 

 

 

 

 

 

 



figure    median solution cost for   coloring random graphs with     nodes as a function of connectivity  using backtrack search with the brelaz heuristic  the solid and dashed curves

correspond to solvable and unsolvable cases respectively  these results started with
        random graphs at each value of    and additional samples were generated at
the extremes to produce at least     samples for each point  for random graphs  the
crossover from mostly solvable to mostly unsolvable occurs around a connectivity of     
error bars showing     confidence intervals are included 

    solvable problems

a peak in search cost for solvable problems such as we observed has also been seen extensively in studies of local repair search methods and for problems generated with a prespecified solution  yugami  ohta    hara        kask   dechter        williams   hogg 
       these search methods start with some assignment to all of the variables in the
problem and then attempt to adjust them until a solution is found  generally  such methods are not systematic searches  they can never determine that a problem has no solution 
thus empirical studies of these methods are restricted to consider solvable problems and
incidentally provide a useful examination of the properties of solvable problems 
furthermore  a study of satisfiability problems with backtracking search is consistent
with a peak in cost for solvable problems  mitchell et al          but there were insucient
highly constrained solvable problems to make a definite conclusion for the behavior with
many constraints 
how does the existence of a peak for solvable problems fit with the explanation given
above  certainly an explanation based on a transition from solvable to unsolvable problems
cannot apply directly to the class of solvable problems  however  the competition between
increased pruning and decreased number of solutions still applies  as shown in figure   
the number of solutions for solvable random csps of size    at first decreases rapidly as
constraints are added but then nears its minimum value of one  giving a slower decrease 
  

fithe easy hard easy pattern of combinatorial search difficulty

solutions
 
  

 
  
 

  

  

  

  

   

   

   

nogoods

figure    mean  solid  and median  dashed  number of solutions on a log scale as a function of the

number of binary nogoods  for solvable problems with    variables    values each  based
on      problems generated by the  generate select  method at each multiple of    binary
nogoods  except for     nogoods  which is based on     problems  at   nogoods there
are             solutions  error bars showing     confidence intervals are included 

except for the change in minimum value from   to   solution  this behavior for the number
of solutions is qualitatively similar to that for the general case including both solvable
and unsolvable problems  the additional constraints continue to increase the pruning of
unproductive search paths  thus the explanation given above might continue to apply but
now predicts the peak will be at the point where solutions can drop no further  i e   one
solution  rather than becoming unsolvable  i e   zero solutions  
figure   evaluates this idea  this figure shows how the fraction of problems with at
least two solutions changes as a function of the number of nogoods divided by the problem
size for random csps with    and    variables  for problems of size     the second to
last solution disappears  on average  between    and     nogoods  the median number
of solutions has dropped to   by    nogoods  and to   by     nogoods  figure     the
peak in solution cost for solvable problems is slightly lower than this  at between    and
   nogoods  close to the crossover point of figure   where half the solvable problems have
only one solution  this is perhaps close enough to be consistent with the explanation given
above  however  this relationship does not hold for problems of size     for this class of
problems  the cost peak of solvable problems is at around     nogoods  m n       whereas
the point at which half the problems have just one solution has still not been reached by
    nogoods  m n        at     nogoods  the median number of solutions is    mean is
       and at     nogoods  the median is still    mean is        this is inconsistent with
the explanation that the cost peak for solvable problems is due to the increasing effect of
pruning given no possible further decrease in number of solutions 
  

fimammen   hogg

fraction
 
   
   
   
   

 

 

 

 

  

  

  

m n

figure    fraction of problems with at least two solutions as a function of number of nogoods di 

vided by problem size  for problems of size     black line  and size     gray line   data for
problems of size    are based on      solvable problems created by the  generate select 
method at each point  except for     solvable problems at m n           nogoods  
data for problems of size    are based on     solvable problems at each point  except for
   solvable problems at m n           nogoods   also created by the  generate select 
method  error bars showing     confidence intervals are included 

since the explanation depending on a change to insolubility does not apply  and the
pruning versus number of solutions explanation does not fit the data  some other factors
must be at work to produce the easy hard easy pattern for solvable problems  we suspect the explanation is related to the idea of minimal unsolvable subproblems  a minimal
unsolvable subproblem is a subproblem that is unsolvable  but for which any subset of variables and their associated constraints is solvable  gent   walsh        have referred to
this aspect of sat problems as the minimal unsatisfiable subset  the idea is that once a
few bad choices have been made initially  such that the remainder of the problem becomes
unsolvable  unsolvability is much harder to determine for some problems than for others 
in particular  the more variables that are involved in a minimal unsolvable subproblem 
the harder it is to determine that the subproblem is unsolvable  we make the conjecture
that the cost peak for solvable problems is tied to the average size of the minimal unsolvable subproblem once a choice has been made that results in the remaining problem being
unsolvable 

    problems with a fixed number of solutions

a more interesting case is the behavior of the problems with no solutions shown in figures
  and    as a further example  figure   shows the solution cost for problems with exactly
one solution  this also shows a peak  these observations on problems with zero or one
  

fithe easy hard easy pattern of combinatorial search difficulty

cost
   
   
   
   
  
  
  

  

  

   

   

   

nogoods

figure    median solution cost as a function of number of nogoods for problems of    variables 

  values each  with exactly one solution  generated using the  generate select  method
 solid line   and by hill climbing down to one solution starting from solvable problems
with many solutions produced using  generate select   dotted line   solved using dynamic
backtracking  each point is the median of      problems each solved     times  except
for hill climbing generated problems at        and    nogoods and  generate select 
generated problems at     nogoods  of which there are      error bars showing    
confidence intervals are included 

solution show that even with the number of solutions held constant  problems exhibit an
easy hard easy pattern of solution cost 
according to the explanation of the transition  if the number of solutions is held constant
then the increase in pruning will be the only factor  giving rise to a monotonic decrease
in search cost as constraints are added  instead  we see in figures      and   that even
when the number of solutions is held fixed at zero or one  there is still a peak in solution
cost  and at a smaller number of nogoods  thus the existing explanation does not capture
the full range of behaviors  instead  it appears that there are other factors at work in
producing hard problems  by focusing more closely on these factors we can hope to gain
a better understanding of the structure of hard problems  which may lead to more precise
predictions of search cost 
we also investigated the effect of algorithm on the pattern of solution cost in unsolvable
problems by repeating the search of random csps using chronological backtrack  a comparison of chronological backtracking search with our previous dynamic backtrack search
results for unsolvable problems is shown in figure    in this figure  the curves for dynamic backtracking are the same as those for the unsolvable problems shown in figure   
except that here the cost curves are shown on a logarithmic scale  interestingly  we do not
see a peak in search cost for unsolvable problems using the less sophisticated method of
chronological backtrack 
  

fimammen   hogg

cost
 
  

 
  

 
  
  

  

  

  

   

   

   

nogoods

figure    comparison of median solution cost on a log scale using the same sets of unsolvable

problems for chronological backtracking  black  and dynamic backtracking  gray   dotted
lines are for problems generated using the  hill climbing  method  solid lines for the
 generate select  method  each point is the median of      problems each solved    
times  except for the  generate select  method at    nogoods  which is based on    
problems  error bars showing     confidence intervals are included  but are smaller
than the size of the plotted points 

this observation raises an important point  the easy hard easy pattern is not a universal
feature of search algorithms for problems restricted to a fixed number of solutions  this
suggests that the competition between number of solutions and pruning  when it occurs 
is suciently powerful to affect most search algorithms  very simple methods  such as
generate and test  do not make use of pruning and show a monotonic increase in search
cost as the number of solutions decreases   but that only some algorithms are able to
exploit the features of weakly constrained problems with a fixed number of solutions that
make them easy 
in contrast to our observations  a monotonic decrease in cost has been reported for
unsolvable binary random constraint problems  smith   dyer        and for unsolvable
 sat problems  mitchell et al          in the case of  sat  the explanation may well be
choice of algorithm  indeed  bayardo   schrag        recently found that incorporating
conict directed backjumping and learning into the tableau algorithm made a difference of
many orders of magnitude in problem diculty specifically for rare   exceptionally hard  
unsatisfiable problems in the underconstrained region  it would be interesting to see whether
the easy hard easy pattern for unsolvable problems would appear using their algorithm 
with respect to smith   dyer s        observations  the difference may be due to the
range of problems generated  resulting from different problem generation methods  smith
and dyer used a method akin to our  random  generation method  that is  generating
  

fithe easy hard easy pattern of combinatorial search difficulty

problems without regard for solvability  then separating out the unsolvable ones  with this
method  the  hit rate  for unsolvable problems in the underconstrained region is very low 
it is possible that smith and dyer s data do not extend down to the point at which the cost
of unsolvable problems begins to decrease simply because they stopped finding unsolvable
problems before that point 
there are two possible reasons why we might have found unsolvable problems using
random generation further into the underconstrained region  where smith and dyer did
not  one possibility is that since we were specifically interested in unsolvable problems as
far into the underconstrained region as possible  we may have spent more computational
effort generating in that region  indeed  at    nogoods  unsolvable problems occurred with
frequency             and at    nogoods  with frequency              at that rate  problems
at    nogoods took about six hours apiece to generate 
a second possibility relates to the details of the generation methods  in smith and
dyer s random generation method  every pair of variables had exactly the same number
of inconsistent value pairs between them  this implies a degree of homogeneity in the
distribution of the nogoods  on the other hand  in our random generation method  each
variable value pair had an equal probability of being selected as a nogood  independent of
one another  thus it was at least possible in our generation method  though still of low
likelihood  for the nogoods to occasionally clump  and to produce an unsolvable problem 
this idea is discussed further in section   
the difference in our observation and smith   dyer s        reinforces an important
point  that a relatively subtle difference in generation methods can affect the class of
problems generated  while the nogoods will be more or less evenly distributed on average
using our generation method  they will also be clumped with some probability  whereas
with smith and dyer s generation method  a homogeneous distribution over variable pairs
is guaranteed  these types of problems could be different enough to sometimes produce
different behavior 

   minimal unsolvable subproblems

our observations on classes of problems with restrictions on the number of solutions they
may have shows that the common identification of the peak in solution cost with the
algorithm independent transition in solvability seen in general problem classes does not
capture the full generality of the easy hard easy pattern 
for solvable problems  this explanation could be readily modified to use a transition
in the existence of solutions beyond those specified by the construction of the class of
problems and symmetries those problems might have that constrain the allowable range of
solutions  this modification is a simple generalization of the existing explanation based
on the competition between the number of solutions and pruning  however  our data
for solvable problems do not support this explanation  in that the search cost peak and
disappearance of the second to last solution coincide only roughly for n       and not at
all for n      
furthermore  when the number of solutions is held constant  competition between increased pruning and decreasing number of solutions cannot possibly be responsible for a
peak in solution cost  the decrease in search cost for highly constrained problems  to
  

fimammen   hogg

the right of the peak  is adequately explained by the prevailing explanation  based on the
increase in pruning with additional constraints  but this does not explain why weakly constrained problems are also found to be easy  at least for some search methods  the low cost
of unsolvable problems in the underconstrained region is a new and unexpected observation
in light of previous studies of the easy hard easy pattern and its explanation  this raises
the question of whether there is a different aspect of problem structure that can account
for the peak in search cost for problems with a fixed number of solutions 
one possibility that is often mentioned in this context is the notion of critically constrained problems  these are problems just on the boundary between solvable and unsolvable problems  i e   neither underconstrained  with many solutions  nor overconstrained
 with none   this notion forms the basis for another common interpretation of the cost
peak  that is  these critically constrained problems will typically be hard to search  because most of the constraints must be instantiated before any unproductive search paths
can be identified  and  since they are concentrated at the transition  smith   dyer        
give rise to the search peak  this explanation does not include any discussion of the changes
in pruning capability as constraints are added  taken at face value  this explanation would
predict no peak at all for solvable problems or when the number of solutions is held constant 
because such classes have no transition from solvable to unsolvable problems  moreover 
this description of critically constrained problems is not simply a characteristic of an individual problem but rather is partly dependent on the class of problems under consideration
because the exact location of the transition depends on the method by which problems
are generated  this observation makes it dicult to characterize the degree to which an
individual problem is critically constrained purely in terms of structural characteristics of
that problem  by contrast  a measure such as the number of solutions is well defined for
individual problem instances  which facilitates using its average behavior for various classes
of problems to approximately locate the transition region  thus  as currently described 
the notion of critically constrained problems does not explain our observations nor does it
give an explicit way to characterize individual problems 
a more precisely defined alternative characteristic is the size of minimal unsolvable subproblems  as we mentioned in section      a minimal unsolvable subproblem is a subproblem
that is unsolvable  but for which any subset of variables and their associated constraints is
solvable 
some problems have more than one minimal unsolvable subproblem  for example  a
problem might have one minimal unsolvable subproblem of five variables  and another  different one  of say  six  we computed all minimal unsatisfiable subproblems for all of the
   variable unsolvable problems we had generated  we found a monotonic positive relationship between mean number of minimal unsolvable subproblems and number of nogoods  for
example  problems with     nogoods have an average of    minimal unsolvable subproblems
 range   to     standard deviation       those with    nogoods have about six  range   to
    standard deviation       and problems with    or fewer nogoods rarely have more than
one minimal unsolvable subproblem  similarly  gent   walsh        observed that unsatisfiable problems in the underconstrained region tend to have small and unique minimal
unsatisfiable subsets 
the behavior of the size of the smallest minimal unsolvable subproblem as a function of
the number of nogoods is shown in figure    comparing with figure    we see that the peak
  

fithe easy hard easy pattern of combinatorial search difficulty

size
  
 
 
 
 
 
 
 

  

  

  

  

   

   

nogoods
   

figure    mean size of smallest minimal unsolvable subproblem as a function of number of nogoods  for unsolvable problems generated using the  hill climbing   dotted line  and
 generate select   solid line  methods  each point is based on      problems  except for
the  generate select  method at    nogoods  which is based on     problems  error bars
showing     confidence intervals are included 

in the minimum size of minimal unsolvable subproblems matches the location of the search
cost peak for unsolvable problems  this result is independent of whether we plot the smallest
minimal unsolvable subproblem size  as shown in figure    or medians or means  which we
have not shown here  moreover  the location of the peaks in minimal unsolvable subproblem
size for the different generation methods correspond to the location of their respective
search cost peaks  the peak in both search cost and minimal unsolvable subproblem size
occurs at around    nogoods for problems generated using the  hill climbing  method 
and significantly higher  around    nogoods  for problems generated using the  generateselect  method  the strong correspondence between minimal unsolvable subproblem size
and search cost is very suggestive that minimal unsolvable subproblem size is a structural
characteristic of problems that plays an important role in search cost  by contrast  number
of minimal unsolvable subproblems does not match the pattern of search cost  as mentioned
above  it increases monotonically with number of nogoods  suggesting that it does not play
a primary role in explaining search cost for unsolvable problems 
the behavior of the minimal unsolvable subproblem size as a function of the number
of constraints has a simple explanation  unsolvable weakly constrained problems will generally need to concentrate most of the available constraints on a few variables in order to
make all assignments inconsistent  this will tend to give one small minimal unsolvable
subproblem  as more constraints are added  this concentration is no longer required and 
since problems where most of the randomly selected constraints happen to be concentrated
on a few variables are rare  we can expect more and larger minimal unsolvable subproblems 
  

fimammen   hogg

cost
   

   

   

   

   

 

 

 

  

size

figure    mean solution cost as a function of size of smallest minimal unsolvable subproblem  for
unsolvable problems with    nogoods generated using the  generate select  method  each
point is the mean of the median solution costs  based on solving each problem     times 
for the set of problems with the corresponding smallest minimal unsolvable subproblem
size  the points are based on following numbers of problems for each smallest minimal
unsolvable subproblem size  totaling      problems                                     
                                error bars showing     confidence intervals are included 
except for the single problem at size   for which confidence intervals cannot be calculated 

finally  as more and more constraints are added  the increased pruning is equivalent to the
notion that instantiating only a few variables is all that is required to find an inconsistency 
this means we can expect a large number of small unsolvable subproblems  this qualitative
description corresponds to what we observe in figure   
our observations of weakly constrained problems suggest that some search algorithms 
such as dynamic backtracking  are able to rapidly focus in on one of the unsolvable subproblems and hence avoid the extensive thrashing  and high search cost  seen in other methods 
in such cases  one would expect that the smaller the unsolvable subproblem  the easier it
will be for the search to determine there are no solutions 
in order to examine the role of minimal unsolvable subproblem in search cost more
closely  we plotted mean search cost versus size of smallest minimal unsolvable subproblem
for unsolvable problems of    variables at each multiple of    nogoods from    to    
nogoods  in every case  mean search cost increased as a function of size of smallest minimal
unsolvable subproblem  figure   shows an example of one of these plots  at the peak
in solution cost for this class of problems     nogoods  it makes sense that the smallest
minimal unsolvable subproblem  being the easiest to detect  would play a significant role
in search cost  however  the situation is surely more complicated than this  suggested by
the fact that there is still variation among problems with the same size smallest minimal
  

fithe easy hard easy pattern of combinatorial search difficulty

unsolvable subproblem  this could be due  for example  to one problem having several
small minimal unsolvable subproblems  while another might have one minimal unsolvable
subproblem  even smaller  number and size of minimal unsolvable subproblems are both
likely to play a role in search cost 
number of minimal unsolvable subproblems does not seem to play as significant a role
as size of smallest minimal unsolvable subproblem  but its effect can also be demonstrated 
for the same sets of unsolvable problems as above  for each multiple of    nogoods from
   to     nogoods  search cost correlates negatively with number of minimal unsolvable
subproblems  however  for unsolvable problems with    to    nogoods  where variance in
number of minimal unsolvable subproblems is lower  but variance in search cost is higher  
there is no relationship between search cost and number of minimal unsolvable subproblems  additional clarification of the role in search cost of both size and number of minimal
unsolvable subproblems is left for further investigation  but size of smallest minimal unsolvable subproblem  which correlates strongly with search cost for     unsolvable problems
taken as a whole  see figures   and    and     unsolvable problems with a fixed number of
nogoods over the full range of number of nogoods  appears to have the more primary effect 
this discussion of minimal unsolvable subproblems is also relevant to solvable problems 
once a series of choices that precludes a solution is made during search  the remaining subproblem is now an unsolvable one  for example  in a    variable csp  suppose values are
given to the first two variables that are incompatible with all solutions to the problem  this
means that in the context of these two assignments  the remaining eight variables constitute an unsolvable subproblem  the number of search steps required to determine that this
subproblem is in fact unsolvable will be the cost added to the search before backtracking to
the original two variables and trying a new assignment for one of them  thus  the cost of
identifying unproductive search choices for solvable problems is determined by how rapidly
the associated unsolvable subproblem can be searched  as described above  when there are
few constraints we can expect that such unsolvable subproblems will themselves have small
minimal unsolvable subproblems and hence be easy to search with methods that are able
to focus on such subproblems  while the unsolvable subproblems associated with incorrect
variable choices in solvable problems may have a different structure  this argument suggests that changes in minimal unsolvable subproblems may explain the behavior of solvable
problems with a fixed number of solutions as well  this could also explain observations of
thrashing behavior for rare exceptionally hard solvable problems in the underconstrained
region  gent   walsh      a  hogg   williams         we would expect such problems
to have a relatively large unsolvable subproblem to detect given the initial variable assignments  finally  it would be interesting to study the behavior of local repair search methods
for problems with a single solution to see if they also are affected by the change in minimal
subproblem size 

   conclusions

we have presented evidence that the explanation of the easy hard easy pattern in solution
cost based on a competition between changes in the number of solutions and pruning is
insucient to explain the phenomenon completely for sophisticated search methods  it
does explain the overall pattern for problems not restricted by solvability or number of
  

fimammen   hogg

solutions  however  the explanation fails when the number of solutions is held constant
and sophisticated search methods are used  in these cases the solution cost peak does not
disappear as would be predicted  alternatively  we can view this explanation as adequate for
less sophisticated methods that are not able to readily focus in on unsolvable subproblems
encountered during the search 
by considering relatively small search problems  we are able to exhaustively examine
the properties of the search space  this allowed us to definitively demonstrate the importance for search behavior of an aspect of problem structure  the size of minimal unsolvable
subproblems  our approach contrasts with much work in this area that involves solving
problems as large as feasible within reasonable time bounds  while the latter approach
gives a better indication of the asymptotic behavior of the transition  it is not suitable
for exhaustive evaluation of the nature of the search spaces encountered  nor for detailed
analysis of aspects of individual problem structure 
we believe that detailed examination of the structure of combinatorial problems can
yield information about why certain types of problems are dicult or easy  as a class  graph
coloring or random csps are np complete  yet in practice many such problems are actually
very easy  in addition  while theoretical work in this area has produced predictions that are
asymptotically correct on average  the variance among individual problems in a predicted
class is enormous  increased understanding of the relationships between problem structure 
problem solving algorithm  and solution cost is important to determining whether  and if so 
how  we can determine prior to problem solving which problems are easy versus infeasibly
hard  in contrast to previous theoretical studies that focus on the number of solutions  this
work suggests that the size of minimal unsolvable subproblems is an alternate characteristic
to study with the potential for producing a more precise characterization of the transition
behavior and the nature of hard search problems 

acknowledgements
much of this research was carried out while the first author was a summer intern at xerox
palo alto research center  this research was also partially supported by the national
science foundation under grant no  iri         to victor r  lesser  any opinions 
findings  and conclusions or recommendations expressed in this material are those of the
authors and do not necessarily reect the views of the national science foundation 

references

asahiro  y   iwama  k     miyano  e          random generation of test instances with
controlled attributes  in second dimacs challenge workshop 
bayardo  jr   r  j     schrag  r          using csp look back techniques to solve exceptionally hard sat instances  in freuder  e  c   ed    principles and practice of
constraint programming   cp    pp        cambridge  ma  springer 
cha  b     iwama  k          performance test of local search algorithms using new
types of random cnf formulas  in proceedings of the fourteenth international joint
  

fithe easy hard easy pattern of combinatorial search difficulty

conference on artificial intelligence  pp          montreal  quebec  canada 

cheeseman  p   kanefsky  b     taylor  w          where the really hard problems are  in
proceedings of the twelfth international joint conference on artificial intelligence 
pp          sydney  australia 
crawford  j  m     auton  l  d          experimental results on the cross over point
in satisfiability problems  in proceedings of the eleventh national conference on
artificial intelligence  pp        washington  dc  usa 
gent  i  p   macintyre  e   prosser  p     walsh  t          scaling effects in the csp phase
transition  in montanari  u     rossi  f   eds    proc  of principles and practices of
constraint programming ppcp    pp         springer verlag 
gent  i  p     walsh  t       a   easy problems are sometimes hard  artificial intelligence 
            
gent  i  p     walsh  t       b   the sat phase transition  in cohn  a   ed    proceedings
of the ecai     pp           john wiley and sons 
gent  i  p     walsh  t          the satisfiability constraint gap  artificial intelligence 
                
ginsberg  m  l          dynamic backtracking  journal of artificial intelligence research 
         
hogg  t          refining the phase transitions in combinatorial search  artificial intelligence              
hogg  t     williams  c  p          the hardest constraint problems  a double phase
transition  artificial intelligence              
johnson  d  s   aragon  c  r   mcgeoch  l  a     schevon  c          optimization by
simulated annealing  an experimental evaluation  part ii  graph coloring and number
partitioning  operations research                  
kask  k     dechter  r          gsat and local consistency  in proceedings of the fourteenth international joint conference on artificial intelligence  pp          montreal 
quebec  canada 
kirkpatrick  s     selman  b          critical behavior in the satisfiability of random
boolean expressions  science                 
mitchell  d   selman  b     levesque  h          hard and easy distributions of sat
problems  in proceedings of the tenth national conference on artificial intelligence 
pp          san jose  ca  usa 
prosser  p          an empirical study of phase transitions in binary constraint satisfaction
problems  artificial intelligence             
  

fimammen   hogg

smith  b  m          phase transition and the mushy region in constraint satisfaction
problems  in cohn  a   ed    proceedings of the ecai     pp           john wiley
and sons 
smith  b  m     dyer  m  e          locating the phase transition in binary constraint
satisfaction problems  artificial intelligence              
snedecor  g  w     cochran  w  g          statistical methods   th edition   iowa state
univ  press  ames  iowa 
williams  c  p     hogg  t          exploiting the deep structure of constraint problems 
artificial intelligence             
yugami  n   ohta  y     hara  h          improving repair based constraint satisfaction
methods by value propagation  in proceedings of the twelfth national conference on
artificial intelligence  pp          seattle  wa  usa 

  

fi