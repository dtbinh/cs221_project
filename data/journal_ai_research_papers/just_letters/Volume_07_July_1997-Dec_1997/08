journal artificial intelligence research                 

submitted       published      

dynamic non bayesian decision making
dov monderer
moshe tennenholtz

dov ie technion ac il
moshet ie technion ac il

industrial engineering management
technion   israel institute technology
haifa        israel

abstract

model non bayesian agent faces repeated game incomplete information nature appropriate tool modeling general agent environment
interactions  model environment state  controlled nature  may change arbitrarily  feedback reward function initially unknown  agent bayesian 
form prior probability neither state selection strategy nature 
reward function  policy agent function assigns action
every history observations actions  two basic feedback structures considered 
one   perfect monitoring case   agent able observe previous
environment state part feedback    imperfect monitoring
case   available agent reward obtained  settings
refer partially observable processes  current environment state unknown 
main result refers competitive ratio criterion perfect monitoring case 
prove existence ecient stochastic policy ensures competitive
ratio obtained almost stages arbitrarily high probability  eciency
measured terms rate convergence  shown optimal
policy exist imperfect monitoring case  moreover  proved
perfect monitoring case exist deterministic policy satisfies long
run optimality criterion  addition  discuss maxmin criterion prove
deterministic ecient optimal strategy exist imperfect monitoring case
criterion  finally show approach long run optimality viewed
qualitative  distinguishes previous work area 

   introduction
decision making central task artificial agents  russell   norvig        wellman 
      wellman   doyle         point time  agent needs select among
several actions  may simple decision  takes place once 
complicated decision series simple decisions made  question
 what right actions be  basic issue discussed settings 
fundamental importance design artificial agents 
static decision making context  problem  artificial agent consists set actions agent may perform  set possible environment states  utility reward
function determines feedback agent performs particular action
particular state  problem best represented matrix columns indexed
states  rows indexed actions rewards entries  reward
function known agent say agent payoff uncertainty
c      ai access foundation morgan kaufmann publishers  rights reserved 

fimonderer tennenholtz

refer problem problem incomplete information fudenberg   tirole        
modeling problem incomplete information one must describe underlying assumptions knowledge agent reward function  example 
agent may know bounds rewards  may know  or partially know  underlying
probabilistic structure    dynamic  multistage  decision making setup agent faces
static decision problems stages  stage agent selects action performed environment selects state  history actions states determines
immediate reward well next one shot decision problem  history actions
states determines next selected state  work reinforcement learning artificial intelligence  kaelbling  littman    moore        adopted view agent
operating probabilistic bayesian setting  agent s last action last state
determine next environment state based given probability distribution  naturally 
learner may a priori familiar probability distribution  existence
underlying probabilistic model key issue system s modeling  however 
assumption ultimate one  particular  much work areas ai
economics dealt non probabilistic settings environment changes
unpredictable manner    agent know uence choices
selection next state  i e   certain environment strategy  
say agent strategic uncertainty 
paper use general model representation agent environment interactions agent payoff strategic uncertainty  deal
non bayesian agent faces repeated game incomplete information nature 
repeated game nature agent faces static decision problem
stage environment state taken action chosen opponents 
decision problem called game stress fact agent s action
state independently chosen  fact game repeated refers fact
set actions  set possible states  one shot utility function
vary time    said  consider agent payoff uncertainty
strategic uncertainty  is  a priori ignorant utility function  i e  
game incomplete information  well state selection strategy nature 
agent non bayesian sense assume probabilistic model
concerning nature s strategy sense assume probabilistic
model concerning reward function  though may assume lower upper bounds   
consider two examples illustrate above mentioned notions model  consider
   example  agent may know probability distribution set reward functions  may assume
probability exists without assumption structure  may partial information
distribution ignorant parameters  e g   may believe reward
function drawn according normal distribution unknown covariance matrix  
   many intermediate cases assumed changes probabilistic nonmarkovian structure 
   general setup  sets may vary time  useful analysis done model
changes completely arbitrary 
   repeated games complete information  generally  multistage games stochastic games
extensively studied game theory economics  partial list includes   shapley 
      blackwell        luce   raiffa         recently  fudenberg   tirole        mertens 
sorin    zamir         evolving literature learning  e g   fudenberg   levine       
incomplete information setup player ignorant game played inspired

   

fidynamic non bayesian decision making

investor    investing daily certain index stock market  daily profits
depends action  selling buying certain amount  environment state
  percentage change price index  investor complete information
reward function knows reward realized particular
investment particular change  strategic uncertainty changes
index price  so  playing repeated game complete information
nature strategic uncertainty 
consider another investor     invests particular mutual fund  fund invests
stock market strategy known investor  assume
state represents vector percentage changes stocks  investor
know reward function  example  cannot say advance would profit
would buy one unit fund stock prices increase   percent  thus   
plays repeated game incomplete information  addition   attempt
construct probabilistic model concerning reward function market behavior 
non bayesian analysis may apply him  another example  assume bob
decide evening whether prepare tea coffee wife gets
home  wife wishes drink either tea coffee wishes ready her 
reaction bob s wife tea coffee may depend state day 
predicted based history actions states previous days  bob
got married cannot tell reward get wife happy makes
cup tea  course may eventually know it  decisions learning
period precisely subject paper 
example generality above mentioned setting  consider model
markov decision processes complete incomplete information  markov decision
process agent s action given state determines  in probabilistic fashion  next
state obtained  is  agent structural assumption state selection
strategy  repeated game nature without added assumptions captures fact
transition state state may depend history arbitrary way 
agent performs action state st   part feedback would u at  st   
u reward function  distinguish two basic feedback structures 
one   perfect monitoring case   agent able observe previous
environment state part feedback    imperfect monitoring
case   available agent reward obtained    notice
feedback structures  current state observed agent called
select action    investors   face repeated game perfect monitoring
percentage changes become public knowledge iteration 
example  bob make decision  situation imperfect
monitoring  bob would able observe reward behavior  e g   whether
 harsanyi         see aumann maschler        comprehensive survey 
literature deals  partially  bayesian agents  rare exceptions cited section   
   notice former assumption popular related game theory literature  aumann  
maschler         many intermediate monitoring structures may interesting well 
   case evolving literature problem controlling partially observable markov
decision processes  lovejoy        cassandra  kaelbling    littman        monahan         contrast 
q learning theory  watkins        watkins   dayan        sutton        assume knowledge
current state 

   

fimonderer tennenholtz

says  thanks    that s terrible    this exactly wanted   etc    perfect
monitoring case  bob able observe wife s state  e g   whether came home
happy  sad  nervous  etc   addition reward  cases bob  like investors 
able observe wife s state making decision particular day 
consider case one stage game nature  utility function
known  agent cannot observe current environment state selecting
action  agent choose action  work decision making uncertainty
suggested several approaches  savage        milnor        luce   raiffa        kreps 
       one approaches maxmin  safety level  approach  according
approach agent would choose action maximizes worst case payoff  another
approach competitive ratio approach  or additive variant  termed minmax
regret decision criterion  milnor         according approach agent would choose
action minimizes worst case ratio payoff could obtained
known environment state payoff would actually obtain   returning
back example  bob would known actual state wife  could choose
action maximizes payoff  since hint state  go ahead
choose action minimizes competitive ratio  example  action leads
competitive ratio two  bob guarantee payoff would get
half payoff could gotten known actual state wife 
given repeated game incomplete information nature  agent would
able choose one stage optimal action  with respect competitive ratio
maxmin value criteria  stage  since utility function initially unknown  so 
bob initially know reward would receive actions function
wife s state  able simply choose action guarantees best
competitive ratio  calls precise definition long run optimality criterion 
paper mainly concerned policies  strategies  guaranteeing optimal
competitive ratio  or maxmin value  obtained stages  interested
particular ecient policies  eciency measured terms rate convergence 
hence bob s case  interested policy adopted bob would guarantee
almost day  high probability  least payoff guaranteed action
leading competitive ratio  moreover  bob wait much
start getting type satisfactory behavior 
section   define mentioned setting introduce preliminaries 
sections     discuss long run competitive ratio criterion  section   show
even perfect monitoring case  deterministic optimal policy exist 
however  show exists ecient stochastic policy ensures
long run competitive ratio criterion holds high probability  section   show
stochastic policies exist imperfect monitoring case  section  
prove perfect imperfect monitoring cases exists deterministic
ecient optimal policy long run maxmin criterion  section   compare
notions long run optimality criteria appearing related literature 
particular  show approach long run optimality interpreted
   competitive ratio decision criterion found useful settings on line
algorithms  e g   papadimitriou   yanakakis        

   

fidynamic non bayesian decision making

qualitative  distinguishes previous work area  discuss
connections work work reinforcement learning 

   preliminaries
 one shot  decision problem  with payoff certainty strategic uncertainty    tuple
   a  s  u    finite sets u real valued function defined
u a  s      every  a  s      elements called actions
called states  u called utility function  interpretation numerical
values u a  s  context dependent    let na denote number actions a  let ns
denote number states let n   max na   ns   
above mentioned setting classical static setting decision making 
uncertainty actual state nature  luce   raiffa         paper
deal dynamic setup  agent faces decision problem d  without
knowing utility function u  infinite number stages                
explained introduction  setting enables us capture general dynamic nonbayesian decision making contexts  environment may change behavior
arbitrary unpredictable fashion  mentioned introduction  best captured
means repeated game nature  state environment point
plays role action taken nature corresponding game  agent knows
sets   know payoff function u   dynamic decision problem
 with payoff uncertainty strategic uncertainty  therefore represented agent
pair dd    a    finite sets    stage t  nature chooses state st    
agent  know chosen state  chooses   a  receives u at  st  
distinguish two informational structures  perfect monitoring case  state
st revealed agent alongside payoff u at  st   imperfect monitoring case 
states revealed agent  generic history available agent stage   
denoted ht   perfect monitoring case  ht   htp    a r   t   r  denotes
set positive real numbers  imperfect monitoring case  ht   htimp    a r   t 
particular case     assume h p   h imp   feg singleton containing
p
imp      h imp   symbol h
empty history e  let h p     
t   ht let h
t  
used h p h imp   strategy   agent dynamic decision problem
function f   h    a     a  denotes
p set probability measures a 
is  every ht   h   f  ht              a a f  ht   a       words 
agent observes history ht chooses at   randomizing amongst actions 
probability f  ht   a  assigned action a  strategy f called pure f  ht  
probability measure concentrated singleton every   
sections     strategy recommended agent chosen according  longrun  decision criterion induced competitive ratio one stage decision criterion 
   see discussion section   
   results paper remain unchanged agent initially know set   rather
upper bound ns  
    notice need include explicit transition function representation  due
fact non bayesian setup every transition possible 
    strategy decision theoretic concept  coincides term policy used control theory
literature  term protocol used distributed systems literature 

   

fimonderer tennenholtz

competitive ratio decision criterion  described below  may used agent
faces decision problem once  knows payoff function u well
sets    reasonable  decision criteria could used instead 
one maxmin decision criterion discussed section    another
minmax regret decision criterion  luce   raiffa        milnor         latter
simple variant competitive ratio  and treated similarly   therefore
treated explicitly paper 
every   let  s  maximal payoff agent get state s 

 s    max
u a  s  
a a
every     define

c a  s    um a  ss    

denote c a    maxs s c a  s   let





cr   min
c a    min
max c a  s   
a a
a a s s
cr called competitive ratio    a  s  u    action cr   c a 
called competitive ratio action  short cr action  agent chooses
  fraction could gotten 
cr action guarantees receiving least cr
 
known state s  is  u a  s  cr  s  every     agent cannot guarantee
bigger fraction 
long run decision problem  non bayesian agent form prior probability
way nature choosing states  nature may choose fixed sequence states or 
generally  use probabilistic strategy g  g   q    s    q     
t   qt  
t  nature viewed second player knows reward function 
  
 



 
t  
strategy may course refer whole history actions states given point
may depend payoff function 
payoff function u pair probabilistic strategies f  g  g depend u 
generate probability measure   f g u set infinite histories q     a   
endowed natural measurable structure  event b q  denote
probability b according  b   prob  b    precisely  probability
measure uniquely defined values finite cylinder sets  let   q   
st   q    coordinate random variables contain values actions
states selected agent environment stage  respectively   is 
at h    st h    st every h     a   s     a   s           q    every  
every   a   s            at   st      qt  

prob   at  st     at  st       


 



t  

f   t    at  g  t    st   

   empty histories   
t       a   s             at    st      

   

fidynamic non bayesian decision making

definition  t   depends monitoring structure  perfect monitoring
case 
 t       a    s   u a   s             at    st    u at    st       
imperfect monitoring case
 t       a   u a   s             at    u at    st       
define auxiliary additional random variables q    p
let xt     c at   st  cr xt     otherwise  let nt   tt   xt     let     
strategy f  optimal exists integer k every payoff function u
every nature s strategy g

prob  nt       t every k      
   
  f g u   strategy f optimal  optimal     
roughly speaking  nt measures number stages competitive ratio  or
better value  obtained first iterations   optimal strategy
exists number k   system runs k iterations get high
probability nt close    i e   almost iterations good ones   optimal

strategy guarantee get close wish situation iterations
good ones  probability high wish  notice require
above mentioned useful property hold every payoff function every strategy
nature  strong requirement consequence non bayesian setup  since
clue reward function strategy selected nature  and
strategy may yield arbitrary sequences states reached   best policy would
insist good behavior behavior adopted nature  notice however
two relaxations introduced here  require successful behavior stages 
whole process would successful  very high  probability 
major objective find policy enable     hold every dynamic
decision problem every nature strategy  moreover  wish     hold small enough
k   k small agent benefit obtaining desired behavior already
early stage     subject following section  complete section
useful technical observation  show strategy f  optimal satisfies
optimality criterion     every reward function every stationary strategy
nature  stationary strategy defined sequence states z    st   
t    
strategy nature chooses st stage t  independent history  indeed  assume f
strategy     holds every reward function every stationary strategy
nature  show f  optimal 
given payoff function u strategy g   optimality respect stationary
strategies implies   f g u  
prob  nt       t every k  js   s               
    note function c a  s  depends payoff function u therefore random variables
xt nt  
    interested reader may wish think long run optimality criteria view original work
pac learning  valiant         setting  pac learning  wish obtain desired behavior 
situations  high probability  relatively fast 

   

fimonderer tennenholtz

probability one  therefore

prob nt       t every k        
roughly speaking  captures fact non bayesian setting
need present strategy good sequence states chosen nature 
regardless way chosen 

   perfect monitoring

section present main result  result refers case perfect monitoring  shows existence  optimal strategy case  guarantees
desired behavior obtained polynomially many stages  result constructive  first present rough idea strategy employed proof  utility
function known agent could use competitive ratio action  since
utility function initially unknown agent use greedy strategy 
selects action optimal far competitive ratio concerned  according
agent s knowledge given point  however  agent try time time
sample random action    strategy chooses right tradeoff exploration
exploitation phases order yield desired result  careful analysis needed
order prove optimality related strategy  fact yields desired
result polynomially many stages 
introduce main theorem 

theorem      let dd    a    dynamic decision problem perfect monitoring 

every     exists  optimal strategy  moreover   optimal strategy
chosen ecient sense k  in      taken polynomial
max n      

proof  recall na ns denote number actions states respectively 
n   max na   ns    proof assume simplicity n   na   ns  
slight modifications required removing assumption  without loss generality 
     define strategy f follows  let       is 

 
    
stage   construct matrices utf   ctf subset actions wt
following way  define u f  a  s    a  s  stage        
performed stage      st    observed  update u replacing
 at     st     entry u at     st      stage    utf  a  s      define
f b s 
ctf  a  s       utf  a  s   
    ctf  a  s    maxfb utf  b s   g uuttf   a s
    finally wt set
  minb a maxs s ctf  b  s  obtained  refer elements wt
temporarily good actions stage   let  zt t  sequence i i d  f    g random
    use uniform probability distribution select among actions exploration phase  result
obtained different exploration techniques well 

   

fidynamic non bayesian decision making

variables prob zt            m    sequence generated part strategy 
independent observed history  stage  choosing action 
agent ips coin  independently past observations  stage agent observes
zt  zt      agent chooses action wt randomizing equal probabilities 
zt     agent randomizes equal probabilities amongst actions a 
complete description strategy f   let u given payoff function  let  st  
t  
given sequence states  proceed show     holds k upper
integer value   max ff       ff       


   
ff       
  ln  

n   n        ln
ff   
 
 



 
 n    


 
p

recall xt     c at  st   cr xt     otherwise  nt   tt   xt  
slight change notation  denote p   prob probability measure induced
f   u sequence states  a f    g    where f    g corresponds zt
values  
let         define

bk  

 t
x

t  

 


 
zt         k  


roughly speaking  bk captures cases temporarily good actions selected
stages 
 chernoff         see  alon  spencer    erdos          every  

p


x
t  

 

 
zt              t e     t  



recall given set   denotes complement  
hence 
 
 

 
 
x
x
x
 
zt            t
e     t  
p  bk   p
therefore
since k   ff   
define 

 k

p  bk  

t  
z 

k   



e     t dt       e 
 

p  bk      

 k

    k    

 

 
   

lk   fnt       t every k g 
roughly speaking  lk captures cases competitive ratio actions  or better

actions regard  selected stages 
order prove f  optimal  i e       satisfied   prove

p  lk    
   

   

fimonderer tennenholtz

    suces prove

   
p  lk jbk    
define every        six auxiliary random variables  yt   rt  yts   rst  yts a   rs a
 
let yt     whenever zt     xt      yt     otherwise  let

rt  


x
t  

yt  

every   let yts     whenever yt     st   s  yts     otherwise  let

x
yts  
t  
yts a    

rst  

every   every   a  let
whenever yts       a 
s a
yt     otherwise  let

x
rs a
 
yts a  

t  

let g integer value    k   show

p  lk jbk   p  t k   rt gjbk  

   

order prove     show

lk   bk f t k   rt g g   bk  
indeed  w path bk every k rt   g   then  w  every
k 

x
x
nt
xt vt   yt 
 tt zt  

t  

vt denotes number stages   zt      since w   bk  
n            t   r              t   g








every k   since m          g    k   nt       t every k   hence 
w   lk  
    implies suces prove

p  t k   rt gjbk    
therefore suces prove every    


p  t k  rst ng jbk  n  
hence suces prove every   every   a 
   

   

fidynamic non bayesian decision making



  p  t k 

g
rs a
n  jbk



 n  

   

g
order prove       note inequality rs a
n  satisfied gw 
c a  s    cr nevertheless considered good action least n  stages
b s    cr  b
   w l o g  assume ng  integer   let b   satisfy uu  a s
 
ever played stage st   s     wt t  therefore




p  t k  b played first ng  stages st   sjbk  
hence
     x   x   e x x   

ut



    

nm

g 
n

 

g
e  n   nm        n    

theorem     shows ecient dynamic non bayesian decisions may obtained
appropriate stochastic policy  moreover  shows  optimality obtained
time  low degree  polynomial max n       interesting question whether
similar results obtained pure deterministic strategy  following example
shows  deterministic strategies suce job 
give example agent optimal pure strategy 

example    let   fa   a g   fs   s g  assume negation agent
optimal pure strategy f  

consider following two decision problems whose rows indexed actions
whose columns indexed states 

d   

    
    

d   

    
    

corresponding ratio matrices 

c   

    
   

c   

    
    
   

 

 

 
 

fimonderer tennenholtz

assume addition cases nature uses strategy g   defined follows 
g  ht    si f  ht    ai           is  every t   at  zt      a   s    at  zt     a   s   
zt denote action state selected stage t  respectively  let        
let nti denote nt decision problem i  since f  optimal  exists k
every k   nt        t nt        t   note sequence
  at  zt  t  generated cases  nk        k implies  a    s   used
half stages              k   hand  nk        k implies  a   s  
used half stages              k   contradiction 
ut
analytical completeness  end section proving existence optimal
strategy  and merely  optimal strategy   optimal strategy obtained
utilizing  optimal strategies  whose existence proved theorem      intervals
stages sizes converge infinity      

corollary      every dynamic decision problem perfect monitoring exists
optimal strategy 

proof     let fm

sequence limm        let
every  

 optimal strategy   m    decreasing
m  
 
 km   


increasing
sequence
integers
m  






prob nt         t every km      m  



pm
k
km     j   j  

let f strategy   utilizes fm stages k            km      
k            km     km  k       easily verified f optimal 

ut

   imperfect monitoring

proceed give example imperfect monitoring case  suciently
small      agent  optimal strategy 

example    non existence  optimal strategies imperfect monitoring case 

let   fa   a g    fs    s   s g  let         defined end
proof  assume negation exists  optimal strategy f   consider
following two decision problems whose rows indexed actions whose columns
indexed states 
 
 

 
b
 
c
d    b c
   

fidynamic non bayesian decision making

 

 a  b  c

d   

b c

a  b c positive numbers satisfying     b     c          let
ci    ci  a  s  a a s s ratio matrices  is 

c   

     
     

 

     ac
 a  b  
b c

c   

 

note a  unique cr action d  a  unique cr action d  
assume nature uses strategy g randomizes stage equal probabilities
 of      amongst   states  given strategy nature  agent cannot distinguish
two decision problems  even knows nature s strategy told
one chosen  implies     probability measures induced
f g  a    decision problems d  d  respectively  every
  f    g  distribution stochastic process  nti   
   respect j   j   f    g 
depend j   is  every  








prob  nti   mt   prob  nti   mt     f    g    
every sequence  mt  tt   mt f             tg    
give complete proof      rather illustrate proving representing
case  reader easily derive complete proof  show

prob   n          prob   n       

   

indeed  j        

probj  n            

 
x

k  

f  e  a  f  a    uj  a    sk    a  

    

let   f       g   f       g defined                           

u  a    sk     u   a   s k   
every   k    therefore      implies     
f  optimal  exists integer k probability least
    respect     hence respect     nt        t every k  
implies probability least       a  played least    
stages time   k   particular   k   choose integer k
suciently large according law large numbers  nature chooses
   

fimonderer tennenholtz

s  least      stages stage k probability least       let cr 
c t denote cr ct decision problem    respectively 
  cr   max   a    b   
 
 c
b c
therefore    a   c  at   st  cr  st  
  s   hence 
 
probability least                         stages c t cr  
therefore f cannot  optimal  choose             

ut

                               

   safety level

sake comparison discuss section safety level  known maxmin 
criterion  let    a  s  u   decision problem  denote

v   max
min
u a  s 
v called safety level agent  or maxmin value   every action
u a  s  v every called safety level action  consider imperfect
monitoring model dynamic decision problem   a     every sequence states
z    st  
t   st   every   every pure strategy f agent induce
z f  
sequence actions  at  
t   corresponding sequence payoffs  ut  t    
z f
uz f
  u at   st  every    let nt denote number stages stage
agent s payoff exceeds safety level v   is 
ntz f    f    uz f
vg

    

say f safety level optimal every decision problem every sequence
states 
lim   n z f     
  

convergence holds uniformly w r t  payoff functions u sequences states
  is  every     exists k   k     ntz f       t every
k every decision problem   a  s  u   every sequence states z  
proposition      every dynamic decision problem possesses safety level optimal strategy
imperfect monitoring case  consequently perfect monitoring case  moreover 
optimal strategy chosen strongly ecient sense every
sequence states exists na     stages agent receives payoff
smaller safety level  na denotes number actions 
proof  let n   na   define strategy f follows  play actions
first n stages  every n      every history h   ht     
   

fidynamic non bayesian decision making

  a   u     a   u           at     ut      define f  h    follows    a  let vth  a   
min ut   min ranges stages       a  define f  h     
maximizes vth  a    a  obvious every sequence states
 
z    st   
t   n     stages u at   st    v    at  t  
z f
sequence actions generated f sequence states  hence nt   n 
ntz f defined       thus k      n   t  ntz f     every k     

ut

   discussion

note notations established section    proposition     well  remain
unchanged assume utility function u takes values totally pre ordered
set without group structure  is  approach decision making qualitative
 or ordinal   distinguishes work previous work non bayesian repeated
games  used probabilistic safety level criterion basic solution concept
one shot game    works  including  blackwell        hannan        banos       
megiddo         recently  auer  cesa bianchi  freund    schapire        hart
  mas colell         used several versions long run solution concepts  based
optimization average utility values time  is  p
papers
goal find strategies guarantee high probability t  tt   u at  st 
close vp  
work is  best knowledge  first introduce ecient dynamic
optimal policy basic competitive ratio context  study results sections    
easily adapted case qualitative competitive ratio well  approach 
utility function takes values totally pre ordered set g addition assume
regret function  maps g g pre ordered set h   g    g    g   g   g  
level regret agent receives utility level g  rather g   given
action state s  regret function determine maximal regret  c a  s    h
agent action performed state s  is 

c a  s    max  u a  s   u b  s   
b ranges actions 
qualitative regret action maximal regret action states 
optimal qualitative competitive ratio obtained using action
qualitative regret minimal  notice arithmetic calculations needed  or make
sense  qualitative version  results adapted case qualitative
competitive ratio  ease exposition  however  used quantitative version
model  numerical utility function represents regret function 
    probabilistic safety value  vp   agent decision problem    a  s  u   maxmin
value max ranges mixed actions 

vp   maxq  a  mins s

x

a a

u a  s q a  

 a  set probability distributions q a 

   

fimonderer tennenholtz

work relevant research reinforcement learning ai  work area 
however  dealt mostly bayesian models  makes work complementary
work  wish brie discuss connections differences
work existing work reinforcement learning 
usual underlying structure reinforcement learning literature environment changes result agent s action based particular probabilistic
function  agent s reward may probabilistic well    notation  markov
decision process  mdp  repeated game nature complete information
strategic certainty  nature s strategy depends probabilistically last action
state chosen     standard partially observable mdp  pomdp  described similarly introducing level monitoring perfect imperfect monitoring 
addition  bandit problems basically modeled repeated games nature
probabilistic structural assumption nature s strategy   strategic uncertainty values transition probabilities  example  nature s action
play role state slot machine basic bandit problem  main difference classical problem problem discussed setting
state slot machine may change setting totally unpredictable manner  e g  
seed machine manually changed iteration   say
solving learning problem solved problem reinforcement learning
mdp  pomdp  bandit problems  later settings  optimal strategy behave
poorly relative strategies obtained theory reinforcement learning  take
particular structure account 
non bayesian qualitative setup call optimality criteria differ
ones used current work reinforcement learning  work reinforcement learning
discusses learning mechanisms optimize expected payoff long run 
qualitative setting  as described above  long run expected payoffs may make much
sense  optimality criteria expresses need obtain desired behavior
stages  one easily construct examples one approaches favorite
one  emphasis obtaining desired behavior relatively short run 
though  analytical results reinforcement learning concerned
eventual convergence desired behavior  policies shown quite
ecient practice 
addition previously mentioned differences work work reinforcement learning  wish emphasize much work pomdp uses information
structures different discussed paper  work pomdp usually
assumes observations current state may available  following presentation smallwood   sondik         although observations previous state
discussed well  boutilier   poole         recall case perfect monitoring
previous environment state revealed  immediate reward revealed
prefect imperfect monitoring  may useful consider situations
    results presented paper extended case randomness
reward obtained agents well 
    likewise  stochastic games  shapley        considered repeated games nature
partial information nature s strategy  matter one redefine concept state
games  state pair  s  a   state system action opponent 

   

fidynamic non bayesian decision making

 partial  observations previous state current state revealed time
time  may used setting completely clear  may serve
subject future research 

references

alon  n   spencer  j     erdos  p          probabilistic method  wiley interscience 
auer  p   cesa bianchi  n   freund  y     schapire  r          gambling rigged
casino  adversial multi armed bandit problem  proceedings   th annual
symposium foundations computer science  pp          
aumann  r     maschler  m          repeated games incomplete information 
mit press 
banos  a          pseudo games  annals mathematical statistics                
blackwell  d          analog minimax theorem vector payoffs  pacific journal
mathematic         
boutilier  c     poole  d          computing optimal policies partially observable
decision processes using compact representations  proceedings   th national
conference artificial intelligence  pp            
cassandra  a   kaelbling  l     littman  m          acting optimally partially observable stochastic domain  proceedings   th national conference artificial
intelligence  pp            
chernoff  h          measure asymptotic eciency tests hypothesis based
sum observations  annals mathematical statistics              
fudenberg  d     levine  d          theory learning games  miemo 
fudenberg  d     tirole  j          game theory  mit press 
hannan  j          approximation bayes risk repeated play  dresher  m   tucker 
a     wolfe  p   eds    contributions theory games  vol  iii  annals
mathematics studies      pp          princeton university press 
harsanyi  j          games incomplete information played bayesian players  parts
i  ii  iii  management science              
hart  s     mas colell  a          simple adaptive procedure leading correlated
equilibrium  discussion paper      center rationality interactive decision
theory  hebrew university 
kaelbling  l   littman  m     moore  a          reinforcement learning  survey  journal
artificial intelligence research             
kreps  d          notes theory choice  westview press 
   

fimonderer tennenholtz

lovejoy  w          survey algorithmic methods partially observed markov decision
processes  annals operations research                  
luce  r  d     raiffa  h          games decisions  introduction critical survey 
john wiley sons 
megiddo  n          repeated games incomplete information played nonbayesian players  international journal game theory             
mertens  j  f   sorin  s     zamir  s          repeated games  part a  core  dp      
milnor  j          games nature  thrall  r  m   coombs  c     davis  r 
 eds    decision processes  john wiley   sons 
monahan  g          survey partially observable markov decision processes  theory 
models algorithms  management science           
papadimitriou  c     yannakakis  m          shortest paths without map  automata 
languages programming    th international colloquium proceedings  pp      
    
russell  s     norvig  p          artificial intelligence  modern approach  prentice hall 
savage  l          foundations statistics  dover publications  new york 
shapley  l          stochastic games  proceeding national academic sciences
 usa                 
smallwood  r     sondik  e          optimal control partially observable markov
processes finite horizon  operations research                
sutton  r          special issue reinforcement learning  machine learning          
valiant  l  g          theory learnable  comm  acm                     
watkins  c     dayan  p          technical note  q learning  machine learning          
        
watkins  c          learning delayed rewards  ph d  thesis  cambridge university 
wellman  m     doyle  j          modular utility representation decision theoretic
planning  proceedings first international conference ai planning systems 
morgan kaufmann 
wellman  m          reasoning preference models  tech  rep  mit lcs tr     
laboratory computer science  mit 

   


