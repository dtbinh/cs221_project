journal of artificial intelligence research                 

submitted       published      

dynamic non bayesian decision making
dov monderer
moshe tennenholtz

dov ie technion ac il
moshet ie technion ac il

industrial engineering and management
technion   israel institute of technology
haifa        israel

abstract

the model of a non bayesian agent who faces a repeated game with incomplete information against nature is an appropriate tool for modeling general agent environment
interactions  in such a model the environment state  controlled by nature  may change arbitrarily  and the feedback reward function is initially unknown  the agent is not bayesian 
that is he does not form a prior probability neither on the state selection strategy of nature 
nor on his reward function  a policy for the agent is a function which assigns an action to
every history of observations and actions  two basic feedback structures are considered 
in one of them   the perfect monitoring case   the agent is able to observe the previous
environment state as part of his feedback  while in the other   the imperfect monitoring
case   all that is available to the agent is the reward obtained  both of these settings
refer to partially observable processes  where the current environment state is unknown 
our main result refers to the competitive ratio criterion in the perfect monitoring case 
we prove the existence of an ecient stochastic policy that ensures that the competitive
ratio is obtained at almost all stages with an arbitrarily high probability  where eciency
is measured in terms of rate of convergence  it is further shown that such an optimal
policy does not exist in the imperfect monitoring case  moreover  it is proved that in the
perfect monitoring case there does not exist a deterministic policy that satisfies our long
run optimality criterion  in addition  we discuss the maxmin criterion and prove that a
deterministic ecient optimal strategy does exist in the imperfect monitoring case under
this criterion  finally we show that our approach to long run optimality can be viewed as
qualitative  which distinguishes it from previous work in this area 

   introduction
decision making is a central task of artificial agents  russell   norvig        wellman 
      wellman   doyle         at each point in time  an agent needs to select among
several actions  this may be a simple decision  which takes place only once  or a more
complicated decision where a series of simple decisions has to be made  the question of
 what should the right actions be  is the basic issue discussed in both of these settings  and
is of fundamental importance to the design of artificial agents 
a static decision making context  problem  for an artificial agent consists of a set of actions that the agent may perform  a set of possible environment states  and a utility reward
function which determines the feedback for the agent when it performs a particular action
in a particular state  such a problem is best represented by a matrix with columns indexed
by the states  rows indexed by the actions and the rewards as entries  when the reward
function is not known to the agent we say that the agent has payoff uncertainty and we
c      ai access foundation and morgan kaufmann publishers  all rights reserved 

fimonderer and tennenholtz

refer to the problem as a problem with incomplete information fudenberg   tirole        
when modeling a problem with incomplete information one must also describe the underlying assumptions on the knowledge of the agent about the reward function  for example  the
agent may know bounds on his rewards  or he may know  or partially know  an underlying
probabilistic structure    in a dynamic  multistage  decision making setup the agent faces
static decision problems over stages  at each stage the agent selects an action to be performed and the environment selects a state  the history of actions and states determines
the immediate reward as well as the next one shot decision problem  the history of actions
and states also determines the next selected state  work on reinforcement learning in artificial intelligence  kaelbling  littman    moore        has adopted the view of an agent
operating in a probabilistic bayesian setting  where the agent s last action and the last state
determine the next environment state based on a given probability distribution  naturally 
the learner may not be a priori familiar with this probability distribution  but the existence
of the underlying probabilistic model is a key issue in the system s modeling  however  this
assumption is not an ultimate one  in particular  much work in other areas in ai and in
economics have dealt with non probabilistic settings in which the environment changes in
an unpredictable manner    when the agent does not know the inuence of his choices on
the selection of the next state  i e   he is not certain about the environment strategy   we
say that the agent has strategic uncertainty 
in this paper we use a general model for the representation of agent environment interactions in which the agent has both payoff and strategic uncertainty  we deal with a
non bayesian agent who faces a repeated game with incomplete information against nature 
in a repeated game against nature the agent faces the same static decision problem at
each stage while the environment state is taken to be an action chosen by his opponents 
the decision problem is called a game to stress the fact that the agent s action and the
state are independently chosen  the fact that the game is repeated refers to the fact
that the set of actions  the set of possible states  and the one shot utility function do not
vary with time    as we said  we consider an agent that has both payoff uncertainty and
strategic uncertainty  that is  he is a priori ignorant about the utility function  i e   the
game is of incomplete information  as well as about the state selection strategy of nature 
the agent is non bayesian in the sense that he does not assume any probabilistic model
concerning nature s strategy and in the sense that he does not assume any probabilistic
model concerning the reward function  though he may assume lower and upper bounds   
we consider two examples to illustrate the above mentioned notions and model  consider
   for example  the agent may know a probability distribution on a set of reward functions  he may assume
that such a probability exists without any assumption on its structure  or he may have partial information
about this distribution but be ignorant about some of its parameters  e g   he may believe that the reward
function is drawn according to a normal distribution with an unknown covariance matrix  
   there are many intermediate cases where it is assumed that the changes are probabilistic with a nonmarkovian structure 
   in the most general setup  those sets may vary with time  no useful analysis can be done in a model
where those changes are completely arbitrary 
   repeated games with complete information  or more generally  multistage games and stochastic games
have been extensively studied in game theory and economics  a very partial list includes   shapley 
      blackwell        luce   raiffa         and more recently  fudenberg   tirole        mertens 
sorin    zamir         and the evolving literature on learning  e g   fudenberg   levine        the
incomplete information setup in which the player is ignorant about the game being played was inspired

   

fidynamic non bayesian decision making

an investor  i   who is investing daily in a certain index of the stock market  his daily profits
depends on his action  selling or buying in a certain amount  and on the environment state
  the percentage change in the price of the index  this investor has complete information
about the reward function because he knows the reward which is realized in a particular
investment and a particular change  but he has strategic uncertainty about the changes
in the index price  so  he is playing a repeated game with complete information against
nature with strategic uncertainty 
consider another investor  i    who invests in a particular mutual fund  this fund invests
in the stock market with a strategy which is not known to the investor  assume that each
state represents the vector of percentage changes in the stocks  then the investor does not
know his reward function  for example  he cannot say in advance what would be his profit
if he would buy one unit of this fund and all stock prices increase in   percent  thus  i  
plays a repeated game with incomplete information  if in addition i   does not attempt to
construct a probabilistic model concerning his reward function or market behavior  then he
is non bayesian and our analysis may apply to him  for another example  assume that bob
has to decide on each evening whether to prepare tea or coffee for his wife before she gets
home  his wife wishes to drink either tea or coffee and he wishes to have it ready for her 
the reaction of bob s wife to tea or coffee may depend on her state that day  which can not
be predicted based on the history of actions and states in previous days  as bob has just
got married he cannot tell what reward he will get if his wife is happy and he makes her
a cup of tea  of course he may eventually know it  but his decisions during this learning
period are precisely the subject of this paper 
as an example for the generality of the above mentioned setting  consider the model of
markov decision processes with complete or incomplete information  in a markov decision
process an agent s action in a given state determines  in a probabilistic fashion  the next
state to be obtained  that is  the agent has a structural assumption on the state selection
strategy  a repeated game against nature without added assumptions captures the fact
that the transition from state to state may depend on the history in an arbitrary way 
when the agent performs an action at in state st   part of his feedback would be u at  st   
where u is the reward function  we distinguish between two basic feedback structures  in
one of them   the perfect monitoring case   the agent is able to observe the previous
environment state as part of his feedback  while on the other   the imperfect monitoring
case   all that is available to the agent is the reward obtained    notice that in both of
these feedback structures  the current state is not observed by the agent when he is called
to select an action    both investors i and i   face a repeated game with perfect monitoring
because the percentage changes become public knowledge after each iteration 
in the other example  when bob has to make his decision  if the situation is of imperfect
monitoring  bob would be only able to observe the reward for his behavior  e g   whether
by  harsanyi         see aumann and maschler        for a comprehensive survey  most of the above
literature deals with  partially  bayesian agents  some of the rare exceptions are cited in section   
   notice that the former assumption is very popular in the related game theory literature  aumann  
maschler         many other intermediate monitoring structures may be interesting as well 
   such is also the case in the evolving literature on the problem of controlling partially observable markov
decision processes  lovejoy        cassandra  kaelbling    littman        monahan         in contrast 
q learning theory  watkins        watkins   dayan        sutton        does assume a knowledge of
the current state 

   

fimonderer and tennenholtz

she says  thanks    that s terrible    this is exactly what i wanted   etc    in the perfect
monitoring case  bob will be able to observe his wife s state  e g   whether she came home
happy  sad  nervous  etc   in addition to his reward  in both cases bob  like the investors 
is not able to observe his wife s state before making his decision in a particular day 
consider the case of a one stage game against nature  in which the utility function
is known  but the agent cannot observe the current environment state when selecting his
action  how should the agent choose his action  work on decision making under uncertainty
has suggested several approaches  savage        milnor        luce   raiffa        kreps 
       one of these approaches is the maxmin  safety level  approach  according to this
approach the agent would choose an action that maximizes his worst case payoff  another
approach is the competitive ratio approach  or its additive variant  termed the minmax
regret decision criterion  milnor         according to this approach an agent would choose
an action that minimizes the worst case ratio between the payoff he could have obtained
had he known the environment state to the payoff he would actually obtain   returning
back to our example  if bob would have known the actual state of his wife  he could choose
an action that maximizes his payoff  since he has no hint about her state  he can go ahead
and choose the action that minimizes his competitive ratio  for example  if this action leads
to a competitive ratio of two  then bob can guarantee himself that the payoff he would get
is at most half the payoff he could have gotten had he known the actual state of his wife 
given a repeated game with incomplete information against nature  the agent would
not be able to choose his one stage optimal action  with respect to the competitive ratio or
maxmin value criteria  at each stage  since the utility function is initially unknown  so  if
bob does not initially know the reward he would receive for his actions as a function of his
wife s state  then he will not be able to simply choose an action that guarantees the best
competitive ratio  this calls for a precise definition of a long run optimality criterion  in
this paper we are mainly concerned with policies  strategies  guaranteeing that the optimal
competitive ratio  or the maxmin value  is obtained in most stages  we are interested in
particular in ecient policies  where eciency is measured in terms of rate of convergence 
hence in bob s case  we are interested in a policy that if adopted by bob would guarantee
him on almost any day  with high probability  at least the payoff guaranteed by an action
leading to the competitive ratio  moreover  bob will not have to wait much before he will
start getting this type of satisfactory behavior 
in section   we define the above mentioned setting and introduce some preliminaries  in
sections   and   we discuss the long run competitive ratio criterion  in section   we show
that even in the perfect monitoring case  a deterministic optimal policy does not exist 
however  we show that there exists an ecient stochastic policy which ensures that the
long run competitive ratio criterion holds with a high probability  in section   we show
that such stochastic policies do not exist in the imperfect monitoring case  in section   we
prove that for both the perfect and imperfect monitoring cases there exists a deterministic
ecient optimal policy for the long run maxmin criterion  in section   we compare our
notions of long run optimality to other criteria appearing in some of the related literature 
in particular  we show that our approach to long run optimality can be interpreted as
   the competitive ratio decision criterion has been found to be most useful in settings such as on line
algorithms  e g   papadimitriou   yanakakis        

   

fidynamic non bayesian decision making

qualitative  which distinguishes it from previous work in this area  we also discuss some of
the connections of our work with work in reinforcement learning 

   preliminaries
a  one shot  decision problem  with payoff certainty and strategic uncertainty  is a   tuple
d    a  s  u    where a and s are finite sets and u is a real valued function defined on
a  s with u a  s      for every  a  s    a  s   elements of a are called actions and those
of s are called states  u is called the utility function  the interpretation of the numerical
values u a  s  is context dependent    let na denote the number of actions in a  let ns
denote the number of states in s and let n   max na   ns   
the above mentioned setting is a classical static setting for decision making  where
there is uncertainty about the actual state of nature  luce   raiffa         in this paper
we deal with a dynamic setup  in which the agent faces the decision problem d  without
knowing the utility function u  over an infinite number of stages  t                as we
have explained in the introduction  this setting enables us to capture general dynamic nonbayesian decision making contexts  where the environment may change its behavior in an
arbitrary and unpredictable fashion  as mentioned in the introduction  this is best captured
by means of a repeated game against nature  the state of the environment at each point
plays the role of an action taken by nature in the corresponding game  the agent knows
the sets a and s   but he does not know the payoff function u   a dynamic decision problem
 with payoff uncertainty and strategic uncertainty  is therefore represented for the agent
by a pair dd    a  s   of finite sets    at each stage t  nature chooses a state st   s  
the agent  who does not know the chosen state  chooses at   a  and receives u at  st   we
distinguish between two informational structures  in the perfect monitoring case  the state
st is revealed to the agent alongside the payoff u at  st   in the imperfect monitoring case 
the states are not revealed to the agent  a generic history available to the agent at stage t   
is denoted by ht   in the perfect monitoring case  ht   htp    a  s  r   t   where r  denotes
the set of positive real numbers  in the imperfect monitoring case  ht   htimp    a  r   t 
in the particular case t     we assume that h p   h imp   feg is a singleton containing
p
imp      h imp   the symbol h will be
the empty history e  let h p     
t   ht and let h
t   t
used for both h p and h imp   a strategy   for the agent in a dynamic decision problem is
a function f   h    a    where  a  denotes the
p set of probability measures over a 
that is  for every ht   h   f  ht     a          and a a f  ht   a       in other words  if
the agent observes the history ht then he chooses at   by randomizing amongst his actions 
with the probability f  ht   a  assigned to the action a  a strategy f is called pure if f  ht  
is a probability measure concentrated on a singleton for every t    
in sections     the strategy recommended to the agent is chosen according to a  longrun  decision criterion which is induced by the competitive ratio one stage decision criterion 
   see the discussion at section   
   all the results of this paper remain unchanged if the agent does not initially know the set s   but rather
an upper bound on ns  
    notice that there is no need to include an explicit transition function in this representation  this is due
to the fact that in the non bayesian setup every transition is possible 
    strategy is a decision theoretic concept  it coincides with the term policy used in the control theory
literature  and with the term protocol used in the distributed systems literature 

   

fimonderer and tennenholtz

the competitive ratio decision criterion  that is described below  may be used by an agent
who faces the decision problem only once  and who knows the payoff function u as well as
the sets a and s   there are other  reasonable  decision criteria that could be used instead 
one of them is the maxmin decision criterion to be discussed in section    while another
is the minmax regret decision criterion  luce   raiffa        milnor         the latter is
a simple variant of the competitive ratio  and can be treated similarly   and therefore will
not be treated explicitly in this paper 
for every s   s let m  s  be the maximal payoff the agent can get when the state is s 
that is
m  s    max
u a  s  
a a
for every a   a and s   s define

c a  s    um a  ss    

denote c a    maxs s c a  s   and let





cr   min
c a    min
max c a  s   
a a
a a s s
cr is called the competitive ratio of d    a  s  u    any action a for which cr   c a 
is called a competitive ratio action  or in short a cr action  an agent which chooses a
  fraction from what it could have gotten  had it
cr action guarantees receiving at least cr
 
known the state s  that is  u a  s   cr m  s  for every s   s   this agent cannot guarantee
a bigger fraction 
in the long run decision problem  a non bayesian agent does not form a prior probability
on the way nature is choosing the states  nature may choose a fixed sequence of states or 
more generally  use a probabilistic strategy g  where g   q    s    and q     
t   qt  
t  nature can be viewed as a second player that knows the reward function  its
  
 
a

s
 
t  
strategy may of course refer to the whole history of actions and states until a given point
and may depend on the payoff function 
a payoff function u and a pair of probabilistic strategies f  g  where g can depend on u 
generate a probability measure    f g u over the set of infinite histories q     a  s   
endowed with the natural measurable structure  for an event b  q  we will denote the
probability of b according to  by  b   or by prob  b    more precisely  the probability
measure  is uniquely defined by its values for finite cylinder sets  let at   q    a and
st   q    s be the coordinate random variables which contain the values of the actions
and states selected by the agent and the environment in stage t  respectively   that is 
at h    at and st h    st for every h     a   s     a   s           in q    then for every t   
and for every   a   s            at   st      qt  

prob   at  st     at  st  for all    t  t    
where

 

t
y
t  

f   t    at  g  t    st   

and    are the empty histories  and for    t  t we have
t       a   s             at    st      

   

fidynamic non bayesian decision making

while the definition of  t   depends on the monitoring structure  in the perfect monitoring
case 
 t       a    s   u a   s             at    st    u at    st       
and in the imperfect monitoring case
 t       a   u a   s             at    u at    st       
we now define some auxiliary additional random variables on q    p
let xt     if c at   st   cr and xt     otherwise  and let nt   tt   xt     let      
a strategy f is   optimal if there exists an integer k such that for every payoff function u
and every nature s strategy g

prob  nt         t for every t  k        
   
where    f g u   a strategy f is optimal if it is   optimal for all      
roughly speaking  nt measures the number of stages in which the competitive ratio  or
a better value  has been obtained in the first t iterations  in an   optimal strategy there
exists a number k   such that if the system runs for t  k iterations we can get with high
probability that nt is close to    i e   almost all iterations are good ones   in an optimal

strategy we guarantee that we can get as close as we wish to the situation where all iterations
are good ones  with a probability that is as high as we wish  notice that we require that the
above mentioned useful property will hold for every payoff function and for every strategy
of nature  this strong requirement is a consequence of the non bayesian setup  since we do
not have any clue about the reward function or about the strategy selected by nature  and
this strategy may yield arbitrary sequences of states to be reached   the best policy would
be to insist on good behavior against any behavior adopted by nature  notice however that
two other relaxations are introduced here  we require successful behavior in most stages 
and that the whole process would be successful only with some  very high  probability 
the major objective is to find a policy that will enable     to hold for every dynamic
decision problem and every nature strategy  moreover  we wish     to hold for small enough
k   if k is small then our agent can benefit from obtaining the desired behavior already in
an early stage     this will be the subject of the following section  we complete this section
with a useful technical observation  we show that a strategy f is   optimal if it satisfies
the optimality criterion     for every reward function and for every stationary strategy of
nature  where a stationary strategy is defined by a sequence of states z    st   
t     in this
strategy nature chooses st at stage t  independent of the history  indeed  assume that f is
a strategy for which     holds for every reward function and for every stationary strategy
of nature  then we show that f is   optimal 
given any payoff function u and any strategy g    optimality with respect to stationary
strategies implies that for    f g u  
prob  nt        t for every t  k  js   s                
    note that the function c a  s  depends on the payoff function u and therefore so do the random variables
xt and nt  
    the interested reader may wish to think of our long run optimality criteria in view of the original work
on pac learning  valiant         in our setting  as in pac learning  we wish to obtain desired behavior 
in most situations  with high probability  and relatively fast 

   

fimonderer and tennenholtz

with probability one  therefore

prob nt         t for every t  k         
roughly speaking  the above captures the fact that in our non bayesian setting we
need to present a strategy that will be good for any sequence of states chosen by nature 
regardless of the way in which it has been chosen 

   perfect monitoring

in this section we present our main result  this result refers to the case of perfect monitoring  and shows the existence of a   optimal strategy in this case  it also guarantees that the
desired behavior will be obtained after polynomially many stages  our result is constructive  we first present the rough idea of the strategy employed in our proof  if the utility
function was known to the agent then he could use the competitive ratio action  since the
utility function is initially unknown then the agent will use a greedy strategy  where he
selects an action that is optimal as far as the competitive ratio is concerned  according to
the agent s knowledge at the given point  however  the agent will try from time to time to
sample a random action    our strategy chooses a right tradeoff between these exploration
and exploitation phases in order to yield the desired result  some careful analysis is needed
in order to prove the optimality of the related strategy  and the fact it yields the desired
result after polynomially many stages 
we now introduce our main theorem 

theorem      let dd    a  s   be a dynamic decision problem with perfect monitoring 

then for every      there exists a   optimal strategy  moreover  the   optimal strategy
can be chosen to be ecient in the sense that k  in      can be taken to be polynomial in
max n      

proof  recall that na and ns denote the number of actions and states respectively  and
that n   max na   ns    in this proof we assume for simplicity that n   na   ns   only
slight modifications are required for removing this assumption  without loss of generality 
      we define a strategy f as follows  let m       that is 

  
m     
at each stage t    we will construct matrices utf   ctf and a subset of the actions wt in
the following way  define u f  a  s     for each a  s  at each stage t      if at    has been
performed in stage t      and st    has been observed  then update u by replacing the 
in the  at     st     entry with u at     st      at each stage t     if utf  a  s      define
f b s 
ctf  a  s       if utf  a  s   
    ctf  a  s    maxfb utf  b s   g uuttf   a s
    finally wt is the set
of all a   a at which minb a maxs s ctf  b  s  is obtained  we refer to elements in wt as
the temporarily good actions at stage t   let  zt t  be a sequence of i i d  f    g random
    we use a uniform probability distribution to select among actions in the exploration phase  our result
can be obtained with different exploration techniques as well 

   

fidynamic non bayesian decision making

variables with prob zt            m    this sequence is generated as part of the strategy 
independent of the observed history  that is at each stage  before choosing an action  the
agent ips a coin  independently of his past observations  at each stage t the agent observes
zt  if zt      the agent chooses an action from wt by randomizing with equal probabilities 
if zt     the agent randomizes with equal probabilities amongst the actions in a  this
complete the description of the strategy f   let u be a given payoff function  and let  st  
t  
be a given sequence of states  we proceed to show that     holds with k being the upper
integer value of ff   max ff       ff        where


   
ff       
  ln  

n   n        ln
and ff   
 
 



  
 n    


 
p

recall that xt     if c at  st    cr and xt     otherwise  and that nt   tt   xt   by
a slight change of notation  we denote by p   prob the probability measure induced by
f   u and the sequence of states on  a  s  f    g    where f    g corresponds to the zt
values  
let         define

bk  

 t
x

t  

 


 
zt      m     t for all t  k  


roughly speaking  bk captures the cases where temporarily good actions are selected
in most stages 
by  chernoff         see also  alon  spencer    erdos          for every t  

p

t
x
t  

 

 
zt              t  e     t  

m

recall that given a set s   s denotes the complement of s  
hence 
 
 
t
 
 
x
x
x
 
zt            t 
e     t  
p  bk    p
therefore
since k   ff   
define 

t  k

p  bk   

t  
z 

k   

m

e     t dt       e 
 

p  bk      

t  k

    k    

 

 
   

lk   fnt         t for every t  k g 
roughly speaking  lk captures the cases where competitive ratio actions  or better

actions in this regard  are selected in most stages 
in order to prove that f is   optimal  i e   that     is satisfied   we have to prove that

p  lk     
   

   

fimonderer and tennenholtz

by     it suces to prove that

   
p  lk jbk     
we now define for every t     s   s and a   a six auxiliary random variables  yt   rt  yts   rst  yts a   rs a
t  
let yt     whenever zt     and xt      and yt     otherwise  let

rt  

t
x
t  

yt  

for every s   s let yts     whenever yt     and st   s  and yts     otherwise  let
t
x
yts  
t  
yts a    

rst  

for every s   s and for every a   a  let
whenever yts     and at   a  and
s a
yt     otherwise  let
t
x
rs a
 
yts a  
t
t  

let g be the integer value of    k   we now show that

p  lk jbk    p  t  k   rt  gjbk  

   

in order to prove     we show that

lk   bk  f t  k   rt  g g   bk  
indeed  if w is a path in bk such that for every t  k rt   g   then  at w  for every
t  k 
t
x
x
nt 
xt  vt   yt 
 tt zt  

t  

where vt denotes the number of stages    t  t for which zt      since w   bk  
n             t   r              t   g
t

m

t

m

for every t  k   since m          and g     k   nt         t for every t  k   hence 
w   lk  
    implies that it suces to prove that

p  t  k   rt  gjbk     
therefore it suces to prove that for every s   s  


p  t  k  rst  ng jbk   n  
hence it suces to prove that for every s   s and every a   a 
   

   

fidynamic non bayesian decision making



   p  t  k 

g
rs a
t  n  jbk



  n  

   

g
in order to prove       note that if the inequality rs a
t  n  is satisfied at gw  then
c a  s    cr and a is nevertheless considered to be a good action in at least n  stages
b s    cr  if b is
   t  t  w l o g  assume that ng  is an integer   let b   a satisfy uu  a s
 
ever played in a stage t with st   s  then a    wt for all t  t  therefore




  p  t  k  b is not played in the first ng  stages at which st   sjbk  
hence
as      x   x    e x for x    

ut



      

nm

 g 
n

 

g
  e  n   nm        n    

theorem     shows that ecient dynamic non bayesian decisions may be obtained by
an appropriate stochastic policy  moreover  it shows that   optimality can be obtained in
time which is a  low degree  polynomial in max n       an interesting question is whether
similar results can be obtained by a pure deterministic strategy  as the following example
shows  deterministic strategies do not suce for that job 
we give an example in which the agent does not have a  optimal pure strategy 

example    let a   fa   a g and s   fs   s g  assume in negation that the agent has a
 optimal pure strategy f  

consider the following two decision problems whose rows are indexed by the actions and
whose columns are indexed by the states 

d   

    
    

d   

    
    

with the corresponding ratio matrices 

c   

    
   

c   

    
    
   

 

 

 
 

fimonderer and tennenholtz

assume in addition that in both cases nature uses the strategy g   defined as follows 
g  ht    si if f  ht    ai   i         that is  for every t   at  zt      a   s   or  at  zt     a   s   
where at and zt denote the action and state selected at stage t  respectively  let         
let nti denote nt for decision problem i  since f is   optimal  there exists k such that
for every t  k   nt          t and nt          t   note also that the same sequence
  at  zt  t  is generated in both cases  nk          k implies that  a    s   is used at more
than half of the stages              k   on the other hand  nk          k implies that  a   s  
is used at more than half of the stages              k   a contradiction 
ut
for analytical completeness  we end this section by proving the existence of an optimal
strategy  and not merely a   optimal strategy   such an optimal strategy is obtained by
utilizing m  optimal strategies  whose existence was proved in theorem      for intervals of
stages with sizes that converge to infinity  when m     

corollary      in every dynamic decision problem with perfect monitoring there exists
an optimal strategy 

proof  for m     let fm be a

sequence with limm   m      let
that for every m   

m  optimal strategy  where  m    is a decreasing
m  
 
 km   
be
an
increasing
sequence
of integers such
m  





m
prob nt          t for every t  km       m  

and

pm
k
km      j   j  
m
let f be the strategy that for m    utilizes fm at the stages k            km        t 
k            km     km  where k       it can be easily verified that f is optimal 

ut

   imperfect monitoring

we proceed to give an example for the imperfect monitoring case  where for suciently
small       the agent does not have a   optimal strategy 

example    non existence of  optimal strategies in the imperfect monitoring case 

let a   fa   a g  and s   fs    s   s g  let        where   is defined at the end
of this proof  assume in negation that there exists a   optimal strategy f   consider the
following two decision problems whose rows are indexed by the actions and whose columns
are indexed by the states 
 
 
a
 
b
 
c
d    a b c
   

fidynamic non bayesian decision making

 

 a  b  c

d   

b c a

where a  b and c are positive numbers satisfying  a    b     c  for i         let
ci    ci  a  s  a a s s be the ratio matrices  that is 

c   

     
     

 

     ac
 a  b  
b c

c   

 

note that a  is the unique cr action in d  and a  is the unique cr action in d  
assume that nature uses strategy g which randomizes at each stage with equal probabilities
 of      amongst all   states  given this strategy of nature  the agent cannot distinguish
between the two decision problems  even if he knows nature s strategy and he is told that
one of them is chosen  this implies that if   and   are the probability measures induced
by f and g on  a  s    in the decision problems d  and d  respectively  then for every
i   f    g  the distribution of the stochastic process  nti   
t    with respect to j   j   f    g 
does not depend on j   that is  for every t   








prob  nti   mt for all t  t   prob  nti   mt for all t  t   i   f    g    
for every sequence  mt  tt   with mt  f             tg for all    t  t  
we do not give a complete proof of      rather we illustrate it by proving a representing
case  the reader can easily derive the complete proof  we show that

prob   n          prob   n       

   

indeed  for j        

probj  n            

 
x

k  

f  e  a  f  a    uj  a    sk    a  

    

let    f       g   f       g be defined by                     and           then

u  a    sk     u   a   s k   
for every    k     therefore      implies     
as f is   optimal  then there exists an integer k such that with a probability of at least
     with respect to     and hence with respect to     nt          t for every t  k  
this implies that with a probability of at least        a  is played at least at      of the
stages up to time t   for all t  k   and in particular for t   k   we choose the integer k
to be suciently large such that according to the law of large numbers  nature chooses
   

fimonderer and tennenholtz

s  in at least       of the stages up to stage k with a probability of at least        let cr 
and c t denote cr and ct of decision problem    respectively  then
a   cr   max   a    b   
 
 c
b c
therefore  if at   a   then c  at   st   cr  if and only if st  
  s   hence  with a
 
probability of at least         in at most                    of these stages c t  cr  
therefore f cannot be   optimal  if we choose   such that            and

ut

                               

   safety level

for the sake of comparison we discuss in this section the safety level  known also as maxmin 
criterion  let d    a  s  u   be a decision problem  denote

v   max
a min
s u a  s 
v is called the safety level of the agent  or the maxmin value   every action a for which
u a  s   v for every s is called a safety level action  we consider now the imperfect
monitoring model for the dynamic decision problem   a  s    every sequence of states
z    st  
t   with st   s for every t    and every pure strategy f of the agent induce
z f  
a sequence of actions  at  
t   and a corresponding sequence of payoffs  ut  t     where
z f
uz f
t   u at   st  for every t     let nt denote the number of stages up to stage t at
which the agent s payoff exceeds the safety level v   that is 
ntz f    f   t  t   uz f
t  vg

    

we say that f is safety level optimal if for every decision problem and for every sequence of
states 
lim   n z f     
t    t t

and the convergence holds uniformly w r t  all payoff functions u and all sequences of states
in s   that is  for every      there exists k   k     such that ntz f         t for every
t  k for every decision problem   a  s  u   and for every sequence of states z  
proposition      every dynamic decision problem possesses a safety level optimal strategy
in the imperfect monitoring case  and consequently in the perfect monitoring case  moreover 
such an optimal strategy can be chosen to be strongly ecient in the sense that for every
sequence of states there exists at most na     stages at which the agent receives a payoff
smaller than his safety level  where na denotes the number of actions 
proof  let n   na   define a strategy f as follows  play each of the actions in
a in the first n stages  for every t  n      and for every history h   ht     
   

fidynamic non bayesian decision making

  a   u     a   u           at     ut      we define f  h    a as follows  for a   a  let vth  a   
min ut   where the min ranges over all stages t  t     for which at   a  define f  h    at  
where at maximizes vth  a  over a   a  it is obvious that for every sequence of states
 
z    st   
t   there are at most n     stages at which u at   st    v   where  at  t   is the
z f
sequence of actions generated by f and the sequence of states  hence nt  t   n  where
ntz f is defined in       thus for k      n   t  ntz f       for every t  k     

ut

   discussion

note that all the notations established in section    and proposition     as well  remain
unchanged if we assume that the utility function u takes values in a totally pre ordered
set without any group structure  that is  our approach to decision making is qualitative
 or ordinal   this distinguishes our work from previous work on non bayesian repeated
games  which used the probabilistic safety level criterion as a basic solution concept for the
one shot game    these works  including  blackwell        hannan        banos       
megiddo         and more recently  auer  cesa bianchi  freund    schapire        hart
  mas colell         used several versions of long run solution concepts  all based on some
optimization of the average of the utility values over time  that is  in p
all of these papers
the goal is to find strategies that guarantee that with high probability t  tt   u at  st  will
be close to vp  
our work is  to the best of our knowledge  the first to introduce an ecient dynamic
optimal policy for the basic competitive ratio context  our study and results in sections    
can be easily adapted to the case of qualitative competitive ratio as well  in this approach 
the utility function takes values in some totally pre ordered set g and in addition we assume
a regret function  that maps g  g to some pre ordered set h   for g    g    g   g   g  
is the level of regret when the agent receives the utility level g  rather than g   given an
action a and a state s  the regret function will determine the maximal regret  c a  s    h
of the agent when action a is performed in state s  that is 

c a  s    max  u a  s   u b  s   
where b ranges over all actions 
the qualitative regret of action a will be the maximal regret of this action over all states 
the optimal qualitative competitive ratio will be obtained by using an action for which the
qualitative regret is minimal  notice that no arithmetic calculations are needed  or make
any sense  for this qualitative version  our results can be adapted to the case of qualitative
competitive ratio  for ease of exposition  however  we used the quantitative version of this
model  where a numerical utility function represents the regret function 
    the probabilistic safety value  vp   of the agent in the decision problem d    a  s  u   is his maxmin
value when the max ranges over all mixed actions  that is

vp   maxq  a  mins s

x

a a

u a  s q a  

where  a  is the set of all probability distributions q on a 

   

fimonderer and tennenholtz

our work is relevant to research on reinforcement learning in ai  work in this area 
however  has dealt mostly with bayesian models  this makes our work complementary to
this work  we wish now to briey discuss some of the connections and differences between
our work and existing work in reinforcement learning 
the usual underlying structure in the reinforcement learning literature is that of an environment which changes as a result of an agent s action based on a particular probabilistic
function  the agent s reward may be probabilistic as well    in our notation  a markov
decision process  mdp  is a repeated game against nature with complete information and
strategic certainty  in which nature s strategy depends probabilistically on the last action
and state chosen     standard partially observable mdp  pomdp  can be described similarly by introducing a level of monitoring in between perfect and imperfect monitoring  in
addition  bandit problems can be basically modeled as repeated games against nature with
a probabilistic structural assumption about nature s strategy   but with strategic uncertainty about the values of the transition probabilities  for example  nature s action can
play the role of the state of a slot machine in a basic bandit problem  the main difference between the classical problem and the problem discussed in our setting is that the
state of the slot machine may change in our setting in a totally unpredictable manner  e g  
the seed of the machine is manually changed at each iteration   this is not to say that
by solving our learning problem we have solved the problem of reinforcement learning in
mdp  in pomdp  or in bandit problems  in the later settings  our optimal strategy behave
poorly relative to strategies obtained in the theory of reinforcement learning  that take the
particular structure into account 
the non bayesian and qualitative setup call for optimality criteria which differ from
the ones used in current work in reinforcement learning  work in reinforcement learning
discusses learning mechanisms that optimize the expected payoff in the long run  in a
qualitative setting  as described above  long run expected payoffs may not make much
sense  our optimality criteria expresses the need to obtain a desired behavior in most
stages  one can easily construct examples where one of these approaches is favorite to the
other one  our emphasis is on obtaining the desired behavior in a relatively short run 
though  most analytical results in reinforcement learning have been concerned only with
eventual convergence to desired behavior  some of the policies have been shown to be quite
ecient in practice 
in addition to the previously mentioned differences between our work and work in reinforcement learning  we wish to emphasize that much work on pomdp uses information
structures which are different from those discussed in this paper  work on pomdp usually
assumes that some observations about the current state may be available  following the presentation by smallwood   sondik         although observations about the previous state
are discussed as well  boutilier   poole         recall that in the case of perfect monitoring
the previous environment state is revealed  and the immediate reward is revealed in both
prefect and imperfect monitoring  it may be useful to consider also situations where some
    the results presented in this paper can be extended to the case where there is some randomness in the
reward obtained by the agents as well 
    likewise  stochastic games  shapley        can be considered as repeated games against nature with
partial information about nature s strategy  for that matter one should redefine the concept of state in
such games  a state is a pair  s  a   where s is a state of the system and a is an action of the opponent 

   

fidynamic non bayesian decision making

 partial  observations about the previous state or the current state are revealed from time
to time  how this may be used in our setting is not completely clear  and may serve as a
subject for future research 

references

alon  n   spencer  j     erdos  p          the probabilistic method  wiley interscience 
auer  p   cesa bianchi  n   freund  y     schapire  r          gambling in a rigged
casino  the adversial multi armed bandit problem  in proceedings of the   th annual
symposium on foundations of computer science  pp          
aumann  r     maschler  m          repeated games with incomplete information  the
mit press 
banos  a          on pseudo games  the annals of mathematical statistics                
blackwell  d          an analog of the minimax theorem for vector payoffs  pacific journal
of mathematic         
boutilier  c     poole  d          computing optimal policies for partially observable
decision processes using compact representations  in proceedings of the   th national
conference on artificial intelligence  pp            
cassandra  a   kaelbling  l     littman  m          acting optimally in partially observable stochastic domain  in proceedings of the   th national conference on artificial
intelligence  pp            
chernoff  h          a measure of the asymptotic eciency for tests of a hypothesis based
on the sum of observations  annals of mathematical statistics              
fudenberg  d     levine  d          theory of learning in games  miemo 
fudenberg  d     tirole  j          game theory  mit press 
hannan  j          approximation to bayes risk in repeated play  in dresher  m   tucker 
a     wolfe  p   eds    contributions to the theory of games  vol  iii  annals of
mathematics studies      pp          princeton university press 
harsanyi  j          games with incomplete information played by bayesian players  parts
i  ii  iii  management science              
hart  s     mas colell  a          a simple adaptive procedure leading to correlated
equilibrium  discussion paper      center for rationality and interactive decision
theory  hebrew university 
kaelbling  l   littman  m     moore  a          reinforcement learning  a survey  journal
of artificial intelligence research             
kreps  d          notes on the theory of choice  westview press 
   

fimonderer and tennenholtz

lovejoy  w          a survey of algorithmic methods for partially observed markov decision
processes  annals of operations research                  
luce  r  d     raiffa  h          games and decisions  introduction and critical survey 
john wiley and sons 
megiddo  n          on repeated games with incomplete information played by nonbayesian players  international journal of game theory             
mertens  j  f   sorin  s     zamir  s          repeated games  part a  core  dp      
milnor  j          games against nature  in thrall  r  m   coombs  c     davis  r 
 eds    decision processes  john wiley   sons 
monahan  g          a survey of partially observable markov decision processes  theory 
models and algorithms  management science           
papadimitriou  c     yannakakis  m          shortest paths without a map  in automata 
languages and programming    th international colloquium proceedings  pp      
    
russell  s     norvig  p          artificial intelligence  a modern approach  prentice hall 
savage  l          the foundations of statistics  dover publications  new york 
shapley  l          stochastic games  proceeding of the national academic of sciences
 usa                 
smallwood  r     sondik  e          the optimal control of partially observable markov
processes over a finite horizon  operations research                
sutton  r          special issue on reinforcement learning  machine learning          
valiant  l  g          a theory of the learnable  comm  acm                     
watkins  c     dayan  p          technical note  q learning  machine learning          
        
watkins  c          learning with delayed rewards  ph d  thesis  cambridge university 
wellman  m     doyle  j          modular utility representation for decision theoretic
planning  in proceedings of the first international conference on ai planning systems 
morgan kaufmann 
wellman  m          reasoning about preference models  tech  rep  mit lcs tr     
laboratory for computer science  mit 

   

fi