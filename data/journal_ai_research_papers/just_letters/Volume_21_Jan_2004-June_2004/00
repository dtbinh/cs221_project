journal artificial intelligence research               

submitted       published     

effective dimensions hierarchical latent class models
nevin l  zhang

lzhang cs ust hk

department computer science
hong kong university science technology  china

tomas kocka

kocka lisp vse cz

laboratory intelligent systems prague
prague university economics  czech republic

abstract
hierarchical latent class  hlc  models tree structured bayesian networks
leaf nodes observed internal nodes latent  theoretically well
justified model selection criteria hlc models particular bayesian networks
latent nodes general  nonetheless  empirical studies suggest bic score
reasonable criterion use practice learning hlc models  empirical studies
suggest sometimes model selection improved standard model dimension
replaced effective model dimension penalty term bic score 
effective dimensions difficult compute  paper  prove theorem
relates effective dimension hlc model effective dimensions number
latent class models  theorem makes computationally feasible compute
effective dimensions large hlc models  theorem used compute
effective dimensions general tree models 

   introduction
hierarchical latent class  hlc  models  zhang        tree structured bayesian networks
 bns  leaf nodes observed internal nodes latent  generalize latent
class models  lazarsfeld henry        first identified potentially useful
class bayesian networks pearl         concerned learning hlc models
data  fundamental question select among competing models 
bic score  schwarz        popular metric researchers use select among
bayesian network models  consists loglikelihood term measures fitness
data penalty term depends linearly upon standard model dimension  i e 
number linearly independent standard model parameters  variables
observed  bic score asymptotic approximation  the logarithm  marginal
likelihood  schwarz         consistent sense that  given sufficient data 
bic score generative model model data sampled larger
models equivalent generative model 
latent variables present  bic score longer asymptotic approximation marginal likelihood  geiger et al          remedied 
extent  using concept effective model dimension  fact replace standard model
dimension effective model dimension bic score  resulting scoring function 
called bice score  asymptotic approximation marginal likelihood almost
everywhere except singular points  rusakov geiger        
c
    
ai access foundation  rights reserved 

fizhang   kocka

neither bic bice proved consistent latent variable models 
matter fact  even defined means model selection criterion
consistent latent variable models  empirical studies suggest bic score
well behaved practice task learning hlc models  three related searchbased algorithms learning hlc models  namely double hill climbing  dhc   zhang 
       single hill climbing  shc   zhang et al          heuristic shc  hshc   zhang 
       absence theoretically well justified model selection criterion  zhang       
tested dhc four existing scoring functions  namely aic score  akaike        
bic score  cheeseman stutz  cs  score  cheeseman stutz         holdout
logarithmic score  hls  cowell et al          real world synthetic data used 
real world data  bic cs enabled dhc find models regarded
best domain experts  synthetic data  bic cs enabled dhc find
models either identical resemble closely true generative models 
coupled aic hls  hand  dhc performed significantly worse  shc
hshc tested synthetic data sampled fairly large hlc models  as much
   nodes   bic used tests  cases  bic enabled shc
hshc find models either identical resemble closely true generative
models  empirical results indicate algorithms perform well 
suggest bic reasonable scoring function use learning hlc models 
experiments reveal model selection sometimes improved bice
score used instead bic score  explain detail section  
order use bice score practice  need way compute effective dimensions  trivial task  effective dimension hlc model rank
jacobian matrix mapping parameters model parameters
joint distribution observed variables  number rows jacobian
matrix increases exponentially number observed variables  construction
jacobian matrix calculation rank computationally demanding 
moreover done algebraically high numerical precision avoid
degenerate cases  necessary precision grows size matrix 
settimi smith              studied effective dimensions two classes models 
trees binary variables latent class  lc  models two observed variables 
obtained complete characterization two classes  geiger et al         computed effective dimensions number models  conjectured rare
effective standard dimensions lc model differ  matter fact 
found one model  kocka zhang        found quite number lc models
whose effective standard dimensions differ  proposed easily computable
formula estimating effective dimensions lc models  estimation formula
empirically shown accurate 
paper  prove theorem relates effective dimension hlc model
effective dimensions two hlc models contain fewer latent variables 
repeated application theorem allows one reduce task computing effective
dimension hlc model subtasks computing effective dimensions lc models 
makes computationally feasible compute effective dimensions large hlc
models 
 

fieffective dimensions hlc models

start section   formal definition effective dimensions bayesian networks latent variables  section    provide empirical evidence suggest use
bice instead bic sometimes improves model selection  section   presents main
theorem section   devoted proof theorem  section    prove theorem effective dimensions general tree models explain main
theorem allows one compute effective dimension arbitrary tree models  finally 
concluding remarks provided section   

   effective dimensions bayesian networks
paper  use capital letters x denote variables lower case
letters x denote states variables  domain cardinality
variable x denoted x  x  respectively  bold face capital letters
denote sets variables  denotes cartesian product domains variables
set y  elements denoted bold lower case letters
sometimes referred states y  consider variables finite
number states 
consider bayesian network model possibly contains latent variables 
standard dimension ds m   number linearly independent parameters
standard parameterization   parameters denote  variable parent
configuration variable  probability variable state  except one 
given parent configuration  suppose consist k variables x    x            xk   let ri
qi respectively number states xi number possible combinations
states parents  xi parent  let qi    ds m   given
ds m    

k
x

qi  ri    

i  

 
notational simplicity  denote standard dimension n  let   
                n  
vector n linearly independent model parameters   let set
observed variables  suppose m   possible states  enumerate first states
y    y            ym  
  mapping
  im   p  yi   function parameters  
n

n dimensional parameter space  a subspace r   r   namely                    n  
 p  y     p  y             p  ym     jacobian matrix mapping following mn
matrix 
     jij       p  yi    
jm   
j
   understanding
convenience  often write matrix jm     p y 
j
elements j th column obtained allowing run possible states
except one 
  commonly used parameterizations
i  p  yi   function  
  hence make following
bayesian networks  actually polynomial function  
assumption 
 

fizhang   kocka

assumption   bayesian network parameterized parameters
joint distribution observed variables polynomial functions parameters
m 
obvious consequence assumption elements jm polynomial
 
functions  
  jm matrix real numbers  due assumption    rank
given value  
matrix constant almost everywhere parameter space  geiger et al  
      see section        specific  rank everywhere except set
measure zero smaller d  constant called regular rank jm  
regular rank jm called effective dimension bayesian network
model   hence denote de m    understand term effective dimension 
consider subspace rm spanned joint probability p  y  observed variables 
equivalently range mapping   term reflects fact that  almost every
  small enough open ball around   
  resembles euclidean space dimension
value  
 geiger et al         
multiple ways parameterize given bayesian network model  however 
choice parameterization affect space spanned joint probability p  y  
together interpretation previous paragraph  implies definition
effective dimension depend particular parameterization one uses 

   selecting among hlc models
hierarchical latent class  hlc  model bayesian network     network structure rooted tree     variables leaf nodes observed
variables not  observed variables sometimes referred manifest variables
variables latent variables  figure   shows structures two hlc
models  latent class  lc  model hlc model one latent variable 
theme paper computation effective dimensions hlc models 
mentioned introduction  interesting effective dimension  used
bic score  gives us better approximation marginal likelihood  section 
give example illustrate use effective dimension sometimes leads
better model selection  motivate introduce concept regularity
used subsequent sections 
    example model selection
consider two hlc models shown figure    one experiment  instantiated
parameters m  random fashion sampled set d         data records
observed variables  ran shc hshc data set d  guidance
bic score  algorithms produced model m    following  explain why 
based d    one would prefer m  m  bic used model selection m 
would preferred bice used instead  argue m  preferred based
d  hence bice better scoring metric case 
 

fieffective dimensions hlc models

x 

x 

x 
y 

y 

x 
y 

y 

y 

y 

y 

y 

m 

y 

x 

y 

y 

y 

m 

figure    two hlc models  shaded variables latent  variables
observed  cardinality x     cardinalities variables
  

bic bice scores model given data set defined follows 
ds m  
bic m  d    logp  d m     
logn 
 
de m  
bice m  d    logp  d m     
logn
 
  maximum likelihood estimate parameters based n
sample size 
example  notice m  includes m  sense m  represent
probability distributions observed variables m  can  fact  make
conditional probability distributions observed variables m  m 
set pm   x    pm   x   x   
pm   x   pm   x   x     

x

pm   x   pm   x   x   pm   x   x    

x 

probability distribution observed variables two models identical 
m  includes m    logp  d   m         logp  d   m          together
fact d  sampled m    implies logp  d   m         logp  d   m        
sufficiently large enough sample size  standard dimension m     
m      hence
bic m   d      bic m   d    
hand  effective dimensions m  m        respectively  hence
bice m   d      bice m   d    
model m  includes m    opposite clearly true effective dimension
m  smaller m    so  m  reality complex model m   
model fit data d  equally well  hence simpler one  i e  m    preferred
other  agrees choice bice score  disagrees choice
bic score  hence  bice appropriate bic case 
 

fizhang   kocka

    regularity
consider another model m  m  except cardinality x 
increased      easy show m  includes m  vice versa  so  two
models equivalent terms capabilities representing probability distributions
observed variables  hence said marginally equivalent  however  m 
standard parameters m  hence would always prefer m  m   
formalize consideration  introduce concept regularity 
latent variable z hlc model  enumerate neighbors  parent children 
x    x            xk   hlc model regular latent variable z 
 z 

qk

i    xi  
 
maxki    xi  

   

strict inequality holds z two neighbors least one
latent node  models m  m  regular  model m  not 
irregular model always exists regular model marginally equivalent fewer standard parameters  zhang      b   regular model
obtained follows  latent node two neighbors
cardinality smaller one neighbors  remove latent node
connect two neighbors  latent node two neighbors
violates      reduce cardinality quantity right hand side  repeat
steps changes made 
interesting note collection regular hlc models given set
observed variables finite  zhang         provides finite search space task
learning regular hlc models   rest paper  consider regular
hlc models 
ending subsection  point nice property effective model dimension
relation model inclusion  hlc model includes another model  effective
dimension less latter  consequence  two marginally equivalent
models effective dimensions hence bice score 
true standard model dimension bic score 
    cs cse scores
argued empirical grounds bic score reasonable scoring function
use learning hlc models bice score sometimes improve model
selection  two scores free problems  one problem derivation
laplace approximations marginal likelihood valid boundary
parameter space  cs score way alleviates problem  involves bic score
based completed data bic score based original data  words 
involves two laplace approximations marginal likelihood  lets errors two
approximation cancel other 
chickering heckerman        empirically found cs score quite accurate
approximation marginal likelihood robust boundary parameter
   definition regularity given paper slightly different one given zhang        
nonetheless  two conclusions mentioned paragraph remain true 

 

fieffective dimensions hlc models

x

z

x









z

z

x


m 

m 

figure    problem reduction 
space  realized need effective model dimension cs score  although
actually use it  would made differences experiments
because  models used  standard effective dimensions agree 
use cse refer scoring function one obtains replacing standard model
dimension cs score effective model dimensions  bice better
bic approximations marginal likelihood  geiger et al          cse better
cs  compute cse  need calculate effective dimensions 

   effective dimensions hlc models
seen  effective model dimension interesting number reasons 
main result paper theorem effective dimension de m   regular
hlc model contains one latent variable  let x root  
latent node  least two latent nodes  must exist another latent
node z child x  following  use terms x branch z branch
respectively refer sets nodes separated z x x z 
let set observed variables z branch let set
observed variables  note x branch doesnt contain node x  relationship
among x  z  y  depicted left most picture figure   
standard parameterization includes parameters p  x  parameters
p  z x   convenience  replace parameters parameters p  x  z  
mentioned end section    reparameterization affect effective
dimension de m    reflect reparameterization  edge x z
directed figure   
   

   

   

suppose p  x  z  k  parameters                 k    suppose conditional distri   

   

   

butions variables x branch consists k  parameters                 k 
   

   

conditional distributions variables z branch consists k  parameters        
   
        k    convenience sometimes refer three groups parameters using
three vectors                  respectively 
following  define two hlc models m  m  starting
establish relationship effective dimensions effective dimension
  context    m    m  regarded purely mathematical objects 
semantics variables concern  particular  variable h latent
 

fizhang   kocka

x 
x     
x 
x     

y     

x     

x     

y 

x 

x 
x 

x     

x 
x 

x 

x 
y     

y     

y     

x 
x 

y     
y 

y 

x 

x 

y 

y 

figure    picture left shows hlc model five observed five latent
variables  variable annotated name cardinality  picture
right shows components decompose hlc model
applying theorem    latent variables shaded  observed variables
not 

might designated observed m  m  part definition
mathematical objects 
obtain bayesian network model b  deleting z branch  strictly
speaking b  bayesian network due parameterization inherits   instead
probability tables p  x  p  z x   table p  x  z   p  x  p  z x 
readily obtained p  x  z   mind  view b  bayesian network 
network obviously tree structured  leaf variables include set
variable z  define m  hlc model share structure b 
variable z variables observed  parameters m 
           
similarly let b  bayesian network model obtained deleting xbranch  tree structure leaf variables include variable x 
define m  hlc model share structure b 
variable x variables observed  parameters m             
theorem   suppose regular hlc model contains two latent nodes 
two hlc models m  m  defined text regular  moreover 
de m     de m    de m    ds m    ds m   ds m    

   

words  effective dimension equals sum effective dimensions m 
m  minus number common parameters m  m  share 
appreciate significance theorem  consider task computing effective dimension regular hlc model contains two latent nodes 
 

fieffective dimensions hlc models

repeatedly applying theorem  reduce task subtasks calculating effective dimensions lc models  example  consider hlc model depicted
picture left figure    theorem   allows us to  purpose computing
effective dimension  decompose hlc model five lc models  shown
right figure   
might one compute effective dimension lc model  one way use
algorithm suggested geiger et al          algorithm first symbolically computes
jacobian matrix  possible due assumption    randomly assigns
values parameters  resulting numerical matrix  rank numerical matrix
computed diagonalization  rank jacobian matrix equals effective
dimension lc model almost everywhere  get regular rank probability
one  algorithm recently implemented rusakov geiger         kocka
zhang        suggest alternative algorithm computes upper bound 
algorithm fast empirically shown produce extremely tight bounds 
going back example  effective dimension lc models x    x    x    x 
x                    respectively  thus effective dimension hlc model
figure                                            contrast 
standard dimension model                                  

   proof main result
section devoted proof theorem    begin properties
jacobian matrices bayesian network models 
    properties jacobian matrices
consider jacobian matrix jm bayesian network model   matrix parameterized parameters     let v    v            vm column vectors jm  
lemma   number column vectors v    v            vm jacobian matrix jm
either linearly dependent everywhere linearly independent almost everywhere 
linearly dependent everywhere exists least one column vector vj
expressed linear combination column vectors everywhere 
proof  consider diagonalizing following transposed matrix 
 v    v            vm  t  
  hence would
according assumption    elements matrix polynomials  of   
multiply rows polynomials fraction polynomials  course  need add
one row another row  end process  get diagonal matrix whose nonzero
elements polynomials fractions polynomials  suppose k nonzero rows
suppose correspond v    v            vk  
elements diagonalized matrix polynomials fractions polynomials 
 
well defined   nonzero almost everywhere  i e  almost values   
k m  vectors linearly independent almost everywhere 
   fraction well defined denominator zero 

 

fizhang   kocka

k m  exist  j  k jm   polynomials fractions polynomials ci
  ik 
vj  

k
x

ci vi  

   

i  

coefficients ci determined tracing diagonalization process  vj
expressed linear combination  vi  i              k  everywhere      
although might sound trivial  lemma actually quite interesting 
jm parameterized matrix  first part  example  implies exist
two subspaces parameter space nonzero measures
vectors linearly independent one subspace linearly dependent other 
total number column vectors jm   get following lemma 
lemma   jacobian matrix jm   exists collection column vectors form
basis column space almost everywhere  number vectors collection
equals regular rank matrix  moreover  collection chosen include
given set column vectors linearly independent almost everywhere 
proof  first part already proved  second part follows definition
regular rank  last part true could start diagonalization process
transpose vectors set top matrix   
    proof theorem  
set prove theorem    straightforward verify hlc models
m  m  regular  suffices prove equation      rest
section 
set observed variables y  set observed variables m 
 z  set observed variables m   x   hence jacobian matrices
models   m    m  respectively written follows 
jm

   

jm 

   

jm 

   

p  o  y 

     

p  o  y  p  o  y 
p  o  y  p  o  y 
p  o  y 
 
 
 
     
     
   
   
   
   
   
k 
 
k 
 
k 

     

p  o  z  p  o  z 
p  o  z 
 
 
     
   
   
   
k 
 
k 

   
 

p  o  z 
   
 

p  x  y 
   
 

     

p  x  y  p  x  y 
p  x  y 
 
 
     
   
   
   
k 
 
k 

  ci might undefined
   subtle point here  fractions polynomials  
  equation     alone  cannot conclude vj linearly depends  vi  i              k 
values  
everywhere 
conclusion nonetheless true two reasons  first set   values ci
undefined measure zero  second  vj linearly depend  vi  i              k  one value
  would true sufficiently small nonetheless measure positive ball around
 
value 

  

fieffective dimensions hlc models

clear one to one correspondence first k   k  column vectors
jm column vectors jm  one to one correspondence
first k  last k  column vectors jm column vectors jm   
first show
claim    first k  vectors jm  jm  jm    linearly independent
almost everywhere 
together lemma    claim   implies collection column vectors
jm  includes first k  vectors basis column space jm  almost
everywhere  particular  implies de m   k    suppose de m    k   r  without
loss generality  suppose basis vectors
p  o  z 
   
 

     

p  o  z  p  o  z 
p  o  z 
 
 
     
   
   
   
k 
 
r

   

symmetry  assume de m    k   s s  following column
vectors form basis jm  almost everywhere 
p  x  y 
   
 

     

p  x  y  p  x  y 
p  x  y 
 
 
     
   
   
   
k 
 


   

consider following list vectors jm  
p  o  y 
   
 

     

p  o  y  p  o  y 
p  o  y  p  o  y 
p  o  y 
 
 
 
     
     
   
   
   
   
   
k 
 
r
 


   

show
claim    column vectors jm linearly depend vectors listed    
everywhere 
claim    vectors listed     linearly independent almost everywhere 
two claims imply vectors listed     form basis column space
jm almost everywhere  therefore
de m     k   r s   de m    de m   k   
clear k   ds m    ds m   ds m    therefore theorem   proved   
    proof claim  
lemma   let z latent node hlc model set observed
nodes subtree rooted z  regular  set conditional distributions
nodes subtree way encode injective mapping z
sense p  y  z  z z      z z  
  

fizhang   kocka

proof  prove lemma induction number latent nodes subtree
rooted z  first consider case one latent node  namely z 
case  z parent nodes y  enumerate nodes y    y            yk  
q
regular   z  ki    yi    hence define injective mapping
q
z   ki   yi   state z z   z  written    y    y            yk   
yi state yi   set
p  yi  yi  z z      
p  y  z  z z      
consider case least two hidden nodes subtree rooted
z  let w one latent node latent node descendants  let    
set observed nodes subtree rooted w y     y y      induction
hypothesis  parameterize subtree rooted w way encodes
injective mapping w y      moreover  nodes w removed  
remains regular hlc model  model  parameterize subtree rooted
z way encodes injective mapping z  w y        w y     
together  two facts prove lemma   
corollary   let z latent node hlc model   suppose z latent neighbor
x  let set observed nodes separated x z  regular 
set probability distributions nodes separated x z way
encode injective mapping z sense p  y  z  z z     
z z  
proof  corollary follows readily lemma   property root walking
operation  zhang          
proof claim    consider following matrix
 
   

   

p  x  z 
   
 

    

p  x  z 
   

k 

 

   

   

                k  parameters joint distribution p  x  z   matrix
identity matrix rows properly arranged  column vectors linearly
independent almost everywhere 
   
   
consider first k  column vectors jm   p  o  y              p  o  y  k   
must linearly independent almost everywhere  not  one vectors  say
   
p  o  y  k    would linearly depend rest everywhere according lemma   
observe   ik    
p  o  y 
   


 

x

p  o x p  y z 

x z

p  x  z 
   



 
   

choose p  o x  p  y z  corollary    vector p  o  y  i might contain zero elements  remove zero elements  remains vector iden   
   
tical p  x  z  i   conclude p  x  z  k  linearly depends
  

fieffective dimensions hlc models

   

   

p  x  z            p  x  z  k    everywhere  contradicts conclusion
previous paragraph  hence first k  vectors jm must linearly independent almost
everywhere 
evident that  using similar arguments  show first k  vectors
jm   jm    linearly independent almost everywhere  claim   therefore proved   
    proof claim  
every column vector jm  linearly depends vectors listed     everywhere  observe

p  o  y 
   


x

p  y z 

z

p  o  y 
   


 
 

x

p  y z 

z

p  o  z 
   

               k 

   

               k   


p  o  z 


therefore every column vector jm corresponds vectors jm  linearly depends
first k   r vectors listed     everywhere 
symmetry  every column vector jm corresponds vectors jm  linearly
depends first k  last vectors listed     everywhere  claim proved 
 
    proof claim  
prove claim contradiction  assume vectors listed     linearly
independent almost everywhere  according lemma    one them  say v  must linearly
depend rest everywhere  claim   lemma    assume v
   
among last r s vectors  without loss generality  assume v p  o  y  s  
  exist real numbers ci   ik     c      ir   c   
value  


  is  
p  o  y 
   


 

k 
x
i  

ci

p  o  y 
   


 

r
x

    p  o  y 
   

i  

ci

 

s 
x

    p  o  y 
 
   

i  

ci

note last term right hand side  runs   s  
parameter vector   consists three subvectors                    set parameters
   
   for x branch  lemma    exists injective mapping x

p  o  x  x x      x x  

   

vectors      consider subvector consisting elements
states images states x mapping   subvectors
   
   
   
denoted p  ox   y  i   p  ox   y  i   p  ox   y  i  
values             still
  

fizhang   kocka

p  ox   y 

 

   


k 
x

ci

p  ox   y 

i  

   


 

r
x

    p  ox   y 
   

i  

ci

 

s 
x

    p  ox   y 
 
   

i  

ci

   

consider first two terms right hand side 
k 
x

ci

r
x

    p  ox   y 
   

i  
k 
r
x
x
p  ox   z 
p  ox   z  x
    x
c
ci
 
p  y z 
p  y z 

   
   


i  
i  
z
z
k 
r
x
x
p  ox   z  x
    p  ox   z 
ci
 
 
p  y z   ci
   
   


i  
i  
z

p  ox   y 
   


i  

 
 

 

ci

p

    fact p  o  z   
x p  x  z p  o x   column vector
   
   
p  ox   z  i identical vector p  x  z  i   argued proving
   
claim    vectors  p  x  z  i  i            k    constitute basis k   dimensional
   
euclidian space  implies that  vectors p  ox   z  i represented
   
linear combination vectors  p  ox   z  i  i              k     consequently 
exist ci   ik   
k 
x

ci

p  ox   z 

i  

   


k 
x

p  ox   y 

 

r
x

    p  ox   z 
   

i  

ci

 

k 
x

ci

p  ox   z 

ci

p  ox   y 

i  

   



hence

i  

ci

 

   


r
x

    p  ox   y 
   

i  

ci

 

k 
x
i  

   



combining equation equation      get
p  ox   y 
   


 

k 
x

ci

p  ox   y 

i  

   


 

s 
x

    p  ox   y 
 
   

i  


ci

p

    fact fact p  o  y    x p  x  y p  o x   column
   
   
vector p  ox   y  i identical vector p  x  y  i column vector
   
   
p  ox   y  i identical vector p  x  y  i   hence
p  x  y 
   


 

k 
x
i  

ci

p  x  y 
   


 

s 
x

    p  x  y 
 
   

i  

ci

contradicts fact vectors equation form basis column space
jm  almost everywhere  see     section      therefore  claim   must true   
  

fieffective dimensions hlc models

   effective dimensions trees
let us use term tree model refer markov random fields undirected trees
finite number random variables  root tree model nodes  get
tree structured bayesian network model  tree model  define leaf nodes
one neighbor  hlc model tree model leaf nodes observed
others latent 
turns theorem   enables us compute effective dimension tree
model  consider arbitrary tree model  leaf nodes latent  remove
nodes without affecting effective dimension 
removing latent leaf nodes  leaf nodes observed  non leaf nodes
observed  decompose model submodels observed non leaf
node  following theorem tells us model submodels related terms
effective dimensions 
theorem   suppose observed non leaf node tree model   decomposes
k submodels m            mk  
de m    

k
x

de mi    k     y      

i  

possible decompositions  final submodels either contain latent nodes
hlc models  effective dimensions submodels latent variables simply
standard dimensions  hlc submodel irregular  make regular applying
transformation mentioned end section      transformation affect
effective dimensions submodels  finally  effective dimensions regular hlc
submodels computed using theorem   
proof theorem    possible prove theorem starting jacobian
matrix  take less formal revealing approach 
suffices consider case k    two submodels m  m  share one
node  namely   let o  o  respectively sets observed nodes two
submodels excluding   root  
p  y  o    o   p  y     p  o     p  o      
let    set parameters distribution p  y          respectively sets
parameters conditional probability distributions nodes m  m    consider
fixing    letting       vary  case  space spanned p  y   consists
one vector  namely    itself  moreover  one to one correspondence vectors
space spanned p  y  o    o    vectors cartesian product spaces
spanned p  o      p  o       let    vary  adds  y    dimensions
four spaces spanned p  y  o    o     p  y    p  o       p  o       consequently 

de m     de m      de m      y      
theorem proved   
  

fizhang   kocka

   concluding remarks
paper study effective dimensions hlc models  work motivated
empirical evidence bic behaves quite well used several hill climbing
algorithms learning hlc models bice score sometimes leads better
model selection bic score  proved theorem relates effective
dimension hlc model effective dimensions two hlc models
contain fewer latent variables  repeated application theorem allows one reduce
task computing effective dimension hlc model subtasks computing
effective dimensions lc models  makes computationally feasible compute
effective dimensions large hlc models  addition  proved theorem
effective dimensions general tree models  main theorem allows one
compute effective dimension arbitrary tree models 
acknowledgements
work initiated authors visiting department computer science 
aalborg university  denmark  thank poul s  eriksen  finn v  jensen  jiri vomlel 
marta vomlelova  thomas d  nielsen  olav bangso  jose pena  kristian g  olesen 
grateful annonymous reviewers whose comments helped us greatly
improving paper  research paper partially supported ga cr grant
            hong kong research grant council grant hkust       e 

references
akaike  h          new look statistical model identification  ieee trans  autom 
contr               
bartholomew  d  j  knott  m          latent variable models factor analysis   nd
edition  kendalls library statistics    london  arnold 
cheeseman  p  stutz  j          bayesian classification  autoclass   theory
results  fayyad  u   piatesky shaoiro  g   smyth  p   uthurusamy  r   eds   
advancesin knowledge discovery data mining  aaai press  menlo park  ca 
chickering d  m  heckerman d          efficient approximations marginal
likelihood bayesian networks hidden variables  machine learning             
cowell  r  g   dawid  a  p   lauritzen  s  l   spiegelhalter  d  j          probabilistic
networks expert systems  springer 
kocka  t  zhang  n  l          dimension correction hierarchical latent class
models  proc    th conference uncertainty artificial intelligence
 uai     
geiger d   heckerman d  meek c          asymptotic model selection directed
networks hidden variables  proc    th conference uncertainty
artificial intelligence          
  

fieffective dimensions hlc models

goodman  l  a          exploratory latent structure analysis using identifiable
unidentifiable models  biometrika              
lazarsfeld  p  f   henry  n w         
mifflin 
rusakov  d  geiger  d         
networks  uai    

latent structure analysis  boston  houghton

asymptotic model selection naive bayesian

rusakov  d  geiger  d          automated analytic asymptotic evaluation marginal
likelihood latent models  uai    
schwarz g          estimating dimension model  annals statistics             
settimi  r  smith  j q          geometry bayesian graphical models
hidden variables  proceedings fourteenth conference uncertainty
artificial intelligence  morgan kaufmann publishers  s  francisco  ca          
settimi  r  smith  j q          geometry  moments bayesian networks hidden
variables  proceedings seventh international workshop artificial intelligence statistics  fort lauderdale  florida      january        morgan kaufmann
publishers  s  francisco  ca 
zhang n  l          hierarchical latent class models cluster analysis  aaai             
zhang  n  l   kocka  t   karciauskas  g   jensen  f  v          learning hierarchical
latent class models  technical report hkust cs       department computer
science  hong kong university science technology 
zhang  n  l         
structural em hierarchical latent class models  technical
report hkust cs       department computer science  hong kong university
science technology 
zhang  n  l       b   hierarchical latent class models cluster analysis  journal
machine learning research  appear 

  


