journal of artificial intelligence research               

submitted       published     

effective dimensions of hierarchical latent class models
nevin l  zhang

lzhang cs ust hk

department of computer science
hong kong university of science and technology  china

tomas kocka

kocka lisp vse cz

laboratory for intelligent systems prague
prague university of economics  czech republic

abstract
hierarchical latent class  hlc  models are tree structured bayesian networks where
leaf nodes are observed while internal nodes are latent  there are no theoretically well
justified model selection criteria for hlc models in particular and bayesian networks with
latent nodes in general  nonetheless  empirical studies suggest that the bic score is a
reasonable criterion to use in practice for learning hlc models  empirical studies also
suggest that sometimes model selection can be improved if standard model dimension is
replaced with effective model dimension in the penalty term of the bic score 
effective dimensions are difficult to compute  in this paper  we prove a theorem that
relates the effective dimension of an hlc model to the effective dimensions of a number
of latent class models  the theorem makes it computationally feasible to compute the
effective dimensions of large hlc models  the theorem can also be used to compute the
effective dimensions of general tree models 

   introduction
hierarchical latent class  hlc  models  zhang        are tree structured bayesian networks
 bns  where leaf nodes are observed while internal nodes are latent  they generalize latent
class models  lazarsfeld and henry        and were first identified as a potentially useful
class of bayesian networks by pearl         we are concerned with learning hlc models
from data  a fundamental question is how to select among competing models 
the bic score  schwarz        is a popular metric that researchers use to select among
bayesian network models  it consists of a loglikelihood term that measures the fitness
to data and a penalty term that depends linearly upon standard model dimension  i e 
the number of linearly independent standard model parameters  when all variables are
observed  the bic score is an asymptotic approximation of  the logarithm  of the marginal
likelihood  schwarz         it is also consistent in the sense that  given sufficient data  the
bic score of the generative model  the model from which data were sampled  is larger
than those of any other models that are not equivalent to the generative model 
when latent variables are present  the bic score is no longer an asymptotic approximation of the marginal likelihood  geiger et al          this can be remedied  to some
extent  using the concept of effective model dimension  in fact if we replace standard model
dimension with effective model dimension in the bic score  the resulting scoring function 
called the bice score  is an asymptotic approximation of the marginal likelihood almost
everywhere except for some singular points  rusakov and geiger        
c
    
ai access foundation  all rights reserved 

fizhang   kocka

neither bic nor bice have been proved to be consistent for latent variable models  as
a matter of fact  it has not even been defined what it means for a model selection criterion
to be consistent for latent variable models  empirical studies suggest that the bic score is
well behaved in practice for the task of learning hlc models  there are three related searchbased algorithms for learning hlc models  namely double hill climbing  dhc   zhang 
       single hill climbing  shc   zhang et al          and heuristic shc  hshc   zhang 
       in the absence of a theoretically well justified model selection criterion  zhang       
tested dhc with four existing scoring functions  namely the aic score  akaike         the
bic score  the cheeseman stutz  cs  score  cheeseman and stutz         and the holdout
logarithmic score  hls  cowell et al          both real world and synthetic data were used 
on the real world data  bic and cs have enabled dhc to find models that are regarded as
the best by domain experts  on the synthetic data  bic and cs have enabled dhc to find
models that either are identical to or resemble closely the true generative models  when
coupled with aic and hls  on the other hand  dhc performed significantly worse  shc
and hshc were tested on synthetic data sampled from fairly large hlc models  as much
as    nodes   only bic was used in those tests  in all cases  bic has enabled shc and
hshc to find models that either are identical to or resemble closely the true generative
models  those empirical results not only indicate that the algorithms perform well  but
also suggest that the bic is a reasonable scoring function to use for learning hlc models 
the experiments also reveal that model selection can sometimes be improved if the bice
score is used instead of the bic score  we will explain this in detail in section  
in order to use the bice score in practice  we need a way to compute effective dimensions  this is not a trivial task  the effective dimension of an hlc model is the rank of
the jacobian matrix of the mapping from the parameters of the model to the parameters
of the joint distribution of the observed variables  the number of rows in the jacobian
matrix increases exponentially with the number of observed variables  the construction of
the jacobian matrix and the calculation of its rank are both computationally demanding 
moreover they have to be done algebraically or with very high numerical precision to avoid
degenerate cases  the necessary precision grows with the size of the matrix 
settimi and smith              studied effective dimensions for two classes of models 
trees with binary variables and latent class  lc  models with two observed variables  they
have obtained a complete characterization of these two classes  geiger et al         computed the effective dimensions of a number of models  they conjectured that it is rare for
the effective and standard dimensions of an lc model to differ  as a matter of fact  they
found only one such model  kocka and zhang        found quite a number of lc models
whose effective and standard dimensions differ  they also proposed an easily computable
formula for estimating effective dimensions of lc models  the estimation formula has been
empirically shown to be very accurate 
in this paper  we prove a theorem that relates the effective dimension of an hlc model
to the effective dimensions of two other hlc models that contain fewer latent variables 
repeated application of the theorem allows one to reduce the task of computing the effective
dimension of an hlc model to subtasks of computing effective dimensions of lc models 
this makes it computationally feasible to compute the effective dimensions of large hlc
models 
 

fieffective dimensions of hlc models

we start in section   with a formal definition of effective dimensions for bayesian networks with latent variables  in section    we provide empirical evidence that suggest the use
of bice instead of bic sometimes improves model selection  section   presents the main
theorem and section   is devoted to the proof of the theorem  in section    we prove a theorem about effective dimensions of general tree models and explain how this and our main
theorem allows one to compute the effective dimension of arbitrary tree models  finally 
concluding remarks are provided in section   

   effective dimensions of bayesian networks
in this paper  we use capital letters such as x and y to denote variables and lower case
letters such as x and y to denote states of variables  the domain and cardinality of a
variable x will be denoted by x and  x  respectively  bold face capital letters such as y
denote sets of variables  y denotes the cartesian product of the domains of all variables
in the set y  elements of y will be denoted by bold lower case letters such as y and will
sometimes be referred to as states of y  we will consider only variables that have a finite
number of states 
consider a bayesian network model m that possibly contains latent variables  the
standard dimension ds m   of m is the number of linearly independent parameters in the
standard parameterization of m   the parameters denote  for each variable and each parent
configuration of the variable  the probability that the variable is in some state  except one 
given the parent configuration  suppose m consist of k variables x    x            xk   let ri and
qi be respectively the number of states of xi and the number of all possible combinations of
the states of its parents  if xi has no parent  let qi be    then ds m   is given by
ds m    

k
x

qi  ri     

i  

 
for notational simplicity  denote the standard dimension of m by n  let   
                n  
be a vector of n linearly independent model parameters of m   further let y be the set of
observed variables  suppose y has m   possible states  we enumerate the first m states
as y    y            ym  
  so we have a mapping from
for any i   im   p  yi   is a function of the parameters  
n
m
the n dimensional parameter space  a subspace of r   to r   namely t                    n   
 p  y     p  y             p  ym     the jacobian matrix of this mapping is the following mn
matrix 
     jij       p  yi    
jm   
j
   with the understanding
for convenience  we will often write the matrix as jm     p y 
j
that elements of the j th column are obtained by allowing y run over all its possible states
except one 
  for most commonly used parameterizations of
for each i  p  yi   is a function of  
  hence we make the following
bayesian networks  it is actually a polynomial function of  
assumption 
 

fizhang   kocka

assumption   the bayesian network m is so parameterized that the parameters for the
joint distribution of the observed variables are polynomial functions of the parameters for
m 
an obvious consequence of the assumption is that elements of jm are also polynomial
 
functions of  
  jm is a matrix of real numbers  due to assumption    the rank
for a given value of  
of this matrix is some constant d almost everywhere in the parameter space  geiger et al  
      also see section        to be more specific  the rank is d everywhere except in a set
of measure zero where it is smaller than d  the constant is called the regular rank of jm  
the regular rank of jm is also called the effective dimension of the bayesian network
model m   hence we denote it by de m    to understand the term effective dimension 
consider the subspace of rm spanned by the joint probability p  y  of observed variables 
or equivalently the range of the mapping t   the term reflects the fact that  for almost every
  a small enough open ball around t   
  resembles euclidean space of dimension d
value of  
 geiger et al         
there are multiple ways to parameterize a given bayesian network model  however  the
choice of parameterization does not affect the space spanned by the joint probability p  y  
together with the interpretation of the previous paragraph  this implies that the definition
of effective dimension does not depend on the particular parameterization that one uses 

   selecting among hlc models
a hierarchical latent class  hlc  model is a bayesian network where     the network structure is a rooted tree and     the variables at the leaf nodes are observed and all the other
variables are not  the observed variables are sometimes referred to as manifest variables
and all the other variables as latent variables  figure   shows the structures of two hlc
models  a latent class  lc  model is an hlc model where there is only one latent variable 
the theme of this paper is the computation of effective dimensions of hlc models  as
mentioned in the introduction  this is interesting because effective dimension  when used in
the bic score  gives us a better approximation of the marginal likelihood  in this section 
we give an example to illustrate that the use of effective dimension sometimes also leads to
better model selection  we will also motivate and introduce the concept of regularity that
will be used in subsequent sections 
    an example of model selection
consider the two hlc models shown in figure    in one experiment  we instantiated the
parameters of m  in a random fashion and sampled a set d  of        data records on the
observed variables  then we ran shc and hshc on the data set d  under the guidance
of the bic score  both algorithms produced model m    in the following  we explain why 
based on d    one would prefer m  over m  if bic is used for model selection and why m 
would be preferred if bice is used instead  we argue that m  should be preferred based on
d  and hence bice is a better scoring metric for this case 
 

fieffective dimensions of hlc models

x 

x 

x 
y 

y 

x 
y 

y 

y 

y 

y 

y 

m 

y 

x 

y 

y 

y 

m 

figure    two hlc models  the shaded variables are latent  while the other variables are
observed  the cardinality of x  is    while cardinalities of all other variables are
  

the bic and bice scores of a model m given a data set d are defined as follows 
ds m  
bic m  d    logp  d m      
logn 
 
de m  
bice m  d    logp  d m      
logn
 
where   is the maximum likelihood estimate of the parameters of m based on d and n is
the sample size 
in our example  notice that m  includes m  in the sense that m  can represent any
probability distributions of the observed variables that m  can  in fact  if we make the
conditional probability distributions of the observed variables in m  the same as in m  and
set pm   x    and pm   x   x    such that
pm   x   pm   x   x     

x

pm   x   pm   x   x   pm   x   x    

x 

then the probability distribution of the observed variables in the two models are identical 
because m  includes m    we have logp  d   m          logp  d   m          together with
the fact that d  is sampled from m    this implies that logp  d   m          logp  d   m        
for sufficiently large enough sample size  the standard dimension of m  is     while that
of m  is     hence
bic m   d      bic m   d    
on the other hand  the effective dimensions of m  and m  are    and    respectively  hence
bice m   d      bice m   d    
model m  includes m    the opposite is clearly not true because the effective dimension
of m  is smaller than that of m    so  m  is in reality a more complex model than m    both
model fit data d  equally well  hence the simpler one  i e  m    should be preferred over
the other  this agrees with the choice of the bice score  while disagrees with the choice of
the bic score  hence  bice is more appropriate than bic in this case 
 

fizhang   kocka

    regularity
now consider another model m  that is the same as m  except that the cardinality of x 
is increased from   to    it is easy to show that m  includes m  and vice versa  so  the two
models are equivalent in terms of their capabilities of representing probability distributions
of the observed variables  they are hence said to be marginally equivalent  however  m 
has more standard parameters than m  and hence we would always prefer m  over m    to
formalize this consideration  we introduce a concept of regularity 
for a latent variable z in an hlc model  enumerate its neighbors  parent and children 
as x    x            xk   an hlc model is regular if for any latent variable z 
 z  

qk

i    xi  
 
maxki    xi  

   

and the strict inequality holds when z has two neighbors and at least one of them is a
latent node  models m  and m  are regular  while model m  is not 
for any irregular model m there always exists a regular model that is marginally equivalent to m and has fewer standard parameters  zhang      b   the regular model can
be obtained from m as follows  for any latent node that has only two neighbors and its
cardinality is no smaller than that of one of the neighbors  then remove the latent node and
connect the two neighbors  for any latent node that has more than two neighbors and that
violates      reduce its cardinality to the quantity on the right hand side  repeat both
steps until no more changes can be made 
it is also interesting to note that the collection of all regular hlc models for a given set
of observed variables is finite  zhang         this provides a finite search space for the task
of learning regular hlc models   in the rest of this paper  we will consider only regular
hlc models 
before ending this subsection  we point out a nice property of effective model dimension
in relation to model inclusion  if an hlc model includes another model  then its effective
dimension is no less than that of the latter  as a consequence  two marginally equivalent
models have the same effective dimensions and hence the same bice score  the same is
not true for standard model dimension and the bic score 
    the cs and cse scores
we have argued on empirical grounds that the bic score is a reasonable scoring function
to use for learning hlc models and that the bice score can sometimes improve model
selection  but the two scores are not free of problems  one problem is that their derivation
as laplace approximations of the marginal likelihood are not valid at the boundary of the
parameter space  the cs score in a way alleviates this problem  it involves the bic score
based on completed data and the bic score based on original data  in other words  it
involves two laplace approximations of the marginal likelihood  it lets errors in the two
approximation cancel each other 
chickering and heckerman        empirically found the cs score to be a quite accurate
approximation of the marginal likelihood and robust at the boundary of the parameter
   the definition of regularity given in this paper is slightly different from the one given in zhang        
nonetheless  the two conclusions mentioned in this paragraph remain true 

 

fieffective dimensions of hlc models

x

z

x

o

y

o

m

z

z

x

y
m 

m 

figure    problem reduction 
space  they realized the need for effective model dimension in the cs score  although they
did not actually use it  this would not have made any differences to their experiments
because  for the models they used  the standard and effective dimensions agree 
we use cse to refer to the scoring function one obtains by replacing standard model
dimension in the cs score with effective model dimensions  just as bice is better than
bic as approximations of the marginal likelihood  geiger et al          cse is better than
cs  to compute cse  we also need to calculate effective dimensions 

   effective dimensions of hlc models
as we have seen  effective model dimension is interesting for a number of reasons  our
main result in this paper is a theorem about the effective dimension de m   of a regular
hlc model m that contains more than one latent variable  let x be the root of m   which
is a latent node  because there are at least two latent nodes  there must exist another latent
node z that is a child of x  in the following  we will use the terms x branch and z branch
to respectively refer to the sets of nodes that are separated from z by x or from x by z 
let y be the set of observed variables in the z branch and let o be the set of all other
observed variables  note that the x branch doesnt contain the node x  the relationship
among x  z  y  and o is depicted in the left most picture of figure   
the standard parameterization of m includes parameters for p  x  and parameters for
p  z x   for convenience  we replace those parameters with parameters for p  x  z   as
mentioned at the end of section    such reparameterization does not affect the effective
dimension de m    to reflect the reparameterization  the edge between x and z is not
directed in figure   
   

   

   

suppose p  x  z  has k  parameters                 k    suppose the conditional distri   

   

   

butions of variables in the x branch consists of k  parameters                 k  and the
   

   

conditional distributions of variables in the z branch consists of k  parameters        
   
        k    for convenience we will sometimes refer to those three groups of parameters using
three vectors             and      respectively 
in the following  we will define two other hlc models m  and m  starting from m and
establish a relationship between their effective dimensions and the effective dimension of
m   in this context  m   m    and m  are regarded purely as mathematical objects  the
semantics of their variables are of no concern  in particular  a variable h that is latent
 

fizhang   kocka

x 
x     
x 
x     

y     

x     

x     

y 

x 

x 
x 

x     

x 
x 

x 

x 
y     

y     

y     

x 
x 

y     
y 

y 

x 

x 

y 

y 

figure    the picture on the left shows an hlc model with five observed and five latent
variables  each variable is annotated by its name and its cardinality  the picture
on the right shows the components we can decompose the hlc model into by
applying theorem    latent variables are shaded  while observed variables are
not 

in m might be designated to be observed in m  or m  as part of the definition of those
mathematical objects 
we obtain a bayesian network model b  from m by deleting the z branch  strictly
speaking b  is not bayesian network due to the parameterization it inherits from m   instead
of probability tables p  x  and p  z x   we have table p  x  z   but p  x  and p  z x  can
readily be obtained from p  x  z   with this in mind  we view b  as a bayesian network 
this network is obviously tree structured  its leaf variables include those in the set o and
the variable z  we define m  to be the hlc model that share the same structure as b 
and where the variable z and all the variables in o are observed  the parameters of m 
are      and       
similarly let b  be the bayesian network model obtained from m by deleting the xbranch  it is a tree structure and its leaf variables include those in y and the variable x 
we define m  to be the hlc model that share the same structure as b  and where the
variable x and all the variables in y are observed  the parameters of m  are      and       
theorem   suppose m is a regular hlc model that contains two or more latent nodes 
then the two hlc models m  and m  defined in the text are also regular  moreover 
de m     de m    de m    ds m    ds m   ds m    

   

in words  the effective dimension of m equals the sum of the effective dimensions of m 
and m  minus the number of common parameters that m  and m  share 
to appreciate the significance of this theorem  consider the task of computing the effective dimension of a regular hlc model that contains two or more latent nodes  by
 

fieffective dimensions of hlc models

repeatedly applying the theorem  we can reduce the task into subtasks of calculating effective dimensions of lc models  as an example  consider the hlc model depicted by the
picture on the left in figure    theorem   allows us to  for the purpose of computing its
effective dimension  decompose the hlc model into five lc models  which are shown on
the right in figure   
how might one compute the effective dimension of an lc model  one way is to use
the algorithm suggested by geiger et al          the algorithm first symbolically computes
the jacobian matrix  which is possible due to assumption    then it randomly assigns
values to the parameters  resulting a numerical matrix  the rank of the numerical matrix
is computed by diagonalization  because the rank of jacobian matrix equals the effective
dimension of the lc model almost everywhere  we get the regular rank with probability
one  this algorithm has recently been implemented by rusakov and geiger         kocka
and zhang        suggest an alternative algorithm that computes an upper bound  the
algorithm is fast and has been empirically shown to produce extremely tight bounds 
going back to our example  the effective dimension of the lc models for x    x    x    x 
and x  are                and    respectively  thus the effective dimension of the hlc model
in figure   is                                          in contrast 
the standard dimension of the model is                                  

   proof of main result
this section is devoted to the proof of theorem    we begin with some properties of
jacobian matrices of bayesian network models 
    properties of jacobian matrices
consider the jacobian matrix jm of a bayesian network model m   it is a matrix parameterized by the parameters   of m   let v    v            vm be column vectors of jm  
lemma   a number of column vectors v    v            vm of the jacobian matrix jm are
either linearly dependent everywhere or linearly independent almost everywhere  they are
linearly dependent everywhere if and only if there exists at least one column vector vj that
can be expressed as a linear combination of other column vectors everywhere 
proof  consider diagonalizing the following transposed matrix 
 v    v            vm  t  
  hence we would
according to assumption    elements of the matrix are polynomials  of   
multiply rows with polynomials or fraction of polynomials  of course  we need also to add
one row to another row  at the end of the process  we get a diagonal matrix whose nonzero
elements are polynomials or fractions of polynomials  suppose there are k nonzero rows
and suppose they correspond to v    v            vk  
because elements of the diagonalized matrix are polynomials or fractions of polynomials 
  if
they are well defined   and nonzero almost everywhere  i e  for almost all values of   
k m  then the m vectors are linearly independent of each other almost everywhere 
   a fraction is not well defined if the denominator is zero 

 

fizhang   kocka

if k m  there exist  for each j  k jm   polynomials or fractions of polynomials ci
  ik  such that
vj  

k
x

ci vi  

   

i  

the coefficients ci s can be determined by tracing the diagonalization process  so vj can be
expressed as a linear combination of  vi  i              k  everywhere      
although it might sound trivial  this lemma is actually quite interesting  this is because
jm is a parameterized matrix  the first part  for example  implies that there do not exist
two subspaces of the parameter space that both have nonzero measures such that the m
vectors are linearly independent in one subspace while linearly dependent in the other 
if m is the total number of column vectors of jm   we get the following lemma 
lemma   in the jacobian matrix jm   there exists a collection of column vectors that form
a basis of its column space almost everywhere  the number of vectors in the collection
equals to the regular rank of the matrix  moreover  the collection can be chosen to include
any given set of column vectors that are linearly independent almost everywhere 
proof  the first part has already been proved  the second part follows from the definition
of regular rank  the last part is true because we could start the diagonalization process
with the transpose of the vectors in the set on the top of the matrix   
    proof of theorem  
we now set out to prove theorem    it is straightforward to verify that the hlc models
m  and m  are regular  so it suffices to prove equation      this is what we do in the rest
of this section 
the set of observed variables in m is o  y  the set of observed variables in m  is
o   z  and the set of observed variables in m  is y   x   hence the jacobian matrices
of models m   m    and m  can be respectively written as follows 
jm

   

jm 

   

jm 

   

p  o  y 

     

p  o  y  p  o  y 
p  o  y  p  o  y 
p  o  y 
 
 
 
     
     
   
   
   
   
   
k 
 
k 
 
k 

     

p  o  z  p  o  z 
p  o  z 
 
 
     
   
   
   
k 
 
k 

   
 

p  o  z 
   
 

p  x  y 
   
 

     

p  x  y  p  x  y 
p  x  y 
 
 
     
   
   
   
k 
 
k 

  the ci s might be undefined for some
   there is a subtle point here  being fractions of polynomials of  
  so from equation     alone  we cannot conclude that vj linearly depends on  vi  i              k 
values of  
everywhere 
the conclusion is nonetheless true for two reasons  first the set of   values where the ci s are
undefined has measure zero  second  if vj does not linearly depend on  vi  i              k  at one value of
  then the same would be true in a sufficiently small and nonetheless measure positive ball around that
 
value 

  

fieffective dimensions of hlc models

it is clear that there is a one to one correspondence between the first k   k  column vectors
of jm with the column vectors of jm  and there is a one to one correspondence between
the first k  and the last k  column vectors of jm with the column vectors of jm    we will
first show
claim    the first k  vectors of jm  jm  or jm    are linearly independent
almost everywhere 
together with lemma    claim   implies that there is a collection of column vectors in
jm  that includes the first k  vectors and that is a basis of the column space of jm  almost
everywhere  in particular  this implies that de m   k    suppose de m    k   r  without
loss of generality  suppose the basis vectors are
p  o  z 
   
 

     

p  o  z  p  o  z 
p  o  z 
 
 
     
   
   
   
k 
 
r

   

by symmetry  we can assume that de m    k   s where s  and that the following column
vectors form a basis for jm  almost everywhere 
p  x  y 
   
 

     

p  x  y  p  x  y 
p  x  y 
 
 
     
   
   
   
k 
 
s

   

now consider the following list of vectors in jm  
p  o  y 
   
 

     

p  o  y  p  o  y 
p  o  y  p  o  y 
p  o  y 
 
 
 
     
     
   
   
   
   
   
k 
 
r
 
s

   

we will show
claim    all column vectors of jm linearly depend on the vectors listed in    
everywhere 
claim    the vectors listed in     are linearly independent almost everywhere 
those two claims imply that the vectors listed in     form a basis of the column space of
jm almost everywhere  therefore
de m     k   r s   de m    de m   k   
it is clear that k   ds m    ds m   ds m    therefore theorem   is proved   
    proof of claim  
lemma   let z be a latent node in an hlc model m and y be the set of the observed
nodes in the subtree rooted at z  if m is regular  then we can set conditional distributions
of nodes in the subtree in such a way that they encode an injective mapping  from z to
y in the sense that p  y  z  z z      for all z  z  
  

fizhang   kocka

proof  we prove this lemma by induction on the number of latent nodes in the subtree
rooted at z  first consider the case when there is only one latent node  namely z  in this
case  z is the parent of all nodes in y  enumerate all these nodes as y    y            yk   because
q
m is regular  we have  z   ki    yi    hence we can define an injective mapping  from
q
z to y   ki   yi   for each state z of z   z  can be written as y    y    y            yk   
where yi is a state of yi   now if we set
p  yi  yi  z z      
then p  y  z  z z      
now consider the case when there are at least two hidden nodes in the subtree rooted
at z  let w be one such latent node that has no latent node descendants  let y     be
the set of observed nodes in the subtree rooted at w and y     y y      by the induction
hypothesis  we can parameterize the subtree rooted at w in such a way that it encodes an
injective mapping from w to y      moreover  if all nodes below w are removed from m  
m remains a regular hlc model  in that model  we can parameterize the subtree rooted at
z in such a way that it encodes an injective mapping from z to  w y        w  y     
together  those two facts prove the lemma   
corollary   let z be a latent node in an hlc model m   suppose z have a latent neighbor
x  let y be the set of the observed nodes separated from x by z  if m is regular  then
we can set probability distributions of nodes separated from x by z in such a way that they
encode an injective mapping  from z to y in the sense that p  y  z  z z      for
all z  z  
proof  the corollary follows readily from lemma   and the property of the root walking
operation  zhang          
proof of claim    consider the following matrix
 
   

   

p  x  z 
   
 

    

p  x  z 
   

k 

 

   

   

because                 k  are the parameters for the joint distribution p  x  z   this matrix
is the identity matrix if the rows are properly arranged  so its column vectors are linearly
independent almost everywhere 
   
   
now consider the first k  column vectors of jm   p  o  y              p  o  y  k   
they must be linearly independent almost everywhere  if not  one of the vectors  say
   
p  o  y  k    would linearly depend on the rest everywhere according to lemma   
observe that for any i   ik    
p  o  y 
   
i

 

x

p  o x p  y z 

x z

p  x  z 
   

i

 
   

choose p  o x  and p  y z  as in corollary    the vector p  o  y  i might contain zero elements  if we remove the zero elements  what remains of the vector is iden   
   
tical to p  x  z  i   so we can conclude that p  x  z  k  linearly depends on
  

fieffective dimensions of hlc models

   

   

p  x  z            p  x  z  k    everywhere  which contradicts the conclusion of the
previous paragraph  hence the first k  vectors of jm must be linearly independent almost
everywhere 
it is evident that  using similar arguments  we can also show that the first k  vectors of
jm   jm    are linearly independent almost everywhere  claim   is therefore proved   
    proof of claim  
every column vector of jm  linearly depends on vectors listed in     everywhere  observe
that
p  o  y 
   
i

x

p  y z 

z

p  o  y 
   
i

 
 

x

p  y z 

z

p  o  z 
   

  i              k 

   

  i              k   

i
p  o  z 
i

therefore every column vector of jm that corresponds to vectors in jm  linearly depends
on the first k   r vectors listed in     everywhere 
by symmetry  every column vector of jm that corresponds to vectors in jm  linearly
depends on the first k  and the last s vectors listed in     everywhere  the claim is proved 
 
    proof of claim  
we prove this claim by contradiction  assume the vectors listed in     were not linearly
independent almost everywhere  according to lemma    one of them  say v  must linearly
depend on the rest everywhere  because of claim   and lemma    we can assume that v is
   
among the last r s vectors  without loss of generality  we assume that v is p  o  y  s  
  there exist real numbers ci   ik     c      ir   and c   
then for any value of  
i
i
  is   such that
p  o  y 
   
s

 

k 
x
i  

ci

p  o  y 
   
i

 

r
x

    p  o  y 
   
i
i  

ci

 

s 
x

    p  o  y 
 
   
i
i  

ci

note that in the last term on the right hand side  i runs from   to s  
the parameter vector   consists of three subvectors             and        set the parameters
   
   for the x branch  as in lemma    then there exists an injective mapping  from x
to o such that
p  o  x  x x      for all x  x  

   

for each of the vectors in      consider the subvector consisting only of elements for
those states of o that are the images of states of x under the mapping   such subvectors
   
   
   
will be denoted by p  ox   y  i   p  ox   y  i   and p  ox   y  i   for any
values of      and        we still have
  

fizhang   kocka

p  ox   y 

 

   
s

k 
x

ci

p  ox   y 

i  

   
i

 

r
x

    p  ox   y 
   
i
i  

ci

 

s 
x

    p  ox   y 
 
   
i
i  

ci

   

consider the first two terms on the right hand side 
k 
x

ci

r
x

    p  ox   y 
   
i
i  
k 
r
x
x
p  ox   z 
p  ox   z  x
    x
c
ci
 
p  y z 
p  y z 
i
   
   
i
i
i  
i  
z
z
k 
r
x
x
p  ox   z  x
    p  ox   z 
ci
 
 
p  y z   ci
   
   
i
i
i  
i  
z

p  ox   y 
   
i

i  

 
 

 

ci

p

because of     and the fact that p  o  z   
x p  x  z p  o x   the column vector
   
   
p  ox   z  i is identical to the vector p  x  z  i   as we have argued when proving
   
claim    the vectors  p  x  z  i  i            k    constitute a basis for the k   dimensional
   
euclidian space  this implies that  each of the vectors p  ox   z  i can be represented
   
as a linear combination of the vectors  p  ox   z  i  i              k     consequently  there
exist ci   ik    such that
k 
x

ci

p  ox   z 

i  

   
i

k 
x

p  ox   y 

 

r
x

    p  ox   z 
   
i
i  

ci

 

k 
x

ci

p  ox   z 

ci

p  ox   y 

i  

   

i

hence

i  

ci

 

   
i

r
x

    p  ox   y 
   
i
i  

ci

 

k 
x
i  

   

i

combining this equation with equation      we get
p  ox   y 
   
s

 

k 
x

ci

p  ox   y 

i  

   
i

 

s 
x

    p  ox   y 
 
   

i  
i

ci

p

because of     and the fact that the fact that p  o  y    x p  x  y p  o x   the column
   
   
vector p  ox   y  i is identical to the vector p  x  y  i and the column vector
   
   
p  ox   y  i is identical to the vector p  x  y  i   hence
p  x  y 
   
s

 

k 
x
i  

ci

p  x  y 
   
i

 

s 
x

    p  x  y 
 
   
i
i  

ci

this contradicts the fact that the vectors in the equation form a basis for the column space
of jm  almost everywhere  see     in section      therefore  claim   must be true   
  

fieffective dimensions of hlc models

   effective dimensions of trees
let us use the term tree model to refer to markov random fields on undirected trees over
a finite number of random variables  if we root a tree model at any of its nodes  we get a
tree structured bayesian network model  in a tree model  define leaf nodes be those that
have only one neighbor  an hlc model is a tree model where all leaf nodes are observed
while all others are latent 
it turns out that theorem   enables us to compute the effective dimension of any tree
model  consider an arbitrary tree model  if some of its leaf nodes are latent  we can remove
such nodes without affecting its effective dimension 
after removing latent leaf nodes  all the leaf nodes are observed  if some non leaf nodes
are also observed  we can decompose the model into submodels at any observed non leaf
node  the following theorem tells us how the model and the submodels are related in terms
of effective dimensions 
theorem   suppose y is an observed non leaf node in a tree model m   if m decomposes
at y into k submodels m            mk   then
de m    

k
x

de mi     k      y       

i  

after all possible decompositions  the final submodels either do not contain latent nodes
or are hlc models  effective dimensions of submodels with no latent variables are simply
their standard dimensions  if an hlc submodel is irregular  we make it regular by applying
the transformation mentioned at the end of section      the transformation does not affect
the effective dimensions of the submodels  finally  effective dimensions of regular hlc
submodels can be computed using theorem   
proof of theorem    it is possible to prove this theorem starting from the jacobian
matrix  here we take a less formal but more revealing approach 
it suffices to consider case of k being    the two submodels m  and m  share only one
node  namely y   let o  and o  be respectively the sets of observed nodes in those two
submodels excluding y   root m at y   then we have
p  y  o    o   p  y     p  o    y  p  o    y   
let    be the set of parameters in the distribution p  y       and    be respectively the sets
of parameters in the conditional probability distributions of nodes in m  and m    consider
fixing    and letting    and    vary  in this case  the space spanned by p  y   consists of only
one vector  namely    itself  moreover  there is a one to one correspondence between vectors
in the space spanned by p  y  o    o    and vectors in the cartesian product of the spaces
spanned by p  o    y   and p  o    y    now let    vary  this adds  y    dimensions to each
of the four spaces spanned by p  y  o    o     p  y    p  o    y    and p  o    y    consequently 
we have
de m     de m      de m       y       
the theorem is proved   
  

fizhang   kocka

   concluding remarks
in this paper we study the effective dimensions of hlc models  the work is motivated by
empirical evidence that the bic behaves quite well when used with several hill climbing
algorithms for learning hlc models and that the bice score sometimes leads to better
model selection than the bic score  we have proved a theorem that relates the effective
dimension of an hlc model to the effective dimensions of two other hlc models that
contain fewer latent variables  repeated application of the theorem allows one to reduce
the task of computing the effective dimension of an hlc model to subtasks of computing
effective dimensions of lc models  this makes it computationally feasible to compute the
effective dimensions of large hlc models  in addition  we have proved a theorem about
effective dimensions of general tree models  this and our main theorem allows one to
compute the effective dimension of arbitrary tree models 
acknowledgements
this work was initiated when the authors were visiting department of computer science 
aalborg university  denmark  we thank poul s  eriksen  finn v  jensen  jiri vomlel 
marta vomlelova  thomas d  nielsen  olav bangso  jose pena  kristian g  olesen  we
are also grateful to the annonymous reviewers whose comments have helped us greatly in
improving this paper  research on this paper was partially supported by ga cr grant
            and by hong kong research grant council under grant hkust       e 

references
akaike  h          a new look at the statistical model identification  ieee trans  autom 
contr               
bartholomew  d  j  and knott  m          latent variable models and factor analysis   nd
edition  kendalls library of statistics    london  arnold 
cheeseman  p  and stutz  j          bayesian classification  autoclass   theory and
results  in fayyad  u   piatesky shaoiro  g   smyth  p   and uthurusamy  r   eds   
advancesin knowledge discovery and data mining  aaai press  menlo park  ca 
chickering d  m  and heckerman d          efficient approximations for the marginal
likelihood of bayesian networks with hidden variables  machine learning             
cowell  r  g   dawid  a  p   lauritzen  s  l   and spiegelhalter  d  j          probabilistic
networks and expert systems  springer 
kocka  t  and zhang  n  l          dimension correction for hierarchical latent class
models  in proc  of the   th conference on uncertainty in artificial intelligence
 uai     
geiger d   heckerman d  and meek c          asymptotic model selection for directed
networks with hidden variables  in proc  of the   th conference on uncertainty in
artificial intelligence          
  

fieffective dimensions of hlc models

goodman  l  a          exploratory latent structure analysis using both identifiable and
unidentifiable models  biometrika              
lazarsfeld  p  f   and henry  n w         
mifflin 
rusakov  d  and geiger  d         
networks  uai    

latent structure analysis  boston  houghton

asymptotic model selection for naive bayesian

rusakov  d  and geiger  d          automated analytic asymptotic evaluation of marginal
likelihood for latent models  uai    
schwarz g          estimating the dimension of a model  in annals of statistics             
settimi  r  and smith  j q          on the geometry of bayesian graphical models with
hidden variables  in proceedings of the fourteenth conference on uncertainty in
artificial intelligence  morgan kaufmann publishers  s  francisco  ca          
settimi  r  and smith  j q          geometry  moments and bayesian networks with hidden
variables  in proceedings of the seventh international workshop on artificial intelligence and statistics  fort lauderdale  florida      january        morgan kaufmann
publishers  s  francisco  ca 
zhang n  l          hierarchical latent class models for cluster analysis  aaai             
zhang  n  l   kocka  t   karciauskas  g   and jensen  f  v          learning hierarchical
latent class models  technical report hkust cs       department of computer
science  hong kong university of science and technology 
zhang  n  l         
structural em for hierarchical latent class models  technical
report hkust cs       department of computer science  hong kong university
of science and technology 
zhang  n  l       b   hierarchical latent class models for cluster analysis  journal of
machine learning research  to appear 

  

fi