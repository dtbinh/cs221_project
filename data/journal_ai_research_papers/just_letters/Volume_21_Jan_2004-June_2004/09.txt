journal of artificial intelligence research                 

submitted        published      

representation dependence in probabilistic inference
joseph y  halpern

halpern cs cornell edu

cornell university  computer science department
ithaca  ny      
http   www cs cornell edu home halpern

daphne koller

koller cs stanford edu

stanford university  computer science department
stanford  ca      
http   www cs stanford edu  koller

abstract
non deductive reasoning systems are often representation dependent  representing the
same situation in two different ways may cause such a system to return two different answers  some have viewed this as a significant problem  for example  the principle of
maximum entropy has been subjected to much criticism due to its representation dependence  there has  however  been almost no work investigating representation dependence 
in this paper  we formalize this notion and show that it is not a problem specific to maximum entropy  in fact  we show that any representation independent probabilistic inference
procedure that ignores irrelevant information is essentially entailment  in a precise sense 
moreover  we show that representation independence is incompatible with even a weak default assumption of independence  we then show that invariance under a restricted class of
representation changes can form a reasonable compromise between representation independence and other desiderata  and provide a construction of a family of inference procedures
that provides such restricted representation independence  using relative entropy 

   introduction
it is well known that the way in which a problem is represented can have a significant impact
on the ease with which people solve it  and on the complexity of an algorithm for solving
it  we are interested in what is arguably an even more fundamental issue  the extent to
which the answers that we get depend on how our input is represented  here too  there is
well known work  particularly by tversky and kahneman  see  for example   kahneman 
slovic    tversky          showing that the answers given by people can vary significantly
 and in systematic ways  depending on how a question is framed  this phenomenon is often
viewed as indicating a problem with human information processing  the implicit assumption
is that although people do make mistakes of this sort  they shouldnt  on the other hand 
there is a competing intuition that suggests that representation does  and should   matter 
representation dependence is just a natural consequence of this fact 
here we consider one type of reasoning  probabilistic inference  and examine the extent
to which answers depend on the representation  the issue of representation dependence is of
particular interest in this context because of the interest in using probability for knowledge
representation  e g    pearl         and because probabilistic inference has been the source
c
    
ai access foundation and morgan kaufmann publishers  all rights reserved 

fihalpern   koller

of many of the concerns expressed regarding representation  however  our approach should
be applicable far more generally 
we begin by noting that the notion of probabilistic inference has two quite different
interpretations  in one interpretation  which forms the basis for the bayesian paradigm 
probabilitic inference consists basically of conditioning  we start out with a prior distribution over some event space  and then condition on whatever observations are obtained  in
the other interpretation  we are given only a set of probabilistic assertions  and our goal
is to reach conclusions about the probabilities of various events  for most of this paper 
we focus on the latter interpretation  although we discuss the relationship to the bayesian
approach in section     
suppose that we have a procedure for making inferences from a probabilistic knowledge
base  how sensitive is it to the way knowledge is represented  consider the following
examples  which use perhaps the best known non deductive notion of probabilistic inference 
maximum entropy  jaynes         
example      suppose that we have no information whatsoever regarding whether an
object is colorful  what probability should we assign to the proposition colorful   symmetry
arguments might suggest      since we have no information  it seems that an object should
be just as likely to be colorful as non colorful  this is also the conclusion reached by
maximum entropy provided that the language has only the proposition colorful   but now
suppose we know about the colors red  blue  and green  and have propositions corresponding
to each of these colors  moreover  by colorful we actually mean red  blue  green  in this
case  maximum entropy dictates that the probability of red  blue  green is      note that 
in both cases  the only conclusion that follows from our constraints is the trivial one  that
the probability of the query is somewhere between   and   
example      suppose that we are told that half of the birds fly  there are two reasonable ways to represent this information  one is to have propositions bird and fly  and
use a knowledge base kb fly
   def  pr fly   bird           a second might be to have as
basic predicates bird and flying bird   and use a knowledge base kb fly
   def   flying bird 
bird    pr flying bird   bird           although the first representation may appear more natural  it seems that both representations are intuitively adequate insofar as representing the
information that we have been given  but if we use an inference method such as maximum
entropy  the first representation leads us to infer pr bird          while the second leads us
to infer pr bird         
examples such as these are the basis for the frequent criticisms of maximum entropy
on the grounds of representation dependence  but other than pointing out these examples 
there has been little work on this problem  in fact  other than the work of salmon       
      and paris         there seems to have been no work on formalizing the notion of representation dependence  one might say that the consensus was  whatever representation
independence is  it is not a property enjoyed by maximum entropy  but are there any
   although much of our discussion is motivated by the representation dependence problem encountered
by maximum entropy  an understanding of maximum entropy and how it works is not essential for
understanding our discussion 

   

firepresentation dependence

other inference procedures that have it  in this paper we attempt to understand the notion
of representation dependence  and to study the extent to which it is achievable 
to study representation dependence  we must first understand what we mean by a
representation  the real world is complex  in any reasoning process  we must focus on
certain details and ignore others  at a semantic level  the relevant distinctions are captured
by using a space x of possible alternatives or states  possible worlds   in example      our
first representation focused on the single attribute colorful   in this case  we have only two
states in the state space  corresponding to colorful being true and false  respectively  the
second representation  using red   blue  and green  has a richer state space  clearly  there
are other distinctions that we could make 
we can also interpret a representation as a syntactic entity  in this case  we typically capture relevant distinctions using some formal language  for example  if we use propositional
logic as our basic knowledge representation language  our choice of primitive propositions
characterizes the distinctions that we have chosen to make  we can then take the states
to be truth assignments to these propositions  similarly  if we use a probabilistic representation language such as belief networks  pearl        as our knowledge representation
language  we must choose some set of relevant random variables  the states are then then
possible assignments of values to these variables 
what does it mean to shift from a representation  i e   state space  x to another representation y   roughly speaking  we want to capture at the level of the state space a shift
from  say  feet to meters  thus  in x distances might be described in terms of feet where
in y they might be described in terms of meters  we would expect there to be a constraint
relating feet to meters  this constraint would not give any extra information about x  it
would just relate worlds in x to worlds in y   thus  we first attempt to capture representation independence somewhat indirectly  by requiring that adding constraints relating x to
y that place no constraints on x itself should not result in different conclusions about x 
the resulting notion  called robustness  turns out to be surprisingly strong  we can show
that every robust inference procedure must behave essentially like logical entailment 
we then try to define representation independence more directly  by using a mapping f
from one representation to another  for example  f could map a world where an individual
is   feet tall to the corresponding world where the individual is      meters tall  some
obvious constraints on f are necessary to ensure that it corresponds to our intuition of a
representation shift  we can then define a representation independent inference procedure
as one that preserves inferences under every legitimate mapping f   i e   for any kb and  
kb    iff f  kb     f    
this definition turns out to be somewhat more reasonable than our first attempt  in that
there exist nontrivial representation independent inference procedures  however  it is still
a strong notion  in particular  any representation independent inference procedure must
act essentially like logical entailment for a knowledge base with only objective information
 i e   essentially non probabilistic information   moreover  we show that representation
independence is incompatible with even the simplest default assumption of independence 
even if we are told nothing about the propositions p and q  representation independence
does not allow us to jump to the conclusion that p and q are independent 
these results suggest that if we want inference procedures that are capable of jumping
to nontrivial conclusions  then we must accept at least some degree of representation de   

fihalpern   koller

pendence  they add support to the claim that the choice of language does carry a great
deal of information  and that complete representation independence is too much to expect 
on a more positive note  we show that we can use the intuition that the choice of language
carries information to get limited forms of representation independence  the idea is that the
language should put further constraints on what counts as an appropriate representation
shift  for example  suppose that certain propositions represent colors while others represent
birds  while we may be willing to transform colorful to red  blue  green  we may not be
willing to transform red to sparrow   there is no reason to demand that an inference procedure behave the same way if we suddenly shift to a wildly inappropriate representation 
where the symbols mean something completely different  we provide a general approach to
constructing inference procedures that are invariant under a specific class of representation
shifts  this construction allows us to combine some degree of representation independence
with certain non deductive properties that we want of our inference procedure  in particular  we present an inference method that supports a default assumption of independence 
and yet is invariant under a natural class of representation shifts 
the rest of this paper is organized as follows  in section    we define probabilistic inference procedures and characterize them  in section    we define robust inference procedures
and show that every robust inference procedure is essentially entailment  in section    we
define representation independence  and show that representation independence is a very
strong requirement  in particular  we show that a representation independent inference
procedure essentially acts like logical entailment on objective knowledge bases and that
representation independence is incompatible with a default assumption of independence 
section   contains some general discussion of the notion of representation independence
and how reasonable it is to assume that the choice of language should affect inference 
while it may indeed seem reasonable to assume that the choice of language should affect
inference  we point out that this assumption has some consequences that some might view
as unfortunate  in section    we discuss how limited forms of representation independence
can be achieved  we discuss related work in section    and conclude in section   

   probabilistic inference
we begin by defining probabilistic inference procedures  as we discussed in the introduction 
there are two quite different ways in which this term is used  in one  we are given a prior
distribution over some probability space  our knowledge then typically consists of events
in that space  which can be used to condition that distribution and obtain a posterior  in
the other  which is the focus of our work  a probabilistic inference procedure takes as input
a probabilistic knowledge base and returns a probabilistic conclusion 
we take both the knowledge base and the conclusion to be assertions about the probabilities of events in some measurable space  x  fx    where a measurable space consists of a
set x and an algebra fx of subsets of x  that is  fx is a set of subsets of x closed under
union and complementation  containing x itself    formally  these assertions can be viewed
as statements about  or constraints on  probability measures on  x  fx    for example  if
   if x is infinite  we may want to consider countably additive probability measures and take fx to be
closed under countable unions  this issue does not play significant role in this paper  for simplicity  we
restrict to finite additivity and require only that fx be closed under finite unions 

   

firepresentation dependence

s  fx   a statement pr s       holds only for distributions where s has probability
at least      therefore  if  x fx   is the set of all probability measures on  x  fx    that
is  all probability measures with domain fx    we can view a knowledge base as a set of
constraints on  x fx     when fx is clear from context  we often omit it from the notation 
writing x rather than  x fx    
we place very few restrictions on the language used to express the constraints  we
assume that it includes assertions of the form pr s    for all subsets s  fx and rational
          and that it is closed under conjunction and negation  so that if kb and kb   are
knowledge bases expressing constraints  then so are kb  kb   and kb    however  the
langauge could include many assertions besides those obtained by starting with assertions
of the form pr s    and closing off under conjunction and negation   since the language
puts constraints on probability measures  we cannot directly say that s  fx must hold 
the closest approximation in the language is the assertion pr s       thus  we call such
constraints objective  a knowledge base consisting of only objective constraints is called an
objective knowledge base  since pr t         pr t        is equivalent to pr t   t        
without loss of generality  an objective knowledge base consists of a single constraint of the
form pr t        given a knowledge base kb placing constraints on x   we write     kb
if  is a measure in x that satisfies the constraints in kb   we use   kb   x to denote all
the measures satisfying these constraints 
in practice  our knowledge is typically represented syntactically  using some logical language to describe the possible states  typical languages include propositional logic  firstorder logic  or a language describing the values for some set of random variables  in general 
a base logic l defines a set of formulas l   for a given vocabulary   in propositional
logic  the vocabulary  is simply a set of propositional symbols  in probability theory  the
vocabulary can consist of a set of random variables  in first order logic  the vocabulary is a
set of constant symbols  function symbols  and predicate symbols  to facilitate comparison
between vocabularies  we assume that for each base logic all the vocabularies are finite
subsets of one fixed infinite vocabulary   
when working with a language  we assume that each state in the state space defines
an interpretation for the symbols in  and hence for the formulas in l    in the case of
propositional logic  we thus assume that we can associate with each state a truth assignment
to the primitive propositions in   for first order logic  we assume that we can associate
with each state a domain and an interpretation of the symbols in   in the probabilistic
setting  we assume that we can associate with each state an assignment of values to the
random variables  it is often convenient to assume that the state space is in fact some
subset w of w    the set of all interpretations for  or assignments to  the vocabulary  
note that the truth of any formula  in l   is determined by a state  if  is true in some
state w  we write w     
the probabilistic extension lpr    of a base logic l   is simply the set of probability
formulas over l    formally  for each   l    pr   is a numeric term  the formulas in
lpr    are defined to be all the boolean combinations of arithmetic expressions involving
numeric terms  for example  pr fly   bird        is a formula in lpr   fly  bird     where
we interpret a conditional probability expression pr      as pr      pr   and then
multiply to clear the denominator   by analogy with constraints  a formula of the form
pr       is called an objective formula 
   

fihalpern   koller

given a set w  w    assume that fw is the algebra consisting of all sets of the
form     w    w   w       for   l     in the case of propositional logic  where 
consists of a finite set of primitive propositions  fw    w   in the case of first order logic 
not all sets are necessarily definable by formulas  so fw may be a strict subset of  w   
let  be a probability measure on  w  fw    we can then ascribe semantics to lpr    in
the probability space  w  fw     in a straightforward way  in particular  we interpret the
numeric term pr   as   w  w   w        since a formula   l   describes an event
in the space w   a formula  in lpr    is clearly a constraint on measures on w   we write
     if the measure   w satisfies the formula  
a syntactic knowledge base kb  lpr    can be viewed as a constraint on w in an
obvious way  formally  kb represents the set of probability measures   kb     w   which
consists of all measures  on w such that     kb  
we say that kb  whether syntactic or semantic  is consistent if   kb   x      i e   if
the constraints are satisfiable  finally  we say that kb entails   where  is another set of
constraints on x    written kb   x   if   kb   x      x   i e   if every measure that satisfies
kb also satisfies   we write   x  if  is satisfied by every measure in x   we omit the
subscript x from    if it is clear from context 
entailment is well known to be to be a very weak method of drawing conclusions from
a knowledge base  in particular with respect to its treatment of irrelevant information 
consider the knowledge base consisting only of the constraint pr fly   bird         even
though we know nothing to suggest that red is at all relevant  entailment will not allow us
to reach any nontrivial conclusion about pr fly   bird  red   
one way to get more powerful conclusions is to consider  not all the measures that satisfy
kb   but a subset of them  intuitively  given a knowledge base kb   an inference procedure
picks a subset of the measures satisfying kb   and infers  if  holds in this subset  clearly 
more conclusions hold for every measure in the subset than hold for every measure in the
entire set 
definition       an  x  fx   inference procedure is a partial function i     x fx    
  x fx   such that i a   a for a   x fx   and i a     iff a    for all a    x fx   in
the domain of i  i e   for all a for which i is defined   we write kb  i  if i   kb   x        x  
when fx is clear from context or irrelevant  we often speak of x inference procedures 
we remark that paris        considers what he calls inference processes  these are just
inference procedures as we have defined them that  given a a set a of probability measures 
return a unique probability measure in a  rather than an arbitrary subset of a   paris
gives a number of examples of inference processes  he also considers various properties that
an inference process might have  some of these are closely related to various properties of
representation indepedence that we consider  we discuss pariss work in section   
entailment is the x inference procedure defined on all sets determined by taking i to
be the identity  maximum entropy is also an inference procedure in this sense 
definition      given a probability measure  on a finite space x  where all sets are
p
measurable   its entropy h   is defined as  xx  x  log  x    the log is taken to the
me  a  consist of the measures in a
base   here   given a set a of measures in x   let ix
   

firepresentation dependence

that have the highest entropy if there are measures in a whose entropy is at least as high
as that of any measure in a  if there are no such measures  inf me  a  is undefined 
it is easy to see that inf me  a  is defined if a is closed  in the topological sense  i e   if
n is a sequence of probability measures in a and n converges to   then   a   thus 
we could take the domain of inf me
x to consist only of the closed sets of measures in x  
there are also open sets a for which inf me  a  is defined  although it is not defined for all
open sets a  for example  suppose x    x    x    and let a        x            let  
be such that    x           it is easy to check that h          and h       for   a 
however  for all   there is some   a such that h          it follows that there is no
measure in a whose entropy is higher than that of any other measure in a  so inf me  a  is
undefined  on the other hand  if a         x            then there is a measure whose
entropy is maximum in the open set a    namely the measure    
there are  of course  many inference procedures besides entailment and maximum entropy that can be defined on a measurable space  in fact  as the following proposition shows 
any binary relation   satisfying certain reasonable properties is an inference procedure of
this type 
proposition      if i is an x inference procedure then the following properties hold for
every kb   kb        over x such that kb is in the domain of i 
 reflexivity  kb  i kb  
 left logical equivalence  if kb is logically equivalent to kb     i e   if    kb  kb    
then for every  kb  i  iff kb    i  
 right weakening  if kb  i  and       then kb  i  
 and  if kb  i  and kb  i   then kb  i    
 consistency  if kb is consistent then kb   i false 
proof  straightforward from the definitions 
interestingly  these properties are commonly viewed as part of a core of reasonable
properties for a nonmonotonic inference relation  kraus  lehmann    magidor        
we would like to also prove a converse  showing that any relation   over probabilistic
constraints on some space x that satisfies the five properties above must have the form
 ix   this is not quite true  as the following example shows 
example      fix a measurable space  x  fx    let the language consist of all  finite 
boolean combination of statements of the form pr s     where s  fx   now fix one
nonempty strict subset s  of x  and let n be the statement pr s       n  define an
inference procedure   as follows  if kb is not equivalent to true  i e  if   kb   x    x   
then kb    iff kb      on the other hand  true    iff n     for all sufficiently large
n  that is  true    if there exists an n such that for all n  n   we have n      it is
easy to check that all five properties in proposition     hold for    however    is not  i
for an x inference procedure i  for suppose it were  note that n    m for all n  m 
   

fihalpern   koller

so true   m for all m  thus  we must have i x      n   x for all n  it follows that
ix  x      pr s         x   and so true  i pr s         however  n     pr s      for any n 
so we do not have true   pr s       this contradicts the assumption that      i  
essentially what we need to get the converse to proposition     is an infinitary version of
v
the and rule  which would say that if kb  i i for all i  then kb  i i i   if the language
were closed under infinite conjunctions  then this rule would in fact be just what we need 
since we have not assumed that the language is closed under infinite conjunctions  we use
a variant of this rule 
 infinitary and  for any set  of statements  if kb  i  for all    and      
then kb  i  
proposition      let   be a relation over probabilistic constraints on x for which the
properties reflexivity  left logical equivalence  right weakening  infinitary and  and consistency hold for all kb in the domain of      that is  if kb is in the domain of   in
that kb    for some   then kb   kb   and so on   then   is  i for some x inference
procedure i 
proof  see appendix a   
we are typically interested not just in an inference procedure defined on one space x 
but in a family of related inference procedures  defined on a number of spaces  for example 
entailment is an inference procedure that is defined on all spaces x  maximum entropy is
defined on all finite measurable spaces  x   x   
definition       if x is a set of measurable spaces  an x  inference procedure is a set
 i x fx      x  fx    x    where i x fx   is an  x  fx   inference procedure for  x  fx    x  
we sometimes talk about an x  inference procedure i  and write kb  i  when  x  fx   
x is clear from context  however  it should be stressed that  formally  an x  inference procedure is a really a set of inference procedures  typically related in some natural way  
clearly entailment is an x  inference procedure for any x   where ix is simply the identity
function for x  x   if x consists of finite measurable spaces where all sets are measurable 
then maximum entropy is an x  inference procedure  we typically denote this inference
procedure   me   thus  kb   me  if  holds for all the probability measures of maximum
entropy satisfying kb  
important assumptions  for the remainder of this paper  we deal only with x inference procedures i for which x satisfies two richness assumptions  these assumptions
hold for all the standard inference procedures that have been considered 
 we assume that x is closed under crossproducts  so that if  x  fx     y  fy    x  
then  x  y  fxy    x   where fxy is the algebra formed by taking finite unions
of disjoint sets of the form s  t   for s  fx and t  fy   it is easy to see that
this is an algebra  since s  t   s  t  s  t  s  t and  s  t     s    t      
 s  s       t  t      from which it also follows that any union of such sets can be
   

firepresentation dependence

written as a disjoint union   note that if x and y are finite sets  fx    x   and
fy    y   then fx  fy    xy   as we shall see  having  x  y  fxy    x if each
of  x  fx   and  y  fy   is in x allows us to relate constraints on x to constraints on
y in a natural way 
 we assume that x contain sets of all finite cardinalities  more precisely  for all n    
there exists a set  x  fx    x such that  x    n and fx    x   this assumption
is not actually needed for any of our results  since the assumption that x is closed
under crossproducts already implies that  for any finite n  there exists a measurable
space  x  fx    x such that  x   n  this already suffices to prove all the results
of the paper  however  assuming that x has sets of all cardinalities does make the
proofs easier 
we also want the domain of i to satisfy certain assumptions  but we defer stating these
assumptions until we have introduced some additional definitions and notation 

   robustness
in order to define robustness to representation shifts  we must first define the notion of
a representation shift  our first attempt at this definition is based on the idea of using
constraints that specify the relationship between the two vocabularies  for example  in example      we might have x    colorful   colorless  and y    red   blue  green  colorless   we
can specify the relationship between x and y via a constraint that asserts that colorful 
 red  blue  green  
of course  not every constraint is a legitimate mapping between representations  for
example  a formula that asserted colorful is obviously not a legitimate representation
shift  at a minimum  we must assume that the constraint does not give any additional
information about x as far as logical inference goes  at a syntactic level  we can use the
following definition  given a knowledge base kb  lpr     we say that   lpr        is
 conservative over kb if  for all formulas   lpr     we have kb     iff kb       
thus  adding  to the knowledge base does not permit any additional logical inferences
in the vocabulary   an inference procedure i is robust if it is unaffected by conservative
extensions  that is  if kb     lpr     then kb  i  iff kb    i  for all  that are
 conservative over kb   roughly speaking  this says that getting new information that is
uninformative as far as logical inference goes does not affect default conclusions 
the formal definition of robustness  which uses semantic rather than syntactic concepts 
extends these intuitions to arbitrary constraints on measures  not just ones that can be
expressed in the language lpr   
definition      for   x     xn   define xi  xi by taking xi  a     x      
xi   a  xi        xn    a constraint  on xi can be viewed as a constraint on
x     xn by taking     x     xn      x     xn   xi       we frequently identify
constraints on xi with constraints on x          xn in this way  for b  x  xn   define
proj xi  b     xi     b   a constraint  on x  xn is said to be xi  conservative
over the constraint kb on xi if proj xi    kb    x  xn       kb   xi  
   

fihalpern   koller

to see that this definition generalizes the earlier language oriented definition  note that
if  and kb are constraints on x and  is a constraint on xy   then kb       iff
proj      kb    xy        x   while kb     iff   kb   x      x  
definition       ix   x  x   is a robust x  inference procedure if for all spaces x  y  x  
constraints kb and  on x   and constraints  on xy that are x conservative over
kb   we have kb  ix  iff kb    ixy    note that this definition implicitly assumes
that x  y  x if x  y  x   an assumption we made explicit earlier  
at first glance  robustness might seem like a reasonable desideratum  after all  why
should adding a constraint on xy that places no restrictions on x change the conclusions that we might reach about x  unfortunately  it turns out that this definition
is deceptively strong  and disallows any interesting inference procedures  in particular 
one property we may hope for in an inference procedure is to draw nontrivial conclusions
about probabilities of events  that is  conclusions that do not follow from entailment  for
example  maximum entropy  or any inference procedure based on symmetry  will conclude
pr p        from the empty knowledge base  we can show that inference procedures
that are robust do not really allow much in the way of nontrivial conclusions about the
probabilities of events 
definition      an  x  fx   inference procedure i is essentially entailment for the knowledge base kb  x if for all s  fx   if kb  i    pr s     then kb      pr s    
i is essentially entailment for x if it is essentially entailment for all knowledge bases kb
in the domain of ix  
thus  when entailment lets us conclude pr s         an inference procedure that is
essentially entailment lets us draw only the slightly stronger conclusion pr s         to
prove this  we need to make three assumptions about the domain of i   for other results 
we need other assumptions about the domain of i  
di     pr s    is in the domain of i x fx   for all s  fx       ir 
di   if kb is in the domain of ix   then it is also in the domain of ixy  when kb is
viewed as a constraint on xy   
di   if kb   and kb   are in the domain of ix   then so is kb    kb    
note that sets of the form   pr s    are closed sets  it certainly seems reasonable
to require that such sets be in the domain of an inference procedure  they correspond to
the most basic observations  di  seems quite innocuous  as observed earlier  we do want
to be able to view constraints on x as constraints on xy   and doing so should not
prevent them from being in the domain of i  di  also seems to be a reasonable assumption 
since if kb   and kb   correspond to possible observations  we want to be able to draw
conclusions from combining the observations  di  holds if the domain of i consists of
closed sets  but note that it does not hold for i me if we take its domain to consist of
all sets that have a measure whose entropy is maximum  for example  if x    x    x    
a              x            and b        x           where    x           then each
of a and b have a measure whose entropy is maximum  but a  b does not have a measure
whose entropy is maximum 
   

firepresentation dependence

theorem      if  ix   x  x   is a robust x  inference procedure that satisfies di   di  
and di   then ix is essentially entailment for all x  x  
proof  see appendix a   
it is possible to construct robust inference procedures that are almost but not quite
entailment  simply by strengthening some conclusions from pr s        to pr s  
      clearly  however  any robust inference procedure is extremely limited in its ability
to jump to conclusions  in the next section  we look at a definition that seems closer to
the intuitive notion of representation independence  and has somewhat more reasonable
consequences 

   representation independence
    representation shifts
if x and y are two different representations of the same phenomena then  intuitively 
there should be a way of relating states in x to corresponding states in y   we want this
correspondence to respect the logical structure of events  formally  we require that it be a
homomorphism with respect to complementation and intersection 
definition      an  x  fx    y  fy   embedding f is a function f   fx   fy such that
f  s  t     f  s   f  t   and f  s    f  s  for all s  t  fx  
as elsewhere  we talk about x y embeddings rather than  x  fx    y  fy   embeddings if
fx and fy do not play a significant role 
our goal is to consider the effect of a transformation on probabilistic formulas  hence 
we are interested in sets of states and their probabilities 
definition      if f is an x y embedding    x   and   y   then  and  correspond
under f if  s     f  s   for all events s  fx   we define a mapping f     x    y as
follows  we first define f  on singleton sets  except that  for convenience  we write f    
rather than f       by taking f          y    f  s      s  for all s  fx    thus 
f     consists of all measures in y that correspond to  under f   if d is an arbitrary
subset of  x   define f   d    d f     for d  x  
if  is a constraint on x expressed in some language  we typically write f     rather than
f       x    we implicitly assume that the language is such that the constraint f     is also
expressible  it is not hard to see that f     is the constraint that results by replacing every
set s  fx that appears in  by f  s  
example      in example      we might have x    colorful   colorless  and y    red   blue 
green  colorless   in this case  we might have f  colorful      red   blue  green  and f  colorless   
 colorless   consider the measure   x such that  colorful         and  colorless   
     then f     is the set of measures  such that the total probability assigned to the set of
states  red   blue  green  by  is      note that there are uncountably many such measures 
it is easy to check that if  is a constraint on x such as pr colorful          then f     is
pr  red   blue  green         
   

fihalpern   koller

embeddings can be viewed as the semantic analogue to the syntactic notion of interpretation defined in  enderton        pp           which has also been used in the recent
literature on abstraction  giunchiglia   walsh        nayak   levy         essentially 
an interpretation maps formulas in a vocabulary  to formulas in a different vocabulary
 by mapping the primitive propositions in   e g   colorful   to formulas over   e g  
red  blue  green  and then extending to complex formulas in the obvious way  the representation shift in example     can also be captured in terms of an interpretation  this one
taking flying bird to fly  bird  
definition      let  and  be two vocabularies  in the propositional case  a interpretation of  into  is a function i that associates with every primitive proposition p  
a formula i p   l    a more complex definition in the same spirit applies to first order
vocabularies  for example  if r is a k ary predicate  then i r  is a formula with k free
variables 
given an interpretation i  we get a syntactic translation from formulas in l   to formulas
in l   using i in the obvious way  for example  i  p  q   r     i p   i q    i r 
 see  enderton        for the details   clearly an interpretation i from  to  induces an
embedding f from w   w   to w   w    we map     w  to   i    w   
of course  not all embeddings count as legitimate representation shifts  for example 
consider an embedding f defined in terms of an interpretation that maps both the propositions p and q to the proposition r  then the process of changing representations using
f gives us the information that p and q are equivalent  information that we might not
have had originally  intuitively  f gives us new information by telling us that a certain
situationthat where p  q holdsis not possible 
more formally  the embedding f
has the following undesirable property  it maps the set of states satisfying p  q to the
empty set  this means a state where p  q holds does not have an analogue in the new
representation  we want to disallow such embeddings 
definition      an x y embedding f is faithful if  for all s  t  fx   we have s  t iff
f  s   f  t   
this definition has the desired consequence of disallowing embeddings that give new
information as far as logical consequence goes 
lemma      an x y embedding f is faithful if and only if for all constraints kb and  
we have kb     iff f   kb      f     
proof  see appendix a   
it is clear that our embedding from example     is faithful  f  colorful      red   blue  green 
and f  colorless    colorless  the following proposition gives further insight into faithful
embeddings 
proposition      let f be a faithful x y embedding  then the following statements are
equivalent 
 a   and  correspond under f  
   

firepresentation dependence

 b  for all formulas        iff     f     
proof  see appendix a   
if the embedding f is a reasonable representation shift  we would like an inference
procedure to return the same answers if we shift representations using f  
definition      if x  y  x   then the x  inference procedure  ix   x  x   is invariant
under the x y embedding f if for all constraints kb and  on x   we have kb  ix  iff
f   kb    iy f       note that  in particular  this means that kb is in the domain of   ix
iff f   kb   is in the domain of   iy   
definition      the x  inference procedure  ix   x  x   is representation independent if
it is invariant under all faithful x y embeddings for all x  y  x  
since the embedding for example     is faithful  any representation independent inference procedure would return the same answers for pr colorful   as for pr red  blue  green  
the issue is somewhat more subtle for example      there  we would like to have an embedding f generated by the interpretation i flying bird     fly  bird and i bird     bird  
this is not a faithful embedding  since flying bird  bird is not a valid formula  while
i flying bird  bird   is  fly  bird    bird which is valid  looking at this problem semantically  we see that the state corresponding to the model where flying bird  bird holds is
mapped to   but this is clearly the source of the problem  according to our linguistic intuitions for this domain  this is not a legitimate state  rather than considering all the states
in w  flying bird   bird     it is perhaps more appropriate to consider the subset x consisting of the truth assignments characterized by the formulas  flying bird  bird   flying bird 
bird   flying bird  bird    if we now use i to embed x into w  fly  bird     the resulting embedding is indeed faithful  so  as for the previous example  invariance under this
embedding would guarantee that we get the same answers under both representations 
    representation independent inference procedures
although the definition of representation independence seems natural  so did the definition
of robustness  how do the two definitions relate to each other  first  we show that representation independence is a weaker notion than robustness  for this result  we need to
consider inference procedures that satisfy two further assumptions 
di   if f is a faithful x y embedding  then kb is in the domain of ix iff f   kb   is in the
domain of iy  
di   if kb is in the domain of ixy   f is a faithful x y embedding  and   is a constraint
on x   then kb      f        is in the domain of ixy  
di  is very natural and is satisfied by all the standard inference procedures  it is easy to
check that if kb is closed iff f   kb   is closed  while di  may not appear so natural  it
does hold for domains consisting of closed sets  since it is not hard to check that   f      
is closed  di  would follow from di  and the assumption that   f       is in the domain
of ixy   but it is actually weaker than the combination of these two assumptions  in
particular  it holds for the domain consisting of all sets on which there is a measure of
maximum entropy 
   

fihalpern   koller

theorem       if an x  inference procedure is robust that satisfies di   di   and di  
then it is representation independent 
proof  see appendix a   
we have already shown that any robust inference procedure must be almost trivial  are
there any interesting representation independent inference procedures  as we shall see  the
answer is mixed  there are nontrivial representation independent inference procedures  but
they are not very interesting 
our first result shows that representation independence  like robustness  trivializes the
inference procedure  but only for some knowledge bases 
theorem       if  ix   x  x   is a representation independent x  inference procedure
then  for all x  x   ix is essentially entailment for all objective knowledge bases in its
domain  
proof  see appendix a   
corollary       if  ix   x  x   is a representation independent x  inference procedure 
kb is objective  and kb  i    pr s     for some     and      then      and
     
this result tells us that from an objective knowledge base pr t        we can reach only
three possible conclusions about a set s  if t  s  then we can conclude that pr s      
if t  s  then we can conclude that pr s       otherwise  the strongest conclusion we can
make about pr s  is that is somewhere between   and   
we can construct a representation independent inference procedure that is not entailment and has precisely this behavior if we restrict attention to countable state spaces  suppose that x is countable  given an objective knowledge base kb of the form pr t       
where t  fx   let kb   consist of all formulas of the form     pr s      for for
all nonempty strict subsets s of t in fx    we now define an x inference procedure
  as follows  if kb is equivalent to an objective knowledge base  then kb    if
ix
i 
kb  kb        if kb is not equivalent to an objective knowledge base  then kb  i    if
  is indeed an inference procedure 
kb      it follows easily from proposition     that ix
moreover  it is not equivalent to the standard notion of entailment  for example  we have
true  i       pr p       while        pr p       nevertheless  we can prove that i   is
representation independent 
    x  x   is a representationlemma       let x consist of only countable sets  then  ix
independent x  inference procedure 

   in an earlier version of this paper  halpern   koller         we claimed that any representationindependent inference procedure that satisfied a minimal irrelevance property  implied by robustness 
but not equivalent to it  is essentially entailment for all knowledge bases  as jaeger        shows  an
inference procedure along the lines of i   described below can be constructed to show that this result is
not correct  we seem to need the full strength of robustness 
   the requirement that x be countable is necessary here  if x is uncountable and every singleton is in fx  
then kb   is inconsistent if both t and t are uncountable  it is impossible that each of an uncountable
collection of points has positive measure 

   

firepresentation dependence

proof  see appendix a   
while objective knowledge bases may not appear so interesting if we restrict to propositional languages  for languages that include first order and statistical information they
become quite interesting  indeed  as shown in  bacchus        bacchus  grove  halpern   
koller         knowledge bases with first order and  objective  statistical information allow
us to express a great deal of the information that we naturally encounter  for example  we
can express the fact that     of birds fly as an objective statement about the number of
flying birds in our domain relative to the overall number of birds  of course  theorem     
applies immediately to such knowledge bases 
theorem      also implies that various inference procedures cannot be representation
independent  in particular  since true   me pr p        for a primitive proposition p  it
follows that maximum entropy is not essentially entailment  this observation provides
another proof that maximum entropy is not representation independent 
it is consistent with theorem      that there are representation independent inference
procedures that are not almost entailment for probabilistic knowledge bases  for example 
  defined as follows  given a     if there exists
consider the x inference procedure ix
x
   a           s        
some s  fx such that a      x    s         then ix
x
   a    a  thus  pr s         pr s        clearly  i   is not essentially
otherwise  ix
x
i 
entailment  yet  we can prove the following result 

lemma       suppose that x consists only of measure spaces of the form  x   x    where
    x  x   is a representation independent x  inference procedure 
x is finite  then  ix

proof  see appendix a   
note that it follows from theorem     that i   cannot be robust  thus  we have shown
that representation independence is a strictly weaker notion than robustness 
this example might lead us to believe that there are representation independent inference procedures that are interesting for probabilistic knowledge bases  however  as we
now show  a representation independent inference procedure cannot satisfy one key desideratum  the ability to conclude independence by default  for example  an important feature of
the maximum entropy approach to nonmononotic reasoning  goldszmidt  morris    pearl 
      has been its ability to ignore irrelevant information  by implicitly assuming independence  of course  maximum entropy does not satisfy representation independence 
our result shows that no approach to probabilistic reasoning can simultaneously assure
representation independence and a default assumption of independence 
we do not try to give a general notion of default assumption of independence here 
since we do not need it for our result  rather  we give a minimal property that we would
hope an inference procedure might have  and show that this property is sufficient to preclude
representation independence  syntactically  the property we want is that if  and  are
disjoint vocabularies  kb  lpr       l    and   l    then kb  i pr      
pr    pr   
   

fihalpern   koller

definition       an x  inference procedure  ix   x  x   enforces minimal default independence if  whenever x and y are in x   kb is a constraint on x in the domain of  ix  
s  fx   and t  fy   then kb  ixy pr s  t     pr s   pr t    
this definition clearly generalizes the syntactic definition 
clearly  entailment does not satisfy minimal default independence  maximum entropy 
however  does  indeed  a semantic property that implies minimal default independence is
used by shore and johnson        as one of the axioms in an axiomatic characterization of
maximum entropy 
theorem       if  ix   x  x   is an x  inference procedure that enforces minimal default
independence and satisfies di   then ix is not representation independent 
proof  see appendix a   
this result is very interesting as far as irrelevance is concerned  we might hope that
learning irrelevant information does not affect our conclusions  while we do not attempt
to define irrelevance here  certainly we would expect that if kb   is in a vocabulary disjoint
from kb and   then  for example  kb  i pr      iff kb  kb    i pr       if kb  
is objective  then the standard probabilistic approach would be to identify learning kb  
with conditioning on kb     suppose that we restrict to inference procedures that do indeed
condition on objective information  as is the case for the class of inference procedures we
consider in section     then kb  kb    i pr      exactly if kb  i pr    kb        
thus  theorem      tells us that inference procedures that condition on new  objective 
information cannot both be representation independent and ignore irrelevant information 
thus  although representation independence  unlike robustness  does not force us to use
entirely trivial inference procedures  it does prevent us from using procedures that have
certain highly desirable properties 

   discussion
these results suggest that any type of representation independence is hard to come by  they
also raise the concern that perhaps our definitions were not quite right  we can provide
what seems to be even more support for the latter point 
example      let q be a unary predicate and c            c      d be constant symbols  suppose
that we have two vocabularies     q  d  and     q  c            c      d   consider the
interpretation i from  to  for which i d    d and i q x     q x   q c            q c      
now  consider kb   xq x   in this case  i kb     x q x   q c            q c      
intuitively  since all the ci s may refer to the same domain element  the only conclusion we
can make with certainty from q c            q c      is that there exists at least one q in the
domain  which gives us no additional information beyond kb   we can convert this example
into a general argument that the embedding f corresponding to i is faithful  intuitively  for
   since we are working in the space x  y   kb should be viewed as a constraint on xy here  pr s 
should be understood as pr s  y    while pr t   should be understood as pr x  t    recall that  by
assumption  x  y  x  

   

firepresentation dependence

any kb   we can only get the conclusion q c            q c      from f   kb   if q x  appears
positively in kb   but  in this case  we already know that there is at least one q  so we
gain no new information from the embedding  but it does not seem unreasonable that an
inference procedure should assign different degrees of belief to q d  given kb   xq x  on
the one hand and given i kb     x q x   q c            q c       on the other   particularly
if the domain is small  in fact  many reasoning systems explicitly adopt a unique names
assumption  which would clearly force different conclusions in these two situations 
this example suggests that  at least in the first order case  even faithful embeddings
do not always match our intuition for a reasonable representation shift  one might
therefore think that perhaps the problem is with our definition even in the propositional
case  maybe there is a totally different definition of representation independence that avoids
these problems  while this is possible  we do not believe it to be the case  the techniques
that we used to prove theorem      and     seem to apply to any reasonable notion of
representation independence   to give the flavor of the type of argument used to prove these
theorems  consider example      and assume that true  i pr colorful      for           
using an embedding g such that g colorful     red   we conclude that true  i pr red      
similarly  we can conclude pr blue     and pr green      but in order for  i to be
invariant under our original embedding  we must have true  i pr red  blue  green     
which is completely inconsistent with our previous conclusions  but the embeddings we use
in this argument are very natural ones  we would not want a definition of representation
independence that disallowed them 
these results can be viewed as support for the position that representation dependence
is justified  the choice of an appropriate representation encodes significant information  in
particular  it encodes the bias of the knowledge base designer about the world  researchers
in machine learning have long realized that bias is an inevitable component of effective
inductive reasoning  i e   learning from evidence   so we should not be completely surprised
if it turns out that other types of leaping to conclusions  as in our context  also depend on
the bias 
but we need to be a little careful here  for example  in some cases we can identify
the vocabulary  and hence  the representation  with the sensors that an agent has at its
disposal  it may not seem that unreasonable that an agent with a temperature sensor and
a motion sensor might carve up the world differently from an agent with a color sensor
and a distance sensor  but consider two agents with different sensors who have not yet
made any observations  suppose that both of them can talk about the distance to a tree 
is it reasonable that the two agents should reach different conclusions about the distance
just because they have different sensors  and thus use different vocabularies   although they
have not made any observations  it would then follow that the agents should change their
conclusions if they switched sensors  despite not having made any observations  this does
not seem so reasonable 
bias and representation independence can be viewed as two extremes in a spectrum 
if we accept that the knowledge base encodes the users bias  there is no obligation to be
   actually  i q d     q d   q c            q c       but the latter is equivalent to q d  given kb  
   they certainly applied to all of the many definitions that we tried 
   in fact  it suffices to assume that true  i pr colorful          as long as      or      

   

fihalpern   koller

invariant under any representation shifts at all  on the other hand  if we assume that the
representation used carries no information  coherence requires that our inference procedure
give the same answers for all equivalent representations  we believe that the right answer lies somewhere in between  there are typically a number of reasonable ways in which
we can represent our information  and we might want our inference procedure to return
the same conclusions no matter which of these we choose  it thus makes sense to require
that our inference procedure be invariant under embeddings that take us from one reasonable representation to another  but it does not follow that it must be invariant under all
embeddings  or even all embeddings that are syntactically similar to the ones we wish to
allow  we may be willing to refine colorful to red  blue  green or to define flying bird
as fly  bird   but not to transform red to sparrow   in the next section  we show how to
construct inference procedures that are representation independent under a limited class of
representation shifts 

   selective invariance
as discussed above  we want to construct an inference procedure i that is invariant only
under certain embeddings  for the purposes of this section  we restrict attention to finite
spaces  where all sets are measurable  that is  we focus on x  inference procedures where
x consists only of measure spaces of the form  x   x    where x is finite 
our first step is to understand the conditions under which an x  inference procedure i
is invariant under a specific x y embedding f   when do we conclude  from kb  x  
recall that an inference procedure ix picks a subset dx   ix  kb    and concludes  iff 
holds for every measure in dx   similarly  when applied to f   kb    y   iy picks a subset
dy   iy  f   kb     for i to be invariant under f with respect to kb   there has to be a
tight connection between dx and dy  
to understand this connection  first consider a pair of measures  on x and  on y  
recall from proposition     that  and  correspond under f iff  for all formulas   we have
     iff     f      to understand how the correspondence extends to sets of probability
measures  consider the following example 
example      consider the embedding f of example      and let dx          where 
is as above  and    colorful          how do we guarantee that we reach the corresponding
conclusions from dx and dy   assume  for example  that dy contains some measure 
that does not correspond to either  or     e g   the measure that assigns probability    
to all four states  in this case  the conclusion pr colorful        holds in dx   because it
holds for both these measures  but the corresponding conclusion pr red blue green      
does not hold in dy   therefore  every probability measure in dy must correspond to some
measure in dx   conversely  every measure in dx must correspond to a measure in dy   for
suppose that there is no measure   dy corresponding to   then we get the conclusion
pr blue  red  green         from dy   but the corresponding conclusion pr colorful         
does not follow from dx   note that these two conditions do not imply that dy must be
precisely the set of measures corresponding to measures in dx   in particular  we might have
dy containing only a single measure  corresponding to   and at least one corresponding
to      e g   one with  red           blue        green         and  colorless        
   

firepresentation dependence

based on this example  we use the following extension to our definition of correspondence 
definition      we say that dx and dy correspond under f if for all   dy   there exists
a corresponding   dx  so that  s     f  s   for all s  x   and for all   dx   there
exists a corresponding   dy  
proposition      suppose that f is a faithful x y embedding  dx  x   and dy  y  
the following two conditions are equivalent 
 a  dx and dy correspond under f  
 b  for all   dx     iff dy    f      
proof  see appendix a   
to produce an inference procedure that is invariant under some x y embedding f  
we must ensure that for every kb   ix  kb   and iy  kb   correspond  at first glance  it
seems rather difficult to guarantee correspondence for every knowledge base  it turns out
that the situation is not that bad  in the remainder of this section  we show how  starting
with a correspondence for the knowledge base truethat is  starting with a correspondence
between ix  x   and iy  y  we can bootstrap to a correspondence for all kb s  using
standard probabilistic updating procedures 
first consider the problem of updating with objective information  the standard way
of doing this update is via conditioning  for a measure   x and an event s  x  define
 s to be the measure that assigns probability  w   s  to every w  s  and zero to all
other states  for a set of measures dx  x   define dx  s to be   s     dx    the
following result is easy to show 
proposition      let s  x be an event and let f be a faithful x y embedding  if  and
 correspond under f   then  s and  f  s  also correspond under f  
proof  almost immediate from the definitions  left to the reader   in any case  note that
this result follows from theorem     below  
clearly  the result extends to sets of measures 
corollary      if f is a faithful x y embedding  and dx and dy correspond under f  
then dx  s and dy  f  s  also correspond under f  
what if we want to update on a constraint that is not objective  the standard extension
of conditioning to this case is via relative entropy or kl divergence  kullback   leibler 
      
   while  a  implies  b  for arbitrary spaces  the implication from  b  to  a  depends on the restriction to
finite spaces made in this section  for suppose that x is the natural numbers n   f is the identity  dx
consists of all probability measures on n   and dy consists of all measures but that measure   such
that    n       n     if the language consists of finite boolean combinations of assertions of the form
pr s     for s  n   then it is easy to see that dx     iff dy     for all formulas   but clearly dx
and dy do not correspond under the identity map 

   

fihalpern   koller

definition      if  and   are measures on x  the relative entropy between   and  
p
denoted klx    k   is defined as xx    x  log    x   x    for a measure  on x and
a constraint   let   denote the set of measures   satisfying  for which klx    k  is
minimal 
intuitively  the kl divergence measures the distance from   to   a measure   satisfying  for which klx    k  is minimal can be thought of as the closest measure to
 that satisfies   if  denotes an objective constraint  then the unique measure satisfying  for which klx    k  is minimal is the conditional measure    kullback   leibler 
        that is why we have deliberately used the same notation here as for conditioning  
moreover  it is easy to show that klx    k      iff       it follows that if     then
     
given a set of measure dx  x and a constraint  on x   define dx   to be dx   
we can now apply a well known result  see  e g    seidenfeld         to generalize proposition     to the case of relative entropy 
theorem      let  be an arbitrary constraint on x   if f is a faithful x y embedding
and  and  correspond under f   then   and  f     also correspond under f  
proof  see appendix a   
again  this result clearly extends to sets of measures 
corollary      if f is a faithful x y embedding  and dx and dy correspond under f  
then dx   and dy  f     also correspond under f  
these results give us a way to bootstrap invariance  we construct an inference procedure that uses relative entropy starting from some set of prior probability measures  intuitively  these encode the users prior beliefs about the domain  as information comes in 
these measures are updated using cross entropy  if we design the priors so that certain invariances hold  corollary     guarantees that these invariances continue to hold throughout
the process 
formally  a prior function p on x maps x  x to a set p x  of probability measures
p  kb     p x  kb   note that
in x   define an inference procedure i p by taking ix
p  true    p x   so that when we have no constraints at all  we use p x  as the basis
ix
for our inference  most of the standard inference procedures are of the form i p for some
prior function p  it is fairly straightforward to verify  for example  that entailment is i p
for p x    x    this is because  as observed earlier   kb    if   kb    standard
bayesian conditioning  defined for objective knowledge bases  is of this form  where we take
p x  to be a single measure for each space x  more interestingly  it is well known  kullback
  leibler        that maximum entropy is i pu where pu  x  is the singleton set containing
only the uniform prior on x 
so what can we say about the robustness of i p to representation shifts  using proposition     and corollary      it is easy to show that if we want i p to be invariant under
some set f of embeddings  then we must ensure that the prior function has the right
correspondence property 
theorem      if f is a faithful x y embedding  then i p is invariant under f iff p x 
and p y   correspond under f  
   

firepresentation dependence

proof  see appendix a   
theorem     sheds some light on the maximum entropy inference procedure  as we
mentioned    me is precisely the inference procedure based on the prior function pu   the
corollary asserts that   me is invariant under f precisely when the uniform priors on x
and y correspond under f   this shows that maximum entropys lack of representation
independence is an immediate consequence of the identical problem for the uniform prior 
is there a class f of embeddings under which maximum entropy is invariant  clearly  the
answer is yes  it is easy to see that any embedding that takes the elements of x to  disjoint 
sets of equal cardinality has the correspondence property required by theorem      it follows
that maximum entropy is invariant under all such embeddings  in fact  the requirement that
maximum entropy be invariant under a subset of these embeddings is one of the axioms in
shore and johnsons        axiomatic characterization of maximum entropy   we remark
that paris        theorem       proves that maximum entropy satisfies a variant of his
atomicity principle  his invariance result is essentially a special case of theorem      
if we do not like the behavior of maximum entropy under representation shifts  theorem     provides a solution  we should simply start out with a different prior function 
if we want to maintain invariance under all representation shifts  p x  must include all
non extreme priors  i e   all the measures  on x such that  a  
         for all a such
that a 
     x    this set of priors gives essential entailment as an inference procedure  if 
however  we have prior knowledge as to which embeddings encode reasonable representation shifts  we can often make do with a smaller class of priors  resulting in an inference
procedure that is more prone to leap to conclusions  given a class of reasonable embeddings f  we can often find a prior function p that is closed under each f  f  i e   for
each measure   p x  and each x y embedding f  f we make sure that there is a
corresponding measure   p y    and vice versa  thus  we can guarantee that p has the
appropriate structure using a process of closing off under each f in f 
of course  we can also execute this process in reverse  suppose that we want to support
a certain reasoning pattern that requires leaping to conclusions  the classical example of
such a reasoning pattern is  of course  a default assumption of independence  what is the
most representation independence that we can get without losing this reasoning pattern 
as we now show  theorem     gives us the answer 
we begin by providing one plausible formulation of the desired reasoning pattern  for
a finite space x  we say that x       xn is the product decomposition of x if x  
x    xn and n is the largest number for which x can be written as a product in this way 
 it is easy to see that if x is finite  then this maximal product decomposition is unique  
a measure   x is a product measure on x if x       xn is the product decomposition
of x and there exist measures i  xi for i              n such that           n   that
q
is   u       un     ni   i  ui    if ui  xi   i              n  let p be the set of all product
measures on x  if p is the prior and the relative entropy rule is used to update the prior
given a knowledge base  then  p satisfies a form of minimal default independence  in
fact  it is easy to show that it satisfies the following stronger property 
   

fihalpern   koller

proposition       suppose that x       xn is the product decomposition on x and  for
each i              n  kb i is a constraint on xi   and si is a subset of xi   then
n
 

kb i  ip pr s          sn    

i  



n
y

pr si   

i  

proof  see appendix a   
theorem      shows that  p cannot be invariant under all embeddings  theorem    
tells us that it is invariant under precisely those embeddings for which p is invariant  these
embeddings can be characterized syntactically in a natural way  suppose that             n is
a partition of a finite set  of primitive propositions  note that a truth assignment to the
primitive propositions in  can be viewed as a crossproduct of truth assignments to the
primitive propositions in             n   under this identification  suppose that a set x of truth
assignments to  is decomposed as x       xn   where xi consists of truth assignments
to i   in that case  if p  j and q  r  k for some j    k  then true  p pr p  q   
pr p   pr q   but since since q and r are in the same subset  we do not have true  p pr r 
q    pr r   pr q   hence  p is not invariant under an interpretation i that maps p to
r and maps q to itself  intuitively  the problem is that i is crossing subset boundaries 
it is mapping primitive propositions that are in different subsets to the same subset  if we
restrict to interpretations thatpreserve subset boundaries  then we avoid this problem 
we can get a semantic characterization of this as follows  if the product decomposition
of x is x       xn and the product decomposition of y is y       yn   then f is
an x y product embedding if f is an x y embedding and there are xi  yi embeddings fi  
i              n  and f  hx            xn i    f   x         fn  xn    product embeddings capture
the intuition of preserving subset boundaries  elements in a given subset xi remain in
the same subset  yi   after the embedding  however  the notion of product embedding is
somewhat restrictive  it requires that elements in the ith subset of x map to elements in
the ith component of y   for i              n  we can still preserve default independence if the
components of a product are permuted  an g is a permutation embedding if there exists a
permutation  of             n  such that g hx            xn i    hx              x n  i 
theorem       the inference procedure ip is invariant under faithful product embeddings
and under permutation embeddings 
theorem     thus provides us with the basic tools to easily define an inference procedure
that enforces minimal default independence for constraints involving disjoint parts of the
language  while at the same time guaranteeing invariance under a large and natural class
of embeddings  given our negative result in theorem       this type of result is the best
that we could possibly hope for  in general  theorem     provides us with a principled
framework for controlling the tradeoff between the strength of the conclusions that can be
reached by an inference procedure and invariance under representation shifts 

   related work
as we mentioned earlier  there are two types of probabilistic inference  we partition our
discussion of related work along those lines 
   

firepresentation dependence

    probabilistic inference from a knowledge base
given the importance of representation in reasoning  and the fact that one of the main criticisms of maximum entropy has been its sensitivity to representation shifts  it is surprising
how little work there has been on the problem of representation dependence  indeed  to the
best of our knowledge  the only work that has focused on representation independence in
the logical sense that we have considered here prior to ours is that of salmon and paris 
salmon        defined a criterion of linguistic invariance  which seems essentially equivalent to our notion of representation independence  he tried to use this criterion to defend
one particular method of inductive inference but  as pointed out by barker in the commentary at the end of  salmon         his preferred method does not satisfy his criterion either 
salmon        then attempted to define a modified inductive inference method that would
satisfy his criterion but it is not clear that this attempt succeeded  in any case  our results
show that his modified method certainly cannot be representation independent in our sense 
as we said earlier  paris        considers inference processes  which given a constraint on
x   choose a unique measure satisfying the constraint  he then considers various properties
that an inference process might have  several of these are closely related to properties that
we have considered here   in describing these notions  we have made some inessential
changes so as to be able to express them in our notation  
 an x  inference process i is language invariant if all x  y  x and all constraints kb
and  on x   we have that kb  ix  iff kb  ixy   clearly language invariance
is a special case of robustness  paris shows that a center of mass inference process
 that  given a set a  x   chooses the measure that is the center of mass of a  is
not language invariant  on the other hand  it is well known that maximum entropy is
language invariant 
 an x  inference process i satisfies the principle of irrelevant information if for all
spaces x  y  x   constraints kb and  on x   and constraints  on y   we have
kb  ix  iff kb    ixy   again  this is a special case of robustness  since a
constraint  on y must be x conservative  paris shows that maximum entropy
satisfies this principle   he restricts the domain of the maximum entropy process to
closed convex sets  so that there is always a unique probability measure that maximizes
entropy  
 an x  inference process i satisfies the renaming principle if  whenever x and y are
finite spaces  g   x  y is an isomorphism  and f    x   y is the faithful embedding
based on g  in that f  s     g s    s  s    then for all constraints kb and  on x  
we have kb  ix  iff f   kb    iy f      clearly  the renaming principle is a special
case of representation independence  paris shows that a number of inference processes
 including maximum entropy  satisfy the renaming principle 
 an x  inference process i satisfies the principle of independence if  whenever x  y  
and z are in x   s  fx   t  fy   u  fz   and kb is the constraint pr u    
a  pr s u     b  pr t  u     c  where a      then kb   pr s  t  u     bc  ignoring the conditional probabilities  this is clearly a special case of minimal default
independence  paris and vencovska        show that maximum entropy is the unique
   

fihalpern   koller

inference process satisfying a number of principles  including renaming  irrelevant
information  and independence 
 an x  inference process i satisfies the atomicity principle if  for all x  y            yn in
x   whenever f   is an embedding from        to x  and f is the obvious extension of
f   to an embedding from to         y          yn to x  y          yn   then for all
constraints kb and  on      y     yn   we have kb  ix  iff f   kb    iy f     
clearly atomicity is a special case of representation independence  paris shows that
there is no inference process that satisfies atomicity  the argument is similar in spirit
to that used to prove theorems      and       but much simpler  since inference
processes return a unique probability measure  not a set of them 
more recently  jaeger         building on our definitions  has examined representation
independence for general nonmonotonic logics  he considers representation independence
with respect to a collection of transformations  and proves results about the degree to
which certain nonmonotonic formalisms  such as rational closure  lehmann   magidor 
       satisfy representation independence 
another line of research that is relevant to representation independence is the work
on abstraction  giunchiglia   walsh        nayak   levy         although the goal of
this work is again to make connections between two different ways of representing the same
situation  there are significant differences in focus  in the work on abstraction  the two ways
of representing the situation are not expected to be equivalent  rather  one representation
typically abstracts away irrelevant details that are present in the other  on the other hand 
their treatment of the issues is in terms of deductive entailment  not in terms of general
inference procedures  it would be interesting to combine these two lines of work 
    bayesian probabilistic inference
bayesian statistics takes a very different perspective on the issues we discuss in this paper 
as we discussed  the bayesian approach generally assumes that we construct a prior  and use
standard probabilistic conditioning to update that prior as new information is obtained  in
this approach  the representation of the knowledge obtained has no effect on the conclusions 
two pieces of information that are semantically equivalent  denote the same event  will have
precisely the same effect when used to condition a distribution 
in this paradigm  our analysis is more directly related to the step that precedes the
probabilistic conditioningthe selection of the prior  when we have very specific beliefs
that we want to encode in a prior distribution  as we do  for example  when constructing
a bayesian network   we design our prior to reflect these beliefs in terms of the vocabulary
used  for example  if we have a particular distribution in mind over the location of an object 
we will encode it one way when representing the space in terms of cartesian coordinates 
and in another way when using polar coordinates  in effect  we can view the representation
transformation as an embedding f   and the two priors as corresponding under f   in the
sense of definition      thus  the design of the prior already takes the representation into
account 
on the other hand  when we are trying to construct an uninformed prior for some class
of problems  the issue of representation independence becomes directly relevant  indeed 
   

firepresentation dependence

most of the standard problems with maximum entropy arise even in the simple case when
we simply do bayesian conditioning starting with a uniform prior over our space 
a standard approach in bayesian statistics is to use the invariance under certain transformations in order to define an appropriate uninformed prior  for example  we might
want a prior over images that is invariant to rotation and translation  in certain cases 
once we specify the transformation under which we want a measure to be invariant  the
measure is uniquely determined  jaynes        kass   wasserman         in this case  the
argument goes  the uniquely determined measure is perforce the right one  this idea of
picking a prior using its invariance properties is in the same spirit as the approach we take
in section    indeed  as this approach simply uses standard probabilistic conditioning for
objective information  such as observations   the bayesian approach with an uninformed
prior invariant to a set of embeddings is  in a sense  a special case  however  our approach
does not force us to choose a unique prior  rather  we allow the use of a set of prior
distributions  allowing us to explore a wider spectrum of inference procedures 
this approach is also related to the work of walley         who observes that representation independence is an important desideratum in certain statistical applications involving
multinomial data  walley proposes the use of sets of dirichlet densities to encode ignorance
about a prior  and shows that this approach is representation independent in its domain of
application 

   conclusions
this paper takes a first step towards understanding the issue of representation dependence in probabilistic reasoning  by defining notions of invariance and representation independence  showing that representation independence is incompatible with drawing many
standard default conclusions  and defining limited notions of invariance that might that
allow a compromise between the desiderata of being able to draw interesting conclusions
 not already entailed by the evidence  and representation independence  our focus here
has been on inference in probabilistic logic  but the notion of representation independence
is just as important in many other contexts  our definitions can clearly be extended to
non probabilistic logics  as we mentioned  jaeger        has obtained some results on representation independence in a more general setting  but there is clearly much more that can
be done  more generally  it would be of interest to understand better the tension between
representation independence and the strength of conclusions that can be drawn from an
inference procedure 

acknowledgments
thanks to ed perkins for pointing us to  keisler   tarski        and  in particular  the
result that a countably additive probability measure defined on a subalgebra of an algebra
f could not necessarily be extended to a countably additive probability measure on f 
thanks to the reviewers of the paper for their perceptive comments and for pointing out
 horn   tarski         much of halperns work on the paper was done while he was at
the ibm almaden research center  his recent work has been supported by nsf under
   

fihalpern   koller

grant iri          and iis         and onr under grant n                 some of
kollers work was done at u c  berkeley  her research was sponsored in part by the air
force office of scientific research  afsc   under contract f         c       and by a
university of california presidents postdoctoral fellowship  daphne kollers later work
on the paper was supported through the generosity of the powell foundation  and by onr
grant n                 a preliminary version of this appears in proceedings of ijcai    
pp           

appendix a  proofs
a   proofs for section  
proposition      let   be a relation on probabilistic constraints on x for which the
properties reflexivity  left logical equivalence  right weakening  infinitary and  and consistency hold for all kb in the domain of      that is  if kb is in the domain of     in
that kb    for some   then kb   kb   and so on   then   is  i for some x inference
procedure i 
proof  define i as follows  if a  x   kb is in the domain of     and a     kb   x
for some statement kb   then a is in the domain of i and i a         x   kb     
note that by left logical equivalence  this is well defined  since if a     kb     x   then
     x   kb            x   kb        if a      kb   x for some statement kb   then a is
not in the domain of i  it remains to check that i is an x inference procedure  i e   that
i a   a and that i a     iff a    for all a in the domain of i   and that      i  
to check that i is an x inference procedure  suppose that a is in the domain of i  thus 
a     kb   x by reflexivity  it easily follows that i   kb   x      kb   x   next suppose that
i   kb   x       it follows that      x   kb         thus      kb        false  by
the infinitary and rule  we must have kb  i false  by the consistency rule  it follows
that   kb   x     thus  i is indeed an x inference procedure  finally  note that if kb   
then  by definition of i  i   kb   x        x   so kb  i   for the opposite inclusion  note
that if kb  i   then     kb          thus  by the infinitary and rule  it follows that
kb   i  
a   proofs for section  
to prove theorem      we need the following lemma 
lemma a    given two spaces x  and x    measures     x   fx    and     x   fx     
and subsets s   fx  and s   fx  such that    s         s     there exists a measure
    x  x   fx  x    such that  xi   i   for i         and    s   s          
proof  for a  b  fx   fx    define
   a  b        a  s      b  s       s           a  s      b  s       s     
where we take    a  s      b  s       s        if    s        and take    a  s      b 
s       s        if    s         extend to disjoint unions of such sets by additivity  since
    if a and b are sets  we use the notation a  b to denote the set  a  b    a  b  

   

firepresentation dependence

all sets in fx  x  can be written as disjoint unions of sets of the form a  b  fx   fx   
this suffices to define     to see that   is actually a measure  note that    x  y    
   s         s         additivity is clearly enforced by the definition  finally  to see that
  has the desired properties  suppose that    s         and    s           the argument is
easier if this is not the case  we leave details to the reader   then
 x   a       a  y        a  s      s       s         a  s     s       s   
     a  s         a  s         a  
since    s         s    by assumption  and so    s         s     
 x   b       x  b       s      b  s       s         s      b  s      s   
     b  s         b  s         b  
this completes the proof 
theorem      if  ix   x  x   is a robust x  inference procedure that satisfies di   di  
and di   then ix is essentially entailment for all x  x  
proof  suppose that  ix   x  x   is robust and ix is not essentially entailment for x  x  
then there must be a constraint kb on x and a set s  fx such that kb  i    pr s   
 and kb       pr s     thus  there must be some  
       such that kb pr s    
is consistent  we can assume without loss of generality that      otherwise we can replace
s by s  
we first construct a space y   x that has subsets u            un with the following properties 
 a  there is no measure   y  such that  ui       for all i           n 
 b  for each i  there is some measure  i  y  such that  i  ui       and  i  uj      for
all j    i 
we proceed as follows  choose n and d such that     d      n       d n     by
assumption  there exists a y   x such that  y      n   n  d    without loss of generality 
we can assume that y  consists of all tuples of the form  a            ad    where the ai s are all
distinct  and between   and n  let ui be consist of all the tuples in y  that have i somewhere
in the subscript  it is easy to see that there are d n       n  d   such tuples  suppose that
 is a probability measure in y    it is easy to see that  u            un     d  since each
tuple in y  is in exactly d of the ui s and so gets counted exactly d times  and the sum of the
probabilities of the tuples is    thus  we cannot have  ui     d n for all i  and  a fortiori 
we cannot have  ui      for all i   this takes care of the first requirement  next  consider
a probability distribution  i that makes all the tuples making up ui equally probable  and
gives all the other tuples probability    then it is easy to see that  i  ui        moreover 
since it is straightforward to check that there are exactly d d     n       n  d   tuples
in ui  uj for j    i  we have  i  uj      d d     n       n  d     d n       n  d     
 d      n      this takes care of the second requirement 
by assumption  there is also a measurable space y  x such that  y        suppose
that y    y  y      let z   x n  y   y n   where the n is the same as the n chosen in the
construction of y    again  by assumption  z  x   for i              n 
   

fihalpern   koller

 if a  x  let ai   x i   a  x ni  y   y n  z 
 let kb i      z   xi  kb   
 let yi be the subset of y n where the ith copy of y is replaced by  y  
 let vi be the subset of z of the form x n  ui  yi  where u            un are the subsets
of yi constructed above  
let  be the following constraint on z  
kb           kb n  pr s   v                pr sn  vn       
let xi denote the ith copy of x in z  that is  for ease of exposition  we view z as
being of the form x       xn  y   y m   although all the xi s are identical  since it is
helpful to be able to refer to a specific xi   we claim that  is xi  conservative over kb  
for i              n  thus  we must show that proj xi    kb i    z       kb   x   it is immediate
that proj xi    kb i    z      kb   x   for the opposite inclusion  suppose that     kb   x  
we must show that there exists some     kb i    z such that xi     we proceed as
follows 
let    be a measure in y  such that     ui       and     uj       for j    i  by
construction of the uj s  such a measure must exist  for j              n   let  j be the measure
in y such that  i  y     s  and if j    i  then  j  y         uj    and  j  y           j  y   
let   be the measure on y   y n that is the crossproduct of                    n   that is 
   t       tn         t          n  tn    by construction     vj      for j    i and
   vi      s  
by assumption  there is a measure    x such that      kb  pr s      we
now proceed inductively to define a measure k  x k y  y n such that  a  pr  s  
v             sk  vk          b  jy     and jxj    for j              k  we define        
for the inductive step  we simply apply lemma a    finally  we take  to be n   our
construction guarantees that x j     hence that     kb j   in addition  the construction
guarantees that     pr s   v                pr sn  vn        hence       as desired 
it follows from di   di   and di  that  is in the domain of iz   since kb i   is
equivalent to   it follows that kb i   is also in the domain of iz   now  by robustness  for
any constraint  on xi   we have kb i   i  iff kb i  i   since kb i  i pr si      and
kb i   is equivalent to   it follows that   i pr si      for i              n  by the and rule
 proposition       it follows that   i pr s               pr sn       since     pr  s  
v      sn  vn         it easily follows that   i pr u               pr un       but our
construction guarantees that pr u           pr un      is inconsistent  thus    i false 
by robustness  it follows that kb i  i false  but this can happen only if kb    false  which
implies that kb      pr s     contradicting our original assumption 
a   proofs for section  
to prove lemma      it is useful to first prove two additional results 
lemma a    if f is an x y embedding  then f  x    y and f       
   

firepresentation dependence

proof  suppose that f is an x y embedding  we first show that f        from the
definition of embedding  it follows that f      f  x      f  x   f     thus  f     f  x  
but the definition of embedding also implies that f      f  x    f  x   thus  we have
f  x   f  x   this can happen only if f  x    y and f      f  x     

lemma a    if f is a faithful x y embedding  then
 a  for any   x   there is a measure   y such that  corresponds to  
 b  for any   y   there is a measure   x such that  corresponds to  
proof  to prove  a   consider the algebra of subsets of y the form f  s   for s  fx  
define a function    on the algebra via     f  s      s   this mapping is well defined 
for if f  s    f  t    then faithfulness guarantees that s   t   moreover     is a probability
measure on the algebra  to see this  note by lemma a   that     y         f  x      x      
moreover  if f  s   f  t       then  by definition of embedding  f  s  t      and so  since
f is faithful  s  t     for otherwise f  s  t     f    by lemma a    but s  t      
thus 
    f  s   f  t          f  s  t       s  t      s     t         f  s         f  t    
as shown by horn and tarski         it is possible to extend    to a probability measure 
on fy     by construction  we have that  corresponds to  
to prove  b   we use a very similar process  define a function  on the algebra of sets
s  x via  s     f  s    it is easy to see that  is already a probability measure in x  
which by construction corresponds to  
we can now prove lemma     
lemma      an x y embedding f is faithful if and only if for all constraints kb and  
we have kb     iff f   kb      f     
proof  suppose that f is faithful  to show that kb     iff f   kb      f      we must
show that   kb   x      x iff   f   kb    y    f     y   the only if direction is immediate
from the definition of f    to prove the if direction  suppose not  then there must exist
some     kb   x      x such that f        f      y   let  be some probability measure
that corresponds to   since   f      f      there must be some        x such that
  f        since        there must be some s  fx such that    s      s   since
  f      f        we must have both  f  s      s  and  f  s        s   but this is a
contradiction  this completes the proof of the if direction 
for the converse  suppose we have kb     iff f   kb      f     for all kb and   given
s  t  fx   we have the following chain of equivalences 
    it is critical for this result that we are working with finitely additive measures  there may not be a
countably additive measure  extending      even if    is countably additive  for example  take fy  to be
the borel sets on        and take fy to be all subsets of         let    be lebesgue measure  it is known
that  under the continuum hypothesis  there is no countably additive measure extending    defined on
all subsets of         ulam         see  keisler   tarski        for further discussion  

   

fihalpern   koller

st
iff pr s         pr t      
iff f   pr s          f   pr t         by assumption 
iff pr f  s          pr f  t         by definition of f   
iff f  s   f  t   
thus  f is faithful 
proposition      let f be a faithful x y embedding  then the following statements are
equivalent 
 a   and  correspond under f  
 b  for all formulas        iff     f     
proof  we first show that  a  implies  b   so suppose that  and  correspond under f  
the only if direction of  b  is trivial  if      then   f      f      since f is faithful 
for the if direction  we proceed much as in the proof of lemma      assume that     f    
but that        since   f      by definition of f  there must be some        x such that
  f        since       whereas        we must have         hence  there must be some
s  fx such that  s        s   since   f      f        it follows that  f  s      s 
and that  f  s        s   which gives the desired contradiction 
we now show that  b  implies  a   assume by contradiction that  and  do not
correspond under f   then there must be some event s  fx such that  s      f  s   
let p    s  and let  be the constraint pr s    p  then       whereas      f     
providing the desired contradiction 
theorem       if an x  inference procedure is robust that satisfies di   di   and di  
then it is representation independent 
proof  suppose that  ix   x  x   is a robust x  inference procedure  we want to show
that it is representation independent  so suppose kb    are constraints on x and f is an
x y embedding  for some x  y  x   we want to show that kb  ix  iff f   kb    iy f     
let  be the following constraint on xy  
   f        kb  f   kb    
we claim that  is x conservative over kb and y  conservative over f   kb    thus  we must
show that proj x    kb    xy       kb   x and proj y    f   kb      xy       f   kb    y  
we show that proj x    kb    xy       kb   x here  the argument that proj y    f   kb   
  xy       f   kb    y is almost identical 
clearly if     kb    xy then x    kb   x   so proj x    kb    xy      kb   x  
for the opposite inclusion  suppose that     kb   x   we want to find a measure    
      let     be any measure in f     and let     
  kb    xy   such that x
xy
      to
be the crossproduct of  and       that is      a  b     a      b   clearly x
see that       kb    xy    it clearly suffices to show that         but since  and    
correspond under f   it is immediate from proposition     that     kb iff        f   kb  
and      iff        f      thus        as desired 
   

firepresentation dependence

now suppose that kb  ix   by di  and di   kb   is in the domain of ixy   by
robustness  kb    ixy   thus  i   kb    xy        xy   since i   kb    xy   
  kb    xy      f      xy   it follows that i   kb    xy      f      xy  
moreover  kb   is equivalent to f   kb      so i   f   kb      xy      f      xy  
i e   f   kb      ixy f      by di   f   kb   is in the domain of iy   since  is y conservative over f   kb    the robustness of  ix   x  x   implies that f   kb    iy f     
the opposite implication  if f   kb    iy f     then kb  ix   goes the same way  thus 
 ix   x  x   is representation independent 
next  we turn our attention to theorems      and       both of these results follow in
a relatively straightforward way from one key proposition  before we state it  we need some
definitions 
definition a    we say that a constraint kb on x depends only on s            sk  fx
 the sets s            sk are not necessarily disjoint  if  whenever      x agree on s            sk  
then     kb iff      kb  
for example  if kb has the form pr s           pr s          then kb depends only
on s  and s    similarly  if kb has the form pr s    s           then kb depends only on
s  and s   
definition a    given s            sk  fx   an atom over s            sk is a set of the form
t          tk   where ti is either si or si  
proposition a    suppose that  ix   x  x   is an x  inference procedure and  for some
x  x   there exist s  s            sk  fx and a consistent constraint kb on x that depends
only on s            sk   such that the following two conditions are satisfied 
 both t  s and t  s are nonempty for every nonempty atom t over s            sk  
 kb  ix    pr s      where either      or      
then  ix   x  x   is not representation independent 
proof  suppose  by way of contradiction  that  ix   x  x   is a representation independent
inference procedure but nevertheless  for some x  x   there exists sets s  s            sk  fx
and a knowledge base kb that satisfies the conditions above  for some     assume that
      a similar argument can be used to deal with the case that       
let t            tm be the nonempty atoms over s            sk   choose n such that   n    
our goal is to find a collection f            fn of embeddings of x into some y  x such that
each of these embeddings has the same effect on kb   but such that the sets fj  s  are disjoint 
since kb  ix pr fj  s      for j              n   and fj  kb     f   kb   for j              n   it
will follow that f   kb    iy pr fj  s      for j              n   a contradiction  we proceed
as follows 
by assumption  there exists a set z in x such that  z    m n   let y   x  z 
since x is closed under crossproducts  y  x   suppose that z    z            zm n    and let
zi    zn  i               zn i    for i              m   thus  the zi s partition z into m disjoint sets 
each of cardinality n   let bi   x  zi   and let bij   x   zn  i   j    for j              n  
it is easy to see that we can find faithful x y embeddings f            fn such that
   

fihalpern   koller

   fj  ti     bi   for i              m   j              n  
   fj  ti  s    bij   for i              m   j              n  
notice that we need the assumption that both ti s and ti s are nonempty for t            tm
 that is  for each nonempty atom over s            sk   to guarantee that we can find such faithful
embeddings  for if ti  s     then since fj is an embedding  f  ti  s        bi   and if
ti  s     then fj  ti  s    fj  ti    f  ti  s       which means that bi   bij   again
inconsistent with the construction 
it is easy to check that  since kb depends only on s            sk   fj  kb   depends only
on fj  s             fj  sk    for j              n   we next show that fj  si   is independent of j 
that is  fj  si     fj    si   for    j  j    n   notice that for h              k  we have that
fj  sh     ti sh fj  ti      i ti sh   bi   thus  fj  sh   is independent of j  as desired  since
fj  kb   depends only on fj  s             fj  sk    it too must be independent of j  let kb  be
f   kb    which  as we have just observed  is identical to f   kb            fk  kb    
since  by assumption   ix   x  x   is representation independent  and kb  ix p r s   
  we have that kb   iy pr fj  s       for j              n   thus  kb   iy pr f   s    
         pr fn  s       but note that  by construction  fj  s     i ti s    bij   thus  the
sets fj  s  are pairwise disjoint  since      n   we cannot have n disjoint sets each with
probability greater than   thus  kb   iy false  but kb is consistent  so kb    fj  kb  
must be as well  thus  iy  kb         by assumption  but this contradicts our conclusion
that kb   iy false  thus   ix   x  x   cannot be representation independent 
we can use proposition a   to help prove theorem      
theorem       if  ix   x  x   is a representation independent x  inference procedure
then  for all x  x   ix is essentially entailment for all objective knowledge bases in its
domain 
proof  suppose  by way of contradiction  that  ix   x  x   is representation independent
but ix is not essentially entailment for some x  x and objective knowledge base kb  
then there must be some set s  fx such that kb  ix    pr s     and kb      
pr s     without loss of generality  we can assume that kb has the form pr t       for
some t  fx   moreover  we can assume that if t      then t has a nonempty  measurable
strict subset   for otherwise  choose y    y  y      x and consider the space x     x  y  
by assumption  x    x   let f be the x y embedding that maps u  fx to u  y   since
i is representation independent  we have that pr t  y        i    pr s  y       and
t   y   t  y   
if t is nonempty  let z be any nonempty  measurable strict subset of t  which exists by
assumption   otherwise let z be the empty set  let u be the set  t  s    t  z   notice
that s t   u t   moreover  since  for any set v   pr t        pr v     pr v t   is valid 
it follows from reflexivity and right weakening that kb  ix pr v     pr v  t    thus 
kb  ix pr s    pr s  t     pr u  t     pr u    it follows that kb  ix    pr u      
we now want to apply proposition a    note that kb depends only on t   thus  we
must show that t  u and t  u are nonempty  and if t is nonempty  then t  u and
t  u are as well  as we observed above  t  u   t  s  thus  if t  u     then t  s 
contradicting our assumption that kb  i pr s       it is easy to see that t  u   t  s 
again  we cannot have t  u     for then t  s  contradicting our assumption that
   

firepresentation dependence

kb  i pr s       by construction  t  u   t  z   z  by assumption  if t      then
z      finally  t  u   t  z  again  by construction  this is a nonempty set if t      it
now follows from proposition a   that  ix   x  x   is not representation independent 
corollary       if  ix   x  x   is a representation independent x  inference procedure 
then for all x  x   if kb is an objective knowledge base putting constraints on x   and
kb  ix    pr s     for some     and      then      and      
proof  assume the hypotheses of the corollary hold  since kb is objective  it is of the
form pr t       for some t  fx   then there are three possibilities  either     t  s     
t  s  or     both t  s and t  s are nonempty  if     holds  we have kb    pr s      
while if     holds  we have kb    pr s       thus  both     and     are incompatible
with kb  ix    pr s      on the other hand  if     holds  it is easy to see that for
all   pr s     is consistent with kb  since there is a probability measure that assigns
probability  to t  s and probability     to t  s   since kb  ix    pr s      by
theorem       we must have kb      pr s     it follows that the only choices of 
and  for which this can be true are      and      
theorem       if  ix   x  x   is an x  inference procedure that enforces minimal
default independence and satisfies di   then ix is not representation independent 
proof  suppose that  ix   x  x   is an x  inference procedure that enforces minimal
default independence and satisfies di   choose x    x  x     x and let kb be     
pr x        by assumption  x  x  x   we can view kb as a constraint on xx  
in this case  it should be interpreted as      pr  x   x        by di   kb is is the
domain of ixx   note that kb is equivalent to the constraint      pr x          by
minimal default independence  we have that kb  ixx pr  x  x     pr x  x    and that
kb  ixx pr  x    x       pr x   x     applying straightforward probabilistic reasoning 
we get that kb  ixx pr   x  x    x    x             we now apply proposition a    taking
s to be   x  x    x    x     and s   to be   x  x    x  x      note that kb depends only on s    
it is almost immediate from the definition of s and s   that all of s  s     s  s     s  s    
and s  s   are nonempty  thus  by proposition a     ix   x  x   is not representation
independent 
    x  x   is a representationlemma       let x consist of only countable sets  then  ix
independent x  inference procedure 

proof  as we said in the main part of the text  it easily follows from proposition     that
  is an inference procedure for all x  x   since it is easily seen to have the five propix
erties described in the proposition  to see that i   is representation independent  suppose
that f is a faithful x y embedding  for x  y  x   clearly kb is objective if and only
if f   kb   is objective  if kb is not objective  then it is easy to see that kb  i    iff
f   kb    i   f      since  i   reduces to entailment in this case  so suppose that kb is objective and has the form pr t        for some t  fx   then kb  i    iff kb kb        by
lemma      this holds iff f   kb  f   kb        f      on the other hand  f   kb    i   f    
iff f   kb     f   kb        f     thus  it suffices to show that f   kb    f   kb        f    
iff f   kb     f   kb        f      it is easy to show that  f   kb     implies f   kb      so
that if f   kb    f   kb        f     then f   kb     f   kb        f      it is not necessarily
   

fihalpern   koller

the case that f   kb     implies  f   kb       for example  consider the embedding described
in example      in that case  if kb is the objective knowledge base pr colorful       
kb   is empty  and hence so is f   kb      while  f   kb     includes constraints such as
    pr green       nevertheless  suppose that f   kb     f   kb        f     and  by way
of contradiction  there is some  such that     f   kb    f   kb      f      choose 
such that   f      then  and  correspond  so     kb  kb      it is easy to
show that there exists     f     such that         s      for all nonempty subsets of s of
f  t    to see this  note that if  x        then it suffices to ensure that     f  x      x  and
    y       for all y in f  x   since y is countable  this is straightforward  since  and   
correspond  we must have that       f      f   kb    by construction         f   kb      
this contradicts the assumption that f   kb     f   kb        f     
lemma       suppose that x consists only of measure spaces of the form  x   x    where
    x  x   is a representation independent x  inference procedure 
x is finite  then  ix
proof  suppose that x  y  x   kb and  are constraints on x   and f is an x y
embedding  we must show that kb  i    iff f   kb    i   f      for the purposes of this
x
y
proof  we say that a subset a of x is interesting if there exists some s  fx such that
a      x    s         it is easy to see that if kb is interesting then f   kb   is
interesting  the converse is also true  given our assumption that x consists of only finite
spaces where all sets are measurable  for suppose that f   kb   is interesting  then there is
a set t  y such that f   kb        y    t          let a    s    x   f  s      t   
since x is finite  so is a  it easily follows that s   a  a    clearly if  s        then
f      f   kb    so     kb   x   thus    kb   x     x    s         on the other
hand  if   kb   then f      f   kb    thus  if   f      since s  a  it must be the
case that  s     f  s     t         thus    kb   x     x    s         it
follows that kb is equivalent to pr s        and so must be interesting   we must also
have t   f  s   although this is not needed for the result  
if kb is not interesting  then kb  i    iff kb     iff f   kb      f      since entailment
x
is representation independent  iff f   kb    i     on the other hand  if kb is interesting 
y
then kb is equivalent to pr s       for some s  x  and f   kb   is equivalent to
pr f  s         moreover  kb  i    iff pr s           iff pr f  s           f     iff
x
f   kb    i     thus  we get representation independence  as desired 
y

a   proofs for section  
proposition      suppose that f is a faithful x y embedding  dx  x   and dy  y  
the following two conditions are equivalent 
 a  dx and dy correspond under f  
 b  for all   dx     iff dy    f     
    this is not in general true if x is infinite without the additional requirement that f  i ai     i f  ai  
for arbitrary unions 

   

firepresentation dependence

proof  to prove that  a  implies  b   assume by way of contradiction that  for some  
dx     but dy     f      then there is some   dy such that      f      let   dx be
a measure corresponding to   then  by proposition      we have that        the desired
contradiction  the proof for the other direction of  a  is identical 
to prove that  b  implies  a   first consider a measure   dx   we must find a   dy
such that  corresponds to   suppose that x    x            xn    recall that we are restricting
to finite spaces in section    and that  xi     ai   i              n  let  be the constraint
ni   pr  xi      ai   by our assumptions about the language  this constraint is in the
language  clearly     x       since   dx   we know that dx       hence  dy    
f      so that there exists   dy such that    f      hence   f       f        by
definition of f     corresponds to  
now consider a measure   dy   and let  be the measure in x that corresponds
to   assume by way of contradiction that    dx   taking  as above  it follows that
dx     and  therefore  by assumption  dy    f      thus      f      but      and 
by assumption   and  correspond  this contradicts proposition     
theorem      let  be an arbitrary constraint on x   if f is a faithful x y embedding
and  and  correspond under f   then   and  f     also correspond under f  
proof  assume that  and  correspond under f   recall that we are assuming in this section
that x is a finite space  let x    x            xn    let yi   f  xi    given any distribution
     y   define i          yi and let  f            denote the unique     x such that
     f        
now suppose that       define     y to be the measure such that
    y       xi    i  y  
where i is the index such that y  yi   since i    yi   it follows that i  yi        thus 
    yi      xi    and    leaves the relative probabilities of elements within each yi the same
as in   it is easy to verify that    and   correspond  hence  by proposition            f     
we claim that      f      to show that  we need show only that kly     k  is minimal
among all kly      k  such that        f      it follows from standard properties of relative
entropy  cover   thomas        theorem        that for all      y   we have
kly      k    klx   f           k f          

n
x

kly  i   ki   

   

i  

note that i   i    so kly  i  ki        for i              n  thus  it follows from     that
kly     k    klx    k  
now  let      y be such that        f     and let       f            since     and   
correspond under f   it follows from proposition     that         using     once again  we
have that
kly      k    klx     k   

n
x
i  

 klx     k  
   

kly  i   ki  

fihalpern   koller

but since       we know that klx    k   klx     k   hence we conclude that
kly      k   kly     k  
so that      f     
theorem      if f is a faithful x y embedding  then i p is invariant under f iff p x 
and p y   correspond under f  
proof  suppose that f is a faithful x y embedding  by definition  i p is invariant under
f iff  for all kb     we have
kb   i p  iff f   kb     i p f     

   

by definition of i p       holds iff
p x  kb      x iff p y   f   kb      f      y for all kb    

   

by proposition          holds iff p x  kb and p y   f   kb   correspond for all kb   by
corollary      if p x  and p y   correspond  then p x  kb and p y   f   kb   correspond
for all kb   on the other hand  if p x  kb and p y   f   kb   correspond for all kb   then
p x  and p y   must correspond  simply take kb   true and observe that p x  kb    
p x  and p y   f   kb     p y   
proposition       suppose that x       xn is the product decomposition on x and 
for each i              n  kb i is a constraint on xi   and si is a subset of xi   then
n
 
i  

kb i  ip pr s          sn    


n
y

pr si   

i  

proof  if kb i is a satisfiable constraint on xi   for i              n  then there exist product
v
measures on x satisfying the constraints ni   kb i   these product measures are precisely
vn
the measures in p    i   kb i    since each of these measures satisfies pr s          sn    
qn
i   pr si   by assumption  the conclusion holds in this case  if any constraint kb i is not
satisfiable  then the result trivially holds 
theorem       the inference procedure ip is invariant under faithful product embeddings and under permutation embeddings 
proof  suppose that f is a faithful x y product embedding  x       xn is the product
decomposition of x  and y       yn is the product decomposition of y   to show that
p is invariant under f   it suffices to show that p  x  and p  y   correspond under f  
supposethat   p  y    then           n   where i is a measure on xi   i              n 
moreover  since f is a product embedding  there exist f            fn such that f   f       fn  
let i  fi  i    for i              n  it is easy to check that           n  f     
conversely  suppose that   p  y    then           n   where i  yi for
i              n  define   xi by setting i  s    i  fi  s    since fi is a faithful xi  yi
   

firepresentation dependence

embedding  is easy to check that i  xi and that i  fi  i    thus    f      this
completes the proof that p is invariant under faithful x y product embeddings 
the argument that p is invariant under faithful x x permutation embeddings is
similar  and easier   we leave details to the reader 

references
bacchus  f          representing and reasoning with probabilistic knowledge  mit press 
cambridge  mass 
bacchus  f   grove  a  j   halpern  j  y     koller  d          from statistical knowledge
bases to degrees of belief  artificial intelligence                 
cover  t  m     thomas  j  a          elements of information theory  wiley  new york 
enderton  h  b          a mathematical introduction to logic  academic press  new york 
giunchiglia  f     walsh  t          a theory of abstraction  artificial intelligence          
       
goldszmidt  m   morris  p     pearl  j          a maximum entropy approach to nonmonotonic reasoning  ieee transactions of pattern analysis and machine intelligence 
               
halpern  j  y     koller  d          representation dependence in probabilistic inference 
in proc  fourteenth international joint conference on artificial intelligence  ijcai
     pp           
horn  a     tarski  a          measures in boolean algebras  transactions of the ams 
               
jaeger  m          representation independence of nonmonotonic inference relations  in
principles of knowledge representation and reasoning  proc  fifth international
conference  kr      pp         
jaynes  e  t          prior probabilities  ieee transactions on systems science and
cybernetics  ssc           
jaynes  e  t          where do we stand on maximum entropy   in levine  r  d     tribus 
m   eds    the maximum entropy formalism  pp         mit press  cambridge 
mass 
kahneman  d   slovic  p     tversky  a   eds            judgment under uncertainty 
heuristics and biases  cambridge university press  cambridge new york 
kass  r  e     wasserman  l          formal rules for selecting prior distributions  a review
and annotated bibliography  tech  rep  technical report       dept  of statistics 
carnegie mellon university 
   

fihalpern   koller

keisler  j     tarski  a          from accessible to inaccessible cardinals  fundamenta
mathematica             
kraus  s   lehmann  d     magidor  m          nonmonotonic reasoning  preferential
models and cumulative logics  artificial intelligence             
kullback  s     leibler  r  a          on information and sufficiency  annals of mathematical statistics           
lehmann  d     magidor  m          what does a conditional knowledge base entail  
artificial intelligence          
nayak  p  p     levy  a  y          a semantic theory of abstractions  in proc  fourteenth
international joint conference on artificial intelligence  ijcai      pp         
paris  j  b          the uncertain reasoners companion  cambridge university press 
cambridge  u k 
paris  j     vencovska  a          a note on the inevitability of maximum entropy  international journal of approximate reasoning                
pearl  j          probabilistic reasoning in intelligent systems  morgan kaufmann  san
francisco 
salmon  w          vindication of induction  in feigl  h     maxwell  g   eds    current
issues in the philosophy of science  pp          holt  rinehart  and winston  new
york 
salmon  w          on vindicating induction  in kyburg  h  e     nagel  e   eds   
induction  some current issues  pp        wesleyan university press  middletown 
conn 
seidenfeld  t          entropy and uncertainty  in macneill  i  b     umphrey  g  j   eds   
foundations of statistical inferences  pp          reidel  dordrecht  netherlands 
shore  j  e     johnson  r  w          axiomatic derivation of the principle of maximum entropy and the principle of minimimum cross entropy  ieee transactions on
information theory  it              
ulam  s          zur masstheorie in der allgemeinen mengenlehre  fundamenta mathematicae             
walley  p          inferences from multinomial data  learning about a bag of marbles 
journal of the royal statistical society  series b               discussion of the
paper by various commentators appears on pp       

   

fi